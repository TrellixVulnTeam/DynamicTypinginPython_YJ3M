commit 3a0f3453ccd45c037568af925250caa54cae59a4
Author: andenixa <37909402+andenixa@users.noreply.github.com>
Date:   Fri Jun 15 01:34:39 2018 +0300

    model update (#417)
    
    New, functional Original 128 model

diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
index dfb7e13..65d4164 100644
--- a/plugins/Model_OriginalHighRes/Model.py
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -4,22 +4,26 @@
 # source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb
 
 
-import os
-import sys
+import enum
+
+import warnings
+warnings.filterwarnings("ignore", category=FutureWarning)
 
 from keras.initializers import RandomNormal
 from keras.layers import Input, Dense, Flatten, Reshape
 from keras.layers import SeparableConv2D, add
 from keras.layers.advanced_activations import LeakyReLU
 from keras.layers.convolutional import Conv2D
+from keras.layers.core import Activation
 from keras.models import Model as KerasModel
 from keras.optimizers import Adam
 from keras.utils import multi_gpu_model
-
 from lib.PixelShuffler import PixelShuffler
+import os
+import sys
 
 from . import __version__
-from keras.layers.core import Activation
+from .instance_normalization import InstanceNormalization
 
 
 if isinstance(__version__, (list, tuple)):
@@ -30,51 +34,62 @@ else:
 
 mswindows = sys.platform=="win32"
 
-
 try:
     from lib.utils import backup_file
 except ImportError:
     pass
 
 
-class Encoders():
-    REGULAR = 'v2' # high memory consumption encoder
-    NEW_SLIM = 'v3' # slightly lighter on resources and taining speed is faster
+conv_init = RandomNormal(0, 0.02)
+
+
+
+def inst_norm():
+    return InstanceNormalization()
 
+class EncoderType(enum.Enum):
+    ORIGINAL = "original"
+    SHAOANLU = "shaoanlu"
     
-ENCODER = Encoders.NEW_SLIM
+ENCODER = EncoderType.SHAOANLU 
 
-hdf = {'encoderH5': 'encoder_{version_str}{ENCODER}.h5'.format(**vars()),
-       'decoder_AH5': 'decoder_A_{version_str}{ENCODER}.h5'.format(**vars()),
-       'decoder_BH5': 'decoder_B_{version_str}{ENCODER}.h5'.format(**vars())}
+hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
+       'decoder_AH5': 'decoder_A_{version_str}{ENCODER.value}.h5'.format(**vars()),
+       'decoder_BH5': 'decoder_B_{version_str}{ENCODER.value}.h5'.format(**vars())}
 
 
 class Model():
-
-    # still playing with dims
-    ENCODER_DIM = 2048
-        
-    IMAGE_SIZE = 128, 128
-    IMAGE_DEPTH = len('RGB') # good to let ppl know what these are...
-    IMAGE_SHAPE = *IMAGE_SIZE, IMAGE_DEPTH
     
-    def __init__(self, model_dir, gpus):
-        
+    ENCODER_DIM = 1024 # dense layer size        
+    IMAGE_SHAPE = 128, 128 # image shape
+    
+    assert [n for n in IMAGE_SHAPE if n>=16]
+    
+    IMAGE_WIDTH = max(IMAGE_SHAPE)
+    IMAGE_WIDTH = (IMAGE_WIDTH//16 + (1 if (IMAGE_WIDTH%16)>=8 else 0))*16
+    IMAGE_SHAPE = IMAGE_WIDTH, IMAGE_WIDTH, len('BRG') # good to let ppl know what these are...
+    
+    
+    def __init__(self, model_dir, gpus, encoder_type=ENCODER):
+                
         if mswindows:  
             from ctypes import cdll    
             mydll = cdll.LoadLibrary("user32.dll")
-            mydll.SetProcessDPIAware(True)        
+            mydll.SetProcessDPIAware(True)                               
+        
+        self._encoder_type = encoder_type
         
         self.model_dir = model_dir
         
         # can't chnage gpu's when the model is initialized no point in making it r/w
         self._gpus = gpus 
         
-        Encoder = getattr(self, "Encoder") if not ENCODER else getattr(self, "Encoder_{}".format(ENCODER))
+        Encoder = getattr(self, "Encoder_{}".format(self._encoder_type.value))
+        Decoder = getattr(self, "Decoder_{}".format(self._encoder_type.value))
         
         self.encoder = Encoder()
-        self.decoder_A = self.Decoder()
-        self.decoder_B = self.Decoder()
+        self.decoder_A = Decoder()
+        self.decoder_B = Decoder()
         
         self.initModel()        
 
@@ -106,37 +121,32 @@ class Model():
             print('loaded model weights')
             return True
         except Exception as e:
-            print('Failed loading existing training data.')
-            print(e)
+            print('Failed loading existing training data.', e)
             return False        
 
+
     def converter(self, swap):
         autoencoder = self.autoencoder_B if not swap else self.autoencoder_A
         return autoencoder.predict
     
-    def conv(self, filters, kernel_size=4, strides=2):
+    
+    def conv(self, filters, kernel_size=5, strides=2, **kwargs):
         def block(x):
-            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)         
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)         
             x = LeakyReLU(0.1)(x)
             return x
         return block    
         
-    def conv_sep(self, filters):
+    def conv_sep3(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=4, strides=2, padding='same')(x)        
-            x = LeakyReLU(0.1)(x)
-            return x
-        return block
-    
-    def conv_sep_v3(self, filters, kernel_size=4, strides=2):
-        def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, padding="same")(x)
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)        
+            if use_instance_norm:
+                x = inst_norm()(x)
             x = Activation("relu")(x)
-            return x
-        return block       
-    
+            return x    
+        return block
     
-    def upscale(self, filters):
+    def upscale(self, filters, **kwargs):
         def block(x):
             x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)
             x = LeakyReLU(0.1)(x)
@@ -144,92 +154,80 @@ class Model():
             return x
         return block  
     
-    def upscale_sep(self, filters):
+    def upscale_sep3(self, filters, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters * 4, kernel_size=3, padding='same')(x)
+            x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
+            if use_instance_norm:
+                x = inst_norm()(x)
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
-        return block      
-    
-    def res(self, filters, dilation_rate=1):
-        def block(x):
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
-            rb = LeakyReLU(alpha=0.2)(rb)
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
-            x = add([rb, x])
-            x = LeakyReLU(alpha=0.2)(x)
-            return x
         return block    
-  
-    
-    def res_block(self, filters, dilation_rate=1):
-        def block(x):
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
-            rb = LeakyReLU(alpha=0.2)(rb)
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
-            x = add([rb, x])
-            x = LeakyReLU(alpha=0.2)(x)
-            return x
-        return block       
-    
     
-    def Encoder_v3(self):
-        """Lighter on resources encoder with bigger first conv layer"""
-        retina = Input(shape=self.IMAGE_SHAPE)
-        x = self.conv_sep_v3(192)(retina)         
+    def Encoder_original(self, **kwargs):
+        impt = Input(shape=self.IMAGE_SHAPE)
+        
+        in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
+        x = self.conv(in_conv_filters)(impt)
         x = self.conv(256)(x)
-        x = self.conv(384)(x)        
-        x = self.conv_sep_v3(512)(x)
-        x = self.conv(768)(x)
-        x = self.conv_sep_v3(1024)(x)
+        x = self.conv(512)(x)
+        x = self.conv(1024)(x)
+        
+        dense_shape = self.IMAGE_SHAPE[0] // 16         
         x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(4 * 4 * 1024)(x)
-        x = Reshape((4, 4, 1024))(x)
-        out = self.upscale(512)(x)
-        return KerasModel(retina, out)    
-    
+        x = Dense(dense_shape * dense_shape * 768)(x)
+        x = Reshape((dense_shape, dense_shape, 768))(x)
+        x = self.upscale(512)(x)
+        
+        return KerasModel(impt, x, **kwargs)    
+          
     
-    def Encoder_v2(self):
-        """Old algorithm; pretty good but slow"""
-        retina = Input(shape=self.IMAGE_SHAPE)
-        x = self.conv(128)(retina)         
-        x = self.conv(144)(x)              
-        x = self.conv_sep(256)(x)
-        x = self.conv(448)(x)        
-        x = self.conv_sep(512)(x)        
-        x = self.conv(768)(x)
-        x = self.conv_sep(1024)(x)
+    def Encoder_shaoanlu(self, **kwargs):
+        impt = Input(shape=self.IMAGE_SHAPE)
+                
+        in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
+        
+        x = Conv2D(in_conv_filters, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding="same")(impt)
+        x = self.conv_sep3(in_conv_filters+32, use_instance_norm=False)(x)
+        x = self.conv_sep3(256)(x)        
+        x = self.conv_sep3(512)(x)
+        x = self.conv_sep3(1024)(x)        
+        
+        dense_shape = self.IMAGE_SHAPE[0] // 16         
         x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(4 * 4 * 1024)(x)
-        x = Reshape((4, 4, 1024))(x)
-        out = self.upscale(512)(x)
-        return KerasModel(retina, out)
-    
+        x = Dense(dense_shape * dense_shape * 768)(x)
+        x = Reshape((dense_shape, dense_shape, 768))(x)
+        x = self.upscale(512)(x)
+        
+        return KerasModel(impt, x, **kwargs)    
 
-    def Decoder(self):
-        inp = Input(shape=(8, 8, 512))
-        x = self.upscale(384)(inp)
-        x = self.res_block(384)(x)
-        x = self.upscale_sep(192)(x)
-        x = self.res_block(192)(x)
-        x = self.upscale(128)(x)
-        x = self.res_block(128)(x)
-        x = self.upscale(64)(x)
-        x = self.res_block(64)(x)                    
-        
-#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(x)
-#         rb = LeakyReLU(alpha=0.2)(rb)
-#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(rb)
-#         x = add([rb, x])
-# 
-        #x = self.upscale(32)(x)        
-                        
-        out = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
-                
-        return KerasModel(inp, out)
+
+    def Decoder_original(self):       
+        decoder_shape = self.IMAGE_SHAPE[0]//8        
+        inpt = Input(shape=(decoder_shape, decoder_shape, 512))
+        
+        x = self.upscale(512, kernel_initializer=RandomNormal(0, 0.02))(inpt)
+        x = self.upscale(256, kernel_initializer=RandomNormal(0, 0.02))(x)
+        x = self.upscale(self.IMAGE_SHAPE[0], kernel_initializer=RandomNormal(0, 0.02))(x)
         
+        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+        
+        return KerasModel(inpt, x)
+    
     
+    def Decoder_shaoanlu(self):       
+        decoder_shape = self.IMAGE_SHAPE[0]//8        
+        inpt = Input(shape=(decoder_shape, decoder_shape, 512))
+        
+        x = self.upscale_sep3(512)(inpt)
+        x = self.upscale_sep3(256)(x)
+        x = self.upscale_sep3(self.IMAGE_SHAPE[0])(x)
+        
+        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+        
+        return KerasModel(inpt, x)    
+
+
     def save_weights(self):
         from threading import Thread
         from time import sleep
@@ -267,10 +265,10 @@ class Model():
     @property
     def model_name(self):
         try:
-            return self._model_nomen
+            return self._model_name
         except AttributeError:
-            self._model_nomen = self._model_nomen = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
-        return self._model_nomen
+            self._model_name = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
+        return self._model_name
              
     
     def __str__(self):
