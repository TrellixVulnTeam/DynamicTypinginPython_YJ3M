commit 5c17417d4d48afb07629d2dfe014cb4b46a736c7
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Wed Jul 24 12:32:21 2019 +0000

    Training: Add Multi-Output Support
    
    - Name output nodes of models
    - Add support for multiple outputs in models and in training
    - Update loss output format for cli and gui
    - Sort graphs tabs on training
    - Fix analysis graph for new loss names
    - Multi output support in convert

diff --git a/lib/gui/display_analysis.py b/lib/gui/display_analysis.py
index a0584e1..d573ac2 100644
--- a/lib/gui/display_analysis.py
+++ b/lib/gui/display_analysis.py
@@ -358,8 +358,8 @@ class StatsData(ttk.Frame):  # pylint: disable=too-many-ancestors
             'iconphoto',
             toplevel._w, get_images().icons["favicon"])  # pylint:disable=protected-access
         position = self.data_popup_get_position()
-        height = int(720 * scaling_factor)
-        width = int(400 * scaling_factor)
+        height = int(900 * scaling_factor)
+        width = int(480 * scaling_factor)
         toplevel.geometry("{}x{}+{}+{}".format(str(height),
                                                str(width),
                                                str(position[0]),
@@ -559,12 +559,13 @@ class SessionPopUp(tk.Toplevel):
             text = loss_key.replace("_", " ").title()
             helptext = "Display {}".format(text)
             var = tk.BooleanVar()
-            if loss_key.startswith("loss"):
+            if loss_key.startswith("total"):
                 var.set(True)
             lk_vars[loss_key] = var
 
             if len(loss_keys) == 1:
                 # Don't display if there's only one item
+                var.set(True)
                 break
 
             if not section_added:
diff --git a/lib/gui/display_command.py b/lib/gui/display_command.py
index f7e87f8..fb9652c 100644
--- a/lib/gui/display_command.py
+++ b/lib/gui/display_command.py
@@ -251,7 +251,11 @@ class GraphDisplay(DisplayOptionalPage):  # pylint: disable=too-many-ancestors
         """ Add a single graph to the graph window """
         logger.trace("Adding graph")
         existing = list(self.subnotebook_get_titles_ids().keys())
-        for loss_key in self.display_item.loss_keys:
+        display_tabs = sorted(self.display_item.loss_keys)
+        if any(key.startswith("total") for key in display_tabs):
+            total_idx = [idx for idx, key in enumerate(display_tabs) if key.startswith("total")][0]
+            display_tabs.insert(0, display_tabs.pop(total_idx))
+        for loss_key in display_tabs:
             tabname = loss_key.replace("_", " ").title()
             if tabname in existing:
                 continue
diff --git a/lib/gui/display_graph.py b/lib/gui/display_graph.py
index d97c8c7..05a2062 100755
--- a/lib/gui/display_graph.py
+++ b/lib/gui/display_graph.py
@@ -83,8 +83,8 @@ class GraphBase(ttk.Frame):  # pylint: disable=too-many-ancestors
 
         self.calcs = data
         self.ylabel = ylabel
-        self.colourmaps = ["Reds", "Blues", "Greens", "Purples", "Oranges",
-                           "Greys", "copper", "summer", "bone"]
+        self.colourmaps = ["Reds", "Blues", "Greens", "Purples", "Oranges", "Greys", "copper",
+                           "summer", "bone", "hot", "cool", "pink", "Wistia", "spring", "winter"]
         self.lines = list()
         self.toolbar = None
         self.fig = Figure(figsize=(4, 4), dpi=75)
diff --git a/lib/gui/wrapper.py b/lib/gui/wrapper.py
index 75a34c2..4c6f87e 100644
--- a/lib/gui/wrapper.py
+++ b/lib/gui/wrapper.py
@@ -148,7 +148,7 @@ class FaceswapControl():
         self.thread = None  # Thread for LongRunningTask termination
         self.train_stats = {"iterations": 0, "timestamp": None}
         self.consoleregex = {
-            "loss": re.compile(r"([a-zA-Z_]+):.*?(\d+\.\d+)"),
+            "loss": re.compile(r"[\W]+(\d+)?[\W]+([a-zA-Z\s]*)[\W]+?(\d+\.\d+)"),
             "tqdm": re.compile(r"(?P<dsc>.*?)(?P<pct>\d+%).*?(?P<itm>\S+/\S+)\W\["
                                r"(?P<tme>\d+:\d+<.*),\W(?P<rte>.*)[a-zA-Z/]*\]"),
             "ffmpeg": re.compile(r"([a-zA-Z]+)=\s*(-?[\d|N/A]\S+)")}
@@ -241,13 +241,12 @@ class FaceswapControl():
             return False
 
         loss = self.consoleregex["loss"].findall(string)
-        if len(loss) < 2:
+        if len(loss) != 2 or not all(len(itm) == 3 for itm in loss):
             logger.trace("Not loss message. Returning False")
             return False
 
-        message = ""
-        for item in loss:
-            message += "{}: {}  ".format(item[0], item[1])
+        message = "Total Iterations: {} | ".format(int(loss[0][0]))
+        message += "  ".join(["{}: {}".format(itm[1], itm[2]) for itm in loss])
         if not message:
             logger.trace("Error creating loss message. Returning False")
             return False
@@ -270,8 +269,9 @@ class FaceswapControl():
         self.train_stats["iterations"] = iterations
 
         elapsed = self.calc_elapsed()
-        message = "Elapsed: {}  Iteration: {}  {}".format(elapsed,
-                                                          self.train_stats["iterations"], message)
+        message = "Elapsed: {} | Session Iterations: {}  {}".format(
+            elapsed,
+            self.train_stats["iterations"], message)
         self.statusbar.progress_update(message, 0, False)
         logger.trace("Succesfully captured loss: %s", message)
         return True
diff --git a/lib/training_data.py b/lib/training_data.py
index 25ebb56..914b0dd 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -21,22 +21,22 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 class TrainingDataGenerator():
     """ Generate training data for models """
-    def __init__(self, model_input_size, model_output_size, training_opts, config):
-        logger.debug("Initializing %s: (model_input_size: %s, model_output_shape: %s, "
+    def __init__(self, model_input_size, model_output_shapes, training_opts, config):
+        logger.debug("Initializing %s: (model_input_size: %s, model_output_shapes: %s, "
                      "training_opts: %s, landmarks: %s, config: %s)",
-                     self.__class__.__name__, model_input_size, model_output_size,
+                     self.__class__.__name__, model_input_size, model_output_shapes,
                      {key: val for key, val in training_opts.items() if key != "landmarks"},
                      bool(training_opts.get("landmarks", None)), config)
         self.batchsize = 0
         self.model_input_size = model_input_size
-        self.model_output_size = model_output_size
+        self.model_output_shapes = model_output_shapes
         self.training_opts = training_opts
         self.mask_class = self.set_mask_class()
         self.landmarks = self.training_opts.get("landmarks", None)
         self.fixed_producer_dispatcher = None  # Set by FPD when loading
         self._nearest_landmarks = None
         self.processing = ImageManipulation(model_input_size,
-                                            model_output_size,
+                                            model_output_shapes,
                                             training_opts.get("coverage_ratio", 0.625),
                                             config)
         logger.debug("Initialized %s", self.__class__.__name__)
@@ -64,10 +64,10 @@ class TrainingDataGenerator():
         training_size = self.training_opts.get("training_size", 256)
         batch_shape = list((
             (batchsize, training_size, training_size, 3),  # sample images
-            (batchsize, self.model_input_size, self.model_input_size, 3),
-            (batchsize, self.model_output_size, self.model_output_size, 3)))
-        if self.mask_class:
-            batch_shape.append((self.batchsize, self.model_output_size, self.model_output_size, 1))
+            (batchsize, self.model_input_size, self.model_input_size, 3)))
+        # Add the output shapes
+        batch_shape.extend(tuple([(batchsize, ) + shape for shape in self.model_output_shapes]))
+        logger.debug("Batch shapes: %s", batch_shape)
 
         self.fixed_producer_dispatcher = FixedProducerDispatcher(
             method=self.load_batches,
@@ -237,18 +237,19 @@ class TrainingDataGenerator():
 
 class ImageManipulation():
     """ Manipulations to be performed on training images """
-    def __init__(self, input_size, output_size, coverage_ratio, config):
+    def __init__(self, input_size, output_shapes, coverage_ratio, config):
         """ input_size: Size of the face input into the model
-            output_size: Size of the face that comes out of the modell
+            output_shapes: Shapes that come out of the model
             coverage_ratio: Coverage ratio of full image. Eg: 256 * 0.625 = 160
         """
-        logger.debug("Initializing %s: (input_size: %s, output_size: %s, coverage_ratio: %s, "
-                     "config: %s)", self.__class__.__name__, input_size, output_size,
+        logger.debug("Initializing %s: (input_size: %s, output_shapes: %s, coverage_ratio: %s, "
+                     "config: %s)", self.__class__.__name__, input_size, output_shapes,
                      coverage_ratio, config)
         self.config = config
         # Transform and Warp args
         self.input_size = input_size
-        self.output_size = output_size
+        self.output_sizes = [shape[1] for shape in output_shapes if shape[2] == 3]
+        logger.debug("Output sizes: %s", self.output_sizes)
         # Warp args
         self.coverage_ratio = coverage_ratio  # Coverage ratio of full image. Eg: 256 * 0.625 = 160
         self.scale = 5  # Normal random variable scale
@@ -386,9 +387,8 @@ class ImageManipulation():
 
         pad = int(1.25 * self.input_size)
         slices = slice(pad // 10, -pad // 10)
-        dst_slice = slice(0, (self.output_size + 1), (self.output_size // 4))
+        dst_slices = [slice(0, (size + 1), (size // 4)) for size in self.output_sizes]
         interp = np.empty((2, self.input_size, self.input_size), dtype='float32')
-        ####
 
         for i, map_ in enumerate([mapx, mapy]):
             map_ = map_ + np.random.normal(size=(5, 5), scale=self.scale)
@@ -399,22 +399,17 @@ class ImageManipulation():
         logger.trace("Warped image shape: %s", warped_image.shape)
 
         src_points = np.stack([mapx.ravel(), mapy.ravel()], axis=-1)
-        dst_points = np.mgrid[dst_slice, dst_slice]
-        mat = umeyama(src_points, True, dst_points.T.reshape(-1, 2))[0:2]
-        target_image = cv2.warpAffine(  # pylint:disable=no-member
-            image, mat, (self.output_size, self.output_size))
-        logger.trace("Target image shape: %s", target_image.shape)
-
-        warped_image, warped_mask = self.separate_mask(warped_image)
-        target_image, target_mask = self.separate_mask(target_image)
+        dst_points = [np.mgrid[dst_slice, dst_slice] for dst_slice in dst_slices]
+        mats = [umeyama(src_points, True, dst_pts.T.reshape(-1, 2))[0:2]
+                for dst_pts in dst_points]
 
-        if target_mask is None:
-            logger.trace("Randomly warped image")
-            return [warped_image, target_image]
+        target_images = [cv2.warpAffine(image,  # pylint:disable=no-member
+                                        mat,
+                                        (self.output_sizes[idx], self.output_sizes[idx]))
+                         for idx, mat in enumerate(mats)]
 
-        logger.trace("Target mask shape: %s", target_mask.shape)
-        logger.trace("Randomly warped image and mask")
-        return [warped_image, target_image, target_mask]
+        logger.trace("Target image shapes: %s", [tgt.shape for tgt in target_images])
+        return self.compile_images(warped_image, target_images)
 
     def random_warp_landmarks(self, image, src_points=None, dst_points=None):
         """ get warped image, target image and target mask
@@ -475,21 +470,33 @@ class ImageManipulation():
             warped_image[slices, slices, :], (self.input_size, self.input_size),
             cv2.INTER_AREA)  # pylint:disable=no-member
         logger.trace("Warped image shape: %s", warped_image.shape)
-        target_image = cv2.resize(  # pylint:disable=no-member
-            target_image[slices, slices, :], (self.output_size, self.output_size),
-            cv2.INTER_AREA)  # pylint:disable=no-member
-        logger.trace("Target image shape: %s", target_image.shape)
-
-        warped_image, warped_mask = self.separate_mask(warped_image)
-        target_image, target_mask = self.separate_mask(target_image)
-
-        if target_mask is None:
-            logger.trace("Randomly warped image")
-            return [warped_image, target_image]
-
-        logger.trace("Target mask shape: %s", target_mask.shape)
-        logger.trace("Randomly warped image and mask")
-        return [warped_image, target_image, target_mask]
+        target_images = [cv2.resize(target_image[slices, slices, :],  # pylint:disable=no-member
+                                    (size, size),
+                                    cv2.INTER_AREA)  # pylint:disable=no-member
+                         for size in self.output_sizes]
+
+        logger.trace("Target image shapea: %s", [img.shape for img in target_images])
+        return self.compile_images(warped_image, target_images)
+
+    def compile_images(self, warped_image, target_images):
+        """ Compile the warped images, target images and mask for feed """
+        warped_image, _ = self.separate_mask(warped_image)
+        final_target_images = list()
+        target_mask = None
+        for target_image in target_images:
+            image, mask = self.separate_mask(target_image)
+            final_target_images.append(image)
+            # Add the mask if it exists and is the same size as our largest output
+            if mask is not None and mask.shape[1] == max(self.output_sizes):
+                target_mask = mask
+
+        retval = [warped_image] + final_target_images
+        if target_mask is not None:
+            logger.trace("Target mask shape: %s", target_mask.shape)
+            retval.append(target_mask)
+
+        logger.trace("Final shapes: %s", [img.shape for img in retval])
+        return retval
 
 
 def stack_images(images):
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 61d94dd..c2db953 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -163,14 +163,49 @@ class ModelBase():
         return retval
 
     @property
-    def output_shape(self):
+    def output_shapes(self):
         """ Return the output shapes from the main AutoEncoder """
         out = list()
         for predictor in self.predictors.values():
             out.extend([K.int_shape(output)[-3:] for output in predictor.outputs])
             break  # Only get output from one autoencoder. Shapes are the same
         # Only return the output shape of the face
-        return tuple(out[0])
+        return [tuple(shape) for shape in out]
+
+    @property
+    def output_shape(self):
+        """ The output shape of the model (shape of largest face output) """
+        return self.output_shapes[self.largest_face_index]
+
+    @property
+    def largest_face_index(self):
+        """ Return the index from model.outputs of the largest face
+            Required for multi-output model prediction. The largest face
+            is assumed to be the final output
+        """
+        sizes = [shape[1] for shape in self.output_shapes if shape[2] == 3]
+        if not sizes:
+            return None
+        max_face = max(sizes)
+        retval = [idx for idx, shape in enumerate(self.output_shapes)
+                  if shape[1] == max_face and shape[2] == 3][0]
+        logger.debug(retval)
+        return retval
+
+    @property
+    def largest_mask_index(self):
+        """ Return the index from model.outputs of the largest mask
+            Required for multi-output model prediction. The largest face
+            is assumed to be the final output
+        """
+        sizes = [shape[1] for shape in self.output_shapes if shape[2] == 1]
+        if not sizes:
+            return None
+        max_mask = max(sizes)
+        retval = [idx for idx, shape in enumerate(self.output_shapes)
+                  if shape[1] == max_mask and shape[2] == 1][0]
+        logger.debug(retval)
+        return retval
 
     @staticmethod
     def set_gradient_type(memory_saving_gradients):
@@ -235,9 +270,10 @@ class ModelBase():
         logger.debug("Getting inputs")
         inputs = [Input(shape=self.input_shape, name="face_in")]
         output_network = [network for network in self.networks.values() if network.is_output][0]
-        if "mask_out" in output_network.output_names:
-            mask_idx = output_network.output_names.index("mask_out")
-            mask_shape = output_network.output_shapes[mask_idx]
+        mask_idx = [idx for idx, name in enumerate(output_network.output_names)
+                    if name.startswith("mask")]
+        if mask_idx:
+            mask_shape = output_network.output_shapes[mask_idx[0]]
             inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
         logger.debug("Got inputs: %s", inputs)
         return inputs
@@ -333,20 +369,13 @@ class ModelBase():
         optimizer = self.get_optimizer(lr=learning_rate, beta_1=0.5, beta_2=0.999)
 
         for side, model in self.predictors.items():
-            mask = [inp for inp in model.inputs if inp.name.startswith("mask")]
-            loss_names = ["loss"]
-            loss_funcs = [self.loss_function(mask, side, initialize)]
-            if mask:
-                loss_names.append("mask_loss")
-                loss_funcs.append(self.mask_loss_function(side, initialize))
-            model.compile(optimizer=optimizer, loss=loss_funcs)
-
-            if len(loss_names) > 1:
-                loss_names.insert(0, "total_loss")
+            mask_input = [inp for inp in model.inputs if inp.name.startswith("mask")]
+            loss = Loss(side, model.outputs, mask_input, self.predict)
+            model.compile(optimizer=optimizer, loss=loss.funcs)
             if initialize:
-                self.state.add_session_loss_names(side, loss_names)
+                self.state.add_session_loss_names(side, loss.names)
                 self.history[side] = list()
-        logger.debug("Compiled Predictors. Losses: %s", loss_names)
+        logger.debug("Compiled Predictors. Losses: %s", loss.names)
 
     def get_optimizer(self, lr=5e-5, beta_1=0.5, beta_2=0.999):  # pylint: disable=invalid-name
         """ Build and return Optimizer """
@@ -363,33 +392,6 @@ class ModelBase():
         logger.debug("Optimizer kwargs: %s", opt_kwargs)
         return Adam(**opt_kwargs, cpu_mode=self.optimizer_savings)
 
-    def loss_function(self, mask, side, initialize):
-        """ Set the loss function
-            Side is input so we only log once """
-        if self.config.get("dssim_loss", False):
-            if side == "a" and not self.predict and initialize:
-                logger.verbose("Using DSSIM Loss")
-            loss_func = DSSIMObjective()
-        else:
-            if side == "a" and not self.predict and initialize:
-                logger.verbose("Using Mean Absolute Error Loss")
-            loss_func = losses.mean_absolute_error
-
-        if mask and self.config.get("penalized_mask_loss", False):
-            loss_mask = mask[0]
-            if side == "a" and not self.predict and initialize:
-                logger.verbose("Penalizing mask for Loss")
-            loss_func = PenalizedLoss(loss_mask, loss_func)
-        return loss_func
-
-    def mask_loss_function(self, side, initialize):
-        """ Set the mask loss function
-            Side is input so we only log once """
-        if side == "a" and not self.predict and initialize:
-            logger.verbose("Using Mean Squared Error Loss for mask")
-        mask_loss_func = losses.mean_squared_error
-        return mask_loss_func
-
     def converter(self, swap):
         """ Converter for autoencoder models """
         logger.debug("Getting Converter: (swap: %s)", swap)
@@ -607,6 +609,91 @@ class ModelBase():
         self.state.save()
 
 
+class Loss():
+    """ Holds loss names and functions for an Autoencoder """
+    def __init__(self, side, outputs, mask_input, predict):
+        logger.debug("Initializing %s: (side: '%s', outputs: '%s', mask_input: '%s', predict: %s",
+                     self.__class__.__name__, side, outputs, mask_input, predict)
+        self.outputs = outputs
+        self.names = self.get_loss_names()
+        self.funcs = self.get_loss_functions(side, predict, mask_input)
+        if len(self.names) > 1:
+            self.names.insert(0, "total_loss")
+        logger.debug("Initialized: %s", self.__class__.__name__)
+
+    @property
+    def config(self):
+        """ Return the global _CONFIG variable """
+        return _CONFIG
+
+    @property
+    def loss_shapes(self):
+        """ The shapes of the output nodes """
+        return [K.int_shape(output)[1:] for output in self.outputs]
+
+    @property
+    def largest_output(self):
+        """ Return the index of the largest face output """
+        max_size = max(shape[0] for shape in self.loss_shapes if shape[2] == 3)
+        largest_idx = [idx for idx, shape in enumerate(self.loss_shapes)
+                       if shape[0] == max_size and shape[2] == 3][0]
+        return largest_idx
+
+    def get_loss_names(self):
+        """ Return the loss names based on model output """
+        output_names = [output.name for output in self.outputs]
+        logger.debug("Model output names: %s", output_names)
+        loss_names = [name[name.find("/") + 1:name.rfind("/")].replace("_out", "")
+                      for name in output_names]
+        if not all(name.startswith("face") or name.startswith("mask") for name in loss_names):
+            # Handle incorrectly named/legacy outputs
+            logger.debug("Renaming loss names from: %s", loss_names)
+            loss_names = self.update_loss_names()
+        loss_names = ["{}_loss".format(name) for name in loss_names]
+        logger.debug(loss_names)
+        return loss_names
+
+    def update_loss_names(self):
+        """ Update loss names if named incorrectly or legacy model """
+        output_types = ["mask" if shape[-1] == 1 else "face" for shape in self.loss_shapes]
+        loss_names = ["{}{}".format(name,
+                                    "" if output_types.count(name) == 1 else "_{}".format(idx))
+                      for idx, name in enumerate(output_types)]
+        logger.debug("Renamed loss names to: %s", loss_names)
+        return loss_names
+
+    def get_loss_functions(self, side, predict, mask):
+        """ Set the loss function """
+        loss_funcs = list()
+        largest_face = self.largest_output
+
+        if self.config.get("dssim_loss", False):
+            if not predict and side.lower() == "a":
+                logger.verbose("Using DSSIM Loss")
+            loss_func = DSSIMObjective()
+        else:
+            loss_func = losses.mean_absolute_error
+            if not predict and side.lower() == "a":
+                logger.verbose("Using Mean Absolute Error Loss")
+
+        for idx, loss_name in enumerate(self.names):
+            if loss_name.startswith("mask"):
+                mask_func = losses.mean_squared_error
+                loss_funcs.append(mask_func)
+                logger.debug("mask loss: %s", mask_func)
+            elif mask and idx == largest_face and self.config.get("penalized_mask_loss", False):
+                face_func = PenalizedLoss(mask[0], loss_func)
+                logger.debug("final face loss: %s", face_func)
+                loss_funcs.append(face_func)
+                if not predict and side.lower() == "a":
+                    logger.verbose("Penalizing mask for Loss")
+            else:
+                logger.debug("face loss func: %s", loss_func)
+                loss_funcs.append(loss_func)
+        logger.debug(loss_funcs)
+        return loss_funcs
+
+
 class NNMeta():
     """ Class to hold a neural network and it's meta data
 
@@ -652,7 +739,7 @@ class NNMeta():
     def output_names(self):
         """ Return output node names """
         output_names = [output.name for output in self.network.outputs]
-        if self.is_output and not any(name == "face_out" for name in output_names):
+        if self.is_output and not any(name.startswith("face_out") for name in output_names):
             # Saved models break if their layer names are changed, so dummy
             # in correct output names for legacy models
             output_names = self.get_output_names()
@@ -665,7 +752,7 @@ class NNMeta():
         output_names = ["{}{}".format(name,
                                       "" if output_types.count(name) == 1 else "_{}".format(idx))
                         for idx, name in enumerate(output_types)]
-        logger.debug(output_names)
+        logger.debug("Overridden output_names: %s", output_names)
         return output_names
 
     def load(self, fullpath=None):
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index 27977af..159888d 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -154,10 +154,7 @@ class TrainerBase():
     def print_loss(self, loss):
         """ Override for specific model loss formatting """
         logger.trace(loss)
-        output = [", ".join(["{}_{}: {:.5f}".format(self.model.state.loss_names[side][idx],
-                                                    side.capitalize(),
-                                                    this_loss)
-                             for idx, this_loss in enumerate(loss[side])])
+        output = ["Loss {}: {:.5f}".format(side.capitalize(), loss[side][0])
                   for side in sorted(list(loss.keys()))]
         output = ", ".join(output)
         print("[{}] [#{:05d}] {}".format(self.timestamp, self.model.iterations, output), end='\r')
@@ -263,10 +260,10 @@ class Batcher():
         """ Pass arguments to TrainingDataGenerator and return object """
         logger.debug("Loading generator: %s", self.side)
         input_size = self.model.input_shape[0]
-        output_size = self.model.output_shape[0]
-        logger.debug("input_size: %s, output_size: %s", input_size, output_size)
+        output_shapes = self.model.output_shapes
+        logger.debug("input_size: %s, output_shapes: %s", input_size, output_shapes)
         generator = TrainingDataGenerator(input_size,
-                                          output_size,
+                                          output_shapes,
                                           self.model.training_opts,
                                           self.config)
         return generator
@@ -296,22 +293,15 @@ class Batcher():
         """ Return the next batch from the generator
             Items should come out as: (warped, target [, mask]) """
         batch = next(self.feed)
-        batch = batch[1:]   # Remove full size samples from batch
+        feed = batch[1]
+        batch = batch[2:]   # Remove full size samples and feed from batch
         if self.use_mask:
-            batch = self.compile_mask(batch)
+            # Add mask to inputs
+            mask = batch[-1]
+            batch = [[feed, mask], batch]
         self.generate_preview(do_preview)
         return batch
 
-    def compile_mask(self, batch):
-        """ Compile the mask into training data """
-        logger.trace("Compiling Mask: (side: '%s')", self.side)
-        mask = batch[-1]
-        retval = list()
-        for idx in range(len(batch) - 1):
-            image = batch[idx]
-            retval.append([image, mask])
-        return retval
-
     def generate_preview(self, do_preview):
         """ Generate the preview if a preview iteration """
         if not do_preview:
@@ -322,11 +312,13 @@ class Batcher():
         if self.preview_feed is None:
             self.set_preview_feed()
         batch = next(self.preview_feed)
-        self.samples = batch[0]
-        batch = batch[1:]   # Remove full size samples from batch
+        self.samples, feed = batch[:2]
+        batch = batch[2:]   # Remove full size samples and feed from batch
+        self.target = batch[self.model.largest_face_index]
         if self.use_mask:
-            batch = self.compile_mask(batch)
-        self.target = batch[1]
+            mask = batch[-1]
+            batch = [[feed, mask], batch]
+            self.target = [self.target, mask]
 
     def set_preview_feed(self):
         """ Set the preview dictionary """
@@ -358,12 +350,14 @@ class Batcher():
     def compile_timelapse_sample(self):
         """ Timelapse samples """
         batch = next(self.timelapse_feed)
-        samples = batch[0]
-        batch = batch[1:]   # Remove full size samples from batch
+        samples, feed = batch[:2]
         batchsize = len(samples)
+        batch = batch[2:]   # Remove full size samples and feed from batch
+        images = batch[self.model.largest_face_index]
         if self.use_mask:
-            batch = self.compile_mask(batch)
-        images = batch[1]
+            mask = batch[-1]
+            batch = [[feed, mask], batch]
+            images = [images, mask]
         sample = self.compile_sample(batchsize, samples=samples, images=images)
         return sample
 
@@ -462,11 +456,10 @@ class Samples():
         preds["b_a"] = self.model.predictors["b"].predict(feed_a)
         preds["a_b"] = self.model.predictors["a"].predict(feed_b)
         preds["b_b"] = self.model.predictors["b"].predict(feed_b)
-
-        # Get the returned image from predictors that emit multiple items
+        # Get the returned largest image from predictors that emit multiple items
         if not isinstance(preds["a_a"], np.ndarray):
             for key, val in preds.items():
-                preds[key] = val[0]
+                preds[key] = val[self.model.largest_face_index]
         logger.debug("Returning predictions: %s", {key: val.shape for key, val in preds.items()})
         return preds
 
diff --git a/scripts/convert.py b/scripts/convert.py
index 9461aaf..6d80539 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -436,6 +436,8 @@ class Predict():
         self.faces_count = 0
         self.verify_output = False
         self.model = self.load_model()
+        self.output_indices = {"face": self.model.largest_face_index,
+                               "mask": self.model.largest_mask_index}
         self.predictor = self.model.converter(self.args.swap_model)
         self.queues = dict()
 
@@ -612,6 +614,8 @@ class Predict():
         predicted = predicted if isinstance(predicted, list) else [predicted]
         logger.trace("Output shape(s): %s", [predict.shape for predict in predicted])
 
+        predicted = self.filter_multi_out(predicted)
+
         # Compile masks into alpha channel or keep raw faces
         predicted = np.concatenate(predicted, axis=-1) if len(predicted) == 2 else predicted[0]
         predicted = predicted.astype("float32")
@@ -619,6 +623,17 @@ class Predict():
         logger.trace("Final shape: %s", predicted.shape)
         return predicted
 
+    def filter_multi_out(self, predicted):
+        """ Filter the predicted output to the final output """
+        if not predicted:
+            return predicted
+        face = predicted[self.output_indices["face"]]
+        mask_idx = self.output_indices["mask"]
+        mask = predicted[mask_idx] if mask_idx is not None else None
+        predicted = [face, mask] if mask is not None else [face]
+        logger.trace("Filtered output shape(s): %s", [predict.shape for predict in predicted])
+        return predicted
+
     def queue_out_frames(self, batch, swapped_faces):
         """ Compile the batch back to original frames and put to out_queue """
         logger.trace("Queueing out batch. Batchsize: %s", len(batch))
