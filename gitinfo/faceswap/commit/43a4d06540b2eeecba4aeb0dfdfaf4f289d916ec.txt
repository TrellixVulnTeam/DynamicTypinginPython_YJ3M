commit 43a4d06540b2eeecba4aeb0dfdfaf4f289d916ec
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Thu Dec 5 16:02:01 2019 +0000

    Smart Masks - Training Implementation (#914)
    
    * Smart Masks - Training
    
    - Reinstate smart mask training code
    - Reinstate mask_type back to model.config
    - change 'replicate_input_mask to 'learn_mask'
    - Add learn mask option
    - Add mask loading from alignments to plugins.train.trainer
    - Add mask_blur and mask threshold options
    - _base.py - Pass mask options through training_opts dict
    - plugins.train.model - check for mask_type not None for learn_mask and penalized_mask_loss
    - Limit alignments loading to just those faces that appear in the training folder
    - Raise error if not all training images have an alignment, and alignment file is required
    - lib.training_data - Mask generation code
    - lib.faces_detect - cv2 dimension stripping bugfix
    - Remove cv2 linting code
    
    * Update mask helptext in cli.py
    
    * Fix Warp to Landmarks
    Remove SHA1 hashing from training data
    
    * Update mask training config
    
    * Capture missing masks at training init
    
    * lib.image.read_image_batch - Return filenames with batch for ordering
    
    * scripts.train - Documentation
    
    * plugins.train.trainer - documentation
    
    * Ensure backward compatibility.
    Fix convert for new predicted masks
    
    * Update removed masks to components for legacy models.

diff --git a/docs/full/plugins.extract.align.rst b/docs/full/plugins.extract.align.rst
deleted file mode 100644
index 7ae7a06..0000000
--- a/docs/full/plugins.extract.align.rst
+++ /dev/null
@@ -1,17 +0,0 @@
-plugins.extract.align package
-=============================
-
-Submodules
-----------
-
-.. toctree::
-
-   plugins.extract.align._base
-   
-Module contents
----------------
-
-.. automodule:: plugins.extract.align
-   :members:
-   :undoc-members:
-   :show-inheritance:
diff --git a/docs/full/plugins.extract.detect._base.rst b/docs/full/plugins.extract.detect._base.rst
index 3ee95a1..d89e4e3 100644
--- a/docs/full/plugins.extract.detect._base.rst
+++ b/docs/full/plugins.extract.detect._base.rst
@@ -1,5 +1,5 @@
 plugins.extract.detect._base module
-======================================
+===================================
 
 .. automodule:: plugins.extract.detect._base
    :members:
diff --git a/docs/full/plugins.extract.detect.rst b/docs/full/plugins.extract.detect.rst
deleted file mode 100644
index 27f2d9d..0000000
--- a/docs/full/plugins.extract.detect.rst
+++ /dev/null
@@ -1,17 +0,0 @@
-plugins.extract.detect package
-==============================
-
-Submodules
-----------
-
-.. toctree::
-
-   plugins.extract.detect._base
-
-Module contents
----------------
-
-.. automodule:: plugins.extract.detect
-   :members:
-   :undoc-members:
-   :show-inheritance:
diff --git a/docs/full/plugins.extract.mask.rst b/docs/full/plugins.extract.mask.rst
deleted file mode 100644
index a748744..0000000
--- a/docs/full/plugins.extract.mask.rst
+++ /dev/null
@@ -1,17 +0,0 @@
-plugins.extract.mask package
-==============================
-
-Submodules
-----------
-
-.. toctree::
-
-   plugins.extract.mask._base
-
-Module contents
----------------
-
-.. automodule:: plugins.extract.mask
-   :members:
-   :undoc-members:
-   :show-inheritance:
diff --git a/docs/full/plugins.extract.rst b/docs/full/plugins.extract.rst
index 3061ac9..8c4ea2e 100644
--- a/docs/full/plugins.extract.rst
+++ b/docs/full/plugins.extract.rst
@@ -6,9 +6,9 @@ Subpackages
 
 .. toctree::
 
-   plugins.extract.align
-   plugins.extract.detect
-   plugins.extract.mask
+   plugins.extract.align._base
+   plugins.extract.detect._base
+   plugins.extract.mask._base
 
 Submodules
 ----------
@@ -17,11 +17,3 @@ Submodules
 
    plugins.extract._base
    plugins.extract.pipeline
-
-Module contents
----------------
-
-.. automodule:: plugins.extract
-   :members:
-   :undoc-members:
-   :show-inheritance:
diff --git a/docs/full/plugins.rst b/docs/full/plugins.rst
index 37ea4a1..bfaecff 100644
--- a/docs/full/plugins.rst
+++ b/docs/full/plugins.rst
@@ -8,11 +8,4 @@ Subpackages
 
    plugins.extract
    plugins.plugin_loader
-
-Module contents
----------------
-
-.. automodule:: plugins
-   :members:
-   :undoc-members:
-   :show-inheritance:
+   plugins.train
diff --git a/docs/full/plugins.train.rst b/docs/full/plugins.train.rst
new file mode 100644
index 0000000..17a8166
--- /dev/null
+++ b/docs/full/plugins.train.rst
@@ -0,0 +1,9 @@
+plugins.train package
+=====================
+
+Subpackages
+-----------
+
+.. toctree::
+
+   plugins.train.trainer._base
diff --git a/docs/full/plugins.train.trainer._base.rst b/docs/full/plugins.train.trainer._base.rst
new file mode 100644
index 0000000..188f869
--- /dev/null
+++ b/docs/full/plugins.train.trainer._base.rst
@@ -0,0 +1,7 @@
+plugins.train.trainer._base module
+==================================
+
+.. automodule:: plugins.train.trainer._base
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/scripts.extract.rst b/docs/full/scripts.extract.rst
index 6caa7b5..742f40b 100644
--- a/docs/full/scripts.extract.rst
+++ b/docs/full/scripts.extract.rst
@@ -1,5 +1,5 @@
 scripts.extract
-=======================
+===============
 
 .. automodule:: scripts.extract
    :members:
diff --git a/docs/full/scripts.rst b/docs/full/scripts.rst
index baa53d1..ebf3c04 100644
--- a/docs/full/scripts.rst
+++ b/docs/full/scripts.rst
@@ -7,6 +7,7 @@ Subpackages
 .. toctree::
 
    scripts.extract
+   scripts.train
 
 Module contents
 ---------------
diff --git a/docs/full/scripts.train.rst b/docs/full/scripts.train.rst
new file mode 100644
index 0000000..111f2b3
--- /dev/null
+++ b/docs/full/scripts.train.rst
@@ -0,0 +1,7 @@
+scripts.train
+=============
+
+.. automodule:: scripts.train
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/lib/cli.py b/lib/cli.py
index d014265..f8d312b 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -569,9 +569,9 @@ class ExtractArgs(ExtractConvertArgs):
             "choices": PluginLoader.get_available_extractors("mask", add_none=True),
             "default": "extended",
             "group": "Plugins",
-            "help": "R|Masker to use. NB: Masker is not currently used by the rest of the process "
-                    "but this will store a mask in the alignments file for use when it has been "
-                    "implemented."
+            "help": "R|Masker to use. NB - masks generated here can be used for training, and "
+                    "converting with the 'predicted' mask. Availability of all masks specified "
+                    "here for convert is coming soon."
                     "\nL|none: Don't use a mask."
                     "\nL|components: Mask designed to provide facial segmentation based on the "
                     "positioning of landmark locations. A convex hull is constructed around the "
diff --git a/lib/convert.py b/lib/convert.py
index e9d6d1e..d292966 100644
--- a/lib/convert.py
+++ b/lib/convert.py
@@ -99,11 +99,8 @@ class Converter():
                                  item["filename"], str(err))
                     image = item["image"]
                     # UNCOMMENT THIS CODE BLOCK TO PRINT TRACEBACK ERRORS
-                    # import sys
-                    # import traceback
-                    # exc_info = sys.exc_info()
-                    # traceback.print_exception(*exc_info)
-
+                    # import sys ; import traceback
+                    # exc_info = sys.exc_info() ; traceback.print_exception(*exc_info)
                 logger.trace("Out queue put: %s", item["filename"])
                 out_queue.put((item["filename"], image))
         logger.debug("Completed convert process")
diff --git a/lib/image.py b/lib/image.py
index 6f30b10..024a65c 100644
--- a/lib/image.py
+++ b/lib/image.py
@@ -96,6 +96,12 @@ def read_image_batch(filenames):
     Leverages multi-threading to load multiple images from disk at the same time
     leading to vastly reduced image read times.
 
+    Notes
+    -----
+    Images are loaded concurrently, so the order of the returned batch will likely not be the same
+    as the order of the input filenames. Filenames are returned with the batch in the correct order
+    corresponding to the returned batch.
+
     Parameters
     ----------
     filenames: list
@@ -103,6 +109,8 @@ def read_image_batch(filenames):
 
     Returns
     -------
+    list
+        Filenames in the correct order as they are returned
     numpy.ndarray
         The batch of images in `BGR` channel order.
 
@@ -113,16 +121,21 @@ def read_image_batch(filenames):
     Example
     -------
     >>> image_filenames = ["/path/to/image_1.png", "/path/to/image_2.png", "/path/to/image_3.png"]
-    >>> images = read_image_batch(image_filenames)
+    >>> filenames, images = read_image_batch(image_filenames)
     """
     logger.trace("Requested batch: '%s'", filenames)
     executor = futures.ThreadPoolExecutor()
     with executor:
-        images = [executor.submit(read_image, filename, raise_error=True)
-                  for filename in filenames]
-        batch = np.array([future.result() for future in futures.as_completed(images)])
-    logger.trace("Returning images: %s", batch.shape)
-    return batch
+        images = {executor.submit(read_image, filename, raise_error=True): filename
+                  for filename in filenames}
+        batch = []
+        filenames = []
+        for future in futures.as_completed(images):
+            batch.append(future.result())
+            filenames.append(images[future])
+        batch = np.array(batch)
+    logger.trace("Returning images: (filenames: %s, batch shape: %s)", filenames, batch.shape)
+    return filenames, batch
 
 
 def read_image_hash(filename):
diff --git a/lib/training_data.py b/lib/training_data.py
index 84c2010..b8cffd7 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -3,7 +3,6 @@
 
 import logging
 
-from hashlib import sha1
 from random import shuffle, choice
 
 import numpy as np
@@ -11,7 +10,6 @@ import cv2
 from scipy.interpolate import griddata
 
 from lib.image import batch_convert_color, read_image_batch
-from lib.model import masks
 from lib.multithreading import BackgroundGenerator
 from lib.utils import FaceswapError
 
@@ -40,7 +38,7 @@ class TrainingDataGenerator():
         contain the following keys:
 
         * **coverage_ratio** (`float`) - The ratio of the training image to be trained on. \
-        Dictates how much of the image will be cropped out. Eg: a coverage ratio of 0.625 \
+        Dictates how much of the image will be cropped out. E.G: a coverage ratio of 0.625 \
         will result in cropping a 160px box from a 256px image (256 * 0.625 = 160).
 
         * **augment_color** (`bool`) - ``True`` if color is to be augmented, otherwise ``False`` \
@@ -48,17 +46,17 @@ class TrainingDataGenerator():
         * **no_flip** (`bool`) - ``True`` if the image shouldn't be randomly flipped as part of \
         augmentation, otherwise ``False``
 
-        * **mask_type** (`str`) - The mask type to be used (as defined in \
-        :mod:`lib.model.masks`). If not ``None`` then the additional key ``landmarks`` must be \
-        provided.
-
         * **warp_to_landmarks** (`bool`) - ``True`` if the random warp method should warp to \
         similar landmarks from the other side, ``False`` if the standard random warp method \
         should be used. If ``True`` then the additional key ``landmarks`` must be provided.
 
-        * **landmarks** (`numpy.ndarray`, `optional`). Required if using a :attr:`mask_type` is \
-        not ``None`` or :attr:`warp_to_landmarks` is ``True``. The 68 point face landmarks from \
-        an alignments file.
+        * **landmarks** (`dict`, `optional`). Required if :attr:`warp_to_landmarks` is \
+        ``True``. Returning dictionary has a key of **side** (`str`) the value of which is a \
+        `dict` of {**filename** (`str`): **68 point landmarks** (`numpy.ndarray`)}.
+
+        * **masks** (`dict`, `optional`). Required if :attr:`penalized_mask_loss` or \
+        :attr:`learn_mask` is ``True``. Returning dictionary has a key of **side** (`str`) the \
+        value of which is a `dict` of {**filename** (`str`): :class:`lib.faces_detect.Mask`}.
 
     config: dict
         The configuration ``dict`` generated from :file:`config.train.ini` containing the trainer \
@@ -66,16 +64,20 @@ class TrainingDataGenerator():
     """
     def __init__(self, model_input_size, model_output_shapes, training_opts, config):
         logger.debug("Initializing %s: (model_input_size: %s, model_output_shapes: %s, "
-                     "training_opts: %s, landmarks: %s, config: %s)",
+                     "training_opts: %s, landmarks: %s, masks: %s, config: %s)",
                      self.__class__.__name__, model_input_size, model_output_shapes,
-                     {key: val for key, val in training_opts.items() if key != "landmarks"},
-                     bool(training_opts.get("landmarks", None)), config)
+                     {key: val
+                      for key, val in training_opts.items() if key not in ("landmarks", "masks")},
+                     {key: len(val)
+                      for key, val in training_opts.get("landmarks", dict()).items()},
+                     {key: len(val) for key, val in training_opts.get("masks", dict()).items()},
+                     config)
         self._config = config
         self._model_input_size = model_input_size
         self._model_output_shapes = model_output_shapes
         self._training_opts = training_opts
-        self._mask_class = self._set__mask_class()
         self._landmarks = self._training_opts.get("landmarks", None)
+        self._masks = self._training_opts.get("masks", None)
         self._nearest_landmarks = {}
 
         # Batchsize and processing class are set when this class is called by a batcher
@@ -90,7 +92,7 @@ class TrainingDataGenerator():
 
         The exit point from this class and the sole attribute that should be referenced. Called
         from :mod:`plugins.train.trainer._base`. Returns an iterator that yields images for
-        training, preview and timelapses.
+        training, preview and time-lapses.
 
         Parameters
         ----------
@@ -103,13 +105,13 @@ class TrainingDataGenerator():
             The side of the model that this iterator is for.
         do_shuffle: bool, optional
             Whether data should be shuffled prior to loading from disk. If true, each time the full
-            list of filenames are processed, the data will be reshuffled to make sure thay are not
+            list of filenames are processed, the data will be reshuffled to make sure they are not
             returned in the same order. Default: ``True``
         is_preview: bool, optional
             Indicates whether this iterator is generating preview images. If ``True`` then certain
             augmentations will not be performed. Default: ``False``
         is_timelapse: bool optional
-            Indicates whether this iterator is generating Timelapse images. If ``True``, then
+            Indicates whether this iterator is generating time-lapse images. If ``True``, then
             certain augmentations will not be performed. Default: ``False``
 
         Yields
@@ -130,14 +132,13 @@ class TrainingDataGenerator():
             :mod:`plugins.train.trainer._base` from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
-            in the ``dict`` if the :attr:`mask_type` is not ``None``
-
-            * **samples** (`numpy.ndarray`) - A 4-dimensional array containg the samples for \
-            feeding to the model's predict function for generating preview and timelapse samples. \
-            The array will be in the format (`batchsize`, `height`, `width`, `channels`). **NB:** \
-            This item will only exist in the ``dict`` if :attr:`is_preview` or \
-            :attr:`is_timelapse` is ``True``
+            the format (`batchsize`, `height`, `width`, `1`).
+
+            * **samples** (`numpy.ndarray`) - A 4-dimensional array containing the samples for \
+            feeding to the model's predict function for generating preview and time-lapse \
+            samples. The array will be in the format (`batchsize`, `height`, `width`, \
+            `channels`). **NB:** This item will only exist in the ``dict`` if :attr:`is_preview` \
+            or :attr:`is_timelapse` is ``True``
         """
         logger.debug("Queue batches: (image_count: %s, batchsize: %s, side: '%s', do_shuffle: %s, "
                      "is_preview, %s, is_timelapse: %s)", len(images), batchsize, side, do_shuffle,
@@ -154,18 +155,6 @@ class TrainingDataGenerator():
         return batcher.iterator()
 
     # << INTERNAL METHODS >> #
-    def _set__mask_class(self):
-        """ Returns the correct mask class from :mod:`lib`.model.masks` as defined in the
-        :attr:`mask_type` parameter. """
-        mask_type = self._training_opts.get("mask_type", None)
-        if mask_type:
-            logger.debug("Mask type: '%s'", mask_type)
-            _mask_class = getattr(masks, mask_type)
-        else:
-            _mask_class = None
-        logger.debug("Mask class: %s", _mask_class)
-        return _mask_class
-
     def _validate_samples(self, data):
         """ Ensures that the total number of images within :attr:`images` is greater or equal to
         the selected :attr:`batchsize`. Raises an exception if this is not the case. """
@@ -205,26 +194,26 @@ class TrainingDataGenerator():
         """ Performs the augmentation and compiles target images and samples. See
         :func:`minibatch_ab` for more details on the output. """
         logger.trace("Process batch: (filenames: '%s', side: '%s')", filenames, side)
-        batch = read_image_batch(filenames)
+        filenames, batch = read_image_batch(filenames)
+        batch = self._apply_mask(filenames, batch, side)
         processed = dict()
-        to_landmarks = self._training_opts["warp_to_landmarks"]
 
         # Initialize processing training size on first image
         if not self._processing.initialized:
             self._processing.initialize(batch.shape[1])
 
         # Get Landmarks prior to manipulating the image
-        if self._mask_class or to_landmarks:
-            batch_src_pts = self._get_landmarks(filenames, batch, side)
+        if self._training_opts["warp_to_landmarks"]:
+            batch_src_pts = self._get_landmarks(filenames, side)
+            batch_dst_pts = self._get_closest_match(filenames, side, batch_src_pts)
+            warp_kwargs = dict(batch_src_points=batch_src_pts,
+                               batch_dst_points=batch_dst_pts)
+        else:
+            warp_kwargs = dict()
 
-        # Color augmentation before mask is added
+        # Color Augmentation of the image only
         if self._training_opts["augment_color"]:
-            batch = self._processing.color_adjust(batch)
-
-        # Add mask to batch prior to transforms and warps
-        if self._mask_class:
-            batch = np.array([self._mask_class(src_pts, image, channels=4).mask
-                              for src_pts, image in zip(batch_src_pts, batch)])
+            batch[..., :3] = self._processing.color_adjust(batch[..., :3])
 
         # Random Transform and flip
         batch = self._processing.transform(batch)
@@ -238,15 +227,10 @@ class TrainingDataGenerator():
         # Get Targets
         processed.update(self._processing.get_targets(batch))
 
-        # Random Warp
-        if to_landmarks:
-            warp_kwargs = dict(batch_src_points=batch_src_pts,
-                               batch_dst_points=self._get_closest_match(filenames,
-                                                                        side,
-                                                                        batch_src_pts))
-        else:
-            warp_kwargs = dict()
-        processed["feed"] = self._processing.warp(batch[..., :3], to_landmarks, **warp_kwargs)
+        # Random Warp # TODO change masks to have a input mask and a warped target mask
+        processed["feed"] = [self._processing.warp(batch[..., :3],
+                                                   self._training_opts["warp_to_landmarks"],
+                                                   **warp_kwargs)]
 
         logger.trace("Processed batch: (filenames: %s, side: '%s', processed: %s)",
                      filenames,
@@ -256,21 +240,53 @@ class TrainingDataGenerator():
 
         return processed
 
-    def _get_landmarks(self, filenames, batch, side):
+    def _apply_mask(self, filenames, batch, side):
+        """ Applies the mask to the 4th channel of the image. If masks are not being used
+        applies a dummy all ones mask """
+        logger.trace("Input batch shape: %s, side: %s", batch.shape, side)
+        if self._masks is None:
+            logger.trace("Creating dummy masks. side: %s", side)
+            masks = np.ones_like(batch[..., :1], dtype=batch.dtype)
+        else:
+            logger.trace("Obtaining masks for batch. side: %s", side)
+            masks = np.array([self._masks[side][filename].mask
+                              for filename, face in zip(filenames, batch)], dtype=batch.dtype)
+            masks = self._resize_masks(batch.shape[1], masks)
+
+        logger.trace("masks shape: %s", masks.shape)
+        batch = np.concatenate((batch, masks), axis=-1)
+        logger.trace("Output batch shape: %s, side: %s", batch.shape, side)
+        return batch
+
+    @staticmethod
+    def _resize_masks(target_size, masks):
+        """ Resize the masks to the target size """
+        logger.trace("target size: %s, masks shape: %s", target_size, masks.shape)
+        mask_size = masks.shape[1]
+        if target_size == mask_size:
+            logger.trace("Mask and targets the same size. Not resizing")
+            return masks
+        interpolator = cv2.INTER_CUBIC if mask_size < target_size else cv2.INTER_AREA
+        masks = np.array([cv2.resize(mask,
+                                     (target_size, target_size),
+                                     interpolation=interpolator)[..., None]
+                          for mask in masks])
+        logger.trace("Resized masks: %s", masks.shape)
+        return masks
+
+    def _get_landmarks(self, filenames, side):
         """ Obtains the 68 Point Landmarks for the images in this batch. This is only called if
-        config item ``warp_to_landmarks`` is ``True`` or if :attr:`mask_type` is not ``None``. If
-        the landmarks for an image cannot be found, then an error is raised. """
+        config item ``warp_to_landmarks`` is ``True``. If the landmarks for an image cannot be
+        found, then an error is raised. """
         logger.trace("Retrieving landmarks: (filenames: %s, side: '%s')", filenames, side)
-        src_points = [self._landmarks[side].get(sha1(face).hexdigest(), None) for face in batch]
-
+        src_points = [self._landmarks[side].get(filename, None) for filename in filenames]
         # Raise error on missing alignments
         if not all(isinstance(pts, np.ndarray) for pts in src_points):
-            indices = [idx for idx, hsh in enumerate(src_points) if hsh is None]
-            missing = [filenames[idx] for idx in indices]
+            missing = [filenames[idx] for idx, pts in enumerate(src_points) if pts is None]
             msg = ("Files missing alignments for this batch: {}"
                    "\nAt least one of your images does not have a matching entry in your "
                    "alignments file."
-                   "\nIf you are training with a mask or using 'warp to landmarks' then every "
+                   "\nIf you are using 'warp to landmarks' then every "
                    "face you intend to train on must exist within the alignments file."
                    "\nThe specific files that caused this failure are listed above."
                    "\nMost likely there will be more than just these files missing from the "
@@ -319,7 +335,7 @@ class ImageAugmentation():
     batchsize: int
         The number of images that will be fed through the augmentation functions at once.
     is_display: bool
-        Whether the images being fed through will be used for Preview or Timelapse. Disables
+        Whether the images being fed through will be used for Preview or Time-lapse. Disables
         the "warp" augmentation for these images.
     input_size: int
         The expected input size for the model. It is assumed that the input to the model is always
@@ -330,8 +346,8 @@ class ImageAugmentation():
         are returned. The tuples should be in (`height`, `width`, `channels`) format.
     coverage_ratio: float
         The ratio of the training image to be trained on. Dictates how much of the image will be
-        cropped out. Eg: a coverage ratio of 0.625 will result in cropping a 160px box from a 256px
-        image (256 * 0.625 = 160).
+        cropped out. E.G: a coverage ratio of 0.625 will result in cropping a 160px box from a "
+        "256px image (256 * 0.625 = 160).
     config: dict
         The configuration ``dict`` generated from :file:`config.train.ini` containing the trainer \
         plugin configuration options.
@@ -342,7 +358,7 @@ class ImageAugmentation():
         Flag to indicate whether :class:`ImageAugmentation` has been initialized with the training
         image size in order to cache certain augmentation operations (see :func:`initialize`)
     is_display: bool
-        Flag to indicate whether these augmentations are for timelapses/preview images (``True``)
+        Flag to indicate whether these augmentations are for time-lapses/preview images (``True``)
         or standard training data (``False)``
     """
     def __init__(self, batchsize, is_display, input_size, output_shapes, coverage_ratio, config):
@@ -449,18 +465,17 @@ class ImageAugmentation():
             output they will be returned as their own item from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
-            in the ``dict`` if a batch of 4 channel images has been passed in :attr:`batch`
+            the format (`batchsize`, `height`, `width`, `1`).
         """
         logger.trace("Compiling targets")
         slices = self._constants["tgt_slices"]
         target_batch = [np.array([cv2.resize(image[slices, slices, :],
                                              (size, size),
                                              cv2.INTER_AREA)
-                                  for image in batch])
+                                  for image in batch], dtype='float32') / 255.
                         for size in self._output_sizes]
         logger.trace("Target image shapes: %s",
-                     [tgt.shape for tgt_images in target_batch for tgt in tgt_images])
+                     [tgt_images.shape[1:] for tgt_images in target_batch])
 
         retval = self._separate_target_mask(target_batch)
         logger.trace("Final targets: %s",
@@ -469,25 +484,19 @@ class ImageAugmentation():
         return retval
 
     @staticmethod
-    def _separate_target_mask(batch):
+    def _separate_target_mask(target_batch):
         """ Return the batch and the batch of final masks
 
         Returns the targets as a list of 4-dimensional ``numpy.ndarray`` s of shape (`batchsize`,
-        `height`, `width`, 3). If the :attr:`batch` is 4 channels, then the masks will be split
-        from the batch, with the largest output masks being returned in their own item.
+        `height`, `width`, 3).
+
+        The target masks are returned as its own item and is the 4th channel of the final target
+        output.
         """
-        batch = [tgt.astype("float32") / 255.0 for tgt in batch]
-        if all(tgt.shape[-1] == 4 for tgt in batch):
-            logger.trace("Batch contains mask")
-            sizes = [item.shape[1] for item in batch]
-            mask_batch = np.expand_dims(batch[sizes.index(max(sizes))][..., -1], axis=-1)
-            batch = [item[..., :3] for item in batch]
-            logger.trace("batch shapes: %s, mask_batch shape: %s",
-                         [tgt.shape for tgt in batch], mask_batch.shape)
-            retval = dict(targets=batch, masks=mask_batch)
-        else:
-            logger.trace("Batch has no mask")
-            retval = dict(targets=batch)
+        logger.trace("target_batch shapes: %s", [tgt.shape for tgt in target_batch])
+        retval = dict(targets=[batch[..., :3] for batch in target_batch],
+                      masks=[target_batch[-1][..., 3:]])
+        logger.trace("returning: %s", {k: [tgt.shape for tgt in v] for k, v in retval.items()})
         return retval
 
     # <<< COLOR AUGMENTATION >>> #
@@ -517,7 +526,7 @@ class ImageAugmentation():
         return batch
 
     def _random_clahe(self, batch):
-        """ Randomly perform Contrast Limited Adaptive Histogram Equilization on
+        """ Randomly perform Contrast Limited Adaptive Histogram Equalization on
         a batch of images """
         base_contrast = self._constants["clahe_base_contrast"]
 
@@ -540,7 +549,8 @@ class ImageAugmentation():
         return batch
 
     def _random_lab(self, batch):
-        """ Perform random color/lightness adjustment in L*a*b* colorspace on a batch of images """
+        """ Perform random color/lightness adjustment in L*a*b* color space on a batch of
+        images """
         amount_l = self._config.get("color_lightness", 30) / 100
         amount_ab = self._config.get("color_ab", 8) / 100
         adjust = np.array([amount_l, amount_ab, amount_ab], dtype="float32")
diff --git a/lib/utils.py b/lib/utils.py
index b9af6cb..272c44f 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -413,6 +413,7 @@ class GetModel():
                     break
                 pbar.update(len(buffer))
                 out_file.write(buffer)
+            pbar.close()
 
     def unzip_model(self):
         """ Unzip the model file to the cachedir """
@@ -446,3 +447,4 @@ class GetModel():
                     pbar.update(len(buffer))
                     out_file.write(buffer)
         zip_file.close()
+        pbar.close()
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index 31d1dd5..0326bc7 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -8,8 +8,8 @@ import sys
 from importlib import import_module
 
 from lib.config import FaceswapConfig
-from lib.model.masks import get_available_masks
 from lib.utils import full_path_split
+from plugins.plugin_loader import PluginLoader
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
@@ -70,19 +70,49 @@ class Config(FaceswapConfig):
                  "\n\t100.0%% is a mugshot.")
         self.add_item(
             section=section, title="mask_type", datatype=str, default="none",
-            choices=get_available_masks(), group="mask",
-            info="The mask to be used for training:"
-                 "\n\t none: Doesn't use any mask."
-                 "\n\t components: An improved face hull mask using a facehull of 8 facial parts"
-                 "\n\t dfl_full: An improved face hull mask using a facehull of 3 facial parts"
-                 "\n\t extended: Based on components mask. Extends the eyebrow points to further "
-                 "up the forehead. May perform badly on difficult angles."
-                 "\n\t facehull: Face cutout based on landmarks")
+            choices=PluginLoader.get_available_extractors("mask", add_none=True), group="mask",
+            gui_radio=True,
+            info="The mask to be used for training. If you have selected 'Learn Mask' or "
+                 "'Penalized Mask Loss' you must select a value other than 'none'. The required "
+                 "mask should have been selected as part of the Extract process. If it does not "
+                 "exist in the alignments file then it will be generated prior to training "
+                 "commencing."
+                 "\n\tnone: Don't use a mask."
+                 "\n\tcomponents: Mask designed to provide facial segmentation based on the "
+                 "positioning of landmark locations. A convex hull is constructed around the "
+                 "exterior of the landmarks to create a mask."
+                 "\n\textended: Mask designed to provide facial segmentation based on the "
+                 "positioning of landmark locations. A convex hull is constructed around the "
+                 "exterior of the landmarks and the mask is extended upwards onto the forehead."
+                 "\n\tvgg-clear: Mask designed to provide smart segmentation of mostly frontal "
+                 "faces clear of obstructions. Profile faces and obstructions may result in "
+                 "sub-par performance."
+                 "\n\tvgg-obstructed: Mask designed to provide smart segmentation of mostly "
+                 "frontal faces. The mask model has been specifically trained to recognize "
+                 "some facial obstructions (hands and eyeglasses). Profile faces may result in "
+                 "sub-par performance."
+                 "\n\tunet-dfl: Mask designed to provide smart segmentation of mostly frontal "
+                 "faces. The mask model has been trained by community members and will need "
+                 "testing for further description. Profile faces may result in sub-par "
+                 "performance.")
         self.add_item(
-            section=section, title="mask_blur", datatype=bool, default=False, group="mask",
+            section=section, title="mask_blur_kernel", datatype=int, min_max=(0, 9),
+            rounding=1, default=3, group="mask",
             info="Apply gaussian blur to the mask input. This has the effect of smoothing the "
-                 "edges of the mask, which can help with poorly calculated masks, and give less "
-                 "of a hard edge to the predicted mask.")
+                 "edges of the mask, which can help with poorly calculated masks and give less "
+                 "of a hard edge to the predicted mask. The size is in pixels (calculated from "
+                 "a 128px mask). Set to 0 to not apply gaussian blur. This value should be odd, "
+                 "if an even number is passed in then it will be rounded to the next odd number.")
+        self.add_item(
+            section=section, title="mask_threshold", datatype=int, default=4,
+            min_max=(0, 50), rounding=1, group="mask",
+            info="Sets pixels that are near white to white and near black to black. Set to 0 for "
+                 "off.")
+        self.add_item(
+            section=section, title="learn_mask", datatype=bool, default=False, group="mask",
+            info="Dedicate a portion of the model to learning how to duplicate the input "
+                 "mask. Increases VRAM usage in exchange for learning a quick ability to try "
+                 "to replicate more complex mask models.")
         self.add_item(
             section=section, title="icnr_init", datatype=bool,
             default=False, group="initialization",
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index df561c3..2b31f97 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -105,7 +105,18 @@ class ModelBase():
                               "augment_color": augment_color,
                               "no_flip": no_flip,
                               "pingpong": self.vram_savings.pingpong,
-                              "snapshot_interval": snapshot_interval}
+                              "snapshot_interval": snapshot_interval,
+                              "training_size": self.state.training_size,
+                              "no_logs": self.state.current_session["no_logs"],
+                              "coverage_ratio": self.calculate_coverage_ratio(),
+                              "mask_type": self.config["mask_type"],
+                              "mask_blur_kernel": self.config["mask_blur_kernel"],
+                              "mask_threshold": self.config["mask_threshold"],
+                              "learn_mask": (self.config["learn_mask"] and
+                                             self.config["mask_type"] is not None),
+                              "penalized_mask_loss": (self.config["penalized_mask_loss"] and
+                                                      self.config["mask_type"] is not None)}
+        logger.debug("training_opts: %s", self.training_opts)
 
         if self.multiple_models_in_folder:
             deprecation_warning("Support for multiple model types within the same folder",
@@ -113,7 +124,6 @@ class ModelBase():
                                                 "avoid issues in future.")
 
         self.build()
-        self.set_training_data()
         logger.debug("Initialized ModelBase (%s)", self.__class__.__name__)
 
     @property
@@ -206,6 +216,12 @@ class ModelBase():
         logger.debug(retval)
         return retval
 
+    @property
+    def feed_mask(self):
+        """ bool: ``True`` if the model expects a mask to be fed into input otherwise ``False`` """
+        return self.config["mask_type"] is not None and (self.config["learn_mask"] or
+                                                         self.config["penalized_mask_loss"])
+
     def load_config(self):
         """ Load the global config for reference in self.config """
         global _CONFIG  # pylint: disable=global-statement
@@ -214,18 +230,6 @@ class ModelBase():
             logger.debug("Loading config for: %s", model_name)
             _CONFIG = Config(model_name, configfile=self.configfile).config_dict
 
-    def set_training_data(self):
-        """ Override to set model specific training data.
-
-            super() this method for defaults otherwise be sure to add """
-        logger.debug("Setting training data")
-        # Force number of preview images to between 2 and 16
-        self.training_opts["training_size"] = self.state.training_size
-        self.training_opts["no_logs"] = self.state.current_session["no_logs"]
-        self.training_opts["mask_type"] = self.config.get("mask_type", None)
-        self.training_opts["coverage_ratio"] = self.calculate_coverage_ratio()
-        logger.debug("Set training data: %s", self.training_opts)
-
     def calculate_coverage_ratio(self):
         """ Coverage must be a ratio, leading to a cropped shape divisible by 2 """
         coverage_ratio = self.config.get("coverage", 62.5) / 100
@@ -260,12 +264,11 @@ class ModelBase():
         logger.debug("Getting inputs")
         inputs = [Input(shape=self.input_shape, name="face_in")]
         output_network = [network for network in self.networks.values() if network.is_output][0]
-        mask_idx = [idx for idx, name in enumerate(output_network.output_names)
-                    if name.startswith("mask")]
-        if mask_idx:
-            # Add the final mask shape as input
-            mask_shape = output_network.output_shapes[mask_idx[0]]
-            inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
+        if self.feed_mask:
+            # TODO penalized mask doesn't have a mask output, so we can't use output shapes
+            # mask should always be last output..this needs to be a rule
+            mask_shape = output_network.output_shapes[-1]
+            inputs.append(Input(shape=(mask_shape[1:-1] + (1,)), name="mask_in"))
         logger.debug("Got inputs: %s", inputs)
         return inputs
 
@@ -374,7 +377,7 @@ class ModelBase():
         opt_kwargs = dict(lr=lr, beta_1=beta_1, beta_2=beta_2)
         if (self.config.get("clipnorm", False) and
                 keras.backend.backend() != "plaidml.keras.backend"):
-            # NB: Clipnorm is ballooning VRAM useage, which is not expected behaviour
+            # NB: Clipnorm is ballooning VRAM usage, which is not expected behavior
             # and may be a bug in Keras/TF.
             # PlaidML has a bug regarding the clipnorm parameter
             # See: https://github.com/plaidml/plaidml/issues/228
@@ -444,7 +447,7 @@ class ModelBase():
             logger.error("Model could not be found in folder '%s'. Exiting", self.model_dir)
             exit(0)
 
-        if not self.is_legacy:
+        if not self.is_legacy or not self.predict:
             K.clear_session()
         model_mapping = self.map_models(swapped)
         for network in self.networks.values():
@@ -579,6 +582,9 @@ class ModelBase():
         self.state.config["subpixel_upscaling"] = False
         self.state.config["reflect_padding"] = False
         self.state.config["mask_type"] = None
+        self.state.config["mask_blur_kernel"] = 3
+        self.state.config["mask_threshold"] = 4
+        self.state.config["learn_mask"] = False
         self.state.config["lowmem"] = False
         self.encoder_dim = 1024
 
@@ -624,7 +630,7 @@ class VRAMSavings():
         return optimizer_savings
 
     def set_gradient_type(self, memory_saving_gradients):
-        """ Monkeypatch Memory Saving Gradients if requested """
+        """ Monkey-patch Memory Saving Gradients if requested """
         if memory_saving_gradients and self.is_plaidml:
             logger.warning("Memory Saving Gradients not supported on plaidML. Disabling")
             memory_saving_gradients = False
@@ -743,7 +749,7 @@ class Loss():
         for idx, loss_name in enumerate(self.names):
             if loss_name.startswith("mask"):
                 loss_funcs.append(self.selected_mask_loss)
-            elif self.mask_input is not None and self.config.get("penalized_mask_loss", False):
+            elif self.config["penalized_mask_loss"] and self.config["mask_type"] is not None:
                 face_size = self.output_shapes[idx][1]
                 mask_size = self.mask_shape[1]
                 scaling = face_size / mask_size
@@ -980,12 +986,12 @@ class State():
             Check for any fixed=False parameters changes and log info changes
         """
         global _CONFIG  # pylint: disable=global-statement
+        legacy_update = self._update_legacy_config()
         # Add any new items to state config for legacy purposes
         for key, val in _CONFIG.items():
             if key not in self.config.keys():
                 logger.info("Adding new config item to state file: '%s': '%s'", key, val)
                 self.config[key] = val
-        legacy_update = self.update_legacy_config()
         self.update_changed_config_items(config_changeable_items)
         logger.debug("Replacing config. Old config: %s", _CONFIG)
         _CONFIG = self.config
@@ -994,18 +1000,63 @@ class State():
         logger.debug("Replaced config. New config: %s", _CONFIG)
         logger.info("Using configuration saved in state file")
 
-    def update_legacy_config(self):
-        """ Update legacy state config files with the new loss formating
+    def _update_legacy_config(self):
+        """ Legacy updates for new config additions.
+
+        When new config items are added to the Faceswap code, existing model state files need to be
+        updated to handle these new items.
+
+        Current existing legacy update items:
+
+            * loss - If old `dssim_loss` is ``true`` set new `loss_function` to `ssim` otherwise
+            set it to `mae`. Remove old `dssim_loss` item
+
+            * masks - If `penalized_mask_loss` exists but `learn_mask` does not, then add the
+            latter and set to the same value as `penalized_mask_loss`.
+
+            * masks type - Replace removed masks 'dfl_full' and 'facehull' with `components` mask
+
+        Returns
+        -------
+        bool
+            ``True`` if legacy items exist and state file has been updated, otherwise ``False``
         """
-        prior = "dssim_loss"
-        new = "loss_function"
-        if prior not in self.config:
-            return False
-        self.config[new] = "ssim" if self.config[prior] else "mae"
-        del self.config[prior]
-        logger.info("Updated config from older dssim format. New config loss function: %s",
-                    self.config[new])
-        return True
+        logger.debug("Checking for legacy state file update")
+        priors = ["dssim_loss", "penalized_mask_loss", "mask_type"]
+        new_items = ["loss_function", "learn_mask", "mask_type"]
+        updated = False
+        for old, new in zip(priors, new_items):
+            if old not in self.config:
+                logger.debug("Legacy item '%s' not in config. Skipping update", old)
+                continue
+
+            # dssim_loss > loss_function
+            if old == "dssim_loss":
+                self.config[new] = "ssim" if self.config[old] else "mae"
+                del self.config[old]
+                updated = True
+                logger.info("Updated config from legacy dssim format. New config loss "
+                            "function: '%s'", self.config[new])
+                continue
+
+            # Add learn mask option and set to True if model has "penalized_mask_loss" specified
+            if old == "penalized_mask_loss" and new not in self.config:
+                self.config[new] = self.config["penalized_mask_loss"]
+                updated = True
+                logger.info("Added new 'learn_mask' config item for this model. Value set to: %s",
+                            self.config[new])
+                continue
+
+            # Replace removed masks with most similar equivalent
+            if old == "mask_type" and self.config[old] in ("facehull", "dfl_full"):
+                old_mask = self.config[old]
+                self.config[new] = "components"
+                updated = True
+                logger.info("Updated 'mask_type' from '%s' to '%s' for this model",
+                            old_mask, self.config[new])
+
+        logger.debug("State file updated for legacy config: %s", updated)
+        return updated
 
     def update_changed_config_items(self, config_changeable_items):
         """ Update any parameters which are not fixed and have been changed """
diff --git a/plugins/train/model/dfaker.py b/plugins/train/model/dfaker.py
index 758c43b..d6b19be 100644
--- a/plugins/train/model/dfaker.py
+++ b/plugins/train/model/dfaker.py
@@ -40,7 +40,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/dfl_h128.py b/plugins/train/model/dfl_h128.py
index 6bf9a6f..887d379 100644
--- a/plugins/train/model/dfl_h128.py
+++ b/plugins/train/model/dfl_h128.py
@@ -50,8 +50,8 @@ class Model(OriginalModel):
                                    activation="sigmoid",
                                    name="face_out")
         outputs = [var_x]
-        # Mask
-        if self.config.get("mask_type", None):
+
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, self.encoder_dim)
             var_y = self.blocks.upscale(var_y, self.encoder_dim // 2)
diff --git a/plugins/train/model/dfl_sae.py b/plugins/train/model/dfl_sae.py
index e79822c..4d22121 100644
--- a/plugins/train/model/dfl_sae.py
+++ b/plugins/train/model/dfl_sae.py
@@ -31,7 +31,7 @@ class Model(ModelBase):
     @property
     def use_mask(self):
         """ Return True if a mask has been set else false """
-        return self.config.get("mask_type", None) is not None
+        return self.config.get("learn_mask", False)
 
     @property
     def ae_dims(self):
diff --git a/plugins/train/model/iae.py b/plugins/train/model/iae.py
index b164fef..775305e 100644
--- a/plugins/train/model/iae.py
+++ b/plugins/train/model/iae.py
@@ -77,7 +77,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/lightweight.py b/plugins/train/model/lightweight.py
index 1963c8c..366e280 100644
--- a/plugins/train/model/lightweight.py
+++ b/plugins/train/model/lightweight.py
@@ -47,7 +47,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/original.py b/plugins/train/model/original.py
index 55d3bea..fa79862 100644
--- a/plugins/train/model/original.py
+++ b/plugins/train/model/original.py
@@ -73,7 +73,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
diff --git a/plugins/train/model/realface.py b/plugins/train/model/realface.py
index 10562b8..48df05c 100644
--- a/plugins/train/model/realface.py
+++ b/plugins/train/model/realface.py
@@ -134,7 +134,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("mask_type", None) is not None:
+        if self.config.get("learn_mask", False):
             var_y = var_xy
             mask_b_complexity = 384
             for idx in range(self.upscalers_no-2):
@@ -184,7 +184,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("mask_type", None) is not None:
+        if self.config.get("learn_mask", False):
             var_y = var_xy
             mask_a_complexity = 384
             for idx in range(self.upscalers_no-2):
diff --git a/plugins/train/model/unbalanced.py b/plugins/train/model/unbalanced.py
index b8c2a08..d7e136f 100644
--- a/plugins/train/model/unbalanced.py
+++ b/plugins/train/model/unbalanced.py
@@ -80,7 +80,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             var_y = self.blocks.upscale(var_y, decoder_complexity)
@@ -129,7 +129,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             if not self.lowmem:
diff --git a/plugins/train/model/villain.py b/plugins/train/model/villain.py
index 4a0a67a..90c2020 100644
--- a/plugins/train/model/villain.py
+++ b/plugins/train/model/villain.py
@@ -78,7 +78,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("learn_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index 2e71e07..98ebd95 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -1,24 +1,45 @@
 #!/usr/bin/env python3
+""" Base Class for Faceswap Trainer plugins. All Trainer plugins should be inherited from
+this class.
 
+At present there is only the :class:`~plugins.train.trainer.original` plugin, so that entirely
+inherits from this class.
 
-""" Base Trainer Class for Faceswap
-
-    Trainers should be inherited from this class.
-
-    A training_opts dictionary can be set in the corresponding model.
-    Accepted values:
-        alignments:         dict containing paths to alignments files for keys 'a' and 'b'
-        preview_scaling:    How much to scale the preview out by
-        training_size:      Size of the training images
-        coverage_ratio:     Ratio of face to be cropped out for training
-        mask_type:          Type of mask to use. See lib.model.masks for valid mask names.
-                            Set to None for not used
-        no_logs:            Disable tensorboard logging
-        snapshot_interval:  Interval for saving model snapshots
-        warp_to_landmarks:  Use random_warp_landmarks instead of random_warp
-        augment_color:      Perform random shifting of L*a*b* colors
-        no_flip:            Don't perform a random flip on the image
-        pingpong:           Train each side seperately per save iteration rather than together
+This class heavily references the :attr:`plugins.train.model._base.ModelBase.training_opts`
+``dict``. The following keys are expected from this ``dict``:
+
+    * **alignments** (`dict`, `optional`) - If training with a mask or the warp to landmarks \
+    command line option is selected then this is required, otherwise it can be ``None``. The \
+    dictionary should contain 2 keys ("a" and "b") with the values being the path to the \
+    alignments file for the corresponding side.
+
+    * **preview_scaling** (`int`) - How much to scale displayed preview image by.
+
+    * **training_size** ('int') - Size of the training images in pixels.
+
+    * **coverage_ratio** ('float') - Ratio of face to be cropped out of the training image.
+
+    * **mask_type** ('str') - The type of mask to select from the alignments file.
+
+    * **mask_blur_kernel** ('int') - The size of the kernel to use for gaussian blurring the mask.
+
+    * **mask_threshold** ('int') - The threshold for min/maxing mask to 0/100.
+
+    * **learn_mask** ('bool') - Whether the mask should be trained in the model.
+
+    * **penalized_mask_loss** ('bool') - Whether the mask should be penalized from loss.
+
+    * **no_logs** ('bool') - Whether Tensorboard logging should be disabled.
+
+    * **snapshot_interval** ('int') - How many iterations between model snapshot saves.
+
+    * **warp_to_landmarks** ('bool') - Whether to use random_warp_landmarks instead of random_warp.
+
+    * **augment_color** ('bool') - Whether to use color augmentation.
+
+    * **no_flip** ('bool') - Whether to turn off random horizontal flipping.
+
+    * **pingpong** ('bool') - Train each side separately per save iteration rather than together.
 """
 
 import logging
@@ -30,9 +51,11 @@ import numpy as np
 
 import tensorflow as tf
 from tensorflow.python import errors_impl as tf_errors  # pylint:disable=no-name-in-module
+from tqdm import tqdm
 
 from lib.alignments import Alignments
 from lib.faces_detect import DetectedFace
+from lib.image import read_image_hash_batch
 from lib.training_data import TrainingDataGenerator
 from lib.utils import FaceswapError, get_folder, get_image_paths
 from plugins.train._config import Config
@@ -40,80 +63,133 @@ from plugins.train._config import Config
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
-def get_config(plugin_name, configfile=None):
-    """ Return the config for the requested model """
+def _get_config(plugin_name, configfile=None):
+    """ Return the configuration for the requested trainer.
+
+    Parameters
+    ----------
+    plugin_name: str
+        The name of the plugin to load the configuration for
+    configfile: str, optional
+        A custom configuration file. If ``None`` then configuration is loaded from the default
+        :file:`.config.train.ini` file. Default: ``None``
+
+    Returns
+    -------
+    :class:`lib.config.FaceswapConfig`
+        The configuration file for the requested plugin
+    """
     return Config(plugin_name, configfile=configfile).config_dict
 
 
 class TrainerBase():
-    """ Base Trainer """
+    """ Trainer plugin base Object.
+
+    All Trainer plugins must inherit from this class.
+
+    Parameters
+    ----------
+    model: plugin from :mod:`plugins.train.model`
+        The model that will be running this trainer
+    images: dict
+        The file paths for the images to be trained on for each side. The dictionary should contain
+        2 keys ("a" and "b") with the values being a list of full paths corresponding to each side.
+    batch_size: int
+        The requested batch size for iteration to be trained through the model.
+    configfile: str
+        The path to a custom configuration file. If ``None`` is passed then configuration is loaded
+        from the default :file:`.config.train.ini` file.
+    """
 
     def __init__(self, model, images, batch_size, configfile):
         logger.debug("Initializing %s: (model: '%s', batch_size: %s)",
                      self.__class__.__name__, model, batch_size)
-        self.config = get_config(".".join(self.__module__.split(".")[-2:]), configfile=configfile)
-        self.batch_size = batch_size
-        self.model = model
-        self.model.state.add_session_batchsize(batch_size)
-        self.images = images
-        self.sides = sorted(key for key in self.images.keys())
-
-        self.process_training_opts()
-        self.pingpong = PingPong(model, self.sides)
-
-        self.batchers = {side: Batcher(side,
-                                       images[side],
-                                       self.model,
-                                       self.use_mask,
-                                       batch_size,
-                                       self.config)
-                         for side in self.sides}
-
-        self.tensorboard = self.set_tensorboard()
-        self.samples = Samples(self.model,
-                               self.use_mask,
-                               self.model.training_opts["coverage_ratio"],
-                               self.model.training_opts["preview_scaling"])
-        self.timelapse = Timelapse(self.model,
-                                   self.use_mask,
-                                   self.model.training_opts["coverage_ratio"],
-                                   self.config.get("preview_images", 14),
-                                   self.batchers)
+        self._config = _get_config(".".join(self.__module__.split(".")[-2:]),
+                                   configfile=configfile)
+        self._model = model
+        self._model.state.add_session_batchsize(batch_size)
+        self._images = images
+        self._sides = sorted(key for key in self._images.keys())
+
+        self._process_training_opts()
+        self._pingpong = PingPong(model, self._sides)
+
+        self._batchers = {side: Batcher(side,
+                                        images[side],
+                                        self._model,
+                                        self._use_mask,
+                                        batch_size,
+                                        self._config)
+                          for side in self._sides}
+
+        self._tensorboard = self._set_tensorboard()
+        self._samples = Samples(self._model,
+                                self._use_mask,
+                                self._model.training_opts["coverage_ratio"],
+                                self._model.training_opts["preview_scaling"])
+        self._timelapse = Timelapse(self._model,
+                                    self._use_mask,
+                                    self._model.training_opts["coverage_ratio"],
+                                    self._config.get("preview_images", 14),
+                                    self._batchers)
         logger.debug("Initialized %s", self.__class__.__name__)
 
     @property
-    def timestamp(self):
-        """ Standardised timestamp for loss reporting """
+    def pingpong(self):
+        """ :class:`pingpong`: Ping-pong object for ping-pong memory saving training. """
+        return self._pingpong
+
+    @property
+    def _timestamp(self):
+        """ str: Current time formatted as HOURS:MINUTES:SECONDS """
         return time.strftime("%H:%M:%S")
 
     @property
-    def landmarks_required(self):
-        """ Return True if Landmarks are required """
-        opts = self.model.training_opts
-        retval = bool(opts.get("mask_type", None) or opts["warp_to_landmarks"])
+    def _landmarks_required(self):
+        """ bool: ``True`` if Landmarks are required otherwise ``False ``"""
+        retval = self._model.training_opts["warp_to_landmarks"]
         logger.debug(retval)
         return retval
 
     @property
-    def use_mask(self):
-        """ Return True if a mask is requested """
-        retval = bool(self.model.training_opts.get("mask_type", None))
+    def _use_mask(self):
+        """ bool: ``True`` if a mask is required otherwise ``False`` """
+        retval = (self._model.training_opts["learn_mask"] or
+                  self._model.training_opts["penalized_mask_loss"])
         logger.debug(retval)
         return retval
 
-    def process_training_opts(self):
-        """ Override for processing model specific training options """
-        logger.debug(self.model.training_opts)
-        if self.landmarks_required:
-            landmarks = Landmarks(self.model.training_opts).landmarks
-            self.model.training_opts["landmarks"] = landmarks
+    def _process_training_opts(self):
+        """ Extrapolate alignments and masks from the alignments file into
+        :attr:`_model.training_opts`."""
+        logger.debug(self._model.training_opts)
+        if not self._landmarks_required and not self._use_mask:
+            return
 
-    def set_tensorboard(self):
-        """ Set up tensorboard callback """
-        if self.model.training_opts["no_logs"]:
+        alignments = TrainingAlignments(self._model.training_opts, self._images)
+        if self._landmarks_required:
+            logger.debug("Adding landmarks to training opts dict")
+            self._model.training_opts["landmarks"] = alignments.landmarks
+
+        if self._use_mask:
+            logger.debug("Adding masks to training opts dict")
+            self._model.training_opts["masks"] = alignments.masks
+
+    def _set_tensorboard(self):
+        """ Set up Tensorboard callback for logging loss.
+
+        Bypassed if command line option "no-logs" has been selected.
+
+        Returns
+        -------
+        dict:
+            2 Dictionary keys of "a" and "b" the values of which are the
+        :class:`tf.keras.callbacks.TensorBoard` objects for the respective sides.
+        """
+        if self._model.training_opts["no_logs"]:
             logger.verbose("TensorBoard logging disabled")
             return None
-        if self.pingpong.active:
+        if self._pingpong.active:
             # Currently TensorBoard uses the tf.session, meaning that VRAM does not
             # get cleared when model switching
             # TODO find a fix for this
@@ -125,21 +201,23 @@ class TrainerBase():
         logger.debug("Enabling TensorBoard Logging")
         tensorboard = dict()
 
-        for side in self.sides:
+        for side in self._sides:
             logger.debug("Setting up TensorBoard Logging. Side: %s", side)
-            log_dir = os.path.join(str(self.model.model_dir),
-                                   "{}_logs".format(self.model.name),
+            log_dir = os.path.join(str(self._model.model_dir),
+                                   "{}_logs".format(self._model.name),
                                    side,
-                                   "session_{}".format(self.model.state.session_id))
-            tbs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, **self.tensorboard_kwargs)
-            tbs.set_model(self.model.predictors[side])
+                                   "session_{}".format(self._model.state.session_id))
+            tbs = tf.keras.callbacks.TensorBoard(log_dir=log_dir, **self._tensorboard_kwargs)
+            tbs.set_model(self._model.predictors[side])
             tensorboard[side] = tbs
         logger.info("Enabled TensorBoard Logging")
         return tensorboard
 
     @property
-    def tensorboard_kwargs(self):
-        """ TF 1.13 + needs an additional kwarg which is not valid for earlier versions """
+    def _tensorboard_kwargs(self):
+        """ dict: The keyword arguments to be passed to :class:`tf.keras.callbacks.TensorBoard`.
+        NB: Tensorflow 1.13 + needs an additional keyword argument which is not valid for earlier
+        versions """
         kwargs = dict(histogram_freq=0,  # Must be 0 or hangs
                       batch_size=64,
                       write_graph=True,
@@ -153,126 +231,198 @@ class TrainerBase():
         logger.debug(kwargs)
         return kwargs
 
-    def print_loss(self, loss):
-        """ Override for specific model loss formatting """
+    def __print_loss(self, loss):
+        """ Outputs the loss for the current iteration to the console.
+
+        Parameters
+        ----------
+        loss: dict
+            The loss for each side. The dictionary should contain 2 keys ("a" and "b") with the
+            values being a list of loss values for the current iteration corresponding to
+            each side.
+         """
         logger.trace(loss)
         output = ["Loss {}: {:.5f}".format(side.capitalize(), loss[side][0])
                   for side in sorted(loss.keys())]
         output = ", ".join(output)
-        print("[{}] [#{:05d}] {}".format(self.timestamp, self.model.iterations, output), end='\r')
+        print("[{}] [#{:05d}] {}".format(self._timestamp,
+                                         self._model.iterations,
+                                         output), end='\r')
 
     def train_one_step(self, viewer, timelapse_kwargs):
-        """ Train a batch """
-        logger.trace("Training one step: (iteration: %s)", self.model.iterations)
+        """ Running training on a batch of images for each side.
+
+        Triggered from the training cycle in :class:`scripts.train.Train`.
+
+        Notes
+        -----
+        As every iteration is called explicitly, the Parameters defined should always be ``None``
+        except on save iterations.
+
+        Parameters
+        ----------
+        viewer: :func:`scripts.train.Train._show`
+            The function that will display the preview image
+        timelapse_kwargs: dict
+            The keyword arguments for generating time-lapse previews. If a time-lapse preview is
+            not required then this should be ``None``. Otherwise all values should be full paths
+            the keys being `input_a`, `input_b`, `output`.
+        """
+        logger.trace("Training one step: (iteration: %s)", self._model.iterations)
         do_preview = viewer is not None
         do_timelapse = timelapse_kwargs is not None
-        snapshot_interval = self.model.training_opts.get("snapshot_interval", 0)
+        snapshot_interval = self._model.training_opts.get("snapshot_interval", 0)
         do_snapshot = (snapshot_interval != 0 and
-                       self.model.iterations >= snapshot_interval and
-                       self.model.iterations % snapshot_interval == 0)
+                       self._model.iterations >= snapshot_interval and
+                       self._model.iterations % snapshot_interval == 0)
 
         loss = dict()
         try:
-            for side, batcher in self.batchers.items():
-                if self.pingpong.active and side != self.pingpong.side:
+            for side, batcher in self._batchers.items():
+                if self._pingpong.active and side != self._pingpong.side:
                     continue
-                loss[side] = batcher.train_one_batch(do_preview)
+                loss[side] = batcher.train_one_batch()
                 if not do_preview and not do_timelapse:
                     continue
                 if do_preview:
-                    self.samples.images[side] = batcher.compile_sample(None)
+                    batcher.generate_preview(do_preview)
+                    self._samples.images[side] = batcher.compile_sample(None)
                 if do_timelapse:
-                    self.timelapse.get_sample(side, timelapse_kwargs)
+                    self._timelapse.get_sample(side, timelapse_kwargs)
 
-            self.model.state.increment_iterations()
+            self._model.state.increment_iterations()
 
             for side, side_loss in loss.items():
-                self.store_history(side, side_loss)
-                self.log_tensorboard(side, side_loss)
+                self._store_history(side, side_loss)
+                self._log_tensorboard(side, side_loss)
 
-            if not self.pingpong.active:
-                self.print_loss(loss)
+            if not self._pingpong.active:
+                self.__print_loss(loss)
             else:
                 for key, val in loss.items():
-                    self.pingpong.loss[key] = val
-                self.print_loss(self.pingpong.loss)
+                    self._pingpong.loss[key] = val
+                self.__print_loss(self._pingpong.loss)
 
             if do_preview:
-                samples = self.samples.show_sample()
+                samples = self._samples.show_sample()
                 if samples is not None:
                     viewer(samples, "Training - 'S': Save Now. 'ENTER': Save and Quit")
 
             if do_timelapse:
-                self.timelapse.output_timelapse()
+                self._timelapse.output_timelapse()
 
             if do_snapshot:
-                self.model.do_snapshot()
+                self._model.do_snapshot()
         except Exception as err:
             raise err
 
-    def store_history(self, side, loss):
-        """ Store the history of this step """
+    def _store_history(self, side, loss):
+        """ Store the loss for this step into :attr:`model.history`.
+
+        Parameters
+        ----------
+        side: {"a", "b"}
+            The side to store the loss for
+        loss: list
+            The list of loss ``floats`` for this side
+        """
         logger.trace("Updating loss history: '%s'", side)
-        self.model.history[side].append(loss[0])  # Either only loss or total loss
+        self._model.history[side].append(loss[0])  # Either only loss or total loss
         logger.trace("Updated loss history: '%s'", side)
 
-    def log_tensorboard(self, side, loss):
-        """ Log loss to TensorBoard log """
-        if not self.tensorboard:
+    def _log_tensorboard(self, side, loss):
+        """ Log current loss to Tensorboard log files
+
+        Parameters
+        ----------
+        side: {"a", "b"}
+            The side to store the loss for
+        loss: list
+            The list of loss ``floats`` for this side
+        """
+        if not self._tensorboard:
             return
         logger.trace("Updating TensorBoard log: '%s'", side)
         logs = {log[0]: log[1]
-                for log in zip(self.model.state.loss_names[side], loss)}
-        self.tensorboard[side].on_batch_end(self.model.state.iterations, logs)
+                for log in zip(self._model.state.loss_names[side], loss)}
+        self._tensorboard[side].on_batch_end(self._model.state.iterations, logs)
         logger.trace("Updated TensorBoard log: '%s'", side)
 
     def clear_tensorboard(self):
-        """ Indicate training end to Tensorboard """
-        if not self.tensorboard:
+        """ Stop Tensorboard logging.
+
+        Tensorboard logging needs to be explicitly shutdown on training termination. Called from
+        :class:`scripts.train.Train` when training is stopped.
+         """
+        if not self._tensorboard:
             return
-        for side, tensorboard in self.tensorboard.items():
+        for side, tensorboard in self._tensorboard.items():
             logger.debug("Ending Tensorboard. Side: '%s'", side)
             tensorboard.on_train_end(None)
 
 
 class Batcher():
-    """ Batch images from a single side """
+    """ Handles the processing of a Batch for a single side.
+
+    Parameters
+    ----------
+    side: {"a" or "b"}
+        The side that this :class:`Batcher` belongs to
+    images: list
+        The list of full paths to the training images for this :class:`Batcher`
+    model: plugin from :mod:`plugins.train.model`
+        The selected model that will be running this trainer
+    use_mask: bool
+        ``True`` if a mask is required for training otherwise ``False``
+    batch_size: int
+        The size of the batch to be processed at each iteration
+    config: :class:`lib.config.FaceswapConfig`
+        The configuration for this trainer
+    """
     def __init__(self, side, images, model, use_mask, batch_size, config):
-        logger.debug("Initializing %s: side: '%s', num_images: %s, batch_size: %s, config: %s)",
-                     self.__class__.__name__, side, len(images), batch_size, config)
-        self.model = model
-        self.use_mask = use_mask
-        self.side = side
-        self.images = images
-        self.config = config
-        self.target = None
-        self.samples = None
-        self.mask = None
-
-        generator = self.load_generator()
-        self.feed = generator.minibatch_ab(images, batch_size, self.side)
-
-        self.preview_feed = None
-        self.timelapse_feed = None
-
-    def load_generator(self):
-        """ Pass arguments to TrainingDataGenerator and return object """
-        logger.debug("Loading generator: %s", self.side)
-        input_size = self.model.input_shape[0]
-        output_shapes = self.model.output_shapes
+        logger.debug("Initializing %s: side: '%s', num_images: %s, use_mask: %s, batch_size: %s, "
+                     "config: %s)",
+                     self.__class__.__name__, side, len(images), use_mask, batch_size, config)
+        self._model = model
+        self._use_mask = use_mask
+        self._side = side
+        self._images = images
+        self._config = config
+        self._target = None
+        self._samples = None
+        self._masks = None
+
+        generator = self._load_generator()
+        self._feed = generator.minibatch_ab(images, batch_size, self._side)
+
+        self._preview_feed = None
+        self._timelapse_feed = None
+        self._set_preview_feed()
+
+    def _load_generator(self):
+        """ Load the :class:`lib.training_data.TrainingDataGenerator` for this batcher """
+        logger.debug("Loading generator: %s", self._side)
+        input_size = self._model.input_shape[0]
+        output_shapes = self._model.output_shapes
         logger.debug("input_size: %s, output_shapes: %s", input_size, output_shapes)
         generator = TrainingDataGenerator(input_size,
                                           output_shapes,
-                                          self.model.training_opts,
-                                          self.config)
+                                          self._model.training_opts,
+                                          self._config)
         return generator
 
-    def train_one_batch(self, do_preview):
-        """ Train a batch """
-        logger.trace("Training one step: (side: %s)", self.side)
-        batch = self.get_next(do_preview)
+    def train_one_batch(self):
+        """ Train on a single batch of images for this :class:`Batcher`
+
+        Returns
+        -------
+        list
+            The list of loss values (as ``float``) for this batch
+        """
+        logger.trace("Training one step: (side: %s)", self._side)
+        model_inputs, model_targets = self._get_next()
         try:
-            loss = self.model.predictors[self.side].train_on_batch(*batch)
+            loss = self._model.predictors[self._side].train_on_batch(model_inputs, model_targets)
         except tf_errors.ResourceExhaustedError as err:
             msg = ("You do not have enough GPU memory available to train the selected model at "
                    "the selected settings. You can try a number of things:"
@@ -288,93 +438,177 @@ class Batcher():
         loss = loss if isinstance(loss, list) else [loss]
         return loss
 
-    def get_next(self, do_preview):
-        """ Return the next batch from the generator
-            Items should come out as: (warped, target [, mask]) """
-        batch = next(self.feed)
-        if self.use_mask:
-            batch = [[batch["feed"], batch["masks"]], batch["targets"] + [batch["masks"]]]
-        else:
-            batch = [batch["feed"], batch["targets"]]
-        self.generate_preview(do_preview)
-        return batch
+    def _get_next(self):
+        """ Return the next batch from the :class:`lib.training_data.TrainingDataGenerator` for
+        this batcher ready for feeding into the model.
+
+        Returns
+        -------
+        model_inputs: list
+            A list of :class:`numpy.ndarray` for feeding into the model
+        model_targets: list
+            A list of :class:`numpy.ndarray` for comparing the output of the model
+        """
+        logger.trace("Generating targets")
+        batch = next(self._feed)
+        targets_use_mask = self._model.training_opts["learn_mask"]
+        model_inputs = batch["feed"] + batch["masks"] if self._use_mask else batch["feed"]
+        model_targets = batch["targets"] + batch["masks"] if targets_use_mask else batch["targets"]
+        return model_inputs, model_targets
 
     def generate_preview(self, do_preview):
-        """ Generate the preview if a preview iteration """
+        """ Generate the preview images.
+
+        Parameters
+        ----------
+        do_preview: bool
+            Whether the previews should be generated. ``True`` if they should ``False`` if they
+            should not be generated, in which case currently stored previews should be deleted.
+        """
         if not do_preview:
-            self.samples = None
-            self.target = None
+            self._samples = None
+            self._target = None
+            self._masks = None
             return
         logger.debug("Generating preview")
-        if self.preview_feed is None:
-            self.set_preview_feed()
-        batch = next(self.preview_feed)
-        self.samples = batch["samples"]
-        self.target = [batch["targets"][self.model.largest_face_index]]
-        if self.use_mask:
-            self.target += [batch["masks"]]
-
-    def set_preview_feed(self):
-        """ Set the preview dictionary """
-        logger.debug("Setting preview feed: (side: '%s')", self.side)
-        preview_images = self.config.get("preview_images", 14)
+        batch = next(self._preview_feed)
+        self._samples = batch["samples"]
+        self._target = batch["targets"][self._model.largest_face_index]
+        self._masks = batch["masks"][0]
+
+    def _set_preview_feed(self):
+        """ Set the preview feed for this batcher.
+
+        Creates a generator from :class:`lib.training_data.TrainingDataGenerator` specifically
+        for previews for the batcher.
+        """
+        logger.debug("Setting preview feed: (side: '%s')", self._side)
+        preview_images = self._config.get("preview_images", 14)
         preview_images = min(max(preview_images, 2), 16)
-        batchsize = min(len(self.images), preview_images)
-        self.preview_feed = self.load_generator().minibatch_ab(self.images,
-                                                               batchsize,
-                                                               self.side,
-                                                               do_shuffle=True,
-                                                               is_preview=True)
+        batchsize = min(len(self._images), preview_images)
+        self._preview_feed = self._load_generator().minibatch_ab(self._images,
+                                                                 batchsize,
+                                                                 self._side,
+                                                                 do_shuffle=True,
+                                                                 is_preview=True)
         logger.debug("Set preview feed. Batchsize: %s", batchsize)
 
-    def compile_sample(self, batch_size, samples=None, images=None):
-        """ Training samples to display in the viewer """
-        num_images = self.config.get("preview_images", 14)
+    def compile_sample(self, batch_size, samples=None, images=None, masks=None):
+        """ Compile the preview samples for display.
+
+        Parameters
+        ----------
+        batch_size: int
+            The requested batch size for each training iterations
+        samples: :class:`numpy.ndarray`, optional
+            The sample images that should be used for creating the preview. If ``None`` then the
+            samples will be generated from the internal random image generator.
+            Default: ``None``
+        images:  :class:`numpy.ndarray`, optional
+            The target images that should be used for creating the preview. If ``None`` then the
+            targets will be generated from the internal random image generator.
+            Default: ``None``
+        masks:  :class:`numpy.ndarray`, optional
+            The masks that should be used for creating the preview. If ``None`` then the
+            masks will be generated from the internal random image generator.
+            Default: ``None``
+
+        Returns
+        -------
+        list
+            The list of samples, targets and masks as :class:`numpy.ndarrays` for creating a
+            preview image
+         """
+        num_images = self._config.get("preview_images", 14)
         num_images = min(batch_size, num_images) if batch_size is not None else num_images
-        logger.debug("Compiling samples: (side: '%s', samples: %s)", self.side, num_images)
-        images = images if images is not None else self.target
-        retval = [samples[0:num_images]] if samples is not None else [self.samples[0:num_images]]
-        if self.use_mask:
-            retval.extend(tgt[0:num_images] for tgt in images)
-        else:
-            retval.extend(images[0:num_images])
+        logger.debug("Compiling samples: (side: '%s', samples: %s)", self._side, num_images)
+        images = images if images is not None else self._target
+        masks = masks if masks is not None else self._masks
+        samples = samples if samples is not None else self._samples
+        retval = [samples[0:num_images], images[0:num_images], masks[0:num_images]]
         return retval
 
     def compile_timelapse_sample(self):
-        """ Timelapse samples """
-        batch = next(self.timelapse_feed)
+        """ Compile the sample images for creating a time-lapse frame.
+
+        Returns
+        -------
+        list
+            The list of samples, targets and masks as :class:`numpy.ndarrays` for creating a
+            time-lapse frame
+        """
+        batch = next(self._timelapse_feed)
         batchsize = len(batch["samples"])
-        images = [batch["targets"][self.model.largest_face_index]]
-        if self.use_mask:
-            images = images + [batch["masks"]]
-        sample = self.compile_sample(batchsize, samples=batch["samples"], images=images)
+        images = batch["targets"][self._model.largest_face_index]
+        masks = batch["masks"][0]
+        sample = self.compile_sample(batchsize,
+                                     samples=batch["samples"],
+                                     images=images,
+                                     masks=masks)
         return sample
 
-    def set_timelapse_feed(self, images, batchsize):
-        """ Set the timelapse dictionary """
-        logger.debug("Setting timelapse feed: (side: '%s', input_images: '%s', batchsize: %s)",
-                     self.side, images, batchsize)
-        self.timelapse_feed = self.load_generator().minibatch_ab(images[:batchsize],
-                                                                 batchsize, self.side,
-                                                                 do_shuffle=False,
-                                                                 is_timelapse=True)
-        logger.debug("Set timelapse feed")
+    def set_timelapse_feed(self, images, batch_size):
+        """ Set the time-lapse feed for this batcher.
+
+        Creates a generator from :class:`lib.training_data.TrainingDataGenerator` specifically
+        for generating time-lapse previews for the batcher.
+
+        Parameters
+        ----------
+        images: list
+            The list of full paths to the images for creating the time-lapse for this
+            :class:`Batcher`
+        batch_size: int
+            The number of images to be used to create the time-lapse preview.
+        """
+        logger.debug("Setting time-lapse feed: (side: '%s', input_images: '%s', batch_size: %s)",
+                     self._side, images, batch_size)
+        self._timelapse_feed = self._load_generator().minibatch_ab(images[:batch_size],
+                                                                   batch_size, self._side,
+                                                                   do_shuffle=False,
+                                                                   is_timelapse=True)
+        logger.debug("Set time-lapse feed")
 
 
 class Samples():
-    """ Display samples for preview and timelapse """
+    """ Compile samples for display for preview and time-lapse
+
+    Parameters
+    ----------
+    model: plugin from :mod:`plugins.train.model`
+        The selected model that will be running this trainer
+    use_mask: bool
+        ``True`` if a mask should be displayed otherwise ``False``
+    coverage_ratio: float
+        Ratio of face to be cropped out of the training image.
+    scaling: float, optional
+        The amount to scale the final preview image by. Default: `1.0`
+
+    Attributes
+    ----------
+    images: dict
+        The :class:`numpy.ndarray` training images for generating previews on each side. The
+        dictionary should contain 2 keys ("a" and "b") with the values being the training images
+        for generating samples corresponding to each side.
+    """
     def __init__(self, model, use_mask, coverage_ratio, scaling=1.0):
         logger.debug("Initializing %s: model: '%s', use_mask: %s, coverage_ratio: %s)",
                      self.__class__.__name__, model, use_mask, coverage_ratio)
-        self.model = model
-        self.use_mask = use_mask
+        self._model = model
+        self._use_mask = use_mask
         self.images = dict()
-        self.coverage_ratio = coverage_ratio
-        self.scaling = scaling
+        self._coverage_ratio = coverage_ratio
+        self._scaling = scaling
         logger.debug("Initialized %s", self.__class__.__name__)
 
     def show_sample(self):
-        """ Display preview data """
+        """ Compile a preview image.
+
+        Returns
+        -------
+        :class:`numpy.ndarry`
+            A compiled preview image ready for display or saving
+        """
         if len(self.images) != 2:
             logger.debug("Ping Pong training - Only one side trained. Aborting preview")
             return None
@@ -384,23 +618,23 @@ class Samples():
         headers = dict()
         for side, samples in self.images.items():
             faces = samples[1]
-            if self.model.input_shape[0] / faces.shape[1] != 1.0:
-                feeds[side] = self.resize_sample(side, faces, self.model.input_shape[0])
-                feeds[side] = feeds[side].reshape((-1, ) + self.model.input_shape)
+            if self._model.input_shape[0] / faces.shape[1] != 1.0:
+                feeds[side] = self._resize_sample(side, faces, self._model.input_shape[0])
+                feeds[side] = feeds[side].reshape((-1, ) + self._model.input_shape)
             else:
                 feeds[side] = faces
-            if self.use_mask:
+            if self._use_mask:
                 mask = samples[-1]
                 feeds[side] = [feeds[side], mask]
 
-        preds = self.get_predictions(feeds["a"], feeds["b"])
+        preds = self._get_predictions(feeds["a"], feeds["b"])
 
         for side, samples in self.images.items():
             other_side = "a" if side == "b" else "b"
             predictions = [preds["{0}_{0}".format(side)],
                            preds["{}_{}".format(other_side, side)]]
-            display = self.to_full_frame(side, samples, predictions)
-            headers[side] = self.get_headers(side, display[0].shape[1])
+            display = self._to_full_frame(side, samples, predictions)
+            headers[side] = self._get_headers(side, display[0].shape[1])
             figures[side] = np.stack([display[0], display[1], display[2], ], axis=1)
             if self.images[side][0].shape[0] % 2 == 1:
                 figures[side] = np.concatenate([figures[side],
@@ -409,73 +643,123 @@ class Samples():
         width = 4
         side_cols = width // 2
         if side_cols != 1:
-            headers = self.duplicate_headers(headers, side_cols)
+            headers = self._duplicate_headers(headers, side_cols)
 
         header = np.concatenate([headers["a"], headers["b"]], axis=1)
         figure = np.concatenate([figures["a"], figures["b"]], axis=0)
         height = int(figure.shape[0] / width)
         figure = figure.reshape((width, height) + figure.shape[1:])
-        figure = stack_images(figure)
-        figure = np.vstack((header, figure))
+        figure = _stack_images(figure)
+        figure = np.concatenate((header, figure), axis=0)
 
         logger.debug("Compiled sample")
         return np.clip(figure * 255, 0, 255).astype('uint8')
 
     @staticmethod
-    def resize_sample(side, sample, target_size):
-        """ Resize samples where predictor expects different shape from processed image """
+    def _resize_sample(side, sample, target_size):
+        """ Resize a given image to the target size.
+
+        Parameters
+        ----------
+        sample: :class:`numpy.ndarray`
+            The sample to be resized
+        target_size: int
+            The size that the sample should be resized to
+
+        Returns
+        -------
+        :class:`numpy.ndarray`
+            The sample resized to the target size
+        """
         scale = target_size / sample.shape[1]
         if scale == 1.0:
             return sample
         logger.debug("Resizing sample: (side: '%s', sample.shape: %s, target_size: %s, scale: %s)",
                      side, sample.shape, target_size, scale)
-        interpn = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA  # pylint: disable=no-member
-        retval = np.array([cv2.resize(img,  # pylint: disable=no-member
-                                      (target_size, target_size),
-                                      interpn)
+        interpn = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA
+        retval = np.array([cv2.resize(img, (target_size, target_size), interpn)
                            for img in sample])
         logger.debug("Resized sample: (side: '%s' shape: %s)", side, retval.shape)
         return retval
 
-    def get_predictions(self, feed_a, feed_b):
-        """ Return the sample predictions from the model """
+    def _get_predictions(self, feed_a, feed_b):
+        """ Feed the samples to the model and return predictions
+
+        Parameters
+        ----------
+        feed_a: list
+            List of :class:`numpy.ndarray` of feed images for the "a" side
+        feed_a: list
+            List of :class:`numpy.ndarray` of feed images for the "b" side
+
+        Returns
+        list:
+            List of :class:`numpy.ndarray` of predictions received from the model
+        """
         logger.debug("Getting Predictions")
         preds = dict()
-        preds["a_a"] = self.model.predictors["a"].predict(feed_a)
-        preds["b_a"] = self.model.predictors["b"].predict(feed_a)
-        preds["a_b"] = self.model.predictors["a"].predict(feed_b)
-        preds["b_b"] = self.model.predictors["b"].predict(feed_b)
+        preds["a_a"] = self._model.predictors["a"].predict(feed_a)
+        preds["b_a"] = self._model.predictors["b"].predict(feed_a)
+        preds["a_b"] = self._model.predictors["a"].predict(feed_b)
+        preds["b_b"] = self._model.predictors["b"].predict(feed_b)
         # Get the returned largest image from predictors that emit multiple items
         if not isinstance(preds["a_a"], np.ndarray):
             for key, val in preds.items():
-                preds[key] = val[self.model.largest_face_index]
+                preds[key] = val[self._model.largest_face_index]
         logger.debug("Returning predictions: %s", {key: val.shape for key, val in preds.items()})
         return preds
 
-    def to_full_frame(self, side, samples, predictions):
-        """ Patch the images into the full frame """
+    def _to_full_frame(self, side, samples, predictions):
+        """ Patch targets and prediction images into images of training image size.
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side that these samples are for
+        samples: list
+            List of :class:`numpy.ndarray` of target images and feed images
+        predictions: list
+            List of :class: `numpy.ndarray` of predictions from the model
+        """
         logger.debug("side: '%s', number of sample arrays: %s, prediction.shapes: %s)",
                      side, len(samples), [pred.shape for pred in predictions])
         full, faces = samples[:2]
         images = [faces] + predictions
         full_size = full.shape[1]
-        target_size = int(full_size * self.coverage_ratio)
+        target_size = int(full_size * self._coverage_ratio)
         if target_size != full_size:
-            frame = self.frame_overlay(full, target_size, (0, 0, 255))
+            frame = self._frame_overlay(full, target_size, (0, 0, 255))
 
-        if self.use_mask:
-            images = self.compile_masked(images, samples[-1])
-        images = [self.resize_sample(side, image, target_size) for image in images]
+        if self._use_mask:
+            images = self._compile_masked(images, samples[-1])
+        images = [self._resize_sample(side, image, target_size) for image in images]
         if target_size != full_size:
-            images = [self.overlay_foreground(frame, image) for image in images]
-        if self.scaling != 1.0:
-            new_size = int(full_size * self.scaling)
-            images = [self.resize_sample(side, image, new_size) for image in images]
+            images = [self._overlay_foreground(frame, image) for image in images]
+        if self._scaling != 1.0:
+            new_size = int(full_size * self._scaling)
+            images = [self._resize_sample(side, image, new_size) for image in images]
         return images
 
     @staticmethod
-    def frame_overlay(images, target_size, color):
-        """ Add roi frame to a backfround image """
+    def _frame_overlay(images, target_size, color):
+        """ Add a frame overlay to preview images indicating the region of interest.
+
+        This is the red border that appears in the preview images.
+
+        Parameters
+        ----------
+        images: :class:`numpy.ndarray`
+            The samples to apply the frame to
+        target_size: int
+            The size of the sample within the full size frame
+        color: tuple
+            The (Blue, Green, Red) color to use for the frame
+
+        Returns
+        -------
+        :class:`numpy,ndarray`
+            The samples with the frame overlay applied
+        """
         logger.debug("full_size: %s, target_size: %s, color: %s",
                      images.shape[1], target_size, color)
         new_images = list()
@@ -484,78 +768,97 @@ class Samples():
         length = target_size // 4
         t_l, b_r = (padding, full_size - padding)
         for img in images:
-            cv2.rectangle(img,  # pylint: disable=no-member
-                          (t_l, t_l),
-                          (t_l + length, t_l + length),
-                          color,
-                          3)
-            cv2.rectangle(img,  # pylint: disable=no-member
-                          (b_r, t_l),
-                          (b_r - length, t_l + length),
-                          color,
-                          3)
-            cv2.rectangle(img,  # pylint: disable=no-member
-                          (b_r, b_r),
-                          (b_r - length,
-                           b_r - length),
-                          color,
-                          3)
-            cv2.rectangle(img,  # pylint: disable=no-member
-                          (t_l, b_r),
-                          (t_l + length, b_r - length),
-                          color,
-                          3)
+            cv2.rectangle(img, (t_l, t_l), (t_l + length, t_l + length), color, 3)
+            cv2.rectangle(img, (b_r, t_l), (b_r - length, t_l + length), color, 3)
+            cv2.rectangle(img, (b_r, b_r), (b_r - length, b_r - length), color, 3)
+            cv2.rectangle(img, (t_l, b_r), (t_l + length, b_r - length), color, 3)
             new_images.append(img)
         retval = np.array(new_images)
         logger.debug("Overlayed background. Shape: %s", retval.shape)
         return retval
 
     @staticmethod
-    def compile_masked(faces, masks):
-        """ Add the mask to the faces for masked preview """
+    def _compile_masked(faces, masks):
+        """ Add the mask to the faces for masked preview.
+
+        Places an opaque red layer over areas of the face that are masked out.
+
+        Parameters
+        ----------
+        faces: :class:`numpy.ndarray`
+            The sample faces that are to have the mask applied
+        masks: :class:`numpy.ndarray`
+            The masks that are to be applied to the faces
+
+        Returns
+        -------
+        list
+            List of :class:`numpy.ndarray` faces with the opaque mask layer applied
+        """
         retval = list()
         masks3 = np.tile(1 - np.rint(masks), 3)
         for mask in masks3:
             mask[np.where((mask == [1., 1., 1.]).all(axis=2))] = [0., 0., 1.]
         for previews in faces:
-            images = np.array([cv2.addWeighted(img, 1.0,  # pylint: disable=no-member
-                                               masks3[idx], 0.3,
-                                               0)
+            images = np.array([cv2.addWeighted(img, 1.0, masks3[idx], 0.3, 0)
                                for idx, img in enumerate(previews)])
             retval.append(images)
         logger.debug("masked shapes: %s", [faces.shape for faces in retval])
         return retval
 
     @staticmethod
-    def overlay_foreground(backgrounds, foregrounds):
-        """ Overlay the training images into the center of the background """
+    def _overlay_foreground(backgrounds, foregrounds):
+        """ Overlay the preview images into the center of the background images
+
+        Parameters
+        ----------
+        backgrounds: list
+            List of :class:`numpy.ndarray` background images for placing the preview images onto
+        backgrounds: list
+            List of :class:`numpy.ndarray` preview images for placing onto the background images
+
+        Returns
+        -------
+        :class:`numpy.ndarray`
+            The preview images compiled into the full frame size for each preview
+        """
         offset = (backgrounds.shape[1] - foregrounds.shape[1]) // 2
         new_images = list()
         for idx, img in enumerate(backgrounds):
             img[offset:offset + foregrounds[idx].shape[0],
-                offset:offset + foregrounds[idx].shape[1]] = foregrounds[idx]
+                offset:offset + foregrounds[idx].shape[1], :3] = foregrounds[idx]
             new_images.append(img)
         retval = np.array(new_images)
         logger.debug("Overlayed foreground. Shape: %s", retval.shape)
         return retval
 
-    def get_headers(self, side, width):
-        """ Set headers for images """
+    def _get_headers(self, side, width):
+        """ Set header row for the final preview frame
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side that the headers should be generated for
+        width: int
+            The width of each column in the preview frame
+
+        Returns
+        -------
+        :class:`numpy.ndarray`
+            The column headings for the given side
+        """
         logger.debug("side: '%s', width: %s",
                      side, width)
         titles = ("Original", "Swap") if side == "a" else ("Swap", "Original")
         side = side.upper()
-        height = int(64 * self.scaling)
+        height = int(64 * self._scaling)
         total_width = width * 3
         logger.debug("height: %s, total_width: %s", height, total_width)
-        font = cv2.FONT_HERSHEY_SIMPLEX  # pylint: disable=no-member
+        font = cv2.FONT_HERSHEY_SIMPLEX
         texts = ["{} ({})".format(titles[0], side),
                  "{0} > {0}".format(titles[0]),
                  "{} > {}".format(titles[0], titles[1])]
-        text_sizes = [cv2.getTextSize(texts[idx],  # pylint: disable=no-member
-                                      font,
-                                      self.scaling * 0.8,
-                                      1)[0]
+        text_sizes = [cv2.getTextSize(texts[idx], font, self._scaling * 0.8, 1)[0]
                       for idx in range(len(texts))]
         text_y = int((height + text_sizes[0][1]) / 2)
         text_x = [int((width - text_sizes[idx][0]) / 2) + width * idx
@@ -564,20 +867,33 @@ class Samples():
                      texts, text_sizes, text_x, text_y)
         header_box = np.ones((height, total_width, 3), np.float32)
         for idx, text in enumerate(texts):
-            cv2.putText(header_box,  # pylint: disable=no-member
+            cv2.putText(header_box,
                         text,
                         (text_x[idx], text_y),
                         font,
-                        self.scaling * 0.8,
+                        self._scaling * 0.8,
                         (0, 0, 0),
                         1,
-                        lineType=cv2.LINE_AA)  # pylint: disable=no-member
+                        lineType=cv2.LINE_AA)
         logger.debug("header_box.shape: %s", header_box.shape)
         return header_box
 
     @staticmethod
-    def duplicate_headers(headers, columns):
-        """ Duplicate headers for the number of columns displayed """
+    def _duplicate_headers(headers, columns):
+        """ Duplicate headers for the number of columns displayed for each side.
+
+        Parameters
+        ----------
+        headers: :class:`numpy.ndarray`
+            The header to be duplicated
+        columns: int
+            The number of columns that the header needs to be duplicated for
+
+        Returns
+        -------
+        :class:`numpy.ndarray`
+            The original headers duplicated by the number of columns
+        """
         for side, header in headers.items():
             duped = tuple([header for _ in range(columns)])
             headers[side] = np.concatenate(duped, axis=1)
@@ -586,114 +902,435 @@ class Samples():
 
 
 class Timelapse():
-    """ Create the timelapse """
-    def __init__(self, model, use_mask, coverage_ratio, preview_images, batchers):
+    """ Create a time-lapse preview image.
+
+    Parameters
+    ----------
+    model: plugin from :mod:`plugins.train.model`
+        The selected model that will be running this trainer
+    use_mask: bool
+        ``True`` if a mask should be displayed otherwise ``False``
+    coverage_ratio: float
+        Ratio of face to be cropped out of the training image.
+    scaling: float, optional
+        The amount to scale the final preview image by. Default: `1.0`
+    image_count: int
+        The number of preview images to be displayed in the time-lapse
+    batchers: dict
+        The dictionary should contain 2 keys ("a" and "b") with the values being the
+        :class:`Batcher` for each side.
+    """
+    def __init__(self, model, use_mask, coverage_ratio, image_count, batchers):
         logger.debug("Initializing %s: model: %s, use_mask: %s, coverage_ratio: %s, "
-                     "preview_images: %s, batchers: '%s')", self.__class__.__name__, model,
-                     use_mask, coverage_ratio, preview_images, batchers)
-        self.preview_images = preview_images
-        self.samples = Samples(model, use_mask, coverage_ratio)
-        self.model = model
-        self.batchers = batchers
-        self.output_file = None
+                     "image_count: %s, batchers: '%s')", self.__class__.__name__, model,
+                     use_mask, coverage_ratio, image_count, batchers)
+        self._num_images = image_count
+        self._samples = Samples(model, use_mask, coverage_ratio)
+        self._model = model
+        self._batchers = batchers
+        self._output_file = None
         logger.debug("Initialized %s", self.__class__.__name__)
 
     def get_sample(self, side, timelapse_kwargs):
-        """ Perform timelapse """
-        logger.debug("Getting timelapse samples: '%s'", side)
-        if not self.output_file:
-            self.setup(**timelapse_kwargs)
-        self.samples.images[side] = self.batchers[side].compile_timelapse_sample()
-        logger.debug("Got timelapse samples: '%s' - %s", side, len(self.samples.images[side]))
-
-    def setup(self, input_a=None, input_b=None, output=None):
-        """ Set the timelapse output folder """
-        logger.debug("Setting up timelapse")
+        """ Compile the time-lapse preview
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side that the time-lapse is being generated for
+        timelapse_kwargs: dict
+            The keyword arguments for setting up the time-lapse. All values should be full paths
+            the keys being `input_a`, `input_b`, `output`
+        """
+        logger.debug("Getting time-lapse samples: '%s'", side)
+        if not self._output_file:
+            self._setup(**timelapse_kwargs)
+        self._samples.images[side] = self._batchers[side].compile_timelapse_sample()
+        logger.debug("Got time-lapse samples: '%s' - %s", side, len(self._samples.images[side]))
+
+    def _setup(self, input_a=None, input_b=None, output=None):
+        """ Setup the time-lapse folder locations and the time-lapse feed.
+
+        Parameters
+        ----------
+        input_a: str
+            The full path to the time-lapse input folder containing faces for the "a" side
+        input_b: str
+            The full path to the time-lapse input folder containing faces for the "b" side
+        output: str, optional
+            The full path to the time-lapse output folder. If ``None`` is provided this will
+            default to the model folder
+        """
+        logger.debug("Setting up time-lapse")
         if output is None:
-            output = str(get_folder(os.path.join(str(self.model.model_dir),
-                                                 "{}_timelapse".format(self.model.name))))
-        self.output_file = str(output)
-        logger.debug("Timelapse output set to '%s'", self.output_file)
+            output = str(get_folder(os.path.join(str(self._model.model_dir),
+                                                 "{}_timelapse".format(self._model.name))))
+        self._output_file = str(output)
+        logger.debug("Time-lapse output set to '%s'", self._output_file)
 
         images = {"a": get_image_paths(input_a), "b": get_image_paths(input_b)}
         batchsize = min(len(images["a"]),
                         len(images["b"]),
-                        self.preview_images)
+                        self._num_images)
         for side, image_files in images.items():
-            self.batchers[side].set_timelapse_feed(image_files, batchsize)
-        logger.debug("Set up timelapse")
+            self._batchers[side].set_timelapse_feed(image_files, batchsize)
+        logger.debug("Set up time-lapse")
 
     def output_timelapse(self):
-        """ Set the timelapse dictionary """
-        logger.debug("Ouputting timelapse")
-        image = self.samples.show_sample()
+        """ Write the created time-lapse to the specified output folder. """
+        logger.debug("Ouputting time-lapse")
+        image = self._samples.show_sample()
         if image is None:
             return
-        filename = os.path.join(self.output_file, str(int(time.time())) + ".jpg")
+        filename = os.path.join(self._output_file, str(int(time.time())) + ".jpg")
 
-        cv2.imwrite(filename, image)  # pylint: disable=no-member
-        logger.debug("Created timelapse: '%s'", filename)
+        cv2.imwrite(filename, image)
+        logger.debug("Created time-lapse: '%s'", filename)
 
 
 class PingPong():
-    """ Side switcher for pingpong training """
+    """ Side switcher for ping-pong training (memory saving feature)
+
+    Parameters
+    ----------
+    model: plugin from :mod:`plugins.train.model`
+        The selected model that will be running this trainer
+    sides: list
+        The sorted sides that are to be trained. Generally ["a", "b"]
+
+    Attributes
+    ----------
+    side: str
+        The side that is currently being trained
+    loss: dict
+        The loss for each side for ping pong training for the current ping pong session
+    """
     def __init__(self, model, sides):
         logger.debug("Initializing %s: (model: '%s')", self.__class__.__name__, model)
-        self.active = model.training_opts.get("pingpong", False)
-        self.model = model
-        self.sides = sides
+        self._model = model
+        self._sides = sides
         self.side = sorted(sides)[0]
         self.loss = {side: [0] for side in sides}
         logger.debug("Initialized %s", self.__class__.__name__)
 
+    @property
+    def active(self):
+        """ bool: ``True`` if Ping Pong training is active otherwise ``False``. """
+        return self._model.training_opts.get("pingpong", False)
+
     def switch(self):
-        """ Switch pingpong side """
+        """ Switch ping-pong training from one side of the model to the other """
         if not self.active:
             return
-        retval = [side for side in self.sides if side != self.side][0]
+        retval = [side for side in self._sides if side != self.side][0]
         logger.info("Switching training to side %s", retval.title())
         self.side = retval
-        self.reload_model()
+        self._reload_model()
 
-    def reload_model(self):
-        """ Load the model for just the current side """
+    def _reload_model(self):
+        """ Clear out the model from VRAM and reload for the next side to be trained with ping-pong
+        training """
         logger.verbose("Ping-Pong re-loading model")
-        self.model.reset_pingpong()
+        self._model.reset_pingpong()
+
+
+class TrainingAlignments():
+    """ Obtain Landmarks and required mask from alignments file.
+
+    Parameters
+    ----------
+    training_opts: dict
+        The dictionary of model training options (see module doc-string for information about
+        contents)
+    image_list: dict
+        The file paths for the images to be trained on for each side. The dictionary should contain
+        2 keys ("a" and "b") with the values being a list of full paths corresponding to each side.
+    """
+    def __init__(self, training_opts, image_list):
+        logger.debug("Initializing %s: (training_opts: '%s', image counts: %s)",
+                     self.__class__.__name__, training_opts,
+                     {k: len(v) for k, v in image_list.items()})
+        self._training_opts = training_opts
+        self._hashes = self._get_image_hashes(image_list)
+        self._detected_faces = self._load_alignments()
+        self._check_all_faces()
+        logger.debug("Initialized %s", self.__class__.__name__)
 
+    @property
+    def landmarks(self):
+        """ dict: The :class:`numpy.ndarray` aligned landmarks for keys "a" and "b" """
+        retval = {side: self._transform_landmarks(side, detected_faces)
+                  for side, detected_faces in self._detected_faces.items()}
+        logger.trace(retval)
+        return retval
 
-class Landmarks():
-    """ Set Landmarks for training into the model's training options"""
-    def __init__(self, training_opts):
-        logger.debug("Initializing %s: (training_opts: '%s')",
-                     self.__class__.__name__, training_opts)
-        self.size = training_opts.get("training_size", 256)
-        self.paths = training_opts["alignments"]
-        self.landmarks = self.get_alignments()
-        logger.debug("Initialized %s", self.__class__.__name__)
+    @property
+    def masks(self):
+        """ dict: The :class:`lib.faces_detect.Mask` objects of requested mask type for
+        keys a" and "b"
+        """
+        retval = {side: self._get_masks(side, detected_faces)
+                  for side, detected_faces in self._detected_faces.items()}
+        logger.trace(retval)
+        return retval
 
-    def get_alignments(self):
-        """ Obtain the landmarks for each faceset """
-        landmarks = dict()
-        for side, fullpath in self.paths.items():
+    # Load alignments
+    @staticmethod
+    def _get_image_hashes(image_list):
+        """ Return the hashes for all images used for training.
+
+        Parameters
+        ----------
+        image_list: dict
+            The file paths for the images to be trained on for each side. The dictionary should
+            contain 2 keys ("a" and "b") with the values being a list of full paths corresponding
+            to each side.
+
+        Returns
+        -------
+        dict
+            For keys "a" and "b" the values are a ``dict`` containing keys "hashes" and "filenames"
+            with their values being a list of hashes and filenames that exist within the training
+            data folder
+        """
+        hashes = {key: dict(hashes=[], filenames=[]) for key in image_list}
+        pbar = tqdm(desc="Reading training images",
+                    total=sum(len(val) for val in image_list.values()))
+        for side, filelist in image_list.items():
+            logger.debug("side: %s, file count: %s", side, len(filelist))
+            for filename, hsh in read_image_hash_batch(filelist):
+                hashes[side]["hashes"].append(hsh)
+                hashes[side]["filenames"].append(filename)
+                pbar.update(1)
+        pbar.close()
+        logger.trace(hashes)
+        return hashes
+
+    def _load_alignments(self):
+        """ Load the alignments and convert to :class:`lib.faces_detect.DetectedFace` objects.
+
+        Returns
+        -------
+        dict
+            For keys "a" and "b" values are a list of :class:`lib.faces_detect.DetectedFace`
+            objects.
+        """
+        logger.debug("Loading alignments")
+        retval = dict()
+        for side, fullpath in self._training_opts["alignments"].items():
+            logger.debug("side: '%s', path: '%s'", side, fullpath)
             path, filename = os.path.split(fullpath)
             alignments = Alignments(path, filename=filename)
-            landmarks[side] = self.transform_landmarks(alignments)
-        return landmarks
+            retval[side] = self._to_detected_faces(alignments, side)
+        logger.debug("Returning: %s", {k: len(v) for k, v in retval.items()})
+        return retval
 
-    def transform_landmarks(self, alignments):
-        """ For each face transform landmarks and return """
-        landmarks = dict()
-        for _, faces, _, _ in alignments.yield_faces():
-            for face in faces:
+    def _to_detected_faces(self, alignments, side):
+        """ Convert alignments to DetectedFace objects.
+
+        Filter the detected faces to only those that exist in the training folders.
+
+        Parameters
+        ----------
+        alignments: :class:`lib.alignments.Alignments`
+            The alignments for the current faces
+        side: {"a" or "b"}
+            The side being processed
+
+        Returns
+        -------
+        list
+            List of :class:`lib.faces_detect.DetectedFace` objects
+        """
+        skip_count = 0
+        side_hashes = set(self._hashes[side]["hashes"])
+        detected_faces = []
+        for _, faces, _, filename in alignments.yield_faces():
+            for idx, face in enumerate(faces):
+                if not self._validate_face(face, filename, idx, side, side_hashes):
+                    skip_count += 1
+                    continue
                 detected_face = DetectedFace()
                 detected_face.from_alignment(face)
-                detected_face.load_aligned(None, size=self.size)
-                landmarks[detected_face.hash] = detected_face.aligned_landmarks
+                detected_faces.append(detected_face)
+        logger.debug("Detected Faces count: %s, Skipped faces count: %s",
+                     len(detected_faces), skip_count)
+        if skip_count != 0:
+            logger.warning("%s alignments have been removed as their corresponding faces do not "
+                           "exist in the input folder for side %s. Run in verbose mode if you "
+                           "wish to see which alignments have been excluded.",
+                           skip_count, side.upper())
+        return detected_faces
+
+    def _validate_face(self, face, filename, idx, side, side_hashes):
+        """ Validate that the currently processing face has a corresponding hash entry and the
+        requested mask exists
+
+        Parameters
+        ----------
+        face: dict
+            A face retrieved from an alignments file
+        filename: str
+            The original frame filename that the given face comes from
+        idx: int
+            The index of the face in the frame
+        side: {'A', 'B'}
+            The side that this face belongs to
+        side_hashes: set
+            A set of hashes that exist in the alignments folder for these faces
+
+        Returns
+        -------
+        bool
+            ``True`` if the face is valid otherwise ``False``
+
+        Raises
+        ------
+        FaceswapError
+            If the current face doesn't pass validation
+        """
+        mask_type = self._training_opts["mask_type"]
+        if mask_type is not None and "mask" not in face:
+            msg = ("You have selected a Mask Type in your training configuration options but at "
+                   "least one face has no mask stored for it.\nYou should generate the required "
+                   "masks with the Mask Tool or set the Mask Type configuration option to `none`."
+                   "\nThe face that caused this failure was side: `{}`, frame: `{}`, index: {}. "
+                   "However there are probably more faces without masks".format(
+                       side.upper(), filename, idx))
+            raise FaceswapError(msg)
+
+        if mask_type is not None and mask_type not in face["mask"]:
+            msg = ("At least one of your faces does not have the mask `{}` stored for it.\nYou "
+                   "should run the Mask Tool to generate this mask for your faceset or "
+                   "select a different mask in the training configuration options.\n"
+                   "The face that caused this failure was [side: `{}`, frame: `{}`, index: {}]. "
+                   "The masks that exist for this face are: {}.\nBe aware that there are probably "
+                   "more faces without this Mask Type".format(
+                       mask_type, side.upper(), filename, idx, list(face["mask"].keys())))
+            raise FaceswapError(msg)
+
+        if face["hash"] not in side_hashes:
+            logger.verbose("Skipping alignment for non-existant face in frame '%s' index: %s",
+                           filename, idx)
+            return False
+        return True
+
+    def _check_all_faces(self):
+        """ Ensure that all faces in the training folder exist in the alignments file.
+        If not, output missing filenames
+
+        Raises
+        ------
+        FaceswapError
+            If there are faces in the training folder which do not exist in the alignments file
+        """
+        logger.debug("Checking faces exist in alignments")
+        missing_alignments = dict()
+        for side, train_hashes in self._hashes.items():
+            align_hashes = set(face.hash for face in self._detected_faces[side])
+            if not align_hashes.issuperset(train_hashes["hashes"]):
+                missing_alignments[side] = [
+                    os.path.basename(filename)
+                    for hsh, filename in zip(train_hashes["hashes"], train_hashes["filenames"])
+                    if hsh not in align_hashes]
+        if missing_alignments:
+            msg = ("There are faces in your training folder(s) which do not exist in your "
+                   "alignments file. Training cannot continue. See above for a full list of "
+                   "files missing alignments.")
+            for side, filelist in missing_alignments.items():
+                logger.error("Faces missing alignments for side %s: %s",
+                             side.capitalize(), filelist)
+            raise FaceswapError(msg)
+
+    # Get landmarks
+    def _transform_landmarks(self, side, detected_faces):
+        """ Transform frame landmarks to their aligned face variant.
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side currently being processed
+        detected_faces: list
+            A list of :class:`lib.faces_detect.DetectedFace` objects
+
+        Returns
+        -------
+        dict
+            The face filenames as keys with the aligned landmarks as value.
+        """
+        landmarks = dict()
+        for face in detected_faces:
+            face.load_aligned(None, size=self._training_opts["training_size"])
+            for filename in self._hash_to_filenames(side, face.hash):
+                landmarks[filename] = face.aligned_landmarks
         return landmarks
 
+    # Get masks
+    def _get_masks(self, side, detected_faces):
+        """ For each face, obtain the mask and set the requested blurring and threshold level.
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side currently being processed
+        detected_faces: list
+            A list of :class:`lib.faces_detect.DetectedFace` objects
+
+        Returns
+        -------
+        dict
+            The face filenames as keys with the :class:`lib.faces_detect.Mask` as value.
+        """
+
+        masks = dict()
+        for face in detected_faces:
+            mask = face.mask[self._training_opts["mask_type"]]
+            mask.set_blur_kernel_and_threshold(blur_kernel=self._training_opts["mask_blur_kernel"],
+                                               threshold=self._training_opts["mask_threshold"])
+            for filename in self._hash_to_filenames(side, face.hash):
+                masks[filename] = mask
+        return masks
+
+    def _hash_to_filenames(self, side, face_hash):
+        """ For a given hash return all the filenames that match for the given side.
+
+        Notes
+        -----
+        Multiple faces can have the same hash, so this makes sure that all filenames are updated
+        for all instances of a hash.
+
+        Parameters
+        ----------
+        side: {"a" or "b"}
+            The side currently being processed
+        face_hash: str
+            The sha1 hash of the face to obtain the filename for
+
+        Returns
+        -------
+        list
+            The filenames that exist for the given hash
+        """
+        side_hashes = self._hashes[side]
+        hash_indices = [idx for idx, hsh in enumerate(side_hashes["hashes"]) if hsh == face_hash]
+        retval = [side_hashes["filenames"][idx] for idx in hash_indices]
+        logger.trace("side: %s, hash: %s, filenames: %s", side, face_hash, retval)
+        return retval
+
+
+def _stack_images(images):
+    """ Stack images evenly for preview.
+
+    Parameters
+    ----------
+    images: :class:`numpy.ndarray`
+        The preview images to be stacked
 
-def stack_images(images):
-    """ Stack images """
+    Returns
+    -------
+    :class:`numpy.ndarray`
+        The stacked preview images
+    """
     logger.debug("Stack images")
 
     def get_transpose_axes(num):
diff --git a/scripts/convert.py b/scripts/convert.py
index 0d90372..47cc6db 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -59,7 +59,7 @@ class Convert():
 
     @property
     def queue_size(self):
-        """ Set 16 for singleprocess otherwise 32 """
+        """ Set 16 for single process otherwise 32 """
         if self.args.singleprocess:
             retval = 16
         else:
@@ -204,7 +204,7 @@ class DiskIO():
         logger.debug(retval)
         return retval
 
-    # Initalization
+    # Initialization
     def get_writer(self):
         """ Return the writer plugin """
         args = [self.args.output_dir]
@@ -311,7 +311,7 @@ class DiskIO():
                 logger.debug("Load Queue: Stop signal received. Terminating")
                 break
             if image is None or (not image.any() and image.ndim not in (2, 3)):
-                # All black frames will return not np.any() so check dims too
+                # All black frames will return not numpy.any() so check dims too
                 logger.warning("Unable to open image. Skipping: '%s'", filename)
                 continue
             if self.check_skipframe(filename):
@@ -462,7 +462,7 @@ class Predict():
     @property
     def has_predicted_mask(self):
         """ Return whether this model has a predicted mask """
-        return bool(self.model.state.mask_shapes)
+        return bool(self.model.state.config.get("learn_mask", False))
 
     @staticmethod
     def get_batchsize(queue_size):
@@ -613,7 +613,7 @@ class Predict():
         """ Perform inference on the feed """
         logger.trace("Predicting: Batchsize: %s", len(feed_faces))
         feed = [feed_faces]
-        if self.has_predicted_mask:
+        if self.model.feed_mask:
             feed.append(np.repeat(self.input_mask, feed_faces.shape[0], axis=0))
         logger.trace("Input shape(s): %s", [item.shape for item in feed])
 
diff --git a/scripts/extract.py b/scripts/extract.py
index 2b1b2c0..bb9c500 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -235,7 +235,8 @@ class Extract():
 
         Loads the aligned face, perform any processing actions and verify the output.
 
-        Parameters:
+        Parameters
+        ----------
         extract_media: :class:`plugins.extract.pipeline.ExtractMedia`
             Output from :class:`plugins.extract.pipeline.Extractor`
         size: int
diff --git a/scripts/train.py b/scripts/train.py
index a3b9a95..ad51d1c 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -1,5 +1,5 @@
 #!/usr/bin python3
-""" The script to run the training process of faceswap """
+""" Main entry point to the training process of FaceSwap """
 
 import logging
 import os
@@ -15,7 +15,6 @@ from keras.backend.tensorflow_backend import set_session
 from lib.image import read_image
 from lib.keypress import KBHit
 from lib.multithreading import MultiThread
-from lib.queue_manager import queue_manager  # noqa pylint:disable=unused-import
 from lib.utils import get_folder, get_image_paths, set_system_verbosity, deprecation_warning
 from plugins.plugin_loader import PluginLoader
 
@@ -23,54 +22,103 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class Train():
-    """ The training process.  """
+    """ The Faceswap Training Process.
+
+    The training process is responsible for training a model on a set of source faces and a set of
+    destination faces.
+
+    The training process is self contained and should not be referenced by any other scripts, so it
+    contains no public properties.
+
+    Parameters
+    ----------
+    arguments: argparse.Namespace
+        The arguments to be passed to the training process as generated from Faceswap's command
+        line arguments
+    """
     def __init__(self, arguments):
         logger.debug("Initializing %s: (args: %s", self.__class__.__name__, arguments)
-        self.args = arguments
-        self.timelapse = self.set_timelapse()
-        self.images = self.get_images()
-        self.stop = False
-        self.save_now = False
-        self.preview_buffer = dict()
-        self.lock = Lock()
-
-        self.trainer_name = self.args.trainer
+        self._args = arguments
+        self._timelapse = self._set_timelapse()
+        self._images = self._get_images()
+        self._stop = False
+        self._save_now = False
+        self._preview_buffer = dict()
+        self._lock = Lock()
+
+        self.trainer_name = self._args.trainer
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def set_timelapse(self):
-        """ Set time-lapse paths if requested """
-        if (not self.args.timelapse_input_a and
-                not self.args.timelapse_input_b and
-                not self.args.timelapse_output):
+    @property
+    def _image_size(self):
+        """ int: The training image size. Reads the first image in the training folder and returns
+        the size. """
+        image = read_image(self._images["a"][0], raise_error=True)
+        size = image.shape[0]
+        logger.debug("Training image size: %s", size)
+        return size
+
+    @property
+    def _alignments_paths(self):
+        """ dict: The alignments paths for each of the source and destination faces. Key is the
+            side, value is the path to the alignments file """
+        alignments_paths = dict()
+        for side in ("a", "b"):
+            alignments_path = getattr(self._args, "alignments_path_{}".format(side))
+            if not alignments_path:
+                image_path = getattr(self._args, "input_{}".format(side))
+                alignments_path = os.path.join(image_path, "alignments.fsa")
+            alignments_paths[side] = alignments_path
+        logger.debug("Alignments paths: %s", alignments_paths)
+        return alignments_paths
+
+    def _set_timelapse(self):
+        """ Set time-lapse paths if requested.
+
+        Returns
+        -------
+        dict
+            The time-lapse keyword arguments for passing to the trainer
+
+        """
+        if (not self._args.timelapse_input_a and
+                not self._args.timelapse_input_b and
+                not self._args.timelapse_output):
             return None
-        if not self.args.timelapse_input_a or not self.args.timelapse_input_b:
+        if not self._args.timelapse_input_a or not self._args.timelapse_input_b:
             raise ValueError("To enable the timelapse, you have to supply "
                              "all the parameters (--timelapse-input-A and "
                              "--timelapse-input-B).")
 
         timelapse_output = None
-        if self.args.timelapse_output is not None:
-            timelapse_output = str(get_folder(self.args.timelapse_output))
+        if self._args.timelapse_output is not None:
+            timelapse_output = str(get_folder(self._args.timelapse_output))
 
-        for folder in (self.args.timelapse_input_a,
-                       self.args.timelapse_input_b,
+        for folder in (self._args.timelapse_input_a,
+                       self._args.timelapse_input_b,
                        timelapse_output):
             if folder is not None and not os.path.isdir(folder):
                 raise ValueError("The Timelapse path '{}' does not exist".format(folder))
 
-        kwargs = {"input_a": self.args.timelapse_input_a,
-                  "input_b": self.args.timelapse_input_b,
+        kwargs = {"input_a": self._args.timelapse_input_a,
+                  "input_b": self._args.timelapse_input_b,
                   "output": timelapse_output}
         logger.debug("Timelapse enabled: %s", kwargs)
         return kwargs
 
-    def get_images(self):
-        """ Check the image folders exist, contain images and return the image
-        objects """
+    def _get_images(self):
+        """ Check the image folders exist and contains images and obtain image paths.
+
+        Returns
+        -------
+        dict
+            The image paths for each side. The key is the side, the value is the list of paths
+            for that side.
+        """
         logger.debug("Getting image paths")
         images = dict()
         for side in ("a", "b"):
-            image_dir = getattr(self.args, "input_{}".format(side))
+            image_dir = getattr(self._args, "input_{}".format(side))
             if not os.path.isdir(image_dir):
                 logger.error("Error: '%s' does not exist", image_dir)
                 exit(1)
@@ -80,45 +128,62 @@ class Train():
                 logger.error("Error: '%s' contains no images", image_dir)
                 exit(1)
 
-        logger.info("Model A Directory: %s", self.args.input_a)
-        logger.info("Model B Directory: %s", self.args.input_b)
+        logger.info("Model A Directory: %s", self._args.input_a)
+        logger.info("Model B Directory: %s", self._args.input_b)
         logger.debug("Got image paths: %s", [(key, str(len(val)) + " images")
                                              for key, val in images.items()])
         return images
 
     def process(self):
-        """ Call the training process object """
+        """ The entry point for triggering the Training Process.
+
+        Should only be called from  :class:`lib.cli.ScriptExecutor`
+        """
         logger.debug("Starting Training Process")
-        logger.info("Training data directory: %s", self.args.model_dir)
+        logger.info("Training data directory: %s", self._args.model_dir)
 
         # TODO Move these args to config and remove these deprecation warnings
-        if hasattr(self.args, "warp_to_landmarks") and self.args.warp_to_landmarks:
+        if hasattr(self._args, "warp_to_landmarks") and self._args.warp_to_landmarks:
             deprecation_warning("`-wl`, ``--warp-to-landmarks``",
                                 additional_info="This option will be available within training "
                                                 "config settings (/config/train.ini).")
-        if hasattr(self.args, "no_augment_color") and self.args.no_flip:
+        if hasattr(self._args, "no_augment_color") and self._args.no_flip:
             deprecation_warning("`-nac`, ``--no-augment-color``",
                                 additional_info="This option will be available within training "
                                                 "config settings (/config/train.ini).")
-        set_system_verbosity(self.args.loglevel)
-        thread = self.start_thread()
-        # queue_manager.debug_monitor(1)
+        set_system_verbosity(self._args.loglevel)
+        thread = self._start_thread()
+        # from lib.queue_manager import queue_manager; queue_manager.debug_monitor(1)
 
-        err = self.monitor(thread)
+        err = self._monitor(thread)
 
-        self.end_thread(thread, err)
+        self._end_thread(thread, err)
         logger.debug("Completed Training Process")
 
-    def start_thread(self):
-        """ Put the training process in a thread so we can keep control """
+    def _start_thread(self):
+        """ Put the :func:`_training` into a background thread so we can keep control.
+
+        Returns
+        -------
+        :class:`lib.multithreading.MultiThread`
+            The background thread for running training
+        """
         logger.debug("Launching Trainer thread")
-        thread = MultiThread(target=self.training)
+        thread = MultiThread(target=self._training)
         thread.start()
         logger.debug("Launched Trainer thread")
         return thread
 
-    def end_thread(self, thread, err):
-        """ On termination output message and join thread back to main """
+    def _end_thread(self, thread, err):
+        """ Output message and join thread back to main on termination.
+
+        Parameters
+        ----------
+        thread: :class:`lib.multithreading.MultiThread`
+            The background training thread
+        err: bool
+            Whether an error has been detected in :func:`_monitor`
+        """
         logger.debug("Ending Training thread")
         if err:
             msg = "Error caught! Exiting..."
@@ -127,27 +192,27 @@ class Train():
             msg = ("Exit requested! The trainer will complete its current cycle, "
                    "save the models and quit (This can take a couple of minutes "
                    "depending on your training speed).")
-            if not self.args.redirect_gui:
+            if not self._args.redirect_gui:
                 msg += " If you want to kill it now, press Ctrl + c"
             log = logger.info
         log(msg)
-        self.stop = True
+        self._stop = True
         thread.join()
         sys.stdout.flush()
-        logger.debug("Ended Training thread")
+        logger.debug("Ended training thread")
 
-    def training(self):
-        """ The training process to be run inside a thread """
+    def _training(self):
+        """ The training process to be run inside a thread. """
         try:
             sleep(1)  # Let preview instructions flush out to logger
             logger.debug("Commencing Training")
             logger.info("Loading data, this may take a while...")
 
-            if self.args.allow_growth:
-                self.set_tf_allow_growth()
-            model = self.load_model()
-            trainer = self.load_trainer(model)
-            self.run_training_cycle(model, trainer)
+            if self._args.allow_growth:
+                self._set_tf_allow_growth()
+            model = self._load_model()
+            trainer = self._load_trainer(model)
+            self._run_training_cycle(model, trainer)
         except KeyboardInterrupt:
             try:
                 logger.debug("Keyboard Interrupt Caught. Saving Weights and exiting")
@@ -159,117 +224,130 @@ class Train():
         except Exception as err:
             raise err
 
-    def load_model(self):
-        """ Load the model requested for training """
+    def _load_model(self):
+        """ Load the model requested for training.
+
+        Returns
+        -------
+        :file:`plugins.train.model` plugin
+            The requested model plugin
+        """
         logger.debug("Loading Model")
-        model_dir = get_folder(self.args.model_dir)
-        configfile = self.args.configfile if hasattr(self.args, "configfile") else None
-        augment_color = not self.args.no_augment_color
+        model_dir = get_folder(self._args.model_dir)
+        configfile = self._args.configfile if hasattr(self._args, "configfile") else None
+        augment_color = not self._args.no_augment_color
         model = PluginLoader.get_model(self.trainer_name)(
             model_dir,
-            gpus=self.args.gpus,
+            gpus=self._args.gpus,
             configfile=configfile,
-            snapshot_interval=self.args.snapshot_interval,
-            no_logs=self.args.no_logs,
-            warp_to_landmarks=self.args.warp_to_landmarks,
+            snapshot_interval=self._args.snapshot_interval,
+            no_logs=self._args.no_logs,
+            warp_to_landmarks=self._args.warp_to_landmarks,
             augment_color=augment_color,
-            no_flip=self.args.no_flip,
-            training_image_size=self.image_size,
-            alignments_paths=self.alignments_paths,
-            preview_scale=self.args.preview_scale,
-            pingpong=self.args.pingpong,
-            memory_saving_gradients=self.args.memory_saving_gradients,
-            optimizer_savings=self.args.optimizer_savings,
+            no_flip=self._args.no_flip,
+            training_image_size=self._image_size,
+            alignments_paths=self._alignments_paths,
+            preview_scale=self._args.preview_scale,
+            pingpong=self._args.pingpong,
+            memory_saving_gradients=self._args.memory_saving_gradients,
+            optimizer_savings=self._args.optimizer_savings,
             predict=False)
         logger.debug("Loaded Model")
         return model
 
-    @property
-    def image_size(self):
-        """ Get the training set image size for storing in model data """
-        image = read_image(self.images["a"][0], raise_error=True)
-        size = image.shape[0]
-        logger.debug("Training image size: %s", size)
-        return size
+    def _load_trainer(self, model):
+        """ Load the trainer requested for training.
 
-    @property
-    def alignments_paths(self):
-        """ Set the alignments path to input folder if not provided """
-        alignments_paths = dict()
-        for side in ("a", "b"):
-            alignments_path = getattr(self.args, "alignments_path_{}".format(side))
-            if not alignments_path:
-                image_path = getattr(self.args, "input_{}".format(side))
-                alignments_path = os.path.join(image_path, "alignments.fsa")
-            alignments_paths[side] = alignments_path
-        logger.debug("Alignments paths: %s", alignments_paths)
-        return alignments_paths
+        Parameters
+        ----------
+        model: :file:`plugins.train.model` plugin
+            The requested model plugin
 
-    def load_trainer(self, model):
-        """ Load the trainer requested for training """
+        Returns
+        -------
+        :file:`plugins.train.trainer` plugin
+            The requested model trainer plugin
+        """
         logger.debug("Loading Trainer")
         trainer = PluginLoader.get_trainer(model.trainer)
         trainer = trainer(model,
-                          self.images,
-                          self.args.batch_size,
-                          self.args.configfile)
+                          self._images,
+                          self._args.batch_size,
+                          self._args.configfile)
         logger.debug("Loaded Trainer")
         return trainer
 
-    def run_training_cycle(self, model, trainer):
-        """ Perform the training cycle """
+    def _run_training_cycle(self, model, trainer):
+        """ Perform the training cycle.
+
+        Handles the background training, updating previews/time-lapse on each save interval,
+        and saving the model.
+
+        Parameters
+        ----------
+        model: :file:`plugins.train.model` plugin
+            The requested model plugin
+        trainer: :file:`plugins.train.trainer` plugin
+            The requested model trainer plugin
+        """
         logger.debug("Running Training Cycle")
-        if self.args.write_image or self.args.redirect_gui or self.args.preview:
-            display_func = self.show
+        if self._args.write_image or self._args.redirect_gui or self._args.preview:
+            display_func = self._show
         else:
             display_func = None
 
-        for iteration in range(0, self.args.iterations):
+        for iteration in range(0, self._args.iterations):
             logger.trace("Training iteration: %s", iteration)
-            save_iteration = iteration % self.args.save_interval == 0
-            viewer = display_func if save_iteration or self.save_now else None
-            timelapse = self.timelapse if save_iteration else None
+            save_iteration = iteration % self._args.save_interval == 0
+            viewer = display_func if save_iteration or self._save_now else None
+            timelapse = self._timelapse if save_iteration else None
             trainer.train_one_step(viewer, timelapse)
-            if self.stop:
+            if self._stop:
                 logger.debug("Stop received. Terminating")
                 break
             if save_iteration:
                 logger.trace("Save Iteration: (iteration: %s", iteration)
-                if self.args.pingpong:
+                if self._args.pingpong:
                     model.save_models()
                     trainer.pingpong.switch()
                 else:
                     model.save_models()
-            elif self.save_now:
+            elif self._save_now:
                 logger.trace("Save Requested: (iteration: %s", iteration)
                 model.save_models()
-                self.save_now = False
+                self._save_now = False
         logger.debug("Training cycle complete")
         model.save_models()
         trainer.clear_tensorboard()
-        self.stop = True
+        self._stop = True
 
-    def monitor(self, thread):
-        """ Monitor the console, and generate + monitor preview if requested """
-        is_preview = self.args.preview
+    def _monitor(self, thread):
+        """ Monitor the background :func:`_training` thread for key presses and errors.
+
+        Returns
+        -------
+        bool
+            ``True`` if there has been an error in the background thread otherwise ``False``
+        """
+        is_preview = self._args.preview
         logger.debug("Launching Monitor")
         logger.info("===================================================")
         logger.info("  Starting")
         if is_preview:
             logger.info("  Using live preview")
         logger.info("  Press '%s' to save and quit",
-                    "Terminate" if self.args.redirect_gui else "ENTER")
-        if not self.args.redirect_gui:
+                    "Terminate" if self._args.redirect_gui else "ENTER")
+        if not self._args.redirect_gui:
             logger.info("  Press 'S' to save model weights immediately")
         logger.info("===================================================")
 
-        keypress = KBHit(is_gui=self.args.redirect_gui)
+        keypress = KBHit(is_gui=self._args.redirect_gui)
         err = False
         while True:
             try:
                 if is_preview:
-                    with self.lock:
-                        for name, image in self.preview_buffer.items():
+                    with self._lock:
+                        for name, image in self._preview_buffer.items():
                             cv2.imshow(name, image)  # pylint: disable=no-member
                     cv_key = cv2.waitKey(1000)  # pylint: disable=no-member
                 else:
@@ -279,7 +357,7 @@ class Train():
                     logger.debug("Thread error detected")
                     err = True
                     break
-                if self.stop:
+                if self._stop:
                     logger.debug("Stop received")
                     break
 
@@ -289,7 +367,7 @@ class Train():
                     break
                 if is_preview and cv_key == ord("s"):
                     logger.info("Save requested")
-                    self.save_now = True
+                    self._save_now = True
 
                 # Console Monitor
                 if keypress.kbhit():
@@ -299,7 +377,7 @@ class Train():
                         break
                     if console_key in ("s", "S"):
                         logger.info("Save requested")
-                        self.save_now = True
+                        self._save_now = True
 
                 sleep(1)
             except KeyboardInterrupt:
@@ -310,14 +388,11 @@ class Train():
         return err
 
     @staticmethod
-    def keypress_monitor(keypress_queue):
-        """ Monitor stdin for key press """
-        while True:
-            keypress_queue.put(sys.stdin.read(1))
+    def _set_tf_allow_growth():
+        """ Allow TensorFlow to manage VRAM growth.
 
-    @staticmethod
-    def set_tf_allow_growth():
-        """ Allow TensorFlow to manage VRAM growth """
+        Enables the Tensorflow allow_growth option if requested in the command line arguments
+        """
         # pylint: disable=no-member
         logger.debug("Setting Tensorflow 'allow_growth' option")
         config = tf.ConfigProto()
@@ -326,28 +401,39 @@ class Train():
         set_session(tf.Session(config=config))
         logger.debug("Set Tensorflow 'allow_growth' option")
 
-    def show(self, image, name=""):
-        """ Generate the preview and write preview file output """
+    def _show(self, image, name=""):
+        """ Generate the preview and write preview file output.
+
+        Handles the output and display of preview images.
+
+        Parameters
+        ----------
+        image: :class:`numpy.ndarray`
+            The preview image to be displayed and/or written out
+        name: str, optional
+            The name of the image for saving or display purposes. If an empty string is passed
+            then it will automatically be names. Default: ""
+        """
         logger.trace("Updating preview: (name: %s)", name)
         try:
             scriptpath = os.path.realpath(os.path.dirname(sys.argv[0]))
-            if self.args.write_image:
+            if self._args.write_image:
                 logger.trace("Saving preview to disk")
                 img = "training_preview.jpg"
                 imgfile = os.path.join(scriptpath, img)
                 cv2.imwrite(imgfile, image)  # pylint: disable=no-member
                 logger.trace("Saved preview to: '%s'", img)
-            if self.args.redirect_gui:
+            if self._args.redirect_gui:
                 logger.trace("Generating preview for GUI")
                 img = ".gui_training_preview.jpg"
                 imgfile = os.path.join(scriptpath, "lib", "gui",
                                        ".cache", "preview", img)
                 cv2.imwrite(imgfile, image)  # pylint: disable=no-member
                 logger.trace("Generated preview for GUI: '%s'", img)
-            if self.args.preview:
+            if self._args.preview:
                 logger.trace("Generating preview for display: '%s'", name)
-                with self.lock:
-                    self.preview_buffer[name] = image
+                with self._lock:
+                    self._preview_buffer[name] = image
                 logger.trace("Generated preview for display: '%s'", name)
         except Exception as err:
             logging.error("could not preview sample")
