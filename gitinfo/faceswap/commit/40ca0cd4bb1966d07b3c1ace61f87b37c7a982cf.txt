commit 40ca0cd4bb1966d07b3c1ace61f87b37c7a982cf
Author: Ivanovar-08 <ivanovar-08@mail.ru>
Date:   Fri Jun 15 14:26:18 2018 +0300

    added new model

diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
index dfb7e13..f48a4e2 100644
--- a/plugins/Model_OriginalHighRes/Model.py
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -1,26 +1,32 @@
+#!/usr/bin/python3
+
 # Based on the original https://www.reddit.com/r/deepfakes/ code sample + contribs
 # Based on https://github.com/iperov/OpenDeepFaceSwap for Decoder multiple res block chain
 # Based on the https://github.com/shaoanlu/faceswap-GAN repo
 # source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb
 
 
+import enum
 import os
 import sys
+import warnings
+
+warnings.filterwarnings("ignore", category=FutureWarning)
 
 from keras.initializers import RandomNormal
 from keras.layers import Input, Dense, Flatten, Reshape
 from keras.layers import SeparableConv2D, add
 from keras.layers.advanced_activations import LeakyReLU
 from keras.layers.convolutional import Conv2D
+from keras.layers.core import Activation
 from keras.models import Model as KerasModel
 from keras.optimizers import Adam
 from keras.utils import multi_gpu_model
 
 from lib.PixelShuffler import PixelShuffler
+import lib.Serializer
 
 from . import __version__
-from keras.layers.core import Activation
-
 
 if isinstance(__version__, (list, tuple)):
     version_str = ".".join([str(n) for n in __version__[1:]])
@@ -30,51 +36,66 @@ else:
 
 mswindows = sys.platform=="win32"
 
-
 try:
     from lib.utils import backup_file
 except ImportError:
     pass
 
 
-class Encoders():
-    REGULAR = 'v2' # high memory consumption encoder
-    NEW_SLIM = 'v3' # slightly lighter on resources and taining speed is faster
+class EncoderType(enum.Enum):
+    ORIGINAL = "original"
+    SHAOANLU = "shaoanlu"
+    
+    
+ENCODER = EncoderType.ORIGINAL
+
 
+if ENCODER==EncoderType.SHAOANLU:
+    from .instance_normalization import InstanceNormalization
+
+conv_init = RandomNormal(0, 0.02)
     
-ENCODER = Encoders.NEW_SLIM
+def inst_norm():
+    return InstanceNormalization()     
 
-hdf = {'encoderH5': 'encoder_{version_str}{ENCODER}.h5'.format(**vars()),
-       'decoder_AH5': 'decoder_A_{version_str}{ENCODER}.h5'.format(**vars()),
-       'decoder_BH5': 'decoder_B_{version_str}{ENCODER}.h5'.format(**vars())}
 
+hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
+       'decoder_AH5': 'decoder_A_{version_str}{ENCODER.value}.h5'.format(**vars()),
+       'decoder_BH5': 'decoder_B_{version_str}{ENCODER.value}.h5'.format(**vars())}
 
-class Model():
 
-    # still playing with dims
-    ENCODER_DIM = 2048
-        
-    IMAGE_SIZE = 128, 128
-    IMAGE_DEPTH = len('RGB') # good to let ppl know what these are...
-    IMAGE_SHAPE = *IMAGE_SIZE, IMAGE_DEPTH
+class Model():
     
-    def __init__(self, model_dir, gpus):
-        
+    ENCODER_DIM = 1024 # dense layer size        
+    IMAGE_SHAPE = 128, 128 # image shape
+    
+    assert [n for n in IMAGE_SHAPE if n>=16]
+    
+    IMAGE_WIDTH = max(IMAGE_SHAPE)
+    IMAGE_WIDTH = (IMAGE_WIDTH//16 + (1 if (IMAGE_WIDTH%16)>=8 else 0))*16
+    IMAGE_SHAPE = IMAGE_WIDTH, IMAGE_WIDTH, len('BRG') # good to let ppl know what these are...
+    
+    
+    def __init__(self, model_dir, gpus, encoder_type=ENCODER):
+                
         if mswindows:  
             from ctypes import cdll    
             mydll = cdll.LoadLibrary("user32.dll")
-            mydll.SetProcessDPIAware(True)        
+            mydll.SetProcessDPIAware(True)                               
+        
+        self._encoder_type = encoder_type
         
         self.model_dir = model_dir
         
         # can't chnage gpu's when the model is initialized no point in making it r/w
         self._gpus = gpus 
         
-        Encoder = getattr(self, "Encoder") if not ENCODER else getattr(self, "Encoder_{}".format(ENCODER))
+        Encoder = getattr(self, "Encoder_{}".format(self._encoder_type.value))
+        Decoder = getattr(self, "Decoder_{}".format(self._encoder_type.value))
         
         self.encoder = Encoder()
-        self.decoder_A = self.Decoder()
-        self.decoder_B = self.Decoder()
+        self.decoder_A = Decoder()
+        self.decoder_B = Decoder()
         
         self.initModel()        
 
@@ -96,8 +117,18 @@ class Model():
         
         
     def load(self, swapped):
-        
+        from json import JSONDecodeError
         face_A, face_B = (hdf['decoder_AH5'], hdf['decoder_BH5']) if not swapped else (hdf['decoder_BH5'], hdf['decoder_AH5'])
+        
+        state_dir = os.path.join(self.model_dir, 'state_{version_str}_{ENCODER.value}.json'.format(**globals()))
+        ser = lib.Serializer.get_serializer('json')
+        try:
+            with open(state_dir, 'rb') as fp:
+                state = ser.unmarshal(fp.read())
+                self._epoch_no = state['epoch_no']
+        except (JSONDecodeError, IOError) as e:
+            print('Failed loading training state metadata', e)
+            self._epoch_no = 0        
 
         try:            
             self.encoder.load_weights(os.path.join(self.model_dir, hdf['encoderH5']))
@@ -106,37 +137,39 @@ class Model():
             print('loaded model weights')
             return True
         except Exception as e:
-            print('Failed loading existing training data.')
-            print(e)
+            print('Failed loading existing training data.', e)
             return False        
 
+
     def converter(self, swap):
         autoencoder = self.autoencoder_B if not swap else self.autoencoder_A
         return autoencoder.predict
     
-    def conv(self, filters, kernel_size=4, strides=2):
+    
+    def conv(self, filters, kernel_size=5, strides=2, **kwargs):
         def block(x):
-            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)         
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)         
             x = LeakyReLU(0.1)(x)
             return x
-        return block    
-        
-    def conv_sep(self, filters):
+        return block   
+
+    def conv_sep2(self, filters, kernel_size=5, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=4, strides=2, padding='same')(x)        
-            x = LeakyReLU(0.1)(x)
-            return x
-        return block
-    
-    def conv_sep_v3(self, filters, kernel_size=4, strides=2):
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)
+            x = Activation("relu")(x)
+            return x    
+        return block 
+        
+    def conv_sep3(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, padding="same")(x)
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)        
+            if use_instance_norm:
+                x = inst_norm()(x)
             x = Activation("relu")(x)
-            return x
-        return block       
-    
+            return x    
+        return block
     
-    def upscale(self, filters):
+    def upscale(self, filters, **kwargs):
         def block(x):
             x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)
             x = LeakyReLU(0.1)(x)
@@ -144,92 +177,81 @@ class Model():
             return x
         return block  
     
-    def upscale_sep(self, filters):
+    def upscale_sep3(self, filters, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters * 4, kernel_size=3, padding='same')(x)
+            x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
+            if use_instance_norm:
+                x = inst_norm()(x)
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
-        return block      
-    
-    def res(self, filters, dilation_rate=1):
-        def block(x):
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
-            rb = LeakyReLU(alpha=0.2)(rb)
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
-            x = add([rb, x])
-            x = LeakyReLU(alpha=0.2)(x)
-            return x
         return block    
-  
     
-    def res_block(self, filters, dilation_rate=1):
-        def block(x):
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
-            rb = LeakyReLU(alpha=0.2)(rb)
-            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
-            x = add([rb, x])
-            x = LeakyReLU(alpha=0.2)(x)
-            return x
-        return block       
-    
-    
-    def Encoder_v3(self):
-        """Lighter on resources encoder with bigger first conv layer"""
-        retina = Input(shape=self.IMAGE_SHAPE)
-        x = self.conv_sep_v3(192)(retina)         
-        x = self.conv(256)(x)
-        x = self.conv(384)(x)        
-        x = self.conv_sep_v3(512)(x)
-        x = self.conv(768)(x)
-        x = self.conv_sep_v3(1024)(x)
+    def Encoder_original(self, **kwargs):
+        impt = Input(shape=self.IMAGE_SHAPE)
+        
+        in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
+
+        x = self.conv(in_conv_filters)(impt)
+        x = self.conv_sep2(256)(x)
+        x = self.conv(512)(x)
+        x = self.conv_sep2(1024)(x)
+        
+        dense_shape = self.IMAGE_SHAPE[0] // 16         
         x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(4 * 4 * 1024)(x)
-        x = Reshape((4, 4, 1024))(x)
-        out = self.upscale(512)(x)
-        return KerasModel(retina, out)    
-    
+        x = Dense(dense_shape * dense_shape * 512)(x)
+        x = Reshape((dense_shape, dense_shape, 512))(x)
+        x = self.upscale(512)(x)
+        
+        return KerasModel(impt, x, **kwargs)    
+          
     
-    def Encoder_v2(self):
-        """Old algorithm; pretty good but slow"""
-        retina = Input(shape=self.IMAGE_SHAPE)
-        x = self.conv(128)(retina)         
-        x = self.conv(144)(x)              
-        x = self.conv_sep(256)(x)
-        x = self.conv(448)(x)        
-        x = self.conv_sep(512)(x)        
-        x = self.conv(768)(x)
-        x = self.conv_sep(1024)(x)
+    def Encoder_shaoanlu(self, **kwargs):
+        impt = Input(shape=self.IMAGE_SHAPE)
+                
+        in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
+        
+        x = Conv2D(in_conv_filters, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding="same")(impt)
+        x = self.conv_sep3(in_conv_filters+32, use_instance_norm=False)(x)
+        x = self.conv_sep3(256)(x)        
+        x = self.conv_sep3(512)(x)
+        x = self.conv_sep3(1024)(x)        
+        
+        dense_shape = self.IMAGE_SHAPE[0] // 16         
         x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(4 * 4 * 1024)(x)
-        x = Reshape((4, 4, 1024))(x)
-        out = self.upscale(512)(x)
-        return KerasModel(retina, out)
-    
+        x = Dense(dense_shape * dense_shape * 768)(x)
+        x = Reshape((dense_shape, dense_shape, 768))(x)
+        x = self.upscale(512)(x)
+        
+        return KerasModel(impt, x, **kwargs)    
 
-    def Decoder(self):
-        inp = Input(shape=(8, 8, 512))
-        x = self.upscale(384)(inp)
-        x = self.res_block(384)(x)
-        x = self.upscale_sep(192)(x)
-        x = self.res_block(192)(x)
-        x = self.upscale(128)(x)
-        x = self.res_block(128)(x)
-        x = self.upscale(64)(x)
-        x = self.res_block(64)(x)                    
+
+    def Decoder_original(self):       
+        decoder_shape = self.IMAGE_SHAPE[0]//8        
+        inpt = Input(shape=(decoder_shape, decoder_shape, 512))
         
-#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(x)
-#         rb = LeakyReLU(alpha=0.2)(rb)
-#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(rb)
-#         x = add([rb, x])
-# 
-        #x = self.upscale(32)(x)        
-                        
-        out = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
-                
-        return KerasModel(inp, out)
+        x = self.upscale(384, kernel_initializer=RandomNormal(0, 0.02))(inpt)
+        x = self.upscale(256-32, kernel_initializer=RandomNormal(0, 0.02))(x)
+        x = self.upscale(self.IMAGE_SHAPE[0], kernel_initializer=RandomNormal(0, 0.02))(x)
         
+        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+        
+        return KerasModel(inpt, x)
+    
     
+    def Decoder_shaoanlu(self):       
+        decoder_shape = self.IMAGE_SHAPE[0]//8        
+        inpt = Input(shape=(decoder_shape, decoder_shape, 512))
+        
+        x = self.upscale_sep3(512)(inpt)
+        x = self.upscale_sep3(256)(x)
+        x = self.upscale_sep3(self.IMAGE_SHAPE[0])(x)
+        
+        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+        
+        return KerasModel(inpt, x)    
+
+
     def save_weights(self):
         from threading import Thread
         from time import sleep
@@ -240,7 +262,18 @@ class Model():
             for model in hdf.values():            
                 backup_file(model_dir, model)
         except NameError:
-            print('backup functionality not available\n')                                      
+            print('backup functionality not available\n')       
+            
+        state_dir = os.path.join(self.model_dir, 'state_{version_str}_{ENCODER.value}.json'.format(**globals()))
+        ser = lib.Serializer.get_serializer('json')
+        try:
+            with open(state_dir, 'wb') as fp:
+                state_json = ser.marshal({
+                    'epoch_no' : self._epoch_no
+                     })
+                fp.write(state_json.encode('utf-8'))
+        except IOError as e:
+            pass                               
         
         # thought maybe I/O bound, sometimes saving in parallel is faster
         threads = []
@@ -267,15 +300,14 @@ class Model():
     @property
     def model_name(self):
         try:
-            return self._model_nomen
+            return self._model_name
         except AttributeError:
-            self._model_nomen = self._model_nomen = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
-        return self._model_nomen
+            self._model_name = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
+        return self._model_name
              
     
     def __str__(self):
         return "<{}: ver={}, nn_dims={}, img_size={}>".format(self.model_name, 
                                                               version_str, 
                                                               self.ENCODER_DIM, 
-                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))        
-        
+                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))                
\ No newline at end of file
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
index e05f90a..4e243b5 100644
--- a/plugins/Model_OriginalHighRes/Trainer.py
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -7,7 +7,6 @@ from lib.training_data import TrainingDataGenerator, stack_images
 
 
 TRANSFORM_PRC = 115.
-#TRANSFORM_PRC = 150.
 
 
 class Trainer():
@@ -22,13 +21,15 @@ class Trainer():
     def __init__(self, model, fn_A, fn_B, batch_size, *args):
         self.batch_size = batch_size
         self.model = model
+        from timeit import default_timer as clock
+        self._clock = clock
+        
 
-        #generator = TrainingDataGenerator(self.random_transform_args, 160)
-                
+        #generator = TrainingDataGenerator(self.random_transform_args, 160)                
         # make sre to keep zoom=2 or you won't get 128x128 vectors as input
         #generator = TrainingDataGenerator(self.random_transform_args, 220, 5, zoom=2)
-        generator = TrainingDataGenerator(self.random_transform_args, 160, 6, zoom=2)
         #generator = TrainingDataGenerator(self.random_transform_args, 180, 7, zoom=2)
+        generator = TrainingDataGenerator(self.random_transform_args, 160, 5, zoom=2)        
         
         self.images_A = generator.minibatchAB(fn_A, self.batch_size)
         self.images_B = generator.minibatchAB(fn_B, self.batch_size)
@@ -37,19 +38,22 @@ class Trainer():
         
 
     def train_one_step(self, iter_no, viewer):
-  
+        when = self._clock()
         _, warped_A, target_A = next(self.images_A)
         _, warped_B, target_B = next(self.images_B)
 
         loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)
-        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)        
-                        
-        print("[{0}] [#{1:05d}] loss_A: {2:.5f}, loss_B: {3:.5f}".format(
-            time.strftime("%H:%M:%S"), iter_no, loss_A, loss_B),
+        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)
+        
+        self.model._epoch_no += 1        
+                    
+        print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
+            time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A, loss_B),
             end='\r')
+        
 
         if viewer is not None:
-            viewer(self.show_sample(target_A[0:24], target_B[0:24]), "training using {}, bs={}".format(self.model, self.batch_size))
+            viewer(self.show_sample(target_A[0:8], target_B[0:8]), "training using {}, bs={}".format(self.model, self.batch_size))
             
 
     def show_sample(self, test_A, test_B):
diff --git a/scripts/convert.py b/scripts/convert.py
index 116d7ed..e4e0c01 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -70,6 +70,8 @@ class Convert(object):
         if not model.load(self.args.swap_model):
             print("Model Not Found! A valid model must be provided to continue!")
             exit(1)
+            
+        self._image_shape = model.IMAGE_SHAPE
 
         return model
 
@@ -87,7 +89,8 @@ class Convert(object):
                                                      erosion_kernel_size=args.erosion_kernel_size,
                                                      match_histogram=args.match_histogram,
                                                      smooth_mask=args.smooth_mask,
-                                                     avg_color_adjust=args.avg_color_adjust)
+                                                     avg_color_adjust=args.avg_color_adjust)        
+        
         return converter
 
     def prepare_images(self):
@@ -134,7 +137,10 @@ class Convert(object):
 
         image = self.images.rotate_image(image, face.r)
         # TODO: This switch between 64 and 128 is a hack for now.
-        # We should have a separate cli option for size        
+        # We should have a separate cli option for size    
+        
+        print('image_shape', self._image_shape)
+            
         size = 128 if (self.args.trainer.strip().lower() in ('gan128', 'originalhighres')) else 64        
         
         image = converter.patch_image(image,
