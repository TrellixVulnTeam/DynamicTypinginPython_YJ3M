commit a06d83abb79ee4341b189665b74c17b5b4ea7067
Author: Hidde Jansen <Ganonmaster@users.noreply.github.com>
Date:   Mon Dec 25 02:29:40 2017 +0100

    PEP8 (#23)

diff --git a/faceswap.py b/faceswap.py
index c47118e..4be491d 100755
--- a/faceswap.py
+++ b/faceswap.py
@@ -7,11 +7,14 @@ from scripts.convert import ConvertImage
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     subparser = parser.add_subparsers()
-    extract = ExtractTrainingData(subparser, "extract", "Extract the faces from a pictures.")
-    train = TrainingProcessor(subparser, "train", "This command trains the model for the two faces A and B.")
-    convert = ConvertImage(subparser, "convert", "Convert a source image to a new one with the face swapped.")
+    extract = ExtractTrainingData(
+        subparser, "extract", "Extract the faces from a pictures.")
+    train = TrainingProcessor(
+        subparser, "train", "This command trains the model for the two faces A and B.")
+    convert = ConvertImage(
+        subparser, "convert", "Convert a source image to a new one with the face swapped.")
     arguments = parser.parse_args()
     try:
         arguments.func(arguments)
     except:
-        parser.print_help()
\ No newline at end of file
+        parser.print_help()
diff --git a/lib/aligner.py b/lib/aligner.py
index 1fbae5b..cc23795 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -6,6 +6,7 @@ import cv2
 import dlib
 import numpy as np
 
+
 class Aligner:
     def __init__(self, pred, detect):
         self.detector = dlib.cnn_face_detection_model_v1(detect)
@@ -45,8 +46,8 @@ class Aligner:
         R = (U * Vt).T
 
         return np.vstack([np.hstack(((s2 / s1) * R,
-                            c2.T - (s2 / s1) * R * c1.T)),
-                            np.matrix([0., 0., 1.])])
+                                     c2.T - (s2 / s1) * R * c1.T)),
+                          np.matrix([0., 0., 1.])])
 
     def warp_im(self, im, ref, M):
         dshape = ref.shape
@@ -73,7 +74,8 @@ class Aligner:
             return None
         landmarks = self.get_landmarks(img, rect)
 
-        transformation_matrix = self.transformation_from_points(ref_landmarks, landmarks)
+        transformation_matrix = self.transformation_from_points(
+            ref_landmarks, landmarks)
         warped_img = self.warp_im(img, ref_img, transformation_matrix)
 
         #cv2.imwrite( 'modified/_aligned.png', warped_img )
diff --git a/lib/faces_process.py b/lib/faces_process.py
index a3ed357..e951cae 100644
--- a/lib/faces_process.py
+++ b/lib/faces_process.py
@@ -15,7 +15,7 @@ def convert_one_image(image, model_dir="models"):
     decoder_B.load_weights(model_dir + "/decoder_B.h5")
 
     autoencoder = autoencoder_B
-    
+
     shapePredictor = "contrib/shape_predictor_68_face_landmarks.dat"
     humanFaceDetector = "contrib/mmod_human_face_detector.dat"
     if not os.path.exists(shapePredictor):
diff --git a/lib/image_augmentation.py b/lib/image_augmentation.py
index 25f812d..64e65f0 100644
--- a/lib/image_augmentation.py
+++ b/lib/image_augmentation.py
@@ -5,19 +5,22 @@ from .umeyama import umeyama
 
 
 def random_transform(image, rotation_range, zoom_range, shift_range, random_flip):
-    h,w = image.shape[0:2]
+    h, w = image.shape[0:2]
     rotation = numpy.random.uniform(-rotation_range, rotation_range)
     scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)
     tx = numpy.random.uniform(-shift_range, shift_range) * w
     ty = numpy.random.uniform(-shift_range, shift_range) * h
     mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)
     mat[:, 2] += (tx, ty)
-    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)
+    result = cv2.warpAffine(
+        image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)
     if numpy.random.random() < random_flip:
         result = result[:, ::-1]
     return result
 
 # get pair of random warped images from aligened face image
+
+
 def random_warp(image):
     assert image.shape == (256, 256, 3)
     range_ = numpy.linspace(128 - 80, 128 + 80, 5)
@@ -39,4 +42,3 @@ def random_warp(image):
     target_image = cv2.warpAffine(image, mat, (64, 64))
 
     return warped_image, target_image
-
diff --git a/lib/model.py b/lib/model.py
index 86813ed..cc8b61d 100644
--- a/lib/model.py
+++ b/lib/model.py
@@ -63,4 +63,3 @@ autoencoder_A = Model(x, decoder_A(encoder(x)))
 autoencoder_B = Model(x, decoder_B(encoder(x)))
 autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')
 autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')
-
diff --git a/lib/training_data.py b/lib/training_data.py
index d9be3fc..83dd279 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -7,7 +7,7 @@ random_transform_args = {
     'zoom_range': 0.05,
     'shift_range': 0.05,
     'random_flip': 0.4,
-    }
+}
 
 
 def get_training_data(images, batch_size):
@@ -18,8 +18,10 @@ def get_training_data(images, batch_size):
         warped_img, target_img = random_warp(image)
 
         if i == 0:
-            warped_images = numpy.empty((batch_size,) + warped_img.shape, warped_img.dtype)
-            target_images = numpy.empty((batch_size,) + target_img.shape, warped_img.dtype)
+            warped_images = numpy.empty(
+                (batch_size,) + warped_img.shape, warped_img.dtype)
+            target_images = numpy.empty(
+                (batch_size,) + target_img.shape, warped_img.dtype)
 
         warped_images[i] = warped_img
         target_images[i] = target_img
