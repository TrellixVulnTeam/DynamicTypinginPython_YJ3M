commit 36be6cd4d832471671d670f9f6c29f47aba07ebc
Author: kvrooman <vrooman.kyle@gmail.com>
Date:   Thu Nov 14 07:01:35 2019 -0600

    Vectorize FAN post-processing (#926)

diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 903afa3..0d2e5d5 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -245,8 +245,7 @@ class Aligner(Extractor):
     @staticmethod
     def _normalize_clahe(face):
         """ Perform Contrast Limited Adaptive Histogram Equalization """
-        clahe = cv2.createCLAHE(clipLimit=2.0,
-                                tileGridSize=(4, 4))
+        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(4, 4))
         for chan in range(3):
             face[:, :, chan] = clahe.apply(face[:, :, chan])
         return face
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
index bee8764..02bf34e 100644
--- a/plugins/extract/align/fan.py
+++ b/plugins/extract/align/fan.py
@@ -25,7 +25,7 @@ class Align(Aligner):
         self.vram_warnings = 512  # Will run at this with warnings
         self.vram_per_batch = 64
         self.batchsize = self.config["batch-size"]
-        self.reference_scale = 195
+        self.reference_scale = 200. / 195.
 
     def init_model(self):
         """ Initialize FAN model """
@@ -36,14 +36,13 @@ class Align(Aligner):
                               allow_growth=self.config["allow_growth"])
         self.model.load_model()
         # Feed a placeholder so Aligner is primed for Manual tool
-        placeholder = np.zeros((self.batchsize, 3, self.input_size, self.input_size),
-                               dtype="float32")
+        placeholder_shape = (self.batchsize, 3, self.input_size, self.input_size)
+        placeholder = np.zeros(placeholder_shape, dtype="float32")
         self.model.predict(placeholder)
 
     def process_input(self, batch):
         """ Compile the detected faces for prediction """
-        # TODO Batching
-        logger.trace("Aligning faces around center")
+        logger.debug("Aligning faces around center")
         batch["center_scale"] = self.get_center_scale(batch["detected_faces"])
         faces = self.crop(batch)
         logger.trace("Aligned image around center")
@@ -53,89 +52,74 @@ class Align(Aligner):
 
     def get_center_scale(self, detected_faces):
         """ Get the center and set scale of bounding box """
-        logger.trace("Calculating center and scale")
-        l_center = []
-        l_scale = []
-        for face in detected_faces:
-            center = np.array([(face.left + face.right) / 2.0, (face.top + face.bottom) / 2.0])
-            center[1] -= face.h * 0.12
-            l_center.append(center)
-            l_scale.append((face.w + face.h) / self.reference_scale)
-        logger.trace("Calculated center and scale: %s, %s", l_center, l_scale)
-        return l_center, l_scale
+        logger.debug("Calculating center and scale")
+        center_scale = np.empty((len(detected_faces), 68, 3), dtype='float32')
+        # TODO modify detected face to hold this data as a matrix
+        for index, face in enumerate(detected_faces):
+            x_center = (face.left + face.right) / 2.0
+            y_center = (face.top + face.bottom) / 2.0 - face.h * 0.12
+            scale = (face.w + face.h) * self.reference_scale
+            center_scale[index, :, 0] = np.full(68, x_center, dtype='float32')
+            center_scale[index, :, 1] = np.full(68, y_center, dtype='float32')
+            center_scale[index, :, 2] = np.full(68, scale, dtype='float32')
+        logger.trace("Calculated center and scale: %s, %s", center_scale)
+        return center_scale
 
     def crop(self, batch):  # pylint:disable=too-many-locals
         """ Crop image around the center point """
-        logger.trace("Cropping images")
+        logger.debug("Cropping images")
+        sizes = (self.input_size, self.input_size)
+        batch_shape = batch["center_scale"].shape[:2]
+        resolutions = np.full(batch_shape, self.input_size, dtype='float32')
+        matrix_ones = np.ones(batch_shape + (3,), dtype='float32')
+        matrix_size = np.full(batch_shape + (3,), self.input_size, dtype='float32')
+        matrix_size[..., 2] = 1.0
+        upper_left = self.transform(matrix_ones, batch["center_scale"], resolutions)
+        bot_right = self.transform(matrix_size, batch["center_scale"], resolutions)
+
+        # TODO second pass .. convert to matrix
         new_images = []
-        for face, center, scale in zip(batch["detected_faces"], *batch["center_scale"]):
-            is_color = face.image.ndim > 2
-            v_ul = self.transform([1, 1], center, scale, self.input_size).astype(np.int)
-            v_br = self.transform([self.input_size, self.input_size],
-                                  center,
-                                  scale,
-                                  self.input_size).astype(np.int)
-            if is_color:
-                new_dim = np.array([v_br[1] - v_ul[1],
-                                    v_br[0] - v_ul[0],
-                                    face.image.shape[2]],
-                                   dtype=np.int32)
-                new_img = np.zeros(new_dim, dtype=np.uint8)
-            else:
-                new_dim = np.array([v_br[1] - v_ul[1],
-                                    v_br[0] - v_ul[0]],
-                                   dtype=np.int)
-                new_img = np.zeros(new_dim, dtype=np.uint8)
-            height = face.image.shape[0]
-            width = face.image.shape[1]
-            new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
-                             dtype=np.int32)
-            new_y = np.array([max(1, -v_ul[1] + 1),
-                              min(v_br[1], height) - v_ul[1]],
-                             dtype=np.int32)
-            old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
-                             dtype=np.int32)
-            old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
-                             dtype=np.int32)
-            if is_color:
-                new_img[new_y[0] - 1:new_y[1],
-                        new_x[0] - 1:new_x[1]] = face.image[old_y[0] - 1:old_y[1],
-                                                            old_x[0] - 1:old_x[1], :]
-            else:
-                new_img[new_y[0] - 1:new_y[1],
-                        new_x[0] - 1:new_x[1]] = face.image[old_y[0] - 1:old_y[1],
-                                                            old_x[0] - 1:old_x[1]]
-
-            if new_img.shape[0] < self.input_size:
-                interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
-            else:
-                interpolation = cv2.INTER_AREA  # pylint:disable=no-member
-
-            new_images.append(cv2.resize(new_img,  # pylint:disable=no-member
-                                         dsize=(int(self.input_size), int(self.input_size)),
-                                         interpolation=interpolation))
+        for face, ul, br in zip(batch["detected_faces"], upper_left, bot_right):
+            height, width = face.image.shape[:2]
+            channels = 3 if face.image.ndim > 2 else 1
+            br_width, br_height = br[0].astype('int32')
+            ul_width, ul_height = ul[0].astype('int32')
+            new_dim = (br_height - ul_height, br_width - ul_width, channels)
+            new_img = np.empty(new_dim, dtype=np.uint8)
+
+            new_x = slice(max(0, -ul_width), min(br_width, width) - ul_width)
+            new_y = slice(max(0, -ul_height), min(br_height, height) - ul_height)
+            old_x = slice(max(0, ul_width), min(br_width, width))
+            old_y = slice(max(0, ul_height), min(br_height, height))
+            new_img[new_y, new_x] = face.image[old_y, old_x]
+
+            interp = cv2.INTER_CUBIC if new_dim[0] < self.input_size else cv2.INTER_AREA
+            new_images.append(cv2.resize(new_img, dsize=sizes, interpolation=interp))
         logger.trace("Cropped images")
         return new_images
 
     @staticmethod
-    def transform(point, center, scale, resolution):
+    def transform(points, center_scales, resolutions):
         """ Transform Image """
-        logger.trace("Transforming Points")
-        pnt = np.array([point[0], point[1], 1.0])
-        hscl = 200.0 * scale
-        eye = np.eye(3)
-        eye[0, 0] = resolution / hscl
-        eye[1, 1] = resolution / hscl
-        eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
-        eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
-        eye = np.linalg.inv(eye)
-        retval = np.matmul(eye, pnt)[0:2]
+        logger.debug("Transforming Points")
+        num_images, num_landmarks = points.shape[:2]
+        transform_matrix = np.eye(3, dtype='float32')
+        transform_matrix = np.repeat(transform_matrix[None, :], num_landmarks, axis=0)
+        transform_matrix = np.repeat(transform_matrix[None, :, :], num_images, axis=0)
+        scales = center_scales[:, :, 2] / resolutions
+        translations = center_scales[..., 2:3] * -0.5 + center_scales[..., :2]
+        transform_matrix[:, :, 0, 0] = scales  # x scale
+        transform_matrix[:, :, 1, 1] = scales  # y scale
+        transform_matrix[:, :, 0, 2] = translations[:, :, 0]  # x translation
+        transform_matrix[:, :, 1, 2] = translations[:, :, 1]  # y translation
+        new_points = np.einsum('abij, abj -> abi', transform_matrix, points, optimize='greedy')
+        retval = new_points[:, :, :2].astype('float32')
         logger.trace("Transformed Points: %s", retval)
         return retval
 
     def predict(self, batch):
         """ Predict the 68 point landmarks """
-        logger.trace("Predicting Landmarks")
+        logger.debug("Predicting Landmarks")
         batch["prediction"] = self.model.predict(batch["feed"])[-1]
         logger.trace([pred.shape for pred in batch["prediction"]])
         return batch
@@ -147,34 +131,26 @@ class Align(Aligner):
 
     def get_pts_from_predict(self, batch):
         """ Get points from predictor """
-        logger.trace("Obtain points from prediction")
-        landmarks = []
-        for prediction, center, scale in zip(batch["prediction"], *batch["center_scale"]):
-            var_b = prediction.reshape((prediction.shape[0],
-                                        prediction.shape[1] * prediction.shape[2]))
-            var_c = var_b.argmax(1).reshape((prediction.shape[0],
-                                             1)).repeat(2,
-                                                        axis=1).astype(np.float)
-            var_c[:, 0] %= prediction.shape[2]
-            var_c[:, 1] = np.apply_along_axis(
-                lambda x: np.floor(x / prediction.shape[2]),
-                0,
-                var_c[:, 1])
-
-            for i in range(prediction.shape[0]):
-                pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
-                if 63 > pt_x > 0 and 63 > pt_y > 0:
-                    diff = np.array([prediction[i, pt_y, pt_x+1]
-                                     - prediction[i, pt_y, pt_x-1],
-                                     prediction[i, pt_y+1, pt_x]
-                                     - prediction[i, pt_y-1, pt_x]])
-
-                    var_c[i] += np.sign(diff)*0.25
-
-            var_c += 0.5
-            landmarks = [self.transform(var_c[i], center, scale, prediction.shape[2])
-                         for i in range(prediction.shape[0])]
-            batch.setdefault("landmarks", []).append(landmarks)
+        logger.debug("Obtain points from prediction")
+        num_images, num_landmarks, height, width = batch["prediction"].shape
+        image_slice = np.repeat(np.arange(num_images)[:, None], num_landmarks, axis=1)
+        landmark_slice = np.repeat(np.arange(num_landmarks)[None, :], num_images, axis=0)
+        resolution = np.full((num_images, num_landmarks), 64, dtype='int32')
+        subpixel_landmarks = np.ones((num_images, num_landmarks, 3), dtype='float32')
+
+        flat_indices = batch["prediction"].reshape(num_images, num_landmarks, -1).argmax(-1)
+        indices = np.array(np.unravel_index(flat_indices, (height, width)))
+        offsets = [(image_slice, landmark_slice, indices[0], indices[1] + 1),
+                   (image_slice, landmark_slice, indices[0], indices[1] - 1),
+                   (image_slice, landmark_slice, indices[0] + 1, indices[1]),
+                   (image_slice, landmark_slice, indices[0] - 1, indices[1])]
+        x_subpixel_shift = batch["prediction"][offsets[0]] - batch["prediction"][offsets[1]]
+        y_subpixel_shift = batch["prediction"][offsets[2]] - batch["prediction"][offsets[3]]
+        # TODO improve rudimentary subpixel logic to centroid of 3x3 window algorithm
+        subpixel_landmarks[:, :, 0] = indices[1] + np.sign(x_subpixel_shift) * 0.25 + 0.5
+        subpixel_landmarks[:, :, 1] = indices[0] + np.sign(y_subpixel_shift) * 0.25 + 0.5
+
+        batch["landmarks"] = self.transform(subpixel_landmarks, batch["center_scale"], resolution)
         logger.trace("Obtained points from prediction: %s", batch["landmarks"])
 
 
