commit e1fca147161a556109628f73ebba96c7ce7dc1f8
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Mon Jul 1 23:28:31 2019 +0000

    Add Restore Model Tool
    
    Tool: Add tool to restore models from backup
    Snapshot: Create snapshot based on total iterations rather than session iterations
    Models: Move backup/snapshot functions to lib/model
    Training: Output average loss since last save at each save iteration
    GUI: display_page.py: minor logging update

diff --git a/lib/gui/display_page.py b/lib/gui/display_page.py
index d944754..a984ffc 100644
--- a/lib/gui/display_page.py
+++ b/lib/gui/display_page.py
@@ -15,7 +15,7 @@ class DisplayPage(ttk.Frame):  # pylint: disable=too-many-ancestors
     """ Parent frame holder for each tab.
         Defines uniform structure for each tab to inherit from """
     def __init__(self, parent, tabname, helptext):
-        logger.debug("Initializing %s: (tabname: '%s', helptext: %s",
+        logger.debug("Initializing %s: (tabname: '%s', helptext: %s)",
                      self.__class__.__name__, tabname, helptext)
         ttk.Frame.__init__(self, parent)
         self.pack(fill=tk.BOTH, side=tk.TOP, anchor=tk.NW)
@@ -151,6 +151,8 @@ class DisplayOptionalPage(DisplayPage):  # pylint: disable=too-many-ancestors
     """ Parent Context Sensitive Display Tab """
 
     def __init__(self, parent, tabname, helptext, waittime, command=None):
+        logger.debug("%s: OptionalPage args: (waittime: %s, command: %s)",
+                     self.__class__.__name__, waittime, command)
         DisplayPage.__init__(self, parent, tabname, helptext)
 
         self.command = command
diff --git a/lib/gui/wrapper.py b/lib/gui/wrapper.py
index abf81a2..32930d1 100644
--- a/lib/gui/wrapper.py
+++ b/lib/gui/wrapper.py
@@ -186,7 +186,7 @@ class FaceswapControl():
                         (self.command == "effmpeg" and self.capture_ffmpeg(output)) or
                         (self.command not in ("train", "effmpeg") and self.capture_tqdm(output))):
                     continue
-                if self.command == "train" and output.strip().endswith("saved models"):
+                if self.command == "train" and "[saved models]" in output.strip().lower():
                     logger.debug("Trigger update preview")
                     self.wrapper.tk_vars["updatepreview"].set(True)
                 print(output.strip())
diff --git a/lib/model/backup_restore.py b/lib/model/backup_restore.py
new file mode 100644
index 0000000..12376c2
--- /dev/null
+++ b/lib/model/backup_restore.py
@@ -0,0 +1,146 @@
+#!/usr/bin/env python3
+
+""" Functions for backing up, restoring and snapshotting models """
+
+import logging
+import os
+from datetime import datetime
+from shutil import copyfile, copytree
+
+from lib import Serializer
+from lib.utils import get_folder
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+class Backup():
+    """ Holds information about model location and functions for backing up
+        Restoring and Snapshotting models """
+    def __init__(self, model_dir, model_name):
+        logger.debug("Initializing %s: (model_dir: '%s', model_name: '%s')",
+                     self.__class__.__name__, model_dir, model_name)
+        self.model_dir = str(model_dir)
+        self.model_name = model_name
+        logger.debug("Initialized %s", self.__class__.__name__)
+
+    def check_valid(self, filename, for_restore=False):
+        """ Check if the passed in filename is valid for a backup operation """
+        fullpath = os.path.join(self.model_dir, filename)
+        if not filename.startswith(self.model_name):
+            # Any filename that does not start with the model name are invalid
+            # for all operations
+            retval = False
+        elif for_restore and filename.endswith(".bk"):
+            # Only filenames ending in .bk are valid for restoring
+            retval = True
+        elif not for_restore and ((os.path.isfile(fullpath) and not filename.endswith(".bk")) or
+                                  (os.path.isdir(fullpath) and
+                                   filename == "{}_logs".format(self.model_name))):
+            # Only filenames that do not end with .bk or folders that are the logs folder
+            # are valid for backup
+            retval = True
+        else:
+            retval = False
+        logger.debug("'%s' valid for backup operation: %s", filename, retval)
+        return retval
+
+    @staticmethod
+    def backup_model(fullpath):
+        """ Backup Model File
+            Fullpath should be the path to an h5.py file or a state.json file """
+        backupfile = fullpath + ".bk"
+        logger.verbose("Backing up: '%s' to '%s'", fullpath, backupfile)
+        if os.path.exists(backupfile):
+            os.remove(backupfile)
+        if os.path.exists(fullpath):
+            os.rename(fullpath, backupfile)
+
+    def snapshot_models(self, iterations):
+        """ Take a snapshot of the model at current state and back up """
+        logger.info("Saving snapshot")
+        dst = str(get_folder("{}_snapshot_{}_iters".format(self.model_dir, iterations)))
+        for filename in os.listdir(self.model_dir):
+            if not self.check_valid(filename, for_restore=False):
+                logger.debug("Not snapshotting file: '%s'", filename)
+                continue
+            srcfile = os.path.join(self.model_dir, filename)
+            dstfile = os.path.join(dst, filename)
+            copyfunc = copytree if os.path.isdir(srcfile) else copyfile
+            logger.debug("Saving snapshot: '%s' > '%s'", srcfile, dstfile)
+            copyfunc(srcfile, dstfile)
+        logger.info("Saved snapshot")
+
+    def restore(self):
+        """ Restores a model from backup.
+            This will place all existing models/logs into a folder named:
+                - "<model_name>_archived_<timestamp>"
+            Copy all .bk files to replace original files
+            Remove logs from after the restore session_id from the logs folder """
+        archive_dir = self.move_archived()
+        self.restore_files()
+        self.restore_logs(archive_dir)
+
+    def move_archived(self):
+        """ Move archived files to archived folder and return archived folder name """
+        logger.info("Archiving existing model files...")
+        now = datetime.now().strftime("%Y%m%d_%H%M%S")
+        archive_dir = os.path.join(self.model_dir, "{}_archived_{}".format(self.model_name, now))
+        os.mkdir(archive_dir)
+        for filename in os.listdir(self.model_dir):
+            if not self.check_valid(filename, for_restore=False):
+                logger.debug("Not moving file to archived: '%s'", filename)
+                continue
+            logger.verbose("Moving '%s' to archived model folder: '%s'", filename, archive_dir)
+            src = os.path.join(self.model_dir, filename)
+            dst = os.path.join(archive_dir, filename)
+            os.rename(src, dst)
+        logger.verbose("Archived existing model files")
+        return archive_dir
+
+    def restore_files(self):
+        """ Restore files from .bk """
+        logger.info("Restoring models from backup...")
+        for filename in os.listdir(self.model_dir):
+            if not self.check_valid(filename, for_restore=True):
+                logger.debug("Not restoring file: '%s'", filename)
+                continue
+            dstfile = os.path.splitext(filename)[0]
+            logger.verbose("Restoring '%s' to '%s'", filename, dstfile)
+            src = os.path.join(self.model_dir, filename)
+            dst = os.path.join(self.model_dir, dstfile)
+            copyfile(src, dst)
+        logger.verbose("Restored models from backup")
+
+    def restore_logs(self, archive_dir):
+        """ Restore the log files since before archive """
+        logger.info("Restoring Logs...")
+        session_names = self.get_session_names()
+        log_dirs = self.get_log_dirs(archive_dir, session_names)
+        for log_dir in log_dirs:
+            src = os.path.join(archive_dir, log_dir)
+            dst = os.path.join(self.model_dir, log_dir)
+            logger.verbose("Restoring logfile: %s", dst)
+            copytree(src, dst)
+        logger.verbose("Restored Logs")
+
+    def get_session_names(self):
+        """ Get the existing session names from state file """
+        serializer = Serializer.get_serializer("json")
+        state_file = os.path.join(self.model_dir,
+                                  "{}_state.{}".format(self.model_name, serializer.ext))
+        with open(state_file, "rb") as inp:
+            state = serializer.unmarshal(inp.read().decode("utf-8"))
+            session_names = ["session_{}".format(key)
+                             for key in state["sessions"].keys()]
+        logger.debug("Session to restore: %s", session_names)
+        return session_names
+
+    def get_log_dirs(self, archive_dir, session_names):
+        """ Get the session logdir paths in the archive folder """
+        archive_logs = os.path.join(archive_dir, "{}_logs".format(self.model_name))
+        paths = [os.path.join(dirpath.replace(archive_dir, "")[1:], folder)
+                 for dirpath, dirnames, _ in os.walk(archive_logs)
+                 for folder in dirnames
+                 if folder in session_names]
+        logger.debug("log folders to restore: %s", paths)
+        return paths
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index ece6e74..6c11e53 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -10,7 +10,6 @@ import sys
 import time
 
 from json import JSONDecodeError
-from shutil import copyfile, copytree
 
 import keras
 from keras import losses
@@ -20,10 +19,11 @@ from keras.optimizers import Adam
 from keras.utils import get_custom_objects, multi_gpu_model
 
 from lib import Serializer
+from lib.model.backup_restore import Backup
 from lib.model.losses import DSSIMObjective, PenalizedLoss
 from lib.model.nn_blocks import NNBlocks
 from lib.multithreading import MultiThread
-from lib.utils import FaceswapError, get_folder
+from lib.utils import FaceswapError
 from plugins.train._config import Config
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -36,6 +36,7 @@ class ModelBase():
                  model_dir,
                  gpus,
                  configfile=None,
+                 snapshot_interval=0,
                  no_logs=False,
                  warp_to_landmarks=False,
                  augment_color=True,
@@ -50,17 +51,18 @@ class ModelBase():
                  memory_saving_gradients=False,
                  predict=False):
         logger.debug("Initializing ModelBase (%s): (model_dir: '%s', gpus: %s, configfile: %s, "
-                     "no_logs: %s, warp_to_landmarks: %s, augment_color: %s, no_flip: %s, "
-                     "training_image_size, %s, alignments_paths: %s, preview_scale: %s, "
-                     "input_shape: %s, encoder_dim: %s, trainer: %s, pingpong: %s, "
-                     "memory_saving_gradients: %s, predict: %s)",
-                     self.__class__.__name__, model_dir, gpus, configfile, no_logs,
-                     warp_to_landmarks, augment_color, no_flip, training_image_size,
+                     "snapshot_interval: %s, no_logs: %s, warp_to_landmarks: %s, augment_color: "
+                     "%s, no_flip: %s, training_image_size, %s, alignments_paths: %s, "
+                     "preview_scale: %s, input_shape: %s, encoder_dim: %s, trainer: %s, "
+                     "pingpong: %s, memory_saving_gradients: %s, predict: %s)",
+                     self.__class__.__name__, model_dir, gpus, configfile, snapshot_interval,
+                     no_logs, warp_to_landmarks, augment_color, no_flip, training_image_size,
                      alignments_paths, preview_scale, input_shape, encoder_dim, trainer, pingpong,
                      memory_saving_gradients, predict)
 
         self.predict = predict
         self.model_dir = model_dir
+        self.backup = Backup(self.model_dir, self.name)
         self.gpus = gpus
         self.configfile = configfile
         self.blocks = NNBlocks(use_subpixel=self.config["subpixel_upscaling"],
@@ -92,7 +94,8 @@ class ModelBase():
                               "warp_to_landmarks": warp_to_landmarks,
                               "augment_color": augment_color,
                               "no_flip": no_flip,
-                              "pingpong": pingpong}
+                              "pingpong": pingpong,
+                              "snapshot_interval": snapshot_interval}
 
         self.set_gradient_type(memory_saving_gradients)
         self.build()
@@ -177,8 +180,8 @@ class ModelBase():
             if "must be from the same graph" in str(err).lower():
                 msg = ("There was an error loading saved weights. This is most likely due to "
                        "model corruption during a previous save."
-                       "\nYou should restore weights from a snapshot or from the .bk files "
-                       "located in your model folder: '{}'".format(self.model_dir))
+                       "\nYou should restore weights from a snapshot or from backup files. "
+                       "You can use the 'Restore' Tool to restore from backup.")
                 raise FaceswapError(msg) from err
         self.log_summary()
         self.compile_predictors(initialize=True)
@@ -382,6 +385,12 @@ class ModelBase():
                 logger.verbose("%s:", name.title())
                 nnmeta.network.summary(print_fn=lambda x: logger.verbose("R|%s", x))
 
+    def do_snapshot(self):
+        """ Perform a model snapshot """
+        logger.debug("Performing snapshot")
+        self.backup.snapshot_models(self.iterations)
+        logger.debug("Performed snapshot")
+
     def load_models(self, swapped):
         """ Load models from file """
         logger.debug("Load model: (swapped: %s)", swapped)
@@ -407,80 +416,79 @@ class ModelBase():
             logger.info("Loaded model from disk: '%s'", self.model_dir)
         return is_loaded
 
-    def save_models(self, snapshot_iteration):
+    def save_models(self):
         """ Backup and save the models """
         logger.debug("Backing up and saving models")
-        should_backup = self.get_save_averages()
+        save_averages = self.get_save_averages()
+        backup_func = self.backup.backup_model if self.should_backup(save_averages) else None
+        if backup_func:
+            logger.info("Backing up models...")
         save_threads = list()
         for network in self.networks.values():
             name = "save_{}".format(network.name)
             save_threads.append(MultiThread(network.save,
                                             name=name,
-                                            should_backup=should_backup))
+                                            backup_func=backup_func))
         save_threads.append(MultiThread(self.state.save,
                                         name="save_state",
-                                        should_backup=should_backup))
+                                        backup_func=backup_func))
         for thread in save_threads:
             thread.start()
         for thread in save_threads:
             if thread.has_error:
                 logger.error(thread.errors[0])
             thread.join()
-        logger.info("saved models")
-        if snapshot_iteration:
-            self.snapshot_models()
-
-    def snapshot_models(self):
-        """ Take a snapshot of the model at current state and back up """
-        logger.info("Saving snapshot")
-        src = str(self.model_dir)
-        dst = str(get_folder("{}_snapshot_{}_iters".format(src, self.iterations)))
-        for filename in os.listdir(src):
-            if filename.endswith(".bk"):
-                continue
-            srcfile = os.path.join(src, filename)
-            dstfile = os.path.join(dst, filename)
-            copyfunc = copytree if os.path.isdir(srcfile) else copyfile
-            logger.debug("Saving snapshot: '%s' > '%s'", srcfile, dstfile)
-            copyfunc(srcfile, dstfile)
-        logger.info("Saved snapshot")
+        msg = "[Saved models]"
+        if save_averages:
+            lossmsg = ["{}_{}: {:.5f}".format(self.state.loss_names[side][0],
+                                              side.capitalize(),
+                                              save_averages[side])
+                       for side in sorted(list(save_averages.keys()))]
+            msg += " - Average since last save: {}".format(", ".join(lossmsg))
+        logger.info(msg)
 
     def get_save_averages(self):
-        """ Return the loss averages since last save and reset historical losses
+        """ Return the average loss since the last save iteration and reset historical loss """
+        logger.debug("Getting save averages")
+        avgs = dict()
+        for side, loss in self.history.items():
+            if not loss:
+                logger.debug("No loss in self.history: %s", side)
+                break
+            avgs[side] = sum(loss) / len(loss)
+            self.history[side] = list()  # Reset historical loss
+        logger.debug("Average losses since last save: %s", avgs)
+        return avgs
+
+    def should_backup(self, save_averages):
+        """ Check whether the loss averages for all losses is the lowest that has been seen.
 
             This protects against model corruption by only backing up the model
             if any of the loss values have fallen.
             TODO This is not a perfect system. If the model corrupts on save_iteration - 1
             then model may still backup
         """
-        logger.debug("Getting Average loss since last save")
-        avgs = dict()
         backup = True
 
-        for side, loss in self.history.items():
-            if not loss:
-                backup = False
-                break
-
-            avgs[side] = sum(loss) / len(loss)
-            self.history[side] = list()  # Reset historical loss
+        if not save_averages:
+            logger.debug("No save averages. Not backing up")
+            return
 
+        for side, loss in save_averages.items():
             if not self.state.lowest_avg_loss.get(side, None):
                 logger.debug("Setting initial save iteration loss average for '%s': %s",
-                             side, avgs[side])
-                self.state.lowest_avg_loss[side] = avgs[side]
+                             side, loss)
+                self.state.lowest_avg_loss[side] = loss
                 continue
-
             if backup:
                 # Only run this if backup is true. All losses must have dropped for a valid backup
-                backup = self.check_loss_drop(side, avgs[side])
+                backup = self.check_loss_drop(side, loss)
 
         logger.debug("Lowest historical save iteration loss average: %s",
                      self.state.lowest_avg_loss)
-        logger.debug("Average loss since last save: %s", avgs)
 
         if backup:  # Update lowest loss values to the state
-            for side, avg_loss in avgs.items():
+            for side, avg_loss in save_averages.items():
                 logger.debug("Updating lowest save iteration average for '%s': %s", side, avg_loss)
                 self.state.lowest_avg_loss[side] = avg_loss
 
@@ -605,30 +613,20 @@ class NNMeta():
         self.network.name = self.type
         return True
 
-    def save(self, fullpath=None, should_backup=False):
+    def save(self, fullpath=None, backup_func=None):
         """ Save model """
         fullpath = fullpath if fullpath else self.filename
-        if should_backup:
-            self.backup(fullpath=fullpath)
+        if backup_func:
+            backup_func(fullpath)
         logger.debug("Saving model: '%s'", fullpath)
         self.weights = self.network.get_weights()
         self.network.save(fullpath)
 
-    def backup(self, fullpath=None):
-        """ Backup Model """
-        origfile = fullpath if fullpath else self.filename
-        backupfile = origfile + ".bk"
-        logger.debug("Backing up: '%s' to '%s'", origfile, backupfile)
-        if os.path.exists(backupfile):
-            os.remove(backupfile)
-        if os.path.exists(origfile):
-            os.rename(origfile, backupfile)
-
     def convert_legacy_weights(self):
         """ Convert legacy weights files to hold the model topology """
         logger.info("Adding model topology to legacy weights file: '%s'", self.filename)
         self.network.load_weights(self.filename)
-        self.save(should_backup=False)
+        self.save(backup_func=None)
         self.network.name = self.type
 
 
@@ -731,11 +729,11 @@ class State():
         except JSONDecodeError as err:
             logger.debug("JSONDecodeError: %s:", str(err))
 
-    def save(self, should_backup=False):
+    def save(self, backup_func=None):
         """ Save iteration number to state file """
         logger.debug("Saving State")
-        if should_backup:
-            self.backup()
+        if backup_func:
+            backup_func(self.filename)
         try:
             with open(self.filename, "wb") as out:
                 state = {"name": self.name,
@@ -751,16 +749,6 @@ class State():
             logger.error("Unable to save model state: %s", str(err.strerror))
         logger.debug("Saved State")
 
-    def backup(self):
-        """ Backup state file """
-        origfile = self.filename
-        backupfile = origfile + ".bk"
-        logger.debug("Backing up: '%s' to '%s'", origfile, backupfile)
-        if os.path.exists(backupfile):
-            os.remove(backupfile)
-        if os.path.exists(origfile):
-            os.rename(origfile, backupfile)
-
     def replace_config(self, config_changeable_items):
         """ Replace the loaded config with the one contained within the state file
             Check for any fixed=False parameters changes and log info changes
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index 753aaa8..4bb827f 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -14,6 +14,7 @@
         mask_type:          Type of mask to use. See lib.model.masks for valid mask names.
                             Set to None for not used
         no_logs:            Disable tensorboard logging
+        snapshot_interval:  Interval for saving model snapshots
         warp_to_landmarks:  Use random_warp_landmarks instead of random_warp
         augment_color:      Perform random shifting of L*a*b* colors
         no_flip:            Don't perform a random flip on the image
@@ -153,13 +154,11 @@ class TrainerBase():
     def print_loss(self, loss):
         """ Override for specific model loss formatting """
         logger.trace(loss)
-        output = list()
-        for side in sorted(list(loss.keys())):
-            display = ", ".join(["{}_{}: {:.5f}".format(self.model.state.loss_names[side][idx],
-                                                        side.capitalize(),
-                                                        this_loss)
-                                 for idx, this_loss in enumerate(loss[side])])
-            output.append(display)
+        output = [", ".join(["{}_{}: {:.5f}".format(self.model.state.loss_names[side][idx],
+                                                    side.capitalize(),
+                                                    this_loss)
+                             for idx, this_loss in enumerate(loss[side])])
+                  for side in sorted(list(loss.keys()))]
         output = ", ".join(output)
         print("[{}] [#{:05d}] {}".format(self.timestamp, self.model.iterations, output), end='\r')
 
@@ -168,6 +167,10 @@ class TrainerBase():
         logger.trace("Training one step: (iteration: %s)", self.model.iterations)
         do_preview = viewer is not None
         do_timelapse = timelapse_kwargs is not None
+        snapshot_interval = self.model.training_opts.get("snapshot_interval", 0)
+        do_snapshot = (snapshot_interval != 0 and
+                       self.model.iterations >= snapshot_interval and
+                       self.model.iterations % snapshot_interval == 0)
         loss = dict()
         for side, batcher in self.batchers.items():
             if self.pingpong.active and side != self.pingpong.side:
@@ -201,6 +204,9 @@ class TrainerBase():
         if do_timelapse:
             self.timelapse.output_timelapse()
 
+        if do_snapshot:
+            self.model.do_snapshot()
+
     def store_history(self, side, loss):
         """ Store the history of this step """
         logger.trace("Updating loss history: '%s'", side)
diff --git a/scripts/train.py b/scripts/train.py
index 52e6544..13ce433 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -141,7 +141,7 @@ class Train():
         except KeyboardInterrupt:
             try:
                 logger.debug("Keyboard Interrupt Caught. Saving Weights and exiting")
-                model.save_models(False)
+                model.save_models()
                 trainer.clear_tensorboard()
             except KeyboardInterrupt:
                 logger.info("Saving model weights has been cancelled!")
@@ -159,6 +159,7 @@ class Train():
             model_dir,
             self.args.gpus,
             configfile=configfile,
+            snapshot_interval=self.args.snapshot_interval,
             no_logs=self.args.no_logs,
             warp_to_landmarks=self.args.warp_to_landmarks,
             augment_color=augment_color,
@@ -214,9 +215,6 @@ class Train():
 
         for iteration in range(0, self.args.iterations):
             logger.trace("Training iteration: %s", iteration)
-            snapshot_iteration = bool(self.args.snapshot_interval != 0 and
-                                      iteration >= self.args.snapshot_interval and
-                                      iteration % self.args.snapshot_interval == 0)
             save_iteration = iteration % self.args.save_interval == 0
             viewer = display_func if save_iteration or self.save_now else None
             timelapse = self.timelapse if save_iteration else None
@@ -227,16 +225,16 @@ class Train():
             if save_iteration:
                 logger.trace("Save Iteration: (iteration: %s", iteration)
                 if self.args.pingpong:
-                    model.save_models(snapshot_iteration)
+                    model.save_models()
                     trainer.pingpong.switch()
                 else:
-                    model.save_models(snapshot_iteration)
+                    model.save_models()
             elif self.save_now:
                 logger.trace("Save Requested: (iteration: %s", iteration)
-                model.save_models(False)
+                model.save_models()
                 self.save_now = False
         logger.debug("Training cycle complete")
-        model.save_models(False)
+        model.save_models()
         trainer.clear_tensorboard()
         self.stop = True
 
diff --git a/tools.py b/tools.py
index 7742a53..0032d74 100755
--- a/tools.py
+++ b/tools.py
@@ -37,6 +37,9 @@ if __name__ == "__main__":
     EFFMPEG = cli.EffmpegArgs(SUBPARSER,
                               "effmpeg",
                               "This command allows you to easily execute common ffmpeg tasks.")
+    RESTORE = cli.RestoreArgs(SUBPARSER,
+                              "restore",
+                              "This command lets you restore models from backup.")
     SORT = cli.SortArgs(SUBPARSER,
                         "sort",
                         "This command lets you sort images using various methods.")
diff --git a/tools/cli.py b/tools/cli.py
index 2b3e557..76a8477 100644
--- a/tools/cli.py
+++ b/tools/cli.py
@@ -390,13 +390,28 @@ class EffmpegArgs(FaceSwapArgs):
         return argument_list
 
 
+class RestoreArgs(FaceSwapArgs):
+    """ Class to restore model files from backup """
+
+    @staticmethod
+    def get_argument_list():
+        """ Put the arguments in a list so that they are accessible from both argparse and gui """
+        argument_list = list()
+        argument_list.append({"opts": ("-m", "--model-dir"),
+                              "action": DirFullPaths,
+                              "dest": "model_dir",
+                              "required": True,
+                              "help": "Model directory. A directory containing the model "
+                                      "you wish to restore from backup."})
+        return argument_list
+
+
 class SortArgs(FaceSwapArgs):
     """ Class to parse the command line arguments for sort tool """
 
     @staticmethod
     def get_argument_list():
-        """ Put the arguments in a list so that they are accessible from both
-        argparse and gui """
+        """ Put the arguments in a list so that they are accessible from both argparse and gui """
         argument_list = list()
         argument_list.append({"opts": ('-i', '--input'),
                               "action": DirFullPaths,
diff --git a/tools/restore.py b/tools/restore.py
new file mode 100644
index 0000000..20b480b
--- /dev/null
+++ b/tools/restore.py
@@ -0,0 +1,47 @@
+#!/usr/bin/env python3
+""" Tool to restore models from backup """
+
+import logging
+import os
+
+from lib.model.backup_restore import Backup
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+class Restore():
+    """ Restore a model from backup """
+
+    def __init__(self, arguments):
+        logger.debug("Initializing %s: (arguments: '%s'", self.__class__.__name__, arguments)
+        self.model_dir = arguments.model_dir
+        self.model_name = None
+
+    def process(self):
+        """ Perform the Restore process """
+        logger.info("Starting Model Restore...")
+        self.validate()
+        backup = Backup(self.model_dir, self.model_name)
+        backup.restore()
+        logger.info("Completed Model Restore")
+
+    def validate(self):
+        """ Make sure there is only one model in the target folder """
+        if not os.path.exists(self.model_dir):
+            logger.error("Folder does not exist: '%s'", self.model_dir)
+            exit(1)
+        chkfiles = [fname for fname in os.listdir(self.model_dir) if fname.endswith("_state.json")]
+        bkfiles = [fname for fname in os.listdir(self.model_dir) if fname.endswith(".bk")]
+        if not chkfiles:
+            logger.error("Could not find a model in the supplied folder: '%s'", self.model_dir)
+            exit(1)
+        if len(chkfiles) > 1:
+            logger.error("More than one model found in the supplied folder: '%s'", self.model_dir)
+            exit(1)
+        if not bkfiles:
+            logger.error("Could not find any backup files in the supplied folder: '%s'",
+                         self.model_dir)
+            exit(1)
+        self.model_name = chkfiles[0].replace("_state.json", "")
+        logger.info("%s Model found", self.model_name.title())
+        logger.verbose("Backup files: %s)", bkfiles)
