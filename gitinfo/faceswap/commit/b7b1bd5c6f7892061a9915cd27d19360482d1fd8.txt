commit b7b1bd5c6f7892061a9915cd27d19360482d1fd8
Author: kvrooman <vrooman.kyle@gmail.com>
Date:   Sat Aug 3 06:38:43 2019 -0500

    documentation, pep8, style, clarity updates  - Prep for Segmentation (#812)
    
    * documentation, pep8, style, clarity updates
    
    * Update cli.py
    
    * Update _config.py
    
    remove extra mask and coverage
    
    mask type as dropdown
    
    * Update training_data.py
    
    move coverage / LR to global
    cut down on loss description
    
    style change
    losses working in PR
    
    * simpler logging
    
    * legacy update

diff --git a/lib/cli.py b/lib/cli.py
index e45df87..8b66c07 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -867,12 +867,6 @@ class TrainArgs(FaceSwapArgs):
                               "required": True,
                               "help": "Input directory. A directory containing training images "
                                       "for face A."})
-        argument_list.append({"opts": ("-B", "--input-B"),
-                              "action": DirFullPaths,
-                              "dest": "input_b",
-                              "required": True,
-                              "help": "Input directory. A directory containing training images "
-                                      "for face B."})
         argument_list.append({"opts": ("-ala", "--alignments-A"),
                               "action": FileFullPaths,
                               "filetypes": 'alignments',
@@ -883,6 +877,23 @@ class TrainArgs(FaceSwapArgs):
                                       "if you are using a masked model or warp-to-landmarks is "
                                       "enabled. Defaults to <input-A>/alignments.json if not "
                                       "provided."})
+        argument_list.append({"opts": ("-tia", "--timelapse-input-A"),
+                              "action": DirFullPaths,
+                              "dest": "timelapse_input_a",
+                              "default": None,
+                              "help": "For if you want a timelapse: "
+                                      "The input folder for the timelapse. "
+                                      "This folder should contain faces of A "
+                                      "which will be converted for the "
+                                      "timelapse. You must supply a "
+                                      "--timelapse-output and a "
+                                      "--timelapse-input-B parameter."})
+        argument_list.append({"opts": ("-B", "--input-B"),
+                              "action": DirFullPaths,
+                              "dest": "input_b",
+                              "required": True,
+                              "help": "Input directory. A directory containing training images "
+                                      "for face B."})
         argument_list.append({"opts": ("-alb", "--alignments-B"),
                               "action": FileFullPaths,
                               "filetypes": 'alignments',
@@ -893,12 +904,17 @@ class TrainArgs(FaceSwapArgs):
                                       "if you are using a masked model or warp-to-landmarks is "
                                       "enabled. Defaults to <input-B>/alignments.json if not "
                                       "provided."})
-        argument_list.append({"opts": ("-m", "--model-dir"),
+        argument_list.append({"opts": ("-tib", "--timelapse-input-B"),
                               "action": DirFullPaths,
-                              "dest": "model_dir",
-                              "required": True,
-                              "help": "Model directory. This is where the training data will be "
-                                      "stored."})
+                              "dest": "timelapse_input_b",
+                              "default": None,
+                              "help": "For if you want a timelapse: "
+                                      "The input folder for the timelapse. "
+                                      "This folder should contain faces of B "
+                                      "which will be converted for the "
+                                      "timelapse. You must supply a "
+                                      "--timelapse-output and a "
+                                      "--timelapse-input-A parameter."})
         argument_list.append({"opts": ("-t", "--trainer"),
                               "action": Radio,
                               "type": str.lower,
@@ -925,6 +941,20 @@ class TrainArgs(FaceSwapArgs):
                                       "\nL|villain: 128px in/out model from villainguy. Very "
                                       "resource hungry (11GB for batchsize 16). Good for "
                                       "details, but more susceptible to color differences."})
+        argument_list.append({"opts": ("-to", "--timelapse-output"),
+                              "action": DirFullPaths,
+                              "dest": "timelapse_output",
+                              "default": None,
+                              "help": "The output folder for the timelapse. "
+                                      "If the input folders are supplied but "
+                                      "no output folder, it will default to "
+                                      "your model folder /timelapse/"})
+        argument_list.append({"opts": ("-m", "--model-dir"),
+                              "action": DirFullPaths,
+                              "dest": "model_dir",
+                              "required": True,
+                              "help": "Model directory. This is where the training data will be "
+                                      "stored."})
         argument_list.append({"opts": ("-s", "--save-interval"),
                               "type": int,
                               "action": Slider,
@@ -1049,36 +1079,6 @@ class TrainArgs(FaceSwapArgs):
                                       "to color differences between the A and B sets, at an "
                                       "increased training time cost. Enable this option to "
                                       "disable color augmentation."})
-        argument_list.append({"opts": ("-tia", "--timelapse-input-A"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_input_a",
-                              "default": None,
-                              "help": "For if you want a timelapse: "
-                                      "The input folder for the timelapse. "
-                                      "This folder should contain faces of A "
-                                      "which will be converted for the "
-                                      "timelapse. You must supply a "
-                                      "--timelapse-output and a "
-                                      "--timelapse-input-B parameter."})
-        argument_list.append({"opts": ("-tib", "--timelapse-input-B"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_input_b",
-                              "default": None,
-                              "help": "For if you want a timelapse: "
-                                      "The input folder for the timelapse. "
-                                      "This folder should contain faces of B "
-                                      "which will be converted for the "
-                                      "timelapse. You must supply a "
-                                      "--timelapse-output and a "
-                                      "--timelapse-input-A parameter."})
-        argument_list.append({"opts": ("-to", "--timelapse-output"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_output",
-                              "default": None,
-                              "help": "The output folder for the timelapse. "
-                                      "If the input folders are supplied but "
-                                      "no output folder, it will default to "
-                                      "your model folder /timelapse/"})
         return argument_list
 
 
diff --git a/lib/model/layers.py b/lib/model/layers.py
index 0bd91f7..4f6bd1e 100644
--- a/lib/model/layers.py
+++ b/lib/model/layers.py
@@ -16,12 +16,12 @@ from keras.engine import InputSpec, Layer
 from keras.utils import conv_utils
 from keras.utils.generic_utils import get_custom_objects
 from keras import initializers
-from keras.layers import ZeroPadding2D
+from keras.layers.pooling import _GlobalPooling2D
 
 if K.backend() == "plaidml.keras.backend":
     from lib.plaidml_utils import pad
 else:
-    from tensorflow import pad 
+    from tensorflow import pad
 
 class PixelShuffler(Layer):
     """ PixelShuffler layer for Keras
@@ -120,6 +120,7 @@ class Scale(Layer):
     """
     def __init__(self, weights=None, axis=-1, gamma_init='zero', **kwargs):
         self.axis = axis
+        self.gamma = None
         self.gamma_init = initializers.get(gamma_init)
         self.initial_weights = weights
         super(Scale, self).__init__(**kwargs)
@@ -270,6 +271,22 @@ class SubPixelUpscaling(Layer):
 
 
 class ReflectionPadding2D(Layer):
+    """Reflection-padding layer for 2D input (e.g. picture).
+    This layer can add rows and columns
+    at the top, bottom, left and right side of an image tensor.
+    Input shape:  ONLY WORKS ON CHANNELS LAST NOW
+      4D tensor with shape:
+      - If `data_format` is `"channels_last"`:
+          `(batch, rows, cols, channels)`
+      - If `data_format` is `"channels_first"`:
+          `(batch, channels, rows, cols)`
+    Output shape:
+      4D tensor with shape:
+      - If `data_format` is `"channels_last"`:
+          `(batch, padded_rows, padded_cols, channels)`
+      - If `data_format` is `"channels_first"`:
+          `(batch, channels, padded_rows, padded_cols)`
+    """
     def __init__(self, stride=2, kernel_size=5, **kwargs):
         '''
         # Arguments
@@ -288,13 +305,13 @@ class ReflectionPadding2D(Layer):
         """ If you are using "channels_last" configuration"""
         input_shape = self.input_spec[0].shape
         in_width, in_height = input_shape[2], input_shape[1]
-        kernel_width, kernel_height  = self.kernel_size, self.kernel_size
+        kernel_width, kernel_height = self.kernel_size, self.kernel_size
 
-        if (in_height % self.stride == 0):
+        if (in_height % self.stride) == 0:
             padding_height = max(kernel_height - self.stride, 0)
         else:
             padding_height = max(kernel_height - (in_height % self.stride), 0)
-        if (in_width % self.stride == 0):
+        if (in_width % self.stride) == 0:
             padding_width = max(kernel_width - self.stride, 0)
         else:
             padding_width = max(kernel_width- (in_width % self.stride), 0)
@@ -307,13 +324,13 @@ class ReflectionPadding2D(Layer):
     def call(self, x, mask=None):
         input_shape = self.input_spec[0].shape
         in_width, in_height = input_shape[2], input_shape[1]
-        kernel_width, kernel_height  = self.kernel_size, self.kernel_size
+        kernel_width, kernel_height = self.kernel_size, self.kernel_size
 
-        if (in_height % self.stride == 0):
+        if (in_height % self.stride) == 0:
             padding_height = max(kernel_height - self.stride, 0)
         else:
             padding_height = max(kernel_height - (in_height % self.stride), 0)
-        if (in_width % self.stride == 0):
+        if (in_width % self.stride) == 0:
             padding_width = max(kernel_width - self.stride, 0)
         else:
             padding_width = max(kernel_width- (in_width % self.stride), 0)
@@ -323,18 +340,84 @@ class ReflectionPadding2D(Layer):
         padding_left = padding_width // 2
         padding_right = padding_width - padding_left
 
-        return pad(x, [[0,0],
-                          [padding_top, padding_bot],
-                          [padding_left, padding_right],
-                          [0,0] ],
-                          'REFLECT')
+        return pad(x,
+                   [[0, 0],
+                    [padding_top, padding_bot],
+                    [padding_left, padding_right],
+                    [0, 0]],
+                   'REFLECT')
 
     def get_config(self):
         config = {'stride': self.stride,
                   'kernel_size': self.kernel_size}
         base_config = super(ReflectionPadding2D, self).get_config()
-        return dict(list(base_config.items()) + list(config.items())) 
+        return dict(list(base_config.items()) + list(config.items()))
+
 
+class GlobalMinPooling2D(_GlobalPooling2D):
+    """Global minimum pooling operation for spatial data.
+    # Arguments
+        data_format: A string,
+            one of `channels_last` (default) or `channels_first`.
+            The ordering of the dimensions in the inputs.
+            `channels_last` corresponds to inputs with shape
+            `(batch, height, width, channels)` while `channels_first`
+            corresponds to inputs with shape
+            `(batch, channels, height, width)`.
+            It defaults to the `image_data_format` value found in your
+            Keras config file at `~/.keras/keras.json`.
+            If you never set it, then it will be "channels_last".
+    # Input shape
+        - If `data_format='channels_last'`:
+            4D tensor with shape:
+            `(batch_size, rows, cols, channels)`
+        - If `data_format='channels_first'`:
+            4D tensor with shape:
+            `(batch_size, channels, rows, cols)`
+    # Output shape
+        2D tensor with shape:
+        `(batch_size, channels)`
+    """
+
+    def call(self, inputs):
+        if self.data_format == 'channels_last':
+            pooled = K.min(inputs, axis=[1, 2])
+        else:
+            pooled = K.min(inputs, axis=[2, 3])
+        return pooled
+
+
+class GlobalStdDevPooling2D(_GlobalPooling2D):
+    """Global standard deviation pooling operation for spatial data.
+    # Arguments
+        data_format: A string,
+            one of `channels_last` (default) or `channels_first`.
+            The ordering of the dimensions in the inputs.
+            `channels_last` corresponds to inputs with shape
+            `(batch, height, width, channels)` while `channels_first`
+            corresponds to inputs with shape
+            `(batch, channels, height, width)`.
+            It defaults to the `image_data_format` value found in your
+            Keras config file at `~/.keras/keras.json`.
+            If you never set it, then it will be "channels_last".
+    # Input shape
+        - If `data_format='channels_last'`:
+            4D tensor with shape:
+            `(batch_size, rows, cols, channels)`
+        - If `data_format='channels_first'`:
+            4D tensor with shape:
+            `(batch_size, channels, rows, cols)`
+    # Output shape
+        2D tensor with shape:
+        `(batch_size, channels)`
+    """
+
+    def call(self, inputs):
+        if self.data_format == 'channels_last':
+            pooled = K.std(inputs, axis=[1, 2])
+        else:
+            pooled = K.std(inputs, axis=[2, 3])
+        return pooled
 
 class L2_normalize(Layer):
     def __init__(self, axis, **kwargs):
diff --git a/lib/model/losses.py b/lib/model/losses.py
index 271f9af..712870d 100644
--- a/lib/model/losses.py
+++ b/lib/model/losses.py
@@ -11,7 +11,7 @@ import keras.backend as K
 from keras.layers import Lambda, concatenate
 import numpy as np
 import tensorflow as tf
-from tensorflow.contrib.distributions import Beta
+from tensorflow.distributions import Beta
 
 from .normalization import InstanceNormalization
 if K.backend() == "plaidml.keras.backend":
@@ -257,8 +257,7 @@ def reconstruction_loss(real, fake_abgr, mask_eyes, model_outputs, **weights):
     alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
     fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
 
-    loss_g = 0
-    loss_g += weights['w_recon'] * calc_loss(fake_bgr, real, "l1")
+    loss_g = weights['w_recon'] * calc_loss(fake_bgr, real, "l1")
     loss_g += weights['w_eyes'] * K.mean(K.abs(mask_eyes*(fake_bgr - real)))
 
     for out in model_outputs[:-1]:
@@ -273,9 +272,8 @@ def edge_loss(real, fake_abgr, mask_eyes, **weights):
     alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
     fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
 
-    loss_g = 0
-    loss_g += weights['w_edge'] * calc_loss(first_order(fake_bgr, axis=1),
-                                            first_order(real, axis=1), "l1")
+    loss_g = weights['w_edge'] * calc_loss(first_order(fake_bgr, axis=1),
+                                           first_order(real, axis=1), "l1")
     loss_g += weights['w_edge'] * calc_loss(first_order(fake_bgr, axis=2),
                                             first_order(real, axis=2), "l1")
     shape_mask_eyes = mask_eyes.get_shape().as_list()
@@ -290,14 +288,14 @@ def edge_loss(real, fake_abgr, mask_eyes, **weights):
     return loss_g
 
 
-def perceptual_loss(real, fake_abgr, distorted, mask_eyes, vggface_feats, **weights):
+def perceptual_loss(real, fake_abgr, distorted, vggface_feats, **weights):
     """ Perceptual Loss Function from Shoanlu GAN """
     alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
     fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
     fake = alpha * fake_bgr + (1-alpha) * distorted
 
     def preprocess_vggface(var_x):
-        var_x = (var_x + 1) / 2 * 255  # channel order: BGR
+        var_x = (var_x + 1.) / 2. * 255.  # channel order: BGR
         var_x -= [91.4953, 103.8827, 131.0912]
         return var_x
 
@@ -327,171 +325,145 @@ def perceptual_loss(real, fake_abgr, distorted, mask_eyes, vggface_feats, **weig
     loss_g += weights['w_pl'][3] * calc_loss(instnorm()(fake_feat112),
                                              instnorm()(real_feat112), "l2")
     return loss_g
-
 # <<< END: from Shoanlu GAN >>> #
 
 
-def generalized_loss_function(y_true, y_pred, var_a=1.0, cnst=1.0/255.0):
+def generalized_loss(y_true, y_pred, alpha=1.0, beta=1.0/255.0):
     """
     generalized function used to return a large variety of mathematical loss functions
     primary benefit is smooth, differentiable version of L1 loss
 
     Barron, J. A More General Robust Loss Function
     https://arxiv.org/pdf/1701.03077.pdf
-
     Parameters:
-        a: penalty factor. larger number give larger weight to large deviations
-        c: scale factor used to adjust to the input scale (i.e. inputs of mean 1e-4 or 256)
-
+        alpha: penalty factor. larger number give larger weight to large deviations
+        beta: scale factor used to adjust to the input scale (i.e. inputs of mean 1e-4 or 256 )
     Return:
         a loss value from the results of function(y_pred - y_true)
-
     Example:
         a=1.0, x>>c , c=1.0/255.0 will give a smoothly differentiable version of L1 / MAE loss
-        a=1.999999 (lim as a->2), c=1.0/255.0 will give L2 / RMSE loss
+        a=1.999999 (lim as a->2), beta=1.0/255.0 will give L2 / RMSE loss
     """
-    var_x = y_pred - y_true
-    loss = (K.abs(2.0-var_a)/var_a) * (K.pow(K.pow(var_x/cnst, 2.0)/K.abs(2.0-var_a) + 1.0,
-                                             (var_a/2.0)) - 1.0)
-    return K.mean(loss, axis=-1) * cnst
+    diff = y_pred - y_true
+    second = (K.pow(K.pow(diff/beta, 2.) / K.abs(2.-alpha) + 1., (alpha/2.)) - 1.)
+    loss = (K.abs(2.-alpha)/alpha) * second
+    loss = K.mean(loss, axis=-1) * beta
+    return loss
 
 
-def staircase_loss(y_true, y_pred, var_a=16.0, cnst=1.0/255.0):
-    """ Keras Staircase Loss """
-    height = cnst
-    width = cnst
-    var_x = K.clip(K.abs(y_true - y_pred) - 0.5 * cnst, 0.0, 1.0)
-    loss = height*(K.tanh(var_a*((var_x/width)-tf.floor(var_x/width)-0.5)) /
-                   (2.0*K.tanh(var_a/2.0)) + 0.5 + tf.floor(var_x/width))
-    loss += 1e-10
-    return K.mean(loss, axis=-1)
+def l_p_norm(y_true, y_pred, p_norm=np.inf):
+    """
+    Calculate the L-p norm as a loss function,
+    valid choics of p are [0,1,no.inf]
+    """
+    diff = y_true - y_pred
+    loss = tf.norm(diff, ord=p_norm, axis=-1)
+    return loss
+
+
+def l_inf_norm(y_true, y_pred):
+    """ Calculate the L-inf norm as a loss function """
+    diff = K.abs(y_true - y_pred)
+    max_loss = K.max(diff, axis=(1, 2), keepdims=True)
+    loss = K.mean(max_loss, axis=-1)
+    return loss
 
 
 def gradient_loss(y_true, y_pred):
     """
-    Calculates the first and second order gradient difference between pixels of an image in the
-    x and y dimensions. These gradients are then compared between the ground truth and the
-    predicted image and the difference is taken. The difference used is a smooth L1 norm
-    (approximate to MAE but differable at zero) When used as a loss, its minimization will result
-    in predicted images approaching the samelevel of sharpness / blurriness as the ground truth.
+    Calculates the first and second order gradient difference between pixels of
+    an image in the x and y dimensions. These gradients are then compared between
+    the ground truth and the predicted image and the difference is taken. When
+    used as a loss, its minimization will result in predicted images approaching
+    the same level of sharpness / blurriness as the ground truth.
 
-    TV+TV2 Regularization with Nonconvex Sparseness-Inducing Penalty for Image Restoration,
-    Chengwu Lu & Hua Huang, 2014 (http://downloads.hindawi.com/journals/mpe/2014/790547.pdf)
+    TV+TV2 Regularization with Nonconvex Sparseness-Inducing Penalty
+    for Image Restoration, Chengwu Lu & Hua Huang, 2014
+    (http://downloads.hindawi.com/journals/mpe/2014/790547.pdf)
 
     Parameters:
-        y_true: The predicted frames at each scale.
+        y_true: The predicted frames at each scale
         y_true: The ground truth frames at each scale
-
     Return:
-        The GD loss.
+        The GD loss
     """
 
-    assert K.ndim(y_true) == 4
-    y_true.set_shape([None, 80, 80, 3])
-    y_pred.set_shape([None, 80, 80, 3])
+    def diff_x(img):
+        x_left = img[:, :, 1:2, :] - img[:, :, 0:1, :]
+        x_inner = img[:, :, 2:, :] - img[:, :, :-2, :]
+        x_right = img[:, :, -1:, :] - img[:, :, -2:-1, :]
+        x_out = K.concatenate([x_left, x_inner, x_right], axis=2)
+        return x_out * 0.5
+
+    def diff_y(img):
+        y_top = img[:, 1:2, :, :] - img[:, 0:1, :, :]
+        y_inner = img[:, 2:, :, :] - img[:, :-2, :, :]
+        y_bot = img[:, -1:, :, :] - img[:, -2:-1, :, :]
+        y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)
+        return y_out * 0.5
+
+    def diff_xx(img):
+        x_left = img[:, :, 1:2, :] + img[:, :, 0:1, :]
+        x_inner = img[:, :, 2:, :] + img[:, :, :-2, :]
+        x_right = img[:, :, -1:, :] + img[:, :, -2:-1, :]
+        x_out = K.concatenate([x_left, x_inner, x_right], axis=2)
+        return x_out - 2.0 * img
+
+    def diff_yy(img):
+        y_top = img[:, 1:2, :, :] + img[:, 0:1, :, :]
+        y_inner = img[:, 2:, :, :] + img[:, :-2, :, :]
+        y_bot = img[:, -1:, :, :] + img[:, -2:-1, :, :]
+        y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)
+        return y_out - 2.0 * img
+
+    def diff_xy(img):
+        # xout1
+        top_left = img[:, 1:2, 1:2, :] + img[:, 0:1, 0:1, :]
+        inner_left = img[:, 2:, 1:2, :] + img[:, :-2, 0:1, :]
+        bot_left = img[:, -1:, 1:2, :] + img[:, -2:-1, 0:1, :]
+        xy_left = K.concatenate([top_left, inner_left, bot_left], axis=1)
+
+        top_mid = img[:, 1:2, 2:, :] + img[:, 0:1, :-2, :]
+        mid_mid = img[:, 2:, 2:, :] + img[:, :-2, :-2, :]
+        bot_mid = img[:, -1:, 2:, :] + img[:, -2:-1, :-2, :]
+        xy_mid = K.concatenate([top_mid, mid_mid, bot_mid], axis=1)
+
+        top_right = img[:, 1:2, -1:, :] + img[:, 0:1, -2:-1, :]
+        inner_right = img[:, 2:, -1:, :] + img[:, :-2, -2:-1, :]
+        bot_right = img[:, -1:, -1:, :] + img[:, -2:-1, -2:-1, :]
+        xy_right = K.concatenate([top_right, inner_right, bot_right], axis=1)
+
+        # Xout2
+        top_left = img[:, 0:1, 1:2, :] + img[:, 1:2, 0:1, :]
+        inner_left = img[:, :-2, 1:2, :] + img[:, 2:, 0:1, :]
+        bot_left = img[:, -2:-1, 1:2, :] + img[:, -1:, 0:1, :]
+        xy_left = K.concatenate([top_left, inner_left, bot_left], axis=1)
+
+        top_mid = img[:, 0:1, 2:, :] + img[:, 1:2, :-2, :]
+        mid_mid = img[:, :-2, 2:, :] + img[:, 2:, :-2, :]
+        bot_mid = img[:, -2:-1, 2:, :] + img[:, -1:, :-2, :]
+        xy_mid = K.concatenate([top_mid, mid_mid, bot_mid], axis=1)
+
+        top_right = img[:, 0:1, -1:, :] + img[:, 1:2, -2:-1, :]
+        inner_right = img[:, :-2, -1:, :] + img[:, 2:, -2:-1, :]
+        bot_right = img[:, -2:-1, -1:, :] + img[:, -1:, -2:-1, :]
+        xy_right = K.concatenate([top_right, inner_right, bot_right], axis=1)
+
+        xy_out1 = K.concatenate([xy_left, xy_mid, xy_right], axis=2)
+        xy_out2 = K.concatenate([xy_left, xy_mid, xy_right], axis=2)
+        return (xy_out1 - xy_out2) * 0.25
+
     tv_weight = 1.0
     tv2_weight = 1.0
     loss = 0.0
-
-    def diff_x(var_x):
-        xleft = var_x[:, :, 1, :] - var_x[:, :, 0, :]
-        xinner = tf.unstack(var_x[:, :, 2:, :] - var_x[:, :, :-2, :], axis=2)
-        xright = var_x[:, :, -1, :] - var_x[:, :, -2, :]
-        xout = [xleft] + xinner + [xright]
-        xout = tf.stack(xout, axis=2)
-        return xout * 0.5
-
-    def diff_y(var_x):
-        xtop = var_x[:, 1, :, :] - var_x[:, 0, :, :]
-        xinner = tf.unstack(var_x[:, 2:, :, :] - var_x[:, :-2, :, :], axis=1)
-        xbot = var_x[:, -1, :, :] - var_x[:, -2, :, :]
-        xout = [xtop] + xinner + [xbot]
-        xout = tf.stack(xout, axis=1)
-        return xout * 0.5
-
-    def diff_xx(var_x):
-        xleft = var_x[:, :, 1, :] + var_x[:, :, 0, :]
-        xinner = tf.unstack(var_x[:, :, 2:, :] + var_x[:, :, :-2, :], axis=2)
-        xright = var_x[:, :, -1, :] + var_x[:, :, -2, :]
-        xout = [xleft] + xinner + [xright]
-        xout = tf.stack(xout, axis=2)
-        return xout - 2.0 * var_x
-
-    def diff_yy(var_x):
-        xtop = var_x[:, 1, :, :] + var_x[:, 0, :, :]
-        xinner = tf.unstack(var_x[:, 2:, :, :] + var_x[:, :-2, :, :], axis=1)
-        xbot = var_x[:, -1, :, :] + var_x[:, -2, :, :]
-        xout = [xtop] + xinner + [xbot]
-        xout = tf.stack(xout, axis=1)
-        return xout - 2.0 * var_x
-
-    def diff_xy(var_x):
-        # xout1
-        top_left = var_x[:, 1, 1, :]+var_x[:, 0, 0, :]
-        inner_left = tf.unstack(var_x[:, 2:, 1, :]+var_x[:, :-2, 0, :], axis=1)
-        bot_left = var_x[:, -1, 1, :]+var_x[:, -2, 0, :]
-        x_left = [top_left] + inner_left + [bot_left]
-        x_left = tf.stack(x_left, axis=1)
-
-        top_mid = var_x[:, 1, 2:, :]+var_x[:, 0, :-2, :]
-        mid_mid = tf.unstack(var_x[:, 2:, 2:, :]+var_x[:, :-2, :-2, :], axis=1)
-        bot_mid = var_x[:, -1, 2:, :]+var_x[:, -2, :-2, :]
-        x_mid = [top_mid] + mid_mid + [bot_mid]
-        x_mid = tf.stack(x_mid, axis=1)
-
-        top_right = var_x[:, 1, -1, :]+var_x[:, 0, -2, :]
-        inner_right = tf.unstack(var_x[:, 2:, -1, :]+var_x[:, :-2, -2, :], axis=1)
-        bot_right = var_x[:, -1, -1, :]+var_x[:, -2, -2, :]
-        x_right = [top_right] + inner_right + [bot_right]
-        x_right = tf.stack(x_right, axis=1)
-
-        x_mid = tf.unstack(x_mid, axis=2)
-        xout1 = [x_left] + x_mid + [x_right]
-        xout1 = tf.stack(xout1, axis=2)
-
-        # xout2
-        top_left = var_x[:, 0, 1, :]+var_x[:, 1, 0, :]
-        inner_left = tf.unstack(var_x[:, :-2, 1, :]+var_x[:, 2:, 0, :], axis=1)
-        bot_left = var_x[:, -2, 1, :]+var_x[:, -1, 0, :]
-        x_left = [top_left] + inner_left + [bot_left]
-        x_left = tf.stack(x_left, axis=1)
-
-        top_mid = var_x[:, 0, 2:, :]+var_x[:, 1, :-2, :]
-        mid_mid = tf.unstack(var_x[:, :-2, 2:, :]+var_x[:, 2:, :-2, :], axis=1)
-        bot_mid = var_x[:, -2, 2:, :]+var_x[:, -1, :-2, :]
-        x_mid = [top_mid] + mid_mid + [bot_mid]
-        x_mid = tf.stack(x_mid, axis=1)
-
-        top_right = var_x[:, 0, -1, :]+var_x[:, 1, -2, :]
-        inner_right = tf.unstack(var_x[:, :-2, -1, :]+var_x[:, 2:, -2, :], axis=1)
-        bot_right = var_x[:, -2, -1, :]+var_x[:, -1, -2, :]
-        x_right = [top_right] + inner_right + [bot_right]
-        x_right = tf.stack(x_right, axis=1)
-
-        x_mid = tf.unstack(x_mid, axis=2)
-        xout2 = [x_left] + x_mid + [x_right]
-        xout2 = tf.stack(xout2, axis=2)
-
-        return (xout1 - xout2) * 0.25
-
-    loss += tv_weight * (generalized_loss_function(diff_x(y_true),
-                                                   diff_x(y_pred),
-                                                   var_a=1.999999) +
-                         generalized_loss_function(diff_y(y_true),
-                                                   diff_y(y_pred),
-                                                   var_a=1.999999))
-
-    loss += tv2_weight * (generalized_loss_function(diff_xx(y_true),
-                                                    diff_xx(y_pred),
-                                                    var_a=1.999999) +
-                          generalized_loss_function(diff_yy(y_true),
-                                                    diff_yy(y_pred),
-                                                    var_a=1.999999) +
-                          2.0 * generalized_loss_function(diff_xy(y_true),
-                                                          diff_xy(y_pred),
-                                                          var_a=1.999999))
-
-    return loss / (tv_weight + tv2_weight)
+    loss += tv_weight * (generalized_loss(diff_x(y_true), diff_x(y_pred), alpha=1.9999) +
+                         generalized_loss(diff_y(y_true), diff_y(y_pred), alpha=1.9999))
+    loss += tv2_weight * (generalized_loss(diff_xx(y_true), diff_xx(y_pred), alpha=1.9999) +
+                          generalized_loss(diff_yy(y_true), diff_yy(y_pred), alpha=1.9999) +
+                          generalized_loss(diff_xy(y_true), diff_xy(y_pred), alpha=1.9999) * 2.)
+    loss = loss / (tv_weight + tv2_weight)
+    # TODO simplify to use MSE instead
+    return loss
 
 
 def scharr_edges(image, magnitude):
@@ -509,46 +481,52 @@ def scharr_edges(image, magnitude):
 
     # Define vertical and horizontal Scharr filters.
     static_image_shape = image.get_shape()
-    image_shape = tf.shape(image)
-
-    # modified 3x3 Scharr
-    # kernels = [[[-17.0, -61.0, -17.0], [0.0, 0.0, 0.0], [17.0, 61.0, 17.0]],
-    #         [[-17.0, 0.0, 17.0], [-61.0, 0.0, 61.0], [-17.0, 0.0, 17.0]]]
-
-    # 5x5 Scharr
-    kernels = [[[-1.0, -2.0, -3.0, -2.0, -1.0],
-                [-1.0, -2.0, -6.0, -2.0, -1.0],
-                [0.0, 0.0, 0.0, 0.0, 0.0],
-                [1.0, 2.0, 6.0, 2.0, 1.0],
-                [1.0, 2.0, 3.0, 2.0, 1.0]],
-               [[-1.0, -1.0, 0.0, 1.0, 1.0],
-                [-2.0, -2.0, 0.0, 2.0, 2.0],
-                [-3.0, -6.0, 0.0, 6.0, 3.0],
-                [-2.0, -2.0, 0.0, 2.0, 2.0],
-                [-1.0, -1.0, 0.0, 1.0, 1.0]]]
-    num_kernels = len(kernels)
-    kernels = np.transpose(np.asarray(kernels), (1, 2, 0))
-    kernels = np.expand_dims(kernels, -2) / np.sum(np.abs(kernels))
-    kernels_tf = tf.constant(kernels, dtype=image.dtype)
-    kernels_tf = tf.tile(kernels_tf, [1, 1, image_shape[-1], 1], name='scharr_filters')
+    image_shape = K.shape(image)
+
+    # 5x5 modified Scharr kernel ( reshape to (5,5,1,2) )
+    matrix = [[[[0.00070, 0.00070]],
+               [[0.00520, 0.00370]],
+               [[0.03700, 0.00000]],
+               [[0.00520, -0.0037]],
+               [[0.00070, -0.0007]]],
+              [[[0.00370, 0.00520]],
+               [[0.11870, 0.11870]],
+               [[0.25890, 0.00000]],
+               [[0.11870, -0.1187]],
+               [[0.00370, -0.0052]]],
+              [[[0.00000, 0.03700]],
+               [[0.00000, 0.25890]],
+               [[0.00000, 0.00000]],
+               [[0.00000, -0.2589]],
+               [[0.00000, -0.0370]]],
+              [[[-0.0037, 0.00520]],
+               [[-0.1187, 0.11870]],
+               [[-0.2589, 0.00000]],
+               [[-0.1187, -0.1187]],
+               [[-0.0037, -0.0052]]],
+              [[[-0.0007, 0.00070]],
+               [[-0.0052, 0.00370]],
+               [[-0.0370, 0.00000]],
+               [[-0.0052, -0.0037]],
+               [[-0.0007, -0.0007]]]]
+    num_kernels = [2]
+    kernels = K.constant(matrix, dtype='float32')
+    kernels = K.tile(kernels, [1, 1, image_shape[-1], 1])
+
 
     # Use depth-wise convolution to calculate edge maps per channel.
+    # Output tensor has shape [batch_size, h, w, d * num_kernels].
     pad_sizes = [[0, 0], [2, 2], [2, 2], [0, 0]]
     padded = tf.pad(image, pad_sizes, mode='REFLECT')
+    output = K.depthwise_conv2d(padded, kernels)
 
-    # Output tensor has shape [batch_size, h, w, d * num_kernels].
-    strides = [1, 1, 1, 1]
-    output = tf.nn.depthwise_conv2d(padded, kernels_tf, strides, 'VALID')
-
-    # Reshape to [batch_size, h, w, d, num_kernels].
-    shape = tf.concat([image_shape, [num_kernels]], 0)
-    output = tf.reshape(output, shape=shape)
-    output.set_shape(static_image_shape.concatenate([num_kernels]))
-
-    if magnitude:  # magnitude of edges
-        output = tf.sqrt(tf.reduce_sum(tf.square(output), axis=-1))
-    else:  # direction of edges
-        output = tf.atan(tf.squeeze(tf.div(output[:, :, :, :, 0]/output[:, :, :, :, 1])))
+    if not magnitude:  # direction of edges
+        # Reshape to [batch_size, h, w, d, num_kernels].
+        shape = K.concatenate([image_shape, num_kernels], axis=0)
+        output = K.reshape(output, shape=shape)
+        output.set_shape(static_image_shape.concatenate(num_kernels))
+        output = tf.atan(K.squeeze(output[:, :, :, :, 0] / output[:, :, :, :, 1]))
+    # magnitude of edges -- unified x & y edges don't work well with NN
 
     return output
 
@@ -559,20 +537,19 @@ def gmsd_loss(y_true, y_pred):
     http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm
     https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
     """
-    true_edge_mag = scharr_edges(y_true, True)
-    pred_edge_mag = scharr_edges(y_pred, True)
-    cnst = 0.002
-    upper = 2.0 * tf.multiply(true_edge_mag, pred_edge_mag) + cnst
-    lower = tf.square(true_edge_mag) + tf.square(pred_edge_mag) + cnst
-    gms = tf.div(upper, lower)
-    _mean, _var = tf.nn.moments(gms, axes=[1, 2], keep_dims=True)
-    # single metric value per image in tensor [?, 1, 1]
-    gmsd = tf.reduce_mean(tf.sqrt(_var), axis=-1)
-    # need to expand to [?, height, width] dimensions for Keras ... modify to not be hard-coded
-    return K.tile(gmsd, [1, 64, 64])
-
-
-def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.2726)):
+
+    true_edge = scharr_edges(y_true, True)
+    pred_edge = scharr_edges(y_pred, True)
+    ephsilon = 0.0025
+    upper = 2.0 * true_edge * pred_edge
+    lower = K.square(true_edge) + K.square(pred_edge)
+    gms = (upper + ephsilon) / (lower + ephsilon)
+    gmsd = K.std(gms, axis=(1, 2, 3), keepdims=True)
+    gmsd = K.squeeze(gmsd, axis=-1)
+    return gmsd
+
+
+def ms_ssim_calc(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.2726)):
     """
     Computes the MS-SSIM between img1 and img2.
     This function assumes that `img1` and `img2` are image batches, i.e. the last
@@ -655,24 +632,21 @@ def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.27
         """
 
         def _fspecial_gauss(size, sigma):
-            """
-            Function to mimic the 'fspecial' gaussian MATLAB function.
-            """
+            """ Function to mimic the 'fspecial' gaussian MATLAB function. """
+
             size = tf.convert_to_tensor(size, 'int32')
             sigma = tf.convert_to_tensor(sigma)
-
             coords = tf.cast(tf.range(size), sigma.dtype)
             coords -= tf.cast(size - 1, sigma.dtype) / 2.0
 
-            var_g = tf.square(coords)
-            var_g *= -0.5 / tf.square(sigma)
-
-            var_g = tf.reshape(var_g, shape=[1, -1]) + tf.reshape(var_g, shape=[-1, 1])
-            var_g = tf.reshape(var_g, shape=[1, -1])  # For tf.nn.softmax().
-            var_g = tf.nn.softmax(var_g)
-            return tf.reshape(var_g, shape=[size, size, 1, 1])
+            gauss = tf.square(coords)
+            gauss *= -0.5 / tf.square(sigma)
+            gauss = tf.reshape(gauss, shape=[1, -1]) + tf.reshape(gauss, shape=[-1, 1])
+            gauss = tf.reshape(gauss, shape=[1, -1])  # For tf.nn.softmax().
+            gauss = tf.nn.softmax(gauss)
+            return tf.reshape(gauss, shape=[size, size, 1, 1])
 
-        def _ssim_helper(var_x, var_y, max_val, kernel, compensation=1.0):
+        def _ssim_helper(img1, img2, max_val, kernel, compensation=1.):
             """
             Helper function for computing SSIM.
             SSIM estimates covariances with weighted sums.  The default parameters
@@ -687,10 +661,10 @@ def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.27
             For SSIM measure with unbiased covariance estimators, pass as `compensation`
             argument (1 - sum_i w_i ^ 2).
             Arguments:
-            x: First set of images.
-            y: Second set of images.
+            img1: First set of images.
+            img2: Second set of images.
             reducer: Function that computes 'local' averages from set of images.
-              For non-covolutional version, this is usually tf.reduce_mean(x, [1, 2]),
+              For non-covolutional version, this is usually tf.reduce_mean(img1, [1, 2]),
               and for convolutional version, this is usually tf.nn.avg_pool or
               tf.nn.conv2d with weighted-sum kernel.
             max_val: The dynamic range (i.e., the difference between the maximum
@@ -700,35 +674,31 @@ def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.27
             A pair containing the luminance measure, and the contrast-structure measure.
             """
 
-            def reducer(var_x, kernel):
-                shape = tf.shape(var_x)
-                var_x = tf.reshape(var_x, shape=tf.concat([[-1], shape[-3:]], 0))
-                var_y = tf.nn.depthwise_conv2d(var_x, kernel, strides=[1, 1, 1, 1], padding='VALID')
-                return tf.reshape(var_y, tf.concat([shape[:-3], tf.shape(var_y)[1:]], 0))
+            def reducer(img1, kernel):
+                shape = tf.shape(img1)
+                img1 = tf.reshape(img1, shape=tf.concat([[-1], shape[-3:]], 0))
+                img2 = tf.nn.depthwise_conv2d(img1, kernel, strides=[1, 1, 1, 1], padding='VALID')
+                return tf.reshape(img2, tf.concat([shape[:-3], tf.shape(img2)[1:]], 0))
 
-            _ssim_k1 = 0.01
-            _ssim_k2 = 0.03
-
-            c_1 = (_ssim_k1 * max_val) ** 2
-            c_2 = (_ssim_k2 * max_val) ** 2
+            c_one = (0.01 * max_val) ** 2
+            c_two = ((0.03 * max_val)) ** 2 * compensation
 
             # SSIM luminance measure is
-            # (2 * mu_x * mu_y + c_1) / (mu_x ** 2 + mu_y ** 2 + c_1).
-            mean0 = reducer(var_x, kernel)
-            mean1 = reducer(var_y, kernel)
-            num0 = mean0 * mean1 * 2.0
+            # (2 * mu_x * mu_y + c_one) / (mu_x ** 2 + mu_y ** 2 + c_one).
+            mean0 = reducer(img1, kernel)
+            mean1 = reducer(img2, kernel)
+            num0 = mean0 * mean1 * 2.
             den0 = tf.square(mean0) + tf.square(mean1)
-            luminance = (num0 + c_1) / (den0 + c_1)
+            luminance = (num0 + c_one) / (den0 + c_one)
 
             # SSIM contrast-structure measure is
-            #   (2 * cov_{xy} + c_2) / (cov_{xx} + cov_{yy} + c_2).
+            #   (2 * cov_{xy} + c_two) / (cov_{xx} + cov_{yy} + c_two).
             # Note that `reducer` is a weighted sum with weight w_k, \sum_i w_i = 1, then
             #   cov_{xy} = \sum_i w_i (x_i - \mu_x) (y_i - \mu_y)
             #          = \sum_i w_i x_i y_i - (\sum_i w_i x_i) (\sum_j w_j y_j).
-            num1 = reducer(var_x * var_y, kernel) * 2.0
-            den1 = reducer(tf.square(var_x) + tf.square(var_y), kernel)
-            c_2 *= compensation
-            c_s = (num1 - num0 + c_2) / (den1 - den0 + c_2)
+            num1 = reducer(img1 * img2, kernel) * 2.0
+            den1 = reducer(tf.square(img1) + tf.square(img2), kernel)
+            c_s = (num1 - num0 + c_two) / (den1 - den0 + c_two)
 
             # SSIM score is the product of the luminance and contrast-structure measures.
             return luminance, c_s
@@ -752,7 +722,7 @@ def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.27
 
         # The correct compensation factor is `1.0 - tf.reduce_sum(tf.square(kernel))`,
         # but to match MATLAB implementation of MS-SSIM, we use 1.0 instead.
-        compensation = 1.0
+        compensation = 1.
 
         # TODO(sjhwang): Try FFT.
         # TODO(sjhwang): Gaussian kernel is separable in space. Consider applying
@@ -805,37 +775,38 @@ def ms_ssim(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.27
         with tf.name_scope(None, 'Scale%d' % k, imgs):
             if k > 0:
                 # Avg pool takes rank 4 tensors. Flatten leading dimensions.
-                flat_imgs = [tf.reshape(x, tf.concat([[-1], t], 0))
-                             for x, t in zip(imgs, tails)]
-
+                zipped = zip(imgs, tails)
+                flat_imgs = [tf.reshape(x, tf.concat([[-1], t], 0)) for x, t in zipped]
                 remainder = tails[0] % divisor_tensor
                 need_padding = tf.reduce_any(tf.not_equal(remainder, 0))
                 padded = tf.cond(need_padding,
-                                 lambda: do_pad(flat_imgs, remainder),
-                                 lambda: flat_imgs)
+                                 lambda: do_pad(flat_imgs, remainder), lambda: flat_imgs)
 
-                downscaled = [tf.nn.avg_pool(x, ksize=divisor, strides=divisor, padding='VALID')
-                              for x in padded]
+                downscaled = [tf.nn.avg_pool(x,
+                                             ksize=divisor,
+                                             strides=divisor,
+                                             padding='VALID') for x in padded]
                 tails = [x[1:] for x in tf.shape_n(downscaled)]
-                imgs = [tf.reshape(x, tf.concat([h, t], 0))
-                        for x, h, t in zip(downscaled, heads, tails)]
+                zipper = zip(downscaled, heads, tails)
+                imgs = [tf.reshape(x, tf.concat([h, t], 0)) for x, h, t in zipper]
 
-                # Overwrite previous ssim value since we only need the last one.
-                ssim_per_channel, c_s = _ssim_per_channel(*imgs, max_val=max_val)
-                mc_s.append(tf.nn.relu(c_s))
+            # Overwrite previous ssim value since we only need the last one.
+            ssim_per_channel, c_s = _ssim_per_channel(*imgs, max_val=max_val)
+            mc_s.append(tf.nn.relu(c_s))
 
     # Remove the c_s score for the last scale. In the MS-SSIM calculation,
     # we use the l(p) at the highest scale. l(p) * c_s(p) is ssim(p).
     mc_s.pop()  # Remove the c_s score for the last scale.
-    mc_s_and_ssim = tf.stack(mc_s + [tf.nn.relu(ssim_per_channel)], axis=-1)
+    mcs_and_ssim = tf.stack(mc_s + [tf.nn.relu(ssim_per_channel)], axis=-1)
     # Take weighted geometric mean across the scale axis.
-    ms_ssim = tf.reduce_prod(tf.pow(mc_s_and_ssim, power_factors), [-1])
+    ms_ssim = tf.reduce_prod(tf.pow(mcs_and_ssim, power_factors), [-1])
 
     return tf.reduce_mean(ms_ssim, [-1])  # Avg over color channels.
 
 
 def ms_ssim_loss(y_true, y_pred):
-    """ Keras ms_ssim loss """
-    msssim = K.expand_dims(K.expand_dims(1.0 - ms_ssim(y_true, y_pred), axis=-1), axis=-1)
-    # need to expand to [1, height, width] dimensions for Keras ... modify to not be hard-coded
-    return K.tile(msssim, [1, 64, 64])
+    """ Keras loss function for MS-SSIM """
+    expanded = K.expand_dims(1.0 - ms_ssim_calc(y_true, y_pred), axis=-1)
+    loss = K.expand_dims(expanded, axis=-1)
+    # need to expand to [1,height,width] dimensions for Keras. modify to not be hard-coded
+    return K.tile(loss, [1, 64, 64])
diff --git a/lib/multithreading.py b/lib/multithreading.py
index 23be0ad..ac1a445 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -100,8 +100,9 @@ class FixedProducerDispatcher():
 
         # Consumer side
         batch_size = 64
-        dispatcher = FixedProducerDispatcher(do_work, shapes=[
-            (batch_size, 256,256,3), (batch_size, 256,256,3)])
+        height = width = 256
+        batch_shapes = (batch_size, height, width, 3)
+        dispatcher = FixedProducerDispatcher(do_work, shapes=[batch_shapes, batch_shapes])
         for batch_wrapper in dispatcher:
             # alternative batch_wrapper.get and batch_wrapper.free can be used
             with batch_wrapper as batch:
diff --git a/lib/training_data.py b/lib/training_data.py
index 914b0dd..947d4c4 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -226,8 +226,7 @@ class TrainingDataGenerator():
         if not closest_hashes:
             dst_points_items = list(landmarks.items())
             dst_points = list(x[1] for x in dst_points_items)
-            closest = (np.mean(np.square(src_points - dst_points),
-                               axis=(1, 2))).argsort()[:10]
+            closest = (np.mean(np.square(src_points - dst_points), axis=(1, 2))).argsort()[:10]
             closest_hashes = tuple(dst_points_items[i][0] for i in closest)
             self._nearest_landmarks[filename] = closest_hashes
         dst_points = landmarks[choice(closest_hashes)]
@@ -367,7 +366,7 @@ class ImageManipulation():
         """ get pair of random warped images from aligned face image """
         logger.trace("Randomly warping image")
         height, width = image.shape[0:2]
-        coverage = self.get_coverage(image)
+        coverage = self.get_coverage(image) // 2
         try:
             assert height == width and height % 2 == 0
         except AssertionError as err:
@@ -378,9 +377,7 @@ class ImageManipulation():
                    "from the Extract process.".format(width, height))
             raise FaceswapError(msg) from err
 
-        range_ = np.linspace(height // 2 - coverage // 2,
-                             height // 2 + coverage // 2,
-                             5, dtype='float32')
+        range_ = np.linspace(height // 2 - coverage, height // 2 + coverage, 5, dtype='float32')
         mapx = np.broadcast_to(range_, (5, 5)).copy()
         mapy = mapx.T
         # mapx, mapy = np.float32(np.meshgrid(range_,range_)) # instead of broadcast
@@ -416,7 +413,7 @@ class ImageManipulation():
             From DFAKER plugin """
         logger.trace("Randomly warping landmarks")
         size = image.shape[0]
-        coverage = self.get_coverage(image)
+        coverage = self.get_coverage(image) // 2
 
         p_mx = size - 1
         p_hf = (size // 2) - 1
@@ -464,7 +461,7 @@ class ImageManipulation():
         target_image = image
 
         # TODO Make sure this replacement is correct
-        slices = slice(size // 2 - coverage // 2, size // 2 + coverage // 2)
+        slices = slice(size // 2 - coverage, size // 2 + coverage)
 #        slices = slice(size // 32, size - size // 32)  # 8px on a 256px image
         warped_image = cv2.resize(  # pylint:disable=no-member
             warped_image[slices, slices, :], (self.input_size, self.input_size),
diff --git a/plugins/convert/mask/_base.py b/plugins/convert/mask/_base.py
index 190d53c..c4c0d35 100644
--- a/plugins/convert/mask/_base.py
+++ b/plugins/convert/mask/_base.py
@@ -99,7 +99,7 @@ class BlurMask():
             ksize = int(kwargs["ksize"][0])
             logger.trace("Pass: %s, kernel_size: %s", i + 1, (ksize, ksize))
             blurred = func(blurred, **kwargs)
-            ksize = int(ksize * self.multipass_factor)
+            ksize = round(ksize * self.multipass_factor)
             kwargs["ksize"] = self.get_kernel_tuple(ksize)
         logger.trace("Returning blurred mask. Shape: %s", blurred.shape)
         return blurred
@@ -109,8 +109,7 @@ class BlurMask():
         """ Multipass Factor
             For multiple passes the kernel must be scaled down. This value is
             different for box filter and gaussian """
-        factor = dict(gaussian=0.8,
-                      normalized=0.5)
+        factor = dict(gaussian=0.8, normalized=0.5)
         return factor[self.blur_type]
 
     @property
@@ -140,8 +139,8 @@ class BlurMask():
     def get_kernel_size(self, radius_ratio):
         """ Set the kernel size to absolute """
         mask_diameter = np.sqrt(np.sum(self.mask))
-        radius = max(1, round(mask_diameter * radius_ratio / 100))
-        kernel_size = int((radius * 2) + 1)
+        radius = round(max(1., mask_diameter * radius_ratio / 100.))
+        kernel_size = radius * 2 + 1
         logger.trace("kernel_size: %s", kernel_size)
         return kernel_size
 
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index 47c9511..8d16d00 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -36,13 +36,94 @@ class Config(FaceswapConfig):
                 self.load_module(filename, import_path, plugin_type)
 
     def set_globals(self):
-        """ Set the global options for training """
+        """
+        Set the global options for training
+
+        Loss Documentation
+        MAE https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine
+            -learners-should-know-4fb140e9d4b0
+        MSE https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine
+            -learners-should-know-4fb140e9d4b0
+        LogCosh https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine
+                -learners-should-know-4fb140e9d4b0
+        Smooth L1 https://arxiv.org/pdf/1701.03077.pdf
+        L_inf_norm https://medium.com/@montjoile/l0-norm-l1-norm-l2-norm-l-infinity
+                   -norm-7a7d18a4f40c
+        SSIM http://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf
+        GMSD https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
+        """
         logger.debug("Setting global config")
         section = "global"
         self.add_section(title=section,
                          info="Options that apply to all models" + ADDITIONAL_INFO)
+        self.add_item(
+            section=section, title="icnr_init", datatype=bool, default=False,
+            info="Use ICNR to tile the default initializer in a repeating pattern. "
+                 "This strategy is designed for pairing with sub-pixel / pixel shuffler "
+                 "to reduce the 'checkerboard effect' in image reconstruction. "
+                 "\n\t https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf")
+        self.add_item(
+            section=section, title="conv_aware_init", datatype=bool, default=False,
+            info="Use Convolution Aware Initialization for convolutional layers. "
+                 "This can help eradicate the vanishing and exploding gradient problem "
+                 "as well as lead to higher accuracy, lower loss and faster convergence. "
+                 "NB This can use more VRAM when creating a new model so you may want to "
+                 "lower the batch size for the first run. The batch size can be raised "
+                 "again when reloading the model. "
+                 "\n\t NB: Building the model will likely take several minutes as the "
+                 "calculations for this initialization technique are expensive.")
+        self.add_item(
+            section=section, title="subpixel_upscaling", datatype=bool, default=False,
+            info="Use subpixel upscaling rather than pixel shuffler. These techniques "
+                 "are both designed to produce better resolving upscaling than other "
+                 "methods. Each perform the same operations, but using different TF opts."
+                 "\n\t https://arxiv.org/pdf/1609.05158.pdf")
+        self.add_item(
+            section=section, title="reflect_padding", datatype=bool, default=False,
+            info="Use reflection padding rather than zero padding with convolutions. "
+                 "Each convolution must pad the image boundaries to maintain the proper "
+                 "sizing. More complex padding schemes can reduce artifacts at the "
+                 "border of the image."
+                 "\n\t http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt")
+        self.add_item(
+            section=section, title="penalized_mask_loss", datatype=bool, default=True,
+            info="Image loss function is weighted by mask presence. For areas of "
+                 "the image without the facial mask, reconstuction errors will be "
+                 "ignored while the masked face area is prioritized. May increase "
+                 "overall quality by focusing attention on the core face area.")
+        self.add_item(
+            section=section, title="loss_function", datatype=str,
+            default="mae",
+            choices=["mae", "mse", "logcosh", "smooth_l1", "l_inf_norm", "ssim", "gmsd",
+                     "pixel_gradient_diff"],
+            info="\n\t MAE - Mean absolute error will guide reconstructions of each pixel "
+                 "towards its median value in the training dataset. Robust to outliers but as "
+                 "a median, it can potentially ignore some infrequent image types in the dataset."
+                 "\n\t MSE - Mean squared error will guide reconstructions of each pixel "
+                 "towards its average value in the training dataset. As an avg, it will be "
+                 "suspectible to outliers and typically produces slightly blurrier results."
+                 "\n\t LogCosh - log(cosh(x)) acts similiar to MSE for small errors and to "
+                 "MAE for large errors. Like MSE, it is very stable and prevents overshoots "
+                 "when errors are near zero. Like MAE, it is robust to outliers."
+                 "\n\t Smooth_L1 --- Modification of the MAE loss to correct two of its "
+                 "disadvantages. This loss has improved stability and guidance for small errors."
+                 "\n\t L_inf_norm --- The L_inf norm will reduce the largest individual pixel "
+                 "error in an image. As each largest error is minimized sequentially, the "
+                 "overall error is improved. This loss will be extremely focused on outliers."
+                 "\n\t SSIM - Structural Similarity Index Metric is a perception-based "
+                 "loss that considers changes in texture, luminance, contrast, and local spatial "
+                 "statistics of an image. Potentially delivers more realistic looking images."
+                 "\n\t GMSD - Gradient Magnitude Similarity Deviation seeks to match "
+                 "the global standard deviation of the pixel to pixel differences between two "
+                 "images. Similiar in approach to SSIM."
+                 "\n\t Pixel_Gradient_Difference - Instead of minimizing the difference between "
+                 "the absolute value of each pixel in two reference images, compute the pixel to "
+                 "pixel spatial difference in each image and then minimize that difference "
+                 "between two images. Allows for large color shifts,but maintains the structure "
+                 "of the image.\n"
+                 )
         self.add_item(section=section, title="mask_type", datatype=str, default="none",
-                      choices=get_available_masks(), gui_radio=True,
+                      choices=get_available_masks(),
                       info="The mask to be used for training:"
                            "\n\t none: Doesn't use any mask."
                            "\n\t components: An improved face hull mask using a facehull of 8 "
@@ -53,38 +134,25 @@ class Config(FaceswapConfig):
                            "to further up the forehead. May perform badly on difficult angles."
                            "\n\t facehull: Face cutout based on landmarks")
         self.add_item(
-            section=section, title="icnr_init", datatype=bool, default=False,
-            info="Use ICNR Kernel Initializer for upscaling.\nThis can help reduce the "
-                 "'checkerboard effect' when upscaling the image.")
-        self.add_item(
-            section=section, title="conv_aware_init", datatype=bool, default=False,
-            info="Use Convolution Aware Initialization for convolutional layers\nThis can help "
-                 "eradicate the vanishing and exploding gradient problem as well as lead to "
-                 "higher accuracy, lower loss and faster convergence."
-                 "\nNB This can use more VRAM when creating a new model so you may want to lower "
-                 "the batch size for the first run. The batch size can be raised again when "
-                 "reloading the model."
-                 "\nNB: Building the model will likely take several minutes as the caluclations "
-                 "for this initialization technique are expensive.")
-        self.add_item(
-            section=section, title="subpixel_upscaling", datatype=bool, default=False,
-            info="Use subpixel upscaling rather than pixel shuffler.\n"
-                 "Might increase speed at cost of VRAM")
-        self.add_item(
-            section=section, title="reflect_padding", datatype=bool, default=False,
-            info="Use reflect padding rather than zero padding. Only enable this option if the "
-                 "model you are training has a distinct line appearing around the edge of the "
-                 "swap area.")
+            section=section, title="learning_rate", datatype=float, default=5e-5,
+            min_max=(1e-6, 1e-4), rounding=6, fixed=False,
+            info="Learning rate - how fast your network will learn (how large are "
+                 "the modifications to the model weights after one batch of training). "
+                 "Values that are too large might result in model crashes and the "
+                 "inability of the model to find the best solution. "
+                 "Values that are too small might be unable to escape from dead-ends "
+                 "and find the best global minimum.")
         self.add_item(
-            section=section, title="dssim_loss", datatype=bool, default=True,
-            info="Use DSSIM for Loss rather than Mean Absolute Error\n"
-                 "May increase overall quality.")
-        self.add_item(
-            section=section, title="penalized_mask_loss", datatype=bool, default=True,
-            info="If using a mask, This penalizes the loss for the masked area, to give higher "
-                 "priority to the face area. \nShould increase overall quality and speed up "
-                 "training. This should probably be left at True")
-        logger.debug("Set global config")
+            section=section, title="coverage", datatype=float, default=68.75,
+            min_max=(62.5, 100.0), rounding=2, fixed=True,
+            info="How much of the extracted image to train on. A lower coverage will limit the model's "
+                 "scope to a zoomed-in central area while higher amounts can include the entire face. "
+                 "A trade-off exists between lower amounts given a bit more detail versus higher amounts "
+                 "avoiding noticeable swap transitions. Sensible values to use are:"
+                 "\n\t62.5%% spans from eyebrow to eyebrow."
+                 "\n\t75.0%% spans from temple to temple."
+                 "\n\t87.5%% spans from ear to ear."
+                 "\n\t100.0%% is a mugshot.")
 
     def load_module(self, filename, module_path, plugin_type):
         """ Load the defaults module and add defaults """
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 1dcd5d8..3da5949 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -20,7 +20,8 @@ from keras.utils import get_custom_objects, multi_gpu_model
 
 from lib import Serializer
 from lib.model.backup_restore import Backup
-from lib.model.losses import DSSIMObjective, PenalizedLoss
+from lib.model.losses import DSSIMObjective, PenalizedLoss, gradient_loss
+from lib.model.losses import generalized_loss, l_inf_norm, gmsd_loss
 from lib.model.nn_blocks import NNBlocks
 from lib.model.optimizers import Adam
 from lib.multithreading import MultiThread
@@ -35,7 +36,7 @@ class ModelBase():
     """ Base class that all models should inherit from """
     def __init__(self,
                  model_dir,
-                 gpus,
+                 gpus=1,
                  configfile=None,
                  snapshot_interval=0,
                  no_logs=False,
@@ -397,10 +398,8 @@ class ModelBase():
     def converter(self, swap):
         """ Converter for autoencoder models """
         logger.debug("Getting Converter: (swap: %s)", swap)
-        if swap:
-            model = self.predictors["a"]
-        else:
-            model = self.predictors["b"]
+        side = "a" if swap else "b"
+        model = self.predictors[side]
         if self.predict:
             # Must compile the model to be thread safe
             model._make_predict_function()  # pylint: disable=protected-access
@@ -666,36 +665,32 @@ class Loss():
 
     def get_loss_functions(self, side, predict, mask):
         """ Set the loss function """
-        loss_funcs = list()
+        loss_funcs = []
         largest_face = self.largest_output
-
-        if self.config.get("dssim_loss", False):
-            if not predict and side.lower() == "a":
-                logger.verbose("Using DSSIM Loss")
-            loss_func = DSSIMObjective()
-        else:
-            loss_func = losses.mean_absolute_error
-            if not predict and side.lower() == "a":
-                logger.verbose("Using Mean Absolute Error Loss")
+        loss_dict = {'mae':                     losses.mean_absolute_error,
+                     'mse':                     losses.mean_squared_error,
+                     'logcosh':                 losses.logcosh,
+                     'smooth_l1':               generalized_loss,
+                     'l_inf_norm':              l_inf_norm,
+                     'ssim':                    DSSIMObjective(),
+                     'gmsd':                    gmsd_loss,
+                     'pixel_gradient_diff':     gradient_loss}
+        img_loss_config = self.config.get("loss_function", "mae")
+        mask_loss_config = "mse"
 
         for idx, loss_name in enumerate(self.names):
             if loss_name.startswith("mask"):
-                mask_func = losses.mean_squared_error
-                loss_funcs.append(mask_func)
-                logger.debug("mask loss: %s", mask_func)
+                loss_funcs.append(loss_dict[mask_loss_config])
+                logger.debug("mask loss: %s", mask_loss_config)
             elif mask and idx == largest_face and self.config.get("penalized_mask_loss", False):
-                face_func = PenalizedLoss(mask[0], loss_func)
-                logger.debug("final face loss: %s", face_func)
-                loss_funcs.append(face_func)
-                if not predict and side.lower() == "a":
-                    logger.verbose("Penalizing mask for Loss")
+                loss_funcs.append(PenalizedLoss(mask[0], loss_dict[img_loss_config]))
+                logger.debug("final face loss: %s", img_loss_config)
             else:
-                logger.debug("face loss func: %s", loss_func)
-                loss_funcs.append(loss_func)
+                loss_funcs.append(loss_dict[img_loss_config])
+                logger.debug("face loss func: %s", img_loss_config)
         logger.debug(loss_funcs)
         return loss_funcs
 
-
 class NNMeta():
     """ Class to hold a neural network and it's meta data
 
@@ -893,6 +888,7 @@ class State():
                 self.inputs = state.get("inputs", dict())
                 self.config = state.get("config", dict())
                 logger.debug("Loaded state: %s", state)
+                self.update_legacy_config()
                 self.replace_config(config_changeable_items)
         except IOError as err:
             logger.warning("No existing state file found. Generating.")
@@ -947,3 +943,13 @@ class State():
                 continue
             self.config[key] = val
             logger.info("Config item: '%s' has been updated from '%s' to '%s'", key, old_val, val)
+
+    def update_legacy_config(self):
+        """ Update legacy state config files with the new loss formating
+        """
+        prior = "dssim_loss"
+        new = "loss_function"
+        if prior in self.config.keys() and new not in self.config.keys():
+            self.config[new] = "ssim" if self.config[prior] is True else "mae"
+            logger.debug("Updated config from older dssim format. New config loss function: %s",
+                         self.config[new])
diff --git a/plugins/train/model/dfaker_defaults.py b/plugins/train/model/dfaker_defaults.py
index b9f8196..3dc6d6c 100755
--- a/plugins/train/model/dfaker_defaults.py
+++ b/plugins/train/model/dfaker_defaults.py
@@ -44,20 +44,4 @@
 _HELPTEXT = "Dfaker Model (Adapted from https://github.com/dfaker/df)"
 
 
-_DEFAULTS = {
-    "coverage": {
-        "default": 100.0,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
-}
+_DEFAULTS = {}
diff --git a/plugins/train/model/dfl_h128_defaults.py b/plugins/train/model/dfl_h128_defaults.py
index cd0fa52..edc69c7 100755
--- a/plugins/train/model/dfl_h128_defaults.py
+++ b/plugins/train/model/dfl_h128_defaults.py
@@ -56,19 +56,4 @@ _DEFAULTS = {
         "gui_radio": False,
         "fixed": True,
     },
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
 }
diff --git a/plugins/train/model/iae_defaults.py b/plugins/train/model/iae_defaults.py
index 50c16a1..915df47 100755
--- a/plugins/train/model/iae_defaults.py
+++ b/plugins/train/model/iae_defaults.py
@@ -47,20 +47,4 @@ _HELPTEXT = (
 )
 
 
-_DEFAULTS = {
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
-}
+_DEFAULTS = {}
diff --git a/plugins/train/model/lightweight_defaults.py b/plugins/train/model/lightweight_defaults.py
index 04554f0..cdada18 100755
--- a/plugins/train/model/lightweight_defaults.py
+++ b/plugins/train/model/lightweight_defaults.py
@@ -48,20 +48,4 @@ _HELPTEXT = (
 )
 
 
-_DEFAULTS = {
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized\n"
-                "to the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
-}
+_DEFAULTS = {}
diff --git a/plugins/train/model/original_defaults.py b/plugins/train/model/original_defaults.py
index f15d211..79e9d3e 100755
--- a/plugins/train/model/original_defaults.py
+++ b/plugins/train/model/original_defaults.py
@@ -56,19 +56,4 @@ _DEFAULTS = {
         "gui_radio": False,
         "fixed": True,
     },
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
 }
diff --git a/plugins/train/model/realface_defaults.py b/plugins/train/model/realface_defaults.py
index 0129dba..9ff86b1 100755
--- a/plugins/train/model/realface_defaults.py
+++ b/plugins/train/model/realface_defaults.py
@@ -49,21 +49,6 @@ _HELPTEXT = (
 
 
 _DEFAULTS = {
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
     "input_size": {
         "default": 64,
         "info": "Resolution (in pixels) of the input image to train on.\n"
@@ -121,16 +106,4 @@ _DEFAULTS = {
         "gui_radio": False,
         "fixed": True,
     },
-    "learning_rate": {
-        "default": 5e-05,
-        "info": "Learning rate - how fast your network will learn.\n"
-                "Note that: Higher values might result in RSoD failure.\n"
-                "This option can be updated for existing models.",
-        "datatype": float,
-        "rounding": 6,
-        "min_max": (5e-06, 0.0001),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": False,
-    },
 }
diff --git a/plugins/train/model/unbalanced_defaults.py b/plugins/train/model/unbalanced_defaults.py
index 3382747..16f430d 100755
--- a/plugins/train/model/unbalanced_defaults.py
+++ b/plugins/train/model/unbalanced_defaults.py
@@ -126,19 +126,4 @@ _DEFAULTS = {
         "gui_radio": False,
         "fixed": True,
     },
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
 }
diff --git a/plugins/train/model/villain_defaults.py b/plugins/train/model/villain_defaults.py
index 18dcba9..e63423d 100755
--- a/plugins/train/model/villain_defaults.py
+++ b/plugins/train/model/villain_defaults.py
@@ -59,19 +59,4 @@ _DEFAULTS = {
         "gui_radio": False,
         "fixed": True,
     },
-    "coverage": {
-        "default": 62.5,
-        "info": "How much of the extracted image to train on. Generally the model is optimized"
-                "\nto the default value. Sensible values to use are:"
-                "\n\t62.5%% spans from eyebrow to eyebrow."
-                "\n\t75.0%% spans from temple to temple."
-                "\n\t87.5%% spans from ear to ear."
-                "\n\t100.0%% is a mugshot.",
-        "datatype": float,
-        "rounding": 1,
-        "min_max": (62.5, 100.0),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
 }
diff --git a/scripts/convert.py b/scripts/convert.py
index 698cb6d..bc6a9d8 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -190,7 +190,7 @@ class DiskIO():
     @property
     def pre_encode(self):
         """ Return the writer's pre-encoder """
-        dummy = np.zeros((20, 20, 3)).astype("uint8")
+        dummy = np.zeros((20, 20, 3), dtype="uint8")
         test = self.writer.pre_encode(dummy)
         retval = None if test is None else self.writer.pre_encode
         logger.debug("Writer pre_encode function: %s", retval)
@@ -464,9 +464,8 @@ class Predict():
     @property
     def input_mask(self):
         """ Return the input mask """
-        mask = np.zeros(self.model.state.mask_shapes[0], dtype="float32")
-        retval = np.expand_dims(mask, 0)
-        return retval
+        mask = np.zeros((1, ) + self.model.state.mask_shapes[0], dtype="float32")
+        return mask
 
     @property
     def has_predicted_mask(self):
diff --git a/scripts/train.py b/scripts/train.py
index fb95265..f834d0d 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -134,7 +134,6 @@ class Train():
 
             if self.args.allow_growth:
                 self.set_tf_allow_growth()
-
             model = self.load_model()
             trainer = self.load_trainer(model)
             self.run_training_cycle(model, trainer)
@@ -157,7 +156,7 @@ class Train():
         augment_color = not self.args.no_augment_color
         model = PluginLoader.get_model(self.trainer_name)(
             model_dir,
-            self.args.gpus,
+            gpus=self.args.gpus,
             configfile=configfile,
             snapshot_interval=self.args.snapshot_interval,
             no_logs=self.args.no_logs,
