commit d93e7b11140c11b43113686849a1c5c74d3a06f2
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Fri Oct 18 15:44:25 2019 +0000

    Smart Mask - Extract code review
    
    - Lint simple_tests.py
    - Only reformat alignments file if it exists otherwise change filename
    - Update legacy alignments to new format at all stages
    - faces_detect.Mask.from_dict - logging format fix
    - convert.py fix otf for new pipeline
    - cli.py - Add note that masks not used. Revert convert masks
    - faces_detect.py - Revert non-extract code
    - Add .p and .pickle extensions for serializer
    - plugins/extract revert some changes
    - scripts/fsmedia - Revert code changes
    - Pipeline - cleanup
    - Consistant alpha channel stripping (fixes single-process)
    - Store landmarks as numpy array
    - Code attribution
    - Normalize feed face and reference face to 0.0 - 1.0 in convert
    - Lock in mask VRAM sized
    - Add documentation to plugin_loader
    - Update alignments tool to work with new format

diff --git a/.pylintrc b/.pylintrc
index 6907995..5c52c8b 100644
--- a/.pylintrc
+++ b/.pylintrc
@@ -19,7 +19,7 @@ ignore-patterns=
 
 # Use multiple processes to speed up Pylint. Specifying 0 will auto-detect the
 # number of processors available to use.
-jobs=1
+jobs=0
 
 # Control the amount of potential inferred values when inferring a single
 # object. This can help the performance when dealing with large functions or
@@ -477,10 +477,10 @@ notes=FIXME,
 [DESIGN]
 
 # Maximum number of arguments for function / method.
-max-args=5
+max-args=10
 
 # Maximum number of attributes for a class (see R0902).
-max-attributes=7
+max-attributes=10
 
 # Maximum number of boolean expressions in an if statement.
 max-bool-expr=5
diff --git a/_travis/simple_tests.py b/_travis/simple_tests.py
index 52d0530..00dd920 100644
--- a/_travis/simple_tests.py
+++ b/_travis/simple_tests.py
@@ -14,8 +14,8 @@ from urllib.request import urlretrieve
 import os
 from os.path import join as pathjoin, expanduser
 
-fail_count = 0
-test_count = 0
+FAIL_COUNT = 0
+TEST_COUNT = 0
 _COLORS = {
     "FAIL": "\033[1;31m",
     "OK": "\033[1;32m",
@@ -26,8 +26,10 @@ _COLORS = {
 
 
 def print_colored(text, color="OK", bold=False):
-    # This might not work on windows,
-    # altho travis runs windows stuff in git bash, so it might ?
+    """ Print colored text
+    This might not work on windows,
+    although travis runs windows stuff in git bash, so it might ?
+    """
     color = _COLORS.get(color, color)
     print("%s%s%s%s" % (
         color, "" if not bold else _COLORS["BOLD"], text, _COLORS["ENDC"]
@@ -35,33 +37,38 @@ def print_colored(text, color="OK", bold=False):
 
 
 def print_ok(text):
+    """ Print ok in colored text """
     print_colored(text, "OK", True)
 
 
 def print_fail(text):
+    """ Print fail in colored text """
     print_colored(text, "FAIL", True)
 
 
 def print_status(text):
+    """ Print status in colored text """
     print_colored(text, "STATUS", True)
 
 
 def run_test(name, cmd):
-    global fail_count, test_count
+    """ run a test """
+    global FAIL_COUNT, TEST_COUNT  # pylint:disable=global-statement
     print_status("[?] running %s" % name)
     print("Cmd: %s" % " ".join(cmd))
-    test_count += 1
+    TEST_COUNT += 1
     try:
         check_call(cmd)
         print_ok("[+] Test success")
         return True
-    except CalledProcessError as e:
-        print_fail("[-] Test failed with %s" % e)
-        fail_count += 1
+    except CalledProcessError as err:
+        print_fail("[-] Test failed with %s" % err)
+        FAIL_COUNT += 1
         return False
 
 
 def download_file(url, filename):  # TODO: retry
+    """ Download a file from given url """
     if os.path.isfile(filename):
         print_status("[?] '%s' already cached as '%s'" % (url, filename))
         return filename
@@ -69,12 +76,13 @@ def download_file(url, filename):  # TODO: retry
         print_status("[?] Downloading '%s' to '%s'" % (url, filename))
         video, _ = urlretrieve(url, filename)
         return video
-    except urllib.error.URLError as e:
-        print_fail("[-] Failed downloading: %s" % e)
+    except urllib.error.URLError as err:
+        print_fail("[-] Failed downloading: %s" % err)
         return None
 
 
 def extract_args(detector, aligner, in_path, out_path, args=None):
+    """ Extraction command """
     py_exe = sys.executable
     _extract_args = "%s faceswap.py extract -i %s -o %s -D %s -A %s" % (
         py_exe, in_path, out_path, detector, aligner
@@ -84,16 +92,18 @@ def extract_args(detector, aligner, in_path, out_path, args=None):
     return _extract_args.split()
 
 
-def train_args(model, model_path, faces, alignments, iterations=5, bs=8, extra_args=""):
+def train_args(model, model_path, faces, alignments, iterations=5, batchsize=8, extra_args=""):
+    """ Train command """
     py_exe = sys.executable
     args = "%s faceswap.py train -A %s -ala %s -B %s -alb %s -m %s -t %s -bs %i -it %s %s" % (
         py_exe, faces, alignments, faces,
-        alignments, model_path, model, bs, iterations, extra_args
+        alignments, model_path, model, batchsize, iterations, extra_args
     )
     return args.split()
 
 
 def convert_args(in_path, out_path, model_path, writer, args=None):
+    """ Convert command """
     py_exe = sys.executable
     conv_args = "%s faceswap.py convert -i %s -o %s -m %s -w %s" % (
         py_exe, in_path, out_path, model_path, writer
@@ -104,6 +114,7 @@ def convert_args(in_path, out_path, model_path, writer, args=None):
 
 
 def sort_args(in_path, out_path, sortby="face", groupby="hist", method="rename"):
+    """ Sort command """
     py_exe = sys.executable
     _sort_args = "%s tools.py sort -i %s -o %s -s %s -fp %s -g %s -k" % (
         py_exe, in_path, out_path, sortby, method, groupby
@@ -111,7 +122,8 @@ def sort_args(in_path, out_path, sortby="face", groupby="hist", method="rename")
     return _sort_args.split()
 
 
-if __name__ == '__main__':
+def main():
+    """ Main testing script """
     vid_src = "https://faceswap.dev/data/test.mp4"
     img_src = "https://archive.org/download/GPN-2003-00070/GPN-2003-00070.jpg"
     base_dir = pathjoin(expanduser("~"), "cache", "tests")
@@ -136,7 +148,7 @@ if __name__ == '__main__':
     if not img_path:
         print_fail("[-] Aborting")
         exit(1)
-    img_extract = run_test(
+    run_test(
         "Extraction images with cv2-dnn detector and cv2-dnn aligner.",
         extract_args("Cv2-Dnn", "Cv2-Dnn", img_base, pathjoin(img_base, "faces"))
     )
@@ -193,9 +205,13 @@ if __name__ == '__main__':
             )
         )
 
-    if fail_count == 0:
-        print_ok("[+] Failed %i/%i tests." % (fail_count, test_count))
+    if FAIL_COUNT == 0:
+        print_ok("[+] Failed %i/%i tests." % (FAIL_COUNT, TEST_COUNT))
         exit(0)
     else:
-        print_fail("[-] Failed %i/%i tests." % (fail_count, test_count))
+        print_fail("[-] Failed %i/%i tests." % (FAIL_COUNT, TEST_COUNT))
         exit(1)
+
+
+if __name__ == '__main__':
+    main()
diff --git a/docs/full/plugins.plugin_loader.rst b/docs/full/plugins.plugin_loader.rst
new file mode 100644
index 0000000..dce7024
--- /dev/null
+++ b/docs/full/plugins.plugin_loader.rst
@@ -0,0 +1,7 @@
+plugins.plugin\_loader module
+=============================
+
+.. automodule:: plugins.plugin_loader
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.rst b/docs/full/plugins.rst
index c0659fe..37ea4a1 100644
--- a/docs/full/plugins.rst
+++ b/docs/full/plugins.rst
@@ -7,6 +7,7 @@ Subpackages
 .. toctree::
 
    plugins.extract
+   plugins.plugin_loader
 
 Module contents
 ---------------
diff --git a/lib/aligner.py b/lib/aligner.py
index 2e00b9f..e1f595c 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -125,5 +125,5 @@ def get_matrix_scaling(mat):
 
 def get_align_mat(face):
     """ Return the alignment Matrix """
-    mat_umeyama = umeyama(np.array(face.landmarks_xy[17:]), True)[0:2]
+    mat_umeyama = umeyama(face.landmarks_xy[17:], True)[0:2]
     return mat_umeyama
diff --git a/lib/alignments.py b/lib/alignments.py
index 09f9b55..64f7ba9 100644
--- a/lib/alignments.py
+++ b/lib/alignments.py
@@ -71,7 +71,7 @@ class Alignments():
         """ Return the path to alignments file """
         logger.debug("Getting location: (folder: '%s', filename: '%s')", folder, filename)
         extension = os.path.splitext(filename)[1]
-        if extension in (".json", ".p", ".yaml", ".yml"):
+        if extension in (".json", ".p", ".pickle", ".yaml", ".yml"):
             # Reformat legacy alignments file
             filename = self.update_file_format(folder, filename)
             logger.debug("Updated legacy alignments. New filename: '%s'", filename)
@@ -82,6 +82,11 @@ class Alignments():
         elif extension != ".fsa":
             raise FaceswapError("{} is not a valid alignments file".format(filename))
         location = os.path.join(str(folder), filename)
+        if not os.path.exists(location):
+            # Test for old format alignments files and reformat if they exist
+            # This will be executed if an alignments file has not been explicitly provided
+            # therefore it will not have been picked up in the extension test
+            self.test_for_legacy(location)
         logger.verbose("Alignments filepath: '%s'", location)
         return location
 
@@ -246,17 +251,34 @@ class Alignments():
 
     # <File Format> #
     # Serializer is now a compressed pickle .fsa format. This used to be any number of serializers
+    def test_for_legacy(self, location):
+        """ For alignments filenames passed in with out an extension, test for legacy formats """
+        logger.debug("Checking for legacy alignments file formats: '%s'", location)
+        filename = os.path.splitext(location)[0]
+        for ext in (".json", ".p", ".pickle", ".yaml"):
+            legacy_filename = "{}{}".format(filename, ext)
+            if os.path.exists(legacy_filename):
+                logger.debug("Legacy alignments file exists: '%s'", legacy_filename)
+                _ = self.update_file_format(*os.path.split(legacy_filename))
+                break
+            logger.debug("Legacy alignments file does not exist: '%s'", legacy_filename)
+
     def update_file_format(self, folder, filename):
         """ Convert old style alignments format to new style format """
         logger.info("Reformatting legacy alignments file...")
         old_location = os.path.join(str(folder), filename)
         new_location = "{}.{}".format(os.path.splitext(old_location)[0],
                                       self.serializer.file_extension)
-        logger.info("Old location: '%s', New location: '%s'", old_location, new_location)
-
-        load_serializer = get_serializer_from_filename(old_location)
-        data = load_serializer.load(old_location)
-        self.serializer.save(new_location, data)
+        if os.path.exists(old_location):
+            if os.path.exists(new_location):
+                logger.info("Using existing updated alignments file found at '%s'. If you do not "
+                            "wish to use this existing file then you should delete or rename it.",
+                            new_location)
+            else:
+                logger.info("Old location: '%s', New location: '%s'", old_location, new_location)
+                load_serializer = get_serializer_from_filename(old_location)
+                data = load_serializer.load(old_location)
+                self.serializer.save(new_location, data)
         return os.path.basename(new_location)
 
     # <landmarks> #
diff --git a/lib/cli.py b/lib/cli.py
index 30bbea4..d491cdf 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -14,6 +14,7 @@ import textwrap
 from importlib import import_module
 
 from lib.logger import crash_log, log_setup
+from lib.model.masks import get_available_masks, get_default_mask
 from lib.utils import FaceswapError, get_backend, safe_shutdown
 from plugins.plugin_loader import PluginLoader
 
@@ -399,38 +400,39 @@ class FaceSwapArgs():
         """ Arguments that are used in ALL parts of Faceswap
             DO NOT override this """
         global_args = list()
-        global_args.append({"opts": ("-C", "--configfile"),
-                            "action": FileFullPaths,
-                            "filetypes": "ini",
-                            "type": str,
-                            "group": "Global Options",
-                            "help": "Optionally overide the saved config with the path to a "
-                                    "custom config file."})
-        global_args.append({"opts": ("-L", "--loglevel"),
-                            "type": str.upper,
-                            "dest": "loglevel",
-                            "default": "INFO",
-                            "choices": ("INFO", "VERBOSE", "DEBUG", "TRACE"),
-                            "group": "Global Options",
-                            "help": "Log level. Stick with INFO or VERBOSE unless you need to "
-                                    "file an error report. Be careful with TRACE as it will "
-                                    "generate a lot of data"})
-        global_args.append({"opts": ("-LF", "--logfile"),
-                            "action": SaveFileFullPaths,
-                            "filetypes": 'log',
-                            "type": str,
-                            "dest": "logfile",
-                            "group": "Global Options",
-                            "help": "Path to store the logfile. Leave blank to store in the "
-                                    "faceswap folder",
-                            "default": None})
+        global_args.append({
+            "opts": ("-C", "--configfile"),
+            "action": FileFullPaths,
+            "filetypes": "ini",
+            "type": str,
+            "group": "Global Options",
+            "help": "Optionally overide the saved config with the path to a custom config file."})
+        global_args.append({
+            "opts": ("-L", "--loglevel"),
+            "type": str.upper,
+            "dest": "loglevel",
+            "default": "INFO",
+            "choices": ("INFO", "VERBOSE", "DEBUG", "TRACE"),
+            "group": "Global Options",
+            "help": "Log level. Stick with INFO or VERBOSE unless you need to file an error "
+                    "report. Be careful with TRACE as it will generate a lot of data"})
+        global_args.append({
+            "opts": ("-LF", "--logfile"),
+            "action": SaveFileFullPaths,
+            "filetypes": 'log',
+            "type": str,
+            "dest": "logfile",
+            "group": "Global Options",
+            "help": "Path to store the logfile. Leave blank to store in the faceswap folder",
+            "default": None})
         # This is a hidden argument to indicate that the GUI is being used,
         # so the preview window should be redirected Accordingly
-        global_args.append({"opts": ("-gui", "--gui"),
-                            "action": "store_true",
-                            "dest": "redirect_gui",
-                            "default": False,
-                            "help": argparse.SUPPRESS})
+        global_args.append({
+            "opts": ("-gui", "--gui"),
+            "action": "store_true",
+            "dest": "redirect_gui",
+            "default": False,
+            "help": argparse.SUPPRESS})
         return global_args
 
     @staticmethod
@@ -482,31 +484,32 @@ class ExtractConvertArgs(FaceSwapArgs):
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
         argument_list = list()
-        argument_list.append({"opts": ("-i", "--input-dir"),
-                              "action": DirOrFileFullPaths,
-                              "filetypes": "video",
-                              "dest": "input_dir",
-                              "required": True,
-                              "group": "Data",
-                              "help": "Input directory or video. Either a directory containing "
-                                      "the image files you wish to process or path to a video "
-                                      "file. NB: This should be the source video/frames NOT the "
-                                      "source faces."})
-        argument_list.append({"opts": ("-o", "--output-dir"),
-                              "action": DirFullPaths,
-                              "dest": "output_dir",
-                              "required": True,
-                              "group": "Data",
-                              "help": "Output directory. This is where the converted files will "
-                                      "be saved."})
-        argument_list.append({"opts": ("-al", "--alignments"),
-                              "action": FileFullPaths,
-                              "filetypes": "alignments",
-                              "type": str,
-                              "dest": "alignments_path",
-                              "group": "Data",
-                              "help": "Optional path to an alignments file. Leave blank if the "
-                                      "alignments file is at the default location."})
+        argument_list.append({
+            "opts": ("-i", "--input-dir"),
+            "action": DirOrFileFullPaths,
+            "filetypes": "video",
+            "dest": "input_dir",
+            "required": True,
+            "group": "Data",
+            "help": "Input directory or video. Either a directory containing the image files you "
+                    "wish to process or path to a video file. NB: This should be the source video/"
+                    "frames NOT the source faces."})
+        argument_list.append({
+            "opts": ("-o", "--output-dir"),
+            "action": DirFullPaths,
+            "dest": "output_dir",
+            "required": True,
+            "group": "Data",
+            "help": "Output directory. This is where the converted files will be saved."})
+        argument_list.append({
+            "opts": ("-al", "--alignments"),
+            "action": FileFullPaths,
+            "filetypes": "alignments",
+            "type": str,
+            "dest": "alignments_path",
+            "group": "Data",
+            "help": "Optional path to an alignments file. Leave blank if the alignments file is "
+                    "at the default location."})
         return argument_list
 
 
@@ -532,219 +535,220 @@ class ExtractArgs(ExtractConvertArgs):
             default_aligner = "fan"
 
         argument_list = []
-        argument_list.append({"opts": ("-D", "--detector"),
-                              "action": Radio,
-                              "type": str.lower,
-                              "choices":  PluginLoader.get_available_extractors("detect"),
-                              "default": default_detector,
-                              "group": "Plugins",
-                              "help": "R|Detector to use. Some of these have configurable "
-                                      "settings in '/config/extract.ini' or 'Settings > Configure "
-                                      "Extract 'Plugins':"
-                                      "\nL|cv2-dnn: A CPU only extractor which is the least "
-                                      "reliable and least resource intensive. Use this if not "
-                                      "using a GPU and time is important."
-                                      "\nL|mtcnn: Good detector. Fast on CPU, faster on GPU. Uses "
-                                      "fewer resources than other GPU detectors but can often "
-                                      "return more false positives."
-                                      "\nL|s3fd: Best detector. Fast on GPU, slow on CPU. Can "
-                                      "detect more faces and fewer false positives than other "
-                                      "GPU detectors, but is a lot more resource intensive."})
-        argument_list.append({"opts": ("-A", "--aligner"),
-                              "action": Radio,
-                              "type": str.lower,
-                              "choices": PluginLoader.get_available_extractors("align"),
-                              "default": default_aligner,
-                              "group": "Plugins",
-                              "help": "R|Aligner to use."
-                                      "\nL|cv2-dnn: A CPU only landmark detector. Faster, less "
-                                      "resource intensive, but less accurate. Only use this if "
-                                      "not using a GPU and time is important."
-                                      "\nL|fan: Best aligner. Fast on GPU, slow on CPU."})
-        argument_list.append({"opts": ("-M", "--masker"),
-                              "action": Radio,
-                              "type": str.lower,
-                              "choices": PluginLoader.get_available_extractors("mask"),
-                              "default": "components",
-                              "group": "Plugins",
-                              "help": "R|Masker to use."
-                                      "\nL|none: An array of all ones is created to provide a 4th "
-                                      "channel that will not mask any portion of the image."
-                                      "\nL|components: Mask designed to provide facial "
-                                      "segmentation based on the positioning of landmark "
-                                      "locations. A convex hull is constructed around the "
-                                      "exterior of the landmarks to create a mask."
-                                      "\nL|extended: Mask designed to provide facial segmentation "
-                                      "based on the positioning of landmark locations. A convex "
-                                      "hull is constructed around the exterior of the landmarks "
-                                      "and the mask is extended upwards onto the forehead."
-                                      "\nL|vgg-clear: Mask designed to provide smart segmentation "
-                                      "of mostly frontal faces clear of obstructions.  Profile "
-                                      "faces and obstructions may result in sub-par performance."
-                                      "\nL|vgg-obstructed: Mask designed to provide smart "
-                                      "segmentation of mostly frontal faces. The mask model has "
-                                      "been specifically trained to recognize some facial "
-                                      "obstructions (hands and eyeglasses). Profile faces may "
-                                      "result in sub-par performance."
-                                      "\nL|unet-dfl: Mask designed to provide smart segmentation "
-                                      "of mostly frontal faces. The mask model has been trained "
-                                      "by community members and will need testing for further "
-                                      "description. Profile faces may result in sub-par "
-                                      "performance."})
-        argument_list.append({"opts": ("-nm", "--normalization"),
-                              "action": Radio,
-                              "type": str.lower,
-                              "dest": "normalization",
-                              "choices": ["none", "clahe", "hist", "mean"],
-                              "default": "none",
-                              "group": "plugins",
-                              "help": "R|Performing normalization can help the aligner better "
-                                      "align faces with difficult lighting conditions at an "
-                                      "extraction speed cost. Different methods will yield "
-                                      "different results on different sets. NB: This does not "
-                                      "impact the output face, just the input to the aligner."
-                                      "\nL|none: Don't perform normalization on the face."
-                                      "\nL|clahe: Perform Contrast Limited Adaptive Histogram "
-                                      "Equalization on the face."
-                                      "\nL|hist: Equalize the histograms on the RGB channels."
-                                      "\nL|mean: Normalize the face colors to the mean."})
-        argument_list.append({"opts": ("-r", "--rotate-images"),
-                              "type": str,
-                              "dest": "rotate_images",
-                              "default": None,
-                              "group": "plugins",
-                              "help": "If a face isn't found, rotate the images to try to find a "
-                                      "face. Can find more faces at the cost of extraction speed. "
-                                      "Pass in a single number to use increments of that size up "
-                                      "to 360, or pass in a list of numbers to enumerate exactly "
-                                      "what angles to check"})
-        argument_list.append({"opts": ("-min", "--min-size"),
-                              "type": int,
-                              "action": Slider,
-                              "dest": "min_size",
-                              "min_max": (0, 1080),
-                              "default": 0,
-                              "rounding": 20,
-                              "group": "Face Processing",
-                              "help": "Filters out faces detected below this size. Length, in "
-                                      "pixels across the diagonal of the bounding box. Set to 0 "
-                                      "for off"})
-        argument_list.append({"opts": ("-n", "--nfilter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "nfilter",
-                              "nargs": "+",
-                              "default": None,
-                              "group": "Face Processing",
-                              "help": "Optionally filter out people who you do not wish to "
-                                      "process by passing in an image of that person. Should be a "
-                                      "front portrait with a single person in the image. Multiple "
-                                      "images can be added space separated. NB: Using face filter "
-                                      "will significantly decrease extraction speed and its "
-                                      "accuracy cannot be guaranteed."})
-        argument_list.append({"opts": ("-f", "--filter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "filter",
-                              "nargs": "+",
-                              "default": None,
-                              "group": "Face Processing",
-                              "help": "Optionally select people you wish to process by passing in "
-                                      "an image of that person. Should be a front portrait with a "
-                                      "single person in the image. Multiple images can be added "
-                                      "space separated. NB: Using face filter will significantly "
-                                      "decrease extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
-        argument_list.append({"opts": ("-l", "--ref_threshold"),
-                              "action": Slider,
-                              "min_max": (0.01, 0.99),
-                              "rounding": 2,
-                              "type": float,
-                              "dest": "ref_threshold",
-                              "default": 0.4,
-                              "group": "Face Processing",
-                              "help": "For use with the optional nfilter/filter files. Threshold "
-                                      "for positive face recognition. Lower values are stricter. "
-                                      "NB: Using face filter will significantly decrease "
-                                      "extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
-        argument_list.append({"opts": ("-bt", "--blur-threshold"),
-                              "type": float,
-                              "action": Slider,
-                              "min_max": (0.0, 100.0),
-                              "rounding": 1,
-                              "dest": "blur_thresh",
-                              "default": 0.0,
-                              "group": "Face Processing",
-                              "help": "Automatically discard images blurrier than the specified "
-                                      "threshold. Discarded images are moved into a \"blurry\" "
-                                      "sub-folder. Lower values allow more blur. Set to 0.0 to "
-                                      "turn off."})
-        argument_list.append({"opts": ("-een", "--extract-every-n"),
-                              "type": int,
-                              "action": Slider,
-                              "dest": "extract_every_n",
-                              "min_max": (1, 100),
-                              "default": 1,
-                              "rounding": 1,
-                              "group": "output",
-                              "help": "Extract every 'nth' frame. This option will skip frames "
-                                      "when extracting faces. For example a value of 1 will "
-                                      "extract faces from every frame, a value of 10 will extract "
-                                      "faces from every 10th frame."})
-        argument_list.append({"opts": ("-sz", "--size"),
-                              "type": int,
-                              "action": Slider,
-                              "min_max": (128, 512),
-                              "default": 256,
-                              "rounding": 64,
-                              "group": "output",
-                              "help": "The output size of extracted faces. Make sure that the "
-                                      "model you intend to train supports your required size. "
-                                      "This will only need to be changed for hi-res models."})
-        argument_list.append({"opts": ("-si", "--save-interval"),
-                              "dest": "save_interval",
-                              "type": int,
-                              "action": Slider,
-                              "min_max": (0, 1000),
-                              "rounding": 10,
-                              "default": 0,
-                              "group": "output",
-                              "help": "Automatically save the alignments file after a set amount "
-                                      "of frames. By default the alignments file is only saved at "
-                                      "the end of the extraction process. NB: If extracting in 2 "
-                                      "passes then the alignments file will only start to be "
-                                      "saved out during the second pass. WARNING: Don't interrupt "
-                                      "the script when writing the file because it might get "
-                                      "corrupted. Set to 0 to turn off"})
-        argument_list.append({"opts": ("-dl", "--debug-landmarks"),
-                              "action": "store_true",
-                              "dest": "debug_landmarks",
-                              "group": "output",
-                              "default": False,
-                              "help": "Draw landmarks on the ouput faces for debugging purposes."})
-        argument_list.append({"opts": ("-sp", "--singleprocess"),
-                              "action": "store_true",
-                              "default": False,
-                              "backend": "nvidia",
-                              "group": "settings",
-                              "help": "Don't run extraction in parallel. Will run detection first "
-                                      "then alignment (2 passes). Useful if VRAM is at a "
-                                      "premium."})
-        argument_list.append({"opts": ("-s", "--skip-existing"),
-                              "action": "store_true",
-                              "dest": "skip_existing",
-                              "group": "settings",
-                              "default": False,
-                              "help": "Skips frames that have already been extracted and exist in "
-                                      "the alignments file"})
-        argument_list.append({"opts": ("-sf", "--skip-existing-faces"),
-                              "action": "store_true",
-                              "dest": "skip_faces",
-                              "group": "settings",
-                              "default": False,
-                              "help": "Skip frames that already have detected faces in the "
-                                      "alignments file"})
+        argument_list.append({
+            "opts": ("-D", "--detector"),
+            "action": Radio,
+            "type": str.lower,
+            "choices":  PluginLoader.get_available_extractors("detect"),
+            "default": default_detector,
+            "group": "Plugins",
+            "help": "R|Detector to use. Some of these have configurable settings in "
+                    "'/config/extract.ini' or 'Settings > Configure Extract 'Plugins':"
+                    "\nL|cv2-dnn: A CPU only extractor which is the least reliable and least "
+                    "resource intensive. Use this if not using a GPU and time is important."
+                    "\nL|mtcnn: Good detector. Fast on CPU, faster on GPU. Uses fewer resources "
+                    "than other GPU detectors but can often return more false positives."
+                    "\nL|s3fd: Best detector. Fast on GPU, slow on CPU. Can detect more faces and "
+                    "fewer false positives than other GPU detectors, but is a lot more resource "
+                    "intensive."})
+        argument_list.append({
+            "opts": ("-A", "--aligner"),
+            "action": Radio,
+            "type": str.lower,
+            "choices": PluginLoader.get_available_extractors("align"),
+            "default": default_aligner,
+            "group": "Plugins",
+            "help": "R|Aligner to use."
+                    "\nL|cv2-dnn: A CPU only landmark detector. Faster, less resource intensive, "
+                    "but less accurate. Only use this if not using a GPU and time is important."
+                    "\nL|fan: Best aligner. Fast on GPU, slow on CPU."})
+        argument_list.append({
+            "opts": ("-M", "--masker"),
+            "action": Radio,
+            "type": str.lower,
+            "choices": PluginLoader.get_available_extractors("mask"),
+            "default": "extended",
+            "group": "Plugins",
+            "help": "R|Masker to use. NB: Masker is not currently used by the rest of the process "
+                    "but this will store a mask in the alignments file for use when it has been "
+                    "implemented."
+                    "\nL|none: An array of all ones is created to provide a 4th channel that will "
+                    "not mask any portion of the image."
+                    "\nL|components: Mask designed to provide facial segmentation based on the "
+                    "positioning of landmark locations. A convex hull is constructed around the "
+                    "exterior of the landmarks to create a mask."
+                    "\nL|extended: Mask designed to provide facial segmentation based on the "
+                    "positioning of landmark locations. A convex hull is constructed around the "
+                    "exterior of the landmarks and the mask is extended upwards onto the forehead."
+                    "\nL|vgg-clear: Mask designed to provide smart segmentation of mostly frontal "
+                    "faces clear of obstructions. Profile faces and obstructions may result in "
+                    "sub-par performance."
+                    "\nL|vgg-obstructed: Mask designed to provide smart segmentation of mostly "
+                    "frontal faces. The mask model has been specifically trained to recognize "
+                    "some facial obstructions (hands and eyeglasses). Profile faces may result in "
+                    "sub-par performance."
+                    "\nL|unet-dfl: Mask designed to provide smart segmentation of mostly frontal "
+                    "faces. The mask model has been trained by community members and will need "
+                    "testing for further description. Profile faces may result in sub-par "
+                    "performance."})
+        argument_list.append({
+            "opts": ("-nm", "--normalization"),
+            "action": Radio,
+            "type": str.lower,
+            "dest": "normalization",
+            "choices": ["none", "clahe", "hist", "mean"],
+            "default": "none",
+            "group": "plugins",
+            "help": "R|Performing normalization can help the aligner better align faces with "
+                    "difficult lighting conditions at an extraction speed cost. Different methods "
+                    "will yield different results on different sets. NB: This does not impact the "
+                    "output face, just the input to the aligner."
+                    "\nL|none: Don't perform normalization on the face."
+                    "\nL|clahe: Perform Contrast Limited Adaptive Histogram Equalization on the "
+                    "face."
+                    "\nL|hist: Equalize the histograms on the RGB channels."
+                    "\nL|mean: Normalize the face colors to the mean."})
+        argument_list.append({
+            "opts": ("-r", "--rotate-images"),
+            "type": str,
+            "dest": "rotate_images",
+            "default": None,
+            "group": "plugins",
+            "help": "If a face isn't found, rotate the images to try to find a face. Can find "
+                    "more faces at the cost of extraction speed. Pass in a single number to use "
+                    "increments of that size up to 360, or pass in a list of numbers to enumerate "
+                    "exactly what angles to check."})
+        argument_list.append({
+            "opts": ("-min", "--min-size"),
+            "type": int,
+            "action": Slider,
+            "dest": "min_size",
+            "min_max": (0, 1080),
+            "default": 0,
+            "rounding": 20,
+            "group": "Face Processing",
+            "help": "Filters out faces detected below this size. Length, in pixels across the "
+                    "diagonal of the bounding box. Set to 0 for off"})
+        argument_list.append({
+            "opts": ("-n", "--nfilter"),
+            "action": FilesFullPaths,
+            "filetypes": "image",
+            "dest": "nfilter",
+            "nargs": "+",
+            "default": None,
+            "group": "Face Processing",
+            "help": "Optionally filter out people who you do not wish to process by passing in an "
+                    "image of that person. Should be a front portrait with a single person in the "
+                    "image. Multiple images can be added space separated. NB: Using face filter "
+                    "will significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-f", "--filter"),
+            "action": FilesFullPaths,
+            "filetypes": "image",
+            "dest": "filter",
+            "nargs": "+",
+            "default": None,
+            "group": "Face Processing",
+            "help": "Optionally select people you wish to process by passing in an image of that "
+                    "person. Should be a front portrait with a single person in the image. "
+                    "Multiple images can be added space separated. NB: Using face filter will "
+                    "significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-l", "--ref_threshold"),
+            "action": Slider,
+            "min_max": (0.01, 0.99),
+            "rounding": 2,
+            "type": float,
+            "dest": "ref_threshold",
+            "default": 0.4,
+            "group": "Face Processing",
+            "help": "For use with the optional nfilter/filter files. Threshold for positive face "
+                    "recognition. Lower values are stricter. NB: Using face filter will "
+                    "significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-bt", "--blur-threshold"),
+            "type": float,
+            "action": Slider,
+            "min_max": (0.0, 100.0),
+            "rounding": 1,
+            "dest": "blur_thresh",
+            "default": 0.0,
+            "group": "Face Processing",
+            "help": "Automatically discard images blurrier than the specified threshold. "
+                    "Discarded images are moved into a \"blurry\" sub-folder. Lower values allow "
+                    "more blur. Set to 0.0 to turn off."})
+        argument_list.append({
+            "opts": ("-een", "--extract-every-n"),
+            "type": int,
+            "action": Slider,
+            "dest": "extract_every_n",
+            "min_max": (1, 100),
+            "default": 1,
+            "rounding": 1,
+            "group": "output",
+            "help": "Extract every 'nth' frame. This option will skip frames when extracting "
+                    "faces. For example a value of 1 will extract faces from every frame, a value "
+                    "of 10 will extract faces from every 10th frame."})
+        argument_list.append({
+            "opts": ("-sz", "--size"),
+            "type": int,
+            "action": Slider,
+            "min_max": (128, 512),
+            "default": 256,
+            "rounding": 64,
+            "group": "output",
+            "help": "The output size of extracted faces. Make sure that the model you intend to "
+                    "train supports your required size. This will only need to be changed for "
+                    "hi-res models."})
+        argument_list.append({
+            "opts": ("-si", "--save-interval"),
+            "dest": "save_interval",
+            "type": int,
+            "action": Slider,
+            "min_max": (0, 1000),
+            "rounding": 10,
+            "default": 0,
+            "group": "output",
+            "help": "Automatically save the alignments file after a set amount of frames. By "
+                    "default the alignments file is only saved at the end of the extraction "
+                    "process. NB: If extracting in 2 passes then the alignments file will only "
+                    "start to be saved out during the second pass. WARNING: Don't interrupt the "
+                    "script when writing the file because it might get corrupted. Set to 0 to "
+                    "turn off"})
+        argument_list.append({
+            "opts": ("-dl", "--debug-landmarks"),
+            "action": "store_true",
+            "dest": "debug_landmarks",
+            "group": "output",
+            "default": False,
+            "help": "Draw landmarks on the ouput faces for debugging purposes."})
+        argument_list.append({
+            "opts": ("-sp", "--singleprocess"),
+            "action": "store_true",
+            "default": False,
+            "backend": "nvidia",
+            "group": "settings",
+            "help": "Don't run extraction in parallel. Will run each part of the extraction "
+                    "process separately (one after the other) rather than all at the smae time. "
+                    "Useful if VRAM is at a premium."})
+        argument_list.append({
+            "opts": ("-s", "--skip-existing"),
+            "action": "store_true",
+            "dest": "skip_existing",
+            "group": "settings",
+            "default": False,
+            "help": "Skips frames that have already been extracted and exist in the alignments "
+                    "file"})
+        argument_list.append({
+            "opts": ("-sf", "--skip-existing-faces"),
+            "action": "store_true",
+            "dest": "skip_faces",
+            "group": "settings",
+            "default": False,
+            "help": "Skip frames that already have detected faces in the alignments file"})
         return argument_list
 
 
@@ -764,22 +768,24 @@ class ConvertArgs(ExtractConvertArgs):
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
         argument_list = []
-        argument_list.append({"opts": ("-ref", "--reference-video"),
-                              "action": FileFullPaths,
-                              "dest": "reference_video",
-                              "filetypes": "video",
-                              "type": str,
-                              "group": "data",
-                              "help": "Only required if converting from images to video. Provide "
-                                      "The original video that the source frames were extracted "
-                                      "from (for extracting the fps and audio)."})
-        argument_list.append({"opts": ("-m", "--model-dir"),
-                              "action": DirFullPaths,
-                              "dest": "model_dir",
-                              "required": True,
-                              "group": "data",
-                              "help": "Model directory. The directory containing the trained "
-                                      "model you wish to use for conversion."})
+        argument_list.append({
+            "opts": ("-ref", "--reference-video"),
+            "action": FileFullPaths,
+            "dest": "reference_video",
+            "filetypes": "video",
+            "type": str,
+            "group": "data",
+            "help": "Only required if converting from images to video. Provide The original video "
+                    "that the source frames were extracted from (for extracting the fps and "
+                    "audio)."})
+        argument_list.append({
+            "opts": ("-m", "--model-dir"),
+            "action": DirFullPaths,
+            "dest": "model_dir",
+            "required": True,
+            "group": "data",
+            "help": "Model directory. The directory containing the trained model you wish to use "
+                    "for conversion."})
         argument_list.append({
             "opts": ("-c", "--color-adjustment"),
             "action": Radio,
@@ -810,7 +816,7 @@ class ConvertArgs(ExtractConvertArgs):
             "action": Radio,
             "type": str.lower,
             "dest": "mask_type",
-            "choices": ["dfl_full", "components", "extended", "predicted"],
+            "choices": get_available_masks() + ["predicted"],
             "group": "plugins",
             "default": "predicted",
             "help": "R|Mask to use to replace faces. Blending of the masks can be adjusted in "
@@ -822,7 +828,8 @@ class ConvertArgs(ExtractConvertArgs):
                     "further up the forehead. May perform badly on difficult angles."
                     "\nL|facehull: Face cutout based on landmarks."
                     "\nL|predicted: The predicted mask generated from the model. If the model was "
-                    "not trained with a mask then this will fallback to components."
+                    "not trained with a mask then this will fallback to "
+                    "'{}'".format(get_default_mask()) +
                     "\nL|none: Don't use a mask."})
         argument_list.append({
             "opts": ("-sc", "--scaling"),
@@ -836,150 +843,148 @@ class ConvertArgs(ExtractConvertArgs):
                     "'/config/convert.ini' or 'Settings > Configure Convert Plugins':"
                     "\nL|sharpen: Perform sharpening on the final face."
                     "\nL|none: Don't perform any scaling operations."})
-        argument_list.append({"opts": ("-w", "--writer"),
-                              "action": Radio,
-                              "type": str,
-                              "choices": PluginLoader.get_available_convert_plugins("writer",
-                                                                                    False),
-                              "group": "plugins",
-                              "default": "opencv",
-                              "help": "R|The plugin to use to output the converted images. The "
-                                      "writers are configurable in '/config/convert.ini' or "
-                                      "'Settings > Configure Convert Plugins:'"
-                                      "\nL|ffmpeg: [video] Writes out the convert straight to "
-                                      "video. When the input is a series of images then the "
-                                      "'-ref' (--reference-video) parameter must be set."
-                                      "\nL|gif: [animated image] Create an animated gif."
-                                      "\nL|opencv: [images] The fastest image writer, but less "
-                                      "options and formats than other plugins."
-                                      "\nL|pillow: [images] Slower than opencv, but has more "
-                                      "options and supports more formats."})
-        argument_list.append({"opts": ("-osc", "--output-scale"),
-                              "dest": "output_scale",
-                              "action": Slider,
-                              "type": int,
-                              "default": 100,
-                              "min_max": (25, 400),
-                              "rounding": 1,
-                              "group": "Frame Processing",
-                              "help": "Scale the final output frames by this amount. 100%% will "
-                                      "output the frames at source dimensions. 50%% at half size "
-                                      "200%% at double size"})
-        argument_list.append({"opts": ("-fr", "--frame-ranges"),
-                              "nargs": "+",
-                              "type": str,
-                              "group": "Frame Processing",
-                              "help": "Frame ranges to apply transfer to e.g. For frames 10 to 50 "
-                                      "and 90 to 100 use --frame-ranges 10-50 90-100. Frames "
-                                      "falling outside of the selected range will be discarded "
-                                      "unless '-k' (--keep-unchanged) is selected. NB: If you are "
-                                      "converting from images, then the filenames must end with "
-                                      "the frame-number!"})
-        argument_list.append({"opts": ("-a", "--input-aligned-dir"),
-                              "action": DirFullPaths,
-                              "dest": "input_aligned_dir",
-                              "group": "Face Processing",
-                              "default": None,
-                              "help": "If you have not cleansed your alignments file, then you "
-                                      "can filter out faces by defining a folder here that "
-                                      "contains the faces extracted from your input files/video. "
-                                      "If this folder is defined, then only faces that exist "
-                                      "within your alignments file and also exist within the "
-                                      "specified folder will be converted. Leaving this blank "
-                                      "will convert all faces that exist within the alignments "
-                                      "file."})
-        argument_list.append({"opts": ("-n", "--nfilter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "nfilter",
-                              "nargs": "+",
-                              "default": None,
-                              "group": "Face Processing",
-                              "help": "Optionally filter out people who you do not wish to "
-                                      "process by passing in an image of that person. Should be a "
-                                      "front portrait with a single person in the image. Multiple "
-                                      "images can be added space separated. NB: Using face filter "
-                                      "will significantly decrease extraction speed and its "
-                                      "accuracy cannot be guaranteed."})
-        argument_list.append({"opts": ("-f", "--filter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "filter",
-                              "nargs": "+",
-                              "default": None,
-                              "group": "Face Processing",
-                              "help": "Optionally select people you wish to process by passing in "
-                                      "an image of that person. Should be a front portrait with a "
-                                      "single person in the image. Multiple images can be added "
-                                      "space separated. NB: Using face filter will significantly "
-                                      "decrease extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
-        argument_list.append({"opts": ("-l", "--ref_threshold"),
-                              "action": Slider,
-                              "min_max": (0.01, 0.99),
-                              "rounding": 2,
-                              "type": float,
-                              "dest": "ref_threshold",
-                              "default": 0.4,
-                              "group": "Face Processing",
-                              "help": "For use with the optional nfilter/filter files. Threshold "
-                                      "for positive face recognition. Lower values are stricter. "
-                                      "NB: Using face filter will significantly decrease "
-                                      "extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
-
-        argument_list.append({"opts": ("-j", "--jobs"),
-                              "dest": "jobs",
-                              "action": Slider,
-                              "group": "settings",
-                              "type": int,
-                              "default": 0,
-                              "min_max": (0, 40),
-                              "rounding": 1,
-                              "help": "The maximum number of parallel processes for performing "
-                                      "conversion. Converting images is system RAM heavy so it is "
-                                      "possible to run out of memory if you have a lot of "
-                                      "processes and not enough RAM to accommodate them all. "
-                                      "Setting this to 0 will use the maximum available. No "
-                                      "matter what you set this to, it will never attempt to use "
-                                      "more processes than are available on your system. If "
-                                      "singleprocess is enabled this setting will be ignored."})
-        argument_list.append({"opts": ("-g", "--gpus"),
-                              "type": int,
-                              "backend": "nvidia",
-                              "action": Slider,
-                              "min_max": (1, 10),
-                              "rounding": 1,
-                              "group": "settings",
-                              "default": 1,
-                              "help": "Number of GPUs to use for conversion"})
-        argument_list.append({"opts": ("-t", "--trainer"),
-                              "type": str.lower,
-                              "choices": PluginLoader.get_available_models(),
-                              "group": "settings",
-                              "help": "[LEGACY] This only needs to be selected if a legacy "
-                                      "model is being loaded or if there are multiple models in "
-                                      "the model folder"})
-        argument_list.append({"opts": ("-k", "--keep-unchanged"),
-                              "action": "store_true",
-                              "dest": "keep_unchanged",
-                              "group": "Frame Processing",
-                              "default": False,
-                              "help": "When used with --frame-ranges outputs the unchanged frames "
-                                      "that are not processed instead of discarding them."})
-        argument_list.append({"opts": ("-s", "--swap-model"),
-                              "action": "store_true",
-                              "dest": "swap_model",
-                              "group": "settings",
-                              "default": False,
-                              "help": "Swap the model. Instead converting from of A -> B, "
-                                      "converts B -> A"})
-        argument_list.append({"opts": ("-sp", "--singleprocess"),
-                              "action": "store_true",
-                              "group": "settings",
-                              "default": False,
-                              "help": "Disable multiprocessing. Slower but less resource "
-                                      "intensive."})
+        argument_list.append({
+            "opts": ("-w", "--writer"),
+            "action": Radio,
+            "type": str,
+            "choices": PluginLoader.get_available_convert_plugins("writer", False),
+            "group": "plugins",
+            "default": "opencv",
+            "help": "R|The plugin to use to output the converted images. The writers are "
+                    "configurable in '/config/convert.ini' or 'Settings > Configure Convert "
+                    "Plugins:'"
+                    "\nL|ffmpeg: [video] Writes out the convert straight to video. When the input "
+                    "is a series of images then the '-ref' (--reference-video) parameter must be "
+                    "set."
+                    "\nL|gif: [animated image] Create an animated gif."
+                    "\nL|opencv: [images] The fastest image writer, but less options and formats "
+                    "than other plugins."
+                    "\nL|pillow: [images] Slower than opencv, but has more options and supports "
+                    "more formats."})
+        argument_list.append({
+            "opts": ("-osc", "--output-scale"),
+            "dest": "output_scale",
+            "action": Slider,
+            "type": int,
+            "default": 100,
+            "min_max": (25, 400),
+            "rounding": 1,
+            "group": "Frame Processing",
+            "help": "Scale the final output frames by this amount. 100%% will output the frames "
+                    "at source dimensions. 50%% at half size 200%% at double size"})
+        argument_list.append({
+            "opts": ("-fr", "--frame-ranges"),
+            "nargs": "+",
+            "type": str,
+            "group": "Frame Processing",
+            "help": "Frame ranges to apply transfer to e.g. For frames 10 to 50 and 90 to 100 use "
+                    "--frame-ranges 10-50 90-100. Frames falling outside of the selected range "
+                    "will be discarded unless '-k' (--keep-unchanged) is selected. NB: If you are "
+                    "converting from images, then the filenames must end with the frame-number!"})
+        argument_list.append({
+            "opts": ("-a", "--input-aligned-dir"),
+            "action": DirFullPaths,
+            "dest": "input_aligned_dir",
+            "group": "Face Processing",
+            "default": None,
+            "help": "If you have not cleansed your alignments file, then you can filter out faces "
+                    "by defining a folder here that contains the faces extracted from your input "
+                    "files/video. If this folder is defined, then only faces that exist within "
+                    "your alignments file and also exist within the specified folder will be "
+                    "converted. Leaving this blank will convert all faces that exist within the "
+                    "alignments file."})
+        argument_list.append({
+            "opts": ("-n", "--nfilter"),
+            "action": FilesFullPaths,
+            "filetypes": "image",
+            "dest": "nfilter",
+            "nargs": "+",
+            "default": None,
+            "group": "Face Processing",
+            "help": "Optionally filter out people who you do not wish to process by passing in an "
+                    "image of that person. Should be a front portrait with a single person in the "
+                    "image. Multiple images can be added space separated. NB: Using face filter "
+                    "will significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-f", "--filter"),
+            "action": FilesFullPaths,
+            "filetypes": "image",
+            "dest": "filter",
+            "nargs": "+",
+            "default": None,
+            "group": "Face Processing",
+            "help": "Optionally select people you wish to process by passing in an image of that "
+                    "person. Should be a front portrait with a single person in the image. "
+                    "Multiple images can be added space separated. NB: Using face filter will "
+                    "significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-l", "--ref_threshold"),
+            "action": Slider,
+            "min_max": (0.01, 0.99),
+            "rounding": 2,
+            "type": float,
+            "dest": "ref_threshold",
+            "default": 0.4,
+            "group": "Face Processing",
+            "help": "For use with the optional nfilter/filter files. Threshold for positive face "
+                    "recognition. Lower values are stricter. NB: Using face filter will "
+                    "significantly decrease extraction speed and its accuracy cannot be "
+                    "guaranteed."})
+        argument_list.append({
+            "opts": ("-j", "--jobs"),
+            "dest": "jobs",
+            "action": Slider,
+            "group": "settings",
+            "type": int,
+            "default": 0,
+            "min_max": (0, 40),
+            "rounding": 1,
+            "help": "The maximum number of parallel processes for performing conversion. "
+                    "Converting images is system RAM heavy so it is possible to run out of memory "
+                    "if you have a lot of processes and not enough RAM to accommodate them all. "
+                    "Setting this to 0 will use the maximum available. No matter what you set "
+                    "this to, it will never attempt to use more processes than are available on "
+                    "your system. If singleprocess is enabled this setting will be ignored."})
+        argument_list.append({
+            "opts": ("-g", "--gpus"),
+            "type": int,
+            "backend": "nvidia",
+            "action": Slider,
+            "min_max": (1, 10),
+            "rounding": 1,
+            "group": "settings",
+            "default": 1,
+            "help": "Number of GPUs to use for conversion"})
+        argument_list.append({
+            "opts": ("-t", "--trainer"),
+            "type": str.lower,
+            "choices": PluginLoader.get_available_models(),
+            "group": "settings",
+            "help": "[LEGACY] This only needs to be selected if a legacy model is being loaded or "
+                    "if there are multiple models in the model folder"})
+        argument_list.append({
+            "opts": ("-k", "--keep-unchanged"),
+            "action": "store_true",
+            "dest": "keep_unchanged",
+            "group": "Frame Processing",
+            "default": False,
+            "help": "When used with --frame-ranges outputs the unchanged frames that are not "
+                    "processed instead of discarding them."})
+        argument_list.append({
+            "opts": ("-s", "--swap-model"),
+            "action": "store_true",
+            "dest": "swap_model",
+            "group": "settings",
+            "default": False,
+            "help": "Swap the model. Instead converting from of A -> B, converts B -> A"})
+        argument_list.append({
+            "opts": ("-sp", "--singleprocess"),
+            "action": "store_true",
+            "group": "settings",
+            "default": False,
+            "help": "Disable multiprocessing. Slower but less resource intensive."})
         return argument_list
 
 
@@ -1270,10 +1275,10 @@ class GuiArgs(FaceSwapArgs):
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
         argument_list = []
-        argument_list.append({"opts": ("-d", "--debug"),
-                              "action": "store_true",
-                              "dest": "debug",
-                              "default": False,
-                              "help": "Output to Shell console instead of "
-                                      "GUI console"})
+        argument_list.append({
+            "opts": ("-d", "--debug"),
+            "action": "store_true",
+            "dest": "debug",
+            "default": False,
+            "help": "Output to Shell console instead of GUI console"})
         return argument_list
diff --git a/lib/convert.py b/lib/convert.py
index 3f782f2..e9d6d1e 100644
--- a/lib/convert.py
+++ b/lib/convert.py
@@ -12,6 +12,7 @@ from plugins.plugin_loader import PluginLoader
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
+
 class Converter():
     """ Swap a source face with a target """
     def __init__(self, output_dir, output_size, output_has_mask,
@@ -141,7 +142,7 @@ class Converter():
                                            predicted["detected_faces"]):
             predicted_mask = new_face[:, :, -1] if new_face.shape[2] == 4 else None
             new_face = new_face[:, :, :3]
-            src_face = detected_face.reference_face
+            src_face = detected_face.reference_face[..., :3] / 255.0
             interpolator = detected_face.reference_interpolators[1]
 
             new_face = self.pre_warp_adjustments(src_face, new_face, detected_face, predicted_mask)
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 8f89d83..5d3a565 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -22,10 +22,8 @@ class DetectedFace():
 
     Parameters
     ----------
-    image: np.ndarray, optional
-        This is a generic image placeholder that should not be relied on to be holding a particular
-        image. It may hold the source frame that holds the face, a cropped face or a scaled image
-        depending on the method using this object.
+    image: numpy.ndarray, optional
+        Original frame that holds this face. Optional (not required if just storing coordinates)
     x: int
         The left most point (in pixels) of the face's bounding box as discovered in
         :mod:`plugins.extract.detect`
@@ -44,6 +42,33 @@ class DetectedFace():
     mask: dict
         The generated mask(s) for the face as generated in :mod:`plugins.extract.mask`. Must be a
         dict of {**name** (`str`): :class:`Mask`}.
+
+    Attributes
+    ----------
+    image: numpy.ndarray, optional
+        This is a generic image placeholder that should not be relied on to be holding a particular
+        image. It may hold the source frame that holds the face, a cropped face or a scaled image
+        depending on the method using this object.
+    x: int
+        The left most point (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    w: int
+        The width (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    y: int
+        The top most point (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    h: int
+        The height (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    landmarks_xy: list
+        The 68 point landmarks as discovered in :mod:`plugins.extract.align`.
+    mask: dict
+        The generated mask(s) for the face as generated in :mod:`plugins.extract.mask`. Is a
+        dict of {**name** (`str`): :class:`Mask`}.
+    hash: str
+        The hash of the face. This cannot be set until the file is saved due to image compression,
+        but will be set if loading data from :func:`from_alignment`
     """
     def __init__(self, image=None, x=None, w=None, y=None, h=None,
                  landmarks_xy=None, mask=None, filename=None):
@@ -61,11 +86,7 @@ class DetectedFace():
         self.h = h  # pylint:disable=invalid-name
         self.landmarks_xy = landmarks_xy
         self.mask = dict() if mask is None else mask
-        self.filename = filename
         self.hash = None
-        self.face = None
-        """ str: The hash of the face. This cannot be set until the file is saved due to image
-        compression, but will be set if loading data from :func:`from_alignment` """
 
         self.aligned = dict()
         self.feed = dict()
@@ -93,9 +114,9 @@ class DetectedFace():
         return self.y + self.h
 
     @property
-    def training_coverage(self):
-        """ The coverage ratio to add for training images """
-        return 1.0
+    def _extract_ratio(self):
+        """ float: The ratio of padding to add for training images """
+        return 0.375
 
     def add_mask(self, name, mask, affine_matrix, frame_dims, interpolator, storage_size=128):
         """ Add a :class:`Mask` to this detected face
@@ -172,7 +193,10 @@ class DetectedFace():
         self.w = alignment["w"]
         self.y = alignment["y"]
         self.h = alignment["h"]
-        self.landmarks_xy = alignment["landmarks_xy"]
+        landmarks = alignment["landmarks_xy"]
+        if not isinstance(landmarks, np.ndarray):
+            landmarks = np.array(landmarks, dtype="int32")
+        self.landmarks_xy = landmarks
         # Manual tool does not know the final hash so default to None
         self.hash = alignment.get("hash", None)
         # Manual tool and legacy alignments will not have a mask
@@ -182,7 +206,6 @@ class DetectedFace():
                 self.mask[name] = Mask()
                 self.mask[name].from_dict(mask_dict)
         if image is not None and image.any():
-            self.image = image
             self._image_to_face(image)
         logger.trace("Created from alignment: (x: %s, w: %s, y: %s. h: %s, "
                      "landmarks: %s, mask: %s)",
@@ -191,11 +214,11 @@ class DetectedFace():
     def _image_to_face(self, image):
         """ set self.image to be the cropped face from detected bounding box """
         logger.trace("Cropping face from image")
-        self.face = image[self.top: self.bottom,
-                          self.left: self.right]
+        self.image = image[self.top: self.bottom,
+                           self.left: self.right]
 
     # <<< Aligned Face methods and properties >>> #
-    def load_aligned(self, image, size=256, coverage_ratio=1.0, dtype=None):
+    def load_aligned(self, image, size=256, dtype=None):
         """ Align a face from a given image.
 
         Aligning a face is a relatively expensive task and is not required for all uses of
@@ -212,8 +235,6 @@ class DetectedFace():
             The image that contains the face to be aligned
         size: int
             The size of the output face in pixels
-        coverage_ratio: float
-            The metric determining the field of view of the returned face
         dtype: str, optional
             Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``
 
@@ -230,28 +251,25 @@ class DetectedFace():
             logger.trace("Skipping alignment calculation for already aligned face")
         else:
             logger.trace("Loading aligned face: (size: %s, dtype: %s)", size, dtype)
+            padding = int(size * self._extract_ratio) // 2
             self.aligned["size"] = size
-            self.aligned["padding"] = self._padding_from_coverage(size, coverage_ratio)
+            self.aligned["padding"] = padding
             self.aligned["matrix"] = get_align_mat(self)
             self.aligned["face"] = None
         if image is not None and self.aligned["face"] is None:
             logger.trace("Getting aligned face")
-            face = AlignerExtract().transform(
-                image,
-                self.aligned["matrix"],
-                size,
-                self.aligned["padding"])
+            face = AlignerExtract().transform(image, self.aligned["matrix"], size, padding)
             self.aligned["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded aligned face: %s", {k: str(v) if isinstance(v, np.ndarray) else v
                                                  for k, v in self.aligned.items()
                                                  if k != "face"})
 
-    @staticmethod
-    def _padding_from_coverage(size, coverage_ratio):
+    def _padding_from_coverage(self, size, coverage_ratio):
         """ Return the image padding for a face from coverage_ratio set against a
             pre-padded training image """
-        padding = int((size * (coverage_ratio - 0.625)) / 2)
+        adjusted_ratio = coverage_ratio - (1 - self._extract_ratio)
+        padding = round((size * adjusted_ratio) / 2)
         logger.trace(padding)
         return padding
 
@@ -281,11 +299,7 @@ class DetectedFace():
         self.feed["size"] = size
         self.feed["padding"] = self._padding_from_coverage(size, coverage_ratio)
         self.feed["matrix"] = get_align_mat(self)
-
-        face = AlignerExtract().transform(image,
-                                          self.feed["matrix"],
-                                          size,
-                                          self.feed["padding"])
+        face = AlignerExtract().transform(image, self.feed["matrix"], size, self.feed["padding"])
         self.feed["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded feed face. (face_shape: %s, matrix: %s)",
@@ -448,7 +462,7 @@ class DetectedFace():
 
     @property
     def reference_matrix(self):
-        """ numpy.ndarray: The adjusted matrix face sized for refence against a face coming out of
+        """ numpy.ndarray: The adjusted matrix face sized for reference against a face coming out of
          a model. Only available after :func:`load_reference_face` has been called, otherwise
          returns ``None``"""
         if not self.reference:
@@ -494,7 +508,7 @@ class Mask():
         self._mask = None
         self._affine_matrix = None
         self._frame_dims = None
-        self._intepolator = None
+        self._interpolator = None
 
     @property
     def mask(self):
@@ -510,7 +524,7 @@ class Mask():
                               self._affine_matrix,
                               self._frame_dims,
                               frame,
-                              flags=cv2.WARP_INVERSE_MAP | self._intepolator,
+                              flags=cv2.WARP_INVERSE_MAP | self._interpolator,
                               borderMode=cv2.BORDER_CONSTANT)
         logger.trace("mask shape: %s, mask dtype: %s, mask min: %s, mask max: %s",
                      mask.shape, mask.dtype, mask.min(), mask.max())
@@ -538,7 +552,7 @@ class Mask():
                      mask.min(), mask.max(), affine_matrix, frame_dims, interpolator)
         self._affine_matrix = self._adjust_affine_matrix(mask.shape[0], affine_matrix)
         self._frame_dims = frame_dims
-        self._intepolator = interpolator
+        self._interpolator = interpolator
         mask = (cv2.resize(mask,
                            (self.stored_size, self.stored_size),
                            interpolation=cv2.INTER_AREA) * 255.0).astype("uint8")
@@ -574,10 +588,10 @@ class Mask():
         -------
         dict:
             The :class:`Mask` for saving to an alignments file. Contains the keys ``mask``,
-            ``affine_matrix``, ``frame_dims``, ``intepolator``, ``stored_size``
+            ``affine_matrix``, ``frame_dims``, ``interpolator``, ``stored_size``
         """
         retval = dict()
-        for key in ("mask", "affine_matrix", "frame_dims", "intepolator", "stored_size"):
+        for key in ("mask", "affine_matrix", "frame_dims", "interpolator", "stored_size"):
             retval[key] = getattr(self, self._attr_name(key))
         logger.trace({k: v if k != "mask" else type(v) for k, v in retval.items()})
         return retval
@@ -589,11 +603,11 @@ class Mask():
         ----------
         mask_dict: dict
             A dictionary stored in an alignments file containing the keys ``mask``,
-            ``affine_matrix``, ``frame_dims``, ``intepolator``, ``stored_size``
+            ``affine_matrix``, ``frame_dims``, ``interpolator``, ``stored_size``
         """
-        for key in ("mask", "affine_matrix", "frame_dims", "intepolator", "stored_size"):
+        for key in ("mask", "affine_matrix", "frame_dims", "interpolator", "stored_size"):
             setattr(self, self._attr_name(key), mask_dict[key])
-            logger.trace("{} - {}", key, mask_dict[key] if key != "mask" else type(mask_dict[key]))
+            logger.trace("%s - %s", key, mask_dict[key] if key != "mask" else type(mask_dict[key]))
 
     @staticmethod
     def _attr_name(dict_key):
diff --git a/lib/serializer.py b/lib/serializer.py
index eca9222..db6b85d 100644
--- a/lib/serializer.py
+++ b/lib/serializer.py
@@ -227,7 +227,7 @@ class _PickleSerializer(Serializer):
         return pickle.loads(data)
 
 
-class _NPYSerializer(Serializer):  # pylint:disable=abstract-method
+class _NPYSerializer(Serializer):
     """ NPY Serializer """
     def __init__(self):
         super().__init__()
@@ -330,7 +330,7 @@ def get_serializer_from_filename(filename):
 
     if extension == ".json":
         retval = _JSONSerializer()
-    elif extension == ".p":
+    elif extension in (".p", ".pickle"):
         retval = _PickleSerializer()
     elif extension == ".npy":
         retval = _NPYSerializer()
diff --git a/plugins/extract/_base.py b/plugins/extract/_base.py
index 2933375..86d4bcc 100644
--- a/plugins/extract/_base.py
+++ b/plugins/extract/_base.py
@@ -18,12 +18,12 @@ from ._config import Config
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
-# TODO Cpu mode
+# TODO CPU mode
 # TODO Run with warnings mode
 
 
 def _get_config(plugin_name, configfile=None):
-    """ Return the config for the requested model
+    """ Return the configuration for the requested model
 
     Parameters
     ----------
@@ -31,12 +31,12 @@ def _get_config(plugin_name, configfile=None):
         The module name of the child plugin.
     configfile: str, optional
         Path to a :file:`./config/<plugin_type>.ini` file for this plugin. Default: use system
-        config.
+        configuration.
 
     Returns
     -------
     config_dict, dict
-       A dictionary of configuration items from the config file
+       A dictionary of configuration items from the configuration file
     """
     return Config(plugin_name, configfile=configfile).config_dict
 
@@ -190,7 +190,7 @@ class Extractor():
         For Detect:
             the expected output for the ``prediction`` key of the :attr:`batch` dict should be a
             ``list`` of :attr:`batchsize` of detected face points. These points should be either
-            a ``list``, ``tuple`` or ``numpy.array`` with the first 4 items being the `left`,
+            a ``list``, ``tuple`` or ``numpy.ndarray`` with the first 4 items being the `left`,
             `top`, `right`, `bottom` points, in that order
         """
         raise NotImplementedError
@@ -209,18 +209,18 @@ class Extractor():
         -----
         For Align:
             The key ``landmarks`` must be returned in the :attr:`batch` ``dict`` from this method.
-            This should be a ``list`` or ``numpy.array`` of :attr:`batchsize` containing a
-            ``list``, ``tuple`` or ``numpy.array`` of `(x, y)` co-ords of the 68 point landmarks
-            as calculated from the :attr:`model`.
+            This should be a ``list`` or ``numpy.ndarray`` of :attr:`batchsize` containing a
+            ``list``, ``tuple`` or ``numpy.ndarray`` of `(x, y)` coordinates of the 68 point
+            landmarks as calculated from the :attr:`model`.
         """
         raise NotImplementedError
 
     def _predict(self, batch):
         """ **Override method** (at `<plugin_type>` level)
 
-        This method is overridable at the `<plugin_type>` level (ie.
+        This method should be overridden at the `<plugin_type>` level (IE.
         ``plugins.extract.detect._base`` or ``plugins.extract.align._base``) and should not
-        be overriden within plugins themselves.
+        be overridden within plugins themselves.
 
         It acts as a wrapper for the plugin's ``self.predict`` method and handles any
         predict processing that is consistent for all plugins within the `plugin_type`
@@ -235,9 +235,9 @@ class Extractor():
     def finalize(self, batch):
         """ **Override method** (at `<plugin_type>` level)
 
-        This method is overridable at the `<plugin_type>` level (ie.
+        This method should be overridden at the `<plugin_type>` level (IE.
         :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
-        be overriden within plugins themselves.
+        be overridden within plugins themselves.
 
         Handles consistent finalization for all plugins that exist within that plugin type. Its
         input is always the output from :func:`process_output()`
@@ -252,9 +252,9 @@ class Extractor():
     def get_batch(self, queue):
         """ **Override method** (at `<plugin_type>` level)
 
-        This method is overridable at the `<plugin_type>` level (ie.
+        This method should be overridden at the `<plugin_type>` level (IE.
         :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
-        be overriden within plugins themselves.
+        be overridden within plugins themselves.
 
         Get items from the queue in batches of :attr:`batchsize`
 
@@ -316,13 +316,13 @@ class Extractor():
 
     # <<< PLUGIN INITIALIZATION >>> #
     def initialize(self, *args, **kwargs):
-        """ Inititalize the extractor plugin
+        """ Initialize the extractor plugin
 
             Should be called from :mod:`~plugins.extract.pipeline`
         """
         logger.debug("initialize %s: (args: %s, kwargs: %s)",
                      self.__class__.__name__, args, kwargs)
-        logger.info("Initializing %s in %s phase...", self.name, self._plugin_type)
+        logger.info("Initializing %s (%s)...", self.name, self._plugin_type.title())
         self.queue_size = 1
         self._add_queues(kwargs["in_queue"], kwargs["out_queue"], ["predict", "post"])
         self._compile_threads()
@@ -341,11 +341,11 @@ class Extractor():
                 raise FaceswapError(msg) from err
             raise err
         logger.info("Initialized %s (%s) with batchsize of %s",
-                    self.name, self._plugin_type, self.batchsize)
+                    self.name, self._plugin_type.title(), self.batchsize)
 
     def _add_queues(self, in_queue, out_queue, queues):
         """ Add the queues
-            in_queue and out_queue should be pre-created queue manager queues
+            in_queue and out_queue should be previously created queue manager queues.
             queues should be a list of queue names """
         self._queues["in"] = in_queue
         self._queues["out"] = out_queue
@@ -440,14 +440,14 @@ class Extractor():
 
     # <<< MISC UTILITY METHODS >>> #
     def _convert_color(self, image):
-        """ Convert the image to the correct color format """
+        """ Convert the image to the correct color format and strip alpha channel """
         logger.trace("Converting image to color format: %s", self.colorformat)
         if self.colorformat == "RGB":
-            cvt_image = image[:, :, ::-1].copy()
+            cvt_image = image[..., 2::-1].copy()
         elif self.colorformat == "GRAY":
-            cvt_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)  # pylint:disable=no-member
+            cvt_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)
         else:
-            cvt_image = image.copy()
+            cvt_image = image[..., :3].copy()
         return cvt_image
 
     @staticmethod
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 4c2d9b0..903afa3 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -75,7 +75,7 @@ class Aligner(Extractor):
         Items are returned from the ``queue`` in batches of
         :attr:`~plugins.extract._base.Extractor.batchsize`
 
-        To ensure consistent batchsizes for aligner the items are split into separate items for
+        To ensure consistent batch sizes for aligner the items are split into separate items for
         each :class:`lib.faces_detect.DetectedFace` object.
 
         Remember to put ``'EOF'`` to the out queue after processing
@@ -183,14 +183,10 @@ class Aligner(Extractor):
 
         """
 
-        generator = zip(batch["detected_faces"],
-                        batch["filename"],
-                        batch["image"],
-                        batch["landmarks"])
-        for face, filename, image, landmarks in generator:
-            face.landmarks_xy = [(int(round(pt[0])), int(round(pt[1]))) for pt in landmarks]
-            face.image = image
-            face.filename = filename
+        for face, landmarks in zip(batch["detected_faces"], batch["landmarks"]):
+            if not isinstance(landmarks, np.ndarray):
+                landmarks = np.array(landmarks)
+            face.landmarks_xy = np.rint(landmarks).astype("int32")
         self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
         logger.trace("Item out: %s", {key: val
                                       for key, val in batch.items()
@@ -218,7 +214,7 @@ class Aligner(Extractor):
     def _normalize_faces(self, faces):
         """ Normalizes the face for feeding into model
 
-        The normalization method is dictated by the cli argument:
+        The normalization method is dictated by the command line argument:
             -nh (--normalization)
         """
         if self.normalize_method is None:
@@ -243,13 +239,13 @@ class Aligner(Extractor):
     def _normalize_hist(face):
         """ Equalize the RGB histogram channels """
         for chan in range(3):
-            face[:, :, chan] = cv2.equalizeHist(face[:, :, chan])  # pylint: disable=no-member
+            face[:, :, chan] = cv2.equalizeHist(face[:, :, chan])
         return face
 
     @staticmethod
     def _normalize_clahe(face):
         """ Perform Contrast Limited Adaptive Histogram Equalization """
-        clahe = cv2.createCLAHE(clipLimit=2.0,  # pylint: disable=no-member
+        clahe = cv2.createCLAHE(clipLimit=2.0,
                                 tileGridSize=(4, 4))
         for chan in range(3):
             face[:, :, chan] = clahe.apply(face[:, :, chan])
diff --git a/plugins/extract/align/cv2_dnn.py b/plugins/extract/align/cv2_dnn.py
index 577a964..a28321b 100644
--- a/plugins/extract/align/cv2_dnn.py
+++ b/plugins/extract/align/cv2_dnn.py
@@ -140,7 +140,7 @@ class Align(Aligner):
 
     @staticmethod
     def pad_image(box, image):
-        """Pad image if facebox falls outside of boundaries """
+        """Pad image if face-box falls outside of boundaries """
         width, height = image.shape[:2]
         pad_l = 1 - box[0] if box[0] < 0 else 0
         pad_t = 1 - box[1] if box[1] < 0 else 0
@@ -178,6 +178,5 @@ class Align(Aligner):
             points *= (roi[2] - roi[0])
             points[:, 0] += roi[0]
             points[:, 1] += roi[1]
-            landmarks = np.rint(points).astype("uint").tolist()
-            batch.setdefault("landmarks", []).append(landmarks)
+            batch.setdefault("landmarks", []).append(points)
         logger.trace("Predicted Landmarks: %s", batch["landmarks"])
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index 21c3183..b68a885 100644
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -85,7 +85,7 @@ class Detector(Extractor):
 
         >>> {'filename': [<filenames of source frames>],
         >>>  'image': [<source images>],
-        >>>  'scaled_image': <np.array of images standardized for prediction>,
+        >>>  'scaled_image': <numpy.ndarray of images standardized for prediction>,
         >>>  'scale': [<scaling factors for each image>],
         >>>  'pad': [<padding for each image>],
         >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
@@ -228,7 +228,7 @@ class Detector(Extractor):
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
     def _compile_detection_image(self, input_image):
         """ Compile the detection image for feeding into the model"""
-        image = self._convert_color(input_image[..., :3])
+        image = self._convert_color(input_image)
 
         image_size = image.shape[:2]
         scale = self._set_scale(image_size)
diff --git a/plugins/extract/mask/_base.py b/plugins/extract/mask/_base.py
index 0f23296..6dca64d 100644
--- a/plugins/extract/mask/_base.py
+++ b/plugins/extract/mask/_base.py
@@ -60,9 +60,9 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         super().__init__(git_model_id,
                          model_filename,
                          configfile=configfile)
-        self.input_size = 256  # Overide for model specific input_size
-        self.blur_kernel = 5  # Overide for model specific blur_kernel size
-        self.coverage_ratio = 1.0  # Overide for model specific coverage_ratio
+        self.input_size = 256  # Override for model specific input_size
+        self.blur_kernel = 5  # Override for model specific blur_kernel size
+        self.coverage_ratio = 1.0  # Override for model specific coverage_ratio
 
         self._plugin_type = "mask"
         self._storage_name = self.__module__.split(".")[-1].replace("_", "-")
@@ -78,7 +78,7 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         Items are returned from the ``queue`` in batches of
         :attr:`~plugins.extract._base.Extractor.batchsize`
 
-        To ensure consistent batchsizes for masker the items are split into separate items for
+        To ensure consistent batch sizes for masker the items are split into separate items for
         each :class:`lib.faces_detect.DetectedFace` object.
 
         Remember to put ``'EOF'`` to the out queue after processing
@@ -117,6 +117,7 @@ class Masker(Extractor):  # pylint:disable=abstract-method
                 self._queues["out"].put(item)
                 continue
             for f_idx, face in enumerate(item["detected_faces"]):
+                face.image = self._convert_color(item["image"])
                 face.load_feed_face(face.image,
                                     size=self.input_size,
                                     coverage_ratio=1.0,
@@ -197,7 +198,6 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         #    predicted = batch["prediction"]
         # predicted[predicted < 0.04] = 0.0
         # predicted[predicted > 0.96] = 1.0
-        # TODO Convert landmarks_xy to numpy arrays
         for mask, face in zip(batch["prediction"], batch["detected_faces"]):
             face.add_mask(self._storage_name,
                           mask,
@@ -205,7 +205,7 @@ class Masker(Extractor):  # pylint:disable=abstract-method
                           (face.image.shape[1], face.image.shape[0]),
                           face.feed_interpolators[1],
                           storage_size=self._storage_size)
-            face.feed = None
+            face.feed = dict()
 
         self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
         logger.trace("Item out: %s", {key: val
diff --git a/plugins/extract/mask/unet_dfl.py b/plugins/extract/mask/unet_dfl.py
index 257e703..74206e3 100644
--- a/plugins/extract/mask/unet_dfl.py
+++ b/plugins/extract/mask/unet_dfl.py
@@ -1,5 +1,17 @@
 #!/usr/bin/env python3
-""" UNET DFL face mask plugin """
+""" UNET DFL face mask plugin
+
+Architecture and Pre-Trained Model based on...
+TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation
+https://arxiv.org/abs/1801.05746
+https://github.com/ternaus/TernausNet
+
+Source Implementation and fine-tune training....
+https://github.com/iperov/DeepFaceLab/blob/master/nnlib/TernausNet.py
+
+Model file sourced from...
+https://github.com/iperov/DeepFaceLab/blob/master/nnlib/FANSeg_256_full_face.h5
+"""
 
 import numpy as np
 from lib.model.session import KSession
@@ -15,8 +27,8 @@ class Mask(Masker):
         self.name = "U-Net"
         self.input_size = 256
         self.vram = 3440
-        self.vram_warnings = 1024  # TODO determine
-        self.vram_per_batch = 64  # TODO determine
+        self.vram_warnings = 256
+        self.vram_per_batch = 48
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/mask/vgg_clear.py b/plugins/extract/mask/vgg_clear.py
index 07b3385..c8828b8 100644
--- a/plugins/extract/mask/vgg_clear.py
+++ b/plugins/extract/mask/vgg_clear.py
@@ -1,5 +1,18 @@
 #!/usr/bin/env python3
-""" VGG Clear face mask plugin """
+""" VGG Clear face mask plugin
+
+Architecture and Pre-Trained Model based on...
+On Face Segmentation, Face Swapping, and Face Perception
+https://arxiv.org/abs/1704.06729
+
+Source Implementation...
+https://github.com/YuvalNirkin/face_segmentation
+
+Model file sourced from...
+https://github.com/YuvalNirkin/face_segmentation/releases/download/1.1/face_seg_fcn8s_300_no_aug.zip
+
+Caffe model reimplemented in Keras by Kyle Vrooman
+"""
 
 import numpy as np
 from lib.model.session import KSession
@@ -14,9 +27,9 @@ class Mask(Masker):
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
         self.name = "VGG Clear"
         self.input_size = 300
-        self.vram = 2000  # TODO determine
-        self.vram_warnings = 1024  # TODO determine
-        self.vram_per_batch = 64  # TODO determine
+        self.vram = 3104
+        self.vram_warnings = 1088  # at BS 1. OOMs at higher batchsizes
+        self.vram_per_batch = 96
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/mask/vgg_obstructed.py b/plugins/extract/mask/vgg_obstructed.py
index 5f55226..8731975 100644
--- a/plugins/extract/mask/vgg_obstructed.py
+++ b/plugins/extract/mask/vgg_obstructed.py
@@ -1,5 +1,18 @@
 #!/usr/bin/env python3
-""" VGG Obstructed face mask plugin """
+""" VGG Obstructed face mask plugin
+
+Architecture and Pre-Trained Model based on...
+On Face Segmentation, Face Swapping, and Face Perception
+https://arxiv.org/abs/1704.06729
+
+Source Implementation...
+https://github.com/YuvalNirkin/face_segmentation
+
+Model file sourced from...
+https://github.com/YuvalNirkin/face_segmentation/releases/download/1.0/face_seg_fcn8s.zip
+
+Caffe model reimplemented in Keras by Kyle Vrooman
+"""
 
 import numpy as np
 from lib.model.session import KSession
@@ -14,9 +27,9 @@ class Mask(Masker):
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
         self.name = "VGG Obstructed"
         self.input_size = 500
-        self.vram = 3000  # TODO determine
-        self.vram_warnings = 1024  # TODO determine
-        self.vram_per_batch = 64  # TODO determine
+        self.vram = 3936
+        self.vram_warnings = 1088  # at BS 1. OOMs at higher batchsizes
+        self.vram_per_batch = 208
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index 4cc28c9..13e33cf 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -6,7 +6,7 @@ Tensorflow does not like to release GPU VRAM, so parallel plugins need to be man
 together.
 
 This module sets up a pipeline for the extraction workflow, loading align and detect plugins
-either in parallal or in series, giving easy access to input and output.
+either in parallel or in series, giving easy access to input and output.
 
  """
 
@@ -65,12 +65,13 @@ class Extractor():
                      "normalize_method: %s)",
                      self.__class__.__name__, detector, aligner, masker, configfile,
                      multiprocess, rotate_images, min_size, normalize_method)
-        self.phase = "detect"
+        self._flow = ["detect", "align", "mask"]
+        self.phase = self._flow[0]
         self._queue_size = 32
         self._vram_buffer = 320  # Leave a buffer for VRAM allocation
-        self._detector = self._load_detector(detector, rotate_images, min_size, configfile)
-        self._aligner = self._load_aligner(aligner, configfile, normalize_method)
-        self._masker = self._load_masker(masker, configfile)
+        self._detect = self._load_detect(detector, rotate_images, min_size, configfile)
+        self._align = self._load_align(aligner, configfile, normalize_method)
+        self._mask = self._load_mask(masker, configfile)
         self._is_parallel = self._set_parallel_processing(multiprocess)
         self._set_extractor_batchsize()
         self._queues = self._add_queues()
@@ -86,19 +87,16 @@ class Extractor():
         For detect/single phase operations:
 
         >>> {'filename': <path to the source image that is to be extracted from>,
-        >>>  'image': <the source image as a numpy.array in BGR color format>}
+        >>>  'image': <the source image as a numpy.ndarray in BGR color format>}
 
         For align (2nd pass operations):
 
         >>> {'filename': <path to the source image that is to be extracted from>,
-        >>>  'image': <the source image as a numpy.array in BGR color format>,
+        >>>  'image': <the source image as a numpy.ndarray in BGR color format>,
         >>>  'detected_faces: [<list of DetectedFace objects as generated from detect>]}
 
         """
-        qname_dict = dict(detect="extract_detect_in",
-                          align="extract_align_in",
-                          mask="extract_mask_in")
-        qname = "extract_detect_in" if self._is_parallel else qname_dict[self.phase]
+        qname = "extract_{}_in".format(self.phase)
         retval = self._queues[qname]
         logger.trace("%s: %s", qname, retval)
         return retval
@@ -116,13 +114,13 @@ class Extractor():
         >>> for phase in extractor.passes:
         >>>     if phase == 1:
         >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": np.array(image)})
+        >>>                                    "image": numpy.array(image)})
         >>>     else:
         >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": np.array(image),
+        >>>                                    "image": numpy.array(image),
         >>>                                    "detected_faces": [<DetectedFace objects]})
         """
-        retval = 1 if self._is_parallel else 3
+        retval = 1 if self._is_parallel else len(self._flow)
         logger.trace(retval)
         return retval
 
@@ -141,24 +139,24 @@ class Extractor():
         >>>     else:
         >>>         <do intermediate processing>
         >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": np.array(image),
+        >>>                                    "image": numpy.array(image),
         >>>                                    "detected_faces": [<DetectedFace objects]})
         """
-        retval = self._is_parallel or self.phase == "mask"
+        retval = self._is_parallel or self.phase == self._final_phase
         logger.trace(retval)
         return retval
 
     def set_batchsize(self, plugin_type, batchsize):
-        """ Set the batchsize of a given :attr:`plugin_type` to the given :attr:`batchsize`.
+        """ Set the batch size of a given :attr:`plugin_type` to the given :attr:`batchsize`.
 
-        This should be set prior to :func:`launch` if the batchsize is to be manually overriden
+        This should be set prior to :func:`launch` if the batch size is to be manually overridden
 
         Parameters
         ----------
         plugin_type: {'aligner', 'detector'}
-            The plugin_type to be overriden
+            The plugin_type to be overridden
         batchsize: int
-            The batchsize to use for this plugin type
+            The batch size to use for this plugin type
         """
         logger.debug("Overriding batchsize for plugin_type: %s to: %s", plugin_type, batchsize)
         plugin = getattr(self, "_{}".format(plugin_type))
@@ -179,15 +177,10 @@ class Extractor():
         """
 
         if self._is_parallel:
-            self._launch_detector()
-            self._launch_aligner()
-            self._launch_masker()
-        elif self.phase == "detect":
-            self._launch_detector()
-        elif self.phase == "align":
-            self._launch_aligner()
-        elif self.phase == "mask":
-            self._launch_masker()
+            for phase in self._flow:
+                self._launch_plugin(phase)
+        else:
+            self._launch_plugin(self.phase)
 
     def detected_faces(self):
         """ Generator that returns results, frame by frame from the extraction pipeline
@@ -198,7 +191,7 @@ class Extractor():
         Yields
         ------
         faces: dict
-            regardless of phase, the returned dictinary will contain, exclusively, ``filename``:
+            regardless of phase, the returned dictionary will contain, exclusively, ``filename``:
             the filename of the source image, ``image``: the ``numpy.array`` of the source image
             in BGR color format, ``detected_faces``: a list of
             :class:`~lib.faces_detect.Detected_Face` objects.
@@ -223,55 +216,78 @@ class Extractor():
                     break
             except QueueEmpty:
                 continue
-
             yield faces
+
         self._join_threads()
         if self.final_pass:
             # Cleanup queues
-            for q_name in self._queues.keys():
+            for q_name in self._queues:
                 queue_manager.del_queue(q_name)
             logger.debug("Detection Complete")
         else:
-            self.phase = "align" if self.phase == "detect" else "mask"
+            self.phase = self._next_phase
             logger.debug("Switching to %s phase", self.phase)
 
     # <<< INTERNAL METHODS >>> #
+    @property
+    def _next_phase(self):
+        """ Return the next phase from the flow list """
+        retval = self._flow[self._flow.index(self.phase) + 1]
+        logger.trace(retval)
+        return retval
+
+    @property
+    def _final_phase(self):
+        """ Return the final phase from the flow list """
+        retval = self._flow[-1]
+        logger.trace(retval)
+        return retval
+
+    @property
+    def _total_vram_required(self):
+        """ Return vram required for all phases plus the buffer """
+        retval = sum([getattr(self, "_{}".format(p)).vram for p in self._flow]) + self._vram_buffer
+        logger.trace(retval)
+        return retval
+
     @property
     def _output_queue(self):
         """ Return the correct output queue depending on the current phase """
-        qname_dict = dict(detect="extract_align_in",
-                          align="extract_mask_in",
-                          mask="extract_mask_out")
-        qname = "extract_mask_out" if self.final_pass else qname_dict[self.phase]
+        if self.final_pass:
+            qname = "extract_{}_out".format(self._final_phase)
+        else:
+            qname = "extract_{}_in".format(self._next_phase)
         retval = self._queues[qname]
         logger.trace("%s: %s", qname, retval)
         return retval
 
+    @property
+    def _all_plugins(self):
+        """ Return list of all plugin objects in this pipeline """
+        retval = [getattr(self, "_{}".format(phase)) for phase in self._flow]
+        logger.trace("All Plugins: %s", retval)
+        return retval
+
     @property
     def _active_plugins(self):
         """ Return the plugins that are currently active based on pass """
         if self.passes == 1:
-            retval = [self._detector, self._aligner, self._masker]
-        elif self.passes == 3 and self.phase == 'detect':
-            retval = [self._detector]
-        elif self.passes == 3 and self.phase == 'align':
-            retval = [self._aligner]
-        elif self.passes == 3 and self.phase == 'mask':
-            retval = [self._masker]
+            retval = self._all_plugins
         else:
-            retval = [None]
+            retval = [getattr(self, "_{}".format(self.phase))]
         logger.trace("Active plugins: %s", retval)
         return retval
 
     def _add_queues(self):
         """ Add the required processing queues to Queue Manager """
         queues = dict()
-        tasks = ["extract_detect_in", "extract_align_in", "extract_mask_in", "extract_mask_out"]
+        tasks = ["extract_{}_in".format(phase) for phase in self._flow]
+        tasks.append("extract_{}_out".format(self._final_phase))
         for task in tasks:
             # Limit queue size to avoid stacking ram
             self._queue_size = 32
-            if task == "extract_detect_in" or (not self._is_parallel
-                                               and task == "extract_align_in"):
+            if task == "extract_{}_in".format(self._flow[0]) or (not self._is_parallel
+                                                                 and not task.endswith("_out")):
                 self._queue_size = 64
             queue_manager.add_queue(task, maxsize=self._queue_size)
             queues[task] = queue_manager.get_queue(task)
@@ -291,20 +307,16 @@ class Extractor():
             return True
 
         if get_backend() == "amd":
-            logger.debug("Parallel processing discabled by amd")
+            logger.debug("Parallel processing disabled by amd")
             return False
 
-        vram_required = (self._detector.vram +
-                         self._aligner.vram +
-                         self._masker.vram +
-                         self._vram_buffer)
         stats = gpu_stats.get_card_most_free()
         vram_free = int(stats["free"])
         logger.verbose("%s - %sMB free of %sMB",
                        stats["device"],
                        vram_free,
                        int(stats["total"]))
-        if vram_free <= vram_required:
+        if vram_free <= self._total_vram_required:
             logger.warning("Not enough free VRAM for parallel processing. "
                            "Switching to serial")
             return False
@@ -312,7 +324,16 @@ class Extractor():
 
     # << INTERNAL PLUGIN HANDLING >> #
     @staticmethod
-    def _load_detector(detector, rotation, min_size, configfile):
+    def _load_align(aligner, configfile, normalize_method):
+        """ Set global arguments and load aligner plugin """
+        aligner_name = aligner.replace("-", "_").lower()
+        logger.debug("Loading Aligner: '%s'", aligner_name)
+        aligner = PluginLoader.get_aligner(aligner_name)(configfile=configfile,
+                                                         normalize_method=normalize_method)
+        return aligner
+
+    @staticmethod
+    def _load_detect(detector, rotation, min_size, configfile):
         """ Set global arguments and load detector plugin """
         detector_name = detector.replace("-", "_").lower()
         logger.debug("Loading Detector: '%s'", detector_name)
@@ -322,81 +343,59 @@ class Extractor():
         return detector
 
     @staticmethod
-    def _load_aligner(aligner, configfile, normalize_method):
-        """ Set global arguments and load aligner plugin """
-        aligner_name = aligner.replace("-", "_").lower()
-        logger.debug("Loading Aligner: '%s'", aligner_name)
-        aligner = PluginLoader.get_aligner(aligner_name)(configfile=configfile,
-                                                         normalize_method=normalize_method)
-        return aligner
-
-    @staticmethod
-    def _load_masker(masker, configfile):
+    def _load_mask(masker, configfile):
         """ Set global arguments and load masker plugin """
         masker_name = masker.replace("-", "_").lower()
         logger.debug("Loading Masker: '%s'", masker_name)
         masker = PluginLoader.get_masker(masker_name)(configfile=configfile)
         return masker
 
-    def _launch_detector(self):
-        """ Launch the face detector """
-        logger.debug("Launching Detector")
-        kwargs = dict(in_queue=self._queues["extract_detect_in"],
-                      out_queue=self._queues["extract_align_in"])
-        self._detector.initialize(**kwargs)
-        self._detector.start()
-        logger.debug("Launched Detector")
-
-    def _launch_aligner(self):
-        """ Launch the face aligner """
-        logger.debug("Launching Aligner")
-        kwargs = dict(in_queue=self._queues["extract_align_in"],
-                      out_queue=self._queues["extract_mask_in"])
-        self._aligner.initialize(**kwargs)
-        self._aligner.start()
-        logger.debug("Launched Aligner")
-
-    def _launch_masker(self):
-        """ Launch the face masker """
-        logger.debug("Launching Masker")
-        kwargs = dict(in_queue=self._queues["extract_mask_in"],
-                      out_queue=self._queues["extract_mask_out"])
-        self._masker.initialize(**kwargs)
-        self._masker.start()
-        logger.debug("Launched Masker")
+    def _launch_plugin(self, phase):
+        """ Launch an extraction plugin """
+        logger.debug("Launching %s plugin", phase)
+        in_qname = "extract_{}_in".format(phase)
+        if phase == self._final_phase:
+            out_qname = "extract_{}_out".format(self._final_phase)
+        else:
+            next_phase = self._flow[self._flow.index(phase) + 1]
+            out_qname = "extract_{}_in".format(next_phase)
+        logger.debug("in_qname: %s, out_qname: %s", in_qname, out_qname)
+        kwargs = dict(in_queue=self._queues[in_qname], out_queue=self._queues[out_qname])
+
+        plugin = getattr(self, "_{}".format(phase))
+        plugin.initialize(**kwargs)
+        plugin.start()
+        logger.debug("Launched %s plugin", phase)
 
     def _set_extractor_batchsize(self):
         """
-        Sets the batchsize of the requested plugins based on their vram and
-        vram_per_batch_requirements if the the configured batchsize requires more
+        Sets the batch size of the requested plugins based on their vram and
+        vram_per_batch_requirements if the the configured batch size requires more
         vram than is available. Nvidia only.
         """
         if get_backend() != "nvidia":
             logger.debug("Backend is not Nvidia. Not updating batchsize requirements")
             return
-        if self._detector.vram == 0 and self._aligner.vram == 0 and self._masker.vram == 0:
-            logger.debug("Either detector, aligner or masker have no VRAM requirements. Not "
-                         "updating batchsize requirements.")
+        if sum([plugin.vram for plugin in self._all_plugins]) == 0:
+            logger.debug("No plugins use VRAM. Not updating batchsize requirements.")
             return
+
         stats = GPUStats().get_card_most_free()
         vram_free = int(stats["free"])
         if self._is_parallel:
-            vram_required = (self._detector.vram + self._aligner.vram + self._masker.vram +
-                             self._vram_buffer)
-            batch_required = ((self._detector.vram_per_batch * self._detector.batchsize) +
-                              (self._aligner.vram_per_batch * self._aligner.batchsize) +
-                              (self._masker.vram_per_batch * self._masker.batchsize))
-            plugin_required = vram_required + batch_required
+            batch_required = sum([plugin.vram_per_batch * plugin.batchsize
+                                  for plugin in self._all_plugins])
+            plugin_required = self._total_vram_required + batch_required
             if plugin_required <= vram_free:
                 logger.debug("Plugin requirements within threshold: (plugin_required: %sMB, "
                              "vram_free: %sMB)", plugin_required, vram_free)
                 return
             # Hacky split across 3 plugins
-            available_vram = (vram_free - vram_required) // 3
-            for plugin in (self._detector, self._aligner, self._masker):
+            available_vram = (vram_free - self._total_vram_required) // 3
+            for plugin in self._all_plugins:
                 self._set_plugin_batchsize(plugin, available_vram)
         else:
-            for plugin in (self._detector, self._aligner, self._masker):
+            for plugin in self._all_plugins:
                 vram_required = plugin.vram + self._vram_buffer
                 batch_required = plugin.vram_per_batch * plugin.batchsize
                 plugin_required = vram_required + batch_required
@@ -409,7 +408,7 @@ class Extractor():
 
     @staticmethod
     def _set_plugin_batchsize(plugin, available_vram):
-        """ Set the batchsize for the given plugin based on given available vram """
+        """ Set the batch size for the given plugin based on given available vram """
         plugin.batchsize = max(1, available_vram // plugin.vram_per_batch)
         logger.verbose("Reset batchsize for %s to %s", plugin.name, plugin.batchsize)
 
diff --git a/plugins/plugin_loader.py b/plugins/plugin_loader.py
index 0973c50..c23e2fd 100644
--- a/plugins/plugin_loader.py
+++ b/plugins/plugin_loader.py
@@ -1,5 +1,5 @@
 #!/usr/bin/env python3
-""" Plugin loader for extract, training and model tasks """
+""" Plugin loader for Faceswap extract, training and convert tasks """
 
 import logging
 import os
@@ -9,40 +9,151 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class PluginLoader():
-    """ Plugin loader for extract, training and model tasks """
+    """ Retrieve, or get information on, Faceswap plugins
+
+    Return a specific plugin, list available plugins, or get the default plugin for a
+    task.
+
+    Example
+    -------
+    >>> from plugins.plugin_loader import PluginLoader
+    >>> align_plugins = PluginLoader.get_available_extractors('align')
+    >>> aligner = PluginLoader.get_aligner('cv2-dnn')
+    """
     @staticmethod
     def get_detector(name, disable_logging=False):
-        """ Return requested detector plugin """
+        """ Return requested detector plugin
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested detector plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.extract.detect` object:
+            An extraction detector plugin
+        """
         return PluginLoader._import("extract.detect", name, disable_logging)
 
     @staticmethod
     def get_aligner(name, disable_logging=False):
-        """ Return requested detector plugin """
+        """ Return requested aligner plugin
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested aligner plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.extract.align` object:
+            An extraction aligner plugin
+        """
         return PluginLoader._import("extract.align", name, disable_logging)
 
     @staticmethod
     def get_masker(name, disable_logging=False):
-        """ Return requested detector plugin """
+        """ Return requested masker plugin
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested masker plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.extract.mask` object:
+            An extraction masker plugin
+        """
         return PluginLoader._import("extract.mask", name, disable_logging)
 
     @staticmethod
     def get_model(name, disable_logging=False):
-        """ Return requested model plugin """
+        """ Return requested training model plugin
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested training model plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.train.model` object:
+            A training model plugin
+        """
         return PluginLoader._import("train.model", name, disable_logging)
 
     @staticmethod
     def get_trainer(name, disable_logging=False):
-        """ Return requested trainer plugin """
+        """ Return requested training trainer plugin
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested training trainer plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.train.trainer` object:
+            A training trainer plugin
+        """
         return PluginLoader._import("train.trainer", name, disable_logging)
 
     @staticmethod
     def get_converter(category, name, disable_logging=False):
-        """ Return the converter sub plugin """
+        """ Return requested converter plugin
+
+        Converters work slightly differently to other faceswap plugins. They are created to do a
+        specific task (e.g. color adjustment, mask blending etc.), so multiple plugins will be
+        loaded in the convert phase, rather than just one plugin for the other phases.
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested converter plugin
+        disable_logging: bool, optional
+            Whether to disable the INFO log message that the plugin is being imported.
+            Default: `False`
+
+        Returns
+        -------
+        :class:`plugins.convert` object:
+            A converter sub plugin
+        """
         return PluginLoader._import("convert.{}".format(category), name, disable_logging)
 
     @staticmethod
     def _import(attr, name, disable_logging):
-        """ Import the plugin's module """
+        """ Import the plugin's module
+
+        Parameters
+        ----------
+        name: str
+            The name of the requested converter plugin
+        disable_logging: bool
+            Whether to disable the INFO log message that the plugin is being imported.
+
+        Returns
+        -------
+        :class:`plugin` object:
+            A plugin
+        """
         name = name.replace("-", "_")
         ttl = attr.split(".")[-1].title()
         if not disable_logging:
@@ -54,7 +165,18 @@ class PluginLoader():
 
     @staticmethod
     def get_available_extractors(extractor_type):
-        """ Return a list of available aligners/detectors """
+        """ Return a list of available extractors of the given type
+
+        Parameters
+        ----------
+        extractor_type: {'aligner', 'detector', 'masker'}
+            The type of extractor to return the plugins for
+
+        Returns
+        -------
+        list:
+            A list of the available extractor plugin names for the given type
+        """
         extractpath = os.path.join(os.path.dirname(__file__),
                                    "extract",
                                    extractor_type)
@@ -68,7 +190,13 @@ class PluginLoader():
 
     @staticmethod
     def get_available_models():
-        """ Return a list of available models """
+        """ Return a list of available training models
+
+        Returns
+        -------
+        list:
+            A list of the available training model plugin names
+        """
         modelpath = os.path.join(os.path.dirname(__file__), "train", "model")
         models = sorted(item.name.replace(".py", "").replace("_", "-")
                         for item in os.scandir(modelpath)
@@ -79,13 +207,34 @@ class PluginLoader():
 
     @staticmethod
     def get_default_model():
-        """ Return the default model """
+        """ Return the default training model plugin name
+
+        Returns
+        -------
+        str:
+            The default faceswap training model
+
+        """
         models = PluginLoader.get_available_models()
         return 'original' if 'original' in models else models[0]
 
     @staticmethod
     def get_available_convert_plugins(convert_category, add_none=True):
-        """ Return a list of available models """
+        """ Return a list of available converter plugins in the given category
+
+        Parameters
+        ----------
+        convert_category: {'color', 'mask', 'scaling', 'writer'}
+            The category of converter plugin to return the plugins for
+        add_none: bool, optional
+            Append "none" to the list of returned plugins. Default: True
+
+        Returns
+        -------
+        list
+            A list of the available converter plugin names in the given category
+        """
+
         convertpath = os.path.join(os.path.dirname(__file__),
                                    "convert",
                                    convert_category)
diff --git a/scripts/convert.py b/scripts/convert.py
index f150c13..390b339 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -255,7 +255,8 @@ class DiskIO():
                        "superior results")
         extractor = Extractor(detector="cv2-dnn",
                               aligner="cv2-dnn",
-                              multiprocess=False,
+                              masker="none",
+                              multiprocess=True,
                               rotate_images=None,
                               min_size=20)
         extractor.launch()
@@ -584,7 +585,8 @@ class Predict():
     def compile_feed_faces(detected_faces):
         """ Compile the faces for feeding into the predictor """
         logger.trace("Compiling feed face. Batchsize: %s", len(detected_faces))
-        feed_faces = np.stack([detected_face.feed_face for detected_face in detected_faces])
+        feed_faces = np.stack([detected_face.feed_face[..., :3]
+                               for detected_face in detected_faces]) / 255.0
         logger.trace("Compiled Feed faces. Shape: %s", feed_faces.shape)
         return feed_faces
 
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index 31a0bb0..b7f99f2 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -323,11 +323,8 @@ class BlurryFaceFilter(PostProcessAction):  # pylint: disable=too-few-public-met
             feature_mask = extractor.get_feature_mask(
                 aligned_landmarks / size,
                 size, padding)
-            feature_mask = cv2.blur(  # pylint: disable=no-member
-                feature_mask, (10, 10))
-            isolated_face = cv2.multiply(  # pylint: disable=no-member
-                feature_mask,
-                resized_face.astype(float)).astype(np.uint8)
+            feature_mask = cv2.blur(feature_mask, (10, 10))
+            isolated_face = cv2.multiply(feature_mask, resized_face.astype(float)).astype(np.uint8)
             blurry, focus_measure = self.is_blurry(isolated_face)
 
             if blurry:
@@ -340,7 +337,7 @@ class BlurryFaceFilter(PostProcessAction):  # pylint: disable=too-few-public-met
     def is_blurry(self, image):
         """ Convert to grayscale, and compute the focus measure of the image using the
             Variance of Laplacian method """
-        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # pylint: disable=no-member
+        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
         focus_measure = self.variance_of_laplacian(gray)
 
         # if the focus measure is less than the supplied threshold,
@@ -353,7 +350,7 @@ class BlurryFaceFilter(PostProcessAction):  # pylint: disable=too-few-public-met
     def variance_of_laplacian(image):
         """ Compute the Laplacian of the image and then return the focus
             measure, which is simply the variance of the Laplacian """
-        retval = cv2.Laplacian(image, cv2.CV_64F).var()  # pylint: disable=no-member
+        retval = cv2.Laplacian(image, cv2.CV_64F).var()
         logger.trace("Returning: %s", retval)
         return retval
 
@@ -370,8 +367,7 @@ class DebugLandmarks(PostProcessAction):  # pylint: disable=too-few-public-metho
                          detected_face["file_location"].parts[-1], idx)
             aligned_landmarks = face.aligned_landmarks
             for (pos_x, pos_y) in aligned_landmarks:
-                cv2.circle(face.feed_landmarks,  # pylint: disable=no-member
-                           (pos_x, pos_y), 2, (0, 0, 255, 255), -1)
+                cv2.circle(face.aligned_face, (pos_x, pos_y), 2, (0, 0, 255), -1)
 
 
 class FaceFilter(PostProcessAction):
diff --git a/tools/alignments.py b/tools/alignments.py
index a76742d..7575a1d 100644
--- a/tools/alignments.py
+++ b/tools/alignments.py
@@ -3,8 +3,8 @@
 import logging
 
 from lib.utils import set_system_verbosity
-from .lib_alignments import (AlignmentData, Check, Draw, # noqa pylint: disable=unused-import
-                             Extract, Manual, Merge, Reformat, Rename,
+from .lib_alignments import (AlignmentData, Check, Dfl, Draw, # noqa pylint: disable=unused-import
+                             Extract, Manual, Merge, Rename,
                              RemoveAlignments, Sort, Spatial, UpdateHashes)
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -42,8 +42,6 @@ class Alignments():
             job = UpdateHashes
         elif self.args.job.startswith("remove-"):
             job = RemoveAlignments
-        elif self.args.job.startswith("sort-"):
-            job = Sort
         elif self.args.job in("missing-alignments", "missing-frames",
                               "multi-faces", "leftover-faces", "no-faces"):
             job = Check
diff --git a/tools/cli.py b/tools/cli.py
index 003775b..c3331de 100644
--- a/tools/cli.py
+++ b/tools/cli.py
@@ -31,13 +31,16 @@ class AlignmentsArgs(FaceSwapArgs):
             "opts": ("-j", "--job"),
             "action": Radio,
             "type": str,
-            "choices": ("draw", "extract", "manual", "merge", "missing-alignments",
+            "choices": ("dfl", "draw", "extract", "manual", "merge", "missing-alignments",
                         "missing-frames", "leftover-faces", "multi-faces", "no-faces",
-                        "reformat", "remove-faces", "remove-frames", "rename", "sort-x", "sort-y",
-                        "spatial", "update-hashes"),
+                        "remove-faces", "remove-frames", "rename", "sort", "spatial",
+                        "update-hashes"),
             "required": True,
             "help": "R|Choose which action you want to perform. "
                     "NB: All actions require an alignments file (-a) to be passed in."
+                    "\nL|'dfl': Create an alignments file from faces extracted from DeepFaceLab. "
+                    "Specify 'dfl' as the 'alignments file' entry and the folder containing the "
+                    "dfl faces as the 'faces folder' ('-a dfl -fc <source faces folder>'"
                     "\nL|'draw': Draw landmarks on frames in the selected folder/video. A "
                     "subfolder will be created within the frames folder to hold the output." +
                     frames_dir +
@@ -60,24 +63,16 @@ class AlignmentsArgs(FaceSwapArgs):
                     "file." + output_opts + frames_or_faces_dir +
                     "\nL|'no-faces': Identify frames that exist within the alignment file but no "
                     "faces were detected." + output_opts + frames_dir +
-                    "\nL|'reformat': Save a copy of alignments file in a different format. "
-                    "Specify a format with the -fmt option. Alignments can be converted from "
-                    "DeepFaceLab by specifing: '-a dfl -fc <source faces folder>'"
                     "\nL|'remove-faces': Remove deleted faces from an alignments file. The "
-                    "original alignments file will be backed up. A different file format for the "
-                    "alignments file can optionally be specified (-fmt)." + faces_dir +
+                    "original alignments file will be backed up." + faces_dir +
                     "\nL|'remove-frames': Remove deleted frames from an alignments file. The "
-                    "original alignments file will be backed up. A different file format for "
-                    "the alignments file can optionally be specified (-fmt)." + frames_dir +
+                    "original alignments file will be backed up." + frames_dir +
                     "\nL|'rename' - Rename faces to correspond with their parent frame and "
                     "position index in the alignments file (i.e. how they are named after running "
                     "extract)." + faces_dir +
-                    "\nL|'sort-x': Re-index the alignments from left to right. For alignments "
+                    "\nL|'sort': Re-index the alignments from left to right. For alignments "
                     "with multiple faces this will ensure that the left-most face is at index 0 "
                     "Optionally pass in a faces folder (-fc) to also rename extracted faces."
-                    "\nL|'sort-y': Re-index the alignments from top to bottom. For alignments "
-                    "with multiple faces this will ensure that the top-most face is at index 0. "
-                    "Optionally pass in a faces folder (-fc) to also  rename extracted faces."
                     "\nL|'spatial': Perform spatial and temporal filtering to smooth alignments "
                     "(EXPERIMENTAL!)"
                     "\nL|'update-hashes': Recalculate the face hashes. Only use this if you have "
diff --git a/tools/lib_alignments/__init__.py b/tools/lib_alignments/__init__.py
index f23a628..9945b7b 100644
--- a/tools/lib_alignments/__init__.py
+++ b/tools/lib_alignments/__init__.py
@@ -1,4 +1,4 @@
 from tools.lib_alignments.media import AlignmentData, ExtractedFaces, Faces, Frames
 from tools.lib_alignments.annotate import Annotate
-from tools.lib_alignments.jobs import Check, Draw, Extract, Merge, Reformat, RemoveAlignments, Rename, Sort, Spatial, UpdateHashes
+from tools.lib_alignments.jobs import Check, Dfl, Draw, Extract, Merge, RemoveAlignments, Rename, Sort, Spatial, UpdateHashes
 from tools.lib_alignments.jobs_manual import Manual
diff --git a/tools/lib_alignments/jobs.py b/tools/lib_alignments/jobs.py
index f2d74cd..df01729 100644
--- a/tools/lib_alignments/jobs.py
+++ b/tools/lib_alignments/jobs.py
@@ -257,6 +257,93 @@ class Check():
             os.rename(src, dst)
 
 
+class Dfl():
+    """ Reformat Alignment file """
+    def __init__(self, alignments, arguments):
+        logger.debug("Initializing %s: (arguments: %s)", self.__class__.__name__, arguments)
+        self.alignments = alignments
+        if self.alignments.file != "dfl.fsa":
+            logger.error("Alignments file must be specified as 'dfl' to reformat dfl alignmnets")
+            exit(0)
+        logger.debug("Loading DFL faces")
+        self.faces = Faces(arguments.faces_dir)
+        logger.debug("Initialized %s", self.__class__.__name__)
+
+    def process(self):
+        """ Run reformat """
+        logger.info("[REFORMAT DFL ALIGNMENTS]")  # Tidy up cli output
+        self.alignments.data = self.load_dfl()
+        self.alignments.file = self.alignments.get_location(self.faces.folder, "alignments")
+        self.alignments.save()
+
+    def load_dfl(self):
+        """ Load alignments from DeepFaceLab and format for Faceswap """
+        alignments = dict()
+        for face in tqdm(self.faces.file_list_sorted, desc="Converting DFL Faces"):
+            if face["face_extension"] not in (".png", ".jpg"):
+                logger.verbose("'%s' is not a png or jpeg. Skipping", face["face_fullname"])
+                continue
+            f_hash = face["face_hash"]
+            fullpath = os.path.join(self.faces.folder, face["face_fullname"])
+            dfl = self.get_dfl_alignment(fullpath)
+
+            if not dfl:
+                continue
+
+            self.convert_dfl_alignment(dfl, f_hash, alignments)
+        return alignments
+
+    @staticmethod
+    def get_dfl_alignment(filename):
+        """ Process the alignment of one face """
+        ext = os.path.splitext(filename)[1]
+
+        if ext.lower() in (".jpg", ".jpeg"):
+            img = Image.open(filename)
+            try:
+                dfl_alignments = pickle.loads(img.app["APP15"])
+                dfl_alignments["source_rect"] = [n.item()  # comes as non-JSONable np.int32
+                                                 for n in dfl_alignments["source_rect"]]
+                return dfl_alignments
+            except pickle.UnpicklingError:
+                return None
+
+        with open(filename, "rb") as dfl:
+            header = dfl.read(8)
+            if header != b"\x89PNG\r\n\x1a\n":
+                logger.error("No Valid PNG header: %s", filename)
+                return None
+            while True:
+                chunk_start = dfl.tell()
+                chunk_hdr = dfl.read(8)
+                if not chunk_hdr:
+                    break
+                chunk_length, chunk_name = struct.unpack("!I4s", chunk_hdr)
+                dfl.seek(chunk_start, os.SEEK_SET)
+                if chunk_name == b"fcWp":
+                    chunk = dfl.read(chunk_length + 12)
+                    retval = pickle.loads(chunk[8:-4])
+                    logger.trace("Loaded DFL Alignment: (filename: '%s', alignment: %s",
+                                 filename, retval)
+                    return retval
+                dfl.seek(chunk_length+12, os.SEEK_CUR)
+            logger.error("Couldn't find DFL alignments: %s", filename)
+
+    @staticmethod
+    def convert_dfl_alignment(dfl_alignments, f_hash, alignments):
+        """ Add DFL Alignments to alignments in Faceswap format """
+        sourcefile = dfl_alignments["source_filename"]
+        left, top, right, bottom = dfl_alignments["source_rect"]
+        alignment = {"x": left,
+                     "w": right - left,
+                     "y": top,
+                     "h": bottom - top,
+                     "hash": f_hash,
+                     "landmarks_xy": np.array(dfl_alignments["source_landmarks"], dtype="uint8")}
+        logger.trace("Adding alignment: (frame: '%s', alignment: %s", sourcefile, alignment)
+        alignments.setdefault(sourcefile, list()).append(alignment)
+
+
 class Draw():
     """ Draw Alignments on passed in images """
     def __init__(self, alignments, arguments):
@@ -515,92 +602,6 @@ class Merge():
         self.final_alignments.file = filename
 
 
-class Reformat():
-    """ Reformat Alignment file """
-    def __init__(self, alignments, arguments):
-        logger.debug("Initializing %s: (arguments: %s)", self.__class__.__name__, arguments)
-        self.alignments = alignments
-        if self.alignments.file == "dfl.json":
-            logger.debug("Loading DFL faces")
-            self.faces = Faces(arguments.faces_dir)
-        logger.debug("Initialized %s", self.__class__.__name__)
-
-    def process(self):
-        """ Run reformat """
-        logger.info("[REFORMAT ALIGNMENTS]")  # Tidy up cli output
-        if self.alignments.file == "dfl.json":
-            self.alignments.data = self.load_dfl()
-            self.alignments.file = self.alignments.get_location(self.faces.folder, "alignments")
-        self.alignments.save()
-
-    def load_dfl(self):
-        """ Load alignments from DeepFaceLab and format for Faceswap """
-        alignments = dict()
-        for face in tqdm(self.faces.file_list_sorted, desc="Converting DFL Faces"):
-            if face["face_extension"] not in (".png", ".jpg"):
-                logger.verbose("'%s' is not a png or jpeg. Skipping", face["face_fullname"])
-                continue
-            f_hash = face["face_hash"]
-            fullpath = os.path.join(self.faces.folder, face["face_fullname"])
-            dfl = self.get_dfl_alignment(fullpath)
-
-            if not dfl:
-                continue
-
-            self.convert_dfl_alignment(dfl, f_hash, alignments)
-        return alignments
-
-    @staticmethod
-    def get_dfl_alignment(filename):
-        """ Process the alignment of one face """
-        ext = os.path.splitext(filename)[1]
-
-        if ext.lower() in (".jpg", ".jpeg"):
-            img = Image.open(filename)
-            try:
-                dfl_alignments = pickle.loads(img.app["APP15"])
-                dfl_alignments["source_rect"] = [n.item()  # comes as non-JSONable np.int32
-                                                 for n in dfl_alignments["source_rect"]]
-                return dfl_alignments
-            except pickle.UnpicklingError:
-                return None
-
-        with open(filename, "rb") as dfl:
-            header = dfl.read(8)
-            if header != b"\x89PNG\r\n\x1a\n":
-                logger.error("No Valid PNG header: %s", filename)
-                return None
-            while True:
-                chunk_start = dfl.tell()
-                chunk_hdr = dfl.read(8)
-                if not chunk_hdr:
-                    break
-                chunk_length, chunk_name = struct.unpack("!I4s", chunk_hdr)
-                dfl.seek(chunk_start, os.SEEK_SET)
-                if chunk_name == b"fcWp":
-                    chunk = dfl.read(chunk_length + 12)
-                    retval = pickle.loads(chunk[8:-4])
-                    logger.trace("Loaded DFL Alignment: (filename: '%s', alignment: %s",
-                                 filename, retval)
-                    return retval
-                dfl.seek(chunk_length+12, os.SEEK_CUR)
-            logger.error("Couldn't find DFL alignments: %s", filename)
-
-    @staticmethod
-    def convert_dfl_alignment(dfl_alignments, f_hash, alignments):
-        """ Add DFL Alignments to alignments in Faceswap format """
-        sourcefile = dfl_alignments["source_filename"]
-        left, top, right, bottom = dfl_alignments["source_rect"]
-        alignment = {"x": left,
-                     "w": right - left,
-                     "y": top,
-                     "h": bottom - top,
-                     "hash": f_hash,
-                     "landmarks_xy": dfl_alignments["source_landmarks"]}
-        logger.trace("Adding alignment: (frame: '%s', alignment: %s", sourcefile, alignment)
-        alignments.setdefault(sourcefile, list()).append(alignment)
-
-
 class RemoveAlignments():
     """ Remove items from alignments file """
     def __init__(self, alignments, arguments):
@@ -755,12 +756,10 @@ class Rename():
 
 
 class Sort():
-    """ Sort alignments' index by the order they appear in
-        an image """
+    """ Sort alignments' index by the order they appear in an image """
     def __init__(self, alignments, arguments):
         logger.debug("Initializing %s: (arguments: %s)", self.__class__.__name__, arguments)
         self.alignments = alignments
-        self.axis = arguments.job.replace("sort-", "")
         self.faces = self.get_faces(arguments)
         logger.debug("Initialized %s", self.__class__.__name__)
 
@@ -791,7 +790,7 @@ class Sort():
             if count <= 1:
                 logger.trace("0 or 1 face in frame. Not sorting: '%s'", frame)
                 continue
-            sorted_alignments = sorted([item for item in alignments], key=lambda x: (x[self.axis]))
+            sorted_alignments = sorted([item for item in alignments], key=lambda x: (x["x"]))
             if sorted_alignments == alignments:
                 logger.trace("Alignments already in correct order. Not sorting: '%s'", frame)
                 continue
diff --git a/tools/lib_alignments/jobs_manual.py b/tools/lib_alignments/jobs_manual.py
index 2cf4d63..fe56e4d 100644
--- a/tools/lib_alignments/jobs_manual.py
+++ b/tools/lib_alignments/jobs_manual.py
@@ -781,11 +781,11 @@ class MouseHandler():
     def init_extractor(self):
         """ Initialize Aligner """
         logger.debug("Initialize Extractor")
-        extractor = Extractor("manual", "fan", multiprocess=True, normalize_method="hist")
+        extractor = Extractor("manual", "fan", "none", multiprocess=True, normalize_method="hist")
         self.queues["in"] = extractor.input_queue
         # Set the batchsizes to 1
-        extractor.set_batchsize("detector", 1)
-        extractor.set_batchsize("aligner", 1)
+        for plugin_type in ("detect", "align", "mask"):
+            extractor.set_batchsize(plugin_type, 1)
         extractor.launch()
         logger.debug("Initialized Extractor")
         return extractor
