commit 98db63c585b8bd039a6f7b3424254c827159d060
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sat Jun 8 15:39:14 2019 +0000

    Bugfixes
    
    - Switch to correct Tools tab when loading recent file.
    - Fix on-the-fly conversion cuDNN error

diff --git a/lib/gui/utils.py b/lib/gui/utils.py
index 33dd859..2f10e4f 100644
--- a/lib/gui/utils.py
+++ b/lib/gui/utils.py
@@ -513,6 +513,12 @@ class Config():
         return {self.command_notebook.tab(tab_id, "text").lower(): tab_id
                 for tab_id in range(0, self.command_notebook.index("end"))}
 
+    @property
+    def tools_command_tabs(self):
+        """ Return dict of tools command tab titles with their IDs """
+        return {self.command_notebook.tools_notebook.tab(tab_id, "text").lower(): tab_id
+                for tab_id in range(0, self.command_notebook.tools_notebook.index("end"))}
+
     @staticmethod
     def set_tk_vars():
         """ TK Variables to be triggered by to indicate
@@ -579,8 +585,11 @@ class Config():
             self.set_command_args(cmd, opts)
 
         if command:
-            self.command_notebook.select(self.command_tabs[command])
-
+            if command in self.command_tabs:
+                self.command_notebook.select(self.command_tabs[command])
+            else:
+                self.command_notebook.select(self.command_tabs["tools"])
+                self.command_notebook.tools_notebook.select(self.tools_command_tabs[command])
         self.add_to_recent(cfgfile.name, command)
         logger.debug("Loaded config: (command: '%s', cfgfile: '%s')", command, cfgfile)
 
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index fac7357..ccbcc06 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -90,12 +90,16 @@ class Extractor():
         """ Set whether to run detect and align together or separately """
         detector_vram = self.detector.vram
         aligner_vram = self.aligner.vram
-        gpu_stats = GPUStats()
-        if detector_vram == 0 or aligner_vram == 0 or gpu_stats.device_count == 0:
+        if detector_vram == 0 or aligner_vram == 0:
             logger.debug("At least one of aligner or detector have no VRAM requirement. "
                          "Enabling parallel processing.")
             return True
 
+        gpu_stats = GPUStats()
+        if gpu_stats.device_count == 0:
+            logger.debug("No GPU detected. Enabling parallel processing.")
+            return True
+
         if not multiprocess:
             logger.info("NB: Parallel processing disabled.You may get faster "
                         "extraction speeds by enabling it with the -mp switch")
diff --git a/scripts/convert.py b/scripts/convert.py
index 31ea501..1827919 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -439,10 +439,12 @@ class Predict():
     @staticmethod
     def get_batchsize(queue_size):
         """ Get the batchsize """
+        logger.debug("Getting batchsize")
         is_cpu = GPUStats().device_count == 0
         batchsize = 1 if is_cpu else 16
         batchsize = min(queue_size, batchsize)
         logger.debug("Batchsize: %s", batchsize)
+        logger.debug("Got batchsize: %s", batchsize)
         return batchsize
 
     def load_model(self):
