commit 25441afe33a8733ade9d54ee77ee3cee34e65080
Author: Ivanovar-08 <ivanovar-08@mail.ru>
Date:   Fri Jun 15 14:35:45 2018 +0300

    added OriginalHighRes model state save

diff --git a/README.md b/README.md
index 292b6a3..890f721 100755
--- a/README.md
+++ b/README.md
@@ -1,10 +1,30 @@
-**Notice:** This repository is not operated or maintained by [/u/deepfakes](https://www.reddit.com/user/deepfakes/). Please read the explanation below for details.
+**Notice:** This repository is not operated or maintained by /u/deepfakes. Please read the explanation below for details.
 
 ---
 
 # deepfakes_faceswap
+
 Faceswap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.
 
+## Manifesto
+
+### Faceswap is not porn.
+
+When faceswaping using an AI was first developed and became published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia. The code was confusing and fragmentary, it required a thorough understanding of state of the art AI techniques and a lot of effort to get anything out of it. One individual brought it together into one cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create porn. The problem was that this was the first AI code that anyone could download, run and learn by experimentation without becoming a PHD candidate in math, computer theory, psychology, and more. Before "deepfakes" these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers.
+
+"Deepfakes" changed all that and anyone could participate in AI development. To us developers, the release of this code has opened up a fantastic learning opportunity. To build on ideas developed by others, to collaborate with coders with a huge variety of skills, to experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses.
+
+Are there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don't even use it to create videos at all, we just tinker with the code to see what it all does. Sadly, the media concentrates only on the unethical uses of this software. That is unfortunately a nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in it's future. Like any technology, it can be used for good or it can be abused. It is our intention to develop faceswap in a way that it's potential for abuse is minimized whilst maximizing it's potential as a tool for learning, experimenting and, yes, for legitimate faceswaping.
+
+We are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it's time to come out with a standard statement of what this software is and isn't as far as us developers are concerned.
+
+- Faceswap is not for creating porn
+- Faceswap is not for changing faces without consent or with the intent of hiding it's use.
+- Faceswap is not for any illicit, unethical, or questionable purposes.
+- Faceswap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses.
+
+We are very troubled by the fact that faceswap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.
+
 ## Overview
 The project has multiple entry points. You will have to:
  - Gather photos (or use the one provided in the training data provided below)
@@ -91,7 +111,7 @@ The joshua-wu repo seems not active. Simple bugs like missing _http://_ in front
 
 ## Why is it named 'deepfakes' if it is not /u/deepfakes?
  1. Because a typosquat would have happened sooner or later as project grows
- 2. Because all glory go to /u/deepfakes
+ 2. Because we wanted to recognize the original author
  3. Because it will better federate contributors and users
  
 ## What if /u/deepfakes feels bad about that?
diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
index d505e6a..0715afc 100644
--- a/plugins/Model_OriginalHighRes/Model.py
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -7,15 +7,9 @@
 
 
 import enum
-<<<<<<< HEAD
 import os
 import sys
 import warnings
-
-=======
-
-import warnings
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
 warnings.filterwarnings("ignore", category=FutureWarning)
 
 from keras.initializers import RandomNormal
@@ -27,19 +21,11 @@ from keras.layers.core import Activation
 from keras.models import Model as KerasModel
 from keras.optimizers import Adam
 from keras.utils import multi_gpu_model
+
 from lib.PixelShuffler import PixelShuffler
-<<<<<<< HEAD
 import lib.Serializer
 
 from . import __version__
-=======
-import os
-import sys
-
-from . import __version__
-from .instance_normalization import InstanceNormalization
-
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
 
 if isinstance(__version__, (list, tuple)):
     version_str = ".".join([str(n) for n in __version__[1:]])
@@ -55,7 +41,7 @@ except ImportError:
     pass
 
 
-<<<<<<< HEAD
+
 class EncoderType(enum.Enum):
     ORIGINAL = "original"
     SHAOANLU = "shaoanlu"
@@ -67,12 +53,10 @@ ENCODER = EncoderType.ORIGINAL
 if ENCODER==EncoderType.SHAOANLU:
     from .instance_normalization import InstanceNormalization
 
-conv_init = RandomNormal(0, 0.02)
     
 def inst_norm():
     return InstanceNormalization()     
 
-=======
 conv_init = RandomNormal(0, 0.02)
 
 
@@ -86,19 +70,12 @@ class EncoderType(enum.Enum):
     
 ENCODER = EncoderType.ORIGINAL 
 
-hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
-       'decoder_AH5': 'decoder_A_{version_str}{ENCODER.value}.h5'.format(**vars()),
-       'decoder_BH5': 'decoder_B_{version_str}{ENCODER.value}.h5'.format(**vars())}
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
+
 
 hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
        'decoder_AH5': 'decoder_A_{version_str}{ENCODER.value}.h5'.format(**vars()),
        'decoder_BH5': 'decoder_B_{version_str}{ENCODER.value}.h5'.format(**vars())}
 
-<<<<<<< HEAD
-
-=======
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
 class Model():
     
     ENCODER_DIM = 1024 # dense layer size        
@@ -259,13 +236,8 @@ class Model():
         x = self.upscale(512)(x)
         
         return KerasModel(impt, x, **kwargs)    
-<<<<<<< HEAD
-
-
-=======
 
 
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
     def Decoder_original(self):       
         decoder_shape = self.IMAGE_SHAPE[0]//8        
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
@@ -273,21 +245,12 @@ class Model():
         x = self.upscale(384, kernel_initializer=RandomNormal(0, 0.02))(inpt)
         x = self.upscale(256-32, kernel_initializer=RandomNormal(0, 0.02))(x)
         x = self.upscale(self.IMAGE_SHAPE[0], kernel_initializer=RandomNormal(0, 0.02))(x)
-<<<<<<< HEAD
-        
-        x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
-        
-        return KerasModel(inpt, x)
-    
-    
-=======
         
         x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
         
         return KerasModel(inpt, x)
     
     
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
     def Decoder_shaoanlu(self):       
         decoder_shape = self.IMAGE_SHAPE[0]//8        
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
@@ -359,8 +322,4 @@ class Model():
         return "<{}: ver={}, nn_dims={}, img_size={}>".format(self.model_name, 
                                                               version_str, 
                                                               self.ENCODER_DIM, 
-<<<<<<< HEAD
-                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))                
-=======
-                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))                
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
+                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
index 3132f5f..41eddfb 100644
--- a/plugins/Model_OriginalHighRes/Trainer.py
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -1,4 +1,3 @@
-
 import time
 import numpy
 
@@ -9,7 +8,7 @@ TRANSFORM_PRC = 115.
 
 
 class Trainer():
-#     
+    
     _random_transform_args = {
         'rotation_range': 10 * (TRANSFORM_PRC * .01),
         'zoom_range': 0.05 * (TRANSFORM_PRC * .01),
@@ -23,11 +22,6 @@ class Trainer():
         from timeit import default_timer as clock
         self._clock = clock
         
-
-        #generator = TrainingDataGenerator(self.random_transform_args, 160)                
-        # make sre to keep zoom=2 or you won't get 128x128 vectors as input
-        #generator = TrainingDataGenerator(self.random_transform_args, 220, 5, zoom=2)
-        #generator = TrainingDataGenerator(self.random_transform_args, 180, 7, zoom=2)
         generator = TrainingDataGenerator(self.random_transform_args, 160, 5, zoom=2)        
         
         self.images_A = generator.minibatchAB(fn_A, self.batch_size)
@@ -42,19 +36,12 @@ class Trainer():
         _, warped_B, target_B = next(self.images_B)
 
         loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)
-<<<<<<< HEAD
         loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)
         
         self.model._epoch_no += 1        
                     
         print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
             time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A, loss_B),
-=======
-        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)        
-                    
-        print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
-            time.strftime("%H:%M:%S"), iter_no, self._clock()-when, loss_A, loss_B),
->>>>>>> 721e80dd773c850769ba6af82daa9f5ec95812bb
             end='\r')
         
 
