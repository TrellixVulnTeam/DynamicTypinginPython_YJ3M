commit 68109fcc80b02c45cd069c2d220b8b9e0705ac3d
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Oct 29 22:41:01 2019 +0000

    bugfix: plugins.extract.pipeline - Exclude CPU plugins from vram calculations

diff --git a/lib/alignments.py b/lib/alignments.py
index bb71083..6752332 100644
--- a/lib/alignments.py
+++ b/lib/alignments.py
@@ -206,10 +206,10 @@ class Alignments():
         self.data[frame][idx] = alignment
 
     def filter_hashes(self, hashlist, filter_out=False):
-        """ Filter in or out faces that match the hashlist
+        """ Filter in or out faces that match the hash list
 
-            filter_out=True: Remove faces that match in the hashlist
-            filter_out=False: Remove faces that are not in the hashlist
+            filter_out=True: Remove faces that match in the hash list
+            filter_out=False: Remove faces that are not in the hash list
         """
         hashset = set(hashlist)
         for filename, frame in self.data.items():
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index cf4777a..5ba84de 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -443,12 +443,16 @@ class Extractor():
                 logger.debug("Plugin requirements within threshold: (plugin_required: %sMB, "
                              "vram_free: %sMB)", plugin_required, vram_free)
                 return
-            # Hacky split across 3 plugins
-            available_vram = (vram_free - self._total_vram_required) // 3
+            # Hacky split across plugins that use vram
+            gpu_plugin_count = sum([1 for plugin in self._all_plugins if plugin.vram != 0])
+            available_vram = (vram_free - self._total_vram_required) // gpu_plugin_count
             for plugin in self._all_plugins:
-                self._set_plugin_batchsize(plugin, available_vram)
+                if plugin.vram != 0:
+                    self._set_plugin_batchsize(plugin, available_vram)
         else:
             for plugin in self._all_plugins:
+                if plugin.vram == 0:
+                    continue
                 vram_required = plugin.vram + self._vram_buffer
                 batch_required = plugin.vram_per_batch * plugin.batchsize
                 plugin_required = vram_required + batch_required
@@ -461,9 +465,13 @@ class Extractor():
 
     @staticmethod
     def _set_plugin_batchsize(plugin, available_vram):
-        """ Set the batch size for the given plugin based on given available vram """
-        plugin.batchsize = int(max(1, available_vram // plugin.vram_per_batch))
-        logger.verbose("Reset batchsize for %s to %s", plugin.name, plugin.batchsize)
+        """ Set the batch size for the given plugin based on given available vram.
+        Do not update plugins which have a vram_per_batch of 0 (CPU plugins) due to
+        zero division error.
+        """
+        if plugin.vram_per_batch != 0:
+            plugin.batchsize = int(max(1, available_vram // plugin.vram_per_batch))
+            logger.verbose("Reset batchsize for %s to %s", plugin.name, plugin.batchsize)
 
     def _join_threads(self):
         """ Join threads for current pass """
