commit 5418fba726d1186b7fdf0bda31a0239568c7a64b
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Jul 16 10:09:29 2019 +0100

    Add Convolutional Aware Initialization (#795)
    
    * Training: Add Convolutional Aware Initialization config option
    
    * Centralize Conv2D layer for handling initializer
    
    * Add 'is-output' to NNMeta to indicate that network is an output to the Model

diff --git a/lib/model/initializers.py b/lib/model/initializers.py
index c2b4ce6..8536a3b 100644
--- a/lib/model/initializers.py
+++ b/lib/model/initializers.py
@@ -3,12 +3,18 @@
     Initializers from:
         shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
 
+import logging
 import sys
 import inspect
+
+import numpy as np
 import tensorflow as tf
+from keras import backend as K
 from keras import initializers
 from keras.utils.generic_utils import get_custom_objects
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 def icnr_keras(shape, dtype=None):
     """
@@ -55,14 +61,14 @@ class ICNR(initializers.Initializer):  # pylint: disable=invalid-name
         if self.scale == 1:
             return self.initializer(shape)
         new_shape = shape[:3] + [shape[3] // (self.scale ** 2)]
-        if type(self.initializer) is dict:
+        if isinstance(self.initializer, dict):
             self.initializer = initializers.deserialize(self.initializer)
         var_x = self.initializer(new_shape, dtype)
         var_x = tf.transpose(var_x, perm=[2, 0, 1, 3])
         var_x = tf.image.resize_nearest_neighbor(
-                         var_x,
-                         size=(shape[0] * self.scale, shape[1] * self.scale),
-                         align_corners=True)
+            var_x,
+            size=(shape[0] * self.scale, shape[1] * self.scale),
+            align_corners=True)
         var_x = tf.space_to_depth(var_x, block_size=self.scale, data_format='NHWC')
         var_x = tf.transpose(var_x, perm=[1, 2, 0, 3])
         return var_x
@@ -75,6 +81,126 @@ class ICNR(initializers.Initializer):  # pylint: disable=invalid-name
         return dict(list(base_config.items()) + list(config.items()))
 
 
+class ConvolutionAware(initializers.Initializer):
+    """
+    Initializer that generates orthogonal convolution filters in the fourier
+    space. If this initializer is passed a shape that is not 3D or 4D,
+    orthogonal initialization will be used.
+    # Arguments
+        eps_std: Standard deviation for the random normal noise used to break
+        symmetry in the inverse fourier transform.
+        seed: A Python integer. Used to seed the random generator.
+    # References
+        Armen Aghajanyan, https://arxiv.org/abs/1702.06295
+    # Adapted and fixed from:
+    https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/initializers/convaware.py
+    """
+
+    def __init__(self, eps_std=0.05, seed=None, init=False):
+        # Convolutional Aware Initialization takes a long time.
+        # Keras model loading loads a model, performs initialization and then
+        # loads weights, which is an unnecessary waste of time.
+        # init defaults to False so that this is bypassed when loading a saved model
+        # passing zeros
+        self._init = init
+        self.eps_std = eps_std
+        self.seed = seed
+        self.orthogonal = initializers.Orthogonal()
+        self.he_uniform = initializers.he_uniform()
+
+    def __call__(self, shape, dtype=None):
+        dtype = K.floatx() if dtype is None else dtype
+        if self._init:
+            logger.info("Calculating Convolution Aware Initializer for shape: %s", shape)
+        else:
+            logger.debug("Bypassing Convolutional Aware Initializer for saved model")
+            # Dummy in he_uniform just in case there aren't any weighs being loaded
+            # and it needs some kind of initialization
+            return self.he_uniform(shape, dtype=dtype)
+
+        rank = len(shape)
+        if self.seed is not None:
+            np.random.seed(self.seed)
+
+        fan_in, _ = initializers._compute_fans(shape)  # pylint:disable=protected-access
+        variance = 2 / fan_in
+
+        if rank == 3:
+            row, stack_size, filters_size = shape
+
+            transpose_dimensions = (2, 1, 0)
+            kernel_shape = (row,)
+            correct_ifft = lambda shape, s=[None]: np.fft.irfft(shape, s[0])  # noqa
+            correct_fft = np.fft.rfft
+
+        elif rank == 4:
+            row, column, stack_size, filters_size = shape
+
+            transpose_dimensions = (2, 3, 1, 0)
+            kernel_shape = (row, column)
+            correct_ifft = np.fft.irfft2
+            correct_fft = np.fft.rfft2
+
+        elif rank == 5:
+            var_x, var_y, var_z, stack_size, filters_size = shape
+
+            transpose_dimensions = (3, 4, 0, 1, 2)
+            kernel_shape = (var_x, var_y, var_z)
+            correct_fft = np.fft.rfftn
+            correct_ifft = np.fft.irfftn
+
+        else:
+            return K.variable(self.orthogonal(shape), dtype=dtype)
+
+        kernel_fourier_shape = correct_fft(np.zeros(kernel_shape)).shape
+        init = []
+        for _ in range(filters_size):
+            basis = self._create_basis(
+                stack_size, np.prod(kernel_fourier_shape), dtype)
+            basis = basis.reshape((stack_size,) + kernel_fourier_shape)
+
+            filters = [correct_ifft(x, kernel_shape) +
+                       np.random.normal(0, self.eps_std, kernel_shape) for
+                       x in basis]
+
+            init.append(filters)
+
+        # Format of array is now: filters, stack, row, column
+        init = np.array(init)
+        init = self._scale_filters(init, variance)
+        return K.variable(init.transpose(transpose_dimensions), dtype=dtype, name="conv_aware")
+
+    def _create_basis(self, filters, size, dtype):
+        if size == 1:
+            return np.random.normal(0.0, self.eps_std, (filters, size))
+
+        nbb = filters // size + 1
+        lst = []
+        for _ in range(nbb):
+            var_a = np.random.normal(0.0, 1.0, (size, size))
+            var_a = self._symmetrize(var_a)
+            var_u, _, _ = np.linalg.svd(var_a)
+            lst.extend(var_u.T.tolist())
+        var_p = np.array(lst[:filters], dtype=dtype)
+        return var_p
+
+    @staticmethod
+    def _symmetrize(var_a):
+        return var_a + var_a.T - np.diag(var_a.diagonal())
+
+    @staticmethod
+    def _scale_filters(filters, variance):
+        c_var = np.var(filters)
+        var_p = np.sqrt(variance / c_var)
+        return filters * var_p
+
+    def get_config(self):
+        return {
+            'eps_std': self.eps_std,
+            'seed': self.seed
+        }
+
+
 # Update initializers into Keras custom objects
 for name, obj in inspect.getmembers(sys.modules[__name__]):
     if inspect.isclass(obj) and obj.__module__ == __name__:
diff --git a/lib/model/nn_blocks.py b/lib/model/nn_blocks.py
index 4eace25..09cbf41 100644
--- a/lib/model/nn_blocks.py
+++ b/lib/model/nn_blocks.py
@@ -14,8 +14,8 @@ from keras.layers import (add, Add, BatchNormalization, concatenate, Lambda, reg
 from keras.layers.advanced_activations import LeakyReLU
 from keras.layers.convolutional import Conv2D
 from keras.layers.core import Activation
-from keras.initializers import he_uniform, Constant, VarianceScaling
-from .initializers import ICNR
+from keras.initializers import he_uniform, VarianceScaling
+from .initializers import ICNR, ConvolutionAware
 from .layers import PixelShuffler, SubPixelUpscaling, ReflectionPadding2D, Scale
 from .normalization import GroupNormalization, InstanceNormalization
 
@@ -24,56 +24,102 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 class NNBlocks():
     """ Blocks to use for creating models """
-    def __init__(self, use_subpixel=False, use_icnr_init=False, use_reflect_padding=False):
-        logger.debug("Initializing %s: (use_subpixel: %s, use_icnr_init: %s, use_reflect_padding: %s)",
-                     self.__class__.__name__, use_subpixel, use_icnr_init, use_reflect_padding)
+    def __init__(self, use_subpixel=False, use_icnr_init=False, use_convaware_init=False,
+                 use_reflect_padding=False, first_run=True):
+        logger.debug("Initializing %s: (use_subpixel: %s, use_icnr_init: %s, use_convaware_init: "
+                     "%s, use_reflect_padding: %s, first_run: %s)",
+                     self.__class__.__name__, use_subpixel, use_icnr_init, use_convaware_init,
+                     use_reflect_padding, first_run)
+        self.first_run = first_run
         self.use_subpixel = use_subpixel
         self.use_icnr_init = use_icnr_init
+        self.use_convaware_init = use_convaware_init
         self.use_reflect_padding = use_reflect_padding
+        if self.use_convaware_init and self.first_run:
+            logger.info("Using Convolutional Aware Initialization. Model generation will take a "
+                        "few minutes...")
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    @staticmethod
-    def update_kwargs(kwargs):
-        """ Set the default kernel initializer to he_uniform() """
-        kwargs["kernel_initializer"] = kwargs.get("kernel_initializer", he_uniform())
+    def update_kwargs(self, kwargs):
+        """ Update Kwargs for conv2D and Seperable conv2D layers.
+            Set the default kernel initializer to conv_aware or he_uniform()
+            if a specific initializer has not been passed in """
+        if self.use_convaware_init:
+            default = ConvolutionAware()
+            if self.first_run:
+                # Indicate the Convolutional Aware should be calculated on first run
+                default._init = True  # pylint:disable=protected-access
+        else:
+            default = he_uniform()
+        kwargs["kernel_initializer"] = kwargs.get("kernel_initializer", default)
+        logger.debug("Set default kernel_initializer to: %s", kwargs["kernel_initializer"])
         return kwargs
 
+    @staticmethod
+    def switch_kernel_initializer(kwargs, initializer):
+        """ Switch the initializer in the given kwargs to the given initializer
+            and return the previous initializer to caller """
+        original = kwargs["kernel_initializer"]
+        kwargs["kernel_initializer"] = initializer
+        logger.debug("Switched kernel_initializer from %s to %s", original, initializer)
+        return original
+
+    def conv2d(self, inp, filters, kernel_size, strides=(1, 1), padding="same",
+               force_initializer=False, **kwargs):
+        """ A standard conv2D layer with correct initialization """
+        logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, padding: %s, "
+                     "force_initializer: %s, kwargs: %s)", inp, filters, kernel_size, strides,
+                     padding, force_initializer, kwargs)
+        if not force_initializer:
+            # Do not update the initializer if force_initializer is true (i.e. initializer is
+            # already correctly set in kwargs)
+            kwargs = self.update_kwargs(kwargs)
+        var_x = Conv2D(filters, kernel_size,
+                       strides=strides,
+                       padding=padding,
+                       **kwargs)(inp)
+        return var_x
+
     # <<< Original Model Blocks >>> #
-    def conv(self, inp, filters, kernel_size=5, strides=2, padding='same', use_instance_norm=False, res_block_follows=False, **kwargs):
+    def conv(self, inp, filters, kernel_size=5, strides=2, padding="same",
+             use_instance_norm=False, res_block_follows=False, **kwargs):
         """ Convolution Layer"""
         logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, use_instance_norm: %s, "
                      "kwargs: %s)", inp, filters, kernel_size, strides, use_instance_norm, kwargs)
-        kwargs = self.update_kwargs(kwargs)
         if self.use_reflect_padding:
             inp = ReflectionPadding2D(stride=strides, kernel_size=kernel_size)(inp)
-            padding = 'valid'
-        var_x = Conv2D(filters,
-                       kernel_size=kernel_size,
-                       strides=strides,
-                       padding=padding,
-                       **kwargs)(inp)
+            padding = "valid"
+        var_x = self.conv2d(inp, filters,
+                            kernel_size=kernel_size,
+                            strides=strides,
+                            padding=padding,
+                            **kwargs)
         if use_instance_norm:
             var_x = InstanceNormalization()(var_x)
         if not res_block_follows:
             var_x = LeakyReLU(0.1)(var_x)
         return var_x
 
-    def upscale(self, inp, filters, kernel_size=3, padding= 'same', use_instance_norm=False, res_block_follows=False, **kwargs):
+    def upscale(self, inp, filters, kernel_size=3, padding="same",
+                use_instance_norm=False, res_block_follows=False, **kwargs):
         """ Upscale Layer """
         logger.debug("inp: %s, filters: %s, kernel_size: %s, use_instance_norm: %s, kwargs: %s)",
                      inp, filters, kernel_size, use_instance_norm, kwargs)
-        kwargs = self.update_kwargs(kwargs)
         if self.use_reflect_padding:
             inp = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(inp)
-            padding = 'valid'
-        temp = kwargs["kernel_initializer"]
+            padding = "valid"
+        kwargs = self.update_kwargs(kwargs)
         if self.use_icnr_init:
-            kwargs["kernel_initializer"] = ICNR(initializer=kwargs["kernel_initializer"])
-        var_x = Conv2D(filters * 4,
-                       kernel_size=kernel_size,
-                       padding=padding,
-                       **kwargs)(inp)
-        kwargs["kernel_initializer"] = temp
+            original_init = self.switch_kernel_initializer(
+                kwargs,
+                ICNR(initializer=kwargs["kernel_initializer"]))
+        var_x = self.conv2d(inp, filters * 4,
+                            kernel_size=kernel_size,
+                            padding=padding,
+                            force_initializer=True,
+                            **kwargs)
+        if self.use_icnr_init:
+            self.switch_kernel_initializer(kwargs, original_init)
         if use_instance_norm:
             var_x = InstanceNormalization()(var_x)
         if not res_block_follows:
@@ -85,32 +131,32 @@ class NNBlocks():
         return var_x
 
     # <<< DFaker Model Blocks >>> #
-    def res_block(self, inp, filters, kernel_size=3, padding= 'same', **kwargs):
+    def res_block(self, inp, filters, kernel_size=3, padding="same", **kwargs):
         """ Residual block """
         logger.debug("inp: %s, filters: %s, kernel_size: %s, kwargs: %s)",
                      inp, filters, kernel_size, kwargs)
-        kwargs = self.update_kwargs(kwargs)
         var_x = LeakyReLU(alpha=0.2)(inp)
         if self.use_reflect_padding:
             var_x = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(var_x)
-            padding = 'valid'
-        var_x = Conv2D(filters,
-                       kernel_size=kernel_size,
-                       padding=padding,
-                       **kwargs)(var_x)
+            padding = "valid"
+        var_x = self.conv2d(inp, filters,
+                            kernel_size=kernel_size,
+                            padding=padding,
+                            **kwargs)
         var_x = LeakyReLU(alpha=0.2)(var_x)
         if self.use_reflect_padding:
             var_x = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(var_x)
-            padding = 'valid'
-        temp = kwargs["kernel_initializer"]
-        kwargs["kernel_initializer"] = VarianceScaling(scale=0.2,
-                                                       mode='fan_in',
-                                                       distribution='uniform')
-        var_x = Conv2D(filters,
-                       kernel_size=kernel_size,
-                       padding=padding,
-                       **kwargs)(var_x)
-        kwargs["kernel_initializer"] = temp
+            padding = "valid"
+        original_init = self.switch_kernel_initializer(kwargs, VarianceScaling(
+            scale=0.2,
+            mode="fan_in",
+            distribution="uniform"))
+        var_x = self.conv2d(var_x, filters,
+                            kernel_size=kernel_size,
+                            padding=padding,
+                            force_initializer=True,
+                            **kwargs)
+        self.switch_kernel_initializer(kwargs, original_init)
         var_x = Add()([var_x, inp])
         var_x = LeakyReLU(alpha=0.2)(var_x)
         return var_x
@@ -124,7 +170,7 @@ class NNBlocks():
         var_x = SeparableConv2D(filters,
                                 kernel_size=kernel_size,
                                 strides=strides,
-                                padding='same',
+                                padding="same",
                                 **kwargs)(inp)
         var_x = Activation("relu")(var_x)
         return var_x
@@ -139,17 +185,17 @@ GAN22_REGULARIZER = 1e-4
 
 
 # Gan Blocks:
-def normalization(inp, norm='none', group='16'):
+def normalization(inp, norm="none", group="16"):
     """ GAN Normalization """
-    if norm == 'layernorm':
+    if norm == "layernorm":
         var_x = GroupNormalization(group=group)(inp)
-    elif norm == 'batchnorm':
+    elif norm == "batchnorm":
         var_x = BatchNormalization()(inp)
-    elif norm == 'groupnorm':
+    elif norm == "groupnorm":
         var_x = GroupNormalization(group=16)(inp)
-    elif norm == 'instancenorm':
+    elif norm == "instancenorm":
         var_x = InstanceNormalization()(inp)
-    elif norm == 'hybrid':
+    elif norm == "hybrid":
         if group % 2 == 1:
             raise ValueError("Output channels must be an even number for hybrid norm, "
                              "received {}.".format(group))
@@ -200,7 +246,7 @@ def reflect_padding_2d(inp, pad=1):
     return var_x
 
 
-def conv_gan(inp, filters, use_norm=False, strides=2, norm='none'):
+def conv_gan(inp, filters, use_norm=False, strides=2, norm="none"):
     """ GAN Conv Block """
     var_x = Conv2D(filters,
                    kernel_size=3,
@@ -214,7 +260,7 @@ def conv_gan(inp, filters, use_norm=False, strides=2, norm='none'):
     return var_x
 
 
-def conv_d_gan(inp, filters, use_norm=False, norm='none'):
+def conv_d_gan(inp, filters, use_norm=False, norm="none"):
     """ GAN Discriminator Conv Block """
     var_x = inp
     var_x = Conv2D(filters,
@@ -229,7 +275,7 @@ def conv_d_gan(inp, filters, use_norm=False, norm='none'):
     return var_x
 
 
-def res_block_gan(inp, filters, use_norm=False, norm='none'):
+def res_block_gan(inp, filters, use_norm=False, norm="none"):
     """ GAN Res Block """
     var_x = Conv2D(filters,
                    kernel_size=3,
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index 7b0af8b..c0a0d8f 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -56,6 +56,13 @@ class Config(FaceswapConfig):
             section=section, title="icnr_init", datatype=bool, default=False,
             info="Use ICNR Kernel Initializer for upscaling.\nThis can help reduce the "
                  "'checkerboard effect' when upscaling the image.")
+        self.add_item(
+            section=section, title="conv_aware_init", datatype=bool, default=False,
+            info="Use Convolution Aware Initialization for convolutional layers\nThis can help "
+                 "eradicate the vanishing and exploding gradient problem as well as lead to "
+                 "higher accuracy, lower loss and faster convergence.\nNB: Building the model "
+                 "will likely take several minutes as the caluclations for this initialization "
+                 "technique are expensive.")
         self.add_item(
             section=section, title="subpixel_upscaling", datatype=bool, default=False,
             info="Use subpixel upscaling rather than pixel shuffler.\n"
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 038d341..257cf13 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -68,20 +68,25 @@ class ModelBase():
         self.backup = Backup(self.model_dir, self.name)
         self.gpus = gpus
         self.configfile = configfile
-        self.blocks = NNBlocks(use_subpixel=self.config["subpixel_upscaling"],
-                               use_icnr_init=self.config["icnr_init"],
-                               use_reflect_padding=self.config["reflect_padding"])
         self.input_shape = input_shape
         self.output_shape = None  # set after model is compiled
         self.encoder_dim = encoder_dim
         self.trainer = trainer
 
+        self.load_config()  # Load config if plugin has not already referenced it
         self.state = State(self.model_dir,
                            self.name,
                            self.config_changeable_items,
                            no_logs,
                            pingpong,
                            training_image_size)
+
+        self.blocks = NNBlocks(use_subpixel=self.config["subpixel_upscaling"],
+                               use_icnr_init=self.config["icnr_init"],
+                               use_convaware_init=self.config["conv_aware_init"],
+                               use_reflect_padding=self.config["reflect_padding"],
+                               first_run=self.state.first_run)
+
         self.is_legacy = False
         self.rename_legacy()
         self.load_state_info()
@@ -166,6 +171,14 @@ class ModelBase():
         from lib.model import memory_saving_gradients
         K.__dict__["gradients"] = memory_saving_gradients.gradients_memory
 
+    def load_config(self):
+        """ Load the global config for reference in self.config """
+        global _CONFIG  # pylint: disable=global-statement
+        if not _CONFIG:
+            model_name = self.config_section
+            logger.debug("Loading config for: %s", model_name)
+            _CONFIG = Config(model_name, configfile=self.configfile).config_dict
+
     def set_training_data(self):
         """ Override to set model specific training data.
 
@@ -235,9 +248,10 @@ class ModelBase():
         logger.debug("Setting input shape from state file: %s", input_shape)
         self.input_shape = input_shape
 
-    def add_network(self, network_type, side, network):
+    def add_network(self, network_type, side, network, is_output=False):
         """ Add a NNMeta object """
-        logger.debug("network_type: '%s', side: '%s', network: '%s'", network_type, side, network)
+        logger.debug("network_type: '%s', side: '%s', network: '%s', is_output: %s",
+                     network_type, side, network, is_output)
         filename = "{}_{}".format(self.name, network_type.lower())
         name = network_type.lower()
         if side:
@@ -246,7 +260,11 @@ class ModelBase():
             name += "_{}".format(side)
         filename += ".h5"
         logger.debug("name: '%s', filename: '%s'", name, filename)
-        self.networks[name] = NNMeta(str(self.model_dir / filename), network_type, side, network)
+        self.networks[name] = NNMeta(str(self.model_dir / filename),
+                                     network_type,
+                                     side,
+                                     network,
+                                     is_output)
 
     def add_predictor(self, side, model):
         """ Add a predictor to the predictors dictionary """
@@ -587,18 +605,20 @@ class NNMeta():
                 Otherwise the type should be completely unique.
     side:       A, B or None. Used to identify which networks can
                 be swapped.
-    network:      Define network to this.
+    network:    Define network to this.
+    is_output:  Set to True to indicate that this network is an output to the Autoencoder
     """
 
-    def __init__(self, filename, network_type, side, network):
+    def __init__(self, filename, network_type, side, network, is_output):
         logger.debug("Initializing %s: (filename: '%s', network_type: '%s', side: '%s', "
-                     "network: %s", self.__class__.__name__, filename, network_type,
-                     side, network)
+                     "network: %s, is_output: %s", self.__class__.__name__, filename,
+                     network_type, side, network, is_output)
         self.filename = filename
         self.type = network_type.lower()
         self.side = side
         self.name = self.set_name()
         self.network = network
+        self.is_output = is_output
         self.network.name = self.name
         self.config = network.get_config()  # For pingpong restore
         self.weights = network.get_weights()  # For pingpong restore
@@ -699,6 +719,11 @@ class State():
         """ Return the current session dict """
         return self.sessions[self.session_id]
 
+    @property
+    def first_run(self):
+        """ Return True if this is the first run else False """
+        return self.session_id == 1
+
     def new_session_id(self):
         """ Return new session_id """
         if not self.sessions:
diff --git a/plugins/train/model/dfaker.py b/plugins/train/model/dfaker.py
index d097e20..fc4a650 100644
--- a/plugins/train/model/dfaker.py
+++ b/plugins/train/model/dfaker.py
@@ -4,7 +4,7 @@
 
 
 from keras.initializers import RandomNormal
-from keras.layers import Conv2D, Input
+from keras.layers import Input
 from keras.models import Model as KerasModel
 
 from .original import logger, Model as OriginalModel
@@ -48,7 +48,7 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, 128, res_block_follows=True)
         var_x = self.blocks.res_block(var_x, 128, kernel_initializer=self.kernel_initializer)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -57,6 +57,9 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
             var_y = self.blocks.upscale(var_y, 64)
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel([input_], outputs=outputs)
diff --git a/plugins/train/model/dfl_h128.py b/plugins/train/model/dfl_h128.py
index 87c7e66..1153fd0 100644
--- a/plugins/train/model/dfl_h128.py
+++ b/plugins/train/model/dfl_h128.py
@@ -3,7 +3,7 @@
     Based on https://github.com/iperov/DeepFaceLab
 """
 
-from keras.layers import Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import Dense, Flatten, Input, Reshape
 from keras.models import Model as KerasModel
 
 from .original import logger, Model as OriginalModel
@@ -44,7 +44,7 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, self.encoder_dim)
         var_x = self.blocks.upscale(var_x, self.encoder_dim // 2)
         var_x = self.blocks.upscale(var_x, self.encoder_dim // 4)
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
         # Mask
         if self.config.get("mask_type", None):
@@ -52,6 +52,9 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, self.encoder_dim)
             var_y = self.blocks.upscale(var_y, self.encoder_dim // 2)
             var_y = self.blocks.upscale(var_y, self.encoder_dim // 4)
-            var_y = Conv2D(1, kernel_size=5, padding="same", activation="sigmoid")(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/iae.py b/plugins/train/model/iae.py
index aaf7a29..e3d5a79 100644
--- a/plugins/train/model/iae.py
+++ b/plugins/train/model/iae.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 """ Improved autoencoder for faceswap """
 
-from keras.layers import Concatenate, Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import Concatenate, Dense, Flatten, Input, Reshape
 from keras.models import Model as KerasModel
 
 from ._base import ModelBase, logger
@@ -21,7 +21,7 @@ class Model(ModelBase):
         """ Add the IAE model weights """
         logger.debug("Adding networks")
         self.add_network("encoder", None, self.encoder())
-        self.add_network("decoder", None, self.decoder())
+        self.add_network("decoder", None, self.decoder(), is_output=True)
         self.add_network("intermediate", "a", self.intermediate())
         self.add_network("intermediate", "b", self.intermediate())
         self.add_network("inter", None, self.intermediate())
@@ -75,7 +75,7 @@ class Model(ModelBase):
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -84,6 +84,9 @@ class Model(ModelBase):
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
             var_y = self.blocks.upscale(var_y, 64)
-            var_y = Conv2D(1, kernel_size=5, padding="same", activation="sigmoid")(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/lightweight.py b/plugins/train/model/lightweight.py
index d19511c..703d8cc 100644
--- a/plugins/train/model/lightweight.py
+++ b/plugins/train/model/lightweight.py
@@ -3,7 +3,7 @@
     Based on the original https://www.reddit.com/r/deepfakes/
     code sample + contribs """
 
-from keras.layers import Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import Dense, Flatten, Input, Reshape
 from keras.models import Model as KerasModel
 
 from .original import logger, Model as OriginalModel
@@ -40,7 +40,7 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, 512)
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -48,6 +48,9 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/original.py b/plugins/train/model/original.py
index 43de0e7..8fbf40e 100644
--- a/plugins/train/model/original.py
+++ b/plugins/train/model/original.py
@@ -3,7 +3,7 @@
     Based on the original https://www.reddit.com/r/deepfakes/
     code sample + contribs """
 
-from keras.layers import Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import Dense, Flatten, Input, Reshape
 
 from keras.models import Model as KerasModel
 
@@ -28,8 +28,8 @@ class Model(ModelBase):
     def add_networks(self):
         """ Add the original model weights """
         logger.debug("Adding networks")
-        self.add_network("decoder", "a", self.decoder())
-        self.add_network("decoder", "b", self.decoder())
+        self.add_network("decoder", "a", self.decoder(), is_output=True)
+        self.add_network("decoder", "b", self.decoder(), is_output=True)
         self.add_network("encoder", None, self.encoder())
         logger.debug("Added networks")
 
@@ -71,7 +71,7 @@ class Model(ModelBase):
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -79,6 +79,9 @@ class Model(ModelBase):
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
             var_y = self.blocks.upscale(var_y, 64)
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/realface.py b/plugins/train/model/realface.py
index 64951ed..5e268f9 100644
--- a/plugins/train/model/realface.py
+++ b/plugins/train/model/realface.py
@@ -9,7 +9,7 @@
     """
 
 from keras.initializers import RandomNormal
-from keras.layers import Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import Dense, Flatten, Input, Reshape
 from keras.models import Model as KerasModel
 
 from ._base import ModelBase, logger
@@ -59,10 +59,10 @@ class Model(ModelBase):
 
     def get_dense_width_upscalers_numbers(self):
         """ Return the dense width and number of upscalers """
-        output_size = self.config['output_size']
+        output_size = self.config["output_size"]
         sides = [(output_size // 2**n, n) for n in [4, 5] if (output_size // 2**n) < 10]
         closest = min([x * self._downscale_ratio for x, _ in sides],
-                      key=lambda x: abs(x - self.config['input_size']))
+                      key=lambda x: abs(x - self.config["input_size"]))
         dense_width, upscalers_no = [(s, n) for s, n in sides
                                      if s * self._downscale_ratio == closest][0]
         logger.debug("dense_width: %s, upscalers_no: %s", dense_width, upscalers_no)
@@ -71,8 +71,8 @@ class Model(ModelBase):
     def add_networks(self):
         """ Add the original model weights """
         logger.debug("Adding networks")
-        self.add_network("decoder", "a", self.decoder_a())
-        self.add_network("decoder", "b", self.decoder_b())
+        self.add_network("decoder", "a", self.decoder_a(), is_output=True)
+        self.add_network("decoder", "b", self.decoder_b(), is_output=True)
         self.add_network("encoder", None, self.encoder())
         logger.debug("Added networks")
 
@@ -97,7 +97,7 @@ class Model(ModelBase):
         input_ = Input(shape=self.input_shape)
         var_x = input_
 
-        encoder_complexity = self.config['complexity_encoder']
+        encoder_complexity = self.config["complexity_encoder"]
 
         for idx in range(self.downscalers_no - 1):
             var_x = self.blocks.conv(var_x, encoder_complexity * 2**idx)
@@ -110,13 +110,13 @@ class Model(ModelBase):
 
     def decoder_b(self):
         """ RealFace Decoder Network """
-        input_filters = self.config['complexity_encoder'] * 2**(self.downscalers_no-1)
-        input_width = self.config['input_size'] // self._downscale_ratio
+        input_filters = self.config["complexity_encoder"] * 2**(self.downscalers_no-1)
+        input_width = self.config["input_size"] // self._downscale_ratio
         input_ = Input(shape=(input_width, input_width, input_filters))
 
         var_xy = input_
 
-        var_xy = Dense(self.config['dense_nodes'])(Flatten()(var_xy))
+        var_xy = Dense(self.config["dense_nodes"])(Flatten()(var_xy))
         var_xy = Dense(self.dense_width * self.dense_width * self.dense_filters)(var_xy)
         var_xy = Reshape((self.dense_width, self.dense_width, self.dense_filters))(var_xy)
         var_xy = self.blocks.upscale(var_xy, self.dense_filters)
@@ -124,14 +124,14 @@ class Model(ModelBase):
         var_x = var_xy
         var_x = self.blocks.res_block(var_x, self.dense_filters, use_bias=False)
 
-        decoder_b_complexity = self.config['complexity_decoder']
+        decoder_b_complexity = self.config["complexity_decoder"]
         for idx in range(self.upscalers_no - 2):
             var_x = self.blocks.upscale(var_x, decoder_b_complexity // 2**idx)
             var_x = self.blocks.res_block(var_x, decoder_b_complexity // 2**idx, use_bias=False)
             var_x = self.blocks.res_block(var_x, decoder_b_complexity // 2**idx, use_bias=True)
         var_x = self.blocks.upscale(var_x, decoder_b_complexity // 2**(idx + 1))
 
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
 
         outputs = [var_x]
 
@@ -142,7 +142,10 @@ class Model(ModelBase):
                 var_y = self.blocks.upscale(var_y, mask_b_complexity // 2**idx)
             var_y = self.blocks.upscale(var_y, mask_b_complexity // 2**(idx + 1))
 
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
 
             outputs += [var_y]
 
@@ -150,13 +153,13 @@ class Model(ModelBase):
 
     def decoder_a(self):
         """ RealFace Decoder (A) Network """
-        input_filters = self.config['complexity_encoder'] * 2**(self.downscalers_no-1)
-        input_width = self.config['input_size'] // self._downscale_ratio
+        input_filters = self.config["complexity_encoder"] * 2**(self.downscalers_no-1)
+        input_width = self.config["input_size"] // self._downscale_ratio
         input_ = Input(shape=(input_width, input_width, input_filters))
 
         var_xy = input_
 
-        dense_nodes = int(self.config['dense_nodes']/1.5)
+        dense_nodes = int(self.config["dense_nodes"]/1.5)
         dense_filters = int(self.dense_filters/1.5)
 
         var_xy = Dense(dense_nodes)(Flatten()(var_xy))
@@ -168,12 +171,12 @@ class Model(ModelBase):
         var_x = var_xy
         var_x = self.blocks.res_block(var_x, dense_filters, use_bias=False)
 
-        decoder_a_complexity = int(self.config['complexity_decoder'] / 1.5)
+        decoder_a_complexity = int(self.config["complexity_decoder"] / 1.5)
         for idx in range(self.upscalers_no-2):
             var_x = self.blocks.upscale(var_x, decoder_a_complexity // 2**idx)
         var_x = self.blocks.upscale(var_x, decoder_a_complexity // 2**(idx + 1))
 
-        var_x = Conv2D(3, kernel_size=5, padding="same", activation="sigmoid")(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
 
         outputs = [var_x]
 
@@ -184,7 +187,10 @@ class Model(ModelBase):
                 var_y = self.blocks.upscale(var_y, mask_a_complexity // 2**idx)
             var_y = self.blocks.upscale(var_y, mask_a_complexity // 2**(idx + 1))
 
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
 
             outputs += [var_y]
 
diff --git a/plugins/train/model/unbalanced.py b/plugins/train/model/unbalanced.py
index 25f9e51..0d4709f 100644
--- a/plugins/train/model/unbalanced.py
+++ b/plugins/train/model/unbalanced.py
@@ -4,7 +4,7 @@
         code sample + contribs """
 
 from keras.initializers import RandomNormal
-from keras.layers import Conv2D, Dense, Flatten, Input, Reshape, SpatialDropout2D
+from keras.layers import Dense, Flatten, Input, Reshape, SpatialDropout2D
 from keras.models import Model as KerasModel
 
 from .original import logger, Model as OriginalModel
@@ -28,8 +28,8 @@ class Model(OriginalModel):
     def add_networks(self):
         """ Add the original model weights """
         logger.debug("Adding networks")
-        self.add_network("decoder", "a", self.decoder_a())
-        self.add_network("decoder", "b", self.decoder_b())
+        self.add_network("decoder", "a", self.decoder_a(), is_output=True)
+        self.add_network("decoder", "b", self.decoder_b(), is_output=True)
         self.add_network("encoder", None, self.encoder())
         logger.debug("Added networks")
 
@@ -73,7 +73,7 @@ class Model(OriginalModel):
             var_x = SpatialDropout2D(0.25)(var_x)
         var_x = self.blocks.upscale(var_x, decoder_complexity // 2, **kwargs)
         var_x = self.blocks.upscale(var_x, decoder_complexity // 4, **kwargs)
-        var_x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -82,7 +82,10 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             var_y = self.blocks.upscale(var_y, decoder_complexity // 2)
             var_y = self.blocks.upscale(var_y, decoder_complexity // 4)
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
 
@@ -114,7 +117,7 @@ class Model(OriginalModel):
             var_x = self.blocks.res_block(var_x, decoder_complexity // 2,
                                           kernel_initializer=self.kernel_initializer)
             var_x = self.blocks.upscale(var_x, decoder_complexity // 4, **kwargs)
-        var_x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -126,6 +129,9 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, decoder_complexity // 4)
             if self.lowmem:
                 var_y = self.blocks.upscale(var_y, decoder_complexity // 8)
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/villain.py b/plugins/train/model/villain.py
index fd873b9..f43af37 100644
--- a/plugins/train/model/villain.py
+++ b/plugins/train/model/villain.py
@@ -4,7 +4,7 @@
     Adapted from a model by VillainGuy (https://github.com/VillainGuy) """
 
 from keras.initializers import RandomNormal
-from keras.layers import add, Conv2D, Dense, Flatten, Input, Reshape
+from keras.layers import add, Dense, Flatten, Input, Reshape
 from keras.models import Model as KerasModel
 
 from lib.model.layers import PixelShuffler
@@ -71,7 +71,7 @@ class Model(OriginalModel):
         var_x = self.blocks.res_block(var_x, 256, **kwargs)
         var_x = self.blocks.upscale(var_x, self.input_shape[0], res_block_follows=True, **kwargs)
         var_x = self.blocks.res_block(var_x, self.input_shape[0], **kwargs)
-        var_x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(var_x)
+        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -79,6 +79,9 @@ class Model(OriginalModel):
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, self.input_shape[0])
-            var_y = Conv2D(1, kernel_size=5, padding='same', activation='sigmoid')(var_y)
+            var_y = self.blocks.conv2d(var_y, 1,
+                                       kernel_size=5,
+                                       padding="same",
+                                       activation="sigmoid")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
