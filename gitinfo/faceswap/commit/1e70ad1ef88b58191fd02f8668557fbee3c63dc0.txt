commit 1e70ad1ef88b58191fd02f8668557fbee3c63dc0
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Thu Aug 9 15:24:08 2018 +0100

    Merge staging to Master (#470)
    
    * DLib scaling amends (#461)
    
    * Scaling area fix and rework of frames and bounding boxes
    
    * Update DLib scaling and initialization
    
    * Extract - Detector reinitialization bugfix
    
    * Slight OriginaHightRes Model re-factoring (#464)
    
    * Model re-factoring, might fix red screen of death on multi-gpu configurations
    
    * Limited trainer support for various INPUT_SIZE
    
    * Auto Extraction multiprocess bugfix
    
    * Sort tool bugfixes and linting

diff --git a/lib/face_alignment/detectors.py b/lib/face_alignment/detectors.py
index 298080c..c1f7b4e 100644
--- a/lib/face_alignment/detectors.py
+++ b/lib/face_alignment/detectors.py
@@ -58,11 +58,8 @@ class DLibDetector(Detector):
                             "the lib!".format(data_path))
         return data_path
 
-    def create_detector(self, verbose, detector):
+    def create_detector(self, verbose, detector, placeholder):
         """ Add the requested detectors """
-        if self.initialized:
-            return
-
         self.verbose = verbose
 
         if detector == "dlib-cnn" or detector == "dlib-all":
@@ -76,16 +73,16 @@ class DLibDetector(Detector):
                 print("Adding DLib - HOG detector")
             self.detectors.append(dlib.get_frontal_face_detector())
 
+        for current_detector in self.detectors:
+            current_detector(placeholder, 0)
+
         self.initialized = True
 
-    def detect_faces(self, images):
-        """ Detect faces in images """
+    def detect_faces(self, image):
+        """ Detect faces in rgb image """
         self.detected_faces = None
-        for current_detector, current_image in(
-                (current_detector, current_image)
-                for current_detector in self.detectors
-                for current_image in images):
-            self.detected_faces = current_detector(current_image, 0)
+        for current_detector in self.detectors:
+            self.detected_faces = current_detector(image, 0)
 
             if self.detected_faces:
                 break
@@ -130,9 +127,6 @@ class MTCNNDetector(Detector):
 
     def create_detector(self, verbose, mtcnn_kwargs):
         """ Create the mtcnn detector """
-        if self.initialized:
-            return
-
         self.verbose = verbose
 
         if self.verbose:
@@ -152,13 +146,10 @@ class MTCNNDetector(Detector):
         self.kwargs["onet"] = onet
         self.initialized = True
 
-    def detect_faces(self, images):
-        """ Detect faces in images """
+    def detect_faces(self, image):
+        """ Detect faces in rgb image """
         self.detected_faces = None
-        for current_image in images:
-            detected_faces = detect_face(current_image, **self.kwargs)
-            self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
-                                                  int(face[2]), int(face[3]))
-                                   for face in detected_faces]
-            if self.detected_faces:
-                break
+        detected_faces = detect_face(image, **self.kwargs)
+        self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
+                                              int(face[2]), int(face[3]))
+                               for face in detected_faces]
diff --git a/lib/face_alignment/extractor.py b/lib/face_alignment/extractor.py
index 229fc1e..80d2905 100644
--- a/lib/face_alignment/extractor.py
+++ b/lib/face_alignment/extractor.py
@@ -25,61 +25,76 @@ class Frame(object):
         self.verbose = verbose
         self.height, self.width = input_image.shape[:2]
 
-        if not VRAM.scale_to and VRAM.device != -1:
-            VRAM.set_scale_to(detector)
+        self.input_scale = 1.0
 
-        if VRAM.device != -1:
-            self.scale_to = VRAM.scale_to
-        else:
-            self.scale_to = self.height * self.width
+        self.image_bgr = input_image
+        self.image_rgb = input_image[:, :, ::-1].copy()
+        self.image_detect = self.scale_image(input_is_predetected_face,
+                                             detector)
 
-        self.input_scale = 1.0
-        self.images = self.process_input(input_image,
-                                         input_is_predetected_face)
-
-    def process_input(self, input_image, input_is_predetected_face):
-        """ Process import image:
-            Size down if required
-            Duplicate into rgb colour space """
-        if not input_is_predetected_face:
-            input_image = self.scale_down(input_image)
-        return self.compile_color_space(input_image)
-
-    def scale_down(self, image):
+    def scale_image(self, input_is_predetected_face, detector):
         """ Scale down large images based on vram amount """
-        pixel_count = self.width * self.height
+        image = self.image_rgb
+        if input_is_predetected_face:
+            return image
 
-        if pixel_count > self.scale_to:
-            self.input_scale = self.scale_to / pixel_count
-            dimensions = (int(self.width * self.input_scale),
-                          int(self.height * self.input_scale))
-            if self.verbose:
-                print("Resizing image from {}x{} "
-                      "to {}.".format(str(self.width), str(self.height),
-                                      "x".join(str(i) for i in dimensions)))
-            image = cv2.resize(image, dimensions, interpolation=cv2.INTER_AREA)
+        if detector == "mtcnn":
+            self.scale_mtcnn()
+        else:
+            self.scale_dlib()
+
+        if self.input_scale == 1.0:
+            return image
+
+        if self.input_scale > 1.0:
+            interpolation = cv2.INTER_LINEAR
+        else:
+            interpolation = cv2.INTER_AREA
+
+        dimensions = (int(self.width * self.input_scale),
+                      int(self.height * self.input_scale))
+        if self.verbose and self.input_scale < 1.0:
+            print("Resizing image from {}x{} "
+                  "to {}.".format(str(self.width), str(self.height),
+                                  "x".join(str(i) for i in dimensions)))
+        image = cv2.resize(image,
+                           dimensions,
+                           interpolation=interpolation).copy()
 
         return image
 
-    @staticmethod
-    def compile_color_space(image_bgr):
-        """ cv2 and numpy inputs differs in rgb-bgr order
-        this affects chance of dlib face detection so
-        pass both versions """
-        image_rgb = image_bgr[:, :, ::-1].copy()
-        return (image_rgb, image_bgr)
+    def scale_mtcnn(self):
+        """ Set scaling for mtcnn """
+        pixel_count = self.width * self.height
+        if pixel_count > VRAM.scale_to:
+            self.input_scale = (VRAM.scale_to / pixel_count)**0.5
+
+    def scale_dlib(self):
+        """ Set scaling for dlib
+
+        DLIB is finickity, and pure pixel count won't help as when an
+        initial portrait image goes in, rotating it to landscape sucks
+        up VRAM for no discernible reason. This does not happen when the
+        initial image is a landscape image.
+        To mitigate this we need to make sure that all images fit within
+        a square based on the pixel count
+        There is also no way to set the acceptable size for a positive
+        match, so all images should be scaled to the maximum possible
+        to detect all available faces """
+
+        max_length_scale = int(VRAM.scale_to ** 0.5)
+        max_length_image = max(self.height, self.width)
+        self.input_scale = max_length_scale / max_length_image
 
 
 class Align(object):
     """ Perform transformation to align and get landmarks """
-    def __init__(self, frame, detected_faces, keras_model, verbose):
+    def __init__(self, image, detected_faces, keras_model, verbose):
         self.verbose = verbose
-        self.frame = frame.images[0]
-        self.input_scale = frame.input_scale
+        self.image = image
         self.detected_faces = detected_faces
         self.keras = keras_model
 
-        self.bounding_box = None
         self.landmarks = self.process_landmarks()
 
     @staticmethod
@@ -167,52 +182,43 @@ class Align(object):
                 print("Warning: No faces were detected.")
             return landmarks
 
-        for d_rect in self.detected_faces:
-            self.get_bounding_box(d_rect)
-            del d_rect
+        for detected_face in self.detected_faces:
 
-            center, scale = self.get_center_scale()
+            center, scale = self.get_center_scale(detected_face)
             image = self.align_image(center, scale)
 
             landmarks_xy = self.predict_landmarks(image, center, scale)
 
-            landmarks.append((
-                (int(self.bounding_box['left'] / self.input_scale),
-                 int(self.bounding_box['top'] / self.input_scale),
-                 int(self.bounding_box['right'] / self.input_scale),
-                 int(self.bounding_box['bottom'] / self.input_scale)),
-                landmarks_xy))
+            landmarks.append(((detected_face['left'],
+                               detected_face['top'],
+                               detected_face['right'],
+                               detected_face['bottom']),
+                              landmarks_xy))
 
         return landmarks
 
-    def get_bounding_box(self, d_rect):
-        """ Return the corner points of the bounding box """
-        self.bounding_box = {'left': d_rect.left(),
-                             'top': d_rect.top(),
-                             'right': d_rect.right(),
-                             'bottom': d_rect.bottom()}
-
-    def get_center_scale(self):
+    @staticmethod
+    def get_center_scale(detected_face):
         """ Get the center and set scale of bounding box """
-        center = np.array([(self.bounding_box['left']
-                            + self.bounding_box['right']) / 2.0,
-                           (self.bounding_box['top']
-                            + self.bounding_box['bottom']) / 2.0])
+        center = np.array([(detected_face['left']
+                            + detected_face['right']) / 2.0,
+                           (detected_face['top']
+                            + detected_face['bottom']) / 2.0])
 
-        center[1] -= (self.bounding_box['bottom']
-                      - self.bounding_box['top']) * 0.12
+        center[1] -= (detected_face['bottom']
+                      - detected_face['top']) * 0.12
 
-        scale = (self.bounding_box['right']
-                 - self.bounding_box['left']
-                 + self.bounding_box['bottom']
-                 - self.bounding_box['top']) / 195.0
+        scale = (detected_face['right']
+                 - detected_face['left']
+                 + detected_face['bottom']
+                 - detected_face['top']) / 195.0
 
         return center, scale
 
     def align_image(self, center, scale):
         """ Crop and align image around center """
         image = self.crop(
-            self.frame,
+            self.image,
             center,
             scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
 
@@ -226,9 +232,7 @@ class Align(object):
                 center,
                 scale)
 
-        return [(int(pt[0] / self.input_scale),
-                 int(pt[1] / self.input_scale))
-                for pt in pts_img]
+        return [(int(pt[0]), int(pt[1])) for pt in pts_img]
 
 
 class Extract(object):
@@ -237,7 +241,6 @@ class Extract(object):
 
     def __init__(self, input_image_bgr, detector, mtcnn_kwargs=None,
                  verbose=False, input_is_predetected_face=False):
-        self.initialized = False
         self.verbose = verbose
         self.keras = KERAS_MODEL
         self.detector = None
@@ -250,27 +253,36 @@ class Extract(object):
                            input_is_predetected_face=input_is_predetected_face)
 
         self.detect_faces(input_is_predetected_face)
-        self.convert_to_dlib_rectangle()
+        self.bounding_boxes = self.get_bounding_boxes()
 
-        self.landmarks = Align(frame=self.frame,
-                               detected_faces=self.detector.detected_faces,
+        self.landmarks = Align(image=self.frame.image_rgb,
+                               detected_faces=self.bounding_boxes,
                                keras_model=self.keras,
                                verbose=self.verbose).landmarks
 
     def initialize(self, detector, mtcnn_kwargs):
         """ initialize Keras and Dlib """
-        if self.initialized:
-            return
-        self.initialize_vram(detector)
+        if not VRAM.initialized:
+            self.initialize_vram(detector)
+
+        if not self.keras.initialized:
+            self.initialize_keras(detector)
+            # VRAM Scaling factor must be set AFTER Keras has loaded
+            VRAM.set_scale_to(detector)
+
+        if detector == "mtcnn":
+            self.detector = MTCNN_DETECTOR
+        else:
+            self.detector = DLIB_DETECTORS
 
-        self.initialize_keras(detector)
-        self.initialize_detector(detector, mtcnn_kwargs)
-        self.initialized = True
+        if not self.detector.initialized:
+            self.initialize_detector(detector, mtcnn_kwargs)
 
     def initialize_vram(self, detector):
         """ Initialize vram based on detector """
         VRAM.verbose = self.verbose
         VRAM.detector = detector
+        VRAM.initialized = True
         VRAM.output_stats()
 
     def initialize_keras(self, detector):
@@ -288,12 +300,18 @@ class Extract(object):
         """ Initialize face detector """
         kwargs = {"verbose": self.verbose}
         if detector == "mtcnn":
-            self.detector = MTCNN_DETECTOR
             mtcnn_kwargs = self.detector.validate_kwargs(mtcnn_kwargs)
             kwargs["mtcnn_kwargs"] = mtcnn_kwargs
         else:
-            self.detector = DLIB_DETECTORS
             kwargs["detector"] = detector
+            scale_to = int(VRAM.scale_to ** 0.5)
+
+            if self.verbose:
+                print("Initializing DLib for frame size {}x{}".format(
+                    str(scale_to), str(scale_to)))
+
+            placeholder = np.zeros((scale_to, scale_to, 3), dtype=np.uint8)
+            kwargs["placeholder"] = placeholder
 
         self.detector.create_detector(**kwargs)
 
@@ -307,13 +325,24 @@ class Extract(object):
         if input_is_predetected_face:
             self.detector.set_predetected(self.frame.width, self.frame.height)
         else:
-            self.detector.detect_faces(self.frame.images)
-
-    def convert_to_dlib_rectangle(self):
+            self.detector.detect_faces(self.frame.image_detect)
+
+    def get_bounding_boxes(self):
+        """ Return the corner points of the bounding box scaled
+            to original image """
+        bounding_boxes = list()
+        for d_rect in self.detector.detected_faces:
+            d_rect = self.convert_to_dlib_rectangle(d_rect)
+            bounding_box = {
+                'left': int(d_rect.left() / self.frame.input_scale),
+                'top': int(d_rect.top() / self.frame.input_scale),
+                'right': int(d_rect.right() / self.frame.input_scale),
+                'bottom': int(d_rect.bottom() / self.frame.input_scale)}
+            bounding_boxes.append(bounding_box)
+        return bounding_boxes
+
+    def convert_to_dlib_rectangle(self, d_rect):
         """ Convert detected faces to dlib_rectangle """
-        detected = [d_rect.rect
-                    if self.detector.is_mmod_rectangle(d_rect)
-                    else d_rect
-                    for d_rect in self.detector.detected_faces]
-
-        self.detector.detected_faces = detected
+        if self.detector.is_mmod_rectangle(d_rect):
+            return d_rect.rect
+        return d_rect
diff --git a/lib/face_alignment/model.py b/lib/face_alignment/model.py
index fc2faa9..1db8520 100644
--- a/lib/face_alignment/model.py
+++ b/lib/face_alignment/model.py
@@ -104,9 +104,6 @@ class KerasModel(object):
 
     def load_model(self, verbose, dummy, ratio):
         """ Load the Keras Model """
-        if self.initialized:
-            return
-
         self.verbose = verbose
         if self.verbose:
             print("Initializing keras model...")
diff --git a/lib/face_alignment/vram_allocation.py b/lib/face_alignment/vram_allocation.py
index 57f2846..239089a 100644
--- a/lib/face_alignment/vram_allocation.py
+++ b/lib/face_alignment/vram_allocation.py
@@ -9,8 +9,8 @@ class GPUMem(object):
         and the ratio of vram to use for tensorflow """
 
     def __init__(self):
+        self.initialized = False
         self.verbose = False
-        self.output_shown = False
         self.stats = GPUStats()
         self.vram_free = None
         self.vram_total = None
@@ -50,13 +50,12 @@ class GPUMem(object):
 
     def output_stats(self):
         """ Output stats in verbose mode """
-        if self.output_shown or not self.verbose:
+        if not self.verbose:
             return
         print("\n----- Initial GPU Stats -----")
         self.stats.print_info()
         print("GPU VRAM free:    {}".format(self.vram_free))
         print("-----------------------------\n")
-        self.output_shown = True
 
     def get_tensor_gpu_ratio(self):
         """ Set the ratio of GPU memory to use for tensorflow session for
@@ -116,7 +115,12 @@ class GPUMem(object):
             gradient = 213 / 524288
             constant = 307
 
-        free_mem = self.vram_free - buffer
+        if self.device != -1:
+            free_mem = self.vram_free - buffer
+        else:
+            # Limit to 2GB if using CPU
+            free_mem = 2048
+
         self.scale_to = int((free_mem - constant) / gradient)
 
         if self.scale_to < 4097:
diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
index d6748b3..cb4a719 100644
--- a/plugins/Model_OriginalHighRes/Model.py
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -44,6 +44,9 @@ class EncoderType(enum.Enum):
     SHAOANLU = "shaoanlu"    
             
 
+_kern_init = RandomNormal(0, 0.02)
+
+
 def inst_norm():
     return InstanceNormalization()
 
@@ -148,21 +151,21 @@ class Model():
     
     def conv(self, filters, kernel_size=5, strides=2, **kwargs):
         def block(x):
-            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)         
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)         
             x = LeakyReLU(0.1)(x)
             return x
         return block   
 
     def conv_sep(self, filters, kernel_size=5, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             x = Activation("relu")(x)
             return x    
         return block 
         
-    def conv_sep3(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
+    def conv_inst_norm(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)        
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)        
             if use_instance_norm:
                 x = inst_norm()(x)
             x = Activation("relu")(x)
@@ -172,16 +175,16 @@ class Model():
     def upscale(self, filters, **kwargs):
         def block(x):
             x = Conv2D(filters * 4, kernel_size=3, padding='same',
-                       kernel_initializer=RandomNormal(0, 0.02))(x)
+                       kernel_initializer=_kern_init)(x)
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
         return block  
     
-    def upscale_sep3(self, filters, use_instance_norm=True, **kwargs):
+    def upscale_inst_norm(self, filters, use_instance_norm=True, **kwargs):
         def block(x):
             x = Conv2D(filters*4, kernel_size=3, use_bias=False, 
-                       kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
+                       kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             if use_instance_norm:
                 x = inst_norm()(x)
             x = LeakyReLU(0.1)(x)
@@ -200,8 +203,8 @@ class Model():
         x = self.conv_sep(1024)(x)
         
         dense_shape = self.IMAGE_SHAPE[0] // 16         
-        x = Dense(self.ENCODER_DIM, kernel_initializer=RandomNormal(0, 0.02))(Flatten()(x))
-        x = Dense(dense_shape * dense_shape * 512, kernel_initializer=RandomNormal(0, 0.02))(x)
+        x = Dense(self.ENCODER_DIM, kernel_initializer=_kern_init)(Flatten()(x))
+        x = Dense(dense_shape * dense_shape * 512, kernel_initializer=_kern_init)(x)
         x = Reshape((dense_shape, dense_shape, 512))(x)
         x = self.upscale(512)(x)
         
@@ -214,14 +217,14 @@ class Model():
         in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
         
         x = Conv2D(in_conv_filters, kernel_size=5, use_bias=False, padding="same")(impt)
-        x = self.conv_sep3(in_conv_filters+32, use_instance_norm=False)(x)
-        x = self.conv_sep3(256)(x)        
-        x = self.conv_sep3(512)(x)
-        x = self.conv_sep3(1024)(x)        
+        x = self.conv_inst_norm(in_conv_filters+32, use_instance_norm=False)(x)
+        x = self.conv_inst_norm(256)(x)        
+        x = self.conv_inst_norm(512)(x)
+        x = self.conv_inst_norm(1024)(x)        
         
         dense_shape = self.IMAGE_SHAPE[0] // 16         
-        x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(dense_shape * dense_shape * 768)(x)
+        x = Dense(self.ENCODER_DIM, kernel_initializer=_kern_init)(Flatten()(x))
+        x = Dense(dense_shape * dense_shape * 768, kernel_initializer=_kern_init)(x)
         x = Reshape((dense_shape, dense_shape, 768))(x)
         x = self.upscale(512)(x)
         
@@ -245,9 +248,9 @@ class Model():
         decoder_shape = self.IMAGE_SHAPE[0]//8        
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
         
-        x = self.upscale_sep3(512)(inpt)
-        x = self.upscale_sep3(256)(x)
-        x = self.upscale_sep3(self.IMAGE_SHAPE[0])(x)
+        x = self.upscale_inst_norm(512)(inpt)
+        x = self.upscale_inst_norm(256)(x)
+        x = self.upscale_inst_norm(self.IMAGE_SHAPE[0])(x)
         
         x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
         
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
index a7a7eb9..ea79d5b 100644
--- a/plugins/Model_OriginalHighRes/Trainer.py
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -22,7 +22,7 @@ class Trainer():
         from timeit import default_timer as clock
         self._clock = clock
         
-        generator = TrainingDataGenerator(self.random_transform_args, 160, 5, zoom=2)        
+        generator = TrainingDataGenerator(self.random_transform_args, 160, 5, zoom=self.model.IMAGE_SHAPE[0]//64)        
         
         self.images_A = generator.minibatchAB(fn_A, self.batch_size)
         self.images_B = generator.minibatchAB(fn_B, self.batch_size)
diff --git a/scripts/extract.py b/scripts/extract.py
index 0336408..2b966e5 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -33,7 +33,9 @@ class Extract(object):
         print('Starting, this may take a while...')
         Utils.set_verbosity(self.args.verbose)
 
-        if self.args.multiprocess and GPUStats().device_count == 0:
+        if (hasattr(self.args, 'multiprocess')
+                and self.args.multiprocess
+                and GPUStats().device_count == 0):
             # TODO Checking that there is no available GPU is not
             # necessarily an indicator of whether the user is actually
             # using the CPU. Maybe look to implement further checks on
diff --git a/tools/sort.py b/tools/sort.py
index bdfc072..87a03f0 100644
--- a/tools/sort.py
+++ b/tools/sort.py
@@ -21,12 +21,14 @@ from . import cli
 
 
 class Sort(object):
+    """ Sorts folders of faces based on input criteria """
     def __init__(self, arguments):
         self.args = arguments
         self.changes = None
         self.serializer = None
 
     def process(self):
+        """ Main processing function of the sort tool """
 
         # Setting default argument values that cannot be set by argparse
 
@@ -96,11 +98,12 @@ class Sort(object):
 
     # Methods for sorting
     def sort_blur(self):
+        """ Sort by blur amount """
         input_dir = self.args.input_dir
 
         print("Sorting by blur...")
-        img_list = [[x, self.estimate_blur(cv2.imread(x))]
-                    for x in
+        img_list = [[img, self.estimate_blur(cv2.imread(img))]
+                    for img in
                     tqdm(self.find_images(input_dir),
                          desc="Loading",
                          file=sys.stdout)]
@@ -111,12 +114,13 @@ class Sort(object):
         return img_list
 
     def sort_face(self):
+        """ Sort by face similarity """
         input_dir = self.args.input_dir
 
         print("Sorting by face similarity...")
 
-        img_list = [[x, face_recognition.face_encodings(cv2.imread(x))]
-                    for x in
+        img_list = [[img, face_recognition.face_encodings(cv2.imread(img))]
+                    for img in
                     tqdm(self.find_images(input_dir),
                          desc="Loading",
                          file=sys.stdout)]
@@ -130,8 +134,7 @@ class Sort(object):
             for j in range(i + 1, len(img_list)):
                 f1encs = img_list[i][1]
                 f2encs = img_list[j][1]
-                if f1encs is not None and f2encs is not None and len(
-                        f1encs) > 0 and len(f2encs) > 0:
+                if f1encs and f2encs:
                     score = face_recognition.face_distance(f1encs[0],
                                                            f2encs)[0]
                 else:
@@ -140,18 +143,19 @@ class Sort(object):
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1] = img_list[j_min_score]
-            img_list[j_min_score] = img_list[i + 1]
-
+            (img_list[i + 1],
+             img_list[j_min_score]) = (img_list[j_min_score],
+                                       img_list[i + 1])
         return img_list
 
     def sort_face_dissim(self):
+        """ Sort by face dissimilarity """
         input_dir = self.args.input_dir
 
         print("Sorting by face dissimilarity...")
 
-        img_list = [[x, face_recognition.face_encodings(cv2.imread(x)), 0]
-                    for x in
+        img_list = [[img, face_recognition.face_encodings(cv2.imread(img)), 0]
+                    for img in
                     tqdm(self.find_images(input_dir),
                          desc="Loading",
                          file=sys.stdout)]
@@ -176,22 +180,22 @@ class Sort(object):
         return img_list
 
     def sort_face_cnn(self):
-
+        """ Sort by dlib CNN similarity """
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn similarity...")
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir),
-                      desc="Loading",
-                      file=sys.stdout):
-            d = face_alignment.Extract(
-                input_image_bgr=cv2.imread(x),
+        for img in tqdm(self.find_images(input_dir),
+                        desc="Loading",
+                        file=sys.stdout):
+            landmarks = face_alignment.Extract(
+                input_image_bgr=cv2.imread(img),
                 detector='dlib-cnn',
                 verbose=True,
                 input_is_predetected_face=True).landmarks
-            img_list.append([x, np.array(d[0][1])
-                             if len(d) > 0
+            img_list.append([img, np.array(landmarks[0][1])
+                             if landmarks
                              else np.zeros((68, 2))])
 
         img_list_len = len(img_list)
@@ -208,27 +212,28 @@ class Sort(object):
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1] = img_list[j_min_score]
-            img_list[j_min_score] = img_list[i + 1]
-
+            (img_list[i + 1],
+             img_list[j_min_score]) = (img_list[j_min_score],
+                                       img_list[i + 1])
         return img_list
 
     def sort_face_cnn_dissim(self):
+        """ Sort by dlib CNN dissimilarity """
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn dissimilarity...")
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir),
-                      desc="Loading",
-                      file=sys.stdout):
-            d = face_alignment.Extract(
-                input_image_bgr=cv2.imread(x),
+        for img in tqdm(self.find_images(input_dir),
+                        desc="Loading",
+                        file=sys.stdout):
+            landmarks = face_alignment.Extract(
+                input_image_bgr=cv2.imread(img),
                 detector='dlib-cnn',
                 verbose=True,
                 input_is_predetected_face=True).landmarks
-            img_list.append([x, np.array(d[0][1])
-                             if len(d) > 0
+            img_list.append([img, np.array(landmarks[0][1])
+                             if landmarks
                              else np.zeros((68, 2)), 0])
 
         img_list_len = len(img_list)
@@ -251,19 +256,20 @@ class Sort(object):
         return img_list
 
     def sort_face_yaw(self):
+        """ Sort by yaw of face """
         input_dir = self.args.input_dir
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir),
-                      desc="Loading",
-                      file=sys.stdout):
-            d = face_alignment.Extract(
-                input_image_bgr=cv2.imread(x),
+        for img in tqdm(self.find_images(input_dir),
+                        desc="Loading",
+                        file=sys.stdout):
+            landmarks = face_alignment.Extract(
+                input_image_bgr=cv2.imread(img),
                 detector='dlib-cnn',
                 verbose=True,
                 input_is_predetected_face=True).landmarks
-            img_list.append([x,
-                             self.calc_landmarks_face_yaw(np.array(d[0][1]))])
+            img_list.append(
+                [img, self.calc_landmarks_face_yaw(np.array(landmarks[0][1]))])
 
         print("Sorting by face-yaw...")
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
@@ -271,13 +277,14 @@ class Sort(object):
         return img_list
 
     def sort_hist(self):
+        """ Sort by histogram of face similarity """
         input_dir = self.args.input_dir
 
         print("Sorting by histogram similarity...")
 
         img_list = [
-            [x, cv2.calcHist([cv2.imread(x)], [0], None, [256], [0, 256])]
-            for x in
+            [img, cv2.calcHist([cv2.imread(img)], [0], None, [256], [0, 256])]
+            for img in
             tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout)
         ]
 
@@ -293,19 +300,21 @@ class Sort(object):
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1] = img_list[j_min_score]
-            img_list[j_min_score] = img_list[i + 1]
-
+            (img_list[i + 1],
+             img_list[j_min_score]) = (img_list[j_min_score],
+                                       img_list[i + 1])
         return img_list
 
     def sort_hist_dissim(self):
+        """ Sort by histigram of face dissimilarity """
         input_dir = self.args.input_dir
 
         print("Sorting by histogram dissimilarity...")
 
         img_list = [
-            [x, cv2.calcHist([cv2.imread(x)], [0], None, [256], [0, 256]), 0]
-            for x in
+            [img,
+             cv2.calcHist([cv2.imread(img)], [0], None, [256], [0, 256]), 0]
+            for img in
             tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout)
         ]
 
@@ -328,6 +337,7 @@ class Sort(object):
 
     # Methods for grouping
     def group_blur(self, img_list):
+        """ Group into bins by blur """
         # Starting the binning process
         num_bins = self.args.num_bins
 
@@ -338,11 +348,9 @@ class Sort(object):
 
         print("Grouping by blur...")
         bins = [[] for _ in range(num_bins)]
-        image_index = 0
         for i in range(num_bins):
-            for j in range(num_per_bin):
-                bins[i].append(img_list[image_index][0])
-                image_index += 1
+            for idx in range(num_per_bin):
+                bins[i].append(img_list[idx][0])
 
         # If remainder is 0, nothing gets added to the last bin.
         for i in range(1, remainder + 1):
@@ -351,6 +359,7 @@ class Sort(object):
         return bins
 
     def group_face(self, img_list):
+        """ Group into bins by face similarity """
         print("Grouping by face similarity...")
 
         # Groups are of the form: group_num -> reference face
@@ -404,6 +413,7 @@ class Sort(object):
         return bins
 
     def group_face_cnn(self, img_list):
+        """ Group into bins by dlib CNN face similarity """
         print("Grouping by face-cnn similarity...")
 
         # Groups are of the form: group_num -> reference faces
@@ -448,6 +458,7 @@ class Sort(object):
         return bins
 
     def group_face_yaw(self, img_list):
+        """ Group into bins by yaw of face """
         # Starting the binning process
         num_bins = self.args.num_bins
 
@@ -458,11 +469,9 @@ class Sort(object):
 
         print("Grouping by face-yaw...")
         bins = [[] for _ in range(num_bins)]
-        image_index = 0
         for i in range(num_bins):
-            for j in range(num_per_bin):
-                bins[i].append(img_list[image_index][0])
-                image_index += 1
+            for idx in range(num_per_bin):
+                bins[i].append(img_list[idx][0])
 
         # If remainder is 0, nothing gets added to the last bin.
         for i in range(1, remainder + 1):
@@ -471,6 +480,7 @@ class Sort(object):
         return bins
 
     def group_hist(self, img_list):
+        """ Group into bins by histogram """
         print("Grouping by histogram...")
 
         # Groups are of the form: group_num -> reference histogram
@@ -506,6 +516,7 @@ class Sort(object):
 
     # Final process methods
     def final_process_rename(self, img_list):
+        """ Rename the files """
         output_dir = self.args.output_dir
 
         process_file = self.set_process_file_method(self.args.log_changes,
@@ -530,8 +541,8 @@ class Sort(object):
             dst = os.path.join(output_dir, '{:05d}_{}'.format(i, src_basename))
             try:
                 process_file(src, dst, self.changes)
-            except FileNotFoundError as e:
-                print(e)
+            except FileNotFoundError as err:
+                print(err)
                 print('fail to rename {}'.format(src))
 
         for i in tqdm(range(0, len(img_list)),
@@ -542,14 +553,15 @@ class Sort(object):
 
             try:
                 os.rename(src, dst)
-            except FileNotFoundError as e:
-                print(e)
+            except FileNotFoundError as err:
+                print(err)
                 print('fail to rename {}'.format(src))
 
         if self.args.log_changes:
             self.write_to_log(self.changes)
 
     def final_process_folders(self, bins):
+        """ Move the files to folders """
         output_dir = self.args.output_dir
 
         process_file = self.set_process_file_method(self.args.log_changes,
@@ -577,8 +589,8 @@ class Sort(object):
                 dst = os.path.join(output_dir, str(i), src_basename)
                 try:
                     process_file(src, dst, self.changes)
-                except FileNotFoundError as e:
-                    print(e)
+                except FileNotFoundError as err:
+                    print(err)
                     print('Failed to move {0} to {1}'.format(src, dst))
 
         if self.args.log_changes:
@@ -586,9 +598,10 @@ class Sort(object):
 
     # Various helper methods
     def write_to_log(self, changes):
+        """ Write the changes to log file """
         print("Writing sort log to: {}".format(self.args.log_file_path))
-        with open(self.args.log_file_path, 'w') as lf:
-            lf.write(self.serializer.marshal(changes))
+        with open(self.args.log_file_path, 'w') as lfile:
+            lfile.write(self.serializer.marshal(changes))
 
     def reload_images(self, group_method, img_list):
         """
@@ -603,46 +616,48 @@ class Sort(object):
         input_dir = self.args.input_dir
         print("Preparing to group...")
         if group_method == 'group_blur':
-            temp_list = [[x, self.estimate_blur(cv2.imread(x))]
-                         for x in
+            temp_list = [[img, self.estimate_blur(cv2.imread(img))]
+                         for img in
                          tqdm(self.find_images(input_dir),
                               desc="Reloading",
                               file=sys.stdout)]
         elif group_method == 'group_face':
-            temp_list = [[x, face_recognition.face_encodings(cv2.imread(x))]
-                         for x in
-                         tqdm(self.find_images(input_dir),
-                              desc="Reloading",
-                              file=sys.stdout)]
+            temp_list = [
+                [img, face_recognition.face_encodings(cv2.imread(img))]
+                for img in tqdm(self.find_images(input_dir),
+                                desc="Reloading",
+                                file=sys.stdout)]
         elif group_method == 'group_face_cnn':
             temp_list = []
-            for x in tqdm(self.find_images(input_dir),
-                          desc="Reloading",
-                          file=sys.stdout):
-                d = face_alignment.Extract(
-                    input_image_bgr=cv2.imread(x),
+            for img in tqdm(self.find_images(input_dir),
+                            desc="Reloading",
+                            file=sys.stdout):
+                landmarks = face_alignment.Extract(
+                    input_image_bgr=cv2.imread(img),
                     detector='dlib-cnn',
                     verbose=True,
                     input_is_predetected_face=True).landmarks
-                temp_list.append([x, np.array(d[0][1])
-                                  if len(d) > 0
+                temp_list.append([img, np.array(landmarks[0][1])
+                                  if landmarks
                                   else np.zeros((68, 2))])
         elif group_method == 'group_face_yaw':
             temp_list = []
-            for x in tqdm(self.find_images(input_dir),
-                          desc="Reloading",
-                          file=sys.stdout):
-                d = face_alignment.Extract(
-                    input_image_bgr=cv2.imread(x),
+            for img in tqdm(self.find_images(input_dir),
+                            desc="Reloading",
+                            file=sys.stdout):
+                landmarks = face_alignment.Extract(
+                    input_image_bgr=cv2.imread(img),
                     detector='dlib-cnn',
                     verbose=True,
                     input_is_predetected_face=True).landmarks
                 temp_list.append(
-                    [x, self.calc_landmarks_face_yaw(np.array(d[0][1]))])
+                    [img,
+                     self.calc_landmarks_face_yaw(np.array(landmarks[0][1]))])
         elif group_method == 'group_hist':
             temp_list = [
-                [x, cv2.calcHist([cv2.imread(x)], [0], None, [256], [0, 256])]
-                for x in
+                [img,
+                 cv2.calcHist([cv2.imread(img)], [0], None, [256], [0, 256])]
+                for img in
                 tqdm(self.find_images(input_dir),
                      desc="Reloading",
                      file=sys.stdout)
@@ -682,9 +697,10 @@ class Sort(object):
 
     @staticmethod
     def find_images(input_dir):
+        """ Return list of images at specified location """
         result = []
         extensions = [".jpg", ".png", ".jpeg"]
-        for root, dirs, files in os.walk(input_dir):
+        for root, _, files in os.walk(input_dir):
             for file in files:
                 if os.path.splitext(file)[1].lower() in extensions:
                     result.append(os.path.join(root, file))
@@ -692,6 +708,7 @@ class Sort(object):
 
     @staticmethod
     def estimate_blur(image):
+        """ Estimate the amount of blur an image has """
         if image.ndim == 3:
             image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
 
@@ -700,19 +717,21 @@ class Sort(object):
         return score
 
     @staticmethod
-    def calc_landmarks_face_pitch(fl):  # unused
-        t = ((fl[6][1] - fl[8][1]) + (fl[10][1] - fl[8][1])) / 2.0
-        b = fl[8][1]
-        return b - t
+    def calc_landmarks_face_pitch(flm):
+        """ UNUSED - Calculate the amount of pitch in a face """
+        var_t = ((flm[6][1] - flm[8][1]) + (flm[10][1] - flm[8][1])) / 2.0
+        var_b = flm[8][1]
+        return var_b - var_t
 
     @staticmethod
-    def calc_landmarks_face_yaw(fl):
-        var_l = ((fl[27][0] - fl[0][0])
-                 + (fl[28][0] - fl[1][0])
-                 + (fl[29][0] - fl[2][0])) / 3.0
-        var_r = ((fl[16][0] - fl[27][0])
-                 + (fl[15][0] - fl[28][0])
-                 + (fl[14][0] - fl[29][0])) / 3.0
+    def calc_landmarks_face_yaw(flm):
+        """ Calculate the amount of yaw in a face """
+        var_l = ((flm[27][0] - flm[0][0])
+                 + (flm[28][0] - flm[1][0])
+                 + (flm[29][0] - flm[2][0])) / 3.0
+        var_r = ((flm[16][0] - flm[27][0])
+                 + (flm[15][0] - flm[28][0])
+                 + (flm[14][0] - flm[29][0])) / 3.0
         return var_r - var_l
 
     @staticmethod
@@ -727,32 +746,38 @@ class Sort(object):
         if log_changes:
             if keep_original:
                 def process_file(src, dst, changes):
+                    """ Process file method if logging changes
+                        and keeping original """
                     copyfile(src, dst)
                     changes[src] = dst
 
-                return process_file
             else:
                 def process_file(src, dst, changes):
+                    """ Process file method if logging changes
+                        and not keeping original """
                     os.rename(src, dst)
                     changes[src] = dst
 
-                return process_file
         else:
             if keep_original:
                 def process_file(src, dst, changes):
+                    """ Process file method if not logging changes
+                        and keeping original """
                     copyfile(src, dst)
 
-                return process_file
             else:
                 def process_file(src, dst, changes):
+                    """ Process file method if not logging changes
+                        and not keeping original """
                     os.rename(src, dst)
-
-                return process_file
+        return process_file
 
     @staticmethod
     def set_renaming_method(log_changes):
+        """ Set the method for renaming files """
         if log_changes:
             def renaming(src, output_dir, i, changes):
+                """ Rename files  method if logging changes """
                 src_basename = os.path.basename(src)
 
                 __src = os.path.join(output_dir,
@@ -762,11 +787,9 @@ class Sort(object):
                     '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
                 changes[src] = dst
                 return __src, dst
-
-            return renaming
-
         else:
             def renaming(src, output_dir, i, changes):
+                """ Rename files method if not logging changes """
                 src_basename = os.path.basename(src)
 
                 src = os.path.join(output_dir,
@@ -775,11 +798,12 @@ class Sort(object):
                     output_dir,
                     '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
                 return src, dst
-
-            return renaming
+        return renaming
 
     @staticmethod
     def get_avg_score_hist(img1, references):
+        """ Return the average histogram score between a face and
+            reference image """
         scores = []
         for img2 in references:
             score = cv2.compareHist(img1, img2, cv2.HISTCMP_BHATTACHARYYA)
@@ -788,6 +812,8 @@ class Sort(object):
 
     @staticmethod
     def get_avg_score_faces(f1encs, references):
+        """ Return the average similarity score between a face and
+            reference image """
         scores = []
         for f2encs in references:
             score = face_recognition.face_distance(f1encs, f2encs)[0]
@@ -796,6 +822,8 @@ class Sort(object):
 
     @staticmethod
     def get_avg_score_faces_cnn(fl1, references):
+        """ Return the average dlib CNN similarity score
+            between a face and reference image """
         scores = []
         for fl2 in references:
             score = np.sum(np.absolute((fl2 - fl1).flatten()))
