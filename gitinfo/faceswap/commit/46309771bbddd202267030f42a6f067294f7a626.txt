commit 46309771bbddd202267030f42a6f067294f7a626
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Fri Nov 29 00:46:27 2019 +0000

    plugins.extract - Create ExtractMedia class for pipeline flow
    Bugfix - Fix memory leak in extract

diff --git a/plugins/extract/_base.py b/plugins/extract/_base.py
index 7e37a93..8cb3ffc 100644
--- a/plugins/extract/_base.py
+++ b/plugins/extract/_base.py
@@ -1,20 +1,18 @@
 #!/usr/bin/env python3
-""" Base class for Faceswap :mod:`~plugins.extract.detect` and :mod:`~plugins.extract.align`
-Plugins
+""" Base class for Faceswap :mod:`~plugins.extract.detect`, :mod:`~plugins.extract.align` and
+:mod:`~plugins.extract.mask` Plugins
 """
 import logging
 import os
 import sys
 
-import cv2
-import numpy as np
-
 from tensorflow.python import errors_impl as tf_errors  # pylint:disable=no-name-in-module
 
 from lib.multithreading import MultiThread
 from lib.queue_manager import queue_manager
 from lib.utils import GetModel, FaceswapError
 from ._config import Config
+from .pipeline import ExtractMedia
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
@@ -44,7 +42,7 @@ def _get_config(plugin_name, configfile=None):
 class Extractor():
     """ Extractor Plugin Object
 
-    All ``_base`` classes for Aligners and Detectors inherit from this class.
+    All ``_base`` classes for Aligners, Detectors and Maskers inherit from this class.
 
     This class sets up a pipeline for working with ML plugins.
 
@@ -96,6 +94,7 @@ class Extractor():
     --------
     plugins.extract.detect._base : Detector parent class for extraction plugins.
     plugins.extract.align._base : Aligner parent class for extraction plugins.
+    plugins.extract.mask._base : Masker parent class for extraction plugins.
     plugins.extract.pipeline : The extract pipeline that configures and calls all plugins
 
     """
@@ -139,6 +138,10 @@ class Extractor():
         self._threads = []
         """ list: Internal threads for this plugin """
 
+        self._extract_media = dict()
+        """ dict: The :class:`plugins.extract.pipeline.ExtractMedia` objects currently being
+        processed. Stored at input for pairing back up on output of extractor process """
+
         # << THE FOLLOWING PROTECTED ATTRIBUTES ARE SET IN PLUGIN TYPE _base.py >>> #
         self._plugin_type = None
         """ str: Plugin type. ``detect`` or ``align``
@@ -236,8 +239,8 @@ class Extractor():
         """ **Override method** (at `<plugin_type>` level)
 
         This method should be overridden at the `<plugin_type>` level (IE.
-        :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
-        be overridden within plugins themselves.
+        :mod:`plugins.extract.detect._base`, :mod:`plugins.extract.align._base` or
+        :mod:`plugins.extract.mask._base`) and should not be overridden within plugins themselves.
 
         Handles consistent finalization for all plugins that exist within that plugin type. Its
         input is always the output from :func:`process_output()`
@@ -253,10 +256,11 @@ class Extractor():
         """ **Override method** (at `<plugin_type>` level)
 
         This method should be overridden at the `<plugin_type>` level (IE.
-        :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
-        be overridden within plugins themselves.
+        :mod:`plugins.extract.detect._base`, :mod:`plugins.extract.align._base` or
+        :mod:`plugins.extract.mask._base`) and should not be overridden within plugins themselves.
 
-        Get items from the queue in batches of :attr:`batchsize`
+        Get :class:`~plugins.extract.pipeline.ExtractMedia` items from the queue in batches of
+        :attr:`batchsize`
 
         Parameters
         ----------
@@ -425,40 +429,19 @@ class Extractor():
         out_queue.put("EOF")
 
     # <<< QUEUE METHODS >>> #
-    @staticmethod
-    def _get_item(queue):
+    def _get_item(self, queue):
         """ Yield one item from a queue """
         item = queue.get()
-        if isinstance(item, dict):
-            logger.trace("item: %s, queue: %s",
-                         {k: v.shape if isinstance(v, np.ndarray) else v
-                          for k, v in item.items()},
-                         queue)
+        if isinstance(item, ExtractMedia):
+            logger.trace("filename: '%s', image shape: %s, detected_faces: %s, queue: %s, "
+                         "item: %s",
+                         item.filename, item.image_shape, item.detected_faces, queue, item)
+            self._extract_media[item.filename] = item
         else:
             logger.trace("item: %s, queue: %s", item, queue)
         return item
 
-    # <<< MISC UTILITY METHODS >>> #
-    def _convert_color(self, image):
-        """ Convert the image to the correct color format and strip alpha channel """
-        logger.trace("Converting image to color format: %s", self.colorformat)
-        if self.colorformat == "RGB":
-            cvt_image = image[..., 2::-1].copy()
-        elif self.colorformat == "GRAY":
-            cvt_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)
-        else:
-            cvt_image = image[..., :3].copy()
-        return cvt_image
-
     @staticmethod
     def _dict_lists_to_list_dicts(dictionary):
         """ Convert a dictionary of lists to a list of dictionaries """
         return [dict(zip(dictionary, val)) for val in zip(*dictionary.values())]
-
-    @staticmethod
-    def _remove_invalid_keys(dictionary, valid_keys):
-        """ Remove items from dict that are no longer required """
-        for key in list(dictionary.keys()):
-            if key not in valid_keys:
-                logger.trace("Removing from output: '%s'", key)
-                del dictionary[key]
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 701fbdd..e6dfc8b 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -4,16 +4,11 @@
 All Aligner Plugins should inherit from this class.
 See the override methods for which methods are required.
 
-The plugin will receive a dict containing:
-
->>> {"filename": [<filename of source frame>],
->>>  "image": [<source image>],
->>>  "detected_faces": [<list of DetectedFace objects]}
+The plugin will receive a :class:`~plugins.extract.pipeline.ExtractMedia` object.
 
 For each source item, the plugin must pass a dict to finalize containing:
 
 >>> {"filename": [<filename of source frame>],
->>>  "image": [<source image>],
 >>>  "landmarks": [list of 68 point face landmarks]
 >>>  "detected_faces": [<list of DetectedFace objects>]}
 """
@@ -22,10 +17,10 @@ For each source item, the plugin must pass a dict to finalize containing:
 import cv2
 import numpy as np
 
-from plugins.extract._base import Extractor, logger
+from plugins.extract._base import Extractor, logger, ExtractMedia
 
 
-class Aligner(Extractor):
+class Aligner(Extractor):  # pylint:disable=abstract-method
     """ Aligner plugin _base Object
 
     All Aligner plugins must inherit from this class
@@ -47,6 +42,7 @@ class Aligner(Extractor):
 
     See Also
     --------
+    plugins.extract.pipeline : The extraction pipeline for calling plugins
     plugins.extract.align : Aligner plugins
     plugins.extract._base : Parent class for all extraction plugins
     plugins.extract.detect._base : Detector parent class for extraction plugins.
@@ -64,7 +60,7 @@ class Aligner(Extractor):
 
         self._plugin_type = "align"
         self._faces_per_filename = dict()  # Tracking for recompiling face batches
-        self._rollover = []  # Items that are rolled over from the previous batch in get_batch
+        self._rollover = None  # Items that are rolled over from the previous batch in get_batch
         self._output_faces = []
         logger.debug("Initialized %s", self.__class__.__name__)
 
@@ -75,8 +71,11 @@ class Aligner(Extractor):
         Items are returned from the ``queue`` in batches of
         :attr:`~plugins.extract._base.Extractor.batchsize`
 
+        Items are received as :class:`~plugins.extract.pipeline.ExtractMedia` objects and converted
+        to ``dict`` for internal processing.
+
         To ensure consistent batch sizes for aligner the items are split into separate items for
-        each :class:`lib.faces_detect.DetectedFace` object.
+        each :class:`~lib.faces_detect.DetectedFace` object.
 
         Remember to put ``'EOF'`` to the out queue after processing
         the final batch
@@ -109,26 +108,25 @@ class Aligner(Extractor):
                 logger.trace("EOF received")
                 exhausted = True
                 break
-
             # Put frames with no faces into the out queue to keep TQDM consistent
-            if not item["detected_faces"]:
+            if not item.detected_faces:
                 self._queues["out"].put(item)
                 continue
 
-            for f_idx, face in enumerate(item["detected_faces"]):
-                face.image = self._convert_color(item["image"])
+            converted_image = item.get_image_copy(self.colorformat)
+            for f_idx, face in enumerate(item.detected_faces):
+                batch.setdefault("image", []).append(converted_image)
                 batch.setdefault("detected_faces", []).append(face)
-                batch.setdefault("filename", []).append(item["filename"])
-                batch.setdefault("image", []).append(item["image"])
+                batch.setdefault("filename", []).append(item.filename)
                 idx += 1
                 if idx == self.batchsize:
-                    frame_faces = len(item["detected_faces"])
+                    frame_faces = len(item.detected_faces)
                     if f_idx + 1 != frame_faces:
-                        self._rollover = {k: v[f_idx + 1:] if k == "detected_faces" else v
-                                          for k, v in item.items()}
+                        self._rollover = ExtractMedia(item.filename, item.image)
+                        self._rollover.add_detected_faces(item.detected_faces[f_idx + 1:])
                         logger.trace("Rolled over %s faces of %s to next batch for '%s'",
-                                     len(self._rollover["detected_faces"]),
-                                     frame_faces, item["filename"])
+                                     len(self._rollover.detected_faces), frame_faces,
+                                     item.filename)
                     break
         if batch:
             logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
@@ -138,20 +136,20 @@ class Aligner(Extractor):
         return exhausted, batch
 
     def _collect_item(self, queue):
-        """ Collect the item from the _rollover dict or from the queue
+        """ Collect the item from the :attr:`_rollover` dict or from the queue
             Add face count per frame to self._faces_per_filename for joining
             batches back up in finalize """
-        if self._rollover:
+        if self._rollover is not None:
             logger.trace("Getting from _rollover: (filename: `%s`, faces: %s)",
-                         self._rollover["filename"], len(self._rollover["detected_faces"]))
+                         self._rollover.filename, len(self._rollover.detected_faces))
             item = self._rollover
-            self._rollover = dict()
+            self._rollover = None
         else:
             item = self._get_item(queue)
             if item != "EOF":
                 logger.trace("Getting from queue: (filename: %s, faces: %s)",
-                             item["filename"], len(item["detected_faces"]))
-                self._faces_per_filename[item["filename"]] = len(item["detected_faces"])
+                             item.filename, len(item.detected_faces))
+                self._faces_per_filename[item.filename] = len(item.detected_faces)
         return item
 
     # <<< FINALIZE METHODS >>> #
@@ -160,49 +158,42 @@ class Aligner(Extractor):
 
         This should be called as the final task of each `plugin`.
 
-        It strips unneeded items from the :attr:`batch` ``dict`` and pairs the detected faces back
-        up with their original frame before yielding each frame.
-
-        Outputs items in the format:
-
-        >>> {'image': [<original frame>],
-        >>>  'filename': [<frame filename>),
-        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
+        Pairs the detected faces back up with their original frame before yielding each frame.
 
         Parameters
         ----------
         batch : dict
             The final ``dict`` from the `plugin` process. It must contain the `keys`:
-            ``detected_faces``, ``landmarks``, ``filename``, ``image``
+            ``detected_faces``, ``landmarks``, ``filename``
 
         Yields
         ------
-        dict
-            A ``dict`` for each frame containing the ``image``, ``filename`` and list of
-            :class:`lib.faces_detect.DetectedFace` objects.
-
+        :class:`~plugins.extract.pipeline.ExtractMedia`
+            The :attr:`DetectedFaces` list will be populated for this class with the bounding boxes
+            and landmarks for the detected faces found in the frame.
         """
 
         for face, landmarks in zip(batch["detected_faces"], batch["landmarks"]):
             if not isinstance(landmarks, np.ndarray):
                 landmarks = np.array(landmarks)
             face.landmarks_xy = landmarks
-        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
-        logger.trace("Item out: %s", {key: val
-                                      for key, val in batch.items()
-                                      if key != "image"})
-        for filename, image, face in zip(batch["filename"],
-                                         batch["image"],
-                                         batch["detected_faces"]):
+
+        logger.trace("Item out: %s", {key: val.shape if isinstance(val, np.ndarray) else val
+                                      for key, val in batch.items()})
+
+        for filename, face in zip(batch["filename"], batch["detected_faces"]):
             self._output_faces.append(face)
             if len(self._output_faces) != self._faces_per_filename[filename]:
                 continue
-            retval = dict(filename=filename, image=image, detected_faces=self._output_faces)
 
+            output = self._extract_media.pop(filename)
+            output.add_detected_faces(self._output_faces)
             self._output_faces = []
-            logger.trace("Yielding: (filename: '%s', image: %s, detected_faces: %s)",
-                         retval["filename"], retval["image"].shape, len(retval["detected_faces"]))
-            yield retval
+
+            logger.trace("Final Output: (filename: '%s', image shape: %s, detected_faces: %s, "
+                         "item: %s)",
+                         output.filename, output.image_shape, output.detected_faces, output)
+            yield output
 
     # <<< PROTECTED METHODS >>> #
     # <<< PREDICT WRAPPER >>> #
@@ -214,8 +205,7 @@ class Aligner(Extractor):
     def _normalize_faces(self, faces):
         """ Normalizes the face for feeding into model
 
-        The normalization method is dictated by the command line argument:
-            -nh (--normalization)
+        The normalization method is dictated by the command line argument `-nh (--normalization)`
         """
         if self.normalize_method is None:
             return faces
diff --git a/plugins/extract/align/cv2_dnn.py b/plugins/extract/align/cv2_dnn.py
index c9308c1..26b2aff 100644
--- a/plugins/extract/align/cv2_dnn.py
+++ b/plugins/extract/align/cv2_dnn.py
@@ -51,18 +51,18 @@ class Align(Aligner):
 
     def process_input(self, batch):
         """ Compile the detected faces for prediction """
-        faces, batch["roi"] = self.align_image(batch["detected_faces"])
+        faces, batch["roi"] = self.align_image(batch)
         faces = self._normalize_faces(faces)
         batch["feed"] = np.array(faces, dtype="float32")[..., :3].transpose((0, 3, 1, 2))
         return batch
 
-    def align_image(self, detected_faces):
+    def align_image(self, batch):
         """ Align the incoming image for prediction """
         logger.trace("Aligning image around center")
         sizes = (self.input_size, self.input_size)
         rois = []
         faces = []
-        for face in detected_faces:
+        for face, image in zip(batch["detected_faces"], batch["image"]):
             box = (face.left,
                    face.top,
                    face.right,
@@ -74,7 +74,7 @@ class Align(Aligner):
             # Make box square.
             roi = self.get_square_box(box_moved)
             # Pad the image if face is outside of boundaries
-            image = self.pad_image(roi, face.image)
+            image = self.pad_image(roi, image)
             face = image[roi[1]: roi[3], roi[0]: roi[2]]
 
             interpolation = cv2.INTER_CUBIC if face.shape[0] < self.input_size else cv2.INTER_AREA
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
index b44fb73..a4924bf 100644
--- a/plugins/extract/align/fan.py
+++ b/plugins/extract/align/fan.py
@@ -54,7 +54,6 @@ class Align(Aligner):
         """ Get the center and set scale of bounding box """
         logger.debug("Calculating center and scale")
         center_scale = np.empty((len(detected_faces), 68, 3), dtype='float32')
-        # TODO modify detected face to hold this data as a matrix
         for index, face in enumerate(detected_faces):
             x_center = (face.left + face.right) / 2.0
             y_center = (face.top + face.bottom) / 2.0 - face.h * 0.12
@@ -79,19 +78,22 @@ class Align(Aligner):
 
         # TODO second pass .. convert to matrix
         new_images = []
-        for face, ul, br in zip(batch["detected_faces"], upper_left, bot_right):
-            height, width = face.image.shape[:2]
-            channels = 3 if face.image.ndim > 2 else 1
-            br_width, br_height = br[0].astype('int32')
-            ul_width, ul_height = ul[0].astype('int32')
-            new_dim = (br_height - ul_height, br_width - ul_width, channels)
+        for image, top_left, bottom_right in zip(batch["image"], upper_left, bot_right):
+            height, width = image.shape[:2]
+            channels = 3 if image.ndim > 2 else 1
+            bottom_right_width, bottom_right_height = bottom_right[0].astype('int32')
+            top_left_width, top_left_height = top_left[0].astype('int32')
+            new_dim = (bottom_right_height - top_left_height,
+                       bottom_right_width - top_left_width,
+                       channels)
             new_img = np.empty(new_dim, dtype=np.uint8)
 
-            new_x = slice(max(0, -ul_width), min(br_width, width) - ul_width)
-            new_y = slice(max(0, -ul_height), min(br_height, height) - ul_height)
-            old_x = slice(max(0, ul_width), min(br_width, width))
-            old_y = slice(max(0, ul_height), min(br_height, height))
-            new_img[new_y, new_x] = face.image[old_y, old_x]
+            new_x = slice(max(0, -top_left_width), min(bottom_right_width, width) - top_left_width)
+            new_y = slice(max(0, -top_left_height),
+                          min(bottom_right_height, height) - top_left_height)
+            old_x = slice(max(0, top_left_width), min(bottom_right_width, width))
+            old_y = slice(max(0, top_left_height), min(bottom_right_height, height))
+            new_img[new_y, new_x] = image[old_y, old_x]
 
             interp = cv2.INTER_CUBIC if new_dim[0] < self.input_size else cv2.INTER_AREA
             new_images.append(cv2.resize(new_img, dsize=sizes, interpolation=interp))
@@ -148,7 +150,7 @@ class Align(Aligner):
                    (image_slice, landmark_slice, max_clipped[0], indices[1])]
         x_subpixel_shift = batch["prediction"][offsets[0]] - batch["prediction"][offsets[1]]
         y_subpixel_shift = batch["prediction"][offsets[2]] - batch["prediction"][offsets[3]]
-        # TODO improve rudimentary subpixel logic to centroid of 3x3 window algorithm
+        # TODO improve rudimentary sub-pixel logic to centroid of 3x3 window algorithm
         subpixel_landmarks[:, :, 0] = indices[1] + np.sign(x_subpixel_shift) * 0.25 + 0.5
         subpixel_landmarks[:, :, 1] = indices[0] + np.sign(y_subpixel_shift) * 0.25 + 0.5
 
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index 0a2fb72..c7ff95f 100644
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -4,10 +4,11 @@
 All Detector Plugins should inherit from this class.
 See the override methods for which methods are required.
 
+The plugin will receive a :class:`~plugins.extract.pipeline.ExtractMedia` object.
+
 For each source frame, the plugin must pass a dict to finalize containing:
 
 >>> {'filename': <filename of source frame>,
->>>  'image':  <source image>,
 >>>  'detected_faces': <list of DetectedFace objects containing bounding box points}}
 
 To get a :class:`~lib.faces_detect.DetectedFace` object use the function:
@@ -22,7 +23,7 @@ from lib.faces_detect import DetectedFace
 from plugins.extract._base import Extractor, logger
 
 
-class Detector(Extractor):
+class Detector(Extractor):  # pylint:disable=abstract-method
     """ Detector Object
 
     Parent class for all Detector plugins
@@ -74,6 +75,9 @@ class Detector(Extractor):
     def get_batch(self, queue):
         """ Get items for inputting to the detector plugin in batches
 
+        Items are received as :class:`~plugins.extract.pipeline.ExtractMedia` objects and converted
+        to ``dict`` for internal processing.
+
         Items are returned from the ``queue`` in batches of
         :attr:`~plugins.extract._base.Extractor.batchsize`
 
@@ -84,8 +88,7 @@ class Detector(Extractor):
         :attr:`~plugins.extract._base.Extractor.batchsize`:
 
         >>> {'filename': [<filenames of source frames>],
-        >>>  'image': [<source images>],
-        >>>  'scaled_image': <numpy.ndarray of images standardized for prediction>,
+        >>>  'image': <numpy.ndarray of images standardized for prediction>,
         >>>  'scale': [<scaling factors for each image>],
         >>>  'pad': [<padding for each image>],
         >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
@@ -110,16 +113,16 @@ class Detector(Extractor):
             if item == "EOF":
                 exhausted = True
                 break
-            for key, val in item.items():
-                batch.setdefault(key, []).append(val)
-            scaled_image, scale, pad = self._compile_detection_image(item["image"])
-            batch.setdefault("scaled_image", []).append(scaled_image)
+            batch.setdefault("filename", []).append(item.filename)
+            image, scale, pad = self._compile_detection_image(item)
+            batch.setdefault("image", []).append(image)
             batch.setdefault("scale", []).append(scale)
             batch.setdefault("pad", []).append(pad)
+
         if batch:
-            batch["scaled_image"] = np.array(batch["scaled_image"], dtype="float32")
+            batch["image"] = np.array(batch["image"], dtype="float32")
             logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
-                                                 for k, v in batch.items() if k != "image"})
+                                                 for k, v in batch.items()})
         else:
             logger.trace(item)
         return exhausted, batch
@@ -130,27 +133,17 @@ class Detector(Extractor):
 
         This should be called as the final task of each ``plugin``.
 
-        It strips unneeded items from the :attr:`batch` ``dict`` and performs standard final
-        processing on each item
-
-        Outputs items in the format:
-
-        >>> {'image': [<original frame>],
-        >>>  'filename': [<frame filename>),
-        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
-
-
         Parameters
         ----------
         batch : dict
-            The final ``dict`` from the `plugin` process. It must contain the keys ``image``,
-            ``filename``, ``faces``
+            The final ``dict`` from the `plugin` process. It must contain the keys  ``filename``,
+            ``faces``
 
         Yields
         ------
-        dict
-            A ``dict`` for each frame containing the ``image``, ``filename`` and ``list`` of
-            ``detected_faces``
+        :class:`~plugins.extract.pipeline.ExtractMedia`
+            The :attr:`DetectedFaces` list will be populated for this class with the bounding boxes
+            for the detected faces found in the frame.
         """
         if not isinstance(batch, dict):
             logger.trace("Item out: %s", batch)
@@ -183,13 +176,14 @@ class Detector(Extractor):
         if self.min_size > 0 and batch.get("detected_faces", None):
             batch["detected_faces"] = self._filter_small_faces(batch["detected_faces"])
 
-        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
         batch = self._dict_lists_to_list_dicts(batch)
-
         for item in batch:
-            logger.trace("final output: %s", {k: v.shape if isinstance(v, np.ndarray) else v
-                                              for k, v in item.items()})
-            yield item
+            output = self._extract_media.pop(item["filename"])
+            output.add_detected_faces(item["detected_faces"])
+            logger.trace("final output: (filename: '%s', image shape: %s, detected_faces: %s, "
+                         "item: %s", output.filename, output.image_shape, output.detected_faces,
+                         output)
+            yield output
 
     @staticmethod
     def to_detected_face(left, top, right, bottom):
@@ -226,15 +220,19 @@ class Detector(Extractor):
         return batch
 
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
-    def _compile_detection_image(self, input_image):
-        """ Compile the detection image for feeding into the model"""
-        image = self._convert_color(input_image)
+    def _compile_detection_image(self, item):
+        """ Compile the detection image for feeding into the model
 
-        image_size = image.shape[:2]
-        scale = self._set_scale(image_size)
-        pad = self._set_padding(image_size, scale)
+        Parameters
+        ----------
+        item: :class:`plugins.extract.pipeline.ExtractMedia`
+            The input item from the pipeline
+        """
+        image = item.get_image_copy(self.colorformat)
+        scale = self._set_scale(item.image_size)
+        pad = self._set_padding(item.image_size, scale)
 
-        image = self._scale_image(image, image_size, scale)
+        image = self._scale_image(image, item.image_size, scale)
         image = self._pad_image(image)
         logger.trace("compiled: (images shape: %s, scale: %s, pad: %s)", image.shape, scale, pad)
         return image, scale, pad
diff --git a/plugins/extract/detect/cv2_dnn.py b/plugins/extract/detect/cv2_dnn.py
index 7e9dbb6..08ce23a 100644
--- a/plugins/extract/detect/cv2_dnn.py
+++ b/plugins/extract/detect/cv2_dnn.py
@@ -27,7 +27,7 @@ class Detect(Detector):
 
     def process_input(self, batch):
         """ Compile the detection image(s) for prediction """
-        batch["feed"] = cv2.dnn.blobFromImages(batch["scaled_image"],  # pylint: disable=no-member
+        batch["feed"] = cv2.dnn.blobFromImages(batch["image"],  # pylint: disable=no-member
                                                scalefactor=1.0,
                                                size=(self.input_size, self.input_size),
                                                mean=[104, 117, 123],
diff --git a/plugins/extract/detect/manual.py b/plugins/extract/detect/manual.py
index 7bd8752..5953e9b 100644
--- a/plugins/extract/detect/manual.py
+++ b/plugins/extract/detect/manual.py
@@ -26,7 +26,7 @@ class Detect(Detector):
 
     def process_input(self, batch):
         """ No pre-processing for Manual. Just set a dummy feed """
-        batch["feed"] = batch["scaled_image"]
+        batch["feed"] = batch["image"]
         return batch
 
     def predict(self, batch):
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
index e60340c..13942d9 100644
--- a/plugins/extract/detect/mtcnn.py
+++ b/plugins/extract/detect/mtcnn.py
@@ -58,7 +58,7 @@ class Detect(Detector):
 
     def process_input(self, batch):
         """ Compile the detection image(s) for prediction """
-        batch["feed"] = (batch["scaled_image"] - 127.5) / 127.5
+        batch["feed"] = (batch["image"] - 127.5) / 127.5
         return batch
 
     def predict(self, batch):
@@ -197,7 +197,7 @@ class MTCNN():
     def __init__(self, model_path, allow_growth, minsize, threshold, factor):
         """
         minsize: minimum faces' size
-        threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
+        threshold: threshold=[th1, th2, th3], th1-3 are three steps threshold
         factor: the factor used to create a scaling pyramid of face sizes to
                 detect in the image.
         pnet, rnet, onet: caffemodel
@@ -256,7 +256,7 @@ class MTCNN():
             cls_prob = np.swapaxes(cls_prob, 1, 2)
             roi = np.swapaxes(roi, 1, 3)
             for idx in range(batch_items):
-                # first index 0 = cls score, 1 = one hot repr
+                # first index 0 = class score, 1 = one hot repr
                 rectangle = detect_face_12net(cls_prob[idx, ...],
                                               roi[idx, ...],
                                               out_side,
@@ -492,7 +492,7 @@ def nms(rectangles, threshold, method):
     s_sort = np.array(var_s.argsort())
     pick = []
     while len(s_sort) > 0:
-        # s_sort[-1] have hightest prob score, s_sort[0:-1]->others
+        # s_sort[-1] have highest prob score, s_sort[0:-1]->others
         xx_1 = np.maximum(x_1[s_sort[-1]], x_1[s_sort[0:-1]])
         yy_1 = np.maximum(y_1[s_sort[-1]], y_1[s_sort[0:-1]])
         xx_2 = np.minimum(x_2[s_sort[-1]], x_2[s_sort[0:-1]])
diff --git a/plugins/extract/detect/s3fd.py b/plugins/extract/detect/s3fd.py
index a0f9188..cc516fd 100644
--- a/plugins/extract/detect/s3fd.py
+++ b/plugins/extract/detect/s3fd.py
@@ -42,7 +42,7 @@ class Detect(Detector):
 
     def process_input(self, batch):
         """ Compile the detection image(s) for prediction """
-        batch["feed"] = self.model.prepare_batch(batch["scaled_image"])
+        batch["feed"] = self.model.prepare_batch(batch["image"])
         return batch
 
     def predict(self, batch):
@@ -277,7 +277,7 @@ class S3fd(KSession):
                 Shape: [num_priors,4]
             priors (tensor): Prior boxes in center-offset form.
                 Shape: [num_priors,4].
-            variances: (list[float]) Variances of priorboxes
+            variances: (list[float]) Variances of prior boxes
         Return:
             decoded bounding box predictions
         """
@@ -288,7 +288,8 @@ class S3fd(KSession):
         boxes[:, 2:] += boxes[:, :2]
         return boxes
 
-    def _nms(self, boxes, threshold):
+    @staticmethod
+    def _nms(boxes, threshold):
         """ Perform Non-Maximum Suppression """
         retained_box_indices = list()
 
diff --git a/plugins/extract/mask/_base.py b/plugins/extract/mask/_base.py
index da15912..37aed6a 100644
--- a/plugins/extract/mask/_base.py
+++ b/plugins/extract/mask/_base.py
@@ -5,23 +5,18 @@ Plugins should inherit from this class
 
 See the override methods for which methods are required.
 
-The plugin will receive a dict containing:
-
->>> {"filename": <filename of source frame>,
->>>  "image": <source image>,
->>>  "detected_faces": <list of bounding box dicts from lib/plugins/extract/detect/_base>}
+The plugin will receive a :class:`~plugins.extract.pipeline.ExtractMedia` object.
 
 For each source item, the plugin must pass a dict to finalize containing:
 
 >>> {"filename": <filename of source frame>,
->>>  "image": <four channel source image>,
 >>>  "detected_faces": <list of bounding box dicts from lib/plugins/extract/detect/_base>}
 """
 
 import cv2
 import numpy as np
 
-from plugins.extract._base import Extractor, logger
+from plugins.extract._base import Extractor, ExtractMedia, logger
 
 
 class Masker(Extractor):  # pylint:disable=abstract-method
@@ -47,6 +42,7 @@ class Masker(Extractor):  # pylint:disable=abstract-method
 
     See Also
     --------
+    plugins.extract.pipeline : The extraction pipeline for calling plugins
     plugins.extract.align : Aligner plugins
     plugins.extract._base : Parent class for all extraction plugins
     plugins.extract.detect._base : Detector parent class for extraction plugins.
@@ -67,7 +63,7 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         self._storage_name = self.__module__.split(".")[-1].replace("_", "-")
         self._storage_size = 128  # Size to store masks at. Leave this at default
         self._faces_per_filename = dict()  # Tracking for recompiling face batches
-        self._rollover = []  # Items that are rolled over from the previous batch in get_batch
+        self._rollover = None  # Items that are rolled over from the previous batch in get_batch
         self._output_faces = []
         logger.debug("Initialized %s", self.__class__.__name__)
 
@@ -77,8 +73,11 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         Items are returned from the ``queue`` in batches of
         :attr:`~plugins.extract._base.Extractor.batchsize`
 
+        Items are received as :class:`~plugins.extract.pipeline.ExtractMedia` objects and converted
+        to ``dict`` for internal processing.
+
         To ensure consistent batch sizes for masker the items are split into separate items for
-        each :class:`lib.faces_detect.DetectedFace` object.
+        each :class:`~lib.faces_detect.DetectedFace` object.
 
         Remember to put ``'EOF'`` to the out queue after processing
         the final batch
@@ -87,7 +86,6 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         :attr:`~plugins.extract._base.Extractor.batchsize`:
 
         >>> {'filename': [<filenames of source frames>],
-        >>>  'image': [<source images>],
         >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
 
         Parameters
@@ -112,28 +110,27 @@ class Masker(Extractor):  # pylint:disable=abstract-method
                 exhausted = True
                 break
             # Put frames with no faces into the out queue to keep TQDM consistent
-            if not item["detected_faces"]:
+            if not item.detected_faces:
                 self._queues["out"].put(item)
                 continue
-            for f_idx, face in enumerate(item["detected_faces"]):
-                face.image = self._convert_color(item["image"])
-                face.load_feed_face(face.image,
+            for f_idx, face in enumerate(item.detected_faces):
+                face.load_feed_face(item.get_image_copy(self.colorformat),
                                     size=self.input_size,
                                     coverage_ratio=1.0,
                                     dtype="float32",
                                     is_aligned_face=self._image_is_aligned)
+
                 batch.setdefault("detected_faces", []).append(face)
-                batch.setdefault("filename", []).append(item["filename"])
-                batch.setdefault("image", []).append(item["image"])
+                batch.setdefault("filename", []).append(item.filename)
                 idx += 1
                 if idx == self.batchsize:
-                    frame_faces = len(item["detected_faces"])
+                    frame_faces = len(item.detected_faces)
                     if f_idx + 1 != frame_faces:
-                        self._rollover = {k: v[f_idx + 1:] if k == "detected_faces" else v
-                                          for k, v in item.items()}
+                        self._rollover = ExtractMedia(item.filename, item.image)
+                        self._rollover.add_detected_faces(item.detected_faces[f_idx + 1:])
                         logger.trace("Rolled over %s faces of %s to next batch for '%s'",
-                                     len(self._rollover["detected_faces"]),
-                                     frame_faces, item["filename"])
+                                     len(self._rollover.detected_faces), frame_faces,
+                                     item.filename)
                     break
         if batch:
             logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
@@ -146,17 +143,17 @@ class Masker(Extractor):  # pylint:disable=abstract-method
         """ Collect the item from the _rollover dict or from the queue
             Add face count per frame to self._faces_per_filename for joining
             batches back up in finalize """
-        if self._rollover:
+        if self._rollover is not None:
             logger.trace("Getting from _rollover: (filename: `%s`, faces: %s)",
-                         self._rollover["filename"], len(self._rollover["detected_faces"]))
+                         self._rollover.filename, len(self._rollover.detected_faces))
             item = self._rollover
-            self._rollover = dict()
+            self._rollover = None
         else:
             item = self._get_item(queue)
             if item != "EOF":
                 logger.trace("Getting from queue: (filename: %s, faces: %s)",
-                             item["filename"], len(item["detected_faces"]))
-                self._faces_per_filename[item["filename"]] = len(item["detected_faces"])
+                             item.filename, len(item.detected_faces))
+                self._faces_per_filename[item.filename] = len(item.detected_faces)
         return item
 
     def _predict(self, batch):
@@ -168,27 +165,19 @@ class Masker(Extractor):  # pylint:disable=abstract-method
 
         This should be called as the final task of each `plugin`.
 
-        It strips unneeded items from the :attr:`batch` ``dict`` and pairs the detected faces back
-        up with their original frame before yielding each frame.
-
-        Outputs items in the format:
-
-        >>> {'image': [<original frame>],
-        >>>  'filename': [<frame filename>),
-        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
+        Pairs the detected faces back up with their original frame before yielding each frame.
 
         Parameters
         ----------
         batch : dict
             The final ``dict`` from the `plugin` process. It must contain the `keys`:
-            ``detected_faces``, ``filename``, ``image``
+            ``detected_faces``, ``filename``
 
         Yields
         ------
-        dict
-            A ``dict`` for each frame containing the ``image``, ``filename`` and list of
-            :class:`lib.faces_detect.DetectedFace` objects.
-
+        :class:`~plugins.extract.pipeline.ExtractMedia`
+            The :attr:`DetectedFaces` list will be populated for this class with the bounding
+            boxes, landmarks and masks for the detected faces found in the frame.
         """
         for mask, face in zip(batch["prediction"], batch["detected_faces"]):
             face.add_mask(self._storage_name,
@@ -198,22 +187,19 @@ class Masker(Extractor):  # pylint:disable=abstract-method
                           storage_size=self._storage_size)
             face.feed = dict()
 
-        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
-        logger.trace("Item out: %s", {key: val
-                                      for key, val in batch.items()
-                                      if key != "image"})
-        for filename, image, face in zip(batch["filename"],
-                                         batch["image"],
-                                         batch["detected_faces"]):
+        logger.trace("Item out: %s", {key: val.shape if isinstance(val, np.ndarray) else val
+                                      for key, val in batch.items()})
+        for filename, face in zip(batch["filename"], batch["detected_faces"]):
             self._output_faces.append(face)
             if len(self._output_faces) != self._faces_per_filename[filename]:
                 continue
-            retval = dict(filename=filename, image=image, detected_faces=self._output_faces)
 
+            output = self._extract_media.pop(filename)
+            output.add_detected_faces(self._output_faces)
             self._output_faces = []
             logger.trace("Yielding: (filename: '%s', image: %s, detected_faces: %s)",
-                         retval["filename"], retval["image"].shape, len(retval["detected_faces"]))
-            yield retval
+                         output.filename, output.image_shape, len(output.detected_faces))
+            yield output
 
     # <<< PROTECTED ACCESS METHODS >>> #
     @staticmethod
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index dedb5ec..6b5546e 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python3
 """
-Return a requested detector/aligner pipeline
+Return a requested detector/aligner/masker pipeline
 
 Tensorflow does not like to release GPU VRAM, so parallel plugins need to be managed to work
 together.
@@ -12,6 +12,8 @@ plugins either in parallel or in series, giving easy access to input and output.
 
 import logging
 
+import cv2
+
 from lib.gpu_stats import GPUStats
 from lib.queue_manager import queue_manager, QueueEmpty
 from lib.utils import get_backend
@@ -21,8 +23,9 @@ logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
 
 
 class Extractor():
-    """ Creates a :mod:`~plugins.extract.detect`/:mod:`~plugins.extract.align` pipeline and yields
-    results frame by frame from the :attr:`detected_faces` generator
+    """ Creates a :mod:`~plugins.extract.detect`/:mod:`~plugins.extract.align``/\
+    :mod:`~plugins.extract.mask` pipeline and yields results frame by frame from the
+    :attr:`detected_faces` generator
 
     :attr:`input_queue` is dynamically set depending on the current :attr:`phase` of extraction
 
@@ -32,6 +35,8 @@ class Extractor():
         The name of a detector plugin as exists in :mod:`plugins.extract.detect`
     aligner: str
         The name of an aligner plugin as exists in :mod:`plugins.extract.align`
+    masker: str
+        The name of a masker plugin as exists in :mod:`plugins.extract.mask`
     configfile: str, optional
         The path to a custom ``extract.ini`` configfile. If ``None`` then the system
         :file:`config/extract.ini` file will be used.
@@ -39,19 +44,19 @@ class Extractor():
         Whether to attempt processing the plugins in parallel. This may get overridden
         internally depending on the plugin combination. Default: ``False``
     rotate_images: str, optional
-        Used to set the :attr:`~plugins.extract.detect.rotation` attribute. Pass in a single number
+        Used to set the :attr:`plugins.extract.detect.rotation` attribute. Pass in a single number
         to use increments of that size up to 360, or pass in a ``list`` of ``ints`` to enumerate
         exactly what angles to check. Can also pass in ``'on'`` to increment at 90 degree
         intervals. Default: ``None``
     min_size: int, optional
-        Used to set the :attr:`~plugins.extract.detect.min_size` attribute Filters out faces
+        Used to set the :attr:`plugins.extract.detect.min_size` attribute Filters out faces
         detected below this size. Length, in pixels across the diagonal of the bounding box. Set
         to ``0`` for off. Default: ``0``
     normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional
-        Used to set the :attr:`~plugins.extract.align.normalize_method` attribute. Normalize the
+        Used to set the :attr:`plugins.extract.align.normalize_method` attribute. Normalize the
         images fed to the aligner.Default: ``None``
     image_is_aligned: bool, optional
-        Used to set the :attr:`~plugins.extract.mask.image_is_aligned` attribute. Indicates to the
+        Used to set the :attr:`plugins.extract.mask.image_is_aligned` attribute. Indicates to the
         masker that the fed in image is an aligned face rather than a frame.Default: ``False``
 
     Attributes
@@ -86,20 +91,14 @@ class Extractor():
     def input_queue(self):
         """ queue: Return the correct input queue depending on the current phase
 
-        The input queue is the entry point into the extraction pipeline. A ``dict`` should
-        be put to the queue in the following format(s):
-
-        For detect/single phase operations:
-
-        >>> {'filename': <path to the source image that is to be extracted from>,
-        >>>  'image': <the source image as a numpy.ndarray in BGR color format>}
-
-        For align (2nd pass operations):
+        The input queue is the entry point into the extraction pipeline. An :class:`ExtractMedia`
+        object should be put to the queue.
 
-        >>> {'filename': <path to the source image that is to be extracted from>,
-        >>>  'image': <the source image as a numpy.ndarray in BGR color format>,
-        >>>  'detected_faces: [<list of DetectedFace objects as generated from detect>]}
+        For detect/single phase operations the :attr:`ExtractMedia.filename` and
+        :attr:`~ExtractMedia.image` attributes should be populated.
 
+        For align/mask (2nd/3rd pass operations) the :attr:`ExtractMedia.detected_faces` should
+        also be populated by calling :func:`ExtractMedia.set_detected_faces`.
         """
         qname = "extract_{}_in".format(self.phase)
         retval = self._queues[qname]
@@ -118,12 +117,11 @@ class Extractor():
         -------
         >>> for phase in extractor.passes:
         >>>     if phase == 1:
-        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": numpy.array(image)})
+        >>>         extract_media = ExtractMedia("path/to/image/file", image)
+        >>>         extractor.input_queue.put(extract_media)
         >>>     else:
-        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": numpy.array(image),
-        >>>                                    "detected_faces": [<DetectedFace objects]})
+        >>>         extract_media.set_image(image)
+        >>>         extractor.input_queue.put(extract_media)
         """
         retval = 1 if self._is_parallel else len(self._flow)
         logger.trace(retval)
@@ -142,10 +140,9 @@ class Extractor():
         >>>     if extractor.final_pass:
         >>>         <do final processing>
         >>>     else:
+        >>>         extract_media.set_image(image)
         >>>         <do intermediate processing>
-        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
-        >>>                                    "image": numpy.array(image),
-        >>>                                    "detected_faces": [<DetectedFace objects]})
+        >>>         extractor.input_queue.put(extract_media)
         """
         retval = self._is_parallel or self.phase == self._final_phase
         logger.trace(retval)
@@ -195,18 +192,15 @@ class Extractor():
 
         Yields
         ------
-        faces: dict
-            regardless of phase, the returned dictionary will contain, exclusively, ``filename``:
-            the filename of the source image, ``image``: the ``numpy.array`` of the source image
-            in BGR color format, ``detected_faces``: a list of
-            :class:`~lib.faces_detect.Detected_Face` objects.
+        faces: :class:`ExtractMedia`
+            The populated extracted media object.
 
         Example
         -------
-        >>> for face in extractor.detected_faces():
-        >>>     filename = face["filename"]
-        >>>     image = face["image"]
-        >>>     detected_faces = face["detected_faces"]
+        >>> for extract_media in extractor.detected_faces():
+        >>>     filename = extract_media.filename
+        >>>     image = extract_media.image
+        >>>     detected_faces = extract_media.detected_faces
         """
         logger.debug("Running Detection. Phase: '%s'", self.phase)
         # If not multiprocessing, intercept the align in queue for
@@ -482,3 +476,128 @@ class Extractor():
             if plugin.check_and_raise_error():
                 return True
         return False
+
+
+class ExtractMedia():
+    """ An object that passes through the :class:`~plugins.extract.pipeline.Extractor` pipeline.
+
+    Parameters
+    ----------
+    filename: str
+        The base name of the original frame's filename
+    image: :class:`numpy.ndarray`
+        The original frame
+    """
+
+    def __init__(self, filename, image):
+        logger.trace("Initializing %s: (filename: '%s', image shape: %s)",
+                     self.__class__.__name__, filename, image.shape)
+        self._filename = filename
+        self._image = image
+        self._detected_faces = None
+
+    @property
+    def filename(self):
+        """ str: The base name of the :attr:`image` filename. """
+        return self._filename
+
+    @property
+    def image(self):
+        """ :class:`numpy.ndarray`: The source frame for this object. """
+        return self._image
+
+    @property
+    def image_shape(self):
+        """ tuple: The shape of the stored :attr:`image`. """
+        return self._image.shape
+
+    @property
+    def image_size(self):
+        """ tuple: The (`height`, `width`) of the stored :attr:`image`. """
+        return self._image.shape[:2]
+
+    @property
+    def detected_faces(self):
+        """list: A list of :class:`~lib.faces_detect.DetectedFace` objects in the
+        :attr:`image`. """
+        return self._detected_faces
+
+    def get_image_copy(self, colorformat):
+        """ Get a copy of the image in the requested color format.
+
+        Parameters
+        ----------
+        colorformat: ['BGR', 'RGB', 'GRAY']
+            The requested color format of :attr:`image`
+
+        Returns
+        -------
+        :class:`numpy.ndarray`:
+            A copy of :attr:`image` in the requested :attr:`colorformat`
+        """
+        logger.trace("Requested color format '%s' for frame '%s'", colorformat, self._filename)
+        image = getattr(self, "_image_as_{}".format(colorformat.lower()))()
+        return image
+
+    def add_detected_faces(self, faces):
+        """ Add detected faces to the object. Called at the end of each extraction phase.
+
+        Parameters
+        ----------
+        faces: list
+            A list of :class:`~lib.faces_detect.DetectedFace` objects
+        """
+        logger.trace("Adding detected faces for filename: '%s'. (faces: %s, lrtb: %s)",
+                     self._filename, faces,
+                     [(face.left, face.right, face.top, face.bottom) for face in faces])
+        self._detected_faces = faces
+
+    def remove_image(self):
+        """ Delete the image and reset :attr:`image` to ``None``.
+
+        Required for multi-phase extraction to avoid the frames stacking RAM.
+        """
+        logger.trace("Removing image for filename: '%s'", self._filename)
+        del self._image
+        self._image = None
+
+    def set_image(self, image):
+        """ Add the image back into :attr:`image`
+
+        Required for multi-phase extraction adds the image back to this object.
+
+        Parameters
+        ----------
+        image: :class:`numpy.ndarry`
+            The original frame to be re-applied to for this :attr:`filename`
+        """
+        logger.trace("Reapplying image: (filename: `%s`, image shape: %s)",
+                     self._filename, image.shape)
+        self._image = image
+
+    def _image_as_bgr(self):
+        """ Get a copy of the source frame in BGR format.
+
+        Returns
+        -------
+        :class:`numpy.ndarray`:
+            A copy of :attr:`image` in BGR color format """
+        return self._image[..., :3].copy()
+
+    def _image_as_rgb(self):
+        """ Get a copy of the source frame in RGB format.
+
+        Returns
+        -------
+        :class:`numpy.ndarray`:
+            A copy of :attr:`image` in RGB color format """
+        return self._image[..., 2::-1].copy()
+
+    def _image_as_gray(self):
+        """ Get a copy of the source frame in gray-scale format.
+
+        Returns
+        -------
+        :class:`numpy.ndarray`:
+            A copy of :attr:`image` in gray-scale color format """
+        return cv2.cvtColor(self._image.copy(), cv2.COLOR_BGR2GRAY)
diff --git a/scripts/extract.py b/scripts/extract.py
index a348d12..2b1b2c0 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -10,7 +10,7 @@ from tqdm import tqdm
 from lib.image import encode_image_with_hash, ImagesLoader, ImagesSaver
 from lib.multithreading import MultiThread
 from lib.utils import get_folder
-from plugins.extract.pipeline import Extractor
+from plugins.extract.pipeline import Extractor, ExtractMedia
 from scripts.fsmedia import Alignments, PostProcess, Utils
 
 tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning
@@ -110,8 +110,7 @@ class Extract():
         Should only be called from  :class:`lib.cli.ScriptExecutor`
         """
         logger.info('Starting, this may take a while...')
-        # from lib.queue_manager import queue_manager
-        # queue_manager.debug_monitor(3)
+        # from lib.queue_manager import queue_manager ; queue_manager.debug_monitor(3)
         self._threaded_redirector("load")
         self._run_extraction()
         for thread in self._threads:
@@ -150,8 +149,7 @@ class Extract():
             if load_queue.shutdown.is_set():
                 logger.debug("Load Queue: Stop signal received. Terminating")
                 break
-            item = {"filename": filename,
-                    "image": image[..., :3]}
+            item = ExtractMedia(filename, image[..., :3])
             load_queue.put(item)
         load_queue.put("EOF")
         logger.debug("Load Images: Complete")
@@ -165,8 +163,8 @@ class Extract():
         Parameters
         ----------
         detected_faces: dict
-            Dictionary of detected_faces with the filename as its key and a list of
-            :class:`lib.faces_detect.DetectedFace` as the values for pairing with reloaded images.
+            Dictionary of :class:`plugins.extract.pipeline.ExtractMedia` with the filename as the
+            key for repopulating the image attribute.
         """
         logger.debug("Reload Images: Start. Detected Faces Count: %s", len(detected_faces))
         load_queue = self._extractor.input_queue
@@ -175,12 +173,12 @@ class Extract():
                 logger.debug("Reload Queue: Stop signal received. Terminating")
                 break
             logger.trace("Reloading image: '%s'", filename)
-            detect_item = detected_faces.pop(filename, None)
-            if not detect_item:
+            extract_media = detected_faces.pop(filename, None)
+            if not extract_media:
                 logger.warning("Couldn't find faces for: %s", filename)
                 continue
-            detect_item["image"] = image
-            load_queue.put(detect_item)
+            extract_media.set_image(image)
+            load_queue.put(extract_media)
         load_queue.put("EOF")
         logger.debug("Reload Images: Complete")
 
@@ -209,21 +207,17 @@ class Extract():
                               total=self._images.process_count,
                               file=sys.stdout,
                               desc=desc)
-            for idx, faces in enumerate(status_bar):
+            for idx, extract_media in enumerate(status_bar):
                 self._check_thread_error()
-                exception = faces.get("exception", False)
-                if exception:
-                    break
-
-                if self._extractor.final_pass:
-                    self._output_processing(faces, size)
-                    self._output_faces(saver, faces)
+                if is_final:
+                    self._output_processing(extract_media, size)
+                    self._output_faces(saver, extract_media)
                     if self._save_interval and (idx + 1) % self._save_interval == 0:
                         self._alignments.save()
                 else:
-                    del faces["image"]
-                    # cache detected faces for next run
-                    detected_faces[faces["filename"]] = faces
+                    extract_media.remove_image()
+                    # cache extract_media for next run
+                    detected_faces[extract_media.filename] = extract_media
                 status_bar.update(1)
 
             if not is_final:
@@ -236,51 +230,53 @@ class Extract():
         for thread in self._threads:
             thread.check_and_raise_error()
 
-    def _output_processing(self, faces, size):
+    def _output_processing(self, extract_media, size):
         """ Prepare faces for output
 
         Loads the aligned face, perform any processing actions and verify the output.
 
         Parameters:
-        faces: dict
-            Dictionary output from :class:`plugins.extract.Pipeline.Extractor`
+        extract_media: :class:`plugins.extract.pipeline.ExtractMedia`
+            Output from :class:`plugins.extract.pipeline.Extractor`
         size: int
             The size that the aligned face should be created at
         """
-        for face in faces["detected_faces"]:
-            face.load_aligned(faces["image"], size=size)
+        for face in extract_media.detected_faces:
+            face.load_aligned(extract_media.image, size=size)
 
-        self._post_process.do_actions(faces)
+        self._post_process.do_actions(extract_media)
+        extract_media.remove_image()
 
-        faces_count = len(faces["detected_faces"])
+        faces_count = len(extract_media.detected_faces)
         if faces_count == 0:
             logger.verbose("No faces were detected in image: %s",
-                           os.path.basename(faces["filename"]))
+                           os.path.basename(extract_media.filename))
 
         if not self._verify_output and faces_count > 1:
             self._verify_output = True
 
-    def _output_faces(self, saver, faces):
+    def _output_faces(self, saver, extract_media):
         """ Output faces to save thread
 
         Set the face filename based on the frame name and put the face to the
-        :class:`lib.image.ImagesSaver` save queue and add the face information to the alignments
+        :class:`~lib.image.ImagesSaver` save queue and add the face information to the alignments
         data.
 
         Parameters
         ----------
         saver: lib.images.ImagesSaver
             The background saver for saving the image
-        faces: dict
-            The output dictionary from :class:`plugins.extract.Pipeline.Extractor`
+        extract_media: :class:`~plugins.extract.pipeline.ExtractMedia`
+            The output from :class:`~plugins.extract.Pipeline.Extractor`
         """
-        logger.trace("Outputting faces for %s", faces["filename"])
+        logger.trace("Outputting faces for %s", extract_media.filename)
         final_faces = list()
-        filename, extension = os.path.splitext(os.path.basename(faces["filename"]))
-        for idx, face in enumerate(faces["detected_faces"]):
+        filename, extension = os.path.splitext(os.path.basename(extract_media.filename))
+        for idx, face in enumerate(extract_media.detected_faces):
             output_filename = "{}_{}{}".format(filename, str(idx), extension)
             face.hash, image = encode_image_with_hash(face.aligned_face, extension)
 
             saver.save(output_filename, image)
             final_faces.append(face.to_alignment())
-        self._alignments.data[os.path.basename(faces["filename"])] = final_faces
+        self._alignments.data[os.path.basename(extract_media.filename)] = final_faces
+        del extract_media
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index dc4ab36..0c5611b 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -99,7 +99,7 @@ class Alignments(AlignmentsBase):
         data = self.serializer.load(self.file)
 
         if skip_faces:
-            # Remove items from algnments that have no faces so they will
+            # Remove items from alignments that have no faces so they will
             # be re-detected
             del_keys = [key for key, val in data.items() if not val]
             logger.debug("Frames with no faces selected for redetection: %s", len(del_keys))
@@ -269,24 +269,23 @@ class PostProcess():
         logger.debug("Postprocess Items: %s", postprocess_items)
         return postprocess_items
 
-    def do_actions(self, output_item):
+    def do_actions(self, extract_media):
         """ Perform the requested post-processing actions """
         for action in self.actions:
             logger.debug("Performing postprocess action: '%s'", action.__class__.__name__)
-            action.process(output_item)
+            action.process(extract_media)
 
 
 class PostProcessAction():  # pylint: disable=too-few-public-methods
-    """ Parent class for Post Processing Actions
-        Usuable in Extract or Convert or both
+    """ Parent class for Post Processing Actions. Usable in Extract or Convert or both
         depending on context """
     def __init__(self, *args, **kwargs):
         logger.debug("Initializing %s: (args: %s, kwargs: %s)",
                      self.__class__.__name__, args, kwargs)
-        self.valid = True  # Set to False if invalid params passed in to disable
+        self.valid = True  # Set to False if invalid parameters passed in to disable
         logger.debug("Initialized base class %s", self.__class__.__name__)
 
-    def process(self, output_item):
+    def process(self, extract_media):
         """ Override for specific post processing action """
         raise NotImplementedError
 
@@ -295,10 +294,10 @@ class DebugLandmarks(PostProcessAction):  # pylint: disable=too-few-public-metho
     """ Draw debug landmarks on face
         Extract Only """
 
-    def process(self, output_item):
+    def process(self, extract_media):
         """ Draw landmarks on image """
-        frame = os.path.splitext(os.path.basename(output_item["filename"]))[0]
-        for idx, face in enumerate(output_item["detected_faces"]):
+        frame = os.path.splitext(os.path.basename(extract_media.filename))[0]
+        for idx, face in enumerate(extract_media.detected_faces):
             logger.trace("Drawing Landmarks. Frame: '%s'. Face: %s", frame, idx)
             aligned_landmarks = face.aligned_landmarks
             for (pos_x, pos_y) in aligned_landmarks:
@@ -352,19 +351,19 @@ class FaceFilter(PostProcessAction):
         logger.debug("Face Filter files: %s", filter_files)
         return filter_files
 
-    def process(self, output_item):
+    def process(self, extract_media):
         """ Filter in/out wanted/unwanted faces """
         if not self.filter:
             return
         ret_faces = list()
-        for idx, detect_face in enumerate(output_item["detected_faces"]):
+        for idx, detect_face in enumerate(extract_media.detected_faces):
             check_item = detect_face["face"] if isinstance(detect_face, dict) else detect_face
-            check_item.load_aligned(output_item["image"])
+            check_item.load_aligned(extract_media.image)
             if not self.filter.check(check_item):
                 logger.verbose("Skipping not recognized face: (Frame: %s Face %s)",
-                               output_item["filename"], idx)
+                               extract_media.filename, idx)
                 continue
             logger.trace("Accepting recognised face. Frame: %s. Face: %s",
-                         output_item["filename"], idx)
+                         extract_media.filename, idx)
             ret_faces.append(detect_face)
-        output_item["detected_faces"] = ret_faces
+        extract_media.detected_faces = ret_faces
