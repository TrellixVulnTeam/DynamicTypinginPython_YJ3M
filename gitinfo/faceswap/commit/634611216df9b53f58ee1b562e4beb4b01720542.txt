commit 634611216df9b53f58ee1b562e4beb4b01720542
Author: andenixa <37909402+andenixa@users.noreply.github.com>
Date:   Sat Jun 16 00:40:52 2018 +0300

    altered trainer (#425)
    
    altered trainer to accommodate with model change

diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
index 259dcdf..41eddfb 100644
--- a/plugins/Model_OriginalHighRes/Trainer.py
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -1,4 +1,3 @@
-
 import time
 import numpy
 
@@ -9,7 +8,7 @@ TRANSFORM_PRC = 115.
 
 
 class Trainer():
-#     
+    
     _random_transform_args = {
         'rotation_range': 10 * (TRANSFORM_PRC * .01),
         'zoom_range': 0.05 * (TRANSFORM_PRC * .01),
@@ -23,11 +22,6 @@ class Trainer():
         from timeit import default_timer as clock
         self._clock = clock
         
-
-        #generator = TrainingDataGenerator(self.random_transform_args, 160)                
-        # make sre to keep zoom=2 or you won't get 128x128 vectors as input
-        #generator = TrainingDataGenerator(self.random_transform_args, 220, 5, zoom=2)
-        #generator = TrainingDataGenerator(self.random_transform_args, 180, 7, zoom=2)
         generator = TrainingDataGenerator(self.random_transform_args, 160, 5, zoom=2)        
         
         self.images_A = generator.minibatchAB(fn_A, self.batch_size)
@@ -42,11 +36,14 @@ class Trainer():
         _, warped_B, target_B = next(self.images_B)
 
         loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)
-        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)        
+        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)
+        
+        self.model._epoch_no += 1        
                     
         print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
-            time.strftime("%H:%M:%S"), iter_no, self._clock()-when, loss_A, loss_B),
+            time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A, loss_B),
             end='\r')
+        
 
         if viewer is not None:
             viewer(self.show_sample(target_A[0:8], target_B[0:8]), "training using {}, bs={}".format(self.model, self.batch_size))
