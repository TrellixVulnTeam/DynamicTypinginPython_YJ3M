commit 7f5391145318ebe8012818eb810070e76be7e3b7
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Dec 4 13:31:49 2018 +0000

    Logging (#541)
    
    * Convert prints to logger. Further logging improvements. Tidy  up
    
    * Fix system verbosity. Allow SystemExit
    
    * Fix reload extract bug
    
    * Child Traceback handling
    
    * Safer shutdown procedure
    
    * Add shutdown event to queue manager
    
    * landmarks_as_xy > property. GUI notes + linting. Aligner bugfix
    
    * fix FaceFilter. Enable nFilter when no Filter is supplied
    
    * Fix blurry face filter
    
    * Continue on IO error. Better error handling
    
    * Explicitly print stack trace tocrash log
    
    * Windows Multiprocessing bugfix
    
    * Add git info and conda version to crash log
    
    * Windows/Anaconda mp bugfix
    
    * Logging fixes for training

diff --git a/INSTALL.md b/INSTALL.md
index a936948..a19a1ba 100755
--- a/INSTALL.md
+++ b/INSTALL.md
@@ -13,10 +13,10 @@
     - [Notes](#notes)
 - [Windows Install Guide](#windows-install-guide)
     - [Prerequisites](#prerequisites-1)
+        - [Microsoft Visual Studio 2015](#microsoft-visual-studio-2015)
         - [Cuda](#cuda)
         - [cuDNN](#cudnn)
         - [CMake](#cmake)
-        - [Microsoft Visual Studio 2015](#microsoft-visual-studio-2015)
         - [Anaconda](#anaconda)
         - [Git](#git)
     - [Setup](#setup-1)
@@ -196,6 +196,20 @@ If you are experiencing issues, please raise them in the [faceswap-playground](h
 Setting up Faceswap can seem a little intimidating to new users, but it isn't that complicated, although a little time consuming. It is recommended to use Linux where possible as Windows will hog about 20% of your GPU Memory, making Faceswap run a little slower, however using Windows is perfectly fine and 100% supported.
 
 ## Prerequisites
+### Microsoft Visual Studio 2015
+**Important** Make sure to downoad the 2015 version of Microsoft Visual Studio
+
+Download and install Microsoft Visual Studio 2015 from: https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409
+
+On the install screen:
+- Select "Custom" then click "Next"\
+![MSVS Custom](https://i.imgur.com/Bx8fjzT.png)
+- Uncheck all previously checked options
+- Expand "Programming Languages" and select "Visual C++"\
+![MSVS C++](https://i.imgur.com/c8k1IYD.png)
+- Select "Next" and "Install"
+
+
 ### Cuda
 **GPU Only** If you do not have an Nvidia GPU you can skip this step.
   
@@ -212,7 +226,7 @@ As with Cuda you will need to install the correct version of cuDNN that the late
 
 Download cuDNN from https://developer.nvidia.com/cudnn. You will need to create an account with Nvidia. 
 
-At the bottom of the list of latest cuDNN release will be a link to "Archived cuDNN Releases". Select this and choose the latest version of cuDNN that supports the version of Cuda you installed and is less than or equal to the latest version that Tensorflow supports. (Eg Tensorflow 1.12 supports Cuda 9.0 and cuDNN 7.2. There is not an archived version of cuDNN 7.2 for Cuda 9.0, so select cuDNN version 7.1)
+At the bottom of the list of latest cuDNN release will be a link to "Archived cuDNN Releases". Select this and choose the latest version of cuDNN that supports the version of Cuda you installed and has a minor version greater than or equal to the latest version that Tensorflow supports. (Eg Tensorflow 1.12 supports Cuda 9.0 and cuDNN 7.2. There is not an archived version of cuDNN 7.2 for Cuda 9.0, so select cuDNN version 7.3)
 - Open the zip file
 - Extract all of the files and folders into your Cuda folder (It is likely to be located in `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA`):\
 ![cudnn to cuda](https://i.imgur.com/X098w0N.png)
@@ -224,19 +238,6 @@ When installing CMake make sure to enable the option to CMake to the system path
 ![cmake path](https://i.imgur.com/XTtacdY.png)
 
 
-### Microsoft Visual Studio 2015
-**Important** Make sure to downoad the 2015 version of Microsoft Visual Studio
-
-Download and install Microsoft Visual Studio 2015 from: https://go.microsoft.com/fwlink/?LinkId=532606&clcid=0x409
-
-On the install screen:
-- Select "Custom" then click "Next"\
-![MSVS Custom](https://i.imgur.com/Bx8fjzT.png)
-- Uncheck all previously checked options
-- Expand "Programming Languages" and select "Visual C++"\
-![MSVS C++](https://i.imgur.com/c8k1IYD.png)
-- Select "Next" and "Install"
-
 ### Anaconda
 Download and install the latest Python 3 Anacconda from: https://www.anaconda.com/download/. Unless you know what you are doing, you can leave all the options at default.
 
diff --git a/lib/FaceFilter.py b/lib/FaceFilter.py
deleted file mode 100644
index b1d711f..0000000
--- a/lib/FaceFilter.py
+++ /dev/null
@@ -1,60 +0,0 @@
-# import dlib
-# import numpy as np
-import face_recognition
-# import face_recognition_models
-
-def avg(arr):
-  return sum(arr)*1.0/len(arr)
-
-class FaceFilter():
-    def __init__(self, reference_file_paths, nreference_file_paths, threshold = 0.6):
-        images = list(map(face_recognition.load_image_file, reference_file_paths))
-        nimages = list(map(face_recognition.load_image_file, nreference_file_paths))
-        # Note: we take only first face, so the reference file should only contain one face.
-        self.encodings = list(map(lambda im: face_recognition.face_encodings(im)[0], images))
-        self.nencodings = list(map(lambda im: face_recognition.face_encodings(im)[0], nimages))
-        self.threshold = threshold
-    
-    def check(self, detected_face):
-        # we could use detected landmarks, but I did not manage to do so. TODO The copy/paste below should help
-        encodings = face_recognition.face_encodings(detected_face.image)
-        if encodings is not None and len(encodings) > 0:
-            distances = list(face_recognition.face_distance(self.encodings, encodings[0]))
-            distance = avg(distances)
-            mindistance = min(distances)
-            maxdistance = max(distances)
-            if distance > self.threshold:
-                print("Distance above threshold: %f < %f" % (distance, self.threshold))
-                return False
-            if len(self.nencodings) > 0:
-              ndistances = list(face_recognition.face_distance(self.nencodings, encodings[0]))
-              ndistance = avg(ndistances)
-              nmindistance = min(ndistances)
-              nmaxdistance = max(ndistances)
-              if (mindistance > nmindistance):
-                  print("Distance to negative sample is smaller")
-                  return False
-              if (distance > ndistance):
-                  print("Average distance to negative sample is smaller")
-                  return False
-              # k-nn classifier
-              K=min(5, min(len(distances), len(ndistances)) + 1)
-              N=sum(list(map(lambda x: x[0],
-                    list(sorted([(1,d) for d in distances] + [(0,d) for d in ndistances],
-                                key=lambda x: x[1]))[:K])))
-              ratio = N/K
-              if (ratio < 0.5):
-                  print("K-nn is %.2f" % ratio)
-                  return False
-            return True
-        else:
-            print("No face encodings found")
-            return False
-
-# # Copy/Paste (mostly) from private method in face_recognition
-# face_recognition_model = face_recognition_models.face_recognition_model_location()
-# face_encoder = dlib.face_recognition_model_v1(face_recognition_model)
-
-# def convert(detected_face):
-#     return np.array(face_encoder.compute_face_descriptor(detected_face.image, detected_face.landmarks, 1))
-# # end of Copy/Paste
diff --git a/lib/Serializer.py b/lib/Serializer.py
index 0254315..23a01d6 100644
--- a/lib/Serializer.py
+++ b/lib/Serializer.py
@@ -2,7 +2,7 @@
 """
 Library providing convenient classes and methods for writing data to files.
 """
-import sys
+import logging
 import json
 import pickle
 
@@ -11,22 +11,28 @@ try:
 except ImportError:
     yaml = None
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
-class Serializer(object):
+
+class Serializer():
+    """ Parent Serializer class """
     ext = ""
     woptions = ""
     roptions = ""
 
     @classmethod
     def marshal(cls, input_data):
+        """ Override for marshalling """
         raise NotImplementedError()
 
     @classmethod
     def unmarshal(cls, input_string):
+        """ Override for unmarshalling """
         raise NotImplementedError()
 
 
 class YAMLSerializer(Serializer):
+    """ YAML Serializer """
     ext = "yml"
     woptions = "w"
     roptions = "r"
@@ -41,6 +47,7 @@ class YAMLSerializer(Serializer):
 
 
 class JSONSerializer(Serializer):
+    """ JSON Serializer """
     ext = "json"
     woptions = "w"
     roptions = "r"
@@ -55,6 +62,7 @@ class JSONSerializer(Serializer):
 
 
 class PickleSerializer(Serializer):
+    """ Picke Serializer """
     ext = "p"
     woptions = "wb"
     roptions = "rb"
@@ -64,31 +72,33 @@ class PickleSerializer(Serializer):
         return pickle.dumps(input_data)
 
     @classmethod
-    def unmarshal(cls, input_bytes):
+    def unmarshal(cls, input_bytes):  # pylint: disable=arguments-differ
         return pickle.loads(input_bytes)
 
 
 def get_serializer(serializer):
+    """ Return requested serializer """
     if serializer == "json":
         return JSONSerializer
-    elif serializer == "pickle":
+    if serializer == "pickle":
         return PickleSerializer
-    elif serializer == "yaml" and yaml is not None:
+    if serializer == "yaml" and yaml is not None:
         return YAMLSerializer
-    elif serializer == "yaml" and yaml is None:
-        print("You must have PyYAML installed to use YAML as the serializer.\n"
-              "Switching to JSON as the serializer.", file=sys.stderr)
+    if serializer == "yaml" and yaml is None:
+        logger.warning("You must have PyYAML installed to use YAML as the serializer."
+                       "Switching to JSON as the serializer.")
     return JSONSerializer
 
 
 def get_serializer_from_ext(ext):
+    """ Get the sertializer from filename extension """
     if ext == ".json":
         return JSONSerializer
-    elif ext == ".p":
+    if ext == ".p":
         return PickleSerializer
-    elif ext in (".yaml", ".yml") and yaml is not None:
+    if ext in (".yaml", ".yml") and yaml is not None:
         return YAMLSerializer
-    elif ext in (".yaml", ".yml") and yaml is None:
-        print("You must have PyYAML installed to use YAML as the serializer.\n"
-              "Switching to JSON as the serializer.", file=sys.stderr)
+    if ext in (".yaml", ".yml") and yaml is None:
+        logger.warning("You must have PyYAML installed to use YAML as the serializer.\n"
+                       "Switching to JSON as the serializer.")
     return JSONSerializer
diff --git a/lib/aligner.py b/lib/aligner.py
index 882e42f..74e1b0d 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -1,12 +1,17 @@
 #!/usr/bin/env python3
 """ Aligner for faceswap.py """
 
+import logging
+
 import cv2
 import numpy as np
 
 from lib.umeyama import umeyama
 from lib.align_eyes import align_eyes as func_align_eyes, FACIAL_LANDMARKS_IDXS
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
 MEAN_FACE_X = np.array([
     0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483,
     0.799124, 0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127,
@@ -36,34 +41,43 @@ class Extract():
 
     def extract(self, image, face, size, align_eyes):
         """ Extract a face from an image """
+        logger.trace("size: %s. align_eyes: %s", size, align_eyes)
         alignment = get_align_mat(face, size, align_eyes)
         extracted = self.transform(image, alignment, size, 48)
+        logger.trace("Returning face and alignment matrix: (alignment_matrix: %s)", alignment)
         return extracted, alignment
 
     @staticmethod
     def transform_matrix(mat, size, padding):
         """ Transform the matrix for current size and padding """
+        logger.trace("size: %s. padding: %s", size, padding)
         matrix = mat * (size - 2 * padding)
         matrix[:, 2] += padding
+        logger.trace("Returning: %s", matrix)
         return matrix
 
     def transform(self, image, mat, size, padding=0):
         """ Transform Image """
+        logger.trace("matrix: %s, size: %s. padding: %s", mat, size, padding)
         matrix = self.transform_matrix(mat, size, padding)
         return cv2.warpAffine(  # pylint: disable=no-member
             image, matrix, (size, size))
 
     def transform_points(self, points, mat, size, padding=0):
         """ Transform points along matrix """
+        logger.trace("points: %s, matrix: %s, size: %s. padding: %s", points, mat, size, padding)
         matrix = self.transform_matrix(mat, size, padding)
         points = np.expand_dims(points, axis=1)
         points = cv2.transform(  # pylint: disable=no-member
             points, matrix, points.shape)
-        return np.squeeze(points)
+        retval = np.squeeze(points)
+        logger.trace("Returning: %s", retval)
+        return retval
 
     def get_original_roi(self, mat, size, padding=0):
         """ Return the square aligned box location on the original
             image """
+        logger.trace("matrix: %s, size: %s. padding: %s", mat, size, padding)
         matrix = self.transform_matrix(mat, size, padding)
         points = np.array([[0, 0],
                            [0, size - 1],
@@ -71,6 +85,7 @@ class Extract():
                            [size - 1, 0]], np.int32)
         points = points.reshape((-1, 1, 2))
         matrix = cv2.invertAffineTransform(matrix)  # pylint: disable=no-member
+        logger.trace("Returning: (points: %s, matrix: %s", points, matrix)
         return cv2.transform(points, matrix)  # pylint: disable=no-member
 
     @staticmethod
@@ -78,7 +93,9 @@ class Extract():
                          padding=0, dilation=30):
         """ Return the face feature mask """
         # pylint: disable=no-member
-        scale = size - 2*padding
+        logger.trace("aligned_landmarks_68: %s, size: %s, padding: %s, dilation: %s",
+                     aligned_landmarks_68, size, padding, dilation)
+        scale = size - 2 * padding
         translation = padding
         pad_mat = np.matrix([[scale, 0.0, translation],
                              [0.0, scale, translation]])
@@ -123,12 +140,14 @@ class Extract():
             kernel = np.ones((dilation, dilation), np.uint8)
             mask = cv2.dilate(mask, kernel, iterations=1)
 
+        logger.trace("Returning: %s", mask)
         return mask
 
 
 def get_align_mat(face, size, should_align_eyes):
     """ Return the alignment Matrix """
-    mat_umeyama = umeyama(np.array(face.landmarks_as_xy()[17:]),
+    logger.trace("size: %s, should_align_eyes: %s", size, should_align_eyes)
+    mat_umeyama = umeyama(np.array(face.landmarks_as_xy[17:]),
                           LANDMARKS_2D,
                           True)[0:2]
 
@@ -138,7 +157,7 @@ def get_align_mat(face, size, should_align_eyes):
     mat_umeyama = mat_umeyama * size
 
     # Convert to matrix
-    landmarks = np.matrix(face.landmarks_as_xy())
+    landmarks = np.matrix(face.landmarks_as_xy)
 
     # cv2 expects points to be in the form
     # np.array([ [[x1, y1]], [[x2, y2]], ... ]), we'll expand the dim
@@ -167,4 +186,5 @@ def get_align_mat(face, size, should_align_eyes):
     # Remove the extra row added, shape needs to be 2x3
     transform_mat = np.delete(transform_mat, 2, 0)
     transform_mat = transform_mat / size
+    logger.trace("Returning: %s", transform_mat)
     return transform_mat
diff --git a/lib/alignments.py b/lib/alignments.py
index eacaafb..e501762 100644
--- a/lib/alignments.py
+++ b/lib/alignments.py
@@ -2,6 +2,7 @@
 """ Alignments file functions for reading, writing and manipulating
     a serialized alignments file """
 
+import logging
 import os
 from datetime import datetime
 
@@ -10,6 +11,8 @@ import cv2
 from lib import Serializer
 from lib.utils import rotate_landmarks
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Alignments():
     """ Holds processes pertaining to the alignments file.
@@ -23,155 +26,198 @@ class Alignments():
                     saved in (if data is to be saved). Can be 'json', 'pickle'
                     or 'yaml'
     """
-    def __init__(self, folder, filename="alignments", serializer="json",
-                 verbose=False):
-        self.verbose = verbose
+    # pylint: disable=too-many-public-methods
+    def __init__(self, folder, filename="alignments", serializer="json"):
+        logger.debug("Initializing %s: (folder: '%s', filename: '%s', serializer: '%s')",
+                     self.__class__.__name__, folder, filename, serializer)
         self.serializer = self.get_serializer(filename, serializer)
         self.file = self.get_location(folder, filename)
 
         self.data = self.load()
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     # << PROPERTIES >> #
 
     @property
     def frames_count(self):
         """ Return current frames count """
-        return len(self.data)
+        retval = len(self.data)
+        logger.trace(retval)
+        return retval
 
     @property
     def faces_count(self):
         """ Return current faces count """
-        return sum(len(faces) for faces in self.data.values())
+        retval = sum(len(faces) for faces in self.data.values())
+        logger.trace(retval)
+        return retval
 
     @property
     def have_alignments_file(self):
         """ Return whether an alignments file exists """
-        return os.path.exists(self.file)
+        retval = os.path.exists(self.file)
+        logger.trace(retval)
+        return retval
 
     # << INIT FUNCTIONS >> #
 
-    def get_serializer(self, filename, serializer):
+    @staticmethod
+    def get_serializer(filename, serializer):
         """ Set the serializer to be used for loading and
             saving alignments
 
             If a filename with a valid extension is passed in
             this will be used as the serializer, otherwise the
             specified serializer will be used """
+        logger.debug("Getting serializer: (filename: '%s', serializer: '%s')",
+                     filename, serializer)
         extension = os.path.splitext(filename)[1]
         if extension in (".json", ".p", ".yaml", ".yml"):
+            logger.debug("Serializer set from file extension: '%s'", extension)
             retval = Serializer.get_serializer_from_ext(extension)
         elif serializer not in ("json", "pickle", "yaml"):
             raise ValueError("Error: {} is not a valid serializer. Use "
                              "'json', 'pickle' or 'yaml'")
         else:
+            logger.debug("Serializer set from argument: '%s'", serializer)
             retval = Serializer.get_serializer(serializer)
-        if self.verbose:
-            print("Using {} serializer for alignments".format(retval.ext))
+        logger.verbose("Using '%s' serializer for alignments", retval.ext)
         return retval
 
     def get_location(self, folder, filename):
         """ Return the path to alignments file """
+        logger.debug("Getting location: (folder: '%s', filename: '%s')", folder, filename)
         extension = os.path.splitext(filename)[1]
         if extension in (".json", ".p", ".yaml", ".yml"):
+            logger.debug("File extension set from filename: '%s'", extension)
             location = os.path.join(str(folder), filename)
         else:
             location = os.path.join(str(folder),
                                     "{}.{}".format(filename,
                                                    self.serializer.ext))
-        if self.verbose:
-            print("Alignments filepath: {}".format(location))
+            logger.debug("File extension set from serializer: '%s'", self.serializer.ext)
+        logger.verbose("Alignments filepath: '%s'", location)
         return location
 
     # << I/O >> #
 
     def load(self):
-        """ Load the alignments data if it exists or create empty dict """
+        """ Load the alignments data
+            Override for custom loading logic """
+        logger.debug("Loading alignments")
         if not self.have_alignments_file:
             raise ValueError("Error: Alignments file not found at "
                              "{}".format(self.file))
 
         try:
-            print("Reading alignments from: {}".format(self.file))
+            logger.info("Reading alignments from: '%s'", self.file)
             with open(self.file, self.serializer.roptions) as align:
                 data = self.serializer.unmarshal(align.read())
         except IOError as err:
-            print("Error: {} not read: {}".format(self.file, err.strerror))
+            logger.error("'%s' not read: %s", self.file, err.strerror)
             exit(1)
+        logger.debug("Loaded alignments")
         return data
 
     def reload(self):
         """ Read the alignments data from the correct format """
+        logger.debug("Re-loading alignments")
         self.data = self.load()
+        logger.debug("Re-loaded alignments")
 
     def save(self):
         """ Write the serialized alignments file """
+        logger.debug("Saving alignments")
         try:
-            print("Writing alignments to: {}".format(self.file))
+            logger.info("Writing alignments to: '%s'", self.file)
             with open(self.file, self.serializer.woptions) as align:
                 align.write(self.serializer.marshal(self.data))
+            logger.debug("Saved alignments")
         except IOError as err:
-            print("Error: {} not written: {}".format(self.file, err.strerror))
+            logger.error("'%s' not written: %s", self.file, err.strerror)
 
     def backup(self):
         """ Backup copy of old alignments """
+        logger.debug("Backing up alignments")
         if not os.path.isfile(self.file):
+            logger.debug("No alignments to back up")
             return
         now = datetime.now().strftime("%Y%m%d_%H%M%S")
         src = self.file
         split = os.path.splitext(src)
         dst = split[0] + "_" + now + split[1]
-        print("Backing up original alignments to {}".format(dst))
+        logger.info("Backing up original alignments to '%s'", dst)
         os.rename(src, dst)
+        logger.debug("Backed up alignments")
 
     # << VALIDATION >> #
 
     def frame_exists(self, frame):
         """ return path of images that have faces """
-        return frame in self.data.keys()
+        retval = frame in self.data.keys()
+        logger.trace(retval)
+        return retval
 
     def frame_has_faces(self, frame):
         """ Return true if frame exists and has faces """
-        return bool(self.data.get(frame, list()))
+        retval = bool(self.data.get(frame, list()))
+        logger.trace(retval)
+        return retval
 
     def frame_has_multiple_faces(self, frame):
         """ Return true if frame exists and has faces """
         if not frame:
-            return False
-        return bool(len(self.data.get(frame, list())) > 1)
+            retval = False
+        else:
+            retval = bool(len(self.data.get(frame, list())) > 1)
+        logger.trace(retval)
+        return retval
 
     # << DATA >> #
 
     def get_faces_in_frame(self, frame):
         """ Return the alignments for the selected frame """
+        logger.trace("Getting faces for frame: '%s'", frame)
         return self.data.get(frame, list())
 
     def get_full_frame_name(self, frame):
         """ Return a frame with extension for when the extension is
             not known """
-        return next(key for key in self.data.keys()
-                    if key.startswith(frame))
+        retval = next(key for key in self.data.keys()
+                      if key.startswith(frame))
+        logger.trace("Requested: '%s', Returning: '%s'", frame, retval)
+        return retval
 
     def count_faces_in_frame(self, frame):
         """ Return number of alignments within frame """
-        return len(self.data.get(frame, list()))
+        retval = len(self.data.get(frame, list()))
+        logger.trace(retval)
+        return retval
 
     # << MANIPULATION >> #
 
     def delete_face_at_index(self, frame, idx):
         """ Delete the face alignment for given frame at given index """
+        logger.debug("Deleting face %s for frame '%s'", idx, frame)
         idx = int(idx)
         if idx + 1 > self.count_faces_in_frame(frame):
+            logger.debug("No face to delete")
             return False
         del self.data[frame][idx]
+        logger.debug("Deleted face")
         return True
 
     def add_face(self, frame, alignment):
         """ Add a new face for a frame and return it's index """
+        logger.debug("Adding face to frame: '%s'", frame)
         self.data[frame].append(alignment)
-        return self.count_faces_in_frame(frame) - 1
+        retval = self.count_faces_in_frame(frame) - 1
+        logger.debug("Returning new face index: %s", retval)
+        return retval
 
     def update_face(self, frame, idx, alignment):
         """ Replace a face for given frame and index """
+        logger.debug("Updating face %s for frame '%s'", idx, frame)
         self.data[frame][idx] = alignment
 
     # << GENERATORS >> #
@@ -180,7 +226,10 @@ class Alignments():
         """ Yield face alignments for one image """
         for frame_fullname, alignments in self.data.items():
             frame_name = os.path.splitext(frame_fullname)[0]
-            yield frame_name, alignments, len(alignments), frame_fullname
+            face_count = len(alignments)
+            logger.trace("Yielding: (frame: '%s', faces: %s, frame_fullname: '%s')",
+                         frame_name, face_count, frame_fullname)
+            yield frame_name, alignments, face_count, frame_fullname
 
     @staticmethod
     def yield_original_index_reverse(image_alignments, number_alignments):
@@ -188,6 +237,7 @@ class Alignments():
             alignment in reverse order """
         for idx, _ in enumerate(reversed(image_alignments)):
             original_idx = number_alignments - 1 - idx
+            logger.trace("Yielding: face index %s", original_idx)
             yield original_idx
 
     # << LEGACY FUNCTIONS >> #
@@ -202,17 +252,20 @@ class Alignments():
     def get_legacy_no_dims(self):
         """ Return a list of frames that do not contain the original frame
             height and width attributes """
+        logger.debug("Getting alignments without frame_dims")
         keys = list()
         for key, val in self.data.items():
             for alignment in val:
                 if "frame_dims" not in alignment.keys():
                     keys.append(key)
                     break
+        logger.debug("Got alignments without frame_dims: %s", len(keys))
         return keys
 
     def add_dimensions(self, frame_name, dimensions):
         """ Backward compatability fix. Add frame dimensions
             to alignments """
+        logger.trace("Adding dimensions: (frame: '%s', dimensions: %s)", frame_name, dimensions)
         for face in self.get_faces_in_frame(frame_name):
             face["frame_dims"] = dimensions
 
@@ -230,10 +283,12 @@ class Alignments():
         """ Return a list of frames with legacy rotations
             Looks for an 'r' value in the alignments file that
             is not zero """
+        logger.debug("Getting alignments containing legacy rotations")
         keys = list()
         for key, val in self.data.items():
             if any(alignment.get("r", None) for alignment in val):
                 keys.append(key)
+        logger.debug("Got alignments containing legacy rotations: %s", len(keys))
         return keys
 
     def rotate_existing_landmarks(self, frame_name):
@@ -242,18 +297,24 @@ class Alignments():
 
             NB: The original frame dimensions must be passed in otherwise
             the transformation cannot be performed """
+        logger.trace("Rotating existing landmarks for frame: '%s'", frame_name)
         for face in self.get_faces_in_frame(frame_name):
             angle = face.get("r", 0)
             if not angle:
+                logger.trace("Landmarks do not require rotation: '%s'", frame_name)
                 return
+            logger.trace("Rotating landmarks: (frame: '%s', angle: %s)", frame_name, angle)
             dims = face["frame_dims"]
             r_mat = self.get_original_rotation_matrix(dims, angle)
             rotate_landmarks(face, r_mat)
             del face["r"]
+        logger.trace("Rotatated existing landmarks for frame: '%s'", frame_name)
 
     @staticmethod
     def get_original_rotation_matrix(dimensions, angle):
         """ Calculate original rotation matrix and invert """
+        logger.trace("Getting original rotation matrix: (dimensions: %s, angle: %s)",
+                     dimensions, angle)
         height, width = dimensions
         center = (width/2, height/2)
         r_mat = cv2.getRotationMatrix2D(  # pylint: disable=no-member
@@ -265,5 +326,5 @@ class Alignments():
         rotated_height = int(height*abs_cos + width*abs_sin)
         r_mat[0, 2] += rotated_width/2 - center[0]
         r_mat[1, 2] += rotated_height/2 - center[1]
-
+        logger.trace("Returning rotation matrix: %s", r_mat)
         return r_mat
diff --git a/lib/cli.py b/lib/cli.py
index a877cc1..e8aee26 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -1,13 +1,19 @@
 #!/usr/bin/env python3
 """ Command Line Arguments """
 import argparse
-from importlib import import_module
+import logging
 import os
 import platform
 import sys
 
+from importlib import import_module
+
+from lib.logger import crash_log, log_setup
+from lib.utils import safe_shutdown
 from plugins.plugin_loader import PluginLoader
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class ScriptExecutor():
     """ Loads the relevant script modules and executes the script.
@@ -47,20 +53,20 @@ class ScriptExecutor():
             traceback errors to console """
 
         try:
-            import tkinter
+            # pylint: disable=unused-variable
+            import tkinter  # noqa
         except ImportError:
-            print(
+            logger.warning(
                 "It looks like TkInter isn't installed for your OS, so "
                 "the GUI has been disabled. To enable the GUI please "
-                "install the TkInter application.\n\n"
-                "You can try:\n"
-                "  Anaconda:           conda install tk\n"
-                "  Windows/macOS:      Install ActiveTcl Community "
-                "Edition from http://www.activestate.com\n"
-                "  Ubuntu/Mint/Debian: sudo apt install python3-tk\n"
-                "  Arch:               sudo pacman -S tk\n"
-                "  CentOS/Redhat:      sudo yum install tkinter\n"
-                "  Fedora:             sudo dnf install python3-tkinter\n")
+                "install the TkInter application. You can try:")
+            logger.info("Anaconda: conda install tk")
+            logger.info("Windows/macOS: Install ActiveTcl Community Edition from "
+                        "http://www.activestate.com")
+            logger.info("Ubuntu/Mint/Debian: sudo apt install python3-tk")
+            logger.info("Arch: sudo pacman -S tk")
+            logger.info("CentOS/Redhat: sudo yum install tkinter")
+            logger.info("Fedora: sudo dnf install python3-tkinter")
             exit(1)
 
     @staticmethod
@@ -68,21 +74,38 @@ class ScriptExecutor():
         """ Check whether there is a display to output the GUI. If running on
             Windows then assume not running in headless mode """
         if not os.environ.get("DISPLAY", None) and os.name != "nt":
-            print("No display detected. GUI mode has been disabled.")
+            logger.warning("No display detected. GUI mode has been disabled.")
             if platform.system() == "Darwin":
-                print("macOS users need to install XQuartz. "
-                      "See https://support.apple.com/en-gb/HT201341")
+                logger.info("macOS users need to install XQuartz. "
+                            "See https://support.apple.com/en-gb/HT201341")
             exit(1)
 
     def execute_script(self, arguments):
         """ Run the script for called command """
-        script = self.import_script()
-        process = script(arguments)
-        process.process()
+        log_setup(arguments.loglevel)
+        logger.debug("Executing: %s. PID: %s", self.command, os.getpid())
+        try:
+            script = self.import_script()
+            process = script(arguments)
+            process.process()
+        except KeyboardInterrupt:  # pylint: disable=try-except-raise
+            raise
+        except SystemExit:
+            pass
+        except Exception:  # pylint: disable=broad-except
+            crash_file = crash_log()
+            logger.exception("Got Exception on main handler:")
+            logger.critical("An unexpected crash has occurred. Crash report written to %s. "
+                            "Please verify you are running the latest version of faceswap "
+                            "before reporting", crash_file)
+
+        finally:
+            safe_shutdown()
 
 
 class FullPaths(argparse.Action):
     """ Expand user- and relative-paths """
+    # pylint: disable=too-few-public-methods
     def __call__(self, parser, namespace, values, option_string=None):
         setattr(namespace, self.dest, os.path.abspath(
             os.path.expanduser(values)))
@@ -90,6 +113,7 @@ class FullPaths(argparse.Action):
 
 class DirFullPaths(FullPaths):
     """ Class that gui uses to determine if you need to open a directory """
+    # pylint: disable=too-few-public-methods
     pass
 
 
@@ -99,6 +123,7 @@ class FileFullPaths(FullPaths):
 
     see lib/gui/utils.py FileHandler for current GUI filetypes
     """
+    # pylint: disable=too-few-public-methods
     def __init__(self, option_strings, dest, nargs=None, filetypes=None,
                  **kwargs):
         super(FileFullPaths, self).__init__(option_strings, dest, **kwargs)
@@ -128,6 +153,7 @@ class SaveFileFullPaths(FileFullPaths):
 
     see lib/gui/utils.py FileHandler for current GUI filetypes
     """
+    # pylint: disable=too-few-public-methods
     pass
 
 
@@ -141,6 +167,7 @@ class ContextFullPaths(FileFullPaths):
 
     Bespoke actions are then set in lib/gui/utils.py FileHandler
     """
+    # pylint: disable=too-few-public-methods, too-many-arguments
     def __init__(self, option_strings, dest, nargs=None, filetypes=None,
                  action_option=None, **kwargs):
         if nargs is not None:
@@ -199,6 +226,7 @@ class FaceSwapArgs():
     def __init__(self, subparser, command,
                  description="default", subparsers=None):
 
+        self.global_arguments = self.get_global_arguments()
         self.argument_list = self.get_argument_list()
         self.optional_arguments = self.get_optional_arguments()
         if not subparser:
@@ -226,6 +254,22 @@ class FaceSwapArgs():
         argument_list = []
         return argument_list
 
+    @staticmethod
+    def get_global_arguments():
+        """ Arguments that are used in ALL parts of Faceswap
+            DO NOT override this """
+        global_args = list()
+        global_args.append({"opts": ("-L", "--loglevel"),
+                            "type": str.upper,
+                            "dest": "loglevel",
+                            "default": "INFO",
+                            "choices": ("INFO", "VERBOSE", "DEBUG", "TRACE"),
+                            "help": "Log level. Stick with INFO or VERBOSE "
+                                    "unless you need to file an error report. Be "
+                                    "careful with TRACE as it will generate a lot "
+                                    "of data"})
+        return global_args
+
     @staticmethod
     def create_parser(subparser, command, description):
         """ Create the parser for the selected command """
@@ -240,7 +284,8 @@ class FaceSwapArgs():
 
     def add_arguments(self):
         """ Parse the arguments passed in from argparse """
-        for option in self.argument_list + self.optional_arguments:
+        options = self.global_arguments + self.argument_list + self.optional_arguments
+        for option in options:
             args = option["opts"]
             kwargs = {key: option[key]
                       for key in option.keys() if key != "opts"}
@@ -304,11 +349,6 @@ class ExtractConvertArgs(FaceSwapArgs):
                                       "want to process. Should be a front "
                                       "portrait. Multiple images can be added "
                                       "space separated"})
-        argument_list.append({"opts": ("-v", "--verbose"),
-                              "action": "store_true",
-                              "dest": "verbose",
-                              "default": False,
-                              "help": "Show verbose output"})
         return argument_list
 
 
@@ -401,7 +441,7 @@ class ExtractArgs(ExtractConvertArgs):
                                       "pass in a list of numbers to enumerate "
                                       "exactly what angles to check"})
         argument_list.append({"opts": ("-bt", "--blur-threshold"),
-                              "type": int,
+                              "type": float,
                               "dest": "blur_thresh",
                               "default": None,
                               "help": "Automatically discard images blurrier "
@@ -679,11 +719,6 @@ class TrainArgs(FaceSwapArgs):
                               "default": False,
                               "help": "Sets allow_growth option of Tensorflow "
                                       "to spare memory on some configs"})
-        argument_list.append({"opts": ("-v", "--verbose"),
-                              "action": "store_true",
-                              "dest": "verbose",
-                              "default": False,
-                              "help": "Show verbose output"})
         argument_list.append({"opts": ("-tia", "--timelapse-input-A"),
                               "action": DirFullPaths,
                               "dest": "timelapse_input_A",
diff --git a/lib/detect_blur.py b/lib/detect_blur.py
deleted file mode 100644
index 15928e9..0000000
--- a/lib/detect_blur.py
+++ /dev/null
@@ -1,17 +0,0 @@
-import cv2
-
-def variance_of_laplacian(image):
-    # compute the Laplacian of the image and then return the focus
-    # measure, which is simply the variance of the Laplacian
-    return cv2.Laplacian(image, cv2.CV_64F).var()
-
-def is_blurry(image, threshold):
-    # Convert to grayscale, and compute the
-    # focus measure of the image using the
-    # Variance of Laplacian method
-    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
-    fm = variance_of_laplacian(gray)
-
-    # if the focus measure is less than the supplied threshold,
-    # then the image should be considered "blurry"
-    return (fm < threshold, fm)
diff --git a/lib/face_filter.py b/lib/face_filter.py
new file mode 100644
index 0000000..a373fcb
--- /dev/null
+++ b/lib/face_filter.py
@@ -0,0 +1,91 @@
+#!/usr/bin python3
+""" Face Filterer for extraction in faceswap.py """
+
+import logging
+
+import face_recognition
+
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+def avg(arr):
+    """ Return an average """
+    return sum(arr) * 1.0 / len(arr)
+
+
+class FaceFilter():
+    """ Face filter for extraction """
+    def __init__(self, reference_file_paths, nreference_file_paths, threshold=0.6):
+        logger.debug("Initializing %s: (reference_file_paths: %s, nreference_file_paths: %s, "
+                     "threshold: %s)", self.__class__.__name__, reference_file_paths,
+                     nreference_file_paths, threshold)
+        images = list(map(face_recognition.load_image_file, reference_file_paths))
+        nimages = list(map(face_recognition.load_image_file, nreference_file_paths))
+        # Note: we take only first face, so the reference file should only contain one face.
+        self.encodings = list(map(lambda im: face_recognition.face_encodings(im)[0], images))
+        self.nencodings = list(map(lambda im: face_recognition.face_encodings(im)[0], nimages))
+        self.threshold = threshold
+        logger.trace("encodings: %s", self.encodings)
+        logger.trace("nencodings: %s", self.nencodings)
+        logger.debug("Initialized %s", self.__class__.__name__)
+
+    def check(self, detected_face):
+        """ Check Face
+            we could use detected landmarks, but I did not manage to do so.
+            TODO The copy/paste below should help """
+        logger.trace("Checking face with FaceFilter")
+        encodings = face_recognition.face_encodings(detected_face.image)
+        if not encodings:
+            logger.verbose("No face encodings found")
+            return False
+
+        if self.encodings:
+            distances = list(face_recognition.face_distance(self.encodings, encodings[0]))
+            logger.trace("Distances: %s", distances)
+            distance = avg(distances)
+            logger.trace("Average Distance: %s", distance)
+            mindistance = min(distances)
+            logger.trace("Minimum Distance: %s", mindistance)
+            if distance > self.threshold:
+                logger.verbose("Distance above threshold: %f < %f", distance, self.threshold)
+                return False
+        if self.nencodings:
+            ndistances = list(face_recognition.face_distance(self.nencodings, encodings[0]))
+            logger.trace("nDistances: %s", ndistances)
+            ndistance = avg(ndistances)
+            logger.trace("Average nDistance: %s", ndistance)
+            nmindistance = min(ndistances)
+            logger.trace("Minimum nDistance: %s", nmindistance)
+            if not self.encodings and ndistance < self.threshold:
+                logger.verbose("nDistance below threshold: %f < %f", ndistance, self.threshold)
+                return False
+            if self.encodings:
+                if mindistance > nmindistance:
+                    logger.verbose("Distance to negative sample is smaller")
+                    return False
+                if distance > ndistance:
+                    logger.verbose("Average distance to negative sample is smaller")
+                    return False
+                # k-nn classifier
+                var_k = min(5, min(len(distances), len(ndistances)) + 1)
+                var_n = sum(list(map(lambda x: x[0],
+                                     list(sorted([(1, d) for d in distances] +
+                                                 [(0, d) for d in ndistances],
+                                                 key=lambda x: x[1]))[:var_k])))
+                ratio = var_n/var_k
+                if ratio < 0.5:
+                    logger.verbose("K-nn is %.2f", ratio)
+                    return False
+        return True
+
+
+# # Copy/Paste (mostly) from private method in face_recognition
+# face_recognition_model = face_recognition_models.face_recognition_model_location()
+# face_encoder = dlib.face_recognition_model_v1(face_recognition_model)
+
+# def convert(detected_face):
+#     return np.array(face_encoder.compute_face_descriptor(detected_face.image,
+#                                                          detected_face.landmarks,
+#                                                          1))
+# # end of Copy/Paste
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 09bfef5..bd69276 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -1,15 +1,19 @@
 #!/usr/bin python3
 """ Face and landmarks detection for faceswap.py """
+import logging
 
 from dlib import rectangle as d_rectangle  # pylint: disable=no-name-in-module
 from lib.aligner import Extract as AlignerExtract, get_align_mat
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class DetectedFace():
     """ Detected face and landmark information """
     def __init__(  # pylint: disable=invalid-name
             self, image=None, x=None, w=None, y=None, h=None,
             frame_dims=None, landmarksXY=None):
+        logger.trace("Initializing %s", self.__class__.__name__)
         self.image = image
         self.x = x
         self.w = w
@@ -19,7 +23,9 @@ class DetectedFace():
         self.landmarksXY = landmarksXY
 
         self.aligned = dict()
+        logger.trace("Initialized %s", self.__class__.__name__)
 
+    @property
     def landmarks_as_xy(self):
         """ Landmarks as XY """
         return self.landmarksXY
@@ -30,10 +36,13 @@ class DetectedFace():
         top = self.y
         right = self.x + self.w
         bottom = self.y + self.h
-        return d_rectangle(left, top, right, bottom)
+        retval = d_rectangle(left, top, right, bottom)
+        logger.trace("Returning: %s", retval)
+        return retval
 
     def from_dlib_rect(self, d_rect, image=None):
         """ Set Bounding Box from a Dlib Rectangle """
+        logger.trace("Creating from dlib_rectangle: %s", d_rect)
         if not isinstance(d_rect, d_rectangle):
             raise ValueError("Supplied Bounding Box is not a dlib.rectangle.")
         self.x = d_rect.left()
@@ -42,10 +51,13 @@ class DetectedFace():
         self.h = d_rect.bottom() - d_rect.top()
         if image.any():
             self.image_to_face(image)
+        logger.trace("Created from dlib_rectangle: (x: %s, w: %s, y: %s. h: %s)",
+                     self.x, self.w, self.y, self.h)
 
     def image_to_face(self, image):
         """ Crop an image around bounding box to the face
             and capture it's dimensions """
+        logger.trace("Cropping face from image")
         self.image = image[self.y: self.y + self.h,
                            self.x: self.x + self.w]
 
@@ -58,10 +70,13 @@ class DetectedFace():
         alignment["h"] = self.h
         alignment["frame_dims"] = self.frame_dims
         alignment["landmarksXY"] = self.landmarksXY
+        logger.trace("Returning: %s", alignment)
         return alignment
 
     def from_alignment(self, alignment, image=None):
         """ Convert a face alignment to detected face object """
+        logger.trace("Creating from alignment: (alignment: %s, has_image: %s)",
+                     alignment, bool(image is not None))
         self.x = alignment["x"]
         self.w = alignment["w"]
         self.y = alignment["y"]
@@ -70,12 +85,17 @@ class DetectedFace():
         self.landmarksXY = alignment["landmarksXY"]
         if image.any():
             self.image_to_face(image)
+        logger.trace("Created from alignment: (x: %s, w: %s, y: %s. h: %s, "
+                     "frame_dims: %s, landmarks: %s)",
+                     self.x, self.w, self.y, self.h, self.frame_dims, self.landmarksXY)
 
     # <<< Aligned Face methods and properties >>> #
     def load_aligned(self, image, size=256, padding=48, align_eyes=False):
         """ No need to load aligned information for all uses of this
             class, so only call this to load the information for easy
             reference to aligned properties for this face """
+        logger.trace("Loading aligned face: (size: %s, padding: %s, align_eyes: %s)",
+                     size, padding, align_eyes)
         self.aligned["size"] = size
         self.aligned["padding"] = padding
         self.aligned["align_eyes"] = align_eyes
@@ -85,22 +105,29 @@ class DetectedFace():
             self.aligned["matrix"],
             size,
             padding)
+        logger.trace("Loaded aligned face: %s", {key: val
+                                                 for key, val in self.aligned.items()
+                                                 if key != "face"})
 
     @property
     def original_roi(self):
         """ Return the square aligned box location on the original
             image """
-        return AlignerExtract().get_original_roi(self.aligned["matrix"],
-                                                 self.aligned["size"],
-                                                 self.aligned["padding"])
+        roi = AlignerExtract().get_original_roi(self.aligned["matrix"],
+                                                self.aligned["size"],
+                                                self.aligned["padding"])
+        logger.trace("Returning: %s", roi)
+        return roi
 
     @property
     def aligned_landmarks(self):
         """ Return the landmarks location transposed to extracted face """
-        return AlignerExtract().transform_points(self.landmarksXY,
-                                                 self.aligned["matrix"],
-                                                 self.aligned["size"],
-                                                 self.aligned["padding"])
+        landmarks = AlignerExtract().transform_points(self.landmarksXY,
+                                                      self.aligned["matrix"],
+                                                      self.aligned["size"],
+                                                      self.aligned["padding"])
+        logger.trace("Returning: %s", landmarks)
+        return landmarks
 
     @property
     def aligned_face(self):
@@ -110,6 +137,8 @@ class DetectedFace():
     @property
     def adjusted_matrix(self):
         """ Return adjusted matrix for size/padding combination """
-        return AlignerExtract().transform_matrix(self.aligned["matrix"],
-                                                 self.aligned["size"],
-                                                 self.aligned["padding"])
+        mat = AlignerExtract().transform_matrix(self.aligned["matrix"],
+                                                self.aligned["size"],
+                                                self.aligned["padding"])
+        logger.trace("Returning: %s", mat)
+        return mat
diff --git a/lib/gpu_stats.py b/lib/gpu_stats.py
index 6911356..11c5fa8 100644
--- a/lib/gpu_stats.py
+++ b/lib/gpu_stats.py
@@ -1,20 +1,26 @@
 #!/usr/bin python3
 """ Information on available Nvidia GPUs """
 
+import logging
 import platform
 
 if platform.system() == 'Darwin':
-    import pynvx
-    is_macos = True
+    import pynvx  # pylint: disable=import-error
+    IS_MACOS = True
 else:
     import pynvml
-    is_macos = False
+    IS_MACOS = False
 
 
 class GPUStats():
     """ Holds information about system GPU(s) """
-    def __init__(self):
-        self.verbose = False
+    def __init__(self, log=True):
+        self.logger = None
+        if log:
+            # Logger is held internally, as we don't want to log
+            # when obtaining system stats on crash
+            self.logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+            self.logger.debug("Initializing %s", self.__class__.__name__)
 
         self.initialized = False
         self.device_count = 0
@@ -33,11 +39,15 @@ class GPUStats():
         self.vram = self.get_vram()
 
         self.shutdown()
+        if self.logger:
+            self.logger.debug("Initialized %s", self.__class__.__name__)
 
     def initialize(self):
         """ Initialize pynvml """
         if not self.initialized:
-            if is_macos:
+            if IS_MACOS:
+                if self.logger:
+                    self.logger.debug("macOS Detected. Using pynvx")
                 try:
                     pynvx.cudaInit()
                 except RuntimeError:
@@ -45,6 +55,8 @@ class GPUStats():
                     return
             else:
                 try:
+                    if self.logger:
+                        self.logger.debug("OS is not macOS. Using pynvml")
                     pynvml.nvmlInit()
                 except pynvml.NVMLError_LibraryNotFound:
                     self.initialized = True
@@ -57,67 +69,76 @@ class GPUStats():
         """ Shutdown pynvml """
         if self.initialized:
             self.handles = None
-            if not is_macos:
+            if not IS_MACOS:
                 pynvml.nvmlShutdown()
             self.initialized = False
 
     def get_device_count(self):
         """ Return count of Nvidia devices """
-        if is_macos:
+        if IS_MACOS:
             self.device_count = pynvx.cudaDeviceGetCount(ignore=True)
         else:
             try:
                 self.device_count = pynvml.nvmlDeviceGetCount()
             except pynvml.NVMLError:
                 self.device_count = 0
+        if self.logger:
+            self.logger.debug("GPU Device count: %s", self.device_count)
 
     def get_handles(self):
         """ Return all listed Nvidia handles """
-        if is_macos:
+        if IS_MACOS:
             self.handles = pynvx.cudaDeviceGetHandles(ignore=True)
         else:
             self.handles = [pynvml.nvmlDeviceGetHandleByIndex(i)
                             for i in range(self.device_count)]
+        if self.logger:
+            self.logger.debug("GPU Handles found: %s", len(self.handles))
 
-    @staticmethod
-    def get_driver():
+    def get_driver(self):
         """ Get the driver version """
-        if is_macos:
+        if IS_MACOS:
             driver = pynvx.cudaSystemGetDriverVersion(ignore=True)
         else:
             try:
                 driver = pynvml.nvmlSystemGetDriverVersion().decode("utf-8")
             except pynvml.NVMLError:
                 driver = "No Nvidia driver found"
+        if self.logger:
+            self.logger.debug("GPU Driver: %s", driver)
         return driver
 
     def get_devices(self):
         """ Return name of devices """
         self.initialize()
-        if is_macos:
+        if IS_MACOS:
             names = [pynvx.cudaGetName(handle, ignore=True)
                      for handle in self.handles]
         else:
             names = [pynvml.nvmlDeviceGetName(handle).decode("utf-8")
                      for handle in self.handles]
+        if self.logger:
+            self.logger.debug("GPU Devices: %s", names)
         return names
 
     def get_vram(self):
         """ Return total vram in megabytes per device """
         self.initialize()
-        if is_macos:
+        if IS_MACOS:
             vram = [pynvx.cudaGetMemTotal(handle, ignore=True) / (1024 * 1024)
                     for handle in self.handles]
         else:
             vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).total /
                     (1024 * 1024)
                     for handle in self.handles]
+        if self.logger:
+            self.logger.debug("GPU VRAM: %s", vram)
         return vram
 
     def get_used(self):
         """ Return the vram in use """
         self.initialize()
-        if is_macos:
+        if IS_MACOS:
             vram = [pynvx.cudaGetMemUsed(handle, ignore=True) / (1024 * 1024)
                     for handle in self.handles]
         else:
@@ -125,21 +146,22 @@ class GPUStats():
                     for handle in self.handles]
         self.shutdown()
 
-        if self.verbose:
-            print("GPU VRAM used:    {}".format(vram))
-
+        if self.logger:
+            self.logger.verbose("GPU VRAM used: %s", vram)
         return vram
 
     def get_free(self):
         """ Return the vram available """
         self.initialize()
-        if is_macos:
+        if IS_MACOS:
             vram = [pynvx.cudaGetMemFree(handle, ignore=True) / (1024 * 1024)
                     for handle in self.handles]
         else:
             vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).free / (1024 * 1024)
                     for handle in self.handles]
         self.shutdown()
+        if self.logger:
+            self.logger.debug("GPU VRAM free: %s", vram)
         return vram
 
     def get_card_most_free(self):
@@ -153,14 +175,10 @@ class GPUStats():
         free_vram = self.get_free()
         vram_free = max(free_vram)
         card_id = free_vram.index(vram_free)
-        return {"card_id": card_id,
-                "device": self.devices[card_id],
-                "free": vram_free,
-                "total": self.vram[card_id]}
-
-    def print_info(self):
-        """ Output GPU info in verbose mode """
-        print("GPU Driver:       {}".format(self.driver))
-        print("GPU Device count: {}".format(self.device_count))
-        print("GPU Devices:      {}".format(self.devices))
-        print("GPU VRAM:         {}".format(self.vram))
+        retval = {"card_id": card_id,
+                  "device": self.devices[card_id],
+                  "free": vram_free,
+                  "total": self.vram[card_id]}
+        if self.logger:
+            self.logger.debug("GPU Card with most free VRAM: %s", retval)
+        return retval
diff --git a/lib/gui/options.py b/lib/gui/options.py
index 80780d9..92fedd0 100644
--- a/lib/gui/options.py
+++ b/lib/gui/options.py
@@ -4,13 +4,15 @@ import inspect
 from argparse import SUPPRESS
 from tkinter import ttk
 
-import lib.cli as cli
+from lib import cli
 from lib.Serializer import JSONSerializer
 import tools.cli as ToolsCli
 from .utils import FileHandler, Images
 
+# TODO Fix the bug that breaks GUI if timeshift isn't the last option in it's group
 
-class CliOptions(object):
+
+class CliOptions():
     """ Class and methods for the command line options """
     def __init__(self):
         self.categories = ("faceswap", "tools")
@@ -70,7 +72,7 @@ class CliOptions(object):
     def get_cli_arguments(cli_source, classname, command):
         """ Extract the options from the main and tools cli files """
         meth = getattr(cli_source, classname)(None, command)
-        return meth.argument_list + meth.optional_arguments
+        return meth.global_arguments + meth.argument_list + meth.optional_arguments
 
     def process_options(self, command_options):
         """ Process the options for a single command """
@@ -202,7 +204,7 @@ class CliOptions(object):
             opt = option["opts"][0]
             if command in ("extract", "convert") and opt == "-o":
                 Images().pathoutput = optval
-            if optval == "False" or optval == "":
+            if optval in ("False", ""):
                 continue
             elif optval == "True":
                 yield (opt, )
@@ -215,7 +217,7 @@ class CliOptions(object):
                 yield opt
 
 
-class Config(object):
+class Config():
     """ Actions for loading and saving Faceswap GUI command configurations """
 
     def __init__(self, cli_opts, tk_vars):
diff --git a/lib/logger.py b/lib/logger.py
new file mode 100644
index 0000000..16e9b25
--- /dev/null
+++ b/lib/logger.py
@@ -0,0 +1,153 @@
+#!/usr/bin/python
+""" Logging Setup """
+import collections
+import logging
+from logging.handlers import QueueHandler, QueueListener, RotatingFileHandler
+import os
+import re
+import sys
+import traceback
+
+from datetime import datetime
+from time import sleep
+
+from lib.queue_manager import queue_manager
+from lib.sysinfo import sysinfo
+
+LOG_QUEUE = queue_manager._log_queue  # pylint: disable=protected-access
+
+
+class MultiProcessingLogger(logging.Logger):
+    """ Create custom logger  with custom levels """
+    def __init__(self, name):
+        for new_level in (('VERBOSE', 15), ('TRACE', 5)):
+            level_name, level_num = new_level
+            if hasattr(logging, level_name):
+                continue
+            logging.addLevelName(level_num, level_name)
+            setattr(logging, level_name, level_num)
+        super().__init__(name)
+
+    def verbose(self, msg, *args, **kwargs):
+        """
+        Log 'msg % args' with severity 'VERBOSE'.
+        """
+        if self.isEnabledFor(15):
+            self._log(15, msg, args, **kwargs)
+
+    def trace(self, msg, *args, **kwargs):
+        """
+        Log 'msg % args' with severity 'VERBOSE'.
+        """
+        if self.isEnabledFor(5):
+            self._log(5, msg, args, **kwargs)
+
+
+class FaceswapFormatter(logging.Formatter):
+    """ Override formatter to strip newlines and multiple spaces from logger """
+    def format(self, record):
+        record.msg = re.sub(" +", " ", record.msg.replace("\n", "\\n").replace("\r", "\\r"))
+        return super().format(record)
+
+
+class RollingBuffer(collections.deque):
+    """File-like that keeps a certain number of lines of text in memory."""
+    def write(self, buffer):
+        """ Write line to buffer """
+        for line in buffer.rstrip().splitlines():
+            self.append(line + "\n")
+
+
+def set_root_logger(loglevel=logging.INFO, queue=LOG_QUEUE):
+    """ Setup the root logger.
+        Loaded in main process and into any spawned processes
+        Automatically added in multithreading.py"""
+    rootlogger = logging.getLogger()
+    q_handler = QueueHandler(queue)
+    rootlogger.addHandler(q_handler)
+    rootlogger.setLevel(loglevel)
+
+
+def log_setup(loglevel):
+    """ initial log set up. """
+    numeric_loglevel = get_loglevel(loglevel)
+    root_loglevel = min(logging.DEBUG, numeric_loglevel)
+    set_root_logger(loglevel=root_loglevel)
+    log_format = FaceswapFormatter("%(asctime)s %(processName)-15s %(threadName)-15s "
+                                   "%(module)-15s %(funcName)-25s %(levelname)-8s %(message)s",
+                                   datefmt="%m/%d/%Y %H:%M:%S")
+    f_handler = file_handler(numeric_loglevel, log_format)
+    s_handler = stream_handler(numeric_loglevel)
+    c_handler = crash_handler(log_format)
+
+    q_listener = QueueListener(LOG_QUEUE, f_handler, s_handler, c_handler,
+                               respect_handler_level=True)
+    q_listener.start()
+    logging.info('Log level set to: %s', loglevel.upper())
+
+
+def file_handler(loglevel, log_format):
+    """ Add a logging rotating file handler """
+    filename = os.path.join(os.path.dirname(os.path.realpath(sys.argv[0])), "faceswap.log")
+    should_rotate = os.path.isfile(filename)
+    log_file = RotatingFileHandler(filename, backupCount=1)
+    if should_rotate:
+        log_file.doRollover()
+    log_file.setFormatter(log_format)
+    log_file.setLevel(loglevel)
+    return log_file
+
+
+def stream_handler(loglevel):
+    """ Add a logging cli handler """
+    # Don't set stdout to lower than verbose
+    loglevel = max(loglevel, 15)
+    log_format = FaceswapFormatter("%(asctime)s %(levelname)-8s %(message)s",
+                                   datefmt="%m/%d/%Y %H:%M:%S")
+
+    log_console = logging.StreamHandler(sys.stdout)
+    log_console.setFormatter(log_format)
+    log_console.setLevel(loglevel)
+    return log_console
+
+
+def crash_handler(log_format):
+    """ Add a handler that sores the last 50 debug lines to `debug_buffer`
+        for use in crash reports """
+    log_crash = logging.StreamHandler(debug_buffer)
+    log_crash.setFormatter(log_format)
+    log_crash.setLevel(logging.DEBUG)
+    return log_crash
+
+
+def get_loglevel(loglevel):
+    ''' Check valid log level supplied and return numeric log level '''
+    numeric_level = getattr(logging, loglevel.upper(), None)
+    if not isinstance(numeric_level, int):
+        raise ValueError('Invalid log level: %s' % loglevel)
+
+    return numeric_level
+
+
+def crash_log():
+    """ Write debug_buffer to a crash log on crash """
+    path = os.getcwd()
+    filename = os.path.join(path, datetime.now().strftime('crash_report.%Y.%m.%d.%H%M%S%f.log'))
+
+    # Wait until all log items have been processed
+    while not LOG_QUEUE.empty():
+        sleep(1)
+
+    freeze_log = list(debug_buffer)
+    with open(filename, "w") as outfile:
+        outfile.writelines(freeze_log)
+        traceback.print_exc(file=outfile)
+        outfile.write(sysinfo.full_info())
+    return filename
+
+
+# Set logger class to custom logger
+logging.setLoggerClass(MultiProcessingLogger)
+
+# Stores the last 50 debug messages
+debug_buffer = RollingBuffer(maxlen=50)  # pylint: disable=invalid-name
diff --git a/lib/multithreading.py b/lib/multithreading.py
index e32b23c..3f4ede1 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -1,81 +1,171 @@
 #!/usr/bin/env python3
 """ Multithreading/processing utils for faceswap """
 
+import logging
 import multiprocessing as mp
 import queue as Queue
+import sys
 import threading
+from lib.logger import LOG_QUEUE, set_root_logger
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+_launched_processes = set()  # pylint: disable=invalid-name
 
 
 class PoolProcess():
     """ Pool multiple processes """
-    def __init__(self, method, processes=None, verbose=False):
-        self.verbose = verbose
-        self.method = method
+    def __init__(self, method, in_queue, out_queue, *args, processes=None, **kwargs):
+        self._name = method.__qualname__
+        logger.debug("Initializing %s: (target: '%s', processes: %s)",
+                     self.__class__.__name__, self._name, processes)
+
         self.procs = self.set_procs(processes)
-        self.pool = None
+        ctx = mp.get_context("spawn")
+        self.pool = ctx.Pool(processes=self.procs)
+
+        self._method = method
+        self._kwargs = self.build_target_kwargs(in_queue, out_queue, kwargs)
+        self._args = args
+
+        logger.debug("Initialized %s: '%s'", self.__class__.__name__, self._name)
+
+    @staticmethod
+    def build_target_kwargs(in_queue, out_queue, kwargs):
+        """ Add standard kwargs to passed in kwargs list """
+        kwargs["log_init"] = set_root_logger
+        kwargs["log_queue"] = LOG_QUEUE
+        kwargs["in_queue"] = in_queue
+        kwargs["out_queue"] = out_queue
+        return kwargs
 
     def set_procs(self, processes):
         """ Set the number of processes to use """
         if processes is None:
             running_processes = len(mp.active_children())
             processes = max(mp.cpu_count() - running_processes, 1)
-        if self.verbose:
-            print("Processing in {} processes".format(processes))
+        logger.verbose("Processing '%s' in %s processes", self._name, processes)
         return processes
 
-    def in_process(self, *args, **kwargs):
+    def start(self):
         """ Run the processing pool """
-        self.pool = mp.Pool(processes=self.procs)
-        for _ in range(self.procs):
-            self.pool.apply_async(self.method, args=args, kwds=kwargs)
+        logging.debug("Pooling Processes: (target: '%s', args: %s, kwargs: %s)",
+                      self._name, self._args, self._kwargs)
+        for idx in range(self.procs):
+            logger.debug("Adding process %s of %s to mp.Pool '%s'",
+                         idx + 1, self.procs, self._name)
+            self.pool.apply_async(self._method, args=self._args, kwds=self._kwargs)
+        logging.debug("Pooled Processes: '%s'", self._name)
 
     def join(self):
         """ Join the process """
+        logger.debug("Joining Pooled Process: '%s'", self._name)
         self.pool.close()
         self.pool.join()
+        logger.debug("Joined Pooled Process: '%s'", self._name)
 
 
-class SpawnProcess():
+class SpawnProcess(mp.context.SpawnProcess):
     """ Process in spawnable context
         Must be spawnable to share CUDA across processes """
-    def __init__(self):
-        self.context = mp.get_context("spawn")
-        self.daemonize = True
-        self.process = None
-        self.event = self.context.Event()
-
-    def in_process(self, target, *args, **kwargs):
-        """ Start a process in the spawn context """
+    def __init__(self, target, in_queue, out_queue, *args, **kwargs):
+        name = target.__qualname__
+        logger.debug("Initializing %s: (target: '%s', args: %s, kwargs: %s)",
+                     self.__class__.__name__, name, args, kwargs)
+        ctx = mp.get_context("spawn")
+        self.event = ctx.Event()
+        kwargs = self.build_target_kwargs(in_queue, out_queue, kwargs)
+        super().__init__(target=target, name=name, args=args, kwargs=kwargs)
+        self.daemon = True
+        logger.debug("Initialized %s: '%s'", self.__class__.__name__, name)
+
+    def build_target_kwargs(self, in_queue, out_queue, kwargs):
+        """ Add standard kwargs to passed in kwargs list """
         kwargs["event"] = self.event
-        self.process = self.context.Process(target=target,
-                                            args=args,
-                                            kwargs=kwargs)
-        self.process.daemon = self.daemonize
-        self.process.start()
+        kwargs["log_init"] = set_root_logger
+        kwargs["log_queue"] = LOG_QUEUE
+        kwargs["in_queue"] = in_queue
+        kwargs["out_queue"] = out_queue
+        return kwargs
+
+    def start(self):
+        """ Add logging to start function """
+        logger.debug("Spawning Process: (name: '%s', args: %s, kwargs: %s, daemon: %s)",
+                     self._name, self._args, self._kwargs, self.daemon)
+        super().start()
+        _launched_processes.add(self)
+        logger.debug("Spawned Process: (name: '%s', PID: %s)", self._name, self.pid)
+
+    def join(self, timeout=None):
+        """ Add logging to join function """
+        logger.debug("Joining Process: (name: '%s', PID: %s)", self._name, self.pid)
+        super().join(timeout=timeout)
+        _launched_processes.remove(self)
+        logger.debug("Joined Process: (name: '%s', PID: %s)", self._name, self.pid)
+
+
+class FSThread(threading.Thread):
+    """ Subclass of thread that passes errors back to parent """
+    def __init__(self, group=None, target=None, name=None,  # pylint: disable=too-many-arguments
+                 args=(), kwargs=None, *, daemon=None):
+        super().__init__(group=group, target=target, name=name,
+                         args=args, kwargs=kwargs, daemon=daemon)
+        self.err = None
 
-    def join(self):
-        """ Join the process """
-        self.process.join()
+    def run(self):
+        try:
+            if self._target:
+                self._target(*self._args, **self._kwargs)
+        except Exception:  # pylint: disable=broad-except
+            self.err = sys.exc_info()
+        finally:
+            # Avoid a refcycle if the thread is running a function with
+            # an argument that has a member that points to the thread.
+            del self._target, self._args, self._kwargs
 
 
 class MultiThread():
-    """ Threading for IO heavy ops """
-    def __init__(self, thread_count=1):
-        self.thread_count = thread_count
-        self.threads = list()
-
-    def in_thread(self, target, *args, **kwargs):
+    """ Threading for IO heavy ops
+        Catches errors in thread and rethrows to parent """
+    def __init__(self, target, *args, thread_count=1, **kwargs):
+        self._name = target.__name__
+        logger.debug("Initializing %s: (target: '%s', thread_count: %s)",
+                     self.__class__.__name__, self._name, thread_count)
+        logger.trace("args: %s, kwargs: %s", args, kwargs)
+        self.daemon = True
+        self._thread_count = thread_count
+        self._threads = list()
+        self._target = target
+        self._args = args
+        self._kwargs = kwargs
+        logger.debug("Initialized %s: '%s'", self.__class__.__name__, self._name)
+
+    def start(self):
         """ Start a thread with the given method and args """
-        for _ in range(self.thread_count):
-            thread = threading.Thread(target=target, args=args, kwargs=kwargs)
-            thread.daemon = True
+        logger.debug("Starting thread(s): '%s'", self._name)
+        for idx in range(self._thread_count):
+            name = "{}_{}".format(self._name, idx)
+            logger.debug("Starting thread %s of %s: '%s'",
+                         idx + 1, self._thread_count, name)
+            thread = FSThread(name=name,
+                              target=self._target,
+                              args=self._args,
+                              kwargs=self._kwargs)
+            thread.daemon = self.daemon
             thread.start()
-            self.threads.append(thread)
+            self._threads.append(thread)
+        logger.debug("Started all threads '%s': %s", self._name, len(self._threads))
 
-    def join_threads(self):
-        """ Join the running threads """
-        for thread in self.threads:
+    def join(self):
+        """ Join the running threads, catching and re-raising any errors """
+        logger.debug("Joining Threads: '%s'", self._name)
+        for thread in self._threads:
+            logger.debug("Joining Thread: '%s'", thread._name)  # pylint: disable=protected-access
             thread.join()
+            if thread.err:
+                logger.error("Caught exception in thread: '%s'",
+                             thread._name)  # pylint: disable=protected-access
+                raise thread.err[1].with_traceback(thread.err[2])
+        logger.debug("Joined all Threads: '%s'", self._name)
 
 
 class BackgroundGenerator(threading.Thread):
@@ -105,3 +195,18 @@ class BackgroundGenerator(threading.Thread):
             if next_item is None:
                 break
             yield next_item
+
+
+def terminate_processes():
+    """ Join all active processes on unexpected shutdown
+
+        If the process is doing long running work, make sure you
+        have a mechanism in place to terminate this work to avoid
+        long blocks
+    """
+    logger.debug("Processes to join: %s", [process.name
+                                           for process in _launched_processes
+                                           if process.is_alive()])
+    for process in list(_launched_processes):
+        if process.is_alive():
+            process.join()
diff --git a/lib/queue_manager.py b/lib/queue_manager.py
index e34c45a..7971af1 100644
--- a/lib/queue_manager.py
+++ b/lib/queue_manager.py
@@ -4,48 +4,78 @@
     NB: Keep this in it's own module! If it gets loaded from
     a multiprocess on a Windows System it will break Faceswap"""
 
+import logging
 import multiprocessing as mp
 import threading
 
-from queue import Empty as QueueEmpty  # Used for imports
+from queue import Empty as QueueEmpty  # pylint: disable=unused-import; # noqa
 from time import sleep
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class QueueManager():
     """ Manage queues for availabilty across processes
         Don't import this class directly, instead
         import the variable: queue_manager """
     def __init__(self):
-        self.manager = mp.Manager()
+        logger.debug("Initializing %s", self.__class__.__name__)
+
+        # Hacky fix to stop multiprocessing spawning managers in child processes
+        if mp.current_process().name == "MainProcess":
+            # Use a Multiprocessing manager in main process
+            self.manager = mp.Manager()
+        else:
+            # Use a standard mp.queue in child process. NB: This will never be used
+            # but spawned processes will load this module, so we need to dummy in a queue
+            self.manager = mp
+        self.shutdown = self.manager.Event()
         self.queues = dict()
+        self._log_queue = self.manager.Queue()
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def add_queue(self, name, maxsize=0):
-        """ Add a queue to the manager """
+        """ Add a queue to the manager
+
+            Adds an event "shutdown" to the queue that can be used to indicate
+            to a process that any activity on the queue should cease """
+
+        logger.debug("QueueManager adding: (name: '%s', maxsize: %s)", name, maxsize)
         if name in self.queues.keys():
             raise ValueError("Queue '{}' already exists.".format(name))
         queue = self.manager.Queue(maxsize=maxsize)
+        setattr(queue, "shutdown", self.shutdown)
         self.queues[name] = queue
+        logger.debug("QueueManager added: (name: '%s')", name)
 
     def del_queue(self, name):
         """ remove a queue from the manager """
+        logger.debug("QueueManager deleting: '%s'", name)
         del self.queues[name]
+        logger.debug("QueueManager deleted: '%s'", name)
 
     def get_queue(self, name, maxsize=0):
         """ Return a queue from the manager
             If it doesn't exist, create it """
+        logger.debug("QueueManager getting: '%s'", name)
         queue = self.queues.get(name, None)
-        if queue:
-            return queue
-        self.add_queue(name, maxsize)
-        return self.queues[name]
+        if not queue:
+            self.add_queue(name, maxsize)
+            queue = self.queues[name]
+        logger.debug("QueueManager got: '%s'", name)
+        return queue
 
     def terminate_queues(self):
-        """ Clear all queues and send EOF
+        """ Set shutdown event, clear and send EOF to all queues
             To be called if there is an error """
-        for queue in self.queues.values():
+        logger.debug("QueueManager terminating all queues")
+        self.shutdown.set()
+        for q_name, queue in self.queues.items():
+            logger.debug("QueueManager terminating: '%s'", q_name)
             while not queue.empty():
-                queue.get()
+                queue.get(True, 1)
             queue.put("EOF")
+        logger.debug("QueueManager terminated all queues")
 
     def debug_monitor(self, update_secs=2):
         """ Debug tool for monitoring queues """
@@ -55,13 +85,13 @@ class QueueManager():
         thread.start()
 
     def debug_queue_sizes(self, update_secs):
-        """ Output the queue sizes """
+        """ Output the queue sizes
+            logged to INFO so it also displays in console
+        """
         while True:
-            print("=== QUEUE SIZES ===")
             for name in sorted(self.queues.keys()):
-                print(name, self.queues[name].qsize())
-            print("====================\n")
+                logger.info("%s: %s", name, self.queues[name].qsize())
             sleep(update_secs)
 
 
-queue_manager = QueueManager()
+queue_manager = QueueManager()  # pylint: disable=invalid-name
diff --git a/lib/sysinfo.py b/lib/sysinfo.py
new file mode 100644
index 0000000..7525536
--- /dev/null
+++ b/lib/sysinfo.py
@@ -0,0 +1,276 @@
+#!/usr/bin python3
+""" Obtain information about the running system, environment and gpu """
+
+import os
+import platform
+import sys
+from subprocess import PIPE, Popen
+
+import psutil
+
+from lib.gpu_stats import GPUStats
+
+
+class SysInfo():
+    """ System and Python Information """
+    # pylint: disable=too-many-instance-attributes,too-many-public-methods
+
+    def __init__(self):
+        gpu_stats = GPUStats(log=False)
+
+        self.platform = platform.platform()
+        self.system = platform.system()
+        self.machine = platform.machine()
+        self.release = platform.release()
+        self.processor = platform.processor()
+        self.cpu_count = os.cpu_count()
+        self.py_implementation = platform.python_implementation()
+        self.py_version = platform.python_version()
+        self._cuda_path = self.get_cuda_path()
+        self.vram = gpu_stats.vram
+        self.gfx_driver = gpu_stats.driver
+        self.gfx_devices = gpu_stats.devices
+
+    @property
+    def is_conda(self):
+        """ Boolean for whether in a conda environment """
+        return "conda" in sys.version.lower()
+
+    @property
+    def is_linux(self):
+        """ Boolean for whether in a conda environment """
+        return self.system.lower() == "linux"
+
+    @property
+    def is_macos(self):
+        """ Boolean for whether in a conda environment """
+        return self.system.lower() == "darwin"
+
+    @property
+    def is_windows(self):
+        """ Boolean for whether in a conda environment """
+        return self.system.lower() == "windows"
+
+    @property
+    def ram(self):
+        """ Return RAM stats """
+        return psutil.virtual_memory()
+
+    @property
+    def ram_free(self):
+        """ return free RAM """
+        return getattr(self.ram, "free")
+
+    @property
+    def ram_total(self):
+        """ return total RAM """
+        return getattr(self.ram, "total")
+
+    @property
+    def ram_available(self):
+        """ return available RAM """
+        return getattr(self.ram, "available")
+
+    @property
+    def ram_used(self):
+        """ return used RAM """
+        return getattr(self.ram, "used")
+
+    @property
+    def fs_command(self):
+        """ Return the executed faceswap command """
+        return " ".join(sys.argv)
+
+    @property
+    def installed_pip(self):
+        """ Installed pip packages """
+        pip = Popen("{} -m pip freeze".format(sys.executable),
+                    shell=True, stdout=PIPE)
+        installed = pip.communicate()[0].decode().splitlines()
+        return "\n".join(installed)
+
+    @property
+    def installed_conda(self):
+        """ Installed Conda packages """
+        if not self.is_conda:
+            return None
+        conda = Popen("conda list", shell=True, stdout=PIPE, stderr=PIPE)
+        stdout, stderr = conda.communicate()
+        if stderr:
+            return "Could not get package list"
+        installed = stdout.decode().splitlines()
+        return "\n".join(installed)
+
+    @property
+    def conda_version(self):
+        """ Get conda version """
+        if not self.is_conda:
+            return "N/A"
+        conda = Popen("conda --version", shell=True, stdout=PIPE, stderr=PIPE)
+        stdout, stderr = conda.communicate()
+        if stderr:
+            return "Conda is used, but version not found"
+        version = stdout.decode().splitlines()
+        return "\n".join(version)
+
+    @property
+    def git_branch(self):
+        """ Get the current git branch """
+        git = Popen("git status", shell=True, stdout=PIPE, stderr=PIPE)
+        stdout, stderr = git.communicate()
+        if stderr:
+            return "Not Found"
+        branch = stdout.decode().splitlines()[0].replace("On branch ", "")
+        return branch
+
+    @property
+    def git_commits(self):
+        """ Get last 5 git commits """
+        git = Popen("git log --pretty=oneline --abbrev-commit -n 5",
+                    shell=True, stdout=PIPE, stderr=PIPE)
+        stdout, stderr = git.communicate()
+        if stderr:
+            return "Not Found"
+        commits = stdout.decode().splitlines()
+        return ". ".join(commits)
+
+    @property
+    def cuda_version(self):
+        """ Get the installed CUDA version """
+        if self.is_linux:
+            version = self.cuda_version_linux()
+        elif self.is_windows:
+            version = self.cuda_version_windows()
+        else:
+            version = "Unsupported OS"
+        return version
+
+    @property
+    def cudnn_version(self):
+        """ Get the installed cuDNN version """
+        if not self._cuda_path:
+            return "Not Found"
+        cudnn_checkfile = os.path.join(self._cuda_path, "include", "cudnn.h")
+        if not os.path.isfile(cudnn_checkfile):
+            return "Not Found"
+        found = 0
+        with open(cudnn_checkfile, "r") as ofile:
+            for line in ofile:
+                if line.lower().startswith("#define cudnn_major"):
+                    major = line[line.rfind(" ") + 1:].strip()
+                    found += 1
+                elif line.lower().startswith("#define cudnn_minor"):
+                    minor = line[line.rfind(" ") + 1:].strip()
+                    found += 1
+                elif line.lower().startswith("#define cudnn_patchlevel"):
+                    patchlevel = line[line.rfind(" ") + 1:].strip()
+                    found += 1
+                if found == 3:
+                    break
+        if found != 3:
+            return "Not Found"
+        return "{}.{}.{}".format(major, minor, patchlevel)
+
+    def get_cuda_path(self):
+        """ Return the correct CUDA Path """
+        if self.is_linux:
+            path = self.cuda_path_linux()
+        elif self.is_windows:
+            path = self.cuda_path_windows()
+        else:
+            path = None
+        return path
+
+    @staticmethod
+    def cuda_path_linux():
+        """ Get the path to Cuda on linux systems """
+        ld_library_path = os.environ.get("LD_LIBRARY_PATH", None)
+        chk = os.popen("ldconfig -p | grep -P \"libcudart.so.\\d+.\\d+\" | head -n 1").read()
+        if ld_library_path and not chk:
+            paths = ld_library_path.split(":")
+            for path in paths:
+                chk = os.popen("ls {} | grep -P -o \"libcudart.so.\\d+.\\d+\" | "
+                               "head -n 1".format(path)).read()
+                if chk:
+                    break
+        if not chk:
+            return None
+        return chk[chk.find("=>") + 3:chk.find("targets") - 1]
+
+    @staticmethod
+    def cuda_path_windows():
+        """ Get the path to Cuda on Windows systems """
+        cuda_path = os.environ.get("CUDA_PATH", None)
+        return cuda_path
+
+    @staticmethod
+    def cuda_version_linux():
+        """ Get CUDA version for linux systems """
+        ld_library_path = os.environ.get("LD_LIBRARY_PATH", None)
+        chk = os.popen("ldconfig -p | grep -P \"libcudart.so.\\d+.\\d+\" | head -n 1").read()
+        if ld_library_path and not chk:
+            paths = ld_library_path.split(":")
+            for path in paths:
+                chk = os.popen("ls {} | grep -P -o \"libcudart.so.\\d+.\\d+\" | "
+                               "head -n 1".format(path)).read()
+                if chk:
+                    break
+        if not chk:
+            return "Not Found"
+        cudavers = chk.strip().replace("libcudart.so.", "")
+        return cudavers[:cudavers.find(" ")]
+
+    @staticmethod
+    def cuda_version_windows():
+        """ Get CUDA version for Windows systems """
+        cuda_keys = [key
+                     for key in os.environ.keys()
+                     if key.lower().startswith("cuda_path_v")]
+        if not cuda_keys:
+            return "Not Found"
+        cudavers = [key.replace("CUDA_PATH_V", "").replace("_", ".") for key in cuda_keys]
+        return " ".join(cudavers)
+
+    def full_info(self):
+        """ Format system info human readable """
+        retval = "\n============ System Information ============\n"
+        sys_info = {"os_platform": self.platform,
+                    "os_machine": self.machine,
+                    "os_release": self.release,
+                    "py_conda_version": self.conda_version,
+                    "py_implementation": self.py_implementation,
+                    "py_version": self.py_version,
+                    "py_command": self.fs_command,
+                    "sys_cores": self.cpu_count,
+                    "sys_processor": self.processor,
+                    "sys_ram": self.format_ram(),
+                    "git_branch": self.git_branch,
+                    "git_commits": self.git_commits,
+                    "gpu_cuda": self.cuda_version,
+                    "gpu_cudnn": self.cudnn_version,
+                    "gpu_driver": self.gfx_driver,
+                    "gpu_devices": ", ".join(["GPU_{}: {}".format(idx, device)
+                                              for idx, device in enumerate(self.gfx_devices)]),
+                    "gpu_vram": ", ".join(["GPU_{}: {}MB".format(idx, int(vram))
+                                           for idx, vram in enumerate(self.vram)])}
+        for key in sorted(sys_info.keys()):
+            retval += ("{0: <18} {1}\n".format(key + ":", sys_info[key]))
+        retval += "\n=============== Pip Packages ===============\n"
+        retval += self.installed_pip
+        if not self.is_conda:
+            return retval
+        retval += "\n\n============== Conda Packages ==============\n"
+        retval += self.installed_conda
+        return retval
+
+    def format_ram(self):
+        """ Format the RAM stats for human output """
+        retval = list()
+        for name in ("total", "available", "used", "free"):
+            value = getattr(self, "ram_{}".format(name))
+            value = int(value / (1024 * 1024))
+            retval.append("{}: {}MB".format(name.capitalize(), value))
+        return ", ".join(retval)
+
+
+sysinfo = SysInfo()  # pylint: disable=invalid-name
diff --git a/lib/utils.py b/lib/utils.py
index 106da5e..81ff5d0 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -1,6 +1,7 @@
 #!/usr/bin python3
 """ Utilities available across all scripts """
 
+import logging
 import os
 import warnings
 
@@ -12,56 +13,66 @@ import cv2
 import numpy as np
 
 import dlib
+
 from lib.faces_detect import DetectedFace
 from lib.training_data import TrainingDataGenerator
 
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 # Global variables
-_image_extensions = ['.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff']
-_video_extensions = ['.avi', '.flv', '.mkv', '.mov', '.mp4', '.mpeg', '.webm']
+_image_extensions = [  # pylint: disable=invalid-name
+    ".bmp", ".jpeg", ".jpg", ".png", ".tif", ".tiff"]
+_video_extensions = [  # pylint: disable=invalid-name
+    ".avi", ".flv", ".mkv", ".mov", ".mp4", ".mpeg", ".webm"]
 
 
 def get_folder(path):
     """ Return a path to a folder, creating it if it doesn't exist """
+    logger.debug("Requested path: '%s'", path)
     output_dir = Path(path)
     output_dir.mkdir(parents=True, exist_ok=True)
+    logger.debug("Returning: '%s'", output_dir)
     return output_dir
 
 
-def get_image_paths(directory, exclude=list(), debug=False):
+def get_image_paths(directory):
     """ Return a list of images that reside in a folder """
     image_extensions = _image_extensions
-    exclude_names = [os.path.basename(Path(x).stem[:Path(x).stem.rfind('_')] +
-                                      Path(x).suffix) for x in exclude]
     dir_contents = list()
 
     if not os.path.exists(directory):
+        logger.debug("Creating folder: '%s'", directory)
         directory = get_folder(directory)
 
     dir_scanned = sorted(os.scandir(directory), key=lambda x: x.name)
+    logger.debug("Scanned Folder contains %s files", len(dir_scanned))
+    logger.trace("Scanned Folder Contents: %s", dir_scanned)
+
     for chkfile in dir_scanned:
         if any([chkfile.name.lower().endswith(ext)
                 for ext in image_extensions]):
-            if chkfile.name in exclude_names:
-                if debug:
-                    print("Already processed %s" % chkfile.name)
-                continue
-            else:
-                dir_contents.append(chkfile.path)
+            logger.trace("Adding '%s' to image list", chkfile.path)
+            dir_contents.append(chkfile.path)
 
+    logger.debug("Returning %s images", len(dir_contents))
     return dir_contents
 
 
 def backup_file(directory, filename):
     """ Backup a given file by appending .bk to the end """
+    logger.trace("Backing up: '%s'", filename)
     origfile = os.path.join(directory, filename)
     backupfile = origfile + '.bk'
     if os.path.exists(backupfile):
+        logger.trace("Removing existing file: '%s'", backup_file)
         os.remove(backupfile)
     if os.path.exists(origfile):
+        logger.trace("Renaming: '%s' to '%s'", origfile, backup_file)
         os.rename(origfile, backupfile)
 
 
-def set_system_verbosity(loglevel):
+def set_system_verbosity():
     """ Set the verbosity level of tensorflow and suppresses
         future and deprecation warnings from any modules
         From:
@@ -72,6 +83,8 @@ def set_system_verbosity(loglevel):
         2 - filter out WARNING logs
         3 - filter out ERROR logs  """
 
+    loglevel = "2" if logger.getEffectiveLevel() > 15 else "0"
+    logger.debug("System Verbosity level: %s", loglevel)
     os.environ['TF_CPP_MIN_LOG_LEVEL'] = loglevel
     if loglevel != '0':
         for warncat in (FutureWarning, DeprecationWarning):
@@ -84,6 +97,7 @@ def add_alpha_channel(image, intensity=100):
         intensity: The opacity of the alpha channel between 0 and 100
                    100 = transparent,
                    0 = solid  """
+    logger.trace("Adding alpha channel: intensity: %s", intensity)
     assert 0 <= intensity <= 100, "Invalid intensity supplied"
     intensity = (255.0 / 100.0) * intensity
 
@@ -95,37 +109,16 @@ def add_alpha_channel(image, intensity=100):
 
     image_bgra = cv2.merge(  # pylint: disable=no-member
         (ch_b, ch_g, ch_r, ch_a))
+    logger.trace("Added alpha channel", intensity)
     return image_bgra.astype(d_type)
 
 
-def rotate_image_by_angle(image, angle,
-                          rotated_width=None, rotated_height=None):
-    """ Rotate an image by a given angle.
-        From: https://stackoverflow.com/questions/22041699 """
-
-    height, width = image.shape[:2]
-    image_center = (width/2, height/2)
-    rotation_matrix = cv2.getRotationMatrix2D(  # pylint: disable=no-member
-        image_center, -1.*angle, 1.)
-    if rotated_width is None or rotated_height is None:
-        abs_cos = abs(rotation_matrix[0, 0])
-        abs_sin = abs(rotation_matrix[0, 1])
-        if rotated_width is None:
-            rotated_width = int(height*abs_sin + width*abs_cos)
-        if rotated_height is None:
-            rotated_height = int(height*abs_cos + width*abs_sin)
-    rotation_matrix[0, 2] += rotated_width/2 - image_center[0]
-    rotation_matrix[1, 2] += rotated_height/2 - image_center[1]
-    return (cv2.warpAffine(image,  # pylint: disable=no-member
-                           rotation_matrix,
-                           (rotated_width, rotated_height)),
-            rotation_matrix)
-
-
 def rotate_landmarks(face, rotation_matrix):
     """ Rotate the landmarks and bounding box for faces
         found in rotated images.
         Pass in a DetectedFace object, Alignments dict or DLib rectangle"""
+    logger.trace("Rotating landmarks: (rotation_matrix: %s, type(face): %s",
+                 rotation_matrix, type(face))
     if isinstance(face, DetectedFace):
         bounding_box = [[face.x, face.y],
                         [face.x + face.w, face.y],
@@ -153,6 +146,8 @@ def rotate_landmarks(face, rotation_matrix):
     else:
         raise ValueError("Unsupported face type")
 
+    logger.trace("Original landmarks: %s", landmarks)
+
     rotation_matrix = cv2.invertAffineTransform(  # pylint: disable=no-member
         rotation_matrix)
     rotated = list()
@@ -179,7 +174,8 @@ def rotate_landmarks(face, rotation_matrix):
         face.h = int(pt_y1 - pt_y)
         face.r = 0
         if len(rotated) > 1:
-            face.landmarksXY = [tuple(point) for point in rotated[1].tolist()]
+            rotated_landmarks = [tuple(point) for point in rotated[1].tolist()]
+            face.landmarksXY = rotated_landmarks
     elif isinstance(face, dict):
         face["x"] = int(pt_x)
         face["y"] = int(pt_y)
@@ -187,12 +183,14 @@ def rotate_landmarks(face, rotation_matrix):
         face["h"] = int(pt_y1 - pt_y)
         face["r"] = 0
         if len(rotated) > 1:
-            face["landmarksXY"] = [tuple(point)
-                                   for point in rotated[1].tolist()]
+            rotated_landmarks = [tuple(point) for point in rotated[1].tolist()]
+            face["landmarksXY"] = rotated_landmarks
     else:
-        face = dlib.rectangle(  # pylint: disable=c-extension-no-member
+        rotated_landmarks = dlib.rectangle(  # pylint: disable=c-extension-no-member
             int(pt_x), int(pt_y), int(pt_x1), int(pt_y1))
+        face = rotated_landmarks
 
+    logger.trace("Rotated landmarks: %s", rotated_landmarks)
     return face
 
 
@@ -229,7 +227,7 @@ class Timelapse:
         self.trainer = trainer
 
         if not os.path.isdir(self.output_dir):
-            print('Error: {} does not exist'.format(self.output_dir))
+            logger.error("'%s' does not exist", self.output_dir)
             exit(1)
 
         self.files_a = self.read_input_images(input_dir_a)
@@ -244,11 +242,11 @@ class Timelapse:
     def read_input_images(input_dir):
         """ Get the image paths """
         if not os.path.isdir(input_dir):
-            print('Error: {} does not exist'.format(input_dir))
+            logger.error("'%s' does not exist", input_dir)
             exit(1)
 
         if not os.listdir(input_dir):
-            print('Error: {} contains no images'.format(input_dir))
+            logger.error("'%s' contains no images", input_dir)
             exit(1)
 
         return get_image_paths(input_dir)
@@ -277,3 +275,17 @@ class Timelapse:
         image = self.trainer.show_sample(self.images_a, self.images_b)
         cv2.imwrite(os.path.join(self.output_dir,  # pylint: disable=no-member
                                  str(int(time())) + ".png"), image)
+
+
+def safe_shutdown():
+    """ Close queues, threads and processes in event of crash """
+    logger.debug("Safely shutting down")
+    from lib.queue_manager import queue_manager
+    from lib.multithreading import terminate_processes
+    queue_manager.terminate_queues()
+    terminate_processes()
+    logger.debug("Cleanup complete. Shutting down queue manager and exiting")
+    queue_manager._log_queue.put(None)  # pylint: disable=protected-access
+    while not queue_manager._log_queue.empty():  # pylint: disable=protected-access
+        continue
+    queue_manager.manager.shutdown()
diff --git a/plugins/convert/Convert_Masked.py b/plugins/convert/Convert_Masked.py
index f03a195..a407f51 100644
--- a/plugins/convert/Convert_Masked.py
+++ b/plugins/convert/Convert_Masked.py
@@ -1,45 +1,65 @@
-# Based on: https://gist.github.com/anonymous/d3815aba83a8f79779451262599b0955 found on https://www.reddit.com/r/deepfakes/
+#!/usr/bin/env python3
+""" Masked converter for faceswap.py
+    Based on: https://gist.github.com/anonymous/d3815aba83a8f79779451262599b0955
+    found on https://www.reddit.com/r/deepfakes/ """
 
+import logging
 import cv2
 import numpy
 
 from lib.aligner import get_align_mat
 from lib.utils import add_alpha_channel
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
 class Convert():
-    def __init__(self, encoder, trainer, blur_size=2, seamless_clone=False, mask_type="facehullandrect", erosion_kernel_size=None, match_histogram=False, sharpen_image=None, draw_transparent=False, **kwargs):
+    def __init__(self, encoder, trainer,
+                 blur_size=2, seamless_clone=False, mask_type="facehullandrect",
+                 erosion_kernel_size=None, match_histogram=False, sharpen_image=None,
+                 draw_transparent=False, **kwargs):
         self.encoder = encoder
         self.trainer = trainer
         self.erosion_kernel = None
         self.erosion_kernel_size = erosion_kernel_size
         if erosion_kernel_size is not None:
             if erosion_kernel_size > 0:
-                self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(erosion_kernel_size,erosion_kernel_size))
+                self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,
+                                                                (erosion_kernel_size,
+                                                                 erosion_kernel_size))
             elif erosion_kernel_size < 0:
-                self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(abs(erosion_kernel_size),abs(erosion_kernel_size)))
+                self.erosion_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,
+                                                                (abs(erosion_kernel_size),
+                                                                 abs(erosion_kernel_size)))
         self.blur_size = blur_size
         self.seamless_clone = seamless_clone
         self.sharpen_image = sharpen_image
         self.match_histogram = match_histogram
-        self.mask_type = mask_type.lower() # Choose in 'FaceHullAndRect','FaceHull','Rect'
+        self.mask_type = mask_type.lower()  # Choose in 'FaceHullAndRect', 'FaceHull', 'Rect'
         self.draw_transparent = draw_transparent
 
-    def patch_image( self, image, face_detected, size ):
+    def patch_image(self, image, face_detected, size):
 
         image_size = image.shape[1], image.shape[0]
 
-        mat = numpy.array(get_align_mat(face_detected, size, should_align_eyes=False)).reshape(2,3)
+        mat = numpy.array(get_align_mat(face_detected,
+                                        size,
+                                        should_align_eyes=False)).reshape(2, 3)
 
         if "GAN" not in self.trainer:
             mat = mat * size
         else:
             padding = int(48/256*size)
             mat = mat * (size - 2 * padding)
-            mat[:,2] += padding
+            mat[:, 2] += padding
 
-        new_face = self.get_new_face(image,mat,size)
+        new_face = self.get_new_face(image, mat, size)
 
-        image_mask = self.get_image_mask( image, new_face, face_detected.landmarks_as_xy(), mat, image_size )
+        image_mask = self.get_image_mask(image,
+                                         new_face,
+                                         face_detected.landmarks_as_xy,
+                                         mat,
+                                         image_size)
 
         return self.apply_new_face(image, new_face, image_mask, mat, image_size, size)
 
@@ -61,10 +81,15 @@ class Convert():
                                                                    image_mask,
                                                                    image_size)
             self.seamless_clone = False  # Alpha channel not supported in seamless
-        base_image = numpy.copy( image )
-        new_image = numpy.copy( image )
+        base_image = numpy.copy(image)
+        new_image = numpy.copy(image)
 
-        cv2.warpAffine( new_face, mat, image_size, new_image, cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC, cv2.BORDER_TRANSPARENT )
+        cv2.warpAffine(new_face,
+                       mat,
+                       image_size,
+                       new_image,
+                       cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC,
+                       cv2.BORDER_TRANSPARENT)
 
         if self.sharpen_image == "bsharpen":
             # Sharpening using filter2D
@@ -79,21 +104,25 @@ class Convert():
 
         outimage = None
         if self.seamless_clone:
-            unitMask = numpy.clip( image_mask * 365, 0, 255 ).astype(numpy.uint8)
-            print(unitMask.shape)
-            print(new_image.shape)
-            print(base_image.shape)
-            maxregion = numpy.argwhere(unitMask==255)
+            unitMask = numpy.clip(image_mask * 365, 0, 255).astype(numpy.uint8)
+            logger.info(unitMask.shape)
+            logger.info(new_image.shape)
+            logger.info(base_image.shape)
+            maxregion = numpy.argwhere(unitMask == 255)
 
             if maxregion.size > 0:
-              miny,minx = maxregion.min(axis=0)[:2]
-              maxy,maxx = maxregion.max(axis=0)[:2]
-              lenx = maxx - minx;
-              leny = maxy - miny;
-              masky = int(minx+(lenx//2))
-              maskx = int(miny+(leny//2))
-              outimage = cv2.seamlessClone(new_image.astype(numpy.uint8),base_image.astype(numpy.uint8),unitMask,(masky,maskx) , cv2.NORMAL_CLONE )
-              return outimage
+                miny, minx = maxregion.min(axis=0)[:2]
+                maxy, maxx = maxregion.max(axis=0)[:2]
+                lenx = maxx - minx
+                leny = maxy - miny
+                masky = int(minx + (lenx // 2))
+                maskx = int(miny + (leny // 2))
+                outimage = cv2.seamlessClone(new_image.astype(numpy.uint8),
+                                             base_image.astype(numpy.uint8),
+                                             unitMask,
+                                             (masky, maskx),
+                                             cv2.NORMAL_CLONE)
+                return outimage
 
         foreground = cv2.multiply(image_mask, new_image.astype(float))
         background = cv2.multiply(1.0 - image_mask, base_image.astype(float))
@@ -117,10 +146,10 @@ class Convert():
         masked_source = masked_source.ravel()
         masked_template = masked_template.ravel()
         s_values, bin_idx, s_counts = numpy.unique(source, return_inverse=True,
-                                                return_counts=True)
+                                                   return_counts=True)
         t_values, t_counts = numpy.unique(template, return_counts=True)
         ms_values, mbin_idx, ms_counts = numpy.unique(source, return_inverse=True,
-                                                return_counts=True)
+                                                      return_counts=True)
         mt_values, mt_counts = numpy.unique(template, return_counts=True)
 
         s_quantiles = numpy.cumsum(s_counts).astype(numpy.float64)
@@ -132,32 +161,32 @@ class Convert():
         return interp_t_values[bin_idx].reshape(oldshape)
 
     def color_hist_match(self, src_im, tar_im, mask):
-        matched_R = self.hist_match(src_im[:,:,0], tar_im[:,:,0], mask)
-        matched_G = self.hist_match(src_im[:,:,1], tar_im[:,:,1], mask)
-        matched_B = self.hist_match(src_im[:,:,2], tar_im[:,:,2], mask)
+        matched_R = self.hist_match(src_im[:, :, 0], tar_im[:, :, 0], mask)
+        matched_G = self.hist_match(src_im[:, :, 1], tar_im[:, :, 1], mask)
+        matched_B = self.hist_match(src_im[:, :, 2], tar_im[:, :, 2], mask)
         matched = numpy.stack((matched_R, matched_G, matched_B), axis=2).astype(src_im.dtype)
         return matched
 
     def get_new_face(self, image, mat, size):
-        face = cv2.warpAffine( image, mat, (size,size) )
-        face = numpy.expand_dims( face, 0 )
-        face_clipped = numpy.clip(face[0], 0, 255).astype( image.dtype )
+        face = cv2.warpAffine(image, mat, (size, size))
+        face = numpy.expand_dims(face, 0)
+        face_clipped = numpy.clip(face[0], 0, 255).astype(image.dtype)
         new_face = None
         mask = None
 
         if "GAN" not in self.trainer:
             normalized_face = face / 255.0
             new_face = self.encoder(normalized_face)[0]
-            new_face = numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )
+            new_face = numpy.clip(new_face * 255, 0, 255).astype(image.dtype)
         else:
             normalized_face = face / 255.0 * 2 - 1
             fake_output = self.encoder(normalized_face)
-            if "128" in self.trainer: # TODO: Another hack to switch between 64 and 128
+            if "128" in self.trainer:  # TODO: Another hack to switch between 64 and 128
                 fake_output = fake_output[0]
-            mask = fake_output[:,:,:, :1]
-            new_face = fake_output[:,:,:, 1:]
+            mask = fake_output[:, :, :, :1]
+            new_face = fake_output[:, :, :, 1:]
             new_face = mask * new_face + (1 - mask) * normalized_face
-            new_face = numpy.clip((new_face[0] + 1) * 255 / 2, 0, 255).astype( image.dtype )
+            new_face = numpy.clip((new_face[0] + 1) * 255 / 2, 0, 255).astype(image.dtype)
 
         if self.match_histogram:
             new_face = self.color_hist_match(new_face, face_clipped, mask)
@@ -166,15 +195,20 @@ class Convert():
 
     def get_image_mask(self, image, new_face, landmarks, mat, image_size):
 
-        face_mask = numpy.zeros(image.shape,dtype=float)
+        face_mask = numpy.zeros(image.shape, dtype=float)
         if 'rect' in self.mask_type:
-            face_src = numpy.ones(new_face.shape,dtype=float)
-            cv2.warpAffine( face_src, mat, image_size, face_mask, cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC, cv2.BORDER_TRANSPARENT )
-
-        hull_mask = numpy.zeros(image.shape,dtype=float)
+            face_src = numpy.ones(new_face.shape, dtype=float)
+            cv2.warpAffine(face_src,
+                           mat,
+                           image_size,
+                           face_mask,
+                           cv2.WARP_INVERSE_MAP | cv2.INTER_CUBIC, cv2.BORDER_TRANSPARENT)
+
+        hull_mask = numpy.zeros(image.shape, dtype=float)
         if 'hull' in self.mask_type:
-            hull = cv2.convexHull( numpy.array( landmarks ).reshape((-1,2)).astype(int) ).flatten().reshape( (-1,2) )
-            cv2.fillConvexPoly( hull_mask,hull,(1,1,1) )
+            hull = cv2.convexHull(
+                numpy.array(landmarks).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
+            cv2.fillConvexPoly(hull_mask, hull, (1, 1, 1))
 
         if self.mask_type == 'rect':
             image_mask = face_mask
@@ -183,15 +217,14 @@ class Convert():
         else:
             image_mask = ((face_mask*hull_mask))
 
-
         if self.erosion_kernel is not None:
             if self.erosion_kernel_size > 0:
-                image_mask = cv2.erode(image_mask,self.erosion_kernel,iterations = 1)
+                image_mask = cv2.erode(image_mask, self.erosion_kernel, iterations=1)
             elif self.erosion_kernel_size < 0:
                 dilation_kernel = abs(self.erosion_kernel)
-                image_mask = cv2.dilate(image_mask,dilation_kernel,iterations = 1)
+                image_mask = cv2.dilate(image_mask, dilation_kernel, iterations=1)
 
-        if self.blur_size!=0:
-            image_mask = cv2.blur(image_mask,(self.blur_size,self.blur_size))
+        if self.blur_size != 0:
+            image_mask = cv2.blur(image_mask, (self.blur_size, self.blur_size))
 
         return image_mask
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 22bd400..a5c0aae 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -17,22 +17,29 @@
      "landmarks": <list of landmarks>}
     """
 
+import logging
 import os
+import traceback
+
+from io import StringIO
 
 from lib.aligner import Extract
 from lib.gpu_stats import GPUStats
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Aligner():
     """ Landmarks Aligner Object """
-    def __init__(self, verbose=False):
-        self.verbose = verbose
+    def __init__(self, loglevel):
+        logger.debug("Initializing %s", self.__class__.__name__)
+        self.loglevel = loglevel
         self.cachepath = os.path.join(os.path.dirname(__file__), ".cache")
         self.extract = Extract()
         self.init = None
 
         # The input and output queues for the plugin.
-        # See lib.multithreading.QueueManager for getting queues
+        # See lib.queue_manager.QueueManager for getting queues
         self.queues = {"in": None, "out": None}
 
         #  Path to model if required
@@ -42,6 +49,7 @@ class Aligner():
         # how many parallel processes / batches can be run.
         # Be conservative to avoid OOM.
         self.vram = None
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     # <<< OVERRIDE METHODS >>> #
     # These methods must be overriden when creating a plugin
@@ -55,6 +63,11 @@ class Aligner():
         """ Inititalize the aligner
             Tasks to be run before any alignments are performed.
             Override for specific detector """
+        logger_init = kwargs["log_init"]
+        log_queue = kwargs["log_queue"]
+        logger_init(self.loglevel, log_queue)
+        logger.debug("_base initialize %s: (PID: %s, args: %s, kwargs: %s)",
+                     self.__class__.__name__, os.getpid(), args, kwargs)
         self.init = kwargs["event"]
         self.queues["in"] = kwargs["in_queue"]
         self.queues["out"] = kwargs["out_queue"]
@@ -67,7 +80,24 @@ class Aligner():
             if not self.init:
                 self.initialize(*args, **kwargs)
         except ValueError as err:
-            print("ERROR: {}".format(err))
+            logger.error(err)
+            exit(1)
+        logger.debug("Launching Align: (args: %s kwargs: %s)", args, kwargs)
+
+    # <<< DETECTION WRAPPER >>> #
+    def run(self, *args, **kwargs):
+        """ Parent align process.
+            This should always be called as the entry point so exceptions
+            are passed back to parent.
+            Do not override """
+        try:
+            self.align(*args, **kwargs)
+        except Exception:  # pylint: disable=broad-except
+            logger.error("Caught exception in child process: %s", os.getpid())
+            tb_buffer = StringIO()
+            traceback.print_exc(file=tb_buffer)
+            exception = {"exception": (os.getpid(), tb_buffer)}
+            self.queues["out"].put(exception)
             exit(1)
 
     # <<< FINALIZE METHODS>>> #
@@ -75,18 +105,40 @@ class Aligner():
         """ This should be called as the final task of each plugin
             aligns faces and puts to the out queue """
         if output == "EOF":
+            logger.trace("Item out: %s", output)
             self.queues["out"].put("EOF")
             return
+        logger.trace("Item out: %s", {key: val
+                                      for key, val in output.items()
+                                      if key != "image"})
         self.queues["out"].put((output))
 
     # <<< MISC METHODS >>> #
-    def get_vram_free(self):
+    @staticmethod
+    def get_vram_free():
         """ Return free and total VRAM on card with most VRAM free"""
         stats = GPUStats()
         vram = stats.get_card_most_free()
-        if self.verbose:
-            print("Using device {} with {}MB free of {}MB".format(
-                vram["device"],
-                int(vram["free"]),
-                int(vram["total"])))
+        logger.verbose("Using device %s with %sMB free of %sMB",
+                       vram["device"],
+                       int(vram["free"]),
+                       int(vram["total"]))
         return int(vram["card_id"]), int(vram["free"]), int(vram["total"])
+
+    def get_item(self):
+        """ Yield one item from the queue """
+        while True:
+            item = self.queues["in"].get()
+            if isinstance(item, dict):
+                logger.trace("Item in: %s", {key: val
+                                             for key, val in item.items()
+                                             if key != "image"})
+                # Pass Detector failures straight out and quit
+                if item.get("exception", None):
+                    self.queues["out"].put(item)
+                    exit(1)
+            else:
+                logger.trace("Item in: %s", item)
+            yield item
+            if item == "EOF":
+                break
diff --git a/plugins/extract/align/dlib.py b/plugins/extract/align/dlib.py
index af5a460..5e8d367 100644
--- a/plugins/extract/align/dlib.py
+++ b/plugins/extract/align/dlib.py
@@ -1,10 +1,9 @@
 #!/usr/bin/env python3
-""" DLib landmarks extractor for faceswap.py
-"""
+""" DLib landmarks extractor for faceswap.py """
 import face_recognition_models
 import dlib
 
-from ._base import Aligner
+from ._base import Aligner, logger
 
 
 class Align(Aligner):
@@ -16,36 +15,42 @@ class Align(Aligner):
 
     def set_model_path(self):
         """ Model path handled by face_recognition_models """
-        return face_recognition_models.pose_predictor_model_location()
+        model_path = face_recognition_models.pose_predictor_model_location()
+        logger.debug("Loading model: '%s'", model_path)
+        return model_path
 
     def initialize(self, *args, **kwargs):
         """ Initialization tasks to run prior to alignments """
         super().initialize(*args, **kwargs)
-        print("Initializing Dlib Pose Predictor...")
+        logger.info("Initializing Dlib Pose Predictor...")
+        logger.debug("dlib initialize: (args: %s kwargs: %s)", args, kwargs)
         self.model = dlib.shape_predictor(self.model_path)  # pylint: disable=c-extension-no-member
         self.init.set()
-        print("Initialized Dlib Pose Predictor.")
+        logger.info("Initialized Dlib Pose Predictor.")
 
     def align(self, *args, **kwargs):
         """ Perform alignments on detected faces """
         super().align(*args, **kwargs)
-        while True:
-            item = self.queues["in"].get()
+        for item in self.get_item():
             if item == "EOF":
-                break
-            if item.get("exception", False):
-                self.queues["out"].put(item)
+                self.finalize(item)
                 break
             image = item["image"][:, :, ::-1].copy()
+
+            logger.trace("Algning faces")
             item["landmarks"] = self.process_landmarks(image, item["detected_faces"])
+            logger.trace("Algned faces: %s", item["landmarks"])
+
             self.finalize(item)
-        self.finalize("EOF")
+        logger.debug("Completed Align")
 
     def process_landmarks(self, image, detected_faces):
         """ Align image and process landmarks """
+        logger.trace("Processing Landmarks")
         retval = list()
         for detected_face in detected_faces:
             pts = self.model(image, detected_face).parts()
             landmarks = [(point.x, point.y) for point in pts]
             retval.append(landmarks)
+        logger.trace("Processed Landmarks: %s", retval)
         return retval
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
index afdd0d4..70b8a03 100644
--- a/plugins/extract/align/fan.py
+++ b/plugins/extract/align/fan.py
@@ -4,12 +4,10 @@
     https://github.com/1adrianb/face-alignment
 """
 import os
-
 import cv2
 import numpy as np
-import tensorflow as tf
 
-from ._base import Aligner
+from ._base import Aligner, logger
 
 
 class Align(Aligner):
@@ -27,12 +25,14 @@ class Align(Aligner):
         if not os.path.exists(model_path):
             raise Exception("Error: Unable to find {}, reinstall "
                             "the lib!".format(model_path))
+        logger.debug("Loading model: '%s'", model_path)
         return model_path
 
     def initialize(self, *args, **kwargs):
         """ Initialization tasks to run prior to alignments """
-        print("Initializing Face Alignment Network...")
         super().initialize(*args, **kwargs)
+        logger.info("Initializing Face Alignment Network...")
+        logger.debug("fan initialize: (args: %s kwargs: %s)", args, kwargs)
 
         card_id, _, vram_total = self.get_vram_free()
         if card_id == -1:
@@ -45,51 +45,44 @@ class Align(Aligner):
             tf_ratio = 1.0
         else:
             tf_ratio = self.vram / vram_total
-        if self.verbose:
-            print("Reserving {}MB for face alignments".format(self.vram))
+        logger.verbose("Reserving %sMB for face alignments", self.vram)
 
-        self.model = FAN(self.model_path,
-                         verbose=self.verbose, ratio=tf_ratio)
+        self.model = FAN(self.model_path, ratio=tf_ratio)
 
         self.init.set()
-        print("Initialized Face Alignment Network.")
+        logger.info("Initialized Face Alignment Network.")
 
     def align(self, *args, **kwargs):
         """ Perform alignments on detected faces """
         super().align(*args, **kwargs)
-        try:
-            while True:
-                # NB: There appears to be a bug somewhere that re-inserts the first item (after
-                # detecting landmarks) back into the in queue. This happens consistently when -mp
-                # is not set, only appears to happen for the first item and always places it in
-                # the same place. It doesn't effect output, but should be squashed.
-                item = self.queues["in"].get()
-                if item == "EOF":
-                    break
-                if item.get("exception", False):
-                    self.queues["out"].put(item)
-                    break
-                image = item["image"][:, :, ::-1].copy()
-                item["landmarks"] = self.process_landmarks(image, item["detected_faces"])
+        for item in self.get_item():
+            if item == "EOF":
                 self.finalize(item)
-            self.finalize("EOF")
-        except:
-            item["exception"] = True
-            self.queues["out"].put(item)
-            raise
+                break
+            image = item["image"][:, :, ::-1].copy()
+
+            logger.trace("Algning faces")
+            item["landmarks"] = self.process_landmarks(image, item["detected_faces"])
+            logger.trace("Algned faces: %s", item["landmarks"])
+
+            self.finalize(item)
+        logger.debug("Completed Align")
 
     def process_landmarks(self, image, detected_faces):
         """ Align image and process landmarks """
+        logger.trace("Processing landmarks")
         retval = list()
         for detected_face in detected_faces:
             center, scale = self.get_center_scale(detected_face)
             aligned_image = self.align_image(image, center, scale)
             landmarks = self.predict_landmarks(aligned_image, center, scale)
             retval.append(landmarks)
+        logger.trace("Processed landmarks: %s", retval)
         return retval
 
     def get_center_scale(self, detected_face):
         """ Get the center and set scale of bounding box """
+        logger.trace("Calculating center and scale")
         center = np.array([(detected_face.left()
                             + detected_face.right()) / 2.0,
                            (detected_face.top()
@@ -103,27 +96,32 @@ class Align(Aligner):
                  + detected_face.bottom()
                  - detected_face.top()) / self.reference_scale
 
+        logger.trace("Calculated center and scale: %s, %s", center, scale)
         return center, scale
 
     def align_image(self, image, center, scale):
         """ Crop and align image around center """
+        logger.trace("Aligning image around center")
         image = self.crop(
             image,
             center,
             scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
-
+        logger.trace("Aligned image around center")
         return np.expand_dims(image, 0)
 
     def predict_landmarks(self, image, center, scale):
         """ Predict the 68 point landmarks """
+        logger.trace("Predicting Landmarks")
         prediction = self.model.predict(image)[-1]
         pts_img = self.get_pts_from_predict(prediction, center, scale)
-
-        return [(int(pt[0]), int(pt[1])) for pt in pts_img]
+        retval = [(int(pt[0]), int(pt[1])) for pt in pts_img]
+        logger.trace("Predicted Landmarks: %s", retval)
+        return retval
 
     @staticmethod
     def transform(point, center, scale, resolution):
         """ Transform Image """
+        logger.trace("Transforming Points")
         pnt = np.array([point[0], point[1], 1.0])
         hscl = 200.0 * scale
         eye = np.eye(3)
@@ -132,10 +130,13 @@ class Align(Aligner):
         eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
         eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
         eye = np.linalg.inv(eye)
-        return np.matmul(eye, pnt)[0:2]
+        retval = np.matmul(eye, pnt)[0:2]
+        logger.trace("Transformed Points: %s", retval)
+        return retval
 
-    def crop(self, image, center, scale, resolution=256.0):
+    def crop(self, image, center, scale, resolution=256.0):  # pylint: disable=too-many-locals
         """ Crop image around the center point """
+        logger.trace("Cropping image")
         v_ul = self.transform([1, 1], center, scale, resolution).astype(np.int)
         v_br = self.transform([resolution, resolution],
                               center,
@@ -172,10 +173,12 @@ class Align(Aligner):
         new_img = cv2.resize(new_img,
                              dsize=(int(resolution), int(resolution)),
                              interpolation=cv2.INTER_LINEAR)
+        logger.trace("Cropped image")
         return new_img
 
     def get_pts_from_predict(self, var_a, center, scale):
         """ Get points from predictor """
+        logger.trace("Obtain points from prediction")
         var_b = var_a.reshape((var_a.shape[0],
                                var_a.shape[1] * var_a.shape[2]))
         var_c = var_b.argmax(1).reshape((var_a.shape[0],
@@ -198,8 +201,11 @@ class Align(Aligner):
                 var_c[i] += np.sign(diff)*0.25
 
         var_c += 0.5
-        return [self.transform(var_c[i], center, scale, var_a.shape[2])
-                for i in range(var_a.shape[0])]
+        retval = [self.transform(var_c[i], center, scale, var_a.shape[2])
+                  for i in range(var_a.shape[0])]
+        logger.trace("Obtained points from prediction: %s", retval)
+
+        return retval
 
 
 class FAN():
@@ -207,8 +213,12 @@ class FAN():
     Converted from pyTorch via ONNX from:
     https://github.com/1adrianb/face-alignment """
 
-    def __init__(self, model_path, verbose=False, ratio=1.0):
-        self.verbose = verbose
+    def __init__(self, model_path, ratio=1.0):
+        # Must import tensorflow inside the spawned process
+        # for Windows machines
+        import tensorflow as tf
+        self.tf = tf
+
         self.model_path = model_path
         self.graph = self.load_graph()
         self.input = self.graph.get_tensor_by_name("fa/0:0")
@@ -218,15 +228,13 @@ class FAN():
     def load_graph(self):
         """ Load the tensorflow Model and weights """
         # pylint: disable=not-context-manager
-        if self.verbose:
-            print("Initializing Face Alignment Network model...")
-
-        with tf.gfile.GFile(self.model_path, "rb") as gfile:
-            graph_def = tf.GraphDef()
+        logger.verbose("Initializing Face Alignment Network model...")
+        with self.tf.gfile.GFile(self.model_path, "rb") as gfile:
+            graph_def = self.tf.GraphDef()
             graph_def.ParseFromString(gfile.read())
-        fa_graph = tf.Graph()
+        fa_graph = self.tf.Graph()
         with fa_graph.as_default():
-            tf.import_graph_def(graph_def, name="fa")
+            self.tf.import_graph_def(graph_def, name="fa")
         return fa_graph
 
     def set_session(self, vram_ratio):
@@ -234,9 +242,9 @@ class FAN():
         # pylint: disable=not-context-manager, no-member
         placeholder = np.zeros((1, 3, 256, 256))
         with self.graph.as_default():
-            config = tf.ConfigProto()
+            config = self.tf.ConfigProto()
             config.gpu_options.per_process_gpu_memory_fraction = vram_ratio
-            session = tf.Session(config=config)
+            session = self.tf.Session(config=config)
             with session.as_default():
                 session.run(self.output, feed_dict={self.input: placeholder})
         return session
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index e46a897..efbf665 100644
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -11,26 +11,32 @@
      "detected_faces": <list of dlib.rectangles>}
     """
 
+import logging
 import os
+import traceback
+from io import StringIO
 
 import cv2
 import dlib
 
 from lib.gpu_stats import GPUStats
-from lib.utils import rotate_image_by_angle, rotate_landmarks
+from lib.utils import rotate_landmarks
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class Detector():
     """ Detector object """
-    def __init__(self, verbose=False, rotation=None):
+    def __init__(self, loglevel, rotation=None):
+        logger.debug("Initializing %s: (rotation: %s)", self.__class__.__name__, rotation)
+        self.loglevel = loglevel
         self.cachepath = os.path.join(os.path.dirname(__file__), ".cache")
-        self.verbose = verbose
         self.rotation = self.get_rotation_angles(rotation)
         self.parent_is_pool = False
         self.init = None
 
         # The input and output queues for the plugin.
-        # See lib.multithreading.QueueManager for getting queues
+        # See lib.queue_manager.QueueManager for getting queues
         self.queues = {"in": None, "out": None}
 
         # Scaling factor for image. Plugin dependent
@@ -53,6 +59,7 @@ class Detector():
         # will support. It is also used for holding the number of threads/
         # processes for parallel processing plugins
         self.batch_size = 1
+        logger.debug("Initialized _base %s", self.__class__.__name__)
 
     # <<< OVERRIDE METHODS >>> #
     # These methods must be overriden when creating a plugin
@@ -66,8 +73,12 @@ class Detector():
         """ Inititalize the detector
             Tasks to be run before any detection is performed.
             Override for specific detector """
-        init = kwargs.get("event", False)
-        self.init = init
+        logger_init = kwargs["log_init"]
+        log_queue = kwargs["log_queue"]
+        logger_init(self.loglevel, log_queue)
+        logger.debug("initialize %s (PID: %s, args: %s, kwargs: %s)",
+                     self.__class__.__name__, os.getpid(), args, kwargs)
+        self.init = kwargs.get("event", False)
         self.queues["in"] = kwargs["in_queue"]
         self.queues["out"] = kwargs["out_queue"]
 
@@ -79,13 +90,36 @@ class Detector():
             if not self.init:
                 self.initialize(*args, **kwargs)
         except ValueError as err:
-            print("ERROR: {}".format(err))
+            logger.error(err)
+            exit(1)
+        logger.debug("Detecting Faces (args: %s, kwargs: %s)", args, kwargs)
+
+    # <<< DETECTION WRAPPER >>> #
+    def run(self, *args, **kwargs):
+        """ Parent detect process.
+            This should always be called as the entry point so exceptions
+            are passed back to parent.
+            Do not override """
+        try:
+            self.detect_faces(*args, **kwargs)
+        except Exception:  # pylint: disable=broad-except
+            logger.error("Caught exception in child process: %s", os.getpid())
+            tb_buffer = StringIO()
+            traceback.print_exc(file=tb_buffer)
+            exception = {"exception": (os.getpid(), tb_buffer)}
+            self.queues["out"].put(exception)
             exit(1)
 
     # <<< FINALIZE METHODS>>> #
     def finalize(self, output):
         """ This should be called as the final task of each plugin
             Performs fianl processing and puts to the out queue """
+        if isinstance(output, dict):
+            logger.trace("Item out: %s", {key: val
+                                          for key, val in output.items()
+                                          if key != "image"})
+        else:
+            logger.trace("Item out: %s", output)
         self.queues["out"].put(output)
 
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
@@ -113,6 +147,7 @@ class Detector():
             self.scale = target / source
         else:
             self.scale = 1.0
+        logger.trace("Detector scale: %s", self.scale)
 
     def set_detect_image(self, input_image):
         """ Convert the image to RGB and scale """
@@ -125,9 +160,9 @@ class Detector():
         interpln = cv2.INTER_LINEAR if self.scale > 1.0 else cv2.INTER_AREA
         dims = (int(width * self.scale), int(height * self.scale))
 
-        if self.verbose and self.scale < 1.0:
-            print("Resizing image from {}x{} to {}.".format(
-                str(width), str(height), "x".join(str(i) for i in dims)))
+        if self.scale < 1.0:
+            logger.verbose("Resizing image from %sx%s to %s.",
+                           width, height, "x".join(str(i) for i in dims))
 
         image = cv2.resize(image, dims, interpolation=interpln)
         return image
@@ -144,6 +179,7 @@ class Detector():
         rotation_angles = [0]
 
         if not rotation or rotation.lower() == "off":
+            logger.debug("Not setting rotation angles")
             return rotation_angles
 
         if rotation.lower() == "on":
@@ -159,23 +195,64 @@ class Detector():
             elif len(passed_angles) > 1:
                 rotation_angles.extend(passed_angles)
 
+        logger.debug("Rotation Angles: %s", rotation_angles)
         return rotation_angles
 
-    @staticmethod
-    def rotate_image(image, angle):
+    def rotate_image(self, image, angle):
         """ Rotate the image by given angle and return
             Image with rotation matrix """
         if angle == 0:
             return image, None
-        return rotate_image_by_angle(image, angle)
+        return self.rotate_image_by_angle(image, angle)
 
     @staticmethod
     def rotate_rect(d_rect, rotation_matrix):
         """ Rotate a dlib rect based on the rotation_matrix"""
+        logger.trace("Rotating d_rectangle")
         d_rect = rotate_landmarks(d_rect, rotation_matrix)
         return d_rect
 
+    @staticmethod
+    def rotate_image_by_angle(image, angle,
+                              rotated_width=None, rotated_height=None):
+        """ Rotate an image by a given angle.
+            From: https://stackoverflow.com/questions/22041699 """
+
+        logger.trace("Rotating image: (angle: %s, rotated_width: %s, rotated_height: %s)",
+                     angle, rotated_width, rotated_height)
+        height, width = image.shape[:2]
+        image_center = (width/2, height/2)
+        rotation_matrix = cv2.getRotationMatrix2D(  # pylint: disable=no-member
+            image_center, -1.*angle, 1.)
+        if rotated_width is None or rotated_height is None:
+            abs_cos = abs(rotation_matrix[0, 0])
+            abs_sin = abs(rotation_matrix[0, 1])
+            if rotated_width is None:
+                rotated_width = int(height*abs_sin + width*abs_cos)
+            if rotated_height is None:
+                rotated_height = int(height*abs_cos + width*abs_sin)
+        rotation_matrix[0, 2] += rotated_width/2 - image_center[0]
+        rotation_matrix[1, 2] += rotated_height/2 - image_center[1]
+        logger.trace("Rotated image: (rotation_matrix: %s", rotation_matrix)
+        return (cv2.warpAffine(image,  # pylint: disable=no-member
+                               rotation_matrix,
+                               (rotated_width, rotated_height)),
+                rotation_matrix)
+
     # << QUEUE METHODS >> #
+    def get_item(self):
+        """ Yield one item from the queue """
+        item = self.queues["in"].get()
+        if isinstance(item, dict):
+            logger.trace("Item in: %s", item["filename"])
+        else:
+            logger.trace("Item in: %s", item)
+        if item == "EOF":
+            logger.debug("In Queue Exhausted")
+            # Re-put EOF into queue for other threads
+            self.queues["in"].put(item)
+        return item
+
     def get_batch(self):
         """ Get items from the queue in batches of
             self.batch_size
@@ -189,11 +266,12 @@ class Detector():
         exhausted = False
         batch = list()
         for _ in range(self.batch_size):
-            item = self.queues["in"].get()
+            item = self.get_item()
             if item == "EOF":
                 exhausted = True
                 break
             batch.append(item)
+        logger.trace("Returning batch size: %s", len(batch))
         return (exhausted, batch)
 
     # <<< DLIB RECTANGLE METHODS >>> #
@@ -212,15 +290,15 @@ class Detector():
         return d_rect
 
     # <<< MISC METHODS >>> #
-    def get_vram_free(self):
+    @staticmethod
+    def get_vram_free():
         """ Return total free VRAM on largest card """
         stats = GPUStats()
         vram = stats.get_card_most_free()
-        if self.verbose:
-            print("Using device {} with {}MB free of {}MB".format(
-                vram["device"],
-                int(vram["free"]),
-                int(vram["total"])))
+        logger.verbose("Using device %s with %sMB free of %sMB",
+                       vram["device"],
+                       int(vram["free"]),
+                       int(vram["total"]))
         return int(vram["free"])
 
     @staticmethod
@@ -230,5 +308,5 @@ class Detector():
         # Landmarks should not be extracted again from predetected faces,
         # because face data is lost, resulting in a large variance
         # against extract from original image
-        return [dlib.rectangle(  # pylint: disable=c-extension-no-member
-            0, 0, width, height)]
+        logger.debug("Setting predetected face")
+        return [dlib.rectangle(0, 0, width, height)]  # pylint: disable=c-extension-no-member
diff --git a/plugins/extract/detect/dlib_cnn.py b/plugins/extract/detect/dlib_cnn.py
index 7b98422..38ec22d 100644
--- a/plugins/extract/detect/dlib_cnn.py
+++ b/plugins/extract/detect/dlib_cnn.py
@@ -3,9 +3,8 @@
 
 import numpy as np
 import face_recognition_models
-from lib.utils import rotate_image_by_angle
 
-from ._base import Detector, dlib
+from ._base import Detector, dlib, logger
 
 
 class Detect(Detector):
@@ -16,35 +15,36 @@ class Detect(Detector):
         self.vram = 1600  # Lower as batch size of 2 gives wiggle room
         self.detector = None
 
-    def compiled_for_cuda(self):
+    @staticmethod
+    def compiled_for_cuda():
         """ Return a message on DLIB Cuda Compilation status """
         cuda = dlib.DLIB_USE_CUDA  # pylint: disable=c-extension-no-member
         msg = "DLib is "
         if not cuda:
             msg += "NOT "
         msg += "compiled to use CUDA"
-        if self.verbose:
-            print(msg)
+        logger.verbose(msg)
         return cuda
 
     def set_model_path(self):
         """ Model path handled by face_recognition_models """
-        return face_recognition_models.cnn_face_detector_model_location()
+        model_path = face_recognition_models.cnn_face_detector_model_location()
+        logger.debug("Loading model: '%s'", model_path)
+        return model_path
 
     def initialize(self, *args, **kwargs):
         """ Calculate batch size """
-        print("Initializing Dlib-CNN Detector...")
         super().initialize(*args, **kwargs)
+        logger.verbose("Initializing Dlib-CNN Detector...")
         self.detector = dlib.cnn_face_detection_model_v1(  # pylint: disable=c-extension-no-member
             self.model_path)
         is_cuda = self.compiled_for_cuda()
         if is_cuda:
+            logger.debug("Using GPU")
             vram_free = self.get_vram_free()
         else:
+            logger.verbose("Using CPU")
             vram_free = 2048
-            if self.verbose:
-                print("Using CPU. Limiting RAM useage to "
-                      "{}MB".format(vram_free))
 
         # Batch size of 2 actually uses about 338MB less than a single image??
         # From there batches increase at ~680MB per item in the batch
@@ -55,78 +55,86 @@ class Detect(Detector):
             raise ValueError("Insufficient VRAM available to continue "
                              "({}MB)".format(int(vram_free)))
 
-        if self.verbose:
-            print("Processing in batches of {}".format(self.batch_size))
+        logger.verbose("Processing in batches of %s", self.batch_size)
 
         self.init.set()
-        print("Initialized Dlib-CNN Detector...")
+        logger.info("Initialized Dlib-CNN Detector...")
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in rgb image """
         super().detect_faces(*args, **kwargs)
-        try:
-            while True:
-                exhausted, batch = self.get_batch()
-                if not batch:
-                    break
-                filenames, images = map(list, zip(*batch))
-                detect_images = self.compile_detection_images(images)
-                batch_detected = self.detect_batch(detect_images)
-                processed = self.process_output(batch_detected,
-                                                indexes=None,
-                                                rotation_matrix=None,
-                                                output=None)
-                if not all(faces
-                           for faces in processed) and self.rotation != [0]:
-                    processed = self.process_rotations(detect_images,
-                                                       processed)
-                for idx, faces in enumerate(processed):
-                    retval = {"filename": filenames[idx],
-                              "image": images[idx],
-                              "detected_faces": faces}
-                    self.finalize(retval)
-                if exhausted:
-                    break
-        except:
-            retval = {"exception": True}
-            self.queues["out"].put(retval)
-            del self.detector  # Free up VRAM
-            raise
-
+        while True:
+            exhausted, batch = self.get_batch()
+            if not batch:
+                break
+            filenames = list()
+            images = list()
+            for item in batch:
+                filenames.append(item["filename"])
+                images.append(item["image"])
+            detect_images = self.compile_detection_images(images)
+            batch_detected = self.detect_batch(detect_images)
+            processed = self.process_output(batch_detected,
+                                            indexes=None,
+                                            rotation_matrix=None,
+                                            output=None)
+            if not all(faces for faces in processed) and self.rotation != [0]:
+                processed = self.process_rotations(detect_images, processed)
+            for idx, faces in enumerate(processed):
+                filename = filenames[idx]
+                for b_idx, item in enumerate(batch):
+                    if item["filename"] == filename:
+                        output = item
+                        del_idx = b_idx
+                        break
+                output["detected_faces"] = faces
+                self.finalize(output)
+                del batch[del_idx]
+            if exhausted:
+                break
         self.queues["out"].put("EOF")
         del self.detector  # Free up VRAM
+        logger.debug("Detecting Faces complete")
 
     def compile_detection_images(self, images):
         """ Compile the detection images into batches """
+        logger.trace("Compiling Detection Images: %s", len(images))
         detect_images = list()
         for image in images:
             self.set_scale(image, is_square=True, scale_up=True)
             detect_images.append(self.set_detect_image(image))
+        logger.trace("Compiled Detection Images")
         return detect_images
 
     def detect_batch(self, detect_images, disable_message=False):
         """ Pass the batch through detector for consistently sized images
             or each image seperately for inconsitently sized images """
+        logger.trace("Detecting Batch")
         can_batch = self.check_batch_dims(detect_images)
         if can_batch:
+            logger.trace("Valid for batching")
             batch_detected = self.detector(detect_images, 0)
         else:
-            if self.verbose and not disable_message:
-                print("Batch has inconsistently sized images. Processing one "
-                      "image at a time")
+            if not disable_message:
+                logger.verbose("Batch has inconsistently sized images. Processing one "
+                               "image at a time")
             batch_detected = dlib.mmod_rectangless(  # pylint: disable=c-extension-no-member
                 [self.detector(detect_image, 0) for detect_image in detect_images])
+        logger.trace("Detected Batch: %s", [item for item in batch_detected])
         return batch_detected
 
     @staticmethod
     def check_batch_dims(images):
         """ Check all images are the same size for batching """
         dims = set(frame.shape[:2] for frame in images)
+        logger.trace("Batch Dimensions: %s", dims)
         return len(dims) == 1
 
     def process_output(self, batch_detected,
                        indexes=None, rotation_matrix=None, output=None):
         """ Process the output images """
+        logger.trace("Processing Output: (batch_detected: %s, indexes: %s, rotation_matrix: %s, "
+                     "output: %s", batch_detected, indexes, rotation_matrix, output)
         output = output if output else list()
         for idx, faces in enumerate(batch_detected):
             detected_faces = list()
@@ -148,10 +156,12 @@ class Detect(Detector):
                 output[target] = detected_faces
             else:
                 output.append(detected_faces)
+        logger.trace("Processed Output: %s", output)
         return output
 
     def process_rotations(self, detect_images, processed):
         """ Rotate frames missing faces until face is found """
+        logger.trace("Processing Rotations")
         for angle in self.rotation:
             if all(faces for faces in processed):
                 break
@@ -163,25 +173,26 @@ class Detect(Detector):
                 angle)
 
             batch_detected = self.detect_batch(reprocess, disable_message=True)
-            if self.verbose and any(item for item in batch_detected):
-                print("found face(s) by rotating image {} degrees".format(
-                    angle))
+            if any(item for item in batch_detected):
+                logger.verbose("found face(s) by rotating image %s degrees", angle)
             processed = self.process_output(batch_detected,
                                             indexes=indexes,
                                             rotation_matrix=rotmat,
                                             output=processed)
+        logger.trace("Processed Rotations")
         return processed
 
-    @staticmethod
-    def compile_reprocess(processed, detect_images, angle):
+    def compile_reprocess(self, processed, detect_images, angle):
         """ Rotate images which did not find a face for reprocessing """
+        logger.trace("Compile images for reprocessing")
         indexes = list()
         to_detect = list()
         for idx, faces in enumerate(processed):
             if faces:
                 continue
             image = detect_images[idx]
-            rot_image, rot_matrix = rotate_image_by_angle(image, angle)
+            rot_image, rot_matrix = self.rotate_image_by_angle(image, angle)
             to_detect.append(rot_image)
             indexes.append(idx)
+        logger.trace("Compiled images for reprocessing")
         return to_detect, indexes, rot_matrix
diff --git a/plugins/extract/detect/dlib_hog.py b/plugins/extract/detect/dlib_hog.py
index b1d36dd..cf07e9d 100644
--- a/plugins/extract/detect/dlib_hog.py
+++ b/plugins/extract/detect/dlib_hog.py
@@ -4,7 +4,7 @@ from time import sleep
 
 import numpy as np
 
-from ._base import Detector, dlib
+from ._base import Detector, dlib, logger
 
 
 class Detect(Detector):
@@ -14,7 +14,7 @@ class Detect(Detector):
         self.parent_is_pool = True
         self.target = (2048, 2048)  # Doesn't use VRAM
         self.vram = 0
-        self.detector = dlib.get_frontal_face_detector()
+        self.detector = dlib.get_frontal_face_detector()  # pylint: disable=c-extension-no-member
         self.iterator = None
 
     def set_model_path(self):
@@ -23,61 +23,54 @@ class Detect(Detector):
 
     def initialize(self, *args, **kwargs):
         """ Calculate batch size """
-        print("Initializing Dlib-HOG Detector...")
         super().initialize(*args, **kwargs)
-        if self.verbose:
-            print("Using CPU for detection")
+        logger.info("Initializing Dlib-HOG Detector...")
+        logger.verbose("Using CPU for detection")
         self.init = True
-        print("Initialized Dlib-HOG Detector...")
+        logger.info("Initialized Dlib-HOG Detector...")
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in rgb image """
         super().detect_faces(*args, **kwargs)
-        try:
-            while True:
-                item = self.queues["in"].get()
-                if item in ("EOF", "END"):
-                    self.queues["in"].put("END")
-                    break
-
-                filename, image = item
-                detect_image = self.compile_detection_image(image, True, True)
+        while True:
+            item = self.get_item()
+            if item == "EOF":
+                break
+            logger.trace("Detecting faces: %s", item["filename"])
+            detect_image = self.compile_detection_image(item["image"], True, True)
 
-                for angle in self.rotation:
-                    current_image, rotmat = self.rotate_image(detect_image,
-                                                              angle)
+            for angle in self.rotation:
+                current_image, rotmat = self.rotate_image(detect_image, angle)
 
-                    faces = self.detector(current_image, 0)
+                logger.trace("Detecting faces")
+                faces = self.detector(current_image, 0)
+                logger.trace("Detected faces: %s", [face for face in faces])
 
-                    if self.verbose and angle != 0 and faces.any():
-                        print("found face(s) by rotating image {} "
-                              "degrees".format(angle))
+                if angle != 0 and faces.any():
+                    logger.verbose("found face(s) by rotating image %s degrees", angle)
 
-                    if faces:
-                        break
+                if faces:
+                    break
 
-                detected_faces = self.process_output(faces, rotmat)
-                retval = {"filename": filename,
-                          "image": image,
-                          "detected_faces": detected_faces}
-                self.finalize(retval)
-        except:
-            retval = {"exception": True}
-            self.queues["out"].put(retval)
-            raise
+            detected_faces = self.process_output(faces, rotmat)
+            item["detected_faces"] = detected_faces
+            self.finalize(item)
 
         if item == "EOF":
             sleep(3)  # Wait for all processes to finish before EOF (hacky!)
             self.queues["out"].put("EOF")
+        logger.debug("Detecting Faces Complete")
 
     def process_output(self, faces, rotation_matrix):
         """ Compile found faces for output """
+        logger.trace("Processing Output: (faces: %s, rotation_matrix: %s)",
+                     faces, rotation_matrix)
         if isinstance(rotation_matrix, np.ndarray):
             faces = [self.rotate_rect(face, rotation_matrix)
                      for face in faces]
-        detected = [dlib.rectangle(int(face.left() / self.scale),
-                                   int(face.top() / self.scale),
-                                   int(face.right() / self.scale),
-                                   int(face.bottom() / self.scale))
+        detected = [dlib.rectangle(  # pylint: disable=c-extension-no-member
+            int(face.left() / self.scale), int(face.top() / self.scale),
+            int(face.right() / self.scale), int(face.bottom() / self.scale))
                     for face in faces]
+        logger.trace("Processed Output: %s", detected)
         return detected
diff --git a/plugins/extract/detect/manual.py b/plugins/extract/detect/manual.py
index b81abac..5b890f8 100644
--- a/plugins/extract/detect/manual.py
+++ b/plugins/extract/detect/manual.py
@@ -1,7 +1,7 @@
 #!/usr/bin/env python3
 """ Manual face detection plugin """
 
-from ._base import Detector, dlib
+from ._base import Detector, dlib, logger
 
 
 class Detect(Detector):
@@ -15,24 +15,23 @@ class Detect(Detector):
 
     def initialize(self, *args, **kwargs):
         """ Create the mtcnn detector """
-        print("Initializing Manual Detector...")
         super().initialize(*args, **kwargs)
+        logger.info("Initializing Manual Detector...")
         self.init.set()
-        print("Initialized Manual Detector.")
+        logger.info("Initialized Manual Detector.")
 
     def detect_faces(self, *args, **kwargs):
         """ Return the given bounding box in a dlib rectangle """
         super().detect_faces(*args, **kwargs)
         while True:
-            item = self.queues["in"].get()
+            item = self.get_item()
             if item == "EOF":
                 break
-            image, face = item
+            face = item["face"]
 
-            bounding_box = [dlib.rectangle(int(face[0]), int(face[1]),
-                                           int(face[2]), int(face[3]))]
-            retval = {"image": image,
-                      "detected_faces": bounding_box}
-            self.finalize(retval)
+            bounding_box = [dlib.rectangle(  # pylint: disable=c-extension-no-member
+                int(face[0]), int(face[1]), int(face[2]), int(face[3]))]
+            item["detected_faces"] = bounding_box
+            self.finalize(item)
 
         self.queues["out"].put("EOF")
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
index 3d7d6c8..ae01135 100644
--- a/plugins/extract/detect/mtcnn.py
+++ b/plugins/extract/detect/mtcnn.py
@@ -9,10 +9,21 @@ from six import string_types, iteritems
 
 import cv2
 import numpy as np
-import tensorflow as tf
 
 from lib.multithreading import MultiThread
-from ._base import Detector, dlib
+from ._base import Detector, dlib, logger
+
+
+# Must import tensorflow inside the spawned process
+# for Windows machines
+tf = None  # pylint: disable = invalid-name
+
+
+def import_tensorflow():
+    """ Import tensorflow from inside spawned process """
+    global tf  # pylint: disable = invalid-name,global-statement
+    import tensorflow as tflow
+    tf = tflow
 
 
 class Detect(Detector):
@@ -39,10 +50,11 @@ class Detect(Detector):
             valid = False
 
         if not valid:
-            print("Invalid MTCNN arguments received. Running with defaults")
-            return {"minsize": 20,                 # minimum size of face
-                    "threshold": [0.6, 0.7, 0.7],  # three steps threshold
-                    "factor": 0.709}               # scale factor
+            kwargs = {"minsize": 20,                 # minimum size of face
+                      "threshold": [0.6, 0.7, 0.7],  # three steps threshold
+                      "factor": 0.709}               # scale factor
+            logger.warning("Invalid MTCNN arguments received. Running with defaults")
+        logger.debug("Using mtcnn kwargs: %s", kwargs)
         return kwargs
 
     def set_model_path(self):
@@ -52,32 +64,44 @@ class Detect(Detector):
             if not os.path.exists(model_path):
                 raise Exception("Error: Unable to find {}, reinstall "
                                 "the lib!".format(model_path))
+            logger.debug("Loading model: '%s'", model_path)
         return self.cachepath
 
     def initialize(self, *args, **kwargs):
         """ Create the mtcnn detector """
-        print("Initializing MTCNN Detector...")
         super().initialize(*args, **kwargs)
+        logger.info("Initializing MTCNN Detector...")
         is_gpu = False
         self.kwargs = kwargs["mtcnn_kwargs"]
 
+        # Must import tensorflow inside the spawned process
+        # for Windows machines
+        import_tensorflow()
+        vram_free = self.get_vram_free()
         mtcnn_graph = tf.Graph()
+
+        # Windows machines sometimes misreport available vram, and overuse
+        # causing OOM. Allow growth fixes that
+        config = tf.ConfigProto()
+        config.gpu_options.allow_growth = True  # pylint: disable=no-member
+
         with mtcnn_graph.as_default():  # pylint: disable=not-context-manager
-            sess = tf.Session()
+            sess = tf.Session(config=config)
             with sess.as_default():  # pylint: disable=not-context-manager
                 pnet, rnet, onet = create_mtcnn(sess, self.model_path)
 
             if any("gpu" in str(device).lower()
                    for device in sess.list_devices()):
+                logger.debug("Using GPU")
                 is_gpu = True
-                alloc = int(sess.run(tf.contrib.memory_stats.BytesLimit()) /
-                            (1024 * 1024))
         mtcnn_graph.finalize()
 
         if not is_gpu:
             alloc = 2048
-            if self.verbose:
-                print("Using CPU. Limiting RAM useage to {}MB".format(alloc))
+            logger.warning("Using CPU")
+        else:
+            alloc = vram_free
+        logger.debug("Allocated for Tensorflow: %sMB", alloc)
 
         self.batch_size = int(alloc / self.vram)
 
@@ -85,59 +109,53 @@ class Detect(Detector):
             raise ValueError("Insufficient VRAM available to continue "
                              "({}MB)".format(int(alloc)))
 
-        if self.verbose:
-            print("Processing in {} threads".format(self.batch_size))
+        logger.verbose("Processing in %s threads", self.batch_size)
 
         self.kwargs["pnet"] = pnet
         self.kwargs["rnet"] = rnet
         self.kwargs["onet"] = onet
 
         self.init.set()
-        print("Initialized MTCNN Detector.")
+        logger.info("Initialized MTCNN Detector.")
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in Multiple Threads """
         super().detect_faces(*args, **kwargs)
-        workers = MultiThread(thread_count=self.batch_size)
-        workers.in_thread(target=self.detect_thread)
-        workers.join_threads()
+        workers = MultiThread(target=self.detect_thread, thread_count=self.batch_size)
+        workers.start()
+        workers.join()
         sentinel = self.queues["in"].get()
         self.queues["out"].put(sentinel)
+        logger.debug("Detecting Faces complete")
 
     def detect_thread(self):
         """ Detect faces in rgb image """
-        try:
-            while True:
-                item = self.queues["in"].get()
-                if item == "EOF":
-                    self.queues["in"].put(item)
+        logger.debug("Launching Detect")
+        while True:
+            item = self.get_item()
+            if item == "EOF":
+                break
+            logger.trace("Detecting faces: '%s'", item["filename"])
+            detect_image = self.compile_detection_image(item["image"], False, False)
+
+            for angle in self.rotation:
+                current_image, rotmat = self.rotate_image(detect_image, angle)
+                faces, points = detect_face(current_image, **self.kwargs)
+                if angle != 0 and faces.any():
+                    logger.verbose("found face(s) by rotating image %s degrees", angle)
+                if faces.any():
                     break
 
-                filename, image = item
-                detect_image = self.compile_detection_image(image, False, False)
-
-                for angle in self.rotation:
-                    current_image, rotmat = self.rotate_image(detect_image, angle)
-                    faces, points = detect_face(current_image, **self.kwargs)
-                    if self.verbose and angle != 0 and faces.any():
-                        print("found face(s) by rotating image {} degrees".format(
-                            angle))
-                    if faces.any():
-                        break
-
-                detected_faces = self.process_output(faces, points, rotmat)
-                retval = {
-                    "filename": filename,
-                    "image": image,
-                    "detected_faces": detected_faces}
-                self.finalize(retval)
-        except:
-            retval = {"exception": True}
-            self.queues["out"].put(retval)
-            raise
+            detected_faces = self.process_output(faces, points, rotmat)
+            item["detected_faces"] = detected_faces
+            self.finalize(item)
+
+        logger.debug("Thread Completed Detect")
 
     def process_output(self, faces, points, rotation_matrix):
         """ Compile found faces for output """
+        logger.trace("Processing Output: (faces: %s, points: %s, rotation_matrix: %s)",
+                     faces, points, rotation_matrix)
         faces = self.recalculate_bounding_box(faces, points)
         faces = [dlib.rectangle(  # pylint: disable=c-extension-no-member
             int(face[0]), int(face[1]), int(face[2]), int(face[3]))
@@ -151,6 +169,7 @@ class Detect(Detector):
             int(face.right() / self.scale),
             int(face.bottom() / self.scale))
                     for face in faces]
+        logger.trace("Processed Output: %s", detected)
         return detected
 
     @staticmethod
@@ -162,6 +181,8 @@ class Detect(Detector):
             Resize the bounding box around features to present
             a better box to Face Alignment. Helps its chances
             on edge cases and helps remove 'jitter' """
+        logger.trace("Recalculating Bounding Boxes: (faces: %s, landmarks: %s)",
+                     faces, landmarks)
         retval = list()
         no_faces = len(faces)
         if no_faces == 0:
@@ -184,8 +205,8 @@ class Detect(Detector):
 
             bounding = [center[0] - padding[0], center[1] - padding[1],
                         center[0] + padding[0], center[1] + padding[1]]
-
             retval.append(bounding)
+        logger.trace("Recalculated Bounding Boxes: %s", retval)
         return retval
 
 
@@ -537,7 +558,6 @@ def detect_face(img, minsize, pnet, rnet,  # pylint: disable=too-many-arguments
     # # # # # # # # # # # # #
     # first stage - fast proposal network (pnet) to obtain face candidates
     # # # # # # # # # # # # #
-
     for scale in scales:
         height_scale = int(np.ceil(height * scale))
         width_scale = int(np.ceil(width * scale))
diff --git a/plugins/model/Model_GAN/Model.py b/plugins/model/Model_GAN/Model.py
index 0f90ed1..820652c 100644
--- a/plugins/model/Model_GAN/Model.py
+++ b/plugins/model/Model_GAN/Model.py
@@ -1,5 +1,7 @@
 # Based on the https://github.com/shaoanlu/faceswap-GAN repo (master/temp/faceswap_GAN_keras.ipynb)
 
+import logging
+
 from keras.models import Model
 from keras.layers import *
 from keras.layers.advanced_activations import LeakyReLU
@@ -14,13 +16,16 @@ from lib.utils import backup_file
 
 from keras.utils import multi_gpu_model
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
 hdf = {'netGAH5': 'netGA_GAN.h5',
        'netGBH5': 'netGB_GAN.h5',
        'netDAH5': 'netDA_GAN.h5',
        'netDBH5': 'netDB_GAN.h5'}
 
 def __conv_init(a):
-    print("conv_init", a)
+    logger.info("conv_init %s", a)
     k = RandomNormal(0, 0.02)(a) # for convolution kernel
     k.conv_weight = True
     return k
@@ -123,9 +128,9 @@ class GANModel():
         try:
             netGA.load_weights(str(self.model_dir / hdf['netGAH5']))
             netGB.load_weights(str(self.model_dir / hdf['netGBH5']))
-            print ("Generator models loaded.")
+            logger.info("Generator models loaded.")
         except:
-            print ("Generator weights files not found.")
+            logger.info("Generator weights files not found.")
             pass
 
         if self.gpus > 1:
@@ -155,15 +160,15 @@ class GANModel():
         try:
             netDA.load_weights(str(self.model_dir / hdf['netDAH5']))
             netDB.load_weights(str(self.model_dir / hdf['netDBH5']))
-            print ("Discriminator models loaded.")
+            logger.info("Discriminator models loaded.")
         except:
-            print ("Discriminator weights files not found.")
+            logger.info("Discriminator weights files not found.")
             pass
         return netDA, netDB
 
     def load(self, swapped):
         if swapped:
-            print("swapping not supported on GAN")
+            logger.warning("swapping not supported on GAN")
             # TODO load is done in __init__ => look how to swap if possible
         return True
 
@@ -179,4 +184,4 @@ class GANModel():
             self.netGB.save_weights(str(self.model_dir / hdf['netGBH5']))
         self.netDA.save_weights(str(self.model_dir / hdf['netDAH5']))
         self.netDB.save_weights(str(self.model_dir / hdf['netDBH5']))
-        print ("Models saved.")
+        logger.info("Models saved.")
diff --git a/plugins/model/Model_GAN128/Model.py b/plugins/model/Model_GAN128/Model.py
index fffa74f..5723636 100644
--- a/plugins/model/Model_GAN128/Model.py
+++ b/plugins/model/Model_GAN128/Model.py
@@ -1,5 +1,6 @@
 # Based on the https://github.com/shaoanlu/faceswap-GAN repo
 # source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb
+import logging
 
 from keras.models import Model
 from keras.layers import *
@@ -15,13 +16,16 @@ from lib.utils import backup_file
 
 from keras.utils import multi_gpu_model
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
 hdf = {'netGAH5':'netGA_GAN128.h5',
        'netGBH5': 'netGB_GAN128.h5',
        'netDAH5': 'netDA_GAN128.h5',
        'netDBH5': 'netDB_GAN128.h5'}
 
 def __conv_init(a):
-    print("conv_init", a)
+    logger.info("conv_init %s", a)
     k = RandomNormal(0, 0.02)(a) # for convolution kernel
     k.conv_weight = True
     return k
@@ -137,9 +141,9 @@ class GANModel():
         try:
             netGA.load_weights(str(self.model_dir / hdf['netGAH5']))
             netGB.load_weights(str(self.model_dir / hdf['netGBH5']))
-            print ("Generator models loaded.")
+            logger.info("Generator models loaded.")
         except:
-            print ("Generator weights files not found.")
+            logger.info("Generator weights files not found.")
             pass
 
         if self.gpus > 1:
@@ -173,15 +177,15 @@ class GANModel():
         try:
             netDA.load_weights(str(self.model_dir / hdf['netDAH5']))
             netDB.load_weights(str(self.model_dir / hdf['netDBH5']))
-            print ("Discriminator models loaded.")
+            logger.info("Discriminator models loaded.")
         except:
-            print ("Discriminator weights files not found.")
+            logger.info("Discriminator weights files not found.")
             pass
         return netDA, netDB
 
     def load(self, swapped):
         if swapped:
-            print("swapping not supported on GAN")
+            logger.warning("swapping not supported on GAN")
             # TODO load is done in __init__ => look how to swap if possible
         return True
 
@@ -197,4 +201,4 @@ class GANModel():
             self.netGB.save_weights(str(self.model_dir / hdf['netGBH5']))
         self.netDA.save_weights(str(self.model_dir / hdf['netDAH5']))
         self.netDB.save_weights(str(self.model_dir / hdf['netDBH5']))
-        print ("Models saved.")
+        logger.info("Models saved.")
diff --git a/plugins/model/Model_IAE/AutoEncoder.py b/plugins/model/Model_IAE/AutoEncoder.py
index dce65fe..3f012db 100644
--- a/plugins/model/Model_IAE/AutoEncoder.py
+++ b/plugins/model/Model_IAE/AutoEncoder.py
@@ -1,5 +1,7 @@
 # Improved-AutoEncoder base classes
 
+import logging
+
 from lib.utils import backup_file
 
 hdf = {'encoderH5': 'IAE_encoder.h5',
@@ -8,6 +10,8 @@ hdf = {'encoderH5': 'IAE_encoder.h5',
        'inter_BH5': 'IAE_inter_B.h5',
        'inter_bothH5': 'IAE_inter_both.h5'}
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class AutoEncoder:
     def __init__(self, model_dir, gpus):
@@ -31,11 +35,10 @@ class AutoEncoder:
             self.inter_both.load_weights(str(self.model_dir / hdf['inter_bothH5']))
             self.inter_A.load_weights(str(self.model_dir / face_A))
             self.inter_B.load_weights(str(self.model_dir / face_B))
-            print('loaded model weights')
+            logger.info('loaded model weights')
             return True
-        except Exception as e:
-            print('Failed loading existing training data.')
-            print(e)
+        except Exception:
+            logger.warning('Failed loading existing training data. Starting a fresh model: %s', self.model_dir)
             return False
 
     def save_weights(self):
@@ -47,4 +50,4 @@ class AutoEncoder:
         self.inter_both.save_weights(str(self.model_dir / hdf['inter_bothH5']))
         self.inter_A.save_weights(str(self.model_dir / hdf['inter_AH5']))
         self.inter_B.save_weights(str(self.model_dir / hdf['inter_BH5']))
-        print('saved model weights')
+        logger.info('saved model weights')
diff --git a/plugins/model/Model_LowMem/AutoEncoder.py b/plugins/model/Model_LowMem/AutoEncoder.py
index 2fb148d..997d564 100644
--- a/plugins/model/Model_LowMem/AutoEncoder.py
+++ b/plugins/model/Model_LowMem/AutoEncoder.py
@@ -1,4 +1,5 @@
 # AutoEncoder base classes
+import logging
 
 from lib.utils import backup_file
 
@@ -6,6 +7,8 @@ hdf = {'encoderH5': 'lowmem_encoder.h5',
        'decoder_AH5': 'lowmem_decoder_A.h5',
        'decoder_BH5': 'lowmem_decoder_B.h5'}
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 #Part of Filename migration, should be remopved some reasonable time after first added
 import os.path
 old_encoderH5 = 'encoder.h5'
@@ -30,23 +33,22 @@ class AutoEncoder:
         try:
             #Part of Filename migration, should be remopved some reasonable time after first added
             if os.path.isfile(str(self.model_dir / old_encoderH5)):
-                print('Migrating to new filenames: ', end='')
+                logger.info('Migrating to new filenames:')
                 if os.path.isfile(str(self.model_dir / hdf['encoderH5'])) is not True:
                     os.rename(str(self.model_dir / old_decoder_AH5), str(self.model_dir / hdf['decoder_AH5']))
                     os.rename(str(self.model_dir / old_decoder_BH5), str(self.model_dir / hdf['decoder_BH5']))
                     os.rename(str(self.model_dir / old_encoderH5), str(self.model_dir / hdf['encoderH5']))
-                    print('Complete')
+                    logger.info('Complete')
                 else:
-                    print('Failed due to existing files in folder.  Loading already migrated files')
+                    logger.warning('Failed due to existing files in folder.  Loading already migrated files')
             #End filename migration
             self.encoder.load_weights(str(self.model_dir / hdf['encoderH5']))
             self.decoder_A.load_weights(str(self.model_dir / face_A))
             self.decoder_B.load_weights(str(self.model_dir / face_B))
-            print('loaded model weights')
+            logger.info('loaded model weights')
             return True
         except Exception as e:
-            print('Failed loading existing training data.')
-            print(e)
+            logger.warning('Failed loading existing training data. Starting a fresh model: %s', self.model_dir)
             return False
 
     def save_weights(self):
@@ -56,4 +58,4 @@ class AutoEncoder:
         self.encoder.save_weights(str(self.model_dir / hdf['encoderH5']))
         self.decoder_A.save_weights(str(self.model_dir / hdf['decoder_AH5']))
         self.decoder_B.save_weights(str(self.model_dir / hdf['decoder_BH5']))
-        print('saved model weights')
+        logger.info('saved model weights')
diff --git a/plugins/model/Model_Original/AutoEncoder.py b/plugins/model/Model_Original/AutoEncoder.py
index 3611910..a0c4f79 100644
--- a/plugins/model/Model_Original/AutoEncoder.py
+++ b/plugins/model/Model_Original/AutoEncoder.py
@@ -1,4 +1,5 @@
 # AutoEncoder base classes
+import logging
 
 from lib.utils import backup_file
 from lib import Serializer
@@ -8,6 +9,8 @@ hdf = {'encoderH5': 'encoder.h5',
        'decoder_AH5': 'decoder_A.h5',
        'decoder_BH5': 'decoder_B.h5',
        'state': 'state'}
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class AutoEncoder:
     def __init__(self, model_dir, gpus):
@@ -22,30 +25,29 @@ class AutoEncoder:
 
     def load(self, swapped):
         serializer = Serializer.get_serializer('json')
-        state_fn = ".".join([hdf['state'], serializer.ext]) 
+        state_fn = ".".join([hdf['state'], serializer.ext])
         try:
             with open(str(self.model_dir / state_fn), 'rb') as fp:
                 state = serializer.unmarshal(fp.read().decode('utf-8'))
                 self._epoch_no = state['epoch_no']
         except IOError as e:
-            print('Error loading training info:', e.strerror)
+            logger.warning('Error loading training info: %s', str(e.strerror))
             self._epoch_no = 0
         except JSONDecodeError as e:
-            print('Error loading training info:', e.msg)
-            self._epoch_no = 0   
-        
-        (face_A,face_B) = (hdf['decoder_AH5'], hdf['decoder_BH5']) if not swapped else (hdf['decoder_BH5'], hdf['decoder_AH5'])                
+            logger.warning('Error loading training info: %s', str(e.msg))
+            self._epoch_no = 0
+
+        (face_A,face_B) = (hdf['decoder_AH5'], hdf['decoder_BH5']) if not swapped else (hdf['decoder_BH5'], hdf['decoder_AH5'])
 
         try:
             self.encoder.load_weights(str(self.model_dir / hdf['encoderH5']))
             self.decoder_A.load_weights(str(self.model_dir / face_A))
             self.decoder_B.load_weights(str(self.model_dir / face_B))
-            print('loaded model weights')
+            logger.info('loaded model weights')
             return True
         except Exception as e:
-            print('Failed loading existing training data.')
-            print(e)
-            return False                            
+            logger.warning('Failed loading existing training data. Starting a fresh model: %s', self.model_dir)
+            return False
 
     def save_weights(self):
         model_dir = str(self.model_dir)
@@ -54,12 +56,12 @@ class AutoEncoder:
         self.encoder.save_weights(str(self.model_dir / hdf['encoderH5']))
         self.decoder_A.save_weights(str(self.model_dir / hdf['decoder_AH5']))
         self.decoder_B.save_weights(str(self.model_dir / hdf['decoder_BH5']))
-        
-        print('saved model weights')
-        
+
+        logger.info('saved model weights')
+
         serializer = Serializer.get_serializer('json')
         state_fn = ".".join([hdf['state'], serializer.ext])
-        state_dir = str(self.model_dir / state_fn)                        
+        state_dir = str(self.model_dir / state_fn)
         try:
             with open(state_dir, 'wb') as fp:
                 state_json = serializer.marshal({
@@ -67,11 +69,9 @@ class AutoEncoder:
                      })
                 fp.write(state_json.encode('utf-8'))
         except IOError as e:
-            print(e.strerror)                   
+            logger.error(e.strerror)
 
     @property
     def epoch_no(self):
         "Get current training epoch number"
         return self._epoch_no
-        
-        
diff --git a/plugins/model/Model_OriginalHighRes/Model.py b/plugins/model/Model_OriginalHighRes/Model.py
index cb4a719..804d948 100644
--- a/plugins/model/Model_OriginalHighRes/Model.py
+++ b/plugins/model/Model_OriginalHighRes/Model.py
@@ -7,6 +7,7 @@
 
 
 import enum
+import logging
 import os
 import sys
 import warnings
@@ -26,23 +27,23 @@ from lib.PixelShuffler import PixelShuffler
 import lib.Serializer
 from lib.utils import backup_file
 
-from . import __version__    
+from . import __version__
 from .instance_normalization import InstanceNormalization
 
 
 if isinstance(__version__, (list, tuple)):
     version_str = ".".join([str(n) for n in __version__[1:]])
-else: 
+else:
     version_str = __version__
 
-
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 mswindows = sys.platform=="win32"
 
 
 class EncoderType(enum.Enum):
     ORIGINAL = "original"
-    SHAOANLU = "shaoanlu"    
-            
+    SHAOANLU = "shaoanlu"
+
 
 _kern_init = RandomNormal(0, 0.02)
 
@@ -59,119 +60,119 @@ hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
        'decoder_BH5': 'decoder_B_{version_str}{ENCODER.value}.h5'.format(**vars())}
 
 class Model():
-    
-    ENCODER_DIM = 1024 # dense layer size        
+
+    ENCODER_DIM = 1024 # dense layer size
     IMAGE_SHAPE = 128, 128 # image shape
-    
+
     assert [n for n in IMAGE_SHAPE if n>=16]
-    
+
     IMAGE_WIDTH = max(IMAGE_SHAPE)
     IMAGE_WIDTH = (IMAGE_WIDTH//16 + (1 if (IMAGE_WIDTH%16)>=8 else 0))*16
     IMAGE_SHAPE = IMAGE_WIDTH, IMAGE_WIDTH, len('BRG') # good to let ppl know what these are...
-    
-    
+
+
     def __init__(self, model_dir, gpus, encoder_type=ENCODER):
-                
-        if mswindows:  
-            from ctypes import cdll    
+
+        if mswindows:
+            from ctypes import cdll
             mydll = cdll.LoadLibrary("user32.dll")
-            mydll.SetProcessDPIAware(True)                               
-        
+            mydll.SetProcessDPIAware(True)
+
         self._encoder_type = encoder_type
-        
+
         self.model_dir = model_dir
-        
+
         # can't chnage gpu's when the model is initialized no point in making it r/w
-        self._gpus = gpus 
-        
+        self._gpus = gpus
+
         Encoder = getattr(self, "Encoder_{}".format(self._encoder_type.value))
         Decoder = getattr(self, "Decoder_{}".format(self._encoder_type.value))
-        
+
         self.encoder = Encoder()
         self.decoder_A = Decoder()
         self.decoder_B = Decoder()
-        
-        self.initModel()        
 
-    
+        self.initModel()
+
+
     def initModel(self):
         optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)
-        
+
         x = Input(shape=self.IMAGE_SHAPE)
-        
+
         self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))
         self.autoencoder_B = KerasModel(x, self.decoder_B(self.encoder(x)))
-        
+
         if self.gpus > 1:
             self.autoencoder_A = multi_gpu_model( self.autoencoder_A , self.gpus)
             self.autoencoder_B = multi_gpu_model( self.autoencoder_B , self.gpus)
-        
-        
+
+
         self.autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')
-        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')                    
-        
-        
-    def load(self, swapped):        
+        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')
+
+
+    def load(self, swapped):
         model_dir = str(self.model_dir)
-        
+
         from json import JSONDecodeError
         face_A, face_B = (hdf['decoder_AH5'], hdf['decoder_BH5']) if not swapped else (hdf['decoder_BH5'], hdf['decoder_AH5'])
-        
+
         state_dir = os.path.join(model_dir, 'state_{version_str}_{ENCODER.value}.json'.format(**globals()))
         ser = lib.Serializer.get_serializer('json')
-        
+
         try:
             with open(state_dir, 'rb') as fp:
                 state = ser.unmarshal(fp.read().decode('utf-8'))
                 self._epoch_no = state['epoch_no']
         except IOError as e:
-            print('Error loading training info:', e.strerror)
+            logger.warning('Error loading training info: %s', str(e.strerror))
             self._epoch_no = 0
         except JSONDecodeError as e:
-            print('Error loading training info:', e.msg)
-            self._epoch_no = 0                         
+            logger.warning('Error loading training info: %s', str(e.msg))
+            self._epoch_no = 0
 
-        try:            
+        try:
             self.encoder.load_weights(os.path.join(model_dir, hdf['encoderH5']))
             self.decoder_A.load_weights(os.path.join(model_dir, face_A))
             self.decoder_B.load_weights(os.path.join(model_dir, face_B))
-            print('loaded model weights')
+            logger.info('loaded model weights')
             return True
         except IOError as e:
-            print('Failed loading training data:', e.strerror)            
+            logger.warning('Error loading training info: %s', str(e.strerror))
         except Exception as e:
-            print('Failed loading training data:', str(e))            
-      
+            logger.warning('Error loading training info: %s', str(e))
+
         return False
 
     def converter(self, swap):
         autoencoder = self.autoencoder_B if not swap else self.autoencoder_A
         return autoencoder.predict
-    
-    
+
+
     def conv(self, filters, kernel_size=5, strides=2, **kwargs):
         def block(x):
-            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)         
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             x = LeakyReLU(0.1)(x)
             return x
-        return block   
+        return block
 
     def conv_sep(self, filters, kernel_size=5, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
             x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             x = Activation("relu")(x)
-            return x    
-        return block 
-        
+            return x
+        return block
+
     def conv_inst_norm(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)        
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             if use_instance_norm:
                 x = inst_norm()(x)
             x = Activation("relu")(x)
-            return x    
+            return x
         return block
-    
+
     def upscale(self, filters, **kwargs):
         def block(x):
             x = Conv2D(filters * 4, kernel_size=3, padding='same',
@@ -179,93 +180,93 @@ class Model():
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
-        return block  
-    
+        return block
+
     def upscale_inst_norm(self, filters, use_instance_norm=True, **kwargs):
         def block(x):
-            x = Conv2D(filters*4, kernel_size=3, use_bias=False, 
+            x = Conv2D(filters*4, kernel_size=3, use_bias=False,
                        kernel_initializer=_kern_init, padding='same', **kwargs)(x)
             if use_instance_norm:
                 x = inst_norm()(x)
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
-        return block    
-    
+        return block
+
     def Encoder_original(self, **kwargs):
         impt = Input(shape=self.IMAGE_SHAPE)
-        
+
         in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
 
         x = self.conv(in_conv_filters)(impt)
         x = self.conv_sep(256)(x)
         x = self.conv(512)(x)
         x = self.conv_sep(1024)(x)
-        
-        dense_shape = self.IMAGE_SHAPE[0] // 16         
+
+        dense_shape = self.IMAGE_SHAPE[0] // 16
         x = Dense(self.ENCODER_DIM, kernel_initializer=_kern_init)(Flatten()(x))
         x = Dense(dense_shape * dense_shape * 512, kernel_initializer=_kern_init)(x)
         x = Reshape((dense_shape, dense_shape, 512))(x)
         x = self.upscale(512)(x)
-        
-        return KerasModel(impt, x, **kwargs)    
-          
-    
+
+        return KerasModel(impt, x, **kwargs)
+
+
     def Encoder_shaoanlu(self, **kwargs):
         impt = Input(shape=self.IMAGE_SHAPE)
-                
+
         in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
-        
+
         x = Conv2D(in_conv_filters, kernel_size=5, use_bias=False, padding="same")(impt)
         x = self.conv_inst_norm(in_conv_filters+32, use_instance_norm=False)(x)
-        x = self.conv_inst_norm(256)(x)        
+        x = self.conv_inst_norm(256)(x)
         x = self.conv_inst_norm(512)(x)
-        x = self.conv_inst_norm(1024)(x)        
-        
-        dense_shape = self.IMAGE_SHAPE[0] // 16         
+        x = self.conv_inst_norm(1024)(x)
+
+        dense_shape = self.IMAGE_SHAPE[0] // 16
         x = Dense(self.ENCODER_DIM, kernel_initializer=_kern_init)(Flatten()(x))
         x = Dense(dense_shape * dense_shape * 768, kernel_initializer=_kern_init)(x)
         x = Reshape((dense_shape, dense_shape, 768))(x)
         x = self.upscale(512)(x)
-        
-        return KerasModel(impt, x, **kwargs)    
+
+        return KerasModel(impt, x, **kwargs)
 
 
-    def Decoder_original(self):       
-        decoder_shape = self.IMAGE_SHAPE[0]//8        
+    def Decoder_original(self):
+        decoder_shape = self.IMAGE_SHAPE[0]//8
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
-        
+
         x = self.upscale(384)(inpt)
         x = self.upscale(256-32)(x)
         x = self.upscale(self.IMAGE_SHAPE[0])(x)
-        
+
         x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
-        
+
         return KerasModel(inpt, x)
-    
-    
-    def Decoder_shaoanlu(self):       
-        decoder_shape = self.IMAGE_SHAPE[0]//8        
+
+
+    def Decoder_shaoanlu(self):
+        decoder_shape = self.IMAGE_SHAPE[0]//8
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
-        
+
         x = self.upscale_inst_norm(512)(inpt)
         x = self.upscale_inst_norm(256)(x)
         x = self.upscale_inst_norm(self.IMAGE_SHAPE[0])(x)
-        
+
         x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
-        
-        return KerasModel(inpt, x)    
+
+        return KerasModel(inpt, x)
 
 
-    def save_weights(self):        
+    def save_weights(self):
         model_dir = str(self.model_dir)
-        
+
         try:
-            for model in hdf.values():            
+            for model in hdf.values():
                 backup_file(model_dir, model)
         except NameError:
-            print('backup functionality not available\n')       
-            
+            logger.error('backup functionality not available\n')
+
         state_dir = os.path.join(model_dir, 'state_{version_str}_{ENCODER.value}.json'.format(**globals()))
         ser = lib.Serializer.get_serializer('json')
         try:
@@ -275,37 +276,37 @@ class Model():
                      })
                 fp.write(state_json.encode('utf-8'))
         except IOError as e:
-            print(e.strerror)                   
-        
-        print('\nsaving model weights', end='', flush=True)        
-        
-        from concurrent.futures import ThreadPoolExecutor, as_completed        
-        
+            logger.error(e.strerror)
+
+        logger.info('saving model weights')
+
+        from concurrent.futures import ThreadPoolExecutor, as_completed
+
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = [executor.submit(getattr(self, mdl_name.rstrip('H5')).save_weights, str(self.model_dir / mdl_H5_fn)) for mdl_name, mdl_H5_fn in hdf.items()]
             for future in as_completed(futures):
                 future.result()
-                print('.', end='', flush=True)  
+                print('.', end='', flush=True)
+
+        logger.info('done')
+
 
-        print('done', flush=True)              
-    
-                           
     @property
     def gpus(self):
         return self._gpus
-    
+
     @property
     def model_name(self):
         try:
             return self._model_name
         except AttributeError:
             import inspect
-            self._model_name = os.path.dirname(inspect.getmodule(self).__file__).rsplit("_", 1)[1]            
+            self._model_name = os.path.dirname(inspect.getmodule(self).__file__).rsplit("_", 1)[1]
         return self._model_name
-             
-    
+
+
     def __str__(self):
-        return "<{}: ver={}, dense_dim={}, img_size={}>".format(self.model_name, 
-                                                              version_str, 
-                                                              self.ENCODER_DIM, 
+        return "<{}: ver={}, dense_dim={}, img_size={}>".format(self.model_name,
+                                                              version_str,
+                                                              self.ENCODER_DIM,
                                                               "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))
diff --git a/plugins/plugin_loader.py b/plugins/plugin_loader.py
index a1316cb..188d922 100644
--- a/plugins/plugin_loader.py
+++ b/plugins/plugin_loader.py
@@ -1,9 +1,12 @@
 #!/usr/bin/env python3
 """ Plugin loader for extract, training and model tasks """
 
+import logging
 import os
 from importlib import import_module
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class PluginLoader():
     """ Plugin loader for extract, training and model tasks """
@@ -36,7 +39,7 @@ class PluginLoader():
     def _import(attr, name):
         """ Import the plugin's module """
         ttl = attr.split(".")[-1].title()
-        print("Loading {} from {} plugin...".format(ttl, name.title()))
+        logger.info("Loading %s from %s plugin...", ttl, name.title())
         attr = "model" if attr == "Trainer" else attr.lower()
         mod = ".".join(("plugins", attr, name))
         module = import_module(mod)
diff --git a/scripts/convert.py b/scripts/convert.py
index 73e8239..fe5a441 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -1,6 +1,7 @@
 #!/usr/bin python3
 """ The script to run the convert process of faceswap """
 
+import logging
 import re
 import os
 import sys
@@ -17,6 +18,8 @@ from lib.utils import get_folder, get_image_paths
 
 from plugins.plugin_loader import PluginLoader
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Convert():
     """ The convert process. """
@@ -30,7 +33,7 @@ class Convert():
         self.alignments = Alignments(self.args, False)
 
         # Update Legacy alignments
-        Legacy(self.alignments, self.args.verbose, self.images.input_images)
+        Legacy(self.alignments, self.images.input_images)
 
         self.post_process = PostProcess(arguments)
         self.verify_output = False
@@ -42,7 +45,7 @@ class Convert():
 
             Note: GAN prediction outputs a mask + an image, while other
             predicts only an image. """
-        Utils.set_verbosity(self.args.verbose)
+        Utils.set_verbosity()
 
         if not self.alignments.have_alignments_file:
             self.load_extractor()
@@ -64,29 +67,29 @@ class Convert():
 
     def load_extractor(self):
         """ Set on the fly extraction """
-        print("\nNo Alignments file found. Extracting on the fly.\n"
-              "NB: This will use the inferior dlib-hog for extraction "
-              "and dlib pose predictor for landmarks.\nIt is recommended "
-              "to perfom Extract first for superior results\n")
+        logger.warning("No Alignments file found. Extracting on the fly.")
+        logger.warning("NB: This will use the inferior dlib-hog for extraction "
+                       "and dlib pose predictor for landmarks. It is recommended "
+                       "to perfom Extract first for superior results")
         for task in ("load", "detect", "align"):
             queue_manager.add_queue(task, maxsize=0)
 
-        detector = PluginLoader.get_detector("dlib_hog")(
-            verbose=self.args.verbose)
-        aligner = PluginLoader.get_aligner("dlib")(verbose=self.args.verbose)
+        detector = PluginLoader.get_detector("dlib_hog")(loglevel=self.args.loglevel)
+        aligner = PluginLoader.get_aligner("dlib")(loglevel=self.args.loglevel)
 
         d_kwargs = {"in_queue": queue_manager.get_queue("load"),
                     "out_queue": queue_manager.get_queue("detect")}
         a_kwargs = {"in_queue": queue_manager.get_queue("detect"),
                     "out_queue": queue_manager.get_queue("align")}
 
-        d_process = SpawnProcess()
+        d_process = SpawnProcess(detector.run, **d_kwargs)
         d_event = d_process.event
-        a_process = SpawnProcess()
+        d_process.start()
+
+        a_process = SpawnProcess(aligner.run, **a_kwargs)
         a_event = a_process.event
+        a_process.start()
 
-        d_process.in_process(detector.detect_faces, **d_kwargs)
-        a_process.in_process(aligner.align, **a_kwargs)
         d_event.wait(10)
         if not d_event.is_set():
             raise ValueError("Error inititalizing Detector")
@@ -105,8 +108,8 @@ class Convert():
         model = PluginLoader.get_model(model_name)(model_dir, num_gpus)
 
         if not model.load(self.args.swap_model):
-            print("Model Not Found! A valid model "
-                  "must be provided to continue!")
+            logger.error("Model Not Found! A valid model "
+                         "must be provided to continue!")
             exit(1)
 
         return model
@@ -161,9 +164,8 @@ class Convert():
 
             if faces_count > 1:
                 self.verify_output = True
-                if self.args.verbose:
-                    print("Warning: found more than one face in "
-                          "an image! {}".format(frame))
+                logger.verbose("Found more than one face in "
+                               "an image! '%s'", frame)
 
             yield filename, image, detected_faces
 
@@ -211,8 +213,7 @@ class Convert():
                 filename = str(self.output_dir / Path(filename).name)
                 cv2.imwrite(filename, image)  # pylint: disable=no-member
         except Exception as err:
-            print("Failed to convert image: {}. "
-                  "Reason: {}".format(filename, err))
+            logger.error("Failed to convert image: '%s'. Reason: %s", filename, err)
             raise
 
     def convert_one_face(self, converter, imagevars):
@@ -253,21 +254,19 @@ class OptionalActions():
         input_aligned_dir = self.args.input_aligned_dir
 
         if input_aligned_dir is None:
-            print("Aligned directory not specified. All faces listed in the "
-                  "alignments file will be converted")
+            logger.info("Aligned directory not specified. All faces listed in the "
+                        "alignments file will be converted")
         elif not os.path.isdir(input_aligned_dir):
-            print("Aligned directory not found. All faces listed in the "
-                  "alignments file will be converted")
+            logger.warning("Aligned directory not found. All faces listed in the "
+                           "alignments file will be converted")
         else:
             faces_to_swap = [Path(path)
                              for path in get_image_paths(input_aligned_dir)]
             if not faces_to_swap:
-                print("Aligned directory is empty, "
-                      "no faces will be converted!")
+                logger.warning("Aligned directory is empty, no faces will be converted!")
             elif len(faces_to_swap) <= len(self.input_images) / 3:
-                print("Aligned directory contains an amount of images much "
-                      "less than the input, are you sure this is the right "
-                      "directory?")
+                logger.warning("Aligned directory contains an amount of images much less than "
+                               "the input, are you sure this is the right directory?")
         return faces_to_swap
 
     # SKIP FRAME RANGES #
@@ -304,8 +303,8 @@ class OptionalActions():
         face_file = Path(self.args.input_aligned_dir) / Path(face_name)
         skip_face = face_file not in self.faces_to_swap
         if skip_face:
-            print("face {} for frame {} was deleted, skipping".format(
-                face_idx, os.path.basename(filename)))
+            logger.info("face %s for frame '%s' was deleted, skipping",
+                        face_idx, os.path.basename(filename))
         return skip_face
 
 
@@ -315,8 +314,7 @@ class Legacy():
         - Add frame dimensions
         - Rotate landmarks and bounding boxes on legacy alignments
         and remove the 'r' parameter """
-    def __init__(self, alignments, verbose, frames):
-        self.verbose = verbose
+    def __init__(self, alignments, frames):
         self.alignments = alignments
         self.frames = {os.path.basename(frame): frame
                        for frame in frames}
@@ -329,10 +327,10 @@ class Legacy():
         if not no_dims and not rotated:
             return
         if no_dims:
-            print("Legacy landmarks found. Adding frame dimensions...")
+            logger.info("Legacy landmarks found. Adding frame dimensions...")
             self.add_dimensions(no_dims)
         if rotated:
-            print("Legacy rotated frames found. Converting...")
+            logger.info("Legacy rotated frames found. Converting...")
             self.rotate_landmarks(rotated)
         self.alignments.save()
 
diff --git a/scripts/extract.py b/scripts/extract.py
index d5fc15c..2d5e41b 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -1,6 +1,7 @@
 #!/usr/bin python3
 """ The script to run the extract process of faceswap """
 
+import logging
 import os
 import sys
 from pathlib import Path
@@ -17,15 +18,16 @@ from plugins.plugin_loader import PluginLoader
 from scripts.fsmedia import Alignments, Images, PostProcess, Utils
 
 tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class Extract():
     """ The extract process. """
-
     def __init__(self, arguments):
+        logger.debug("Initializing %s: (args: %s", self.__class__.__name__, arguments)
         self.args = arguments
         self.output_dir = get_folder(self.args.output_dir)
-        print("Output Directory: {}".format(self.args.output_dir))
+        logger.info("Output Directory: %s", self.args.output_dir)
         self.images = Images(self.args)
         self.alignments = Alignments(self.args, True)
         self.plugins = Plugins(self.args)
@@ -37,11 +39,12 @@ class Extract():
         self.save_interval = None
         if hasattr(self.args, "save_interval"):
             self.save_interval = self.args.save_interval
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def process(self):
         """ Perform the extraction process """
-        print('Starting, this may take a while...')
-        Utils.set_verbosity(self.args.verbose)
+        logger.info('Starting, this may take a while...')
+        Utils.set_verbosity()
 #        queue_manager.debug_monitor(1)
         self.threaded_io("load")
         save_thread = self.threaded_io("save")
@@ -50,11 +53,10 @@ class Extract():
         Utils.finalize(self.images.images_found,
                        self.alignments.faces_count,
                        self.verify_output)
-        self.plugins.process_detect.join()
-        self.plugins.process_align.join()
 
     def threaded_io(self, task, io_args=None):
         """ Load images in a background thread """
+        logger.debug("Threading task: (Task: '%s')", task)
         io_args = tuple() if io_args is None else (io_args, )
         if task == "load":
             func = self.load_images
@@ -62,38 +64,59 @@ class Extract():
             func = self.save_faces
         elif task == "reload":
             func = self.reload_images
-        io_thread = MultiThread(thread_count=1)
-        io_thread.in_thread(func, *io_args)
+        io_thread = MultiThread(func, *io_args, thread_count=1)
+        io_thread.start()
         return io_thread
 
     def load_images(self):
         """ Load the images """
+        logger.debug("Load Images: Start")
         load_queue = queue_manager.get_queue("load")
         for filename, image in self.images.load():
+            if load_queue.shutdown.is_set():
+                logger.debug("Load Queue: Stop signal received. Terminating")
+                break
             imagename = os.path.basename(filename)
             if imagename in self.alignments.data.keys():
+                logger.trace("Skipping image: '%s'", filename)
                 continue
-            load_queue.put((filename, image))
+            item = {"filename": filename,
+                    "image": image}
+            load_queue.put(item)
         load_queue.put("EOF")
+        logger.debug("Load Images: Complete")
 
     def reload_images(self, detected_faces):
         """ Reload the images and pair to detected face """
+        logger.debug("Reload Images: Start. Detected Faces Count: %s", len(detected_faces))
         load_queue = queue_manager.get_queue("detect")
         for filename, image in self.images.load():
+            if load_queue.shutdown.is_set():
+                logger.debug("Reload Queue: Stop signal received. Terminating")
+                break
+            logger.trace("Reloading image: '%s'", filename)
             detect_item = detected_faces.pop(filename, None)
             if not detect_item:
+                logger.warning("Couldn't find faces for: %s", filename)
                 continue
             detect_item["image"] = image
             load_queue.put(detect_item)
         load_queue.put("EOF")
+        logger.debug("Reload Images: Complete")
 
     def save_faces(self):
         """ Save the generated faces """
+        logger.debug("Save Faces: Start")
         if not self.export_face:
+            logger.debug("Not exporting faces")
+            logger.debug("Save Faces: Complete")
             return
 
         save_queue = queue_manager.get_queue("save")
         while True:
+            if save_queue.shutdown.is_set():
+                logger.debug("Save Queue: Stop signal received. Terminating")
+                break
             item = save_queue.get()
             if item == "EOF":
                 break
@@ -101,20 +124,28 @@ class Extract():
             out_filename = "{}_{}{}".format(str(output_file),
                                             str(idx),
                                             Path(filename).suffix)
-            # pylint: disable=no-member
-            cv2.imwrite(out_filename, resized_face)
+            logger.trace("Saving face: '%s'", out_filename)
+            try:
+                cv2.imwrite(out_filename, resized_face)  # pylint: disable=no-member
+            except Exception as err:  # pylint: disable=broad-except
+                logger.error("Failed to save image '%s'. Original Error: %s", out_filename, err)
+                continue
+        logger.debug("Save Faces: Complete")
 
     def run_extraction(self, save_thread):
         """ Run Face Detection """
+        save_queue = queue_manager.get_queue("save")
         to_process = self.process_item_count()
         frame_no = 0
         size = self.args.size if hasattr(self.args, "size") else 256
         align_eyes = self.args.align_eyes if hasattr(self.args, "align_eyes") else False
 
         if self.plugins.is_parallel:
+            logger.debug("Using parallel processing")
             self.plugins.launch_aligner()
             self.plugins.launch_detector()
         if not self.plugins.is_parallel:
+            logger.debug("Using serial processing")
             self.run_detection(to_process)
             self.plugins.launch_aligner()
 
@@ -123,24 +154,20 @@ class Extract():
                           file=sys.stdout,
                           desc="Extracting faces"):
 
-            exception = faces.get("exception", False)
-            if exception:
-                exit(1)
             filename = faces["filename"]
-            faces["output_file"] = self.output_dir / Path(filename).stem
 
-            self.align_face(faces, align_eyes, size)
+            self.align_face(faces, align_eyes, size, filename)
             self.post_process.do_actions(faces)
 
             faces_count = len(faces["detected_faces"])
-            if self.args.verbose and faces_count == 0:
-                print("Warning: No faces were detected in image: "
-                      "{}".format(os.path.basename(filename)))
+            if faces_count == 0:
+                logger.verbose("No faces were detected in image: %s",
+                               os.path.basename(filename))
 
             if not self.verify_output and faces_count > 1:
                 self.verify_output = True
 
-            self.process_faces(filename, faces)
+            self.process_faces(filename, faces, save_queue)
 
             frame_no += 1
             if frame_no == self.save_interval:
@@ -148,22 +175,24 @@ class Extract():
                 frame_no = 0
 
         if self.export_face:
-            queue_manager.get_queue("save").put("EOF")
-        save_thread.join_threads()
+            save_queue.put("EOF")
+        save_thread.join()
 
     def process_item_count(self):
         """ Return the number of items to be processedd """
         processed = sum(os.path.basename(frame) in self.alignments.data.keys()
                         for frame in self.images.input_images)
+        logger.debug("Items already processed: %s", processed)
 
         if processed != 0 and self.args.skip_existing:
-            print("Skipping {} previously extracted frames".format(processed))
+            logger.info("Skipping previously extracted frames: %s", processed)
         if processed != 0 and self.args.skip_faces:
-            print("Skipping {} frames with detected faces".format(processed))
+            logger.info("Skipping frames with detected faces: %s", processed)
 
         to_process = self.images.images_found - processed
+        logger.debug("Items to be Processed: %s", to_process)
         if to_process == 0:
-            print("No frames to process. Exiting")
+            logger.error("No frames to process. Exiting")
             queue_manager.terminate_queues()
             exit(0)
         return to_process
@@ -187,9 +216,8 @@ class Extract():
 
         self.threaded_io("reload", detected_faces)
 
-    @staticmethod
-    def align_face(faces, align_eyes, size, padding=48):
-        """ Align the detected face """
+    def align_face(self, faces, align_eyes, size, filename, padding=48):
+        """ Align the detected face and add the destination file path """
         final_faces = list()
         image = faces["image"]
         landmarks = faces["landmarks"]
@@ -203,30 +231,30 @@ class Extract():
                                        size=size,
                                        padding=padding,
                                        align_eyes=align_eyes)
-            final_faces.append(detected_face)
+            final_faces.append({"file_location": self.output_dir / Path(filename).stem,
+                                "face": detected_face})
         faces["detected_faces"] = final_faces
 
-    def process_faces(self, filename, faces):
+    def process_faces(self, filename, faces, save_queue):
         """ Perform processing on found faces """
         final_faces = list()
-        save_queue = queue_manager.get_queue("save")
         filename = faces["filename"]
-        output_file = faces["output_file"]
 
-        for idx, face in enumerate(faces["detected_faces"]):
+        for idx, detected_face in enumerate(faces["detected_faces"]):
             if self.export_face:
                 save_queue.put((filename,
-                                output_file,
-                                face.aligned_face,
+                                detected_face["file_location"],
+                                detected_face["face"].aligned_face,
                                 idx))
 
-            final_faces.append(face.to_alignment())
+            final_faces.append(detected_face["face"].to_alignment())
         self.alignments.data[os.path.basename(filename)] = final_faces
 
 
 class Plugins():
     """ Detector and Aligner Plugins and queues """
     def __init__(self, arguments):
+        logger.debug("Initializing %s", self.__class__.__name__)
         self.args = arguments
         self.detector = self.load_detector()
         self.aligner = self.load_aligner()
@@ -235,6 +263,7 @@ class Plugins():
         self.process_detect = None
         self.process_align = None
         self.add_queues()
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def set_parallel_processing(self):
         """ Set whether to run detect and align together or seperately """
@@ -244,24 +273,25 @@ class Plugins():
         if (detector_vram == 0
                 or aligner_vram == 0
                 or gpu_stats.device_count == 0):
+            logger.debug("At least one of aligner or detector have no VRAM requirement. "
+                         "Enabling parallel processing.")
             return True
 
         if hasattr(self.args, "multiprocess") and not self.args.multiprocess:
-            print("\nNB: Parallel processing disabled.\nYou may get faster "
-                  "extraction speeds by enabling it with the -mp switch\n")
+            logger.info("NB: Parallel processing disabled.You may get faster "
+                        "extraction speeds by enabling it with the -mp switch")
             return False
 
         required_vram = detector_vram + aligner_vram + 320  # 320MB buffer
         stats = gpu_stats.get_card_most_free()
         free_vram = int(stats["free"])
-        if self.args.verbose:
-            print("{} - {}MB free of {}MB".format(stats["device"],
-                                                  free_vram,
-                                                  int(stats["total"])))
+        logger.verbose("%s - %sMB free of %sMB",
+                       stats["device"],
+                       free_vram,
+                       int(stats["total"]))
         if free_vram <= required_vram:
-            if self.args.verbose:
-                print("Not enough free VRAM for parallel processing. "
-                      "Switching to serial")
+            logger.warning("Not enough free VRAM for parallel processing. "
+                           "Switching to serial")
             return False
         return True
 
@@ -276,14 +306,14 @@ class Plugins():
     def load_detector(self):
         """ Set global arguments and load detector plugin """
         detector_name = self.args.detector.replace("-", "_").lower()
-
+        logger.debug("Loading Detector: '%s'", detector_name)
         # Rotation
         rotation = None
         if hasattr(self.args, "rotate_images"):
             rotation = self.args.rotate_images
 
         detector = PluginLoader.get_detector(detector_name)(
-            verbose=self.args.verbose,
+            loglevel=self.args.loglevel,
             rotation=rotation)
 
         return detector
@@ -291,22 +321,23 @@ class Plugins():
     def load_aligner(self):
         """ Set global arguments and load aligner plugin """
         aligner_name = self.args.aligner.replace("-", "_").lower()
+        logger.debug("Loading Aligner: '%s'", aligner_name)
 
         aligner = PluginLoader.get_aligner(aligner_name)(
-            verbose=self.args.verbose)
+            loglevel=self.args.loglevel)
 
         return aligner
 
     def launch_aligner(self):
         """ Launch the face aligner """
+        logger.debug("Launching Aligner")
         out_queue = queue_manager.get_queue("align")
         kwargs = {"in_queue": queue_manager.get_queue("detect"),
                   "out_queue": out_queue}
 
-        self.process_align = SpawnProcess()
+        self.process_align = SpawnProcess(self.aligner.run, **kwargs)
         event = self.process_align.event
-
-        self.process_align.in_process(self.aligner.align, **kwargs)
+        self.process_align.start()
 
         # Wait for Aligner to take it's VRAM
         # The first ever load of the model for FAN has reportedly taken
@@ -314,24 +345,13 @@ class Plugins():
         # TODO investigate why this is and fix if possible
         event.wait(300)
         if not event.is_set():
-            raise ValueError("Error inititalizing Aligner")
-
-        try:
-            err = None
-            err = out_queue.get(True, 1)
-        except QueueEmpty:
-            pass
-
-        if err:
-            if isinstance(err, str):
-                queue_manager.terminate_queues()
-                print(err)
-                exit(1)
-            else:
-                queue_manager.get_queue("detect").put(err)
+            raise ValueError("Error initializing Aligner")
+
+        logger.debug("Launched Aligner")
 
     def launch_detector(self):
         """ Launch the face detector """
+        logger.debug("Launching Detector")
         out_queue = queue_manager.get_queue("detect")
         kwargs = {"in_queue": queue_manager.get_queue("load"),
                   "out_queue": out_queue}
@@ -341,23 +361,23 @@ class Plugins():
                 self.get_mtcnn_kwargs())
             kwargs["mtcnn_kwargs"] = mtcnn_kwargs
 
-        if self.detector.parent_is_pool:
-            self.process_detect = PoolProcess(self.detector.detect_faces)
-        else:
-            self.process_detect = SpawnProcess()
+        mp_func = PoolProcess if self.detector.parent_is_pool else SpawnProcess
+        self.process_detect = mp_func(self.detector.run, **kwargs)
 
         event = None
         if hasattr(self.process_detect, "event"):
             event = self.process_detect.event
 
-        self.process_detect.in_process(self.detector.detect_faces, **kwargs)
+        self.process_detect.start()
 
-        if not event:
+        if event is None:
+            logger.debug("Launched Detector")
             return
 
         event.wait(60)
         if not event.is_set():
             raise ValueError("Error inititalizing Detector")
+        logger.debug("Launched Detector")
 
     def get_mtcnn_kwargs(self):
         """ Add the mtcnn arguments into a kwargs dictionary """
@@ -369,6 +389,7 @@ class Plugins():
 
     def detect_faces(self, extract_pass="detect"):
         """ Detect faces from in an image """
+        logger.debug("Running Detection. Pass: '%s'", extract_pass)
         if self.is_parallel or extract_pass == "align":
             out_queue = queue_manager.get_queue("align")
         if not self.is_parallel and extract_pass == "detect":
@@ -379,12 +400,13 @@ class Plugins():
                 faces = out_queue.get(True, 1)
                 if faces == "EOF":
                     break
-                exception = faces.get("exception", None)
-                if exception is not None:
-                    queue_manager.terminate_queues()
-                    yield faces
-                    break
+                if isinstance(faces, dict) and faces.get("exception"):
+                    pid = faces["exception"][0]
+                    t_back = faces["exception"][1].getvalue()
+                    err = "Error in child process {}. {}".format(pid, t_back)
+                    raise Exception(err)
             except QueueEmpty:
                 continue
 
             yield faces
+        logger.debug("Detection Complete")
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index 120f6fd..5e7c988 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -5,6 +5,7 @@
             Faces
             Alignments"""
 
+import logging
 import os
 from pathlib import Path
 
@@ -13,69 +14,75 @@ import numpy as np
 
 from lib.aligner import Extract as AlignerExtract
 from lib.alignments import Alignments as AlignmentsBase
-from lib.detect_blur import is_blurry
-from lib.FaceFilter import FaceFilter as FilterFunc
+from lib.face_filter import FaceFilter as FilterFunc
 from lib.utils import (camel_case_split, get_folder, get_image_paths,
                        set_system_verbosity)
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Utils():
     """ Holds utility functions that are required by more than one media
         object """
 
     @staticmethod
-    def set_verbosity(verbose):
+    def set_verbosity():
         """ Set the system output verbosity """
-        lvl = '0' if verbose else '2'
-        set_system_verbosity(lvl)
+        set_system_verbosity()
 
     @staticmethod
     def finalize(images_found, num_faces_detected, verify_output):
         """ Finalize the image processing """
-        print("-------------------------")
-        print("Images found:        {}".format(images_found))
-        print("Faces detected:      {}".format(num_faces_detected))
-        print("-------------------------")
+        logger.info("-------------------------")
+        logger.info("Images found:        %s", images_found)
+        logger.info("Faces detected:      %s", num_faces_detected)
+        logger.info("-------------------------")
 
         if verify_output:
-            print("Note:")
-            print("Multiple faces were detected in one or more pictures.")
-            print("Double check your results.")
-            print("-------------------------")
+            logger.info("Note:")
+            logger.info("Multiple faces were detected in one or more pictures.")
+            logger.info("Double check your results.")
+            logger.info("-------------------------")
 
-        print("Done!")
+        logger.info("Process Succesfully Completed. Shutting Down...")
 
 
 class Alignments(AlignmentsBase):
     """ Override main alignments class for extract """
     def __init__(self, arguments, is_extract):
+        logger.debug("Initializing %s: (is_extract: %s)", self.__class__.__name__, is_extract)
         self.args = arguments
         self.is_extract = is_extract
         folder, filename = self.set_folder_filename()
         serializer = self.set_serializer()
         super().__init__(folder,
                          filename=filename,
-                         serializer=serializer,
-                         verbose=self.args.verbose)
+                         serializer=serializer)
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def set_folder_filename(self):
         """ Return the folder for the alignments file"""
         if self.args.alignments_path:
+            logger.debug("Alignments File provided: '%s'", self.args.alignments_path)
             folder, filename = os.path.split(str(self.args.alignments_path))
         else:
+            logger.debug("Alignments from Input Folder: '%s'", self.args.input_dir)
             folder = str(self.args.input_dir)
             filename = "alignments"
+        logger.debug("Setting Alignments: (folder: '%s' filename: '%s')", folder, filename)
         return folder, filename
 
     def set_serializer(self):
         """ Set the serializer to be used for loading and
             saving alignments """
         if hasattr(self.args, "serializer") and self.args.serializer:
+            logger.debug("Serializer provided: '%s'", self.args.serializer)
             serializer = self.args.serializer
         else:
             # If there is a full filename then this will be overriden
             # by filename extension
             serializer = "json"
+            logger.debug("No Serializer defaulting to: '%s'", serializer)
         return serializer
 
     def load(self):
@@ -91,26 +98,28 @@ class Alignments(AlignmentsBase):
                           and self.args.skip_faces)
 
         if not skip_existing and not skip_faces:
+            logger.debug("No skipping selected. Returning empty dictionary")
             return data
 
         if not self.have_alignments_file and (skip_existing or skip_faces):
-            print("Skip Existing/Skip Faces selected, but no alignments file "
-                  "found!")
+            logger.warning("Skip Existing/Skip Faces selected, but no alignments file found!")
             return data
 
         try:
             with open(self.file, self.serializer.roptions) as align:
                 data = self.serializer.unmarshal(align.read())
         except IOError as err:
-            print("Error: {} not read: {}".format(self.file, err.strerror))
+            logger.error("Error: '%s' not read: %s", self.file, err.strerror)
             exit(1)
 
         if skip_faces:
             # Remove items from algnments that have no faces so they will
             # be re-detected
             del_keys = [key for key, val in data.items() if not val]
+            logger.debug("Frames with no faces selected for redetection: %s", len(del_keys))
             for key in del_keys:
                 if key in data:
+                    logger.trace("Selected for redetection: '%s'", key)
                     del data[key]
         return data
 
@@ -118,17 +127,19 @@ class Alignments(AlignmentsBase):
 class Images():
     """ Holds the full frames/images """
     def __init__(self, arguments):
+        logger.debug("Initializing %s", self.__class__.__name__)
         self.args = arguments
         self.input_images = self.get_input_images()
         self.images_found = len(self.input_images)
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def get_input_images(self):
         """ Return the list of images that are to be processed """
         if not os.path.exists(self.args.input_dir):
-            print("Input directory {} not found.".format(self.args.input_dir))
+            logger.error("Input directory %s not found.", self.args.input_dir)
             exit(1)
 
-        print("Input Directory: {}".format(self.args.input_dir))
+        logger.info("Input Directory: %s", self.args.input_dir)
         input_images = get_image_paths(self.args.input_dir)
 
         return input_images
@@ -136,20 +147,48 @@ class Images():
     def load(self):
         """ Load an image and yield it with it's filename """
         for filename in self.input_images:
-            yield filename, cv2.imread(filename)  # pylint: disable=no-member
+            logger.trace("Loading image: '%s'", filename)
+            try:
+                image = cv2.imread(filename)  # pylint: disable=no-member
+            except Exception as err:  # pylint: disable=broad-except
+                logger.error("Failed to load image '%s'. Original Error: %s", filename, err)
+                continue
+            yield filename, image
 
     @staticmethod
     def load_one_image(filename):
         """ load requested image """
+        logger.trace("Loading image: '%s'", filename)
         return cv2.imread(filename)  # pylint: disable=no-member
 
 
 class PostProcess():
     """ Optional post processing tasks """
     def __init__(self, arguments):
+        logger.debug("Initializing %s", self.__class__.__name__)
         self.args = arguments
-        self.verbose = self.args.verbose
         self.actions = self.set_actions()
+        logger.debug("Initialized %s", self.__class__.__name__)
+
+    def set_actions(self):
+        """ Compile the actions to be performed into a list """
+        postprocess_items = self.get_items()
+        actions = list()
+        for action, options in postprocess_items.items():
+            options = dict() if options is None else options
+            args = options.get("args", tuple())
+            kwargs = options.get("kwargs", dict())
+            args = args if isinstance(args, tuple) else tuple()
+            kwargs = kwargs if isinstance(kwargs, dict) else dict()
+            task = globals()[action](*args, **kwargs)
+            logger.debug("Adding Postprocess action: '%s'", task)
+            actions.append(task)
+
+        for action in actions:
+            action_name = camel_case_split(action.__class__.__name__)
+            logger.info("Adding post processing item: %s", " ".join(action_name))
+
+        return actions
 
     def get_items(self):
         """ Set the post processing actions """
@@ -179,59 +218,46 @@ class PostProcess():
             face_filter["filter_lists"] = filter_lists
             postprocess_items["FaceFilter"] = {"kwargs": face_filter}
 
+        logger.debug("Postprocess Items: %s", postprocess_items)
         return postprocess_items
 
-    def set_actions(self):
-        """ Compile the actions to be performed into a list """
-        postprocess_items = self.get_items()
-        actions = list()
-        for action, options in postprocess_items.items():
-            options = dict() if options is None else options
-            args = options.get("args", tuple())
-            kwargs = options.get("kwargs", dict())
-            args = args if isinstance(args, tuple) else tuple()
-            kwargs = kwargs if isinstance(kwargs, dict) else dict()
-            kwargs["verbose"] = self.verbose
-            task = globals()[action](*args, **kwargs)
-            actions.append(task)
-
-        for action in actions:
-            action_name = camel_case_split(action.__class__.__name__)
-            print("Adding post processing item: "
-                  "{}".format(" ".join(action_name)))
-
-        return actions
-
     def do_actions(self, output_item):
         """ Perform the requested post-processing actions """
         for action in self.actions:
+            logger.debug("Performing postprocess action: '%s'", action.__class__.__name__)
             action.process(output_item)
 
 
-class PostProcessAction():
+class PostProcessAction():  # pylint: disable=too-few-public-methods
     """ Parent class for Post Processing Actions
         Usuable in Extract or Convert or both
         depending on context """
     def __init__(self, *args, **kwargs):
-        self.verbose = kwargs["verbose"]
+        logger.debug("Initializing %s: (args: %s, kwargs: %s)",
+                     self.__class__.__name__, args, kwargs)
+        logger.debug("Initialized base class %s", self.__class__.__name__)
 
     def process(self, output_item):
         """ Override for specific post processing action """
         raise NotImplementedError
 
 
-class BlurryFaceFilter(PostProcessAction):
+class BlurryFaceFilter(PostProcessAction):  # pylint: disable=too-few-public-methods
     """ Move blurry faces to a different folder
         Extract Only """
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.blur_thresh = kwargs["blur_thresh"]
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def process(self, output_item):
         """ Detect and move blurry face """
         extractor = AlignerExtract()
 
-        for face in output_item["detected_faces"]:
+        for idx, detected_face in enumerate(output_item["detected_faces"]):
+            frame_name = detected_face["file_location"].parts[-1]
+            face = detected_face["face"]
+            logger.trace("Checking for blurriness. Frame: '%s', Face: %s", frame_name, idx)
             aligned_landmarks = face.aligned_landmarks
             resized_face = face.aligned_face
             size = face.aligned["size"]
@@ -243,26 +269,46 @@ class BlurryFaceFilter(PostProcessAction):
             isolated_face = cv2.multiply(  # pylint: disable=no-member
                 feature_mask,
                 resized_face.astype(float)).astype(np.uint8)
-            blurry, focus_measure = is_blurry(isolated_face, self.blur_thresh)
+            blurry, focus_measure = self.is_blurry(isolated_face)
 
             if blurry:
-                blur_folder = output_item["output_file"].parts[:-1]
+                blur_folder = detected_face["file_location"].parts[:-1]
                 blur_folder = get_folder(Path(*blur_folder) / Path("blurry"))
-                frame_name = output_item["output_file"].parts[-1]
-                output_item["output_file"] = blur_folder / Path(frame_name)
-                if self.verbose:
-                    print("{}'s focus measure of {} was below the blur "
-                          "threshold, moving to \"blurry\"".format(
-                              frame_name, focus_measure))
+                detected_face["file_location"] = blur_folder / Path(frame_name)
+                logger.verbose("%s's focus measure of %s was below the blur threshold, "
+                               "moving to 'blurry'", frame_name, "{0:.2f}".format(focus_measure))
+
+    def is_blurry(self, image):
+        """ Convert to grayscale, and compute the focus measure of the image using the
+            Variance of Laplacian method """
+        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # pylint: disable=no-member
+        focus_measure = self.variance_of_laplacian(gray)
+
+        # if the focus measure is less than the supplied threshold,
+        # then the image should be considered "blurry"
+        retval = (focus_measure < self.blur_thresh, focus_measure)
+        logger.trace("Returning: (is_blurry: %s, focus_measure %s)", retval[0], retval[1])
+        return retval
+
+    @staticmethod
+    def variance_of_laplacian(image):
+        """ Compute the Laplacian of the image and then return the focus
+            measure, which is simply the variance of the Laplacian """
+        retval = cv2.Laplacian(image, cv2.CV_64F).var()  # pylint: disable=no-member
+        logger.trace("Returning: %s", retval)
+        return retval
 
 
-class DebugLandmarks(PostProcessAction):
+class DebugLandmarks(PostProcessAction):  # pylint: disable=too-few-public-methods
     """ Draw debug landmarks on face
         Extract Only """
 
     def process(self, output_item):
         """ Draw landmarks on image """
-        for face in output_item["detected_faces"]:
+        for idx, detected_face in enumerate(output_item["detected_faces"]):
+            face = detected_face["face"]
+            logger.trace("Drawing Landmarks. Frame: '%s'. Face: %s",
+                         detected_face["file_location"].parts[-1], idx)
             aligned_landmarks = face.aligned_landmarks
             for (pos_x, pos_y) in aligned_landmarks:
                 cv2.circle(  # pylint: disable=no-member
@@ -279,19 +325,21 @@ class FaceFilter(PostProcessAction):
         filter_lists = kwargs["filter_lists"]
         ref_threshold = kwargs.get("ref_threshold", 0.6)
         self.filter = self.load_face_filter(filter_lists, ref_threshold)
+        logger.debug("Initialized %s", self.__class__.__name__)
 
     def load_face_filter(self, filter_lists, ref_threshold):
         """ Load faces to filter out of images """
         if not any(val for val in filter_lists.values()):
             return None
 
-        filter_files = [self.set_face_filter(key, val)
-                        for key, val in filter_lists.items()]
+        filter_files = [self.set_face_filter(f_type, filter_lists[f_type])
+                        for f_type in ("filter", "nfilter")]
 
         if any(filters for filters in filter_files):
             facefilter = FilterFunc(filter_files[0],
                                     filter_files[1],
                                     ref_threshold)
+        logger.debug("Face filter: %s", facefilter)
         return facefilter
 
     @staticmethod
@@ -300,10 +348,10 @@ class FaceFilter(PostProcessAction):
         if not f_args:
             return list()
 
-        print("{}: {}".format(f_type.title(), f_args))
+        logger.info("%s: %s", f_type.title(), f_args)
         filter_files = f_args if isinstance(f_args, list) else [f_args]
-        filter_files = list(filter(lambda fnc: Path(fnc).exists(),
-                                   filter_files))
+        filter_files = list(filter(lambda fpath: Path(fpath).exists(), filter_files))
+        logger.debug("Face Filter files: %s", filter_files)
         return filter_files
 
     def process(self, output_item):
@@ -311,12 +359,13 @@ class FaceFilter(PostProcessAction):
         if not self.filter:
             return
 
-        detected_faces = output_item["detected_faces"]
         ret_faces = list()
-        for face in detected_faces:
-            if not self.filter.check(face):
-                if self.verbose:
-                    print("Skipping not recognized face!")
+        for idx, detected_face in enumerate(output_item["detected_faces"]):
+            if not self.filter.check(detected_face["face"]):
+                logger.verbose("Skipping not recognized face! Frame: %s Face %s",
+                               detected_face["file_location"].parts[-1], idx)
                 continue
-            ret_faces.append(face)
+            logger.trace("Accepting recognised face. Frame: %s. Face: %s",
+                         detected_face["file_location"].parts[-1], idx)
+            ret_faces.append(detected_face)
         output_item["detected_faces"] = ret_faces
diff --git a/scripts/gui.py b/scripts/gui.py
index 0970f2f..6de7f69 100644
--- a/scripts/gui.py
+++ b/scripts/gui.py
@@ -1,6 +1,9 @@
 #!/usr/bin python3
 """ The optional GUI for faceswap """
 
+# NB: The GUI can't currently log as it is a wrapper for the python scripts, so don't
+#     implement logging unless you can handle the conflicts
+
 import os
 import sys
 import tkinter as tk
@@ -124,7 +127,7 @@ class FaceswapGui(tk.Tk):
         exit()
 
 
-class Gui(object):
+class Gui():  # pylint: disable=too-few-public-methods
     """ The GUI process. """
     def __init__(self, arguments):
         cmd = sys.argv[0]
diff --git a/scripts/train.py b/scripts/train.py
index bc376c7..3e7a87d 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -1,6 +1,7 @@
 #!/usr/bin python3
 """ The script to run the training process of faceswap """
 
+import logging
 import os
 import sys
 import threading
@@ -13,6 +14,8 @@ from lib.utils import (get_folder, get_image_paths, set_system_verbosity,
                        Timelapse)
 from plugins.plugin_loader import PluginLoader
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Train():
     """ The training process.  """
@@ -33,9 +36,8 @@ class Train():
 
     def process(self):
         """ Call the training process object """
-        print("Training data directory: {}".format(self.args.model_dir))
-        lvl = '0' if self.args.verbose else '2'
-        set_system_verbosity(lvl)
+        logger.info("Training data directory: %s", self.args.model_dir)
+        set_system_verbosity()
         thread = self.start_thread()
 
         if self.args.preview:
@@ -51,16 +53,16 @@ class Train():
         images = []
         for image_dir in [self.args.input_A, self.args.input_B]:
             if not os.path.isdir(image_dir):
-                print('Error: {} does not exist'.format(image_dir))
+                logger.error("Error: '%s' does not exist", image_dir)
                 exit(1)
 
             if not os.listdir(image_dir):
-                print('Error: {} contains no images'.format(image_dir))
+                logger.error("Error: '%s' contains no images", image_dir)
                 exit(1)
 
             images.append(get_image_paths(image_dir))
-        print("Model A Directory: {}".format(self.args.input_A))
-        print("Model B Directory: {}".format(self.args.input_B))
+        logger.info("Model A Directory: %s", self.args.input_A)
+        logger.info("Model B Directory: %s", self.args.input_B)
         return images
 
     def start_thread(self):
@@ -71,10 +73,10 @@ class Train():
 
     def end_thread(self, thread):
         """ On termination output message and join thread back to main """
-        print("Exit requested! The trainer will complete its current cycle, "
-              "save the models and quit (it can take up a couple of seconds "
-              "depending on your training speed). If you want to kill it now, "
-              "press Ctrl + c")
+        logger.info("Exit requested! The trainer will complete its current cycle, "
+                    "save the models and quit (it can take up a couple of seconds "
+                    "depending on your training speed). If you want to kill it now, "
+                    "press Ctrl + c")
         self.stop = True
         thread.join()
         sys.stdout.flush()
@@ -82,7 +84,7 @@ class Train():
     def process_thread(self):
         """ The training process to be run inside a thread """
         try:
-            print("Loading data, this may take a while...")
+            logger.info("Loading data, this may take a while...")
 
             if self.args.allow_growth:
                 self.set_tf_allow_growth()
@@ -101,7 +103,7 @@ class Train():
             try:
                 model.save_weights()
             except KeyboardInterrupt:
-                print("Saving model weights has been cancelled!")
+                logger.info("Saving model weights has been cancelled!")
             exit(0)
         except Exception as err:
             raise err
@@ -147,10 +149,10 @@ class Train():
 
     def monitor_preview(self):
         """ Generate the preview window and wait for keyboard input """
-        print("Using live preview.\n"
-              "Press 'ENTER' on the preview window to save and quit.\n"
-              "Press 'S' on the preview window to save model weights "
-              "immediately")
+        logger.info("Using live preview.\n"
+                    "Press 'ENTER' on the preview window to save and quit.\n"
+                    "Press 'S' on the preview window to save model weights "
+                    "immediately")
         while True:
             try:
                 with self.lock:
@@ -177,7 +179,7 @@ class Train():
         # reached. At the moment, setting a target iteration and using the -p
         # flag is the only guaranteed way to exit the training loop on
         # hitting target iterations.
-        print("Starting. Press 'ENTER' to stop training and save model")
+        logger.info("Starting. Press 'ENTER' to stop training and save model")
         try:
             input()
         except KeyboardInterrupt:
@@ -208,5 +210,5 @@ class Train():
                 with self.lock:
                     self.preview_buffer[name] = image
         except Exception as err:
-            print("could not preview sample")
+            logging.error("could not preview sample")
             raise err
diff --git a/setup.py b/setup.py
index 8804a7a..e007054 100755
--- a/setup.py
+++ b/setup.py
@@ -445,7 +445,7 @@ def update_tf_dep(cpu_only):
         cuda_req = val[0]
         cudnn_req = val[1].split(".")
         if cuda_req == CUDA_VERSION and (cudnn_req[0] == cudnn_inst[0] and
-                                         cudnn_req[1] >= cudnn_inst[1]):
+                                         cudnn_req[1] <= cudnn_inst[1]):
             tf_ver = key
             break
     if tf_ver:
diff --git a/tools/alignments.py b/tools/alignments.py
index 5e71437..9b94ba1 100644
--- a/tools/alignments.py
+++ b/tools/alignments.py
@@ -11,18 +11,11 @@ class Alignments():
     """ Perform tasks relating to alignments file """
     def __init__(self, arguments):
         self.args = arguments
-        self.set_verbosity(arguments.verbose)
+        set_system_verbosity()
 
         dest_format = self.get_dest_format()
         self.alignments = AlignmentData(self.args.alignments_file,
-                                        dest_format,
-                                        self.args.verbose)
-
-    @staticmethod
-    def set_verbosity(verbose):
-        """ Set the system output verbosity """
-        lvl = '0' if verbose else '2'
-        set_system_verbosity(lvl)
+                                        dest_format)
 
     def get_dest_format(self):
         """ Set the destination format for Alignments """
diff --git a/tools/cli.py b/tools/cli.py
index 1c4b87e..72ab05b 100644
--- a/tools/cli.py
+++ b/tools/cli.py
@@ -141,11 +141,6 @@ class AlignmentsArgs(FaceSwapArgs):
                               "help": "Enable this option if manual "
                                       "alignments window is closing "
                                       "instantly. (Manual only)"})
-        argument_list.append({"opts": ("-v", "--verbose"),
-                              "action": "store_true",
-                              "dest": "verbose",
-                              "default": False,
-                              "help": "Show verbose output"})
         return argument_list
 
 
diff --git a/tools/effmpeg.py b/tools/effmpeg.py
index a7bf9d5..3933039 100644
--- a/tools/effmpeg.py
+++ b/tools/effmpeg.py
@@ -8,6 +8,7 @@ Created on 2018-03-16 15:14
 # TODO: integrate preview into gui window
 # TODO: add preview support when muxing audio
 #       -> figure out if ffmpeg | ffplay would work on windows and mac
+import logging
 import os
 import sys
 import subprocess
@@ -25,8 +26,10 @@ if sys.version_info[0] < 3:
 if sys.version_info[0] == 3 and sys.version_info[1] < 2:
     raise Exception("This program requires at least python3.2")
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
-class DataItem(object):
+
+class DataItem():
     """
     A simple class used for storing the media data items and directories that
     Effmpeg uses for 'input', 'output' and 'ref_vid'.
@@ -201,9 +204,9 @@ class Effmpeg(object):
                                  "{}".format(self.ref_vid.path))
         elif self.args.action in self._actions_can_use_ref_video:
             if self.ref_vid.is_type("none"):
-                print("Warning: no reference video was supplied, even though "
-                      "one may be used with the chosen action. If this is "
-                      "intentional then ignore this warning.", file=sys.stderr)
+                logger.warning("Warning: no reference video was supplied, even though "
+                               "one may be used with the chosen action. If this is "
+                               "intentional then ignore this warning.")
 
         # Process start and duration arguments
         self.start = self.parse_time(self.args.start)
@@ -248,8 +251,8 @@ class Effmpeg(object):
             try:
                 int(self.args.degrees)
             except ValueError as ve:
-                print("You have entered an invalid value for degrees: "
-                      "{}".format(self.args.degrees), file=sys.stderr)
+                logger.error("You have entered an invalid value for degrees: %s",
+                             self.args.degrees)
                 exit(1)
 
         # Set executable based on whether previewing or not
@@ -329,7 +332,7 @@ class Effmpeg(object):
         _fps = ff.run(stdout=subprocess.PIPE)[0].decode("utf-8")
         _fps = _fps.strip()
         if print_:
-            print("Video fps:", _fps)
+            logger.info("Video fps: %s", _fps)
         else:
             return _fps
 
@@ -341,7 +344,7 @@ class Effmpeg(object):
         out = ff.run(stdout=subprocess.PIPE,
                      stderr=subprocess.STDOUT)[0].decode('utf-8')
         if print_:
-            print(out)
+            logger.info(out)
         else:
             return out
 
@@ -399,7 +402,7 @@ class Effmpeg(object):
 
     @staticmethod
     def slice(input_=None, output=None, start=None, duration=None,
-              preview=None, exe=None,  **kwargs):
+              preview=None, exe=None, **kwargs):
         _input_opts = Effmpeg._common_ffmpeg_args[:]
         _input_opts += "-ss " + start
         _output_opts = "-t " + duration + " "
@@ -547,4 +550,3 @@ if __name__ == "__main__":
     PARSER.set_defaults(func=bad_args)
     ARGUMENTS = PARSER.parse_args()
     ARGUMENTS.func(ARGUMENTS)
-
diff --git a/tools/lib_alignments/jobs.py b/tools/lib_alignments/jobs.py
index 35aac48..5418728 100644
--- a/tools/lib_alignments/jobs.py
+++ b/tools/lib_alignments/jobs.py
@@ -1,6 +1,7 @@
 #!/usr/bin/env python3
 """ Tools for manipulating the alignments seralized file """
 
+import logging
 import os
 import pickle
 import struct
@@ -13,6 +14,8 @@ from tqdm import tqdm
 
 from . import Annotate, ExtractedFaces, Faces, Frames
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Check():
     """ Frames and faces checking tasks """
@@ -22,7 +25,7 @@ class Check():
         self.type = None
         self.output = arguments.output
         self.source_dir = self.get_source_dir(arguments)
-        self.items = self.get_items(arguments)
+        self.items = self.get_items()
 
         self.output_message = ""
 
@@ -35,18 +38,18 @@ class Check():
             self.type = "frames"
             source_dir = arguments.frames_dir
         else:
-            print("No source folder (-fr or -fc) was provided")
+            logger.error("No source folder (-fr or -fc) was provided")
             exit(0)
         return source_dir
 
-    def get_items(self, arguments):
+    def get_items(self):
         """ Set the correct items to process """
         items = globals()[self.type.title()]
-        return items(self.source_dir, arguments.verbose).file_list_sorted
+        return items(self.source_dir).file_list_sorted
 
     def process(self):
         """ Process the frames check against the alignments file """
-        print("\n[CHECK {}]".format(self.type.upper()))
+        logger.info("[CHECK %s]", self.type.upper())
         self.validate()
         items_output = self.compile_output()
         self.output_results(items_output)
@@ -55,15 +58,13 @@ class Check():
         """ Check that the selected type is valid for
             selected task and job """
         if self.job == "missing-frames" and self.output == "move":
-            print("WARNING: missing_frames was selected with move output, but "
-                  "there will be nothing to move. "
-                  "Defaulting to output: console")
+            logger.warning("Missing_frames was selected with move output, but there will "
+                           "be nothing to move. Defaulting to output: console")
             self.output = "console"
         elif self.type == "faces" and self.job not in ("multi-faces",
                                                        "leftover-faces"):
-            print("WARNING: The selected folder is not valid. "
-                  "Only folder set with '-fc' is supported for "
-                  "'multi-faces' and 'leftover-faces'")
+            logger.warning("The selected folder is not valid. Only folder set with '-fc' is "
+                           "supported for 'multi-faces' and 'leftover-faces'")
             exit(0)
 
     def compile_output(self):
@@ -134,7 +135,7 @@ class Check():
     def output_results(self, items_output):
         """ Output the results in the requested format """
         if not items_output:
-            print("No {} were found meeting the criteria".format(self.type))
+            logger.info("No %s were found meeting the criteria", self.type)
             return
         if self.output == "move":
             self.move_file(items_output)
@@ -155,8 +156,7 @@ class Check():
         filename = self.output_message.replace(" ", "_").lower()
         filename += "_" + now + ".txt"
         output_file = os.path.join(self.source_dir, filename)
-        print("Saving {} result(s) to {}".format(items_discovered,
-                                                 output_file))
+        logger.info("Saving %s result(s) to '%s'", items_discovered, output_file)
         with open(output_file, "w") as f_output:
             f_output.write(output_message)
 
@@ -172,8 +172,7 @@ class Check():
 
     def move_frames(self, output_folder, items_output):
         """ Move frames into single subfolder """
-        print("Moving {} frame(s) to {}".format(len(items_output),
-                                                output_folder))
+        logger.info("Moving %s frame(s) to '%s'", len(items_output), output_folder)
         for frame in items_output:
             src = os.path.join(self.source_dir, frame)
             dst = os.path.join(output_folder, frame)
@@ -182,8 +181,7 @@ class Check():
     def move_faces(self, output_folder, items_output):
         """ Make additional subdirs for each face that appears
             Enables easier manual sorting """
-        print("Moving {} faces(s) to {}".format(len(items_output),
-                                                output_folder))
+        logger.info("Moving %s faces(s) to '%s'", len(items_output), output_folder)
         for frame in items_output:
             idx = frame[frame.rfind("_") + 1:frame.rfind(".")]
             src = os.path.join(self.source_dir, frame)
@@ -198,9 +196,8 @@ class Draw():
     """ Draw Alignments on passed in images """
     def __init__(self, alignments, arguments):
         self.arguments = arguments
-        self.verbose = arguments.verbose
         self.alignments = alignments
-        self.frames = Frames(arguments.frames_dir, self.verbose)
+        self.frames = Frames(arguments.frames_dir)
         self.output_folder = self.set_output()
         self.extracted_faces = None
 
@@ -218,7 +215,7 @@ class Draw():
                         frames=self.frames, child_process=True)
         legacy.process()
 
-        print("\n[DRAW LANDMARKS]")  # Tidy up cli output
+        logger.info("[DRAW LANDMARKS]")  # Tidy up cli output
         self.extracted_faces = ExtractedFaces(
             self.frames,
             self.alignments,
@@ -230,14 +227,12 @@ class Draw():
             frame_name = frame["frame_fullname"]
 
             if not self.alignments.frame_exists(frame_name):
-                if self.verbose:
-                    print("Skipping {} - Alignments "
-                          "not found".format(frame_name))
+                logger.verbose("Skipping '%s' - Alignments not found", frame_name)
                 continue
 
             self.annotate_image(frame_name)
             frames_drawn += 1
-        print("{} Frame(s) output".format(frames_drawn))
+        logger.info("%s Frame(s) output", frames_drawn)
 
     def annotate_image(self, frame):
         """ Draw the alignments """
@@ -260,18 +255,17 @@ class Extract():
     """ Re-extract faces from source frames based on
         Alignment data """
     def __init__(self, alignments, arguments):
-        self.verbose = arguments.verbose
         self.alignments = alignments
         self.type = arguments.job.replace("extract-", "")
         self.faces_dir = arguments.faces_dir
-        self.frames = Frames(arguments.frames_dir, self.verbose)
+        self.frames = Frames(arguments.frames_dir)
         self.extracted_faces = ExtractedFaces(self.frames,
                                               self.alignments,
                                               align_eyes=arguments.align_eyes)
 
     def process(self):
         """ Run extraction """
-        print("\n[EXTRACT FACES]")  # Tidy up cli output
+        logger.info("[EXTRACT FACES]")  # Tidy up cli output
         self.check_folder()
         self.export_faces()
 
@@ -284,10 +278,9 @@ class Extract():
         elif os.path.isdir(self.faces_dir):
             err = "ERROR: Folder already exists at {}".format(self.faces_dir)
         if err:
-            print(err)
+            logger.error(err)
             exit(0)
-        if self.verbose:
-            print("Creating output folder at {}".format(self.faces_dir))
+        logger.verbose("Creating output folder at '%s'", self.faces_dir)
         os.makedirs(self.faces_dir)
 
     def export_faces(self):
@@ -300,13 +293,11 @@ class Extract():
             frame_name = frame["frame_fullname"]
 
             if not self.alignments.frame_exists(frame_name):
-                if self.verbose:
-                    print("Skipping {} - Alignments "
-                          "not found".format(frame_name))
+                logger.verbose("Skipping '%s' - Alignments not found", frame_name)
                 continue
             extracted_faces += self.output_faces(frame)
 
-        print("{} face(s) extracted".format(extracted_faces))
+        logger.info("%s face(s) extracted", extracted_faces)
 
     def output_faces(self, frame):
         """ Output the frame's faces to file """
@@ -337,16 +328,13 @@ class Extract():
 class Reformat():
     """ Reformat Alignment file """
     def __init__(self, alignments, arguments):
-        self.verbose = arguments.verbose
         self.alignments = alignments
         if self.alignments.file == "dfl":
-            self.faces = Faces(arguments.faces_dir,
-                               self.verbose,
-                               dfl=True)
+            self.faces = Faces(arguments.faces_dir, dfl=True)
 
     def process(self):
         """ Run reformat """
-        print("\n[REFORMAT ALIGNMENTS]")  # Tidy up cli output
+        logger.info("[REFORMAT ALIGNMENTS]")  # Tidy up cli output
         if self.alignments.file == "dfl":
             self.alignments.data = self.load_dfl()
             self.alignments.file = self.alignments.get_location(
@@ -359,9 +347,7 @@ class Reformat():
         alignments = dict()
         for face in self.faces.file_list_sorted:
             if face["face_extension"] != ".png":
-                if self.verbose:
-                    print("{} is not a png. "
-                          "Skipping".format(face["face_fullname"]))
+                logger.verbose("'%s' is not a png. Skipping", face["face_fullname"])
                 continue
 
             fullpath = os.path.join(self.faces.folder, face["face_fullname"])
@@ -379,7 +365,7 @@ class Reformat():
         with open(filename, "rb") as dfl:
             header = dfl.read(8)
             if header != b"\x89PNG\r\n\x1a\n":
-                print("ERROR: No Valid PNG header: {}".format(filename))
+                logger.error("No Valid PNG header: %s", filename)
                 return None
             while True:
                 chunk_start = dfl.tell()
@@ -392,7 +378,7 @@ class Reformat():
                     chunk = dfl.read(chunk_length + 12)
                     return pickle.loads(chunk[8:-4])
                 dfl.seek(chunk_length+12, os.SEEK_CUR)
-            print("ERROR: Couldn't find DFL alignments: {}".format(filename))
+            logger.error("Couldn't find DFL alignments: %s", filename)
 
     @staticmethod
     def convert_dfl_alignment(dfl_alignments, alignments):
@@ -414,7 +400,6 @@ class Reformat():
 class RemoveAlignments():
     """ Remove items from alignments file """
     def __init__(self, alignments, arguments):
-        self.verbose = arguments.verbose
         self.alignments = alignments
         self.type = arguments.job.replace("remove-", "")
         self.items = self.get_items(arguments)
@@ -424,15 +409,14 @@ class RemoveAlignments():
         """ Set the correct items to process """
         retval = None
         if self.type == "frames":
-            retval = list(Frames(arguments.frames_dir,
-                                 self.verbose).items.keys())
+            retval = list(Frames(arguments.frames_dir).items.keys())
         elif self.type == "faces":
-            retval = Faces(arguments.faces_dir, self.verbose)
+            retval = Faces(arguments.faces_dir)
         return retval
 
     def process(self):
         """ run removal """
-        print("\n[REMOVE ALIGNMENTS DATA]")  # Tidy up cli output
+        logger.info("[REMOVE ALIGNMENTS DATA]")  # Tidy up cli output
         del_count = 0
 
         iterator = self.alignments.yield_faces
@@ -446,11 +430,10 @@ class RemoveAlignments():
             del_count += task(item)
 
         if del_count == 0:
-            print("No changes made to alignments file. Exiting")
+            logger.info("No changes made to alignments file. Exiting")
             return
 
-        print("{} alignment(s) were removed from "
-              "alignments file".format(del_count))
+        logger.info("%s alignment(s) were removed from alignments file", del_count)
         self.alignments.save()
 
         if self.type == "faces":
@@ -485,9 +468,8 @@ class RemoveAlignments():
             if idx not in face_indexes:
                 del alignments[idx]
                 self.removed.add(frame_name)
-                if self.verbose:
-                    print("Removed alignment data for image:{} "
-                          "index: {}".format(frame_name, str(idx)))
+                logger.verbose("Removed alignment data for image: '%s'"
+                               "index: %s", frame_name, str(idx))
                 del_count += 1
         return del_count
 
@@ -514,10 +496,9 @@ class RemoveAlignments():
 
             current_index += 1
         if rename_count == 0:
-            print("No files were renamed. Exiting")
+            logger.info("No files were renamed. Exiting")
             return
-        print("{} face(s) were renamed to match with "
-              "alignments file".format(rename_count))
+        logger.info("%s face(s) were renamed to match with alignments file", rename_count)
 
     @staticmethod
     def set_image_index(index, current, original):
@@ -534,8 +515,7 @@ class RemoveAlignments():
         src = os.path.join(self.items.folder, old_file)
         dst = os.path.join(self.items.folder, new_file)
         os.rename(src, dst)
-        if self.verbose:
-            print("Renamed {} to {}".format(src, dst))
+        logger.verbose("Renamed '%s' to '%s'", src, dst)
         return 1
 
 
@@ -548,12 +528,11 @@ class Legacy():
 
     def __init__(self, alignments, arguments,
                  frames=None, child_process=False):
-        self.verbose = arguments.verbose
         self.alignments = alignments
         self.child_process = child_process
         self.frames = frames
         if not frames:
-            self.frames = Frames(arguments.frames_dir, self.verbose)
+            self.frames = Frames(arguments.frames_dir)
 
     def process(self):
         """ Run the rotate alignments process """
@@ -561,16 +540,16 @@ class Legacy():
         rotated = self.alignments.get_legacy_rotation()
         if self.child_process and not rotated and not no_dims:
             return
-        print("\n[UPDATE LEGACY LANDMARKS]")  # Tidy up cli output
+        logger.info("[UPDATE LEGACY LANDMARKS]")  # Tidy up cli output
 
         if no_dims:
             if self.child_process:
-                print("Legacy landmarks found. Adding frame dimensions...")
+                logger.info("Legacy landmarks found. Adding frame dimensions...")
             self.add_dimensions(no_dims)
 
         if rotated:
             if self.child_process:
-                print("Legacy rotated frames found. Rotating landmarks")
+                logger.info("Legacy rotated frames found. Rotating landmarks")
             self.rotate_landmarks(rotated)
 
         self.alignments.save()
@@ -595,21 +574,21 @@ class Sort():
     """ Sort alignments' index by the order they appear in
         an image """
     def __init__(self, alignments, arguments):
-        self.verbose = arguments.verbose
         self.alignments = alignments
         self.axis = arguments.job.replace("sort-", "")
         self.faces = self.get_faces(arguments)
 
-    def get_faces(self, arguments):
+    @staticmethod
+    def get_faces(arguments):
         """ If faces argument is specified, load faces_dir
             otherwise return None """
         if not hasattr(arguments, "faces_dir") or not arguments.faces_dir:
             return None
-        return Faces(arguments.faces_dir, self.verbose)
+        return Faces(arguments.faces_dir)
 
     def process(self):
         """ Execute the sort process """
-        print("\n[SORT INDEXES]")  # Tidy up cli output
+        logger.info("[SORT INDEXES]")  # Tidy up cli output
         self.check_rotated()
         self.reindex_faces()
         self.alignments.save()
@@ -620,12 +599,12 @@ class Sort():
         if any(alignment.get("r", None)
                for val in self.alignments.data.values()
                for alignment in val):
-            print("WARNING: There are rotated frames in the alignments "
-                  "file.\n\t Position of faces will not be correctly "
-                  "calculated for these frames.\n\t You should run rotation "
-                  "tool to update the file prior to running sort:\n\t "
-                  "'python tools.py alignments -j rotate -a "
-                  "<alignments_file> -fr <frames_folder>'")
+            logger.error("There are rotated frames in the alignments "
+                         "file. Position of faces will not be correctly "
+                         "calculated for these frames. You should run rotation "
+                         "tool to update the file prior to running sort: "
+                         "'python tools.py alignments -j rotate -a "
+                         "<alignments_file> -fr <frames_folder>'")
             exit(0)
 
     def reindex_faces(self):
@@ -647,7 +626,7 @@ class Sort():
             self.rename_faces(map_faces)
             self.alignments.data[key] = sorted_alignments
             reindexed += 1
-        print("{} Frames had their faces reindexed".format(reindexed))
+        logger.info("%s Frames had their faces reindexed", reindexed)
 
     def map_face_names(self, alignments, sorted_alignments, frame):
         """ Map the old and new indexes for face renaming """
@@ -664,8 +643,7 @@ class Sort():
                        if face["frame_name"] == frame
                        and face["face_index"] == idx]
             if not mapping:
-                print("WARNING: No face image found "
-                      "for frame '{}' at index '{}'".format(frame, idx))
+                logger.warning("No face image found for frame '%s' at index %s", frame, idx)
             map_faces.extend(mapping)
         return map_faces
 
@@ -684,8 +662,8 @@ class Sort():
                 src = os.path.join(self.faces.folder, old_file)
                 dst = os.path.join(self.faces.folder, new_file)
                 os.rename(src, dst)
-                if self.verbose and action == "final":
-                    print("Renamed {} to {}".format(old, new))
+                if action == "final":
+                    logger.verbose("Renamed '%s' to '%s'", old, new)
 
 
 class Spatial():
@@ -694,7 +672,6 @@ class Spatial():
         https://www.kaggle.com/selfishgene/animating-and-smoothing-3d-facial-keypoints/notebook """
 
     def __init__(self, alignments, arguments):
-        self.verbose = arguments.verbose
         self.arguments = arguments
         self.alignments = alignments
         self.mappings = dict()
@@ -703,12 +680,11 @@ class Spatial():
 
     def process(self):
         """ Perform spatial filtering """
-        print("\n[SPATIO-TEMPORAL FILTERING]")  # Tidy up cli output
-        print("NB: The process only processes the alignments for the first "
-              "face it finds for any given frame\n"
-              "    For best results only run this when:\n"
-              "      - there is only a single face in the alignments file\n"
-              "      - all false positives have been removed\n")
+        logger.info("[SPATIO-TEMPORAL FILTERING]")  # Tidy up cli output
+        logger.info("NB: The process only processes the alignments for the first "
+                    "face it finds for any given frame. For best results only run this when "
+                    "there is only a single face in the alignments file and all false positives "
+                    "have been removed")
 
         self.normalize()
         self.shape_model()
@@ -717,9 +693,9 @@ class Spatial():
         self.update_alignments(landmarks)
         self.alignments.save()
 
-        print("\nDone! To re-extract faces run:\n    python tools.py "
-              "alignments -j extract -a {} -fr <path_to_frames_dir> -fc "
-              "<output_folder>\n".format(self.arguments.alignments_file))
+        logger.info("Done! To re-extract faces run: python tools.py "
+                    "alignments -j extract -a %s -fr <path_to_frames_dir> -fc "
+                    "<output_folder>", self.arguments.alignments_file)
 
     # define shape normalization utility functions
     @staticmethod
@@ -793,8 +769,8 @@ class Spatial():
             whiten=True,
             random_state=1).fit(normalized_shapes_tbl)
         explained = self.shapes_model.explained_variance_ratio_.sum()
-        print("\nTotal explained percent by PCA model with {} components is "
-              "{}%\n".format(num_components, round(100 * explained, 1)))
+        logger.info("Total explained percent by PCA model with %s components is %s%%",
+                    num_components, round(100 * explained, 1))
 
     def spatially_filter(self):
         """ interpret the shapes using our shape model
diff --git a/tools/lib_alignments/jobs_manual.py b/tools/lib_alignments/jobs_manual.py
index 7c86101..14eb4db 100644
--- a/tools/lib_alignments/jobs_manual.py
+++ b/tools/lib_alignments/jobs_manual.py
@@ -1,6 +1,7 @@
 #!/usr/bin/env python3
 """ Manual processing of alignments """
 
+import logging
 import platform
 import sys
 import cv2
@@ -11,6 +12,8 @@ from lib.queue_manager import queue_manager, QueueEmpty
 from plugins.plugin_loader import PluginLoader
 from . import Annotate, ExtractedFaces, Frames, Legacy
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Interface():
     """ Key controls and interfacing options for OpenCV """
@@ -145,7 +148,7 @@ class Interface():
 
         return state
 
-    def save_alignments(self, *args):
+    def save_alignments(self, *args):  # pylint: disable=unused-argument
         """ Save alignments """
         if not self.state["edit"]["updated"]:
             return
@@ -153,7 +156,7 @@ class Interface():
         self.state["edit"]["updated"] = False
         self.set_redraw(True)
 
-    def reload_alignments(self, *args):
+    def reload_alignments(self, *args):  # pylint: disable=unused-argument
         """ Reload alignments """
         if not self.state["edit"]["updated"]:
             return
@@ -162,7 +165,7 @@ class Interface():
         self.state["edit"]["update_faces"] = True
         self.set_redraw(True)
 
-    def delete_alignment(self, *args):
+    def delete_alignment(self, *args):  # pylint: disable=unused-argument
         """ Save alignments """
         selected_face = self.get_selected_face_id()
         if self.get_edit_mode() == "View" or selected_face is None:
@@ -391,10 +394,9 @@ class Manual():
     """ Manually adjust or create landmarks data """
     def __init__(self, alignments, arguments):
         self.arguments = arguments
-        self.verbose = arguments.verbose
         self.alignments = alignments
         self.align_eyes = arguments.align_eyes
-        self.frames = Frames(arguments.frames_dir, self.verbose)
+        self.frames = Frames(arguments.frames_dir)
         self.extracted_faces = None
         self.interface = None
         self.help = None
@@ -406,13 +408,13 @@ class Manual():
                         frames=self.frames, child_process=True)
         legacy.process()
 
-        print("\n[MANUAL PROCESSING]")  # Tidy up cli output
+        logger.info("[MANUAL PROCESSING]")  # Tidy up cli output
         self.extracted_faces = ExtractedFaces(self.frames,
                                               self.alignments,
                                               align_eyes=self.align_eyes)
         self.interface = Interface(self.alignments, self.frames)
         self.help = Help(self.interface)
-        self.mouse_handler = MouseHandler(self.interface, self.verbose)
+        self.mouse_handler = MouseHandler(self.interface, self.arguments.loglevel)
 
         print(self.help.helptext)
         max_idx = self.frames.count - 1
@@ -693,13 +695,13 @@ class FacesDisplay():
 
 class MouseHandler():
     """ Manual Extraction """
-    def __init__(self, interface, verbose):
+    def __init__(self, interface, loglevel):
         self.interface = interface
         self.alignments = interface.alignments
         self.frames = interface.frames
 
         self.extractor = dict()
-        self.init_extractor(verbose)
+        self.init_extractor(loglevel)
 
         self.mouse_state = None
         self.last_move = None
@@ -711,7 +713,7 @@ class MouseHandler():
                       "bounding_last": list(),
                       "bounding_box_orig": list()}
 
-    def init_extractor(self, verbose):
+    def init_extractor(self, loglevel):
         """ Initialize FAN """
         out_queue = queue_manager.get_queue("out")
 
@@ -719,17 +721,17 @@ class MouseHandler():
                     "out_queue": queue_manager.get_queue("align")}
         a_kwargs = {"in_queue": queue_manager.get_queue("align"),
                     "out_queue": out_queue}
-        detect_process = SpawnProcess()
-        align_process = SpawnProcess()
-        d_event = detect_process.event
-        a_event = align_process.event
 
-        detector = PluginLoader.get_detector("manual")(verbose=verbose)
-        detect_process.in_process(detector.detect_faces, **d_kwargs)
+        detector = PluginLoader.get_detector("manual")(loglevel=loglevel)
+        detect_process = SpawnProcess(detector.run, **d_kwargs)
+        d_event = detect_process.event
+        detect_process.start()
 
         for plugin in ("fan", "dlib"):
-            aligner = PluginLoader.get_aligner(plugin)(verbose=verbose)
-            align_process.in_process(aligner.align, **a_kwargs)
+            aligner = PluginLoader.get_aligner(plugin)(loglevel=loglevel)
+            align_process = SpawnProcess(aligner.run, **a_kwargs)
+            a_event = align_process.event
+            align_process.start()
 
             # Wait for Aligner to take init
             # The first ever load of the model for FAN has reportedly taken
@@ -738,7 +740,7 @@ class MouseHandler():
             if not a_event.is_set():
                 if plugin == "fan":
                     align_process.join()
-                    print("Error initializing FAN. Trying Dlib")
+                    logger.error("Error initializing FAN. Trying Dlib")
                     continue
                 else:
                     raise ValueError("Error inititalizing Aligner")
@@ -753,7 +755,7 @@ class MouseHandler():
             if not err:
                 break
             align_process.join()
-            print("Error initializing FAN. Trying Dlib")
+            logger.error("Error initializing FAN. Trying Dlib")
 
         d_event.wait(10)
         if not d_event.is_set():
@@ -762,7 +764,7 @@ class MouseHandler():
         self.extractor["detect"] = detector
         self.extractor["align"] = aligner
 
-    def on_event(self, event, x, y, flags, param):
+    def on_event(self, event, x, y, flags, param):  # pylint: disable=unused-argument,invalid-name
         """ Handle the mouse events """
         # pylint: disable=no-member
         if self.interface.get_edit_mode() != "Edit":
@@ -885,8 +887,8 @@ class MouseHandler():
 
     def update_landmarks(self):
         """ Update the landmarks """
-        queue_manager.get_queue("in").put((self.media["image"],
-                                           self.media["bounding_box"]))
+        queue_manager.get_queue("in").put({"image": self.media["image"],
+                                           "face": self.media["bounding_box"]})
         landmarks = queue_manager.get_queue("out").get()
         if landmarks == "EOF":
             exit(0)
diff --git a/tools/lib_alignments/media.py b/tools/lib_alignments/media.py
index 0403398..9f03afd 100644
--- a/tools/lib_alignments/media.py
+++ b/tools/lib_alignments/media.py
@@ -2,6 +2,7 @@
 """ Media items (Alignments, Faces, Frames)
     for alignments tool """
 
+import logging
 import os
 
 import cv2
@@ -10,40 +11,39 @@ from lib.alignments import Alignments
 from lib.faces_detect import DetectedFace
 from lib.utils import _image_extensions
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class AlignmentData(Alignments):
     """ Class to hold the alignment data """
 
-    def __init__(self, alignments_file, destination_format, verbose):
-        print("\n[ALIGNMENT DATA]")  # Tidy up cli output
-        folder, filename = self.check_file_exists(alignments_file, verbose)
+    def __init__(self, alignments_file, destination_format):
+        logger.info("[ALIGNMENT DATA]")  # Tidy up cli output
+        folder, filename = self.check_file_exists(alignments_file)
         if filename == "dfl":
-            self.set_dfl(verbose, destination_format)
+            self.set_dfl(destination_format)
             return
-        super().__init__(folder, filename=filename, verbose=verbose)
+        super().__init__(folder, filename=filename)
         self.set_destination_format(destination_format)
-        if self.verbose:
-            print("{} items loaded".format(self.frames_count))
+        logger.verbose("%s items loaded", self.frames_count)
 
     @staticmethod
-    def check_file_exists(alignments_file, verbose):
+    def check_file_exists(alignments_file):
         """ Check the alignments file exists"""
         folder, filename = os.path.split(alignments_file)
         if filename.lower() == "dfl":
             folder = None
             filename = "dfl"
-            print("Using extracted pngs for alignments")
+            logger.info("Using extracted pngs for alignments")
         elif not os.path.isfile(alignments_file):
-            print("ERROR: alignments file not "
-                  "found at: {}".format(alignments_file))
+            logger.error("ERROR: alignments file not found at: '%s'", alignments_file)
             exit(0)
-        if verbose and folder:
-            print("Alignments file exists at {}".format(alignments_file))
+        if folder:
+            logger.verbose("Alignments file exists at '%s'", alignments_file)
         return folder, filename
 
-    def set_dfl(self, verbose, destination_format):
+    def set_dfl(self, destination_format):
         """ Set the alignments for dfl alignments """
-        self.verbose = verbose
         self.file = "dfl"
         self.set_destination_format(destination_format)
 
@@ -63,12 +63,10 @@ class AlignmentData(Alignments):
         elif file_ext in extensions.keys():
             dst_fmt = extensions[file_ext]
         else:
-            print("{} is not a supported serializer. "
-                  "Exiting".format(file_ext))
+            logger.error("'%s' is not a supported serializer. Exiting", file_ext)
             exit(0)
 
-        if self.verbose:
-            print("Destination format set to {}".format(dst_fmt))
+        logger.verbose("Destination format set to '%s'", dst_fmt)
 
         self.serializer = self.get_serializer("", dst_fmt)
         filename = os.path.splitext(self.file)[0]
@@ -82,16 +80,14 @@ class AlignmentData(Alignments):
 
 class MediaLoader():
     """ Class to load filenames from folder """
-    def __init__(self, folder, verbose):
-        print("\n[{} DATA]".format(self.__class__.__name__.upper()))
-        self.verbose = verbose
+    def __init__(self, folder):
+        logger.info("[%s DATA]", self.__class__.__name__.upper())
         self.folder = folder
         self.check_folder_exists()
         self.file_list_sorted = self.sorted_items()
         self.items = self.load_items()
         self.count = len(self.file_list_sorted)
-        if self.verbose:
-            print("{} items loaded".format(self.count))
+        logger.verbose("%s items loaded", self.count)
 
     def check_folder_exists(self):
         """ makes sure that the faces folder exists """
@@ -103,11 +99,10 @@ class MediaLoader():
             err = ("ERROR: The {} folder {} could not be "
                    "found".format(loadtype, self.folder))
         if err:
-            print(err)
+            logger.error(err)
             exit(0)
 
-        if self.verbose:
-            print("Folder exists at {}".format(self.folder))
+        logger.verbose("Folder exists at '%s'", self.folder)
 
     @staticmethod
     def valid_extension(filename):
@@ -145,13 +140,13 @@ class MediaLoader():
 
 class Faces(MediaLoader):
     """ Object to hold the faces that are to be swapped out """
-    def __init__(self, folder, verbose, dfl=False):
+    def __init__(self, folder, dfl=False):
         self.dfl = dfl
-        super().__init__(folder, verbose)
+        super().__init__(folder)
 
     def process_folder(self):
         """ Iterate through the faces dir pulling out various information """
-        print("Loading file list from {}".format(self.folder))
+        logger.info("Loading file list from %s", self.folder)
         for face in os.listdir(self.folder):
             if not self.valid_extension(face):
                 continue
@@ -187,7 +182,7 @@ class Frames(MediaLoader):
 
     def process_folder(self):
         """ Iterate through the frames dir pulling the base filename """
-        print("Loading file list from {}".format(self.folder))
+        logger.info("Loading file list from %s", self.folder)
         for frame in os.listdir(self.folder):
             if not self.valid_extension(frame):
                 continue
diff --git a/tools/sort.py b/tools/sort.py
index c2e6c6b..c77d316 100644
--- a/tools/sort.py
+++ b/tools/sort.py
@@ -2,6 +2,7 @@
 """
 A tool that allows for sorting and grouping images in different ways.
 """
+import logging
 import os
 import sys
 import operator
@@ -23,6 +24,8 @@ from plugins.plugin_loader import PluginLoader
 
 from . import cli
 
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
 
 class Sort():
     """ Sorts folders of faces based on input criteria """
@@ -84,12 +87,12 @@ class Sort():
         out_queue = queue_manager.get_queue("out")
         kwargs = {"in_queue": queue_manager.get_queue("in"),
                   "out_queue": out_queue}
-        process = SpawnProcess()
-        event = process.event
 
         for plugin in ("fan", "dlib"):
             aligner = PluginLoader.get_aligner(plugin)()
-            process.in_process(aligner.align, **kwargs)
+            process = SpawnProcess(aligner.run, **kwargs)
+            event = process.event
+            process.start()
             # Wait for Aligner to take init
             # The first ever load of the model for FAN has reportedly taken
             # up to 3-4 minutes, hence high timeout.
@@ -98,7 +101,7 @@ class Sort():
             if not event.is_set():
                 if plugin == "fan":
                     process.join()
-                    print("Error initializing FAN. Trying Dlib")
+                    logger.error("Error initializing FAN. Trying Dlib")
                     continue
                 else:
                     raise ValueError("Error inititalizing Aligner")
@@ -113,7 +116,7 @@ class Sort():
             if not err:
                 break
             process.join()
-            print("Error initializing FAN. Trying Dlib")
+            logger.error("Error initializing FAN. Trying Dlib")
 
     @staticmethod
     def alignment_dict(image):
@@ -151,20 +154,20 @@ class Sort():
 
         getattr(self, final_method)(img_list)
 
-        print("Done.")
+        logger.info("Done.")
 
     # Methods for sorting
     def sort_blur(self):
         """ Sort by blur amount """
         input_dir = self.args.input_dir
 
-        print("Sorting by blur...")
+        logger.info("Sorting by blur...")
         img_list = [[img, self.estimate_blur(cv2.imread(img))]
                     for img in
                     tqdm(self.find_images(input_dir),
                          desc="Loading",
                          file=sys.stdout)]
-        print("Sorting...")
+        logger.info("Sorting...")
 
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
 
@@ -174,7 +177,7 @@ class Sort():
         """ Sort by face similarity """
         input_dir = self.args.input_dir
 
-        print("Sorting by face similarity...")
+        logger.info("Sorting by face similarity...")
 
         img_list = [[img, face_recognition.face_encodings(cv2.imread(img))]
                     for img in
@@ -209,7 +212,7 @@ class Sort():
         """ Sort by face dissimilarity """
         input_dir = self.args.input_dir
 
-        print("Sorting by face dissimilarity...")
+        logger.info("Sorting by face dissimilarity...")
 
         img_list = [[img, face_recognition.face_encodings(cv2.imread(img)), 0]
                     for img in
@@ -232,7 +235,7 @@ class Sort():
 
             img_list[i][2] = score_total
 
-        print("Sorting...")
+        logger.info("Sorting...")
         img_list = sorted(img_list, key=operator.itemgetter(2), reverse=True)
         return img_list
 
@@ -241,7 +244,7 @@ class Sort():
         self.launch_aligner()
         input_dir = self.args.input_dir
 
-        print("Sorting by face-cnn similarity...")
+        logger.info("Sorting by face-cnn similarity...")
         img_list = []
         for img in tqdm(self.find_images(input_dir),
                         desc="Loading",
@@ -276,7 +279,7 @@ class Sort():
         self.launch_aligner()
         input_dir = self.args.input_dir
 
-        print("Sorting by face-cnn dissimilarity...")
+        logger.info("Sorting by face-cnn dissimilarity...")
 
         img_list = []
         for img in tqdm(self.find_images(input_dir),
@@ -301,7 +304,7 @@ class Sort():
 
             img_list[i][2] = score_total
 
-        print("Sorting...")
+        logger.info("Sorting...")
         img_list = sorted(img_list, key=operator.itemgetter(2), reverse=True)
 
         return img_list
@@ -319,7 +322,7 @@ class Sort():
             img_list.append(
                 [img, self.calc_landmarks_face_yaw(np.array(landmarks))])
 
-        print("Sorting by face-yaw...")
+        logger.info("Sorting by face-yaw...")
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
 
         return img_list
@@ -328,7 +331,7 @@ class Sort():
         """ Sort by histogram of face similarity """
         input_dir = self.args.input_dir
 
-        print("Sorting by histogram similarity...")
+        logger.info("Sorting by histogram similarity...")
 
         img_list = [
             [img, cv2.calcHist([cv2.imread(img)], [0], None, [256], [0, 256])]
@@ -357,7 +360,7 @@ class Sort():
         """ Sort by histigram of face dissimilarity """
         input_dir = self.args.input_dir
 
-        print("Sorting by histogram dissimilarity...")
+        logger.info("Sorting by histogram dissimilarity...")
 
         img_list = [
             [img,
@@ -378,7 +381,7 @@ class Sort():
 
             img_list[i][2] = score_total
 
-        print("Sorting...")
+        logger.info("Sorting...")
         img_list = sorted(img_list, key=operator.itemgetter(2), reverse=True)
 
         return img_list
@@ -394,7 +397,7 @@ class Sort():
         num_per_bin = len(img_list) // num_bins
         remainder = len(img_list) % num_bins
 
-        print("Grouping by blur...")
+        logger.info("Grouping by blur...")
         bins = [[] for _ in range(num_bins)]
         idx = 0
         for i in range(num_bins):
@@ -410,7 +413,7 @@ class Sort():
 
     def group_face(self, img_list):
         """ Group into bins by face similarity """
-        print("Grouping by face similarity...")
+        logger.info("Grouping by face similarity...")
 
         # Groups are of the form: group_num -> reference face
         reference_groups = dict()
@@ -464,7 +467,7 @@ class Sort():
 
     def group_face_cnn(self, img_list):
         """ Group into bins by CNN face similarity """
-        print("Grouping by face-cnn similarity...")
+        logger.info("Grouping by face-cnn similarity...")
 
         # Groups are of the form: group_num -> reference faces
         reference_groups = dict()
@@ -517,7 +520,7 @@ class Sort():
         num_per_bin = len(img_list) // num_bins
         remainder = len(img_list) % num_bins
 
-        print("Grouping by face-yaw...")
+        logger.info("Grouping by face-yaw...")
         bins = [[] for _ in range(num_bins)]
         idx = 0
         for i in range(num_bins):
@@ -533,7 +536,7 @@ class Sort():
 
     def group_hist(self, img_list):
         """ Group into bins by histogram """
-        print("Grouping by histogram...")
+        logger.info("Grouping by histogram...")
 
         # Groups are of the form: group_num -> reference histogram
         reference_groups = dict()
@@ -594,8 +597,8 @@ class Sort():
             try:
                 process_file(src, dst, self.changes)
             except FileNotFoundError as err:
-                print(err)
-                print('fail to rename {}'.format(src))
+                logger.error(err)
+                logger.error('fail to rename %s', src)
 
         for i in tqdm(range(0, len(img_list)),
                       desc=description,
@@ -606,8 +609,8 @@ class Sort():
             try:
                 os.rename(src, dst)
             except FileNotFoundError as err:
-                print(err)
-                print('fail to rename {}'.format(src))
+                logger.error(err)
+                logger.error('fail to rename %s', format(src))
 
         if self.args.log_changes:
             self.write_to_log(self.changes)
@@ -621,7 +624,7 @@ class Sort():
 
         # First create new directories to avoid checking
         # for directory existence in the moving loop
-        print("Creating group directories.")
+        logger.info("Creating group directories.")
         for i in range(len(bins)):
             directory = os.path.join(output_dir, str(i))
             if not os.path.exists(directory):
@@ -632,7 +635,7 @@ class Sort():
             else "Moving into Groups"
         )
 
-        print("Total groups found: {}".format(len(bins)))
+        logger.info("Total groups found: %s", len(bins))
         for i in tqdm(range(len(bins)), desc=description, file=sys.stdout):
             for j in range(len(bins[i])):
                 src = bins[i][j]
@@ -642,8 +645,8 @@ class Sort():
                 try:
                     process_file(src, dst, self.changes)
                 except FileNotFoundError as err:
-                    print(err)
-                    print('Failed to move {0} to {1}'.format(src, dst))
+                    logger.error(err)
+                    logger.error("Failed to move '%s' to '%s'", src, dst)
 
         if self.args.log_changes:
             self.write_to_log(self.changes)
@@ -651,7 +654,7 @@ class Sort():
     # Various helper methods
     def write_to_log(self, changes):
         """ Write the changes to log file """
-        print("Writing sort log to: {}".format(self.args.log_file_path))
+        logger.info("Writing sort log to: '%s'", self.args.log_file_path)
         with open(self.args.log_file_path, 'w') as lfile:
             lfile.write(self.serializer.marshal(changes))
 
@@ -666,7 +669,7 @@ class Sort():
         grouping method expects.
         """
         input_dir = self.args.input_dir
-        print("Preparing to group...")
+        logger.info("Preparing to group...")
         if group_method == 'group_blur':
             temp_list = [[img, self.estimate_blur(cv2.imread(img))]
                          for img in
