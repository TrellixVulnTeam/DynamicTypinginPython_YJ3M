commit 815c843f63ea7cba29915c609550a4047a945794
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue May 12 22:46:04 2020 +0000

    Simple backend unit tests (#1020)
    
    * Add simple backend tests for lib.model
    * Document lib.model
    * Fix GMSD Loss for AMD
    * Remove obsolete code from lib.model

diff --git a/.gitignore b/.gitignore
index 7fc677a..8eaaea6 100644
--- a/.gitignore
+++ b/.gitignore
@@ -33,6 +33,9 @@
 !plugins/train/*
 !plugins/convert/*
 !.pylintrc
+!tests
+!tests/*
+!tests/*/*
 !tools
 !tools/*
 !_travis
diff --git a/.travis.yml b/.travis.yml
index 026d8b7..84e56cc 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -87,10 +87,16 @@ install:
   - conda env list | grep faceswap || conda create -q --name faceswap python=$CONDA_PYTHON;
   - conda activate faceswap;
   - conda --version ; python --version ; pip --version;
-  - python setup.py --installer;
+  # We set up for plaidML as we can then use both the plaidML and Tensorflow backends for testing
+  - python setup.py --installer --amd;
+  - conda install pytest;
   # For debugging purposes
   - df -h
 
 script:
-  - python _travis/simple_tests.py;
-
+  - rm -f ~/.plaidml;
+  - echo "{\"PLAIDML_DEVICE_IDS\":[\"llvm_cpu.0\"],\"PLAIDML_EXPERIMENTAL\":true}" > ~/.plaidml;
+  - FACESWAP_BACKEND="amd" KERAS_BACKEND="plaidml.keras.backend" PYTHONPATH=$PWD:$PYTHONPATH py.test -v tests/;
+  - rm -f ~/.plaidml;
+  - FACESWAP_BACKEND="cpu" KERAS_BACKEND="tensorflow" PYTHONPATH=$PWD:$PYTHONPATH py.test -v tests/;
+  - FACESWAP_BACKEND="cpu" KERAS_BACKEND="tensorflow" python _travis/simple_tests.py;
diff --git a/docs/full/lib/model.rst b/docs/full/lib/model.rst
index 4fa62d4..d8402af 100755
--- a/docs/full/lib/model.rst
+++ b/docs/full/lib/model.rst
@@ -23,6 +23,80 @@ model.initializers module
    :undoc-members:
    :show-inheritance:
 
+model.layers module
+-------------------
+
+.. rubric:: Module Summary
+
+.. autosummary::
+   :nosignatures:
+   
+   ~lib.model.layers.GlobalMinPooling2D
+   ~lib.model.layers.GlobalStdDevPooling2D
+   ~lib.model.layers.L2_normalize
+   ~lib.model.layers.PixelShuffler
+   ~lib.model.layers.ReflectionPadding2D
+   ~lib.model.layers.SubPixelUpscaling
+   
+.. automodule:: lib.model.layers
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+model.losses module
+-------------------
+
+.. rubric:: Module Summary
+
+.. autosummary::
+   :nosignatures:
+
+   ~lib.model.losses.DSSIMObjective
+   ~lib.model.losses.PenalizedLoss
+   ~lib.model.losses.gaussian_blur
+   ~lib.model.losses.generalized_loss
+   ~lib.model.losses.gmsd_loss
+   ~lib.model.losses.gradient_loss
+   ~lib.model.losses.l_inf_norm
+   ~lib.model.losses.mask_loss_wrapper
+   ~lib.model.losses.scharr_edges
+
+.. automodule:: lib.model.losses
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+model.nn_blocks module
+----------------------
+
+.. automodule:: lib.model.nn_blocks
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+model.normalization module
+--------------------------
+
+.. rubric:: Module Summary
+
+.. autosummary::
+   :nosignatures:
+   
+   ~lib.model.normalization.InstanceNormalization
+   
+.. automodule:: lib.model.normalization
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
+model.optimizers module
+-----------------------
+
+.. automodule:: lib.model.optimizers
+   :members:
+   :undoc-members:
+   :show-inheritance:
+
 model.session module
 ---------------------
 
diff --git a/lib/model/initializers.py b/lib/model/initializers.py
index 3ed5ad2..7d4f28b 100644
--- a/lib/model/initializers.py
+++ b/lib/model/initializers.py
@@ -33,7 +33,7 @@ class ICNR(initializers.Initializer):  # pylint: disable=invalid-name
 
     Example
     -------
-        >>> x = conv2d(... weights_initializer=ICNR(initializer=he_uniform(), scale=2))
+    >>> x = conv2d(... weights_initializer=ICNR(initializer=he_uniform(), scale=2))
 
     References
     ----------
diff --git a/lib/model/layers.py b/lib/model/layers.py
index 4f6bd1e..206b4ea 100644
--- a/lib/model/layers.py
+++ b/lib/model/layers.py
@@ -1,8 +1,5 @@
 #!/usr/bin/env python3
-""" Custom Layers for faceswap.py
-    Layers from:
-        the original https://www.reddit.com/r/deepfakes/ code sample + contribs
-        shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
+""" Custom Layers for faceswap.py. """
 
 from __future__ import absolute_import
 
@@ -15,25 +12,75 @@ import keras.backend as K
 from keras.engine import InputSpec, Layer
 from keras.utils import conv_utils
 from keras.utils.generic_utils import get_custom_objects
-from keras import initializers
 from keras.layers.pooling import _GlobalPooling2D
 
-if K.backend() == "plaidml.keras.backend":
+from lib.utils import get_backend
+
+if get_backend() == "amd":
     from lib.plaidml_utils import pad
 else:
     from tensorflow import pad
 
+
 class PixelShuffler(Layer):
-    """ PixelShuffler layer for Keras
-       by t-ae: https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981 """
-    # pylint: disable=C0103
+    """ PixelShuffler layer for Keras.
+
+    This layer requires a Convolution2D prior to it, having output filters computed according to
+    the formula :math:`filters = k * (scale_factor * scale_factor)` where `k` is a user defined
+    number of filters (generally larger than 32) and `scale_factor` is the up-scaling factor
+    (generally 2).
+
+    This layer performs the depth to space operation on the convolution filters, and returns a
+    tensor with the size as defined below.
+
+    Notes
+    -----
+    In practice, it is useful to have a second convolution layer after the
+    :class:`PixelShuffler` layer to speed up the learning process. However, if you are stacking
+    multiple :class:`PixelShuffler` blocks, it may increase the number of parameters greatly,
+    so the Convolution layer after :class:`PixelShuffler` layer can be removed.
+
+    Example
+    -------
+    >>> # A standard sub-pixel up-scaling block
+    >>> x = Convolution2D(256, 3, 3, padding="same", activation="relu")(...)
+    >>> u = PixelShuffler(size=(2, 2))(x)
+    [Optional]
+    >>> x = Convolution2D(256, 3, 3, padding="same", activation="relu")(u)
+
+    Parameters
+    ----------
+    size: tuple, optional
+        The (`h`, `w`) scaling factor for up-scaling. Default: `(2, 2)`
+    data_format: ["channels_first", "channels_last", ``None``], optional
+        The data format for the input. Default: ``None``
+    kwargs: dict
+        The standard Keras Layer keyword arguments (if any)
+
+    References
+    ----------
+    https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981
+    """
     def __init__(self, size=(2, 2), data_format=None, **kwargs):
-        super(PixelShuffler, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self.data_format = K.normalize_data_format(data_format)
-        self.size = conv_utils.normalize_tuple(size, 2, 'size')
+        self.size = conv_utils.normalize_tuple(size, 2, "size")
 
     def call(self, inputs, **kwargs):
-
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
         input_shape = K.int_shape(inputs)
         if len(input_shape) != 4:
             raise ValueError('Inputs should have rank ' +
@@ -41,31 +88,45 @@ class PixelShuffler(Layer):
                              '; Received input shape:', str(input_shape))
 
         if self.data_format == 'channels_first':
-            batch_size, c, h, w = input_shape
+            batch_size, channels, height, width = input_shape
             if batch_size is None:
                 batch_size = -1
-            rh, rw = self.size
-            oh, ow = h * rh, w * rw
-            oc = c // (rh * rw)
+            r_height, r_width = self.size
+            o_height, o_width = height * r_height, width * r_width
+            o_channels = channels // (r_height * r_width)
 
-            out = K.reshape(inputs, (batch_size, rh, rw, oc, h, w))
+            out = K.reshape(inputs, (batch_size, r_height, r_width, o_channels, height, width))
             out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))
-            out = K.reshape(out, (batch_size, oc, oh, ow))
+            out = K.reshape(out, (batch_size, o_channels, o_height, o_width))
         elif self.data_format == 'channels_last':
-            batch_size, h, w, c = input_shape
+            batch_size, height, width, channels = input_shape
             if batch_size is None:
                 batch_size = -1
-            rh, rw = self.size
-            oh, ow = h * rh, w * rw
-            oc = c // (rh * rw)
+            r_height, r_width = self.size
+            o_height, o_width = height * r_height, width * r_width
+            o_channels = channels // (r_height * r_width)
 
-            out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))
+            out = K.reshape(inputs, (batch_size, height, width, r_height, r_width, o_channels))
             out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))
-            out = K.reshape(out, (batch_size, oh, ow, oc))
+            out = K.reshape(out, (batch_size, o_height, o_width, o_channels))
         return out
 
     def compute_output_shape(self, input_shape):
+        """Computes the output shape of the layer.
 
+        Assumes that the layer will be built to match that input shape provided.
+
+        Parameters
+        ----------
+        input_shape: tuple or list of tuples
+            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the
+            layer).  Shape tuples can include None for free dimensions, instead of an integer.
+
+        Returns
+        -------
+        tuple
+            An input shape tuple
+        """
         if len(input_shape) != 4:
             raise ValueError('Inputs should have rank ' +
                              str(4) +
@@ -106,87 +167,70 @@ class PixelShuffler(Layer):
         return retval
 
     def get_config(self):
-        config = {'size': self.size,
-                  'data_format': self.data_format}
-        base_config = super(PixelShuffler, self).get_config()
-
-        return dict(list(base_config.items()) + list(config.items()))
+        """Returns the config of the layer.
 
+        A layer config is a Python dictionary (serializable) containing the configuration of a
+        layer. The same layer can be reinstated later (without its trained weights) from this
+        configuration.
 
-class Scale(Layer):
-    """
-    GAN Custom Scal Layer
-    Code borrows from https://github.com/flyyufelix/cnn_finetune
-    """
-    def __init__(self, weights=None, axis=-1, gamma_init='zero', **kwargs):
-        self.axis = axis
-        self.gamma = None
-        self.gamma_init = initializers.get(gamma_init)
-        self.initial_weights = weights
-        super(Scale, self).__init__(**kwargs)
-
-    def build(self, input_shape):
-        self.input_spec = [InputSpec(shape=input_shape)]
+        The configuration of a layer does not include connectivity information, nor the layer
+        class name. These are handled by `Network` (one layer of abstraction above).
 
-        # Compatibility with TensorFlow >= 1.0.0
-        self.gamma = K.variable(self.gamma_init((1,)), name='{}_gamma'.format(self.name))
-        self.trainable_weights = [self.gamma]
-
-        if self.initial_weights is not None:
-            self.set_weights(self.initial_weights)
-            del self.initial_weights
-
-    def call(self, x, mask=None):
-        return self.gamma * x
+        Returns
+        --------
+        dict
+            A python dictionary containing the layer configuration
+        """
+        config = {'size': self.size,
+                  'data_format': self.data_format}
+        base_config = super(PixelShuffler, self).get_config()
 
-    def get_config(self):
-        config = {"axis": self.axis}
-        base_config = super(Scale, self).get_config()
         return dict(list(base_config.items()) + list(config.items()))
 
 
 class SubPixelUpscaling(Layer):
-    # pylint: disable=C0103
-    """ Sub-pixel convolutional upscaling layer based on the paper "Real-Time
-    Single Image and Video Super-Resolution Using an Efficient Sub-Pixel
-    Convolutional Neural Network" (https://arxiv.org/abs/1609.05158).
-    This layer requires a Convolution2D prior to it, having output filters
-    computed according to the formula :
-        filters = k * (scale_factor * scale_factor)
-        where k = a user defined number of filters (generally larger than 32)
-              scale_factor = the upscaling factor (generally 2)
-    This layer performs the depth to space operation on the convolution
-    filters, and returns a tensor with the size as defined below.
-    # Example :
-    ```python
-        # A standard subpixel upscaling block
-        x = Convolution2D(256, 3, 3, padding="same", activation="relu")(...)
-        u = SubPixelUpscaling(scale_factor=2)(x)
-        [Optional]
-        x = Convolution2D(256, 3, 3, padding="same", activation="relu")(u)
-    ```
-        In practice, it is useful to have a second convolution layer after the
-        SubPixelUpscaling layer to speed up the learning process.
-        However, if you are stacking multiple SubPixelUpscaling blocks,
-        it may increase the number of parameters greatly, so the Convolution
-        layer after SubPixelUpscaling layer can be removed.
-    # Arguments
-        scale_factor: Upscaling factor.
-        data_format: Can be None, "channels_first" or "channels_last".
-    # Input shape
-        4D tensor with shape:
-        `(samples, k * (scale_factor * scale_factor) channels, rows, cols)`
-            if data_format="channels_first"
-        or 4D tensor with shape:
-        `(samples, rows, cols, k * (scale_factor * scale_factor) channels)`
-            if data_format="channels_last".
-    # Output shape
-        4D tensor with shape:
-        `(samples, k channels, rows * scale_factor, cols * scale_factor))`
-            if data_format="channels_first"
-        or 4D tensor with shape:
-        `(samples, rows * scale_factor, cols * scale_factor, k channels)`
-            if data_format="channels_last".
+    """ Sub-pixel convolutional up-scaling layer.
+
+    This layer requires a Convolution2D prior to it, having output filters computed according to
+    the formula :math:`filters = k * (scale_factor * scale_factor)` where `k` is a user defined
+    number of filters (generally larger than 32) and `scale_factor` is the up-scaling factor
+    (generally 2).
+
+    This layer performs the depth to space operation on the convolution filters, and returns a
+    tensor with the size as defined below.
+
+    Notes
+    -----
+    This method is deprecated as it just performs the same as :class:`PixelShuffler`
+    using explicit Tensorflow ops. The method is kept in the repository to support legacy
+    models that have been created with this layer.
+
+    In practice, it is useful to have a second convolution layer after the
+    :class:`SubPixelUpscaling` layer to speed up the learning process. However, if you are stacking
+    multiple :class:`SubPixelUpscaling` blocks, it may increase the number of parameters greatly,
+    so the Convolution layer after :class:`SubPixelUpscaling` layer can be removed.
+
+    Example
+    -------
+    >>> # A standard sub-pixel up-scaling block
+    >>> x = Convolution2D(256, 3, 3, padding="same", activation="relu")(...)
+    >>> u = SubPixelUpscaling(scale_factor=2)(x)
+    [Optional]
+    >>> x = Convolution2D(256, 3, 3, padding="same", activation="relu")(u)
+
+    Parameters
+    ----------
+    size: int, optional
+        The up-scaling factor. Default: `2`
+    data_format: ["channels_first", "channels_last", ``None``], optional
+        The data format for the input. Default: ``None``
+    kwargs: dict
+        The standard Keras Layer keyword arguments (if any)
+
+    References
+    ----------
+    based on the paper "Real-Time Single Image and Video Super-Resolution Using an Efficient
+    Sub-Pixel Convolutional Neural Network" (https://arxiv.org/abs/1609.05158).
     """
 
     def __init__(self, scale_factor=2, data_format=None, **kwargs):
@@ -196,29 +240,67 @@ class SubPixelUpscaling(Layer):
         self.data_format = K.normalize_data_format(data_format)
 
     def build(self, input_shape):
+        """Creates the layer weights.
+
+        Must be implemented on all layers that have weights.
+
+        Parameters
+        ----------
+        input_shape: tensor
+            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to
+            reference for weight shape computations.
+        """
         pass
 
-    def call(self, x, mask=None):
-        y = self.depth_to_space(x, self.scale_factor, self.data_format)
-        return y
+    def call(self, input_tensor, mask=None):  # pylint:disable=unused-argument,arguments-differ
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
+        retval = self._depth_to_space(input_tensor, self.scale_factor, self.data_format)
+        return retval
 
     def compute_output_shape(self, input_shape):
+        """Computes the output shape of the layer.
+
+        Assumes that the layer will be built to match that input shape provided.
+
+        Parameters
+        ----------
+        input_shape: tuple or list of tuples
+            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the
+            layer).  Shape tuples can include None for free dimensions, instead of an integer.
+
+        Returns
+        -------
+        tuple
+            An input shape tuple
+        """
         if self.data_format == "channels_first":
-            b, k, r, c = input_shape
-            return (b,
-                    k // (self.scale_factor ** 2),
-                    r * self.scale_factor,
-                    c * self.scale_factor)
-        b, r, c, k = input_shape
-        return (b,
-                r * self.scale_factor,
-                c * self.scale_factor,
-                k // (self.scale_factor ** 2))
+            batch, channels, rows, columns = input_shape
+            return (batch,
+                    channels // (self.scale_factor ** 2),
+                    rows * self.scale_factor,
+                    columns * self.scale_factor)
+        batch, rows, columns, channels = input_shape
+        return (batch,
+                rows * self.scale_factor,
+                columns * self.scale_factor,
+                channels // (self.scale_factor ** 2))
 
     @classmethod
-    def depth_to_space(cls, ipt, scale, data_format=None):
-        """ Uses phase shift algorithm to convert channels/depth
-            for spatial resolution """
+    def _depth_to_space(cls, ipt, scale, data_format=None):
+        """ Uses phase shift algorithm to convert channels/depth for spatial resolution """
         if data_format is None:
             data_format = K.image_data_format()
         data_format = data_format.lower()
@@ -228,42 +310,69 @@ class SubPixelUpscaling(Layer):
         return out
 
     @staticmethod
-    def _postprocess_conv2d_output(x, data_format):
+    def _postprocess_conv2d_output(input_tensor, data_format):
         """Transpose and cast the output from conv2d if needed.
-        # Arguments
-            x: A tensor.
-            data_format: string, `"channels_last"` or `"channels_first"`.
-        # Returns
-            A tensor.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input that requires transposing and casting
+        data_format: str
+            `"channels_last"` or `"channels_first"`
+
+        Returns
+        -------
+        tensor
+            The transposed and cast input tensor
         """
 
         if data_format == "channels_first":
-            x = tf.transpose(x, (0, 3, 1, 2))
+            input_tensor = tf.transpose(input_tensor, (0, 3, 1, 2))
 
         if K.floatx() == "float64":
-            x = tf.cast(x, "float64")
-        return x
+            input_tensor = tf.cast(input_tensor, "float64")
+        return input_tensor
 
     @staticmethod
-    def _preprocess_conv2d_input(x, data_format):
+    def _preprocess_conv2d_input(input_tensor, data_format):
         """Transpose and cast the input before the conv2d.
-        # Arguments
-            x: input tensor.
-            data_format: string, `"channels_last"` or `"channels_first"`.
-        # Returns
-            A tensor.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input that requires transposing and casting
+        data_format: str
+            `"channels_last"` or `"channels_first"`
+
+        Returns
+        -------
+        tensor
+            The transposed and cast input tensor
         """
-        if K.dtype(x) == "float64":
-            x = tf.cast(x, "float32")
+        if K.dtype(input_tensor) == "float64":
+            input_tensor = tf.cast(input_tensor, "float32")
         if data_format == "channels_first":
-            # TF uses the last dimension as channel dimension,
-            # instead of the 2nd one.
-            # TH input shape: (samples, input_depth, rows, cols)
-            # TF input shape: (samples, rows, cols, input_depth)
-            x = tf.transpose(x, (0, 2, 3, 1))
-        return x
+            # Tensorflow uses the last dimension as channel dimension, instead of the 2nd one.
+            # Theano input shape: (samples, input_depth, rows, cols)
+            # Tensorflow input shape: (samples, rows, cols, input_depth)
+            input_tensor = tf.transpose(input_tensor, (0, 2, 3, 1))
+        return input_tensor
 
     def get_config(self):
+        """Returns the config of the layer.
+
+        A layer config is a Python dictionary (serializable) containing the configuration of a
+        layer. The same layer can be reinstated later (without its trained weights) from this
+        configuration.
+
+        The configuration of a layer does not include connectivity information, nor the layer
+        class name. These are handled by `Network` (one layer of abstraction above).
+
+        Returns
+        --------
+        dict
+            A python dictionary containing the layer configuration
+        """
         config = {"scale_factor": self.scale_factor,
                   "data_format": self.data_format}
         base_config = super(SubPixelUpscaling, self).get_config()
@@ -272,37 +381,53 @@ class SubPixelUpscaling(Layer):
 
 class ReflectionPadding2D(Layer):
     """Reflection-padding layer for 2D input (e.g. picture).
-    This layer can add rows and columns
-    at the top, bottom, left and right side of an image tensor.
-    Input shape:  ONLY WORKS ON CHANNELS LAST NOW
-      4D tensor with shape:
-      - If `data_format` is `"channels_last"`:
-          `(batch, rows, cols, channels)`
-      - If `data_format` is `"channels_first"`:
-          `(batch, channels, rows, cols)`
-    Output shape:
-      4D tensor with shape:
-      - If `data_format` is `"channels_last"`:
-          `(batch, padded_rows, padded_cols, channels)`
-      - If `data_format` is `"channels_first"`:
-          `(batch, channels, padded_rows, padded_cols)`
+
+    This layer can add rows and columns at the top, bottom, left and right side of an image tensor.
+
+    Parameters
+    ----------
+    stride: int, optional
+        The stride of the following convolution. Default: `2`
+    kernel_size: int, optional
+        The kernel size of the following convolution. Default: `5`
+    kwargs: dict
+        The standard Keras Layer keyword arguments (if any)
     """
     def __init__(self, stride=2, kernel_size=5, **kwargs):
-        '''
-        # Arguments
-            stride: stride of following convolution (2)
-            kernel_size: kernel size of following convolution (5,5)
-        '''
         self.stride = stride
         self.kernel_size = kernel_size
-        super(ReflectionPadding2D, self).__init__(**kwargs)
+        super().__init__(**kwargs)
 
     def build(self, input_shape):
+        """Creates the layer weights.
+
+        Must be implemented on all layers that have weights.
+
+        Parameters
+        ----------
+        input_shape: tensor
+            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to
+            reference for weight shape computations.
+        """
         self.input_spec = [InputSpec(shape=input_shape)]
-        super(ReflectionPadding2D, self).build(input_shape)
+        super().build(input_shape)
 
     def compute_output_shape(self, input_shape):
-        """ If you are using "channels_last" configuration"""
+        """Computes the output shape of the layer.
+
+        Assumes that the layer will be built to match that input shape provided.
+
+        Parameters
+        ----------
+        input_shape: tuple or list of tuples
+            Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the
+            layer).  Shape tuples can include None for free dimensions, instead of an integer.
+
+        Returns
+        -------
+        tuple
+            An input shape tuple
+        """
         input_shape = self.input_spec[0].shape
         in_width, in_height = input_shape[2], input_shape[1]
         kernel_width, kernel_height = self.kernel_size, self.kernel_size
@@ -314,14 +439,28 @@ class ReflectionPadding2D(Layer):
         if (in_width % self.stride) == 0:
             padding_width = max(kernel_width - self.stride, 0)
         else:
-            padding_width = max(kernel_width- (in_width % self.stride), 0)
+            padding_width = max(kernel_width - (in_width % self.stride), 0)
 
         return (input_shape[0],
                 input_shape[1] + padding_height,
                 input_shape[2] + padding_width,
                 input_shape[3])
 
-    def call(self, x, mask=None):
+    def call(self, x, mask=None):  # pylint:disable=unused-argument,arguments-differ
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
         input_shape = self.input_spec[0].shape
         in_width, in_height = input_shape[2], input_shape[1]
         kernel_width, kernel_height = self.kernel_size, self.kernel_size
@@ -333,7 +472,7 @@ class ReflectionPadding2D(Layer):
         if (in_width % self.stride) == 0:
             padding_width = max(kernel_width - self.stride, 0)
         else:
-            padding_width = max(kernel_width- (in_width % self.stride), 0)
+            padding_width = max(kernel_width - (in_width % self.stride), 0)
 
         padding_top = padding_height // 2
         padding_bot = padding_height - padding_top
@@ -348,6 +487,20 @@ class ReflectionPadding2D(Layer):
                    'REFLECT')
 
     def get_config(self):
+        """Returns the config of the layer.
+
+        A layer config is a Python dictionary (serializable) containing the configuration of a
+        layer. The same layer can be reinstated later (without its trained weights) from this
+        configuration.
+
+        The configuration of a layer does not include connectivity information, nor the layer
+        class name. These are handled by `Network` (one layer of abstraction above).
+
+        Returns
+        --------
+        dict
+            A python dictionary containing the layer configuration
+        """
         config = {'stride': self.stride,
                   'kernel_size': self.kernel_size}
         base_config = super(ReflectionPadding2D, self).get_config()
@@ -355,31 +508,23 @@ class ReflectionPadding2D(Layer):
 
 
 class GlobalMinPooling2D(_GlobalPooling2D):
-    """Global minimum pooling operation for spatial data.
-    # Arguments
-        data_format: A string,
-            one of `channels_last` (default) or `channels_first`.
-            The ordering of the dimensions in the inputs.
-            `channels_last` corresponds to inputs with shape
-            `(batch, height, width, channels)` while `channels_first`
-            corresponds to inputs with shape
-            `(batch, channels, height, width)`.
-            It defaults to the `image_data_format` value found in your
-            Keras config file at `~/.keras/keras.json`.
-            If you never set it, then it will be "channels_last".
-    # Input shape
-        - If `data_format='channels_last'`:
-            4D tensor with shape:
-            `(batch_size, rows, cols, channels)`
-        - If `data_format='channels_first'`:
-            4D tensor with shape:
-            `(batch_size, channels, rows, cols)`
-    # Output shape
-        2D tensor with shape:
-        `(batch_size, channels)`
-    """
+    """Global minimum pooling operation for spatial data. """
 
     def call(self, inputs):
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
         if self.data_format == 'channels_last':
             pooled = K.min(inputs, axis=[1, 2])
         else:
@@ -388,52 +533,81 @@ class GlobalMinPooling2D(_GlobalPooling2D):
 
 
 class GlobalStdDevPooling2D(_GlobalPooling2D):
-    """Global standard deviation pooling operation for spatial data.
-    # Arguments
-        data_format: A string,
-            one of `channels_last` (default) or `channels_first`.
-            The ordering of the dimensions in the inputs.
-            `channels_last` corresponds to inputs with shape
-            `(batch, height, width, channels)` while `channels_first`
-            corresponds to inputs with shape
-            `(batch, channels, height, width)`.
-            It defaults to the `image_data_format` value found in your
-            Keras config file at `~/.keras/keras.json`.
-            If you never set it, then it will be "channels_last".
-    # Input shape
-        - If `data_format='channels_last'`:
-            4D tensor with shape:
-            `(batch_size, rows, cols, channels)`
-        - If `data_format='channels_first'`:
-            4D tensor with shape:
-            `(batch_size, channels, rows, cols)`
-    # Output shape
-        2D tensor with shape:
-        `(batch_size, channels)`
-    """
+    """Global standard deviation pooling operation for spatial data. """
 
     def call(self, inputs):
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
         if self.data_format == 'channels_last':
             pooled = K.std(inputs, axis=[1, 2])
         else:
             pooled = K.std(inputs, axis=[2, 3])
         return pooled
 
-class L2_normalize(Layer):
+
+class L2_normalize(Layer):  # Pylint:disable=invalid-name
+    """ Normalizes a tensor w.r.t. the L2 norm alongside the specified axis.
+
+    Parameters
+    ----------
+    axis: int
+        The axis to perform normalization across
+    kwargs: dict
+        The standard Keras Layer keyword arguments (if any)
+    """
     def __init__(self, axis, **kwargs):
         self.axis = axis
         super(L2_normalize, self).__init__(**kwargs)
 
-    def call(self, x):
-        return K.l2_normalize(x, self.axis)
+    def call(self, inputs):  # pylint:disable=arguments-differ
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+        kwargs: dict
+            Additional keyword arguments
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
+        return K.l2_normalize(inputs, self.axis)
 
     def get_config(self):
+        """Returns the config of the layer.
+
+        A layer config is a Python dictionary (serializable) containing the configuration of a
+        layer. The same layer can be reinstated later (without its trained weights) from this
+        configuration.
+
+        The configuration of a layer does not include connectivity information, nor the layer
+        class name. These are handled by `Network` (one layer of abstraction above).
+
+        Returns
+        --------
+        dict
+            A python dictionary containing the layer configuration
+        """
         config = super(L2_normalize, self).get_config()
         config["axis"] = self.axis
         return config
 
 
-
 # Update layers into Keras custom objects
 for name, obj in inspect.getmembers(sys.modules[__name__]):
     if inspect.isclass(obj) and obj.__module__ == __name__:
diff --git a/lib/model/losses.py b/lib/model/losses.py
index 95d77db..706e6f7 100644
--- a/lib/model/losses.py
+++ b/lib/model/losses.py
@@ -1,37 +1,37 @@
 #!/usr/bin/env python3
-""" Custom Loss Functions for faceswap.py
-    Losses from:
-        keras.contrib
-        dfaker: https://github.com/dfaker/df
-        shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
+""" Custom Loss Functions for faceswap.py """
 
 from __future__ import absolute_import
 
 import logging
 
 import keras.backend as K
-from keras.layers import Lambda, concatenate
 import numpy as np
 import tensorflow as tf
-from tensorflow.distributions import Beta
 
-from .normalization import InstanceNormalization
-if K.backend() == "plaidml.keras.backend":
+from lib.utils import get_backend
+
+if get_backend() == "amd":
     from plaidml.op import extract_image_patches
+    from lib.plaidml_utils import pad
 else:
     from tensorflow import extract_image_patches  # pylint: disable=ungrouped-imports
-
+    from tensorflow import pad
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 def mask_loss_wrapper(loss_func, preprocessing_func=None):
-    """ A wrapper for mask loss that can perform pre-processing on the input
-        prior to calling the loss function
-        loss_func: The loss function to use
-        preprocessing_func: The preprocessing function to use. Should take a Keras Input
-        as it's only argument """
-
+    """ A wrapper for mask loss that can perform pre-processing on the input prior to calling the
+    loss function.
+
+    Parameters
+    ----------
+    loss_func: class or function
+        The actual loss function to use
+    preprocessing_func: function
+        The pre-processing function to use. Should take a Keras Input as it's only argument
+    """
     def func(y_true, y_pred):
         """ Process input if a processing function has been passed, otherwise just return loss """
         if preprocessing_func is not None:
@@ -44,7 +44,25 @@ def mask_loss_wrapper(loss_func, preprocessing_func=None):
 class DSSIMObjective():
     """ DSSIM Loss Function
 
-    Code copy and pasted, with minor ammendments from:
+    Difference of Structural Similarity (DSSIM loss function). Clipped between 0 and 0.5
+
+    Parameters
+    ----------
+    k_1: float, optional
+        Parameter of the SSIM. Default: `0.01`
+    k_2: float, optional
+        Parameter of the SSIM. Default: `0.03`
+    kernel_size: int, optional
+        Size of the sliding window Default: `3`
+    max_value: float, optional
+        Max value of the output. Default: `1.0`
+
+    Notes
+    ------
+    You should add a regularization term like a l2 loss in addition to this one.
+
+    References
+    ----------
     https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/losses/dssim.py
 
     MIT License
@@ -67,46 +85,59 @@ class DSSIMObjective():
     AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
     LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
     OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-    SOFTWARE. """
-    # pylint: disable=C0103
-    def __init__(self, k1=0.01, k2=0.03, kernel_size=3, max_value=1.0):
-        """
-        Difference of Structural Similarity (DSSIM loss function). Clipped
-        between 0 and 0.5
-        Note : You should add a regularization term like a l2 loss in
-               addition to this one.
-        Note : In theano, the `kernel_size` must be a factor of the output
-               size. So 3 could not be the `kernel_size` for an output of 32.
-        # Arguments
-            k1: Parameter of the SSIM (default 0.01)
-            k2: Parameter of the SSIM (default 0.03)
-            kernel_size: Size of the sliding window (default 3)
-            max_value: Max value of the output (default 1.0)
-        """
+    SOFTWARE.
+    """
+    def __init__(self, k_1=0.01, k_2=0.03, kernel_size=3, max_value=1.0):
         self.__name__ = 'DSSIMObjective'
         self.kernel_size = kernel_size
-        self.k1 = k1
-        self.k2 = k2
+        self.k_1 = k_1
+        self.k_2 = k_2
         self.max_value = max_value
-        self.c_1 = (self.k1 * self.max_value) ** 2
-        self.c_2 = (self.k2 * self.max_value) ** 2
+        self.c_1 = (self.k_1 * self.max_value) ** 2
+        self.c_2 = (self.k_2 * self.max_value) ** 2
         self.dim_ordering = K.image_data_format()
         self.backend = K.backend()
 
     @staticmethod
-    def __int_shape(x):
-        return K.int_shape(x)
+    def __int_shape(input_tensor):
+        """ Returns the shape of tensor or variable as a tuple of int or None entries.
+
+        Parameters
+        ----------
+        input_tensor: tensor or variable
+            The input to return the shape for
+
+        Returns
+        -------
+        tuple
+            A tuple of integers (or None entries)
+        """
+        return K.int_shape(input_tensor)
 
     def __call__(self, y_true, y_pred):
-        # There are additional parameters for this function
-        # Note: some of the 'modes' for edge behavior do not yet have a
-        # gradient definition in the Theano tree and cannot be used for
-        # learning
+        """ Call the DSSIM Loss Function.
+
+        Parameters
+        ----------
+        y_true: tensor or variable
+            The ground truth value
+        y_pred: tensor or variable
+            The predicted value
+
+        Returns
+        -------
+        tensor
+            The DSSIM Loss value
+
+        Notes
+        -----
+        There are additional parameters for this function. some of the 'modes' for edge behavior
+        do not yet have a gradient definition in the Theano tree and cannot be used for learning
+        """
 
         kernel = [self.kernel_size, self.kernel_size]
         y_true = K.reshape(y_true, [-1] + list(self.__int_shape(y_pred)[1:]))
         y_pred = K.reshape(y_pred, [-1] + list(self.__int_shape(y_pred)[1:]))
-
         patches_pred = self.extract_image_patches(y_pred,
                                                   kernel,
                                                   kernel,
@@ -124,7 +155,7 @@ class DSSIMObjective():
         # Get variance
         var_true = K.var(patches_true, axis=-1)
         var_pred = K.var(patches_pred, axis=-1)
-        # Get std dev
+        # Get standard deviation
         covar_true_pred = K.mean(
             patches_true * patches_pred, axis=-1) - u_true * u_pred
 
@@ -132,18 +163,27 @@ class DSSIMObjective():
             2 * covar_true_pred + self.c_2)
         denom = (K.square(u_true) + K.square(u_pred) + self.c_1) * (
             var_pred + var_true + self.c_2)
-        ssim /= denom  # no need for clipping, c_1 + c_2 make the denom non-zero
+        ssim /= denom  # no need for clipping, c_1 + c_2 make the denorm non-zero
         return K.mean((1.0 - ssim) / 2.0)
 
     @staticmethod
     def _preprocess_padding(padding):
-        """Convert keras' padding to tensorflow's padding.
-        # Arguments
-            padding: string, `"same"` or `"valid"`.
-        # Returns
-            a string, `"SAME"` or `"VALID"`.
-        # Raises
-            ValueError: if `padding` is invalid.
+        """Convert keras padding to tensorflow padding.
+
+        Parameters
+        ----------
+        padding: string,
+            `"same"` or `"valid"`.
+
+        Returns
+        -------
+        str
+            `"SAME"` or `"VALID"`.
+
+        Raises
+        ------
+        ValueError
+            If `padding` is invalid.
         """
         if padding == 'same':
             padding = 'SAME'
@@ -153,43 +193,76 @@ class DSSIMObjective():
             raise ValueError('Invalid padding:', padding)
         return padding
 
-    def extract_image_patches(self, x, ksizes, ssizes, padding='same',
-                              data_format='channels_last'):
-        """
-        Extract the patches from an image
-        # Parameters
-            x : The input image
-            ksizes : 2-d tuple with the kernel size
-            ssizes : 2-d tuple with the strides size
-            padding : 'same' or 'valid'
-            data_format : 'channels_last' or 'channels_first'
-        # Returns
-            The (k_w, k_h) patches extracted
-            TF ==> (batch_size, w, h, k_w, k_h, c)
-            TH ==> (batch_size, w, h, c, k_w, k_h)
+    def extract_image_patches(self, input_tensor, k_sizes, s_sizes,
+                              padding='same', data_format='channels_last'):
+        """ Extract the patches from an image.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input image
+        k_sizes: tuple
+            2-d tuple with the kernel size
+        s_sizes: tuple
+            2-d tuple with the strides size
+        padding: str, optional
+            `"same"` or `"valid"`. Default: `"same"`
+        data_format: str, optional.
+            `"channels_last"` or `"channels_first"`. Default: `"channels_last"`
+
+        Returns
+        -------
+        The (k_w, k_h) patches extracted
+            Tensorflow ==> (batch_size, w, h, k_w, k_h, c)
+            Theano ==> (batch_size, w, h, c, k_w, k_h)
         """
-        kernel = [1, ksizes[0], ksizes[1], 1]
-        strides = [1, ssizes[0], ssizes[1], 1]
+        kernel = [1, k_sizes[0], k_sizes[1], 1]
+        strides = [1, s_sizes[0], s_sizes[1], 1]
         padding = self._preprocess_padding(padding)
         if data_format == 'channels_first':
-            x = K.permute_dimensions(x, (0, 2, 3, 1))
-        patches = extract_image_patches(x, kernel, strides, [1, 1, 1, 1], padding)
+            input_tensor = K.permute_dimensions(input_tensor, (0, 2, 3, 1))
+        patches = extract_image_patches(input_tensor, kernel, strides, [1, 1, 1, 1], padding)
         return patches
 
 
 # <<< START: from Dfaker >>> #
 def PenalizedLoss(mask, loss_func,  # pylint: disable=invalid-name
                   mask_prop=1.0, mask_scaling=1.0, preprocessing_func=None):
-    """ Plaidml + tf Penalized loss function
-        mask_scaling: For multi-decoder output the target mask will likely be at
-                      full size scaling, so this is the scaling factor to reduce
-                      the mask by.
-        preprocessing_func: The preprocessing function to use. Should take a Keras Input
-                            as it's only input
+    """ Plaidml and Tensorflow Penalized Loss function.
+
+    Applies the given loss function just to the masked area of the image.
+
+    Parameters
+    ----------
+    mask: input tensor
+        The mask for the current image
+    loss_func: function
+        The actual loss function to use
+    mask_prop: float, optional
+        The amount of mask propagation. Default: `1.0`
+    mask_scaling: float, optional
+        For multi-decoder output the target mask will likely be at full size scaling, so this is
+        the scaling factor to reduce the mask by. Default: `1.0`
+    preprocessing_func: function, optional
+        If preprocessing is required on the input mask, then this should be the function to use.
+        The function should take a Keras Input as it's only argument. Set to ``None`` if no
+        preprocessing is to be performed. Default: ``None``
     """
-
-    def scale_mask(mask, scaling):
-        """ Scale the input mask to be the same size as the input face """
+    def _scale_mask(mask, scaling):
+        """ Scale the input mask to be the same size as the input face
+
+        Parameters
+        ----------
+        mask: input tensor
+            The mask for the current image
+        scaling: float
+            The amount to scale the input mask by
+
+        Returns
+        -------
+        tensor
+            The resized input mask
+        """
         if scaling != 1.0:
             size = round(1 / scaling)
             mask = K.pool2d(mask,
@@ -201,15 +274,32 @@ def PenalizedLoss(mask, loss_func,  # pylint: disable=invalid-name
         logger.debug("resized tensor: %s", mask)
         return mask
 
-    mask = scale_mask(mask, mask_scaling)
+    mask = _scale_mask(mask, mask_scaling)
     if preprocessing_func is not None:
         mask = preprocessing_func(mask)
     mask_as_k_inv_prop = 1 - mask_prop
     mask = (mask * mask_prop) + mask_as_k_inv_prop
 
-    def inner_loss(y_true, y_pred):
-        # Branching because tensorflows broadcasting is wonky and
-        # plaidmls concatenate is implemented ineficient.
+    def _inner_loss(y_true, y_pred):
+        """ Apply the loss function to the masked area of the image.
+
+         Parameters
+        ----------
+        y_true: tensor or variable
+            The ground truth value
+        y_pred: tensor or variable
+            The predicted value
+
+        Returns
+        -------
+        tensor
+            The Loss value
+
+        Notes
+        -----
+        Branching because TensorFlow's broadcasting is wonky and plaidML's concatenate is
+        implemented inefficiently.
+        """
         if K.backend() == "plaidml.keras.backend":
             n_true = y_true * mask
             n_pred = y_pred * mask
@@ -217,235 +307,40 @@ def PenalizedLoss(mask, loss_func,  # pylint: disable=invalid-name
             n_true = K.concatenate([y_true[:, :, :, i:i+1] * mask for i in range(3)], axis=-1)
             n_pred = K.concatenate([y_pred[:, :, :, i:i+1] * mask for i in range(3)], axis=-1)
         return loss_func(n_true, n_pred)
-    return inner_loss
+    return _inner_loss
 # <<< END: from Dfaker >>> #
 
 
-# <<< START: from DFL >>> #
-def style_loss(gaussian_blur_radius=0.0, loss_weight=1.0, wnd_size=0, step_size=1):
-    """ Style Loss from DeepFaceLab
-        https://github.com/iperov/DeepFaceLab """
-
-    if gaussian_blur_radius > 0.0:
-        gblur = gaussian_blur(gaussian_blur_radius)
-
-    def std(content, style, loss_weight):
-        content_nc = K.int_shape(content)[-1]
-        style_nc = K.int_shape(style)[-1]
-        if content_nc != style_nc:
-            raise Exception("style_loss() content_nc != style_nc")
-
-        axes = [1, 2]
-        c_mean, c_var = K.mean(content, axis=axes, keepdims=True), K.var(content,
-                                                                         axis=axes,
-                                                                         keepdims=True)
-        s_mean, s_var = K.mean(style, axis=axes, keepdims=True), K.var(style,
-                                                                       axis=axes,
-                                                                       keepdims=True)
-        c_std, s_std = K.sqrt(c_var + 1e-5), K.sqrt(s_var + 1e-5)
-
-        mean_loss = K.sum(K.square(c_mean-s_mean))
-        std_loss = K.sum(K.square(c_std-s_std))
-
-        return (mean_loss + std_loss) * (loss_weight / float(content_nc))
-
-    def func(target, style):
-        if wnd_size == 0:
-            if gaussian_blur_radius > 0.0:
-                return std(gblur(target), gblur(style), loss_weight=loss_weight)
-            return std(target, style, loss_weight=loss_weight)
-
-        # currently unused
-        if K.backend() == "plaidml.keras.backend":
-            logger.warning("plaidML backend does not support style_loss. Disabling")
-            return 0
-        shp = K.int_shape(target)[1]
-        k = (shp - wnd_size) // step_size + 1
-        if gaussian_blur_radius > 0.0:
-            target, style = gblur(target), gblur(style)
-        target = tf.image.extract_image_patches(target,
-                                                [1, k, k, 1],
-                                                [1, 1, 1, 1],
-                                                [1, step_size, step_size, 1],
-                                                "VALID")
-        style = tf.image.extract_image_patches(style,
-                                               [1, k, k, 1],
-                                               [1, 1, 1, 1],
-                                               [1, step_size, step_size, 1],
-                                               "VALID")
-        return std(target, style, loss_weight)
-
-    return func
-# <<< END: from DFL >>> #
-
-
-# <<< START: from Shoanlu GAN >>> #
-def first_order(var_x, axis=1):
-    """ First Order Function from Shoanlu GAN """
-    img_nrows = var_x.shape[1]
-    img_ncols = var_x.shape[2]
-    if axis == 1:
-        return K.abs(var_x[:, :img_nrows - 1, :img_ncols - 1, :] - var_x[:, 1:, :img_ncols - 1, :])
-    if axis == 2:
-        return K.abs(var_x[:, :img_nrows - 1, :img_ncols - 1, :] - var_x[:, :img_nrows - 1, 1:, :])
-    return None
-
-
-def calc_loss(pred, target, loss='l2'):
-    """ Calculate Loss from Shoanlu GAN """
-    if loss.lower() == "l2":
-        return K.mean(K.square(pred - target))
-    if loss.lower() == "l1":
-        return K.mean(K.abs(pred - target))
-    if loss.lower() == "cross_entropy":
-        return -K.mean(K.log(pred + K.epsilon()) * target +
-                       K.log(1 - pred + K.epsilon()) * (1 - target))
-    raise ValueError('Recieve an unknown loss type: {}.'.format(loss))
-
-
-def cyclic_loss(net_g1, net_g2, real1):
-    """ Cyclic Loss Function from Shoanlu GAN """
-    fake2 = net_g2(real1)[-1]  # fake2 ABGR
-    fake2 = Lambda(lambda x: x[:, :, :, 1:])(fake2)  # fake2 BGR
-    cyclic1 = net_g1(fake2)[-1]  # cyclic1 ABGR
-    cyclic1 = Lambda(lambda x: x[:, :, :, 1:])(cyclic1)  # cyclic1 BGR
-    loss = calc_loss(cyclic1, real1, loss='l1')
-    return loss
-
-
-def adversarial_loss(net_d, real, fake_abgr, distorted, gan_training="mixup_LSGAN", **weights):
-    """ Adversarial Loss Function from Shoanlu GAN """
-    alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
-    fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
-    fake = alpha * fake_bgr + (1-alpha) * distorted
-
-    if gan_training == "mixup_LSGAN":
-        dist = Beta(0.2, 0.2)
-        lam = dist.sample()
-        mixup = lam * concatenate([real, distorted]) + (1 - lam) * concatenate([fake, distorted])
-        pred_fake = net_d(concatenate([fake, distorted]))
-        pred_mixup = net_d(mixup)
-        loss_d = calc_loss(pred_mixup, lam * K.ones_like(pred_mixup), "l2")
-        loss_g = weights['w_D'] * calc_loss(pred_fake, K.ones_like(pred_fake), "l2")
-        mixup2 = lam * concatenate([real,
-                                    distorted]) + (1 - lam) * concatenate([fake_bgr,
-                                                                           distorted])
-        pred_fake_bgr = net_d(concatenate([fake_bgr, distorted]))
-        pred_mixup2 = net_d(mixup2)
-        loss_d += calc_loss(pred_mixup2, lam * K.ones_like(pred_mixup2), "l2")
-        loss_g += weights['w_D'] * calc_loss(pred_fake_bgr, K.ones_like(pred_fake_bgr), "l2")
-    elif gan_training == "relativistic_avg_LSGAN":
-        real_pred = net_d(concatenate([real, distorted]))
-        fake_pred = net_d(concatenate([fake, distorted]))
-        loss_d = K.mean(K.square(real_pred - K.ones_like(fake_pred)))/2
-        loss_d += K.mean(K.square(fake_pred - K.zeros_like(fake_pred)))/2
-        loss_g = weights['w_D'] * K.mean(K.square(fake_pred - K.ones_like(fake_pred)))
-
-        fake_pred2 = net_d(concatenate([fake_bgr, distorted]))
-        loss_d += K.mean(K.square(real_pred - K.mean(fake_pred2, axis=0) -
-                                  K.ones_like(fake_pred2)))/2
-        loss_d += K.mean(K.square(fake_pred2 - K.mean(real_pred, axis=0) -
-                                  K.zeros_like(fake_pred2)))/2
-        loss_g += weights['w_D'] * K.mean(K.square(real_pred - K.mean(fake_pred2, axis=0) -
-                                                   K.zeros_like(fake_pred2)))/2
-        loss_g += weights['w_D'] * K.mean(K.square(fake_pred2 - K.mean(real_pred, axis=0) -
-                                                   K.ones_like(fake_pred2)))/2
-    else:
-        raise ValueError("Receive an unknown GAN training method: {gan_training}")
-    return loss_d, loss_g
-
-
-def reconstruction_loss(real, fake_abgr, mask_eyes, model_outputs, **weights):
-    """ Reconstruction Loss Function from Shoanlu GAN """
-    alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
-    fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
-
-    loss_g = weights['w_recon'] * calc_loss(fake_bgr, real, "l1")
-    loss_g += weights['w_eyes'] * K.mean(K.abs(mask_eyes*(fake_bgr - real)))
-
-    for out in model_outputs[:-1]:
-        out_size = out.get_shape().as_list()
-        resized_real = tf.image.resize_images(real, out_size[1:3])
-        loss_g += weights['w_recon'] * calc_loss(out, resized_real, "l1")
-    return loss_g
-
-
-def edge_loss(real, fake_abgr, mask_eyes, **weights):
-    """ Edge Loss Function from Shoanlu GAN """
-    alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
-    fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
-
-    loss_g = weights['w_edge'] * calc_loss(first_order(fake_bgr, axis=1),
-                                           first_order(real, axis=1), "l1")
-    loss_g += weights['w_edge'] * calc_loss(first_order(fake_bgr, axis=2),
-                                            first_order(real, axis=2), "l1")
-    shape_mask_eyes = mask_eyes.get_shape().as_list()
-    resized_mask_eyes = tf.image.resize_images(mask_eyes,
-                                               [shape_mask_eyes[1]-1, shape_mask_eyes[2]-1])
-    loss_g += weights['w_eyes'] * K.mean(K.abs(resized_mask_eyes *
-                                               (first_order(fake_bgr, axis=1) -
-                                                first_order(real, axis=1))))
-    loss_g += weights['w_eyes'] * K.mean(K.abs(resized_mask_eyes *
-                                               (first_order(fake_bgr, axis=2) -
-                                                first_order(real, axis=2))))
-    return loss_g
-
-
-def perceptual_loss(real, fake_abgr, distorted, vggface_feats, **weights):
-    """ Perceptual Loss Function from Shoanlu GAN """
-    alpha = Lambda(lambda x: x[:, :, :, :1])(fake_abgr)
-    fake_bgr = Lambda(lambda x: x[:, :, :, 1:])(fake_abgr)
-    fake = alpha * fake_bgr + (1-alpha) * distorted
-
-    def preprocess_vggface(var_x):
-        var_x = (var_x + 1.) / 2. * 255.  # channel order: BGR
-        var_x -= [91.4953, 103.8827, 131.0912]
-        return var_x
-
-    real_sz224 = tf.image.resize_images(real, [224, 224])
-    real_sz224 = Lambda(preprocess_vggface)(real_sz224)
-    dist = Beta(0.2, 0.2)
-    lam = dist.sample()  # use mixup trick here to reduce foward pass from 2 times to 1.
-    mixup = lam*fake_bgr + (1-lam)*fake
-    fake_sz224 = tf.image.resize_images(mixup, [224, 224])
-    fake_sz224 = Lambda(preprocess_vggface)(fake_sz224)
-    real_feat112, real_feat55, real_feat28, real_feat7 = vggface_feats(real_sz224)
-    fake_feat112, fake_feat55, fake_feat28, fake_feat7 = vggface_feats(fake_sz224)
-
-    # Apply instance norm on VGG(ResNet) features
-    # From MUNIT https://github.com/NVlabs/MUNIT
-    loss_g = 0
-
-    def instnorm():
-        return InstanceNormalization()
-
-    loss_g += weights['w_pl'][0] * calc_loss(instnorm()(fake_feat7),
-                                             instnorm()(real_feat7), "l2")
-    loss_g += weights['w_pl'][1] * calc_loss(instnorm()(fake_feat28),
-                                             instnorm()(real_feat28), "l2")
-    loss_g += weights['w_pl'][2] * calc_loss(instnorm()(fake_feat55),
-                                             instnorm()(real_feat55), "l2")
-    loss_g += weights['w_pl'][3] * calc_loss(instnorm()(fake_feat112),
-                                             instnorm()(real_feat112), "l2")
-    return loss_g
-# <<< END: from Shoanlu GAN >>> #
-
-
 def generalized_loss(y_true, y_pred, alpha=1.0, beta=1.0/255.0):
-    """
-    generalized function used to return a large variety of mathematical loss functions
-    primary benefit is smooth, differentiable version of L1 loss
-
-    Barron, J. A More General Robust Loss Function
-    https://arxiv.org/pdf/1701.03077.pdf
-    Parameters:
-        alpha: penalty factor. larger number give larger weight to large deviations
-        beta: scale factor used to adjust to the input scale (i.e. inputs of mean 1e-4 or 256 )
-    Return:
-        a loss value from the results of function(y_pred - y_true)
-    Example:
-        a=1.0, x>>c , c=1.0/255.0 will give a smoothly differentiable version of L1 / MAE loss
-        a=1.999999 (lim as a->2), beta=1.0/255.0 will give L2 / RMSE loss
+    """ Generalized function used to return a large variety of mathematical loss functions.
+
+    The primary benefit is a smooth, differentiable version of L1 loss.
+
+    Parameters
+    ----------
+    y_true: tensor or variable
+        The ground truth value
+    y_pred: tensor or variable
+        The predicted value
+    alpha: float, optional
+        Penalty factor. Larger number give larger weight to large deviations. Default: `1.0`
+    beta: float, optional
+        Scale factor used to adjust to the input scale (i.e. inputs of mean `1e-4` or `256`).
+        Default: `1.0/255.0`
+
+    Returns
+    -------
+    tensor
+        The loss value from the results of function(y_pred - y_true)
+
+    References
+    ----------
+    Barron, J. A More General Robust Loss Function - https://arxiv.org/pdf/1701.03077.pdf
+
+    Example
+    -------
+    >>> a=1.0, x>>c , c=1.0/255.0  # will give a smoothly differentiable version of L1 / MAE loss
+    >>> a=1.999999 (limit as a->2), beta=1.0/255.0 # will give L2 / RMSE loss
     """
     diff = y_pred - y_true
     second = (K.pow(K.pow(diff/beta, 2.) / K.abs(2.-alpha) + 1., (alpha/2.)) - 1.)
@@ -454,18 +349,21 @@ def generalized_loss(y_true, y_pred, alpha=1.0, beta=1.0/255.0):
     return loss
 
 
-def l_p_norm(y_true, y_pred, p_norm=np.inf):
-    """
-    Calculate the L-p norm as a loss function,
-    valid choics of p are [0,1,no.inf]
-    """
-    diff = y_true - y_pred
-    loss = tf.norm(diff, ord=p_norm, axis=-1)
-    return loss
-
-
 def l_inf_norm(y_true, y_pred):
-    """ Calculate the L-inf norm as a loss function """
+    """ Calculate the L-inf norm as a loss function.
+
+    Parameters
+    ----------
+    y_true: tensor or variable
+        The ground truth value
+    y_pred: tensor or variable
+        The predicted value
+
+    Returns
+    -------
+    tensor
+        The loss value
+    """
     diff = K.abs(y_true - y_pred)
     max_loss = K.max(diff, axis=(1, 2), keepdims=True)
     loss = K.mean(max_loss, axis=-1)
@@ -473,53 +371,65 @@ def l_inf_norm(y_true, y_pred):
 
 
 def gradient_loss(y_true, y_pred):
-    """
-    Calculates the first and second order gradient difference between pixels of
-    an image in the x and y dimensions. These gradients are then compared between
-    the ground truth and the predicted image and the difference is taken. When
-    used as a loss, its minimization will result in predicted images approaching
-    the same level of sharpness / blurriness as the ground truth.
-
-    TV+TV2 Regularization with Nonconvex Sparseness-Inducing Penalty
-    for Image Restoration, Chengwu Lu & Hua Huang, 2014
-    (http://downloads.hindawi.com/journals/mpe/2014/790547.pdf)
-
-    Parameters:
-        y_true: The predicted frames at each scale
-        y_true: The ground truth frames at each scale
-    Return:
-        The GD loss
+    """ Gradient Loss Function.
+
+    Calculates the first and second order gradient difference between pixels of an image in the x
+    and y dimensions. These gradients are then compared between the ground truth and the predicted
+    image and the difference is taken. When used as a loss, its minimization will result in
+    predicted images approaching the same level of sharpness / blurriness as the ground truth.
+
+    Parameters
+    ----------
+    y_true: tensor or variable
+        The ground truth value
+    y_pred: tensor or variable
+        The predicted value
+
+    Returns
+    -------
+    tensor
+        The loss value
+
+    References
+    ----------
+    TV+TV2 Regularization with Non-Convex Sparseness-Inducing Penalty for Image Restoration,
+    Chengwu Lu & Hua Huang, 2014 - http://downloads.hindawi.com/journals/mpe/2014/790547.pdf
     """
 
-    def diff_x(img):
+    def _diff_x(img):
+        """ X Difference """
         x_left = img[:, :, 1:2, :] - img[:, :, 0:1, :]
         x_inner = img[:, :, 2:, :] - img[:, :, :-2, :]
         x_right = img[:, :, -1:, :] - img[:, :, -2:-1, :]
         x_out = K.concatenate([x_left, x_inner, x_right], axis=2)
         return x_out * 0.5
 
-    def diff_y(img):
+    def _diff_y(img):
+        """ Y Difference """
         y_top = img[:, 1:2, :, :] - img[:, 0:1, :, :]
         y_inner = img[:, 2:, :, :] - img[:, :-2, :, :]
         y_bot = img[:, -1:, :, :] - img[:, -2:-1, :, :]
         y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)
         return y_out * 0.5
 
-    def diff_xx(img):
+    def _diff_xx(img):
+        """ X-X Difference """
         x_left = img[:, :, 1:2, :] + img[:, :, 0:1, :]
         x_inner = img[:, :, 2:, :] + img[:, :, :-2, :]
         x_right = img[:, :, -1:, :] + img[:, :, -2:-1, :]
         x_out = K.concatenate([x_left, x_inner, x_right], axis=2)
         return x_out - 2.0 * img
 
-    def diff_yy(img):
+    def _diff_yy(img):
+        """ Y-Y Difference """
         y_top = img[:, 1:2, :, :] + img[:, 0:1, :, :]
         y_inner = img[:, 2:, :, :] + img[:, :-2, :, :]
         y_bot = img[:, -1:, :, :] + img[:, -2:-1, :, :]
         y_out = K.concatenate([y_top, y_inner, y_bot], axis=1)
         return y_out - 2.0 * img
 
-    def diff_xy(img):
+    def _diff_xy(img):
+        """ X-Y Difference """
         # xout1
         top_left = img[:, 1:2, 1:2, :] + img[:, 0:1, 0:1, :]
         inner_left = img[:, 2:, 1:2, :] + img[:, :-2, 0:1, :]
@@ -559,59 +469,65 @@ def gradient_loss(y_true, y_pred):
     tv_weight = 1.0
     tv2_weight = 1.0
     loss = 0.0
-    loss += tv_weight * (generalized_loss(diff_x(y_true), diff_x(y_pred), alpha=1.9999) +
-                         generalized_loss(diff_y(y_true), diff_y(y_pred), alpha=1.9999))
-    loss += tv2_weight * (generalized_loss(diff_xx(y_true), diff_xx(y_pred), alpha=1.9999) +
-                          generalized_loss(diff_yy(y_true), diff_yy(y_pred), alpha=1.9999) +
-                          generalized_loss(diff_xy(y_true), diff_xy(y_pred), alpha=1.9999) * 2.)
+    loss += tv_weight * (generalized_loss(_diff_x(y_true), _diff_x(y_pred), alpha=1.9999) +
+                         generalized_loss(_diff_y(y_true), _diff_y(y_pred), alpha=1.9999))
+    loss += tv2_weight * (generalized_loss(_diff_xx(y_true), _diff_xx(y_pred), alpha=1.9999) +
+                          generalized_loss(_diff_yy(y_true), _diff_yy(y_pred), alpha=1.9999) +
+                          generalized_loss(_diff_xy(y_true), _diff_xy(y_pred), alpha=1.9999) * 2.)
     loss = loss / (tv_weight + tv2_weight)
     # TODO simplify to use MSE instead
     return loss
 
 
 def scharr_edges(image, magnitude):
-    """
-    Returns a tensor holding modified Scharr edge maps.
-    Arguments:
-    image: Image tensor with shape [batch_size, h, w, d] and type float32.
-    The image(s) must be 2x2 or larger.
-    magnitude: Boolean to determine if the edge magnitude or edge direction is returned
-    Returns:
-    Tensor holding edge maps for each channel. Returns a tensor with shape
-    [batch_size, h, w, d, 2] where the last two dimensions hold [[dy[0], dx[0]],
-    [dy[1], dx[1]], ..., [dy[d-1], dx[d-1]]] calculated using the Scharr filter.
+    """ Returns a tensor holding modified Scharr edge maps.
+
+    Parameters
+    ----------
+    image: tensor
+        Image tensor with shape [batch_size, h, w, d] and type float32. The image(s) must be 2x2
+        or larger.
+    magnitude: bool
+        Boolean to determine if the edge magnitude or edge direction is returned
+
+    Returns
+    -------
+    tensor
+        Tensor holding edge maps for each channel. Returns a tensor with shape `[batch_size, h, w,
+        d, 2]` where the last two dimensions hold `[[dy[0], dx[0]], [dy[1], dx[1]], ..., [dy[d-1],
+        dx[d-1]]]` calculated using the Scharr filter.
     """
 
     # Define vertical and horizontal Scharr filters.
-    static_image_shape = image.get_shape()
+    static_image_shape = image.shape.dims if get_backend() == "amd" else image.get_shape()
     image_shape = K.shape(image)
 
     # 5x5 modified Scharr kernel ( reshape to (5,5,1,2) )
-    matrix = [[[[0.00070, 0.00070]],
-               [[0.00520, 0.00370]],
-               [[0.03700, 0.00000]],
-               [[0.00520, -0.0037]],
-               [[0.00070, -0.0007]]],
-              [[[0.00370, 0.00520]],
-               [[0.11870, 0.11870]],
-               [[0.25890, 0.00000]],
-               [[0.11870, -0.1187]],
-               [[0.00370, -0.0052]]],
-              [[[0.00000, 0.03700]],
-               [[0.00000, 0.25890]],
-               [[0.00000, 0.00000]],
-               [[0.00000, -0.2589]],
-               [[0.00000, -0.0370]]],
-              [[[-0.0037, 0.00520]],
-               [[-0.1187, 0.11870]],
-               [[-0.2589, 0.00000]],
-               [[-0.1187, -0.1187]],
-               [[-0.0037, -0.0052]]],
-              [[[-0.0007, 0.00070]],
-               [[-0.0052, 0.00370]],
-               [[-0.0370, 0.00000]],
-               [[-0.0052, -0.0037]],
-               [[-0.0007, -0.0007]]]]
+    matrix = np.array([[[[0.00070, 0.00070]],
+                        [[0.00520, 0.00370]],
+                        [[0.03700, 0.00000]],
+                        [[0.00520, -0.0037]],
+                        [[0.00070, -0.0007]]],
+                       [[[0.00370, 0.00520]],
+                        [[0.11870, 0.11870]],
+                        [[0.25890, 0.00000]],
+                        [[0.11870, -0.1187]],
+                        [[0.00370, -0.0052]]],
+                       [[[0.00000, 0.03700]],
+                        [[0.00000, 0.25890]],
+                        [[0.00000, 0.00000]],
+                        [[0.00000, -0.2589]],
+                        [[0.00000, -0.0370]]],
+                       [[[-0.0037, 0.00520]],
+                        [[-0.1187, 0.11870]],
+                        [[-0.2589, 0.00000]],
+                        [[-0.1187, -0.1187]],
+                        [[-0.0037, -0.0052]]],
+                       [[[-0.0007, 0.00070]],
+                        [[-0.0052, 0.00370]],
+                        [[-0.0370, 0.00000]],
+                        [[-0.0052, -0.0037]],
+                        [[-0.0007, -0.0007]]]])
     num_kernels = [2]
     kernels = K.constant(matrix, dtype='float32')
     kernels = K.tile(kernels, [1, 1, image_shape[-1], 1])
@@ -619,7 +535,7 @@ def scharr_edges(image, magnitude):
     # Use depth-wise convolution to calculate edge maps per channel.
     # Output tensor has shape [batch_size, h, w, d * num_kernels].
     pad_sizes = [[0, 0], [2, 2], [2, 2], [0, 0]]
-    padded = tf.pad(image, pad_sizes, mode='REFLECT')
+    padded = pad(image, pad_sizes, mode='REFLECT')
     output = K.depthwise_conv2d(padded, kernels)
 
     if not magnitude:  # direction of edges
@@ -627,19 +543,34 @@ def scharr_edges(image, magnitude):
         shape = K.concatenate([image_shape, num_kernels], axis=0)
         output = K.reshape(output, shape=shape)
         output.set_shape(static_image_shape.concatenate(num_kernels))
-        output = tf.atan(K.squeeze(output[:, :, :, :, 0] / output[:, :, :, :, 1]))
-    # magnitude of edges -- unified x & y edges don't work well with NN
-
+        output = tf.atan(K.squeeze(output[:, :, :, :, 0] / output[:, :, :, :, 1], axis=None))
+    # magnitude of edges -- unified x & y edges don't work well with Neural Networks
     return output
 
 
 def gmsd_loss(y_true, y_pred):
-    """
-    Improved image quality metric over MS-SSIM with easier calc
+    """ Gradient Magnitude Similarity Deviation Loss.
+
+    Improved image quality metric over MS-SSIM with easier calculations
+
+    Parameters
+    ----------
+    y_true: tensor or variable
+        The ground truth value
+    y_pred: tensor or variable
+        The predicted value
+
+    Returns
+    -------
+    tensor
+        The loss value
+
+    References
+    ----------
     http://www4.comp.polyu.edu.hk/~cslzhang/IQA/GMSD/GMSD.htm
     https://arxiv.org/ftp/arxiv/papers/1308/1308.3052.pdf
-    """
 
+    """
     true_edge = scharr_edges(y_true, True)
     pred_edge = scharr_edges(y_pred, True)
     ephsilon = 0.0025
@@ -651,291 +582,58 @@ def gmsd_loss(y_true, y_pred):
     return gmsd
 
 
-def ms_ssim_calc(img1, img2, max_val=1.0, power_factors=(0.0517, 0.3295, 0.3462, 0.2726)):
-    """
-    Computes the MS-SSIM between img1 and img2.
-    This function assumes that `img1` and `img2` are image batches, i.e. the last
-    three dimensions are [height, width, channels].
-    Note: The true SSIM is only defined on grayscale.  This function does not
-    perform any colorspace transform.  (If input is already YUV, then it will
-    compute YUV SSIM average.)
-    Original paper: Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. "Multiscale
-    structural similarity for image quality assessment." Signals, Systems and
-    Computers, 2004.
-    Arguments:
-    img1: First image batch.
-    img2: Second image batch. Must have the same rank as img1.
-    max_val: The dynamic range of the images (i.e., the difference between the
-      maximum the and minimum allowed values).
-    power_factors: Iterable of weights for each of the scales. The number of
-      scales used is the length of the list. Index 0 is the unscaled
-      resolution's weight and each increasing scale corresponds to the image
-      being downsampled by 2.  Defaults to (0.0448, 0.2856, 0.3001, 0.2363,
-      0.1333), which are the values obtained in the original paper.
-    Returns:
-    A tensor containing an MS-SSIM value for each image in batch.  The values
-    are in range [0, 1].  Returns a tensor with shape:
-    broadcast(img1.shape[:-3], img2.shape[:-3]).
-    """
-
-    def _verify_compatible_image_shapes(img1, img2):
-        """
-        Checks if two image tensors are compatible for applying SSIM or PSNR.
-        This function checks if two sets of images have ranks at least 3, and if the
-        last three dimensions match.
-        Args:
-        img1: Tensor containing the first image batch.
-        img2: Tensor containing the second image batch.
-        Returns:
-        A tuple containing: the first tensor shape, the second tensor shape, and a
-        list of control_flow_ops.Assert() ops implementing the checks.
-        Raises:
-        ValueError: When static shape check fails.
-        """
-        shape1 = img1.get_shape().with_rank_at_least(3)
-        shape2 = img2.get_shape().with_rank_at_least(3)
-        shape1[-3:].assert_is_compatible_with(shape2[-3:])
-
-        if shape1.ndims is not None and shape2.ndims is not None:
-            for dim1, dim2 in zip(reversed(shape1[:-3]), reversed(shape2[:-3])):
-                if not (dim1 == 1 or dim2 == 1 or dim1.is_compatible_with(dim2)):
-                    raise ValueError('Two images are not compatible: %s and %s' % (shape1, shape2))
-
-        # Now assign shape tensors.
-        shape1, shape2 = tf.shape_n([img1, img2])
-
-        # TODO(sjhwang): Check if shape1[:-3] and shape2[:-3] are broadcastable.
-        checks = []
-        checks.append(tf.Assert(tf.greater_equal(tf.size(shape1), 3),
-                                [shape1, shape2], summarize=10))
-        checks.append(tf.Assert(tf.reduce_all(tf.equal(shape1[-3:], shape2[-3:])),
-                                [shape1, shape2], summarize=10))
-
-        return shape1, shape2, checks
-
-    def _ssim_per_channel(img1, img2, max_val=1.0):
-        """
-        Computes SSIM index between img1 and img2 per color channel.
-        This function matches the standard SSIM implementation from:
-        Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image
-        quality assessment: from error visibility to structural similarity. IEEE
-        transactions on image processing.
-        Details:
-        - 11x11 Gaussian filter of width 1.5 is used.
-        - k1 = 0.01, k2 = 0.03 as in the original paper.
-        Args:
-        img1: First image batch.
-        img2: Second image batch.
-        max_val: The dynamic range of the images (i.e., the difference between the
-          maximum the and minimum allowed values).
-        Returns:
-        A pair of tensors containing and channel-wise SSIM and contrast-structure
-        values. The shape is [..., channels].
-        """
-
-        def _fspecial_gauss(size, sigma):
-            """ Function to mimic the 'fspecial' gaussian MATLAB function. """
-
-            size = tf.convert_to_tensor(size, 'int32')
-            sigma = tf.convert_to_tensor(sigma)
-            coords = tf.cast(tf.range(size), sigma.dtype)
-            coords -= tf.cast(size - 1, sigma.dtype) / 2.0
-
-            gauss = tf.square(coords)
-            gauss *= -0.5 / tf.square(sigma)
-            gauss = tf.reshape(gauss, shape=[1, -1]) + tf.reshape(gauss, shape=[-1, 1])
-            gauss = tf.reshape(gauss, shape=[1, -1])  # For tf.nn.softmax().
-            gauss = tf.nn.softmax(gauss)
-            return tf.reshape(gauss, shape=[size, size, 1, 1])
-
-        def _ssim_helper(img1, img2, max_val, kernel, compensation=1.):
-            """
-            Helper function for computing SSIM.
-            SSIM estimates covariances with weighted sums.  The default parameters
-            use a biased estimate of the covariance:
-            Suppose `reducer` is a weighted sum, then the mean estimators are
-            mu_x = sum_i w_i x_i,
-            mu_y = sum_i w_i y_i,
-            where w_i's are the weighted-sum weights, and covariance estimator is
-            cov_{xy} = sum_i w_i (x_i - mu_x) (y_i - mu_y)
-            with assumption sum_i w_i = 1. This covariance estimator is biased, since
-            E[cov_{xy}] = (1 - sum_i w_i ^ 2) Cov(X, Y).
-            For SSIM measure with unbiased covariance estimators, pass as `compensation`
-            argument (1 - sum_i w_i ^ 2).
-            Arguments:
-            img1: First set of images.
-            img2: Second set of images.
-            reducer: Function that computes 'local' averages from set of images.
-              For non-covolutional version, this is usually tf.reduce_mean(img1, [1, 2]),
-              and for convolutional version, this is usually tf.nn.avg_pool or
-              tf.nn.conv2d with weighted-sum kernel.
-            max_val: The dynamic range (i.e., the difference between the maximum
-              possible allowed value and the minimum allowed value).
-            compensation: Compensation factor. See above.
-            Returns:
-            A pair containing the luminance measure, and the contrast-structure measure.
-            """
-
-            def reducer(img1, kernel):
-                shape = tf.shape(img1)
-                img1 = tf.reshape(img1, shape=tf.concat([[-1], shape[-3:]], 0))
-                img2 = tf.nn.depthwise_conv2d(img1, kernel, strides=[1, 1, 1, 1], padding='VALID')
-                return tf.reshape(img2, tf.concat([shape[:-3], tf.shape(img2)[1:]], 0))
-
-            c_one = (0.01 * max_val) ** 2
-            c_two = ((0.03 * max_val)) ** 2 * compensation
-
-            # SSIM luminance measure is
-            # (2 * mu_x * mu_y + c_one) / (mu_x ** 2 + mu_y ** 2 + c_one).
-            mean0 = reducer(img1, kernel)
-            mean1 = reducer(img2, kernel)
-            num0 = mean0 * mean1 * 2.
-            den0 = tf.square(mean0) + tf.square(mean1)
-            luminance = (num0 + c_one) / (den0 + c_one)
-
-            # SSIM contrast-structure measure is
-            #   (2 * cov_{xy} + c_two) / (cov_{xx} + cov_{yy} + c_two).
-            # Note that `reducer` is a weighted sum with weight w_k, \sum_i w_i = 1, then
-            #   cov_{xy} = \sum_i w_i (x_i - \mu_x) (y_i - \mu_y)
-            #          = \sum_i w_i x_i y_i - (\sum_i w_i x_i) (\sum_j w_j y_j).
-            num1 = reducer(img1 * img2, kernel) * 2.0
-            den1 = reducer(tf.square(img1) + tf.square(img2), kernel)
-            c_s = (num1 - num0 + c_two) / (den1 - den0 + c_two)
-
-            # SSIM score is the product of the luminance and contrast-structure measures.
-            return luminance, c_s
-
-        filter_size = tf.constant(9, dtype='int32')  # changed from 11 to 9 due
-        filter_sigma = tf.constant(1.5, dtype=img1.dtype)
-
-        shape1, shape2 = tf.shape_n([img1, img2])
-        checks = [tf.Assert(tf.reduce_all(tf.greater_equal(shape1[-3:-1], filter_size)),
-                            [shape1, filter_size], summarize=8),
-                  tf.Assert(tf.reduce_all(tf.greater_equal(shape2[-3:-1], filter_size)),
-                            [shape2, filter_size], summarize=8)]
-
-        # Enforce the check to run before computation.
-        with tf.control_dependencies(checks):
-            img1 = tf.identity(img1)
-
-        # TODO(sjhwang): Try to cache kernels and compensation factor.
-        kernel = _fspecial_gauss(filter_size, filter_sigma)
-        kernel = tf.tile(kernel, multiples=[1, 1, shape1[-1], 1])
-
-        # The correct compensation factor is `1.0 - tf.reduce_sum(tf.square(kernel))`,
-        # but to match MATLAB implementation of MS-SSIM, we use 1.0 instead.
-        compensation = 1.
-
-        # TODO(sjhwang): Try FFT.
-        # TODO(sjhwang): Gaussian kernel is separable in space. Consider applying
-        #   1-by-n and n-by-1 Gaussain filters instead of an n-by-n filter.
-
-        luminance, c_s = _ssim_helper(img1, img2, max_val, kernel, compensation)
-
-        # Average over the second and the third from the last: height, width.
-        axes = tf.constant([-3, -2], dtype='int32')
-        ssim_val = tf.reduce_mean(luminance * c_s, axes)
-        c_s = tf.reduce_mean(c_s, axes)
-        return ssim_val, c_s
-
-    def do_pad(images, remainder):
-        padding = tf.expand_dims(remainder, -1)
-        padding = tf.pad(padding, [[1, 0], [1, 0]])
-        return [tf.pad(x, padding, mode='SYMMETRIC') for x in images]
-
-    # Shape checking.
-    shape1 = img1.get_shape().with_rank_at_least(3)
-    shape2 = img2.get_shape().with_rank_at_least(3)
-    shape1[-3:].merge_with(shape2[-3:])
-
-    with tf.name_scope(None, 'MS-SSIM', [img1, img2]):
-        shape1, shape2, checks = _verify_compatible_image_shapes(img1, img2)
-    with tf.control_dependencies(checks):
-        img1 = tf.identity(img1)
-
-    # Need to convert the images to float32.  Scale max_val accordingly so that
-    # SSIM is computed correctly.
-    max_val = tf.cast(max_val, img1.dtype)
-    max_val = tf.image.convert_image_dtype(max_val, 'float32')
-    img1 = tf.image.convert_image_dtype(img1, 'float32')
-    img2 = tf.image.convert_image_dtype(img2, 'float32')
-
-    imgs = [img1, img2]
-    shapes = [shape1, shape2]
-
-    # img1 and img2 are assumed to be a (multi-dimensional) batch of
-    # 3-dimensional images (height, width, channels). `heads` contain the batch
-    # dimensions, and `tails` contain the image dimensions.
-    heads = [s[:-3] for s in shapes]
-    tails = [s[-3:] for s in shapes]
-
-    divisor = [1, 2, 2, 1]
-    divisor_tensor = tf.constant(divisor[1:], dtype='int32')
-
-    mc_s = []
-    for k in range(len(power_factors)):
-        with tf.name_scope(None, 'Scale%d' % k, imgs):
-            if k > 0:
-                # Avg pool takes rank 4 tensors. Flatten leading dimensions.
-                zipped = zip(imgs, tails)
-                flat_imgs = [tf.reshape(x, tf.concat([[-1], t], 0)) for x, t in zipped]
-                remainder = tails[0] % divisor_tensor
-                need_padding = tf.reduce_any(tf.not_equal(remainder, 0))
-                padded = tf.cond(need_padding,
-                                 lambda: do_pad(flat_imgs, remainder), lambda: flat_imgs)
-
-                downscaled = [tf.nn.avg_pool(x,
-                                             ksize=divisor,
-                                             strides=divisor,
-                                             padding='VALID') for x in padded]
-                tails = [x[1:] for x in tf.shape_n(downscaled)]
-                zipper = zip(downscaled, heads, tails)
-                imgs = [tf.reshape(x, tf.concat([h, t], 0)) for x, h, t in zipper]
-
-            # Overwrite previous ssim value since we only need the last one.
-            ssim_per_channel, c_s = _ssim_per_channel(*imgs, max_val=max_val)
-            mc_s.append(tf.nn.relu(c_s))
-
-    # Remove the c_s score for the last scale. In the MS-SSIM calculation,
-    # we use the l(p) at the highest scale. l(p) * c_s(p) is ssim(p).
-    mc_s.pop()  # Remove the c_s score for the last scale.
-    mcs_and_ssim = tf.stack(mc_s + [tf.nn.relu(ssim_per_channel)], axis=-1)
-    # Take weighted geometric mean across the scale axis.
-    ms_ssim = tf.reduce_prod(tf.pow(mcs_and_ssim, power_factors), [-1])
-
-    return tf.reduce_mean(ms_ssim, [-1])  # Avg over color channels.
-
-
-def ms_ssim_loss(y_true, y_pred):
-    """ Keras loss function for MS-SSIM """
-    expanded = K.expand_dims(1.0 - ms_ssim_calc(y_true, y_pred), axis=-1)
-    loss = K.expand_dims(expanded, axis=-1)
-    # need to expand to [1,height,width] dimensions for Keras. modify to not be hard-coded
-    return K.tile(loss, [1, 64, 64])
-
-
 # Gaussian Blur is here as it is only used for losses.
 # It was previously kept in lib/model/masks but the import of keras backend
 # breaks plaidml
 def gaussian_blur(radius=2.0):
-    """ From https://github.com/iperov/DeepFaceLab
-        Used for blurring mask in training """
-    def gaussian(var_x, radius, sigma):
+    """ Apply gaussian blur to an input.
+
+    Used for blurring mask in training.
+
+    Parameters
+    ----------
+    radius: float, optional
+        The kernel radius for applying gaussian blur. Default: `2.0`
+
+    Returns
+    -------
+    tensor
+        The input tensor with gaussian blurring applied
+
+    References
+    ----------
+    https://github.com/iperov/DeepFaceLab
+    """
+    def _gaussian(var_x, radius, sigma):
+        """ Obtain the gaussian kernel. """
         return np.exp(-(float(var_x) - float(radius)) ** 2 / (2 * sigma ** 2))
 
-    def make_kernel(sigma):
+    def _make_kernel(sigma):
+        """ Make the gaussian kernel. """
         kernel_size = max(3, int(2 * 2 * sigma + 1))
         mean = np.floor(0.5 * kernel_size)
-        kernel_1d = np.array([gaussian(x, mean, sigma) for x in range(kernel_size)])
+        kernel_1d = np.array([_gaussian(x, mean, sigma) for x in range(kernel_size)])
         np_kernel = np.outer(kernel_1d, kernel_1d).astype(dtype=K.floatx())
         kernel = np_kernel / np.sum(np_kernel)
         return kernel
 
-    gauss_kernel = make_kernel(radius)
+    gauss_kernel = _make_kernel(radius)
     gauss_kernel = gauss_kernel[:, :, np.newaxis, np.newaxis]
 
-    def func(input_):
-        inputs = [input_[:, :, :, i:i + 1] for i in range(K.int_shape(input_)[-1])]
+    def func(input_tensor):
+        """ Apply gaussian blurring to the input tensor
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input to have gaussian blurring applied.
+
+        Returns
+        -------
+        tensor
+            The input with gaussian blurring applied
+        """
+        inputs = [input_tensor[:, :, :, i:i + 1] for i in range(K.int_shape(input_tensor)[-1])]
         outputs = [K.conv2d(inp, K.constant(gauss_kernel), strides=(1, 1), padding="same")
                    for inp in inputs]
         return K.concatenate(outputs, axis=-1)
diff --git a/lib/model/nn_blocks.py b/lib/model/nn_blocks.py
index 516e421..b2d3aeb 100644
--- a/lib/model/nn_blocks.py
+++ b/lib/model/nn_blocks.py
@@ -1,31 +1,54 @@
 #!/usr/bin/env python3
-""" Neural Network Blocks for faceswap.py
-    Blocks from:
-        the original https://www.reddit.com/r/deepfakes/ code sample + contribs
-        dfaker: https://github.com/dfaker/df
-        shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
+""" Neural Network Blocks for faceswap.py. """
 
 import logging
-import tensorflow as tf
-import keras.backend as K
 
-from keras.layers import (add, Add, BatchNormalization, concatenate, Lambda, regularizers,
-                          Permute, Reshape, SeparableConv2D, Softmax, UpSampling2D)
+from keras.layers import Add, SeparableConv2D
 from keras.layers.advanced_activations import LeakyReLU
 from keras.layers.convolutional import Conv2D
 from keras.layers.core import Activation
 from keras.initializers import he_uniform, VarianceScaling
 from .initializers import ICNR, ConvolutionAware
-from .layers import PixelShuffler, SubPixelUpscaling, ReflectionPadding2D, Scale
-from .normalization import GroupNormalization, InstanceNormalization
+from .layers import PixelShuffler, SubPixelUpscaling, ReflectionPadding2D
+from .normalization import InstanceNormalization
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class NNBlocks():
-    """ Blocks to use for creating models """
-    def __init__(self, use_subpixel=False, use_icnr_init=False, use_convaware_init=False,
-                 use_reflect_padding=False, first_run=True):
+    """ Blocks that are often used for multiple models are stored here for easy access.
+
+    This class is always brought in as ``self.blocks`` in all model plugins so that all models
+    have access to them.
+
+    The parameters passed into this class should ultimately originate from the user's training
+    configuration file, rather than being hard-coded at the plugin level.
+
+    Parameters
+    ----------
+    use_subpixel: bool, Optional
+        ``True`` if sub-pixel up-scaling layer should be used instead of pixel shuffler for
+        up-scaling. This option is deprecated as sub-pixel up-scaling is Nvidia only, but is kept
+        for legacy models. Default: ``False``
+    use_icnr_init: bool, Optional
+        ``True`` if ICNR initialization should be used rather than the default. Default: ``False``
+    use_convaware_init: bool, Optional
+        ``True`` if Convolutional Aware initialization should be used rather than the default.
+        Default: ``False``
+    use_reflect_padding: bool, Optional
+        ``True`` if Reflect Padding initialization should be used rather than the padding.
+        Default: ``False``
+    first_run: bool, Optional
+        ``True`` if a model is being created for the first time, ``False`` if a model is being
+        resumed. Used to prevent Convolutional Aware weights from being calculated when a model
+        is being reloaded. Default: ``True``
+    """
+    def __init__(self,
+                 use_subpixel=False,
+                 use_icnr_init=False,
+                 use_convaware_init=False,
+                 use_reflect_padding=False,
+                 first_run=True):
         logger.debug("Initializing %s: (use_subpixel: %s, use_icnr_init: %s, use_convaware_init: "
                      "%s, use_reflect_padding: %s, first_run: %s)",
                      self.__class__.__name__, use_subpixel, use_icnr_init, use_convaware_init,
@@ -41,18 +64,45 @@ class NNBlocks():
                         "few minutes...")
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def get_name(self, name):
-        """ Return unique layer name for requested block """
+    def _get_name(self, name):
+        """ Return unique layer name for requested block.
+
+        As blocks can be used multiple times, auto appends an integer to the end of the requested
+        name to keep all block names unique
+
+        Parameters
+        ----------
+        name: str
+            The requested name for the layer
+
+        Returns
+        -------
+        str
+            The unique name for this layer
+        """
         self.names[name] = self.names.setdefault(name, -1) + 1
         name = "{}_{}".format(name, self.names[name])
         logger.debug("Generating block name: %s", name)
         return name
 
-    def set_default_initializer(self, kwargs):
-        """ Sets the default initializer for conv2D and Seperable conv2D layers
-            to conv_aware or he_uniform().
-            if a specific initializer has been passed in then the specified initializer
-            will be used rather than the default """
+    def _set_default_initializer(self, kwargs):
+        """ Sets the default initializer for convolution 2D and Seperable convolution 2D layers
+            to Convolutional Aware or he_uniform.
+
+            if a specific initializer has been passed in from the model plugin, then the specified
+            initializer will be used rather than the default.
+
+            Parameters
+            ----------
+            kwargs: dict
+                The keyword arguments for the current layer
+
+            Returns
+            -------
+            dict
+                The keyword arguments for the current layer with the initializer updated to
+                the select default value
+            """
         if "kernel_initializer" in kwargs:
             logger.debug("Using model specified initializer: %s", kwargs["kernel_initializer"])
             return kwargs
@@ -69,40 +119,126 @@ class NNBlocks():
         return kwargs
 
     @staticmethod
-    def switch_kernel_initializer(kwargs, initializer):
-        """ Switch the initializer in the given kwargs to the given initializer
-            and return the previous initializer to caller """
+    def _switch_kernel_initializer(kwargs, initializer):
+        """ Switch the initializer in the given kwargs to the given initializer and return the
+        previous initializer to caller.
+
+        For residual blocks and up-scaling, user selected initializer methods should replace those
+        set by the model. This method updates the initializer for the layer, and returns the
+        original initializer so that it can be set back to the layer's key word arguments for
+        subsequent layers where the initializer should not be switched.
+
+        Parameters
+        ----------
+        kwargs: dict
+            The keyword arguments for the current layer
+        initializer: keras or faceswap initializer class
+            The initializer that should replace the current initializer that exists in keyword
+            arguments
+
+        Returns
+        -------
+        keras or faceswap initializer class
+            The original initializer that existed in the given keyword arguments
+        """
         original = kwargs.get("kernel_initializer", None)
         kwargs["kernel_initializer"] = initializer
         logger.debug("Switched kernel_initializer from %s to %s", original, initializer)
         return original
 
-    def conv2d(self, inp, filters, kernel_size, strides=(1, 1), padding="same", **kwargs):
-        """ A standard conv2D layer with correct initialization """
-        logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, padding: %s, "
-                     "kwargs: %s)", inp, filters, kernel_size, strides, padding, kwargs)
+    def conv2d(self, input_tensor, filters, kernel_size, strides=(1, 1), padding="same", **kwargs):
+        """ A standard Convolution 2D layer with correct initialization.
+
+        This layer creates a convolution kernel that is convolved with the layer input to produce
+        a tensor of outputs.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input tensor to the layer
+        filters: int
+            The dimensionality of the output space (i.e. the number of output filters in the
+            convolution)
+        kernel_size: int
+            An integer or tuple/list of 2 integers, specifying the height and width of the 2D
+            convolution window. Can be a single integer to specify the same value for all spatial
+            dimensions
+        strides: tuple, optional
+            An integer or tuple/list of 2 integers, specifying the strides of the convolution along
+            the height and width. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: `(1, 1)`
+        padding: ["valid", "same"], optional
+            The padding to use. Default: `"same"`
+        kwargs: dict
+            Any additional Keras standard layer keyword arguments
+
+        Returns
+        -------
+        tensor
+            The output tensor from the Convolution 2D Layer
+        """
+        logger.debug("input_tensor: %s, filters: %s, kernel_size: %s, strides: %s, padding: %s, "
+                     "kwargs: %s)", input_tensor, filters, kernel_size, strides, padding, kwargs)
         if kwargs.get("name", None) is None:
-            kwargs["name"] = self.get_name("conv2d_{}".format(inp.shape[1]))
-        kwargs = self.set_default_initializer(kwargs)
+            kwargs["name"] = self._get_name("conv2d_{}".format(input_tensor.shape[1]))
+        kwargs = self._set_default_initializer(kwargs)
         var_x = Conv2D(filters, kernel_size,
                        strides=strides,
                        padding=padding,
-                       **kwargs)(inp)
+                       **kwargs)(input_tensor)
         return var_x
 
     # <<< Original Model Blocks >>> #
-    def conv(self, inp, filters, kernel_size=5, strides=2, padding="same",
+    def conv(self, input_tensor, filters, kernel_size=5, strides=2, padding="same",
              use_instance_norm=False, res_block_follows=False, **kwargs):
-        """ Convolution Layer"""
-        logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, use_instance_norm: %s, "
-                     "kwargs: %s)", inp, filters, kernel_size, strides, use_instance_norm, kwargs)
-        name = self.get_name("conv_{}".format(inp.shape[1]))
+        """ A standard Convolution 2D layer which applies user specified configuration to the
+        layer.
+
+        Adds reflection padding if it has been selected by the user, and other post-processing
+        if requested by the plugin.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input tensor to the layer
+        filters: int
+            The dimensionality of the output space (i.e. the number of output filters in the
+            convolution)
+        kernel_size: int, optional
+            An integer or tuple/list of 2 integers, specifying the height and width of the 2D
+            convolution window. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: 5
+        strides: tuple or int, optional
+            An integer or tuple/list of 2 integers, specifying the strides of the convolution along
+            the height and width. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: `2`
+        padding: ["valid", "same"], optional
+            The padding to use. Default: `"same"`
+        use_instance_norm: bool, optional
+            ``True`` if instance normalization should be applied after the convolutional layer.
+            Default: ``False``
+        res_block_follows: bool, optional
+            If a residual block will follow this layer, then this should be set to `True` to add
+            a leaky ReLu after the convolutional layer. Default: ``False``
+        kwargs: dict
+            Any additional Keras standard layer keyword arguments
+
+        Returns
+        -------
+        tensor
+            The output tensor from the Convolution 2D Layer
+        """
+        logger.debug("input_tensor: %s, filters: %s, kernel_size: %s, strides: %s, "
+                     "use_instance_norm: %s, kwargs: %s)", input_tensor, filters, kernel_size,
+                     strides, use_instance_norm, kwargs)
+        name = self._get_name("conv_{}".format(input_tensor.shape[1]))
         if self.use_reflect_padding:
-            inp = ReflectionPadding2D(stride=strides,
-                                      kernel_size=kernel_size,
-                                      name="{}_reflectionpadding2d".format(name))(inp)
+            input_tensor = ReflectionPadding2D(
+                stride=strides,
+                kernel_size=kernel_size,
+                name="{}_reflectionpadding2d".format(name))(input_tensor)
             padding = "valid"
-        var_x = self.conv2d(inp, filters,
+        var_x = self.conv2d(input_tensor, filters,
                             kernel_size=kernel_size,
                             strides=strides,
                             padding=padding,
@@ -114,29 +250,63 @@ class NNBlocks():
             var_x = LeakyReLU(0.1, name="{}_leakyrelu".format(name))(var_x)
         return var_x
 
-    def upscale(self, inp, filters, kernel_size=3, padding="same",
+    def upscale(self, input_tensor, filters, kernel_size=3, padding="same",
                 use_instance_norm=False, res_block_follows=False, scale_factor=2, **kwargs):
-        """ Upscale Layer """
-        logger.debug("inp: %s, filters: %s, kernel_size: %s, use_instance_norm: %s, kwargs: %s)",
-                     inp, filters, kernel_size, use_instance_norm, kwargs)
-        name = self.get_name("upscale_{}".format(inp.shape[1]))
+        """ An upscale layer for sub-pixel up-scaling.
+
+        Adds reflection padding if it has been selected by the user, and other post-processing
+        if requested by the plugin.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input tensor to the layer
+        filters: int
+            The dimensionality of the output space (i.e. the number of output filters in the
+            convolution)
+        kernel_size: int, optional
+            An integer or tuple/list of 2 integers, specifying the height and width of the 2D
+            convolution window. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: 3
+        padding: ["valid", "same"], optional
+            The padding to use. Default: `"same"`
+        use_instance_norm: bool, optional
+            ``True`` if instance normalization should be applied after the convolutional layer.
+            Default: ``False``
+        res_block_follows: bool, optional
+            If a residual block will follow this layer, then this should be set to `True` to add
+            a leaky ReLu after the convolutional layer. Default: ``False``
+        scale_factor: int, optional
+            The amount to upscale the image. Default: `2`
+        kwargs: dict
+            Any additional Keras standard layer keyword arguments
+
+        Returns
+        -------
+        tensor
+            The output tensor from the Upscale layer
+        """
+        logger.debug("input_tensor: %s, filters: %s, kernel_size: %s, use_instance_norm: %s, "
+                     "kwargs: %s)", input_tensor, filters, kernel_size, use_instance_norm, kwargs)
+        name = self._get_name("upscale_{}".format(input_tensor.shape[1]))
         if self.use_reflect_padding:
-            inp = ReflectionPadding2D(stride=1,
-                                      kernel_size=kernel_size,
-                                      name="{}_reflectionpadding2d".format(name))(inp)
+            input_tensor = ReflectionPadding2D(
+                stride=1,
+                kernel_size=kernel_size,
+                name="{}_reflectionpadding2d".format(name))(input_tensor)
             padding = "valid"
-        kwargs = self.set_default_initializer(kwargs)
+        kwargs = self._set_default_initializer(kwargs)
         if self.use_icnr_init:
-            original_init = self.switch_kernel_initializer(
+            original_init = self._switch_kernel_initializer(
                 kwargs,
                 ICNR(initializer=kwargs["kernel_initializer"]))
-        var_x = self.conv2d(inp, filters * scale_factor * scale_factor,
+        var_x = self.conv2d(input_tensor, filters * scale_factor * scale_factor,
                             kernel_size=kernel_size,
                             padding=padding,
                             name="{}_conv2d".format(name),
                             **kwargs)
         if self.use_icnr_init:
-            self.switch_kernel_initializer(kwargs, original_init)
+            self._switch_kernel_initializer(kwargs, original_init)
         if use_instance_norm:
             var_x = InstanceNormalization(name="{}_instancenorm".format(name))(var_x)
         if not res_block_follows:
@@ -149,12 +319,34 @@ class NNBlocks():
         return var_x
 
     # <<< DFaker Model Blocks >>> #
-    def res_block(self, inp, filters, kernel_size=3, padding="same", **kwargs):
-        """ Residual block """
-        logger.debug("inp: %s, filters: %s, kernel_size: %s, kwargs: %s)",
-                     inp, filters, kernel_size, kwargs)
-        name = self.get_name("residual_{}".format(inp.shape[1]))
-        var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_0".format(name))(inp)
+    def res_block(self, input_tensor, filters, kernel_size=3, padding="same", **kwargs):
+        """ Residual block.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input tensor to the layer
+        filters: int
+            The dimensionality of the output space (i.e. the number of output filters in the
+            convolution)
+        kernel_size: int, optional
+            An integer or tuple/list of 2 integers, specifying the height and width of the 2D
+            convolution window. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: 3
+        padding: ["valid", "same"], optional
+            The padding to use. Default: `"same"`
+        kwargs: dict
+            Any additional Keras standard layer keyword arguments
+
+        Returns
+        -------
+        tensor
+            The output tensor from the Upscale layer
+        """
+        logger.debug("input_tensor: %s, filters: %s, kernel_size: %s, kwargs: %s)",
+                     input_tensor, filters, kernel_size, kwargs)
+        name = self._get_name("residual_{}".format(input_tensor.shape[1]))
+        var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_0".format(name))(input_tensor)
         if self.use_reflect_padding:
             var_x = ReflectionPadding2D(stride=1,
                                         kernel_size=kernel_size,
@@ -172,7 +364,7 @@ class NNBlocks():
                                         name="{}_reflectionpadding2d_1".format(name))(var_x)
             padding = "valid"
         if not self.use_convaware_init:
-            original_init = self.switch_kernel_initializer(kwargs, VarianceScaling(
+            original_init = self._switch_kernel_initializer(kwargs, VarianceScaling(
                 scale=0.2,
                 mode="fan_in",
                 distribution="uniform"))
@@ -181,178 +373,47 @@ class NNBlocks():
                             padding=padding,
                             **kwargs)
         if not self.use_convaware_init:
-            self.switch_kernel_initializer(kwargs, original_init)
-        var_x = Add()([var_x, inp])
+            self._switch_kernel_initializer(kwargs, original_init)
+        var_x = Add()([var_x, input_tensor])
         var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_3".format(name))(var_x)
         return var_x
 
     # <<< Unbalanced Model Blocks >>> #
-    def conv_sep(self, inp, filters, kernel_size=5, strides=2, **kwargs):
-        """ Seperable Convolution Layer """
-        logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, kwargs: %s)",
-                     inp, filters, kernel_size, strides, kwargs)
-        name = self.get_name("separableconv2d_{}".format(inp.shape[1]))
-        kwargs = self.set_default_initializer(kwargs)
+    def conv_sep(self, input_tensor, filters, kernel_size=5, strides=2, **kwargs):
+        """ Seperable Convolution Layer.
+
+        Parameters
+        ----------
+        input_tensor: tensor
+            The input tensor to the layer
+        filters: int
+            The dimensionality of the output space (i.e. the number of output filters in the
+            convolution)
+        kernel_size: int, optional
+            An integer or tuple/list of 2 integers, specifying the height and width of the 2D
+            convolution window. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: 5
+        strides: tuple or int, optional
+            An integer or tuple/list of 2 integers, specifying the strides of the convolution along
+            the height and width. Can be a single integer to specify the same value for all spatial
+            dimensions. Default: `2`
+        kwargs: dict
+            Any additional Keras standard layer keyword arguments
+
+        Returns
+        -------
+        tensor
+            The output tensor from the Upscale layer
+        """
+        logger.debug("input_tensor: %s, filters: %s, kernel_size: %s, strides: %s, kwargs: %s)",
+                     input_tensor, filters, kernel_size, strides, kwargs)
+        name = self._get_name("separableconv2d_{}".format(input_tensor.shape[1]))
+        kwargs = self._set_default_initializer(kwargs)
         var_x = SeparableConv2D(filters,
                                 kernel_size=kernel_size,
                                 strides=strides,
                                 padding="same",
                                 name="{}_seperableconv2d".format(name),
-                                **kwargs)(inp)
+                                **kwargs)(input_tensor)
         var_x = Activation("relu", name="{}_relu".format(name))(var_x)
         return var_x
-
-# <<< GAN V2.2 Blocks >>> #
-# TODO Merge these into NNBLock class when porting GAN2.2
-
-
-# Gan Constansts:
-GAN22_CONV_INIT = "he_normal"
-GAN22_REGULARIZER = 1e-4
-
-
-# Gan Blocks:
-def normalization(inp, norm="none", group="16"):
-    """ GAN Normalization """
-    if norm == "layernorm":
-        var_x = GroupNormalization(group=group)(inp)
-    elif norm == "batchnorm":
-        var_x = BatchNormalization()(inp)
-    elif norm == "groupnorm":
-        var_x = GroupNormalization(group=16)(inp)
-    elif norm == "instancenorm":
-        var_x = InstanceNormalization()(inp)
-    elif norm == "hybrid":
-        if group % 2 == 1:
-            raise ValueError("Output channels must be an even number for hybrid norm, "
-                             "received {}.".format(group))
-        filt = group
-        var_x_0 = Lambda(lambda var_x: var_x[..., :filt // 2])(var_x)
-        var_x_1 = Lambda(lambda var_x: var_x[..., filt // 2:])(var_x)
-        var_x_0 = Conv2D(filt // 2,
-                         kernel_size=1,
-                         kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                         kernel_initializer=GAN22_CONV_INIT)(var_x_0)
-        var_x_1 = InstanceNormalization()(var_x_1)
-        var_x = concatenate([var_x_0, var_x_1], axis=-1)
-    else:
-        var_x = inp
-    return var_x
-
-
-def upscale_ps(inp, filters, initializer, use_norm=False, norm="none"):
-    """ GAN Upscaler - Pixel Shuffler """
-    var_x = Conv2D(filters * 4,
-                   kernel_size=3,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer=initializer,
-                   padding="same")(inp)
-    var_x = LeakyReLU(0.2)(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    var_x = PixelShuffler()(var_x)
-    return var_x
-
-
-def upscale_nn(inp, filters, use_norm=False, norm="none"):
-    """ GAN Neural Network """
-    var_x = UpSampling2D()(inp)
-    var_x = reflect_padding_2d(var_x, 1)
-    var_x = Conv2D(filters,
-                   kernel_size=3,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer="he_normal")(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    return var_x
-
-
-def reflect_padding_2d(inp, pad=1):
-    """ GAN Reflect Padding (2D) """
-    var_x = Lambda(lambda var_x: tf.pad(var_x,
-                                        [[0, 0], [pad, pad], [pad, pad], [0, 0]],
-                                        mode="REFLECT"))(inp)
-    return var_x
-
-
-def conv_gan(inp, filters, use_norm=False, strides=2, norm="none"):
-    """ GAN Conv Block """
-    var_x = Conv2D(filters,
-                   kernel_size=3,
-                   strides=strides,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer=GAN22_CONV_INIT,
-                   use_bias=False,
-                   padding="same")(inp)
-    var_x = Activation("relu")(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    return var_x
-
-
-def conv_d_gan(inp, filters, use_norm=False, norm="none"):
-    """ GAN Discriminator Conv Block """
-    var_x = inp
-    var_x = Conv2D(filters,
-                   kernel_size=4,
-                   strides=2,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer=GAN22_CONV_INIT,
-                   use_bias=False,
-                   padding="same")(var_x)
-    var_x = LeakyReLU(alpha=0.2)(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    return var_x
-
-
-def res_block_gan(inp, filters, use_norm=False, norm="none"):
-    """ GAN Res Block """
-    var_x = Conv2D(filters,
-                   kernel_size=3,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer=GAN22_CONV_INIT,
-                   use_bias=False,
-                   padding="same")(inp)
-    var_x = LeakyReLU(alpha=0.2)(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    var_x = Conv2D(filters,
-                   kernel_size=3,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER),
-                   kernel_initializer=GAN22_CONV_INIT,
-                   use_bias=False,
-                   padding="same")(var_x)
-    var_x = add([var_x, inp])
-    var_x = LeakyReLU(alpha=0.2)(var_x)
-    var_x = normalization(var_x, norm, filters) if use_norm else var_x
-    return var_x
-
-
-def self_attn_block(inp, n_c, squeeze_factor=8):
-    """ GAN Self Attention Block
-    Code borrows from https://github.com/taki0112/Self-Attention-GAN-Tensorflow
-    """
-    msg = "Input channels must be >= {}, recieved nc={}".format(squeeze_factor, n_c)
-    assert n_c // squeeze_factor > 0, msg
-    var_x = inp
-    shape_x = var_x.get_shape().as_list()
-
-    var_f = Conv2D(n_c // squeeze_factor, 1,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER))(var_x)
-    var_g = Conv2D(n_c // squeeze_factor, 1,
-                   kernel_regularizer=regularizers.l2(GAN22_REGULARIZER))(var_x)
-    var_h = Conv2D(n_c, 1, kernel_regularizer=regularizers.l2(GAN22_REGULARIZER))(var_x)
-
-    shape_f = var_f.get_shape().as_list()
-    shape_g = var_g.get_shape().as_list()
-    shape_h = var_h.get_shape().as_list()
-    flat_f = Reshape((-1, shape_f[-1]))(var_f)
-    flat_g = Reshape((-1, shape_g[-1]))(var_g)
-    flat_h = Reshape((-1, shape_h[-1]))(var_h)
-
-    var_s = Lambda(lambda var_x: K.batch_dot(var_x[0],
-                                             Permute((2, 1))(var_x[1])))([flat_g, flat_f])
-
-    beta = Softmax(axis=-1)(var_s)
-    var_o = Lambda(lambda var_x: K.batch_dot(var_x[0], var_x[1]))([beta, flat_h])
-    var_o = Reshape(shape_x[1:])(var_o)
-    var_o = Scale()(var_o)
-
-    out = add([var_o, inp])
-    return out
diff --git a/lib/model/normalization.py b/lib/model/normalization.py
index ec4dbb1..60036fd 100644
--- a/lib/model/normalization.py
+++ b/lib/model/normalization.py
@@ -1,7 +1,5 @@
 #!/usr/bin/env python3
-""" Normaliztion methods for faceswap.py
-    Code from:
-        shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
+""" Normalization methods for faceswap.py. """
 
 import sys
 import inspect
@@ -12,58 +10,57 @@ from keras import backend as K
 from keras.utils.generic_utils import get_custom_objects
 
 
-def to_list(inp):
-    """ Convert to list """
-    if not isinstance(inp, (list, tuple)):
-        return [inp]
-    return list(inp)
-
-
 class InstanceNormalization(Layer):
     """Instance normalization layer (Lei Ba et al, 2016, Ulyanov et al., 2016).
-    Normalize the activations of the previous layer at each step,
-    i.e. applies a transformation that maintains the mean activation
-    close to 0 and the activation standard deviation close to 1.
-    # Arguments
-        axis: Integer, the axis that should be normalized
-            (typically the features axis).
-            For instance, after a `Conv2D` layer with
-            `data_format="channels_first"`,
-            set `axis=1` in `InstanceNormalization`.
-            Setting `axis=None` will normalize all values in each instance of the batch.
-            Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid errors.
-        epsilon: Small float added to variance to avoid dividing by zero.
-        center: If True, add offset of `beta` to normalized tensor.
-            If False, `beta` is ignored.
-        scale: If True, multiply by `gamma`.
-            If False, `gamma` is not used.
-            When the next layer is linear (also e.g. `nn.relu`),
-            this can be disabled since the scaling
-            will be done by the next layer.
-        beta_initializer: Initializer for the beta weight.
-        gamma_initializer: Initializer for the gamma weight.
-        beta_regularizer: Optional regularizer for the beta weight.
-        gamma_regularizer: Optional regularizer for the gamma weight.
-        beta_constraint: Optional constraint for the beta weight.
-        gamma_constraint: Optional constraint for the gamma weight.
-    # Input shape
-        Arbitrary. Use the keyword argument `input_shape`
-        (tuple of integers, does not include the samples axis)
-        when using this layer as the first layer in a model.
-    # Output shape
-        Same shape as input.
-    # References
-        - [Layer Normalization](https://arxiv.org/abs/1607.06450)
-        - [Instance Normalization: The Missing Ingredient for Fast
-                                   Stylization](https://arxiv.org/abs/1607.08022)
+
+    Normalize the activations of the previous layer at each step, i.e. applies a transformation
+    that maintains the mean activation close to 0 and the activation standard deviation close to 1.
+
+    Parameters
+    ----------
+    axis: int, optional
+        The axis that should be normalized (typically the features axis). For instance, after a
+        `Conv2D` layer with `data_format="channels_first"`, set `axis=1` in
+        :class:`InstanceNormalization`. Setting `axis=None` will normalize all values in each
+        instance of the batch. Axis 0 is the batch dimension. `axis` cannot be set to 0 to avoid
+        errors. Default: ``None``
+    epsilon: float, optional
+        Small float added to variance to avoid dividing by zero. Default: `1e-3`
+    center: bool, optional
+        If ``True``, add offset of `beta` to normalized tensor. If ``False``, `beta` is ignored.
+        Default: ``True``
+    scale: bool, optional
+        If ``True``, multiply by `gamma`. If ``False``, `gamma` is not used. When the next layer
+        is linear (also e.g. `relu`), this can be disabled since the scaling will be done by
+        the next layer. Default: ``True``
+    beta_initializer: str, optional
+        Initializer for the beta weight. Default: `"zeros"`
+    gamma_initializer: str, optional
+        Initializer for the gamma weight. Default: `"ones"`
+    beta_regularizer: str, optional
+        Optional regularizer for the beta weight. Default: ``None``
+    gamma_regularizer: str, optional
+        Optional regularizer for the gamma weight. Default: ``None``
+    beta_constraint: float, optional
+        Optional constraint for the beta weight. Default: ``None``
+    gamma_constraint: float, optional
+        Optional constraint for the gamma weight. Default: ``None``
+
+    References
+    ----------
+        - Layer Normalization - https://arxiv.org/abs/1607.06450
+
+        - Instance Normalization: The Missing Ingredient for Fast Stylization -
+    https://arxiv.org/abs/1607.08022
+
     """
     def __init__(self,
                  axis=None,
                  epsilon=1e-3,
                  center=True,
                  scale=True,
-                 beta_initializer='zeros',
-                 gamma_initializer='ones',
+                 beta_initializer="zeros",
+                 gamma_initializer="ones",
                  beta_regularizer=None,
                  gamma_regularizer=None,
                  beta_constraint=None,
@@ -71,7 +68,7 @@ class InstanceNormalization(Layer):
                  **kwargs):
         self.beta = None
         self.gamma = None
-        super(InstanceNormalization, self).__init__(**kwargs)
+        super().__init__(**kwargs)
         self.supports_masking = True
         self.axis = axis
         self.epsilon = epsilon
@@ -85,12 +82,22 @@ class InstanceNormalization(Layer):
         self.gamma_constraint = constraints.get(gamma_constraint)
 
     def build(self, input_shape):
+        """Creates the layer weights.
+
+        Must be implemented on all layers that have weights.
+
+        Parameters
+        ----------
+        input_shape: tensor
+            Keras tensor (future input to layer) or ``list``/``tuple`` of Keras tensors to
+            reference for weight shape computations.
+        """
         ndim = len(input_shape)
         if self.axis == 0:
-            raise ValueError('Axis cannot be zero')
+            raise ValueError("Axis cannot be zero")
 
         if (self.axis is not None) and (ndim == 2):
-            raise ValueError('Cannot specify axis for rank 1 tensor')
+            raise ValueError("Cannot specify axis for rank 1 tensor")
 
         self.input_spec = InputSpec(ndim=ndim)
 
@@ -101,7 +108,7 @@ class InstanceNormalization(Layer):
 
         if self.scale:
             self.gamma = self.add_weight(shape=shape,
-                                         name='gamma',
+                                         name="gamma",
                                          initializer=self.gamma_initializer,
                                          regularizer=self.gamma_regularizer,
                                          constraint=self.gamma_constraint)
@@ -109,7 +116,7 @@ class InstanceNormalization(Layer):
             self.gamma = None
         if self.center:
             self.beta = self.add_weight(shape=shape,
-                                        name='beta',
+                                        name="beta",
                                         initializer=self.beta_initializer,
                                         regularizer=self.beta_regularizer,
                                         constraint=self.beta_constraint)
@@ -117,7 +124,19 @@ class InstanceNormalization(Layer):
             self.beta = None
         self.built = True
 
-    def call(self, inputs, training=None):
+    def call(self, inputs, training=None):  # pylint:disable=arguments-differ,unused-argument
+        """This is where the layer's logic lives.
+
+        Parameters
+        ----------
+        inputs: tensor
+            Input tensor, or list/tuple of input tensors
+
+        Returns
+        -------
+        tensor
+            A tensor or list/tuple of tensors
+        """
         input_shape = K.int_shape(inputs)
         reduction_axes = list(range(0, len(input_shape)))
 
@@ -144,146 +163,22 @@ class InstanceNormalization(Layer):
 
     def get_config(self):
         config = {
-            'axis': self.axis,
-            'epsilon': self.epsilon,
-            'center': self.center,
-            'scale': self.scale,
-            'beta_initializer': initializers.serialize(self.beta_initializer),
-            'gamma_initializer': initializers.serialize(self.gamma_initializer),
-            'beta_regularizer': regularizers.serialize(self.beta_regularizer),
-            'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),
-            'beta_constraint': constraints.serialize(self.beta_constraint),
-            'gamma_constraint': constraints.serialize(self.gamma_constraint)
+            "axis": self.axis,
+            "epsilon": self.epsilon,
+            "center": self.center,
+            "scale": self.scale,
+            "beta_initializer": initializers.serialize(self.beta_initializer),
+            "gamma_initializer": initializers.serialize(self.gamma_initializer),
+            "beta_regularizer": regularizers.serialize(self.beta_regularizer),
+            "gamma_regularizer": regularizers.serialize(self.gamma_regularizer),
+            "beta_constraint": constraints.serialize(self.beta_constraint),
+            "gamma_constraint": constraints.serialize(self.gamma_constraint)
         }
         base_config = super(InstanceNormalization, self).get_config()
         return dict(list(base_config.items()) + list(config.items()))
 
 
-class GroupNormalization(Layer):
-    """ Group Normalization
-        from: shoanlu GAN: https://github.com/shaoanlu/faceswap-GAN"""
-
-    def __init__(self, axis=-1,
-                 gamma_init='one', beta_init='zero',
-                 gamma_regularizer=None, beta_regularizer=None,
-                 epsilon=1e-6,
-                 group=32,
-                 data_format=None,
-                 **kwargs):
-        self.beta = None
-        self.gamma = None
-        super(GroupNormalization, self).__init__(**kwargs)
-
-        self.axis = to_list(axis)
-        self.gamma_init = initializers.get(gamma_init)
-        self.beta_init = initializers.get(beta_init)
-        self.gamma_regularizer = regularizers.get(gamma_regularizer)
-        self.beta_regularizer = regularizers.get(beta_regularizer)
-        self.epsilon = epsilon
-        self.group = group
-        self.data_format = K.normalize_data_format(data_format)
-
-        self.supports_masking = True
-
-    def build(self, input_shape):
-        self.input_spec = [InputSpec(shape=input_shape)]
-        shape = [1 for _ in input_shape]
-        if self.data_format == 'channels_last':
-            channel_axis = -1
-            shape[channel_axis] = input_shape[channel_axis]
-        elif self.data_format == 'channels_first':
-            channel_axis = 1
-            shape[channel_axis] = input_shape[channel_axis]
-        # for i in self.axis:
-        #    shape[i] = input_shape[i]
-        self.gamma = self.add_weight(shape=shape,
-                                     initializer=self.gamma_init,
-                                     regularizer=self.gamma_regularizer,
-                                     name='gamma')
-        self.beta = self.add_weight(shape=shape,
-                                    initializer=self.beta_init,
-                                    regularizer=self.beta_regularizer,
-                                    name='beta')
-        self.built = True
-
-    def call(self, inputs, mask=None):
-        input_shape = K.int_shape(inputs)
-        if len(input_shape) != 4 and len(input_shape) != 2:
-            raise ValueError('Inputs should have rank ' +
-                             str(4) + " or " + str(2) +
-                             '; Received input shape:', str(input_shape))
-
-        if len(input_shape) == 4:
-            if self.data_format == 'channels_last':
-                batch_size, height, width, channels = input_shape
-                if batch_size is None:
-                    batch_size = -1
-
-                if channels < self.group:
-                    raise ValueError('Input channels should be larger than group size' +
-                                     '; Received input channels: ' + str(channels) +
-                                     '; Group size: ' + str(self.group))
-
-                var_x = K.reshape(inputs, (batch_size,
-                                           height,
-                                           width,
-                                           self.group,
-                                           channels // self.group))
-                mean = K.mean(var_x, axis=[1, 2, 4], keepdims=True)
-                std = K.sqrt(K.var(var_x, axis=[1, 2, 4], keepdims=True) + self.epsilon)
-                var_x = (var_x - mean) / std
-
-                var_x = K.reshape(var_x, (batch_size, height, width, channels))
-                retval = self.gamma * var_x + self.beta
-            elif self.data_format == 'channels_first':
-                batch_size, channels, height, width = input_shape
-                if batch_size is None:
-                    batch_size = -1
-
-                if channels < self.group:
-                    raise ValueError('Input channels should be larger than group size' +
-                                     '; Received input channels: ' + str(channels) +
-                                     '; Group size: ' + str(self.group))
-
-                var_x = K.reshape(inputs, (batch_size,
-                                           self.group,
-                                           channels // self.group,
-                                           height,
-                                           width))
-                mean = K.mean(var_x, axis=[2, 3, 4], keepdims=True)
-                std = K.sqrt(K.var(var_x, axis=[2, 3, 4], keepdims=True) + self.epsilon)
-                var_x = (var_x - mean) / std
-
-                var_x = K.reshape(var_x, (batch_size, channels, height, width))
-                retval = self.gamma * var_x + self.beta
-
-        elif len(input_shape) == 2:
-            reduction_axes = list(range(0, len(input_shape)))
-            del reduction_axes[0]
-            batch_size, _ = input_shape
-            if batch_size is None:
-                batch_size = -1
-
-            mean = K.mean(inputs, keepdims=True)
-            std = K.sqrt(K.var(inputs, keepdims=True) + self.epsilon)
-            var_x = (inputs - mean) / std
-
-            retval = self.gamma * var_x + self.beta
-        return retval
-
-    def get_config(self):
-        config = {'epsilon': self.epsilon,
-                  'axis': self.axis,
-                  'gamma_init': initializers.serialize(self.gamma_init),
-                  'beta_init': initializers.serialize(self.beta_init),
-                  'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),
-                  'beta_regularizer': regularizers.serialize(self.gamma_regularizer),
-                  'group': self.group}
-        base_config = super(GroupNormalization, self).get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-# Update normalizations into Keras custom objects
+# Update normalization into Keras custom objects
 for name, obj in inspect.getmembers(sys.modules[__name__]):
     if inspect.isclass(obj) and obj.__module__ == __name__:
         get_custom_objects().update({name: obj})
diff --git a/lib/model/optimizers.py b/lib/model/optimizers.py
index 3d8bffe..e2b50e4 100644
--- a/lib/model/optimizers.py
+++ b/lib/model/optimizers.py
@@ -12,25 +12,86 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class Adam(KerasAdam):
-    """Adapted Keras Adam Optimizer to allow support of calculations
-       on CPU for Tensorflow.
-
-       Adapted from https://github.com/iperov/DeepFaceLab
+    """Adapted Keras Adam Optimizer to allow support of calculations on CPU for Tensorflow.
+
+    Default parameters follow those provided in the original paper. Adapted from
+    https://github.com/iperov/DeepFaceLab
+
+    Parameters
+    ----------
+    lr: float, optional
+        >= `0`. Learning rate. Default: `0.001`
+    beta_1: float, optional
+        `0` < beta < `1` Generally close to `1`. Default: `0.9`
+    beta_2: float, optional
+        `0` < beta < `1`. Generally close to `1`. Default: `0.999`
+    epsilon: float, optional
+        >= `0`. Fuzz factor. If ``None``, defaults to `K.epsilon()`. Default: ``None``
+    decay: float, optional
+        >= 0. Learning rate decay over each update. Default: `0`
+    amsgrad: bool, optional
+        ``True`` to apply the AMSGrad variant of this algorithm from the paper "On the Convergence
+        of Adam and Beyond" otherwise ``False``. Default: ``False``
+    cpu_mode: bool, optional
+        Set to ``True`` to perform some of the calculations on CPU for Nvidia backends, otherwise
+        ``False``. Default: ``False``
+    kwargs: dict
+        Any additional standard Keras optimizer keyword arguments
+
+    References
+    ----------
+    - Adam - A Method for Stochastic Optimization - https://arxiv.org/abs/1412.6980v8
+
+    - On the Convergence of Adam and Beyond - https://openreview.net/forum?id=ryQu7f-RZ
     """
 
-    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,
-                 epsilon=None, decay=0., amsgrad=False, cpu_mode=0, **kwargs):
+    def __init__(self,
+                 lr=0.001,
+                 beta_1=0.9,
+                 beta_2=0.999,
+                 epsilon=None,
+                 decay=0.,
+                 amsgrad=False,
+                 cpu_mode=False,
+                 **kwargs):
         super().__init__(lr, beta_1, beta_2, epsilon, decay, **kwargs)
-        self.cpu_mode = self.set_cpu_mode(cpu_mode)
+        self.cpu_mode = self._set_cpu_mode(cpu_mode)
 
     @staticmethod
-    def set_cpu_mode(cpu_mode):
-        """ Set the CPU mode to 0 if not using tensorflow, else passed in arg """
+    def _set_cpu_mode(cpu_mode):
+        """ Sets the CPU mode to False if not using Tensorflow, otherwise the given value.
+
+        Parameters
+        ----------
+        cpu_mode: bool
+            Set to ``True`` to perform some of the calculations on CPU for Nvidia backends,
+            otherwise ``False``.
+
+        Returns
+        -------
+        bool
+            ``True`` if some calculations should be performed on CPU otherwise ``False``
+        """
         retval = False if K.backend() != "tensorflow" else cpu_mode
         logger.debug("Optimizer CPU Mode set to %s", retval)
         return retval
 
     def get_updates(self, loss, params):
+        """ Obtain the optimizer loss updates.
+
+        Parameters
+        ----------
+        loss: list
+            List of tensors
+
+        params: list
+            List of tensors
+
+        Returns
+        -------
+        list
+            List of tensors
+        """
         grads = self.get_gradients(loss, params)
         self.updates = [K.update_add(self.iterations, 1)]
 
@@ -46,9 +107,9 @@ class Adam(KerasAdam):
         # Pass off to CPU if requested
         if self.cpu_mode:
             with K.tf.device("/cpu:0"):
-                ms, vs, vhats = self.update_1(params)
+                ms, vs, vhats = self._update_1(params)
         else:
-            ms, vs, vhats = self.update_1(params)
+            ms, vs, vhats = self._update_1(params)
 
         self.weights = [self.iterations] + ms + vs + vhats
 
@@ -73,8 +134,9 @@ class Adam(KerasAdam):
             self.updates.append(K.update(p, new_p))
         return self.updates
 
-    def update_1(self, params):
-        """ First update on CPU or GPU """
+    def _update_1(self, params):
+        """ Perform the first update. Run under CPU context if running on Tensorflow and CPU mode
+        is enabled, otherwise run on the default device. """
         ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
         vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
         if self.amsgrad:
diff --git a/lib/utils.py b/lib/utils.py
index f65a054..b74760a 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -25,8 +25,10 @@ _video_extensions = [  # pylint:disable=invalid-name
 
 
 class Backend():
-    """ Return the backend from config/.faceswap
-        if file doesn't exist, create it """
+    """ Return the backend from config/.faceswap of from the `FACESWAP_BACKEND` Environment
+    Variable.
+
+    If file doesn't exist and a variable hasn't been set, create the config file. """
     def __init__(self):
         self.backends = {"1": "amd", "2": "cpu", "3": "nvidia"}
         self.config_file = self.get_config_file()
@@ -40,7 +42,14 @@ class Backend():
         return config_file
 
     def get_backend(self):
-        """ Return the backend from config/.faceswap """
+        """ Return the backend from either the `FACESWAP_BACKEND` Environment Variable or from
+        the :loc:`config/.faceswap` configuration file. """
+        # Check if environment variable is set, if so use that
+        if "FACESWAP_BACKEND" in os.environ:
+            fs_backend = os.environ["FACESWAP_BACKEND"].lower()
+            print("Setting Faceswap backend from environment variable to "
+                  "{}".format(fs_backend.upper()))
+            return fs_backend
         # Intercept for sphinx docs build
         if sys.argv[0].endswith("sphinx-build"):
             return "nvidia"
diff --git a/plugins/train/model/dlight.py b/plugins/train/model/dlight.py
index 679edaf..8e1e677 100644
--- a/plugins/train/model/dlight.py
+++ b/plugins/train/model/dlight.py
@@ -31,7 +31,7 @@ def upscale2x_hyb(self, inp, filters, kernel_size=3, padding='same',
                   sr_ratio=0.5, scale_factor=2, interpolation='bilinear',
                   res_block_follows=False, **kwargs):
     """Hybrid Upscale Layer"""
-    name = self.get_name("upscale2x_hyb")
+    name = self._get_name("upscale2x_hyb")
     var_x = inp
 
     sr_filters = int(filters * sr_ratio)
@@ -56,7 +56,7 @@ def upscale2x_fast(self, inp, filters, kernel_size=3, padding='same',
                    sr_ratio=0.5, scale_factor=2, interpolation='bilinear',
                    res_block_follows=False, **kwargs):
     """Fast Upscale Layer"""
-    name = self.get_name("upscale2x_fast")
+    name = self._get_name("upscale2x_fast")
     var_x = inp
 
     var_x2 = self.conv2d(var_x, filters,  kernel_size=3, padding=padding,
diff --git a/tests/__init__.py b/tests/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/lib/__init__.py b/tests/lib/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/lib/model/__init__.py b/tests/lib/model/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/lib/model/initializers_test.py b/tests/lib/model/initializers_test.py
new file mode 100644
index 0000000..5d6e85a
--- /dev/null
+++ b/tests/lib/model/initializers_test.py
@@ -0,0 +1,62 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Initializers.
+
+Adapted from Keras tests.
+"""
+
+from keras import initializers as k_initializers
+from keras import backend as K
+import pytest
+import numpy as np
+
+from lib.model import initializers
+from lib.utils import get_backend
+
+
+CONV_SHAPE = (3, 3, 256, 2048)
+CONV_ID = get_backend().upper()
+
+
+def _runner(init, shape, target_mean=None, target_std=None,
+            target_max=None, target_min=None):
+    variable = K.variable(init(shape))
+    output = K.get_value(variable)
+    lim = 3e-2
+    if target_std is not None:
+        assert abs(output.std() - target_std) < lim
+    if target_mean is not None:
+        assert abs(output.mean() - target_mean) < lim
+    if target_max is not None:
+        assert abs(output.max() - target_max) < lim
+    if target_min is not None:
+        assert abs(output.min() - target_min) < lim
+
+
+@pytest.mark.parametrize('tensor_shape', [CONV_SHAPE], ids=[CONV_ID])
+def test_icnr(tensor_shape):
+    """ ICNR Initialization Test
+
+    Parameters
+    ----------
+    tensor_shape: tuple
+        The shape of the tensor to feed to the initializer
+    """
+    fan_in, _ = k_initializers._compute_fans(tensor_shape)  # pylint:disable=protected-access
+    std = np.sqrt(2. / fan_in)
+    _runner(initializers.ICNR(initializer=k_initializers.he_uniform(), scale=2), tensor_shape,
+            target_mean=0, target_std=std)
+
+
+@pytest.mark.parametrize('tensor_shape', [CONV_SHAPE], ids=[CONV_ID])
+def test_convolution_aware(tensor_shape):
+    """ Convolution Aware Initialization Test
+
+    Parameters
+    ----------
+    tensor_shape: tuple
+        The shape of the tensor to feed to the initializer
+    """
+    fan_in, _ = k_initializers._compute_fans(tensor_shape)  # pylint:disable=protected-access
+    std = np.sqrt(2. / fan_in)
+    _runner(initializers.ConvolutionAware(seed=123, init=True), tensor_shape,
+            target_mean=0, target_std=std)
diff --git a/tests/lib/model/layers_test.py b/tests/lib/model/layers_test.py
new file mode 100644
index 0000000..9eb4463
--- /dev/null
+++ b/tests/lib/model/layers_test.py
@@ -0,0 +1,138 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Custom Layers.
+
+Adapted from Keras tests.
+"""
+
+
+import pytest
+import numpy as np
+from keras import Input, Model, backend as K
+from keras.utils.generic_utils import has_arg
+
+from numpy.testing import assert_allclose
+
+from lib.model import layers
+from lib.utils import get_backend
+
+
+CONV_SHAPE = (3, 3, 256, 2048)
+CONV_ID = get_backend().upper()
+
+
+def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,
+               input_data=None, expected_output=None,
+               expected_output_dtype=None, fixed_batch_size=False):
+    """Test routine for a layer with a single input tensor
+    and single output tensor.
+    """
+    # generate input data
+    if input_data is None:
+        assert input_shape
+        if not input_dtype:
+            input_dtype = K.floatx()
+        input_data_shape = list(input_shape)
+        for i, var_e in enumerate(input_data_shape):
+            if var_e is None:
+                input_data_shape[i] = np.random.randint(1, 4)
+        input_data = (10 * np.random.random(input_data_shape))
+        input_data = input_data.astype(input_dtype)
+    else:
+        if input_shape is None:
+            input_shape = input_data.shape
+        if input_dtype is None:
+            input_dtype = input_data.dtype
+    if expected_output_dtype is None:
+        expected_output_dtype = input_dtype
+
+    # instantiation
+    layer = layer_cls(**kwargs)
+
+    # test get_weights , set_weights at layer level
+    weights = layer.get_weights()
+    layer.set_weights(weights)
+
+    if isinstance(layer, layers.ReflectionPadding2D):
+        layer.build(input_shape)
+    expected_output_shape = layer.compute_output_shape(input_shape)
+
+    # test in functional API
+    if fixed_batch_size:
+        inp = Input(batch_shape=input_shape, dtype=input_dtype)
+    else:
+        inp = Input(shape=input_shape[1:], dtype=input_dtype)
+    outp = layer(inp)
+    assert K.dtype(outp) == expected_output_dtype
+
+    # check with the functional API
+    model = Model(inp, outp)
+
+    actual_output = model.predict(input_data)
+    actual_output_shape = actual_output.shape
+    for expected_dim, actual_dim in zip(expected_output_shape,
+                                        actual_output_shape):
+        if expected_dim is not None:
+            assert expected_dim == actual_dim
+
+    if expected_output is not None:
+        assert_allclose(actual_output, expected_output, rtol=1e-3)
+
+    # test serialization, weight setting at model level
+    model_config = model.get_config()
+    recovered_model = model.__class__.from_config(model_config)
+    if model.weights:
+        weights = model.get_weights()
+        recovered_model.set_weights(weights)
+        _output = recovered_model.predict(input_data)
+        assert_allclose(_output, actual_output, rtol=1e-3)
+
+    # test training mode (e.g. useful when the layer has a
+    # different behavior at training and testing time).
+    if has_arg(layer.call, 'training'):
+        model.compile('rmsprop', 'mse')
+        model.train_on_batch(input_data, actual_output)
+
+    # test instantiation from layer config
+    layer_config = layer.get_config()
+    layer_config['batch_input_shape'] = input_shape
+    layer = layer.__class__.from_config(layer_config)
+
+    # for further checks in the caller function
+    return actual_output
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_pixel_shuffler(dummy):  # pylint:disable=unused-argument
+    """ Pixel Shuffler layer test """
+    layer_test(layers.PixelShuffler, input_shape=(2, 4, 4, 1024))
+
+
+@pytest.mark.skipif(get_backend() == "amd", reason="amd does not support this layer")
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_subpixel_upscaling(dummy):  # pylint:disable=unused-argument
+    """ Sub Pixel Upscaling layer test """
+    layer_test(layers.SubPixelUpscaling, input_shape=(2, 4, 4, 1024))
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_reflection_padding_2d(dummy):  # pylint:disable=unused-argument
+    """ Reflection Padding 2D layer test """
+    layer_test(layers.ReflectionPadding2D, input_shape=(2, 4, 4, 512))
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_global_min_pooling_2d(dummy):  # pylint:disable=unused-argument
+    """ Global Min Pooling 2D layer test """
+    layer_test(layers.GlobalMinPooling2D, input_shape=(2, 4, 4, 1024))
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_global_std_pooling_2d(dummy):  # pylint:disable=unused-argument
+    """ Global Standard Deviation Pooling 2D layer test """
+    layer_test(layers.GlobalStdDevPooling2D, input_shape=(2, 4, 4, 1024))
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_l2_normalize(dummy):  # pylint:disable=unused-argument
+    """ L2 Normalize layer test """
+    layer_test(layers.L2_normalize, kwargs={"axis": 1}, input_shape=(2, 4, 4, 1024))
diff --git a/tests/lib/model/losses_test.py b/tests/lib/model/losses_test.py
new file mode 100644
index 0000000..0276a1b
--- /dev/null
+++ b/tests/lib/model/losses_test.py
@@ -0,0 +1,75 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Losses.
+
+Adapted from Keras tests.
+"""
+
+import pytest
+import numpy as np
+from numpy.testing import assert_allclose
+
+from keras import backend as K
+from keras.layers import Conv2D
+from keras.models import Sequential
+from keras.optimizers import Adam
+
+from lib.model import losses
+from lib.utils import get_backend
+
+
+_PARAMS = [(losses.gradient_loss, (1, 5, 6, 7), (1, 5, 6)),
+           (losses.generalized_loss, (5, 6, 7), (5, 6)),
+           # TODO Make sure these output dimensions are correct
+           (losses.l_inf_norm, (1, 5, 6, 7), (1, 1, 1)),
+           # TODO Make sure these output dimensions are correct
+           (losses.gmsd_loss, (1, 5, 6, 7), (1, 1, 1))]
+_IDS = ["gradient_loss", "generalized_loss", "l_inf_norm", "gmsd_loss"]
+_IDS = ["{}[{}]".format(loss, get_backend().upper()) for loss in _IDS]
+
+
+@pytest.mark.parametrize(["loss_func", "input_shape", "output_shape"], _PARAMS, ids=_IDS)
+def test_objective_shapes(loss_func, input_shape, output_shape):
+    """ Basic shape tests for loss functions. """
+    y_a = K.variable(np.random.random(input_shape))
+    y_b = K.variable(np.random.random(input_shape))
+    objective_output = loss_func(y_a, y_b)
+    assert K.eval(objective_output).shape == output_shape
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+@pytest.mark.xfail(get_backend() == "amd", reason="plaidML generates NaNs")
+def test_dssim_channels_last(dummy):  # pylint:disable=unused-argument
+    """ Basic test for DSSIM Loss """
+    prev_data = K.image_data_format()
+    K.set_image_data_format('channels_last')
+    for input_dim, kernel_size in zip([32, 33], [2, 3]):
+        input_shape = [input_dim, input_dim, 3]
+        var_x = np.random.random_sample(4 * input_dim * input_dim * 3)
+        var_x = var_x.reshape([4] + input_shape)
+        var_y = np.random.random_sample(4 * input_dim * input_dim * 3)
+        var_y = var_y.reshape([4] + input_shape)
+
+        model = Sequential()
+        model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape,
+                         activation='relu'))
+        model.add(Conv2D(3, (3, 3), padding='same', input_shape=input_shape,
+                         activation='relu'))
+        adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)
+        model.compile(loss=losses.DSSIMObjective(kernel_size=kernel_size),
+                      metrics=['mse'],
+                      optimizer=adam)
+        model.fit(var_x, var_y, batch_size=2, epochs=1, shuffle='batch')
+
+        # Test same
+        x_1 = K.constant(var_x, 'float32')
+        x_2 = K.constant(var_x, 'float32')
+        dssim = losses.DSSIMObjective(kernel_size=kernel_size)
+        assert_allclose(0.0, K.eval(dssim(x_1, x_2)), atol=1e-4)
+
+        # Test opposite
+        x_1 = K.zeros([4] + input_shape)
+        x_2 = K.ones([4] + input_shape)
+        dssim = losses.DSSIMObjective(kernel_size=kernel_size)
+        assert_allclose(0.5, K.eval(dssim(x_1, x_2)), atol=1e-4)
+
+    K.set_image_data_format(prev_data)
diff --git a/tests/lib/model/nn_blocks_test.py b/tests/lib/model/nn_blocks_test.py
new file mode 100644
index 0000000..9c2c5c9
--- /dev/null
+++ b/tests/lib/model/nn_blocks_test.py
@@ -0,0 +1,78 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Custom Layers.
+
+Adapted from Keras tests.
+"""
+
+from itertools import product
+
+import pytest
+import numpy as np
+
+from keras import Input, Model, backend as K
+from numpy.testing import assert_allclose
+
+from lib.model.nn_blocks import NNBlocks
+from lib.utils import get_backend
+
+_PARAMS = ["use_subpixel", "use_icnr_init", "use_convaware_init", "use_reflect_padding"]
+_VALUES = list(product([True, False], repeat=len(_PARAMS)))
+_IDS = ["{}[{}]".format("|".join([_PARAMS[idx] for idx, b in enumerate(v) if b]),
+                        get_backend().upper()) for v in _VALUES]
+
+
+def block_test(layer_func, kwargs={}, input_shape=None):
+    """Test routine for a faceswaps neural network blocks.
+
+    Tests are simple and are to ensure that the blocks compile on both tensorflow
+    and plaidml backends
+    """
+    # generate input data
+    assert input_shape
+    input_dtype = K.floatx()
+    input_data_shape = list(input_shape)
+    for i, var_e in enumerate(input_data_shape):
+        if var_e is None:
+            input_data_shape[i] = np.random.randint(1, 4)
+    input_data = (10 * np.random.random(input_data_shape))
+    input_data = input_data.astype(input_dtype)
+    expected_output_dtype = input_dtype
+
+    # test in functional API
+    inp = Input(shape=input_shape[1:], dtype=input_dtype)
+    outp = layer_func(inp, **kwargs)
+    assert K.dtype(outp) == expected_output_dtype
+
+    # check with the functional API
+    model = Model(inp, outp)
+
+    actual_output = model.predict(input_data)
+
+    # test serialization, weight setting at model level
+    model_config = model.get_config()
+    recovered_model = model.__class__.from_config(model_config)
+    if model.weights:
+        weights = model.get_weights()
+        recovered_model.set_weights(weights)
+        _output = recovered_model.predict(input_data)
+        assert_allclose(_output, actual_output, rtol=1e-3)
+
+    # for further checks in the caller function
+    return actual_output
+
+
+@pytest.mark.parametrize(_PARAMS, _VALUES, ids=_IDS)
+def test_blocks(use_subpixel, use_icnr_init, use_convaware_init, use_reflect_padding):
+    """ Test for all blocks contained within the NNBlocks Class """
+    if get_backend() == "amd" and use_subpixel:
+        # Subpixel upscaling does not work on plaidml so skip this test
+        pytest.skip("Subpixel upscaling not supported in plaidML")
+    cls_ = NNBlocks(use_subpixel=use_subpixel,
+                    use_icnr_init=use_icnr_init,
+                    use_convaware_init=use_convaware_init,
+                    use_reflect_padding=use_reflect_padding)
+    block_test(cls_.conv2d, input_shape=(2, 5, 5, 128), kwargs=dict(filters=1024, kernel_size=3))
+    block_test(cls_.conv, input_shape=(2, 8, 8, 32), kwargs=dict(filters=64))
+    block_test(cls_.conv_sep, input_shape=(2, 8, 8, 32), kwargs=dict(filters=64))
+    block_test(cls_.upscale, input_shape=(2, 4, 4, 128), kwargs=dict(filters=64))
+    block_test(cls_.res_block, input_shape=(2, 2, 2, 64), kwargs=dict(filters=64))
diff --git a/tests/lib/model/normalization_test.py b/tests/lib/model/normalization_test.py
new file mode 100644
index 0000000..49715b0
--- /dev/null
+++ b/tests/lib/model/normalization_test.py
@@ -0,0 +1,37 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Normalization.
+
+Adapted from Keras tests.
+"""
+
+from keras import regularizers
+import pytest
+
+from lib.model import normalization
+from lib.utils import get_backend
+
+from .layers_test import layer_test
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_instance_normalization(dummy):  # pylint:disable=unused-argument
+    """ Basic test for instance normalization. """
+    layer_test(normalization.InstanceNormalization,
+               kwargs={'epsilon': 0.1,
+                       'gamma_regularizer': regularizers.l2(0.01),
+                       'beta_regularizer': regularizers.l2(0.01)},
+               input_shape=(3, 4, 2))
+    layer_test(normalization.InstanceNormalization,
+               kwargs={'epsilon': 0.1,
+                       'axis': 1},
+               input_shape=(1, 4, 1))
+    layer_test(normalization.InstanceNormalization,
+               kwargs={'gamma_initializer': 'ones',
+                       'beta_initializer': 'ones'},
+               input_shape=(3, 4, 2, 4))
+    layer_test(normalization.InstanceNormalization,
+               kwargs={'epsilon': 0.1,
+                       'axis': 1,
+                       'scale': False,
+                       'center': False},
+               input_shape=(3, 4, 2, 4))
diff --git a/tests/lib/model/optimizers_test.py b/tests/lib/model/optimizers_test.py
new file mode 100644
index 0000000..09c446d
--- /dev/null
+++ b/tests/lib/model/optimizers_test.py
@@ -0,0 +1,77 @@
+#!/usr/bin/env python3
+""" Tests for Faceswap Initializers.
+
+Adapted from Keras tests.
+"""
+import pytest
+
+from keras import optimizers as k_optimizers
+from keras.layers import Dense, Activation
+from keras.models import Sequential
+from keras.utils import test_utils
+from keras.utils.np_utils import to_categorical
+import numpy as np
+from numpy.testing import assert_allclose
+
+from lib.model import optimizers
+from lib.utils import get_backend
+
+
+def get_test_data():
+    """ Obtain radomized test data for training """
+    np.random.seed(1337)
+    (x_train, y_train), _ = test_utils.get_test_data(num_train=1000,
+                                                     num_test=200,
+                                                     input_shape=(10,),
+                                                     classification=True,
+                                                     num_classes=2)
+    y_train = to_categorical(y_train)
+    return x_train, y_train
+
+
+def _test_optimizer(optimizer, target=0.75):
+    x_train, y_train = get_test_data()
+
+    model = Sequential()
+    model.add(Dense(10, input_shape=(x_train.shape[1],)))
+    model.add(Activation('relu'))
+    model.add(Dense(y_train.shape[1]))
+    model.add(Activation('softmax'))
+    model.compile(loss='categorical_crossentropy',
+                  optimizer=optimizer,
+                  metrics=['accuracy'])
+
+    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)
+    # TODO PlaidML fails this test
+    assert history.history['acc'][-1] >= target
+    config = k_optimizers.serialize(optimizer)
+    optim = k_optimizers.deserialize(config)
+    new_config = k_optimizers.serialize(optim)
+    new_config['class_name'] = new_config['class_name'].lower()
+    assert config == new_config
+
+    # Test constraints.
+    model = Sequential()
+    dense = Dense(10,
+                  input_shape=(x_train.shape[1],),
+                  kernel_constraint=lambda x: 0. * x + 1.,
+                  bias_constraint=lambda x: 0. * x + 2.,)
+    model.add(dense)
+    model.add(Activation('relu'))
+    model.add(Dense(y_train.shape[1]))
+    model.add(Activation('softmax'))
+    model.compile(loss='categorical_crossentropy',
+                  optimizer=optimizer,
+                  metrics=['accuracy'])
+    model.train_on_batch(x_train[:10], y_train[:10])
+    kernel, bias = dense.get_weights()
+    assert_allclose(kernel, 1.)
+    assert_allclose(bias, 2.)
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+@pytest.mark.xfail(get_backend() == "amd", reason="plaidML fails the standard accuracy test")
+def test_adam(dummy):  # pylint:disable=unused-argument
+    """ Test for custom adam optimizer """
+    _test_optimizer(optimizers.Adam())
+    _test_optimizer(optimizers.Adam(decay=1e-3))
diff --git a/tests/startup_test.py b/tests/startup_test.py
new file mode 100644
index 0000000..4389fca
--- /dev/null
+++ b/tests/startup_test.py
@@ -0,0 +1,18 @@
+#!/usr/bin/env python3
+""" Sanity checks for Faceswap. """
+
+import inspect
+
+import pytest
+from keras import backend as K
+
+from lib.utils import get_backend
+
+
+@pytest.mark.parametrize('dummy', [None], ids=[get_backend().upper()])
+def test_backend(dummy):  # pylint:disable=unused-argument
+    """ Sanity check to ensure that Keras backend is returning the correct object type. """
+    backend = get_backend()
+    test_var = K.variable((1, 1, 4, 4))
+    lib = inspect.getmodule(test_var).__name__.split(".")[0]
+    assert (backend == "cpu" and lib == "tensorflow") or (backend == "amd" and lib == "plaidml")
