commit ca6324299604e693f3925ebecfe03bcade68865c
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sat Oct 27 10:12:08 2018 +0100

    Extraction - Speed improvements (#522) (#523)
    
    * Extraction - Speed improvements (#522)
    
    * Initial Plugin restructure
    
    * Detectors to plugins. Detector speed improvements
    
    * Re-implement dlib aligner, remove models, FAN to TF. Parallel processing
    
    * Update manual, update convert, implement parallel/serial switching
    
    * linting + fix cuda check (setup.py). requirements update keras 2.2.4
    
    * Add extract size option. Fix dlib hog init
    
    * GUI: Increase tooltip width
    
    * Update alignment tool to support new DetectedFace
    
    * Add skip existing faces option
    
    * Fix sort tool to new plugin structure
    
    * remove old align plugin
    
    * fix convert -skip faces bug
    
    * Fix convert skipping no faces frames
    
    * Convert - draw onto transparent layer
    
    * Fix blur threshold bug
    
    * fix skip_faces convert bug
    
    * Fix training

diff --git a/.gitignore b/.gitignore
index 990a596..ca01c79 100644
--- a/.gitignore
+++ b/.gitignore
@@ -5,19 +5,22 @@
 !*.txt
 !*.png
 !*.h5
+!*.pb
 !*.dat
 !*.npy
 !Dockerfile*
 !requirements*
+!.cache
 !lib
 !lib/face_alignment
-!lib/face_alignment/.cache
 !lib/gui
-!lib/gui/.cache
 !lib/gui/.cache/preview
 !lib/gui/.cache/icons
 !scripts
-!plugins
+!plugins/
+!plugins/*
+!plugins/extract/*
+!plugins/model/*
 !tools
 !tools/lib*
 
diff --git a/lib/cli.py b/lib/cli.py
index 26bb2c9..a9d44a3 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -6,7 +6,7 @@ import os
 import platform
 import sys
 
-from plugins.PluginLoader import PluginLoader
+from plugins.plugin_loader import PluginLoader
 
 
 class ScriptExecutor():
@@ -280,81 +280,6 @@ class ExtractConvertArgs(FaceSwapArgs):
                               "type": str,
                               "dest": "alignments_path",
                               "help": "Optional path to an alignments file."})
-        argument_list.append({"opts": ("--serializer", ),
-                              "type": str.lower,
-                              "dest": "serializer",
-                              "default": "json",
-                              "choices": ("json", "pickle", "yaml"),
-                              "help": "Serializer for alignments file. If "
-                                      "yaml is chosen and not available, then "
-                                      "json will be used as the default "
-                                      "fallback."})
-        argument_list.append({"opts": ("-D", "--detector"),
-                              "type": str,
-                              # case sensitive because this is used to load a
-                              # plugin.
-                              "choices": ("dlib-hog", "dlib-cnn",
-                                          "dlib-all", "mtcnn"),
-                              "default": "mtcnn",
-                              "help": "R|Detector to use.\n'dlib-hog': uses "
-                                      "least resources, but is the least\n\t"
-                                      "reliable.\n'dlib-cnn': faster than "
-                                      "mtcnn but detects fewer faces\n\tand "
-                                      "fewer false positives.\n'dlib-all': "
-                                      "attempts to find faces using "
-                                      "dlib-cnn,\n\tif none are found, "
-                                      "attempts to find faces\n\tusing "
-                                      "dlib-hog.\n'mtcnn': slower than dlib, "
-                                      "but uses fewer resources\n\twhilst "
-                                      "detecting more faces and more false\n\t"
-                                      "positives. Has superior alignment to "
-                                      "dlib"})
-        argument_list.append({"opts": ("-mtms", "--mtcnn-minsize"),
-                              "type": int,
-                              "dest": "mtcnn_minsize",
-                              "default": 20,
-                              "help": "The minimum size of a face to be "
-                                      "accepted. Lower values use "
-                                      "significantly more VRAM. Minimum "
-                                      "value is 10. Default is 20 "
-                                      "(MTCNN detector only)"})
-        argument_list.append({"opts": ("-mtth", "--mtcnn-threshold"),
-                              "nargs": "+",
-                              "type": str,
-                              "dest": "mtcnn_threshold",
-                              "default": ["0.6", "0.7", "0.7"],
-                              "help": "R|Three step threshold for face "
-                                      "detection. Should be\nthree decimal "
-                                      "numbers each less than 1. Eg:\n"
-                                      "'--mtcnn-threshold 0.6 0.7 0.7'.\n"
-                                      "1st stage: obtains face candidates.\n"
-                                      "2nd stage: refinement of face "
-                                      "candidates.\n3rd stage: further "
-                                      "refinement of face candidates.\n"
-                                      "Default is 0.6 0.7 0.7 "
-                                      "(MTCNN detector only)"})
-        argument_list.append({"opts": ("-mtsc", "--mtcnn-scalefactor"),
-                              "type": float,
-                              "dest": "mtcnn_scalefactor",
-                              "default": 0.709,
-                              "help": "The scale factor for the image "
-                                      "pyramid. Should be a decimal number "
-                                      "less than one. Default is 0.709 "
-                                      "(MTCNN detector only)"})
-        argument_list.append({"opts": ("-dbf", "--dlib-buffer"),
-                              "type": int,
-                              "dest": "dlib_buffer",
-                              "default": 64,
-                              "help": "This should only be increased if you "
-                                      "are having issues extracting with "
-                                      "DLib-cnn. The calculation of RAM "
-                                      "required is approximate, so some RAM "
-                                      " is held back in reserve (64MB by "
-                                      "default). If this is not enough "
-                                      "increase this figure by providing an "
-                                      "integer representing the amount of "
-                                      "megabytes to reserve. (DLIB-CNN "
-                                      "Only)"})
         argument_list.append({"opts": ("-l", "--ref_threshold"),
                               "type": float,
                               "dest": "ref_threshold",
@@ -397,6 +322,73 @@ class ExtractArgs(ExtractConvertArgs):
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
         argument_list = []
+        argument_list.append({"opts": ("--serializer", ),
+                              "type": str.lower,
+                              "dest": "serializer",
+                              "default": "json",
+                              "choices": ("json", "pickle", "yaml"),
+                              "help": "Serializer for alignments file. If "
+                                      "yaml is chosen and not available, then "
+                                      "json will be used as the default "
+                                      "fallback."})
+        argument_list.append({
+            "opts": ("-D", "--detector"),
+            "type": str,
+            "choices":  PluginLoader.get_available_extractors(
+                "detect"),
+            "default": "mtcnn",
+            "help": "R|Detector to use."
+                    "\n'dlib-hog': uses least resources, but is the"
+                    "\n\tleast reliable."
+                    "\n'dlib-cnn': faster than mtcnn but detects"
+                    "\n\tfewer faces and fewer false positives."
+                    "\n'mtcnn': slower than dlib, but uses fewer"
+                    "\n\tresources whilst detecting more faces and"
+                    "\n\tmore false positives. Has superior"
+                    "\n\talignment to dlib"})
+        argument_list.append({
+            "opts": ("-A", "--aligner"),
+            "type": str,
+            "choices": PluginLoader.get_available_extractors(
+                "align"),
+            "default": "fan",
+            "help": "R|Aligner to use."
+                    "\n'dlib': Dlib Pose Predictor. Faster, less "
+                    "\n\tresource intensive, but less accurate."
+                    "\n'fan': Face Alignment Network. Best aligner."
+                    "\n\tGPU heavy."})
+        argument_list.append({"opts": ("-mtms", "--mtcnn-minsize"),
+                              "type": int,
+                              "dest": "mtcnn_minsize",
+                              "default": 20,
+                              "help": "The minimum size of a face to be "
+                                      "accepted. Lower values use "
+                                      "significantly more VRAM. Minimum "
+                                      "value is 10. Default is 20 "
+                                      "(MTCNN detector only)"})
+        argument_list.append({"opts": ("-mtth", "--mtcnn-threshold"),
+                              "nargs": "+",
+                              "type": str,
+                              "dest": "mtcnn_threshold",
+                              "default": ["0.6", "0.7", "0.7"],
+                              "help": "R|Three step threshold for face "
+                                      "detection. Should be\nthree decimal "
+                                      "numbers each less than 1. Eg:\n"
+                                      "'--mtcnn-threshold 0.6 0.7 0.7'.\n"
+                                      "1st stage: obtains face candidates.\n"
+                                      "2nd stage: refinement of face "
+                                      "candidates.\n3rd stage: further "
+                                      "refinement of face candidates.\n"
+                                      "Default is 0.6 0.7 0.7 "
+                                      "(MTCNN detector only)"})
+        argument_list.append({"opts": ("-mtsc", "--mtcnn-scalefactor"),
+                              "type": float,
+                              "dest": "mtcnn_scalefactor",
+                              "default": 0.709,
+                              "help": "The scale factor for the image "
+                                      "pyramid. Should be a decimal number "
+                                      "less than one. Default is 0.709 "
+                                      "(MTCNN detector only)"})
         argument_list.append({"opts": ("-r", "--rotate-images"),
                               "type": str,
                               "dest": "rotate_images",
@@ -420,14 +412,34 @@ class ExtractArgs(ExtractConvertArgs):
         argument_list.append({"opts": ("-mp", "--multiprocess"),
                               "action": "store_true",
                               "default": False,
-                              "help": "Run extraction on all available "
-                                      "cores. (CPU only)"})
+                              "help": "Run extraction in parallel. Offers "
+                                      "speed up for some extractor/detector "
+                                      "combinations, less so for others. "
+                                      "Only has an effect if both the "
+                                      "aligner and detector use the GPU, "
+                                      "otherwise this is automatic."})
+        argument_list.append({"opts": ("-sz", "--size"),
+                              "type": int,
+                              "default": 256,
+                              "help": "The output size of extracted faces. "
+                                      "Make sure that the model you intend "
+                                      "to train supports your required "
+                                      "size. This will only need to be "
+                                      "changed for hi-res models."})
         argument_list.append({"opts": ("-s", "--skip-existing"),
                               "action": "store_true",
                               "dest": "skip_existing",
                               "default": False,
                               "help": "Skips frames that have already been "
-                                      "extracted"})
+                                      "extracted and exist in the alignments "
+                                      "file"})
+        argument_list.append({"opts": ("-sf", "--skip-existing-faces"),
+                              "action": "store_true",
+                              "dest": "skip_faces",
+                              "default": False,
+                              "help": "Skip frames that already have "
+                                      "detected faces in the alignments "
+                                      "file"})
         argument_list.append({"opts": ("-dl", "--debug-landmarks"),
                               "action": "store_true",
                               "dest": "debug_landmarks",
@@ -579,6 +591,13 @@ class ConvertArgs(ExtractConvertArgs):
                               "default": True,
                               "help": "Average color adjust. "
                                       "(Adjust converter only)"})
+        argument_list.append({"opts": ("-dt", "--draw-transparent"),
+                              "action": "store_true",
+                              "dest": "draw_transparent",
+                              "default": False,
+                              "help": "Place the swapped face on a "
+                                      "transparent layer rather than the "
+                                      "original frame."})
         return argument_list
 
 
diff --git a/lib/face_alignment/.cache/mmod_human_face_detector.dat b/lib/face_alignment/.cache/mmod_human_face_detector.dat
deleted file mode 100644
index f1f73a5..0000000
Binary files a/lib/face_alignment/.cache/mmod_human_face_detector.dat and /dev/null differ
diff --git a/lib/face_alignment/__init__.py b/lib/face_alignment/__init__.py
deleted file mode 100644
index c874031..0000000
--- a/lib/face_alignment/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from .extractor import Extract
\ No newline at end of file
diff --git a/lib/face_alignment/detectors.py b/lib/face_alignment/detectors.py
deleted file mode 100644
index 2b3a777..0000000
--- a/lib/face_alignment/detectors.py
+++ /dev/null
@@ -1,221 +0,0 @@
-#!/usr/bin python3
-""" DLIB Detector for face alignment
-    Code adapted and modified from:
-    https://github.com/1adrianb/face-alignment """
-
-import os
-import numpy as np
-
-from tensorflow import Graph, Session
-
-import dlib
-
-from .mtcnn import create_mtcnn, detect_face
-
-
-CACHE_PATH = os.path.join(os.path.dirname(__file__), ".cache")
-
-
-class Detector(object):
-    """ Detector object """
-    def __init__(self):
-        self.initialized = False
-        self.verbose = False
-        self.data_path = self.set_data_path()
-        self.detected_faces = None
-
-    @staticmethod
-    def set_data_path():
-        """ path to data file/models
-            override for specific detector """
-        pass
-
-    def set_predetected(self, width, height):
-        """ Set a dlib rectangle for predetected faces """
-        # Predetected_face is used for sort tool.
-        # Landmarks should not be extracted again from predetected faces,
-        # because face data is lost, resulting in a large variance
-        # against extract from original image
-        self.detected_faces = [dlib.rectangle(0, 0, width, height)]
-
-    @staticmethod
-    def is_mmod_rectangle(d_rectangle):
-        """ Return whether the passed in object is
-            a dlib.mmod_rectangle """
-        return isinstance(d_rectangle, dlib.mmod_rectangle)
-
-
-class ManualDetector(Detector):
-    """ Manual Detector """
-    def set_data_path(self):
-        return None
-
-    def create_detector(self, verbose):
-        """ Create the mtcnn detector """
-        self.verbose = verbose
-
-        if self.verbose:
-            print("Adding Manual detector")
-
-    def detect_faces(self, bounding_box):
-        """ Return the given bounding box in a dlib rectangle """
-        face = bounding_box
-        self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
-                                              int(face[2]), int(face[3]))]
-
-
-class DLibDetector(Detector):
-    """ Dlib detector for face recognition """
-    def __init__(self):
-        Detector.__init__(self)
-        self.detectors = list()
-
-    @staticmethod
-    def compiled_for_cuda():
-        """ Return a message on DLIB Cuda Compilation status """
-        msg = "DLib IS "
-        if not dlib.DLIB_USE_CUDA:
-            msg += "NOT "
-        msg += "compiled to use CUDA"
-        return msg
-
-    @staticmethod
-    def set_data_path():
-        """ Load the face detector data """
-        data_path = os.path.join(CACHE_PATH,
-                                 "mmod_human_face_detector.dat")
-        if not os.path.exists(data_path):
-            raise Exception("Error: Unable to find {}, reinstall "
-                            "the lib!".format(data_path))
-        return data_path
-
-    def create_detector(self, verbose, detector, placeholder):
-        """ Add the requested detectors """
-        self.verbose = verbose
-
-        if detector == "dlib-cnn" or detector == "dlib-all":
-            if self.verbose:
-                print("Adding DLib - CNN detector")
-            self.detectors.append(dlib.cnn_face_detection_model_v1(
-                self.data_path))
-
-        if detector == "dlib-hog" or detector == "dlib-all":
-            if self.verbose:
-                print("Adding DLib - HOG detector")
-            self.detectors.append(dlib.get_frontal_face_detector())
-
-        for current_detector in self.detectors:
-            current_detector(placeholder, 0)
-
-        self.initialized = True
-
-    def detect_faces(self, image):
-        """ Detect faces in rgb image """
-        self.detected_faces = None
-        for current_detector in self.detectors:
-            self.detected_faces = current_detector(image, 0)
-
-            if self.detected_faces:
-                break
-
-
-class MTCNNDetector(Detector):
-    """ MTCNN detector for face recognition """
-    def __init__(self):
-        Detector.__init__(self)
-        self.kwargs = None
-
-    @staticmethod
-    def validate_kwargs(kwargs):
-        """ Validate that cli kwargs are correct. If not reset to default """
-        valid = True
-        if kwargs['minsize'] < 10:
-            valid = False
-        elif len(kwargs['threshold']) != 3:
-            valid = False
-        elif not all(0.0 < threshold < 1.0
-                     for threshold in kwargs['threshold']):
-            valid = False
-        elif not 0.0 < kwargs['factor'] < 1.0:
-            valid = False
-
-        if not valid:
-            print("Invalid MTCNN arguments received. Running with defaults")
-            return {"minsize": 20,                 # minimum size of face
-                    "threshold": [0.6, 0.7, 0.7],  # three steps threshold
-                    "factor": 0.709}               # scale factor
-        return kwargs
-
-    @staticmethod
-    def set_data_path():
-        """ Load the mtcnn models """
-        for model in ("det1.npy", "det2.npy", "det3.npy"):
-            model_path = os.path.join(CACHE_PATH, model)
-            if not os.path.exists(model_path):
-                raise Exception("Error: Unable to find {}, reinstall "
-                                "the lib!".format(model_path))
-        return CACHE_PATH
-
-    def create_detector(self, verbose, mtcnn_kwargs):
-        """ Create the mtcnn detector """
-        self.verbose = verbose
-
-        if self.verbose:
-            print("Adding MTCNN detector")
-
-        self.kwargs = mtcnn_kwargs
-
-        mtcnn_graph = Graph()
-        with mtcnn_graph.as_default():
-            mtcnn_session = Session()
-            with mtcnn_session.as_default():
-                pnet, rnet, onet = create_mtcnn(mtcnn_session, self.data_path)
-        mtcnn_graph.finalize()
-
-        self.kwargs["pnet"] = pnet
-        self.kwargs["rnet"] = rnet
-        self.kwargs["onet"] = onet
-        self.initialized = True
-
-    def detect_faces(self, image):
-        """ Detect faces in rgb image """
-        self.detected_faces = None
-        detected_faces, points = detect_face(image, **self.kwargs)
-        detected_faces = self.recalculate_bounding_box(detected_faces, points)
-        self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
-                                              int(face[2]), int(face[3]))
-                               for face in detected_faces]
-
-    @staticmethod
-    def recalculate_bounding_box(faces, landmarks):
-        """ Recalculate the bounding box for Face Alignment.
-
-            Face Alignment was built to expect a DLIB bounding
-            box and calculates center and scale based on that.
-            Resize the bounding box around features to present
-            a better box to Face Alignment. Helps its chances
-            on edge cases and helps remove 'jitter' """
-        retval = list()
-        no_faces = len(faces)
-        if no_faces == 0:
-            return retval
-        face_landmarks = np.hsplit(landmarks, no_faces)
-        for idx in range(no_faces):
-            pts = np.reshape(face_landmarks[idx], (5, 2), order="F")
-            nose = pts[2]
-
-            amin, amax = np.amin(pts, axis=0), np.amax(pts, axis=0)
-            pad_x, pad_y = (amax[0] - amin[0]) / 2, (amax[1] - amin[1]) / 2
-
-            center = (amax[0] - pad_x, amax[1] - pad_y)
-            offset = (center[0] - nose[0], nose[1] - center[1])
-            center = (center[0] + offset[0], center[1] + offset[1])
-
-            pad_x += pad_x
-            pad_y += pad_y
-
-            bounding = [center[0] - pad_x, center[1] - pad_y,
-                        center[0] + pad_x, center[1] + pad_y]
-
-            retval.append(bounding)
-        return retval
diff --git a/lib/face_alignment/extractor.py b/lib/face_alignment/extractor.py
deleted file mode 100644
index cf4d416..0000000
--- a/lib/face_alignment/extractor.py
+++ /dev/null
@@ -1,370 +0,0 @@
-#!/usr/bin python3
-""" Facial landmarks extractor for faceswap.py
-    Code adapted and modified from:
-    https://github.com/1adrianb/face-alignment
-"""
-
-import cv2
-import numpy as np
-
-from .detectors import DLibDetector, MTCNNDetector, ManualDetector
-from .vram_allocation import GPUMem
-from .model import KerasModel
-
-DLIB_DETECTORS = DLibDetector()
-MTCNN_DETECTOR = MTCNNDetector()
-MANUAL_DETECTOR = ManualDetector()
-VRAM = GPUMem()
-KERAS_MODEL = KerasModel()
-
-
-class Frame():
-    """ The current frame for processing """
-
-    def __init__(self, detector, input_image,
-                 verbose, input_is_predetected_face):
-        self.verbose = verbose
-        self.height, self.width = input_image.shape[:2]
-
-        self.input_scale = 1.0
-
-        self.image_bgr = input_image
-        self.image_rgb = input_image[:, :, ::-1].copy()
-        self.image_detect = self.scale_image(input_is_predetected_face,
-                                             detector)
-
-    def scale_image(self, input_is_predetected_face, detector):
-        """ Scale down large images based on vram amount """
-        image = self.image_rgb
-        if input_is_predetected_face:
-            return image
-
-        if detector == "mtcnn":
-            self.scale_mtcnn()
-        elif detector == "manual":
-            self.input_scale = 1.0
-        else:
-            self.scale_dlib()
-
-        if self.input_scale == 1.0:
-            return image
-
-        if self.input_scale > 1.0:
-            interpolation = cv2.INTER_LINEAR
-        else:
-            interpolation = cv2.INTER_AREA
-
-        dimensions = (int(self.width * self.input_scale),
-                      int(self.height * self.input_scale))
-        if self.verbose and self.input_scale < 1.0:
-            print("Resizing image from {}x{} "
-                  "to {}.".format(str(self.width), str(self.height),
-                                  "x".join(str(i) for i in dimensions)))
-        image = cv2.resize(image,
-                           dimensions,
-                           interpolation=interpolation).copy()
-
-        return image
-
-    def scale_mtcnn(self):
-        """ Set scaling for mtcnn """
-        pixel_count = self.width * self.height
-        if pixel_count > VRAM.scale_to:
-            self.input_scale = (VRAM.scale_to / pixel_count)**0.5
-
-    def scale_dlib(self):
-        """ Set scaling for dlib
-
-        DLIB is finickity, and pure pixel count won't help as when an
-        initial portrait image goes in, rotating it to landscape sucks
-        up VRAM for no discernible reason. This does not happen when the
-        initial image is a landscape image.
-        To mitigate this we need to make sure that all images fit within
-        a square based on the pixel count
-        There is also no way to set the acceptable size for a positive
-        match, so all images should be scaled to the maximum possible
-        to detect all available faces """
-
-        max_length_scale = int(VRAM.scale_to ** 0.5)
-        max_length_image = max(self.height, self.width)
-        self.input_scale = max_length_scale / max_length_image
-
-
-class Align():
-    """ Perform transformation to align and get landmarks """
-    def __init__(self, image, detected_faces, keras_model, verbose):
-        self.verbose = verbose
-        self.image = image
-        self.detected_faces = detected_faces
-        self.keras = keras_model
-
-        self.landmarks = self.process_landmarks()
-
-    @staticmethod
-    def transform(point, center, scale, resolution):
-        """ Transform Image """
-        pnt = np.array([point[0], point[1], 1.0])
-        hscl = 200.0 * scale
-        eye = np.eye(3)
-        eye[0, 0] = resolution / hscl
-        eye[1, 1] = resolution / hscl
-        eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
-        eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
-        eye = np.linalg.inv(eye)
-        return np.matmul(eye, pnt)[0:2]
-
-    def crop(self, image, center, scale, resolution=256.0):
-        """ Crop image around the center point """
-        v_ul = self.transform([1, 1], center, scale, resolution).astype(np.int)
-        v_br = self.transform([resolution, resolution],
-                              center,
-                              scale,
-                              resolution).astype(np.int)
-        if image.ndim > 2:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0],
-                                image.shape[2]],
-                               dtype=np.int32)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        else:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0]],
-                               dtype=np.int)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        height = image.shape[0]
-        width = image.shape[1]
-        new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
-                         dtype=np.int32)
-        new_y = np.array([max(1, -v_ul[1] + 1),
-                          min(v_br[1], height) - v_ul[1]],
-                         dtype=np.int32)
-        old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
-                         dtype=np.int32)
-        old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
-                         dtype=np.int32)
-        new_img[new_y[0] - 1:new_y[1],
-                new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
-                                               old_x[0] - 1:old_x[1], :]
-        new_img = cv2.resize(new_img,
-                             dsize=(int(resolution), int(resolution)),
-                             interpolation=cv2.INTER_LINEAR)
-        return new_img
-
-    def get_pts_from_predict(self, var_a, center, scale):
-        """ Get points from predictor """
-        var_b = var_a.reshape((var_a.shape[0],
-                               var_a.shape[1] * var_a.shape[2]))
-        var_c = var_b.argmax(1).reshape((var_a.shape[0],
-                                         1)).repeat(2,
-                                                    axis=1).astype(np.float)
-        var_c[:, 0] %= var_a.shape[2]
-        var_c[:, 1] = np.apply_along_axis(
-            lambda x: np.floor(x / var_a.shape[2]),
-            0,
-            var_c[:, 1])
-
-        for i in range(var_a.shape[0]):
-            pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
-            if pt_x > 0 and pt_x < 63 and pt_y > 0 and pt_y < 63:
-                diff = np.array([var_a[i, pt_y, pt_x+1]
-                                 - var_a[i, pt_y, pt_x-1],
-                                 var_a[i, pt_y+1, pt_x]
-                                 - var_a[i, pt_y-1, pt_x]])
-
-                var_c[i] += np.sign(diff)*0.25
-
-        var_c += 0.5
-        return [self.transform(var_c[i], center, scale, var_a.shape[2])
-                for i in range(var_a.shape[0])]
-
-    def process_landmarks(self):
-        """ Align image and process landmarks """
-        landmarks = list()
-        if not self.detected_faces:
-            if self.verbose:
-                print("Warning: No faces were detected.")
-            return landmarks
-
-        for detected_face in self.detected_faces:
-
-            center, scale = self.get_center_scale(detected_face)
-            image = self.align_image(center, scale)
-
-            landmarks_xy = self.predict_landmarks(image, center, scale)
-
-            landmarks.append(((detected_face['left'],
-                               detected_face['top'],
-                               detected_face['right'],
-                               detected_face['bottom']),
-                              landmarks_xy))
-
-        return landmarks
-
-    @staticmethod
-    def get_center_scale(detected_face):
-        """ Get the center and set scale of bounding box """
-        center = np.array([(detected_face['left']
-                            + detected_face['right']) / 2.0,
-                           (detected_face['top']
-                            + detected_face['bottom']) / 2.0])
-
-        center[1] -= (detected_face['bottom']
-                      - detected_face['top']) * 0.12
-
-        scale = (detected_face['right']
-                 - detected_face['left']
-                 + detected_face['bottom']
-                 - detected_face['top']) / 195.0
-
-        return center, scale
-
-    def align_image(self, center, scale):
-        """ Crop and align image around center """
-        image = self.crop(
-            self.image,
-            center,
-            scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
-
-        return np.expand_dims(image, 0)
-
-    def predict_landmarks(self, image, center, scale):
-        """ Predict the 68 point landmarks """
-        with self.keras.session.as_default():
-            pts_img = self.get_pts_from_predict(
-                self.keras.model.predict(image)[-1][0],
-                center,
-                scale)
-
-        return [(int(pt[0]), int(pt[1])) for pt in pts_img]
-
-
-class Extract():
-    """ Extracts faces from an image, crops and
-        calculates landmarks """
-
-    def __init__(self, input_image_bgr, detector, dlib_buffer=64,
-                 mtcnn_kwargs=None, verbose=False,
-                 input_is_predetected_face=False,
-                 initialize_only=False):
-        self.verbose = verbose
-        self.keras = KERAS_MODEL
-        self.detector_name = detector
-        self.detector = None
-        self.frame = None
-        self.bounding_boxes = None
-        self.landmarks = None
-
-        self.initialize(mtcnn_kwargs, dlib_buffer)
-
-        if not initialize_only:
-            self.execute(input_image_bgr, input_is_predetected_face)
-
-    def initialize(self, mtcnn_kwargs, dlib_buffer):
-        """ initialize Keras and Dlib """
-        if not VRAM.initialized:
-            self.initialize_vram(dlib_buffer)
-
-        if not self.keras.initialized:
-            self.initialize_keras()
-            # VRAM Scaling factor must be set AFTER Keras has loaded
-            VRAM.set_scale_to(self.detector_name)
-
-        if self.detector_name == "mtcnn":
-            self.detector = MTCNN_DETECTOR
-        elif self.detector_name == "manual":
-            self.detector = MANUAL_DETECTOR
-        else:
-            self.detector = DLIB_DETECTORS
-
-        if not self.detector.initialized:
-            self.initialize_detector(mtcnn_kwargs)
-
-    def initialize_vram(self, dlib_buffer):
-        """ Initialize vram based on detector """
-        VRAM.verbose = self.verbose
-        VRAM.detector = self.detector_name
-        if dlib_buffer > VRAM.dlib_buffer:
-            VRAM.dlib_buffer = dlib_buffer
-        VRAM.initialized = True
-        VRAM.output_stats()
-
-    def initialize_keras(self):
-        """ Initialize keras. Allocate vram to tensorflow
-            based on detector """
-        ratio = None
-        if self.detector_name != "mtcnn" and VRAM.device != -1:
-            ratio = VRAM.get_tensor_gpu_ratio()
-        placeholder = np.zeros((1, 3, 256, 256))
-        self.keras.load_model(verbose=self.verbose,
-                              ratio=ratio,
-                              dummy=placeholder)
-
-    def initialize_detector(self, mtcnn_kwargs):
-        """ Initialize face detector """
-        kwargs = {"verbose": self.verbose}
-        if self.detector_name == "mtcnn":
-            mtcnn_kwargs = self.detector.validate_kwargs(mtcnn_kwargs)
-            kwargs["mtcnn_kwargs"] = mtcnn_kwargs
-        elif self.detector_name != "manual":
-            kwargs["detector"] = self.detector_name
-            scale_to = int(VRAM.scale_to ** 0.5)
-
-            if self.verbose:
-                print(self.detector.compiled_for_cuda())
-                print("Initializing DLib for frame size {}x{}".format(
-                    str(scale_to), str(scale_to)))
-
-            placeholder = np.zeros((scale_to, scale_to, 3), dtype=np.uint8)
-            kwargs["placeholder"] = placeholder
-
-        self.detector.create_detector(**kwargs)
-
-    def execute(self, input_image_bgr,
-                input_is_predetected_face=False, manual_face=None):
-        """ Execute extract """
-        self.frame = Frame(detector=self.detector_name,
-                           input_image=input_image_bgr,
-                           verbose=self.verbose,
-                           input_is_predetected_face=input_is_predetected_face)
-
-        self.detect_faces(input_is_predetected_face, manual_face)
-        self.bounding_boxes = self.get_bounding_boxes()
-
-        self.landmarks = Align(image=self.frame.image_rgb,
-                               detected_faces=self.bounding_boxes,
-                               keras_model=self.keras,
-                               verbose=self.verbose).landmarks
-
-    def detect_faces(self, input_is_predetected_face, manual_face):
-        """ Detect faces """
-        # Predetected_face is used for sort tool.
-        # Landmarks should not be extracted again from predetected faces,
-        # because face data is lost, resulting in a large variance
-        # against extract from original image
-
-        if input_is_predetected_face:
-            self.detector.set_predetected(self.frame.width, self.frame.height)
-        elif manual_face:
-            self.detector.detect_faces(manual_face)
-        else:
-            self.detector.detect_faces(self.frame.image_detect)
-
-    def get_bounding_boxes(self):
-        """ Return the corner points of the bounding box scaled
-            to original image """
-        bounding_boxes = list()
-        for d_rect in self.detector.detected_faces:
-            d_rect = self.convert_to_dlib_rectangle(d_rect)
-            bounding_box = {
-                'left': int(d_rect.left() / self.frame.input_scale),
-                'top': int(d_rect.top() / self.frame.input_scale),
-                'right': int(d_rect.right() / self.frame.input_scale),
-                'bottom': int(d_rect.bottom() / self.frame.input_scale)}
-            bounding_boxes.append(bounding_box)
-        return bounding_boxes
-
-    def convert_to_dlib_rectangle(self, d_rect):
-        """ Convert detected faces to dlib_rectangle """
-        if self.detector.is_mmod_rectangle(d_rect):
-            return d_rect.rect
-        return d_rect
diff --git a/lib/face_alignment/model.py b/lib/face_alignment/model.py
deleted file mode 100644
index 1db8520..0000000
--- a/lib/face_alignment/model.py
+++ /dev/null
@@ -1,125 +0,0 @@
-#!/usr/bin python3
-""" FAN model for face alignment
-    Code adapted and modified from:
-    https://github.com/1adrianb/face-alignment """
-
-import os
-
-import keras
-from keras import backend as K
-from tensorflow import ConfigProto, Graph, Session
-
-
-class TorchBatchNorm2D(keras.engine.base_layer.Layer):
-    """" Keras Model """
-    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
-        super(TorchBatchNorm2D, self).__init__(**kwargs)
-        self.supports_masking = True
-        self.axis = axis
-        self.momentum = momentum
-        self.epsilon = epsilon
-
-        self.built = False
-        self.gamma = None
-        self.beta = None
-        self.moving_mean = None
-        self.moving_variance = None
-
-    def build(self, input_shape):
-        dim = input_shape[self.axis]
-        if dim is None:
-            raise ValueError("Axis {} of input tensor should have a "
-                             "defined dimension but the layer received "
-                             "an input with  shape {}."
-                             .format(str(self.axis), str(input_shape)))
-        shape = (dim,)
-        self.gamma = self.add_weight(shape=shape,
-                                     name='gamma',
-                                     initializer='ones',
-                                     regularizer=None,
-                                     constraint=None)
-        self.beta = self.add_weight(shape=shape,
-                                    name='beta',
-                                    initializer='zeros',
-                                    regularizer=None,
-                                    constraint=None)
-        self.moving_mean = self.add_weight(shape=shape,
-                                           name='moving_mean',
-                                           initializer='zeros',
-                                           trainable=False)
-        self.moving_variance = self.add_weight(shape=shape,
-                                               name='moving_variance',
-                                               initializer='ones',
-                                               trainable=False)
-        self.built = True
-
-    def call(self, inputs, **kwargs):
-        input_shape = K.int_shape(inputs)
-
-        broadcast_shape = [1] * len(input_shape)
-        broadcast_shape[self.axis] = input_shape[self.axis]
-
-        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
-        broadcast_moving_variance = K.reshape(self.moving_variance,
-                                              broadcast_shape)
-        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
-        broadcast_beta = K.reshape(self.beta, broadcast_shape)
-        invstd = (K.ones(shape=broadcast_shape,
-                         dtype='float32')
-                  / K.sqrt(broadcast_moving_variance
-                           + K.constant(self.epsilon,
-                                        dtype='float32')))
-
-        return((inputs - broadcast_moving_mean)
-               * invstd
-               * broadcast_gamma
-               + broadcast_beta)
-
-    def get_config(self):
-        config = {'axis': self.axis,
-                  'momentum': self.momentum,
-                  'epsilon': self.epsilon}
-        base_config = super(TorchBatchNorm2D, self).get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-class KerasModel(object):
-    "Load the Keras Model"
-    def __init__(self):
-        self.initialized = False
-        self.verbose = False
-        self.model_path = self.set_model_path()
-        self.model = None
-        self.session = None
-
-    @staticmethod
-    def set_model_path():
-        """ Set the path to the Face Alignment Network Model """
-        model_path = os.path.join(os.path.dirname(__file__),
-                                  ".cache", "2DFAN-4.h5")
-        if not os.path.exists(model_path):
-            raise Exception("Error: Unable to find {}, "
-                            "reinstall the lib!".format(model_path))
-        return model_path
-
-    def load_model(self, verbose, dummy, ratio):
-        """ Load the Keras Model """
-        self.verbose = verbose
-        if self.verbose:
-            print("Initializing keras model...")
-
-        keras_graph = Graph()
-        with keras_graph.as_default():
-            config = ConfigProto()
-            if ratio:
-                config.gpu_options.per_process_gpu_memory_fraction = ratio
-            self.session = Session(config=config)
-            with self.session.as_default():
-                self.model = keras.models.load_model(
-                    self.model_path,
-                    custom_objects={'TorchBatchNorm2D':
-                                    TorchBatchNorm2D})
-                self.model.predict(dummy)
-        keras_graph.finalize()
-
-        self.initialized = True
diff --git a/lib/face_alignment/mtcnn.py b/lib/face_alignment/mtcnn.py
deleted file mode 100644
index 4413dd7..0000000
--- a/lib/face_alignment/mtcnn.py
+++ /dev/null
@@ -1,561 +0,0 @@
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-#!/usr/bin python3
-""" MTCNN Detector for face alignment
-    Code adapted from:
-    https://github.com/davidsandberg/facenet """
-
-""" Tensorflow implementation of the face detection / alignment algorithm found at
-https://github.com/kpzhang93/MTCNN_face_detection_alignment
-"""
-# MIT License
-#
-# Copyright (c) 2016 David Sandberg
-#
-# Permission is hereby granted, free of charge, to any person obtaining a copy
-# of this software and associated documentation files (the "Software"), to deal
-# in the Software without restriction, including without limitation the rights
-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-# copies of the Software, and to permit persons to whom the Software is
-# furnished to do so, subject to the following conditions:
-#
-# The above copyright notice and this permission notice shall be included in all
-# copies or substantial portions of the Software.
-#
-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-# SOFTWARE.
-
-from six import string_types, iteritems
-
-import numpy as np
-import tensorflow as tf
-#from math import floor
-import cv2
-import os
-
-def layer(op):
-    """Decorator for composable network layers."""
-
-    def layer_decorated(self, *args, **kwargs):
-        # Automatically set a name if not provided.
-        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))
-        # Figure out the layer inputs.
-        if len(self.terminals) == 0:
-            raise RuntimeError('No input variables found for layer %s.' % name)
-        elif len(self.terminals) == 1:
-            layer_input = self.terminals[0]
-        else:
-            layer_input = list(self.terminals)
-        # Perform the operation and get the output.
-        layer_output = op(self, layer_input, *args, **kwargs)
-        # Add to layer LUT.
-        self.layers[name] = layer_output
-        # This output is now the input for the next layer.
-        self.feed(layer_output)
-        # Return self for chained calls.
-        return self
-
-    return layer_decorated
-
-class Network(object):
-
-    def __init__(self, inputs, trainable=True):
-        # The input nodes for this network
-        self.inputs = inputs
-        # The current list of terminal nodes
-        self.terminals = []
-        # Mapping from layer names to layers
-        self.layers = dict(inputs)
-        # If true, the resulting variables are set as trainable
-        self.trainable = trainable
-
-        self.setup()
-
-    def setup(self):
-        """Construct the network. """
-        raise NotImplementedError('Must be implemented by the subclass.')
-
-    def load(self, data_path, session, ignore_missing=False):
-        """Load network weights.
-        data_path: The path to the numpy-serialized network weights
-        session: The current TensorFlow session
-        ignore_missing: If true, serialized weights for missing layers are ignored.
-        """
-        data_dict = np.load(data_path, encoding='latin1').item() #pylint: disable=no-member
-
-        for op_name in data_dict:
-            with tf.variable_scope(op_name, reuse=True):
-                for param_name, data in iteritems(data_dict[op_name]):
-                    try:
-                        var = tf.get_variable(param_name)
-                        session.run(var.assign(data))
-                    except ValueError:
-                        if not ignore_missing:
-                            raise
-
-    def feed(self, *args):
-        """Set the input(s) for the next operation by replacing the terminal nodes.
-        The arguments can be either layer names or the actual layers.
-        """
-        assert len(args) != 0
-        self.terminals = []
-        for fed_layer in args:
-            if isinstance(fed_layer, string_types):
-                try:
-                    fed_layer = self.layers[fed_layer]
-                except KeyError:
-                    raise KeyError('Unknown layer name fed: %s' % fed_layer)
-            self.terminals.append(fed_layer)
-        return self
-
-    def get_output(self):
-        """Returns the current network output."""
-        return self.terminals[-1]
-
-    def get_unique_name(self, prefix):
-        """Returns an index-suffixed unique name for the given prefix.
-        This is used for auto-generating layer names based on the type-prefix.
-        """
-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1
-        return '%s_%d' % (prefix, ident)
-
-    def make_var(self, name, shape):
-        """Creates a new TensorFlow variable."""
-        return tf.get_variable(name, shape, trainable=self.trainable)
-
-    def validate_padding(self, padding):
-        """Verifies that the padding is one of the supported ones."""
-        assert padding in ('SAME', 'VALID')
-
-    @layer
-    def conv(self,
-             inp,
-             k_h,
-             k_w,
-             c_o,
-             s_h,
-             s_w,
-             name,
-             relu=True,
-             padding='SAME',
-             group=1,
-             biased=True):
-        # Verify that the padding is acceptable
-        self.validate_padding(padding)
-        # Get the number of channels in the input
-        c_i = int(inp.get_shape()[-1])
-        # Verify that the grouping parameter is valid
-        assert c_i % group == 0
-        assert c_o % group == 0
-        # Convolution for a given input and kernel
-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)
-        with tf.variable_scope(name) as scope:
-            kernel = self.make_var('weights', shape=[k_h, k_w, c_i // group, c_o])
-            # This is the common-case. Convolve the input without any further complications.
-            output = convolve(inp, kernel)
-            # Add the biases
-            if biased:
-                biases = self.make_var('biases', [c_o])
-                output = tf.nn.bias_add(output, biases)
-            if relu:
-                # ReLU non-linearity
-                output = tf.nn.relu(output, name=scope.name)
-            return output
-
-    @layer
-    def prelu(self, inp, name):
-        with tf.variable_scope(name):
-            i = int(inp.get_shape()[-1])
-            alpha = self.make_var('alpha', shape=(i,))
-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))
-        return output
-
-    @layer
-    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding='SAME'):
-        self.validate_padding(padding)
-        return tf.nn.max_pool(inp,
-                              ksize=[1, k_h, k_w, 1],
-                              strides=[1, s_h, s_w, 1],
-                              padding=padding,
-                              name=name)
-
-    @layer
-    def fc(self, inp, num_out, name, relu=True):
-        with tf.variable_scope(name):
-            input_shape = inp.get_shape()
-            if input_shape.ndims == 4:
-                # The input is spatial. Vectorize it first.
-                dim = 1
-                for d in input_shape[1:].as_list():
-                    dim *= int(d)
-                feed_in = tf.reshape(inp, [-1, dim])
-            else:
-                feed_in, dim = (inp, input_shape[-1].value)
-            weights = self.make_var('weights', shape=[dim, num_out])
-            biases = self.make_var('biases', [num_out])
-            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b
-            fc = op(feed_in, weights, biases, name=name)
-            return fc
-
-
-    """
-    Multi dimensional softmax,
-    refer to https://github.com/tensorflow/tensorflow/issues/210
-    compute softmax along the dimension of target
-    the native softmax only supports batch_size x dimension
-    """
-    @layer
-    def softmax(self, target, axis, name=None):
-        max_axis = tf.reduce_max(target, axis, keepdims=True)
-        target_exp = tf.exp(target-max_axis)
-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)
-        softmax = tf.div(target_exp, normalize, name)
-        return softmax
-
-class PNet(Network):
-    def setup(self):
-        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
-             .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')
-             .prelu(name='PReLU1')
-             .max_pool(2, 2, 2, 2, name='pool1')
-             .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')
-             .prelu(name='PReLU2')
-             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')
-             .prelu(name='PReLU3')
-             .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')
-             .softmax(3,name='prob1'))
-
-        (self.feed('PReLU3') #pylint: disable=no-value-for-parameter
-             .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))
-
-class RNet(Network):
-    def setup(self):
-        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
-             .conv(3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')
-             .prelu(name='prelu1')
-             .max_pool(3, 3, 2, 2, name='pool1')
-             .conv(3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')
-             .prelu(name='prelu2')
-             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
-             .conv(2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')
-             .prelu(name='prelu3')
-             .fc(128, relu=False, name='conv4')
-             .prelu(name='prelu4')
-             .fc(2, relu=False, name='conv5-1')
-             .softmax(1,name='prob1'))
-
-        (self.feed('prelu4') #pylint: disable=no-value-for-parameter
-             .fc(4, relu=False, name='conv5-2'))
-
-class ONet(Network):
-    def setup(self):
-        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
-             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')
-             .prelu(name='prelu1')
-             .max_pool(3, 3, 2, 2, name='pool1')
-             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')
-             .prelu(name='prelu2')
-             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
-             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')
-             .prelu(name='prelu3')
-             .max_pool(2, 2, 2, 2, name='pool3')
-             .conv(2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')
-             .prelu(name='prelu4')
-             .fc(256, relu=False, name='conv5')
-             .prelu(name='prelu5')
-             .fc(2, relu=False, name='conv6-1')
-             .softmax(1, name='prob1'))
-
-        (self.feed('prelu5') #pylint: disable=no-value-for-parameter
-             .fc(4, relu=False, name='conv6-2'))
-
-        (self.feed('prelu5') #pylint: disable=no-value-for-parameter
-             .fc(10, relu=False, name='conv6-3'))
-
-def create_mtcnn(sess, model_path):
-    if not model_path:
-        model_path,_ = os.path.split(os.path.realpath(__file__))
-
-    with tf.variable_scope('pnet'):
-        data = tf.placeholder(tf.float32, (None,None,None,3), 'input')
-        pnet = PNet({'data':data})
-        pnet.load(os.path.join(model_path, 'det1.npy'), sess)
-    with tf.variable_scope('rnet'):
-        data = tf.placeholder(tf.float32, (None,24,24,3), 'input')
-        rnet = RNet({'data':data})
-        rnet.load(os.path.join(model_path, 'det2.npy'), sess)
-    with tf.variable_scope('onet'):
-        data = tf.placeholder(tf.float32, (None,48,48,3), 'input')
-        onet = ONet({'data':data})
-        onet.load(os.path.join(model_path, 'det3.npy'), sess)
-
-    pnet_fun = lambda img : sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0':img})
-    rnet_fun = lambda img : sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0':img})
-    onet_fun = lambda img : sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0':img})
-    return pnet_fun, rnet_fun, onet_fun
-
-def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):
-    """Detects faces in an image, and returns bounding boxes and points for them.
-    img: input image
-    minsize: minimum faces' size
-    pnet, rnet, onet: caffemodel
-    threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
-    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.
-    """
-    factor_count=0
-    total_boxes=np.empty((0,9))
-    points=np.empty(0)
-    h=img.shape[0]
-    w=img.shape[1]
-    minl=np.amin([h, w])
-    m=12.0/minsize
-    minl=minl*m
-    # create scale pyramid
-    scales=[]
-    while minl>=12:
-        scales += [m*np.power(factor, factor_count)]
-        minl = minl*factor
-        factor_count += 1
-
-    # # # # # # # # # # # # #
-    # first stage - fast proposal network (pnet) to obtain face candidates
-    # # # # # # # # # # # # #
-
-    for scale in scales:
-        hs=int(np.ceil(h*scale))
-        ws=int(np.ceil(w*scale))
-        im_data = imresample(img, (hs, ws))
-        im_data = (im_data-127.5)*0.0078125
-        img_x = np.expand_dims(im_data, 0)
-        img_y = np.transpose(img_x, (0,2,1,3))
-        out = pnet(img_y)
-        out0 = np.transpose(out[0], (0,2,1,3))
-        out1 = np.transpose(out[1], (0,2,1,3))
-
-        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])
-
-        # inter-scale nms
-        pick = nms(boxes.copy(), 0.5, 'Union')
-        if boxes.size>0 and pick.size>0:
-            boxes = boxes[pick,:]
-            total_boxes = np.append(total_boxes, boxes, axis=0)
-
-    numbox = total_boxes.shape[0]
-    if numbox>0:
-        pick = nms(total_boxes.copy(), 0.7, 'Union')
-        total_boxes = total_boxes[pick,:]
-        regw = total_boxes[:,2]-total_boxes[:,0]
-        regh = total_boxes[:,3]-total_boxes[:,1]
-        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw
-        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh
-        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw
-        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh
-        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))
-        total_boxes = rerec(total_boxes.copy())
-        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)
-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)
-
-    numbox = total_boxes.shape[0]
-
-    # # # # # # # # # # # # #
-    # second stage - refinement of face candidates with rnet
-    # # # # # # # # # # # # #
-
-    if numbox>0:
-        tempimg = np.zeros((24,24,3,numbox))
-        for k in range(0,numbox):
-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))
-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]
-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:
-                tempimg[:,:,:,k] = imresample(tmp, (24, 24))
-            else:
-                return np.empty()
-        tempimg = (tempimg-127.5)*0.0078125
-        tempimg1 = np.transpose(tempimg, (3,1,0,2))
-        out = rnet(tempimg1)
-        out0 = np.transpose(out[0])
-        out1 = np.transpose(out[1])
-        score = out1[1,:]
-        ipass = np.where(score>threshold[1])
-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])
-        mv = out0[:,ipass[0]]
-        if total_boxes.shape[0]>0:
-            pick = nms(total_boxes, 0.7, 'Union')
-            total_boxes = total_boxes[pick,:]
-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))
-            total_boxes = rerec(total_boxes.copy())
-
-    numbox = total_boxes.shape[0]
-
-    # # # # # # # # # # # # #
-    # third stage - further refinement and facial landmarks positions with onet
-    # NB: Facial landmarks code commented out for faceswap
-    # # # # # # # # # # # # #
-
-    if numbox>0:
-        # third stage
-        total_boxes = np.fix(total_boxes).astype(np.int32)
-        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)
-        tempimg = np.zeros((48,48,3,numbox))
-        for k in range(0,numbox):
-            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))
-            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]
-            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:
-                tempimg[:,:,:,k] = imresample(tmp, (48, 48))
-            else:
-                return np.empty()
-        tempimg = (tempimg-127.5)*0.0078125
-        tempimg1 = np.transpose(tempimg, (3,1,0,2))
-        out = onet(tempimg1)
-        out0 = np.transpose(out[0])
-        out1 = np.transpose(out[1])
-        out2 = np.transpose(out[2])
-        score = out2[1,:]
-        points = out1
-        ipass = np.where(score>threshold[2])
-        points = points[:,ipass[0]]
-        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])
-        mv = out0[:,ipass[0]]
-
-        w = total_boxes[:,2]-total_boxes[:,0]+1
-        h = total_boxes[:,3]-total_boxes[:,1]+1
-        points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1
-        points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1
-        if total_boxes.shape[0]>0:
-            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))
-            pick = nms(total_boxes.copy(), 0.7, 'Min')
-            total_boxes = total_boxes[pick,:]
-            points = points[:,pick]
-
-    return total_boxes, points
-    
-# function [boundingbox] = bbreg(boundingbox,reg)
-def bbreg(boundingbox,reg):
-    """Calibrate bounding boxes"""
-    if reg.shape[1]==1:
-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))
-
-    w = boundingbox[:,2]-boundingbox[:,0]+1
-    h = boundingbox[:,3]-boundingbox[:,1]+1
-    b1 = boundingbox[:,0]+reg[:,0]*w
-    b2 = boundingbox[:,1]+reg[:,1]*h
-    b3 = boundingbox[:,2]+reg[:,2]*w
-    b4 = boundingbox[:,3]+reg[:,3]*h
-    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))
-    return boundingbox
-
-def generateBoundingBox(imap, reg, scale, t):
-    """Use heatmap to generate bounding boxes"""
-    stride=2
-    cellsize=12
-
-    imap = np.transpose(imap)
-    dx1 = np.transpose(reg[:,:,0])
-    dy1 = np.transpose(reg[:,:,1])
-    dx2 = np.transpose(reg[:,:,2])
-    dy2 = np.transpose(reg[:,:,3])
-    y, x = np.where(imap >= t)
-    if y.shape[0]==1:
-        dx1 = np.flipud(dx1)
-        dy1 = np.flipud(dy1)
-        dx2 = np.flipud(dx2)
-        dy2 = np.flipud(dy2)
-    score = imap[(y,x)]
-    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))
-    if reg.size==0:
-        reg = np.empty((0,3))
-    bb = np.transpose(np.vstack([y,x]))
-    q1 = np.fix((stride*bb+1)/scale)
-    q2 = np.fix((stride*bb+cellsize-1+1)/scale)
-    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])
-    return boundingbox, reg
-
-# function pick = nms(boxes,threshold,type)
-def nms(boxes, threshold, method):
-    if boxes.size==0:
-        return np.empty((0,3))
-    x1 = boxes[:,0]
-    y1 = boxes[:,1]
-    x2 = boxes[:,2]
-    y2 = boxes[:,3]
-    s = boxes[:,4]
-    area = (x2-x1+1) * (y2-y1+1)
-    I = np.argsort(s)
-    pick = np.zeros_like(s, dtype=np.int16)
-    counter = 0
-    while I.size>0:
-        i = I[-1]
-        pick[counter] = i
-        counter += 1
-        idx = I[0:-1]
-        xx1 = np.maximum(x1[i], x1[idx])
-        yy1 = np.maximum(y1[i], y1[idx])
-        xx2 = np.minimum(x2[i], x2[idx])
-        yy2 = np.minimum(y2[i], y2[idx])
-        w = np.maximum(0.0, xx2-xx1+1)
-        h = np.maximum(0.0, yy2-yy1+1)
-        inter = w * h
-        if method is 'Min':
-            o = inter / np.minimum(area[i], area[idx])
-        else:
-            o = inter / (area[i] + area[idx] - inter)
-        I = I[np.where(o<=threshold)]
-    pick = pick[0:counter]
-    return pick
-
-# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)
-def pad(total_boxes, w, h):
-    """Compute the padding coordinates (pad the bounding boxes to square)"""
-    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)
-    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)
-    numbox = total_boxes.shape[0]
-
-    dx = np.ones((numbox), dtype=np.int32)
-    dy = np.ones((numbox), dtype=np.int32)
-    edx = tmpw.copy().astype(np.int32)
-    edy = tmph.copy().astype(np.int32)
-
-    x = total_boxes[:,0].copy().astype(np.int32)
-    y = total_boxes[:,1].copy().astype(np.int32)
-    ex = total_boxes[:,2].copy().astype(np.int32)
-    ey = total_boxes[:,3].copy().astype(np.int32)
-
-    tmp = np.where(ex>w)
-    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)
-    ex[tmp] = w
-
-    tmp = np.where(ey>h)
-    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)
-    ey[tmp] = h
-
-    tmp = np.where(x<1)
-    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)
-    x[tmp] = 1
-
-    tmp = np.where(y<1)
-    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)
-    y[tmp] = 1
-
-    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph
-
-# function [bboxA] = rerec(bboxA)
-def rerec(bboxA):
-    """Convert bboxA to square."""
-    h = bboxA[:,3]-bboxA[:,1]
-    w = bboxA[:,2]-bboxA[:,0]
-    l = np.maximum(w, h)
-    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5
-    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5
-    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))
-    return bboxA
-
-def imresample(img, sz):
-    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable
-    return im_data
-
diff --git a/lib/face_alignment/vram_allocation.py b/lib/face_alignment/vram_allocation.py
deleted file mode 100644
index a2aefd6..0000000
--- a/lib/face_alignment/vram_allocation.py
+++ /dev/null
@@ -1,138 +0,0 @@
-#!/usr/bin python3
-""" GPU VRAM allocator calculations """
-
-from lib.gpu_stats import GPUStats
-
-
-class GPUMem():
-    """ Sets the scale to factor for dlib images
-        and the ratio of vram to use for tensorflow """
-
-    def __init__(self):
-        self.initialized = False
-        self.verbose = False
-        self.stats = GPUStats()
-        self.dlib_buffer = 64
-        self.vram_free = None
-        self.vram_total = None
-        self.scale_to = None
-
-        self.device = self.set_device()
-
-        if self.device == -1:
-            # Limit ram usage to 2048 for CPU
-            self.vram_total = 2048
-        else:
-            self.vram_total = self.stats.vram[self.device]
-
-        self.get_available_vram()
-
-    def set_device(self):
-        """ Set the default device """
-        if self.stats.device_count == 0:
-            return -1
-        return 0
-        # TF selects first device, so this is used for stats
-        # TODO select and use device with most available VRAM
-        # TODO create virtual devices/allow multiple GPUs for
-        # parallel processing
-
-    def set_device_with_max_free_vram(self):
-        """ Set the device with the most available free vram """
-        # TODO Implement this to select the device with most available VRAM
-        free_mem = self.stats.get_free()
-        self.vram_free = max(free_mem)
-        self.device = free_mem.index(self.vram_free)
-
-    def get_available_vram(self):
-        """ Recalculate the available vram """
-        if self.device == -1:
-            # Limit RAM to 2GB for non-gpu
-            self.vram_free = 2048
-        else:
-            free_mem = self.stats.get_free()
-            self.vram_free = free_mem[self.device]
-
-        if self.verbose:
-            if self.device == -1:
-                print("No GPU. Limiting RAM usage to "
-                      "{}MB".format(self.vram_free))
-            print("GPU VRAM free:    {}".format(self.vram_free))
-
-    def output_stats(self):
-        """ Output stats in verbose mode """
-        if not self.verbose:
-            return
-        print("\n----- Initial GPU Stats -----")
-        if self.device == -1:
-            print("No GPU. Limiting RAM usage to {}MB".format(self.vram_free))
-        self.stats.print_info()
-        print("GPU VRAM free:    {}".format(self.vram_free))
-        print("-----------------------------\n")
-
-    def get_tensor_gpu_ratio(self):
-        """ Set the ratio of GPU memory to use for tensorflow session for
-            keras points predictor.
-
-            Ideally 2304MB is required, but will run with less
-            (with warnings).
-
-            This is only required if running with DLIB. MTCNN will share
-            the tensorflow session. """
-        if self.vram_free < 2030:
-            ratio = 1024.0 / self.vram_total
-        elif self.vram_free < 3045:
-            ratio = 1560.0 / self.vram_total
-        elif self.vram_free < 4060:
-            ratio = 2048.0 / self.vram_total
-        else:
-            ratio = 2304.0 / self.vram_total
-
-        return ratio
-
-    def set_scale_to(self, detector):
-        """ Set the size to scale images down to for specific detector
-            and available VRAM
-
-            DLIB VRAM allocation is linear to pixel count
-
-            MTCNN is weird. Not linear at low levels,
-            then fairly linear up to 3360x1890 then
-            requirements drop again.
-            As 3360x1890 is hi-res, just this scale is
-            used for calculating image scaling """
-
-        # MTCNN VRAM Usage Stats
-        # Crudely Calculated at default values
-        # The formula may need ammending, but it should
-        # work for most use cases
-        # 480x270 = 267.56 MB
-        # 960x540 = 333.18 MB
-        # 1280x720 = 592.32 MB
-        # 1440x810 = 746.56 MB
-        # 1920x1080 = 1.30 GB
-        # 2400x1350 = 2.03 GB
-        # 2880x1620 = 2.93 GB
-        # 3360x1890 = 3.98 GB
-        # 3840x2160 = 2.62 GB <--??
-        # 4280x2800 = 3.69 GB
-
-        detector = "dlib" if detector in ("dlib-cnn",
-                                          "dlib-hog",
-                                          "dlib-all") else detector
-        gradient = 3483.2 / 9651200  # MTCNN
-        constant = 1.007533156  # MTCNN
-        if detector == "dlib":
-            self.get_available_vram()
-            gradient = 213 / 524288
-            constant = 307
-
-        free_mem = self.vram_free - self.dlib_buffer  # overhead buffer
-        if self.verbose:
-            print("Allocating for Detector: {}".format(free_mem))
-
-        self.scale_to = int((free_mem - constant) / gradient)
-
-        if self.scale_to < 4097:
-            raise ValueError("Images would be shrunk too much "
-                             "for successful extraction")
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 47dda81..e521d7b 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -1,37 +1,14 @@
 #!/usr/bin python3
 """ Face and landmarks detection for faceswap.py """
 
-from lib import face_alignment
-
-
-def detect_faces(frame, detector, verbose, rotation=0,
-                 dlib_buffer=64, mtcnn_kwargs=None):
-    """ Detect faces and draw landmarks in an image """
-    face_detect = face_alignment.Extract(frame,
-                                         detector,
-                                         dlib_buffer,
-                                         mtcnn_kwargs,
-                                         verbose)
-    for face in face_detect.landmarks:
-        ax_x, ax_y = face[0][0], face[0][1]
-        right, bottom = face[0][2], face[0][3]
-        landmarks = face[1]
-
-        yield DetectedFace(frame[ax_y: bottom, ax_x: right],
-                           rotation,
-                           ax_x,
-                           right - ax_x,
-                           ax_y,
-                           bottom - ax_y,
-                           landmarksXY=landmarks)
+from dlib import rectangle as d_rectangle
 
 
 class DetectedFace():
     """ Detected face and landmark information """
-    def __init__(self, image=None, r=0, x=None,
-                 w=None, y=None, h=None, landmarksXY=None):
+    def __init__(self, image=None, x=None, w=None, y=None, h=None,
+                 landmarksXY=None):
         self.image = image
-        self.r = r
         self.x = x
         self.w = w
         self.y = y
@@ -41,3 +18,45 @@ class DetectedFace():
     def landmarks_as_xy(self):
         """ Landmarks as XY """
         return self.landmarksXY
+
+    def to_dlib_rect(self):
+        """ Return Bounding Box as Dlib Rectangle """
+        left = self.x
+        top = self.y
+        right = self.x + self.w
+        bottom = self.y + self.h
+        return d_rectangle(left, top, right, bottom)
+
+    def from_dlib_rect(self, d_rect):
+        """ Set Bounding Box from a Dlib Rectangle """
+        if not isinstance(d_rect, d_rectangle):
+            raise ValueError("Supplied Bounding Box is not a dlib.rectangle.")
+        self.x = d_rect.left()
+        self.w = d_rect.right() - d_rect.left()
+        self.y = d_rect.top()
+        self.h = d_rect.bottom() - d_rect.top()
+
+    def image_to_face(self, image):
+        """ Crop an image around bounding box to the face """
+        self.image = image[self.y: self.y + self.h,
+                           self.x: self.x + self.w]
+
+    def to_alignment(self):
+        """ Convert a detected face to alignment dict """
+        alignment = dict()
+        alignment["x"] = self.x
+        alignment["w"] = self.w
+        alignment["y"] = self.y
+        alignment["h"] = self.h
+        alignment["landmarksXY"] = self.landmarksXY
+        return alignment
+
+    def from_alignment(self, alignment, image=None):
+        """ Convert a face alignment to detected face object """
+        self.x = alignment["x"]
+        self.w = alignment["w"]
+        self.y = alignment["y"]
+        self.h = alignment["h"]
+        self.landmarksXY = alignment["landmarksXY"]
+        if image.any():
+            self.image_to_face(image)
diff --git a/lib/gpu_stats.py b/lib/gpu_stats.py
index ec790ea..443f3f9 100644
--- a/lib/gpu_stats.py
+++ b/lib/gpu_stats.py
@@ -141,6 +141,18 @@ class GPUStats(object):
         self.shutdown()
         return vram
 
+    def get_card_most_free(self):
+        """ Return the card and available VRAM for card with
+            most VRAM free """
+        free_vram = self.get_free()
+        vram_free = max(free_vram)
+        card_id = free_vram.index(vram_free)
+        return {"card_id": card_id,
+                "device": self.devices[card_id],
+                "free": vram_free,
+                "total": self.vram[card_id]}
+
+    
     def print_info(self):
         """ Output GPU info in verbose mode """
         print("GPU Driver:       {}".format(self.driver))
diff --git a/lib/gui/command.py b/lib/gui/command.py
index 270d7b3..362519e 100644
--- a/lib/gui/command.py
+++ b/lib/gui/command.py
@@ -257,7 +257,7 @@ class OptionControl(object):
         if control == ttk.Combobox:
             ctl["values"] = [choice for choice in choices]
 
-        Tooltip(ctl, text=helptext, wraplength=400)
+        Tooltip(ctl, text=helptext, wraplength=720)
 
     def add_browser_buttons(self, frame, sysbrowser, filepath):
         """ Add correct file browser button for control """
diff --git a/lib/multithreading.py b/lib/multithreading.py
index ed9d57f..7d22022 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -1,16 +1,157 @@
+#!/usr/bin/env python3
+""" Multithreading/processing utils for faceswap """
+
 import multiprocessing as mp
+import queue as Queue
+from queue import Empty as QueueEmpty  # Used for imports
+import threading
+from time import sleep
+
+
+class QueueManager():
+    """ Manage queues for availabilty across processes
+        Don't import this class directly, instead
+        import the variable: queue_manager """
+    def __init__(self):
+        self.manager = mp.Manager()
+        self.queues = dict()
+
+    def add_queue(self, name, maxsize=0):
+        """ Add a queue to the manager """
+        if name in self.queues.keys():
+            raise ValueError("Queue '{}' already exists.".format(name))
+        queue = self.manager.Queue(maxsize=maxsize)
+        self.queues[name] = queue
+
+    def del_queue(self, name):
+        """ remove a queue from the manager """
+        del self.queues[name]
+
+    def get_queue(self, name, maxsize=0):
+        """ Return a queue from the manager
+            If it doesn't exist, create it """
+        queue = self.queues.get(name, None)
+        if queue:
+            return queue
+        self.add_queue(name, maxsize)
+        return self.queues[name]
+
+    def terminate_queues(self):
+        """ Clear all queues and send EOF
+            To be called if there is an error """
+        for queue in self.queues.values():
+            while not queue.empty():
+                queue.get()
+            queue.put("EOF")
+
+    def debug_monitor(self, update_secs=2):
+        """ Debug tool for monitoring queues """
+        thread = MultiThread(thread_count=update_secs)
+        thread.in_thread(self.debug_queue_sizes)
+
+    def debug_queue_sizes(self):
+        """ Output the queue sizes """
+        while True:
+            print("=== QUEUE SIZES ===")
+            for name in sorted(self.queues.keys()):
+                print(name, self.queues[name].qsize())
+            print("====================\n")
+            sleep(2)
+
+
+queue_manager = QueueManager()
+
+
+class PoolProcess():
+    """ Pool multiple processes """
+    def __init__(self, method, processes=None, verbose=False):
+        self.verbose = verbose
+        self.method = method
+        self.procs = self.set_procs(processes)
+
+    def set_procs(self, processes):
+        """ Set the number of processes to use """
+        if processes is None:
+            running_processes = len(mp.active_children())
+            processes = max(mp.cpu_count() - running_processes, 1)
+        if self.verbose:
+            print("Processing in {} processes".format(processes))
+        return processes
+
+    def in_process(self, *args, **kwargs):
+        """ Run the processing pool """
+        pool = mp.Pool(processes=self.procs)
+        for _ in range(self.procs):
+            pool.apply_async(self.method, args=args, kwds=kwargs)
+
+
+class SpawnProcess():
+    """ Process in spawnable context
+        Must be spawnable to share CUDA across processes """
+    def __init__(self):
+        self.context = mp.get_context("spawn")
+        self.daemonize = True
+        self.process = None
+        self.event = self.context.Event()
+
+    def in_process(self, target, *args, **kwargs):
+        """ Start a process in the spawn context """
+        kwargs["event"] = self.event
+        self.process = self.context.Process(target=target,
+                                            args=args,
+                                            kwargs=kwargs)
+        self.process.daemon = self.daemonize
+        self.process.start()
+
+    def join(self):
+        """ Join the process """
+        self.process.join()
+
+
+class MultiThread():
+    """ Threading for IO heavy ops """
+    def __init__(self, thread_count=1):
+        self.thread_count = thread_count
+        self.threads = list()
+
+    def in_thread(self, target, *args, **kwargs):
+        """ Start a thread with the given method and args """
+        for _ in range(self.thread_count):
+            thread = threading.Thread(target=target, args=args, kwargs=kwargs)
+            thread.daemon = True
+            thread.start()
+            self.threads.append(thread)
+
+    def join_threads(self):
+        """ Join the running threads """
+        for thread in self.threads:
+            thread.join()
+
 
-method = None
+class BackgroundGenerator(threading.Thread):
+    """ Run a queue in the background. From:
+        https://stackoverflow.com/questions/7323664/ """
+    # See below why prefetch count is flawed
+    def __init__(self, generator, prefetch=1):
+        threading.Thread.__init__(self)
+        self.queue = Queue.Queue(maxsize=prefetch)
+        self.generator = generator
+        self.daemon = True
+        self.start()
 
-def pool_process(method_to_run, data, processes=None):
-    global method
-    if processes is None:
-        processes = mp.cpu_count()
-    method = method_to_run
-    pool = mp.Pool(processes=processes)
+    def run(self):
+        """ Put until queue size is reached.
+            Note: put blocks only if put is called while queue has already
+            reached max size => this makes 2 prefetched items! One in the
+            queue, one waiting for insertion! """
+        for item in self.generator:
+            self.queue.put(item)
+        self.queue.put(None)
 
-    for i in pool.imap_unordered(runner, data):
-        yield i if i is not None else 0
-    
-def runner(item):
-    return method(item)
+    def iterator(self):
+        """ Iterate items out of the queue """
+        while True:
+            next_item = self.queue.get()
+            if next_item is None:
+                break
+            yield next_item
diff --git a/lib/training_data.py b/lib/training_data.py
index 59a6f25..fad52ee 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -1,9 +1,9 @@
+from random import shuffle
 import cv2
 import numpy
-from random import shuffle
 
-import lib.utils
-from lib.umeyama import umeyama
+from .multithreading import BackgroundGenerator
+from .umeyama import umeyama
 
 class TrainingDataGenerator():
     def __init__(self, random_transform_args, coverage, scale=5, zoom=1): #TODO thos default should stay in the warp function
@@ -13,7 +13,7 @@ class TrainingDataGenerator():
         self.zoom = zoom
 
     def minibatchAB(self, images, batchsize, doShuffle=True):
-        batch = lib.utils.BackgroundGenerator(self.minibatch(images, batchsize, doShuffle), 1)
+        batch = BackgroundGenerator(self.minibatch(images, batchsize, doShuffle), 1)
         for ep1, warped_img, target_img in batch.iterator():
             yield ep1, warped_img, target_img
 
@@ -33,21 +33,21 @@ class TrainingDataGenerator():
                 epoch+=1
             rtn = numpy.float32([self.read_image(img) for img in data[i:i+size]])
             i+=size
-            yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:]       
+            yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:]
 
     def color_adjust(self, img):
         return img / 255.0
-    
+
     def read_image(self, fn):
         try:
             image = self.color_adjust(cv2.imread(fn))
         except TypeError:
             raise Exception("Error while reading image", fn)
-        
+
         image = cv2.resize(image, (256,256))
         image = self.random_transform( image, **self.random_transform_args )
         warped_img, target_img = self.random_warp( image, self.coverage, self.scale, self.zoom )
-        
+
         return warped_img, target_img
 
     def random_transform(self, image, rotation_range, zoom_range, shift_range, random_flip):
@@ -96,7 +96,7 @@ def stack_images(images):
             y_axes = list(range(0, n - 1, 2))
             x_axes = list(range(1, n - 1, 2))
         return y_axes, x_axes, [n - 1]
-    
+
     images_shape = numpy.array(images.shape)
     new_axes = get_transpose_axes(len(images_shape))
     new_shape = [numpy.prod(images_shape[x]) for x in new_axes]
diff --git a/lib/utils.py b/lib/utils.py
index c04be48..29bb2dc 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -2,19 +2,18 @@
 """ Utilities available across all scripts """
 
 import os
-from os.path import basename, exists, join
-import queue as Queue
-import threading
 import warnings
 
 from pathlib import Path
+from re import finditer
+from time import time
 
 import cv2
 import numpy as np
 
-import lib.training_data
-from time import time
-
+import dlib
+from lib.faces_detect import DetectedFace
+from lib.training_data import TrainingDataGenerator
 
 # Global variables
 _image_extensions = ['.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff']
@@ -31,11 +30,11 @@ def get_folder(path):
 def get_image_paths(directory, exclude=list(), debug=False):
     """ Return a list of images that reside in a folder """
     image_extensions = _image_extensions
-    exclude_names = [basename(Path(x).stem[:Path(x).stem.rfind('_')] +
-                              Path(x).suffix) for x in exclude]
+    exclude_names = [os.path.basename(Path(x).stem[:Path(x).stem.rfind('_')] +
+                                      Path(x).suffix) for x in exclude]
     dir_contents = list()
 
-    if not exists(directory):
+    if not os.path.exists(directory):
         directory = get_folder(directory)
 
     dir_scanned = sorted(os.scandir(directory), key=lambda x: x.name)
@@ -54,11 +53,11 @@ def get_image_paths(directory, exclude=list(), debug=False):
 
 def backup_file(directory, filename):
     """ Backup a given file by appending .bk to the end """
-    origfile = join(directory, filename)
+    origfile = os.path.join(directory, filename)
     backupfile = origfile + '.bk'
-    if exists(backupfile):
+    if os.path.exists(backupfile):
         os.remove(backupfile)
-    if exists(origfile):
+    if os.path.exists(origfile):
         os.rename(origfile, backupfile)
 
 
@@ -72,7 +71,6 @@ def set_system_verbosity(loglevel):
         1 - filter out INFO logs
         2 - filter out WARNING logs
         3 - filter out ERROR logs  """
-    # TODO suppress tensorflow deprecation warnings """
 
     os.environ['TF_CPP_MIN_LOG_LEVEL'] = loglevel
     if loglevel != '0':
@@ -106,15 +104,38 @@ def rotate_image_by_angle(image, angle,
 def rotate_landmarks(face, rotation_matrix):
     """ Rotate the landmarks and bounding box for faces
         found in rotated images.
-        Pass in a DetectedFace object"""
+        Pass in a DetectedFace object, Alignments dict or DLib rectangle"""
+    if isinstance(face, DetectedFace):
+        bounding_box = [[face.x, face.y],
+                        [face.x + face.w, face.y],
+                        [face.x + face.w, face.y + face.h],
+                        [face.x, face.y + face.h]]
+        landmarks = face.landmarksXY
+
+    elif isinstance(face, dict):
+        bounding_box = [[face.get("x", 0), face.get("y", 0)],
+                        [face.get("x", 0) + face.get("w", 0),
+                         face.get("y", 0)],
+                        [face.get("x", 0) + face.get("w", 0),
+                         face.get("y", 0) + face.get("h", 0)],
+                        [face.get("x", 0),
+                         face.get("y", 0) + face.get("h", 0)]]
+        landmarks = face.get("landmarksXY", list())
+
+    elif isinstance(face, dlib.rectangle):
+        bounding_box = [[face.left(), face.top()],
+                        [face.right(), face.top()],
+                        [face.right(), face.bottom()],
+                        [face.left(), face.bottom()]]
+        landmarks = list()
+    else:
+        raise ValueError("Unsupported face type")
+
     rotation_matrix = cv2.invertAffineTransform(rotation_matrix)
-    bounding_box = [[face.x, face.y],
-                    [face.x + face.w, face.y],
-                    [face.x + face.w, face.y + face.h],
-                    [face.x, face.y + face.h]]
-    landmarks = face.landmarksXY
     rotated = list()
     for item in (bounding_box, landmarks):
+        if not item:
+            continue
         points = np.array(item, np.int32)
         points = np.expand_dims(points, axis=0)
         transformed = cv2.transform(points,
@@ -123,67 +144,63 @@ def rotate_landmarks(face, rotation_matrix):
 
     # Bounding box should follow x, y planes, so get min/max
     # for non-90 degree rotations
-    pnt_x = min([pnt[0] for pnt in rotated[0]])
-    pnt_y = min([pnt[1] for pnt in rotated[0]])
-    pnt_x1 = max([pnt[0] for pnt in rotated[0]])
-    pnt_y1 = max([pnt[1] for pnt in rotated[0]])
-    face.x = int(pnt_x)
-    face.y = int(pnt_y)
-    face.w = int(pnt_x1 - pnt_x)
-    face.h = int(pnt_y1 - pnt_y)
-    face.r = 0
-    face.landmarksXY = [tuple(point) for point in rotated[1].tolist()]
+    pt_x = min([pnt[0] for pnt in rotated[0]])
+    pt_y = min([pnt[1] for pnt in rotated[0]])
+    pt_x1 = max([pnt[0] for pnt in rotated[0]])
+    pt_y1 = max([pnt[1] for pnt in rotated[0]])
+
+    if isinstance(face, DetectedFace):
+        face.x = int(pt_x)
+        face.y = int(pt_y)
+        face.w = int(pt_x1 - pt_x)
+        face.h = int(pt_y1 - pt_y)
+        face.r = 0
+        if len(rotated) > 1:
+            face.landmarksXY = [tuple(point) for point in rotated[1].tolist()]
+    elif isinstance(face, dict):
+        face["x"] = int(pt_x)
+        face["y"] = int(pt_y)
+        face["w"] = int(pt_x1 - pt_x)
+        face["h"] = int(pt_y1 - pt_y)
+        face["r"] = 0
+        if len(rotated) > 1:
+            face["landmarksXY"] = [tuple(point)
+                                   for point in rotated[1].tolist()]
+    else:
+        face = dlib.rectangle(int(pt_x), int(pt_y), int(pt_x1), int(pt_y1))
+
     return face
 
 
-class BackgroundGenerator(threading.Thread):
-    """ Run a queue in the background. From:
-        https://stackoverflow.com/questions/7323664/ """
-    # See below why prefetch count is flawed
-    def __init__(self, generator, prefetch=1):
-        threading.Thread.__init__(self)
-        self.queue = Queue.Queue(prefetch)
-        self.generator = generator
-        self.daemon = True
-        self.start()
-
-    def run(self):
-        """ Put until queue size is reached.
-            Note: put blocks only if put is called while queue has already
-            reached max size => this makes 2 prefetched items! One in the
-            queue, one waiting for insertion! """
-        for item in self.generator:
-            self.queue.put(item)
-        self.queue.put(None)
-
-    def iterator(self):
-        """ Iterate items out of the queue """
-        while True:
-            next_item = self.queue.get()
-            if next_item is None:
-                break
-            yield next_item
+def camel_case_split(identifier):
+    """ Split a camel case name
+        from: https://stackoverflow.com/questions/29916065 """
+    matches = finditer(
+        ".+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)",
+        identifier)
+    return [m.group(0) for m in matches]
+
 
 class Timelapse:
+    """ Time lapse function for training """
     @classmethod
-    def CreateTimelapse(test, input_dir_A, input_dir_B, output_dir, trainer):
-        #self.input_dir = input
-        #self.output_dir = output
-        #self.trainer = trainer
-
-        if input_dir_A is None and input_dir_B is None and output_dir is None:
+    def create_timelapse(cls, input_dir_a, input_dir_b, output_dir, trainer):
+        """ Create the time lapse """
+        if input_dir_a is None and input_dir_b is None and output_dir is None:
             return None
 
-        if input_dir_A is None or input_dir_B is None:
-            raise Exception("To enable the timelapse, you have to supply all the parameters "
-                            "(--timelapse-input-A and --timelapse-input-B).")
+        if input_dir_a is None or input_dir_b is None:
+            raise ValueError("To enable the timelapse, you have to supply "
+                             "all the parameters (--timelapse-input-A and "
+                             "--timelapse-input-B).")
 
         if output_dir is None:
-            output_dir = get_folder(os.path.join(trainer.model.model_dir, "timelapse"))
+            output_dir = get_folder(os.path.join(trainer.model.model_dir,
+                                                 "timelapse"))
 
-        return Timelapse(input_dir_A, input_dir_B, output_dir, trainer)
+        return Timelapse(input_dir_a, input_dir_b, output_dir, trainer)
 
-    def __init__(self, input_dir_A, input_dir_B, output, trainer):
+    def __init__(self, input_dir_a, input_dir_b, output, trainer):
         self.output_dir = output
         self.trainer = trainer
 
@@ -191,15 +208,17 @@ class Timelapse:
             print('Error: {} does not exist'.format(self.output_dir))
             exit(1)
 
-        self.files_A = self.read_input_images(input_dir_A)
-        self.files_B = self.read_input_images(input_dir_B)
+        self.files_a = self.read_input_images(input_dir_a)
+        self.files_b = self.read_input_images(input_dir_b)
 
-        bs = min(len(self.files_A), len(self.files_B)) 
+        btchsz = min(len(self.files_a), len(self.files_b))
 
-        self.images_A = self.get_image_data(self.files_A, bs)
-        self.images_B = self.get_image_data(self.files_B, bs)
+        self.images_a = self.get_image_data(self.files_a, btchsz)
+        self.images_b = self.get_image_data(self.files_b, btchsz)
 
-    def read_input_images(self, input_dir):
+    @staticmethod
+    def read_input_images(input_dir):
+        """ Get the image paths """
         if not os.path.isdir(input_dir):
             print('Error: {} does not exist'.format(input_dir))
             exit(1)
@@ -211,6 +230,7 @@ class Timelapse:
         return get_image_paths(input_dir)
 
     def get_image_data(self, input_images, batch_size):
+        """ Get training images """
         random_transform_args = {
             'rotation_range': 0,
             'zoom_range': 0,
@@ -218,13 +238,18 @@ class Timelapse:
             'random_flip': 0
         }
 
-        zoom = self.trainer.model.IMAGE_SHAPE[0] // 64 if hasattr(self.trainer.model, 'IMAGE_SHAPE') else 1
+        zoom = 1
+        if hasattr(self.trainer.model, 'IMAGE_SHAPE'):
+            zoom = self.trainer.model.IMAGE_SHAPE[0] // 64
 
-        generator = lib.training_data.TrainingDataGenerator(random_transform_args, 160, zoom)
-        batch = generator.minibatchAB(input_images, batch_size, doShuffle=False)
+        generator = TrainingDataGenerator(random_transform_args, 160, zoom)
+        batch = generator.minibatchAB(input_images, batch_size,
+                                      doShuffle=False)
 
         return next(batch)[2]
 
     def work(self):
-        image = self.trainer.show_sample(self.images_A, self.images_B)
-        cv2.imwrite(os.path.join(self.output_dir, str(int(time())) + ".png"), image)
\ No newline at end of file
+        """ Write out timelapse image """
+        image = self.trainer.show_sample(self.images_a, self.images_b)
+        cv2.imwrite(os.path.join(self.output_dir,
+                                 str(int(time())) + ".png"), image)
diff --git a/plugins/Extract_Align.py b/plugins/Extract_Align.py
deleted file mode 100644
index f009ade..0000000
--- a/plugins/Extract_Align.py
+++ /dev/null
@@ -1,68 +0,0 @@
-# Based on the original https://www.reddit.com/r/deepfakes/ code sample + contribs
-
-import cv2
-import numpy as np
-
-from lib.aligner import get_align_mat
-from lib.align_eyes import FACIAL_LANDMARKS_IDXS
-
-class Extract(object):
-    def extract(self, image, face, size, align_eyes):
-        alignment = get_align_mat(face, size, align_eyes)
-        extracted = self.transform(image, alignment, size, 48)
-        return extracted, alignment
-
-    def transform(self, image, mat, size, padding=0):
-        matrix = mat * (size - 2 * padding)
-        matrix[:,2] += padding
-        return cv2.warpAffine(image, matrix, (size, size))
-
-    def transform_points(self, points, mat, size, padding=0):
-        matrix = mat * (size - 2 * padding)
-        matrix[:,2] += padding
-        points = np.expand_dims(points, axis=1)
-        points = cv2.transform(points, matrix, points.shape)
-        points = np.squeeze(points)
-        return points
-
-    def get_feature_mask(self, aligned_landmarks_68, size, padding=0, dilation=30):
-        scale = size - 2*padding
-        translation = padding
-        pad_mat = np.matrix([[scale, 0.0, translation], [0.0, scale, translation]])
-        aligned_landmarks_68 = np.expand_dims(aligned_landmarks_68, axis=1)
-        aligned_landmarks_68 = cv2.transform(aligned_landmarks_68, pad_mat, aligned_landmarks_68.shape)
-        aligned_landmarks_68 = np.squeeze(aligned_landmarks_68)
-
-        (lStart, lEnd) = FACIAL_LANDMARKS_IDXS["left_eye"]
-        (rStart, rEnd) = FACIAL_LANDMARKS_IDXS["right_eye"]
-        (mStart, mEnd) = FACIAL_LANDMARKS_IDXS["mouth"]
-        (nStart, nEnd) = FACIAL_LANDMARKS_IDXS["nose"]
-        (lbStart, lbEnd) = FACIAL_LANDMARKS_IDXS["left_eyebrow"]
-        (rbStart, rbEnd) = FACIAL_LANDMARKS_IDXS["right_eyebrow"]
-        (cStart, cEnd) = FACIAL_LANDMARKS_IDXS["chin"]
-
-        l_eye_points = aligned_landmarks_68[lStart:lEnd].tolist()
-        l_brow_points = aligned_landmarks_68[lbStart:lbEnd].tolist()
-        r_eye_points = aligned_landmarks_68[rStart:rEnd].tolist()
-        r_brow_points = aligned_landmarks_68[rbStart:rbEnd].tolist()
-        nose_points = aligned_landmarks_68[nStart:nEnd].tolist()
-        chin_points = aligned_landmarks_68[cStart:cEnd].tolist()
-        mouth_points = aligned_landmarks_68[mStart:mEnd].tolist()
-        l_eye_points = l_eye_points + l_brow_points
-        r_eye_points = r_eye_points + r_brow_points
-        mouth_points = mouth_points + nose_points + chin_points
-
-        l_eye_hull = cv2.convexHull(np.array(l_eye_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
-        r_eye_hull = cv2.convexHull(np.array(r_eye_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
-        mouth_hull = cv2.convexHull(np.array(mouth_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
-
-        mask = np.zeros((size, size, 3), dtype=float)
-        cv2.fillConvexPoly(mask, l_eye_hull, (1,1,1))
-        cv2.fillConvexPoly(mask, r_eye_hull, (1,1,1))
-        cv2.fillConvexPoly(mask, mouth_hull, (1,1,1))
-
-        if dilation > 0:
-            kernel = np.ones((dilation, dilation), np.uint8)
-            mask = cv2.dilate(mask, kernel, iterations=1)
-
-        return mask
diff --git a/plugins/PluginLoader.py b/plugins/PluginLoader.py
deleted file mode 100644
index de527e0..0000000
--- a/plugins/PluginLoader.py
+++ /dev/null
@@ -1,37 +0,0 @@
-import os
-
-class PluginLoader():
-    @staticmethod
-    def get_extractor(name):
-        return PluginLoader._import("Extract", "Extract_{0}".format(name))
-    
-    @staticmethod
-    def get_converter(name):
-        return PluginLoader._import("Convert", "Convert_{0}".format(name))
-    
-    @staticmethod
-    def get_model(name):
-        return PluginLoader._import("Model", "Model_{0}".format(name))
-    
-    @staticmethod
-    def get_trainer(name):
-        return PluginLoader._import("Trainer", "Model_{0}".format(name))
-    
-    @staticmethod
-    def _import(attr, name):
-        print("Loading {} from {} plugin...".format(attr, name))
-        module = __import__(name, globals(), locals(), [], 1)
-        return getattr(module, attr)
-
-    @staticmethod
-    def get_available_models():
-        models = ()
-        for dir in next(os.walk( os.path.dirname(__file__) ))[1]:
-            if dir[0:6].lower() == 'model_':
-                models += (dir[6:],)
-        return models
-        
-    @staticmethod
-    def get_default_model():
-        models = PluginLoader.get_available_models()
-        return 'Original' if 'Original' in models else models[0]
\ No newline at end of file
diff --git a/plugins/Convert_Adjust.py b/plugins/convert/Convert_Adjust.py
similarity index 100%
rename from plugins/Convert_Adjust.py
rename to plugins/convert/Convert_Adjust.py
diff --git a/plugins/Convert_Masked.py b/plugins/convert/Convert_Masked.py
similarity index 84%
rename from plugins/Convert_Masked.py
rename to plugins/convert/Convert_Masked.py
index 1e516fe..dc635bf 100644
--- a/plugins/Convert_Masked.py
+++ b/plugins/convert/Convert_Masked.py
@@ -6,7 +6,7 @@ import numpy
 from lib.aligner import get_align_mat
 
 class Convert():
-    def __init__(self, encoder, trainer, blur_size=2, seamless_clone=False, mask_type="facehullandrect", erosion_kernel_size=None, match_histogram=False, sharpen_image=None, **kwargs):
+    def __init__(self, encoder, trainer, blur_size=2, seamless_clone=False, mask_type="facehullandrect", erosion_kernel_size=None, match_histogram=False, sharpen_image=None, draw_transparent=False, **kwargs):
         self.encoder = encoder
         self.trainer = trainer
         self.erosion_kernel = None
@@ -21,6 +21,7 @@ class Convert():
         self.sharpen_image = sharpen_image
         self.match_histogram = match_histogram
         self.mask_type = mask_type.lower() # Choose in 'FaceHullAndRect','FaceHull','Rect'
+        self.draw_transparent = draw_transparent
 
     def patch_image( self, image, face_detected, size ):
 
@@ -41,7 +42,30 @@ class Convert():
 
         return self.apply_new_face(image, new_face, image_mask, mat, image_size, size)
 
+    @staticmethod
+    def convert_transparent(image, new_face, image_mask, image_size):
+        """ Add alpha channels to images and change to 
+            transparent background """
+        image = numpy.zeros((image_size[1], image_size[0], 4),
+                            dtype=numpy.uint8)
+        
+        mask_b, mask_g, mask_r = cv2.split(image_mask)
+        face_b, face_g, face_r = cv2.split(new_face)
+
+        alpha_mask = numpy.ones(mask_b.shape, dtype=mask_b.dtype) * 50
+        alpha_face = numpy.ones(face_b.shape, dtype=face_b.dtype) * 50
+
+        image_mask = cv2.merge((mask_b, mask_g, mask_r, alpha_mask))
+        new_face = cv2.merge((face_b, face_g, face_r, alpha_face))
+        return image, new_face, image_mask
+    
     def apply_new_face(self, image, new_face, image_mask, mat, image_size, size):
+    
+        if self.draw_transparent:
+            image, new_face, image_mask = self.convert_transparent(image,
+                                                                   new_face,
+                                                                   image_mask,
+                                                                   image_size)
         base_image = numpy.copy( image )
         new_image = numpy.copy( image )
 
diff --git a/plugins/convert/__init__.py b/plugins/convert/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/extract/__init__.py b/plugins/extract/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/lib/face_alignment/.cache/2DFAN-4.h5 b/plugins/extract/align/.cache/2DFAN-4.pb
similarity index 83%
rename from lib/face_alignment/.cache/2DFAN-4.h5
rename to plugins/extract/align/.cache/2DFAN-4.pb
index 8c5079a..fec04ef 100644
Binary files a/lib/face_alignment/.cache/2DFAN-4.h5 and b/plugins/extract/align/.cache/2DFAN-4.pb differ
diff --git a/plugins/extract/align/__init__.py b/plugins/extract/align/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
new file mode 100644
index 0000000..1149390
--- /dev/null
+++ b/plugins/extract/align/_base.py
@@ -0,0 +1,196 @@
+#!/usr/bin/env python3
+""" Base class for Face Aligner plugins
+    Plugins should inherit from this class
+
+    See the override methods for which methods are
+    required.
+
+    The plugin will receive a dict containing:
+    {"filename": <filename of source frame>,
+     "image": <source image>,
+     "detected_faces": <list of DetectedFaces objects without landmarks>}
+
+    For each source item, the plugin must pass a dict to finalize containing:
+    {"filename": <filename of source frame>,
+     "image": <source image>,
+     "detected_faces": <list of final DetectedFaces objects>}
+    """
+
+import os
+import cv2
+import numpy as np
+
+from lib.aligner import get_align_mat
+from lib.align_eyes import FACIAL_LANDMARKS_IDXS
+
+from lib.gpu_stats import GPUStats
+
+
+class Aligner():
+    """ Landmarks Aligner Object """
+    def __init__(self, verbose=False, align_eyes=False, size=256):
+        self.verbose = verbose
+        self.size = size
+        self.cachepath = os.path.join(os.path.dirname(__file__), ".cache")
+        self.align_eyes = align_eyes
+        self.extract = Extract()
+        self.init = None
+
+        # The input and output queues for the plugin.
+        # See lib.multithreading.QueueManager for getting queues
+        self.queues = {"in": None, "out": None}
+
+        #  Path to model if required
+        self.model_path = self.set_model_path()
+
+        # Approximate VRAM required for aligner. Used to calculate
+        # how many parallel processes / batches can be run.
+        # Be conservative to avoid OOM.
+        self.vram = None
+
+    # <<< OVERRIDE METHODS >>> #
+    # These methods must be overriden when creating a plugin
+    @staticmethod
+    def set_model_path():
+        """ path to data file/models
+            override for specific detector """
+        raise NotImplementedError()
+
+    def initialize(self, *args, **kwargs):
+        """ Inititalize the aligner
+            Tasks to be run before any alignments are performed.
+            Override for specific detector """
+        self.init = kwargs["event"]
+        self.queues["in"] = kwargs["in_queue"]
+        self.queues["out"] = kwargs["out_queue"]
+
+    def align(self, *args, **kwargs):
+        """ Process landmarks
+            Override for specific detector
+            Must return a list of dlib rects"""
+        try:
+            if not self.init:
+                self.initialize(*args, **kwargs)
+        except ValueError as err:
+            print("ERROR: {}".format(err))
+            exit(1)
+
+    # <<< FINALIZE METHODS>>> #
+    def finalize(self, output):
+        """ This should be called as the final task of each plugin
+            aligns faces and puts to the out queue """
+        if output == "EOF":
+            self.queues["out"].put("EOF")
+            return
+        self.align_faces(output)
+        self.queues["out"].put((output))
+
+    def align_faces(self, output):
+        """ Align the faces """
+        detected_faces = output["detected_faces"]
+        image = output["image"]
+
+        resized_faces = list()
+        t_mats = list()
+
+        for face in detected_faces:
+            resized_face, t_mat = self.extract.extract(image,
+                                                       face,
+                                                       self.size,
+                                                       self.align_eyes)
+            resized_faces.append(resized_face)
+            t_mats.append(t_mat)
+
+        output["resized_faces"] = resized_faces
+        output["t_mats"] = t_mats
+
+    # <<< MISC METHODS >>> #
+    def get_vram_free(self):
+        """ Return free and total VRAM on card with most VRAM free"""
+        stats = GPUStats()
+        vram = stats.get_card_most_free()
+        if self.verbose:
+            print("Using device {} with {}MB free of {}MB".format(
+                vram["device"],
+                int(vram["free"]),
+                int(vram["total"])))
+        return int(vram["free"]), int(vram["total"])
+
+
+class Extract():
+    """ Based on the original https://www.reddit.com/r/deepfakes/
+        code sample + contribs """
+
+    def extract(self, image, face, size, align_eyes):
+        """ Extract a face from an image """
+        alignment = get_align_mat(face, size, align_eyes)
+        extracted = self.transform(image, alignment, size, 48)
+        return extracted, alignment
+
+    @staticmethod
+    def transform(image, mat, size, padding=0):
+        """ Transform Image """
+        matrix = mat * (size - 2 * padding)
+        matrix[:, 2] += padding
+        return cv2.warpAffine(image, matrix, (size, size))
+
+    @staticmethod
+    def transform_points(points, mat, size, padding=0):
+        """ Transform points along matrix """
+        matrix = mat * (size - 2 * padding)
+        matrix[:, 2] += padding
+        points = np.expand_dims(points, axis=1)
+        points = cv2.transform(points, matrix, points.shape)
+        points = np.squeeze(points)
+        return points
+
+    @staticmethod
+    def get_feature_mask(aligned_landmarks_68, size,
+                         padding=0, dilation=30):
+        """ Return the face feature mask """
+        scale = size - 2*padding
+        translation = padding
+        pad_mat = np.matrix([[scale, 0.0, translation],
+                             [0.0, scale, translation]])
+        aligned_landmarks_68 = np.expand_dims(aligned_landmarks_68, axis=1)
+        aligned_landmarks_68 = cv2.transform(aligned_landmarks_68,
+                                             pad_mat,
+                                             aligned_landmarks_68.shape)
+        aligned_landmarks_68 = np.squeeze(aligned_landmarks_68)
+
+        (l_start, l_end) = FACIAL_LANDMARKS_IDXS["left_eye"]
+        (r_start, r_end) = FACIAL_LANDMARKS_IDXS["right_eye"]
+        (m_start, m_end) = FACIAL_LANDMARKS_IDXS["mouth"]
+        (n_start, n_end) = FACIAL_LANDMARKS_IDXS["nose"]
+        (lb_start, lb_end) = FACIAL_LANDMARKS_IDXS["left_eyebrow"]
+        (rb_start, rb_end) = FACIAL_LANDMARKS_IDXS["right_eyebrow"]
+        (c_start, c_end) = FACIAL_LANDMARKS_IDXS["chin"]
+
+        l_eye_points = aligned_landmarks_68[l_start:l_end].tolist()
+        l_brow_points = aligned_landmarks_68[lb_start:lb_end].tolist()
+        r_eye_points = aligned_landmarks_68[r_start:r_end].tolist()
+        r_brow_points = aligned_landmarks_68[rb_start:rb_end].tolist()
+        nose_points = aligned_landmarks_68[n_start:n_end].tolist()
+        chin_points = aligned_landmarks_68[c_start:c_end].tolist()
+        mouth_points = aligned_landmarks_68[m_start:m_end].tolist()
+        l_eye_points = l_eye_points + l_brow_points
+        r_eye_points = r_eye_points + r_brow_points
+        mouth_points = mouth_points + nose_points + chin_points
+
+        l_eye_hull = cv2.convexHull(np.array(l_eye_points).reshape(
+            (-1, 2)).astype(int)).flatten().reshape((-1, 2))
+        r_eye_hull = cv2.convexHull(np.array(r_eye_points).reshape(
+            (-1, 2)).astype(int)).flatten().reshape((-1, 2))
+        mouth_hull = cv2.convexHull(np.array(mouth_points).reshape(
+            (-1, 2)).astype(int)).flatten().reshape((-1, 2))
+
+        mask = np.zeros((size, size, 3), dtype=float)
+        cv2.fillConvexPoly(mask, l_eye_hull, (1, 1, 1))
+        cv2.fillConvexPoly(mask, r_eye_hull, (1, 1, 1))
+        cv2.fillConvexPoly(mask, mouth_hull, (1, 1, 1))
+
+        if dilation > 0:
+            kernel = np.ones((dilation, dilation), np.uint8)
+            mask = cv2.dilate(mask, kernel, iterations=1)
+
+        return mask
diff --git a/plugins/extract/align/dlib.py b/plugins/extract/align/dlib.py
new file mode 100644
index 0000000..2d97572
--- /dev/null
+++ b/plugins/extract/align/dlib.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+""" DLib landmarks extractor for faceswap.py
+"""
+import face_recognition_models
+import dlib
+
+from ._base import Aligner
+
+
+class Align(Aligner):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.vram = 0  # Doesn't use GPU
+        self.model = None
+
+    def set_model_path(self):
+        """ Model path handled by face_recognition_models """
+        return face_recognition_models.pose_predictor_model_location()
+
+    def initialize(self, *args, **kwargs):
+        """ Initialization tasks to run prior to alignments """
+        super().initialize(*args, **kwargs)
+        print("Initializing Dlib Pose Predictor...")
+        self.model = dlib.shape_predictor(self.model_path)
+        self.init.set()
+        print("Initialized Dlib Pose Predictor.")
+
+    def align(self, *args, **kwargs):
+        """ Perform alignments on detected faces """
+        super().align(*args, **kwargs)
+        while True:
+            item = self.queues["in"].get()
+            if item == "EOF":
+                break
+            image = item["image"][:, :, ::-1].copy()
+            self.process_landmarks(image, item["detected_faces"])
+            self.finalize(item)
+        self.finalize("EOF")
+
+    def process_landmarks(self, image, detected_faces):
+        """ Align image and process landmarks """
+        for detected_face in detected_faces:
+            process_face = detected_face.to_dlib_rect()
+            pts = self.model(image, process_face).parts()
+            detected_face.landmarksXY = [(point.x, point.y) for point in pts]
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
new file mode 100644
index 0000000..84bb216
--- /dev/null
+++ b/plugins/extract/align/fan.py
@@ -0,0 +1,231 @@
+#!/usr/bin/env python3
+""" Facial landmarks extractor for faceswap.py
+    Code adapted and modified from:
+    https://github.com/1adrianb/face-alignment
+"""
+import os
+
+import cv2
+import numpy as np
+import tensorflow as tf
+
+from ._base import Aligner
+
+
+class Align(Aligner):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.vram = 2240
+        self.reference_scale = 195.0
+        self.model = None
+        self.test = None
+
+    def set_model_path(self):
+        """ Load the mtcnn models """
+        model_path = os.path.join(self.cachepath, "2DFAN-4.pb")
+        if not os.path.exists(model_path):
+            raise Exception("Error: Unable to find {}, reinstall "
+                            "the lib!".format(model_path))
+        return model_path
+
+    def initialize(self, *args, **kwargs):
+        """ Initialization tasks to run prior to alignments """
+        print("Initializing Face Alignment Network...")
+        super().initialize(*args, **kwargs)
+
+        _, vram_total = self.get_vram_free()
+        if vram_total <= self.vram:
+            tf_ratio = 1.0
+        else:
+            tf_ratio = self.vram / vram_total
+        if self.verbose:
+            print("Reserving {}MB for face alignments".format(self.vram))
+
+        self.model = FAN(self.model_path,
+                         verbose=self.verbose, ratio=tf_ratio)
+
+        self.init.set()
+        print("Initialized Face Alignment Network.")
+
+    def align(self, *args, **kwargs):
+        """ Perform alignments on detected faces """
+        super().align(*args, **kwargs)
+        try:
+            while True:
+                item = self.queues["in"].get()
+                if item == "EOF":
+                    break
+                image = item["image"][:, :, ::-1].copy()
+                self.process_landmarks(image, item["detected_faces"])
+                self.finalize(item)
+            self.finalize("EOF")
+        except:
+            item["exception"] = True
+            self.queues["out"].put(item)
+            raise
+
+    def process_landmarks(self, image, detected_faces):
+        """ Align image and process landmarks """
+        for detected_face in detected_faces:
+            process_face = detected_face.to_dlib_rect()
+            center, scale = self.get_center_scale(process_face)
+            aligned_image = self.align_image(image, center, scale)
+            detected_face.landmarksXY = self.predict_landmarks(aligned_image,
+                                                               center,
+                                                               scale)
+
+    def get_center_scale(self, detected_face):
+        """ Get the center and set scale of bounding box """
+        center = np.array([(detected_face.left()
+                            + detected_face.right()) / 2.0,
+                           (detected_face.top()
+                            + detected_face.bottom()) / 2.0])
+
+        center[1] -= (detected_face.bottom()
+                      - detected_face.top()) * 0.12
+
+        scale = (detected_face.right()
+                 - detected_face.left()
+                 + detected_face.bottom()
+                 - detected_face.top()) / self.reference_scale
+
+        return center, scale
+
+    def align_image(self, image, center, scale):
+        """ Crop and align image around center """
+        image = self.crop(
+            image,
+            center,
+            scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
+
+        return np.expand_dims(image, 0)
+
+    def predict_landmarks(self, image, center, scale):
+        """ Predict the 68 point landmarks """
+        prediction = self.model.predict(image)[-1]
+        pts_img = self.get_pts_from_predict(prediction, center, scale)
+
+        return [(int(pt[0]), int(pt[1])) for pt in pts_img]
+
+    @staticmethod
+    def transform(point, center, scale, resolution):
+        """ Transform Image """
+        pnt = np.array([point[0], point[1], 1.0])
+        hscl = 200.0 * scale
+        eye = np.eye(3)
+        eye[0, 0] = resolution / hscl
+        eye[1, 1] = resolution / hscl
+        eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
+        eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
+        eye = np.linalg.inv(eye)
+        return np.matmul(eye, pnt)[0:2]
+
+    def crop(self, image, center, scale, resolution=256.0):
+        """ Crop image around the center point """
+        v_ul = self.transform([1, 1], center, scale, resolution).astype(np.int)
+        v_br = self.transform([resolution, resolution],
+                              center,
+                              scale,
+                              resolution).astype(np.int)
+        if image.ndim > 2:
+            new_dim = np.array([v_br[1] - v_ul[1],
+                                v_br[0] - v_ul[0],
+                                image.shape[2]],
+                               dtype=np.int32)
+            self.test = new_dim
+            new_img = np.zeros(new_dim, dtype=np.uint8)
+        else:
+            new_dim = np.array([v_br[1] - v_ul[1],
+                                v_br[0] - v_ul[0]],
+                               dtype=np.int)
+            self.test = new_dim
+            new_img = np.zeros(new_dim, dtype=np.uint8)
+        height = image.shape[0]
+        width = image.shape[1]
+        new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
+                         dtype=np.int32)
+        new_y = np.array([max(1, -v_ul[1] + 1),
+                          min(v_br[1], height) - v_ul[1]],
+                         dtype=np.int32)
+        old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
+                         dtype=np.int32)
+        old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
+                         dtype=np.int32)
+        new_img[new_y[0] - 1:new_y[1],
+                new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
+                                               old_x[0] - 1:old_x[1], :]
+        new_img = cv2.resize(new_img,
+                             dsize=(int(resolution), int(resolution)),
+                             interpolation=cv2.INTER_LINEAR)
+        return new_img
+
+    def get_pts_from_predict(self, var_a, center, scale):
+        """ Get points from predictor """
+        var_b = var_a.reshape((var_a.shape[0],
+                               var_a.shape[1] * var_a.shape[2]))
+        var_c = var_b.argmax(1).reshape((var_a.shape[0],
+                                         1)).repeat(2,
+                                                    axis=1).astype(np.float)
+        var_c[:, 0] %= var_a.shape[2]
+        var_c[:, 1] = np.apply_along_axis(
+            lambda x: np.floor(x / var_a.shape[2]),
+            0,
+            var_c[:, 1])
+
+        for i in range(var_a.shape[0]):
+            pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
+            if pt_x > 0 and pt_x < 63 and pt_y > 0 and pt_y < 63:
+                diff = np.array([var_a[i, pt_y, pt_x+1]
+                                 - var_a[i, pt_y, pt_x-1],
+                                 var_a[i, pt_y+1, pt_x]
+                                 - var_a[i, pt_y-1, pt_x]])
+
+                var_c[i] += np.sign(diff)*0.25
+
+        var_c += 0.5
+        return [self.transform(var_c[i], center, scale, var_a.shape[2])
+                for i in range(var_a.shape[0])]
+
+
+class FAN(object):
+    """The FAN Model.
+    Converted from pyTorch via ONNX from:
+    https://github.com/1adrianb/face-alignment """
+
+    def __init__(self, model_path, verbose=False, ratio=1.0):
+        self.verbose = verbose
+        self.model_path = model_path
+        self.graph = self.load_graph()
+        self.input = self.graph.get_tensor_by_name("fa/0:0")
+        self.output = self.graph.get_tensor_by_name("fa/Add_95:0")
+        self.session = self.set_session(ratio)
+
+    def load_graph(self):
+        """ Load the tensorflow Model and weights """
+        if self.verbose:
+            print("Initializing Face Alignment Network model...")
+
+        with tf.gfile.GFile(self.model_path, "rb") as gfile:
+            graph_def = tf.GraphDef()
+            graph_def.ParseFromString(gfile.read())
+        fa_graph = tf.Graph()
+        with fa_graph.as_default():
+            tf.import_graph_def(graph_def, name="fa")
+        return fa_graph
+
+    def set_session(self, vram_ratio):
+        """ Set the TF Session and initialize """
+        placeholder = np.zeros((1, 3, 256, 256))
+        with self.graph.as_default():
+            config = tf.ConfigProto()
+            config.gpu_options.per_process_gpu_memory_fraction = vram_ratio
+            session = tf.Session(config=config)
+            with session.as_default():
+                session.run(self.output, feed_dict={self.input: placeholder})
+        return session
+
+    def predict(self, feed_item):
+        """ Predict landmarks in session """
+        return self.session.run(self.output,
+                                feed_dict={self.input: feed_item})
diff --git a/lib/face_alignment/.cache/det1.npy b/plugins/extract/detect/.cache/det1.npy
similarity index 100%
rename from lib/face_alignment/.cache/det1.npy
rename to plugins/extract/detect/.cache/det1.npy
diff --git a/lib/face_alignment/.cache/det2.npy b/plugins/extract/detect/.cache/det2.npy
similarity index 100%
rename from lib/face_alignment/.cache/det2.npy
rename to plugins/extract/detect/.cache/det2.npy
diff --git a/lib/face_alignment/.cache/det3.npy b/plugins/extract/detect/.cache/det3.npy
similarity index 100%
rename from lib/face_alignment/.cache/det3.npy
rename to plugins/extract/detect/.cache/det3.npy
diff --git a/plugins/extract/detect/__init__.py b/plugins/extract/detect/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
new file mode 100644
index 0000000..47fc2f8
--- /dev/null
+++ b/plugins/extract/detect/_base.py
@@ -0,0 +1,250 @@
+#!/usr/bin/env python3
+""" Base class for Face Detector plugins
+    Plugins should inherit from this class
+
+    See the override methods for which methods are
+    required.
+
+    For each source frame, the plugin must pass a dict to finalize containing:
+    {"filename": <filename of source frame>,
+     "image": <source image>,
+     "detected_faces": <list of dlib.rectangles>}
+    """
+
+import os
+
+import cv2
+import dlib
+
+from lib.faces_detect import DetectedFace
+from lib.gpu_stats import GPUStats
+from lib.utils import rotate_image_by_angle, rotate_landmarks
+
+
+class Detector():
+    """ Detector object """
+    def __init__(self, verbose=False, rotation=None):
+        self.cachepath = os.path.join(os.path.dirname(__file__), ".cache")
+        self.verbose = verbose
+        self.rotation = self.get_rotation_angles(rotation)
+        self.parent_is_pool = False
+        self.init = None
+
+        # The input and output queues for the plugin.
+        # See lib.multithreading.QueueManager for getting queues
+        self.queues = {"in": None, "out": None}
+
+        # Scaling factor for image. Plugin dependent
+        self.scale = 1.0
+
+        #  Path to model if required
+        self.model_path = self.set_model_path()
+
+        # Target image size for passing images through the detector
+        # Set to tuple of dimensions (x, y) or int of pixel count
+        self.target = None
+
+        # Approximate VRAM used for the set target. Used to calculate
+        # how many parallel processes / batches can be run.
+        # Be conservative to avoid OOM.
+        self.vram = None
+
+        # For detectors that support batching, this should be set to
+        # the calculated batch size that the amount of available VRAM
+        # will support. It is also used for holding the number of threads/
+        # processes for parallel processing plugins
+        self.batch_size = 1
+
+    # <<< OVERRIDE METHODS >>> #
+    # These methods must be overriden when creating a plugin
+    @staticmethod
+    def set_model_path():
+        """ path to data file/models
+            override for specific detector """
+        raise NotImplementedError()
+
+    def initialize(self, *args, **kwargs):
+        """ Inititalize the detector
+            Tasks to be run before any detection is performed.
+            Override for specific detector """
+        init = kwargs.get("event", False)
+        self.init = init
+        self.queues["in"] = kwargs["in_queue"]
+        self.queues["out"] = kwargs["out_queue"]
+
+    def detect_faces(self, *args, **kwargs):
+        """ Detect faces in rgb image
+            Override for specific detector
+            Must return a list of dlib rects"""
+        try:
+            if not self.init:
+                self.initialize(*args, **kwargs)
+        except ValueError as err:
+            print("ERROR: {}".format(err))
+            exit(1)
+
+    # <<< FINALIZE METHODS>>> #
+    def finalize(self, output):
+        """ This should be called as the final task of each plugin
+            Performs fianl processing and puts to the out queue """
+        detected_faces = self.to_detected_face(output["image"],
+                                               output["detected_faces"])
+        output["detected_faces"] = detected_faces
+        self.queues["out"].put(output)
+
+    @staticmethod
+    def to_detected_face(image, dlib_rects):
+        """ Convert list of dlib rectangles to a
+            list of DetectedFace objects
+            and add the cropped face """
+        retval = list()
+        for d_rect in dlib_rects:
+            if not isinstance(d_rect, dlib.rectangle):
+                retval.append(list())
+                continue
+            detected_face = DetectedFace()
+            detected_face.from_dlib_rect(d_rect)
+            detected_face.image_to_face(image)
+            retval.append(detected_face)
+        return retval
+
+    # <<< DETECTION IMAGE COMPILATION METHODS >>> #
+    def compile_detection_image(self, image, is_square, scale_up):
+        """ Compile the detection image """
+        self.set_scale(image, is_square=is_square, scale_up=scale_up)
+        return self.set_detect_image(image)
+
+    def set_scale(self, image, is_square=False, scale_up=False):
+        """ Set the scale factor for incoming image """
+        height, width = image.shape[:2]
+        if is_square:
+            if isinstance(self.target, int):
+                dims = (self.target ** 0.5, self.target ** 0.5)
+                self.target = dims
+            source = max(height, width)
+            target = max(self.target)
+        else:
+            if isinstance(self.target, tuple):
+                self.target = self.target[0] * self.target[1]
+            source = width * height
+            target = self.target
+
+        if scale_up or target < source:
+            self.scale = target / source
+        else:
+            self.scale = 1.0
+
+    def set_detect_image(self, input_image):
+        """ Convert the image to RGB and scale """
+        image = input_image[:, :, ::-1].copy()
+        if self.scale == 1.0:
+            return image
+
+        height, width = image.shape[:2]
+        interpln = cv2.INTER_LINEAR if self.scale > 1.0 else cv2.INTER_AREA
+        dims = (int(width * self.scale), int(height * self.scale))
+
+        if self.verbose and self.scale < 1.0:
+            print("Resizing image from {}x{} to {}.".format(
+                str(width), str(height), "x".join(str(i) for i in dims)))
+
+        image = cv2.resize(image, dims, interpolation=interpln)
+        return image
+
+    # <<< IMAGE ROTATION METHODS >>> #
+    @staticmethod
+    def get_rotation_angles(rotation):
+        """ Set the rotation angles. Includes backwards compatibility for the
+            'on' and 'off' options:
+                - 'on' - increment 90 degrees
+                - 'off' - disable
+                - 0 is prepended to the list, as whatever happens, we want to
+                  scan the image in it's upright state """
+        rotation_angles = [0]
+
+        if not rotation or rotation.lower() == "off":
+            return rotation_angles
+
+        if rotation.lower() == "on":
+            rotation_angles.extend(range(90, 360, 90))
+        else:
+            passed_angles = [int(angle)
+                             for angle in rotation.split(",")]
+            if len(passed_angles) == 1:
+                rotation_step_size = passed_angles[0]
+                rotation_angles.extend(range(rotation_step_size,
+                                             360,
+                                             rotation_step_size))
+            elif len(passed_angles) > 1:
+                rotation_angles.extend(passed_angles)
+
+        return rotation_angles
+
+    @staticmethod
+    def rotate_image(image, angle):
+        """ Rotate the image by given angle and return
+            Image with rotation matrix """
+        if angle == 0:
+            return image, None
+        return rotate_image_by_angle(image, angle)
+
+    @staticmethod
+    def rotate_rect(d_rect, rotation_matrix):
+        """ Rotate a dlib rect based on the rotation_matrix"""
+        d_rect = rotate_landmarks(d_rect, rotation_matrix)
+        return d_rect
+
+    # << QUEUE METHODS >> #
+    def get_batch(self):
+        """ Get items from the queue in batches of
+            self.batch_size
+
+            First item in output tuple indicates whether the
+            queue is exhausted.
+            Second item is the batch
+
+            Remember to put "EOF" to the out queue after processing
+            the final batch """
+        exhausted = False
+        batch = list()
+        for _ in range(self.batch_size):
+            item = self.queues["in"].get()
+            if item == "EOF":
+                exhausted = True
+                break
+            batch.append(item)
+        return (exhausted, batch)
+
+    # <<< DLIB RECTANGLE METHODS >>> #
+    @staticmethod
+    def is_mmod_rectangle(d_rectangle):
+        """ Return whether the passed in object is
+            a dlib.mmod_rectangle """
+        return isinstance(d_rectangle, dlib.mmod_rectangle)
+
+    def convert_to_dlib_rectangle(self, d_rect):
+        """ Convert detected mmod_rects to dlib_rectangle """
+        if self.is_mmod_rectangle(d_rect):
+            return d_rect.rect
+        return d_rect
+
+    # <<< MISC METHODS >>> #
+    def get_vram_free(self):
+        """ Return total free VRAM on largest card """
+        stats = GPUStats()
+        vram = stats.get_card_most_free()
+        if self.verbose:
+            print("Using device {} with {}MB free of {}MB".format(
+                vram["device"],
+                int(vram["free"]),
+                int(vram["total"])))
+        return int(vram["free"])
+
+    @staticmethod
+    def set_predetected(width, height):
+        """ Set a dlib rectangle for predetected faces """
+        # Predetected_face is used for sort tool.
+        # Landmarks should not be extracted again from predetected faces,
+        # because face data is lost, resulting in a large variance
+        # against extract from original image
+        return [dlib.rectangle(0, 0, width, height)]
diff --git a/plugins/extract/detect/dlib_cnn.py b/plugins/extract/detect/dlib_cnn.py
new file mode 100644
index 0000000..a50fd7d
--- /dev/null
+++ b/plugins/extract/detect/dlib_cnn.py
@@ -0,0 +1,165 @@
+#!/usr/bin/env python3
+""" DLIB CNN Face detection plugin """
+
+import numpy as np
+import face_recognition_models
+from lib.utils import rotate_image_by_angle
+
+from ._base import Detector, dlib
+
+
+class Detect(Detector):
+    """ Dlib detector for face recognition """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.target = (2048, 2048)  # Uses approx 1805MB of VRAM
+        self.vram = 1600  # Lower as batch size of 2 gives wiggle room
+        self.detector = None
+
+    def compiled_for_cuda(self):
+        """ Return a message on DLIB Cuda Compilation status """
+        cuda = dlib.DLIB_USE_CUDA
+        msg = "DLib is "
+        if not cuda:
+            msg += "NOT "
+        msg += "compiled to use CUDA"
+        if self.verbose:
+            print(msg)
+        return cuda
+
+    def set_model_path(self):
+        """ Model path handled by face_recognition_models """
+        return face_recognition_models.cnn_face_detector_model_location()
+
+    def initialize(self, *args, **kwargs):
+        """ Calculate batch size """
+        print("Initializing Dlib-CNN Detector...")
+        super().initialize(*args, **kwargs)
+        self.detector = dlib.cnn_face_detection_model_v1(self.model_path)
+        is_cuda = self.compiled_for_cuda()
+        if is_cuda:
+            vram_free = self.get_vram_free()
+        else:
+            vram_free = 2048
+            if self.verbose:
+                print("Using CPU. Limiting RAM useage to "
+                      "{}MB".format(vram_free))
+
+        # Batch size of 2 actually uses about 338MB less than a single image??
+        # From there batches increase at ~680MB per item in the batch
+
+        self.batch_size = int(((vram_free - self.vram) / 680) + 2)
+
+        if self.batch_size < 1:
+            raise ValueError("Insufficient VRAM available to continue "
+                             "({}MB)".format(int(vram_free)))
+
+        if self.verbose:
+            print("Processing in batches of {}".format(self.batch_size))
+
+        self.init.set()
+        print("Initialized Dlib-CNN Detector...")
+
+    def detect_faces(self, *args, **kwargs):
+        """ Detect faces in rgb image """
+        super().detect_faces(*args, **kwargs)
+        try:
+            while True:
+                exhausted, batch = self.get_batch()
+                filenames, images = map(list, zip(*batch))
+                detect_images = self.compile_detection_images(images)
+                batch_detected = self.detector(detect_images, 0)
+                processed = self.process_output(batch_detected,
+                                                indexes=None,
+                                                rotation_matrix=None,
+                                                output=None)
+                if not all(faces
+                           for faces in processed) and self.rotation != [0]:
+                    processed = self.process_rotations(detect_images,
+                                                       processed)
+                for idx, faces in enumerate(processed):
+                    retval = {"filename": filenames[idx],
+                              "image": images[idx],
+                              "detected_faces": faces}
+                    self.finalize(retval)
+                if exhausted:
+                    self.queues["out"].put("EOF")
+                    break
+        except:
+            retval = {"exception": True}
+            self.queues["out"].put(retval)
+            # Free up VRAM
+            del self.detector
+            raise
+
+        # Free up VRAM
+        del self.detector
+
+    def compile_detection_images(self, images):
+        """ Compile the detection images into batches """
+        detect_images = list()
+        for image in images:
+            self.set_scale(image, is_square=True, scale_up=True)
+            detect_images.append(self.set_detect_image(image))
+        return detect_images
+
+    def process_output(self, batch_detected,
+                       indexes=None, rotation_matrix=None, output=None):
+        """ Process the output images """
+        output = output if output else list()
+        for idx, faces in enumerate(batch_detected):
+            detected_faces = list()
+
+            if isinstance(rotation_matrix, np.ndarray):
+                faces = [self.rotate_rect(face.rect, rotation_matrix)
+                         for face in faces]
+
+            for face in faces:
+                face = self.convert_to_dlib_rectangle(face)
+                face = dlib.rectangle(int(face.left() / self.scale),
+                                      int(face.top() / self.scale),
+                                      int(face.right() / self.scale),
+                                      int(face.bottom() / self.scale))
+                detected_faces.append(face)
+            if indexes:
+                target = indexes[idx]
+                output[target] = detected_faces
+            else:
+                output.append(detected_faces)
+        return output
+
+    def process_rotations(self, detect_images, processed):
+        """ Rotate frames missing faces until face is found """
+        for angle in self.rotation:
+            if all(faces for faces in processed):
+                break
+            if angle == 0:
+                continue
+            reprocess, indexes, rotmat = self.compile_reprocess(
+                processed,
+                detect_images,
+                angle)
+
+            batch_detected = self.detector(reprocess, 0)
+            if self.verbose and any(item.any() for item in batch_detected):
+                print("found face(s) by rotating image {} degrees".format(
+                    angle))
+            processed = self.process_output(batch_detected,
+                                            indexes=indexes,
+                                            rotation_matrix=rotmat,
+                                            output=processed)
+        return processed
+
+    @staticmethod
+    def compile_reprocess(processed, detect_images, angle):
+        """ Rotate images which did not find a face for reprocessing """
+        indexes = list()
+        to_detect = list()
+        for idx, faces in enumerate(processed):
+            if faces:
+                continue
+            image = detect_images[idx]
+            rot_image, rot_matrix = rotate_image_by_angle(image, angle)
+            to_detect.append(rot_image)
+            indexes.append(idx)
+        return to_detect, indexes, rot_matrix
diff --git a/plugins/extract/detect/dlib_hog.py b/plugins/extract/detect/dlib_hog.py
new file mode 100644
index 0000000..b1d36dd
--- /dev/null
+++ b/plugins/extract/detect/dlib_hog.py
@@ -0,0 +1,83 @@
+#!/usr/bin/env python3
+""" DLIB CNN Face detection plugin """
+from time import sleep
+
+import numpy as np
+
+from ._base import Detector, dlib
+
+
+class Detect(Detector):
+    """ Dlib detector for face recognition """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.parent_is_pool = True
+        self.target = (2048, 2048)  # Doesn't use VRAM
+        self.vram = 0
+        self.detector = dlib.get_frontal_face_detector()
+        self.iterator = None
+
+    def set_model_path(self):
+        """ No model for dlib Hog """
+        pass
+
+    def initialize(self, *args, **kwargs):
+        """ Calculate batch size """
+        print("Initializing Dlib-HOG Detector...")
+        super().initialize(*args, **kwargs)
+        if self.verbose:
+            print("Using CPU for detection")
+        self.init = True
+        print("Initialized Dlib-HOG Detector...")
+
+    def detect_faces(self, *args, **kwargs):
+        """ Detect faces in rgb image """
+        super().detect_faces(*args, **kwargs)
+        try:
+            while True:
+                item = self.queues["in"].get()
+                if item in ("EOF", "END"):
+                    self.queues["in"].put("END")
+                    break
+
+                filename, image = item
+                detect_image = self.compile_detection_image(image, True, True)
+
+                for angle in self.rotation:
+                    current_image, rotmat = self.rotate_image(detect_image,
+                                                              angle)
+
+                    faces = self.detector(current_image, 0)
+
+                    if self.verbose and angle != 0 and faces.any():
+                        print("found face(s) by rotating image {} "
+                              "degrees".format(angle))
+
+                    if faces:
+                        break
+
+                detected_faces = self.process_output(faces, rotmat)
+                retval = {"filename": filename,
+                          "image": image,
+                          "detected_faces": detected_faces}
+                self.finalize(retval)
+        except:
+            retval = {"exception": True}
+            self.queues["out"].put(retval)
+            raise
+
+        if item == "EOF":
+            sleep(3)  # Wait for all processes to finish before EOF (hacky!)
+            self.queues["out"].put("EOF")
+
+    def process_output(self, faces, rotation_matrix):
+        """ Compile found faces for output """
+        if isinstance(rotation_matrix, np.ndarray):
+            faces = [self.rotate_rect(face, rotation_matrix)
+                     for face in faces]
+        detected = [dlib.rectangle(int(face.left() / self.scale),
+                                   int(face.top() / self.scale),
+                                   int(face.right() / self.scale),
+                                   int(face.bottom() / self.scale))
+                    for face in faces]
+        return detected
diff --git a/plugins/extract/detect/manual.py b/plugins/extract/detect/manual.py
new file mode 100644
index 0000000..b81abac
--- /dev/null
+++ b/plugins/extract/detect/manual.py
@@ -0,0 +1,38 @@
+#!/usr/bin/env python3
+""" Manual face detection plugin """
+
+from ._base import Detector, dlib
+
+
+class Detect(Detector):
+    """ Manual Detector """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    def set_model_path(self):
+        """ No model required for Manual Detector """
+        return None
+
+    def initialize(self, *args, **kwargs):
+        """ Create the mtcnn detector """
+        print("Initializing Manual Detector...")
+        super().initialize(*args, **kwargs)
+        self.init.set()
+        print("Initialized Manual Detector.")
+
+    def detect_faces(self, *args, **kwargs):
+        """ Return the given bounding box in a dlib rectangle """
+        super().detect_faces(*args, **kwargs)
+        while True:
+            item = self.queues["in"].get()
+            if item == "EOF":
+                break
+            image, face = item
+
+            bounding_box = [dlib.rectangle(int(face[0]), int(face[1]),
+                                           int(face[2]), int(face[3]))]
+            retval = {"image": image,
+                      "detected_faces": bounding_box}
+            self.finalize(retval)
+
+        self.queues["out"].put("EOF")
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
new file mode 100644
index 0000000..6e720e1
--- /dev/null
+++ b/plugins/extract/detect/mtcnn.py
@@ -0,0 +1,776 @@
+#!/usr/bin/env python3
+""" MTCNN Face detection plugin """
+
+from __future__ import absolute_import, division, print_function
+
+import os
+
+from six import string_types, iteritems
+
+import cv2
+import numpy as np
+import tensorflow as tf
+
+from lib.multithreading import MultiThread
+from ._base import Detector, dlib
+
+
+class Detect(Detector):
+    """ MTCNN detector for face recognition """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.kwargs = None
+        self.name = "mtcnn"
+        self.target = 2073600  # Uses approx 1.30 GB of VRAM
+        self.vram = 1408
+
+    @staticmethod
+    def validate_kwargs(kwargs):
+        """ Validate that cli kwargs are correct. If not reset to default """
+        valid = True
+        if kwargs['minsize'] < 10:
+            valid = False
+        elif len(kwargs['threshold']) != 3:
+            valid = False
+        elif not all(0.0 < threshold < 1.0
+                     for threshold in kwargs['threshold']):
+            valid = False
+        elif not 0.0 < kwargs['factor'] < 1.0:
+            valid = False
+
+        if not valid:
+            print("Invalid MTCNN arguments received. Running with defaults")
+            return {"minsize": 20,                 # minimum size of face
+                    "threshold": [0.6, 0.7, 0.7],  # three steps threshold
+                    "factor": 0.709}               # scale factor
+        return kwargs
+
+    def set_model_path(self):
+        """ Load the mtcnn models """
+        for model in ("det1.npy", "det2.npy", "det3.npy"):
+            model_path = os.path.join(self.cachepath, model)
+            if not os.path.exists(model_path):
+                raise Exception("Error: Unable to find {}, reinstall "
+                                "the lib!".format(model_path))
+        return self.cachepath
+
+    def initialize(self, *args, **kwargs):
+        """ Create the mtcnn detector """
+        print("Initializing MTCNN Detector...")
+        super().initialize(*args, **kwargs)
+        is_gpu = False
+        self.kwargs = kwargs["mtcnn_kwargs"]
+
+        mtcnn_graph = tf.Graph()
+        with mtcnn_graph.as_default():
+            sess = tf.Session()
+            with sess.as_default():
+                pnet, rnet, onet = create_mtcnn(sess, self.model_path)
+
+            if any("gpu" in str(device).lower()
+                   for device in sess.list_devices()):
+                is_gpu = True
+                alloc = int(sess.run(tf.contrib.memory_stats.BytesLimit()) /
+                            (1024 * 1024))
+        mtcnn_graph.finalize()
+
+        if not is_gpu:
+            alloc = 2048
+            if self.verbose:
+                print("Using CPU. Limiting RAM useage to {}MB".format(alloc))
+
+        self.batch_size = int(alloc / self.vram)
+
+        if self.batch_size < 1:
+            raise ValueError("Insufficient VRAM available to continue "
+                             "({}MB)".format(int(alloc)))
+
+        if self.verbose:
+            print("Processing in {} threads".format(self.batch_size))
+
+        self.kwargs["pnet"] = pnet
+        self.kwargs["rnet"] = rnet
+        self.kwargs["onet"] = onet
+
+        self.init.set()
+        print("Initialized MTCNN Detector.")
+
+    def detect_faces(self, *args, **kwargs):
+        """ Detect faces in Multiple Threads """
+        super().detect_faces(*args, **kwargs)
+        workers = MultiThread(thread_count=self.batch_size)
+        workers.in_thread(target=self.detect_thread)
+        workers.join_threads()
+        self.queues["out"].put("EOF")
+
+    def detect_thread(self):
+        """ Detect faces in rgb image """
+        try:
+            while True:
+                item = self.queues["in"].get()
+                if item == "EOF":
+                    self.queues["in"].put(item)
+                    break
+
+                filename, image = item
+                detect_image = self.compile_detection_image(image, False, False)
+
+                for angle in self.rotation:
+                    current_image, rotmat = self.rotate_image(detect_image, angle)
+                    faces, points = detect_face(current_image, **self.kwargs)
+                    if self.verbose and angle != 0 and faces.any():
+                        print("found face(s) by rotating image {} degrees".format(
+                            angle))
+                    if faces.any():
+                        break
+
+                detected_faces = self.process_output(faces, points, rotmat)
+                retval = {
+                    "filename": filename,
+                    "image": image,
+                    "detected_faces": detected_faces}
+                self.finalize(retval)
+        except:
+            retval = {"exception": True}
+            self.queues["out"].put(retval)
+            raise
+
+    def process_output(self, faces, points, rotation_matrix):
+        """ Compile found faces for output """
+        faces = self.recalculate_bounding_box(faces, points)
+        faces = [dlib.rectangle(int(face[0]), int(face[1]),
+                                int(face[2]), int(face[3]))
+                 for face in faces]
+        if isinstance(rotation_matrix, np.ndarray):
+            faces = [self.rotate_rect(face, rotation_matrix)
+                     for face in faces]
+        detected = [dlib.rectangle(int(face.left() / self.scale),
+                                   int(face.top() / self.scale),
+                                   int(face.right() / self.scale),
+                                   int(face.bottom() / self.scale))
+                    for face in faces]
+        return detected
+
+    @staticmethod
+    def recalculate_bounding_box(faces, landmarks):
+        """ Recalculate the bounding box for Face Alignment.
+
+            Face Alignment was built to expect a DLIB bounding
+            box and calculates center and scale based on that.
+            Resize the bounding box around features to present
+            a better box to Face Alignment. Helps its chances
+            on edge cases and helps remove 'jitter' """
+        retval = list()
+        no_faces = len(faces)
+        if no_faces == 0:
+            return retval
+        face_landmarks = np.hsplit(landmarks, no_faces)
+        for idx in range(no_faces):
+            pts = np.reshape(face_landmarks[idx], (5, 2), order="F")
+            nose = pts[2]
+
+            minmax = (np.amin(pts, axis=0), np.amax(pts, axis=0))
+            padding = [(minmax[1][0] - minmax[0][0]) / 2,
+                       (minmax[1][1] - minmax[0][1]) / 2]
+
+            center = (minmax[1][0] - padding[0], minmax[1][1] - padding[1])
+            offset = (center[0] - nose[0], nose[1] - center[1])
+            center = (center[0] + offset[0], center[1] + offset[1])
+
+            padding[0] += padding[0]
+            padding[1] += padding[1]
+
+            bounding = [center[0] - padding[0], center[1] - padding[1],
+                        center[0] + padding[0], center[1] + padding[1]]
+
+            retval.append(bounding)
+        return retval
+
+
+# MTCNN Detector for face alignment
+# Code adapted from: https://github.com/davidsandberg/facenet
+
+# Tensorflow implementation of the face detection / alignment algorithm
+# found at
+# https://github.com/kpzhang93/MTCNN_face_detection_alignment
+
+# MIT License
+#
+# Copyright (c) 2016 David Sandberg
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+
+def layer(op):
+    """Decorator for composable network layers."""
+
+    def layer_decorated(self, *args, **kwargs):
+        # Automatically set a name if not provided.
+        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))
+        # Figure out the layer inputs.
+        if len(self.terminals) == 0:
+            raise RuntimeError('No input variables found for layer %s.' % name)
+        elif len(self.terminals) == 1:
+            layer_input = self.terminals[0]
+        else:
+            layer_input = list(self.terminals)
+        # Perform the operation and get the output.
+        layer_output = op(self, layer_input, *args, **kwargs)
+        # Add to layer LUT.
+        self.layers[name] = layer_output
+        # This output is now the input for the next layer.
+        self.feed(layer_output)
+        # Return self for chained calls.
+        return self
+
+    return layer_decorated
+
+
+class Network(object):
+    def __init__(self, inputs, trainable=True):
+        # The input nodes for this network
+        self.inputs = inputs
+        # The current list of terminal nodes
+        self.terminals = []
+        # Mapping from layer names to layers
+        self.layers = dict(inputs)
+        # If true, the resulting variables are set as trainable
+        self.trainable = trainable
+
+        self.setup()
+
+    def setup(self):
+        """Construct the network. """
+        raise NotImplementedError('Must be implemented by the subclass.')
+
+    def load(self, model_path, session, ignore_missing=False):
+        """Load network weights.
+        model_path: The path to the numpy-serialized network weights
+        session: The current TensorFlow session
+        ignore_missing: If true, serialized weights for missing layers are
+                        ignored.
+        """
+        # pylint: disable=no-member
+        data_dict = np.load(model_path, encoding='latin1').item()
+
+        for op_name in data_dict:
+            with tf.variable_scope(op_name, reuse=True):
+                for param_name, data in iteritems(data_dict[op_name]):
+                    try:
+                        var = tf.get_variable(param_name)
+                        session.run(var.assign(data))
+                    except ValueError:
+                        if not ignore_missing:
+                            raise
+
+    def feed(self, *args):
+        """Set the input(s) for the next operation by replacing the terminal nodes.
+        The arguments can be either layer names or the actual layers.
+        """
+        assert len(args) != 0
+        self.terminals = []
+        for fed_layer in args:
+            if isinstance(fed_layer, string_types):
+                try:
+                    fed_layer = self.layers[fed_layer]
+                except KeyError:
+                    raise KeyError('Unknown layer name fed: %s' % fed_layer)
+            self.terminals.append(fed_layer)
+        return self
+
+    def get_output(self):
+        """Returns the current network output."""
+        return self.terminals[-1]
+
+    def get_unique_name(self, prefix):
+        """Returns an index-suffixed unique name for the given prefix.
+        This is used for auto-generating layer names based on the type-prefix.
+        """
+        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1
+        return '%s_%d' % (prefix, ident)
+
+    def make_var(self, name, shape):
+        """Creates a new TensorFlow variable."""
+        return tf.get_variable(name, shape, trainable=self.trainable)
+
+    def validate_padding(self, padding):
+        """Verifies that the padding is one of the supported ones."""
+        assert padding in ('SAME', 'VALID')
+
+    @layer
+    def conv(self,
+             inp,
+             k_h,
+             k_w,
+             c_o,
+             s_h,
+             s_w,
+             name,
+             relu=True,
+             padding='SAME',
+             group=1,
+             biased=True):
+        # Verify that the padding is acceptable
+        self.validate_padding(padding)
+        # Get the number of channels in the input
+        c_i = int(inp.get_shape()[-1])
+        # Verify that the grouping parameter is valid
+        assert c_i % group == 0
+        assert c_o % group == 0
+        # Convolution for a given input and kernel
+        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1],
+                                             padding=padding)
+        with tf.variable_scope(name) as scope:
+            kernel = self.make_var('weights',
+                                   shape=[k_h, k_w, c_i // group, c_o])
+            # This is the common-case. Convolve the input without any
+            # further complications.
+            output = convolve(inp, kernel)
+            # Add the biases
+            if biased:
+                biases = self.make_var('biases', [c_o])
+                output = tf.nn.bias_add(output, biases)
+            if relu:
+                # ReLU non-linearity
+                output = tf.nn.relu(output, name=scope.name)
+            return output
+
+    @layer
+    def prelu(self, inp, name):
+        with tf.variable_scope(name):
+            i = int(inp.get_shape()[-1])
+            alpha = self.make_var('alpha', shape=(i,))
+            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))
+        return output
+
+    @layer
+    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding='SAME'):
+        self.validate_padding(padding)
+        return tf.nn.max_pool(inp,
+                              ksize=[1, k_h, k_w, 1],
+                              strides=[1, s_h, s_w, 1],
+                              padding=padding,
+                              name=name)
+
+    @layer
+    def fc(self, inp, num_out, name, relu=True):
+        with tf.variable_scope(name):
+            input_shape = inp.get_shape()
+            if input_shape.ndims == 4:
+                # The input is spatial. Vectorize it first.
+                dim = 1
+                for d in input_shape[1:].as_list():
+                    dim *= int(d)
+                feed_in = tf.reshape(inp, [-1, dim])
+            else:
+                feed_in, dim = (inp, input_shape[-1].value)
+            weights = self.make_var('weights', shape=[dim, num_out])
+            biases = self.make_var('biases', [num_out])
+            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b
+            fc = op(feed_in, weights, biases, name=name)
+            return fc
+
+    # Multi dimensional softmax,
+    # refer to https://github.com/tensorflow/tensorflow/issues/210
+    # compute softmax along the dimension of target
+    # the native softmax only supports batch_size x dimension
+    @layer
+    def softmax(self, target, axis, name=None):
+        max_axis = tf.reduce_max(target, axis, keepdims=True)
+        target_exp = tf.exp(target-max_axis)
+        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)
+        softmax = tf.div(target_exp, normalize, name)
+        return softmax
+
+
+class PNet(Network):
+    def setup(self):
+        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
+         .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')
+         .prelu(name='PReLU1')
+         .max_pool(2, 2, 2, 2, name='pool1')
+         .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')
+         .prelu(name='PReLU2')
+         .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')
+         .prelu(name='PReLU3')
+         .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')
+         .softmax(3, name='prob1'))
+
+        (self.feed('PReLU3')  # pylint: disable=no-value-for-parameter
+         .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))
+
+
+class RNet(Network):
+    def setup(self):
+        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
+         .conv(3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')
+         .prelu(name='prelu1')
+         .max_pool(3, 3, 2, 2, name='pool1')
+         .conv(3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')
+         .prelu(name='prelu2')
+         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
+         .conv(2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')
+         .prelu(name='prelu3')
+         .fc(128, relu=False, name='conv4')
+         .prelu(name='prelu4')
+         .fc(2, relu=False, name='conv5-1')
+         .softmax(1, name='prob1'))
+
+        (self.feed('prelu4')  # pylint: disable=no-value-for-parameter
+         .fc(4, relu=False, name='conv5-2'))
+
+
+class ONet(Network):
+    def setup(self):
+        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
+         .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')
+         .prelu(name='prelu1')
+         .max_pool(3, 3, 2, 2, name='pool1')
+         .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')
+         .prelu(name='prelu2')
+         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
+         .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')
+         .prelu(name='prelu3')
+         .max_pool(2, 2, 2, 2, name='pool3')
+         .conv(2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')
+         .prelu(name='prelu4')
+         .fc(256, relu=False, name='conv5')
+         .prelu(name='prelu5')
+         .fc(2, relu=False, name='conv6-1')
+         .softmax(1, name='prob1'))
+
+        (self.feed('prelu5')  # pylint: disable=no-value-for-parameter
+         .fc(4, relu=False, name='conv6-2'))
+
+        (self.feed('prelu5')  # pylint: disable=no-value-for-parameter
+         .fc(10, relu=False, name='conv6-3'))
+
+
+def create_mtcnn(sess, model_path):
+    """ Create the network """
+    if not model_path:
+        model_path, _ = os.path.split(os.path.realpath(__file__))
+
+    with tf.variable_scope('pnet'):
+        data = tf.placeholder(tf.float32, (None, None, None, 3), 'input')
+        pnet = PNet({'data': data})
+        pnet.load(os.path.join(model_path, 'det1.npy'), sess)
+    with tf.variable_scope('rnet'):
+        data = tf.placeholder(tf.float32, (None, 24, 24, 3), 'input')
+        rnet = RNet({'data': data})
+        rnet.load(os.path.join(model_path, 'det2.npy'), sess)
+    with tf.variable_scope('onet'):
+        data = tf.placeholder(tf.float32, (None, 48, 48, 3), 'input')
+        onet = ONet({'data': data})
+        onet.load(os.path.join(model_path, 'det3.npy'), sess)
+
+    pnet_fun = lambda img: sess.run(('pnet/conv4-2/BiasAdd:0',
+                                     'pnet/prob1:0'),
+                                    feed_dict={'pnet/input:0': img})
+    rnet_fun = lambda img: sess.run(('rnet/conv5-2/conv5-2:0',
+                                     'rnet/prob1:0'),
+                                    feed_dict={'rnet/input:0': img})
+    onet_fun = lambda img: sess.run(('onet/conv6-2/conv6-2:0',
+                                     'onet/conv6-3/conv6-3:0',
+                                     'onet/prob1:0'),
+                                    feed_dict={'onet/input:0': img})
+    return pnet_fun, rnet_fun, onet_fun
+
+
+def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):
+    """Detects faces in an image, and returns bounding boxes and points for them.
+    img: input image
+    minsize: minimum faces' size
+    pnet, rnet, onet: caffemodel
+    threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
+    factor: the factor used to create a scaling pyramid of face sizes to
+            detect in the image.
+    """
+    factor_count = 0
+    total_boxes = np.empty((0, 9))
+    points = np.empty(0)
+    h = img.shape[0]
+    w = img.shape[1]
+    minl = np.amin([h, w])
+    m = 12.0/minsize
+    minl = minl*m
+    # create scale pyramid
+    scales = []
+    while minl >= 12:
+        scales += [m*np.power(factor, factor_count)]
+        minl = minl*factor
+        factor_count += 1
+
+    # # # # # # # # # # # # #
+    # first stage - fast proposal network (pnet) to obtain face candidates
+    # # # # # # # # # # # # #
+
+    for scale in scales:
+        hs = int(np.ceil(h*scale))
+        ws = int(np.ceil(w*scale))
+        im_data = imresample(img, (hs, ws))
+        im_data = (im_data-127.5)*0.0078125
+        img_x = np.expand_dims(im_data, 0)
+        img_y = np.transpose(img_x, (0, 2, 1, 3))
+        out = pnet(img_y)
+        out0 = np.transpose(out[0], (0, 2, 1, 3))
+        out1 = np.transpose(out[1], (0, 2, 1, 3))
+
+        boxes, _ = generateBoundingBox(out1[0, :, :, 1].copy(),
+                                       out0[0, :, :, :].copy(),
+                                       scale, threshold[0])
+
+        # inter-scale nms
+        pick = nms(boxes.copy(), 0.5, 'Union')
+        if boxes.size > 0 and pick.size > 0:
+            boxes = boxes[pick, :]
+            total_boxes = np.append(total_boxes, boxes, axis=0)
+
+    numbox = total_boxes.shape[0]
+    if numbox > 0:
+        pick = nms(total_boxes.copy(), 0.7, 'Union')
+        total_boxes = total_boxes[pick, :]
+        regw = total_boxes[:, 2]-total_boxes[:, 0]
+        regh = total_boxes[:, 3]-total_boxes[:, 1]
+        qq1 = total_boxes[:, 0]+total_boxes[:, 5]*regw
+        qq2 = total_boxes[:, 1]+total_boxes[:, 6]*regh
+        qq3 = total_boxes[:, 2]+total_boxes[:, 7]*regw
+        qq4 = total_boxes[:, 3]+total_boxes[:, 8]*regh
+        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4,
+                                              total_boxes[:, 4]]))
+        total_boxes = rerec(total_boxes.copy())
+        total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)
+        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(),
+                                                         w, h)
+
+    numbox = total_boxes.shape[0]
+
+    # # # # # # # # # # # # #
+    # second stage - refinement of face candidates with rnet
+    # # # # # # # # # # # # #
+
+    if numbox > 0:
+        tempimg = np.zeros((24, 24, 3, numbox))
+        for k in range(0, numbox):
+            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))
+            tmp[dy[k]-1:edy[k], dx[k]-1:edx[k], :] = img[y[k]-1:ey[k],
+                                                         x[k]-1:ex[k], :]
+            if (tmp.shape[0] > 0 and tmp.shape[1] > 0 or
+                    tmp.shape[0] == 0 and tmp.shape[1] == 0):
+                tempimg[:, :, :, k] = imresample(tmp, (24, 24))
+            else:
+                return np.empty()
+        tempimg = (tempimg-127.5)*0.0078125
+        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))
+        out = rnet(tempimg1)
+        out0 = np.transpose(out[0])
+        out1 = np.transpose(out[1])
+        score = out1[1, :]
+        ipass = np.where(score > threshold[1])
+        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(),
+                                 np.expand_dims(score[ipass].copy(), 1)])
+        mv = out0[:, ipass[0]]
+        if total_boxes.shape[0] > 0:
+            pick = nms(total_boxes, 0.7, 'Union')
+            total_boxes = total_boxes[pick, :]
+            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:, pick]))
+            total_boxes = rerec(total_boxes.copy())
+
+    numbox = total_boxes.shape[0]
+
+    # # # # # # # # # # # # #
+    # third stage - further refinement and facial landmarks positions with onet
+    # NB: Facial landmarks code commented out for faceswap
+    # # # # # # # # # # # # #
+
+    if numbox > 0:
+        # third stage
+        total_boxes = np.fix(total_boxes).astype(np.int32)
+        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(),
+                                                         w, h)
+        tempimg = np.zeros((48, 48, 3, numbox))
+        for k in range(0, numbox):
+            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))
+            tmp[dy[k]-1:edy[k], dx[k]-1:edx[k], :] = img[y[k]-1:ey[k],
+                                                         x[k]-1:ex[k], :]
+            if (tmp.shape[0] > 0 and tmp.shape[1] > 0 or
+                    tmp.shape[0] == 0 and tmp.shape[1] == 0):
+                tempimg[:, :, :, k] = imresample(tmp, (48, 48))
+            else:
+                return np.empty()
+        tempimg = (tempimg-127.5)*0.0078125
+        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))
+        out = onet(tempimg1)
+        out0 = np.transpose(out[0])
+        out1 = np.transpose(out[1])
+        out2 = np.transpose(out[2])
+        score = out2[1, :]
+        points = out1
+        ipass = np.where(score > threshold[2])
+        points = points[:, ipass[0]]
+        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(),
+                                 np.expand_dims(score[ipass].copy(), 1)])
+        mv = out0[:, ipass[0]]
+
+        w = total_boxes[:, 2]-total_boxes[:, 0]+1
+        h = total_boxes[:, 3]-total_boxes[:, 1]+1
+        points[0:5, :] = (np.tile(w, (5, 1))*points[0:5, :] +
+                          np.tile(total_boxes[:, 0], (5, 1))-1)
+        points[5:10, :] = (np.tile(h, (5, 1))*points[5:10, :] +
+                           np.tile(total_boxes[:, 1], (5, 1))-1)
+        if total_boxes.shape[0] > 0:
+            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))
+            pick = nms(total_boxes.copy(), 0.7, 'Min')
+            total_boxes = total_boxes[pick, :]
+            points = points[:, pick]
+
+    return total_boxes, points
+
+
+# function [boundingbox] = bbreg(boundingbox,reg)
+def bbreg(boundingbox, reg):
+    """Calibrate bounding boxes"""
+    if reg.shape[1] == 1:
+        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))
+
+    w = boundingbox[:, 2]-boundingbox[:, 0]+1
+    h = boundingbox[:, 3]-boundingbox[:, 1]+1
+    b1 = boundingbox[:, 0]+reg[:, 0]*w
+    b2 = boundingbox[:, 1]+reg[:, 1]*h
+    b3 = boundingbox[:, 2]+reg[:, 2]*w
+    b4 = boundingbox[:, 3]+reg[:, 3]*h
+    boundingbox[:, 0:4] = np.transpose(np.vstack([b1, b2, b3, b4]))
+    return boundingbox
+
+
+def generateBoundingBox(imap, reg, scale, t):
+    """Use heatmap to generate bounding boxes"""
+    stride = 2
+    cellsize = 12
+
+    imap = np.transpose(imap)
+    dx1 = np.transpose(reg[:, :, 0])
+    dy1 = np.transpose(reg[:, :, 1])
+    dx2 = np.transpose(reg[:, :, 2])
+    dy2 = np.transpose(reg[:, :, 3])
+    y, x = np.where(imap >= t)
+    if y.shape[0] == 1:
+        dx1 = np.flipud(dx1)
+        dy1 = np.flipud(dy1)
+        dx2 = np.flipud(dx2)
+        dy2 = np.flipud(dy2)
+    score = imap[(y, x)]
+    reg = np.transpose(np.vstack([dx1[(y, x)], dy1[(y, x)],
+                                  dx2[(y, x)], dy2[(y, x)]]))
+    if reg.size == 0:
+        reg = np.empty((0, 3))
+    bb = np.transpose(np.vstack([y, x]))
+    q1 = np.fix((stride*bb+1)/scale)
+    q2 = np.fix((stride*bb+cellsize-1+1)/scale)
+    boundingbox = np.hstack([q1, q2, np.expand_dims(score, 1), reg])
+    return boundingbox, reg
+
+
+# function pick = nms(boxes,threshold,type)
+def nms(boxes, threshold, method):
+    if boxes.size == 0:
+        return np.empty((0, 3))
+    x1 = boxes[:, 0]
+    y1 = boxes[:, 1]
+    x2 = boxes[:, 2]
+    y2 = boxes[:, 3]
+    s = boxes[:, 4]
+    area = (x2-x1+1) * (y2-y1+1)
+    I = np.argsort(s)
+    pick = np.zeros_like(s, dtype=np.int16)
+    counter = 0
+    while I.size > 0:
+        i = I[-1]
+        pick[counter] = i
+        counter += 1
+        idx = I[0:-1]
+        xx1 = np.maximum(x1[i], x1[idx])
+        yy1 = np.maximum(y1[i], y1[idx])
+        xx2 = np.minimum(x2[i], x2[idx])
+        yy2 = np.minimum(y2[i], y2[idx])
+        w = np.maximum(0.0, xx2-xx1+1)
+        h = np.maximum(0.0, yy2-yy1+1)
+        inter = w * h
+        if method == 'Min':
+            o = inter / np.minimum(area[i], area[idx])
+        else:
+            o = inter / (area[i] + area[idx] - inter)
+        I = I[np.where(o <= threshold)]
+    pick = pick[0:counter]
+    return pick
+
+
+# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)
+def pad(total_boxes, w, h):
+    """Compute the padding coordinates (pad the bounding boxes to square)"""
+    tmpw = (total_boxes[:, 2]-total_boxes[:, 0]+1).astype(np.int32)
+    tmph = (total_boxes[:, 3]-total_boxes[:, 1]+1).astype(np.int32)
+    numbox = total_boxes.shape[0]
+
+    dx = np.ones((numbox), dtype=np.int32)
+    dy = np.ones((numbox), dtype=np.int32)
+    edx = tmpw.copy().astype(np.int32)
+    edy = tmph.copy().astype(np.int32)
+
+    x = total_boxes[:, 0].copy().astype(np.int32)
+    y = total_boxes[:, 1].copy().astype(np.int32)
+    ex = total_boxes[:, 2].copy().astype(np.int32)
+    ey = total_boxes[:, 3].copy().astype(np.int32)
+
+    tmp = np.where(ex > w)
+    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp], 1)
+    ex[tmp] = w
+
+    tmp = np.where(ey > h)
+    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp], 1)
+    ey[tmp] = h
+
+    tmp = np.where(x < 1)
+    dx.flat[tmp] = np.expand_dims(2-x[tmp], 1)
+    x[tmp] = 1
+
+    tmp = np.where(y < 1)
+    dy.flat[tmp] = np.expand_dims(2-y[tmp], 1)
+    y[tmp] = 1
+
+    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph
+
+
+# function [bboxA] = rerec(bboxA)
+def rerec(bboxA):
+    """Convert bboxA to square."""
+    h = bboxA[:, 3]-bboxA[:, 1]
+    w = bboxA[:, 2]-bboxA[:, 0]
+    l = np.maximum(w, h)
+    bboxA[:, 0] = bboxA[:, 0]+w*0.5-l*0.5
+    bboxA[:, 1] = bboxA[:, 1]+h*0.5-l*0.5
+    bboxA[:, 2:4] = bboxA[:, 0:2] + np.transpose(np.tile(l, (2, 1)))
+    return bboxA
+
+
+def imresample(img, size):
+    """ Resample image """
+    im_data = cv2.resize(img, (size[1], size[0]),
+                         interpolation=cv2.INTER_AREA)  # @UndefinedVariable
+    return im_data
diff --git a/plugins/Model_GAN/Model.py b/plugins/model/Model_GAN/Model.py
similarity index 100%
rename from plugins/Model_GAN/Model.py
rename to plugins/model/Model_GAN/Model.py
diff --git a/plugins/Model_GAN/Trainer.py b/plugins/model/Model_GAN/Trainer.py
similarity index 100%
rename from plugins/Model_GAN/Trainer.py
rename to plugins/model/Model_GAN/Trainer.py
diff --git a/plugins/Model_GAN/__init__.py b/plugins/model/Model_GAN/__init__.py
similarity index 100%
rename from plugins/Model_GAN/__init__.py
rename to plugins/model/Model_GAN/__init__.py
diff --git a/plugins/Model_GAN/instance_normalization.py b/plugins/model/Model_GAN/instance_normalization.py
similarity index 100%
rename from plugins/Model_GAN/instance_normalization.py
rename to plugins/model/Model_GAN/instance_normalization.py
diff --git a/plugins/Model_GAN128/Model.py b/plugins/model/Model_GAN128/Model.py
similarity index 100%
rename from plugins/Model_GAN128/Model.py
rename to plugins/model/Model_GAN128/Model.py
diff --git a/plugins/Model_GAN128/Trainer.py b/plugins/model/Model_GAN128/Trainer.py
similarity index 100%
rename from plugins/Model_GAN128/Trainer.py
rename to plugins/model/Model_GAN128/Trainer.py
diff --git a/plugins/Model_GAN128/__init__.py b/plugins/model/Model_GAN128/__init__.py
similarity index 100%
rename from plugins/Model_GAN128/__init__.py
rename to plugins/model/Model_GAN128/__init__.py
diff --git a/plugins/Model_GAN128/instance_normalization.py b/plugins/model/Model_GAN128/instance_normalization.py
similarity index 100%
rename from plugins/Model_GAN128/instance_normalization.py
rename to plugins/model/Model_GAN128/instance_normalization.py
diff --git a/plugins/Model_IAE/AutoEncoder.py b/plugins/model/Model_IAE/AutoEncoder.py
similarity index 100%
rename from plugins/Model_IAE/AutoEncoder.py
rename to plugins/model/Model_IAE/AutoEncoder.py
diff --git a/plugins/Model_IAE/Model.py b/plugins/model/Model_IAE/Model.py
similarity index 100%
rename from plugins/Model_IAE/Model.py
rename to plugins/model/Model_IAE/Model.py
diff --git a/plugins/Model_IAE/Trainer.py b/plugins/model/Model_IAE/Trainer.py
similarity index 100%
rename from plugins/Model_IAE/Trainer.py
rename to plugins/model/Model_IAE/Trainer.py
diff --git a/plugins/Model_IAE/__init__.py b/plugins/model/Model_IAE/__init__.py
similarity index 100%
rename from plugins/Model_IAE/__init__.py
rename to plugins/model/Model_IAE/__init__.py
diff --git a/plugins/Model_LowMem/AutoEncoder.py b/plugins/model/Model_LowMem/AutoEncoder.py
similarity index 100%
rename from plugins/Model_LowMem/AutoEncoder.py
rename to plugins/model/Model_LowMem/AutoEncoder.py
diff --git a/plugins/Model_LowMem/Model.py b/plugins/model/Model_LowMem/Model.py
similarity index 100%
rename from plugins/Model_LowMem/Model.py
rename to plugins/model/Model_LowMem/Model.py
diff --git a/plugins/Model_LowMem/Trainer.py b/plugins/model/Model_LowMem/Trainer.py
similarity index 100%
rename from plugins/Model_LowMem/Trainer.py
rename to plugins/model/Model_LowMem/Trainer.py
diff --git a/plugins/Model_LowMem/__init__.py b/plugins/model/Model_LowMem/__init__.py
similarity index 100%
rename from plugins/Model_LowMem/__init__.py
rename to plugins/model/Model_LowMem/__init__.py
diff --git a/plugins/Model_Original/AutoEncoder.py b/plugins/model/Model_Original/AutoEncoder.py
similarity index 100%
rename from plugins/Model_Original/AutoEncoder.py
rename to plugins/model/Model_Original/AutoEncoder.py
diff --git a/plugins/Model_Original/Model.py b/plugins/model/Model_Original/Model.py
similarity index 100%
rename from plugins/Model_Original/Model.py
rename to plugins/model/Model_Original/Model.py
diff --git a/plugins/Model_Original/Trainer.py b/plugins/model/Model_Original/Trainer.py
similarity index 100%
rename from plugins/Model_Original/Trainer.py
rename to plugins/model/Model_Original/Trainer.py
diff --git a/plugins/Model_Original/__init__.py b/plugins/model/Model_Original/__init__.py
similarity index 100%
rename from plugins/Model_Original/__init__.py
rename to plugins/model/Model_Original/__init__.py
diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/model/Model_OriginalHighRes/Model.py
similarity index 100%
rename from plugins/Model_OriginalHighRes/Model.py
rename to plugins/model/Model_OriginalHighRes/Model.py
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/model/Model_OriginalHighRes/Trainer.py
similarity index 100%
rename from plugins/Model_OriginalHighRes/Trainer.py
rename to plugins/model/Model_OriginalHighRes/Trainer.py
diff --git a/plugins/Model_OriginalHighRes/__init__.py b/plugins/model/Model_OriginalHighRes/__init__.py
similarity index 100%
rename from plugins/Model_OriginalHighRes/__init__.py
rename to plugins/model/Model_OriginalHighRes/__init__.py
diff --git a/plugins/Model_OriginalHighRes/_version.py b/plugins/model/Model_OriginalHighRes/_version.py
similarity index 100%
rename from plugins/Model_OriginalHighRes/_version.py
rename to plugins/model/Model_OriginalHighRes/_version.py
diff --git a/plugins/Model_OriginalHighRes/instance_normalization.py b/plugins/model/Model_OriginalHighRes/instance_normalization.py
similarity index 100%
rename from plugins/Model_OriginalHighRes/instance_normalization.py
rename to plugins/model/Model_OriginalHighRes/instance_normalization.py
diff --git a/plugins/model/__init__.py b/plugins/model/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/plugin_loader.py b/plugins/plugin_loader.py
new file mode 100644
index 0000000..a1316cb
--- /dev/null
+++ b/plugins/plugin_loader.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python3
+""" Plugin loader for extract, training and model tasks """
+
+import os
+from importlib import import_module
+
+
+class PluginLoader():
+    """ Plugin loader for extract, training and model tasks """
+    @staticmethod
+    def get_detector(name):
+        """ Return requested detector plugin """
+        return PluginLoader._import("extract.detect", name)
+
+    @staticmethod
+    def get_aligner(name):
+        """ Return requested detector plugin """
+        return PluginLoader._import("extract.align", name)
+
+    @staticmethod
+    def get_converter(name):
+        """ Return requested converter plugin """
+        return PluginLoader._import("Convert", "Convert_{0}".format(name))
+
+    @staticmethod
+    def get_model(name):
+        """ Return requested model plugin """
+        return PluginLoader._import("Model", "Model_{0}".format(name))
+
+    @staticmethod
+    def get_trainer(name):
+        """ Return requested trainer plugin """
+        return PluginLoader._import("Trainer", "Model_{0}".format(name))
+
+    @staticmethod
+    def _import(attr, name):
+        """ Import the plugin's module """
+        ttl = attr.split(".")[-1].title()
+        print("Loading {} from {} plugin...".format(ttl, name.title()))
+        attr = "model" if attr == "Trainer" else attr.lower()
+        mod = ".".join(("plugins", attr, name))
+        module = import_module(mod)
+        return getattr(module, ttl)
+
+    @staticmethod
+    def get_available_models():
+        """ Return a list of available models """
+        models = ()
+        modelpath = os.path.join(os.path.dirname(__file__), "model")
+        for modeldir in next(os.walk(modelpath))[1]:
+            if modeldir[0:6].lower() == 'model_':
+                models += (modeldir[6:],)
+        return models
+
+    @staticmethod
+    def get_available_extractors(extractor_type):
+        """ Return a list of available models """
+        extractpath = os.path.join(os.path.dirname(__file__),
+                                   "extract",
+                                   extractor_type)
+        extractors = sorted(item.name.replace(".py", "").replace("_", "-")
+                            for item in os.scandir(extractpath)
+                            if not item.name.startswith("_")
+                            and item.name.endswith(".py")
+                            and item.name != "manual.py")
+        return extractors
+
+    @staticmethod
+    def get_default_model():
+        """ Return the default model """
+        models = PluginLoader.get_available_models()
+        return 'Original' if 'Original' in models else models[0]
diff --git a/requirements.txt b/requirements.txt
index f4b9b6c..457db96 100755
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,10 +1,10 @@
 pathlib==1.0.1
 scandir==1.7
 h5py==2.8.0
-Keras==2.2.2
-opencv-python==3.4.1.15
+Keras==2.2.4
+opencv-python
 scikit-image
-scikit-learn==0.20.0
+scikit-learn
 face_recognition
 cmake
 dlib
diff --git a/scripts/convert.py b/scripts/convert.py
index 34a8c2b..b2ca7a7 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -6,24 +6,34 @@ import os
 import sys
 from pathlib import Path
 
+import cv2
+import numpy as np
 from tqdm import tqdm
 
-from scripts.fsmedia import Alignments, Images, Faces, Utils
-from scripts.extract import Extract
-from lib.utils import BackgroundGenerator, get_folder, get_image_paths
+from scripts.fsmedia import Alignments, Images, PostProcess, Utils
+from lib.faces_detect import DetectedFace
+from lib.multithreading import BackgroundGenerator, SpawnProcess, queue_manager
+from lib.utils import get_folder, get_image_paths
 
-from plugins.PluginLoader import PluginLoader
+from plugins.plugin_loader import PluginLoader
 
 
-class Convert(object):
+class Convert():
     """ The convert process. """
     def __init__(self, arguments):
         self.args = arguments
         self.output_dir = get_folder(self.args.output_dir)
+        self.extract_faces = False
+        self.faces_count = 0
 
         self.images = Images(self.args)
-        self.faces = Faces(self.args)
-        self.alignments = Alignments(self.args)
+        self.alignments = Alignments(self.args, False)
+
+        # Legacy rotation conversion
+        Rotate(self.alignments, self.args.verbose, self.images.input_images)
+
+        self.post_process = PostProcess(arguments)
+        self.verify_output = False
 
         self.opts = OptionalActions(self.args, self.images.input_images)
 
@@ -35,9 +45,7 @@ class Convert(object):
         Utils.set_verbosity(self.args.verbose)
 
         if not self.alignments.have_alignments_file:
-            self.generate_alignments()
-
-        self.faces.faces_detected = self.alignments.read_alignments()
+            self.load_extractor()
 
         model = self.load_model()
         converter = self.load_converter(model)
@@ -47,17 +55,46 @@ class Convert(object):
         for item in batch.iterator():
             self.convert(converter, item)
 
-        Utils.finalize(self.images.images_found,
-                       self.faces.num_faces_detected,
-                       self.faces.verify_output)
+        if self.extract_faces:
+            queue_manager.terminate_queues()
 
-    def generate_alignments(self):
-        """ Generate an alignments file if one does not already
-        exist. Does not save extracted faces """
-        print('Alignments file not found. Generating at default values...')
-        extract = Extract(self.args)
-        extract.export_face = False
-        extract.process()
+        Utils.finalize(self.images.images_found,
+                       self.faces_count,
+                       self.verify_output)
+
+    def load_extractor(self):
+        """ Set on the fly extraction """
+        print("\nNo Alignments file found. Extracting on the fly.\n"
+              "NB: This will use the inferior dlib-hog for extraction "
+              "and dlib pose predictor for landmarks.\nIt is recommended "
+              "to perfom Extract first for superior results\n")
+        for task in ("load", "detect", "align"):
+            queue_manager.add_queue(task, maxsize=0)
+
+        detector = PluginLoader.get_detector("dlib_hog")(
+            verbose=self.args.verbose)
+        aligner = PluginLoader.get_aligner("dlib")(verbose=self.args.verbose)
+
+        d_kwargs = {"in_queue": queue_manager.get_queue("load"),
+                    "out_queue": queue_manager.get_queue("detect")}
+        a_kwargs = {"in_queue": queue_manager.get_queue("detect"),
+                    "out_queue": queue_manager.get_queue("align")}
+
+        d_process = SpawnProcess()
+        d_event = d_process.event
+        a_process = SpawnProcess()
+        a_event = a_process.event
+
+        d_process.in_process(detector.detect_faces, **d_kwargs)
+        a_process.in_process(aligner.align, **a_kwargs)
+        d_event.wait(10)
+        if not d_event.is_set():
+            raise ValueError("Error inititalizing Detector")
+        a_event.wait(10)
+        if not a_event.is_set():
+            raise ValueError("Error inititalizing Aligner")
+
+        self.extract_faces = True
 
     def load_model(self):
         """ Load the model requested for conversion """
@@ -89,29 +126,75 @@ class Convert(object):
             erosion_kernel_size=args.erosion_kernel_size,
             match_histogram=args.match_histogram,
             smooth_mask=args.smooth_mask,
-            avg_color_adjust=args.avg_color_adjust)
+            avg_color_adjust=args.avg_color_adjust,
+            draw_transparent=args.draw_transparent)
 
         return converter
 
     def prepare_images(self):
         """ Prepare the images for conversion """
         filename = ""
-        for filename in tqdm(self.images.input_images, file=sys.stdout):
-            if not self.check_alignments(filename):
+        for filename in tqdm(self.images.input_images,
+                             total=self.images.images_found,
+                             file=sys.stdout):
+
+            if (self.args.discard_frames and
+                    self.opts.check_skipframe(filename) == "discard"):
                 continue
-            image = Utils.cv2_read_write('read', filename)
-            faces = self.faces.get_faces_alignments(filename, image)
-            if not faces:
+
+            frame = os.path.basename(filename)
+            if self.extract_faces:
+                convert_item = self.detect_faces(filename)
+            else:
+                convert_item = self.alignments_faces(filename, frame)
+
+            if not convert_item:
                 continue
+            image, detected_faces = convert_item
+
+            faces_count = len(detected_faces)
+            if faces_count != 0:
+                # Post processing requires a dict with "detected_faces" key
+                self.post_process.do_actions({"detected_faces": detected_faces})
+                self.faces_count += faces_count
+
+            if faces_count > 1:
+                self.verify_output = True
+                if self.args.verbose:
+                    print("Warning: found more than one face in "
+                          "an image! {}".format(frame))
+
+            yield filename, image, detected_faces
+
+    def detect_faces(self, filename):
+        """ Extract the face from a frame (If not alignments file found) """
+        image = self.images.load_one_image(filename)
+        queue_manager.get_queue("load").put((filename, image))
+        item = queue_manager.get_queue("align").get()
+        detected_faces = item["detected_faces"]
+        return image, detected_faces
+
+    def alignments_faces(self, filename, frame):
+        """ Get the face from alignments file """
+        if not self.check_alignments(frame):
+            return None
+
+        faces = self.alignments.get_alignments_for_frame(frame)
+        image = self.images.load_one_image(filename)
+        detected_faces = list()
 
-            yield filename, image, faces
+        for rawface in faces:
+            face = DetectedFace()
+            face.from_alignment(rawface, image=image)
+            detected_faces.append(face)
+        return image, detected_faces
 
-    def check_alignments(self, filename):
+    def check_alignments(self, frame):
         """ If we have no alignments for this image, skip it """
-        have_alignments = self.faces.have_face(filename)
+        have_alignments = self.alignments.frame_exists(frame)
         if not have_alignments:
             tqdm.write("No alignment found for {}, "
-                       "skipping".format(os.path.basename(filename)))
+                       "skipping".format(frame))
         return have_alignments
 
     def convert(self, converter, item):
@@ -121,15 +204,15 @@ class Convert(object):
             skip = self.opts.check_skipframe(filename)
 
             if not skip:
-                for idx, face in faces:
+                for idx, face in enumerate(faces):
                     image = self.convert_one_face(converter,
                                                   (filename, image, idx, face))
-            if skip != "discard":
                 filename = str(self.output_dir / Path(filename).name)
-                Utils.cv2_read_write('write', filename, image)
+                cv2.imwrite(filename, image)
         except Exception as err:
             print("Failed to convert image: {}. "
                   "Reason: {}".format(filename, err))
+            raise
 
     def convert_one_face(self, converter, imagevars):
         """ Perform the conversion on the given frame for a single face """
@@ -138,24 +221,18 @@ class Convert(object):
         if self.opts.check_skipface(filename, idx):
             return image
 
-        # Rotating an image is legacy code. Landmarks are now
-        # rotated at extract stage. For newer extracts face.r
-        # will always be zero.
-        image = self.images.rotate_image(image, face.r)
         # TODO: This switch between 64 and 128 is a hack for now.
         # We should have a separate cli option for size
-        
         size = 128 if (self.args.trainer.strip().lower()
                        in ('gan128', 'originalhighres')) else 64
 
         image = converter.patch_image(image,
                                       face,
                                       size)
-        image = self.images.rotate_image(image, face.r, reverse=True)
         return image
 
 
-class OptionalActions(object):
+class OptionalActions():
     """ Process the optional actions for convert """
 
     def __init__(self, args, input_images):
@@ -229,3 +306,33 @@ class OptionalActions(object):
             print("face {} for frame {} was deleted, skipping".format(
                 face_idx, os.path.basename(filename)))
         return skip_face
+
+
+class Rotate():
+    """ Rotate landmarks and bounding boxes on legacy alignments
+        and remove the 'r' parameter """
+    def __init__(self, alignments, verbose, frames):
+        self.verbose = verbose
+        self.alignments = alignments
+        self.frames = {os.path.basename(frame): frame
+                       for frame in frames}
+        self.process()
+
+    def process(self):
+        """ Run the rotate alignments process """
+        rotated = self.alignments.get_legacy_frames()
+        if not rotated:
+            return
+        print("Legacy rotated frames found. Converting...")
+        self.rotate_landmarks(rotated)
+        self.alignments.save()
+
+    def rotate_landmarks(self, rotated):
+        """ Rotate the landmarks """
+        for rotate_item in tqdm(rotated,
+                                desc="Rotating Landmarks"):
+            if rotate_item not in self.frames.keys():
+                continue
+            filename = self.frames[rotate_item]
+            dims = cv2.imread(filename).shape[:2]
+            self.alignments.rotate_existing_landmarks(rotate_item, dims)
diff --git a/scripts/extract.py b/scripts/extract.py
index 84e4ca8..c3ef112 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -1,16 +1,20 @@
 #!/usr/bin python3
 """ The script to run the extract process of faceswap """
+# TODO S3FD Detector
 
 import os
 import sys
 from pathlib import Path
 
+import cv2
 from tqdm import tqdm
 
 from lib.gpu_stats import GPUStats
-from lib.multithreading import pool_process
-from lib.utils import rotate_image_by_angle, rotate_landmarks
-from scripts.fsmedia import Alignments, Faces, Images, Utils
+from lib.multithreading import (MultiThread, PoolProcess, QueueEmpty,
+                                SpawnProcess, queue_manager)
+from lib.utils import get_folder
+from plugins.plugin_loader import PluginLoader
+from scripts.fsmedia import Alignments, Images, PostProcess, Utils
 
 tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning
 
@@ -20,14 +24,16 @@ class Extract():
 
     def __init__(self, arguments):
         self.args = arguments
-
+        self.output_dir = get_folder(self.args.output_dir)
+        print("Output Directory: {}".format(self.args.output_dir))
         self.images = Images(self.args)
-        self.faces = Faces(self.args)
-        self.alignments = Alignments(self.args)
+        self.alignments = Alignments(self.args, True)
+        self.plugins = Plugins(self.args)
 
-        self.output_dir = self.faces.output_dir
+        self.post_process = PostProcess(arguments)
 
         self.export_face = True
+        self.verify_output = False
         self.save_interval = None
         if hasattr(self.args, "save_interval"):
             self.save_interval = self.args.save_interval
@@ -36,126 +42,319 @@ class Extract():
         """ Perform the extraction process """
         print('Starting, this may take a while...')
         Utils.set_verbosity(self.args.verbose)
+#        queue_manager.debug_monitor(1)
+        self.threaded_io("load")
+        save_thread = self.threaded_io("save")
+        self.run_extraction(save_thread)
+        self.alignments.save()
+        Utils.finalize(self.images.images_found,
+                       self.alignments.faces_count(),
+                       self.verify_output)
 
-        if (hasattr(self.args, 'multiprocess')
-                and self.args.multiprocess
-                and GPUStats().device_count == 0):
-            # TODO Checking that there is no available GPU is not
-            # necessarily an indicator of whether the user is actually
-            # using the CPU. Maybe look to implement further checks on
-            # dlib/tensorflow compilations
-            self.extract_multi_process()
-        else:
-            self.extract_single_process()
+    def threaded_io(self, task, io_args=None):
+        """ Load images in a background thread """
+        io_args = tuple() if io_args is None else (io_args, )
+        if task == "load":
+            func = self.load_images
+        elif task == "save":
+            func = self.save_faces
+        elif task == "reload":
+            func = self.reload_images
+        io_thread = MultiThread(thread_count=1)
+        io_thread.in_thread(func, *io_args)
+        return io_thread
 
-        self.write_alignments()
-        images, faces = Utils.finalize(self.images.images_found,
-                                       self.faces.num_faces_detected,
-                                       self.faces.verify_output)
-        self.images.images_found = images
-        self.faces.num_faces_detected = faces
+    def load_images(self):
+        """ Load the images """
+        load_queue = queue_manager.get_queue("load")
+        for filename, image in self.images.load():
+            imagename = os.path.basename(filename)
+            if imagename in self.alignments.data.keys():
+                continue
+            load_queue.put((filename, image))
+        load_queue.put("EOF")
 
-    def write_alignments(self):
-        """ Save the alignments file """
-        self.alignments.write_alignments(self.faces.faces_detected)
+    def reload_images(self, detected_faces):
+        """ Reload the images and pair to detected face """
+        load_queue = queue_manager.get_queue("detect")
+        for filename, image in self.images.load():
+            detect_item = detected_faces.pop(filename, None)
+            if not detect_item:
+                continue
+            detect_item["image"] = image
+            load_queue.put(detect_item)
+        load_queue.put("EOF")
 
-    def extract_single_process(self):
-        """ Run extraction in a single process """
-        frame_no = 0
-        for filename in tqdm(self.images.input_images, file=sys.stdout):
-            filename, faces = self.process_single_image(filename)
-            self.faces.faces_detected[os.path.basename(filename)] = faces
-            frame_no += 1
-            if frame_no == self.save_interval:
-                self.write_alignments()
-                frame_no = 0
+    def save_faces(self):
+        """ Save the generated faces """
+        if not self.export_face:
+            return
 
-    def extract_multi_process(self):
-        """ Run the extraction on the correct number of processes """
+        save_queue = queue_manager.get_queue("save")
+        while True:
+            item = save_queue.get()
+            if item == "EOF":
+                break
+            filename, output_file, resized_face, idx = item
+            out_filename = "{}_{}{}".format(str(output_file),
+                                            str(idx),
+                                            Path(filename).suffix)
+            cv2.imwrite(out_filename, resized_face)
+
+    def run_extraction(self, save_thread):
+        """ Run Face Detection """
+        to_process = self.process_item_count()
         frame_no = 0
-        for filename, faces in tqdm(
-                pool_process(
-                    self.process_single_image,
-                    self.images.input_images),
-                total=self.images.images_found,
-                file=sys.stdout):
-            self.faces.num_faces_detected += 1
-            self.faces.faces_detected[os.path.basename(filename)] = faces
+        if self.plugins.is_parallel:
+            self.plugins.launch_aligner()
+            self.plugins.launch_detector()
+
+        if not self.plugins.is_parallel:
+            self.run_detection(to_process)
+            self.plugins.launch_aligner()
+
+        for faces in tqdm(self.plugins.detect_faces(extract_pass="align"),
+                          total=to_process,
+                          file=sys.stdout,
+                          desc="Extracting faces"):
+
+            exception = faces.get("exception", False)
+            if exception:
+                break
+            filename = faces["filename"]
+
+            faces["output_file"] = self.output_dir / Path(filename).stem
+
+            self.post_process.do_actions(faces)
+
+            faces_count = len(faces["detected_faces"])
+            if self.args.verbose and faces_count == 0:
+                print("Warning: No faces were detected in image: "
+                      "{}".format(os.path.basename(filename)))
+
+            if not self.verify_output and faces_count > 1:
+                self.verify_output = True
+
+            self.process_faces(filename, faces)
+
             frame_no += 1
             if frame_no == self.save_interval:
-                self.write_alignments()
+                self.alignments.save()
                 frame_no = 0
 
-    def process_single_image(self, filename):
-        """ Detect faces in an image. Rotate the image the specified amount
-            until at least one face is found, or until image rotations are
-            depleted.
-            Once at least one face has been detected, pass to
-            process_single_face to process the individual faces """
-        retval = filename, list()
-        try:
-            image = Utils.cv2_read_write('read', filename)
-
-            for angle in self.images.rotation_angles:
-                currentimage, rotation_matrix = rotate_image_by_angle(image,
-                                                                      angle)
-                faces = self.faces.get_faces(currentimage, angle)
-                process_faces = [[idx, face] for idx, face in faces]
-                if not process_faces:
-                    continue
-
-                if angle != 0 and self.args.verbose:
-                    print("found face(s) by rotating image "
-                          "{} degrees".format(angle))
-                if angle != 0:
-                    process_faces = [[idx,
-                                      rotate_landmarks(face, rotation_matrix)]
-                                     for idx, face in process_faces]
-
-                if process_faces:
-                    break
+        if self.export_face:
+            queue_manager.get_queue("save").put("EOF")
+        save_thread.join_threads()
 
-            final_faces = [self.process_single_face(idx,
-                                                    face,
-                                                    filename,
-                                                    image)
-                           for idx, face in process_faces]
+    def process_item_count(self):
+        """ Return the number of items to be processedd """
+        processed = sum(os.path.basename(frame) in self.alignments.data.keys()
+                        for frame in self.images.input_images)
 
-            retval = filename, final_faces
-        except Exception as err:
-            if self.args.verbose:
-                print("Failed to extract from image: "
-                      "{}. Reason: {}".format(filename, err))
-            raise
-        return retval
+        if processed != 0 and self.args.skip_existing:
+            print("Skipping {} previously extracted frames".format(processed))
+        if processed != 0 and self.args.skip_faces:
+            print("Skipping {} frames with detected faces".format(processed))
+
+        to_process = self.images.images_found - processed
+        if to_process == 0:
+            print("No frames to process. Exiting")
+            queue_manager.terminate_queues()
+            exit(0)
+        return to_process
+
+    def run_detection(self, to_process):
+        """ Run detection only """
+        self.plugins.launch_detector()
+        detected_faces = dict()
+        for detected in tqdm(self.plugins.detect_faces(extract_pass="detect"),
+                             total=to_process,
+                             file=sys.stdout,
+                             desc="Detecting faces"):
+            exception = detected.get("exception", False)
+            if exception:
+                break
+
+            del detected["image"]
+            filename = detected["filename"]
+
+            detected_faces[filename] = detected
 
-    def process_single_face(self, idx, face, filename, image):
+        self.threaded_io("reload", detected_faces)
+
+    def process_faces(self, filename, faces):
         """ Perform processing on found faces """
-        output_file = self.output_dir / Path(
-            filename).stem if self.export_face else None
+        final_faces = list()
+        save_queue = queue_manager.get_queue("save")
 
-        self.faces.draw_landmarks_on_face(face, image)
+        filename = faces["filename"]
+        output_file = faces["output_file"]
+        resized_faces = faces["resized_faces"]
 
-        resized_face, t_mat = self.faces.extractor.extract(
-            image,
-            face,
-            256,
-            self.faces.align_eyes)
+        for idx, face in enumerate(faces["detected_faces"]):
+            if self.export_face:
+                save_queue.put((filename,
+                                output_file,
+                                resized_faces[idx],
+                                idx))
 
-        blurry_file = self.faces.detect_blurry_faces(face,
-                                                     t_mat,
-                                                     resized_face,
-                                                     filename)
-        output_file = blurry_file if blurry_file else output_file
+            final_faces.append(face.to_alignment())
+        self.alignments.data[os.path.basename(filename)] = final_faces
 
-        if self.export_face:
-            filename = "{}_{}{}".format(str(output_file),
-                                        str(idx),
-                                        Path(filename).suffix)
-            Utils.cv2_read_write('write', filename, resized_face)
-
-        return {"x": face.x,
-                "w": face.w,
-                "y": face.y,
-                "h": face.h,
-                "landmarksXY": face.landmarks_as_xy()}
+
+class Plugins():
+    """ Detector and Aligner Plugins and queues """
+    def __init__(self, arguments):
+        self.args = arguments
+        self.detector = self.load_detector()
+        self.aligner = self.load_aligner()
+        self.is_parallel = self.set_parallel_processing()
+
+        self.add_queues()
+
+    def set_parallel_processing(self):
+        """ Set whether to run detect and align together or seperately """
+        detector_vram = self.detector.vram
+        aligner_vram = self.aligner.vram
+        gpu_stats = GPUStats()
+        if (detector_vram == 0
+                or aligner_vram == 0
+                or gpu_stats.device_count == 0):
+            return True
+
+        if hasattr(self.args, "multiprocess") and not self.args.multiprocess:
+            print("\nNB: Parallel processing disabled.\nYou may get faster "
+                  "extraction speeds by enabling it with the -mp switch\n")
+            return False
+
+        required_vram = detector_vram + aligner_vram + 320  # 320MB buffer
+        stats = gpu_stats.get_card_most_free()
+        free_vram = int(stats["free"])
+        if self.args.verbose:
+            print("{} - {}MB free of {}MB".format(stats["device"],
+                                                  free_vram,
+                                                  int(stats["total"])))
+        if free_vram <= required_vram:
+            if self.args.verbose:
+                print("Not enough free VRAM for parallel processing. "
+                      "Switching to serial")
+            return False
+        return True
+
+    def add_queues(self):
+        """ Add the required processing queues to Queue Manager """
+        for task in ("load", "detect", "align", "save"):
+            size = 0
+            if task == "load" or (not self.is_parallel and task == "detect"):
+                size = 100
+            queue_manager.add_queue(task, maxsize=size)
+
+    def load_detector(self):
+        """ Set global arguments and load detector plugin """
+        detector_name = self.args.detector.replace("-", "_").lower()
+
+        # Rotation
+        rotation = None
+        if hasattr(self.args, "rotate_images"):
+            rotation = self.args.rotate_images
+
+        detector = PluginLoader.get_detector(detector_name)(
+            verbose=self.args.verbose,
+            rotation=rotation)
+
+        return detector
+
+    def load_aligner(self):
+        """ Set global arguments and load aligner plugin """
+        aligner_name = self.args.aligner.replace("-", "_").lower()
+
+        # Align Eyes
+        align_eyes = False
+        if hasattr(self.args, 'align_eyes'):
+            align_eyes = self.args.align_eyes
+
+        # Extracted Face Size
+        size = 256
+        if hasattr(self.args, 'size'):
+            size = self.args.size
+
+        aligner = PluginLoader.get_aligner(aligner_name)(
+            verbose=self.args.verbose,
+            align_eyes=align_eyes,
+            size=size)
+
+        return aligner
+
+    def launch_aligner(self):
+        """ Launch the face aligner """
+        out_queue = queue_manager.get_queue("align")
+        kwargs = {"in_queue": queue_manager.get_queue("detect"),
+                  "out_queue": out_queue}
+
+        align_process = SpawnProcess()
+        event = align_process.event
+
+        align_process.in_process(self.aligner.align, **kwargs)
+
+        # Wait for Aligner to take it's VRAM
+        event.wait(60)
+        if not event.is_set():
+            raise ValueError("Error inititalizing Aligner")
+
+    def launch_detector(self):
+        """ Launch the face detector """
+        out_queue = queue_manager.get_queue("detect")
+        kwargs = {"in_queue": queue_manager.get_queue("load"),
+                  "out_queue": out_queue}
+        if self.args.detector == "mtcnn":
+            mtcnn_kwargs = self.detector.validate_kwargs(
+                self.get_mtcnn_kwargs())
+            kwargs["mtcnn_kwargs"] = mtcnn_kwargs
+
+        if self.detector.parent_is_pool:
+            detect_process = PoolProcess(self.detector.detect_faces)
+        else:
+            detect_process = SpawnProcess()
+
+        event = None
+        if hasattr(detect_process, "event"):
+            event = detect_process.event
+
+        detect_process.in_process(self.detector.detect_faces, **kwargs)
+
+        if not event:
+            return
+
+        event.wait(60)
+        if not event.is_set():
+            raise ValueError("Error inititalizing Detector")
+
+    def get_mtcnn_kwargs(self):
+        """ Add the mtcnn arguments into a kwargs dictionary """
+        mtcnn_threshold = [float(thr.strip())
+                           for thr in self.args.mtcnn_threshold]
+        return {"minsize": self.args.mtcnn_minsize,
+                "threshold": mtcnn_threshold,
+                "factor": self.args.mtcnn_scalefactor}
+
+    def detect_faces(self, extract_pass="detect"):
+        """ Detect faces from in an image """
+        if self.is_parallel or extract_pass == "align":
+            out_queue = queue_manager.get_queue("align")
+        if not self.is_parallel and extract_pass == "detect":
+            out_queue = queue_manager.get_queue("detect")
+
+        while True:
+            try:
+                faces = out_queue.get(True, 1)
+                if faces == "EOF":
+                    break
+                exception = faces.get("exception", None)
+                if exception is not None:
+                    queue_manager.terminate_queues()
+                    yield faces
+                    break
+            except QueueEmpty:
+                continue
+
+            yield faces
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index 975303f..f48338d 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -11,13 +11,12 @@ from pathlib import Path
 import cv2
 import numpy as np
 
-from lib.detect_blur import is_blurry
 from lib import Serializer
-from lib.faces_detect import detect_faces, DetectedFace
-from lib.FaceFilter import FaceFilter
-from lib.utils import (get_folder, get_image_paths, rotate_image_by_angle,
-                       set_system_verbosity)
-from plugins.PluginLoader import PluginLoader
+from lib.detect_blur import is_blurry
+from lib.FaceFilter import FaceFilter as FilterFunc
+from lib.utils import (camel_case_split, get_folder, get_image_paths,
+                       rotate_landmarks, set_system_verbosity)
+from plugins.extract.align._base import Extract as AlignerExtract
 
 
 class Utils():
@@ -30,15 +29,6 @@ class Utils():
         lvl = '0' if verbose else '2'
         set_system_verbosity(lvl)
 
-    @staticmethod
-    def cv2_read_write(action, filename, image=None):
-        """ Read or write an image using cv2 """
-        if action == 'read':
-            image = cv2.imread(filename)
-        if action == 'write':
-            cv2.imwrite(filename, image)
-        return image
-
     @staticmethod
     def finalize(images_found, num_faces_detected, verify_output):
         """ Finalize the image processing """
@@ -53,304 +43,356 @@ class Utils():
             print("Double check your results.")
             print("-------------------------")
 
-        images_found = 0
-        num_faces_detected = 0
         print("Done!")
-        return images_found, num_faces_detected
 
 
-class Images():
-    """ Holds the full frames/images """
-    def __init__(self, arguments):
+class Alignments():
+    """ Holds processes pertaining to the alignments file """
+    def __init__(self, arguments, is_extract):
+        self.is_extract = is_extract
         self.args = arguments
-        self.rotation_angles = self.get_rotation_angles()
-        self.already_processed = self.get_already_processed()
-        self.input_images = self.get_input_images()
-        self.images_found = len(self.input_images)
+        self.serializer = self.get_serializer()
+        self.location = self.get_location()
+        self.have_alignments_file = os.path.exists(self.location)
+        self.data = self.load()
 
-        self.rotation_width = 0
-        self.rotation_height = 0
-
-    def get_rotation_angles(self):
-        """ Set the rotation angles. Includes backwards compatibility for the
-            'on' and 'off' options:
-                - 'on' - increment 90 degrees
-                - 'off' - disable
-                - 0 is prepended to the list, as whatever happens, we want to
-                  scan the image in it's upright state """
-        rotation_angles = [0]
-
-        if (not hasattr(self.args, 'rotate_images')
-                or not self.args.rotate_images
-                or self.args.rotate_images == "off"):
-            return rotation_angles
-
-        if self.args.rotate_images == "on":
-            rotation_angles.extend(range(90, 360, 90))
-        else:
-            passed_angles = [int(angle)
-                             for angle in self.args.rotate_images.split(",")]
-            if len(passed_angles) == 1:
-                rotation_step_size = passed_angles[0]
-                rotation_angles.extend(range(rotation_step_size,
-                                             360,
-                                             rotation_step_size))
-            elif len(passed_angles) > 1:
-                rotation_angles.extend(passed_angles)
-
-        return rotation_angles
-
-    def get_already_processed(self):
-        """ Return the images that already exist in the output directory """
-        print("Output Directory: {}".format(self.args.output_dir))
-
-        if (not hasattr(self.args, 'skip_existing')
-                or not self.args.skip_existing):
-            return None
+    def frames_count(self):
+        """ Return current frames count """
+        return len(self.data)
 
-        return get_image_paths(self.args.output_dir)
+    def faces_count(self):
+        """ Return current faces count """
+        return sum(len(faces) for faces in self.data.values())
 
-    def get_input_images(self):
-        """ Return the list of images that are to be processed """
-        if not os.path.exists(self.args.input_dir):
-            print("Input directory {} not found.".format(self.args.input_dir))
-            exit(1)
-
-        print("Input Directory: {}".format(self.args.input_dir))
+    def get_serializer(self):
+        """ Set the serializer to be used for loading and
+            saving alignments """
+        if (not hasattr(self.args, "serializer")
+                or not self.args.serializer):
+            if self.args.alignments_path:
+                ext = os.path.splitext(self.args.alignments_path)[-1]
+            else:
+                ext = "json"
+            serializer = Serializer.get_serializer_from_ext(ext)
+        else:
+            serializer = Serializer.get_serializer(self.args.serializer)
+        print("Using {} serializer".format(serializer.ext))
+        return serializer
 
-        if hasattr(self.args, 'skip_existing') and self.args.skip_existing:
-            input_images = get_image_paths(self.args.input_dir,
-                                           self.already_processed)
-            print("Excluding %s files" % len(self.already_processed))
+    def get_location(self):
+        """ Return the path to alignments file """
+        if self.args.alignments_path:
+            alignfile = self.args.alignments_path
         else:
-            input_images = get_image_paths(self.args.input_dir)
+            alignfile = os.path.join(
+                str(self.args.input_dir),
+                "alignments.{}".format(self.serializer.ext))
+        print("Alignments filepath: %s" % alignfile)
+        return alignfile
 
-        return input_images
+    def load(self):
+        """ Load the alignments data if it exists or create empty dict """
+        data = dict()
+        skip_faces = None
+        if self.is_extract:
+            skip_existing = bool(hasattr(self.args, 'skip_existing')
+                                 and self.args.skip_existing)
+            skip_faces = bool(hasattr(self.args, 'skip_faces')
+                              and self.args.skip_faces)
+
+            if not self.have_alignments_file:
+                if skip_existing or skip_faces:
+                    print("Skip Existing/Skip Faces selected, but no "
+                          "alignments file found!")
+                return data
+            if not skip_existing and not skip_faces:
+                return data
 
-    def rotate_image(self, image, rotation, reverse=False):
-        """ Rotate the image forwards or backwards """
-        if rotation == 0:
-            return image
-        if not reverse:
-            self.rotation_height, self.rotation_width = image.shape[:2]
-            image, _ = rotate_image_by_angle(image, rotation)
-        else:
-            image, _ = rotate_image_by_angle(
-                image,
-                rotation * -1,
-                rotated_width=self.rotation_width,
-                rotated_height=self.rotation_height)
-        return image
+        try:
+            with open(self.location, self.serializer.roptions) as align:
+                data = self.serializer.unmarshal(align.read())
 
+        except Exception as err:
+            print("{} not read!".format(self.location))
+            print(str(err))
+            data = dict()
 
-class Faces():
-    """ Holds the faces """
-    def __init__(self, arguments):
-        self.args = arguments
-        self.extractor = self.load_extractor()
-        self.mtcnn_kwargs = self.get_mtcnn_kwargs()
-        self.filter = self.load_face_filter()
-        self.align_eyes = self.args.align_eyes if hasattr(
-            self.args, 'align_eyes') else False
-        self.output_dir = get_folder(self.args.output_dir)
+        if skip_faces:
+            # Remove items from algnments that have no faces so they will
+            # be re-detected
+            del_keys = [key for key, val in data.items() if not val]
+            for key in del_keys:
+                if key in data:
+                    del data[key]
 
-        self.faces_detected = dict()
-        self.num_faces_detected = 0
-        self.verify_output = False
+        return data
 
-    @staticmethod
-    def load_extractor(extractor_name="Align"):
-        """ Load the requested extractor for extraction """
-        extractor = PluginLoader.get_extractor(extractor_name)()
+    def save(self):
+        """ Write the serialized alignments file """
+        try:
+            print("Writing alignments to: {}".format(self.location))
+            with open(self.location, self.serializer.woptions) as align:
+                align.write(self.serializer.marshal(self.data))
+        except Exception as err:
+            print("{} not written!".format(self.location))
+            print(str(err))
 
-        return extractor
+    def frame_exists(self, frame):
+        """ return path of images that have faces """
+        return frame in self.data.keys()
+
+    def get_alignments_for_frame(self, frame):
+        """ Return the alignments for the selected frame """
+        return self.data.get(frame, list())
+
+    def get_legacy_frames(self):
+        """ Return a list of frames with legacy rotations """
+        keys = list()
+        for key, val in self.data.items():
+            if any(alignment.get("r", None) for alignment in val):
+                keys.append(key)
+        return keys
+
+    def rotate_existing_landmarks(self, frame, dimensions):
+        """ Backwards compatability fix. Rotates the landmarks to
+            their correct position and deletes r """
+        for face in self.get_alignments_for_frame(frame):
+            angle = face.get("r", 0)
+            if not angle:
+                return
+            rotation_matrix = self.get_original_rotation_matrix(dimensions,
+                                                                angle)
+            rotate_landmarks(face, rotation_matrix)
+            del face["r"]
 
-    def get_mtcnn_kwargs(self):
-        """ Add the mtcnn arguments into a kwargs dictionary """
-        mtcnn_threshold = [float(thr.strip())
-                           for thr in self.args.mtcnn_threshold]
-        return {"minsize": self.args.mtcnn_minsize,
-                "threshold": mtcnn_threshold,
-                "factor": self.args.mtcnn_scalefactor}
+    @staticmethod
+    def get_original_rotation_matrix(dimensions, angle):
+        """ Calculate original rotation matrix and invert """
+        height, width = dimensions
+        center = (width/2, height/2)
+        rotation_matrix = cv2.getRotationMatrix2D(center, -1.0*angle, 1.)
 
-    def load_face_filter(self):
-        """ Load faces to filter out of images """
-        facefilter = None
-        filter_files = [self.set_face_filter(filter_type)
-                        for filter_type in ('filter', 'nfilter')]
+        abs_cos = abs(rotation_matrix[0, 0])
+        abs_sin = abs(rotation_matrix[0, 1])
+        rotated_width = int(height*abs_sin + width*abs_cos)
+        rotated_height = int(height*abs_cos + width*abs_sin)
+        rotation_matrix[0, 2] += rotated_width/2 - center[0]
+        rotation_matrix[1, 2] += rotated_height/2 - center[1]
 
-        if any(filters for filters in filter_files):
-            facefilter = FaceFilter(filter_files[0],
-                                    filter_files[1],
-                                    self.args.ref_threshold)
-        return facefilter
+        return rotation_matrix
 
-    def set_face_filter(self, filter_list):
-        """ Set the required filters """
-        filter_files = list()
-        filter_args = getattr(self.args, filter_list)
-        if filter_args:
-            print("{}: {}".format(filter_list.title(), filter_args))
-            filter_files = filter_args
-            if not isinstance(filter_args, list):
-                filter_files = [filter_args]
-            filter_files = list(filter(lambda fnc: Path(fnc).exists(),
-                                       filter_files))
-        return filter_files
 
-    def have_face(self, filename):
-        """ return path of images that have faces """
-        return os.path.basename(filename) in self.faces_detected
-
-    def get_faces(self, image, rotation=0):
-        """ Extract the faces from an image """
-        faces_count = 0
-        faces = detect_faces(image,
-                             self.args.detector,
-                             self.args.verbose,
-                             rotation=rotation,
-                             dlib_buffer=self.args.dlib_buffer,
-                             mtcnn_kwargs=self.mtcnn_kwargs)
-
-        for face in faces:
-            if self.filter and not self.filter.check(face):
-                if self.args.verbose:
-                    print("Skipping not recognized face!")
-                continue
-            yield faces_count, face
-
-            self.num_faces_detected += 1
-            faces_count += 1
-
-        if faces_count > 1 and self.args.verbose:
-            self.verify_output = True
-
-    def get_faces_alignments(self, filename, image):
-        """ Retrieve the face alignments from an image """
-        faces_count = 0
-        faces = self.faces_detected[os.path.basename(filename)]
-        for rawface in faces:
-            face = DetectedFace(**rawface)
-            # Rotate the image if necessary
-            # NB: Rotation of landmarks now occurs at extract stage
-            # This is here for legacy alignments
-            if face.r != 0:
-                image, _ = rotate_image_by_angle(image, face.r)
-            face.image = image[face.y: face.y + face.h,
-                               face.x: face.x + face.w]
-            if self.filter and not self.filter.check(face):
-                if self.args.verbose:
-                    print("Skipping not recognized face!")
-                continue
+class Images():
+    """ Holds the full frames/images """
+    def __init__(self, arguments):
+        self.args = arguments
+        self.input_images = self.get_input_images()
+        self.images_found = len(self.input_images)
 
-            yield faces_count, face
-            self.num_faces_detected += 1
-            faces_count += 1
-        if faces_count > 1 and self.args.verbose:
-            print("Note: Found more than one face in "
-                  "an image! File: {}".format(filename))
-            self.verify_output = True
-
-    def draw_landmarks_on_face(self, face, image):
-        """ Draw debug landmarks on extracted face """
-        if (not hasattr(self.args, 'debug_landmarks')
-                or not self.args.debug_landmarks):
-            return
+    def get_input_images(self):
+        """ Return the list of images that are to be processed """
+        if not os.path.exists(self.args.input_dir):
+            print("Input directory {} not found.".format(self.args.input_dir))
+            exit(1)
 
-        for (pos_x, pos_y) in face.landmarks_as_xy():
-            cv2.circle(image, (pos_x, pos_y), 2, (0, 0, 255), -1)
+        print("Input Directory: {}".format(self.args.input_dir))
+        input_images = get_image_paths(self.args.input_dir)
 
-    def detect_blurry_faces(self, face, t_mat, resized_image, filename):
-        """ Detect and move blurry face """
-        if not hasattr(self.args, 'blur_thresh') or not self.args.blur_thresh:
-            return None
+        return input_images
 
-        blurry_file = None
-        aligned_landmarks = self.extractor.transform_points(
-            face.landmarks_as_xy(),
-            t_mat,
-            256,
-            48)
-        feature_mask = self.extractor.get_feature_mask(aligned_landmarks / 256,
-                                                       256,
-                                                       48)
-        feature_mask = cv2.blur(feature_mask, (10, 10))
-        isolated_face = cv2.multiply(
-            feature_mask,
-            resized_image.astype(float)).astype(np.uint8)
-        blurry, focus_measure = is_blurry(isolated_face, self.args.blur_thresh)
-
-        if blurry:
-            print("{}'s focus measure of {} was below the blur threshold, "
-                  "moving to \"blurry\"".format(Path(filename).stem,
-                                                focus_measure))
-            blurry_file = get_folder(Path(self.output_dir) /
-                                     Path("blurry")) / Path(filename).stem
-        return blurry_file
+    def load(self):
+        """ Load an image and yield it with it's filename """
+        for filename in self.input_images:
+            yield filename, cv2.imread(filename)
 
+    @staticmethod
+    def load_one_image(filename):
+        """ load requested image """
+        return cv2.imread(filename)
 
-class Alignments():
-    """ Holds processes pertaining to the alignments file """
+
+class PostProcess():
+    """ Optional post processing tasks """
     def __init__(self, arguments):
         self.args = arguments
-        self.serializer = self.get_serializer()
-        self.alignments_path = self.get_alignments_path()
-        self.have_alignments_file = os.path.exists(self.alignments_path)
+        self.verbose = self.args.verbose
+        self.actions = self.set_actions()
+
+    def get_items(self):
+        """ Set the post processing actions """
+        postprocess_items = dict()
+        # Debug Landmarks
+        if (hasattr(self.args, 'debug_landmarks')
+                and self.args.debug_landmarks):
+            postprocess_items["DebugLandmarks"] = None
+
+        # Blurry Face
+        if hasattr(self.args, 'blur_thresh') and self.args.blur_thresh:
+            kwargs = {"blur_thresh": self.args.blur_thresh}
+            postprocess_items["BlurryFaceFilter"] = {"kwargs": kwargs}
+
+        # Face Filter post processing
+        if ((hasattr(self.args, "filter") and self.args.filter is not None) or
+                (hasattr(self.args, "nfilter") and
+                 self.args.nfilter is not None)):
+            face_filter = dict()
+            filter_lists = dict()
+            if hasattr(self.args, "ref_threshold"):
+                face_filter["ref_threshold"] = self.args.ref_threshold
+            for filter_type in ('filter', 'nfilter'):
+                filter_args = getattr(self.args, filter_type, None)
+                filter_args = None if not filter_args else filter_args
+                filter_lists[filter_type] = filter_args
+            face_filter["filter_lists"] = filter_lists
+            postprocess_items["FaceFilter"] = {"kwargs": face_filter}
+
+        return postprocess_items
+
+    def set_actions(self):
+        """ Compile the actions to be performed into a list """
+        postprocess_items = self.get_items()
+        actions = list()
+        for action, options in postprocess_items.items():
+            options = dict() if options is None else options
+            args = options.get("args", tuple())
+            kwargs = options.get("kwargs", dict())
+            args = args if isinstance(args, tuple) else tuple()
+            kwargs = kwargs if isinstance(kwargs, dict) else dict()
+            kwargs["verbose"] = self.verbose
+            task = globals()[action](*args, **kwargs)
+            actions.append(task)
+
+        for action in actions:
+            action_name = camel_case_split(action.__class__.__name__)
+            print("Adding post processing item: "
+                  "{}".format(" ".join(action_name)))
+
+        return actions
+
+    def do_actions(self, output_item):
+        """ Perform the requested post-processing actions """
+        for action in self.actions:
+            action.process(output_item)
+
+
+class PostProcessAction():
+    """ Parent class for Post Processing Actions
+        Usuable in Extract or Convert or both
+        depending on context """
+    def __init__(self, *args, **kwargs):
+        self.verbose = kwargs["verbose"]
+
+    def process(self, output_item):
+        """ Override for specific post processing action """
+        raise NotImplementedError
+
+
+class BlurryFaceFilter(PostProcessAction):
+    """ Move blurry faces to a different folder
+        Extract Only """
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        self.blur_thresh = kwargs["blur_thresh"]
+
+    def process(self, output_item):
+        """ Detect and move blurry face """
+        extractor = AlignerExtract()
+
+        for idx, face in enumerate(output_item["detected_faces"]):
+            resized_face = output_item["resized_faces"][idx]
+            dims = resized_face.shape[:2]
+            size = dims[0]
+            t_mat = output_item["t_mats"][idx]
+
+            aligned_landmarks = extractor.transform_points(
+                face.landmarksXY,
+                t_mat, size, 48)
+            feature_mask = extractor.get_feature_mask(
+                aligned_landmarks / size,
+                size, 48)
+            feature_mask = cv2.blur(feature_mask, (10, 10))
+            isolated_face = cv2.multiply(
+                feature_mask,
+                resized_face.astype(float)).astype(np.uint8)
+            blurry, focus_measure = is_blurry(isolated_face, self.blur_thresh)
+
+            if blurry:
+                blur_folder = output_item["output_file"].parts[:-1]
+                blur_folder = get_folder(Path(*blur_folder) / Path("blurry"))
+                frame_name = output_item["output_file"].parts[-1]
+                output_item["output_file"] = blur_folder / Path(frame_name)
+                if self.verbose:
+                    print("{}'s focus measure of {} was below the blur "
+                          "threshold, moving to \"blurry\"".format(
+                              frame_name, focus_measure))
+
+
+class DebugLandmarks(PostProcessAction):
+    """ Draw debug landmarks on face
+        Extract Only """
+
+    def process(self, output_item):
+        """ Draw landmarks on image """
+        transform_points = AlignerExtract().transform_points
+        for idx, face in enumerate(output_item["detected_faces"]):
+            dims = output_item["resized_faces"][idx].shape[:2]
+            size = dims[0]
+            landmarks = transform_points(face.landmarksXY,
+                                         output_item["t_mats"][idx],
+                                         size,
+                                         48)
+            for (pos_x, pos_y) in landmarks:
+                cv2.circle(output_item["resized_faces"][idx],
+                           (pos_x, pos_y), 2, (0, 0, 255), -1)
+
+
+class FaceFilter(PostProcessAction):
+    """ Filter in or out faces based on input image(s)
+        Extract or Convert """
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        filter_lists = kwargs["filter_lists"]
+        ref_threshold = kwargs.get("ref_threshold", 0.6)
+        self.filter = self.load_face_filter(filter_lists, ref_threshold)
+
+    def load_face_filter(self, filter_lists, ref_threshold):
+        """ Load faces to filter out of images """
+        if not any(val for val in filter_lists.values()):
+            return None
 
-    def get_serializer(self):
-        """ Set the serializer to be used for loading and saving alignments """
-        if not self.args.serializer and self.args.alignments_path:
-            ext = os.path.splitext(self.args.alignments_path)[-1]
-            serializer = Serializer.get_serializer_from_ext(ext)
-            print("Alignments Output: {}".format(self.args.alignments_path))
-        else:
-            serializer = Serializer.get_serializer(self.args.serializer)
-        print("Using {} serializer".format(serializer.ext))
-        return serializer
+        filter_files = [self.set_face_filter(key, val)
+                        for key, val in filter_lists.items()]
 
-    def get_alignments_path(self):
-        """ Return the path to alignments file """
-        if self.args.alignments_path:
-            alignfile = self.args.alignments_path
-        else:
-            alignfile = os.path.join(
-                str(self.args.input_dir),
-                "alignments.{}".format(self.serializer.ext))
-        print("Alignments filepath: %s" % alignfile)
-        return alignfile
+        if any(filters for filters in filter_files):
+            facefilter = FilterFunc(filter_files[0],
+                                    filter_files[1],
+                                    ref_threshold)
+        return facefilter
 
-    def read_alignments(self):
-        """ Read the serialized alignments file """
-        try:
-            with open(self.alignments_path, self.serializer.roptions) as align:
-                faces_detected = self.serializer.unmarshal(align.read())
-        except Exception as err:
-            print("{} not read!".format(self.alignments_path))
-            print(str(err))
-            faces_detected = dict()
-        return faces_detected
+    @staticmethod
+    def set_face_filter(f_type, f_args):
+        """ Set the required filters """
+        if not f_args:
+            return list()
 
-    def write_alignments(self, faces_detected):
-        """ Write the serialized alignments file """
-        if hasattr(self.args, 'skip_existing') and self.args.skip_existing:
-            faces_detected = self.load_skip_alignments(self.alignments_path,
-                                                       faces_detected)
+        print("{}: {}".format(f_type.title(), f_args))
+        filter_files = f_args if isinstance(f_args, list) else [f_args]
+        filter_files = list(filter(lambda fnc: Path(fnc).exists(),
+                                   filter_files))
+        return filter_files
 
-        try:
-            print("Writing alignments to: {}".format(self.alignments_path))
-            with open(self.alignments_path, self.serializer.woptions) as align:
-                align.write(self.serializer.marshal(faces_detected))
-        except Exception as err:
-            print("{} not written!".format(self.alignments_path))
-            print(str(err))
+    def process(self, output_item):
+        """ Filter in/out wanted/unwanted faces """
+        if not self.filter:
+            return
 
-    def load_skip_alignments(self, alignfile, faces_detected):
-        """ Load existing alignments if skipping existing images """
-        if self.have_alignments_file:
-            existing_alignments = self.read_alignments()
-            for key, val in existing_alignments.items():
-                if val:
-                    faces_detected[key] = val
-        else:
-            print("Existing alignments file '{}' not found.".format(alignfile))
-        return faces_detected
+        detected_faces = output_item["detected_faces"]
+        ret_faces = list()
+        for face in detected_faces:
+            if not self.filter.check(face):
+                if self.verbose:
+                    print("Skipping not recognized face!")
+                continue
+            ret_faces.append(face)
+        output_item["detected_faces"] = ret_faces
diff --git a/scripts/train.py b/scripts/train.py
index 7942b57..bc376c7 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -9,11 +9,12 @@ import cv2
 import tensorflow as tf
 from keras.backend.tensorflow_backend import set_session
 
-from lib.utils import get_folder, get_image_paths, set_system_verbosity, Timelapse
-from plugins.PluginLoader import PluginLoader
+from lib.utils import (get_folder, get_image_paths, set_system_verbosity,
+                       Timelapse)
+from plugins.plugin_loader import PluginLoader
 
 
-class Train(object):
+class Train():
     """ The training process.  """
     def __init__(self, arguments):
         self.args = arguments
@@ -25,7 +26,9 @@ class Train(object):
 
         # this is so that you can enter case insensitive values for trainer
         trainer_name = self.args.trainer
-        self.trainer_name = "LowMem" if trainer_name.lower() == "lowmem" else trainer_name
+        self.trainer_name = trainer_name
+        if trainer_name.lower() == "lowmem":
+            self.trainer_name = "LowMem"
         self.timelapse = None
 
     def process(self):
@@ -87,10 +90,11 @@ class Train(object):
             model = self.load_model()
             trainer = self.load_trainer(model)
 
-            self.timelapse = Timelapse.CreateTimelapse(self.args.timelapse_input_A,
-                                                       self.args.timelapse_input_B,
-                                                       self.args.timelapse_output,
-                                                       trainer)
+            self.timelapse = Timelapse.create_timelapse(
+                self.args.timelapse_input_A,
+                self.args.timelapse_input_B,
+                self.args.timelapse_output,
+                trainer)
 
             self.run_training_cycle(model, trainer)
         except KeyboardInterrupt:
@@ -105,7 +109,8 @@ class Train(object):
     def load_model(self):
         """ Load the model requested for training """
         model_dir = get_folder(self.args.model_dir)
-        model = PluginLoader.get_model(self.trainer_name)(model_dir, self.args.gpus)
+        model = PluginLoader.get_model(self.trainer_name)(model_dir,
+                                                          self.args.gpus)
 
         model.load(swapped=False)
         return model
@@ -168,10 +173,10 @@ class Train(object):
         # TODO: how to catch a specific key instead of Enter?
         # there isn't a good multiplatform solution:
         # https://stackoverflow.com/questions/3523174
-        # TODO: Find a way to interrupt input() if the target iterations are reached.
-        # At the moment, setting a target iteration and using the -p flag is
-        # the only guaranteed way to exit the training loop on hitting target
-        # iterations.
+        # TODO: Find a way to interrupt input() if the target iterations are
+        # reached. At the moment, setting a target iteration and using the -p
+        # flag is the only guaranteed way to exit the training loop on
+        # hitting target iterations.
         print("Starting. Press 'ENTER' to stop training and save model")
         try:
             input()
@@ -196,7 +201,8 @@ class Train(object):
                 cv2.imwrite(imgfile, image)
             if self.args.redirect_gui:
                 img = ".gui_preview_{}.jpg".format(name)
-                imgfile = os.path.join(scriptpath, "lib", "gui", ".cache", "preview", img)
+                imgfile = os.path.join(scriptpath, "lib", "gui",
+                                       ".cache", "preview", img)
                 cv2.imwrite(imgfile, image)
             if self.args.preview:
                 with self.lock:
diff --git a/setup.py b/setup.py
index 3e53cba..4c2de1e 100755
--- a/setup.py
+++ b/setup.py
@@ -1,259 +1,318 @@
 #!/usr/bin/env python3
+""" Install packages for faceswap.py """
 
-### >>> ENV
+# >>> ENV
 import os
 import sys
 import platform
-OS_Version = (platform.system(), platform.release())
-Py_Version = (platform.python_version(), platform.architecture()[0])
-Is_MacOS = (platform.system() == 'Darwin')
+OS_VERSION = (platform.system(), platform.release())
+PY_VERSION = (platform.python_version(), platform.architecture()[0])
+IS_MACOS = (platform.system() == 'Darwin')
 LD_LIBRARY_PATH = os.environ.get("LD_LIBRARY_PATH", None)
 IS_ADMIN = False
 IS_VIRTUALENV = False
-CUDA_Version = ""
+CUDA_VERSION = ""
 ENABLE_DOCKER = True
 ENABLE_CUDA = True
 COMPILE_DLIB_WITH_AVX_CUDA = True
-Required_Packages = [
-"tensorflow"
-]
-MacOS_Required_Packages = [
-"pynvx==0.0.4"
-]
-Installed_Packages = {}
-Missing_Packages = []
+REQUIRED_PACKAGES = [
+    "tensorflow"
+    ]
+MACOS_REQUIRED_PACKAGES = [
+    "pynvx==0.0.4"
+    ]
+INSTALLED_PACKAGES = {}
+MISSING_PACKAGES = []
 
 # load requirements list
 with open("requirements.txt") as req:
     for r in req.readlines():
         r = r.strip()
         if r and (not r.startswith("#")):
-            Required_Packages.append(r)
+            REQUIRED_PACKAGES.append(r)
 
-### <<< ENV
+# <<< ENV
 
-### >>> OUTPUT
-color_red = "\033[31m"
-color_green = "\033[32m"
-color_yellow = "\033[33m"
-color_default = "\033[0m"
+# >>> OUTPUT
+COLOR_RED = "\033[31m"
+COLOR_GREEN = "\033[32m"
+COLOR_YELLOW = "\033[33m"
+COLOR_DEFAULT = "\033[0m"
 
-def __indent_text_block(text):
-    a = text.splitlines()
-    if len(a)>1:
-        b = a[0] + "\r\n"
-        for i in range(1, len(a)-1):
-            b = b + "        " + a[i] + "\r\n"
-        b = b +  "        " + a[-1]
-        return b
-    else:
-        return text
-
-def Term_Support_Color():
-    global OS_Version
-    return (OS_Version[0] == "Linux" or OS_Version[0] == "Darwin")
-
-def INFO(text):
-    t = "%sINFO   %s " % (color_green, color_default) if Term_Support_Color() else "INFO    "
-    print(t + __indent_text_block(text))
 
-def WARNING(text):
-    t = "%sWARNING%s " % (color_yellow, color_default) if Term_Support_Color() else "WARNING "
-    print(t + __indent_text_block(text))
-
-def ERROR(text):
-    t = "%sERROR  %s " % (color_red, color_default) if Term_Support_Color() else "ERROR   "
-    print(t + __indent_text_block(text))
+def __indent_text_block(text):
+    """ Indent a text block """
+    lines = text.splitlines()
+    if len(lines) > 1:
+        out = lines[0] + "\r\n"
+        for i in range(1, len(lines)-1):
+            out = out + "        " + lines[i] + "\r\n"
+        out = out + "        " + lines[-1]
+        return out
+    return text
+
+
+def term_support_color():
+    """ Set whether OS Support terminal colour """
+    global OS_VERSION
+    return OS_VERSION[0] == "Linux" or OS_VERSION[0] == "Darwin"
+
+
+def out_info(text):
+    """ Format INFO Text """
+    trm = "INFO    "
+    if term_support_color():
+        trm = "{}INFO   {} ".format(COLOR_GREEN, COLOR_DEFAULT)
+    print(trm + __indent_text_block(text))
+
+
+def out_warning(text):
+    """ Format WARNING Text """
+    trm = "WARNING "
+    if term_support_color():
+        trm = "{}WARNING{} ".format(COLOR_YELLOW, COLOR_DEFAULT)
+    print(trm + __indent_text_block(text))
+
+
+def out_error(text):
+    """ Format ERROR Text """
+    trm = "ERROR   "
+    if term_support_color():
+        trm = "{}ERROR  {} ".format(COLOR_RED, COLOR_DEFAULT)
+    print(trm + __indent_text_block(text))
     exit(1)
 
-### <<< OUTPUT
+# <<< OUTPUT
 
-def Check_Permission():
-    import ctypes, os
+
+def check_permission():
+    """ Check for Admin permissions """
+    import ctypes
     global IS_ADMIN
     try:
         IS_ADMIN = os.getuid() == 0
     except AttributeError:
         IS_ADMIN = ctypes.windll.shell32.IsUserAnAdmin() != 0
     if IS_ADMIN:
-        INFO("Running as Root/Admin")
+        out_info("Running as Root/Admin")
     else:
-        WARNING("Running without root/admin privileges")
+        out_warning("Running without root/admin privileges")
+
 
-def Check_System():
-    global OS_Version
-    INFO("The tool provides tips for installation\nand installs required python packages")
-    INFO("Setup in %s %s" % (OS_Version[0], OS_Version[1]))
-    if not OS_Version[0] in ["Windows", "Linux", "Darwin"]:
-        ERROR("Your system %s is not supported!" % OS_Version[0])
+def check_system():
+    """ Check the system """
+    global OS_VERSION
+    out_info("The tool provides tips for installation\n"
+             "and installs required python packages")
+    out_info("Setup in %s %s" % (OS_VERSION[0], OS_VERSION[1]))
+    if not OS_VERSION[0] in ["Windows", "Linux", "Darwin"]:
+        out_error("Your system %s is not supported!" % OS_VERSION[0])
 
-def Enable_CUDA():
+
+def ask_enable_cuda():
+    """ Enable or disable CUDA """
     global ENABLE_CUDA
     i = input("Enable  CUDA? [Y/n] ")
-    if i == "" or i == "Y" or i == "y":
-        INFO("CUDA Enabled")
+    if i in ("", "Y", "y"):
+        out_info("CUDA Enabled")
         ENABLE_CUDA = True
     else:
-        INFO("CUDA Disabled")
+        out_info("CUDA Disabled")
         ENABLE_CUDA = False
 
-def Enable_Docker():
+
+def ask_enable_docker():
+    """ Enable or disable Docker """
     global ENABLE_DOCKER
     i = input("Enable  Docker? [Y/n] ")
-    if i == "" or i == "Y" or i == "y":
-        INFO("Docker Enabled")
+    if i in ("", "Y", "y"):
+        out_info("Docker Enabled")
         ENABLE_DOCKER = True
     else:
-        INFO("Docker Disabled")
+        out_info("Docker Disabled")
         ENABLE_DOCKER = False
 
-def Check_Python():
-    global Py_Version, IS_VIRTUALENV
+
+def check_python():
+    """ Check python and virtual environment status """
+    global PY_VERSION, IS_VIRTUALENV
     # check if in virtualenv
     IS_VIRTUALENV = (hasattr(sys, "real_prefix")
-                     or (hasattr(sys, "base_prefix") and sys.base_prefix != sys.prefix))
-    if Py_Version[0].split(".")[0] == "3" and  Py_Version[1] == "64bit":
-        INFO("Installed Python: {0} {1}".format(Py_Version[0],Py_Version[1]))
+                     or (hasattr(sys, "base_prefix") and
+                         sys.base_prefix != sys.prefix))
+    if PY_VERSION[0].split(".")[0] == "3" and PY_VERSION[1] == "64bit":
+        out_info("Installed Python: {0} {1}".format(PY_VERSION[0],
+                                                    PY_VERSION[1]))
         return True
-    else:
-        ERROR("Please run this script with Python3 64bit and try again.")
-        return False
 
-def Check_PIP():
+    out_error("Please run this script with Python3 64bit and try again.")
+    return False
+
+
+def check_pip():
+    """ Check installed pip version """
     try:
-        try: # for pip >= 10
-            from pip._internal.utils.misc import get_installed_distributions, get_installed_version
-        except ImportError: # for pip <= 9.0.3
-            from pip.utils import get_installed_distributions, get_installed_version
-        global Installed_Packages
-        Installed_Packages = {pkg.project_name:pkg.version for pkg in get_installed_distributions()}
-        INFO("Installed PIP: " + get_installed_version("pip"))
+        try:  # for pip >= 10
+            from pip._internal.utils.misc import (get_installed_distributions,
+                                                  get_installed_version)
+        except ImportError:  # for pip <= 9.0.3
+            from pip.utils import (get_installed_distributions,
+                                   get_installed_version)
+        global INSTALLED_PACKAGES
+        INSTALLED_PACKAGES = {pkg.project_name: pkg.version
+                              for pkg in get_installed_distributions()}
+        out_info("Installed PIP: " + get_installed_version("pip"))
         return True
     except ImportError:
-        ERROR("Import pip failed. Please Install python3-pip and try again")
+        out_error("Import pip failed. Please Install python3-pip "
+                  "and try again")
         return False
 
+
 # only invoked in linux
-def Check_CUDA():
-    global CUDA_Version
-    a=os.popen("ldconfig -p | grep -P -o \"libcudart.so.\d.\d\" | head -n 1")
-    libcudart = a.read()
+def check_cuda():
+    """ Check CUDA Version """
+    global CUDA_VERSION
+    chk = os.popen("ldconfig -p | grep -P -o \"libcudart.so.\d+.\d+\" | "
+                   "head -n 1")
+    libcudart = chk.read()
     if LD_LIBRARY_PATH and not libcudart:
         paths = LD_LIBRARY_PATH.split(":")
         for path in paths:
-            a = os.popen("ls {} | grep -P -o \"libcudart.so.\d.\d\" | head -n 1".format(path))
-            libcudart = a.read()
+            chk = os.popen("ls {} | grep -P -o \"libcudart.so.\d+.\d+\" | "
+                           "head -n 1".format(path))
+            libcudart = chk.read()
             if libcudart:
                 break
     if libcudart:
-        CUDA_Version = libcudart[13:].rstrip()
-        if CUDA_Version:
-            INFO("CUDA version: " + CUDA_Version)
+        CUDA_VERSION = libcudart[13:].rstrip()
+        if CUDA_VERSION:
+            out_info("CUDA version: " + CUDA_VERSION)
     else:
-        ERROR("""CUDA not found. Install and try again.
+        out_error("""CUDA not found. Install and try again.
 Recommended version:      CUDA 9.0     cuDNN 7.1.3
 CUDA: https://developer.nvidia.com/cuda-downloads
 cuDNN: https://developer.nvidia.com/rdp/cudnn-download
 """)
 
+
 # only invoked in linux
-def Check_cuDNN():
-    a=os.popen("ldconfig -p | grep -P -o \"libcudnn.so.\d\" | head -n 1")
-    libcudnn = a.read()
+def check_cudnn():
+    """ Check cuDNN Version """
+    chk = os.popen("ldconfig -p | grep -P -o \"libcudnn.so.\d\" | head -n 1")
+    libcudnn = chk.read()
     if LD_LIBRARY_PATH and not libcudnn:
         paths = LD_LIBRARY_PATH.split(":")
         for path in paths:
-            a = os.popen("ls {} | grep -P -o \"libcudnn.so.\d\" | head -n 1".format(path))
-            libcudnn = a.read()
+            chk = os.popen("ls {} | grep -P -o \"libcudnn.so.\d\" | "
+                           "head -n 1".format(path))
+            libcudnn = chk.read()
             if libcudnn:
                 break
     if libcudnn:
         cudnn_version = libcudnn[12:].rstrip()
         if cudnn_version:
-            INFO("cuDNN version: " + cudnn_version)
+            out_info("cuDNN version: " + cudnn_version)
     else:
-        ERROR("""cuDNN not found. Install and try again.
+        out_error("""cuDNN not found. Install and try again.
 Recommended version:      CUDA 9.0     cuDNN 7.1.3
 CUDA: https://developer.nvidia.com/cuda-downloads
 cuDNN: https://developer.nvidia.com/rdp/cudnn-download
 """)
 
-def Continue():
+
+def ask_continue():
+    """ Ask Continue with Install """
     i = input("Are System Dependencies met? [y/N] ")
-    if i == "" or i == "N" or i == "n":
-        ERROR('Please install system dependencies to continue')
-
-def Check_Missing_Dep():
-    global Missing_Packages, Installed_Packages, ENABLE_CUDA, Is_MacOS
-    if ENABLE_CUDA and Is_MacOS:
-        Required_Packages.extend(MacOS_Required_Packages)
-    Missing_Packages = []
-    for pkg in Required_Packages:
+    if i in ("", "N", "n"):
+        out_error('Please install system dependencies to continue')
+
+
+def check_missing_dep():
+    """ Check for missing dependencies """
+    global MISSING_PACKAGES, INSTALLED_PACKAGES, ENABLE_CUDA, IS_MACOS
+    if ENABLE_CUDA and IS_MACOS:
+        REQUIRED_PACKAGES.extend(MACOS_REQUIRED_PACKAGES)
+    MISSING_PACKAGES = []
+    for pkg in REQUIRED_PACKAGES:
         key = pkg.split("==")[0]
-        if not key in Installed_Packages:
-            Missing_Packages.append(pkg)
+        if key not in INSTALLED_PACKAGES:
+            MISSING_PACKAGES.append(pkg)
             continue
         else:
-            if len(pkg.split("=="))>1:
-                if pkg.split("==")[1] != Installed_Packages.get(key):
-                    Missing_Packages.append(pkg)
+            if len(pkg.split("==")) > 1:
+                if pkg.split("==")[1] != INSTALLED_PACKAGES.get(key):
+                    MISSING_PACKAGES.append(pkg)
                     continue
 
-def Check_dlib():
-    global Missing_Packages, COMPILE_DLIB_WITH_AVX_CUDA
-    if "dlib" in Missing_Packages:
+
+def check_dlib():
+    """ Check dlib install requirements """
+    global MISSING_PACKAGES, COMPILE_DLIB_WITH_AVX_CUDA
+    if "dlib" in MISSING_PACKAGES:
         i = input("Compile dlib with AVX (and CUDA if enabled)? [Y/n] ")
-        if i == "" or i == "Y" or i == "y":
-            INFO("dlib Configured")
-            WARNING("Make sure you are using gcc-5/g++-5 and CUDA bin/lib in path")
+        if i in ("", "Y", "y"):
+            out_info("dlib Configured")
+            out_warning("Make sure you are using gcc-5/g++-5 "
+                        "and CUDA bin/lib in path")
             COMPILE_DLIB_WITH_AVX_CUDA = True
         else:
             COMPILE_DLIB_WITH_AVX_CUDA = False
 
-def Install_Missing_Dep():
-    global Missing_Packages
-    if len(Missing_Packages):
-        INFO("""Installing Required Python Packages. This may take some time...""")
+
+def install_missing_dep():
+    """ Install missing dependencies """
+    global MISSING_PACKAGES
+    if MISSING_PACKAGES:
+        out_info("Installing Required Python Packages. "
+                 "This may take some time...")
         try:
             from pip._internal import main as pipmain
         except:
             from pip import main as pipmain
-        for m in Missing_Packages:
-            msg = "Installing {}".format(m)
-            INFO(msg)
+        for pkg in MISSING_PACKAGES:
+            msg = "Installing {}".format(pkg)
+            out_info(msg)
             # hide info/warning and fix cache hang
             pipargs = ["install", "-qq", "--no-cache-dir"]
             # install as user to solve perm restriction
             if not IS_ADMIN and not IS_VIRTUALENV:
                 pipargs.append("--user")
             # compile dlib with AVX ins and CUDA
-            if m.startswith("dlib") and COMPILE_DLIB_WITH_AVX_CUDA:
-                pipargs.extend(["--install-option=--yes", "--install-option=USE_AVX_INSTRUCTIONS"])
-            pipargs.append(m)
-            # pip install -qq (--user) (--install-options) m
+            if pkg.startswith("dlib") and COMPILE_DLIB_WITH_AVX_CUDA:
+                pipargs.extend(["--install-option=--yes",
+                                "--install-option=USE_AVX_INSTRUCTIONS"])
+            pipargs.append(pkg)
+            # pip install -qq (--user) (--install-options) pkg
             pipmain(pipargs)
 
-def Update_TF_Dep():
-    global CUDA_Version
-    Required_Packages[0] = "tensorflow-gpu"
-    if CUDA_Version.startswith("8.0"):
-        Required_Packages[0] += "==1.4.0"
-    elif not CUDA_Version.startswith("9.0"):
-            WARNING("Tensorflow has no official prebuild for CUDA 9.1 currently.\r\n"
-                    "To continue, You have to build and install your own tensorflow-gpu.\r\n"
-                    "Help: https://www.tensorflow.org/install/install_sources")
-            custom_tf = input("Location of custom tensorflow-gpu wheel (leave blank to manually install): ")
-            if not custom_tf:
-                del Required_Packages[0]
-                return
-            if os.path.isfile(custom_tf):
-                Required_Packages[0] = custom_tf
-            else:
-                ERROR("{} not found".format(custom_tf))
+
+def update_tf_dep():
+    """ Update Tensorflow Dependency """
+    global CUDA_VERSION
+    REQUIRED_PACKAGES[0] = "tensorflow-gpu"
+    if CUDA_VERSION.startswith("8.0"):
+        REQUIRED_PACKAGES[0] += "==1.4.0"
+    elif not CUDA_VERSION.startswith("9.0"):
+        out_warning("Tensorflow has currently no official prebuild for CUDA "
+                    "versions above 9.0.\r\nTo continue, You have to build "
+                    "and install your own tensorflow-gpu.\r\n"
+                    "Help: "
+                    "https://www.tensorflow.org/install/install_sources")
+        custom_tf = input("Location of custom tensorflow-gpu wheel (leave "
+                          "blank to manually install): ")
+        if not custom_tf:
+            del REQUIRED_PACKAGES[0]
+            return
+        if os.path.isfile(custom_tf):
+            REQUIRED_PACKAGES[0] = custom_tf
+        else:
+            out_error("{} not found".format(custom_tf))
 
 
-def Tips_1_1():
-    INFO("""1. Install Docker
+def tips_1_1():
+    """ Output Tips """
+    out_info("""1. Install Docker
 https://www.docker.com/community-edition
 
 2. Build Docker Image For Faceswap
@@ -285,10 +344,12 @@ nvidia-docker run -p 8888:8888 \\
 4. Open a new terminal to run faceswap.py in /srv
 docker exec -it deepfakes-cpu bash
 """.format(path=sys.path[0]))
-    INFO("That's all you need to do with a docker. Have fun.")
+    out_info("That's all you need to do with a docker. Have fun.")
 
-def Tips_1_2():
-    INFO("""1. Install Docker
+
+def tips_1_2():
+    """ Output Tips """
+    out_info("""1. Install Docker
 https://www.docker.com/community-edition
 
 2. Install latest CUDA
@@ -328,12 +389,15 @@ nvidia-docker run -p 8888:8888 \\
 docker exec deepfakes-gpu python /srv/tools.py gui
 """.format(path=sys.path[0]))
 
-def Tips_2_1():
-    INFO("""Tensorflow has no official prebuilts for CUDA 9.1 currently.
+
+def tips_2_1():
+    """ Output Tips """
+    out_info("""Tensorflow has no official prebuilts for CUDA 9.1 currently.
 
 1. Install CUDA 9.0 and cuDNN
 CUDA: https://developer.nvidia.com/cuda-downloads
-cuDNN: https://developer.nvidia.com/rdp/cudnn-download (Add DLL to %PATH% in Windows)
+cuDNN: https://developer.nvidia.com/rdp/cudnn-download (Add DLL to "
+"%PATH% in Windows)
 
 2. Install System Dependencies.
 In Windows:
@@ -348,8 +412,9 @@ to fix Unicode issues on Windows when installing dependencies
 """)
 
 
-def Tips_2_2():
-    INFO("""1. Install System Dependencies.
+def tips_2_2():
+    """ Output Tips """
+    out_info("""1. Install System Dependencies.
 In Windows:
 Install CMake x64: https://cmake.org/download/
 
@@ -362,49 +427,51 @@ to fix Unicode issues on Windows when installing dependencies
 """)
 
 
-def Main():
-    global ENABLE_DOCKER, ENABLE_CUDA, CUDA_Version, OS_Version
-    Check_System()
-    Check_Python()
-    Check_PIP()
+def main():
+    """" Run Setup """
+    global ENABLE_DOCKER, ENABLE_CUDA, CUDA_VERSION, OS_VERSION
+    check_system()
+    check_python()
+    check_pip()
     # ask questions
-    Enable_Docker()
-    Enable_CUDA()
+    ask_enable_docker()
+    ask_enable_cuda()
     # warn if nvidia-docker on non-linux system
-    if OS_Version[0] != "Linux" and ENABLE_DOCKER and ENABLE_CUDA:
-        WARNING("Nvidia-Docker is only supported on Linux.\r\nOnly CPU is supported in Docker for your system")
-        Enable_Docker()
+    if OS_VERSION[0] != "Linux" and ENABLE_DOCKER and ENABLE_CUDA:
+        out_warning("Nvidia-Docker is only supported on Linux.\r\n"
+                    "Only CPU is supported in Docker for your system")
+        ask_enable_docker()
         if ENABLE_DOCKER:
-            WARNING("CUDA Disabled")
+            out_warning("CUDA Disabled")
             ENABLE_CUDA = False
-    
+
     # provide tips
     if ENABLE_DOCKER:
         # docker, quick help
         if not ENABLE_CUDA:
-            Tips_1_1()
+            tips_1_1()
         else:
-            Tips_1_2()
+            tips_1_2()
     else:
         if ENABLE_CUDA:
             # update dep info if cuda enabled
-            if OS_Version[0] == "Linux":
-                Check_CUDA()
-                Check_cuDNN()
+            if OS_VERSION[0] == "Linux":
+                check_cuda()
+                check_cudnn()
             else:
-                Tips_2_1()
-                WARNING("Cannot find CUDA on non-Linux system")
-                CUDA_Version = input("Manually specify CUDA version: ")
-            Update_TF_Dep()
+                tips_2_1()
+                out_warning("Cannot find CUDA on non-Linux system")
+                CUDA_VERSION = input("Manually specify CUDA version: ")
+            update_tf_dep()
         else:
-            Tips_2_2()
+            tips_2_2()
         # finally check dep
-        Continue()
-        Check_Missing_Dep()
-        Check_dlib()
-        Install_Missing_Dep()
-        INFO("All python3 dependencies are met.\r\nYou are good to go.")
+        ask_continue()
+        check_missing_dep()
+        check_dlib()
+        install_missing_dep()
+        out_info("All python3 dependencies are met.\r\nYou are good to go.")
 
-if __name__ == "__main__":
-    Main()
 
+if __name__ == "__main__":
+    main()
diff --git a/tools/lib_alignments/jobs_manual.py b/tools/lib_alignments/jobs_manual.py
index 0eeee07..6c85977 100644
--- a/tools/lib_alignments/jobs_manual.py
+++ b/tools/lib_alignments/jobs_manual.py
@@ -3,11 +3,11 @@
 
 import platform
 import sys
-
 import cv2
 import numpy as np
 
-from lib.face_alignment import Extract
+from lib.multithreading import SpawnProcess, queue_manager
+from plugins.plugin_loader import PluginLoader
 from . import Annotate, ExtractedFaces, Frames, Rotate
 
 
@@ -433,6 +433,7 @@ class Manual():
             key = cv2.waitKey(1)
 
             if self.window_closed(is_windows, is_conda, key):
+                queue_manager.terminate_queues()
                 break
 
             if key in press.keys():
@@ -693,8 +694,12 @@ class MouseHandler():
         self.interface = interface
         self.alignments = interface.alignments
         self.frames = interface.frames
-        self.extract = Extract(None, "manual",
-                               initialize_only=True, verbose=verbose)
+
+        self.extractor = {
+            "detect": PluginLoader.get_detector("manual")(verbose=verbose),
+            "align": PluginLoader.get_aligner("fan")(verbose=verbose)}
+        self.init_extractor()
+
         self.mouse_state = None
         self.last_move = None
         self.center = None
@@ -705,6 +710,38 @@ class MouseHandler():
                       "bounding_last": list(),
                       "bounding_box_orig": list()}
 
+    def init_extractor(self):
+        """ Initialize FAN """
+        aligner = self.extractor["align"]
+        detector = self.extractor["detect"]
+
+        in_queue = queue_manager.get_queue("in")
+        align_queue = queue_manager.get_queue("align")
+        out_queue = queue_manager.get_queue("out")
+
+        d_kwargs = {"in_queue": in_queue,
+                    "out_queue": align_queue}
+        a_kwargs = {"in_queue": align_queue,
+                    "out_queue": out_queue}
+
+        detect_process = SpawnProcess()
+        align_process = SpawnProcess()
+
+        d_event = detect_process.event
+        a_event = align_process.event
+
+        detect_process.in_process(detector.detect_faces, **d_kwargs)
+        align_process.in_process(aligner.align, **a_kwargs)
+
+        # Wait for Aligner to take init
+        a_event.wait(60)
+        if not a_event.is_set():
+            raise ValueError("Error inititalizing Aligner")
+
+        d_event.wait(10)
+        if not d_event.is_set():
+            raise ValueError("Error inititalizing Detector")
+
     def on_event(self, event, x, y, flags, param):
         """ Handle the mouse events """
         if self.interface.get_edit_mode() != "Edit":
@@ -827,15 +864,17 @@ class MouseHandler():
 
     def update_landmarks(self):
         """ Update the landmarks """
-        self.extract.execute(self.media["image"],
-                             manual_face=self.media["bounding_box"])
-        landmarks = self.extract.landmarks[0][1]
-        left, top, right, bottom = self.media["bounding_box"]
-        alignment = {"x": left,
-                     "w": right - left,
-                     "y": top,
-                     "h": bottom - top,
-                     "landmarksXY": landmarks}
+        queue_manager.get_queue("in").put((self.media["image"],
+                                           self.media["bounding_box"]))
+        landmarks = queue_manager.get_queue("out").get()
+        if landmarks == "EOF":
+            exit(0)
+        face = landmarks["detected_faces"][0]
+        alignment = {"x": face.x,
+                     "w": face.w,
+                     "y": face.y,
+                     "h": face.h,
+                     "landmarksXY": face.landmarksXY}
         frame = self.media["frame_id"]
 
         if self.interface.get_selected_face_id() is None:
diff --git a/tools/lib_alignments/media.py b/tools/lib_alignments/media.py
index 60d061e..24e94bb 100644
--- a/tools/lib_alignments/media.py
+++ b/tools/lib_alignments/media.py
@@ -9,8 +9,9 @@ import cv2
 import numpy as np
 
 from lib import Serializer
+from lib.faces_detect import DetectedFace
 from lib.utils import _image_extensions, rotate_landmarks
-from plugins.PluginLoader import PluginLoader
+from plugins.extract.align._base import Extract as AlignerExtract
 
 
 class AlignmentData():
@@ -200,10 +201,7 @@ class AlignmentData():
                 return
             rotation_matrix = self.get_original_rotation_matrix(dimensions,
                                                                 angle)
-            face = DetectedFace()
-            face.alignment_to_face(None, alignment)
-            face = rotate_landmarks(face, rotation_matrix)
-            alignment = face.face_to_alignment(alignment)
+            rotate_landmarks(alignment, rotation_matrix)
             del alignment["r"]
 
     @staticmethod
@@ -355,39 +353,6 @@ class Frames(MediaLoader):
                       key=lambda x: (x["frame_name"]))
 
 
-class DetectedFace():
-    """ Detected face and landmark information """
-    def __init__(self):
-        self.image = None
-        self.x = None
-        self.w = None
-        self.y = None
-        self.h = None
-        self.landmarksXY = None
-
-    def alignment_to_face(self, image, alignment):
-        """ Convert a face alignment to detected face object """
-        self.image = image
-        self.x = alignment["x"]
-        self.w = alignment["w"]
-        self.y = alignment["y"]
-        self.h = alignment["h"]
-        self.landmarksXY = alignment["landmarksXY"]
-
-    def face_to_alignment(self, alignment):
-        """ Convert a face alignment to detected face object """
-        alignment["x"] = self.x
-        alignment["w"] = self.w
-        alignment["y"] = self.y
-        alignment["h"] = self.h
-        alignment["landmarksXY"] = self.landmarksXY
-        return alignment
-
-    def landmarks_as_xy(self):
-        """ Landmarks as XY """
-        return self.landmarksXY
-
-
 class ExtractedFaces():
     """ Holds the extracted faces and matrix for
         alignments """
@@ -396,7 +361,7 @@ class ExtractedFaces():
         self.size = size
         self.padding = padding
         self.align_eyes = align_eyes
-        self.extractor = PluginLoader.get_extractor("Align")()
+        self.extractor = AlignerExtract()
         self.alignments = alignments
         self.frames = frames
 
@@ -423,7 +388,7 @@ class ExtractedFaces():
     def extract_one_face(self, alignment, image):
         """ Extract one face from image """
         face = DetectedFace()
-        face.alignment_to_face(image, alignment)
+        face.from_alignment(alignment, image=image)
         return self.extractor.extract(image, face, self.size, self.align_eyes)
 
     def original_roi(self, matrix):
@@ -470,7 +435,7 @@ class ExtractedFaces():
 
     def get_aligned_landmarks_for_frame(self, frame, landmarks_xy,
                                         update=False):
-        """ Return the original rois for the selected frame """
+        """ Return the transposed landmarks for the selected face """
         if self.current_frame != frame or update:
             self.get_faces(frame)
         aligned_landmarks = list()
diff --git a/tools/sort.py b/tools/sort.py
index ab6f06b..d65bf0f 100644
--- a/tools/sort.py
+++ b/tools/sort.py
@@ -15,12 +15,15 @@ from tqdm import tqdm
 import face_recognition
 
 from lib.cli import FullHelpArgumentParser
-from lib import face_alignment, Serializer
+from lib import Serializer
+from lib.faces_detect import DetectedFace
+from lib.multithreading import queue_manager, SpawnProcess
+from plugins.plugin_loader import PluginLoader
 
 from . import cli
 
 
-class Sort(object):
+class Sort():
     """ Sorts folders of faces based on input criteria """
     def __init__(self, arguments):
         self.args = arguments
@@ -73,6 +76,35 @@ class Sort(object):
 
         self.sort_process()
 
+    @staticmethod
+    def launch_aligner():
+        """ Load the aligner plugin to retrieve landmarks """
+        aligner = PluginLoader.get_aligner("fan")()
+        kwargs = {"in_queue": queue_manager.get_queue("in"),
+                  "out_queue": queue_manager.get_queue("out")}
+        process = SpawnProcess()
+        event = process.event
+
+        process.in_process(aligner.align, **kwargs)
+        event.wait(60)
+        if not event.is_set():
+            raise ValueError("Error inititalizing Aligner")
+
+    @staticmethod
+    def alignment_dict(image):
+        """ Set the image to a dict for alignment """
+        height, width = image.shape[:2]
+        face = DetectedFace(x=0, w=width, y=0, h=height)
+        return {"image": image,
+                "detected_faces": [face]}
+
+    def get_landmarks(self, filename):
+        """ Get landmarks for current image """
+        image = cv2.imread(filename)
+        queue_manager.get_queue("in").put(self.alignment_dict(image))
+        face = queue_manager.get_queue("out").get()
+        return face["detected_faces"][0].landmarksXY
+
     def sort_process(self):
         """
         This method dynamically assigns the functions that will be used to run
@@ -180,24 +212,21 @@ class Sort(object):
         return img_list
 
     def sort_face_cnn(self):
-        """ Sort by dlib CNN similarity """
+        """ Sort by CNN similarity """
+        self.launch_aligner()
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn similarity...")
-
         img_list = []
         for img in tqdm(self.find_images(input_dir),
                         desc="Loading",
                         file=sys.stdout):
-            landmarks = face_alignment.Extract(
-                input_image_bgr=cv2.imread(img),
-                detector='dlib-cnn',
-                verbose=True,
-                input_is_predetected_face=True).landmarks
-            img_list.append([img, np.array(landmarks[0][1])
+            landmarks = self.get_landmarks(img)
+            img_list.append([img, np.array(landmarks)
                              if landmarks
                              else np.zeros((68, 2))])
 
+        queue_manager.terminate_queues()
         img_list_len = len(img_list)
         for i in tqdm(range(0, img_list_len - 1),
                       desc="Sorting",
@@ -218,7 +247,8 @@ class Sort(object):
         return img_list
 
     def sort_face_cnn_dissim(self):
-        """ Sort by dlib CNN dissimilarity """
+        """ Sort by CNN dissimilarity """
+        self.launch_aligner()
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn dissimilarity...")
@@ -227,12 +257,8 @@ class Sort(object):
         for img in tqdm(self.find_images(input_dir),
                         desc="Loading",
                         file=sys.stdout):
-            landmarks = face_alignment.Extract(
-                input_image_bgr=cv2.imread(img),
-                detector='dlib-cnn',
-                verbose=True,
-                input_is_predetected_face=True).landmarks
-            img_list.append([img, np.array(landmarks[0][1])
+            landmarks = self.get_landmarks(img)
+            img_list.append([img, np.array(landmarks)
                              if landmarks
                              else np.zeros((68, 2)), 0])
 
@@ -257,19 +283,16 @@ class Sort(object):
 
     def sort_face_yaw(self):
         """ Sort by yaw of face """
+        self.launch_aligner()
         input_dir = self.args.input_dir
 
         img_list = []
         for img in tqdm(self.find_images(input_dir),
                         desc="Loading",
                         file=sys.stdout):
-            landmarks = face_alignment.Extract(
-                input_image_bgr=cv2.imread(img),
-                detector='dlib-cnn',
-                verbose=True,
-                input_is_predetected_face=True).landmarks
+            landmarks = self.get_landmarks(img)
             img_list.append(
-                [img, self.calc_landmarks_face_yaw(np.array(landmarks[0][1]))])
+                [img, self.calc_landmarks_face_yaw(np.array(landmarks))])
 
         print("Sorting by face-yaw...")
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
@@ -415,7 +438,7 @@ class Sort(object):
         return bins
 
     def group_face_cnn(self, img_list):
-        """ Group into bins by dlib CNN face similarity """
+        """ Group into bins by CNN face similarity """
         print("Grouping by face-cnn similarity...")
 
         # Groups are of the form: group_num -> reference faces
@@ -632,31 +655,25 @@ class Sort(object):
                                 desc="Reloading",
                                 file=sys.stdout)]
         elif group_method == 'group_face_cnn':
+            self.launch_aligner()
             temp_list = []
             for img in tqdm(self.find_images(input_dir),
                             desc="Reloading",
                             file=sys.stdout):
-                landmarks = face_alignment.Extract(
-                    input_image_bgr=cv2.imread(img),
-                    detector='dlib-cnn',
-                    verbose=True,
-                    input_is_predetected_face=True).landmarks
-                temp_list.append([img, np.array(landmarks[0][1])
+                landmarks = self.get_landmarks(img)
+                temp_list.append([img, np.array(landmarks)
                                   if landmarks
                                   else np.zeros((68, 2))])
         elif group_method == 'group_face_yaw':
+            self.launch_aligner()
             temp_list = []
             for img in tqdm(self.find_images(input_dir),
                             desc="Reloading",
                             file=sys.stdout):
-                landmarks = face_alignment.Extract(
-                    input_image_bgr=cv2.imread(img),
-                    detector='dlib-cnn',
-                    verbose=True,
-                    input_is_predetected_face=True).landmarks
+                landmarks = self.get_landmarks(img)
                 temp_list.append(
                     [img,
-                     self.calc_landmarks_face_yaw(np.array(landmarks[0][1]))])
+                     self.calc_landmarks_face_yaw(np.array(landmarks))])
         elif group_method == 'group_hist':
             temp_list = [
                 [img,
@@ -826,7 +843,7 @@ class Sort(object):
 
     @staticmethod
     def get_avg_score_faces_cnn(fl1, references):
-        """ Return the average dlib CNN similarity score
+        """ Return the average CNN similarity score
             between a face and reference image """
         scores = []
         for fl2 in references:
