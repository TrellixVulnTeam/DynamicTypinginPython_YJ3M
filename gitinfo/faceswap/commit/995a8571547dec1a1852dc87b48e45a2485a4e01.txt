commit 995a8571547dec1a1852dc87b48e45a2485a4e01
Author: kvrooman <vrooman.kyle@gmail.com>
Date:   Mon Oct 7 10:16:18 2019 -0500

    Smart Mask Exposure for Extraction & Training (#831)
    
    Smart Masks - Initial Commit

diff --git a/lib/cli.py b/lib/cli.py
index 9264e60..06350a5 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -15,7 +15,6 @@ from importlib import import_module
 
 from lib.logger import crash_log, log_setup
 from lib.utils import FaceswapError, get_backend, safe_shutdown
-from lib.model.masks import get_available_masks, get_default_mask
 from plugins.plugin_loader import PluginLoader
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -542,36 +541,65 @@ class ExtractArgs(ExtractConvertArgs):
                               "help": "Serializer for alignments file. If yaml is chosen and not "
                                       "available, then json will be used as the default "
                                       "fallback."})
-        argument_list.append({
-            "opts": ("-D", "--detector"),
-            "action": Radio,
-            "type": str.lower,
-            "choices":  PluginLoader.get_available_extractors("detect"),
-            "default": default_detector,
-            "group": "Plugins",
-            "help": "R|Detector to use. Some of these have configurable settings in "
-                    "'/config/extract.ini' or 'Settings > Configure Extract Plugins':"
-                    "\nL|cv2-dnn: A CPU only extractor, is the least reliable, but uses least "
-                    "resources and runs fast on CPU. Use this if not using a GPU and time is "
-                    "important."
-                    "\nL|mtcnn: Fast on CPU, Faster on GPU. Uses far fewer resources than other "
-                    "GPU detectors but can often return more false positives."
-                    "\nL|s3fd: Fast on GPU, slow on CPU. Can detect more faces and "
-                    "fewer false positives than other GPU detectors, but is a lot more resource "
-                    "intensive."})
-        argument_list.append({
-            "opts": ("-A", "--aligner"),
-            "action": Radio,
-            "type": str.lower,
-            "choices": PluginLoader.get_available_extractors("align"),
-            "default": default_aligner,
-            "group": "Plugins",
-            "help": "R|Aligner to use."
-                    "\nL|cv2-dnn: A cpu only CNN based landmark detector. Faster, less "
-                    "resource intensive, but less accurate. Only use this if not using a gpu "
-                    " and time is important."
-                    "\nL|fan: Face Alignment Network. Best aligner. GPU "
-                    "heavy, slow when not running on GPU"})
+        argument_list.append({"opts": ("-D", "--detector"),
+                              "action": Radio,
+                              "type": str.lower,
+                              "choices":  PluginLoader.get_available_extractors("detect"),
+                              "default": default_detector,
+                              "group": "Plugins",
+                              "help": "R|Detector to use. Some of these have configurable "
+                                      "settings in '/config/extract.ini' or 'Settings > Configure "
+                                      "Extract 'Plugins':"
+                                      "\nL|cv2-dnn: A CPU only extractor which is the least "
+                                      "reliable and least resource intensive. Use this if not "
+                                      "using a GPU and time is important."
+                                      "\nL|mtcnn: Good detector. Fast on CPU, faster on GPU. Uses "
+                                      "fewer resources than other GPU detectors but can often "
+                                      "return more false positives."
+                                      "\nL|s3fd: Best detector. Fast on GPU, slow on CPU. Can "
+                                      "detect more faces and fewer false positives than other "
+                                      "GPU detectors, but is a lot more resource intensive."})
+        argument_list.append({"opts": ("-A", "--aligner"),
+                              "action": Radio,
+                              "type": str.lower,
+                              "choices": PluginLoader.get_available_extractors("align"),
+                              "default": default_aligner,
+                              "group": "Plugins",
+                              "help": "R|Aligner to use."
+                                      "\nL|cv2-dnn: A CPU only landmark detector. Faster, less "
+                                      "resource intensive, but less accurate. Only use this if "
+                                      "not using a GPU and time is important."
+                                      "\nL|fan: Best aligner. Fast on GPU, slow on CPU."})
+        argument_list.append({"opts": ("-M", "--masker"),
+                              "action": Radio,
+                              "type": str.lower,
+                              "choices": PluginLoader.get_available_extractors("mask"),
+                              "default": "components",
+                              "group": "Plugins",
+                              "help": "R|Masker to use."
+                                      "\nL|none: An array of all ones is created to provide a 4th "
+                                      "channel that will not mask any portion of the image."
+                                      "\nL|components: Mask designed to provide facial "
+                                      "segmentation based on the positioning of landmark "
+                                      "locations. A convenx hull is constructed around the "
+                                      "exterior of the landmarks to create a mask."
+                                      "\nL|extended: Mask designed to provide facial segmentation "
+                                      "based on the positioning of landmark locations. A convenx "
+                                      "hull is constructed around the exterior of the landmarks "
+                                      "and the mask is extended upwards onto the forehead."
+                                      "\nL|vgg-clear: Mask designed to provide smart segmentation "
+                                      "of mostly frontal faces clear of obstructions.  Profile "
+                                      "faces and obstructions may result in sub-par performance."
+                                      "\nL|vgg-obstructed: Mask designed to provide smart "
+                                      "segmentation of mostly frontal faces. The mask model has "
+                                      "been specifically trained to recognize some facial "
+                                      "obstructions (hands and eyeglasses). Profile faces may "
+                                      "result in sub-par performance."
+                                      "\nL|unet-dfl: Mask designed to provide smart segmentation "
+                                      "of mostly frontal faces. The mask model has been trained "
+                                      "by community members and will need testing for further "
+                                      "description. Profile faces may result in sub-par "
+                                      "performance."})
         argument_list.append({"opts": ("-nm", "--normalization"),
                               "action": Radio,
                               "type": str.lower,
@@ -791,7 +819,7 @@ class ConvertArgs(ExtractConvertArgs):
             "action": Radio,
             "type": str.lower,
             "dest": "mask_type",
-            "choices": get_available_masks() + ["predicted"],
+            "choices": ["dfl_full", "components", "extended", "predicted"],
             "group": "plugins",
             "default": "predicted",
             "help": "R|Mask to use to replace faces. Blending of the masks can be adjusted in "
@@ -803,8 +831,7 @@ class ConvertArgs(ExtractConvertArgs):
                     "further up the forehead. May perform badly on difficult angles."
                     "\nL|facehull: Face cutout based on landmarks."
                     "\nL|predicted: The predicted mask generated from the model. If the model was "
-                    "not trained with a mask then this will fallback to "
-                    "'{}'".format(get_default_mask()) +
+                    "not trained with a mask then this will fallback to components."
                     "\nL|none: Don't use a mask."})
         argument_list.append({
             "opts": ("-sc", "--scaling"),
diff --git a/lib/convert.py b/lib/convert.py
index 3f782f2..bd7d6cd 100644
--- a/lib/convert.py
+++ b/lib/convert.py
@@ -141,7 +141,7 @@ class Converter():
                                            predicted["detected_faces"]):
             predicted_mask = new_face[:, :, -1] if new_face.shape[2] == 4 else None
             new_face = new_face[:, :, :3]
-            src_face = detected_face.reference_face
+            src_face = detected_face.reference_face / np.array(255.0, dtype="float32")
             interpolator = detected_face.reference_interpolators[1]
 
             new_face = self.pre_warp_adjustments(src_face, new_face, detected_face, predicted_mask)
diff --git a/lib/face_filter.py b/lib/face_filter.py
index 36ef194..096709d 100644
--- a/lib/face_filter.py
+++ b/lib/face_filter.py
@@ -36,7 +36,7 @@ class FaceFilter():
         # already performed allocation. For now we force CPU detectors.
 
         # self.align_faces(detector, aligner, multiprocess)
-        self.align_faces("cv2-dnn", "cv2-dnn", multiprocess)
+        self.align_faces("cv2-dnn", "cv2-dnn", "none", multiprocess)
 
         self.get_filter_encodings()
         self.threshold = threshold
@@ -56,9 +56,12 @@ class FaceFilter():
         return retval
 
     # Extraction pipeline
-    def align_faces(self, detector_name, aligner_name, multiprocess):
+    def align_faces(self, detector_name, aligner_name, masker_name, multiprocess):
         """ Use the requested detectors to retrieve landmarks for filter images """
-        extractor = Extractor(detector_name, aligner_name, multiprocess=multiprocess)
+        extractor = Extractor(detector_name,
+                              aligner_name,
+                              masker_name,
+                              multiprocess=multiprocess)
         self.run_extractor(extractor)
         del extractor
         self.load_aligned_face()
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 94707b0..21bb53a 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -40,18 +40,22 @@ class DetectedFace():
         The 68 point landmarks as discovered in :mod:`plugins.extract.align`. Should be a ``list``
         of 68 `(x, y)` ``tuples`` with each of the landmark co-ordinates.
     """
-    def __init__(self, image=None, x=None, w=None, y=None, h=None, landmarks_xy=None):
-        logger.trace("Initializing %s: (image: %s, x: %s, w: %s, y: %s, h:%s, landmarks_xy: %s)",
+    def __init__(self, image=None, x=None, w=None, y=None, h=None,
+                 landmarks_xy=None, filename=None):
+        logger.trace("Initializing %s: (image: %s, x: %s, w: %s, y: %s, h:%s, "
+                     "landmarks_xy: %s, filename: %s)",
                      self.__class__.__name__,
                      image.shape if image is not None and image.any() else image,
-                     x, w, y, h, landmarks_xy)
+                     x, w, y, h, landmarks_xy, filename)
         self.image = image
         self.x = x
         self.w = w
         self.y = y
         self.h = h
         self.landmarks_xy = landmarks_xy
+        self.filename = filename
         self.hash = None
+        self.face = None
         """ str: The hash of the face. This cannot be set until the file is saved due to image
         compression, but will be set if loading data from :func:`from_alignment` """
 
@@ -81,9 +85,9 @@ class DetectedFace():
         return self.y + self.h
 
     @property
-    def _extract_ratio(self):
-        """ float: The ratio of padding to add for training images """
-        return 0.375
+    def training_coverage(self):
+        """ The coverage ratio to add for training images """
+        return 1.0
 
     def to_alignment(self):
         """  Return the detected face formatted for an alignments file
@@ -130,6 +134,7 @@ class DetectedFace():
         # Manual tool does not know the final hash so default to None
         self.hash = alignment.get("hash", None)
         if image is not None and image.any():
+            self.image = image
             self._image_to_face(image)
         logger.trace("Created from alignment: (x: %s, w: %s, y: %s. h: %s, "
                      "landmarks: %s)",
@@ -138,11 +143,11 @@ class DetectedFace():
     def _image_to_face(self, image):
         """ set self.image to be the cropped face from detected bounding box """
         logger.trace("Cropping face from image")
-        self.image = image[self.top: self.bottom,
+        self.face = image[self.top: self.bottom,
                            self.left: self.right]
 
     # <<< Aligned Face methods and properties >>> #
-    def load_aligned(self, image, size=256, dtype=None):
+    def load_aligned(self, image, size=256, coverage_ratio=1.0, dtype=None):
         """ Align a face from a given image.
 
         Aligning a face is a relatively expensive task and is not required for all uses of
@@ -159,8 +164,8 @@ class DetectedFace():
             The image that contains the face to be aligned
         size: int
             The size of the output face in pixels
-        align_eyes: bool, optional
-            Optionally perform additional alignment to align eyes. Default: `False`
+        coverage_ratio: float
+            The metric determining the field of view of the returned face
         dtype: str, optional
             Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``
 
@@ -177,9 +182,8 @@ class DetectedFace():
             logger.trace("Skipping alignment calculation for already aligned face")
         else:
             logger.trace("Loading aligned face: (size: %s, dtype: %s)", size, dtype)
-            padding = int(size * self._extract_ratio) // 2
             self.aligned["size"] = size
-            self.aligned["padding"] = padding
+            self.aligned["padding"] = self._padding_from_coverage(size, coverage_ratio)
             self.aligned["matrix"] = get_align_mat(self)
             self.aligned["face"] = None
         if image is not None and self.aligned["face"] is None:
@@ -188,7 +192,7 @@ class DetectedFace():
                 image,
                 self.aligned["matrix"],
                 size,
-                padding)
+                self.aligned["padding"])
             self.aligned["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded aligned face: %s", {k: str(v) if isinstance(v, np.ndarray) else v
@@ -198,8 +202,7 @@ class DetectedFace():
     def _padding_from_coverage(self, size, coverage_ratio):
         """ Return the image padding for a face from coverage_ratio set against a
             pre-padded training image """
-        adjusted_ratio = coverage_ratio - (1 - self._extract_ratio)
-        padding = round((size * adjusted_ratio) / 2)
+        padding = int((size * (coverage_ratio - 0.625)) / 2)
         logger.trace(padding)
         return padding
 
@@ -230,8 +233,10 @@ class DetectedFace():
         self.feed["padding"] = self._padding_from_coverage(size, coverage_ratio)
         self.feed["matrix"] = get_align_mat(self)
 
-        face = AlignerExtract().transform(image, self.feed["matrix"], size, self.feed["padding"])
-        face = np.clip(face[:, :, :3] / 255., 0., 1.)
+        face = AlignerExtract().transform(image,
+                                          self.feed["matrix"],
+                                          size,
+                                          self.feed["padding"])
         self.feed["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded feed face. (face_shape: %s, matrix: %s)",
@@ -270,7 +275,6 @@ class DetectedFace():
                                           self.reference["matrix"],
                                           size,
                                           self.reference["padding"])
-        face = np.clip(face[:, :, :3] / 255., 0., 1.)
         self.reference["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded reference face. (face_shape: %s, matrix: %s)",
@@ -335,6 +339,20 @@ class DetectedFace():
             return None
         return self.feed["face"]
 
+    @property
+    def feed_landmarks(self):
+        """ numpy.ndarray: The 68 point landmarks location transposed to the feed face box.
+        Only available after :func:`load_reference_face` has been called, otherwise returns
+        ``None``"""
+        if not self.feed:
+            return None
+        landmarks = AlignerExtract().transform_points(self.landmarks_xy,
+                                                      self.feed["matrix"],
+                                                      self.feed["size"],
+                                                      self.feed["padding"])
+        logger.trace("Returning: %s", landmarks)
+        return landmarks
+
     @property
     def _feed_matrix(self):
         """ numpy.ndarray: The adjusted matrix face sized for feeding into a model. Only available
diff --git a/lib/image.py b/lib/image.py
index 1c5a39f..f822d2b 100644
--- a/lib/image.py
+++ b/lib/image.py
@@ -56,7 +56,7 @@ def read_image(filename, raise_error=False):
     success = True
     image = None
     try:
-        image = cv2.imread(filename)
+        image = cv2.imread(filename, cv2.IMREAD_UNCHANGED)
         if image is None:
             raise ValueError
     except TypeError:
diff --git a/lib/training_data.py b/lib/training_data.py
index 52886b6..dce6367 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -11,7 +11,6 @@ import cv2
 from scipy.interpolate import griddata
 
 from lib.image import batch_convert_color, read_image_batch
-from lib.model import masks
 from lib.multithreading import BackgroundGenerator
 from lib.utils import FaceswapError
 
@@ -48,17 +47,12 @@ class TrainingDataGenerator():
         * **no_flip** (`bool`) - ``True`` if the image shouldn't be randomly flipped as part of \
         augmentation, otherwise ``False``
 
-        * **mask_type** (`str`) - The mask type to be used (as defined in \
-        :mod:`lib.model.masks`). If not ``None`` then the additional key ``landmarks`` must be \
-        provided.
-
         * **warp_to_landmarks** (`bool`) - ``True`` if the random warp method should warp to \
         similar landmarks from the other side, ``False`` if the standard random warp method \
         should be used. If ``True`` then the additional key ``landmarks`` must be provided.
 
-        * **landmarks** (`numpy.ndarray`, `optional`). Required if using a :attr:`mask_type` is \
-        not ``None`` or :attr:`warp_to_landmarks` is ``True``. The 68 point face landmarks from \
-        an alignments file.
+        * **landmarks** (`numpy.ndarray`, `optional`). Required if :attr:`warp_to_landmarks` is \
+        ``True``. The 68 point face landmarks from an alignments file.
 
     config: dict
         The configuration ``dict`` generated from :file:`config.train.ini` containing the trainer \
@@ -74,7 +68,6 @@ class TrainingDataGenerator():
         self._model_input_size = model_input_size
         self._model_output_shapes = model_output_shapes
         self._training_opts = training_opts
-        self._mask_class = self._set__mask_class()
         self._landmarks = self._training_opts.get("landmarks", None)
         self._nearest_landmarks = {}
 
@@ -130,8 +123,7 @@ class TrainingDataGenerator():
             :mod:`plugins.train.trainer._base` from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
-            in the ``dict`` if the :attr:`mask_type` is not ``None``
+            the format (`batchsize`, `height`, `width`, `1`).
 
             * **samples** (`numpy.ndarray`) - A 4-dimensional array containg the samples for \
             feeding to the model's predict function for generating preview and timelapse samples. \
@@ -154,18 +146,6 @@ class TrainingDataGenerator():
         return batcher.iterator()
 
     # << INTERNAL METHODS >> #
-    def _set__mask_class(self):
-        """ Returns the correct mask class from :mod:`lib`.model.masks` as defined in the
-        :attr:`mask_type` parameter. """
-        mask_type = self._training_opts.get("mask_type", None)
-        if mask_type:
-            logger.debug("Mask type: '%s'", mask_type)
-            _mask_class = getattr(masks, mask_type)
-        else:
-            _mask_class = None
-        logger.debug("Mask class: %s", _mask_class)
-        return _mask_class
-
     def _validate_samples(self, data):
         """ Ensures that the total number of images within :attr:`images` is greater or equal to
         the selected :attr:`batchsize`. Raises an exception if this is not the case. """
@@ -207,24 +187,23 @@ class TrainingDataGenerator():
         logger.trace("Process batch: (filenames: '%s', side: '%s')", filenames, side)
         batch = read_image_batch(filenames)
         processed = dict()
-        to_landmarks = self._training_opts["warp_to_landmarks"]
 
         # Initialize processing training size on first image
         if not self._processing.initialized:
             self._processing.initialize(batch.shape[1])
 
         # Get Landmarks prior to manipulating the image
-        if self._mask_class or to_landmarks:
+        if self._training_opts["warp_to_landmarks"]:
             batch_src_pts = self._get_landmarks(filenames, batch, side)
+            batch_dst_pts = self._get_closest_match(filenames, side, batch_src_pts)
+            warp_kwargs = dict(batch_src_points=batch_src_pts,
+                               batch_dst_points=batch_dst_pts)
+        else:
+            warp_kwargs = dict()
 
-        # Color augmentation before mask is added
+        # Color Augmentation of the image only
         if self._training_opts["augment_color"]:
-            batch = self._processing.color_adjust(batch)
-
-        # Add mask to batch prior to transforms and warps
-        if self._mask_class:
-            batch = np.array([self._mask_class(src_pts, image, channels=4).mask
-                              for src_pts, image in zip(batch_src_pts, batch)])
+            batch[..., :3] = self._processing.color_adjust(batch[..., :3])
 
         # Random Transform and flip
         batch = self._processing.transform(batch)
@@ -238,15 +217,10 @@ class TrainingDataGenerator():
         # Get Targets
         processed.update(self._processing.get_targets(batch))
 
-        # Random Warp
-        if to_landmarks:
-            warp_kwargs = dict(batch_src_points=batch_src_pts,
-                               batch_dst_points=self._get_closest_match(filenames,
-                                                                        side,
-                                                                        batch_src_pts))
-        else:
-            warp_kwargs = dict()
-        processed["feed"] = self._processing.warp(batch[..., :3], to_landmarks, **warp_kwargs)
+        # Random Warp # TODO change masks to have a input mask and a warped target mask
+        processed["feed"] = [self._processing.warp(batch[..., :3],
+                                                   self._training_opts["warp_to_landmarks"],
+                                                   **warp_kwargs)]
 
         logger.trace("Processed batch: (filenames: %s, side: '%s', processed: %s)",
                      filenames,
@@ -258,8 +232,8 @@ class TrainingDataGenerator():
 
     def _get_landmarks(self, filenames, batch, side):
         """ Obtains the 68 Point Landmarks for the images in this batch. This is only called if
-        config item ``warp_to_landmarks`` is ``True`` or if :attr:`mask_type` is not ``None``. If
-        the landmarks for an image cannot be found, then an error is raised. """
+        config item ``warp_to_landmarks`` is ``True``. If the landmarks for an image cannot be
+        found, then an error is raised. """
         logger.trace("Retrieving landmarks: (filenames: %s, side: '%s')", filenames, side)
         src_points = [self._landmarks[side].get(sha1(face).hexdigest(), None) for face in batch]
 
@@ -270,7 +244,7 @@ class TrainingDataGenerator():
             msg = ("Files missing alignments for this batch: {}"
                    "\nAt least one of your images does not have a matching entry in your "
                    "alignments file."
-                   "\nIf you are training with a mask or using 'warp to landmarks' then every "
+                   "\nIf you are using 'warp to landmarks' then every "
                    "face you intend to train on must exist within the alignments file."
                    "\nThe specific files that caused this failure are listed above."
                    "\nMost likely there will be more than just these files missing from the "
@@ -449,18 +423,17 @@ class ImageAugmentation():
             output they will be returned as their own item from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
-            in the ``dict`` if a batch of 4 channel images has been passed in :attr:`batch`
+            the format (`batchsize`, `height`, `width`, `1`).
         """
         logger.trace("Compiling targets")
         slices = self._constants["tgt_slices"]
         target_batch = [np.array([cv2.resize(image[slices, slices, :],
                                              (size, size),
                                              cv2.INTER_AREA)
-                                  for image in batch])
+                                  for image in batch], dtype='float32') / 255.
                         for size in self._output_sizes]
         logger.trace("Target image shapes: %s",
-                     [tgt.shape for tgt_images in target_batch for tgt in tgt_images])
+                     [tgt_images.shape[1:] for tgt_images in target_batch])
 
         retval = self._separate_target_mask(target_batch)
         logger.trace("Final targets: %s",
@@ -469,25 +442,19 @@ class ImageAugmentation():
         return retval
 
     @staticmethod
-    def _separate_target_mask(batch):
+    def _separate_target_mask(size_list_of_batches):
         """ Return the batch and the batch of final masks
 
         Returns the targets as a list of 4-dimensional ``numpy.ndarray`` s of shape (`batchsize`,
         `height`, `width`, 3). If the :attr:`batch` is 4 channels, then the masks will be split
         from the batch, with the largest output masks being returned in their own item.
         """
-        batch = [tgt.astype("float32") / 255.0 for tgt in batch]
-        if all(tgt.shape[-1] == 4 for tgt in batch):
-            logger.trace("Batch contains mask")
-            sizes = [item.shape[1] for item in batch]
-            mask_batch = np.expand_dims(batch[sizes.index(max(sizes))][..., -1], axis=-1)
-            batch = [item[..., :3] for item in batch]
-            logger.trace("batch shapes: %s, mask_batch shape: %s",
-                         [tgt.shape for tgt in batch], mask_batch.shape)
-            retval = dict(targets=batch, masks=mask_batch)
+        targets = [batch[..., :3] for batch in size_list_of_batches]
+        if size_list_of_batches[-1].shape[-1] == 4:
+            masks = [size_list_of_batches[-1][..., 3:]]
         else:
-            logger.trace("Batch has no mask")
-            retval = dict(targets=batch)
+            masks = [np.ones((size_list_of_batches[-1].shape[:-1] + (1,)), dtype='float32')]
+        retval = dict(targets=targets, masks=masks)
         return retval
 
     # <<< COLOR AUGMENTATION >>> #
diff --git a/lib/vgg_face.py b/lib/vgg_face.py
index cd38270..ed92fc8 100644
--- a/lib/vgg_face.py
+++ b/lib/vgg_face.py
@@ -38,7 +38,7 @@ class VGGFace():
     def get_model(self, git_model_id, model_filename, backend):
         """ Check if model is available, if not, download and unzip it """
         root_path = os.path.abspath(os.path.dirname(sys.argv[0]))
-        cache_path = os.path.join(root_path, "plugins", "extract", ".cache")
+        cache_path = os.path.join(root_path, "plugins", "extract", "recognition", ".cache")
         model = GetModel(model_filename, cache_path, git_model_id).model_path
         model = cv2.dnn.readNetFromCaffe(model[1], model[0])  # pylint: disable=no-member
         model.setPreferableTarget(self.get_backend(backend))
@@ -57,7 +57,7 @@ class VGGFace():
         """ Return encodings for given image from vgg_face """
         if face.shape[0] != self.input_size:
             face = self.resize_face(face)
-        blob = cv2.dnn.blobFromImage(face,  # pylint: disable=no-member
+        blob = cv2.dnn.blobFromImage(face[..., :3],  # pylint: disable=no-member
                                      1.0,
                                      (self.input_size, self.input_size),
                                      self.average_img,
diff --git a/lib/vgg_face2_keras.py b/lib/vgg_face2_keras.py
index 5b11fc3..d66cf99 100644
--- a/lib/vgg_face2_keras.py
+++ b/lib/vgg_face2_keras.py
@@ -41,7 +41,7 @@ class VGGFace2():
     def get_model(self, git_model_id, model_filename, backend):
         """ Check if model is available, if not, download and unzip it """
         root_path = os.path.abspath(os.path.dirname(sys.argv[0]))
-        cache_path = os.path.join(root_path, "plugins", "extract", ".cache")
+        cache_path = os.path.join(root_path, "plugins", "extract", "recognition", ".cache")
         model = GetModel(model_filename, cache_path, git_model_id).model_path
         if backend == "CPU":
             if os.environ.get("KERAS_BACKEND", "") == "plaidml.keras.backend":
@@ -63,7 +63,7 @@ class VGGFace2():
         """ Return encodings for given image from vgg_face """
         if face.shape[0] != self.input_size:
             face = self.resize_face(face)
-        face = np.expand_dims(face - self.average_img, axis=0)
+        face = face[None, :, :, :3] - self.average_img
         preds = self.model.predict(face)
         return preds[0, :]
 
diff --git a/plugins/extract/_base.py b/plugins/extract/_base.py
index 5382bd3..8085499 100644
--- a/plugins/extract/_base.py
+++ b/plugins/extract/_base.py
@@ -306,7 +306,7 @@ class Extractor():
             logger.debug("No git_model_id specified. Returning None")
             return None
         plugin_path = os.path.join(*self.__module__.split(".")[:-1])
-        if os.path.basename(plugin_path) in ("detect", "align"):
+        if os.path.basename(plugin_path) in ("detect", "align", "mask", "recognition"):
             base_path = os.path.dirname(os.path.realpath(sys.argv[0]))
             cache_path = os.path.join(base_path, plugin_path, ".cache")
         else:
@@ -322,13 +322,13 @@ class Extractor():
         """
         logger.debug("initialize %s: (args: %s, kwargs: %s)",
                      self.__class__.__name__, args, kwargs)
-        p_type = "Detector" if self._plugin_type == "detect" else "Aligner"
-        logger.info("Initializing %s %s...", self.name, p_type)
+        logger.info("Initializing %s in %s phase...", self.name, self._plugin_type)
         self.queue_size = 1
         self._add_queues(kwargs["in_queue"], kwargs["out_queue"], ["predict", "post"])
         self._compile_threads()
         self.init_model()
-        logger.info("Initialized %s %s with batchsize of %s", self.name, p_type, self.batchsize)
+        logger.info("Initialized %s (%s) with batchsize of %s",
+                    self.name, self._plugin_type, self.batchsize)
 
     def _add_queues(self, in_queue, out_queue, queues):
         """ Add the queues
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index c552741..4c2d9b0 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -50,10 +50,10 @@ class Aligner(Extractor):
     plugins.extract.align : Aligner plugins
     plugins.extract._base : Parent class for all extraction plugins
     plugins.extract.detect._base : Detector parent class for extraction plugins.
-
+    plugins.extract.mask._base : Masker parent class for extraction plugins.
     """
 
-    def __init__(self, git_model_id, model_filename,
+    def __init__(self, git_model_id=None, model_filename=None,
                  configfile=None, normalize_method=None):
         logger.debug("Initializing %s: (normalize_method: %s)", self.__class__.__name__,
                      normalize_method)
@@ -183,9 +183,14 @@ class Aligner(Extractor):
 
         """
 
-        for face, landmarks in zip(batch["detected_faces"], batch["landmarks"]):
+        generator = zip(batch["detected_faces"],
+                        batch["filename"],
+                        batch["image"],
+                        batch["landmarks"])
+        for face, filename, image, landmarks in generator:
             face.landmarks_xy = [(int(round(pt[0])), int(round(pt[1]))) for pt in landmarks]
-
+            face.image = image
+            face.filename = filename
         self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
         logger.trace("Item out: %s", {key: val
                                       for key, val in batch.items()
diff --git a/plugins/extract/align/cv2_dnn.py b/plugins/extract/align/cv2_dnn.py
index 919e35e..577a964 100644
--- a/plugins/extract/align/cv2_dnn.py
+++ b/plugins/extract/align/cv2_dnn.py
@@ -41,6 +41,7 @@ class Align(Aligner):
         self.input_size = 128
         self.colorformat = "RGB"
         self.vram = 0  # Doesn't use GPU
+        self.vram_per_batch = 0
         self.batchsize = 1
 
     def init_model(self):
@@ -52,7 +53,7 @@ class Align(Aligner):
         """ Compile the detected faces for prediction """
         faces, batch["roi"] = self.align_image(batch["detected_faces"])
         faces = self._normalize_faces(faces)
-        batch["feed"] = np.array(faces, dtype="float32").transpose((0, 3, 1, 2))
+        batch["feed"] = np.array(faces, dtype="float32")[..., :3].transpose((0, 3, 1, 2))
         return batch
 
     def align_image(self, detected_faces):
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
index 71e5606..bee8764 100644
--- a/plugins/extract/align/fan.py
+++ b/plugins/extract/align/fan.py
@@ -48,7 +48,7 @@ class Align(Aligner):
         faces = self.crop(batch)
         logger.trace("Aligned image around center")
         faces = self._normalize_faces(faces)
-        batch["feed"] = np.array(faces, dtype="float32").transpose((0, 3, 1, 2)) / 255.0
+        batch["feed"] = np.array(faces, dtype="float32")[..., :3].transpose((0, 3, 1, 2)) / 255.0
         return batch
 
     def get_center_scale(self, detected_faces):
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
old mode 100755
new mode 100644
index 6a29de1..81907e0
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -53,7 +53,7 @@ class Detector(Extractor):
     plugins.extract.detect : Detector plugins
     plugins.extract._base : Parent class for all extraction plugins
     plugins.extract.align._base : Aligner parent class for extraction plugins.
-
+    plugins.extract.mask._base : Masker parent class for extraction plugins.
     """
 
     def __init__(self, git_model_id=None, model_filename=None,
@@ -228,7 +228,7 @@ class Detector(Extractor):
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
     def _compile_detection_image(self, input_image):
         """ Compile the detection image for feeding into the model"""
-        image = self._convert_color(input_image)
+        image = self._convert_color(input_image[..., :3])
 
         image_size = image.shape[:2]
         scale = self._set_scale(image_size)
@@ -272,13 +272,12 @@ class Detector(Extractor):
             pad_r = (self.input_size - width) - pad_l
             pad_t = (self.input_size - height) // 2
             pad_b = (self.input_size - height) - pad_t
-            image = cv2.copyMakeBorder(  # pylint:disable=no-member
-                image,
-                pad_t,
-                pad_b,
-                pad_l,
-                pad_r,
-                cv2.BORDER_CONSTANT)  # pylint:disable=no-member
+            image = cv2.copyMakeBorder(image,  # pylint:disable=no-member
+                                       pad_t,
+                                       pad_b,
+                                       pad_l,
+                                       pad_r,
+                                       cv2.BORDER_CONSTANT)  # pylint:disable=no-member
         logger.trace("Padded image shape: %s", image.shape)
         return image
 
@@ -289,11 +288,11 @@ class Detector(Extractor):
             or face falls entirely outside of image """
         dims = [img.shape[:2] for img in batch["image"]]
         logger.trace("image dims: %s", dims)
-        batch["detected_faces"] = [[face for face in faces
+        batch["detected_faces"] = [[face
+                                    for face in faces
                                     if face.right > 0 and face.left < dim[1]
                                     and face.bottom > 0 and face.top < dim[0]]
-                                   for dim, faces in zip(dims,
-                                                         batch.get("detected_faces", list()))]
+                                   for dim, faces in zip(dims, batch.get("detected_faces", []))]
 
     def _filter_small_faces(self, detected_faces):
         """ Filter out any faces smaller than the min size threshold """
diff --git a/plugins/extract/detect/cv2_dnn.py b/plugins/extract/detect/cv2_dnn.py
old mode 100755
new mode 100644
index 7e2330b..7e9dbb6
--- a/plugins/extract/detect/cv2_dnn.py
+++ b/plugins/extract/detect/cv2_dnn.py
@@ -15,6 +15,7 @@ class Detect(Detector):
         self.name = "cv2-DNN Detector"
         self.input_size = 300
         self.vram = 0  # CPU Only. Doesn't use VRAM
+        self.vram_per_batch = 0
         self.batchsize = 1
         self.confidence = self.config["confidence"] / 100
 
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
old mode 100755
new mode 100644
diff --git a/plugins/extract/.cache/.keep b/plugins/extract/mask/.cache/.keep
similarity index 100%
rename from plugins/extract/.cache/.keep
rename to plugins/extract/mask/.cache/.keep
diff --git a/plugins/extract/mask/__init__.py b/plugins/extract/mask/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/extract/mask/_base.py b/plugins/extract/mask/_base.py
new file mode 100644
index 0000000..9539ab8
--- /dev/null
+++ b/plugins/extract/mask/_base.py
@@ -0,0 +1,258 @@
+#!/usr/bin/env python3
+""" Base class for Face Masker plugins
+    Plugins should inherit from this class
+
+    See the override methods for which methods are required.
+
+    The plugin will receive a dict containing:
+    {"filename": <filename of source frame>,
+     "image": <source image>,
+     "detected_faces": <list of bounding box dicts from lib/plugins/extract/detect/_base>}
+
+    For each source item, the plugin must pass a dict to finalize containing:
+    {"filename": <filename of source frame>,
+     "image": <four channel source image>,
+     "detected_faces": <list of bounding box dicts from lib/plugins/extract/detect/_base>
+    """
+
+import logging
+import os
+import traceback
+import cv2
+import numpy as np
+import keras
+
+from io import StringIO
+from lib.faces_detect import DetectedFace
+from lib.aligner import Extract
+from plugins.extract._base import Extractor, logger
+
+logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
+
+
+class Masker(Extractor):
+    """ Aligner plugin _base Object
+
+    All Aligner plugins must inherit from this class
+
+    Parameters
+    ----------
+    git_model_id: int
+        The second digit in the github tag that identifies this model. See
+        https://github.com/deepfakes-models/faceswap-models for more information
+    model_filename: str
+        The name of the model file to be loaded
+    normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional
+        Normalize the images fed to the aligner. Default: ``None``
+
+    Other Parameters
+    ----------------
+    configfile: str, optional
+        Path to a custom configuration ``ini`` file. Default: Use system configfile
+
+    See Also
+    --------
+    plugins.extract.align : Aligner plugins
+    plugins.extract._base : Parent class for all extraction plugins
+    plugins.extract.detect._base : Detector parent class for extraction plugins.
+    plugins.extract.align._base : Aligner parent class for extraction plugins.
+    """
+
+    def __init__(self, git_model_id=None, model_filename=None,
+                 configfile=None, input_size=256, output_size=256, coverage_ratio=1.):
+        logger.debug("Initializing %s: (configfile: %s, input_size: %s, "
+                     "output_size: %s, coverage_ratio: %s)",
+                     self.__class__.__name__, configfile, input_size, output_size, coverage_ratio)
+        super().__init__(git_model_id,
+                         model_filename,
+                         configfile=configfile)
+        self.input_size = input_size
+        self.output_size = output_size
+        self.coverage_ratio = coverage_ratio
+        self.extract = Extract()
+
+        self._plugin_type = "mask"
+        self._faces_per_filename = dict()  # Tracking for recompiling face batches
+        self._rollover = []  # Items that are rolled over from the previous batch in get_batch
+        self._output_faces = []
+        logger.debug("Initialized %s", self.__class__.__name__)
+
+    def get_batch(self, queue):
+        """ Get items for inputting into the aligner from the queue in batches
+
+        Items are returned from the ``queue`` in batches of
+        :attr:`~plugins.extract._base.Extractor.batchsize`
+
+        To ensure consistent batchsizes for aligner the items are split into separate items for
+        each :class:`lib.faces_detect.DetectedFace` object.
+
+        Remember to put ``'EOF'`` to the out queue after processing
+        the final batch
+
+        Outputs items in the following format. All lists are of length
+        :attr:`~plugins.extract._base.Extractor.batchsize`:
+
+        >>> {'filename': [<filenames of source frames>],
+        >>>  'image': [<source images>],
+        >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
+
+        Parameters
+        ----------
+        queue : queue.Queue()
+            The ``queue`` that the plugin will be fed from.
+
+        Returns
+        -------
+        exhausted, bool
+            ``True`` if queue is exhausted, ``False`` if not
+        batch, dict
+            A dictionary of lists of :attr:`~plugins.extract._base.Extractor.batchsize`:
+        """
+        exhausted = False
+        batch = dict()
+        idx = 0
+        while idx < self.batchsize:
+            item = self._collect_item(queue)
+            if item == "EOF":
+                logger.trace("EOF received")
+                exhausted = True
+                break
+            # Put frames with no faces into the out queue to keep TQDM consistent
+            if not item["detected_faces"]:
+                self._queues["out"].put(item)
+                continue
+            for f_idx, face in enumerate(item["detected_faces"]):
+                batch.setdefault("detected_faces", []).append(face)
+                batch.setdefault("filename", []).append(item["filename"])
+                batch.setdefault("image", []).append(item["image"])
+                idx += 1
+                if idx == self.batchsize:
+                    frame_faces = len(item["detected_faces"])
+                    if f_idx + 1 != frame_faces:
+                        self._rollover = {k: v[f_idx + 1:] if k == "detected_faces" else v
+                                          for k, v in item.items()}
+                        logger.trace("Rolled over %s faces of %s to next batch for '%s'",
+                                     len(self._rollover["detected_faces"]),
+                                     frame_faces, item["filename"])
+                    break
+        if batch:
+            logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
+                                                 for k, v in batch.items()})
+        else:
+            logger.trace(item)
+        return exhausted, batch
+
+    def _collect_item(self, queue):
+        """ Collect the item from the _rollover dict or from the queue
+            Add face count per frame to self._faces_per_filename for joining
+            batches back up in finalize """
+        if self._rollover:
+            logger.trace("Getting from _rollover: (filename: `%s`, faces: %s)",
+                         self._rollover["filename"], len(self._rollover["detected_faces"]))
+            item = self._rollover
+            self._rollover = dict()
+        else:
+            item = self._get_item(queue)
+            if item != "EOF":
+                logger.trace("Getting from queue: (filename: %s, faces: %s)",
+                             item["filename"], len(item["detected_faces"]))
+                self._faces_per_filename[item["filename"]] = len(item["detected_faces"])
+        return item
+
+    def _predict(self, batch):
+        """ Just return the aligner's predict function """
+        return self.predict(batch)
+
+    def finalize(self, batch):
+        """ Finalize the output from Aligner
+
+        This should be called as the final task of each `plugin`.
+
+        It strips unneeded items from the :attr:`batch` ``dict`` and pairs the detected faces back
+        up with their original frame before yielding each frame.
+
+        Outputs items in the format:
+
+        >>> {'image': [<original frame>],
+        >>>  'filename': [<frame filename>),
+        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
+
+        Parameters
+        ----------
+        batch : dict
+            The final ``dict`` from the `plugin` process. It must contain the `keys`:
+            ``detected_faces``, ``landmarks``, ``filename``, ``image``
+
+        Yields
+        ------
+        dict
+            A ``dict`` for each frame containing the ``image``, ``filename`` and list of
+            :class:`lib.faces_detect.DetectedFace` objects.
+
+        """
+        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
+        logger.trace("Item out: %s", {key: val
+                                      for key, val in batch.items()
+                                      if key != "image"})
+        for filename, image, face in zip(batch["filename"],
+                                         batch["image"],
+                                         batch["detected_faces"]):
+            self._output_faces.append(face)
+            if len(self._output_faces) != self._faces_per_filename[filename]:
+                continue
+            retval = dict(filename=filename, image=image, detected_faces=self._output_faces)
+
+            self._output_faces = []
+            logger.trace("Yielding: (filename: '%s', image: %s, detected_faces: %s)",
+                         retval["filename"], retval["image"].shape, len(retval["detected_faces"]))
+            yield retval
+
+    # <<< PROTECTED ACCESS METHODS >>> #
+    @staticmethod
+    def _resize(image, target_size):
+        """ resize input and output of mask models appropriately """
+        height, width, channels = image.shape
+        image_size = max(height, width)
+        scale = target_size / image_size
+        if scale == 1.:
+            return image
+        method = cv2.INTER_CUBIC if scale > 1. else cv2.INTER_AREA  # pylint: disable=no-member
+        resized = cv2.resize(image, (0, 0), fx=scale, fy=scale, interpolation=method)
+        resized = resized if channels > 1 else resized[..., None]
+        return resized
+
+    @staticmethod
+    def postprocessing(mask):
+        """ Post-processing of Nirkin style segmentation masks """
+        # Select_largest_segment
+        if pop_small_segments:
+            results = cv2.connectedComponentsWithStats(mask,  # pylint: disable=no-member
+                                                       4,
+                                                       cv2.CV_32S)  # pylint: disable=no-member
+            _, labels, stats, _ = results
+            segments_ranked_by_area = np.argsort(stats[:, -1])[::-1]
+            mask[labels != segments_ranked_by_area[0, 0]] = 0.
+
+        # Smooth contours
+        if smooth_contours:
+            iters = 2
+            kernel = cv2.getStructuringElement(cv2.MORPH_RECT,  # pylint: disable=no-member
+                                               (5, 5))
+            cv2.morphologyEx(mask, cv2.MORPH_OPEN,  # pylint: disable=no-member
+                             kernel, iterations=iters)
+            cv2.morphologyEx(mask, cv2.MORPH_CLOSE,  # pylint: disable=no-member
+                             kernel, iterations=iters)
+            cv2.morphologyEx(mask, cv2.MORPH_CLOSE,  # pylint: disable=no-member
+                             kernel, iterations=iters)
+            cv2.morphologyEx(mask, cv2.MORPH_OPEN,  # pylint: disable=no-member
+                             kernel, iterations=iters)
+
+        # Fill holes
+        if fill_holes:
+            not_holes = mask.copy()
+            not_holes = np.pad(not_holes, ((2, 2), (2, 2), (0, 0)), 'constant')
+            cv2.floodFill(not_holes, None, (0, 0), 255)  # pylint: disable=no-member
+            holes = cv2.bitwise_not(not_holes)[2:-2, 2:-2]  # pylint: disable=no-member
+            mask = cv2.bitwise_or(mask, holes)  # pylint: disable=no-member
+            mask = np.expand_dims(mask, axis=-1)
+        return mask
diff --git a/plugins/extract/mask/components.py b/plugins/extract/mask/components.py
new file mode 100644
index 0000000..af225e0
--- /dev/null
+++ b/plugins/extract/mask/components.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python3
+
+import cv2
+import numpy as np
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = None
+        model_filename = None
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "Components"
+        self.colorformat = "BGR"
+        self.vram = 0
+        self.vram_warnings = 0
+        self.vram_per_batch = 30
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        logger.debug("No mask model to initialize")
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        batch["feed"] = np.array([face.image for face in batch["detected_faces"]])
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        masks = np.zeros(batch["feed"].shape[:-1] + (1,), dtype='uint8')
+        for mask, face in zip(masks, batch["detected_faces"]):
+            parts = self.parse_parts(np.array(face.landmarks_xy))
+            for item in parts:
+                item = np.concatenate(item)
+                hull = cv2.convexHull(item).astype('int32')  # pylint: disable=no-member
+                cv2.fillConvexPoly(mask, hull, 255, lineType=cv2.LINE_AA)
+        batch["prediction"] = masks
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        generator = zip(batch["feed"], batch["detected_faces"], batch["prediction"])
+        for feed, face, prediction in generator:
+            face.image = np.concatenate((feed, prediction), axis=-1)
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+        return batch
+
+    @staticmethod
+    def parse_parts(landmarks):
+        """ Component facehull mask """
+        r_jaw = (landmarks[0:9], landmarks[17:18])
+        l_jaw = (landmarks[8:17], landmarks[26:27])
+        r_cheek = (landmarks[17:20], landmarks[8:9])
+        l_cheek = (landmarks[24:27], landmarks[8:9])
+        nose_ridge = (landmarks[19:25], landmarks[8:9],)
+        r_eye = (landmarks[17:22],
+                 landmarks[27:28],
+                 landmarks[31:36],
+                 landmarks[8:9])
+        l_eye = (landmarks[22:27],
+                 landmarks[27:28],
+                 landmarks[31:36],
+                 landmarks[8:9])
+        nose = (landmarks[27:31], landmarks[31:36])
+        parts = [r_jaw, l_jaw, r_cheek, l_cheek, nose_ridge, r_eye, l_eye, nose]
+        return parts
diff --git a/plugins/extract/mask/components_defaults.py b/plugins/extract/mask/components_defaults.py
new file mode 100644
index 0000000..721ee94
--- /dev/null
+++ b/plugins/extract/mask/components_defaults.py
@@ -0,0 +1,68 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap VGG clear plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "Components options. Mask designed to provide facial segmentation based on the positioning of "
+    "landmark locations. A convenx hull is constructed around the exterior of the landmarks to "
+    "create a mask."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/mask/extended.py b/plugins/extract/mask/extended.py
new file mode 100644
index 0000000..6e61733
--- /dev/null
+++ b/plugins/extract/mask/extended.py
@@ -0,0 +1,92 @@
+#!/usr/bin/env python3
+
+import cv2
+import numpy as np
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = None
+        model_filename = None
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "Extended"
+        self.colorformat = "BGR"
+        self.vram = 0
+        self.vram_warnings = 0
+        self.vram_per_batch = 30
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        logger.debug("No mask model to initialize")
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        batch["feed"] = np.array([face.image for face in batch["detected_faces"]])
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        masks = np.zeros(batch["feed"].shape[:-1] + (1,), dtype='uint8')
+        for mask, face in zip(masks, batch["detected_faces"]):
+            parts = self.parse_parts(np.array(face.landmarks_xy))
+            for item in parts:
+                item = np.concatenate(item)
+                hull = cv2.convexHull(item).astype('int32')  # pylint: disable=no-member
+                cv2.fillConvexPoly(mask, hull, 255, lineType=cv2.LINE_AA)
+        batch["prediction"] = masks
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        generator = zip(batch["feed"], batch["detected_faces"], batch["prediction"])
+        for feed, face, prediction in generator:
+            face.image = np.concatenate((feed, prediction), axis=-1)
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+        return batch
+
+    @staticmethod
+    def parse_parts(landmarks):
+        """ Extended facehull mask """
+        # mid points between the side of face and eye point
+        ml_pnt = (landmarks[36] + landmarks[0]) // 2
+        mr_pnt = (landmarks[16] + landmarks[45]) // 2
+
+        # mid points between the mid points and eye
+        ql_pnt = (landmarks[36] + ml_pnt) // 2
+        qr_pnt = (landmarks[45] + mr_pnt) // 2
+
+        # Top of the eye arrays
+        bot_l = np.array((ql_pnt, landmarks[36], landmarks[37], landmarks[38], landmarks[39]))
+        bot_r = np.array((landmarks[42], landmarks[43], landmarks[44], landmarks[45], qr_pnt))
+
+        # Eyebrow arrays
+        top_l = landmarks[17:22]
+        top_r = landmarks[22:27]
+
+        # Adjust eyebrow arrays
+        landmarks[17:22] = top_l + ((top_l - bot_l) // 2)
+        landmarks[22:27] = top_r + ((top_r - bot_r) // 2)
+
+        r_jaw = (landmarks[0:9], landmarks[17:18])
+        l_jaw = (landmarks[8:17], landmarks[26:27])
+        r_cheek = (landmarks[17:20], landmarks[8:9])
+        l_cheek = (landmarks[24:27], landmarks[8:9])
+        nose_ridge = (landmarks[19:25], landmarks[8:9],)
+        r_eye = (landmarks[17:22],
+                 landmarks[27:28],
+                 landmarks[31:36],
+                 landmarks[8:9])
+        l_eye = (landmarks[22:27],
+                 landmarks[27:28],
+                 landmarks[31:36],
+                 landmarks[8:9])
+        nose = (landmarks[27:31], landmarks[31:36])
+        parts = [r_jaw, l_jaw, r_cheek, l_cheek, nose_ridge, r_eye, l_eye, nose]
+        return parts
diff --git a/plugins/extract/mask/extended_defaults.py b/plugins/extract/mask/extended_defaults.py
new file mode 100644
index 0000000..f85996e
--- /dev/null
+++ b/plugins/extract/mask/extended_defaults.py
@@ -0,0 +1,68 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap extended mask plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "Extended options. Mask designed to provide facial segmentation based on the positioning of "
+    "landmark locations. A convenx hull is constructed around the landmarks and the mask is "
+    "extended upwards onto the forehead."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/mask/none.py b/plugins/extract/mask/none.py
new file mode 100644
index 0000000..7ba4403
--- /dev/null
+++ b/plugins/extract/mask/none.py
@@ -0,0 +1,46 @@
+#!/usr/bin/env python3
+
+import numpy as np
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = None
+        model_filename = None
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "without a mask"
+        self.colorformat = "BGR"
+        self.vram = 0
+        self.vram_warnings = 0
+        self.vram_per_batch = 30
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        logger.debug("No mask model to initialize")
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        batch["feed"] = np.array([face.image for face in batch["detected_faces"]])
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        batch["prediction"] = np.full(batch["feed"].shape[:-1] + (1,),
+                                      fill_value=255,
+                                      dtype='uint8')
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        generator = zip(batch["feed"], batch["detected_faces"], batch["prediction"])
+        for feed, face, prediction in generator:
+            face.image = np.concatenate((feed, prediction), axis=-1)
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+        return batch
diff --git a/plugins/extract/mask/none_defaults.py b/plugins/extract/mask/none_defaults.py
new file mode 100644
index 0000000..8a97a6e
--- /dev/null
+++ b/plugins/extract/mask/none_defaults.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap VGG clear plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "None options. An array of all ones is created to provide a 4th channel that will not mask "
+    "any portion of the image."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/mask/unet_dfl.py b/plugins/extract/mask/unet_dfl.py
new file mode 100644
index 0000000..9e2151f
--- /dev/null
+++ b/plugins/extract/mask/unet_dfl.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python3
+
+import cv2
+import keras
+import numpy as np
+from lib.model.session import KSession
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = 6
+        model_filename = "DFL_256_sigmoid_v1.h5"
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "U-Net Mask Network(256)"
+        self.mask_in_size = 256
+        self.colorformat = "BGR"
+        self.vram = 3440
+        self.vram_warnings = 1024  # TODO determine
+        self.vram_per_batch = 64  # TODO determine
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        self.model = KSession(self.name, self.model_path, model_kwargs=dict())
+        self.model.load_model()
+        self.input = np.zeros((self.batchsize, self.mask_in_size, self.mask_in_size, 3),
+                              dtype="float32")
+        self.model.predict(self.input)
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        for index, face in enumerate(batch["detected_faces"]):
+            face.load_aligned(face.image,
+                              size=self.mask_in_size,
+                              dtype='float32')
+            self.input[index] = face.aligned["face"][..., :3]
+        batch["feed"] = self.input / 255.
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        predictions = self.model.predict(batch["feed"])
+        batch["prediction"] = predictions * 255.
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        for idx, (face, predicts) in enumerate(zip(batch["detected_faces"], batch["prediction"])):
+            generator = (cv2.GaussianBlur(mask, (7, 7), 0) for mask in predicts)
+            predicted = np.array(tuple(generator))
+            predicted[predicted < 10.] = 0.
+            predicted[predicted > 245.] = 255.
+
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            feed_face = face.feed["face"][..., :3]
+            feed_mask = self._resize(predicted, self.input_size).astype('uint8')
+            batch["detected_faces"][idx].feed["face"] = np.concatenate((feed_face,
+                                                                        feed_mask),
+                                                                       axis=-1)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+            ref_face = face.reference["face"][..., :3]
+            ref_mask = self._resize(predicted, self.output_size).astype('uint8')
+            batch["detected_faces"][idx].reference["face"] = np.concatenate((ref_face,
+                                                                             ref_mask),
+                                                                            axis=-1)
+        return batch
diff --git a/plugins/extract/mask/unet_dfl_defaults.py b/plugins/extract/mask/unet_dfl_defaults.py
new file mode 100644
index 0000000..c153920
--- /dev/null
+++ b/plugins/extract/mask/unet_dfl_defaults.py
@@ -0,0 +1,68 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap UNET dfl plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "UNET_DFL options. Mask designed to provide smart segmentation of mostly frontal faces. "
+    "The mask model has been trained by community members. Insert more commentary on testing "
+    "here. Profile faces may result in sub-par performance."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/mask/vgg_clear.py b/plugins/extract/mask/vgg_clear.py
new file mode 100644
index 0000000..5ee02a8
--- /dev/null
+++ b/plugins/extract/mask/vgg_clear.py
@@ -0,0 +1,74 @@
+#!/usr/bin/env python3
+
+import cv2
+import keras
+import numpy as np
+from lib.model.session import KSession
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = 8
+        model_filename = "Nirkin_300_softmax_v1.h5"
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "VGG Mask Network(300)"
+        self.mask_in_size = 300
+        self.colorformat = "BGR"
+        self.vram = 2000  # TODO determine
+        self.vram_warnings = 1024  # TODO determine
+        self.vram_per_batch = 64  # TODO determine
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        self.model = KSession(self.name, self.model_path, model_kwargs=dict())
+        self.model.load_model()
+        o = keras.layers.core.Activation('softmax',
+                                         name='softmax')(self.model._model.layers[-1].output)
+        self.model._model = keras.models.Model(inputs=self.model._model.input, outputs=[o])
+        self.input = np.zeros((self.batchsize, self.mask_in_size, self.mask_in_size, 3),
+                              dtype="float32")
+        self.model.predict(self.input)
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        for index, face in enumerate(batch["detected_faces"]):
+            face.load_aligned(face.image,
+                              size=self.mask_in_size,
+                              dtype='float32')
+            self.input[index] = face.aligned["face"][..., :3]
+        batch["feed"] = self.input - np.mean(self.input, axis=(1, 2))[:, None, None, :]
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        predictions = self.model.predict(batch["feed"])
+        batch["prediction"] = predictions[..., 1:2] * 255.
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        for idx, (face, predicts) in enumerate(zip(batch["detected_faces"], batch["prediction"])):
+            generator = (cv2.GaussianBlur(mask, (7, 7), 0) for mask in predicts)
+            predicted = np.array(tuple(generator))
+            predicted[predicted < 10.] = 0.
+            predicted[predicted > 245.] = 255.
+
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            feed_face = face.feed["face"][..., :3]
+            feed_mask = self._resize(predicted, self.input_size).astype('uint8')
+            batch["detected_faces"][idx].feed["face"] = np.concatenate((feed_face,
+                                                                        feed_mask),
+                                                                       axis=-1)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+            ref_face = face.reference["face"][..., :3]
+            ref_mask = self._resize(predicted, self.output_size).astype('uint8')
+            batch["detected_faces"][idx].reference["face"] = np.concatenate((ref_face,
+                                                                             ref_mask),
+                                                                            axis=-1)
+        return batch
diff --git a/plugins/extract/mask/vgg_clear_defaults.py b/plugins/extract/mask/vgg_clear_defaults.py
new file mode 100644
index 0000000..5b12069
--- /dev/null
+++ b/plugins/extract/mask/vgg_clear_defaults.py
@@ -0,0 +1,67 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap VGG clear plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "VGG_Clear options. Mask designed to provide smart segmentation of mostly frontal faces clear "
+    "of obstructions.  Profile faces and obstructions may result in sub-par performance."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/mask/vgg_obstructed.py b/plugins/extract/mask/vgg_obstructed.py
new file mode 100644
index 0000000..85ae018
--- /dev/null
+++ b/plugins/extract/mask/vgg_obstructed.py
@@ -0,0 +1,74 @@
+#!/usr/bin/env python3
+
+import cv2
+import keras
+import numpy as np
+from lib.model.session import KSession
+from ._base import Masker, logger
+
+
+class Mask(Masker):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, **kwargs):
+        git_model_id = 5
+        model_filename = "Nirkin_500_softmax_v1.h5"
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "VGG Mask Network(500)"
+        self.mask_in_size = 500
+        self.colorformat = "BGR"
+        self.vram = 3000  # TODO determine
+        self.vram_warnings = 1024  # TODO determine
+        self.vram_per_batch = 64  # TODO determine
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        self.model = KSession(self.name, self.model_path, model_kwargs=dict())
+        self.model.load_model()
+        o = keras.layers.core.Activation('softmax',
+                                         name='softmax')(self.model._model.layers[-1].output)
+        self.model._model = keras.models.Model(inputs=self.model._model.input, outputs=[o])
+        self.input = np.zeros((self.batchsize, self.mask_in_size, self.mask_in_size, 3),
+                              dtype="float32")
+        self.model.predict(self.input)
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        for index, face in enumerate(batch["detected_faces"]):
+            face.load_aligned(face.image,
+                              size=self.mask_in_size,
+                              dtype='float32')
+            self.input[index] = face.aligned["face"][..., :3]
+        batch["feed"] = self.input - np.mean(self.input, axis=(1, 2))[:, None, None, :]
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        predictions = self.model.predict(batch["feed"])
+        batch["prediction"] = predictions[..., 0:1] * -255. + 255.
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        for idx, (face, predicts) in enumerate(zip(batch["detected_faces"], batch["prediction"])):
+            generator = (cv2.GaussianBlur(mask, (7, 7), 0) for mask in predicts)
+            predicted = np.array(tuple(generator))
+            predicted[predicted < 10.] = 0.
+            predicted[predicted > 245.] = 255.
+
+            face.load_feed_face(face.image,
+                                size=self.input_size,
+                                coverage_ratio=self.coverage_ratio)
+            feed_face = face.feed["face"][..., :3]
+            feed_mask = self._resize(predicted, self.input_size).astype('uint8')
+            batch["detected_faces"][idx].feed["face"] = np.concatenate((feed_face,
+                                                                        feed_mask),
+                                                                       axis=-1)
+            face.load_reference_face(face.image,
+                                     size=self.output_size,
+                                     coverage_ratio=self.coverage_ratio)
+            ref_face = face.reference["face"][..., :3]
+            ref_mask = self._resize(predicted, self.output_size).astype('uint8')
+            batch["detected_faces"][idx].reference["face"] = np.concatenate((ref_face,
+                                                                             ref_mask),
+                                                                            axis=-1)
+        return batch
diff --git a/plugins/extract/mask/vgg_obstructed_defaults.py b/plugins/extract/mask/vgg_obstructed_defaults.py
new file mode 100644
index 0000000..d1ed3bf
--- /dev/null
+++ b/plugins/extract/mask/vgg_obstructed_defaults.py
@@ -0,0 +1,68 @@
+#!/usr/bin/env python3
+"""
+    The default options for the faceswap VGG obstructed plugin.
+
+    Defaults files should be named <plugin_name>_defaults.py
+    Any items placed into this file will automatically get added to the relevant config .ini files
+    within the faceswap/config folder.
+
+    The following variables should be defined:
+        _HELPTEXT: A string describing what this plugin does
+        _DEFAULTS: A dictionary containing the options, defaults and meta information. The
+                   dictionary should be defined as:
+                       {<option_name>: {<metadata>}}
+
+                   <option_name> should always be lower text.
+                   <metadata> dictionary requirements are listed below.
+
+    The following keys are expected for the _DEFAULTS <metadata> dict:
+        datatype:  [required] A python type class. This limits the type of data that can be
+                   provided in the .ini file and ensures that the value is returned in the
+                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                   <class 'str'>, <class 'bool'>.
+        default:   [required] The default value for this option.
+        info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
+        choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                   selections can be defined here. This validates the option and also enables
+                   a combobox / radio option in the GUI.
+        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                   radio buttons rather than a combobox to display this option.
+        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                   otherwise it is ignored. Should be a tuple of min and max accepted values.
+                   This is used for controlling the GUI slider range. Values are not enforced.
+        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                   required otherwise it is ignored. Used for the GUI slider. For floats, this
+                   is the number of decimal places to display. For ints this is the step size.
+        fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                   created, and then reloaded from the state file. Marking an item as fixed=False
+                   indicates that this value can be changed for existing models, and will override
+                   the value saved in the state file with the updated value in config. If not
+                   provided this will default to True.
+"""
+
+
+_HELPTEXT = (
+    "VGG_Obstructed options. Mask designed to provide smart segmentation of mostly frontal faces. "
+    "The mask model has been specifically trained to recognize some facial obstructions ( "
+    "hands and eyeglasses ). Profile faces may result in sub-par performance."
+    )
+
+
+_DEFAULTS = {
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about xxxx GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    }
+}
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index bc31147..14293af 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -57,18 +57,25 @@ class Extractor():
         The current phase that the pipeline is running. Used in conjunction with :attr:`passes` and
         :attr:`final_pass` to indicate to the caller which phase is being processed
     """
-    def __init__(self, detector, aligner,
-                 configfile=None, multiprocess=False, rotate_images=None, min_size=20,
-                 normalize_method=None):
-        logger.debug("Initializing %s: (detector: %s, aligner: %s, configfile: %s, "
-                     "multiprocess: %s, rotate_images: %s, min_size: %s, "
-                     "normalize_method: %s)", self.__class__.__name__, detector, aligner,
-                     configfile, multiprocess, rotate_images, min_size, normalize_method)
+    def __init__(self, detector, aligner, masker, configfile=None,
+                 multiprocess=False, rotate_images=None, min_size=20,
+                 normalize_method=None, input_size=256, output_size=256, coverage_ratio=1.):
+        logger.debug("Initializing %s: (detector: %s, aligner: %s, masker: %s, "
+                     "configfile: %s, multiprocess: %s, rotate_images: %s, min_size: %s, "
+                     "normalize_method: %s, input_size: %s, output_size: %s, coverage_ratio: %s)",
+                     self.__class__.__name__, detector, aligner, masker, configfile,
+                     multiprocess, rotate_images, min_size, normalize_method, input_size,
+                     output_size, coverage_ratio)
         self.phase = "detect"
         self._queue_size = 32
         self._vram_buffer = 320  # Leave a buffer for VRAM allocation
         self._detector = self._load_detector(detector, rotate_images, min_size, configfile)
         self._aligner = self._load_aligner(aligner, configfile, normalize_method)
+        self._masker = self._load_masker(masker,
+                                         configfile,
+                                         input_size,
+                                         output_size,
+                                         coverage_ratio)
         self._is_parallel = self._set_parallel_processing(multiprocess)
         self._set_extractor_batchsize()
         self._queues = self._add_queues()
@@ -93,10 +100,10 @@ class Extractor():
         >>>  'detected_faces: [<list of DetectedFace objects as generated from detect>]}
 
         """
-        if self._is_parallel or self.phase == "detect":
-            qname = "extract_detect_in"
-        else:
-            qname = "extract_align_in"
+        qname_dict = dict(detect="extract_detect_in",
+                          align="extract_align_in",
+                          mask="extract_mask_in")
+        qname = "extract_detect_in" if self._is_parallel else qname_dict[self.phase]
         retval = self._queues[qname]
         logger.trace("%s: %s", qname, retval)
         return retval
@@ -120,7 +127,7 @@ class Extractor():
         >>>                                    "image": np.array(image),
         >>>                                    "detected_faces": [<DetectedFace objects]})
         """
-        retval = 1 if self._is_parallel else 2
+        retval = 1 if self._is_parallel else 3
         logger.trace(retval)
         return retval
 
@@ -142,7 +149,7 @@ class Extractor():
         >>>                                    "image": np.array(image),
         >>>                                    "detected_faces": [<DetectedFace objects]})
         """
-        retval = self._is_parallel or self.phase == "align"
+        retval = self._is_parallel or self.phase == "mask"
         logger.trace(retval)
         return retval
 
@@ -177,12 +184,15 @@ class Extractor():
         """
 
         if self._is_parallel:
-            self._launch_aligner()
             self._launch_detector()
+            self._launch_aligner()
+            self._launch_masker()
         elif self.phase == "detect":
             self._launch_detector()
-        else:
+        elif self.phase == "align":
             self._launch_aligner()
+        elif self.phase == "mask":
+            self._launch_masker()
 
     def detected_faces(self):
         """ Generator that returns results, frame by frame from the extraction pipeline
@@ -227,14 +237,17 @@ class Extractor():
                 queue_manager.del_queue(q_name)
             logger.debug("Detection Complete")
         else:
-            logger.debug("Switching to align phase")
-            self.phase = "align"
+            self.phase = "align" if self.phase == "detect" else "mask"
+            logger.debug("Switching to %s phase", self.phase)
 
     # <<< INTERNAL METHODS >>> #
     @property
     def _output_queue(self):
         """ Return the correct output queue depending on the current phase """
-        qname = "extract_align_out" if self.final_pass else "extract_align_in"
+        qname_dict = dict(detect="extract_align_in",
+                          align="extract_mask_in",
+                          mask="extract_mask_out")
+        qname = "extract_mask_out" if self.final_pass else qname_dict[self.phase]
         retval = self._queues[qname]
         logger.trace("%s: %s", qname, retval)
         return retval
@@ -243,18 +256,23 @@ class Extractor():
     def _active_plugins(self):
         """ Return the plugins that are currently active based on pass """
         if self.passes == 1:
-            retval = [self._detector, self._aligner]
-        elif self.passes == 2 and not self.final_pass:
+            retval = [self._detector, self._aligner, self._masker]
+        elif self.passes == 3 and self.phase == 'detect':
             retval = [self._detector]
-        else:
+        elif self.passes == 3 and self.phase == 'align':
             retval = [self._aligner]
+        elif self.passes == 3 and self.phase == 'mask':
+            retval = [self._masker]
+        else:
+            retval = [None]
         logger.trace("Active plugins: %s", retval)
         return retval
 
     def _add_queues(self):
         """ Add the required processing queues to Queue Manager """
         queues = dict()
-        for task in ("extract_detect_in", "extract_align_in", "extract_align_out"):
+        tasks = ["extract_detect_in", "extract_align_in", "extract_mask_in", "extract_mask_out"]
+        for task in tasks:
             # Limit queue size to avoid stacking ram
             self._queue_size = 32
             if task == "extract_detect_in" or (not self._is_parallel
@@ -266,11 +284,7 @@ class Extractor():
         return queues
 
     def _set_parallel_processing(self, multiprocess):
-        """ Set whether to run detect and align together or separately """
-        if self._detector.vram == 0 or self._aligner.vram == 0:
-            logger.debug("At least one of aligner or detector have no VRAM requirement. "
-                         "Enabling parallel processing.")
-            return True
+        """ Set whether to run detect, align, and mask together or separately """
 
         if not multiprocess:
             logger.debug("Parallel processing disabled by cli.")
@@ -285,7 +299,10 @@ class Extractor():
             logger.debug("Parallel processing discabled by amd")
             return False
 
-        vram_required = self._detector.vram + self._aligner.vram + self._vram_buffer
+        vram_required = (self._detector.vram +
+                         self._aligner.vram +
+                         self._masker.vram +
+                         self._vram_buffer)
         stats = gpu_stats.get_card_most_free()
         vram_free = int(stats["free"])
         logger.verbose("%s - %sMB free of %sMB",
@@ -318,14 +335,16 @@ class Extractor():
                                                          normalize_method=normalize_method)
         return aligner
 
-    def _launch_aligner(self):
-        """ Launch the face aligner """
-        logger.debug("Launching Aligner")
-        kwargs = dict(in_queue=self._queues["extract_align_in"],
-                      out_queue=self._queues["extract_align_out"])
-        self._aligner.initialize(**kwargs)
-        self._aligner.start()
-        logger.debug("Launched Aligner")
+    @staticmethod
+    def _load_masker(masker, configfile, input_size, output_size, coverage_ratio):
+        """ Set global arguments and load masker plugin """
+        masker_name = masker.replace("-", "_").lower()
+        logger.debug("Loading Masker: '%s'", masker_name)
+        masker = PluginLoader.get_masker(masker_name)(configfile=configfile,
+                                                      input_size=input_size,
+                                                      output_size=output_size,
+                                                      coverage_ratio=coverage_ratio)
+        return masker
 
     def _launch_detector(self):
         """ Launch the face detector """
@@ -336,31 +355,54 @@ class Extractor():
         self._detector.start()
         logger.debug("Launched Detector")
 
+    def _launch_aligner(self):
+        """ Launch the face aligner """
+        logger.debug("Launching Aligner")
+        kwargs = dict(in_queue=self._queues["extract_align_in"],
+                      out_queue=self._queues["extract_mask_in"])
+        self._aligner.initialize(**kwargs)
+        self._aligner.start()
+        logger.debug("Launched Aligner")
+
+    def _launch_masker(self):
+        """ Launch the face masker """
+        logger.debug("Launching Masker")
+        kwargs = dict(in_queue=self._queues["extract_mask_in"],
+                      out_queue=self._queues["extract_mask_out"])
+        self._masker.initialize(**kwargs)
+        self._masker.start()
+        logger.debug("Launched Masker")
+
     def _set_extractor_batchsize(self):
-        """ Sets the batchsize of the requested plugins based on their vram and
-            vram_per_batch_requirements if the the configured batchsize requires more
-            vram than is available. Nvidia only. """
-        if (self._detector.vram == 0 and self._aligner.vram == 0) or get_backend() != "nvidia":
+        """ 
+        Sets the batchsize of the requested plugins based on their vram and
+        vram_per_batch_requirements if the the configured batchsize requires more
+        vram than is available. Nvidia only.
+        """
+        if (self._detector.vram == 0 and self._aligner.vram == 0 and self._masker.vram == 0
+            or get_backend() != "nvidia"):
             logger.debug("Either detector and aligner have no VRAM requirements or not running "
                          "on Nvidia. Not updating batchsize requirements.")
             return
         stats = GPUStats().get_card_most_free()
         vram_free = int(stats["free"])
         if self._is_parallel:
-            vram_required = self._detector.vram + self._aligner.vram + self._vram_buffer
-            batch_required = ((self._aligner.vram_per_batch * self._aligner.batchsize) +
-                              (self._detector.vram_per_batch * self._detector.batchsize))
+            vram_required = (self._detector.vram + self._aligner.vram + self._masker.vram +
+                             self._vram_buffer)
+            batch_required = ((self._detector.vram_per_batch * self._detector.batchsize) +
+                              (self._aligner.vram_per_batch * self._aligner.batchsize) +
+                              (self._masker.vram_per_batch * self._masker.batchsize))
             plugin_required = vram_required + batch_required
             if plugin_required <= vram_free:
                 logger.debug("Plugin requirements within threshold: (plugin_required: %sMB, "
                              "vram_free: %sMB)", plugin_required, vram_free)
                 return
-            # Hacky split across 2 plugins
-            available_vram = (vram_free - vram_required) // 2
-            for plugin in (self._aligner, self._detector):
+            # Hacky split across 3 plugins
+            available_vram = (vram_free - vram_required) // 3
+            for plugin in (self._detector, self._aligner, self._masker):
                 self._set_plugin_batchsize(plugin, available_vram)
         else:
-            for plugin in (self._aligner, self._detector):
+            for plugin in (self._detector, self._aligner, self._masker):
                 vram_required = plugin.vram + self._vram_buffer
                 batch_required = plugin.vram_per_batch * plugin.batchsize
                 plugin_required = vram_required + batch_required
diff --git a/plugins/extract/recognition/.cache/.keep b/plugins/extract/recognition/.cache/.keep
new file mode 100644
index 0000000..e69de29
diff --git a/plugins/plugin_loader.py b/plugins/plugin_loader.py
index 75d8655..0973c50 100644
--- a/plugins/plugin_loader.py
+++ b/plugins/plugin_loader.py
@@ -1,94 +1,99 @@
-#!/usr/bin/env python3
-""" Plugin loader for extract, training and model tasks """
-
-import logging
-import os
-from importlib import import_module
-
-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
-
-
-class PluginLoader():
-    """ Plugin loader for extract, training and model tasks """
-    @staticmethod
-    def get_detector(name, disable_logging=False):
-        """ Return requested detector plugin """
-        return PluginLoader._import("extract.detect", name, disable_logging)
-
-    @staticmethod
-    def get_aligner(name, disable_logging=False):
-        """ Return requested detector plugin """
-        return PluginLoader._import("extract.align", name, disable_logging)
-
-    @staticmethod
-    def get_model(name, disable_logging=False):
-        """ Return requested model plugin """
-        return PluginLoader._import("train.model", name, disable_logging)
-
-    @staticmethod
-    def get_trainer(name, disable_logging=False):
-        """ Return requested trainer plugin """
-        return PluginLoader._import("train.trainer", name, disable_logging)
-
-    @staticmethod
-    def get_converter(category, name, disable_logging=False):
-        """ Return the converter sub plugin """
-        return PluginLoader._import("convert.{}".format(category), name, disable_logging)
-
-    @staticmethod
-    def _import(attr, name, disable_logging):
-        """ Import the plugin's module """
-        name = name.replace("-", "_")
-        ttl = attr.split(".")[-1].title()
-        if not disable_logging:
-            logger.info("Loading %s from %s plugin...", ttl, name.title())
-        attr = "model" if attr == "Trainer" else attr.lower()
-        mod = ".".join(("plugins", attr, name))
-        module = import_module(mod)
-        return getattr(module, ttl)
-
-    @staticmethod
-    def get_available_extractors(extractor_type):
-        """ Return a list of available aligners/detectors """
-        extractpath = os.path.join(os.path.dirname(__file__),
-                                   "extract",
-                                   extractor_type)
-        extractors = sorted(item.name.replace(".py", "").replace("_", "-")
-                            for item in os.scandir(extractpath)
-                            if not item.name.startswith("_")
-                            and not item.name.endswith("defaults.py")
-                            and item.name.endswith(".py")
-                            and item.name != "manual.py")
-        return extractors
-
-    @staticmethod
-    def get_available_models():
-        """ Return a list of available models """
-        modelpath = os.path.join(os.path.dirname(__file__), "train", "model")
-        models = sorted(item.name.replace(".py", "").replace("_", "-")
-                        for item in os.scandir(modelpath)
-                        if not item.name.startswith("_")
-                        and not item.name.endswith("defaults.py")
-                        and item.name.endswith(".py"))
-        return models
-
-    @staticmethod
-    def get_default_model():
-        """ Return the default model """
-        models = PluginLoader.get_available_models()
-        return 'original' if 'original' in models else models[0]
-
-    @staticmethod
-    def get_available_convert_plugins(convert_category, add_none=True):
-        """ Return a list of available models """
-        convertpath = os.path.join(os.path.dirname(__file__),
-                                   "convert",
-                                   convert_category)
-        converters = sorted(item.name.replace(".py", "").replace("_", "-")
-                            for item in os.scandir(convertpath)
-                            if not item.name.startswith("_")
-                            and not item.name.endswith("defaults.py")
-                            and item.name.endswith(".py"))
-        if add_none:
-            converters.insert(0, "none")
-        return converters
+#!/usr/bin/env python3
+""" Plugin loader for extract, training and model tasks """
+
+import logging
+import os
+from importlib import import_module
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+class PluginLoader():
+    """ Plugin loader for extract, training and model tasks """
+    @staticmethod
+    def get_detector(name, disable_logging=False):
+        """ Return requested detector plugin """
+        return PluginLoader._import("extract.detect", name, disable_logging)
+
+    @staticmethod
+    def get_aligner(name, disable_logging=False):
+        """ Return requested detector plugin """
+        return PluginLoader._import("extract.align", name, disable_logging)
+
+    @staticmethod
+    def get_masker(name, disable_logging=False):
+        """ Return requested detector plugin """
+        return PluginLoader._import("extract.mask", name, disable_logging)
+
+    @staticmethod
+    def get_model(name, disable_logging=False):
+        """ Return requested model plugin """
+        return PluginLoader._import("train.model", name, disable_logging)
+
+    @staticmethod
+    def get_trainer(name, disable_logging=False):
+        """ Return requested trainer plugin """
+        return PluginLoader._import("train.trainer", name, disable_logging)
+
+    @staticmethod
+    def get_converter(category, name, disable_logging=False):
+        """ Return the converter sub plugin """
+        return PluginLoader._import("convert.{}".format(category), name, disable_logging)
+
+    @staticmethod
+    def _import(attr, name, disable_logging):
+        """ Import the plugin's module """
+        name = name.replace("-", "_")
+        ttl = attr.split(".")[-1].title()
+        if not disable_logging:
+            logger.info("Loading %s from %s plugin...", ttl, name.title())
+        attr = "model" if attr == "Trainer" else attr.lower()
+        mod = ".".join(("plugins", attr, name))
+        module = import_module(mod)
+        return getattr(module, ttl)
+
+    @staticmethod
+    def get_available_extractors(extractor_type):
+        """ Return a list of available aligners/detectors """
+        extractpath = os.path.join(os.path.dirname(__file__),
+                                   "extract",
+                                   extractor_type)
+        extractors = sorted(item.name.replace(".py", "").replace("_", "-")
+                            for item in os.scandir(extractpath)
+                            if not item.name.startswith("_")
+                            and not item.name.endswith("defaults.py")
+                            and item.name.endswith(".py")
+                            and item.name != "manual.py")
+        return extractors
+
+    @staticmethod
+    def get_available_models():
+        """ Return a list of available models """
+        modelpath = os.path.join(os.path.dirname(__file__), "train", "model")
+        models = sorted(item.name.replace(".py", "").replace("_", "-")
+                        for item in os.scandir(modelpath)
+                        if not item.name.startswith("_")
+                        and not item.name.endswith("defaults.py")
+                        and item.name.endswith(".py"))
+        return models
+
+    @staticmethod
+    def get_default_model():
+        """ Return the default model """
+        models = PluginLoader.get_available_models()
+        return 'original' if 'original' in models else models[0]
+
+    @staticmethod
+    def get_available_convert_plugins(convert_category, add_none=True):
+        """ Return a list of available models """
+        convertpath = os.path.join(os.path.dirname(__file__),
+                                   "convert",
+                                   convert_category)
+        converters = sorted(item.name.replace(".py", "").replace("_", "-")
+                            for item in os.scandir(convertpath)
+                            if not item.name.startswith("_")
+                            and not item.name.endswith("defaults.py")
+                            and item.name.endswith(".py"))
+        if add_none:
+            converters.insert(0, "none")
+        return converters
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index 31d1dd5..14db591 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -8,7 +8,6 @@ import sys
 from importlib import import_module
 
 from lib.config import FaceswapConfig
-from lib.model.masks import get_available_masks
 from lib.utils import full_path_split
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -69,15 +68,11 @@ class Config(FaceswapConfig):
                  "\n\t87.5%% spans from ear to ear."
                  "\n\t100.0%% is a mugshot.")
         self.add_item(
-            section=section, title="mask_type", datatype=str, default="none",
-            choices=get_available_masks(), group="mask",
-            info="The mask to be used for training:"
-                 "\n\t none: Doesn't use any mask."
-                 "\n\t components: An improved face hull mask using a facehull of 8 facial parts"
-                 "\n\t dfl_full: An improved face hull mask using a facehull of 3 facial parts"
-                 "\n\t extended: Based on components mask. Extends the eyebrow points to further "
-                 "up the forehead. May perform badly on difficult angles."
-                 "\n\t facehull: Face cutout based on landmarks")
+            section=section, title="replicate_input_mask", datatype=bool,
+            default=False, group="mask",
+            info="Dedicate portions of the model to learning how to duplicate the input "
+                 "mask. Increases VRAM usage in exchange for a learning a quick ability "
+                 "to try to replicate more complex mask models.")
         self.add_item(
             section=section, title="mask_blur", datatype=bool, default=False, group="mask",
             info="Apply gaussian blur to the mask input. This has the effect of smoothing the "
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 954b238..0401218 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -106,7 +106,10 @@ class ModelBase():
                               "augment_color": augment_color,
                               "no_flip": no_flip,
                               "pingpong": self.vram_savings.pingpong,
-                              "snapshot_interval": snapshot_interval}
+                              "snapshot_interval": snapshot_interval,
+                              "replicate_input_mask": self.config["replicate_input_mask"],
+                              "penalized_mask_loss": self.config["penalized_mask_loss"]}
+
 
         if self.multiple_models_in_folder:
             deprecation_warning("Support for multiple model types within the same folder",
@@ -223,7 +226,6 @@ class ModelBase():
         # Force number of preview images to between 2 and 16
         self.training_opts["training_size"] = self.state.training_size
         self.training_opts["no_logs"] = self.state.current_session["no_logs"]
-        self.training_opts["mask_type"] = self.config.get("mask_type", None)
         self.training_opts["coverage_ratio"] = self.calculate_coverage_ratio()
         logger.debug("Set training data: %s", self.training_opts)
 
@@ -261,12 +263,11 @@ class ModelBase():
         logger.debug("Getting inputs")
         inputs = [Input(shape=self.input_shape, name="face_in")]
         output_network = [network for network in self.networks.values() if network.is_output][0]
-        mask_idx = [idx for idx, name in enumerate(output_network.output_names)
-                    if name.startswith("mask")]
-        if mask_idx:
-            # Add the final mask shape as input
-            mask_shape = output_network.output_shapes[mask_idx[0]]
-            inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
+        if self.config["replicate_input_mask"] or self.config["penalized_mask_loss"]:
+            # penalized mask doesn't have a mask ouput, so we can't use output shapes
+            # mask should always be last output..this needs to be a rule
+            mask_shape = output_network.output_shapes[-1]
+            inputs.append(Input(shape=(mask_shape[1:-1] + (1,)), name="mask_in"))
         logger.debug("Got inputs: %s", inputs)
         return inputs
 
@@ -445,7 +446,7 @@ class ModelBase():
             logger.error("Model could not be found in folder '%s'. Exiting", self.model_dir)
             exit(0)
 
-        if not self.is_legacy:
+        if not self.is_legacy or not self.predict:
             K.clear_session()
         model_mapping = self.map_models(swapped)
         for network in self.networks.values():
@@ -579,7 +580,7 @@ class ModelBase():
         self.state.config["coverage"] = 62.5
         self.state.config["subpixel_upscaling"] = False
         self.state.config["reflect_padding"] = False
-        self.state.config["mask_type"] = None
+        self.state.config["replicate_input_mask"] = False
         self.state.config["lowmem"] = False
         self.encoder_dim = 1024
 
@@ -744,7 +745,7 @@ class Loss():
         for idx, loss_name in enumerate(self.names):
             if loss_name.startswith("mask"):
                 loss_funcs.append(self.selected_mask_loss)
-            elif self.mask_input is not None and self.config.get("penalized_mask_loss", False):
+            elif self.config["penalized_mask_loss"]:
                 face_size = self.output_shapes[idx][1]
                 mask_size = self.mask_shape[1]
                 scaling = face_size / mask_size
diff --git a/plugins/train/model/dfaker.py b/plugins/train/model/dfaker.py
index 758c43b..9f41e6d 100644
--- a/plugins/train/model/dfaker.py
+++ b/plugins/train/model/dfaker.py
@@ -40,7 +40,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/dfl_h128.py b/plugins/train/model/dfl_h128.py
index 6bf9a6f..78e140b 100644
--- a/plugins/train/model/dfl_h128.py
+++ b/plugins/train/model/dfl_h128.py
@@ -50,8 +50,8 @@ class Model(OriginalModel):
                                    activation="sigmoid",
                                    name="face_out")
         outputs = [var_x]
-        # Mask
-        if self.config.get("mask_type", None):
+
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, self.encoder_dim)
             var_y = self.blocks.upscale(var_y, self.encoder_dim // 2)
diff --git a/plugins/train/model/dfl_sae.py b/plugins/train/model/dfl_sae.py
index e79822c..fd9ad53 100644
--- a/plugins/train/model/dfl_sae.py
+++ b/plugins/train/model/dfl_sae.py
@@ -31,7 +31,7 @@ class Model(ModelBase):
     @property
     def use_mask(self):
         """ Return True if a mask has been set else false """
-        return self.config.get("mask_type", None) is not None
+        return self.config.get("replicate_input_mask", False)
 
     @property
     def ae_dims(self):
diff --git a/plugins/train/model/iae.py b/plugins/train/model/iae.py
index b164fef..4f1c1e8 100644
--- a/plugins/train/model/iae.py
+++ b/plugins/train/model/iae.py
@@ -77,7 +77,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/lightweight.py b/plugins/train/model/lightweight.py
index 1963c8c..44f2092 100644
--- a/plugins/train/model/lightweight.py
+++ b/plugins/train/model/lightweight.py
@@ -47,7 +47,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/original.py b/plugins/train/model/original.py
index 55d3bea..09bedff 100644
--- a/plugins/train/model/original.py
+++ b/plugins/train/model/original.py
@@ -73,7 +73,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
diff --git a/plugins/train/model/realface.py b/plugins/train/model/realface.py
index 10562b8..d950451 100644
--- a/plugins/train/model/realface.py
+++ b/plugins/train/model/realface.py
@@ -134,7 +134,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("mask_type", None) is not None:
+        if self.config.get("replicate_input_mask", False):
             var_y = var_xy
             mask_b_complexity = 384
             for idx in range(self.upscalers_no-2):
@@ -184,7 +184,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("mask_type", None) is not None:
+        if self.config.get("replicate_input_mask", False):
             var_y = var_xy
             mask_a_complexity = 384
             for idx in range(self.upscalers_no-2):
diff --git a/plugins/train/model/unbalanced.py b/plugins/train/model/unbalanced.py
index b8c2a08..323639c 100644
--- a/plugins/train/model/unbalanced.py
+++ b/plugins/train/model/unbalanced.py
@@ -80,7 +80,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             var_y = self.blocks.upscale(var_y, decoder_complexity)
@@ -129,7 +129,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             if not self.lowmem:
diff --git a/plugins/train/model/villain.py b/plugins/train/model/villain.py
index 4a0a67a..c55b293 100644
--- a/plugins/train/model/villain.py
+++ b/plugins/train/model/villain.py
@@ -78,7 +78,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("mask_type", None):
+        if self.config.get("replicate_input_mask", False):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index e4aef16..170d476 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -7,18 +7,17 @@
 
     A training_opts dictionary can be set in the corresponding model.
     Accepted values:
-        alignments:         dict containing paths to alignments files for keys 'a' and 'b'
-        preview_scaling:    How much to scale the preview out by
-        training_size:      Size of the training images
-        coverage_ratio:     Ratio of face to be cropped out for training
-        mask_type:          Type of mask to use. See lib.model.masks for valid mask names.
-                            Set to None for not used
-        no_logs:            Disable tensorboard logging
-        snapshot_interval:  Interval for saving model snapshots
-        warp_to_landmarks:  Use random_warp_landmarks instead of random_warp
-        augment_color:      Perform random shifting of L*a*b* colors
-        no_flip:            Don't perform a random flip on the image
-        pingpong:           Train each side seperately per save iteration rather than together
+        alignments:             dict containing paths to alignments files for keys 'a' and 'b'
+        preview_scaling:        How much to scale the preview out by
+        training_size:          Size of the training images
+        coverage_ratio:         Ratio of face to be cropped out for training
+        replicate_input_mask:   Replicate input masks with additional model dedicated layers
+        no_logs:                Disable tensorboard logging
+        snapshot_interval:      Interval for saving model snapshots
+        warp_to_landmarks:      Use random_warp_landmarks instead of random_warp
+        augment_color:          Perform random shifting of L*a*b* colors
+        no_flip:                Don't perform a random flip on the image
+        pingpong:               Train each side seperately per save iteration rather than together
 """
 
 import logging
@@ -28,14 +27,13 @@ import time
 import cv2
 import numpy as np
 
-import tensorflow as tf
-from tensorflow.python import errors_impl as tf_errors  # pylint:disable=no-name-in-module
-
 from lib.alignments import Alignments
 from lib.faces_detect import DetectedFace
 from lib.training_data import TrainingDataGenerator
 from lib.utils import FaceswapError, get_folder, get_image_paths
 from plugins.train._config import Config
+from tensorflow.python import errors_impl as tf_errors  # pylint:disable=no-name-in-module
+import tensorflow as tf
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
@@ -90,14 +88,15 @@ class TrainerBase():
     def landmarks_required(self):
         """ Return True if Landmarks are required """
         opts = self.model.training_opts
-        retval = bool(opts.get("mask_type", None) or opts["warp_to_landmarks"])
+        retval = opts["warp_to_landmarks"]
         logger.debug(retval)
         return retval
 
     @property
     def use_mask(self):
         """ Return True if a mask is requested """
-        retval = bool(self.model.training_opts.get("mask_type", None))
+        retval = (self.model.training_opts.get("replicate_input_mask", False) or
+                  self.model.training_opts.get("penalized_mask_loss", True))
         logger.debug(retval)
         return retval
 
@@ -176,10 +175,11 @@ class TrainerBase():
             for side, batcher in self.batchers.items():
                 if self.pingpong.active and side != self.pingpong.side:
                     continue
-                loss[side] = batcher.train_one_batch(do_preview)
+                loss[side] = batcher.train_one_batch()
                 if not do_preview and not do_timelapse:
                     continue
                 if do_preview:
+                    batcher.generate_preview(do_preview)
                     self.samples.images[side] = batcher.compile_sample(None)
                 if do_timelapse:
                     self.timelapse.get_sample(side, timelapse_kwargs)
@@ -247,13 +247,14 @@ class Batcher():
         self.config = config
         self.target = None
         self.samples = None
-        self.mask = None
+        self.masks = None
 
         generator = self.load_generator()
         self.feed = generator.minibatch_ab(images, batch_size, self.side)
 
         self.preview_feed = None
         self.timelapse_feed = None
+        self.set_preview_feed()
 
     def load_generator(self):
         """ Pass arguments to TrainingDataGenerator and return object """
@@ -267,12 +268,12 @@ class Batcher():
                                           self.config)
         return generator
 
-    def train_one_batch(self, do_preview):
+    def train_one_batch(self):
         """ Train a batch """
         logger.trace("Training one step: (side: %s)", self.side)
-        batch = self.get_next(do_preview)
+        model_inputs, model_targets = self.get_next()
         try:
-            loss = self.model.predictors[self.side].train_on_batch(*batch)
+            loss = self.model.predictors[self.side].train_on_batch(x=model_inputs, y=model_targets)
         except tf_errors.ResourceExhaustedError as err:
             msg = ("You do not have enough GPU memory available to train the selected model at "
                    "the selected settings. You can try a number of things:"
@@ -288,31 +289,28 @@ class Batcher():
         loss = loss if isinstance(loss, list) else [loss]
         return loss
 
-    def get_next(self, do_preview):
+    def get_next(self):
         """ Return the next batch from the generator
-            Items should come out as: (warped, target [, mask]) """
+            Items should come out as: (sample, warped, targets, [mask]) """
+        logger.debug("Generating targets")
         batch = next(self.feed)
-        if self.use_mask:
-            batch = [[batch["feed"], batch["masks"]], batch["targets"] + [batch["masks"]]]
-        else:
-            batch = [batch["feed"], batch["targets"]]
-        self.generate_preview(do_preview)
-        return batch
+        targets_use_mask = self.model.training_opts["replicate_input_mask"]
+        model_inputs = batch["feed"] + batch["masks"] if self.use_mask else batch["feed"]
+        model_targets = batch["targets"] + batch["masks"] if targets_use_mask else batch["targets"]
+        return model_inputs, model_targets
 
     def generate_preview(self, do_preview):
         """ Generate the preview if a preview iteration """
         if not do_preview:
             self.samples = None
             self.target = None
+            self.masks = None
             return
         logger.debug("Generating preview")
-        if self.preview_feed is None:
-            self.set_preview_feed()
         batch = next(self.preview_feed)
         self.samples = batch["samples"]
-        self.target = [batch["targets"][self.model.largest_face_index]]
-        if self.use_mask:
-            self.target += [batch["masks"]]
+        self.target = batch["targets"][self.model.largest_face_index]
+        self.masks = batch["masks"][0]
 
     def set_preview_feed(self):
         """ Set the preview dictionary """
@@ -327,27 +325,27 @@ class Batcher():
                                                                is_preview=True)
         logger.debug("Set preview feed. Batchsize: %s", batchsize)
 
-    def compile_sample(self, batch_size, samples=None, images=None):
+    def compile_sample(self, batch_size, samples=None, images=None, masks=None):
         """ Training samples to display in the viewer """
         num_images = self.config.get("preview_images", 14)
         num_images = min(batch_size, num_images) if batch_size is not None else num_images
         logger.debug("Compiling samples: (side: '%s', samples: %s)", self.side, num_images)
         images = images if images is not None else self.target
-        retval = [samples[0:num_images]] if samples is not None else [self.samples[0:num_images]]
-        if self.use_mask:
-            retval.extend(tgt[0:num_images] for tgt in images)
-        else:
-            retval.extend(images[0:num_images])
+        masks = masks if masks is not None else self.masks
+        samples = samples if samples is not None else self.samples
+        retval = [samples[0:num_images], images[0:num_images], masks[0:num_images]]
         return retval
 
     def compile_timelapse_sample(self):
         """ Timelapse samples """
         batch = next(self.timelapse_feed)
         batchsize = len(batch["samples"])
-        images = [batch["targets"][self.model.largest_face_index]]
-        if self.use_mask:
-            images = images + [batch["masks"]]
-        sample = self.compile_sample(batchsize, samples=batch["samples"], images=images)
+        images = batch["targets"][self.model.largest_face_index]
+        masks = batch["masks"][0]
+        sample = self.compile_sample(batchsize, 
+                                     samples=batch["samples"],
+                                     images=images,
+                                     masks=masks)
         return sample
 
     def set_timelapse_feed(self, images, batchsize):
@@ -416,7 +414,7 @@ class Samples():
         height = int(figure.shape[0] / width)
         figure = figure.reshape((width, height) + figure.shape[1:])
         figure = stack_images(figure)
-        figure = np.vstack((header, figure))
+        figure = np.concatenate((header, figure), axis=0)
 
         logger.debug("Compiled sample")
         return np.clip(figure * 255, 0, 255).astype('uint8')
@@ -518,9 +516,8 @@ class Samples():
         for mask in masks3:
             mask[np.where((mask == [1., 1., 1.]).all(axis=2))] = [0., 0., 1.]
         for previews in faces:
-            images = np.array([cv2.addWeighted(img, 1.0,  # pylint: disable=no-member
-                                               masks3[idx], 0.3,
-                                               0)
+            images = np.array([cv2.addWeighted(img,  # pylint: disable=no-member
+                                               1.0, masks3[idx], 0.3, 0)
                                for idx, img in enumerate(previews)])
             retval.append(images)
         logger.debug("masked shapes: %s", [faces.shape for faces in retval])
@@ -533,7 +530,7 @@ class Samples():
         new_images = list()
         for idx, img in enumerate(backgrounds):
             img[offset:offset + foregrounds[idx].shape[0],
-                offset:offset + foregrounds[idx].shape[1]] = foregrounds[idx]
+                offset:offset + foregrounds[idx].shape[1], :3] = foregrounds[idx]
             new_images.append(img)
         retval = np.array(new_images)
         logger.debug("Overlayed foreground. Shape: %s", retval.shape)
diff --git a/scripts/convert.py b/scripts/convert.py
index a8f4e35..0e3a146 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -587,7 +587,7 @@ class Predict():
     def compile_feed_faces(detected_faces):
         """ Compile the faces for feeding into the predictor """
         logger.trace("Compiling feed face. Batchsize: %s", len(detected_faces))
-        feed_faces = np.stack([detected_face.feed_face for detected_face in detected_faces])
+        feed_faces = np.stack([detected_face.feed_face / 255. for detected_face in detected_faces])
         logger.trace("Compiled Feed faces. Shape: %s", feed_faces.shape)
         return feed_faces
 
diff --git a/scripts/extract.py b/scripts/extract.py
index 5306c3e..b07d853 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -34,6 +34,7 @@ class Extract():
         normalization = None if self.args.normalization == "none" else self.args.normalization
         self.extractor = Extractor(self.args.detector,
                                    self.args.aligner,
+                                   self.args.masker,
                                    configfile=configfile,
                                    multiprocess=not self.args.singleprocess,
                                    rotate_images=self.args.rotate_images,
@@ -103,7 +104,7 @@ class Extract():
                 logger.trace("Skipping image: '%s'", filename)
                 continue
             item = {"filename": filename,
-                    "image": image}
+                    "image": image[..., :3]}
             load_queue.put(item)
         load_queue.put("EOF")
         logger.debug("Load Images: Complete")
@@ -224,7 +225,12 @@ class Extract():
 
     def output_processing(self, faces, size, filename):
         """ Prepare faces for output """
-        self.align_face(faces, size, filename)
+        final_faces = list()
+        for detected_face in faces["detected_faces"]:
+            filename = self.output_dir / Path(detected_face.filename).stem
+            final_faces.append({"file_location": filename,
+                                "face": detected_face})
+        faces["detected_faces"] = final_faces
         self.post_process.do_actions(faces)
 
         faces_count = len(faces["detected_faces"])
@@ -234,28 +240,16 @@ class Extract():
         if not self.verify_output and faces_count > 1:
             self.verify_output = True
 
-    def align_face(self, faces, size, filename):
-        """ Align the detected face and add the destination file path """
-        final_faces = list()
-        image = faces["image"]
-        detected_faces = faces["detected_faces"]
-        for face in detected_faces:
-            face.load_aligned(image, size=size)
-            final_faces.append({"file_location": self.output_dir / Path(filename).stem,
-                                "face": face})
-        faces["detected_faces"] = final_faces
-
     def output_faces(self, filename, faces):
         """ Output faces to save thread """
         final_faces = list()
         for idx, detected_face in enumerate(faces["detected_faces"]):
             output_file = detected_face["file_location"]
-            extension = Path(filename).suffix
+            extension = '.png'
             out_filename = "{}_{}{}".format(str(output_file), str(idx), extension)
 
             face = detected_face["face"]
-            resized_face = face.aligned_face
-
+            resized_face = face.feed_face
             face.hash, img = encode_image_with_hash(resized_face, extension)
             self.save_queue.put((out_filename, img))
             final_faces.append(face.to_alignment())
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index bc866ca..533e07b 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -388,11 +388,10 @@ class DebugLandmarks(PostProcessAction):  # pylint: disable=too-few-public-metho
             face = detected_face["face"]
             logger.trace("Drawing Landmarks. Frame: '%s'. Face: %s",
                          detected_face["file_location"].parts[-1], idx)
-            aligned_landmarks = face.aligned_landmarks
+            aligned_landmarks = face.feed_landmarks
             for (pos_x, pos_y) in aligned_landmarks:
-                cv2.circle(  # pylint: disable=no-member
-                    face.aligned_face,
-                    (pos_x, pos_y), 2, (0, 0, 255), -1)
+                cv2.circle(face.feed_face,  # pylint: disable=no-member
+                           (pos_x, pos_y), 2, (0, 0, 255, 255), -1)
 
 
 class FaceFilter(PostProcessAction):
diff --git a/tools/preview.py b/tools/preview.py
index 87595c0..ef803ec 100644
--- a/tools/preview.py
+++ b/tools/preview.py
@@ -21,7 +21,6 @@ from lib.gui.tooltip import Tooltip
 from lib.gui.control_helper import set_slider_rounding
 from lib.convert import Converter
 from lib.faces_detect import DetectedFace
-from lib.model.masks import get_available_masks
 from lib.multithreading import MultiThread
 from lib.utils import FaceswapError, set_system_verbosity
 from lib.queue_manager import queue_manager
@@ -733,7 +732,7 @@ class ActionFrame(ttk.Frame):  # pylint: disable=too-many-ancestors
         """ Add the comboboxes to the Action Frame """
         for opt in self.options:
             if opt == "mask_type":
-                choices = get_available_masks() + ["predicted"]
+                choices = ["dfl_full", "components", "extended", "predicted"]
             else:
                 choices = PluginLoader.get_available_convert_plugins(opt, True)
             choices = [self.format_to_display(choice) for choice in choices]
