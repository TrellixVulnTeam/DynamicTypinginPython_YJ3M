commit e1fdb256734fd3dee287bf4492ee4954e7bb70b7
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Fri Aug 23 02:48:59 2019 +0000

    Add grouping for GUI options

diff --git a/lib/cli.py b/lib/cli.py
index 1856d9d..8db49b1 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -392,6 +392,7 @@ class FaceSwapArgs():
                             "action": FileFullPaths,
                             "filetypes": "ini",
                             "type": str,
+                            "group": "Global Options",
                             "help": "Optionally overide the saved config with the path to a "
                                     "custom config file."})
         global_args.append({"opts": ("-L", "--loglevel"),
@@ -399,6 +400,7 @@ class FaceSwapArgs():
                             "dest": "loglevel",
                             "default": "INFO",
                             "choices": ("INFO", "VERBOSE", "DEBUG", "TRACE"),
+                            "group": "Global Options",
                             "help": "Log level. Stick with INFO or VERBOSE unless you need to "
                                     "file an error report. Be careful with TRACE as it will "
                                     "generate a lot of data"})
@@ -407,6 +409,7 @@ class FaceSwapArgs():
                             "filetypes": 'log',
                             "type": str,
                             "dest": "logfile",
+                            "group": "Global Options",
                             "help": "Path to store the logfile. Leave blank to store in the "
                                     "faceswap folder",
                             "default": None})
@@ -436,7 +439,7 @@ class FaceSwapArgs():
         for option in options:
             args = option["opts"]
             kwargs = {key: option[key]
-                      for key in option.keys() if key != "opts"}
+                      for key in option.keys() if key not in ("opts", "group")}
             self.parser.add_argument(*args, **kwargs)
 
     def process_suppressions(self):
@@ -473,6 +476,7 @@ class ExtractConvertArgs(FaceSwapArgs):
                               "filetypes": "video",
                               "dest": "input_dir",
                               "required": True,
+                              "group": "Data",
                               "help": "Input directory or video. Either a directory containing "
                                       "the image files you wish to process or path to a video "
                                       "file. NB: This should be the source video/frames NOT the "
@@ -481,6 +485,7 @@ class ExtractConvertArgs(FaceSwapArgs):
                               "action": DirFullPaths,
                               "dest": "output_dir",
                               "required": True,
+                              "group": "Data",
                               "help": "Output directory. This is where the converted files will "
                                       "be saved."})
         argument_list.append({"opts": ("-al", "--alignments"),
@@ -488,44 +493,9 @@ class ExtractConvertArgs(FaceSwapArgs):
                               "filetypes": "alignments",
                               "type": str,
                               "dest": "alignments_path",
+                              "group": "Data",
                               "help": "Optional path to an alignments file. Leave blank if the "
                                       "alignments file is at the default location."})
-        argument_list.append({"opts": ("-n", "--nfilter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "nfilter",
-                              "nargs": "+",
-                              "default": None,
-                              "help": "Optionally filter out people who you do not wish to "
-                                      "process by passing in an image of that person. Should be a "
-                                      "front portrait with a single person in the image. Multiple "
-                                      "images can be added space separated. NB: Using face filter "
-                                      "will significantly decrease extraction speed and its "
-                                      "accuracy cannot be guaranteed."})
-        argument_list.append({"opts": ("-f", "--filter"),
-                              "action": FilesFullPaths,
-                              "filetypes": "image",
-                              "dest": "filter",
-                              "nargs": "+",
-                              "default": None,
-                              "help": "Optionally select people you wish to process by passing in "
-                                      "an image of that person. Should be a front portrait with a "
-                                      "single person in the image. Multiple images can be added "
-                                      "space separated. NB: Using face filter will significantly "
-                                      "decrease extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
-        argument_list.append({"opts": ("-l", "--ref_threshold"),
-                              "action": Slider,
-                              "min_max": (0.01, 0.99),
-                              "rounding": 2,
-                              "type": float,
-                              "dest": "ref_threshold",
-                              "default": 0.4,
-                              "help": "For use with the optional nfilter/filter files. Threshold "
-                                      "for positive face recognition. Lower values are stricter. "
-                                      "NB: Using face filter will significantly decrease "
-                                      "extraction speed and its accuracy cannot be "
-                                      "guaranteed."})
         return argument_list
 
 
@@ -545,6 +515,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "dest": "serializer",
                               "default": "json",
                               "choices": ("json", "pickle", "yaml"),
+                              "group": "Data",
                               "help": "Serializer for alignments file. If yaml is chosen and not "
                                       "available, then json will be used as the default "
                                       "fallback."})
@@ -567,6 +538,7 @@ class ExtractArgs(ExtractConvertArgs):
             "type": str.lower,
             "choices":  PluginLoader.get_available_extractors("detect"),
             "default": default_detector,
+            "group": "Plugins",
             "help": "R|Detector to use. Some of these have configurable settings in "
                     "'/config/extract.ini' or 'Edit > Configure Extract Plugins':"
                     "\nL|'cv2-dnn': A CPU only extractor, is the least reliable, but uses least "
@@ -584,6 +556,7 @@ class ExtractArgs(ExtractConvertArgs):
             "type": str.lower,
             "choices": PluginLoader.get_available_extractors("align"),
             "default": default_aligner,
+            "group": "Plugins",
             "help": "R|Aligner to use."
                     "\nL|'cv2-dnn': A cpu only CNN based landmark detector. Faster, less "
                     "resource intensive, but less accurate. Only use this if not using a gpu "
@@ -596,6 +569,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "dest": "normalization",
                               "choices": ["none", "clahe", "hist", "mean"],
                               "default": "none",
+                              "group": "plugins",
                               "help": "R|Performing normalization can help the aligner better "
                                       "align faces with difficult lighting conditions at an "
                                       "extraction speed cost. Different methods will yield "
@@ -610,11 +584,51 @@ class ExtractArgs(ExtractConvertArgs):
                               "type": str,
                               "dest": "rotate_images",
                               "default": None,
+                              "group": "plugins",
                               "help": "If a face isn't found, rotate the images to try to find a "
                                       "face. Can find more faces at the cost of extraction speed. "
                                       "Pass in a single number to use increments of that size up "
                                       "to 360, or pass in a list of numbers to enumerate exactly "
                                       "what angles to check"})
+        argument_list.append({"opts": ("-n", "--nfilter"),
+                              "action": FilesFullPaths,
+                              "filetypes": "image",
+                              "dest": "nfilter",
+                              "nargs": "+",
+                              "default": None,
+                              "group": "Face Processing",
+                              "help": "Optionally filter out people who you do not wish to "
+                                      "process by passing in an image of that person. Should be a "
+                                      "front portrait with a single person in the image. Multiple "
+                                      "images can be added space separated. NB: Using face filter "
+                                      "will significantly decrease extraction speed and its "
+                                      "accuracy cannot be guaranteed."})
+        argument_list.append({"opts": ("-f", "--filter"),
+                              "action": FilesFullPaths,
+                              "filetypes": "image",
+                              "dest": "filter",
+                              "nargs": "+",
+                              "default": None,
+                              "group": "Face Processing",
+                              "help": "Optionally select people you wish to process by passing in "
+                                      "an image of that person. Should be a front portrait with a "
+                                      "single person in the image. Multiple images can be added "
+                                      "space separated. NB: Using face filter will significantly "
+                                      "decrease extraction speed and its accuracy cannot be "
+                                      "guaranteed."})
+        argument_list.append({"opts": ("-l", "--ref_threshold"),
+                              "action": Slider,
+                              "min_max": (0.01, 0.99),
+                              "rounding": 2,
+                              "type": float,
+                              "dest": "ref_threshold",
+                              "default": 0.4,
+                              "group": "Face Processing",
+                              "help": "For use with the optional nfilter/filter files. Threshold "
+                                      "for positive face recognition. Lower values are stricter. "
+                                      "NB: Using face filter will significantly decrease "
+                                      "extraction speed and its accuracy cannot be "
+                                      "guaranteed."})
         argument_list.append({"opts": ("-bt", "--blur-threshold"),
                               "type": float,
                               "action": Slider,
@@ -622,6 +636,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "rounding": 1,
                               "dest": "blur_thresh",
                               "default": 0.0,
+                              "group": "Face Processing",
                               "help": "Automatically discard images blurrier than the specified "
                                       "threshold. Discarded images are moved into a \"blurry\" "
                                       "sub-folder. Lower values allow more blur. Set to 0.0 to "
@@ -630,6 +645,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "action": "store_true",
                               "default": False,
                               "backend": "nvidia",
+
                               "help": "Don't run extraction in parallel. Will run detection first "
                                       "then alignment (2 passes). Useful if VRAM is at a "
                                       "premium."})
@@ -639,6 +655,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "min_max": (128, 512),
                               "default": 256,
                               "rounding": 64,
+                              "group": "output",
                               "help": "The output size of extracted faces. Make sure that the "
                                       "model you intend to train supports your required size. "
                                       "This will only need to be changed for hi-res models."})
@@ -649,6 +666,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "min_max": (0, 1080),
                               "default": 0,
                               "rounding": 20,
+                              "group": "Face Processing",
                               "help": "Filters out faces detected below this size. Length, in "
                                       "pixels across the diagonal of the bounding box. Set to 0 "
                                       "for off"})
@@ -659,6 +677,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "min_max": (1, 100),
                               "default": 1,
                               "rounding": 1,
+                              "group": "Face Processing",
                               "help": "Extract every 'nth' frame. This option will skip frames "
                                       "when extracting faces. For example a value of 1 will "
                                       "extract faces from every frame, a value of 10 will extract "
@@ -693,6 +712,7 @@ class ExtractArgs(ExtractConvertArgs):
                               "min_max": (0, 1000),
                               "rounding": 10,
                               "default": 0,
+                              "group": "output",
                               "help": "Automatically save the alignments file after a set amount "
                                       "of frames. By default the alignments file is only saved at "
                                       "the end of the extraction process. NB: If extracting in 2 "
@@ -713,10 +733,20 @@ class ConvertArgs(ExtractConvertArgs):
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
         argument_list = []
+        argument_list.append({"opts": ("-ref", "--reference-video"),
+                              "action": FileFullPaths,
+                              "dest": "reference_video",
+                              "filetypes": "video",
+                              "type": str,
+                              "group": "data",
+                              "help": "Only required if converting from images to video. Provide "
+                                      "The original video that the source frames were extracted "
+                                      "from (for extracting the fps and audio)."})
         argument_list.append({"opts": ("-m", "--model-dir"),
                               "action": DirFullPaths,
                               "dest": "model_dir",
                               "required": True,
+                              "group": "data",
                               "help": "Model directory. The directory containing the trained "
                                       "model you wish to use for conversion."})
         argument_list.append({
@@ -726,6 +756,7 @@ class ConvertArgs(ExtractConvertArgs):
             "dest": "color_adjustment",
             "choices": PluginLoader.get_available_convert_plugins("color", True),
             "default": "avg-color",
+            "group": "plugins",
             "help": "R|Performs color adjustment to the swapped face. Some of these options have "
                     "configurable settings in '/config/convert.ini' or 'Edit > Configure "
                     "Convert Plugins':"
@@ -743,23 +774,13 @@ class ConvertArgs(ExtractConvertArgs):
                     "gradients at the mask seam by smoothing colors. Generally does not give "
                     "very satisfactory results."
                     "\nL|none: Don't perform color adjustment."})
-        argument_list.append({
-            "opts": ("-sc", "--scaling"),
-            "action": Radio,
-            "type": str.lower,
-            "choices": PluginLoader.get_available_convert_plugins("scaling", True),
-            "default": "none",
-            "help": "R|Performs a scaling process to attempt to get better definition on the "
-                    "final swap. Some of these options have configurable settings in "
-                    "'/config/convert.ini' or 'Edit > Configure Convert Plugins':"
-                    "\nL|sharpen: Perform sharpening on the final face."
-                    "\nL|none: Don't perform any scaling operations."})
         argument_list.append({
             "opts": ("-M", "--mask-type"),
             "action": Radio,
             "type": str.lower,
             "dest": "mask_type",
             "choices": get_available_masks() + ["predicted"],
+            "group": "plugins",
             "default": "predicted",
             "help": "R|Mask to use to replace faces. Blending of the masks can be adjusted in "
                     "'/config/convert.ini' or 'Edit > Configure Convert Plugins':"
@@ -773,11 +794,24 @@ class ConvertArgs(ExtractConvertArgs):
                     "not trained with a mask then this will fallback to "
                     "'{}'".format(get_default_mask()) +
                     "\nL|none: Don't use a mask."})
+        argument_list.append({
+            "opts": ("-sc", "--scaling"),
+            "action": Radio,
+            "type": str.lower,
+            "choices": PluginLoader.get_available_convert_plugins("scaling", True),
+            "group": "plugins",
+            "default": "none",
+            "help": "R|Performs a scaling process to attempt to get better definition on the "
+                    "final swap. Some of these options have configurable settings in "
+                    "'/config/convert.ini' or 'Edit > Configure Convert Plugins':"
+                    "\nL|sharpen: Perform sharpening on the final face."
+                    "\nL|none: Don't perform any scaling operations."})
         argument_list.append({"opts": ("-w", "--writer"),
                               "action": Radio,
                               "type": str,
                               "choices": PluginLoader.get_available_convert_plugins("writer",
                                                                                     False),
+                              "group": "plugins",
                               "default": "opencv",
                               "help": "R|The plugin to use to output the converted images. The "
                                       "writers are configurable in '/config/convert.ini' or 'Edit "
@@ -797,35 +831,24 @@ class ConvertArgs(ExtractConvertArgs):
                               "default": 100,
                               "min_max": (25, 400),
                               "rounding": 1,
+                              "group": "Frame Processing",
                               "help": "Scale the final output frames by this amount. 100%% will "
                                       "output the frames at source dimensions. 50%% at half size "
                                       "200%% at double size"})
-        argument_list.append({"opts": ("-j", "--jobs"),
-                              "dest": "jobs",
-                              "action": Slider,
-                              "type": int,
-                              "default": 0,
-                              "min_max": (0, 40),
-                              "rounding": 1,
-                              "help": "The maximum number of parallel processes for performing "
-                                      "conversion. Converting images is system RAM heavy so it is "
-                                      "possible to run out of memory if you have a lot of "
-                                      "processes and not enough RAM to accomodate them all. "
-                                      "Setting this to 0 will use the maximum available. No "
-                                      "matter what you set this to, it will never attempt to use "
-                                      "more processes than are available on your system. If "
-                                      "singleprocess is enabled this setting will be ignored."})
-        argument_list.append({"opts": ("-g", "--gpus"),
-                              "type": int,
-                              "backend": "nvidia",
-                              "action": Slider,
-                              "min_max": (1, 10),
-                              "rounding": 1,
-                              "default": 1,
-                              "help": "Number of GPUs to use for conversion"})
+        argument_list.append({"opts": ("-fr", "--frame-ranges"),
+                              "nargs": "+",
+                              "type": str,
+                              "group": "Frame Processing",
+                              "help": "Frame ranges to apply transfer to e.g. For frames 10 to 50 "
+                                      "and 90 to 100 use --frame-ranges 10-50 90-100. Frames "
+                                      "falling outside of the selected range will be discarded "
+                                      "unless '-k' (--keep-unchanged) is selected. NB: If you are "
+                                      "converting from images, then the filenames must end with "
+                                      "the frame-number!"})
         argument_list.append({"opts": ("-a", "--input-aligned-dir"),
                               "action": DirFullPaths,
                               "dest": "input_aligned_dir",
+                              "group": "Face Processing",
                               "default": None,
                               "help": "If you have not cleansed your alignments file, then you "
                                       "can filter out faces by defining a folder here that "
@@ -835,23 +858,45 @@ class ConvertArgs(ExtractConvertArgs):
                                       "specified folder will be converted. Leaving this blank "
                                       "will convert all faces that exist within the alignments "
                                       "file."})
-        argument_list.append({"opts": ("-ref", "--reference-video"),
-                              "action": FileFullPaths,
-                              "dest": "reference_video",
-                              "filetypes": "video",
-                              "type": str,
-                              "help": "Only required if converting from images to video. Provide "
-                                      "The original video that the source frames were extracted "
-                                      "from (for extracting the fps and audio)."})
-        argument_list.append({"opts": ("-fr", "--frame-ranges"),
+        argument_list.append({"opts": ("-n", "--nfilter"),
+                              "action": FilesFullPaths,
+                              "filetypes": "image",
+                              "dest": "nfilter",
                               "nargs": "+",
-                              "type": str,
-                              "help": "Frame ranges to apply transfer to e.g. For frames 10 to 50 "
-                                      "and 90 to 100 use --frame-ranges 10-50 90-100. Frames "
-                                      "falling outside of the selected range will be discarded "
-                                      "unless '-k' (--keep-unchanged) is selected. NB: If you are "
-                                      "converting from images, then the filenames must end with "
-                                      "the frame-number!"})
+                              "default": None,
+                              "group": "Face Processing",
+                              "help": "Optionally filter out people who you do not wish to "
+                                      "process by passing in an image of that person. Should be a "
+                                      "front portrait with a single person in the image. Multiple "
+                                      "images can be added space separated. NB: Using face filter "
+                                      "will significantly decrease extraction speed and its "
+                                      "accuracy cannot be guaranteed."})
+        argument_list.append({"opts": ("-f", "--filter"),
+                              "action": FilesFullPaths,
+                              "filetypes": "image",
+                              "dest": "filter",
+                              "nargs": "+",
+                              "default": None,
+                              "group": "Face Processing",
+                              "help": "Optionally select people you wish to process by passing in "
+                                      "an image of that person. Should be a front portrait with a "
+                                      "single person in the image. Multiple images can be added "
+                                      "space separated. NB: Using face filter will significantly "
+                                      "decrease extraction speed and its accuracy cannot be "
+                                      "guaranteed."})
+        argument_list.append({"opts": ("-l", "--ref_threshold"),
+                              "action": Slider,
+                              "min_max": (0.01, 0.99),
+                              "rounding": 2,
+                              "type": float,
+                              "dest": "ref_threshold",
+                              "default": 0.4,
+                              "group": "Face Processing",
+                              "help": "For use with the optional nfilter/filter files. Threshold "
+                                      "for positive face recognition. Lower values are stricter. "
+                                      "NB: Using face filter will significantly decrease "
+                                      "extraction speed and its accuracy cannot be "
+                                      "guaranteed."})
         argument_list.append({"opts": ("-k", "--keep-unchanged"),
                               "action": "store_true",
                               "dest": "keep_unchanged",
@@ -891,6 +936,7 @@ class TrainArgs(FaceSwapArgs):
                               "action": DirFullPaths,
                               "dest": "input_a",
                               "required": True,
+                              "group": "faces",
                               "help": "Input directory. A directory containing training images "
                                       "for face A. This is the original face, i.e. the face that "
                                       "you want to remove and replace with face B."})
@@ -900,24 +946,16 @@ class TrainArgs(FaceSwapArgs):
                               "type": str,
                               "dest": "alignments_path_a",
                               "default": None,
+                              "group": "faces",
                               "help": "Path to alignments file for training set A. Only required "
                                       "if you are using a masked model or warp-to-landmarks is "
                                       "enabled. Defaults to <input-A>/alignments.json if not "
                                       "provided."})
-        argument_list.append({"opts": ("-tia", "--timelapse-input-A"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_input_a",
-                              "default": None,
-                              "help": "Optional for creating a timelapse. Timelapse will save an "
-                                      "image of your selected faces into the timelapse-output "
-                                      "folder at every save iteration. This should be the "
-                                      "input folder of 'A' faces that you would like to use for "
-                                      "creating the timelapse. You must also supply a "
-                                      "--timelapse-output and a --timelapse-input-B parameter."})
         argument_list.append({"opts": ("-B", "--input-B"),
                               "action": DirFullPaths,
                               "dest": "input_b",
                               "required": True,
+                              "group": "faces",
                               "help": "Input directory. A directory containing training images "
                                       "for face B. This is the swap face, i.e. the face that "
                                       "you want to place onto the head of person A."})
@@ -927,33 +965,16 @@ class TrainArgs(FaceSwapArgs):
                               "type": str,
                               "dest": "alignments_path_b",
                               "default": None,
+                              "group": "faces",
                               "help": "Path to alignments file for training set B. Only required "
                                       "if you are using a masked model or warp-to-landmarks is "
                                       "enabled. Defaults to <input-B>/alignments.json if not "
                                       "provided."})
-        argument_list.append({"opts": ("-tib", "--timelapse-input-B"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_input_b",
-                              "default": None,
-                              "help": "Optional for creating a timelapse. Timelapse will save an "
-                                      "image of your selected faces into the timelapse-output "
-                                      "folder at every save iteration. This should be the "
-                                      "input folder of 'B' faces that you would like to use for "
-                                      "creating the timelapse. You must also supply a "
-                                      "--timelapse-output and a --timelapse-input-A parameter."})
-        argument_list.append({"opts": ("-to", "--timelapse-output"),
-                              "action": DirFullPaths,
-                              "dest": "timelapse_output",
-                              "default": None,
-                              "help": "Optional for creating a timelapse. Timelapse will save an "
-                                      "image of your selected faces into the timelapse-output "
-                                      "folder at every save iteration. If the input folders are "
-                                      "supplied but no output folder, it will default to your "
-                                      "model folder /timelapse/"})
         argument_list.append({"opts": ("-m", "--model-dir"),
                               "action": DirFullPaths,
                               "dest": "model_dir",
                               "required": True,
+                              "group": "model",
                               "help": "Model directory. This is where the training data will be "
                                       "stored. You should always specify a new folder for new "
                                       "models. If starting a new model, select either an empty "
@@ -965,6 +986,7 @@ class TrainArgs(FaceSwapArgs):
                               "type": str.lower,
                               "choices": PluginLoader.get_available_models(),
                               "default": PluginLoader.get_default_model(),
+                              "group": "model",
                               "help": "R|Select which trainer to use. Trainers can be"
                                       "configured from the edit menu or the config folder."
                                       "\nL|original: The original model created by /u/deepfakes."
@@ -987,24 +1009,6 @@ class TrainArgs(FaceSwapArgs):
                                       "\nL|villain: 128px in/out model from villainguy. Very "
                                       "resource hungry (11GB for batchsize 16). Good for "
                                       "details, but more susceptible to color differences."})
-        argument_list.append({"opts": ("-s", "--save-interval"),
-                              "type": int,
-                              "action": Slider,
-                              "min_max": (10, 1000),
-                              "rounding": 10,
-                              "dest": "save_interval",
-                              "default": 100,
-                              "help": "Sets the number of iterations between each model save."})
-        argument_list.append({"opts": ("-ss", "--snapshot-interval"),
-                              "type": int,
-                              "action": Slider,
-                              "min_max": (0, 100000),
-                              "rounding": 5000,
-                              "dest": "snapshot_interval",
-                              "default": 25000,
-                              "help": "Sets the number of iterations before saving a backup "
-                                      "snapshot of the model in it's current state. Set to 0 for "
-                                      "off."})
         argument_list.append({"opts": ("-bs", "--batch-size"),
                               "type": int,
                               "action": Slider,
@@ -1012,6 +1016,7 @@ class TrainArgs(FaceSwapArgs):
                               "rounding": 2,
                               "dest": "batch_size",
                               "default": 64,
+                              "group": "training",
                               "help": "Batch size. This is the number of images processed through "
                                       "the model for each iteration. Larger batches require more "
                                       "GPU RAM."})
@@ -1021,6 +1026,7 @@ class TrainArgs(FaceSwapArgs):
                               "min_max": (0, 5000000),
                               "rounding": 20000,
                               "default": 1000000,
+                              "group": "training",
                               "help": "Length of training in iterations. This is only really used "
                                       "for automation. There is no 'correct' number of iterations "
                                       "a model should be trained for. You should stop training "
@@ -1033,6 +1039,7 @@ class TrainArgs(FaceSwapArgs):
                               "action": Slider,
                               "min_max": (1, 10),
                               "rounding": 1,
+                              "group": "training",
                               "default": 1,
                               "help": "Number of GPUs to use for training"})
         argument_list.append({"opts": ("-ps", "--preview-scale"),
@@ -1040,9 +1047,62 @@ class TrainArgs(FaceSwapArgs):
                               "action": Slider,
                               "dest": "preview_scale",
                               "min_max": (25, 200),
+                              "group": "training",
                               "rounding": 25,
                               "default": 50,
                               "help": "Percentage amount to scale the preview by."})
+        argument_list.append({"opts": ("-s", "--save-interval"),
+                              "type": int,
+                              "action": Slider,
+                              "min_max": (10, 1000),
+                              "rounding": 10,
+                              "dest": "save_interval",
+                              "group": "Saving",
+                              "default": 100,
+                              "help": "Sets the number of iterations between each model save."})
+        argument_list.append({"opts": ("-ss", "--snapshot-interval"),
+                              "type": int,
+                              "action": Slider,
+                              "min_max": (0, 100000),
+                              "rounding": 5000,
+                              "dest": "snapshot_interval",
+                              "group": "Saving",
+                              "default": 25000,
+                              "help": "Sets the number of iterations before saving a backup "
+                                      "snapshot of the model in it's current state. Set to 0 for "
+                                      "off."})
+        argument_list.append({"opts": ("-tia", "--timelapse-input-A"),
+                              "action": DirFullPaths,
+                              "dest": "timelapse_input_a",
+                              "default": None,
+                              "group": "timelapse",
+                              "help": "Optional for creating a timelapse. Timelapse will save an "
+                                      "image of your selected faces into the timelapse-output "
+                                      "folder at every save iteration. This should be the "
+                                      "input folder of 'A' faces that you would like to use for "
+                                      "creating the timelapse. You must also supply a "
+                                      "--timelapse-output and a --timelapse-input-B parameter."})
+        argument_list.append({"opts": ("-tib", "--timelapse-input-B"),
+                              "action": DirFullPaths,
+                              "dest": "timelapse_input_b",
+                              "default": None,
+                              "group": "timelapse",
+                              "help": "Optional for creating a timelapse. Timelapse will save an "
+                                      "image of your selected faces into the timelapse-output "
+                                      "folder at every save iteration. This should be the "
+                                      "input folder of 'B' faces that you would like to use for "
+                                      "creating the timelapse. You must also supply a "
+                                      "--timelapse-output and a --timelapse-input-A parameter."})
+        argument_list.append({"opts": ("-to", "--timelapse-output"),
+                              "action": DirFullPaths,
+                              "dest": "timelapse_output",
+                              "default": None,
+                              "group": "timelapse",
+                              "help": "Optional for creating a timelapse. Timelapse will save an "
+                                      "image of your selected faces into the timelapse-output "
+                                      "folder at every save iteration. If the input folders are "
+                                      "supplied but no output folder, it will default to your "
+                                      "model folder /timelapse/"})
         argument_list.append({"opts": ("-p", "--preview"),
                               "action": "store_true",
                               "dest": "preview",
diff --git a/lib/config.py b/lib/config.py
index ffbac83..c4e41f7 100644
--- a/lib/config.py
+++ b/lib/config.py
@@ -118,7 +118,8 @@ class FaceswapConfig():
         self.defaults[title]["helptext"] = info
 
     def add_item(self, section=None, title=None, datatype=str, default=None, info=None,
-                 rounding=None, min_max=None, choices=None, gui_radio=False, fixed=True):
+                 rounding=None, min_max=None, choices=None, gui_radio=False, fixed=True,
+                 group=None):
         """ Add a default item to a config section
 
             For int or float values, rounding and min_max must be set
@@ -138,11 +139,13 @@ class FaceswapConfig():
             existing models, and will overide the value saved in the state file with the
             updated value in config.
 
+            The 'Group' parameter allows you to assign the config item to a group in the GUI
+
         """
         logger.debug("Add item: (section: '%s', title: '%s', datatype: '%s', default: '%s', "
                      "info: '%s', rounding: '%s', min_max: %s, choices: %s, gui_radio: %s, "
-                     "fixed: %s)", section, title, datatype, default, info, rounding, min_max,
-                     choices, gui_radio, fixed)
+                     "fixed: %s, group: %s)", section, title, datatype, default, info, rounding,
+                     min_max, choices, gui_radio, fixed, group)
 
         choices = list() if not choices else choices
 
@@ -168,7 +171,8 @@ class FaceswapConfig():
                                          "min_max": min_max,
                                          "choices": choices,
                                          "gui_radio": gui_radio,
-                                         "fixed": fixed}
+                                         "fixed": fixed,
+                                         "group": group}
 
     @staticmethod
     def expand_helptext(helptext, choices, default, datatype, min_max, fixed):
diff --git a/lib/gui/command.py b/lib/gui/command.py
index 47950dc..774f691 100644
--- a/lib/gui/command.py
+++ b/lib/gui/command.py
@@ -122,6 +122,7 @@ class OptionsFrame(ttk.Frame):  # pylint:disable=too-many-ancestors
         self.canvas.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)
 
         self.optsframe = ttk.Frame(self.canvas)
+        self.group_frames = dict()
         self.optscanvas = self.canvas.create_window((0, 0),
                                                     window=self.optsframe,
                                                     anchor=tk.NW)
@@ -159,9 +160,19 @@ class OptionsFrame(ttk.Frame):  # pylint:disable=too-many-ancestors
 
         cli_opts = get_config().cli_opts
         for option in cli_opts.gen_command_options(self.command):
+            group = option["group"]
+            frame = self.optsframe
+            if group is not None:
+                group = group.lower()
+                if self.group_frames.get(group, None) is None:
+                    group_frame = ttk.LabelFrame(self.optsframe, text=group.title())
+                    group_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)
+                    self.group_frames[group] = group_frame
+                frame = self.group_frames[group]
+
             optioncontrol = OptionControl(self.command,
                                           option,
-                                          self.optsframe,
+                                          frame,
                                           self.chkbtns[1])
             optioncontrol.build_full_control()
 
diff --git a/lib/gui/options.py b/lib/gui/options.py
index 8e8e71c..609a058 100644
--- a/lib/gui/options.py
+++ b/lib/gui/options.py
@@ -92,12 +92,13 @@ class CliOptions():
             if opt.get("help", "") == SUPPRESS:
                 logger.trace("Skipping suppressed option: %s", opt)
                 continue
-            ctl, sysbrowser, filetypes, action_option = self.set_control(opt)
+            ctl, sysbrowser, filetypes, action_option, group = self.set_control(opt)
             opt["control_title"] = self.set_control_title(opt.get("opts", ""))
             opt["control"] = ctl
             opt["filesystem_browser"] = sysbrowser
             opt["filetypes"] = filetypes
             opt["action_option"] = action_option
+            opt["group"] = group
             final_options.append(opt)
             logger.trace("Processed: %s", opt)
         return final_options
@@ -112,6 +113,7 @@ class CliOptions():
     def set_control(self, option):
         """ Set the control and filesystem browser to use for each option """
         sysbrowser = None
+        group = option.get("group", None)
         action = option.get("action", None)
         action_option = option.get("action_option", None)
         filetypes = option.get("filetypes", None)
@@ -134,7 +136,7 @@ class CliOptions():
             ctl = ttk.Combobox
         elif option.get("action", "") == "store_true":
             ctl = ttk.Checkbutton
-        return ctl, sysbrowser, filetypes, action_option
+        return ctl, sysbrowser, filetypes, action_option, group
 
     @staticmethod
     def set_sysbrowser(action, filetypes, action_option):
diff --git a/lib/gui/popup_configure.py b/lib/gui/popup_configure.py
index 382ea49..ae6925b 100644
--- a/lib/gui/popup_configure.py
+++ b/lib/gui/popup_configure.py
@@ -193,6 +193,7 @@ class ConfigFrame(ttk.Frame):  # pylint: disable=too-many-ancestors
 
         self.optsframe = ttk.Frame(self.canvas)
         self.optscanvas = self.canvas.create_window((0, 0), window=self.optsframe, anchor=tk.NW)
+        self.group_frames = dict()
 
         self.build_frame()
         logger.debug("Initialized %s", self.__class__.__name__)
@@ -207,7 +208,17 @@ class ConfigFrame(ttk.Frame):  # pylint: disable=too-many-ancestors
         for key, val in self.options.items():
             if key == "helptext":
                 continue
-            ctl = ControlBuilder(self.optsframe,
+            group = val["group"]
+            frame = self.optsframe
+            if group is not None:
+                group = group.lower()
+                if self.group_frames.get(group, None) is None:
+                    group_frame = ttk.LabelFrame(self.optsframe, text=group.title())
+                    group_frame.pack(side=tk.TOP, fill=tk.X, padx=5, pady=5)
+                    self.group_frames[group] = group_frame
+                frame = self.group_frames[group]
+
+            ctl = ControlBuilder(frame,
                                  key,
                                  val["type"],
                                  val["default"],
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index d2a41f9..c855d45 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -57,13 +57,42 @@ class Config(FaceswapConfig):
         self.add_section(title=section,
                          info="Options that apply to all models" + ADDITIONAL_INFO)
         self.add_item(
-            section=section, title="icnr_init", datatype=bool, default=False,
+            section=section, title="coverage", datatype=float, default=68.75,
+            min_max=(62.5, 100.0), rounding=2, fixed=True,
+            info="How much of the extracted image to train on. A lower coverage will limit the "
+                 "model's scope to a zoomed-in central area while higher amounts can include the "
+                 "entire face. A trade-off exists between lower amounts given more detail "
+                 "versus higher amounts avoiding noticeable swap transitions. Sensible values to "
+                 "use are:"
+                 "\n\t62.5%% spans from eyebrow to eyebrow."
+                 "\n\t75.0%% spans from temple to temple."
+                 "\n\t87.5%% spans from ear to ear."
+                 "\n\t100.0%% is a mugshot.")
+        self.add_item(
+            section=section, title="mask_type", datatype=str, default="none",
+            choices=get_available_masks(), group="mask",
+            info="The mask to be used for training:"
+                 "\n\t none: Doesn't use any mask."
+                 "\n\t components: An improved face hull mask using a facehull of 8 facial parts"
+                 "\n\t dfl_full: An improved face hull mask using a facehull of 3 facial parts"
+                 "\n\t extended: Based on components mask. Extends the eyebrow points to further "
+                 "up the forehead. May perform badly on difficult angles."
+                 "\n\t facehull: Face cutout based on landmarks")
+        self.add_item(
+            section=section, title="mask_blur", datatype=bool, default=False, group="mask",
+            info="Apply gaussian blur to the mask input. This has the effect of smoothing the "
+                 "edges of the mask, which can help with poorly calculated masks, and give less "
+                 "of a hard edge to the predicted mask.")
+        self.add_item(
+            section=section, title="icnr_init", datatype=bool,
+            default=False, group="initialization",
             info="Use ICNR to tile the default initializer in a repeating pattern. "
                  "This strategy is designed for pairing with sub-pixel / pixel shuffler "
                  "to reduce the 'checkerboard effect' in image reconstruction. "
                  "\n\t https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf")
         self.add_item(
-            section=section, title="conv_aware_init", datatype=bool, default=False,
+            section=section, title="conv_aware_init", datatype=bool,
+            default=False, group="initialization",
             info="Use Convolution Aware Initialization for convolutional layers. "
                  "This can help eradicate the vanishing and exploding gradient problem "
                  "as well as lead to higher accuracy, lower loss and faster convergence.\nNB:"
@@ -77,26 +106,29 @@ class Config(FaceswapConfig):
                  "for this initialization technique are expensive. This will only impact starting "
                  "a new model.")
         self.add_item(
-            section=section, title="subpixel_upscaling", datatype=bool, default=False,
+            section=section, title="subpixel_upscaling", datatype=bool,
+            default=False, group="network",
             info="Use subpixel upscaling rather than pixel shuffler. These techniques "
                  "are both designed to produce better resolving upscaling than other "
                  "methods. Each perform the same operations, but using different TF opts."
                  "\n\t https://arxiv.org/pdf/1609.05158.pdf")
         self.add_item(
-            section=section, title="reflect_padding", datatype=bool, default=False,
+            section=section, title="reflect_padding", datatype=bool,
+            default=False, group="network",
             info="Use reflection padding rather than zero padding with convolutions. "
                  "Each convolution must pad the image boundaries to maintain the proper "
                  "sizing. More complex padding schemes can reduce artifacts at the "
                  "border of the image."
                  "\n\t http://www-cs.engr.ccny.cuny.edu/~wolberg/cs470/hw/hw2_pad.txt")
         self.add_item(
-            section=section, title="penalized_mask_loss", datatype=bool, default=True,
+            section=section, title="penalized_mask_loss", datatype=bool,
+            default=True, group="loss",
             info="Image loss function is weighted by mask presence. For areas of "
                  "the image without the facial mask, reconstuction errors will be "
                  "ignored while the masked face area is prioritized. May increase "
                  "overall quality by focusing attention on the core face area.")
         self.add_item(
-            section=section, title="loss_function", datatype=str,
+            section=section, title="loss_function", datatype=str, group="loss",
             default="mae",
             choices=["mae", "mse", "logcosh", "smooth_l1", "l_inf_norm", "ssim", "gmsd",
                      "pixel_gradient_diff"],
@@ -125,42 +157,15 @@ class Config(FaceswapConfig):
                  "pixel spatial difference in each image and then minimize that difference "
                  "between two images. Allows for large color shifts,but maintains the structure "
                  "of the image.\n")
-        self.add_item(
-            section=section, title="mask_type", datatype=str, default="none",
-            choices=get_available_masks(),
-            info="The mask to be used for training:"
-                 "\n\t none: Doesn't use any mask."
-                 "\n\t components: An improved face hull mask using a facehull of 8 facial parts"
-                 "\n\t dfl_full: An improved face hull mask using a facehull of 3 facial parts"
-                 "\n\t extended: Based on components mask. Extends the eyebrow points to further "
-                 "up the forehead. May perform badly on difficult angles."
-                 "\n\t facehull: Face cutout based on landmarks")
-        self.add_item(
-            section=section, title="mask_blur", datatype=bool, default=False,
-            info="Apply gaussian blur to the mask input. This has the effect of smoothing the "
-                 "edges of the mask, which can help with poorly calculated masks, and give less "
-                 "of a hard edge to the predicted mask.")
         self.add_item(
             section=section, title="learning_rate", datatype=float, default=5e-5,
-            min_max=(1e-6, 1e-4), rounding=6, fixed=False,
+            min_max=(1e-6, 1e-4), rounding=6, fixed=False, group="optimizer",
             info="Learning rate - how fast your network will learn (how large are "
                  "the modifications to the model weights after one batch of training). "
                  "Values that are too large might result in model crashes and the "
                  "inability of the model to find the best solution. "
                  "Values that are too small might be unable to escape from dead-ends "
                  "and find the best global minimum.")
-        self.add_item(
-            section=section, title="coverage", datatype=float, default=68.75,
-            min_max=(62.5, 100.0), rounding=2, fixed=True,
-            info="How much of the extracted image to train on. A lower coverage will limit the "
-                 "model's scope to a zoomed-in central area while higher amounts can include the "
-                 "entire face. A trade-off exists between lower amounts given more detail "
-                 "versus higher amounts avoiding noticeable swap transitions. Sensible values to "
-                 "use are:"
-                 "\n\t62.5%% spans from eyebrow to eyebrow."
-                 "\n\t75.0%% spans from temple to temple."
-                 "\n\t87.5%% spans from ear to ear."
-                 "\n\t100.0%% is a mugshot.")
 
     def load_module(self, filename, module_path, plugin_type):
         """ Load the defaults module and add defaults """
diff --git a/plugins/train/model/dfl_sae_defaults.py b/plugins/train/model/dfl_sae_defaults.py
index 3eacc79..5c627c9 100644
--- a/plugins/train/model/dfl_sae_defaults.py
+++ b/plugins/train/model/dfl_sae_defaults.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python3
 """
-    The default options for the faceswap Dfl_H128 Model plugin.
+    The default options for the faceswap Dfl_SAE Model plugin.
 
     Defaults files should be named <plugin_name>_defaults.py
     Any items placed into this file will automatically get added to the relevant config .ini files
@@ -45,16 +45,6 @@ _HELPTEXT = "DFL SAE Model (Adapted from https://github.com/iperov/DeepFaceLab)"
 
 
 _DEFAULTS = {
-    "architecture": {
-        "default": "df",
-        "info": "Model architecture:"
-                "\n\t'df': Keeps the faces more natural."
-                "\n\t'liae': Can help fix overly different face shapes.",
-        "datatype": str,
-        "choices": ["df", "liae"],
-        "gui_radio": True,
-        "fixed": True,
-    },
     "input_size": {
         "default": 128,
         "info": "Resolution (in pixels) of the input image to train on.\n"
@@ -65,6 +55,24 @@ _DEFAULTS = {
         "min_max": (64, 256),
         "fixed": True,
     },
+    "clipnorm": {
+        "default": True,
+        "info": "Controls gradient clipping of the optimizer. Can prevent model corruption at "
+                "the expense of VRAM.",
+        "datatype": bool,
+        "fixed": False,
+    },
+    "architecture": {
+        "default": "df",
+        "info": "Model architecture:"
+                "\n\t'df': Keeps the faces more natural."
+                "\n\t'liae': Can help fix overly different face shapes.",
+        "datatype": str,
+        "choices": ["df", "liae"],
+        "gui_radio": True,
+        "fixed": True,
+        "group": "network",
+    },
     "autoencoder_dims": {
         "default": 0,
         "info": "Face information is stored in AutoEncoder dimensions. If there are not enough "
@@ -75,6 +83,7 @@ _DEFAULTS = {
         "rounding": 32,
         "min_max": (0, 1024),
         "fixed": True,
+        "group": "network",
     },
     "encoder_dims": {
         "default": 42,
@@ -84,6 +93,7 @@ _DEFAULTS = {
         "rounding": 1,
         "min_max": (21, 85),
         "fixed": True,
+        "group": "network",
     },
     "decoder_dims": {
         "default": 21,
@@ -93,18 +103,13 @@ _DEFAULTS = {
         "rounding": 1,
         "min_max": (10, 85),
         "fixed": True,
+        "group": "network",
     },
     "multiscale_decoder": {
         "default": False,
         "info": "Multiscale decoder can help to obtain better details.",
         "datatype": bool,
         "fixed": True,
-    },
-    "clipnorm": {
-        "default": True,
-        "info": "Controls gradient clipping of the optimizer. Can prevent model corruption at "
-                "the expense of VRAM.",
-        "datatype": bool,
-        "fixed": False,
+        "group": "network",
     },
 }
diff --git a/plugins/train/model/realface_defaults.py b/plugins/train/model/realface_defaults.py
index 9ff86b1..cdf001f 100755
--- a/plugins/train/model/realface_defaults.py
+++ b/plugins/train/model/realface_defaults.py
@@ -61,6 +61,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "size"
     },
     "output_size": {
         "default": 128,
@@ -73,6 +74,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "size"
     },
     "dense_nodes": {
         "default": 1536,
@@ -85,6 +87,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network"
     },
     "complexity_encoder": {
         "default": 128,
@@ -95,6 +98,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network"
     },
     "complexity_decoder": {
         "default": 512,
@@ -105,5 +109,6 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network"
     },
 }
diff --git a/plugins/train/model/unbalanced_defaults.py b/plugins/train/model/unbalanced_defaults.py
index 16f430d..b92c5e5 100755
--- a/plugins/train/model/unbalanced_defaults.py
+++ b/plugins/train/model/unbalanced_defaults.py
@@ -48,6 +48,20 @@ _HELPTEXT = (
 
 
 _DEFAULTS = {
+    "input_size": {
+        "default": 128,
+        "info": "Resolution (in pixels) of the image to train on.\n"
+                "BE AWARE Larger resolution will dramatically increaseVRAM requirements.\n"
+                "Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\n"
+                "NB: Your faceset must be at least 1.6x larger than your required input "
+                "size.\n(e.g. 160 is the maximum input size for a 256x256 faceset).",
+        "datatype": int,
+        "rounding": 64,
+        "min_max": (64, 512),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
+    },
     "lowmem": {
         "default": False,
         "info": "Lower memory mode. Set to 'True' if having issues with VRAM useage.\n"
@@ -81,6 +95,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network",
     },
     "complexity_encoder": {
         "default": 128,
@@ -91,6 +106,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network",
     },
     "complexity_decoder_a": {
         "default": 384,
@@ -101,6 +117,7 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+        "group": "network",
     },
     "complexity_decoder_b": {
         "default": 512,
@@ -111,19 +128,6 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
-    },
-    "input_size": {
-        "default": 128,
-        "info": "Resolution (in pixels) of the image to train on.\n"
-                "BE AWARE Larger resolution will dramatically increaseVRAM requirements.\n"
-                "Make sure your resolution is divisible by 64 (e.g. 64, 128, 256 etc.).\n"
-                "NB: Your faceset must be at least 1.6x larger than your required input "
-                "size.\n(e.g. 160 is the maximum input size for a 256x256 faceset).",
-        "datatype": int,
-        "rounding": 64,
-        "min_max": (64, 512),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
+        "group": "network",
     },
 }
diff --git a/plugins/train/trainer/original_defaults.py b/plugins/train/trainer/original_defaults.py
index 7aa168f..07a5774 100755
--- a/plugins/train/trainer/original_defaults.py
+++ b/plugins/train/trainer/original_defaults.py
@@ -60,6 +60,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 25),
+        "group": "image augmentation",
     },
     "rotation_range": {
         "default": 10,
@@ -67,6 +68,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 25),
+        "group": "image augmentation",
     },
     "shift_range": {
         "default": 5,
@@ -75,6 +77,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 25),
+        "group": "image augmentation",
     },
     "flip_chance": {
         "default": 50,
@@ -83,6 +86,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 75),
+        "group": "image augmentation",
     },
     "color_lightness": {
         "default": 30,
@@ -91,6 +95,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 75),
+        "group": "color augmentation",
     },
     "color_ab": {
         "default": 8,
@@ -100,6 +105,7 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (0, 50),
+        "group": "color augmentation",
     },
     "color_clahe_chance": {
         "default": 50,
@@ -110,6 +116,7 @@ _DEFAULTS = {
         "rounding": 1,
         "min_max": (0, 75),
         "fixed": False,
+        "group": "color augmentation",
     },
     "color_clahe_max_size": {
         "default": 4,
@@ -121,5 +128,6 @@ _DEFAULTS = {
         "datatype": int,
         "rounding": 1,
         "min_max": (1, 8),
+        "group": "color augmentation",
     },
 }
diff --git a/tools/cli.py b/tools/cli.py
index 2c868fd..51aa56e 100644
--- a/tools/cli.py
+++ b/tools/cli.py
@@ -1,5 +1,7 @@
 #!/usr/bin/env python3
 """ Command Line Arguments for tools """
+from argparse import SUPPRESS
+
 from lib.cli import FaceSwapArgs
 from lib.cli import (ContextFullPaths, DirOrFileFullPaths, DirFullPaths, FileFullPaths,
                      FilesFullPaths, SaveFileFullPaths, Radio, Slider)
@@ -87,6 +89,7 @@ class AlignmentsArgs(FaceSwapArgs):
                               "action": FilesFullPaths,
                               "dest": "alignments_file",
                               "nargs": "+",
+                              "group": "data",
                               "required": True,
                               "filetypes": "alignments",
                               "help": "Full path to the alignments file to be processed. If "
@@ -95,16 +98,19 @@ class AlignmentsArgs(FaceSwapArgs):
         argument_list.append({"opts": ("-fc", "-faces_folder"),
                               "action": DirFullPaths,
                               "dest": "faces_dir",
+                              "group": "data",
                               "help": "Directory containing extracted faces."})
         argument_list.append({"opts": ("-fr", "-frames_folder"),
                               "action": DirOrFileFullPaths,
                               "dest": "frames_dir",
                               "filetypes": "video",
+                              "group": "data",
                               "help": "Directory containing source frames "
                                       "that faces were extracted from."})
         argument_list.append({"opts": ("-fmt", "--alignment_format"),
                               "type": str,
                               "choices": ("json", "pickle", "yaml"),
+                              "group": "data",
                               "help": "The file format to save the alignment "
                                       "data in. Defaults to same as source."})
         argument_list.append({
@@ -112,6 +118,7 @@ class AlignmentsArgs(FaceSwapArgs):
             "action": Radio,
             "type": str,
             "choices": ("console", "file", "move"),
+            "group": "output",
             "default": "console",
             "help": "R|How to output discovered items ('faces' and 'frames' only):"
                     "\nL|'console': Print the list of frames to the screen. (DEFAULT)"
@@ -126,6 +133,7 @@ class AlignmentsArgs(FaceSwapArgs):
                               "min_max": (1, 100),
                               "default": 1,
                               "rounding": 1,
+                              "group": "output",
                               "help": "Extract every 'nth' frame. This option will skip frames "
                                       "when extracting faces. For example a value of 1 will "
                                       "extract faces from every frame, a value of 10 will extract "
@@ -135,6 +143,7 @@ class AlignmentsArgs(FaceSwapArgs):
                               "action": Slider,
                               "min_max": (128, 512),
                               "default": 256,
+                              "group": "output",
                               "rounding": 64,
                               "help": "The output size of extracted faces. (extract only)"})
         argument_list.append({"opts": ("-ae", "--align-eyes"),
@@ -164,6 +173,7 @@ class PreviewArgs(FaceSwapArgs):
                               "action": DirOrFileFullPaths,
                               "filetypes": "video",
                               "dest": "input_dir",
+                              "group": "data",
                               "required": True,
                               "help": "Input directory or video. Either a directory containing "
                                       "the image files you wish to process or path to a video "
@@ -172,12 +182,14 @@ class PreviewArgs(FaceSwapArgs):
                               "action": FileFullPaths,
                               "filetypes": "alignments",
                               "type": str,
+                              "group": "data",
                               "dest": "alignments_path",
                               "help": "Path to the alignments file for the input, if not at the "
                                       "default location"})
         argument_list.append({"opts": ("-m", "--model-dir"),
                               "action": DirFullPaths,
                               "dest": "model_dir",
+                              "group": "data",
                               "required": True,
                               "help": "Model directory. A directory containing the trained model "
                                       "you wish to process."})
@@ -231,14 +243,16 @@ class EffmpegArgs(FaceSwapArgs):
                               "dest": "input",
                               "default": "input",
                               "help": "Input file.",
+                              "group": "data",
                               "required": True,
                               "action_option": "-a",
                               "filetypes": "video"})
 
         argument_list.append({"opts": ('-o', '--output'),
                               "action": ContextFullPaths,
-                              "dest": "output",
+                              "group": "data",
                               "default": "",
+                              "dest": "output",
                               "help": "Output file. If no output is "
                                       "specified then: if the output is "
                                       "meant to be a video then a video "
@@ -257,6 +271,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-r', '--reference-video'),
                               "action": FileFullPaths,
                               "dest": "ref_vid",
+                              "group": "data",
                               "default": None,
                               "help": "Path to reference video if 'input' "
                                       "was not a video.",
@@ -265,6 +280,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-fps', '--fps'),
                               "type": str,
                               "dest": "fps",
+                              "group": "output",
                               "default": "-1.0",
                               "help": "Provide video fps. Can be an integer, "
                                       "float or fraction. Negative values "
@@ -276,6 +292,7 @@ class EffmpegArgs(FaceSwapArgs):
                               "action": Radio,
                               "choices": _image_extensions,
                               "dest": "extract_ext",
+                              "group": "output",
                               "default": ".png",
                               "help": "Image format that extracted images "
                                       "should be saved as. '.bmp' will offer "
@@ -287,6 +304,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-s', '--start'),
                               "type": str,
                               "dest": "start",
+                              "group": "clip",
                               "default": "00:00:00",
                               "help": "Enter the start time from which an "
                                       "action is to be applied. "
@@ -298,6 +316,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-e', '--end'),
                               "type": str,
                               "dest": "end",
+                              "group": "clip",
                               "default": "00:00:00",
                               "help": "Enter the end time to which an action "
                                       "is to be applied. If both an end time "
@@ -309,6 +328,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-d', '--duration'),
                               "type": str,
                               "dest": "duration",
+                              "group": "clip",
                               "default": "00:00:00",
                               "help": "Enter the duration of the chosen "
                                       "action, for example if you enter "
@@ -324,6 +344,7 @@ class EffmpegArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-m', '--mux-audio'),
                               "action": "store_true",
                               "dest": "mux_audio",
+                              "group": "output",
                               "default": False,
                               "help": "Mux the audio from the reference "
                                       "video into the input video. This "
@@ -339,6 +360,7 @@ class EffmpegArgs(FaceSwapArgs):
                          "(3, 90Clockwise&VerticalFlip)"),
              "type": lambda v: self.__parse_transpose(v),
              "dest": "transpose",
+             "group": "rotate",
              "default": None,
              "help": "Transpose the video. If transpose is "
                      "set, then degrees will be ignored. For "
@@ -351,12 +373,14 @@ class EffmpegArgs(FaceSwapArgs):
                               "type": str,
                               "dest": "degrees",
                               "default": None,
+                              "group": "rotate",
                               "help": "Rotate the video clockwise by the "
                                       "given number of degrees."})
 
         argument_list.append({"opts": ('-sc', '--scale'),
                               "type": str,
                               "dest": "scale",
+                              "group": "output",
                               "default": "1920x1080",
                               "help": "Set the new resolution scale if the "
                                       "chosen action is 'rescale'."})
@@ -365,10 +389,13 @@ class EffmpegArgs(FaceSwapArgs):
                               "action": "store_true",
                               "dest": "preview",
                               "default": False,
-                              "help": "Uses ffplay to preview the effects of "
-                                      "actions that have a video output. "
-                                      "Currently preview does not work when "
-                                      "muxing audio."})
+                              # TODO Fix preview or remove
+                              "help": SUPPRESS,
+                              # "help": "Uses ffplay to preview the effects of "
+                              #         "actions that have a video output. "
+                              #         "Currently preview does not work when "
+                              #         "muxing audio."
+                              })
 
         argument_list.append({"opts": ('-q', '--quiet'),
                               "action": "store_true",
@@ -416,32 +443,17 @@ class SortArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-i', '--input'),
                               "action": DirFullPaths,
                               "dest": "input_dir",
-                              "default": "input_dir",
+                              "group": "data",
                               "help": "Input directory of aligned faces.",
                               "required": True})
 
         argument_list.append({"opts": ('-o', '--output'),
                               "action": DirFullPaths,
                               "dest": "output_dir",
-                              "default": "_output_dir",
+                              "group": "data",
                               "help": "Output directory for sorted aligned "
                                       "faces."})
 
-        argument_list.append({"opts": ('-fp', '--final-process'),
-                              "action": Radio,
-                              "type": str,
-                              "choices": ("folders", "rename"),
-                              "dest": 'final_process',
-                              "default": "rename",
-                              "help": "R|Default: rename."
-                                      "\nL|'folders': files are sorted using "
-                                      "the -s/--sort-by method, then they "
-                                      "are organized into folders using "
-                                      "the -g/--group-by grouping method."
-                                      "\nL|'rename': files are sorted using "
-                                      "the -s/--sort-by then they are "
-                                      "renamed."})
-
         argument_list.append({"opts": ('-k', '--keep'),
                               "action": 'store_true',
                               "dest": 'keep_original',
@@ -459,6 +471,7 @@ class SortArgs(FaceSwapArgs):
                               "choices": ("blur", "face", "face-cnn", "face-cnn-dissim",
                                           "face-yaw", "hist", "hist-dissim"),
                               "dest": 'sort_method',
+                              "group": "sort settings",
                               "default": "hist",
                               "help": "R|Sort by method. Choose how images are sorted. "
                                       "\nL|'blur': Sort faces by blurriness."
@@ -481,24 +494,13 @@ class SortArgs(FaceSwapArgs):
                                       "dissimilarity."
                                       "\nDefault: hist"})
 
-        argument_list.append({"opts": ('-g', '--group-by'),
-                              "action": Radio,
-                              "type": str,
-                              "choices": ("blur", "face-cnn", "face-yaw", "hist"),
-                              "dest": 'group_method',
-                              "default": "hist",
-                              "help": "Group by method. "
-                                      "When -fp/--final-processing by "
-                                      "folders choose the how the images are "
-                                      "grouped after sorting. "
-                                      "Default: hist"})
-
         argument_list.append({"opts": ('-t', '--ref_threshold'),
                               "action": Slider,
                               "min_max": (-1.0, 10.0),
                               "rounding": 2,
                               "type": float,
                               "dest": 'min_threshold',
+                              "group": "sort settings",
                               "default": -1.0,
                               "help": "Float value. "
                                       "Minimum threshold to use for grouping comparison with "
@@ -512,12 +514,42 @@ class SortArgs(FaceSwapArgs):
                                       "could result in a lot of directories being created. "
                                       "Defaults: face-cnn 7.2, hist 0.3"})
 
+        argument_list.append({"opts": ('-fp', '--final-process'),
+                              "action": Radio,
+                              "type": str,
+                              "choices": ("folders", "rename"),
+                              "dest": 'final_process',
+                              "default": "rename",
+                              "group": "output",
+                              "help": "R|Default: rename."
+                                      "\nL|'folders': files are sorted using "
+                                      "the -s/--sort-by method, then they "
+                                      "are organized into folders using "
+                                      "the -g/--group-by grouping method."
+                                      "\nL|'rename': files are sorted using "
+                                      "the -s/--sort-by then they are "
+                                      "renamed."})
+
+        argument_list.append({"opts": ('-g', '--group-by'),
+                              "action": Radio,
+                              "type": str,
+                              "choices": ("blur", "face-cnn", "face-yaw", "hist"),
+                              "dest": 'group_method',
+                              "group": "output",
+                              "default": "hist",
+                              "help": "Group by method. "
+                                      "When -fp/--final-processing by "
+                                      "folders choose the how the images are "
+                                      "grouped after sorting. "
+                                      "Default: hist"})
+
         argument_list.append({"opts": ('-b', '--bins'),
                               "action": Slider,
                               "min_max": (1, 100),
                               "rounding": 1,
                               "type": int,
                               "dest": 'num_bins',
+                              "group": "output",
                               "default": 5,
                               "help": "Integer value. "
                                       "Number of folders that will be used "
@@ -544,12 +576,13 @@ class SortArgs(FaceSwapArgs):
                               "type": str.upper,
                               "choices": ("CPU", "GPU"),
                               "default": "GPU",
+                              "group": "sort settings",
                               "help": "Backend to use for VGG Face inference."
                                       "Only used for sort by 'face'."})
 
         argument_list.append({"opts": ('-l', '--log-changes'),
                               "action": 'store_true',
-                              "dest": 'log_changes',
+                              "group": "output",
                               "default": False,
                               "help": "Logs file renaming changes if "
                                       "grouping by renaming, or it logs the "
@@ -562,6 +595,7 @@ class SortArgs(FaceSwapArgs):
         argument_list.append({"opts": ('-lf', '--log-file'),
                               "action": SaveFileFullPaths,
                               "filetypes": "alignments",
+                              "group": "output",
                               "dest": 'log_file_path',
                               "default": 'sort_log.json',
                               "help": "Specify a log file to use for saving "
