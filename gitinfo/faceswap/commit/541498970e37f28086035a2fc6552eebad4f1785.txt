commit 541498970e37f28086035a2fc6552eebad4f1785
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sun Jul 7 12:23:01 2019 +0000

    Training Fixups
    
    Only show model folder deprecation warning for existing models
    Gracefully exit FixedProducerDispatcher on error

diff --git a/lib/training_data.py b/lib/training_data.py
index 672f30e..25ebb56 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -33,6 +33,7 @@ class TrainingDataGenerator():
         self.training_opts = training_opts
         self.mask_class = self.set_mask_class()
         self.landmarks = self.training_opts.get("landmarks", None)
+        self.fixed_producer_dispatcher = None  # Set by FPD when loading
         self._nearest_landmarks = None
         self.processing = ImageManipulation(model_input_size,
                                             model_output_size,
@@ -68,15 +69,24 @@ class TrainingDataGenerator():
         if self.mask_class:
             batch_shape.append((self.batchsize, self.model_output_size, self.model_output_size, 1))
 
-        load_process = FixedProducerDispatcher(
+        self.fixed_producer_dispatcher = FixedProducerDispatcher(
             method=self.load_batches,
             shapes=batch_shape,
             in_queue=queue_in,
             out_queue=queue_out,
             args=(images, side, is_display, do_shuffle, batchsize))
-        load_process.start()
+        self.fixed_producer_dispatcher.start()
         logger.debug("Batching to queue: (side: '%s', is_display: %s)", side, is_display)
-        return self.minibatch(side, is_display, load_process)
+        return self.minibatch(side, is_display, self.fixed_producer_dispatcher)
+
+    def join_subprocess(self):
+        """ Join the FixedProduceerDispatcher subprocess from outside this module """
+        logger.debug("Joining FixedProducerDispatcher")
+        if self.fixed_producer_dispatcher is None:
+            logger.debug("FixedProducerDispatcher not yet initialized. Exiting")
+            return
+        self.fixed_producer_dispatcher.join()
+        logger.debug("Joined FixedProducerDispatcher")
 
     @staticmethod
     def make_queues(side, is_preview, is_timelapse):
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 3e6f608..2628e09 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -150,7 +150,7 @@ class ModelBase():
     def multiple_models_in_folder(self):
         """ Return true if there are multiple model types in the same folder, else false """
         model_files = [fname for fname in os.listdir(self.model_dir) if fname.endswith(".h5")]
-        retval = os.path.commonprefix(model_files) == ""
+        retval = False if not model_files else os.path.commonprefix(model_files) == ""
         logger.debug("model_files: %s, retval: %s", model_files, retval)
         return retval
 
@@ -486,7 +486,7 @@ class ModelBase():
 
         if not save_averages:
             logger.debug("No save averages. Not backing up")
-            return
+            return False
 
         for side, loss in save_averages.items():
             if not self.state.lowest_avg_loss.get(side, None):
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index 4bb827f..edb45d9 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -172,40 +172,46 @@ class TrainerBase():
                        self.model.iterations >= snapshot_interval and
                        self.model.iterations % snapshot_interval == 0)
         loss = dict()
-        for side, batcher in self.batchers.items():
-            if self.pingpong.active and side != self.pingpong.side:
-                continue
-            loss[side] = batcher.train_one_batch(do_preview)
-            if not do_preview and not do_timelapse:
-                continue
-            if do_preview:
-                self.samples.images[side] = batcher.compile_sample(None)
-            if do_timelapse:
-                self.timelapse.get_sample(side, timelapse_kwargs)
-
-        self.model.state.increment_iterations()
-
-        for side, side_loss in loss.items():
-            self.store_history(side, side_loss)
-            self.log_tensorboard(side, side_loss)
-
-        if not self.pingpong.active:
-            self.print_loss(loss)
-        else:
-            for key, val in loss.items():
-                self.pingpong.loss[key] = val
-            self.print_loss(self.pingpong.loss)
+        try:
+            for side, batcher in self.batchers.items():
+                if self.pingpong.active and side != self.pingpong.side:
+                    continue
+                loss[side] = batcher.train_one_batch(do_preview)
+                if not do_preview and not do_timelapse:
+                    continue
+                if do_preview:
+                    self.samples.images[side] = batcher.compile_sample(None)
+                if do_timelapse:
+                    self.timelapse.get_sample(side, timelapse_kwargs)
+
+            self.model.state.increment_iterations()
+
+            for side, side_loss in loss.items():
+                self.store_history(side, side_loss)
+                self.log_tensorboard(side, side_loss)
+
+            if not self.pingpong.active:
+                self.print_loss(loss)
+            else:
+                for key, val in loss.items():
+                    self.pingpong.loss[key] = val
+                self.print_loss(self.pingpong.loss)
 
-        if do_preview:
-            samples = self.samples.show_sample()
-            if samples is not None:
-                viewer(samples, "Training - 'S': Save Now. 'ENTER': Save and Quit")
+            if do_preview:
+                samples = self.samples.show_sample()
+                if samples is not None:
+                    viewer(samples, "Training - 'S': Save Now. 'ENTER': Save and Quit")
 
-        if do_timelapse:
-            self.timelapse.output_timelapse()
+            if do_timelapse:
+                self.timelapse.output_timelapse()
 
-        if do_snapshot:
-            self.model.do_snapshot()
+            if do_snapshot:
+                self.model.do_snapshot()
+        except Exception as err:
+            #  Shutdown the FixedProducerDispatchers then continue to raise error
+            for batcher in self.batchers.values():
+                batcher.shutdown_feed()
+            raise err
 
     def store_history(self, side, loss):
         """ Store the history of this step """
@@ -246,7 +252,10 @@ class Batcher():
         self.samples = None
         self.mask = None
 
-        self.feed = self.load_generator().minibatch_ab(images, batch_size, self.side)
+        generator = self.load_generator()
+        self.feed = generator.minibatch_ab(images, batch_size, self.side)
+        self.shutdown_feed = generator.join_subprocess
+
         self.preview_feed = None
         self.timelapse_feed = None
 
