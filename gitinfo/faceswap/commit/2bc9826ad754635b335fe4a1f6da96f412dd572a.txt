commit 2bc9826ad754635b335fe4a1f6da96f412dd572a
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Thu Mar 7 18:05:34 2019 +0000

    Add clipnorm back in as optimizer option for unbalanced

diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index d7464a8..f5be229 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -126,6 +126,10 @@ class Config(FaceswapConfig):
             section=section, title="dssim_loss", datatype=bool, default=False,
             info="Use DSSIM for Loss rather than Mean Absolute Error\n"
                  "May increase overall quality.")
+        self.add_item(
+            section=section, title="clipnorm", datatype=bool, default=True,
+            info="Controls gradient clipping of the optimizer. Can prevent model corruption at "
+                 "the expense of VRAM")
         self.add_item(
             section=section, title="mask_type", datatype=str, default="none",
             choices=MASK_TYPES, info=MASK_INFO)
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 51fdd59..63d5864 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -218,20 +218,7 @@ class ModelBase():
     def compile_predictors(self):
         """ Compile the predictors """
         logger.debug("Compiling Predictors")
-        # TODO Look to re-instate clipnorm
-        # Clipnorm is ballooning VRAM useage, which is not expected behaviour
-        # and may be a bug in Keras/TF?
-        # For now this is commented out, but revisit in future to reinstate
-
-        # # PlaidML has a bug regarding the clipnorm parameter
-        # # See: https://github.com/plaidml/plaidml/issues/228
-        # # Workaround by simply removing it.
-        # # TODO: Remove this as soon it is fixed in PlaidML.
-        # if keras.backend.backend() == "plaidml.keras.backend":
-        #    optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)
-        # else:
-        #    optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999, clipnorm=1.0)
-        optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)
+        optimizer = self.get_optimizer(lr=5e-5, beta_1=0.5, beta_2=0.999)
 
         for side, model in self.predictors.items():
             loss_names = ["loss"]
@@ -248,6 +235,21 @@ class ModelBase():
             self.history[side] = list()
         logger.debug("Compiled Predictors. Losses: %s", loss_names)
 
+    def get_optimizer(self, lr=5e-5, beta_1=0.5, beta_2=0.999):  # pylint: disable=invalid-name
+        """ Build and return Optimizer """
+        opt_kwargs = dict(lr=lr, beta_1=beta_1, beta_2=beta_2)
+        if (self.config.get("clipnorm", False) and
+                keras.backend.backend() != "plaidml.keras.backend"):
+            # NB: Clipnorm is ballooning VRAM useage, which is not expected behaviour
+            # and may be a bug in Keras/TF.
+            # PlaidML has a bug regarding the clipnorm parameter
+            # See: https://github.com/plaidml/plaidml/issues/228
+            # Workaround by simply removing it.
+            # TODO: Remove this as soon it is fixed in PlaidML.
+            opt_kwargs["clipnorm"] = 1.0
+        logger.debug("Optimizer kwargs: %s", opt_kwargs)
+        return Adam(**opt_kwargs)
+
     def loss_function(self, side):
         """ Set the loss function """
         if self.config.get("dssim_loss", False):
