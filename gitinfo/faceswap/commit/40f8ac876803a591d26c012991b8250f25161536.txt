commit 40f8ac876803a591d26c012991b8250f25161536
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Wed Aug 7 13:46:36 2019 +0000

    Fix penalized loss for multiscale decoders

diff --git a/lib/model/losses.py b/lib/model/losses.py
index 71ebc5d..2d0c2b2 100644
--- a/lib/model/losses.py
+++ b/lib/model/losses.py
@@ -15,6 +15,7 @@ import numpy as np
 import tensorflow as tf
 from tensorflow.distributions import Beta
 
+from .masks import gaussian_blur
 from .normalization import InstanceNormalization
 if K.backend() == "plaidml.keras.backend":
     from plaidml.op import extract_image_patches
@@ -162,8 +163,23 @@ class DSSIMObjective():
 
 
 # <<< START: from Dfaker >>> #
-def PenalizedLoss(mask, loss_func, mask_prop=1.0):  # pylint: disable=invalid-name
+def PenalizedLoss(mask, loss_func, mask_prop=1.0, mask_scaling=1.0):  # pylint: disable=invalid-name
     """ Plaidml + tf Penalized loss function """
+
+    def scale_mask(mask, scaling):
+        """ Scale the input mask to be the same size as the input face """
+        if scaling != 1.0:
+            size = round(1 / scaling)
+            mask = K.pool2d(mask,
+                            pool_size=(size, size),
+                            strides=(size, size),
+                            padding="valid",
+                            data_format=K.image_data_format(),
+                            pool_mode="avg")
+        logger.debug("resized tensor: %s", mask)
+        return mask
+
+    mask = scale_mask(mask, mask_scaling)
     mask_as_k_inv_prop = 1 - mask_prop
     mask = (mask * mask_prop) + mask_as_k_inv_prop
 
@@ -185,27 +201,6 @@ def PenalizedLoss(mask, loss_func, mask_prop=1.0):  # pylint: disable=invalid-na
 def style_loss(gaussian_blur_radius=0.0, loss_weight=1.0, wnd_size=0, step_size=1):
     """ Style Loss from DeepFaceLab
         https://github.com/iperov/DeepFaceLab """
-    def gaussian_blur(radius=2.0):
-        def gaussian(var_x, radius, sigma):
-            return np.exp(-(float(var_x) - float(radius)) ** 2 / (2 * sigma ** 2))
-
-        def make_kernel(sigma):
-            kernel_size = max(3, int(2 * 2 * sigma + 1))
-            mean = np.floor(0.5 * kernel_size)
-            kernel_1d = np.array([gaussian(x, mean, sigma) for x in range(kernel_size)])
-            np_kernel = np.outer(kernel_1d, kernel_1d).astype(dtype=K.floatx())
-            kernel = np_kernel / np.sum(np_kernel)
-            return kernel
-
-        gauss_kernel = make_kernel(radius)
-        gauss_kernel = gauss_kernel[:, :, np.newaxis, np.newaxis]
-
-        def func(input_):
-            inputs = [input_[:, :, :, i:i + 1] for i in range(K.int_shape(input_)[-1])]
-            outputs = [K.conv2d(inp, K.constant(gauss_kernel), strides=(1, 1), padding="same")
-                       for inp in inputs]
-            return K.concatenate(outputs, axis=-1)
-        return func
 
     if gaussian_blur_radius > 0.0:
         gblur = gaussian_blur(gaussian_blur_radius)
diff --git a/lib/model/masks.py b/lib/model/masks.py
index cb41bf7..3745548 100644
--- a/lib/model/masks.py
+++ b/lib/model/masks.py
@@ -5,6 +5,7 @@ import inspect
 import logging
 import sys
 
+import keras.backend as K
 import cv2
 import numpy as np
 
@@ -29,6 +30,31 @@ def get_default_mask():
     return default
 
 
+def gaussian_blur(radius=2.0):
+    """ From https://github.com/iperov/DeepFaceLab
+        Used for blurring mask in training """
+    def gaussian(var_x, radius, sigma):
+        return np.exp(-(float(var_x) - float(radius)) ** 2 / (2 * sigma ** 2))
+
+    def make_kernel(sigma):
+        kernel_size = max(3, int(2 * 2 * sigma + 1))
+        mean = np.floor(0.5 * kernel_size)
+        kernel_1d = np.array([gaussian(x, mean, sigma) for x in range(kernel_size)])
+        np_kernel = np.outer(kernel_1d, kernel_1d).astype(dtype=K.floatx())
+        kernel = np_kernel / np.sum(np_kernel)
+        return kernel
+
+    gauss_kernel = make_kernel(radius)
+    gauss_kernel = gauss_kernel[:, :, np.newaxis, np.newaxis]
+
+    def func(input_):
+        inputs = [input_[:, :, :, i:i + 1] for i in range(K.int_shape(input_)[-1])]
+        outputs = [K.conv2d(inp, K.constant(gauss_kernel), strides=(1, 1), padding="same")
+                   for inp in inputs]
+        return K.concatenate(outputs, axis=-1)
+    return func
+
+
 class Mask():
     """ Parent class for masks
 
diff --git a/lib/training_data.py b/lib/training_data.py
index 947d4c4..6f4c65a 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -64,8 +64,8 @@ class TrainingDataGenerator():
         training_size = self.training_opts.get("training_size", 256)
         batch_shape = list((
             (batchsize, training_size, training_size, 3),  # sample images
-            (batchsize, self.model_input_size, self.model_input_size, 3)))
-        # Add the output shapes
+            (batchsize, self.model_input_size, self.model_input_size, 3)))  # Training Image
+        # Target images
         batch_shape.extend(tuple([(batchsize, ) + shape for shape in self.model_output_shapes]))
         logger.debug("Batch shapes: %s", batch_shape)
 
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index e2fa307..80b706e 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -170,7 +170,6 @@ class ModelBase():
         for predictor in self.predictors.values():
             out.extend([K.int_shape(output)[-3:] for output in predictor.outputs])
             break  # Only get output from one autoencoder. Shapes are the same
-        # Only return the output shape of the face
         return [tuple(shape) for shape in out]
 
     @property
@@ -265,6 +264,7 @@ class ModelBase():
         mask_idx = [idx for idx, name in enumerate(output_network.output_names)
                     if name.startswith("mask")]
         if mask_idx:
+            # Add the final mask shape as input
             mask_shape = output_network.output_shapes[mask_idx[0]]
             inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
         logger.debug("Got inputs: %s", inputs)
@@ -363,8 +363,7 @@ class ModelBase():
         optimizer = self.get_optimizer(lr=learning_rate, beta_1=0.5, beta_2=0.999)
 
         for side, model in self.predictors.items():
-            mask_input = [inp for inp in model.inputs if inp.name.startswith("mask")]
-            loss = Loss(model.outputs, mask_input)
+            loss = Loss(model.inputs, model.outputs)
             model.compile(optimizer=optimizer, loss=loss.funcs)
             if initialize:
                 self.state.add_session_loss_names(side, loss.names)
@@ -648,33 +647,68 @@ class VRAMSavings():
 
 class Loss():
     """ Holds loss names and functions for an Autoencoder """
-    def __init__(self, outputs, mask_input):
-        logger.debug("Initializing %s: (outputs: '%s', mask_input: '%s')",
-                     self.__class__.__name__, outputs, mask_input)
+    def __init__(self, inputs, outputs):
+        logger.debug("Initializing %s: (inputs: %s, outputs: %s)",
+                     self.__class__.__name__, inputs, outputs)
+        self.inputs = inputs
         self.outputs = outputs
         self.names = self.get_loss_names()
-        self.funcs = self.get_loss_functions(mask_input)
+        self.funcs = self.get_loss_functions()
         if len(self.names) > 1:
             self.names.insert(0, "total_loss")
         logger.debug("Initialized: %s", self.__class__.__name__)
 
+    @property
+    def loss_dict(self):
+        """ Return the loss dict """
+        loss_dict = dict(mae=losses.mean_absolute_error,
+                         mse=losses.mean_squared_error,
+                         logcosh=losses.logcosh,
+                         smooth_l=generalized_loss,
+                         l_inf_norm=l_inf_norm,
+                         ssim=DSSIMObjective(),
+                         gmsd=gmsd_loss,
+                         pixel_gradient_diff=gradient_loss)
+        return loss_dict
+
     @property
     def config(self):
         """ Return the global _CONFIG variable """
         return _CONFIG
 
     @property
-    def loss_shapes(self):
+    def selected_loss(self):
+        """ Return the selected loss function """
+        retval = self.loss_dict[self.config.get("loss_function", "mae")]
+        logger.debug(retval)
+        return retval
+
+    @property
+    def selected_mask_loss(self):
+        """ Return the selected mask loss function. Currently returns mse """
+        retval = self.loss_dict["mse"]
+        logger.debug(retval)
+        return retval
+
+    @property
+    def output_shapes(self):
         """ The shapes of the output nodes """
         return [K.int_shape(output)[1:] for output in self.outputs]
 
     @property
-    def largest_output(self):
-        """ Return the index of the largest face output """
-        max_size = max(shape[0] for shape in self.loss_shapes if shape[2] == 3)
-        largest_idx = [idx for idx, shape in enumerate(self.loss_shapes)
-                       if shape[0] == max_size and shape[2] == 3][0]
-        return largest_idx
+    def mask_input(self):
+        """ Return the mask input or None """
+        mask_inputs = [inp for inp in self.inputs if inp.name.startswith("mask")]
+        if not mask_inputs:
+            return None
+        return mask_inputs[0]
+
+    @property
+    def mask_shape(self):
+        """ Return the mask shape """
+        if self.mask_input is None:
+            return None
+        return K.int_shape(self.mask_input)[1:]
 
     def get_loss_names(self):
         """ Return the loss names based on model output """
@@ -692,38 +726,30 @@ class Loss():
 
     def update_loss_names(self):
         """ Update loss names if named incorrectly or legacy model """
-        output_types = ["mask" if shape[-1] == 1 else "face" for shape in self.loss_shapes]
+        output_types = ["mask" if shape[-1] == 1 else "face" for shape in self.output_shapes]
         loss_names = ["{}{}".format(name,
                                     "" if output_types.count(name) == 1 else "_{}".format(idx))
                       for idx, name in enumerate(output_types)]
         logger.debug("Renamed loss names to: %s", loss_names)
         return loss_names
 
-    def get_loss_functions(self, mask):
+    def get_loss_functions(self):
         """ Set the loss function """
         loss_funcs = []
-        largest_face = self.largest_output
-        loss_dict = {'mae':                     losses.mean_absolute_error,
-                     'mse':                     losses.mean_squared_error,
-                     'logcosh':                 losses.logcosh,
-                     'smooth_l1':               generalized_loss,
-                     'l_inf_norm':              l_inf_norm,
-                     'ssim':                    DSSIMObjective(),
-                     'gmsd':                    gmsd_loss,
-                     'pixel_gradient_diff':     gradient_loss}
-        img_loss_config = self.config.get("loss_function", "mae")
-        mask_loss_config = "mse"
-
         for idx, loss_name in enumerate(self.names):
             if loss_name.startswith("mask"):
-                loss_funcs.append(loss_dict[mask_loss_config])
-                logger.debug("mask loss: %s", mask_loss_config)
-            elif mask and idx == largest_face and self.config.get("penalized_mask_loss", False):
-                loss_funcs.append(PenalizedLoss(mask[0], loss_dict[img_loss_config]))
-                logger.debug("final face loss: %s", img_loss_config)
+                loss_funcs.append(self.selected_mask_loss)
+            elif self.mask_input is not None and self.config.get("penalized_mask_loss", False):
+                face_size = self.output_shapes[idx][1]
+                mask_size = self.mask_shape[1]
+                scaling = face_size / mask_size
+                logger.debug("face_size: %s mask_size: %s, mask_scaling: %s",
+                             face_size, mask_size, scaling)
+                loss_funcs.append(PenalizedLoss(self.mask_input, self.selected_loss,
+                                                mask_scaling=scaling))
             else:
-                loss_funcs.append(loss_dict[img_loss_config])
-                logger.debug("face loss func: %s", img_loss_config)
+                loss_funcs.append(self.selected_loss)
+            logger.debug("%s: %s", loss_name, loss_funcs[-1])
         logger.debug(loss_funcs)
         return loss_funcs
 
