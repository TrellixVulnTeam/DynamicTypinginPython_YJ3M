commit 22f22d9b25b2e70fa17617a33063710ffbb45191
Author: 武见 <joshuawu.wujian@alibaba-inc.com>
Date:   Fri Dec 15 19:47:32 2017 +0800

    first commit

diff --git a/image_augmentation.py b/image_augmentation.py
new file mode 100755
index 0000000..6fd6dc1
--- /dev/null
+++ b/image_augmentation.py
@@ -0,0 +1,41 @@
+import cv2
+import numpy
+
+from umeyama import umeyama
+
+def random_transform( image, rotation_range, zoom_range, shift_range, random_flip ):
+    h,w = image.shape[0:2]
+    rotation = numpy.random.uniform( -rotation_range, rotation_range )
+    scale = numpy.random.uniform( 1 - zoom_range, 1 + zoom_range )
+    tx = numpy.random.uniform( -shift_range, shift_range ) * w
+    ty = numpy.random.uniform( -shift_range, shift_range ) * h
+    mat = cv2.getRotationMatrix2D( (w//2,h//2), rotation, scale )
+    mat[:,2] += (tx,ty)
+    result = cv2.warpAffine( image, mat, (w,h), borderMode=cv2.BORDER_REPLICATE )
+    if numpy.random.random() < random_flip:
+        result = result[:,::-1]
+    return result
+
+# get pair of random warped images from aligened face image
+def random_warp( image ):
+    assert image.shape == (256,256,3)
+    range_ = numpy.linspace( 128-80, 128+80, 5 )
+    mapx = numpy.broadcast_to( range_, (5,5) )
+    mapy = mapx.T
+
+    mapx = mapx + numpy.random.normal( size=(5,5), scale=5 )
+    mapy = mapy + numpy.random.normal( size=(5,5), scale=5 )
+
+    interp_mapx = cv2.resize( mapx, (80,80) )[8:72,8:72].astype('float32')
+    interp_mapy = cv2.resize( mapy, (80,80) )[8:72,8:72].astype('float32')
+
+    warped_image = cv2.remap( image, interp_mapx, interp_mapy, cv2.INTER_LINEAR )
+
+    src_points = numpy.stack( [ mapx.ravel(), mapy.ravel() ], axis=-1 )
+    dst_points = numpy.mgrid[0:65:16,0:65:16].T.reshape(-1,2)
+    mat = umeyama( src_points, dst_points, True )[0:2]
+
+    target_image = cv2.warpAffine( image, mat, (64,64) )
+
+    return warped_image, target_image
+
diff --git a/model.py b/model.py
new file mode 100755
index 0000000..8c3b003
--- /dev/null
+++ b/model.py
@@ -0,0 +1,61 @@
+from keras.models import Model
+from keras.layers import Input, Dense, Flatten, Reshape
+from keras.layers.advanced_activations import LeakyReLU
+from keras.layers.convolutional import Conv2D
+from keras.optimizers import Adam
+
+from pixel_shuffler import PixelShuffler
+
+optimizer = Adam( lr=5e-5, beta_1=0.5, beta_2=0.999 )
+
+IMAGE_SHAPE = (64,64,3)
+ENCODER_DIM = 1024
+
+def conv( filters ):
+    def block(x):
+        x = Conv2D( filters, kernel_size=5, strides=2, padding='same' )(x)
+        x = LeakyReLU(0.1)(x)
+        return x
+    return block
+
+def upscale( filters ):
+    def block(x):
+        x = Conv2D( filters*4, kernel_size=3, padding='same' )(x)
+        x = LeakyReLU(0.1)(x)
+        x = PixelShuffler()(x)
+        return x
+    return block
+
+def Encoder():
+    input_ = Input( shape=IMAGE_SHAPE )
+    x = input_
+    x = conv( 128)(x)
+    x = conv( 256)(x)
+    x = conv( 512)(x)
+    x = conv(1024)(x)
+    x = Dense( ENCODER_DIM )( Flatten()(x) )
+    x = Dense(4*4*1024)(x)
+    x = Reshape((4,4,1024))(x)
+    x = upscale(512)(x)
+    return Model( input_, x )
+
+def Decoder():
+    input_ = Input( shape=(8,8,512) )
+    x = input_
+    x = upscale(256)(x)
+    x = upscale(128)(x)
+    x = upscale( 64)(x)
+    x = Conv2D( 3, kernel_size=5, padding='same', activation='sigmoid' )(x)
+    return Model( input_, x )
+
+encoder = Encoder()
+decoder_A = Decoder()
+decoder_B = Decoder()
+
+x = Input( shape=IMAGE_SHAPE )
+
+autoencoder_A = Model( x, decoder_A( encoder(x) ) )
+autoencoder_B = Model( x, decoder_B( encoder(x) ) )
+autoencoder_A.compile( optimizer=optimizer, loss='mean_absolute_error' )
+autoencoder_B.compile( optimizer=optimizer, loss='mean_absolute_error' )
+
diff --git a/pixel_shuffler.py b/pixel_shuffler.py
new file mode 100755
index 0000000..2f4fc68
--- /dev/null
+++ b/pixel_shuffler.py
@@ -0,0 +1,87 @@
+# PixelShuffler layer for Keras
+# by t-ae
+# https://gist.github.com/t-ae/6e1016cc188104d123676ccef3264981
+
+from keras.utils import conv_utils
+from keras.engine.topology import Layer
+import keras.backend as K
+
+class PixelShuffler(Layer):
+    def __init__(self, size=(2, 2), data_format=None, **kwargs):
+        super(PixelShuffler, self).__init__(**kwargs)
+        self.data_format = conv_utils.normalize_data_format(data_format)
+        self.size = conv_utils.normalize_tuple(size, 2, 'size')
+
+    def call(self, inputs):
+
+        input_shape = K.int_shape(inputs)
+        if len(input_shape) != 4:
+            raise ValueError('Inputs should have rank ' +
+                             str(4) +
+                             '; Received input shape:', str(input_shape))
+
+        if self.data_format == 'channels_first':
+            batch_size, c, h, w = input_shape
+            if batch_size is None:
+                batch_size = -1
+            rh, rw = self.size
+            oh, ow = h * rh, w * rw
+            oc = c // (rh * rw)
+
+            out = K.reshape(inputs, (batch_size, rh, rw, oc, h, w))
+            out = K.permute_dimensions(out, (0, 3, 4, 1, 5, 2))
+            out = K.reshape(out, (batch_size, oc, oh, ow))
+            return out
+
+        elif self.data_format == 'channels_last':
+            batch_size, h, w, c = input_shape
+            if batch_size is None:
+                batch_size = -1
+            rh, rw = self.size
+            oh, ow = h * rh, w * rw
+            oc = c // (rh * rw)
+
+            out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))
+            out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))
+            out = K.reshape(out, (batch_size, oh, ow, oc))
+            return out
+
+    def compute_output_shape(self, input_shape):
+
+        if len(input_shape) != 4:
+            raise ValueError('Inputs should have rank ' +
+                             str(4) +
+                             '; Received input shape:', str(input_shape))
+
+        if self.data_format == 'channels_first':
+            height = input_shape[2] * self.size[0] if input_shape[2] is not None else None
+            width = input_shape[3] * self.size[1] if input_shape[3] is not None else None
+            channels = input_shape[1] // self.size[0] // self.size[1]
+
+            if channels * self.size[0] * self.size[1] != input_shape[1]:
+                raise ValueError('channels of input and size are incompatible')
+
+            return (input_shape[0],
+                    channels,
+                    height,
+                    width)
+
+        elif self.data_format == 'channels_last':
+            height = input_shape[1] * self.size[0] if input_shape[1] is not None else None
+            width = input_shape[2] * self.size[1] if input_shape[2] is not None else None
+            channels = input_shape[3] // self.size[0] // self.size[1]
+
+            if channels * self.size[0] * self.size[1] != input_shape[3]:
+                raise ValueError('channels of input and size are incompatible')
+
+            return (input_shape[0],
+                    height,
+                    width,
+                    channels)
+
+    def get_config(self):
+        config = {'size': self.size,
+                  'data_format': self.data_format}
+        base_config = super(PixelShuffler, self).get_config()
+
+        return dict(list(base_config.items()) + list(config.items()))
diff --git a/script.py b/script.py
new file mode 100644
index 0000000..6abd79e
--- /dev/null
+++ b/script.py
@@ -0,0 +1,38 @@
+import cv2
+import numpy
+from pathlib import Path
+
+from utils import get_image_paths
+
+from model import autoencoder_A
+from model import autoencoder_B
+from model import encoder, decoder_A, decoder_B
+
+encoder  .load_weights( "models/encoder.h5"   )
+decoder_A.load_weights( "models/decoder_A.h5" )
+decoder_B.load_weights( "models/decoder_B.h5" )
+
+images_A = get_image_paths( "data/trump" )
+images_B = get_image_paths( "data/cage" )
+
+def convert_one_image( autoencoder, image ):
+    assert image.shape == (256,256,3)
+    crop = slice(48,208)
+    face = image[crop,crop]
+    face = cv2.resize( face, (64,64) )
+    face = numpy.expand_dims( face, 0 )
+    new_face = autoencoder.predict( face / 255.0 )[0]
+    new_face = numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )
+    new_face = cv2.resize( new_face, (160,160) )
+    new_image = image.copy()
+    new_image[crop,crop] = new_face
+    return new_image
+
+output_dir = Path( 'output' )
+output_dir.mkdir( parents=True, exist_ok=True )
+
+for fn in images_A:
+    image = cv2.imread(fn)
+    new_image = convert_one_image( autoencoder_B, image )
+    output_file = output_dir / Path(fn).name
+    cv2.imwrite( str(output_file), new_image )
\ No newline at end of file
diff --git a/train.py b/train.py
new file mode 100755
index 0000000..eecdf7d
--- /dev/null
+++ b/train.py
@@ -0,0 +1,69 @@
+import cv2
+import numpy
+
+from utils import get_image_paths, load_images, stack_images
+from training_data import get_training_data
+
+from model import autoencoder_A
+from model import autoencoder_B
+from model import encoder, decoder_A, decoder_B
+
+try:
+    encoder  .load_weights( "models/encoder.h5"   )
+    decoder_A.load_weights( "models/decoder_A.h5" )
+    decoder_B.load_weights( "models/decoder_B.h5" )
+except:
+    pass
+
+def save_model_weights():
+    encoder  .save_weights( "models/encoder.h5"   )
+    decoder_A.save_weights( "models/decoder_A.h5" )
+    decoder_B.save_weights( "models/decoder_B.h5" )
+    print( "save model weights" )
+
+images_A = get_image_paths( "data/trump" )
+images_B = get_image_paths( "data/cage"  )
+images_A = load_images( images_A ) / 255.0
+images_B = load_images( images_B ) / 255.0
+
+images_A += images_B.mean( axis=(0,1,2) ) - images_A.mean( axis=(0,1,2) )
+
+print( "press 'q' to stop training and save model" )
+
+for epoch in range(1000000):
+    batch_size = 64
+    warped_A, target_A = get_training_data( images_A, batch_size )
+    warped_B, target_B = get_training_data( images_B, batch_size )
+
+    loss_A = autoencoder_A.train_on_batch( warped_A, target_A )
+    loss_B = autoencoder_B.train_on_batch( warped_B, target_B )
+    print( loss_A, loss_B )
+
+    if epoch % 100 == 0:
+        save_model_weights()
+        test_A = target_A[0:14]
+        test_B = target_B[0:14]
+
+    figure_A = numpy.stack([
+        test_A,
+        autoencoder_A.predict( test_A ),
+        autoencoder_B.predict( test_A ),
+        ], axis=1 )
+    figure_B = numpy.stack([
+        test_B,
+        autoencoder_B.predict( test_B ),
+        autoencoder_A.predict( test_B ),
+        ], axis=1 )
+
+    figure = numpy.concatenate( [ figure_A, figure_B ], axis=0 )
+    figure = figure.reshape( (4,7) + figure.shape[1:] )
+    figure = stack_images( figure )
+
+    figure = numpy.clip( figure * 255, 0, 255 ).astype('uint8')
+
+    cv2.imshow( "", figure )
+    key = cv2.waitKey(1)
+    if key == ord('q'):
+        save_model_weights()
+        exit()
+
diff --git a/training_data.py b/training_data.py
new file mode 100755
index 0000000..c5c56c5
--- /dev/null
+++ b/training_data.py
@@ -0,0 +1,26 @@
+import numpy
+from image_augmentation import random_transform
+from image_augmentation import random_warp
+
+random_transform_args = {
+    'rotation_range': 10,
+    'zoom_range': 0.05,
+    'shift_range': 0.05,
+    'random_flip': 0.4,
+    }
+
+def get_training_data( images, batch_size ):
+    indices = numpy.random.randint( len(images), size=batch_size )
+    for i,index in enumerate(indices):
+        image = images[index]
+        image = random_transform( image, **random_transform_args )
+        warped_img, target_img = random_warp( image )
+
+        if i == 0:
+            warped_images = numpy.empty( (batch_size,) + warped_img.shape, warped_img.dtype )
+            target_images = numpy.empty( (batch_size,) + target_img.shape, warped_img.dtype )
+
+        warped_images[i] = warped_img
+        target_images[i] = target_img
+
+    return warped_images, target_images
diff --git a/umeyama.py b/umeyama.py
new file mode 100755
index 0000000..b5ac96c
--- /dev/null
+++ b/umeyama.py
@@ -0,0 +1,84 @@
+## License (Modified BSD)
+## Copyright (C) 2011, the scikit-image team All rights reserved.
+##
+## Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
+##
+## Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
+## Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
+## Neither the name of skimage nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.
+## THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+
+# umeyama function from scikit-image/skimage/transform/_geometric.py
+
+import numpy as np
+
+def umeyama( src, dst, estimate_scale ):
+    """Estimate N-D similarity transformation with or without scaling.
+    Parameters
+    ----------
+    src : (M, N) array
+        Source coordinates.
+    dst : (M, N) array
+        Destination coordinates.
+    estimate_scale : bool
+        Whether to estimate scaling factor.
+    Returns
+    -------
+    T : (N + 1, N + 1)
+        The homogeneous similarity transformation matrix. The matrix contains
+        NaN values only if the problem is not well-conditioned.
+    References
+    ----------
+    .. [1] "Least-squares estimation of transformation parameters between two
+            point patterns", Shinji Umeyama, PAMI 1991, DOI: 10.1109/34.88573
+    """
+
+    num = src.shape[0]
+    dim = src.shape[1]
+
+    # Compute mean of src and dst.
+    src_mean = src.mean(axis=0)
+    dst_mean = dst.mean(axis=0)
+
+    # Subtract mean from src and dst.
+    src_demean = src - src_mean
+    dst_demean = dst - dst_mean
+
+    # Eq. (38).
+    A = np.dot(dst_demean.T, src_demean) / num
+
+    # Eq. (39).
+    d = np.ones((dim,), dtype=np.double)
+    if np.linalg.det(A) < 0:
+        d[dim - 1] = -1
+
+    T = np.eye(dim + 1, dtype=np.double)
+
+    U, S, V = np.linalg.svd(A)
+
+    # Eq. (40) and (43).
+    rank = np.linalg.matrix_rank(A)
+    if rank == 0:
+        return np.nan * T
+    elif rank == dim - 1:
+        if np.linalg.det(U) * np.linalg.det(V) > 0:
+            T[:dim, :dim] = np.dot(U, V)
+        else:
+            s = d[dim - 1]
+            d[dim - 1] = -1
+            T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V))
+            d[dim - 1] = s
+    else:
+        T[:dim, :dim] = np.dot(U, np.dot(np.diag(d), V.T))
+
+    if estimate_scale:
+        # Eq. (41) and (42).
+        scale = 1.0 / src_demean.var(axis=0).sum() * np.dot(S, d)
+    else:
+        scale = 1.0
+
+    T[:dim, dim] = dst_mean - scale * np.dot(T[:dim, :dim], src_mean.T)
+    T[:dim, :dim] *= scale
+
+    return T
+
diff --git a/utils.py b/utils.py
new file mode 100755
index 0000000..3282e3b
--- /dev/null
+++ b/utils.py
@@ -0,0 +1,34 @@
+import cv2
+import numpy
+import os
+
+def get_image_paths( directory ):
+    return [ x.path for x in os.scandir( directory ) if x.name.endswith(".jpg") or x.name.endswith(".png") ]
+
+def load_images( image_paths, convert=None ):
+    iter_all_images = ( cv2.imread(fn) for fn in image_paths )
+    if convert:
+        iter_all_images = ( convert(img) for img in iter_all_images )
+    for i,image in enumerate( iter_all_images ):
+        if i == 0:
+            all_images = numpy.empty( ( len(image_paths), ) + image.shape, dtype=image.dtype )
+        all_images[i] = image
+    return all_images
+
+def get_transpose_axes( n ):
+    if n % 2 == 0:
+        y_axes = list( range( 1, n-1, 2 ) )
+        x_axes = list( range( 0, n-1, 2 ) )
+    else:
+        y_axes = list( range( 0, n-1, 2 ) )
+        x_axes = list( range( 1, n-1, 2 ) )
+    return y_axes, x_axes, [n-1]
+
+def stack_images( images ):
+    images_shape = numpy.array( images.shape )
+    new_axes = get_transpose_axes( len( images_shape ) )
+    new_shape = [ numpy.prod( images_shape[x] ) for x in new_axes ]
+    return numpy.transpose(
+        images,
+        axes = numpy.concatenate( new_axes )
+        ).reshape( new_shape )
