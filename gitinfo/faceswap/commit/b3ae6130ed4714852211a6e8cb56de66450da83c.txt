commit b3ae6130ed4714852211a6e8cb56de66450da83c
Author: Clorr <Github@clorr.fr>
Date:   Wed Feb 7 13:42:19 2018 +0100

    Misc updates on master before GAN. Added multithreading + mmod face detector (#109)
    
    * Preparing GAN plugin
    
    * Adding multithreading for extract
    
    * Adding support for mmod human face detector
    
    * Adding face filter argument
    
    * Added process number argument to multiprocessing extractor.
    
    Fixed progressbar for multiprocessing.
    
    * Added tiff as image type.
    compression artefacts hurt my feelings.
    
    * Cleanup

diff --git a/lib/FaceFilter.py b/lib/FaceFilter.py
index 113678a..b8e3089 100644
--- a/lib/FaceFilter.py
+++ b/lib/FaceFilter.py
@@ -6,15 +6,18 @@ import face_recognition
 class FaceFilter():
     def __init__(self, reference_file_path, threshold = 0.6):
         image = face_recognition.load_image_file(reference_file_path)
-        self.encoding = face_recognition.face_encodings(image)[0] # Note: we take only first face, so the reference file should only contain one face. We could also keep all faces found and filter against multiple faces
+        self.encoding = face_recognition.face_encodings(image)[0] # Note: we take only first face, so the reference file should only contain one face.
         self.threshold = threshold
     
     def check(self, detected_face):
-        encodings = face_recognition.face_encodings(detected_face.image)[0] # we could use detected landmarks, but I did not manage to do so
-        score = face_recognition.face_distance([self.encoding], encodings)
-        print(score)
-        return score <= self.threshold
-
+        encodings = face_recognition.face_encodings(detected_face.image) # we could use detected landmarks, but I did not manage to do so. TODO The copy/paste below should help
+        if encodings is not None and len(encodings) > 0:
+            score = face_recognition.face_distance([self.encoding], encodings[0])
+            print(score)
+            return score <= self.threshold
+        else:
+            print("No face encodings found")
+            return False
 
 # # Copy/Paste (mostly) from private method in face_recognition
 # face_recognition_model = face_recognition_models.face_recognition_model_location()
diff --git a/lib/ModelAE.py b/lib/ModelAE.py
index 2dd6f90..d9bff06 100644
--- a/lib/ModelAE.py
+++ b/lib/ModelAE.py
@@ -2,7 +2,7 @@
 
 import time
 import numpy
-from lib.training_data import minibatchAB, stack_images
+from lib.training_data import TrainingDataGenerator, stack_images
 
 encoderH5 = '/encoder.h5'
 decoder_AH5 = '/decoder_A.h5'
@@ -40,11 +40,20 @@ class ModelAE:
         print('saved model weights')
 
 class TrainerAE():
+    random_transform_args = {
+        'rotation_range': 10,
+        'zoom_range': 0.05,
+        'shift_range': 0.05,
+        'random_flip': 0.4,
+    }
+
     def __init__(self, model, fn_A, fn_B, batch_size=64):
         self.batch_size = batch_size
         self.model = model
-        self.images_A = minibatchAB(fn_A, self.batch_size)
-        self.images_B = minibatchAB(fn_B, self.batch_size)
+
+        generator = TrainingDataGenerator(self.random_transform_args, 160)
+        self.images_A = generator.minibatchAB(fn_A, self.batch_size)
+        self.images_B = generator.minibatchAB(fn_B, self.batch_size)
 
     def train_one_step(self, iter, viewer):
         epoch, warped_A, target_A = next(self.images_A)
diff --git a/lib/cli.py b/lib/cli.py
index 2166322..59f1f82 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -1,7 +1,6 @@
 import argparse
 import os
 import time
-from tqdm import tqdm
 
 from pathlib import Path
 from lib.FaceFilter import FaceFilter
@@ -28,7 +27,6 @@ class DirectoryProcessor(object):
 
     verify_output = False
     images_found = 0
-    images_processed = 0
     faces_detected = 0
 
     def __init__(self, subparser, command, description='default'):
@@ -48,22 +46,17 @@ class DirectoryProcessor(object):
             print('Input directory not found. Please ensure it exists.')
             exit(1)
 
-        self.images_found = len(self.input_dir)
         self.filter = self.load_filter()
         self.process()
         self.finalize()
         
     def read_directory(self):
-        for filename in tqdm(self.input_dir):
-            if self.arguments.verbose:
-                print('Processing: {}'.format(os.path.basename(filename)))
-
-            yield filename
-            self.images_processed = self.images_processed + 1
+        self.images_found = len(self.input_dir)
+        return self.input_dir
     
     def get_faces(self, image):
         faces_count = 0
-        for face in detect_faces(image):
+        for face in detect_faces(image, self.arguments.detector):
             if self.filter is not None and not self.filter.check(face):
                 print('Skipping not recognized face!')
                 continue
@@ -78,7 +71,7 @@ class DirectoryProcessor(object):
             self.verify_output = True
     
     def load_filter(self):
-        filter_file = "filter.jpg" # TODO Pass as argument
+        filter_file = self.arguments.filter
         if Path(filter_file).exists():
             print('Loading reference image for filtering')
             return FaceFilter(filter_file)
@@ -125,7 +118,6 @@ class DirectoryProcessor(object):
     def finalize(self):
         print('-------------------------')
         print('Images found:        {}'.format(self.images_found))
-        print('Images processed:    {}'.format(self.images_processed))
         print('Faces detected:      {}'.format(self.faces_detected))
         print('-------------------------')
 
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 0d650bd..0960ed0 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -2,8 +2,8 @@ import dlib
 import face_recognition
 import face_recognition_models
 
-def detect_faces(frame):
-    face_locations = face_recognition.face_locations(frame)
+def detect_faces(frame, model="hog"):
+    face_locations = face_recognition.face_locations(frame, model=model)
     landmarks = _raw_face_landmarks(frame, face_locations)
 
     for ((y, right, bottom, x), landmarks) in zip(face_locations, landmarks):
diff --git a/lib/multithreading.py b/lib/multithreading.py
new file mode 100644
index 0000000..ed9d57f
--- /dev/null
+++ b/lib/multithreading.py
@@ -0,0 +1,16 @@
+import multiprocessing as mp
+
+method = None
+
+def pool_process(method_to_run, data, processes=None):
+    global method
+    if processes is None:
+        processes = mp.cpu_count()
+    method = method_to_run
+    pool = mp.Pool(processes=processes)
+
+    for i in pool.imap_unordered(runner, data):
+        yield i if i is not None else 0
+    
+def runner(item):
+    return method(item)
diff --git a/lib/training_data.py b/lib/training_data.py
index f836e47..e194398 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -5,97 +5,90 @@ from random import shuffle
 from .utils import BackgroundGenerator
 from .umeyama import umeyama
 
-coverage = 220 # Coverage of the face for training. Larger value will cover more features. @shaoanlu recommends 220. Original is 160
-
-random_transform_args = {
-    'rotation_range': 10,
-    'zoom_range': 0.05,
-    'shift_range': 0.05,
-    'random_flip': 0.4,
-}
-
-# GAN
-# random_transform_args = {
-#     'rotation_range': 20,
-#     'zoom_range': 0.1,
-#     'shift_range': 0.05,
-#     'random_flip': 0.5,
-#     }
-def read_image(fn, random_transform_args=random_transform_args):
-    image = cv2.imread(fn) / 255.0
-    image = cv2.resize(image, (256,256))
-    image = random_transform( image, **random_transform_args )
-    warped_img, target_img = random_warp( image )
+class TrainingDataGenerator():
+    def __init__(self, random_transform_args, coverage):
+        self.random_transform_args = random_transform_args
+        self.coverage = coverage
+
+    def minibatchAB(self, images, batchsize):
+        batch = BackgroundGenerator(self.minibatch(images, batchsize), 1)
+        for ep1, warped_img, target_img in batch.iterator():
+            yield ep1, warped_img, target_img
+
+    # A generator function that yields epoch, batchsize of warped_img and batchsize of target_img
+    def minibatch(self, data, batchsize):
+        length = len(data)
+        assert length >= batchsize, "Number of images is lower than batch-size (Note that too few images may lead to bad training). # images: {}, batch-size: {}".format(length, batchsize)
+        epoch = i = 0
+        shuffle(data)
+        while True:
+            size = batchsize
+            if i+size > length:
+                shuffle(data)
+                i = 0
+                epoch+=1
+            rtn = numpy.float32([self.read_image(img) for img in data[i:i+size]])
+            i+=size
+            yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:]       
+
+    def color_adjust(self, img):
+        return img / 255.0
     
-    return warped_img, target_img
-
-# A generator function that yields epoch, batchsize of warped_img and batchsize of target_img
-def minibatch(data, batchsize):
-    length = len(data)
-    epoch = i = 0
-    shuffle(data)
-    while True:
-        size = batchsize
-        if i+size > length:
-            shuffle(data)
-            i = 0
-            epoch+=1        
-        rtn = numpy.float32([read_image(data[j]) for j in range(i,i+size)])
-        i+=size
-        yield epoch, rtn[:,0,:,:,:], rtn[:,1,:,:,:]       
-
-def minibatchAB(images, batchsize):
-    batch = BackgroundGenerator(minibatch(images, batchsize), 1)
-    for ep1, warped_img, target_img in batch.iterator():
-        yield ep1, warped_img, target_img
-
-def random_transform(image, rotation_range, zoom_range, shift_range, random_flip):
-    h, w = image.shape[0:2]
-    rotation = numpy.random.uniform(-rotation_range, rotation_range)
-    scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)
-    tx = numpy.random.uniform(-shift_range, shift_range) * w
-    ty = numpy.random.uniform(-shift_range, shift_range) * h
-    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)
-    mat[:, 2] += (tx, ty)
-    result = cv2.warpAffine(
-        image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)
-    if numpy.random.random() < random_flip:
-        result = result[:, ::-1]
-    return result
-
-# get pair of random warped images from aligned face image
-def random_warp(image):
-    assert image.shape == (256, 256, 3)
-    range_ = numpy.linspace(128 - coverage//2, 128 + coverage//2, 5)
-    mapx = numpy.broadcast_to(range_, (5, 5))
-    mapy = mapx.T
-
-    mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)
-    mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)
-
-    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')
-    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')
-
-    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)
-
-    src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)
-    dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)
-    mat = umeyama(src_points, dst_points, True)[0:2]
-
-    target_image = cv2.warpAffine(image, mat, (64, 64))
-
-    return warped_image, target_image
-
-def get_transpose_axes(n):
-    if n % 2 == 0:
-        y_axes = list(range(1, n - 1, 2))
-        x_axes = list(range(0, n - 1, 2))
-    else:
-        y_axes = list(range(0, n - 1, 2))
-        x_axes = list(range(1, n - 1, 2))
-    return y_axes, x_axes, [n - 1]
+    def read_image(self, fn):
+        image = self.color_adjust(cv2.imread(fn))
+        image = cv2.resize(image, (256,256))
+        image = self.random_transform( image, **self.random_transform_args )
+        warped_img, target_img = self.random_warp( image, self.coverage )
+        
+        return warped_img, target_img
+
+    def random_transform(self, image, rotation_range, zoom_range, shift_range, random_flip):
+        h, w = image.shape[0:2]
+        rotation = numpy.random.uniform(-rotation_range, rotation_range)
+        scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)
+        tx = numpy.random.uniform(-shift_range, shift_range) * w
+        ty = numpy.random.uniform(-shift_range, shift_range) * h
+        mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)
+        mat[:, 2] += (tx, ty)
+        result = cv2.warpAffine(
+            image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)
+        if numpy.random.random() < random_flip:
+            result = result[:, ::-1]
+        return result
+
+    # get pair of random warped images from aligned face image
+    def random_warp(self, image, coverage):
+        assert image.shape == (256, 256, 3)
+        range_ = numpy.linspace(128 - coverage//2, 128 + coverage//2, 5)
+        mapx = numpy.broadcast_to(range_, (5, 5))
+        mapy = mapx.T
+
+        mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)
+        mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)
+
+        interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')
+        interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')
+
+        warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)
+
+        src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)
+        dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)
+        mat = umeyama(src_points, dst_points, True)[0:2]
+
+        target_image = cv2.warpAffine(image, mat, (64, 64))
+
+        return warped_image, target_image
 
 def stack_images(images):
+    def get_transpose_axes(n):
+        if n % 2 == 0:
+            y_axes = list(range(1, n - 1, 2))
+            x_axes = list(range(0, n - 1, 2))
+        else:
+            y_axes = list(range(0, n - 1, 2))
+            x_axes = list(range(1, n - 1, 2))
+        return y_axes, x_axes, [n - 1]
+    
     images_shape = numpy.array(images.shape)
     new_axes = get_transpose_axes(len(images_shape))
     new_shape = [numpy.prod(images_shape[x]) for x in new_axes]
diff --git a/lib/utils.py b/lib/utils.py
index beb5473..8ef81e3 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -4,7 +4,7 @@ import sys
 from pathlib import Path
 from scandir import scandir
 
-image_extensions = [".jpg", ".jpeg", ".png"]
+image_extensions = [".jpg", ".jpeg", ".png", ".tif", ".tiff"]
 
 def get_folder(path):
     output_dir = Path(path)
diff --git a/scripts/convert.py b/scripts/convert.py
index bfbecfa..e5db356 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -1,9 +1,11 @@
 import cv2
 import re
+
 from pathlib import Path
+from tqdm import tqdm
+
 from lib.cli import DirectoryProcessor, FullPaths
 from lib.utils import BackgroundGenerator
-from lib.faces_detect import detect_faces
 
 from plugins.PluginLoader import PluginLoader
 
@@ -44,6 +46,12 @@ class ConvertImage(DirectoryProcessor):
                             default="Masked",
                             help="Converter to use.")
 
+        parser.add_argument('-D', '--detector',
+                            type=str,
+                            choices=("hog", "cnn"), # case sensitive because this is used to load a plugin.
+                            default="hog",
+                            help="Detector to use. 'cnn' detects much more angles but will be much more resource intensive and may fail on large files.")
+
         parser.add_argument('-fr', '--frame-ranges',
                             nargs="+",
                             type=str,
@@ -58,6 +66,13 @@ class ConvertImage(DirectoryProcessor):
                             help="When used with --frame-ranges discards frames that are not processed instead of writing them out unchanged."
                             )
 
+        parser.add_argument('-f', '--filter',
+                            type=str,
+                            dest="filter",
+                            default="filter.jpg",
+                            help="Reference image for the person you want to process. Should be a front portrait"
+                            )
+
         parser.add_argument('-b', '--blur-size',
                             type=int,
                             default=2,
@@ -97,10 +112,7 @@ class ConvertImage(DirectoryProcessor):
         return parser
     
     def process(self):
-        # Original model goes with Adjust or Masked converter
-        # does the LowMem one work with only one? 
-        # seems to work with both in testing - although Adjust with LowMem 
-        # looks a real mess - you can see that it is "working"
+        # Original & LowMem models go with Adjust or Masked converter
         model_name = self.arguments.trainer
         conv_name = self.arguments.converter
         
@@ -138,7 +150,7 @@ class ConvertImage(DirectoryProcessor):
         for item in batch.iterator():
             self.convert(converter, item)
     
-    def check_skip(self, filename):
+    def check_skipframe(self, filename):
         try:
             idx = int(self.imageidxre.findall(filename)[0])
             return not any(map(lambda b: b[0]<=idx<=b[1], self.frame_ranges))
@@ -149,9 +161,7 @@ class ConvertImage(DirectoryProcessor):
         try:
             (filename, image, faces) = item
             
-            skip = self.check_skip(filename)
-
-            if not skip: # process as normal
+            if not self.check_skipframe(filename): # process as normal
                 for idx, face in faces:
                     image = converter.patch_image(image, face)
 
@@ -164,6 +174,6 @@ class ConvertImage(DirectoryProcessor):
             print('Failed to convert image: {}. Reason: {}'.format(filename, e))
 
     def prepare_images(self):
-        for filename in self.read_directory():
+        for filename in tqdm(self.read_directory()):
             image = cv2.imread(filename)
             yield filename, image, self.get_faces(image)
diff --git a/scripts/extract.py b/scripts/extract.py
index 58fe5fa..c2f42c8 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -1,7 +1,10 @@
 import cv2
 
 from pathlib import Path
+from tqdm import tqdm
+
 from lib.cli import DirectoryProcessor
+from lib.multithreading import pool_process
 from plugins.PluginLoader import PluginLoader
 
 class ExtractTrainingData(DirectoryProcessor):
@@ -13,18 +16,56 @@ class ExtractTrainingData(DirectoryProcessor):
             epilog="Questions and feedback: \
             https://github.com/deepfakes/faceswap-playground"
         )
-        
+
+    def add_optional_arguments(self, parser):
+        parser.add_argument('-D', '--detector',
+                            type=str,
+                            choices=("hog", "cnn"), # case sensitive because this is used to load a plugin.
+                            default="hog",
+                            help="Detector to use. 'cnn' detects much more angles but will be much more resource intensive and may fail on large files.")
+
+        parser.add_argument('-f', '--filter',
+                            type=str,
+                            dest="filter",
+                            default="filter.jpg",
+                            help="Reference image for the person you want to process. Should be a front portrait"
+                            )
+
+        parser.add_argument('-j', '--processes',
+                            type=int,
+                            help="Number of processes to use.")
+        return parser
+
     def process(self):
         extractor_name = "Align" # TODO Pass as argument
-        extractor = PluginLoader.get_extractor(extractor_name)()
+        self.extractor = PluginLoader.get_extractor(extractor_name)()
+        self.faces_detected = 0
+        processes = self.arguments.processes
+        if processes != 1:
+            files = list(self.read_directory())
+            for _ in tqdm(pool_process(self.processFiles, files, processes=processes), total = len(files)):
+                self.faces_detected +=1
+        else:
+            try:
+                for filename in tqdm(self.read_directory()):
+                    self.handleImage(filename)
+
+            except Exception as e:
+                print('Failed to extract from image: {}. Reason: {}'.format(filename, e))
 
+    def processFiles(self, filename):
         try:
-            for filename in self.read_directory():
-                image = cv2.imread(filename)
-                for idx, face in self.get_faces(image):
-                    resized_image = extractor.extract(image, face, 256)
-                    output_file = self.output_dir / Path(filename).stem
-                    cv2.imwrite(str(output_file) + str(idx) + Path(filename).suffix, resized_image)
-                
+            return self.handleImage(filename)
         except Exception as e:
             print('Failed to extract from image: {}. Reason: {}'.format(filename, e))
+    
+    def handleImage(self, filename):
+        count = 0
+        image = cv2.imread(filename)
+        for idx, face in self.get_faces(image):
+            count = idx
+            resized_image = self.extractor.extract(image, face, 256)
+            output_file = self.output_dir / Path(filename).stem
+            cv2.imwrite(str(output_file) + str(idx) + Path(filename).suffix, resized_image)
+        return count + 1
+
diff --git a/scripts/train.py b/scripts/train.py
index 113eca7..8e67a86 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -2,6 +2,7 @@ import cv2
 import numpy
 import time
 
+from threading import Lock
 from lib.utils import get_image_paths
 from lib.cli import FullPaths
 from plugins.PluginLoader import PluginLoader
@@ -11,6 +12,7 @@ class TrainingProcessor(object):
 
     def __init__(self, subparser, command, description='default'):
         self.parse_arguments(description, subparser, command)
+        self.lock = Lock()
 
     def process_arguments(self, arguments):
         self.arguments = arguments
@@ -96,8 +98,9 @@ class TrainingProcessor(object):
             print('Using live preview')
             while True:
                 try:
-                    for name, image in self.preview_buffer.items():
-                        cv2.imshow(name, image)
+                    with self.lock:
+                        for name, image in self.preview_buffer.items():
+                            cv2.imshow(name, image)
 
                     key = cv2.waitKey(1000)
                     if key == ord('\n') or key == ord('\r'):
@@ -118,16 +121,14 @@ class TrainingProcessor(object):
         print('Loading data, this may take a while...')
         # this is so that you can enter case insensitive values for trainer
         trainer = self.arguments.trainer
-        trainer = trainer if trainer != "Lowmem" else "LowMem"
+        trainer = "LowMem" if trainer.lower() == "lowmem" else trainer
         model = PluginLoader.get_model(trainer)(self.arguments.model_dir)
         model.load(swapped=False)
 
         images_A = get_image_paths(self.arguments.input_A)
         images_B = get_image_paths(self.arguments.input_B)
-        trainer = PluginLoader.get_trainer(trainer)(model,
-                                                                   images_A,
-                                                                   images_B,
-                                                                   batch_size=self.arguments.batch_size)
+        trainer = PluginLoader.get_trainer(trainer)
+        trainer = trainer(model, images_A, images_B, batch_size=self.arguments.batch_size)
 
         try:
             print('Starting. Press "Enter" to stop training and save model')
@@ -155,13 +156,17 @@ class TrainingProcessor(object):
             except KeyboardInterrupt:
                 print('Saving model weights has been cancelled!')
             exit(0)
-
+        except Exception as e:
+            print(e)
+            exit(1)
+    
     preview_buffer = {}
 
     def show(self, image, name=''):
         try:
             if self.arguments.preview:
-                self.preview_buffer[name] = image
+                with self.lock:
+                    self.preview_buffer[name] = image
             elif self.arguments.write_image:
                 cv2.imwrite('_sample_{}.jpg'.format(name), image)
         except Exception as e:
