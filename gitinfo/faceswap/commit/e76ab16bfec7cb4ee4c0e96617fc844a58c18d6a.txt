commit e76ab16bfec7cb4ee4c0e96617fc844a58c18d6a
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Jul 3 10:42:58 2018 +0200

    Effmpeg preview, MTCNN Extractor. HiRes model fixes (#458)
    
    * Add preview functionality to effmpeg.  (#435)
    
    * Add preview functionality to effmpeg.
    
    effmpeg tool:
    Preview for actions that have a video output now available.
    Preview does not work when muxing audio.
    
    * Model json unicode fix1 (#443)
    
    * fixed Windows 10 path error while loading weights
    
    * - fixed TypeError: the JSON object must be str, not 'bytes' with OriginalHighRes Model
    
    * MTCNN Extractor and Extraction refactor (#453)
    
    * implement mtcnn extractor
    
    * mtcnn refactor and vram management changes
    
    * cli arguments update for mtcnn/dlib split
    
    * Add mtcnn models to gitignore
    
    * Change multiprocessing on extract
    
    * GUI changes to handle nargs defaults
    
    * Early exit bugfix (#455)
    
    * Fix extract early termination bug
    
    * Fix extract early exit bug
    
    * Multi face detection bugfix (#456)
    
    * Multi face extraction fix
    
    * Original high res cleanup 1 (#457)
    
    * slight model re-factoring
      - removed excess threading code
      - added random kernel initialization to dense layer
    
    * Slight OriginalHighRes re-factoring an code cleanup

diff --git a/.gitignore b/.gitignore
index 6e3a72f..bd31020 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,9 +4,14 @@
 !*.md
 !*.txt
 !*.png
+!*.h5
+!*.dat
+!*.npy
 !Dockerfile*
 !requirements*
 !lib
+!lib/face_alignment
+!lib/face_alignment/.cache
 !lib/gui
 !lib/gui/.cache
 !lib/gui/.cache/preview
diff --git a/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py b/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py
deleted file mode 100644
index edbee58..0000000
--- a/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py
+++ /dev/null
@@ -1,187 +0,0 @@
-import atexit
-import numpy as np
-import os
-import cv2
-import dlib
-import keras
-from keras import backend as K
-
-dlib_detectors = []
-keras_model = None
-is_initialized = False
-
-@atexit.register
-def onExit():
-    global dlib_detectors
-    global keras_model
-    
-    if keras_model is not None:
-        del keras_model
-        K.clear_session()
-        
-    for detector in dlib_detectors:
-        del detector
-        
-class TorchBatchNorm2D(keras.engine.base_layer.Layer):
-    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
-        super(TorchBatchNorm2D, self).__init__(**kwargs)
-        self.supports_masking = True
-        self.axis = axis
-        self.momentum = momentum
-        self.epsilon = epsilon
-
-    def build(self, input_shape):
-        dim = input_shape[self.axis]
-        if dim is None:
-            raise ValueError('Axis ' + str(self.axis) + ' of ' 'input tensor should have a defined dimension ' 'but the layer received an input with shape ' + str(input_shape) + '.')
-        shape = (dim,)
-        self.gamma = self.add_weight(shape=shape, name='gamma', initializer='ones', regularizer=None, constraint=None)
-        self.beta = self.add_weight(shape=shape, name='beta', initializer='zeros', regularizer=None, constraint=None)
-        self.moving_mean = self.add_weight(shape=shape, name='moving_mean', initializer='zeros', trainable=False)            
-        self.moving_variance = self.add_weight(shape=shape, name='moving_variance', initializer='ones', trainable=False)            
-        self.built = True
-
-    def call(self, inputs, training=None):
-        input_shape = K.int_shape(inputs)
-
-        broadcast_shape = [1] * len(input_shape)
-        broadcast_shape[self.axis] = input_shape[self.axis]
-        
-        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
-        broadcast_moving_variance = K.reshape(self.moving_variance, broadcast_shape)
-        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
-        broadcast_beta = K.reshape(self.beta, broadcast_shape)        
-        invstd = K.ones (shape=broadcast_shape, dtype='float32') / K.sqrt(broadcast_moving_variance + K.constant(self.epsilon, dtype='float32'))
-        
-        return (inputs - broadcast_moving_mean) * invstd * broadcast_gamma + broadcast_beta
-       
-    def get_config(self):
-        config = { 'axis': self.axis, 'momentum': self.momentum, 'epsilon': self.epsilon }
-        base_config = super(TorchBatchNorm2D, self).get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-def transform(point, center, scale, resolution):
-    pt = np.array ( [point[0], point[1], 1.0] )            
-    h = 200.0 * scale
-    m = np.eye(3)
-    m[0,0] = resolution / h
-    m[1,1] = resolution / h
-    m[0,2] = resolution * ( -center[0] / h + 0.5 )
-    m[1,2] = resolution * ( -center[1] / h + 0.5 )
-    m = np.linalg.inv(m)
-    return np.matmul (m, pt)[0:2]
-    
-def crop(image, center, scale, resolution=256.0):
-    ul = transform([1, 1], center, scale, resolution).astype( np.int )
-    br = transform([resolution, resolution], center, scale, resolution).astype( np.int )
-    if image.ndim > 2:
-        newDim = np.array([br[1] - ul[1], br[0] - ul[0], image.shape[2]], dtype=np.int32)
-        newImg = np.zeros(newDim, dtype=np.uint8)
-    else:
-        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)
-        newImg = np.zeros(newDim, dtype=np.uint8)
-    ht = image.shape[0]
-    wd = image.shape[1]
-    newX = np.array([max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)
-    newY = np.array([max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)
-    oldX = np.array([max(1, ul[0] + 1), min(br[0], wd)], dtype=np.int32)
-    oldY = np.array([max(1, ul[1] + 1), min(br[1], ht)], dtype=np.int32)
-    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1] ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]
-    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)), interpolation=cv2.INTER_LINEAR)
-    return newImg
-           
-def get_pts_from_predict(a, center, scale):
-    b = a.reshape ( (a.shape[0], a.shape[1]*a.shape[2]) )    
-    c = b.argmax(1).reshape ( (a.shape[0], 1) ).repeat(2, axis=1).astype(np.float)
-    c[:,0] %= a.shape[2]    
-    c[:,1] = np.apply_along_axis ( lambda x: np.floor(x / a.shape[2]), 0, c[:,1] )
-
-    for i in range(a.shape[0]):
-        pX, pY = int(c[i,0]), int(c[i,1])
-        if pX > 0 and pX < 63 and pY > 0 and pY < 63:
-            diff = np.array ( [a[i,pY,pX+1]-a[i,pY,pX-1], a[i,pY+1,pX]-a[i,pY-1,pX]] )
-            c[i] += np.sign(diff)*0.25
-   
-    c += 0.5
-    return [ transform (c[i], center, scale, a.shape[2]) for i in range(a.shape[0]) ]
-
-def initialize(detector, scale_to=2048):
-    global dlib_detectors
-    global keras_model
-    global is_initialized
-    if not is_initialized:
-        dlib_cnn_face_detector_path = os.path.join(os.path.dirname(__file__), "mmod_human_face_detector.dat")
-        if not os.path.exists(dlib_cnn_face_detector_path):
-            raise Exception ("Error: Unable to find %s, reinstall the lib !" % (dlib_cnn_face_detector_path) )
-        
-        if detector == 'cnn' or detector == "all":
-            dlib_cnn_face_detector = dlib.cnn_face_detection_model_v1(dlib_cnn_face_detector_path)            
-            #DLIB and TF competiting for VRAM, so dlib must do first allocation to prevent OOM error 
-            dlib_cnn_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 ) 
-            dlib_detectors.append(dlib_cnn_face_detector)
-        
-        if detector == "hog" or detector == "all":
-            dlib_face_detector = dlib.get_frontal_face_detector()
-            dlib_face_detector ( np.zeros ( (scale_to, scale_to, 3), dtype=np.uint8), 0 )
-            dlib_detectors.append(dlib_face_detector)        
-    
-        keras_model_path = os.path.join( os.path.dirname(__file__) , "2DFAN-4.h5" )
-        if not os.path.exists(keras_model_path):
-            print ("Error: Unable to find %s, reinstall the lib !" % (keras_model_path) )
-        else:
-            print ("Info: initializing keras model...")
-            keras_model = keras.models.load_model (keras_model_path, custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D} ) 
-            
-        is_initialized = True
-
-#scale_to=2048 with dlib upsamples=0 for 3GB VRAM Windows 10 users        
-#you should not extract landmarks again from predetected face, because many face data lost, so result will be much different against extract from original big image
-def extract(input_image_bgr, detector, verbose, all_faces=True, input_is_predetected_face=False, scale_to=2048):
-    initialize(detector, scale_to)
-    global dlib_detectors
-    global keras_model
-    
-    (h, w, ch) = input_image_bgr.shape
-
-    detected_faces = []
-    
-    if input_is_predetected_face:
-        input_scale = 1.0
-        detected_faces = [ dlib.rectangle(0, 0, w, h) ]
-        input_image = input_image_bgr[:,:,::-1].copy()
-    else:
-        input_scale = scale_to / (w if w > h else h)
-        input_image_bgr = cv2.resize (input_image_bgr, ( int(w*input_scale), int(h*input_scale) ), interpolation=cv2.INTER_LINEAR)
-        input_image = input_image_bgr[:,:,::-1].copy() #cv2 and numpy inputs differs in rgb-bgr order, this affects chance of dlib face detection
-        input_images = [input_image, input_image_bgr]
-        for current_detector, current_image in ((current_detector, current_image) for current_detector in dlib_detectors for current_image in input_images):
-            detected_faces = current_detector(current_image, 0)
-            if len(detected_faces) != 0:
-                break
-
-    landmarks = []
-    if len(detected_faces) > 0:        
-        for i, d_rect in enumerate(detected_faces):
-            if i > 0 and not all_faces:
-                break
-        
-            if type(d_rect) == dlib.mmod_rectangle:
-                d_rect = d_rect.rect
-            
-            left, top, right, bottom = d_rect.left(), d_rect.top(), d_rect.right(), d_rect.bottom()
-            del d_rect
-    
-            center = np.array( [ (left + right) / 2.0, (top + bottom) / 2.0] )
-            center[1] -= (bottom - top) * 0.12
-            scale = (right - left + bottom - top) / 195.0
-        
-            image = crop(input_image, center, scale).transpose ( (2,0,1) ).astype(np.float32) / 255.0
-            image = np.expand_dims(image, 0)
-            
-            pts_img = get_pts_from_predict ( keras_model.predict (image)[-1][0], center, scale)
-            pts_img = [ ( int(pt[0]/input_scale), int(pt[1]/input_scale) ) for pt in pts_img ]             
-            landmarks.append ( ((  int(left/input_scale), int(top/input_scale), int(right/input_scale), int(bottom/input_scale) ),pts_img) )
-    elif verbose:
-        print("Warning: No faces were detected.")
-        
-    return landmarks
diff --git a/lib/FaceLandmarksExtractor/__init__.py b/lib/FaceLandmarksExtractor/__init__.py
deleted file mode 100644
index 6da6296..0000000
--- a/lib/FaceLandmarksExtractor/__init__.py
+++ /dev/null
@@ -1 +0,0 @@
-from .FaceLandmarksExtractor import extract
\ No newline at end of file
diff --git a/lib/aligner.py b/lib/aligner.py
index 5017dc6..d946be2 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -26,7 +26,7 @@ mean_face_y = numpy.array([
 landmarks_2D = numpy.stack( [ mean_face_x, mean_face_y ], axis=1 )
 
 def get_align_mat(face, size, should_align_eyes):
-    mat_umeyama = umeyama(numpy.array(face.landmarksAsXY()[17:]), landmarks_2D, True)[0:2]
+    mat_umeyama = umeyama(numpy.array(face.landmarks_as_xy()[17:]), landmarks_2D, True)[0:2]
 
     if should_align_eyes is False:
         return mat_umeyama
@@ -34,7 +34,7 @@ def get_align_mat(face, size, should_align_eyes):
     mat_umeyama = mat_umeyama * size
 
     # Convert to matrix
-    landmarks = numpy.matrix(face.landmarksAsXY())
+    landmarks = numpy.matrix(face.landmarks_as_xy())
 
     # cv2 expects points to be in the form np.array([ [[x1, y1]], [[x2, y2]], ... ]), we'll expand the dim
     landmarks = numpy.expand_dims(landmarks, axis=1)
diff --git a/lib/cli.py b/lib/cli.py
index f4a326b..01476ad 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -180,7 +180,8 @@ class FaceSwapArgs(object):
     """ Faceswap argument parser functions that are universal
         to all commands. Should be the parent function of all
         subsequent argparsers """
-    def __init__(self, subparser, command, description="default", subparsers=None):
+    def __init__(self, subparser, command,
+                 description="default", subparsers=None):
 
         self.argument_list = self.get_argument_list()
         self.optional_arguments = self.get_optional_arguments()
@@ -224,7 +225,8 @@ class FaceSwapArgs(object):
         """ Parse the arguments passed in from argparse """
         for option in self.argument_list + self.optional_arguments:
             args = option["opts"]
-            kwargs = {key: option[key] for key in option.keys() if key != "opts"}
+            kwargs = {key: option[key]
+                      for key in option.keys() if key != "opts"}
             self.parser.add_argument(*args, **kwargs)
 
 
@@ -274,17 +276,59 @@ class ExtractConvertArgs(FaceSwapArgs):
                               "type": str,
                               # case sensitive because this is used to load a
                               # plugin.
-                              "choices": ("hog", "cnn", "all"),
-                              "default": "hog",
-                              "help": "Detector to use. 'cnn' detects many "
-                                      "more angles but will be much more "
-                                      "resource intensive and may fail on "
-                                      "large files"})
+                              "choices": ("dlib-hog", "dlib-cnn",
+                                          "dlib-all", "mtcnn"),
+                              "default": "mtcnn",
+                              "help": "Detector to use. 'dlib-hog': uses "
+                                      "least resources, but is the least "
+                                      "reliable. 'dlib-cnn': faster than "
+                                      "mtcnn but detects fewer faces and "
+                                      "fewer false positives. 'dlib-all': "
+                                      "attempts to find faces using "
+                                      "dlib-cnn, if none are found, attempts "
+                                      "to find faces using dlib-hog. "
+                                      "'mtcnn': slower than dlib, but uses "
+                                      "fewer resources whilst detecting more "
+                                      "faces and more false positives. "
+                                      "Has superior alignment to dlib"})
+        argument_list.append({"opts": ("-mtms", "--mtcnn-minsize"),
+                              "type": int,
+                              "dest": "mtcnn_minsize",
+                              "default": 20,
+                              "help": "The minimum size of a face to be "
+                                      "accepted. Lower values use "
+                                      "significantly more VRAM. Minimum "
+                                      "value is 10. Default is 20 "
+                                      "(MTCNN detector only)"})
+        argument_list.append({"opts": ("-mtth", "--mtcnn-threshold"),
+                              "nargs": "+",
+                              "type": str,
+                              "dest": "mtcnn_threshold",
+                              "default": ["0.6", "0.7", "0.7"],
+                              "help": "Three step threshold for face "
+                                      "detection. Should be three decimal "
+                                      "numbers each less than 1. Eg: "
+                                      "'--mtcnn-threshold 0.6 0.7 0.7'. "
+                                      "1st stage: obtains face candidates, "
+                                      "2nd stage: refinement of face "
+                                      "candidates, 3rd stage: further "
+                                      "refinement of face candidates. "
+                                      "Default is 0.6 0.7 0.7 "
+                                      "(MTCNN detector only)"})
+        argument_list.append({"opts": ("-mtsc", "--mtcnn-scalefactor"),
+                              "type": float,
+                              "dest": "mtcnn_scalefactor",
+                              "default": 0.709,
+                              "help": "The scale factor for the image "
+                                      "pyramid. Should be a decimal number "
+                                      "less than one. Default is 0.709 "
+                                      "(MTCNN detector only)"})
         argument_list.append({"opts": ("-l", "--ref_threshold"),
                               "type": float,
                               "dest": "ref_threshold",
                               "default": 0.6,
-                              "help": "Threshold for positive face recognition"})
+                              "help": "Threshold for positive face "
+                                      "recognition"})
         argument_list.append({"opts": ("-n", "--nfilter"),
                               "type": str,
                               "dest": "nfilter",
@@ -341,24 +385,23 @@ class ExtractArgs(ExtractConvertArgs):
                                       "Discarded images are moved into a "
                                       "\"blurry\" sub-folder. Lower values "
                                       "allow more blur"})
-        argument_list.append({"opts": ("-j", "--processes"),
-                              "type": int,
-                              "default": 1,
-                              "help": "Number of CPU processes to use. "
-                                      "WARNING: ONLY USE THIS IF YOU ARE NOT "
-                                      "EXTRACTING ON A GPU. Anything above 1 "
-                                      "process on a GPU will run out of "
-                                      "memory and will crash"})
+        argument_list.append({"opts": ("-mp", "--multiprocess"),
+                              "action": "store_true",
+                              "default": False,
+                              "help": "Run extraction on all available "
+                                      "cores. (CPU only)"})
         argument_list.append({"opts": ("-s", "--skip-existing"),
                               "action": "store_true",
                               "dest": "skip_existing",
                               "default": False,
-                              "help": "Skips frames that have already been extracted"})
+                              "help": "Skips frames that have already been "
+                                      "extracted"})
         argument_list.append({"opts": ("-dl", "--debug-landmarks"),
                               "action": "store_true",
                               "dest": "debug_landmarks",
                               "default": False,
-                              "help": "Draw landmarks on the ouput faces for debug"})
+                              "help": "Draw landmarks on the ouput faces for "
+                                      "debug"})
         argument_list.append({"opts": ("-ae", "--align-eyes"),
                               "action": "store_true",
                               "dest": "align_eyes",
@@ -396,16 +439,20 @@ class ConvertArgs(ExtractConvertArgs):
                                       "files. If you delete faces from this "
                                       "folder, they'll be skipped during "
                                       "conversion. If no aligned dir is "
-                                      "specified, all faces will be converted"})
+                                      "specified, all faces will be "
+                                      "converted"})
         argument_list.append({"opts": ("-t", "--trainer"),
                               "type": str,
-                              # case sensitive because this is used to load a plug-in.
+                              # case sensitive because this is used to
+                              # load a plug-in.
                               "choices": PluginLoader.get_available_models(),
                               "default": PluginLoader.get_default_model(),
-                              "help": "Select the trainer that was used to create the model"})
+                              "help": "Select the trainer that was used to "
+                                      "create the model"})
         argument_list.append({"opts": ("-c", "--converter"),
                               "type": str,
-                              # case sensitive because this is used to load a plugin.
+                              # case sensitive because this is used
+                              # to load a plugin.
                               "choices": ("Masked", "Adjust"),
                               "default": "Masked",
                               "help": "Converter to use"})
@@ -424,12 +471,16 @@ class ConvertArgs(ExtractConvertArgs):
                                       "swapped face to cover more space. "
                                       "(Masked converter only)"})
         argument_list.append({"opts": ("-M", "--mask-type"),
-                              # lowercase this, because it's just a string later on.
+                              # lowercase this, because it's just a
+                              # string later on.
                               "type": str.lower,
                               "dest": "mask_type",
-                              "choices": ["rect", "facehull", "facehullandrect"],
+                              "choices": ["rect",
+                                          "facehull",
+                                          "facehullandrect"],
                               "default": "facehullandrect",
-                              "help": "Mask to use to replace faces. (Masked converter only)"})
+                              "help": "Mask to use to replace faces. "
+                                      "(Masked converter only)"})
         argument_list.append({"opts": ("-sh", "--sharpen"),
                               "type": str.lower,
                               "dest": "sharpen_image",
@@ -461,17 +512,20 @@ class ConvertArgs(ExtractConvertArgs):
                               "action": "store_true",
                               "dest": "swap_model",
                               "default": False,
-                              "help": "Swap the model. Instead of A -> B, swap B -> A"})
+                              "help": "Swap the model. Instead of A -> B, "
+                                      "swap B -> A"})
         argument_list.append({"opts": ("-S", "--seamless"),
                               "action": "store_true",
                               "dest": "seamless_clone",
                               "default": False,
-                              "help": "Use cv2's seamless clone. (Masked converter only)"})
+                              "help": "Use cv2's seamless clone. "
+                                      "(Masked converter only)"})
         argument_list.append({"opts": ("-mh", "--match-histogram"),
                               "action": "store_true",
                               "dest": "match_histogram",
                               "default": False,
-                              "help": "Use histogram matching. (Masked converter only)"})
+                              "help": "Use histogram matching. "
+                                      "(Masked converter only)"})
         argument_list.append({"opts": ("-sm", "--smooth-mask"),
                               "action": "store_true",
                               "dest": "smooth_mask",
@@ -481,7 +535,8 @@ class ConvertArgs(ExtractConvertArgs):
                               "action": "store_true",
                               "dest": "avg_color_adjust",
                               "default": True,
-                              "help": "Average color adjust. (Adjust converter only)"})
+                              "help": "Average color adjust. "
+                                      "(Adjust converter only)"})
         return argument_list
 
 
@@ -530,7 +585,8 @@ class TrainArgs(FaceSwapArgs):
         argument_list.append({"opts": ("-bs", "--batch-size"),
                               "type": int,
                               "default": 64,
-                              "help": "Batch size, as a power of 2 (64, 128, 256, etc)"})
+                              "help": "Batch size, as a power of 2 "
+                                      "(64, 128, 256, etc)"})
         argument_list.append({"opts": ("-it", "--iterations"),
                               "type": int,
                               "default": 1000000,
@@ -589,5 +645,6 @@ class GuiArgs(FaceSwapArgs):
                               "action": "store_true",
                               "dest": "debug",
                               "default": False,
-                              "help": "Output to Shell console instead of GUI console"})
+                              "help": "Output to Shell console instead of "
+                                      "GUI console"})
         return argument_list
diff --git a/lib/FaceLandmarksExtractor/2DFAN-4.h5 b/lib/face_alignment/.cache/2DFAN-4.h5
similarity index 100%
rename from lib/FaceLandmarksExtractor/2DFAN-4.h5
rename to lib/face_alignment/.cache/2DFAN-4.h5
diff --git a/lib/face_alignment/.cache/det1.npy b/lib/face_alignment/.cache/det1.npy
new file mode 100755
index 0000000..7c05a2c
Binary files /dev/null and b/lib/face_alignment/.cache/det1.npy differ
diff --git a/lib/face_alignment/.cache/det2.npy b/lib/face_alignment/.cache/det2.npy
new file mode 100755
index 0000000..85d5bf0
Binary files /dev/null and b/lib/face_alignment/.cache/det2.npy differ
diff --git a/lib/face_alignment/.cache/det3.npy b/lib/face_alignment/.cache/det3.npy
new file mode 100755
index 0000000..90d5ba9
Binary files /dev/null and b/lib/face_alignment/.cache/det3.npy differ
diff --git a/lib/FaceLandmarksExtractor/mmod_human_face_detector.dat b/lib/face_alignment/.cache/mmod_human_face_detector.dat
similarity index 100%
rename from lib/FaceLandmarksExtractor/mmod_human_face_detector.dat
rename to lib/face_alignment/.cache/mmod_human_face_detector.dat
diff --git a/lib/face_alignment/__init__.py b/lib/face_alignment/__init__.py
new file mode 100644
index 0000000..c874031
--- /dev/null
+++ b/lib/face_alignment/__init__.py
@@ -0,0 +1 @@
+from .extractor import Extract
\ No newline at end of file
diff --git a/lib/face_alignment/detectors.py b/lib/face_alignment/detectors.py
new file mode 100644
index 0000000..298080c
--- /dev/null
+++ b/lib/face_alignment/detectors.py
@@ -0,0 +1,164 @@
+#!/usr/bin python3
+""" DLIB Detector for face alignment
+    Code adapted and modified from:
+    https://github.com/1adrianb/face-alignment """
+
+import os
+from tensorflow import Graph, Session
+import dlib
+
+from .mtcnn import create_mtcnn, detect_face
+
+
+CACHE_PATH = os.path.join(os.path.dirname(__file__), ".cache")
+
+
+class Detector(object):
+    """ Detector object """
+    def __init__(self):
+        self.initialized = False
+        self.verbose = False
+        self.data_path = self.set_data_path()
+        self.detected_faces = None
+
+    @staticmethod
+    def set_data_path():
+        """ path to data file/models
+            override for specific detector """
+        pass
+
+    def set_predetected(self, width, height):
+        """ Set a dlib rectangle for predetected faces """
+        # Predetected_face is used for sort tool.
+        # Landmarks should not be extracted again from predetected faces,
+        # because face data is lost, resulting in a large variance
+        # against extract from original image
+        self.detected_faces = [dlib.rectangle(0, 0, width, height)]
+
+    @staticmethod
+    def is_mmod_rectangle(d_rectangle):
+        """ Return whether the passed in object is
+            a dlib.mmod_rectangle """
+        return isinstance(d_rectangle, dlib.mmod_rectangle)
+
+
+class DLibDetector(Detector):
+    """ Dlib detector for face recognition """
+    def __init__(self):
+        Detector.__init__(self)
+        self.detectors = list()
+
+    @staticmethod
+    def set_data_path():
+        """ Load the face detector data """
+        data_path = os.path.join(CACHE_PATH,
+                                 "mmod_human_face_detector.dat")
+        if not os.path.exists(data_path):
+            raise Exception("Error: Unable to find {}, reinstall "
+                            "the lib!".format(data_path))
+        return data_path
+
+    def create_detector(self, verbose, detector):
+        """ Add the requested detectors """
+        if self.initialized:
+            return
+
+        self.verbose = verbose
+
+        if detector == "dlib-cnn" or detector == "dlib-all":
+            if self.verbose:
+                print("Adding DLib - CNN detector")
+            self.detectors.append(dlib.cnn_face_detection_model_v1(
+                self.data_path))
+
+        if detector == "dlib-hog" or detector == "dlib-all":
+            if self.verbose:
+                print("Adding DLib - HOG detector")
+            self.detectors.append(dlib.get_frontal_face_detector())
+
+        self.initialized = True
+
+    def detect_faces(self, images):
+        """ Detect faces in images """
+        self.detected_faces = None
+        for current_detector, current_image in(
+                (current_detector, current_image)
+                for current_detector in self.detectors
+                for current_image in images):
+            self.detected_faces = current_detector(current_image, 0)
+
+            if self.detected_faces:
+                break
+
+
+class MTCNNDetector(Detector):
+    """ MTCNN detector for face recognition """
+    def __init__(self):
+        Detector.__init__(self)
+        self.kwargs = None
+
+    @staticmethod
+    def validate_kwargs(kwargs):
+        """ Validate that cli kwargs are correct. If not reset to default """
+        valid = True
+        if kwargs['minsize'] < 10:
+            valid = False
+        elif len(kwargs['threshold']) != 3:
+            valid = False
+        elif not all(0.0 < threshold < 1.0
+                     for threshold in kwargs['threshold']):
+            valid = False
+        elif not 0.0 < kwargs['factor'] < 1.0:
+            valid = False
+
+        if not valid:
+            print("Invalid MTCNN arguments received. Running with defaults")
+            return {"minsize": 20,                 # minimum size of face
+                    "threshold": [0.6, 0.7, 0.7],  # three steps threshold
+                    "factor": 0.709}               # scale factor
+        return kwargs
+
+    @staticmethod
+    def set_data_path():
+        """ Load the mtcnn models """
+        for model in ("det1.npy", "det2.npy", "det3.npy"):
+            model_path = os.path.join(CACHE_PATH, model)
+            if not os.path.exists(model_path):
+                raise Exception("Error: Unable to find {}, reinstall "
+                                "the lib!".format(model_path))
+        return CACHE_PATH
+
+    def create_detector(self, verbose, mtcnn_kwargs):
+        """ Create the mtcnn detector """
+        if self.initialized:
+            return
+
+        self.verbose = verbose
+
+        if self.verbose:
+            print("Adding MTCNN detector")
+
+        self.kwargs = mtcnn_kwargs
+
+        mtcnn_graph = Graph()
+        with mtcnn_graph.as_default():
+            mtcnn_session = Session()
+            with mtcnn_session.as_default():
+                pnet, rnet, onet = create_mtcnn(mtcnn_session, self.data_path)
+        mtcnn_graph.finalize()
+
+        self.kwargs["pnet"] = pnet
+        self.kwargs["rnet"] = rnet
+        self.kwargs["onet"] = onet
+        self.initialized = True
+
+    def detect_faces(self, images):
+        """ Detect faces in images """
+        self.detected_faces = None
+        for current_image in images:
+            detected_faces = detect_face(current_image, **self.kwargs)
+            self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
+                                                  int(face[2]), int(face[3]))
+                                   for face in detected_faces]
+            if self.detected_faces:
+                break
diff --git a/lib/face_alignment/extractor.py b/lib/face_alignment/extractor.py
new file mode 100644
index 0000000..651eb87
--- /dev/null
+++ b/lib/face_alignment/extractor.py
@@ -0,0 +1,319 @@
+#!/usr/bin python3
+""" Facial landmarks extractor for faceswap.py
+    Code adapted and modified from:
+    https://github.com/1adrianb/face-alignment
+"""
+
+import cv2
+import numpy as np
+
+from .detectors import DLibDetector, MTCNNDetector
+from .vram_allocation import GPUMem
+from .model import KerasModel
+
+DLIB_DETECTORS = DLibDetector()
+MTCNN_DETECTOR = MTCNNDetector()
+VRAM = GPUMem()
+KERAS_MODEL = KerasModel()
+
+
+class Frame(object):
+    """ The current frame for processing """
+
+    def __init__(self, detector, input_image,
+                 verbose, input_is_predetected_face):
+        self.verbose = verbose
+        self.height, self.width = input_image.shape[:2]
+
+        if not VRAM.scale_to and VRAM.device != -1:
+            VRAM.set_scale_to(detector)
+
+        if VRAM.device != -1:
+            self.scale_to = VRAM.scale_to
+        else:
+            self.scale_to = self.height * self.width
+
+        self.input_scale = 1.0
+        self.images = self.process_input(input_image,
+                                         input_is_predetected_face)
+
+    def process_input(self, input_image, input_is_predetected_face):
+        """ Process import image:
+            Size down if required
+            Duplicate into rgb colour space """
+        if not input_is_predetected_face:
+            input_image = self.scale_down(input_image)
+        return self.compile_color_space(input_image)
+
+    def scale_down(self, image):
+        """ Scale down large images based on vram amount """
+        pixel_count = self.width * self.height
+
+        if pixel_count > self.scale_to:
+            self.input_scale = self.scale_to / pixel_count
+            dimensions = (int(self.width * self.input_scale),
+                          int(self.height * self.input_scale))
+            if self.verbose:
+                print("Resizing image from {}x{} "
+                      "to {}.".format(str(self.width), str(self.height),
+                                      "x".join(str(i) for i in dimensions)))
+            image = cv2.resize(image, dimensions, interpolation=cv2.INTER_AREA)
+
+        return image
+
+    @staticmethod
+    def compile_color_space(image_bgr):
+        """ cv2 and numpy inputs differs in rgb-bgr order
+        this affects chance of dlib face detection so
+        pass both versions """
+        image_rgb = image_bgr[:, :, ::-1].copy()
+        return (image_rgb, image_bgr)
+
+
+class Align(object):
+    """ Perform transformation to align and get landmarks """
+    def __init__(self, frame, detected_faces, keras_model, verbose):
+        self.verbose = verbose
+        self.frame = frame.images[0]
+        self.input_scale = frame.input_scale
+        self.detected_faces = detected_faces
+        self.keras = keras_model
+
+        self.bounding_box = None
+        self.landmarks = self.process_landmarks()
+
+    @staticmethod
+    def transform(point, center, scale, resolution):
+        """ Transform Image """
+        pnt = np.array([point[0], point[1], 1.0])
+        hscl = 200.0 * scale
+        eye = np.eye(3)
+        eye[0, 0] = resolution / hscl
+        eye[1, 1] = resolution / hscl
+        eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
+        eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
+        eye = np.linalg.inv(eye)
+        return np.matmul(eye, pnt)[0:2]
+
+    def crop(self, image, center, scale, resolution=256.0):
+        """ Crop image around the center point """
+        v_ul = self.transform([1, 1], center, scale, resolution).astype(np.int)
+        v_br = self.transform([resolution, resolution],
+                              center,
+                              scale,
+                              resolution).astype(np.int)
+        if image.ndim > 2:
+            new_dim = np.array([v_br[1] - v_ul[1],
+                                v_br[0] - v_ul[0],
+                                image.shape[2]],
+                               dtype=np.int32)
+            new_img = np.zeros(new_dim, dtype=np.uint8)
+        else:
+            new_dim = np.array([v_br[1] - v_ul[1],
+                                v_br[0] - v_ul[0]],
+                               dtype=np.int)
+            new_img = np.zeros(new_dim, dtype=np.uint8)
+        height = image.shape[0]
+        width = image.shape[1]
+        new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
+                         dtype=np.int32)
+        new_y = np.array([max(1, -v_ul[1] + 1),
+                          min(v_br[1], height) - v_ul[1]],
+                         dtype=np.int32)
+        old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
+                         dtype=np.int32)
+        old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
+                         dtype=np.int32)
+        new_img[new_y[0] - 1:new_y[1],
+                new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
+                                               old_x[0] - 1:old_x[1], :]
+        new_img = cv2.resize(new_img,
+                             dsize=(int(resolution), int(resolution)),
+                             interpolation=cv2.INTER_LINEAR)
+        return new_img
+
+    def get_pts_from_predict(self, var_a, center, scale):
+        """ Get points from predictor """
+        var_b = var_a.reshape((var_a.shape[0],
+                               var_a.shape[1] * var_a.shape[2]))
+        var_c = var_b.argmax(1).reshape((var_a.shape[0],
+                                         1)).repeat(2,
+                                                    axis=1).astype(np.float)
+        var_c[:, 0] %= var_a.shape[2]
+        var_c[:, 1] = np.apply_along_axis(
+            lambda x: np.floor(x / var_a.shape[2]),
+            0,
+            var_c[:, 1])
+
+        for i in range(var_a.shape[0]):
+            pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
+            if pt_x > 0 and pt_x < 63 and pt_y > 0 and pt_y < 63:
+                diff = np.array([var_a[i, pt_y, pt_x+1]
+                                 - var_a[i, pt_y, pt_x-1],
+                                 var_a[i, pt_y+1, pt_x]
+                                 - var_a[i, pt_y-1, pt_x]])
+
+                var_c[i] += np.sign(diff)*0.25
+
+        var_c += 0.5
+        return [self.transform(var_c[i], center, scale, var_a.shape[2])
+                for i in range(var_a.shape[0])]
+
+    def process_landmarks(self):
+        """ Align image and process landmarks """
+        landmarks = list()
+        if not self.detected_faces:
+            if self.verbose:
+                print("Warning: No faces were detected.")
+            return landmarks
+
+        for d_rect in self.detected_faces:
+            self.get_bounding_box(d_rect)
+            del d_rect
+
+            center, scale = self.get_center_scale()
+            image = self.align_image(center, scale)
+
+            landmarks_xy = self.predict_landmarks(image, center, scale)
+
+            landmarks.append((
+                (int(self.bounding_box['left'] / self.input_scale),
+                 int(self.bounding_box['top'] / self.input_scale),
+                 int(self.bounding_box['right'] / self.input_scale),
+                 int(self.bounding_box['bottom'] / self.input_scale)),
+                landmarks_xy))
+
+        return landmarks
+
+    def get_bounding_box(self, d_rect):
+        """ Return the corner points of the bounding box """
+        self.bounding_box = {'left': d_rect.left(),
+                             'top': d_rect.top(),
+                             'right': d_rect.right(),
+                             'bottom': d_rect.bottom()}
+
+    def get_center_scale(self):
+        """ Get the center and set scale of bounding box """
+        center = np.array([(self.bounding_box['left']
+                            + self.bounding_box['right']) / 2.0,
+                           (self.bounding_box['top']
+                            + self.bounding_box['bottom']) / 2.0])
+
+        center[1] -= (self.bounding_box['bottom']
+                      - self.bounding_box['top']) * 0.12
+
+        scale = (self.bounding_box['right']
+                 - self.bounding_box['left']
+                 + self.bounding_box['bottom']
+                 - self.bounding_box['top']) / 195.0
+
+        return center, scale
+
+    def align_image(self, center, scale):
+        """ Crop and align image around center """
+        image = self.crop(
+            self.frame,
+            center,
+            scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
+
+        return np.expand_dims(image, 0)
+
+    def predict_landmarks(self, image, center, scale):
+        """ Predict the 68 point landmarks """
+        with self.keras.session.as_default():
+            pts_img = self.get_pts_from_predict(
+                self.keras.model.predict(image)[-1][0],
+                center,
+                scale)
+
+        return [(int(pt[0] / self.input_scale),
+                 int(pt[1] / self.input_scale))
+                for pt in pts_img]
+
+
+class Extract(object):
+    """ Extracts faces from an image, crops and
+        calculates landmarks """
+
+    def __init__(self, input_image_bgr, detector, mtcnn_kwargs,
+                 verbose, input_is_predetected_face=False):
+        self.initialized = False
+        self.verbose = verbose
+        self.keras = KERAS_MODEL
+        self.detector = None
+
+        self.initialize(detector, mtcnn_kwargs)
+
+        self.frame = Frame(detector=detector,
+                           input_image=input_image_bgr,
+                           verbose=verbose,
+                           input_is_predetected_face=input_is_predetected_face)
+
+        self.detect_faces(input_is_predetected_face)
+        self.convert_to_dlib_rectangle()
+
+        self.landmarks = Align(frame=self.frame,
+                               detected_faces=self.detector.detected_faces,
+                               keras_model=self.keras,
+                               verbose=self.verbose).landmarks
+
+    def initialize(self, detector, mtcnn_kwargs):
+        """ initialize Keras and Dlib """
+        if self.initialized:
+            return
+        self.initialize_vram(detector)
+
+        self.initialize_keras(detector)
+        self.initialize_detector(detector, mtcnn_kwargs)
+        self.initialized = True
+
+    def initialize_vram(self, detector):
+        """ Initialize vram based on detector """
+        VRAM.verbose = self.verbose
+        VRAM.detector = detector
+        VRAM.output_stats()
+
+    def initialize_keras(self, detector):
+        """ Initialize keras. Allocate vram to tensorflow
+            based on detector """
+        ratio = None
+        if detector != "mtcnn" and VRAM.device != -1:
+            ratio = VRAM.get_tensor_gpu_ratio()
+        placeholder = np.zeros((1, 3, 256, 256))
+        self.keras.load_model(verbose=self.verbose,
+                              ratio=ratio,
+                              dummy=placeholder)
+
+    def initialize_detector(self, detector, mtcnn_kwargs):
+        """ Initialize face detector """
+        kwargs = {"verbose": self.verbose}
+        if detector == "mtcnn":
+            self.detector = MTCNN_DETECTOR
+            mtcnn_kwargs = self.detector.validate_kwargs(mtcnn_kwargs)
+            kwargs["mtcnn_kwargs"] = mtcnn_kwargs
+        else:
+            self.detector = DLIB_DETECTORS
+            kwargs["detector"] = detector
+
+        self.detector.create_detector(**kwargs)
+
+    def detect_faces(self, input_is_predetected_face):
+        """ Detect faces """
+        # Predetected_face is used for sort tool.
+        # Landmarks should not be extracted again from predetected faces,
+        # because face data is lost, resulting in a large variance
+        # against extract from original image
+
+        if input_is_predetected_face:
+            self.detector.set_predetected(self.frame.width, self.frame.height)
+        else:
+            self.detector.detect_faces(self.frame.images)
+
+    def convert_to_dlib_rectangle(self):
+        """ Convert detected faces to dlib_rectangle """
+        detected = [d_rect.rect
+                    if self.detector.is_mmod_rectangle(d_rect)
+                    else d_rect
+                    for d_rect in self.detector.detected_faces]
+
+        self.detector.detected_faces = detected
diff --git a/lib/face_alignment/model.py b/lib/face_alignment/model.py
new file mode 100644
index 0000000..fc2faa9
--- /dev/null
+++ b/lib/face_alignment/model.py
@@ -0,0 +1,128 @@
+#!/usr/bin python3
+""" FAN model for face alignment
+    Code adapted and modified from:
+    https://github.com/1adrianb/face-alignment """
+
+import os
+
+import keras
+from keras import backend as K
+from tensorflow import ConfigProto, Graph, Session
+
+
+class TorchBatchNorm2D(keras.engine.base_layer.Layer):
+    """" Keras Model """
+    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
+        super(TorchBatchNorm2D, self).__init__(**kwargs)
+        self.supports_masking = True
+        self.axis = axis
+        self.momentum = momentum
+        self.epsilon = epsilon
+
+        self.built = False
+        self.gamma = None
+        self.beta = None
+        self.moving_mean = None
+        self.moving_variance = None
+
+    def build(self, input_shape):
+        dim = input_shape[self.axis]
+        if dim is None:
+            raise ValueError("Axis {} of input tensor should have a "
+                             "defined dimension but the layer received "
+                             "an input with  shape {}."
+                             .format(str(self.axis), str(input_shape)))
+        shape = (dim,)
+        self.gamma = self.add_weight(shape=shape,
+                                     name='gamma',
+                                     initializer='ones',
+                                     regularizer=None,
+                                     constraint=None)
+        self.beta = self.add_weight(shape=shape,
+                                    name='beta',
+                                    initializer='zeros',
+                                    regularizer=None,
+                                    constraint=None)
+        self.moving_mean = self.add_weight(shape=shape,
+                                           name='moving_mean',
+                                           initializer='zeros',
+                                           trainable=False)
+        self.moving_variance = self.add_weight(shape=shape,
+                                               name='moving_variance',
+                                               initializer='ones',
+                                               trainable=False)
+        self.built = True
+
+    def call(self, inputs, **kwargs):
+        input_shape = K.int_shape(inputs)
+
+        broadcast_shape = [1] * len(input_shape)
+        broadcast_shape[self.axis] = input_shape[self.axis]
+
+        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
+        broadcast_moving_variance = K.reshape(self.moving_variance,
+                                              broadcast_shape)
+        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
+        broadcast_beta = K.reshape(self.beta, broadcast_shape)
+        invstd = (K.ones(shape=broadcast_shape,
+                         dtype='float32')
+                  / K.sqrt(broadcast_moving_variance
+                           + K.constant(self.epsilon,
+                                        dtype='float32')))
+
+        return((inputs - broadcast_moving_mean)
+               * invstd
+               * broadcast_gamma
+               + broadcast_beta)
+
+    def get_config(self):
+        config = {'axis': self.axis,
+                  'momentum': self.momentum,
+                  'epsilon': self.epsilon}
+        base_config = super(TorchBatchNorm2D, self).get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+
+class KerasModel(object):
+    "Load the Keras Model"
+    def __init__(self):
+        self.initialized = False
+        self.verbose = False
+        self.model_path = self.set_model_path()
+        self.model = None
+        self.session = None
+
+    @staticmethod
+    def set_model_path():
+        """ Set the path to the Face Alignment Network Model """
+        model_path = os.path.join(os.path.dirname(__file__),
+                                  ".cache", "2DFAN-4.h5")
+        if not os.path.exists(model_path):
+            raise Exception("Error: Unable to find {}, "
+                            "reinstall the lib!".format(model_path))
+        return model_path
+
+    def load_model(self, verbose, dummy, ratio):
+        """ Load the Keras Model """
+        if self.initialized:
+            return
+
+        self.verbose = verbose
+        if self.verbose:
+            print("Initializing keras model...")
+
+        keras_graph = Graph()
+        with keras_graph.as_default():
+            config = ConfigProto()
+            if ratio:
+                config.gpu_options.per_process_gpu_memory_fraction = ratio
+            self.session = Session(config=config)
+            with self.session.as_default():
+                self.model = keras.models.load_model(
+                    self.model_path,
+                    custom_objects={'TorchBatchNorm2D':
+                                    TorchBatchNorm2D})
+                self.model.predict(dummy)
+        keras_graph.finalize()
+
+        self.initialized = True
diff --git a/lib/face_alignment/mtcnn.py b/lib/face_alignment/mtcnn.py
new file mode 100644
index 0000000..29f2939
--- /dev/null
+++ b/lib/face_alignment/mtcnn.py
@@ -0,0 +1,562 @@
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+#!/usr/bin python3
+""" MTCNN Detector for face alignment
+    Code adapted from:
+    https://github.com/davidsandberg/facenet """
+
+""" Tensorflow implementation of the face detection / alignment algorithm found at
+https://github.com/kpzhang93/MTCNN_face_detection_alignment
+"""
+# MIT License
+#
+# Copyright (c) 2016 David Sandberg
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+# SOFTWARE.
+
+from six import string_types, iteritems
+
+import numpy as np
+import tensorflow as tf
+#from math import floor
+import cv2
+import os
+
+def layer(op):
+    """Decorator for composable network layers."""
+
+    def layer_decorated(self, *args, **kwargs):
+        # Automatically set a name if not provided.
+        name = kwargs.setdefault('name', self.get_unique_name(op.__name__))
+        # Figure out the layer inputs.
+        if len(self.terminals) == 0:
+            raise RuntimeError('No input variables found for layer %s.' % name)
+        elif len(self.terminals) == 1:
+            layer_input = self.terminals[0]
+        else:
+            layer_input = list(self.terminals)
+        # Perform the operation and get the output.
+        layer_output = op(self, layer_input, *args, **kwargs)
+        # Add to layer LUT.
+        self.layers[name] = layer_output
+        # This output is now the input for the next layer.
+        self.feed(layer_output)
+        # Return self for chained calls.
+        return self
+
+    return layer_decorated
+
+class Network(object):
+
+    def __init__(self, inputs, trainable=True):
+        # The input nodes for this network
+        self.inputs = inputs
+        # The current list of terminal nodes
+        self.terminals = []
+        # Mapping from layer names to layers
+        self.layers = dict(inputs)
+        # If true, the resulting variables are set as trainable
+        self.trainable = trainable
+
+        self.setup()
+
+    def setup(self):
+        """Construct the network. """
+        raise NotImplementedError('Must be implemented by the subclass.')
+
+    def load(self, data_path, session, ignore_missing=False):
+        """Load network weights.
+        data_path: The path to the numpy-serialized network weights
+        session: The current TensorFlow session
+        ignore_missing: If true, serialized weights for missing layers are ignored.
+        """
+        data_dict = np.load(data_path, encoding='latin1').item() #pylint: disable=no-member
+
+        for op_name in data_dict:
+            with tf.variable_scope(op_name, reuse=True):
+                for param_name, data in iteritems(data_dict[op_name]):
+                    try:
+                        var = tf.get_variable(param_name)
+                        session.run(var.assign(data))
+                    except ValueError:
+                        if not ignore_missing:
+                            raise
+
+    def feed(self, *args):
+        """Set the input(s) for the next operation by replacing the terminal nodes.
+        The arguments can be either layer names or the actual layers.
+        """
+        assert len(args) != 0
+        self.terminals = []
+        for fed_layer in args:
+            if isinstance(fed_layer, string_types):
+                try:
+                    fed_layer = self.layers[fed_layer]
+                except KeyError:
+                    raise KeyError('Unknown layer name fed: %s' % fed_layer)
+            self.terminals.append(fed_layer)
+        return self
+
+    def get_output(self):
+        """Returns the current network output."""
+        return self.terminals[-1]
+
+    def get_unique_name(self, prefix):
+        """Returns an index-suffixed unique name for the given prefix.
+        This is used for auto-generating layer names based on the type-prefix.
+        """
+        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1
+        return '%s_%d' % (prefix, ident)
+
+    def make_var(self, name, shape):
+        """Creates a new TensorFlow variable."""
+        return tf.get_variable(name, shape, trainable=self.trainable)
+
+    def validate_padding(self, padding):
+        """Verifies that the padding is one of the supported ones."""
+        assert padding in ('SAME', 'VALID')
+
+    @layer
+    def conv(self,
+             inp,
+             k_h,
+             k_w,
+             c_o,
+             s_h,
+             s_w,
+             name,
+             relu=True,
+             padding='SAME',
+             group=1,
+             biased=True):
+        # Verify that the padding is acceptable
+        self.validate_padding(padding)
+        # Get the number of channels in the input
+        c_i = int(inp.get_shape()[-1])
+        # Verify that the grouping parameter is valid
+        assert c_i % group == 0
+        assert c_o % group == 0
+        # Convolution for a given input and kernel
+        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)
+        with tf.variable_scope(name) as scope:
+            kernel = self.make_var('weights', shape=[k_h, k_w, c_i // group, c_o])
+            # This is the common-case. Convolve the input without any further complications.
+            output = convolve(inp, kernel)
+            # Add the biases
+            if biased:
+                biases = self.make_var('biases', [c_o])
+                output = tf.nn.bias_add(output, biases)
+            if relu:
+                # ReLU non-linearity
+                output = tf.nn.relu(output, name=scope.name)
+            return output
+
+    @layer
+    def prelu(self, inp, name):
+        with tf.variable_scope(name):
+            i = int(inp.get_shape()[-1])
+            alpha = self.make_var('alpha', shape=(i,))
+            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))
+        return output
+
+    @layer
+    def max_pool(self, inp, k_h, k_w, s_h, s_w, name, padding='SAME'):
+        self.validate_padding(padding)
+        return tf.nn.max_pool(inp,
+                              ksize=[1, k_h, k_w, 1],
+                              strides=[1, s_h, s_w, 1],
+                              padding=padding,
+                              name=name)
+
+    @layer
+    def fc(self, inp, num_out, name, relu=True):
+        with tf.variable_scope(name):
+            input_shape = inp.get_shape()
+            if input_shape.ndims == 4:
+                # The input is spatial. Vectorize it first.
+                dim = 1
+                for d in input_shape[1:].as_list():
+                    dim *= int(d)
+                feed_in = tf.reshape(inp, [-1, dim])
+            else:
+                feed_in, dim = (inp, input_shape[-1].value)
+            weights = self.make_var('weights', shape=[dim, num_out])
+            biases = self.make_var('biases', [num_out])
+            op = tf.nn.relu_layer if relu else tf.nn.xw_plus_b
+            fc = op(feed_in, weights, biases, name=name)
+            return fc
+
+
+    """
+    Multi dimensional softmax,
+    refer to https://github.com/tensorflow/tensorflow/issues/210
+    compute softmax along the dimension of target
+    the native softmax only supports batch_size x dimension
+    """
+    @layer
+    def softmax(self, target, axis, name=None):
+        max_axis = tf.reduce_max(target, axis, keepdims=True)
+        target_exp = tf.exp(target-max_axis)
+        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)
+        softmax = tf.div(target_exp, normalize, name)
+        return softmax
+
+class PNet(Network):
+    def setup(self):
+        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
+             .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')
+             .prelu(name='PReLU1')
+             .max_pool(2, 2, 2, 2, name='pool1')
+             .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')
+             .prelu(name='PReLU2')
+             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')
+             .prelu(name='PReLU3')
+             .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')
+             .softmax(3,name='prob1'))
+
+        (self.feed('PReLU3') #pylint: disable=no-value-for-parameter
+             .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))
+
+class RNet(Network):
+    def setup(self):
+        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
+             .conv(3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')
+             .prelu(name='prelu1')
+             .max_pool(3, 3, 2, 2, name='pool1')
+             .conv(3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')
+             .prelu(name='prelu2')
+             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
+             .conv(2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')
+             .prelu(name='prelu3')
+             .fc(128, relu=False, name='conv4')
+             .prelu(name='prelu4')
+             .fc(2, relu=False, name='conv5-1')
+             .softmax(1,name='prob1'))
+
+        (self.feed('prelu4') #pylint: disable=no-value-for-parameter
+             .fc(4, relu=False, name='conv5-2'))
+
+class ONet(Network):
+    def setup(self):
+        (self.feed('data') #pylint: disable=no-value-for-parameter, no-member
+             .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')
+             .prelu(name='prelu1')
+             .max_pool(3, 3, 2, 2, name='pool1')
+             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')
+             .prelu(name='prelu2')
+             .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
+             .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')
+             .prelu(name='prelu3')
+             .max_pool(2, 2, 2, 2, name='pool3')
+             .conv(2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')
+             .prelu(name='prelu4')
+             .fc(256, relu=False, name='conv5')
+             .prelu(name='prelu5')
+             .fc(2, relu=False, name='conv6-1')
+             .softmax(1, name='prob1'))
+
+        (self.feed('prelu5') #pylint: disable=no-value-for-parameter
+             .fc(4, relu=False, name='conv6-2'))
+
+        (self.feed('prelu5') #pylint: disable=no-value-for-parameter
+             .fc(10, relu=False, name='conv6-3'))
+
+def create_mtcnn(sess, model_path):
+    if not model_path:
+        model_path,_ = os.path.split(os.path.realpath(__file__))
+
+    with tf.variable_scope('pnet'):
+        data = tf.placeholder(tf.float32, (None,None,None,3), 'input')
+        pnet = PNet({'data':data})
+        pnet.load(os.path.join(model_path, 'det1.npy'), sess)
+    with tf.variable_scope('rnet'):
+        data = tf.placeholder(tf.float32, (None,24,24,3), 'input')
+        rnet = RNet({'data':data})
+        rnet.load(os.path.join(model_path, 'det2.npy'), sess)
+    with tf.variable_scope('onet'):
+        data = tf.placeholder(tf.float32, (None,48,48,3), 'input')
+        onet = ONet({'data':data})
+        onet.load(os.path.join(model_path, 'det3.npy'), sess)
+
+    pnet_fun = lambda img : sess.run(('pnet/conv4-2/BiasAdd:0', 'pnet/prob1:0'), feed_dict={'pnet/input:0':img})
+    rnet_fun = lambda img : sess.run(('rnet/conv5-2/conv5-2:0', 'rnet/prob1:0'), feed_dict={'rnet/input:0':img})
+    onet_fun = lambda img : sess.run(('onet/conv6-2/conv6-2:0', 'onet/conv6-3/conv6-3:0', 'onet/prob1:0'), feed_dict={'onet/input:0':img})
+    return pnet_fun, rnet_fun, onet_fun
+
+def detect_face(img, minsize, pnet, rnet, onet, threshold, factor):
+    """Detects faces in an image, and returns bounding boxes and points for them.
+    img: input image
+    minsize: minimum faces' size
+    pnet, rnet, onet: caffemodel
+    threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
+    factor: the factor used to create a scaling pyramid of face sizes to detect in the image.
+    """
+    factor_count=0
+    total_boxes=np.empty((0,9))
+    # points=np.empty(0)
+    h=img.shape[0]
+    w=img.shape[1]
+    minl=np.amin([h, w])
+    m=12.0/minsize
+    minl=minl*m
+    # create scale pyramid
+    scales=[]
+    while minl>=12:
+        scales += [m*np.power(factor, factor_count)]
+        minl = minl*factor
+        factor_count += 1
+
+    # # # # # # # # # # # # #
+    # first stage - fast proposal network (pnet) to obtain face candidates
+    # # # # # # # # # # # # #
+
+    for scale in scales:
+        hs=int(np.ceil(h*scale))
+        ws=int(np.ceil(w*scale))
+        im_data = imresample(img, (hs, ws))
+        im_data = (im_data-127.5)*0.0078125
+        img_x = np.expand_dims(im_data, 0)
+        img_y = np.transpose(img_x, (0,2,1,3))
+        out = pnet(img_y)
+        out0 = np.transpose(out[0], (0,2,1,3))
+        out1 = np.transpose(out[1], (0,2,1,3))
+
+        boxes, _ = generateBoundingBox(out1[0,:,:,1].copy(), out0[0,:,:,:].copy(), scale, threshold[0])
+
+        # inter-scale nms
+        pick = nms(boxes.copy(), 0.5, 'Union')
+        if boxes.size>0 and pick.size>0:
+            boxes = boxes[pick,:]
+            total_boxes = np.append(total_boxes, boxes, axis=0)
+
+    numbox = total_boxes.shape[0]
+    if numbox>0:
+        pick = nms(total_boxes.copy(), 0.7, 'Union')
+        total_boxes = total_boxes[pick,:]
+        regw = total_boxes[:,2]-total_boxes[:,0]
+        regh = total_boxes[:,3]-total_boxes[:,1]
+        qq1 = total_boxes[:,0]+total_boxes[:,5]*regw
+        qq2 = total_boxes[:,1]+total_boxes[:,6]*regh
+        qq3 = total_boxes[:,2]+total_boxes[:,7]*regw
+        qq4 = total_boxes[:,3]+total_boxes[:,8]*regh
+        total_boxes = np.transpose(np.vstack([qq1, qq2, qq3, qq4, total_boxes[:,4]]))
+        total_boxes = rerec(total_boxes.copy())
+        total_boxes[:,0:4] = np.fix(total_boxes[:,0:4]).astype(np.int32)
+        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)
+
+    numbox = total_boxes.shape[0]
+
+    # # # # # # # # # # # # #
+    # second stage - refinement of face candidates with rnet
+    # # # # # # # # # # # # #
+
+    if numbox>0:
+        tempimg = np.zeros((24,24,3,numbox))
+        for k in range(0,numbox):
+            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))
+            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]
+            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:
+                tempimg[:,:,:,k] = imresample(tmp, (24, 24))
+            else:
+                return np.empty()
+        tempimg = (tempimg-127.5)*0.0078125
+        tempimg1 = np.transpose(tempimg, (3,1,0,2))
+        out = rnet(tempimg1)
+        out0 = np.transpose(out[0])
+        out1 = np.transpose(out[1])
+        score = out1[1,:]
+        ipass = np.where(score>threshold[1])
+        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])
+        mv = out0[:,ipass[0]]
+        if total_boxes.shape[0]>0:
+            pick = nms(total_boxes, 0.7, 'Union')
+            total_boxes = total_boxes[pick,:]
+            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv[:,pick]))
+            total_boxes = rerec(total_boxes.copy())
+
+    numbox = total_boxes.shape[0]
+
+    # # # # # # # # # # # # #
+    # third stage - further refinement and facial landmarks positions with onet
+    # NB: Facial landmarks code commented out for faceswap
+    # # # # # # # # # # # # #
+
+    if numbox>0:
+        # third stage
+        total_boxes = np.fix(total_boxes).astype(np.int32)
+        dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph = pad(total_boxes.copy(), w, h)
+        tempimg = np.zeros((48,48,3,numbox))
+        for k in range(0,numbox):
+            tmp = np.zeros((int(tmph[k]),int(tmpw[k]),3))
+            tmp[dy[k]-1:edy[k],dx[k]-1:edx[k],:] = img[y[k]-1:ey[k],x[k]-1:ex[k],:]
+            if tmp.shape[0]>0 and tmp.shape[1]>0 or tmp.shape[0]==0 and tmp.shape[1]==0:
+                tempimg[:,:,:,k] = imresample(tmp, (48, 48))
+            else:
+                return np.empty()
+        tempimg = (tempimg-127.5)*0.0078125
+        tempimg1 = np.transpose(tempimg, (3,1,0,2))
+        out = onet(tempimg1)
+        out0 = np.transpose(out[0])
+        out1 = np.transpose(out[1])
+        out2 = np.transpose(out[2])
+        score = out2[1,:]
+        # points = out1
+        ipass = np.where(score>threshold[2])
+        # points = points[:,ipass[0]]
+        total_boxes = np.hstack([total_boxes[ipass[0],0:4].copy(), np.expand_dims(score[ipass].copy(),1)])
+        mv = out0[:,ipass[0]]
+
+        w = total_boxes[:,2]-total_boxes[:,0]+1
+        h = total_boxes[:,3]-total_boxes[:,1]+1
+        # points[0:5,:] = np.tile(w,(5, 1))*points[0:5,:] + np.tile(total_boxes[:,0],(5, 1))-1
+        # points[5:10,:] = np.tile(h,(5, 1))*points[5:10,:] + np.tile(total_boxes[:,1],(5, 1))-1
+        if total_boxes.shape[0]>0:
+            total_boxes = bbreg(total_boxes.copy(), np.transpose(mv))
+            pick = nms(total_boxes.copy(), 0.7, 'Min')
+            total_boxes = total_boxes[pick,:]
+            # points = points[:,pick]
+
+    # return total_boxes, points
+    return total_boxes
+
+# function [boundingbox] = bbreg(boundingbox,reg)
+def bbreg(boundingbox,reg):
+    """Calibrate bounding boxes"""
+    if reg.shape[1]==1:
+        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))
+
+    w = boundingbox[:,2]-boundingbox[:,0]+1
+    h = boundingbox[:,3]-boundingbox[:,1]+1
+    b1 = boundingbox[:,0]+reg[:,0]*w
+    b2 = boundingbox[:,1]+reg[:,1]*h
+    b3 = boundingbox[:,2]+reg[:,2]*w
+    b4 = boundingbox[:,3]+reg[:,3]*h
+    boundingbox[:,0:4] = np.transpose(np.vstack([b1, b2, b3, b4 ]))
+    return boundingbox
+
+def generateBoundingBox(imap, reg, scale, t):
+    """Use heatmap to generate bounding boxes"""
+    stride=2
+    cellsize=12
+
+    imap = np.transpose(imap)
+    dx1 = np.transpose(reg[:,:,0])
+    dy1 = np.transpose(reg[:,:,1])
+    dx2 = np.transpose(reg[:,:,2])
+    dy2 = np.transpose(reg[:,:,3])
+    y, x = np.where(imap >= t)
+    if y.shape[0]==1:
+        dx1 = np.flipud(dx1)
+        dy1 = np.flipud(dy1)
+        dx2 = np.flipud(dx2)
+        dy2 = np.flipud(dy2)
+    score = imap[(y,x)]
+    reg = np.transpose(np.vstack([ dx1[(y,x)], dy1[(y,x)], dx2[(y,x)], dy2[(y,x)] ]))
+    if reg.size==0:
+        reg = np.empty((0,3))
+    bb = np.transpose(np.vstack([y,x]))
+    q1 = np.fix((stride*bb+1)/scale)
+    q2 = np.fix((stride*bb+cellsize-1+1)/scale)
+    boundingbox = np.hstack([q1, q2, np.expand_dims(score,1), reg])
+    return boundingbox, reg
+
+# function pick = nms(boxes,threshold,type)
+def nms(boxes, threshold, method):
+    if boxes.size==0:
+        return np.empty((0,3))
+    x1 = boxes[:,0]
+    y1 = boxes[:,1]
+    x2 = boxes[:,2]
+    y2 = boxes[:,3]
+    s = boxes[:,4]
+    area = (x2-x1+1) * (y2-y1+1)
+    I = np.argsort(s)
+    pick = np.zeros_like(s, dtype=np.int16)
+    counter = 0
+    while I.size>0:
+        i = I[-1]
+        pick[counter] = i
+        counter += 1
+        idx = I[0:-1]
+        xx1 = np.maximum(x1[i], x1[idx])
+        yy1 = np.maximum(y1[i], y1[idx])
+        xx2 = np.minimum(x2[i], x2[idx])
+        yy2 = np.minimum(y2[i], y2[idx])
+        w = np.maximum(0.0, xx2-xx1+1)
+        h = np.maximum(0.0, yy2-yy1+1)
+        inter = w * h
+        if method is 'Min':
+            o = inter / np.minimum(area[i], area[idx])
+        else:
+            o = inter / (area[i] + area[idx] - inter)
+        I = I[np.where(o<=threshold)]
+    pick = pick[0:counter]
+    return pick
+
+# function [dy edy dx edx y ey x ex tmpw tmph] = pad(total_boxes,w,h)
+def pad(total_boxes, w, h):
+    """Compute the padding coordinates (pad the bounding boxes to square)"""
+    tmpw = (total_boxes[:,2]-total_boxes[:,0]+1).astype(np.int32)
+    tmph = (total_boxes[:,3]-total_boxes[:,1]+1).astype(np.int32)
+    numbox = total_boxes.shape[0]
+
+    dx = np.ones((numbox), dtype=np.int32)
+    dy = np.ones((numbox), dtype=np.int32)
+    edx = tmpw.copy().astype(np.int32)
+    edy = tmph.copy().astype(np.int32)
+
+    x = total_boxes[:,0].copy().astype(np.int32)
+    y = total_boxes[:,1].copy().astype(np.int32)
+    ex = total_boxes[:,2].copy().astype(np.int32)
+    ey = total_boxes[:,3].copy().astype(np.int32)
+
+    tmp = np.where(ex>w)
+    edx.flat[tmp] = np.expand_dims(-ex[tmp]+w+tmpw[tmp],1)
+    ex[tmp] = w
+
+    tmp = np.where(ey>h)
+    edy.flat[tmp] = np.expand_dims(-ey[tmp]+h+tmph[tmp],1)
+    ey[tmp] = h
+
+    tmp = np.where(x<1)
+    dx.flat[tmp] = np.expand_dims(2-x[tmp],1)
+    x[tmp] = 1
+
+    tmp = np.where(y<1)
+    dy.flat[tmp] = np.expand_dims(2-y[tmp],1)
+    y[tmp] = 1
+
+    return dy, edy, dx, edx, y, ey, x, ex, tmpw, tmph
+
+# function [bboxA] = rerec(bboxA)
+def rerec(bboxA):
+    """Convert bboxA to square."""
+    h = bboxA[:,3]-bboxA[:,1]
+    w = bboxA[:,2]-bboxA[:,0]
+    l = np.maximum(w, h)
+    bboxA[:,0] = bboxA[:,0]+w*0.5-l*0.5
+    bboxA[:,1] = bboxA[:,1]+h*0.5-l*0.5
+    bboxA[:,2:4] = bboxA[:,0:2] + np.transpose(np.tile(l,(2,1)))
+    return bboxA
+
+def imresample(img, sz):
+    im_data = cv2.resize(img, (sz[1], sz[0]), interpolation=cv2.INTER_AREA) #@UndefinedVariable
+    return im_data
+
diff --git a/lib/face_alignment/vram_allocation.py b/lib/face_alignment/vram_allocation.py
new file mode 100644
index 0000000..57f2846
--- /dev/null
+++ b/lib/face_alignment/vram_allocation.py
@@ -0,0 +1,124 @@
+#!/usr/bin python3
+""" GPU VRAM allocator calculations """
+
+from lib.gpu_stats import GPUStats
+
+
+class GPUMem(object):
+    """ Sets the scale to factor for dlib images
+        and the ratio of vram to use for tensorflow """
+
+    def __init__(self):
+        self.verbose = False
+        self.output_shown = False
+        self.stats = GPUStats()
+        self.vram_free = None
+        self.vram_total = None
+        self.scale_to = None
+
+        self.device = self.set_device()
+
+        if self.device == -1:
+            return
+
+        self.vram_total = self.stats.vram[self.device]
+        self.get_available_vram()
+
+    def set_device(self):
+        """ Set the default device """
+        if self.stats.device_count == 0:
+            return -1
+        return 0
+        # TF selects first device, so this is used for stats
+        # TODO select and use device with most available VRAM
+        # TODO create virtual devices/allow multiple GPUs for
+        # parallel processing
+
+    def set_device_with_max_free_vram(self):
+        """ Set the device with the most available free vram """
+        # TODO Implement this to select the device with most available VRAM
+        free_mem = self.stats.get_free()
+        self.vram_free = max(free_mem)
+        self.device = free_mem.index(self.vram_free)
+
+    def get_available_vram(self):
+        """ Recalculate the available vram """
+        free_mem = self.stats.get_free()
+        self.vram_free = free_mem[self.device]
+        if self.verbose:
+            print("GPU VRAM free:    {}".format(self.vram_free))
+
+    def output_stats(self):
+        """ Output stats in verbose mode """
+        if self.output_shown or not self.verbose:
+            return
+        print("\n----- Initial GPU Stats -----")
+        self.stats.print_info()
+        print("GPU VRAM free:    {}".format(self.vram_free))
+        print("-----------------------------\n")
+        self.output_shown = True
+
+    def get_tensor_gpu_ratio(self):
+        """ Set the ratio of GPU memory to use for tensorflow session for
+            keras points predictor.
+
+            Ideally 2304MB is required, but will run with less
+            (with warnings).
+
+            This is only required if running with DLIB. MTCNN will share
+            the tensorflow session. """
+        if self.vram_free < 2030:
+            ratio = 1024.0 / self.vram_total
+        elif self.vram_free < 3045:
+            ratio = 1560.0 / self.vram_total
+        elif self.vram_free < 4060:
+            ratio = 2048.0 / self.vram_total
+        else:
+            ratio = 2304.0 / self.vram_total
+
+        return ratio
+
+    def set_scale_to(self, detector):
+        """ Set the size to scale images down to for specific detector
+            and available VRAM
+
+            DLIB VRAM allocation is linear to pixel count
+
+            MTCNN is weird. Not linear at low levels,
+            then fairly linear up to 3360x1890 then
+            requirements drop again.
+            As 3360x1890 is hi-res, just this scale is
+            used for calculating image scaling """
+
+        # MTCNN VRAM Usage Stats
+        # Crudely Calculated at default values
+        # The formula may need ammending, but it should
+        # work for most use cases
+        # 480x270 = 267.56 MB
+        # 960x540 = 333.18 MB
+        # 1280x720 = 592.32 MB
+        # 1440x810 = 746.56 MB
+        # 1920x1080 = 1.30 GB
+        # 2400x1350 = 2.03 GB
+        # 2880x1620 = 2.93 GB
+        # 3360x1890 = 3.98 GB
+        # 3840x2160 = 2.62 GB <--??
+        # 4280x2800 = 3.69 GB
+
+        detector = "dlib" if detector in ("dlib-cnn",
+                                          "dlib-hog",
+                                          "dlib-all") else detector
+        buffer = 64  # 64MB overhead buffer
+        gradient = 3483.2 / 9651200  # MTCNN
+        constant = 1.007533156  # MTCNN
+        if detector == "dlib":
+            self.get_available_vram()
+            gradient = 213 / 524288
+            constant = 307
+
+        free_mem = self.vram_free - buffer
+        self.scale_to = int((free_mem - constant) / gradient)
+
+        if self.scale_to < 4097:
+            raise ValueError("Images would be shrunk too much "
+                             "for successful extraction")
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 5ce4eb2..3f56cf2 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -1,13 +1,33 @@
-from lib import FaceLandmarksExtractor
+#!/usr/bin python3
+""" Face and landmarks detection for faceswap.py """
+
+from lib import face_alignment
+
+
+def detect_faces(frame, detector, verbose, rotation=0, mtcnn_kwargs=None):
+    """ Detect faces and draw landmarks in an image """
+    face_detect = face_alignment.Extract(frame,
+                                         detector,
+                                         mtcnn_kwargs,
+                                         verbose)
+    for face in face_detect.landmarks:
+        ax_x, ax_y = face[0][0], face[0][1]
+        right, bottom = face[0][2], face[0][3]
+        landmarks = face[1]
+
+        yield DetectedFace(frame[ax_y: bottom, ax_x: right],
+                           rotation,
+                           ax_x,
+                           right - ax_x,
+                           ax_y,
+                           bottom - ax_y,
+                           landmarksXY=landmarks)
 
-def detect_faces(frame, detector, verbose, rotation=0):
-    fd = FaceLandmarksExtractor.extract (frame, detector, verbose)
-    for face in fd:
-        x, y, right, bottom, landmarks = face[0][0], face[0][1], face[0][2], face[0][3], face[1]
-        yield DetectedFace(frame[y: bottom, x: right], rotation, x, right - x, y, bottom - y, landmarksXY=landmarks)
 
 class DetectedFace(object):
-    def __init__(self, image=None, r=0, x=None, w=None, y=None, h=None, landmarksXY=None):
+    """ Detected face and landmark information """
+    def __init__(self, image=None, r=0, x=None,
+                 w=None, y=None, h=None, landmarksXY=None):
         self.image = image
         self.r = r
         self.x = x
@@ -16,5 +36,6 @@ class DetectedFace(object):
         self.h = h
         self.landmarksXY = landmarksXY
 
-    def landmarksAsXY(self):
+    def landmarks_as_xy(self):
+        """ Landmarks as XY """
         return self.landmarksXY
diff --git a/lib/gpu_stats.py b/lib/gpu_stats.py
new file mode 100644
index 0000000..2af71fd
--- /dev/null
+++ b/lib/gpu_stats.py
@@ -0,0 +1,107 @@
+#!/usr/bin python3
+""" Information on available Nvidia GPUs """
+
+import pynvml
+
+
+class GPUStats(object):
+    """ Holds information about system GPU(s) """
+    def __init__(self):
+        self.verbose = False
+
+        self.initialized = False
+        self.device_count = 0
+        self.handles = None
+        self.driver = None
+        self.devices = None
+        self.vram = None
+
+        self.initialize()
+
+        if self.device_count == 0:
+            return
+
+        self.driver = self.get_driver()
+        self.devices = self.get_devices()
+        self.vram = self.get_vram()
+
+        self.shutdown()
+
+    def initialize(self):
+        """ Initialize pynvml """
+        if not self.initialized:
+            try:
+                pynvml.nvmlInit()
+            except pynvml.NVMLError_LibraryNotFound:
+                self.initialized = True
+                return
+            self.initialized = True
+            self.get_device_count()
+            self.get_handles()
+
+    def shutdown(self):
+        """ Shutdown pynvml """
+        if self.initialized:
+            self.handles = None
+            pynvml.nvmlShutdown()
+            self.initialized = False
+
+    def get_device_count(self):
+        """ Return count of Nvidia devices """
+        try:
+            self.device_count = pynvml.nvmlDeviceGetCount()
+        except pynvml.NVMLError:
+            self.device_count = 0
+
+    def get_handles(self):
+        """ Return all listed Nvidia handles """
+        self.handles = [pynvml.nvmlDeviceGetHandleByIndex(i)
+                        for i in range(self.device_count)]
+
+    @staticmethod
+    def get_driver():
+        """ Get the driver version """
+        try:
+            driver = pynvml.nvmlSystemGetDriverVersion().decode("utf-8")
+        except pynvml.NVMLError:
+            driver = "No Nvidia driver found"
+        return driver
+
+    def get_devices(self):
+        """ Return total vram in megabytes per device """
+        vram = [pynvml.nvmlDeviceGetName(handle).decode("utf-8")
+                for handle in self.handles]
+        return vram
+
+    def get_vram(self):
+        """ Return total vram in megabytes per device """
+        vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).total / (1024 * 1024)
+                for handle in self.handles]
+        return vram
+
+    def get_used(self):
+        """ Return the vram in use """
+        self.initialize()
+        vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).used / (1024 * 1024)
+                for handle in self.handles]
+        self.shutdown()
+
+        if self.verbose:
+            print("GPU VRAM used:    {}".format(vram))
+
+        return vram
+
+    def get_free(self):
+        """ Return the vram available """
+        self.initialize()
+        vram = [pynvml.nvmlDeviceGetMemoryInfo(handle).free / (1024 * 1024)
+                for handle in self.handles]
+        self.shutdown()
+        return vram
+
+    def print_info(self):
+        """ Output GPU info in verbose mode """
+        print("GPU Driver:       {}".format(self.driver))
+        print("GPU Device count: {}".format(self.device_count))
+        print("GPU Devices:      {}".format(self.devices))
+        print("GPU VRAM:         {}".format(self.vram))
diff --git a/lib/gui/command.py b/lib/gui/command.py
index b53a359..f4b5889 100644
--- a/lib/gui/command.py
+++ b/lib/gui/command.py
@@ -172,6 +172,8 @@ class OptionControl(object):
         ctlhelp = ". ".join(i.capitalize() for i in ctlhelp.split(". "))
         ctlhelp = ctltitle + " - " + ctlhelp
         dflt = self.option.get("default", "")
+        if self.option.get("nargs", None) and isinstance(dflt, (list, tuple)):
+            dflt = ' '.join(str(val) for val in dflt)
         if ctl == ttk.Checkbutton:
             dflt = self.option.get("default", False)
         choices = self.option["choices"] if ctl == ttk.Combobox else None
diff --git a/lib/gui/options.py b/lib/gui/options.py
index 11a4d8d..80780d9 100644
--- a/lib/gui/options.py
+++ b/lib/gui/options.py
@@ -157,6 +157,9 @@ class CliOptions(object):
         for option in self.options_to_process(command):
             default = option.get("default", "")
             default = "" if default is None else default
+            if (option.get("nargs", None)
+                    and isinstance(default, (list, tuple))):
+                default = ' '.join(str(val) for val in default)
             option["value"].set(default)
 
     def clear(self, command=None):
diff --git a/plugins/Convert_Masked.py b/plugins/Convert_Masked.py
index 117398b..1e516fe 100644
--- a/plugins/Convert_Masked.py
+++ b/plugins/Convert_Masked.py
@@ -37,7 +37,7 @@ class Convert():
 
         new_face = self.get_new_face(image,mat,size)
 
-        image_mask = self.get_image_mask( image, new_face, face_detected.landmarksAsXY(), mat, image_size )
+        image_mask = self.get_image_mask( image, new_face, face_detected.landmarks_as_xy(), mat, image_size )
 
         return self.apply_new_face(image, new_face, image_mask, mat, image_size, size)
 
diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
index dd873df..d6748b3 100644
--- a/plugins/Model_OriginalHighRes/Model.py
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -26,7 +26,7 @@ from lib.PixelShuffler import PixelShuffler
 import lib.Serializer
 from lib.utils import backup_file
 
-from . import __version__
+from . import __version__    
 from .instance_normalization import InstanceNormalization
 
 
@@ -41,18 +41,14 @@ mswindows = sys.platform=="win32"
 
 class EncoderType(enum.Enum):
     ORIGINAL = "original"
-    SHAOANLU = "shaoanlu"
-    
-    
-ENCODER = EncoderType.ORIGINAL
-
-         
-conv_init = RandomNormal(0, 0.02)
+    SHAOANLU = "shaoanlu"    
+            
 
 def inst_norm():
     return InstanceNormalization()
 
-ENCODER = EncoderType.ORIGINAL 
+
+ENCODER = EncoderType.ORIGINAL
 
 
 hdf = {'encoderH5': 'encoder_{version_str}{ENCODER.value}.h5'.format(**vars()),
@@ -107,8 +103,9 @@ class Model():
             self.autoencoder_A = multi_gpu_model( self.autoencoder_A , self.gpus)
             self.autoencoder_B = multi_gpu_model( self.autoencoder_B , self.gpus)
         
+        
         self.autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')
-        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')
+        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')                    
         
         
     def load(self, swapped):        
@@ -151,21 +148,21 @@ class Model():
     
     def conv(self, filters, kernel_size=5, strides=2, **kwargs):
         def block(x):
-            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)         
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)         
             x = LeakyReLU(0.1)(x)
             return x
         return block   
 
-    def conv_sep2(self, filters, kernel_size=5, strides=2, use_instance_norm=True, **kwargs):
+    def conv_sep(self, filters, kernel_size=5, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
             x = Activation("relu")(x)
             return x    
         return block 
         
     def conv_sep3(self, filters, kernel_size=3, strides=2, use_instance_norm=True, **kwargs):
         def block(x):
-            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=conv_init, padding='same', **kwargs)(x)        
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)        
             if use_instance_norm:
                 x = inst_norm()(x)
             x = Activation("relu")(x)
@@ -174,7 +171,8 @@ class Model():
     
     def upscale(self, filters, **kwargs):
         def block(x):
-            x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)
+            x = Conv2D(filters * 4, kernel_size=3, padding='same',
+                       kernel_initializer=RandomNormal(0, 0.02))(x)
             x = LeakyReLU(0.1)(x)
             x = PixelShuffler()(x)
             return x
@@ -182,7 +180,8 @@ class Model():
     
     def upscale_sep3(self, filters, use_instance_norm=True, **kwargs):
         def block(x):
-            x = Conv2D(filters*4, kernel_size=3, use_bias=False, kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
+            x = Conv2D(filters*4, kernel_size=3, use_bias=False, 
+                       kernel_initializer=RandomNormal(0, 0.02), padding='same', **kwargs)(x)
             if use_instance_norm:
                 x = inst_norm()(x)
             x = LeakyReLU(0.1)(x)
@@ -196,13 +195,13 @@ class Model():
         in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
 
         x = self.conv(in_conv_filters)(impt)
-        x = self.conv_sep2(256)(x)
+        x = self.conv_sep(256)(x)
         x = self.conv(512)(x)
-        x = self.conv_sep2(1024)(x)
+        x = self.conv_sep(1024)(x)
         
         dense_shape = self.IMAGE_SHAPE[0] // 16         
-        x = Dense(self.ENCODER_DIM)(Flatten()(x))
-        x = Dense(dense_shape * dense_shape * 512)(x)
+        x = Dense(self.ENCODER_DIM, kernel_initializer=RandomNormal(0, 0.02))(Flatten()(x))
+        x = Dense(dense_shape * dense_shape * 512, kernel_initializer=RandomNormal(0, 0.02))(x)
         x = Reshape((dense_shape, dense_shape, 512))(x)
         x = self.upscale(512)(x)
         
@@ -214,7 +213,7 @@ class Model():
                 
         in_conv_filters = self.IMAGE_SHAPE[0] if self.IMAGE_SHAPE[0] <= 128 else 128 + (self.IMAGE_SHAPE[0]-128)//4
         
-        x = Conv2D(in_conv_filters, kernel_size=5, kernel_initializer=conv_init, use_bias=False, padding="same")(impt)
+        x = Conv2D(in_conv_filters, kernel_size=5, use_bias=False, padding="same")(impt)
         x = self.conv_sep3(in_conv_filters+32, use_instance_norm=False)(x)
         x = self.conv_sep3(256)(x)        
         x = self.conv_sep3(512)(x)
@@ -233,9 +232,9 @@ class Model():
         decoder_shape = self.IMAGE_SHAPE[0]//8        
         inpt = Input(shape=(decoder_shape, decoder_shape, 512))
         
-        x = self.upscale(384, kernel_initializer=RandomNormal(0, 0.02))(inpt)
-        x = self.upscale(256-32, kernel_initializer=RandomNormal(0, 0.02))(x)
-        x = self.upscale(self.IMAGE_SHAPE[0], kernel_initializer=RandomNormal(0, 0.02))(x)
+        x = self.upscale(384)(inpt)
+        x = self.upscale(256-32)(x)
+        x = self.upscale(self.IMAGE_SHAPE[0])(x)
         
         x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
         
@@ -255,10 +254,7 @@ class Model():
         return KerasModel(inpt, x)    
 
 
-    def save_weights(self):
-        from threading import Thread
-        from time import sleep
-        
+    def save_weights(self):        
         model_dir = str(self.model_dir)
         
         try:
@@ -276,12 +272,12 @@ class Model():
                      })
                 fp.write(state_json.encode('utf-8'))
         except IOError as e:
-            pass                               
+            print(e.strerror)                   
         
         print('\nsaving model weights', end='', flush=True)        
-        from concurrent.futures import ThreadPoolExecutor, as_completed
         
-        # thought maybe I/O bound, sometimes saving in parallel is faster
+        from concurrent.futures import ThreadPoolExecutor, as_completed        
+        
         with ThreadPoolExecutor(max_workers=4) as executor:
             futures = [executor.submit(getattr(self, mdl_name.rstrip('H5')).save_weights, str(self.model_dir / mdl_H5_fn)) for mdl_name, mdl_H5_fn in hdf.items()]
             for future in as_completed(futures):
@@ -300,7 +296,8 @@ class Model():
         try:
             return self._model_name
         except AttributeError:
-            self._model_name = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
+            import inspect
+            self._model_name = os.path.dirname(inspect.getmodule(self).__file__).rsplit("_", 1)[1]            
         return self._model_name
              
     
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
index 41eddfb..a7a7eb9 100644
--- a/plugins/Model_OriginalHighRes/Trainer.py
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -39,11 +39,15 @@ class Trainer():
         loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)
         
         self.model._epoch_no += 1        
-                    
-        print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
-            time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A, loss_B),
-            end='\r')
-        
+                 
+        if isinstance(loss_A, (list, tuple)):
+            print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
+                time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A[1], loss_B[1]),
+                end='\r')
+        else:
+            print("[{0}] [#{1:05d}] [{2:.3f}s] loss_A: {3:.5f}, loss_B: {4:.5f}".format(
+                time.strftime("%H:%M:%S"), self.model._epoch_no, self._clock()-when, loss_A, loss_B),
+                end='\r')         
 
         if viewer is not None:
             viewer(self.show_sample(target_A[0:8], target_B[0:8]), "training using {}, bs={}".format(self.model, self.batch_size))
@@ -55,13 +59,14 @@ class Trainer():
             self.model.autoencoder_A.predict(test_A),
             self.model.autoencoder_B.predict(test_A),
         ], axis=1)
+        
         figure_B = numpy.stack([
             test_B,
             self.model.autoencoder_B.predict(test_B),
             self.model.autoencoder_A.predict(test_B),
         ], axis=1)
 
-        if test_A.shape[0] % 2 == 1:
+        if (test_A.shape[0] % 2)!=0:
             figure_A = numpy.concatenate ([figure_A, numpy.expand_dims(figure_A[0],0) ])
             figure_B = numpy.concatenate ([figure_B, numpy.expand_dims(figure_B[0],0) ])
 
diff --git a/plugins/Model_OriginalHighRes/__init__.py b/plugins/Model_OriginalHighRes/__init__.py
index f1322c9..e6601b4 100644
--- a/plugins/Model_OriginalHighRes/__init__.py
+++ b/plugins/Model_OriginalHighRes/__init__.py
@@ -5,3 +5,4 @@ __author__ = """Based on https://reddit.com/u/deepfakes/"""
 from ._version import __version__
 from .Model import Model
 from .Trainer import Trainer
+
diff --git a/requirements.txt b/requirements.txt
index 8fc8ff5..8bf58f6 100755
--- a/requirements.txt
+++ b/requirements.txt
@@ -10,6 +10,7 @@ dlib
 tqdm
 matplotlib==2.2.2
 ffmpy==0.2.2
+nvidia-ml-py3
 
 # tensorflow is included within the docker image.
 # If you are looking for dependencies for a manual install,
diff --git a/scripts/extract.py b/scripts/extract.py
index 530b9d4..0336408 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -6,11 +6,13 @@ import sys
 from pathlib import Path
 
 from tqdm import tqdm
-tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning
 
+from lib.gpu_stats import GPUStats
 from lib.multithreading import pool_process
 from scripts.fsmedia import Alignments, Faces, Images, Utils
 
+tqdm.monitor_interval = 0  # workaround for TqdmSynchronisationWarning
+
 
 class Extract(object):
     """ The extract process. """
@@ -31,7 +33,11 @@ class Extract(object):
         print('Starting, this may take a while...')
         Utils.set_verbosity(self.args.verbose)
 
-        if hasattr(self.args, 'processes') and self.args.processes > 1:
+        if self.args.multiprocess and GPUStats().device_count == 0:
+            # TODO Checking that there is no available GPU is not
+            # necessarily an indicator of whether the user is actually
+            # using the CPU. Maybe look to implement further checks on
+            # dlib/tensorflow compilations
             self.extract_multi_process()
         else:
             self.extract_single_process()
@@ -52,11 +58,12 @@ class Extract(object):
 
     def extract_multi_process(self):
         """ Run the extraction on the correct number of processes """
-        for filename, faces in tqdm(pool_process(self.process_single_image,
-                                                 self.images.input_images,
-                                                 processes=self.args.processes),
-                                    total=self.images.images_found,
-                                    file=sys.stdout):
+        for filename, faces in tqdm(
+                pool_process(
+                    self.process_single_image,
+                    self.images.input_images),
+                total=self.images.images_found,
+                file=sys.stdout):
             self.faces.num_faces_detected += 1
             self.faces.faces_detected[os.path.basename(filename)] = faces
 
@@ -75,35 +82,47 @@ class Extract(object):
                 faces = self.faces.get_faces(currentimage, angle)
                 process_faces = [(idx, face) for idx, face in faces]
                 if process_faces and angle != 0 and self.args.verbose:
-                    print("found face(s) by rotating image {} degrees".format(angle))
+                    print("found face(s) by rotating image "
+                          "{} degrees".format(angle))
                 if process_faces:
                     break
 
-            final_faces = [self.process_single_face(idx, face, filename, currentimage)
+            final_faces = [self.process_single_face(idx,
+                                                    face,
+                                                    filename,
+                                                    currentimage)
                            for idx, face in process_faces]
 
             retval = filename, final_faces
         except Exception as err:
             if self.args.verbose:
-                print("Failed to extract from image: {}. Reason: {}".format(filename, err))
+                print("Failed to extract from image: "
+                      "{}. Reason: {}".format(filename, err))
         return retval
 
     def process_single_face(self, idx, face, filename, image):
         """ Perform processing on found faces """
-        output_file = self.output_dir / Path(filename).stem if self.export_face else None
+        output_file = self.output_dir / Path(
+            filename).stem if self.export_face else None
 
         self.faces.draw_landmarks_on_face(face, image)
 
-        resized_face, t_mat = self.faces.extractor.extract(image,
-                                                           face,
-                                                           256,
-                                                           self.faces.align_eyes)
+        resized_face, t_mat = self.faces.extractor.extract(
+            image,
+            face,
+            256,
+            self.faces.align_eyes)
 
-        blurry_file = self.faces.detect_blurry_faces(face, t_mat, resized_face, filename)
+        blurry_file = self.faces.detect_blurry_faces(face,
+                                                     t_mat,
+                                                     resized_face,
+                                                     filename)
         output_file = blurry_file if blurry_file else output_file
 
         if self.export_face:
-            filename = "{}_{}{}".format(str(output_file), str(idx), Path(filename).suffix)
+            filename = "{}_{}{}".format(str(output_file),
+                                        str(idx),
+                                        Path(filename).suffix)
             Utils.cv2_read_write('write', filename, resized_face)
 
         return {"r": face.r,
@@ -111,4 +130,4 @@ class Extract(object):
                 "w": face.w,
                 "y": face.y,
                 "h": face.h,
-                "landmarksXY": face.landmarksAsXY()}
+                "landmarksXY": face.landmarks_as_xy()}
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index 12ea060..124652a 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -30,10 +30,13 @@ class Utils(object):
         set_system_verbosity(lvl)
 
     @staticmethod
-    def rotate_image_by_angle(image, angle, rotated_width=None, rotated_height=None):
-        """ Rotate an image by a given angle. From:
-            https://stackoverflow.com/questions/22041699
-            This is required by both Faces and Images so placed here for now """
+    def rotate_image_by_angle(image, angle,
+                              rotated_width=None, rotated_height=None):
+        """ Rotate an image by a given angle.
+            From: https://stackoverflow.com/questions/22041699
+
+            This is required by both Faces and Images
+            so placed here for now """
         height, width = image.shape[:2]
         image_center = (width/2, height/2)
         rotation_matrix = cv2.getRotationMatrix2D(image_center, -1.*angle, 1.)
@@ -46,7 +49,9 @@ class Utils(object):
                 rotated_height = int(height*abs_cos + width*abs_sin)
         rotation_matrix[0, 2] += rotated_width/2 - image_center[0]
         rotation_matrix[1, 2] += rotated_height/2 - image_center[1]
-        return cv2.warpAffine(image, rotation_matrix, (rotated_width, rotated_height))
+        return cv2.warpAffine(image,
+                              rotation_matrix,
+                              (rotated_width, rotated_height))
 
     @staticmethod
     def cv2_read_write(action, filename, image=None):
@@ -110,7 +115,9 @@ class Images(object):
                              for angle in self.args.rotate_images.split(",")]
             if len(passed_angles) == 1:
                 rotation_step_size = passed_angles[0]
-                rotation_angles.extend(range(rotation_step_size, 360, rotation_step_size))
+                rotation_angles.extend(range(rotation_step_size,
+                                             360,
+                                             rotation_step_size))
             elif len(passed_angles) > 1:
                 rotation_angles.extend(passed_angles)
 
@@ -120,7 +127,8 @@ class Images(object):
         """ Return the images that already exist in the output directory """
         print("Output Directory: {}".format(self.args.output_dir))
 
-        if not hasattr(self.args, 'skip_existing') or not self.args.skip_existing:
+        if (not hasattr(self.args, 'skip_existing')
+                or not self.args.skip_existing):
             return None
 
         return get_image_paths(self.args.output_dir)
@@ -134,7 +142,8 @@ class Images(object):
         print("Input Directory: {}".format(self.args.input_dir))
 
         if hasattr(self.args, 'skip_existing') and self.args.skip_existing:
-            input_images = get_image_paths(self.args.input_dir, self.already_processed)
+            input_images = get_image_paths(self.args.input_dir,
+                                           self.already_processed)
             print("Excluding %s files" % len(self.already_processed))
         else:
             input_images = get_image_paths(self.args.input_dir)
@@ -148,10 +157,11 @@ class Images(object):
                 self.rotation_height, self.rotation_width = image.shape[:2]
                 image = Utils.rotate_image_by_angle(image, rotation)
             else:
-                image = Utils.rotate_image_by_angle(image,
-                                                    rotation * -1,
-                                                    rotated_width=self.rotation_width,
-                                                    rotated_height=self.rotation_height)
+                image = Utils.rotate_image_by_angle(
+                    image,
+                    rotation * -1,
+                    rotated_width=self.rotation_width,
+                    rotated_height=self.rotation_height)
         return image
 
 
@@ -160,8 +170,10 @@ class Faces(object):
     def __init__(self, arguments):
         self.args = arguments
         self.extractor = self.load_extractor()
+        self.mtcnn_kwargs = self.get_mtcnn_kwargs()
         self.filter = self.load_face_filter()
-        self.align_eyes = self.args.align_eyes if hasattr(self.args, 'align_eyes') else False
+        self.align_eyes = self.args.align_eyes if hasattr(
+            self.args, 'align_eyes') else False
         self.output_dir = get_folder(self.args.output_dir)
 
         self.faces_detected = dict()
@@ -175,6 +187,14 @@ class Faces(object):
 
         return extractor
 
+    def get_mtcnn_kwargs(self):
+        """ Add the mtcnn arguments into a kwargs dictionary """
+        mtcnn_threshold = [float(thr.strip())
+                           for thr in self.args.mtcnn_threshold]
+        return {"minsize": self.args.mtcnn_minsize,
+                "threshold": mtcnn_threshold,
+                "factor": self.args.mtcnn_scalefactor}
+
     def load_face_filter(self):
         """ Load faces to filter out of images """
         facefilter = None
@@ -182,7 +202,9 @@ class Faces(object):
                         for filter_type in ('filter', 'nfilter')]
 
         if any(filters for filters in filter_files):
-            facefilter = FaceFilter(filter_files[0], filter_files[1], self.args.ref_threshold)
+            facefilter = FaceFilter(filter_files[0],
+                                    filter_files[1],
+                                    self.args.ref_threshold)
         return facefilter
 
     def set_face_filter(self, filter_list):
@@ -194,7 +216,8 @@ class Faces(object):
             filter_files = filter_args
             if not isinstance(filter_args, list):
                 filter_files = [filter_args]
-            filter_files = list(filter(lambda fnc: Path(fnc).exists(), filter_files))
+            filter_files = list(filter(lambda fnc: Path(fnc).exists(),
+                                       filter_files))
         return filter_files
 
     def have_face(self, filename):
@@ -204,7 +227,8 @@ class Faces(object):
     def get_faces(self, image, rotation=0):
         """ Extract the faces from an image """
         faces_count = 0
-        faces = detect_faces(image, self.args.detector, self.args.verbose, rotation)
+        faces = detect_faces(image, self.args.detector, self.args.verbose,
+                             rotation=rotation, mtcnn_kwargs=self.mtcnn_kwargs)
 
         for face in faces:
             if self.filter and not self.filter.check(face):
@@ -228,7 +252,8 @@ class Faces(object):
             # Rotate the image if necessary
             if face.r != 0:
                 image = Utils.rotate_image_by_angle(image, face.r)
-            face.image = image[face.y : face.y + face.h, face.x : face.x + face.w]
+            face.image = image[face.y: face.y + face.h,
+                               face.x: face.x + face.w]
             if self.filter and not self.filter.check(face):
                 if self.args.verbose:
                     print("Skipping not recognized face!")
@@ -238,15 +263,17 @@ class Faces(object):
             self.num_faces_detected += 1
             faces_count += 1
         if faces_count > 1 and self.args.verbose:
-            print("Note: Found more than one face in an image! File: {}".format(filename))
+            print("Note: Found more than one face in "
+                  "an image! File: {}".format(filename))
             self.verify_output = True
 
     def draw_landmarks_on_face(self, face, image):
         """ Draw debug landmarks on extracted face """
-        if not hasattr(self.args, 'debug_landmarks') or not self.args.debug_landmarks:
+        if (not hasattr(self.args, 'debug_landmarks')
+                or not self.args.debug_landmarks):
             return
 
-        for (pos_x, pos_y) in face.landmarksAsXY():
+        for (pos_x, pos_y) in face.landmarks_as_xy():
             cv2.circle(image, (pos_x, pos_y), 2, (0, 0, 255), -1)
 
     def detect_blurry_faces(self, face, t_mat, resized_image, filename):
@@ -255,17 +282,26 @@ class Faces(object):
             return None
 
         blurry_file = None
-        aligned_landmarks = self.extractor.transform_points(face.landmarksAsXY(), t_mat, 256, 48)
-        feature_mask = self.extractor.get_feature_mask(aligned_landmarks / 256, 256, 48)
+        aligned_landmarks = self.extractor.transform_points(
+            face.landmarks_as_xy(),
+            t_mat,
+            256,
+            48)
+        feature_mask = self.extractor.get_feature_mask(aligned_landmarks / 256,
+                                                       256,
+                                                       48)
         feature_mask = cv2.blur(feature_mask, (10, 10))
-        isolated_face = cv2.multiply(feature_mask, resized_image.astype(float)).astype(np.uint8)
+        isolated_face = cv2.multiply(
+            feature_mask,
+            resized_image.astype(float)).astype(np.uint8)
         blurry, focus_measure = is_blurry(isolated_face, self.args.blur_thresh)
 
         if blurry:
             print("{}'s focus measure of {} was below the blur threshold, "
-                  "moving to \"blurry\"".format(Path(filename).stem, focus_measure))
-            blurry_file = get_folder(Path(self.output_dir) / Path("blurry")) / \
-                Path(filename).stem
+                  "moving to \"blurry\"".format(Path(filename).stem,
+                                                focus_measure))
+            blurry_file = get_folder(Path(self.output_dir) /
+                                     Path("blurry")) / Path(filename).stem
         return blurry_file
 
 
@@ -293,8 +329,9 @@ class Alignments(object):
         if self.args.alignments_path:
             alignfile = self.args.alignments_path
         else:
-            alignfile = os.path.join(str(self.args.input_dir),
-                                     "alignments.{}".format(self.serializer.ext))
+            alignfile = os.path.join(
+                str(self.args.input_dir),
+                "alignments.{}".format(self.serializer.ext))
         print("Alignments filepath: %s" % alignfile)
         return alignfile
 
@@ -312,7 +349,8 @@ class Alignments(object):
     def write_alignments(self, faces_detected):
         """ Write the serialized alignments file """
         if hasattr(self.args, 'skip_existing') and self.args.skip_existing:
-            faces_detected = self.load_skip_alignments(self.alignments_path, faces_detected)
+            faces_detected = self.load_skip_alignments(self.alignments_path,
+                                                       faces_detected)
 
         try:
             print("Writing alignments to: {}".format(self.alignments_path))
diff --git a/tools/cli.py b/tools/cli.py
index c6591ad..b578102 100644
--- a/tools/cli.py
+++ b/tools/cli.py
@@ -177,9 +177,9 @@ class EffmpegArgs(FaceSwapArgs):
                               "dest": "preview",
                               "default": False,
                               "help": "Uses ffplay to preview the effects of "
-                                      "the intended action. "
-                                      "This functionality is not yet fully "
-                                      "implemented."})
+                                      "actions that have a video output. "
+                                      "Currently preview does not work when "
+                                      "muxing audio."})
 
         argument_list.append({"opts": ('-q', '--quiet'),
                               "action": "store_true",
diff --git a/tools/effmpeg.py b/tools/effmpeg.py
index 5ac4c9d..a7bf9d5 100644
--- a/tools/effmpeg.py
+++ b/tools/effmpeg.py
@@ -5,9 +5,9 @@ Created on 2018-03-16 15:14
 
 @author: Lev Velykoivanenko (velykoivanenko.lev@gmail.com)
 """
-# TODO: fix file handlers for effmpeg in gui (changes what needs to be opened)
-# TODO: add basic cli preview to effmpeg
 # TODO: integrate preview into gui window
+# TODO: add preview support when muxing audio
+#       -> figure out if ffmpeg | ffplay would work on windows and mac
 import os
 import sys
 import subprocess
@@ -141,6 +141,7 @@ class Effmpeg(object):
 
     def __init__(self, arguments):
         self.args = arguments
+        self.exe = "ffmpeg"
         self.input = DataItem()
         self.output = DataItem()
         self.ref_vid = DataItem()
@@ -252,11 +253,9 @@ class Effmpeg(object):
                 exit(1)
 
         # Set executable based on whether previewing or not
-        """
         if self.args.preview and self.args.action in self._actions_can_preview:
-            Effmpeg._executable = 'ffplay'
+            self.exe = 'ffplay'
             self.output = DataItem()
-        """
 
         # Set verbosity of output
         self.__set_verbosity(self.args.quiet, self.args.verbose)
@@ -280,7 +279,8 @@ class Effmpeg(object):
                   "transpose": self.args.transpose,
                   "scale": self.args.scale,
                   "print_": self.print_,
-                  "preview": self.args.preview}
+                  "preview": self.args.preview,
+                  "exe": self.exe}
         action = getattr(self, self.args.action)
         action(**kwargs)
 
@@ -297,19 +297,24 @@ class Effmpeg(object):
 
     @staticmethod
     def gen_vid(input_=None, output=None, fps=None, mux_audio=False,
-                ref_vid=None, preview=None, **kwargs):
+                ref_vid=None, preview=False, exe=None, **kwargs):
         filename = Effmpeg.__get_extracted_filename(input_.path)
         _input_opts = Effmpeg._common_ffmpeg_args[:]
         _input_path = os.path.join(input_.path, filename)
-        _output_opts = '-y -c:v libx264 -vf fps="' + str(fps) + '" '
+        _output_opts = '-vf fps="' + str(fps) + '" '
+        if not preview:
+            _output_opts = '-y ' + _output_opts + ' -c:v libx264'
         if mux_audio:
             _ref_vid_opts = '-c copy -map 0:0 -map 1:1'
+            if preview:
+                raise ValueError("Preview for gen-vid with audio muxing is "
+                                 "not supported.")
             _output_opts = _ref_vid_opts + ' ' + _output_opts
             _inputs = {_input_path: _input_opts, ref_vid.path: None}
         else:
             _inputs = {_input_path: _input_opts}
         _outputs = {output.path: _output_opts}
-        Effmpeg.__run_ffmpeg(inputs=_inputs, outputs=_outputs)
+        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)
 
     @staticmethod
     def get_fps(input_=None, print_=False, **kwargs):
@@ -341,23 +346,28 @@ class Effmpeg(object):
             return out
 
     @staticmethod
-    def rescale(input_=None, output=None, scale=None, preview=None, **kwargs):
+    def rescale(input_=None, output=None, scale=None, preview=False, exe=None,
+                **kwargs):
         _input_opts = Effmpeg._common_ffmpeg_args[:]
-        _output_opts = '-y -vf scale="' + str(scale) + '"'
+        _output_opts = '-vf scale="' + str(scale) + '"'
+        if not preview:
+            _output_opts = '-y ' + _output_opts
         _inputs = {input_.path: _input_opts}
         _outputs = {output.path: _output_opts}
-        Effmpeg.__run_ffmpeg(inputs=_inputs, outputs=_outputs)
+        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)
 
     @staticmethod
     def rotate(input_=None, output=None, degrees=None, transpose=None,
-               preview=None, **kwargs):
+               preview=None, exe=None, **kwargs):
         if transpose is None and degrees is None:
             raise ValueError("You have not supplied a valid transpose or "
                              "degrees value:\ntranspose: {}\ndegrees: "
                              "{}".format(transpose, degrees))
 
         _input_opts = Effmpeg._common_ffmpeg_args[:]
-        _output_opts = '-y -c:a copy -vf '
+        _output_opts = '-vf '
+        if not preview:
+            _output_opts = '-y -c:a copy ' + _output_opts
         _bilinear = ''
         if transpose is not None:
             _output_opts += 'transpose="' + str(transpose) + '"'
@@ -369,28 +379,35 @@ class Effmpeg(object):
 
         _inputs = {input_.path: _input_opts}
         _outputs = {output.path: _output_opts}
-        Effmpeg.__run_ffmpeg(inputs=_inputs, outputs=_outputs)
+        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)
 
     @staticmethod
     def mux_audio(input_=None, output=None, ref_vid=None, preview=None,
-                  **kwargs):
+                  exe=None, **kwargs):
         _input_opts = Effmpeg._common_ffmpeg_args[:]
         _ref_vid_opts = None
         _output_opts = '-y -c copy -map 0:0 -map 1:1 -shortest'
+        if preview:
+            raise ValueError("Preview with audio muxing is not supported.")
+        """
+        if not preview:
+            _output_opts = '-y ' + _output_opts
+        """
         _inputs = {input_.path: _input_opts, ref_vid.path: _ref_vid_opts}
         _outputs = {output.path: _output_opts}
-        Effmpeg.__run_ffmpeg(inputs=_inputs, outputs=_outputs)
+        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_outputs)
 
     @staticmethod
     def slice(input_=None, output=None, start=None, duration=None,
-              preview=None,  **kwargs):
+              preview=None, exe=None,  **kwargs):
         _input_opts = Effmpeg._common_ffmpeg_args[:]
         _input_opts += "-ss " + start
-        _output_opts = "-y -t " + duration + " "
-        _output_opts += "-vcodec copy -acodec copy"
+        _output_opts = "-t " + duration + " "
+        if not preview:
+            _output_opts = '-y ' + _output_opts + "-vcodec copy -acodec copy"
         _inputs = {input_.path: _input_opts}
         _output = {output.path: _output_opts}
-        Effmpeg.__run_ffmpeg(inputs=_inputs, outputs=_output)
+        Effmpeg.__run_ffmpeg(exe=exe, inputs=_inputs, outputs=_output)
 
     # Various helper methods
     @classmethod
@@ -431,9 +448,9 @@ class Effmpeg(object):
 
         return all(getattr(self, i).fps is None for i in items_to_check)
 
-    @classmethod
-    def __run_ffmpeg(cls, inputs=None, outputs=None):
-        ff = FFmpeg(executable=cls._executable, inputs=inputs, outputs=outputs)
+    @staticmethod
+    def __run_ffmpeg(exe="ffmpeg", inputs=None, outputs=None):
+        ff = FFmpeg(executable=exe, inputs=inputs, outputs=outputs)
         try:
             ff.run(stderr=subprocess.STDOUT)
         except FFRuntimeError as ffe:
diff --git a/tools/sort.py b/tools/sort.py
index 260d75d..51de8f3 100644
--- a/tools/sort.py
+++ b/tools/sort.py
@@ -5,28 +5,19 @@ A tool that allows for sorting and grouping images in different ways.
 import os
 import sys
 import operator
+from shutil import copyfile
+
 import numpy as np
 import cv2
 from tqdm import tqdm
-from shutil import copyfile
 
 # faceswap imports
 import face_recognition
 
-from lib.cli import DirFullPaths, FileFullPaths, FullHelpArgumentParser
-import lib.Serializer as Serializer
-from . import cli
-
-# DLIB is a GPU Memory hog, so the following modules should only be imported
-# when required
-FaceLandmarksExtractor = None
+from lib.cli import FullHelpArgumentParser
+from lib import face_alignment, Serializer
 
-def import_FaceLandmarksExtractor():
-    """ Import the FaceLandmarksExtractor module only when it is required """
-    global FaceLandmarksExtractor
-    if FaceLandmarksExtractor is None:
-        import lib.FaceLandmarksExtractor
-        FaceLandmarksExtractor = lib.FaceLandmarksExtractor
+from . import cli
 
 
 class Sort(object):
@@ -45,7 +36,8 @@ class Sort(object):
             self.args.output_dir = self.args.input_dir
 
         # Assigning default threshold values based on grouping method
-        if self.args.final_process == "folders" and self.args.min_threshold == -1.0:
+        if (self.args.final_process == "folders"
+                and self.args.min_threshold == -1.0):
             method = self.args.group_method.lower()
             if method == 'face':
                 self.args.min_threshold = 0.6
@@ -64,8 +56,10 @@ class Sort(object):
                                                        'sort_log.json')
 
             # Set serializer based on logfile extension
-            serializer_ext = os.path.splitext(self.args.log_file_path)[-1]
-            self.serializer = Serializer.get_serializer_from_ext(serializer_ext)
+            serializer_ext = os.path.splitext(
+                self.args.log_file_path)[-1]
+            self.serializer = Serializer.get_serializer_from_ext(
+                serializer_ext)
 
         # Prepare sort, group and final process method names
         _sort = "sort_" + self.args.sort_method.lower()
@@ -107,7 +101,9 @@ class Sort(object):
         print("Sorting by blur...")
         img_list = [[x, self.estimate_blur(cv2.imread(x))]
                     for x in
-                    tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout)]
+                    tqdm(self.find_images(input_dir),
+                         desc="Loading",
+                         file=sys.stdout)]
         print("Sorting...")
 
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
@@ -121,10 +117,14 @@ class Sort(object):
 
         img_list = [[x, face_recognition.face_encodings(cv2.imread(x))]
                     for x in
-                    tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout)]
+                    tqdm(self.find_images(input_dir),
+                         desc="Loading",
+                         file=sys.stdout)]
 
         img_list_len = len(img_list)
-        for i in tqdm(range(0, img_list_len - 1), desc="Sorting", file=sys.stdout):
+        for i in tqdm(range(0, img_list_len - 1),
+                      desc="Sorting",
+                      file=sys.stdout):
             min_score = float("inf")
             j_min_score = i + 1
             for j in range(i + 1, len(img_list)):
@@ -132,14 +132,16 @@ class Sort(object):
                 f2encs = img_list[j][1]
                 if f1encs is not None and f2encs is not None and len(
                         f1encs) > 0 and len(f2encs) > 0:
-                    score = face_recognition.face_distance(f1encs[0], f2encs)[0]
+                    score = face_recognition.face_distance(f1encs[0],
+                                                           f2encs)[0]
                 else:
                     score = float("inf")
 
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1], img_list[j_min_score] = img_list[j_min_score], img_list[i + 1]
+            img_list[i + 1] = img_list[j_min_score]
+            img_list[j_min_score] = img_list[i + 1]
 
         return img_list
 
@@ -150,7 +152,9 @@ class Sort(object):
 
         img_list = [[x, face_recognition.face_encodings(cv2.imread(x)), 0]
                     for x in
-                    tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout)]
+                    tqdm(self.find_images(input_dir),
+                         desc="Loading",
+                         file=sys.stdout)]
 
         img_list_len = len(img_list)
         for i in tqdm(range(0, img_list_len), desc="Sorting", file=sys.stdout):
@@ -159,7 +163,9 @@ class Sort(object):
                 if i == j:
                     continue
                 try:
-                    score_total += face_recognition.face_distance([img_list[i][1]], [img_list[j][1]])
+                    score_total += face_recognition.face_distance(
+                        [img_list[i][1]],
+                        [img_list[j][1]])
                 except:
                     pass
 
@@ -170,19 +176,28 @@ class Sort(object):
         return img_list
 
     def sort_face_cnn(self):
-        import_FaceLandmarksExtractor()
 
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn similarity...")
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout):
-            d = FaceLandmarksExtractor.extract(cv2.imread(x), 'cnn', True, input_is_predetected_face=True)
-            img_list.append([x, np.array(d[0][1]) if len(d) > 0 else np.zeros((68, 2))])
+        for x in tqdm(self.find_images(input_dir),
+                      desc="Loading",
+                      file=sys.stdout):
+            d = face_alignment.Extract(
+                cv2.imread(x),
+                'dlib-cnn',
+                True,
+                input_is_predetected_face=True).landmarks
+            img_list.append([x, np.array(d[0][1])
+                             if len(d) > 0
+                             else np.zeros((68, 2))])
 
         img_list_len = len(img_list)
-        for i in tqdm(range(0, img_list_len - 1), desc="Sorting", file=sys.stdout):
+        for i in tqdm(range(0, img_list_len - 1),
+                      desc="Sorting",
+                      file=sys.stdout):
             min_score = float("inf")
             j_min_score = i + 1
             for j in range(i + 1, len(img_list)):
@@ -193,24 +208,33 @@ class Sort(object):
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1], img_list[j_min_score] = img_list[j_min_score], img_list[i + 1]
+            img_list[i + 1] = img_list[j_min_score]
+            img_list[j_min_score] = img_list[i + 1]
 
         return img_list
 
     def sort_face_cnn_dissim(self):
-        import_FaceLandmarksExtractor()
-
         input_dir = self.args.input_dir
 
         print("Sorting by face-cnn dissimilarity...")
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout):
-            d = FaceLandmarksExtractor.extract(cv2.imread(x), 'cnn', True, input_is_predetected_face=True)
-            img_list.append([x, np.array(d[0][1]) if len(d) > 0 else np.zeros((68, 2)), 0])
+        for x in tqdm(self.find_images(input_dir),
+                      desc="Loading",
+                      file=sys.stdout):
+            d = face_alignment.Extract(
+                cv2.imread(x),
+                'dlib-cnn',
+                True,
+                input_is_predetected_face=True).landmarks
+            img_list.append([x, np.array(d[0][1])
+                             if len(d) > 0
+                             else np.zeros((68, 2)), 0])
 
         img_list_len = len(img_list)
-        for i in tqdm(range(0, img_list_len - 1), desc="Sorting", file=sys.stdout):
+        for i in tqdm(range(0, img_list_len - 1),
+                      desc="Sorting",
+                      file=sys.stdout):
             score_total = 0
             for j in range(i + 1, len(img_list)):
                 if i == j:
@@ -227,13 +251,19 @@ class Sort(object):
         return img_list
 
     def sort_face_yaw(self):
-        import_FaceLandmarksExtractor()
         input_dir = self.args.input_dir
 
         img_list = []
-        for x in tqdm(self.find_images(input_dir), desc="Loading", file=sys.stdout):
-            d = FaceLandmarksExtractor.extract(cv2.imread(x), 'cnn', True, input_is_predetected_face=True)
-            img_list.append([x, self.calc_landmarks_face_yaw(np.array(d[0][1]))])
+        for x in tqdm(self.find_images(input_dir),
+                      desc="Loading",
+                      file=sys.stdout):
+            d = face_alignment.Extract(
+                cv2.imread(x),
+                'dlib-cnn',
+                True,
+                input_is_predetected_face=True).landmarks
+            img_list.append([x,
+                             self.calc_landmarks_face_yaw(np.array(d[0][1]))])
 
         print("Sorting by face-yaw...")
         img_list = sorted(img_list, key=operator.itemgetter(1), reverse=True)
@@ -263,7 +293,8 @@ class Sort(object):
                 if score < min_score:
                     min_score = score
                     j_min_score = j
-            img_list[i + 1], img_list[j_min_score] = img_list[j_min_score], img_list[i + 1]
+            img_list[i + 1] = img_list[j_min_score]
+            img_list[j_min_score] = img_list[i + 1]
 
         return img_list
 
@@ -336,7 +367,9 @@ class Sort(object):
 
         img_list_len = len(img_list)
 
-        for i in tqdm(range(1, img_list_len), desc="Grouping", file=sys.stdout):
+        for i in tqdm(range(1, img_list_len),
+                      desc="Grouping",
+                      file=sys.stdout):
             f1encs = img_list[i][1]
 
             # Check if current image is a face, if not then
@@ -388,7 +421,9 @@ class Sort(object):
 
         img_list_len = len(img_list)
 
-        for i in tqdm(range(0, img_list_len - 1), desc="Grouping", file=sys.stdout):
+        for i in tqdm(range(0, img_list_len - 1),
+                      desc="Grouping",
+                      file=sys.stdout):
             fl1 = img_list[i][1]
 
             current_best = [-1, float("inf")]
@@ -451,7 +486,9 @@ class Sort(object):
         reference_groups[0] = [img_list[0][1]]
         bins.append([img_list[0][0]])
 
-        for i in tqdm(range(1, img_list_len), desc="Grouping", file=sys.stdout):
+        for i in tqdm(range(1, img_list_len),
+                      desc="Grouping",
+                      file=sys.stdout):
             current_best = [-1, float("inf")]
             for key, value in reference_groups.items():
                 score = self.get_avg_score_hist(img_list[i][1], value)
@@ -483,7 +520,10 @@ class Sort(object):
             else "Moving and Renaming"
         )
 
-        for i in tqdm(range(0, len(img_list)), desc=description, leave=False, file=sys.stdout):
+        for i in tqdm(range(0, len(img_list)),
+                      desc=description,
+                      leave=False,
+                      file=sys.stdout):
             src = img_list[i][0]
             src_basename = os.path.basename(src)
 
@@ -494,7 +534,9 @@ class Sort(object):
                 print(e)
                 print('fail to rename {}'.format(src))
 
-        for i in tqdm(range(0, len(img_list)), desc=description, file=sys.stdout):
+        for i in tqdm(range(0, len(img_list)),
+                      desc=description,
+                      file=sys.stdout):
             renaming = self.set_renaming_method(self.args.log_changes)
             src, dst = renaming(img_list[i][0], output_dir, i, self.changes)
 
@@ -563,30 +605,47 @@ class Sort(object):
         if group_method == 'group_blur':
             temp_list = [[x, self.estimate_blur(cv2.imread(x))]
                          for x in
-                         tqdm(self.find_images(input_dir), desc="Reloading", file=sys.stdout)]
+                         tqdm(self.find_images(input_dir),
+                              desc="Reloading",
+                              file=sys.stdout)]
         elif group_method == 'group_face':
             temp_list = [[x, face_recognition.face_encodings(cv2.imread(x))]
                          for x in
-                         tqdm(self.find_images(input_dir), desc="Reloading", file=sys.stdout)]
+                         tqdm(self.find_images(input_dir),
+                              desc="Reloading",
+                              file=sys.stdout)]
         elif group_method == 'group_face_cnn':
-            import_FaceLandmarksExtractor()
             temp_list = []
-            for x in tqdm(self.find_images(input_dir), desc="Reloading", file=sys.stdout):
-                d = FaceLandmarksExtractor.extract(cv2.imread(x), 'cnn', True,
-                                                   input_is_predetected_face=True)
-                temp_list.append([x, np.array(d[0][1]) if len(d) > 0 else np.zeros((68, 2))])
+            for x in tqdm(self.find_images(input_dir),
+                          desc="Reloading",
+                          file=sys.stdout):
+                d = face_alignment.Extract(
+                    cv2.imread(x),
+                    'dlib-cnn',
+                    True,
+                    input_is_predetected_face=True).landmarks
+                temp_list.append([x, np.array(d[0][1])
+                                  if len(d) > 0
+                                  else np.zeros((68, 2))])
         elif group_method == 'group_face_yaw':
-            import_FaceLandmarksExtractor()
             temp_list = []
-            for x in tqdm(self.find_images(input_dir), desc="Reloading", file=sys.stdout):
-                d = FaceLandmarksExtractor.extract(cv2.imread(x), 'cnn', True,
-                                                   input_is_predetected_face=True)
-                temp_list.append([x, self.calc_landmarks_face_yaw(np.array(d[0][1]))])
+            for x in tqdm(self.find_images(input_dir),
+                          desc="Reloading",
+                          file=sys.stdout):
+                d = face_alignment.Extract(
+                    cv2.imread(x),
+                    'dlib-cnn',
+                    True,
+                    input_is_predetected_face=True).landmarks
+                temp_list.append(
+                    [x, self.calc_landmarks_face_yaw(np.array(d[0][1]))])
         elif group_method == 'group_hist':
             temp_list = [
                 [x, cv2.calcHist([cv2.imread(x)], [0], None, [256], [0, 256])]
                 for x in
-                tqdm(self.find_images(input_dir), desc="Reloading", file=sys.stdout)
+                tqdm(self.find_images(input_dir),
+                     desc="Reloading",
+                     file=sys.stdout)
             ]
         else:
             raise ValueError("{} group_method not found.".format(group_method))
@@ -612,7 +671,9 @@ class Sort(object):
         new_list = []
         # Make new list of just image paths to serve as an index
         val_index_list = [i[0] for i in new_vals_list]
-        for i in tqdm(range(len(sorted_list)), desc="Splicing", file=sys.stdout):
+        for i in tqdm(range(len(sorted_list)),
+                      desc="Splicing",
+                      file=sys.stdout):
             current_image = sorted_list[i][0]
             new_val_index = val_index_list.index(current_image)
             new_list.append([current_image, new_vals_list[new_val_index][1]])
@@ -646,15 +707,13 @@ class Sort(object):
 
     @staticmethod
     def calc_landmarks_face_yaw(fl):
-        l = ((fl[27][0] - fl[0][0])
-             + (fl[28][0] - fl[1][0])
-             + (fl[29][0] - fl[2][0])) \
-            / 3.0
-        r = ((fl[16][0] - fl[27][0])
-             + (fl[15][0] - fl[28][0])
-             + (fl[14][0] - fl[29][0])) \
-            / 3.0
-        return r - l
+        var_l = ((fl[27][0] - fl[0][0])
+                 + (fl[28][0] - fl[1][0])
+                 + (fl[29][0] - fl[2][0])) / 3.0
+        var_r = ((fl[16][0] - fl[27][0])
+                 + (fl[15][0] - fl[28][0])
+                 + (fl[14][0] - fl[29][0])) / 3.0
+        return var_r - var_l
 
     @staticmethod
     def set_process_file_method(log_changes, keep_original):
@@ -696,8 +755,11 @@ class Sort(object):
             def renaming(src, output_dir, i, changes):
                 src_basename = os.path.basename(src)
 
-                __src = os.path.join(output_dir, '{:05d}_{}'.format(i, src_basename))
-                dst = os.path.join(output_dir, '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
+                __src = os.path.join(output_dir,
+                                     '{:05d}_{}'.format(i, src_basename))
+                dst = os.path.join(
+                    output_dir,
+                    '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
                 changes[src] = dst
                 return __src, dst
 
@@ -707,8 +769,11 @@ class Sort(object):
             def renaming(src, output_dir, i, changes):
                 src_basename = os.path.basename(src)
 
-                src = os.path.join(output_dir, '{:05d}_{}'.format(i, src_basename))
-                dst = os.path.join(output_dir, '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
+                src = os.path.join(output_dir,
+                                   '{:05d}_{}'.format(i, src_basename))
+                dst = os.path.join(
+                    output_dir,
+                    '{:05d}{}'.format(i, os.path.splitext(src_basename)[1]))
                 return src, dst
 
             return renaming
@@ -751,7 +816,6 @@ if __name__ == "__main__":
     print(__warning_string)
     print("Images sort tool.\n")
 
-
     PARSER = FullHelpArgumentParser()
     SUBPARSER = PARSER.add_subparsers()
     SORT = cli.SortArgs(
