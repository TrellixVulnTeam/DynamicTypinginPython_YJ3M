commit a5d1d609927906e2df42ab1949ba2de8da4881c0
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Jun 4 12:43:49 2019 +0100

    Minor changes
    
    Convert - Force prediction batchsize to 1 when converting on CPU
    Training: Move snapshot saving to outside of the save threads to avoid race condition
    GUI Summary stats. Show iteration count for current training session (fixed to last save amount).

diff --git a/lib/gui/display_analysis.py b/lib/gui/display_analysis.py
index 4646fe9..de759a6 100644
--- a/lib/gui/display_analysis.py
+++ b/lib/gui/display_analysis.py
@@ -100,6 +100,8 @@ class Analysis(DisplayPage):  # pylint: disable=too-many-ancestors
             return
         msg = "Currently running training session"
         self.session = session
+        # Reload the state file to get approx currently training iterations
+        self.session.load_state_file()
         self.set_session_summary(msg)
 
     def set_session_summary(self, message):
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index a51ff13..f85a374 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -405,16 +405,31 @@ class ModelBase():
                                             should_backup=should_backup))
         save_threads.append(MultiThread(self.state.save,
                                         name="save_state",
-                                        should_backup=should_backup,
-                                        snapshot=snapshot_iteration))
+                                        should_backup=should_backup))
         for thread in save_threads:
             thread.start()
         for thread in save_threads:
             if thread.has_error:
                 logger.error(thread.errors[0])
             thread.join()
-        # Put in a line break to avoid jumbled console
         logger.info("saved models")
+        if snapshot_iteration:
+            self.snapshot_models()
+    
+    def snapshot_models(self):
+        """ Take a snapshot of the model at current state and back up """
+        logger.info("Saving snapshot")
+        src = self.model_dir
+        dst = get_folder("{}_{}".format(self.model_dir, self.iterations))
+        for filename in os.listdir(src):
+            if filename.endswith(".bk"):
+                continue
+            srcfile = os.path.join(src, filename)
+            dstfile = os.path.join(dst, filename)
+            copyfunc = copytree if os.path.isdir(srcfile) else copyfile
+            logger.debug("Saving snapshot: '%s' > '%s'", srcfile, dstfile)
+            copyfunc(srcfile, dstfile)
+        logger.info("Saved snapshot")
 
     def get_save_averages(self):
         """ Return the loss averages since last save and reset historical losses
@@ -702,7 +717,7 @@ class State():
         except JSONDecodeError as err:
             logger.debug("JSONDecodeError: %s:", str(err))
 
-    def save(self, should_backup=False, snapshot=False):
+    def save(self, should_backup=False):
         """ Save iteration number to state file """
         logger.debug("Saving State")
         if should_backup:
@@ -721,8 +736,6 @@ class State():
         except IOError as err:
             logger.error("Unable to save model state: %s", str(err.strerror))
         logger.debug("Saved State")
-        if snapshot:
-            self.snapshot_model()
 
     def backup(self):
         """ Backup state file """
@@ -734,21 +747,6 @@ class State():
         if os.path.exists(origfile):
             os.rename(origfile, backupfile)
 
-    def snapshot_model(self):
-        """ Take a snapshot of the model at current state and back up """
-        logger.info("Saving snapshot")
-        src = os.path.dirname(self.filename)
-        dst = get_folder("{}_{}".format(src, self.iterations))
-        for filename in os.listdir(src):
-            if filename.endswith(".bk"):
-                continue
-            srcfile = os.path.join(src, filename)
-            dstfile = os.path.join(dst, filename)
-            copyfunc = copytree if os.path.isdir(srcfile) else copyfile
-            logger.debug("Saving snapshot: '%s' > '%s'", srcfile, dstfile)
-            copyfunc(srcfile, dstfile)
-        logger.info("Saved snapshot")
-
     def replace_config(self, config_changeable_items):
         """ Replace the loaded config with the one contained within the state file
             Check for any fixed=False parameters changes and log info changes
diff --git a/scripts/convert.py b/scripts/convert.py
index 7c51269..31ea501 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -15,6 +15,7 @@ from scripts.fsmedia import Alignments, Images, PostProcess, Utils
 from lib import Serializer
 from lib.convert import Converter
 from lib.faces_detect import DetectedFace
+from lib.gpu_stats import GPUStats
 from lib.multithreading import MultiThread, PoolProcess, total_cpus
 from lib.queue_manager import queue_manager
 from lib.utils import get_folder, get_image_paths, hash_image_file
@@ -393,7 +394,7 @@ class Predict():
     def __init__(self, in_queue, queue_size, arguments):
         logger.debug("Initializing %s: (args: %s, queue_size: %s, in_queue: %s)",
                      self.__class__.__name__, arguments, queue_size, in_queue)
-        self.batchsize = min(queue_size, 16)
+        self.batchsize = self.get_batchsize(queue_size)
         self.args = arguments
         self.in_queue = in_queue
         self.out_queue = queue_manager.get_queue("patch")
@@ -435,6 +436,15 @@ class Predict():
         """ Return whether this model has a predicted mask """
         return bool(self.model.state.mask_shapes)
 
+    @staticmethod
+    def get_batchsize(queue_size):
+        """ Get the batchsize """
+        is_cpu = GPUStats().device_count == 0
+        batchsize = 1 if is_cpu else 16
+        batchsize = min(queue_size, batchsize)
+        logger.debug("Batchsize: %s", batchsize)
+        return batchsize
+
     def load_model(self):
         """ Load the model requested for conversion """
         logger.debug("Loading Model")
