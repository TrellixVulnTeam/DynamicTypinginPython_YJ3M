commit 6c6aac77aa3460fa500432c04bc3b27fed448a71
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Wed May 1 22:46:33 2019 +0000

    Dlib Detector Removal. CV2 Detector add. Bugfix on-the-fly convert
    
    Remove dlib-hog + dlib-cnn detectors
    Add cv2-dnn detector
    Change on-the-fly detector to cv2-dnn
    Bugfix: on-the-fly conversion
    Bugfix: Pool processes

diff --git a/.gitignore b/.gitignore
index 1382e22..078aaf2 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,5 +1,6 @@
 *
 !setup.cfg
+!*.caffemodel
 !*.dat
 !*.h5
 !*.ico
@@ -10,6 +11,7 @@
 !*.nsi
 !*.pb
 !*.png
+!*.prototxt
 !*.py
 !*.txt
 !.cache
diff --git a/lib/cli.py b/lib/cli.py
index 184a4e6..7319673 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -489,17 +489,15 @@ class ExtractArgs(ExtractConvertArgs):
             "type": str.lower,
             "choices":  PluginLoader.get_available_extractors("detect"),
             "default": "mtcnn",
-            "help": "R|Detector to use."
-                    "\nL|'dlib-hog': uses least resources, but is the "
-                    "least reliable."
-                    "\nL|'dlib-cnn': faster than mtcnn but detects "
-                    "fewer faces and fewer false positives."
-                    "\nL|'mtcnn': slower than dlib, but uses fewer "
-                    "resources whilst detecting more faces and "
-                    "more false positives. Has superior "
-                    "alignment to dlib"
-                    "\nL|'s3fd': Can detect more faces than mtcnn, but "
-                    "is a lot more resource intensive"})
+            "help": "R|Detector to use. Some of these have configurable settings in "
+                    "'/config/extract.ini' or 'Edit > Configure Extract Plugins':"
+                    "\nL|'cv2-dnn': A CPU only extractor, is the least reliable, but uses least "
+                    "resources and runs fast on CPU. Use this if not using a GPU and time is "
+                    "important."
+                    "\nL|'mtcnn': Fast on GPU, slow on CPU. Uses fewer resources than other GPU "
+                    "detectors but can often return more false positives."
+                    "\nL|'s3fd': Fast on GPU, slow on CPU. Can detect more faces and fewer false "
+                    "positives than other GPU detectors, but is a lot more resource intensive."})
         argument_list.append({
             "opts": ("-A", "--aligner"),
             "action": Radio,
diff --git a/lib/convert.py b/lib/convert.py
index 97cf77c..c2988f9 100644
--- a/lib/convert.py
+++ b/lib/convert.py
@@ -174,7 +174,7 @@ class Converter():
 
     def add_alpha_mask(self, frame, predicted):
         """ Adding a 4th channel should happen after all other channel operations
-            Add the default mask as 4th channel for saving as png with alpha channel """
+            Add the default mask as 4th channel for saving as image with alpha channel """
         if not self.draw_transparent:
             return frame
         logger.trace("Creating transparent image: '%s'", predicted["filename"])
diff --git a/lib/multithreading.py b/lib/multithreading.py
index 08e5a74..c8e7f20 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -298,6 +298,7 @@ class PoolProcess():
 
     def set_procs(self, processes):
         """ Set the number of processes to use """
+        processes = mp.cpu_count() if processes is None else processes
         running_processes = len(mp.active_children())
         avail_processes = max(mp.cpu_count() - running_processes, 1)
         processes = min(avail_processes, processes)
diff --git a/plugins/extract/_config.py b/plugins/extract/_config.py
index 734954f..ad59985 100644
--- a/plugins/extract/_config.py
+++ b/plugins/extract/_config.py
@@ -20,10 +20,26 @@ class Config(FaceswapConfig):
 #        self.add_section(title=section,
 #                         info="Options that apply to all models")
 
+        # << S3FD DETECTOR OPTIONS >> #
+        section = "detect.cv2_dnn"
+        self.add_section(title=section,
+                         info="CV2 DNN Detector options."
+                              "\nA CPU only extractor, is the least reliable, but uses least "
+                              "resources and runs fast on CPU. Use this if not using a GPU and "
+                              "time is important")
+        self.add_item(
+            section=section, title="confidence", datatype=int, default=50, rounding=5,
+            min_max=(25, 100),
+            info="The confidence level at which the detector has succesfully found a face.\n"
+                 "Higher levels will be more discriminating, lower levels will have more false "
+                 "positives")
+
         # << MTCNN DETECTOR OPTIONS >> #
         section = "detect.mtcnn"
         self.add_section(title=section,
-                         info="MTCNN Detector options")
+                         info="MTCNN Detector options."
+                              "\nFast on GPU, slow on CPU. Uses fewer resources than other GPU "
+                              "detectors but can often return more false positives.")
         self.add_item(
             section=section, title="minsize", datatype=int, default=20, rounding=10,
             min_max=(20, 1000),
@@ -50,7 +66,10 @@ class Config(FaceswapConfig):
         # << S3FD DETECTOR OPTIONS >> #
         section = "detect.s3fd"
         self.add_section(title=section,
-                         info="S3FD Detector options")
+                         info="S3FD Detector options."
+                              "Fast on GPU, slow on CPU. Can detect more faces and fewer false "
+                              "positives than other GPU detectors, but is a lot more resource "
+                              "intensive.")
         self.add_item(
             section=section, title="confidence", datatype=int, default=50, rounding=5,
             min_max=(25, 100),
diff --git a/plugins/extract/detect/.cache/deploy.prototxt b/plugins/extract/detect/.cache/deploy.prototxt
new file mode 100644
index 0000000..e9b7db0
--- /dev/null
+++ b/plugins/extract/detect/.cache/deploy.prototxt
@@ -0,0 +1,1789 @@
+input: "data"
+input_shape {
+  dim: 1
+  dim: 3
+  dim: 300
+  dim: 300
+}
+
+layer {
+  name: "data_bn"
+  type: "BatchNorm"
+  bottom: "data"
+  top: "data_bn"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "data_scale"
+  type: "Scale"
+  bottom: "data_bn"
+  top: "data_bn"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "conv1_h"
+  type: "Convolution"
+  bottom: "data_bn"
+  top: "conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 32
+    pad: 3
+    kernel_size: 7
+    stride: 2
+    weight_filler {
+      type: "msra"
+      variance_norm: FAN_OUT
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "conv1_bn_h"
+  type: "BatchNorm"
+  bottom: "conv1_h"
+  top: "conv1_h"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "conv1_scale_h"
+  type: "Scale"
+  bottom: "conv1_h"
+  top: "conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "conv1_relu"
+  type: "ReLU"
+  bottom: "conv1_h"
+  top: "conv1_h"
+}
+layer {
+  name: "conv1_pool"
+  type: "Pooling"
+  bottom: "conv1_h"
+  top: "conv1_pool"
+  pooling_param {
+    kernel_size: 3
+    stride: 2
+  }
+}
+layer {
+  name: "layer_64_1_conv1_h"
+  type: "Convolution"
+  bottom: "conv1_pool"
+  top: "layer_64_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 32
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_64_1_bn2_h"
+  type: "BatchNorm"
+  bottom: "layer_64_1_conv1_h"
+  top: "layer_64_1_conv1_h"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_64_1_scale2_h"
+  type: "Scale"
+  bottom: "layer_64_1_conv1_h"
+  top: "layer_64_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_64_1_relu2"
+  type: "ReLU"
+  bottom: "layer_64_1_conv1_h"
+  top: "layer_64_1_conv1_h"
+}
+layer {
+  name: "layer_64_1_conv2_h"
+  type: "Convolution"
+  bottom: "layer_64_1_conv1_h"
+  top: "layer_64_1_conv2_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 32
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_64_1_sum"
+  type: "Eltwise"
+  bottom: "layer_64_1_conv2_h"
+  bottom: "conv1_pool"
+  top: "layer_64_1_sum"
+}
+layer {
+  name: "layer_128_1_bn1_h"
+  type: "BatchNorm"
+  bottom: "layer_64_1_sum"
+  top: "layer_128_1_bn1_h"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_128_1_scale1_h"
+  type: "Scale"
+  bottom: "layer_128_1_bn1_h"
+  top: "layer_128_1_bn1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_128_1_relu1"
+  type: "ReLU"
+  bottom: "layer_128_1_bn1_h"
+  top: "layer_128_1_bn1_h"
+}
+layer {
+  name: "layer_128_1_conv1_h"
+  type: "Convolution"
+  bottom: "layer_128_1_bn1_h"
+  top: "layer_128_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 128
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_128_1_bn2"
+  type: "BatchNorm"
+  bottom: "layer_128_1_conv1_h"
+  top: "layer_128_1_conv1_h"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_128_1_scale2"
+  type: "Scale"
+  bottom: "layer_128_1_conv1_h"
+  top: "layer_128_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_128_1_relu2"
+  type: "ReLU"
+  bottom: "layer_128_1_conv1_h"
+  top: "layer_128_1_conv1_h"
+}
+layer {
+  name: "layer_128_1_conv2"
+  type: "Convolution"
+  bottom: "layer_128_1_conv1_h"
+  top: "layer_128_1_conv2"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 128
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_128_1_conv_expand_h"
+  type: "Convolution"
+  bottom: "layer_128_1_bn1_h"
+  top: "layer_128_1_conv_expand_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 128
+    bias_term: false
+    pad: 0
+    kernel_size: 1
+    stride: 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_128_1_sum"
+  type: "Eltwise"
+  bottom: "layer_128_1_conv2"
+  bottom: "layer_128_1_conv_expand_h"
+  top: "layer_128_1_sum"
+}
+layer {
+  name: "layer_256_1_bn1"
+  type: "BatchNorm"
+  bottom: "layer_128_1_sum"
+  top: "layer_256_1_bn1"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_256_1_scale1"
+  type: "Scale"
+  bottom: "layer_256_1_bn1"
+  top: "layer_256_1_bn1"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_256_1_relu1"
+  type: "ReLU"
+  bottom: "layer_256_1_bn1"
+  top: "layer_256_1_bn1"
+}
+layer {
+  name: "layer_256_1_conv1"
+  type: "Convolution"
+  bottom: "layer_256_1_bn1"
+  top: "layer_256_1_conv1"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 256
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_256_1_bn2"
+  type: "BatchNorm"
+  bottom: "layer_256_1_conv1"
+  top: "layer_256_1_conv1"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_256_1_scale2"
+  type: "Scale"
+  bottom: "layer_256_1_conv1"
+  top: "layer_256_1_conv1"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_256_1_relu2"
+  type: "ReLU"
+  bottom: "layer_256_1_conv1"
+  top: "layer_256_1_conv1"
+}
+layer {
+  name: "layer_256_1_conv2"
+  type: "Convolution"
+  bottom: "layer_256_1_conv1"
+  top: "layer_256_1_conv2"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 256
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_256_1_conv_expand"
+  type: "Convolution"
+  bottom: "layer_256_1_bn1"
+  top: "layer_256_1_conv_expand"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 256
+    bias_term: false
+    pad: 0
+    kernel_size: 1
+    stride: 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_256_1_sum"
+  type: "Eltwise"
+  bottom: "layer_256_1_conv2"
+  bottom: "layer_256_1_conv_expand"
+  top: "layer_256_1_sum"
+}
+layer {
+  name: "layer_512_1_bn1"
+  type: "BatchNorm"
+  bottom: "layer_256_1_sum"
+  top: "layer_512_1_bn1"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_512_1_scale1"
+  type: "Scale"
+  bottom: "layer_512_1_bn1"
+  top: "layer_512_1_bn1"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_512_1_relu1"
+  type: "ReLU"
+  bottom: "layer_512_1_bn1"
+  top: "layer_512_1_bn1"
+}
+layer {
+  name: "layer_512_1_conv1_h"
+  type: "Convolution"
+  bottom: "layer_512_1_bn1"
+  top: "layer_512_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 128
+    bias_term: false
+    pad: 1
+    kernel_size: 3
+    stride: 1 # 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_512_1_bn2_h"
+  type: "BatchNorm"
+  bottom: "layer_512_1_conv1_h"
+  top: "layer_512_1_conv1_h"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "layer_512_1_scale2_h"
+  type: "Scale"
+  bottom: "layer_512_1_conv1_h"
+  top: "layer_512_1_conv1_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "layer_512_1_relu2"
+  type: "ReLU"
+  bottom: "layer_512_1_conv1_h"
+  top: "layer_512_1_conv1_h"
+}
+layer {
+  name: "layer_512_1_conv2_h"
+  type: "Convolution"
+  bottom: "layer_512_1_conv1_h"
+  top: "layer_512_1_conv2_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 256
+    bias_term: false
+    pad: 2 # 1
+    kernel_size: 3
+    stride: 1
+    dilation: 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_512_1_conv_expand_h"
+  type: "Convolution"
+  bottom: "layer_512_1_bn1"
+  top: "layer_512_1_conv_expand_h"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  convolution_param {
+    num_output: 256
+    bias_term: false
+    pad: 0
+    kernel_size: 1
+    stride: 1 # 2
+    weight_filler {
+      type: "msra"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0.0
+    }
+  }
+}
+layer {
+  name: "layer_512_1_sum"
+  type: "Eltwise"
+  bottom: "layer_512_1_conv2_h"
+  bottom: "layer_512_1_conv_expand_h"
+  top: "layer_512_1_sum"
+}
+layer {
+  name: "last_bn_h"
+  type: "BatchNorm"
+  bottom: "layer_512_1_sum"
+  top: "layer_512_1_sum"
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+  param {
+    lr_mult: 0.0
+  }
+}
+layer {
+  name: "last_scale_h"
+  type: "Scale"
+  bottom: "layer_512_1_sum"
+  top: "layer_512_1_sum"
+  param {
+    lr_mult: 1.0
+    decay_mult: 1.0
+  }
+  param {
+    lr_mult: 2.0
+    decay_mult: 1.0
+  }
+  scale_param {
+    bias_term: true
+  }
+}
+layer {
+  name: "last_relu"
+  type: "ReLU"
+  bottom: "layer_512_1_sum"
+  top: "fc7"
+}
+
+layer {
+  name: "conv6_1_h"
+  type: "Convolution"
+  bottom: "fc7"
+  top: "conv6_1_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 128
+    pad: 0
+    kernel_size: 1
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv6_1_relu"
+  type: "ReLU"
+  bottom: "conv6_1_h"
+  top: "conv6_1_h"
+}
+layer {
+  name: "conv6_2_h"
+  type: "Convolution"
+  bottom: "conv6_1_h"
+  top: "conv6_2_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 256
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv6_2_relu"
+  type: "ReLU"
+  bottom: "conv6_2_h"
+  top: "conv6_2_h"
+}
+layer {
+  name: "conv7_1_h"
+  type: "Convolution"
+  bottom: "conv6_2_h"
+  top: "conv7_1_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 64
+    pad: 0
+    kernel_size: 1
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv7_1_relu"
+  type: "ReLU"
+  bottom: "conv7_1_h"
+  top: "conv7_1_h"
+}
+layer {
+  name: "conv7_2_h"
+  type: "Convolution"
+  bottom: "conv7_1_h"
+  top: "conv7_2_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 2
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv7_2_relu"
+  type: "ReLU"
+  bottom: "conv7_2_h"
+  top: "conv7_2_h"
+}
+layer {
+  name: "conv8_1_h"
+  type: "Convolution"
+  bottom: "conv7_2_h"
+  top: "conv8_1_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 64
+    pad: 0
+    kernel_size: 1
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv8_1_relu"
+  type: "ReLU"
+  bottom: "conv8_1_h"
+  top: "conv8_1_h"
+}
+layer {
+  name: "conv8_2_h"
+  type: "Convolution"
+  bottom: "conv8_1_h"
+  top: "conv8_2_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv8_2_relu"
+  type: "ReLU"
+  bottom: "conv8_2_h"
+  top: "conv8_2_h"
+}
+layer {
+  name: "conv9_1_h"
+  type: "Convolution"
+  bottom: "conv8_2_h"
+  top: "conv9_1_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 64
+    pad: 0
+    kernel_size: 1
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv9_1_relu"
+  type: "ReLU"
+  bottom: "conv9_1_h"
+  top: "conv9_1_h"
+}
+layer {
+  name: "conv9_2_h"
+  type: "Convolution"
+  bottom: "conv9_1_h"
+  top: "conv9_2_h"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 128
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv9_2_relu"
+  type: "ReLU"
+  bottom: "conv9_2_h"
+  top: "conv9_2_h"
+}
+layer {
+  name: "conv4_3_norm"
+  type: "Normalize"
+  bottom: "layer_256_1_bn1"
+  top: "conv4_3_norm"
+  norm_param {
+    across_spatial: false
+    scale_filler {
+      type: "constant"
+      value: 20
+    }
+    channel_shared: false
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_loc"
+  type: "Convolution"
+  bottom: "conv4_3_norm"
+  top: "conv4_3_norm_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 16
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_loc_perm"
+  type: "Permute"
+  bottom: "conv4_3_norm_mbox_loc"
+  top: "conv4_3_norm_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "conv4_3_norm_mbox_loc_perm"
+  top: "conv4_3_norm_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_conf"
+  type: "Convolution"
+  bottom: "conv4_3_norm"
+  top: "conv4_3_norm_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 8 # 84
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_conf_perm"
+  type: "Permute"
+  bottom: "conv4_3_norm_mbox_conf"
+  top: "conv4_3_norm_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "conv4_3_norm_mbox_conf_perm"
+  top: "conv4_3_norm_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv4_3_norm_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "conv4_3_norm"
+  bottom: "data"
+  top: "conv4_3_norm_mbox_priorbox"
+  prior_box_param {
+    min_size: 30.0
+    max_size: 60.0
+    aspect_ratio: 2
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 8
+    offset: 0.5
+  }
+}
+layer {
+  name: "fc7_mbox_loc"
+  type: "Convolution"
+  bottom: "fc7"
+  top: "fc7_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 24
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "fc7_mbox_loc_perm"
+  type: "Permute"
+  bottom: "fc7_mbox_loc"
+  top: "fc7_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "fc7_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "fc7_mbox_loc_perm"
+  top: "fc7_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "fc7_mbox_conf"
+  type: "Convolution"
+  bottom: "fc7"
+  top: "fc7_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 12 # 126
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "fc7_mbox_conf_perm"
+  type: "Permute"
+  bottom: "fc7_mbox_conf"
+  top: "fc7_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "fc7_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "fc7_mbox_conf_perm"
+  top: "fc7_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "fc7_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "fc7"
+  bottom: "data"
+  top: "fc7_mbox_priorbox"
+  prior_box_param {
+    min_size: 60.0
+    max_size: 111.0
+    aspect_ratio: 2
+    aspect_ratio: 3
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 16
+    offset: 0.5
+  }
+}
+layer {
+  name: "conv6_2_mbox_loc"
+  type: "Convolution"
+  bottom: "conv6_2_h"
+  top: "conv6_2_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 24
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv6_2_mbox_loc_perm"
+  type: "Permute"
+  bottom: "conv6_2_mbox_loc"
+  top: "conv6_2_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv6_2_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "conv6_2_mbox_loc_perm"
+  top: "conv6_2_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv6_2_mbox_conf"
+  type: "Convolution"
+  bottom: "conv6_2_h"
+  top: "conv6_2_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 12 # 126
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv6_2_mbox_conf_perm"
+  type: "Permute"
+  bottom: "conv6_2_mbox_conf"
+  top: "conv6_2_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv6_2_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "conv6_2_mbox_conf_perm"
+  top: "conv6_2_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv6_2_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "conv6_2_h"
+  bottom: "data"
+  top: "conv6_2_mbox_priorbox"
+  prior_box_param {
+    min_size: 111.0
+    max_size: 162.0
+    aspect_ratio: 2
+    aspect_ratio: 3
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 32
+    offset: 0.5
+  }
+}
+layer {
+  name: "conv7_2_mbox_loc"
+  type: "Convolution"
+  bottom: "conv7_2_h"
+  top: "conv7_2_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 24
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv7_2_mbox_loc_perm"
+  type: "Permute"
+  bottom: "conv7_2_mbox_loc"
+  top: "conv7_2_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv7_2_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "conv7_2_mbox_loc_perm"
+  top: "conv7_2_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv7_2_mbox_conf"
+  type: "Convolution"
+  bottom: "conv7_2_h"
+  top: "conv7_2_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 12 # 126
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv7_2_mbox_conf_perm"
+  type: "Permute"
+  bottom: "conv7_2_mbox_conf"
+  top: "conv7_2_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv7_2_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "conv7_2_mbox_conf_perm"
+  top: "conv7_2_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv7_2_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "conv7_2_h"
+  bottom: "data"
+  top: "conv7_2_mbox_priorbox"
+  prior_box_param {
+    min_size: 162.0
+    max_size: 213.0
+    aspect_ratio: 2
+    aspect_ratio: 3
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 64
+    offset: 0.5
+  }
+}
+layer {
+  name: "conv8_2_mbox_loc"
+  type: "Convolution"
+  bottom: "conv8_2_h"
+  top: "conv8_2_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 16
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv8_2_mbox_loc_perm"
+  type: "Permute"
+  bottom: "conv8_2_mbox_loc"
+  top: "conv8_2_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv8_2_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "conv8_2_mbox_loc_perm"
+  top: "conv8_2_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv8_2_mbox_conf"
+  type: "Convolution"
+  bottom: "conv8_2_h"
+  top: "conv8_2_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 8 # 84
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv8_2_mbox_conf_perm"
+  type: "Permute"
+  bottom: "conv8_2_mbox_conf"
+  top: "conv8_2_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv8_2_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "conv8_2_mbox_conf_perm"
+  top: "conv8_2_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv8_2_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "conv8_2_h"
+  bottom: "data"
+  top: "conv8_2_mbox_priorbox"
+  prior_box_param {
+    min_size: 213.0
+    max_size: 264.0
+    aspect_ratio: 2
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 100
+    offset: 0.5
+  }
+}
+layer {
+  name: "conv9_2_mbox_loc"
+  type: "Convolution"
+  bottom: "conv9_2_h"
+  top: "conv9_2_mbox_loc"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 16
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv9_2_mbox_loc_perm"
+  type: "Permute"
+  bottom: "conv9_2_mbox_loc"
+  top: "conv9_2_mbox_loc_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv9_2_mbox_loc_flat"
+  type: "Flatten"
+  bottom: "conv9_2_mbox_loc_perm"
+  top: "conv9_2_mbox_loc_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv9_2_mbox_conf"
+  type: "Convolution"
+  bottom: "conv9_2_h"
+  top: "conv9_2_mbox_conf"
+  param {
+    lr_mult: 1
+    decay_mult: 1
+  }
+  param {
+    lr_mult: 2
+    decay_mult: 0
+  }
+  convolution_param {
+    num_output: 8 # 84
+    pad: 1
+    kernel_size: 3
+    stride: 1
+    weight_filler {
+      type: "xavier"
+    }
+    bias_filler {
+      type: "constant"
+      value: 0
+    }
+  }
+}
+layer {
+  name: "conv9_2_mbox_conf_perm"
+  type: "Permute"
+  bottom: "conv9_2_mbox_conf"
+  top: "conv9_2_mbox_conf_perm"
+  permute_param {
+    order: 0
+    order: 2
+    order: 3
+    order: 1
+  }
+}
+layer {
+  name: "conv9_2_mbox_conf_flat"
+  type: "Flatten"
+  bottom: "conv9_2_mbox_conf_perm"
+  top: "conv9_2_mbox_conf_flat"
+  flatten_param {
+    axis: 1
+  }
+}
+layer {
+  name: "conv9_2_mbox_priorbox"
+  type: "PriorBox"
+  bottom: "conv9_2_h"
+  bottom: "data"
+  top: "conv9_2_mbox_priorbox"
+  prior_box_param {
+    min_size: 264.0
+    max_size: 315.0
+    aspect_ratio: 2
+    flip: true
+    clip: false
+    variance: 0.1
+    variance: 0.1
+    variance: 0.2
+    variance: 0.2
+    step: 300
+    offset: 0.5
+  }
+}
+layer {
+  name: "mbox_loc"
+  type: "Concat"
+  bottom: "conv4_3_norm_mbox_loc_flat"
+  bottom: "fc7_mbox_loc_flat"
+  bottom: "conv6_2_mbox_loc_flat"
+  bottom: "conv7_2_mbox_loc_flat"
+  bottom: "conv8_2_mbox_loc_flat"
+  bottom: "conv9_2_mbox_loc_flat"
+  top: "mbox_loc"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "mbox_conf"
+  type: "Concat"
+  bottom: "conv4_3_norm_mbox_conf_flat"
+  bottom: "fc7_mbox_conf_flat"
+  bottom: "conv6_2_mbox_conf_flat"
+  bottom: "conv7_2_mbox_conf_flat"
+  bottom: "conv8_2_mbox_conf_flat"
+  bottom: "conv9_2_mbox_conf_flat"
+  top: "mbox_conf"
+  concat_param {
+    axis: 1
+  }
+}
+layer {
+  name: "mbox_priorbox"
+  type: "Concat"
+  bottom: "conv4_3_norm_mbox_priorbox"
+  bottom: "fc7_mbox_priorbox"
+  bottom: "conv6_2_mbox_priorbox"
+  bottom: "conv7_2_mbox_priorbox"
+  bottom: "conv8_2_mbox_priorbox"
+  bottom: "conv9_2_mbox_priorbox"
+  top: "mbox_priorbox"
+  concat_param {
+    axis: 2
+  }
+}
+
+layer {
+  name: "mbox_conf_reshape"
+  type: "Reshape"
+  bottom: "mbox_conf"
+  top: "mbox_conf_reshape"
+  reshape_param {
+    shape {
+      dim: 0
+      dim: -1
+      dim: 2
+    }
+  }
+}
+layer {
+  name: "mbox_conf_softmax"
+  type: "Softmax"
+  bottom: "mbox_conf_reshape"
+  top: "mbox_conf_softmax"
+  softmax_param {
+    axis: 2
+  }
+}
+layer {
+  name: "mbox_conf_flatten"
+  type: "Flatten"
+  bottom: "mbox_conf_softmax"
+  top: "mbox_conf_flatten"
+  flatten_param {
+    axis: 1
+  }
+}
+
+layer {
+  name: "detection_out"
+  type: "DetectionOutput"
+  bottom: "mbox_loc"
+  bottom: "mbox_conf_flatten"
+  bottom: "mbox_priorbox"
+  top: "detection_out"
+  include {
+    phase: TEST
+  }
+  detection_output_param {
+    num_classes: 2
+    share_location: true
+    background_label_id: 0
+    nms_param {
+      nms_threshold: 0.3
+      top_k: 400
+    }
+    code_type: CENTER_SIZE
+    keep_top_k: 200
+    confidence_threshold: 0.01
+  }
+}
diff --git a/plugins/extract/detect/.cache/res10_300x300_ssd_iter_140000_fp16.caffemodel b/plugins/extract/detect/.cache/res10_300x300_ssd_iter_140000_fp16.caffemodel
new file mode 100644
index 0000000..0e9cd4a
Binary files /dev/null and b/plugins/extract/detect/.cache/res10_300x300_ssd_iter_140000_fp16.caffemodel differ
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index 5f72fcf..057b9c5 100755
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -106,6 +106,7 @@ class Detector():
             are passed back to parent.
             Do not override """
         try:
+            logger.debug("Executing detector run function")
             self.detect_faces(*args, **kwargs)
         except Exception as err:  # pylint: disable=broad-except
             logger.error("Caught exception in child process: %s: %s", os.getpid(), str(err))
@@ -147,9 +148,14 @@ class Detector():
         return retval
 
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
-    def compile_detection_image(self, input_image, is_square, scale_up, to_rgb):
+    def compile_detection_image(self, input_image,
+                                is_square=False, scale_up=False, to_rgb=False, to_grayscale=False):
         """ Compile the detection image """
-        image = input_image[:, :, ::-1].copy() if to_rgb else input_image.copy()
+        image = input_image.copy()
+        if to_rgb:
+            image = image[:, :, ::-1]
+        elif to_grayscale:
+            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # pylint: disable=no-member
         scale = self.set_scale(image, is_square=is_square, scale_up=scale_up)
         image = self.scale_image(image, scale)
         return [image, scale]
diff --git a/plugins/extract/detect/cv2_dnn.py b/plugins/extract/detect/cv2_dnn.py
new file mode 100755
index 0000000..de13e76
--- /dev/null
+++ b/plugins/extract/detect/cv2_dnn.py
@@ -0,0 +1,111 @@
+#!/usr/bin/env python3
+""" OpenCV DNN Face detection plugin """
+import os
+from time import sleep
+
+import numpy as np
+
+from ._base import cv2, Detector, dlib, logger
+
+
+class Detect(Detector):
+    """ CV2 DNN detector for face recognition """
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+        self.parent_is_pool = True
+        self.target = (300, 300)  # Doesn't use VRAM
+        self.vram = 0
+        self.detector = None
+        self.confidence = self.config["confidence"] / 100
+
+    def set_model_path(self):
+        """ CV2 DNN model file """
+        model_path = os.path.join(self.cachepath, "res10_300x300_ssd_iter_140000_fp16.caffemodel")
+        if not os.path.exists(model_path):
+            raise Exception("Error: Unable to find {}, reinstall "
+                            "the lib!".format(model_path))
+        logger.debug("Loading model: '%s'", model_path)
+        return model_path
+
+    def initialize(self, *args, **kwargs):
+        """ Calculate batch size """
+        super().initialize(*args, **kwargs)
+        logger.info("Initializing CV2-DNN Detector...")
+        logger.verbose("Using CPU for detection")
+
+        config_file = os.path.join(self.cachepath, "deploy.prototxt")
+        self.detector = cv2.dnn.readNetFromCaffe(config_file,  # pylint: disable=no-member
+                                                 self.model_path)
+        self.detector.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)  # pylint: disable=no-member
+        self.init = True
+        logger.info("Initialized CV2-DNN Detector...")
+
+    def detect_faces(self, *args, **kwargs):
+        """ Detect faces in grayscale image """
+        super().detect_faces(*args, **kwargs)
+        while True:
+            item = self.get_item()
+            if item == "EOF":
+                break
+            logger.trace("Detecting faces: %s", item["filename"])
+            [detect_image, scale] = self.compile_detection_image(item["image"],
+                                                                 is_square=True,
+                                                                 scale_up=True)
+            height, width = detect_image.shape[:2]
+            for angle in self.rotation:
+                current_image, rotmat = self.rotate_image(detect_image, angle)
+                logger.trace("Detecting faces")
+
+                blob = cv2.dnn.blobFromImage(current_image,  # pylint: disable=no-member
+                                             1.0,
+                                             self.target,
+                                             [104, 117, 123],
+                                             False,
+                                             False)
+                self.detector.setInput(blob)
+                detected = self.detector.forward()
+                faces = list()
+                for i in range(detected.shape[2]):
+                    confidence = detected[0, 0, i, 2]
+                    if confidence >= self.confidence:
+                        logger.trace("Accepting due to confidence %s >= %s",
+                                     confidence, self.confidence)
+                        faces.append([(detected[0, 0, i, 3] * width),
+                                      (detected[0, 0, i, 4] * height),
+                                      (detected[0, 0, i, 5] * width),
+                                      (detected[0, 0, i, 6] * height)])
+
+                logger.trace("Detected faces: %s", [face for face in faces])
+
+                if angle != 0 and faces:
+                    logger.verbose("found face(s) by rotating image %s degrees", angle)
+
+                if faces:
+                    break
+
+            detected_faces = self.process_output(faces, rotmat, scale)
+            item["detected_faces"] = detected_faces
+            self.finalize(item)
+
+        if item == "EOF":
+            sleep(3)  # Wait for all processes to finish before EOF (hacky!)
+            self.queues["out"].put("EOF")
+        logger.debug("Detecting Faces Complete")
+
+    def process_output(self, faces, rotation_matrix, scale):
+        """ Compile found faces for output """
+        logger.trace("Processing Output: (faces: %s, rotation_matrix: %s)",
+                     faces, rotation_matrix)
+
+        faces = [dlib.rectangle(  # pylint: disable=c-extension-no-member
+            int(face[0]), int(face[1]), int(face[2]), int(face[3])) for face in faces]
+        if isinstance(rotation_matrix, np.ndarray):
+            faces = [self.rotate_rect(face, rotation_matrix)
+                     for face in faces]
+        detected = [dlib.rectangle(  # pylint: disable=c-extension-no-member
+            int(face.left() / scale), int(face.top() / scale),
+            int(face.right() / scale), int(face.bottom() / scale))
+                    for face in faces]
+
+        logger.trace("Processed Output: %s", detected)
+        return detected
diff --git a/plugins/extract/detect/dlib_cnn.py b/plugins/extract/detect/dlib_cnn.py
deleted file mode 100755
index d5b1f8a..0000000
--- a/plugins/extract/detect/dlib_cnn.py
+++ /dev/null
@@ -1,209 +0,0 @@
-#!/usr/bin/env python3
-""" DLIB CNN Face detection plugin """
-
-import numpy as np
-import face_recognition_models
-
-from ._base import Detector, dlib, logger
-
-
-class Detect(Detector):
-    """ Dlib detector for face recognition """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.target = (1792, 1792)  # Uses approx 1805MB of VRAM
-        self.vram = 1600  # Lower as batch size of 2 gives wiggle room
-        self.detector = None
-
-    @staticmethod
-    def compiled_for_cuda():
-        """ Return a message on DLIB Cuda Compilation status """
-        cuda = dlib.DLIB_USE_CUDA  # pylint: disable=c-extension-no-member
-        msg = "DLib is "
-        if not cuda:
-            msg += "NOT "
-        msg += "compiled to use CUDA"
-        logger.verbose(msg)
-        return cuda
-
-    def set_model_path(self):
-        """ Model path handled by face_recognition_models """
-        model_path = face_recognition_models.cnn_face_detector_model_location()
-        logger.debug("Loading model: '%s'", model_path)
-        return model_path
-
-    def initialize(self, *args, **kwargs):
-        """ Calculate batch size """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.verbose("Initializing Dlib-CNN Detector...")
-            self.detector = dlib.cnn_face_detection_model_v1(  # pylint: disable=c-extension-no-member
-                self.model_path)
-            is_cuda = self.compiled_for_cuda()
-            if is_cuda:
-                logger.debug("Using GPU")
-                _, vram_free, _ = self.get_vram_free()
-            else:
-                logger.verbose("Using CPU")
-                vram_free = 2048
-
-            # Batch size of 2 actually uses about 338MB less than a single image??
-            # From there batches increase at ~680MB per item in the batch
-
-            self.batch_size = int(((vram_free - self.vram) / 680) + 2)
-
-            if self.batch_size < 1:
-                raise ValueError("Insufficient VRAM available to continue "
-                                 "({}MB)".format(int(vram_free)))
-
-            logger.verbose("Processing in batches of %s", self.batch_size)
-
-            self.init.set()
-            logger.info("Initialized Dlib-CNN Detector...")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in rgb image """
-        super().detect_faces(*args, **kwargs)
-        while True:
-            exhausted, batch = self.get_batch()
-            if not batch:
-                break
-            filenames = list()
-            images = list()
-            for item in batch:
-                filenames.append(item["filename"])
-                images.append(item["image"])
-
-            [detect_images, scales] = self.compile_detection_images(images)
-            batch_detected = self.detect_batch(detect_images)
-            processed = self.process_output(batch_detected,
-                                            indexes=None,
-                                            rotation_matrix=None,
-                                            output=None,
-                                            scales=scales)
-            if not all(faces for faces in processed) and self.rotation != [0]:
-                processed = self.process_rotations(detect_images, processed, scales)
-            for idx, faces in enumerate(processed):
-                filename = filenames[idx]
-                for b_idx, item in enumerate(batch):
-                    if item["filename"] == filename:
-                        output = item
-                        del_idx = b_idx
-                        break
-                output["detected_faces"] = faces
-                self.finalize(output)
-                del batch[del_idx]
-            if exhausted:
-                break
-        self.queues["out"].put("EOF")
-        del self.detector  # Free up VRAM
-        logger.debug("Detecting Faces complete")
-
-    def compile_detection_images(self, images):
-        """ Compile the detection images into batches """
-        logger.trace("Compiling Detection Images: %s", len(images))
-        detect_images = list()
-        scales = list()
-        for image in images:
-            detect_image, scale = self.compile_detection_image(image, True, True, True)
-            detect_images.append(detect_image)
-            scales.append(scale)
-        logger.trace("Compiled Detection Images")
-        return [detect_images, scales]
-
-    def detect_batch(self, detect_images, disable_message=False):
-        """ Pass the batch through detector for consistently sized images
-            or each image separately for inconsitently sized images """
-        logger.trace("Detecting Batch")
-        can_batch = self.check_batch_dims(detect_images)
-        if can_batch:
-            logger.trace("Valid for batching")
-            batch_detected = self.detector(detect_images, 0)
-        else:
-            if not disable_message:
-                logger.verbose("Batch has inconsistently sized images. Processing one "
-                               "image at a time")
-            batch_detected = dlib.mmod_rectangless(  # pylint: disable=c-extension-no-member
-                [self.detector(detect_image, 0) for detect_image in detect_images])
-        logger.trace("Detected Batch: %s", [item for item in batch_detected])
-        return batch_detected
-
-    @staticmethod
-    def check_batch_dims(images):
-        """ Check all images are the same size for batching """
-        dims = set(frame.shape[:2] for frame in images)
-        logger.trace("Batch Dimensions: %s", dims)
-        return len(dims) == 1
-
-    def process_output(self, batch_detected,
-                       indexes=None, rotation_matrix=None, output=None, scales=None):
-        """ Process the output images """
-        logger.trace("Processing Output: (batch_detected: %s, indexes: %s, rotation_matrix: %s, "
-                     "output: %s, scales: %s",
-                     batch_detected, indexes, rotation_matrix, output, scales)
-        output = output if output else list()
-        for idx, faces in enumerate(batch_detected):
-            detected_faces = list()
-            scale = scales[idx]
-
-            if isinstance(rotation_matrix, np.ndarray):
-                faces = [self.rotate_rect(face.rect, rotation_matrix)
-                         for face in faces]
-
-            for face in faces:
-                face = self.convert_to_dlib_rectangle(face)
-                face = dlib.rectangle(  # pylint: disable=c-extension-no-member
-                    int(face.left() / scale),
-                    int(face.top() / scale),
-                    int(face.right() / scale),
-                    int(face.bottom() / scale))
-                detected_faces.append(face)
-            if indexes:
-                target = indexes[idx]
-                output[target] = detected_faces
-            else:
-                output.append(detected_faces)
-        logger.trace("Processed Output: %s", output)
-        return output
-
-    def process_rotations(self, detect_images, processed, scales):
-        """ Rotate frames missing faces until face is found """
-        logger.trace("Processing Rotations")
-        for angle in self.rotation:
-            if all(faces for faces in processed):
-                break
-            if angle == 0:
-                continue
-            reprocess, indexes, rotmat = self.compile_reprocess(
-                processed,
-                detect_images,
-                angle)
-
-            batch_detected = self.detect_batch(reprocess, disable_message=True)
-            if any(item for item in batch_detected):
-                logger.verbose("found face(s) by rotating image %s degrees", angle)
-            processed = self.process_output(batch_detected,
-                                            indexes=indexes,
-                                            rotation_matrix=rotmat,
-                                            output=processed,
-                                            scales=scales)
-        logger.trace("Processed Rotations")
-        return processed
-
-    def compile_reprocess(self, processed, detect_images, angle):
-        """ Rotate images which did not find a face for reprocessing """
-        logger.trace("Compile images for reprocessing")
-        indexes = list()
-        to_detect = list()
-        for idx, faces in enumerate(processed):
-            if faces:
-                continue
-            image = detect_images[idx]
-            rot_image, rot_matrix = self.rotate_image_by_angle(image, angle)
-            to_detect.append(rot_image)
-            indexes.append(idx)
-        logger.trace("Compiled images for reprocessing")
-        return to_detect, indexes, rot_matrix
diff --git a/plugins/extract/detect/dlib_hog.py b/plugins/extract/detect/dlib_hog.py
deleted file mode 100755
index 0df0c97..0000000
--- a/plugins/extract/detect/dlib_hog.py
+++ /dev/null
@@ -1,76 +0,0 @@
-#!/usr/bin/env python3
-""" DLIB CNN Face detection plugin """
-from time import sleep
-
-import numpy as np
-
-from ._base import Detector, dlib, logger
-
-
-class Detect(Detector):
-    """ Dlib detector for face recognition """
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-        self.parent_is_pool = True
-        self.target = (2048, 2048)  # Doesn't use VRAM
-        self.vram = 0
-        self.detector = dlib.get_frontal_face_detector()  # pylint: disable=c-extension-no-member
-        self.iterator = None
-
-    def set_model_path(self):
-        """ No model for dlib Hog """
-        pass
-
-    def initialize(self, *args, **kwargs):
-        """ Calculate batch size """
-        super().initialize(*args, **kwargs)
-        logger.info("Initializing Dlib-HOG Detector...")
-        logger.verbose("Using CPU for detection")
-        self.init = True
-        logger.info("Initialized Dlib-HOG Detector...")
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in rgb image """
-        super().detect_faces(*args, **kwargs)
-        while True:
-            item = self.get_item()
-            if item == "EOF":
-                break
-            logger.trace("Detecting faces: %s", item["filename"])
-            [detect_image, scale] = self.compile_detection_image(item["image"], True, True, True)
-
-            for angle in self.rotation:
-                current_image, rotmat = self.rotate_image(detect_image, angle)
-
-                logger.trace("Detecting faces")
-                faces = self.detector(current_image, 0)
-                logger.trace("Detected faces: %s", [face for face in faces])
-
-                if angle != 0 and faces.any():
-                    logger.verbose("found face(s) by rotating image %s degrees", angle)
-
-                if faces:
-                    break
-
-            detected_faces = self.process_output(faces, rotmat, scale)
-            item["detected_faces"] = detected_faces
-            self.finalize(item)
-
-        if item == "EOF":
-            sleep(3)  # Wait for all processes to finish before EOF (hacky!)
-            self.queues["out"].put("EOF")
-        logger.debug("Detecting Faces Complete")
-
-    def process_output(self, faces, rotation_matrix, scale):
-        """ Compile found faces for output """
-        logger.trace("Processing Output: (faces: %s, rotation_matrix: %s)",
-                     faces, rotation_matrix)
-        if isinstance(rotation_matrix, np.ndarray):
-            faces = [self.rotate_rect(face, rotation_matrix)
-                     for face in faces]
-        detected = [dlib.rectangle(  # pylint: disable=c-extension-no-member
-            int(face.left() / scale), int(face.top() / scale),
-            int(face.right() / scale), int(face.bottom() / scale))
-                    for face in faces]
-        logger.trace("Processed Output: %s", detected)
-        return detected
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
index 09b3bfb..e1d005b 100755
--- a/plugins/extract/detect/mtcnn.py
+++ b/plugins/extract/detect/mtcnn.py
@@ -143,7 +143,7 @@ class Detect(Detector):
             if item == "EOF":
                 break
             logger.trace("Detecting faces: '%s'", item["filename"])
-            [detect_image, scale] = self.compile_detection_image(item["image"], False, False, True)
+            [detect_image, scale] = self.compile_detection_image(item["image"], to_rgb=True)
 
             for angle in self.rotation:
                 current_image, rotmat = self.rotate_image(detect_image, angle)
diff --git a/plugins/extract/detect/s3fd.py b/plugins/extract/detect/s3fd.py
index a525196..c8e5c58 100644
--- a/plugins/extract/detect/s3fd.py
+++ b/plugins/extract/detect/s3fd.py
@@ -96,7 +96,7 @@ class Detect(Detector):
             if item == "EOF":
                 break
             logger.trace("Detecting faces: '%s'", item["filename"])
-            detect_image, scale = self.compile_detection_image(item["image"], True, False, False)
+            detect_image, scale = self.compile_detection_image(item["image"], is_square=True)
             for angle in self.rotation:
                 current_image, rotmat = self.rotate_image(detect_image, angle)
                 faces = self.model.detect_face(current_image)
diff --git a/scripts/convert.py b/scripts/convert.py
index 944d917..12c7f9c 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -87,7 +87,7 @@ class Convert():
     def add_queues(self):
         """ Add the queues for convert """
         logger.debug("Adding queues. Queue size: %s", self.queue_size)
-        for qname in ("convert_in", "save", "patch"):
+        for qname in ("convert_in", "convert_out", "patch"):
             queue_manager.add_queue(qname, self.queue_size)
 
     def process(self):
@@ -106,7 +106,7 @@ class Convert():
     def convert_images(self):
         """ Convert the images """
         logger.debug("Converting images")
-        save_queue = queue_manager.get_queue("save")
+        save_queue = queue_manager.get_queue("convert_out")
         patch_queue = queue_manager.get_queue("patch")
         pool = PoolProcess(self.converter.process, patch_queue, save_queue,
                            processes=self.pool_processes)
@@ -238,13 +238,13 @@ class DiskIO():
         """ Set on the fly extraction """
         logger.debug("Loading extractor")
         logger.warning("No Alignments file found. Extracting on the fly.")
-        logger.warning("NB: This will use the inferior dlib-hog for extraction "
+        logger.warning("NB: This will use the inferior cv2-dnn for extraction "
                        "and dlib pose predictor for landmarks. It is recommended "
                        "to perfom Extract first for superior results")
-        extract_args = {"detector": "dlib-hog",
+        extract_args = {"detector": "cv2_dnn",
                         "aligner": "dlib",
                         "loglevel": self.args.loglevel}
-        self.extractor = Extractor(None, extract_args)
+        self.extractor = Extractor(None, converter_args=extract_args)
         self.extractor.launch_detector()
         self.extractor.launch_aligner()
         logger.debug("Loaded extractor")
@@ -260,7 +260,12 @@ class DiskIO():
     def add_queue(self, task):
         """ Add the queue to queue_manager and set queue attribute """
         logger.debug("Adding queue for task: '%s'", task)
-        q_name = "convert_in" if task == "load" else task
+        if task == "load":
+            q_name = "convert_in"
+        elif task == "save":
+            q_name = "convert_out"
+        else:
+            q_name = task
         setattr(self, "{}_queue".format(task), queue_manager.get_queue(q_name))
         logger.debug("Added queue for task: '%s'", task)
 
diff --git a/scripts/extract.py b/scripts/extract.py
index 8a93e18..501f459 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -48,7 +48,7 @@ class Extract():
         """ Perform the extraction process """
         logger.info('Starting, this may take a while...')
         Utils.set_verbosity(self.args.loglevel)
-#        queue_manager.debug_monitor(1)
+        queue_manager.debug_monitor(1)
         self.threaded_io("load")
         save_thread = self.threaded_io("save")
         self.run_extraction()
@@ -75,7 +75,7 @@ class Extract():
     def load_images(self):
         """ Load the images """
         logger.debug("Load Images: Start")
-        load_queue = queue_manager.get_queue("load")
+        load_queue = queue_manager.get_queue("extract_in")
         idx = 0
         for filename, image in self.images.load():
             idx += 1
@@ -121,7 +121,7 @@ class Extract():
     def save_faces():
         """ Save the generated faces """
         logger.debug("Save Faces: Start")
-        save_queue = queue_manager.get_queue("save")
+        save_queue = queue_manager.get_queue("extract_out")
         while True:
             if save_queue.shutdown.is_set():
                 logger.debug("Save Queue: Stop signal received. Terminating")
@@ -142,7 +142,7 @@ class Extract():
 
     def run_extraction(self):
         """ Run Face Detection """
-        save_queue = queue_manager.get_queue("save")
+        save_queue = queue_manager.get_queue("extract_out")
         to_process = self.process_item_count()
         frame_no = 0
         size = self.args.size if hasattr(self.args, "size") else 256
@@ -257,7 +257,8 @@ class Extract():
 class Plugins():
     """ Detector and Aligner Plugins and queues """
     def __init__(self, arguments, converter_args=None):
-        logger.debug("Initializing %s", self.__class__.__name__)
+        logger.debug("Initializing %s: (converter_args: %s)",
+                     self.__class__.__name__, converter_args)
         self.args = arguments
         self.converter_args = converter_args  # Arguments from converter for on the fly extract
         if converter_args is not None:
@@ -310,7 +311,13 @@ class Plugins():
             size = 0
             if task == "load" or (not self.is_parallel and task == "detect"):
                 size = 100
-            queue_manager.add_queue(task, maxsize=size)
+            if task == "load":
+                q_name = "extract_in"
+            elif task == "save":
+                q_name = "extract_out"
+            else:
+                q_name = task
+            queue_manager.add_queue(q_name, maxsize=size)
 
     def load_detector(self):
         """ Set global arguments and load detector plugin """
@@ -380,7 +387,7 @@ class Plugins():
         """ Launch the face detector """
         logger.debug("Launching Detector")
         out_queue = queue_manager.get_queue("detect")
-        kwargs = {"in_queue": queue_manager.get_queue("load"),
+        kwargs = {"in_queue": queue_manager.get_queue("extract_in"),
                   "out_queue": out_queue}
         if self.converter_args:
             kwargs["processes"] = 1
