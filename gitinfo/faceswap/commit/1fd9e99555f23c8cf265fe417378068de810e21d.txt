commit 1fd9e99555f23c8cf265fe417378068de810e21d
Author: Clorr <Github@clorr.fr>
Date:   Wed Feb 7 15:00:49 2018 +0100

    Adding GAN plugin (#102)
    
    Update GAN plugin to latest official version

diff --git a/plugins/Convert_GAN.py b/plugins/Convert_GAN.py
new file mode 100644
index 0000000..340eb0a
--- /dev/null
+++ b/plugins/Convert_GAN.py
@@ -0,0 +1,18 @@
+# Based on the https://github.com/shaoanlu/faceswap-GAN repo (master/FaceSwap_GAN_v2_train.ipynb)
+
+import cv2
+import numpy
+
+class Convert(object):
+    def __init__(self, encoder, **kwargs):
+        self.encoder = encoder
+
+    def patch_image( self, original, face_detected ):
+        face = cv2.resize(face_detected.image, (64, 64))
+        face = numpy.expand_dims(face, 0) / 255.0 * 2 - 1
+        mask, new_face = self.encoder(face)
+        new_face = mask * new_face + (1 - mask) * face
+        new_face = numpy.clip((new_face[0] + 1) * 255 / 2, 0, 255).astype('uint8')
+
+        original[face_detected.y: face_detected.y + face_detected.h, face_detected.x: face_detected.x + face_detected.w] = cv2.resize(new_face, (face_detected.w, face_detected.h))
+        return original
diff --git a/plugins/Model_GAN/Model.py b/plugins/Model_GAN/Model.py
new file mode 100644
index 0000000..666ab60
--- /dev/null
+++ b/plugins/Model_GAN/Model.py
@@ -0,0 +1,179 @@
+# Based on the https://github.com/shaoanlu/faceswap-GAN repo (master/temp/faceswap_GAN_keras.ipynb)
+
+from keras.models import Model
+from keras.layers import *
+from keras.layers.advanced_activations import LeakyReLU
+from keras.activations import relu
+from keras.initializers import RandomNormal
+from keras.applications import *
+from keras.optimizers import Adam
+
+from lib.PixelShuffler import PixelShuffler
+
+netGAH5 = '/netGA_GAN.h5'
+netGBH5 = '/netGB_GAN.h5'
+netDAH5 = '/netDA_GAN.h5'
+netDBH5 = '/netDB_GAN.h5'
+
+class GANModel():
+    img_size = 64 
+    channels = 3
+    img_shape = (img_size, img_size, channels)
+    encoded_dim = 1024
+    
+    def __init__(self, model_dir):
+        self.model_dir = model_dir
+
+        optimizer = Adam(1e-4, 0.5)
+
+        # Build and compile the discriminator
+        self.netDA, self.netDB = self.build_discriminator()
+
+        # For the adversarial_autoencoder model we will only train the generator
+        self.netDA.trainable = False
+        self.netDB.trainable = False
+        
+        self.netDA.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
+        self.netDB.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])
+
+        # Build and compile the generator
+        self.netGA, self.netGB = self.build_generator()
+        self.netGA.compile(loss=['mae', 'mse'], optimizer=optimizer)
+        self.netGB.compile(loss=['mae', 'mse'], optimizer=optimizer)
+
+        img = Input(shape=self.img_shape)
+        alphaA, reconstructed_imgA = self.netGA(img)
+        alphaB, reconstructed_imgB = self.netGB(img)
+
+        def one_minus(x): return 1 - x
+        # masked_img = alpha * reconstructed_img + (1 - alpha) * img
+        masked_imgA = add([multiply([alphaA, reconstructed_imgA]), multiply([Lambda(one_minus)(alphaA), img])])
+        masked_imgB = add([multiply([alphaB, reconstructed_imgB]), multiply([Lambda(one_minus)(alphaB), img])])
+        out_discriminatorA = self.netDA(concatenate([masked_imgA, img], axis=-1))
+        out_discriminatorB = self.netDB(concatenate([masked_imgB, img], axis=-1))
+
+        # The adversarial_autoencoder model  (stacked generator and discriminator) takes
+        # img as input => generates encoded represenation and reconstructed image => determines validity 
+        self.adversarial_autoencoderA = Model(img, [reconstructed_imgA, out_discriminatorA])
+        self.adversarial_autoencoderB = Model(img, [reconstructed_imgB, out_discriminatorB])
+        self.adversarial_autoencoderA.compile(loss=['mae', 'mse'],
+                                              loss_weights=[1, 0.5],
+                                              optimizer=optimizer)
+        self.adversarial_autoencoderB.compile(loss=['mae', 'mse'],
+                                              loss_weights=[1, 0.5],
+                                              optimizer=optimizer)
+
+    def converter(self, swap):
+        predictor = self.netGB if not swap else self.netGA
+        return lambda img: predictor.predict(img)
+
+    def build_generator(self):
+        def conv_block(input_tensor, f):
+            x = input_tensor
+            x = Conv2D(f, kernel_size=3, strides=2, kernel_initializer=RandomNormal(0, 0.02), 
+                       use_bias=False, padding="same")(x)
+            x = LeakyReLU(alpha=0.2)(x)
+            return x
+
+        def res_block(input_tensor, f):
+            x = input_tensor
+            x = Conv2D(f, kernel_size=3, kernel_initializer=RandomNormal(0, 0.02), 
+                       use_bias=False, padding="same")(x)
+            x = LeakyReLU(alpha=0.2)(x)
+            x = Conv2D(f, kernel_size=3, kernel_initializer=RandomNormal(0, 0.02), 
+                       use_bias=False, padding="same")(x)
+            x = add([x, input_tensor])
+            x = LeakyReLU(alpha=0.2)(x)
+            return x
+
+        def upscale_ps(filters, use_norm=True):
+            def block(x):
+                x = Conv2D(filters*4, kernel_size=3, use_bias=False, 
+                           kernel_initializer=RandomNormal(0, 0.02), padding='same' )(x)
+                x = LeakyReLU(0.1)(x)
+                x = PixelShuffler()(x)
+                return x
+            return block
+
+        def Encoder(img_shape):
+            inp = Input(shape=img_shape)
+            x = Conv2D(64, kernel_size=5, kernel_initializer=RandomNormal(0, 0.02), 
+                       use_bias=False, padding="same")(inp)
+            x = conv_block(x,128)
+            x = conv_block(x,256)
+            x = conv_block(x,512) 
+            x = conv_block(x,1024)
+            x = Dense(1024)(Flatten()(x))
+            x = Dense(4*4*1024)(x)
+            x = Reshape((4, 4, 1024))(x)
+            out = upscale_ps(512)(x)
+            return Model(inputs=inp, outputs=out)
+
+        def Decoder_ps(img_shape):
+            nc_in = 512
+            input_size = img_shape[0]//8
+            inp = Input(shape=(input_size, input_size, nc_in))
+            x = inp
+            x = upscale_ps(256)(x)
+            x = upscale_ps(128)(x)
+            x = upscale_ps(64)(x)
+            x = res_block(x, 64)
+            x = res_block(x, 64)
+            alpha = Conv2D(1, kernel_size=5, padding='same', activation="sigmoid")(x)
+            rgb = Conv2D(3, kernel_size=5, padding='same', activation="tanh")(x)
+            return Model(inp, [alpha, rgb])
+        
+        encoder = Encoder(self.img_shape)
+        decoder_A = Decoder_ps(self.img_shape)
+        decoder_B = Decoder_ps(self.img_shape)    
+        x = Input(shape=self.img_shape)
+        netGA = Model(x, decoder_A(encoder(x)))
+        netGB = Model(x, decoder_B(encoder(x)))           
+        try:
+            netGA.load_weights(self.model_dir + netGAH5)
+            netGB.load_weights(self.model_dir + netGBH5)
+            print ("Generator models loaded.")
+        except:
+            print ("Generator weights files not found.")
+            pass
+        return netGA, netGB, 
+
+    def build_discriminator(self):  
+        def conv_block_d(input_tensor, f, use_instance_norm=True):
+            x = input_tensor
+            x = Conv2D(f, kernel_size=4, strides=2, kernel_initializer=RandomNormal(0, 0.02), 
+                       use_bias=False, padding="same")(x)
+            x = LeakyReLU(alpha=0.2)(x)
+            return x   
+        def Discriminator(img_shape):
+            inp = Input(shape=(img_shape[0], img_shape[1], img_shape[2]*2))
+            x = conv_block_d(inp, 64, False)
+            x = conv_block_d(x, 128, False)
+            x = conv_block_d(x, 256, False)
+            out = Conv2D(1, kernel_size=4, kernel_initializer=RandomNormal(0, 0.02), 
+                         use_bias=False, padding="same", activation="sigmoid")(x)   
+            return Model(inputs=[inp], outputs=out) 
+        
+        netDA = Discriminator(self.img_shape)
+        netDB = Discriminator(self.img_shape)        
+        try:
+            netDA.load_weights(self.model_dir + netDAH5) 
+            netDB.load_weights(self.model_dir + netDBH5) 
+            print ("Discriminator models loaded.")
+        except:
+            print ("Discriminator weights files not found.")
+            pass
+        return netDA, netDB    
+    
+    def load(self, swapped):
+        if swapped:
+            print("swapping not supported on GAN")
+            # TODO load is done in __init__ => look how to swap if possible
+        return True
+    
+    def save_weights(self):
+        self.netGA.save_weights(self.model_dir + netGAH5)
+        self.netGB.save_weights(self.model_dir + netGBH5)
+        self.netDA.save_weights(self.model_dir + netDAH5)
+        self.netDB.save_weights(self.model_dir + netDBH5)
+        print ("Models saved.")
\ No newline at end of file
diff --git a/plugins/Model_GAN/Trainer.py b/plugins/Model_GAN/Trainer.py
new file mode 100644
index 0000000..01b16dd
--- /dev/null
+++ b/plugins/Model_GAN/Trainer.py
@@ -0,0 +1,153 @@
+import time
+import cv2
+import numpy as np
+
+from lib.training_data import TrainingDataGenerator, stack_images
+
+class GANTrainingDataGenerator(TrainingDataGenerator):
+    def __init__(self, random_transform_args, coverage):
+        super().__init__(random_transform_args, coverage)
+
+    def color_adjust(self, img):
+        return img / 255.0 * 2 - 1
+
+class Trainer():
+    random_transform_args = {
+        'rotation_range': 20,
+        'zoom_range': 0.05,
+        'shift_range': 0.05,
+        'random_flip': 0.5,
+        }
+
+    def __init__(self, model, fn_A, fn_B, batch_size):
+        assert batch_size % 2 == 0, "batch_size must be an even number"
+        self.batch_size = batch_size
+        self.model = model
+        
+        self.use_mixup = True
+        self.mixup_alpha = 0.2
+
+        generator = GANTrainingDataGenerator(self.random_transform_args, 220)
+        self.train_batchA = generator.minibatchAB(fn_A, batch_size)
+        self.train_batchB = generator.minibatchAB(fn_B, batch_size)
+    
+    def train_one_step(self, iter, viewer):
+        # ---------------------
+        #  Train Discriminators
+        # ---------------------
+
+        # Select a random half batch of images
+        epoch, warped_A, target_A = next(self.train_batchA) 
+        epoch, warped_B, target_B = next(self.train_batchB) 
+
+        # Generate a half batch of new images
+        gen_alphasA, gen_imgsA = self.model.netGA.predict(warped_A)
+        gen_alphasB, gen_imgsB = self.model.netGB.predict(warped_B)
+        #gen_masked_imgsA = gen_alphasA * gen_imgsA + (1 - gen_alphasA) * warped_A
+        #gen_masked_imgsB = gen_alphasB * gen_imgsB + (1 - gen_alphasB) * warped_B
+        gen_masked_imgsA = np.array([gen_alphasA[i] * gen_imgsA[i] + (1 - gen_alphasA[i]) * warped_A[i] 
+                                     for i in range(self.batch_size)])
+        gen_masked_imgsB = np.array([gen_alphasB[i] * gen_imgsB[i] + (1 - gen_alphasB[i]) * warped_B[i]
+                                     for i in range (self.batch_size)])
+
+        valid = np.ones((self.batch_size, ) + self.model.netDA.output_shape[1:])
+        fake = np.zeros((self.batch_size, ) + self.model.netDA.output_shape[1:])
+
+        concat_real_inputA = np.array([np.concatenate([target_A[i], warped_A[i]], axis=-1) 
+                                       for i in range(self.batch_size)])
+        concat_real_inputB = np.array([np.concatenate([target_B[i], warped_B[i]], axis=-1) 
+                                       for i in range(self.batch_size)])
+        concat_fake_inputA = np.array([np.concatenate([gen_masked_imgsA[i], warped_A[i]], axis=-1) 
+                                       for i in range(self.batch_size)])
+        concat_fake_inputB = np.array([np.concatenate([gen_masked_imgsB[i], warped_B[i]], axis=-1) 
+                                       for i in range(self.batch_size)])
+        if self.use_mixup:
+            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)
+            mixup_A = lam * concat_real_inputA + (1 - lam) * concat_fake_inputA
+            mixup_B = lam * concat_real_inputB + (1 - lam) * concat_fake_inputB
+
+        # Train the discriminators
+        #print ("Train the discriminators.")
+        if self.use_mixup:
+            d_lossA = self.model.netDA.train_on_batch(mixup_A, lam * valid)
+            d_lossB = self.model.netDB.train_on_batch(mixup_B, lam * valid)
+        else:
+            d_lossA = self.model.netDA.train_on_batch(np.concatenate([concat_real_inputA, concat_fake_inputA], axis=0), 
+                                                np.concatenate([valid, fake], axis=0))
+            d_lossB = self.model.netDB.train_on_batch(np.concatenate([concat_real_inputB, concat_fake_inputB], axis=0),
+                                                np.concatenate([valid, fake], axis=0))
+
+        # ---------------------
+        #  Train Generators
+        # ---------------------
+
+        # Train the generators
+        #print ("Train the generators.")
+        g_lossA = self.model.adversarial_autoencoderA.train_on_batch(warped_A, [target_A, valid])
+        g_lossB = self.model.adversarial_autoencoderB.train_on_batch(warped_B, [target_B, valid])            
+        
+        print('[%s] [%d/%s][%d] Loss_DA: %f Loss_DB: %f Loss_GA: %f Loss_GB: %f'
+              % (time.strftime("%H:%M:%S"), epoch, "num_epochs", iter, d_lossA[0], d_lossB[0], g_lossA[0], g_lossB[0]),
+              end='\r')
+        
+        if viewer is not None:
+            self.show_sample(viewer)
+    
+    def show_sample(self, display_fn):
+        _, wA, tA = next(self.train_batchA)
+        _, wB, tB = next(self.train_batchB)
+        self.showG(tA, tB, display_fn)
+
+    def showG(self, test_A, test_B, display_fn):
+        def display_fig(name, figure_A, figure_B):
+            figure = np.concatenate([figure_A, figure_B], axis=0 )
+            columns = 4
+            elements = figure.shape[0]
+            figure = figure.reshape((columns,(elements//columns)) + figure.shape[1:])
+            figure = stack_images(figure)
+            figure = np.clip((figure + 1) * 255 / 2, 0, 255).astype('uint8')
+            display_fn(figure, name)
+
+        out_test_A_netGA = self.model.netGA.predict(test_A)
+        out_test_A_netGB = self.model.netGB.predict(test_A)
+        out_test_B_netGA = self.model.netGA.predict(test_B)
+        out_test_B_netGB = self.model.netGB.predict(test_B)
+
+        figure_A = np.stack([
+            test_A,
+            out_test_A_netGA[1],
+            out_test_A_netGB[1],
+            ], axis=1 )
+        figure_B = np.stack([
+            test_B,
+            out_test_B_netGB[1],
+            out_test_B_netGA[1],
+            ], axis=1 )
+        
+        display_fig("raw", figure_A, figure_B)       
+
+        figure_A = np.stack([
+            test_A,
+            np.tile(out_test_A_netGA[0],3) * 2 - 1,
+            np.tile(out_test_A_netGB[0],3) * 2 - 1,
+            ], axis=1 )
+        figure_B = np.stack([
+            test_B,
+            np.tile(out_test_B_netGB[0],3) * 2 - 1,
+            np.tile(out_test_B_netGA[0],3) * 2 - 1,
+            ], axis=1 )
+
+        display_fig("alpha_masks", figure_A, figure_B)
+
+        figure_A = np.stack([
+            test_A,
+            out_test_A_netGA[0] * out_test_A_netGA[1] + (1 - out_test_A_netGA[0]) * test_A,
+            out_test_A_netGB[0] * out_test_A_netGB[1] + (1 - out_test_A_netGB[0]) * test_A,
+            ], axis=1 )
+        figure_B = np.stack([
+            test_B,
+            out_test_B_netGB[0] * out_test_B_netGB[1] + (1 - out_test_B_netGB[0]) * test_B,
+            out_test_B_netGA[0] * out_test_B_netGA[1] + (1 - out_test_B_netGA[0]) * test_B,
+            ], axis=1 )
+        
+        display_fig("masked", figure_A, figure_B)
diff --git a/plugins/Model_GAN/__init__.py b/plugins/Model_GAN/__init__.py
new file mode 100644
index 0000000..0722bba
--- /dev/null
+++ b/plugins/Model_GAN/__init__.py
@@ -0,0 +1,7 @@
+# -*- coding: utf-8 -*-
+
+__author__ = """Based on https://github.com/shaoanlu/"""
+__version__ = '0.1.0'
+
+from .Model import GANModel as Model
+from .Trainer import Trainer
\ No newline at end of file
diff --git a/scripts/convert.py b/scripts/convert.py
index 001592b..a85954a 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -30,7 +30,7 @@ class ConvertImage(DirectoryProcessor):
 
         parser.add_argument('-t', '--trainer',
                             type=str,
-                            choices=("Original", "LowMem"), # case sensitive because this is used to load a plug-in.
+                            choices=("Original", "LowMem", "GAN"), # case sensitive because this is used to load a plug-in.
                             default="Original",
                             help="Select the trainer that was used to create the model.")
                             
@@ -42,7 +42,7 @@ class ConvertImage(DirectoryProcessor):
 
         parser.add_argument('-c', '--converter',
                             type=str,
-                            choices=("Masked", "Adjust"), # case sensitive because this is used to load a plug-in.
+                            choices=("Masked", "Adjust", "GAN"), # case sensitive because this is used to load a plugin.
                             default="Masked",
                             help="Converter to use.")
 
@@ -113,9 +113,16 @@ class ConvertImage(DirectoryProcessor):
     
     def process(self):
         # Original & LowMem models go with Adjust or Masked converter
+        # GAN converter & model must go together
+        # Note: GAN prediction outputs a mask + an image, while other predicts only an image
         model_name = self.arguments.trainer
         conv_name = self.arguments.converter
         
+        if conv_name.startswith("GAN"):
+            assert model_name.startswith("GAN") is True, "GAN converter can only be used with GAN model!"
+        else:
+            assert model_name.startswith("GAN") is False, "GAN model can only be used with GAN converter!"
+
         model = PluginLoader.get_model(model_name)(get_folder(self.arguments.model_dir))
         if not model.load(self.arguments.swap_model):
             print('Model Not Found! A valid model must be provided to continue!')
diff --git a/scripts/train.py b/scripts/train.py
index fbf63e4..6d70e20 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -72,7 +72,7 @@ class TrainingProcessor(object):
                             help="Writes the training result to a file even on preview mode.")
         parser.add_argument('-t', '--trainer',
                             type=str,
-                            choices=("Original", "LowMem"),
+                            choices=("Original", "LowMem", "GAN"),
                             default="Original",
                             help="Select which trainer to use, LowMem for cards < 2gb.")
         parser.add_argument('-bs', '--batch-size',
