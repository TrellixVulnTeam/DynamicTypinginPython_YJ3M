commit 03d419c505ef88086af58c16441ffb8487cb8fe5
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Wed Jul 10 22:35:41 2019 +0000

    Training improvements
    
    Add Optimizer Savings option to offload some optimizer calculations to the CPU
    Add output_shape to NNMeta class for referencing when building models

diff --git a/lib/cli.py b/lib/cli.py
index 66855f2..b1985b6 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -997,25 +997,33 @@ class TrainArgs(FaceSwapArgs):
                               "help": "Disables TensorBoard logging. NB: Disabling logs means "
                                       "that you will not be able to use the graph or analysis "
                                       "for this session in the GUI."})
-        argument_list.append({"opts": ("-pp", "--ping-pong"),
-                              "action": "store_true",
-                              "dest": "pingpong",
-                              "default": False,
-                              "help": "Enable ping pong training. Trains one side at a time, "
-                                      "switching sides at each save iteration. Training will take "
-                                      "2 to 4 times longer, with about a 30%%-50%% reduction in "
-                                      "VRAM useage. NB: Preview won't show until both sides have "
-                                      "been trained once."})
         argument_list.append({"opts": ("-msg", "--memory-saving-gradients"),
                               "action": "store_true",
                               "dest": "memory_saving_gradients",
                               "default": False,
-                              "help": "Trades off VRAM useage against computation time. Can fit "
+                              "help": "Trades off VRAM usage against computation time. Can fit "
                                       "larger models into memory at a cost of slower training "
                                       "speed. 50%%-150%% batch size increase for 20%%-50%% longer "
                                       "training time. NB: Launch time will be significantly "
                                       "delayed. Switching sides using ping-pong training will "
                                       "take longer."})
+        argument_list.append({"opts": ("-o", "--optimizer-savings"),
+                              "dest": "optimizer_savings",
+                              "action": "store_true",
+                              "default": False,
+                              "help": "To save VRAM some optimizer gradient calculations can be "
+                                      "performed on the CPU rather than the GPU. This allows you "
+                                      "to increase batchsize at a training speed cost. Nvidia "
+                                      "only. This option will have no effect for plaidML users."})
+        argument_list.append({"opts": ("-pp", "--ping-pong"),
+                              "action": "store_true",
+                              "dest": "pingpong",
+                              "default": False,
+                              "help": "Enable ping pong training. Trains one side at a time, "
+                                      "switching sides at each save iteration. Training will take "
+                                      "2 to 4 times longer, with about a 30%%-50%% reduction in "
+                                      "VRAM useage. NB: Preview won't show until both sides have "
+                                      "been trained once."})
         argument_list.append({"opts": ("-wl", "--warp-to-landmarks"),
                               "action": "store_true",
                               "dest": "warp_to_landmarks",
diff --git a/lib/model/optimizers.py b/lib/model/optimizers.py
new file mode 100644
index 0000000..3d8bffe
--- /dev/null
+++ b/lib/model/optimizers.py
@@ -0,0 +1,84 @@
+#!/usr/bin/env python3
+""" Optimizers for faceswap.py """
+# Naming convention inherited from Keras so ignore invalid names
+# pylint:disable=invalid-name
+
+import logging
+
+from keras import backend as K
+from keras.optimizers import Adam as KerasAdam
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+
+class Adam(KerasAdam):
+    """Adapted Keras Adam Optimizer to allow support of calculations
+       on CPU for Tensorflow.
+
+       Adapted from https://github.com/iperov/DeepFaceLab
+    """
+
+    def __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999,
+                 epsilon=None, decay=0., amsgrad=False, cpu_mode=0, **kwargs):
+        super().__init__(lr, beta_1, beta_2, epsilon, decay, **kwargs)
+        self.cpu_mode = self.set_cpu_mode(cpu_mode)
+
+    @staticmethod
+    def set_cpu_mode(cpu_mode):
+        """ Set the CPU mode to 0 if not using tensorflow, else passed in arg """
+        retval = False if K.backend() != "tensorflow" else cpu_mode
+        logger.debug("Optimizer CPU Mode set to %s", retval)
+        return retval
+
+    def get_updates(self, loss, params):
+        grads = self.get_gradients(loss, params)
+        self.updates = [K.update_add(self.iterations, 1)]
+
+        lr = self.lr
+        if self.initial_decay > 0:
+            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,
+                                                      K.dtype(self.decay))))
+
+        t = K.cast(self.iterations, K.floatx()) + 1
+        lr_t = lr * (K.sqrt(1. - K.pow(self.beta_2, t)) /
+                     (1. - K.pow(self.beta_1, t)))
+
+        # Pass off to CPU if requested
+        if self.cpu_mode:
+            with K.tf.device("/cpu:0"):
+                ms, vs, vhats = self.update_1(params)
+        else:
+            ms, vs, vhats = self.update_1(params)
+
+        self.weights = [self.iterations] + ms + vs + vhats
+
+        for p, g, m, v, vhat in zip(params, grads, ms, vs, vhats):
+            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g
+            v_t = (self.beta_2 * v) + (1. - self.beta_2) * K.square(g)
+            if self.amsgrad:
+                vhat_t = K.maximum(vhat, v_t)
+                p_t = p - lr_t * m_t / (K.sqrt(vhat_t) + self.epsilon)
+                self.updates.append(K.update(vhat, vhat_t))
+            else:
+                p_t = p - lr_t * m_t / (K.sqrt(v_t) + self.epsilon)
+
+            self.updates.append(K.update(m, m_t))
+            self.updates.append(K.update(v, v_t))
+            new_p = p_t
+
+            # Apply constraints.
+            if getattr(p, 'constraint', None) is not None:
+                new_p = p.constraint(new_p)
+
+            self.updates.append(K.update(p, new_p))
+        return self.updates
+
+    def update_1(self, params):
+        """ First update on CPU or GPU """
+        ms = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
+        vs = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
+        if self.amsgrad:
+            vhats = [K.zeros(K.int_shape(p), dtype=K.dtype(p)) for p in params]
+        else:
+            vhats = [K.zeros(1) for _ in params]
+        return ms, vs, vhats
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index f562c2a..038d341 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -15,13 +15,13 @@ import keras
 from keras import losses
 from keras import backend as K
 from keras.models import load_model, Model
-from keras.optimizers import Adam
 from keras.utils import get_custom_objects, multi_gpu_model
 
 from lib import Serializer
 from lib.model.backup_restore import Backup
 from lib.model.losses import DSSIMObjective, PenalizedLoss
 from lib.model.nn_blocks import NNBlocks
+from lib.model.optimizers import Adam
 from lib.multithreading import MultiThread
 from lib.utils import deprecation_warning, FaceswapError
 from plugins.train._config import Config
@@ -49,16 +49,18 @@ class ModelBase():
                  trainer="original",
                  pingpong=False,
                  memory_saving_gradients=False,
+                 optimizer_savings="none",
                  predict=False):
         logger.debug("Initializing ModelBase (%s): (model_dir: '%s', gpus: %s, configfile: %s, "
                      "snapshot_interval: %s, no_logs: %s, warp_to_landmarks: %s, augment_color: "
                      "%s, no_flip: %s, training_image_size, %s, alignments_paths: %s, "
                      "preview_scale: %s, input_shape: %s, encoder_dim: %s, trainer: %s, "
-                     "pingpong: %s, memory_saving_gradients: %s, predict: %s)",
+                     "pingpong: %s, memory_saving_gradients: %s, optimizer_savings: %s, "
+                     "predict: %s)",
                      self.__class__.__name__, model_dir, gpus, configfile, snapshot_interval,
                      no_logs, warp_to_landmarks, augment_color, no_flip, training_image_size,
                      alignments_paths, preview_scale, input_shape, encoder_dim, trainer, pingpong,
-                     memory_saving_gradients, predict)
+                     memory_saving_gradients, optimizer_savings, predict)
 
         self.predict = predict
         self.model_dir = model_dir
@@ -98,6 +100,7 @@ class ModelBase():
                               "pingpong": pingpong,
                               "snapshot_interval": snapshot_interval}
 
+        self.optimizer_savings = optimizer_savings
         self.set_gradient_type(memory_saving_gradients)
         if self.multiple_models_in_folder:
             deprecation_warning("Support for multiple model types within the same folder",
@@ -328,7 +331,7 @@ class ModelBase():
             # TODO: Remove this as soon it is fixed in PlaidML.
             opt_kwargs["clipnorm"] = 1.0
         logger.debug("Optimizer kwargs: %s", opt_kwargs)
-        return Adam(**opt_kwargs)
+        return Adam(**opt_kwargs, cpu_mode=self.optimizer_savings)
 
     def loss_function(self, mask, side, initialize):
         """ Set the loss function
@@ -601,6 +604,11 @@ class NNMeta():
         self.weights = network.get_weights()  # For pingpong restore
         logger.debug("Initialized %s", self.__class__.__name__)
 
+    @property
+    def output_shapes(self):
+        """ Return the output shapes from the stored network """
+        return [K.int_shape(output) for output in self.network.outputs]
+
     def set_name(self):
         """ Set the network name """
         name = self.type
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index edb45d9..27977af 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -284,7 +284,8 @@ class Batcher():
                    "particularly bad for this)."
                    "\n2) Lower the batchsize (the amount of images fed into the model each "
                    "iteration)."
-                   "\n3) Try Memory Saving Gradients and/or Ping Pong Training."
+                   "\n3) Try 'Memory Saving Gradients' and/or 'Optimizer Savings' and/or 'Ping "
+                   "Pong Training'."
                    "\n4) Use a more lightweight model, or select the model's 'LowMem' option "
                    "(in config) if it has one.")
             raise FaceswapError(msg) from err
diff --git a/scripts/train.py b/scripts/train.py
index 13ce433..fb95265 100644
--- a/scripts/train.py
+++ b/scripts/train.py
@@ -169,6 +169,7 @@ class Train():
             preview_scale=self.args.preview_scale,
             pingpong=self.args.pingpong,
             memory_saving_gradients=self.args.memory_saving_gradients,
+            optimizer_savings=self.args.optimizer_savings,
             predict=False)
         logger.debug("Loaded Model")
         return model
