commit 9696b5606fd0963814fc0c3644565aa60face69d
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Tue Jul 10 13:29:27 2018 +0200

    DLib scaling amends (#461)
    
    * Scaling area fix and rework of frames and bounding boxes
    
    * Update DLib scaling and initialization

diff --git a/lib/face_alignment/detectors.py b/lib/face_alignment/detectors.py
index 298080c..cbcbec0 100644
--- a/lib/face_alignment/detectors.py
+++ b/lib/face_alignment/detectors.py
@@ -58,7 +58,7 @@ class DLibDetector(Detector):
                             "the lib!".format(data_path))
         return data_path
 
-    def create_detector(self, verbose, detector):
+    def create_detector(self, verbose, detector, placeholder):
         """ Add the requested detectors """
         if self.initialized:
             return
@@ -76,16 +76,16 @@ class DLibDetector(Detector):
                 print("Adding DLib - HOG detector")
             self.detectors.append(dlib.get_frontal_face_detector())
 
+        for current_detector in self.detectors:
+            current_detector(placeholder, 0)
+
         self.initialized = True
 
-    def detect_faces(self, images):
-        """ Detect faces in images """
+    def detect_faces(self, image):
+        """ Detect faces in rgb image """
         self.detected_faces = None
-        for current_detector, current_image in(
-                (current_detector, current_image)
-                for current_detector in self.detectors
-                for current_image in images):
-            self.detected_faces = current_detector(current_image, 0)
+        for current_detector in self.detectors:
+            self.detected_faces = current_detector(image, 0)
 
             if self.detected_faces:
                 break
@@ -152,13 +152,10 @@ class MTCNNDetector(Detector):
         self.kwargs["onet"] = onet
         self.initialized = True
 
-    def detect_faces(self, images):
-        """ Detect faces in images """
+    def detect_faces(self, image):
+        """ Detect faces in rgb image """
         self.detected_faces = None
-        for current_image in images:
-            detected_faces = detect_face(current_image, **self.kwargs)
-            self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
-                                                  int(face[2]), int(face[3]))
-                                   for face in detected_faces]
-            if self.detected_faces:
-                break
+        detected_faces = detect_face(image, **self.kwargs)
+        self.detected_faces = [dlib.rectangle(int(face[0]), int(face[1]),
+                                              int(face[2]), int(face[3]))
+                               for face in detected_faces]
diff --git a/lib/face_alignment/extractor.py b/lib/face_alignment/extractor.py
index 229fc1e..e4ca335 100644
--- a/lib/face_alignment/extractor.py
+++ b/lib/face_alignment/extractor.py
@@ -25,61 +25,76 @@ class Frame(object):
         self.verbose = verbose
         self.height, self.width = input_image.shape[:2]
 
-        if not VRAM.scale_to and VRAM.device != -1:
-            VRAM.set_scale_to(detector)
+        self.input_scale = 1.0
 
-        if VRAM.device != -1:
-            self.scale_to = VRAM.scale_to
-        else:
-            self.scale_to = self.height * self.width
+        self.image_bgr = input_image
+        self.image_rgb = input_image[:, :, ::-1].copy()
+        self.image_detect = self.scale_image(input_is_predetected_face,
+                                             detector)
 
-        self.input_scale = 1.0
-        self.images = self.process_input(input_image,
-                                         input_is_predetected_face)
-
-    def process_input(self, input_image, input_is_predetected_face):
-        """ Process import image:
-            Size down if required
-            Duplicate into rgb colour space """
-        if not input_is_predetected_face:
-            input_image = self.scale_down(input_image)
-        return self.compile_color_space(input_image)
-
-    def scale_down(self, image):
+    def scale_image(self, input_is_predetected_face, detector):
         """ Scale down large images based on vram amount """
-        pixel_count = self.width * self.height
+        image = self.image_rgb
+        if input_is_predetected_face:
+            return image
 
-        if pixel_count > self.scale_to:
-            self.input_scale = self.scale_to / pixel_count
-            dimensions = (int(self.width * self.input_scale),
-                          int(self.height * self.input_scale))
-            if self.verbose:
-                print("Resizing image from {}x{} "
-                      "to {}.".format(str(self.width), str(self.height),
-                                      "x".join(str(i) for i in dimensions)))
-            image = cv2.resize(image, dimensions, interpolation=cv2.INTER_AREA)
+        if detector == "mtcnn":
+            self.scale_mtcnn()
+        else:
+            self.scale_dlib()
+
+        if self.input_scale == 1.0:
+            return image
+
+        if self.input_scale > 1.0:
+            interpolation = cv2.INTER_LINEAR
+        else:
+            interpolation = cv2.INTER_AREA
+
+        dimensions = (int(self.width * self.input_scale),
+                      int(self.height * self.input_scale))
+        if self.verbose and self.input_scale < 1.0:
+            print("Resizing image from {}x{} "
+                  "to {}.".format(str(self.width), str(self.height),
+                                  "x".join(str(i) for i in dimensions)))
+        image = cv2.resize(image,
+                           dimensions,
+                           interpolation=interpolation).copy()
 
         return image
 
-    @staticmethod
-    def compile_color_space(image_bgr):
-        """ cv2 and numpy inputs differs in rgb-bgr order
-        this affects chance of dlib face detection so
-        pass both versions """
-        image_rgb = image_bgr[:, :, ::-1].copy()
-        return (image_rgb, image_bgr)
+    def scale_mtcnn(self):
+        """ Set scaling for mtcnn """
+        pixel_count = self.width * self.height
+        if pixel_count > VRAM.scale_to:
+            self.input_scale = (VRAM.scale_to / pixel_count)**0.5
+
+    def scale_dlib(self):
+        """ Set scaling for dlib
+
+        DLIB is finickity, and pure pixel count won't help as when an
+        initial portrait image goes in, rotating it to landscape sucks
+        up VRAM for no discernible reason. This does not happen when the
+        initial image is a landscape image.
+        To mitigate this we need to make sure that all images fit within
+        a square based on the pixel count
+        There is also no way to set the acceptable size for a positive
+        match, so all images should be scaled to the maximum possible
+        to detect all available faces """
+
+        max_length_scale = int(VRAM.scale_to ** 0.5)
+        max_length_image = max(self.height, self.width)
+        self.input_scale = max_length_scale / max_length_image
 
 
 class Align(object):
     """ Perform transformation to align and get landmarks """
-    def __init__(self, frame, detected_faces, keras_model, verbose):
+    def __init__(self, image, detected_faces, keras_model, verbose):
         self.verbose = verbose
-        self.frame = frame.images[0]
-        self.input_scale = frame.input_scale
+        self.image = image
         self.detected_faces = detected_faces
         self.keras = keras_model
 
-        self.bounding_box = None
         self.landmarks = self.process_landmarks()
 
     @staticmethod
@@ -167,52 +182,43 @@ class Align(object):
                 print("Warning: No faces were detected.")
             return landmarks
 
-        for d_rect in self.detected_faces:
-            self.get_bounding_box(d_rect)
-            del d_rect
+        for detected_face in self.detected_faces:
 
-            center, scale = self.get_center_scale()
+            center, scale = self.get_center_scale(detected_face)
             image = self.align_image(center, scale)
 
             landmarks_xy = self.predict_landmarks(image, center, scale)
 
-            landmarks.append((
-                (int(self.bounding_box['left'] / self.input_scale),
-                 int(self.bounding_box['top'] / self.input_scale),
-                 int(self.bounding_box['right'] / self.input_scale),
-                 int(self.bounding_box['bottom'] / self.input_scale)),
-                landmarks_xy))
+            landmarks.append(((detected_face['left'],
+                               detected_face['top'],
+                               detected_face['right'],
+                               detected_face['bottom']),
+                              landmarks_xy))
 
         return landmarks
 
-    def get_bounding_box(self, d_rect):
-        """ Return the corner points of the bounding box """
-        self.bounding_box = {'left': d_rect.left(),
-                             'top': d_rect.top(),
-                             'right': d_rect.right(),
-                             'bottom': d_rect.bottom()}
-
-    def get_center_scale(self):
+    @staticmethod
+    def get_center_scale(detected_face):
         """ Get the center and set scale of bounding box """
-        center = np.array([(self.bounding_box['left']
-                            + self.bounding_box['right']) / 2.0,
-                           (self.bounding_box['top']
-                            + self.bounding_box['bottom']) / 2.0])
+        center = np.array([(detected_face['left']
+                            + detected_face['right']) / 2.0,
+                           (detected_face['top']
+                            + detected_face['bottom']) / 2.0])
 
-        center[1] -= (self.bounding_box['bottom']
-                      - self.bounding_box['top']) * 0.12
+        center[1] -= (detected_face['bottom']
+                      - detected_face['top']) * 0.12
 
-        scale = (self.bounding_box['right']
-                 - self.bounding_box['left']
-                 + self.bounding_box['bottom']
-                 - self.bounding_box['top']) / 195.0
+        scale = (detected_face['right']
+                 - detected_face['left']
+                 + detected_face['bottom']
+                 - detected_face['top']) / 195.0
 
         return center, scale
 
     def align_image(self, center, scale):
         """ Crop and align image around center """
         image = self.crop(
-            self.frame,
+            self.image,
             center,
             scale).transpose((2, 0, 1)).astype(np.float32) / 255.0
 
@@ -226,9 +232,7 @@ class Align(object):
                 center,
                 scale)
 
-        return [(int(pt[0] / self.input_scale),
-                 int(pt[1] / self.input_scale))
-                for pt in pts_img]
+        return [(int(pt[0]), int(pt[1])) for pt in pts_img]
 
 
 class Extract(object):
@@ -250,10 +254,10 @@ class Extract(object):
                            input_is_predetected_face=input_is_predetected_face)
 
         self.detect_faces(input_is_predetected_face)
-        self.convert_to_dlib_rectangle()
+        self.bounding_boxes = self.get_bounding_boxes()
 
-        self.landmarks = Align(frame=self.frame,
-                               detected_faces=self.detector.detected_faces,
+        self.landmarks = Align(image=self.frame.image_rgb,
+                               detected_faces=self.bounding_boxes,
                                keras_model=self.keras,
                                verbose=self.verbose).landmarks
 
@@ -262,8 +266,10 @@ class Extract(object):
         if self.initialized:
             return
         self.initialize_vram(detector)
-
         self.initialize_keras(detector)
+        # VRAM Scaling factor must be set AFTER Keras has loaded
+        VRAM.set_scale_to(detector)
+
         self.initialize_detector(detector, mtcnn_kwargs)
         self.initialized = True
 
@@ -295,6 +301,15 @@ class Extract(object):
             self.detector = DLIB_DETECTORS
             kwargs["detector"] = detector
 
+            scale_to = int(VRAM.scale_to ** 0.5)
+
+            if self.verbose:
+                print("Initializing DLib for frame size {}x{}".format(
+                    str(scale_to), str(scale_to)))
+
+            placeholder = np.zeros((scale_to, scale_to, 3), dtype=np.uint8)
+            kwargs["placeholder"] = placeholder
+
         self.detector.create_detector(**kwargs)
 
     def detect_faces(self, input_is_predetected_face):
@@ -307,13 +322,24 @@ class Extract(object):
         if input_is_predetected_face:
             self.detector.set_predetected(self.frame.width, self.frame.height)
         else:
-            self.detector.detect_faces(self.frame.images)
-
-    def convert_to_dlib_rectangle(self):
+            self.detector.detect_faces(self.frame.image_detect)
+
+    def get_bounding_boxes(self):
+        """ Return the corner points of the bounding box scaled
+            to original image """
+        bounding_boxes = list()
+        for d_rect in self.detector.detected_faces:
+            d_rect = self.convert_to_dlib_rectangle(d_rect)
+            bounding_box = {
+                'left': int(d_rect.left() / self.frame.input_scale),
+                'top': int(d_rect.top() / self.frame.input_scale),
+                'right': int(d_rect.right() / self.frame.input_scale),
+                'bottom': int(d_rect.bottom() / self.frame.input_scale)}
+            bounding_boxes.append(bounding_box)
+        return bounding_boxes
+
+    def convert_to_dlib_rectangle(self, d_rect):
         """ Convert detected faces to dlib_rectangle """
-        detected = [d_rect.rect
-                    if self.detector.is_mmod_rectangle(d_rect)
-                    else d_rect
-                    for d_rect in self.detector.detected_faces]
-
-        self.detector.detected_faces = detected
+        if self.detector.is_mmod_rectangle(d_rect):
+            return d_rect.rect
+        return d_rect
diff --git a/lib/face_alignment/vram_allocation.py b/lib/face_alignment/vram_allocation.py
index 57f2846..1aa1299 100644
--- a/lib/face_alignment/vram_allocation.py
+++ b/lib/face_alignment/vram_allocation.py
@@ -116,7 +116,12 @@ class GPUMem(object):
             gradient = 213 / 524288
             constant = 307
 
-        free_mem = self.vram_free - buffer
+        if self.device != -1:
+            free_mem = self.vram_free - buffer
+        else:
+            # Limit to 2GB if using CPU
+            free_mem = 2048
+
         self.scale_to = int((free_mem - constant) / gradient)
 
         if self.scale_to < 4097:
