commit b8ade19b05fb7755eed459fdcb715a13c4c18286
Author: Othniel Cundangan <othniel.cundangan@gmail.com>
Date:   Tue Mar 13 20:35:14 2018 -0400

    Align Eyes Horizontally After Umeyama + Blur Detection (#242)
    
    * Align eyes after umeyama
    
    * Remove comment
    
    * Add cli option
    
    * Update Extract_Crop.py
    
    * Fix convert
    
    * Add blur threshold
    
    * Use mask in blur detection
    
    * Improve blur detection
    
    * Fix indents
    
    * Update extract.py

diff --git a/lib/align_eyes.py b/lib/align_eyes.py
new file mode 100644
index 0000000..dc8a1ef
--- /dev/null
+++ b/lib/align_eyes.py
@@ -0,0 +1,71 @@
+# Code borrowed from https://github.com/jrosebr1/imutils/blob/d5cb29d02cf178c399210d5a139a821dfb0ae136/imutils/face_utils/helpers.py
+"""
+The MIT License (MIT)
+
+Copyright (c) 2015-2016 Adrian Rosebrock, http://www.pyimagesearch.com
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+"""
+
+from collections import OrderedDict
+import numpy as np
+import cv2
+
+# define a dictionary that maps the indexes of the facial
+# landmarks to specific face regions
+FACIAL_LANDMARKS_IDXS = OrderedDict([
+    ("mouth", (48, 68)),
+    ("right_eyebrow", (17, 22)),
+    ("left_eyebrow", (22, 27)),
+    ("right_eye", (36, 42)),
+    ("left_eye", (42, 48)),
+    ("nose", (27, 36)),
+    ("jaw", (0, 17)),
+    ("chin", (8, 11))
+])
+
+# Returns a rotation matrix that when applied to the 68 input facial landmarks
+# results in landmarks with eyes aligned horizontally
+def align_eyes(landmarks, size):
+    desiredLeftEye = (0.35, 0.35) # (y, x) value
+    desiredFaceWidth = desiredFaceHeight = size
+
+    # extract the left and right eye (x, y)-coordinates
+    (lStart, lEnd) = FACIAL_LANDMARKS_IDXS["left_eye"]
+    (rStart, rEnd) = FACIAL_LANDMARKS_IDXS["right_eye"]
+    leftEyePts = landmarks[lStart:lEnd]
+    rightEyePts = landmarks[rStart:rEnd]
+
+    # compute the center of mass for each eye
+    leftEyeCenter = leftEyePts.mean(axis=0).astype("int")
+    rightEyeCenter = rightEyePts.mean(axis=0).astype("int")
+
+    # compute the angle between the eye centroids
+    dY = rightEyeCenter[0,1] - leftEyeCenter[0,1]
+    dX = rightEyeCenter[0,0] - leftEyeCenter[0,0]
+    angle = np.degrees(np.arctan2(dY, dX)) - 180
+
+    # compute center (x, y)-coordinates (i.e., the median point)
+    # between the two eyes in the input image
+    eyesCenter = ((leftEyeCenter[0,0] + rightEyeCenter[0,0]) // 2, (leftEyeCenter[0,1] + rightEyeCenter[0,1]) // 2)
+
+    # grab the rotation matrix for rotating and scaling the face
+    M = cv2.getRotationMatrix2D(eyesCenter, angle, 1.0)
+
+    return M
diff --git a/lib/aligner.py b/lib/aligner.py
index 7b85f03..5017dc6 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -1,26 +1,62 @@
-import numpy
-
-from lib.umeyama import umeyama
-
-mean_face_x = numpy.array([
-0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,
-0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,
-0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,
-0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,
-0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,
-0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,
-0.553364, 0.490127, 0.42689 ])
-
-mean_face_y = numpy.array([
-0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,
-0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,
-0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,
-0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,
-0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,
-0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,
-0.784792, 0.824182, 0.831803, 0.824182 ])
-
-landmarks_2D = numpy.stack( [ mean_face_x, mean_face_y ], axis=1 )
-
-def get_align_mat(face):
-    return umeyama( numpy.array(face.landmarksAsXY()[17:]), landmarks_2D, True )[0:2]
+import numpy
+
+from lib.umeyama import umeyama
+from lib.align_eyes import align_eyes
+from numpy.linalg import inv
+import cv2
+
+mean_face_x = numpy.array([
+0.000213256, 0.0752622, 0.18113, 0.29077, 0.393397, 0.586856, 0.689483, 0.799124,
+0.904991, 0.98004, 0.490127, 0.490127, 0.490127, 0.490127, 0.36688, 0.426036,
+0.490127, 0.554217, 0.613373, 0.121737, 0.187122, 0.265825, 0.334606, 0.260918,
+0.182743, 0.645647, 0.714428, 0.793132, 0.858516, 0.79751, 0.719335, 0.254149,
+0.340985, 0.428858, 0.490127, 0.551395, 0.639268, 0.726104, 0.642159, 0.556721,
+0.490127, 0.423532, 0.338094, 0.290379, 0.428096, 0.490127, 0.552157, 0.689874,
+0.553364, 0.490127, 0.42689 ])
+
+mean_face_y = numpy.array([
+0.106454, 0.038915, 0.0187482, 0.0344891, 0.0773906, 0.0773906, 0.0344891,
+0.0187482, 0.038915, 0.106454, 0.203352, 0.307009, 0.409805, 0.515625, 0.587326,
+0.609345, 0.628106, 0.609345, 0.587326, 0.216423, 0.178758, 0.179852, 0.231733,
+0.245099, 0.244077, 0.231733, 0.179852, 0.178758, 0.216423, 0.244077, 0.245099,
+0.780233, 0.745405, 0.727388, 0.742578, 0.727388, 0.745405, 0.780233, 0.864805,
+0.902192, 0.909281, 0.902192, 0.864805, 0.784792, 0.778746, 0.785343, 0.778746,
+0.784792, 0.824182, 0.831803, 0.824182 ])
+
+landmarks_2D = numpy.stack( [ mean_face_x, mean_face_y ], axis=1 )
+
+def get_align_mat(face, size, should_align_eyes):
+    mat_umeyama = umeyama(numpy.array(face.landmarksAsXY()[17:]), landmarks_2D, True)[0:2]
+
+    if should_align_eyes is False:
+        return mat_umeyama
+
+    mat_umeyama = mat_umeyama * size
+
+    # Convert to matrix
+    landmarks = numpy.matrix(face.landmarksAsXY())
+
+    # cv2 expects points to be in the form np.array([ [[x1, y1]], [[x2, y2]], ... ]), we'll expand the dim
+    landmarks = numpy.expand_dims(landmarks, axis=1)
+
+    # Align the landmarks using umeyama
+    umeyama_landmarks = cv2.transform(landmarks, mat_umeyama, landmarks.shape)
+
+    # Determine a rotation matrix to align eyes horizontally
+    mat_align_eyes = align_eyes(umeyama_landmarks, size)
+
+    # Extend the 2x3 transform matrices to 3x3 so we can multiply them
+    # and combine them as one
+    mat_umeyama = numpy.matrix(mat_umeyama)
+    mat_umeyama.resize((3, 3))
+    mat_align_eyes = numpy.matrix(mat_align_eyes)
+    mat_align_eyes.resize((3, 3))
+    mat_umeyama[2] = mat_align_eyes[2] = [0, 0, 1]
+
+    # Combine the umeyama transform with the extra rotation matrix
+    transform_mat = mat_align_eyes * mat_umeyama
+
+    # Remove the extra row added, shape needs to be 2x3
+    transform_mat = numpy.delete(transform_mat, 2, 0)
+    transform_mat = transform_mat / size
+    return transform_mat
diff --git a/lib/detect_blur.py b/lib/detect_blur.py
new file mode 100644
index 0000000..15928e9
--- /dev/null
+++ b/lib/detect_blur.py
@@ -0,0 +1,17 @@
+import cv2
+
+def variance_of_laplacian(image):
+    # compute the Laplacian of the image and then return the focus
+    # measure, which is simply the variance of the Laplacian
+    return cv2.Laplacian(image, cv2.CV_64F).var()
+
+def is_blurry(image, threshold):
+    # Convert to grayscale, and compute the
+    # focus measure of the image using the
+    # Variance of Laplacian method
+    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
+    fm = variance_of_laplacian(gray)
+
+    # if the focus measure is less than the supplied threshold,
+    # then the image should be considered "blurry"
+    return (fm < threshold, fm)
diff --git a/plugins/Convert_Masked.py b/plugins/Convert_Masked.py
index 77058ad..b962d80 100644
--- a/plugins/Convert_Masked.py
+++ b/plugins/Convert_Masked.py
@@ -26,7 +26,7 @@ class Convert():
 
         image_size = image.shape[1], image.shape[0]
 
-        mat = numpy.array(get_align_mat(face_detected)).reshape(2,3)
+        mat = numpy.array(get_align_mat(face_detected, size, should_align_eyes=False)).reshape(2,3)
 
         if "GAN" not in self.trainer:
             mat = mat * size
@@ -37,7 +37,7 @@ class Convert():
 
         new_face = self.get_new_face(image,mat,size)
 
-        image_mask = self.get_image_mask( image, new_face, face_detected, mat, image_size )
+        image_mask = self.get_image_mask( image, new_face, face_detected.landmarksAsXY(), mat, image_size )
 
         return self.apply_new_face(image, new_face, image_mask, mat, image_size, size)
 
@@ -146,7 +146,7 @@ class Convert():
 
         return new_face
 
-    def get_image_mask(self, image, new_face, face_detected, mat, image_size):
+    def get_image_mask(self, image, new_face, landmarks, mat, image_size):
 
         face_mask = numpy.zeros(image.shape,dtype=float)
         if 'rect' in self.mask_type:
@@ -155,7 +155,7 @@ class Convert():
 
         hull_mask = numpy.zeros(image.shape,dtype=float)
         if 'hull' in self.mask_type:
-            hull = cv2.convexHull( numpy.array( face_detected.landmarksAsXY() ).reshape((-1,2)).astype(int) ).flatten().reshape( (-1,2) )
+            hull = cv2.convexHull( numpy.array( landmarks ).reshape((-1,2)).astype(int) ).flatten().reshape( (-1,2) )
             cv2.fillConvexPoly( hull_mask,hull,(1,1,1) )
 
         if self.mask_type == 'rect':
diff --git a/plugins/Extract_Align.py b/plugins/Extract_Align.py
index f14cca2..f009ade 100644
--- a/plugins/Extract_Align.py
+++ b/plugins/Extract_Align.py
@@ -1,15 +1,68 @@
 # Based on the original https://www.reddit.com/r/deepfakes/ code sample + contribs
 
 import cv2
+import numpy as np
 
 from lib.aligner import get_align_mat
+from lib.align_eyes import FACIAL_LANDMARKS_IDXS
 
-class Extract:
-    def extract(self, image, face, size):
-        alignment = get_align_mat( face )
-        return self.transform( image, alignment, size, 48 )
+class Extract(object):
+    def extract(self, image, face, size, align_eyes):
+        alignment = get_align_mat(face, size, align_eyes)
+        extracted = self.transform(image, alignment, size, 48)
+        return extracted, alignment
 
-    def transform( self, image, mat, size, padding=0 ):
-        mat = mat * (size - 2 * padding)
-        mat[:,2] += padding
-        return cv2.warpAffine( image, mat, ( size, size ) )
+    def transform(self, image, mat, size, padding=0):
+        matrix = mat * (size - 2 * padding)
+        matrix[:,2] += padding
+        return cv2.warpAffine(image, matrix, (size, size))
+
+    def transform_points(self, points, mat, size, padding=0):
+        matrix = mat * (size - 2 * padding)
+        matrix[:,2] += padding
+        points = np.expand_dims(points, axis=1)
+        points = cv2.transform(points, matrix, points.shape)
+        points = np.squeeze(points)
+        return points
+
+    def get_feature_mask(self, aligned_landmarks_68, size, padding=0, dilation=30):
+        scale = size - 2*padding
+        translation = padding
+        pad_mat = np.matrix([[scale, 0.0, translation], [0.0, scale, translation]])
+        aligned_landmarks_68 = np.expand_dims(aligned_landmarks_68, axis=1)
+        aligned_landmarks_68 = cv2.transform(aligned_landmarks_68, pad_mat, aligned_landmarks_68.shape)
+        aligned_landmarks_68 = np.squeeze(aligned_landmarks_68)
+
+        (lStart, lEnd) = FACIAL_LANDMARKS_IDXS["left_eye"]
+        (rStart, rEnd) = FACIAL_LANDMARKS_IDXS["right_eye"]
+        (mStart, mEnd) = FACIAL_LANDMARKS_IDXS["mouth"]
+        (nStart, nEnd) = FACIAL_LANDMARKS_IDXS["nose"]
+        (lbStart, lbEnd) = FACIAL_LANDMARKS_IDXS["left_eyebrow"]
+        (rbStart, rbEnd) = FACIAL_LANDMARKS_IDXS["right_eyebrow"]
+        (cStart, cEnd) = FACIAL_LANDMARKS_IDXS["chin"]
+
+        l_eye_points = aligned_landmarks_68[lStart:lEnd].tolist()
+        l_brow_points = aligned_landmarks_68[lbStart:lbEnd].tolist()
+        r_eye_points = aligned_landmarks_68[rStart:rEnd].tolist()
+        r_brow_points = aligned_landmarks_68[rbStart:rbEnd].tolist()
+        nose_points = aligned_landmarks_68[nStart:nEnd].tolist()
+        chin_points = aligned_landmarks_68[cStart:cEnd].tolist()
+        mouth_points = aligned_landmarks_68[mStart:mEnd].tolist()
+        l_eye_points = l_eye_points + l_brow_points
+        r_eye_points = r_eye_points + r_brow_points
+        mouth_points = mouth_points + nose_points + chin_points
+
+        l_eye_hull = cv2.convexHull(np.array(l_eye_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
+        r_eye_hull = cv2.convexHull(np.array(r_eye_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
+        mouth_hull = cv2.convexHull(np.array(mouth_points).reshape((-1, 2)).astype(int)).flatten().reshape((-1, 2))
+
+        mask = np.zeros((size, size, 3), dtype=float)
+        cv2.fillConvexPoly(mask, l_eye_hull, (1,1,1))
+        cv2.fillConvexPoly(mask, r_eye_hull, (1,1,1))
+        cv2.fillConvexPoly(mask, mouth_hull, (1,1,1))
+
+        if dilation > 0:
+            kernel = np.ones((dilation, dilation), np.uint8)
+            mask = cv2.dilate(mask, kernel, iterations=1)
+
+        return mask
diff --git a/plugins/Extract_Crop.py b/plugins/Extract_Crop.py
deleted file mode 100644
index aa1bcf7..0000000
--- a/plugins/Extract_Crop.py
+++ /dev/null
@@ -1,7 +0,0 @@
-# Based on the original https://www.reddit.com/r/deepfakes/ code sample
-
-import cv2
-
-class Extract(object):
-    def extract(self, image, face, size):
-        return cv2.resize(face.image, (size, size))
\ No newline at end of file
diff --git a/scripts/extract.py b/scripts/extract.py
index 09a76b6..e2f8860 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -3,10 +3,12 @@ import cv2
 from pathlib import Path
 from tqdm import tqdm
 import os
+import numpy as np
 
 from lib.cli import DirectoryProcessor, rotate_image
 from lib.utils import get_folder
 from lib.multithreading import pool_process
+from lib.detect_blur import is_blurry
 from plugins.PluginLoader import PluginLoader
 
 class ExtractTrainingData(DirectoryProcessor):
@@ -53,13 +55,13 @@ class ExtractTrainingData(DirectoryProcessor):
                             type=int,
                             default=1,
                             help="Number of processes to use.")
-        
+
         parser.add_argument('-s', '--skip-existing',
                             action='store_true',
                             dest='skip_existing',
                             default=False,
                             help="Skips frames already extracted.")
-        
+
         parser.add_argument('-dl', '--debug-landmarks',
                             action="store_true",
                             dest="debug_landmarks",
@@ -74,6 +76,19 @@ class ExtractTrainingData(DirectoryProcessor):
                             help="If a face isn't found, rotate the images through 90 degree "
                                  "iterations to try to find a face. Can find more faces at the "
                                  "cost of extraction speed.")
+
+        parser.add_argument('-ae', '--align-eyes',
+                            action="store_true",
+                            dest="align_eyes",
+                            default=False,
+                            help="Perform extra alignment to ensure left/right eyes lie at the same height")
+
+        parser.add_argument('-bt', '--blur-threshold',
+                            type=int,
+                            dest="blur_thresh",
+                            default=None,
+                            help="Automatically discard images blurrier than the specified threshold. Discarded images are moved into a \"blurry\" sub-folder. Lower values allow more blur")
+
         return parser
 
     def process(self):
@@ -121,12 +136,12 @@ class ExtractTrainingData(DirectoryProcessor):
                 break
             angle += 90
         return rotated_faces, rotated_image
-        
+
     def handleImage(self, image, filename):
         faces = self.get_faces(image)
         process_faces = [(idx, face) for idx, face in faces]
 
-        # Run image rotator if requested and no faces found        
+        # Run image rotator if requested and no faces found
         if self.arguments.rotate_images.lower() == 'on' and len(process_faces) == 0:
             process_faces, image = self.imageRotator(image)
 
@@ -136,9 +151,25 @@ class ExtractTrainingData(DirectoryProcessor):
             if self.arguments.debug_landmarks:
                 for (x, y) in face.landmarksAsXY():
                     cv2.circle(image, (x, y), 2, (0, 0, 255), -1)
-            
-            resized_image = self.extractor.extract(image, face, 256)
+
+            resized_image, t_mat = self.extractor.extract(image, face, 256, self.arguments.align_eyes)
             output_file = get_folder(self.output_dir) / Path(filename).stem
+
+            # Detect blurry images
+            if self.arguments.blur_thresh is not None:
+                aligned_landmarks = self.extractor.transform_points(face.landmarksAsXY(), t_mat, 256, 48)
+                feature_mask = self.extractor.get_feature_mask(aligned_landmarks / 256, 256, 48)
+                feature_mask = cv2.blur(feature_mask, (10, 10))
+                isolated_face = cv2.multiply(feature_mask, resized_image.astype(float)).astype(np.uint8)
+                blurry, focus_measure = is_blurry(isolated_face, self.arguments.blur_thresh)
+                # print("{} focus measure: {}".format(Path(filename).stem, focus_measure))
+                # cv2.imshow("Isolated Face", isolated_face)
+                # cv2.waitKey(0)
+                # cv2.destroyAllWindows()
+                if blurry:
+                    print("{}'s focus measure of {} was below the blur threshold, moving to \"blurry\"".format(Path(filename).stem, focus_measure))
+                    output_file = get_folder(Path(self.output_dir) / Path("blurry")) / Path(filename).stem
+
             cv2.imwrite('{}_{}{}'.format(str(output_file), str(idx), Path(filename).suffix), resized_image)
             f = {
                 "r": face.r,
