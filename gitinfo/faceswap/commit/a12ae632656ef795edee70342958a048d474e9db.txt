commit a12ae632656ef795edee70342958a048d474e9db
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sat Mar 9 12:43:40 2019 +0000

    Better extractor error handling on initialization

diff --git a/lib/multithreading.py b/lib/multithreading.py
index 0ba29ca..4851f6d 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -326,6 +326,7 @@ class SpawnProcess(mp.context.SpawnProcess):
                      self.__class__.__name__, name, args, kwargs)
         ctx = mp.get_context("spawn")
         self.event = ctx.Event()
+        self.error = ctx.Event()
         kwargs = self.build_target_kwargs(in_queue, out_queue, kwargs)
         super().__init__(target=target, name=name, args=args, kwargs=kwargs)
         self.daemon = True
@@ -334,6 +335,7 @@ class SpawnProcess(mp.context.SpawnProcess):
     def build_target_kwargs(self, in_queue, out_queue, kwargs):
         """ Add standard kwargs to passed in kwargs list """
         kwargs["event"] = self.event
+        kwargs["error"] = self.error
         kwargs["log_init"] = set_root_logger
         kwargs["log_queue"] = LOG_QUEUE
         kwargs["log_level"] = logger.getEffectiveLevel()
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index 321a4ce..5f72fcf 100755
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -43,6 +43,7 @@ class Detector():
         self.min_size = min_size
         self.parent_is_pool = False
         self.init = None
+        self.error = None
 
         # The input and output queues for the plugin.
         # See lib.queue_manager.QueueManager for getting queues
@@ -82,6 +83,7 @@ class Detector():
         logger.debug("initialize %s (PID: %s, args: %s, kwargs: %s)",
                      self.__class__.__name__, os.getpid(), args, kwargs)
         self.init = kwargs.get("event", False)
+        self.error = kwargs.get("error", False)
         self.queues["in"] = kwargs["in_queue"]
         self.queues["out"] = kwargs["out_queue"]
 
diff --git a/plugins/extract/detect/dlib_cnn.py b/plugins/extract/detect/dlib_cnn.py
index 2a60a4e..d5b1f8a 100755
--- a/plugins/extract/detect/dlib_cnn.py
+++ b/plugins/extract/detect/dlib_cnn.py
@@ -34,31 +34,35 @@ class Detect(Detector):
 
     def initialize(self, *args, **kwargs):
         """ Calculate batch size """
-        super().initialize(*args, **kwargs)
-        logger.verbose("Initializing Dlib-CNN Detector...")
-        self.detector = dlib.cnn_face_detection_model_v1(  # pylint: disable=c-extension-no-member
-            self.model_path)
-        is_cuda = self.compiled_for_cuda()
-        if is_cuda:
-            logger.debug("Using GPU")
-            _, vram_free, _ = self.get_vram_free()
-        else:
-            logger.verbose("Using CPU")
-            vram_free = 2048
+        try:
+            super().initialize(*args, **kwargs)
+            logger.verbose("Initializing Dlib-CNN Detector...")
+            self.detector = dlib.cnn_face_detection_model_v1(  # pylint: disable=c-extension-no-member
+                self.model_path)
+            is_cuda = self.compiled_for_cuda()
+            if is_cuda:
+                logger.debug("Using GPU")
+                _, vram_free, _ = self.get_vram_free()
+            else:
+                logger.verbose("Using CPU")
+                vram_free = 2048
 
-        # Batch size of 2 actually uses about 338MB less than a single image??
-        # From there batches increase at ~680MB per item in the batch
+            # Batch size of 2 actually uses about 338MB less than a single image??
+            # From there batches increase at ~680MB per item in the batch
 
-        self.batch_size = int(((vram_free - self.vram) / 680) + 2)
+            self.batch_size = int(((vram_free - self.vram) / 680) + 2)
 
-        if self.batch_size < 1:
-            raise ValueError("Insufficient VRAM available to continue "
-                             "({}MB)".format(int(vram_free)))
+            if self.batch_size < 1:
+                raise ValueError("Insufficient VRAM available to continue "
+                                 "({}MB)".format(int(vram_free)))
 
-        logger.verbose("Processing in batches of %s", self.batch_size)
+            logger.verbose("Processing in batches of %s", self.batch_size)
 
-        self.init.set()
-        logger.info("Initialized Dlib-CNN Detector...")
+            self.init.set()
+            logger.info("Initialized Dlib-CNN Detector...")
+        except Exception as err:
+            self.error.set()
+            raise err
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in rgb image """
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
index 0d29126..09b3bfb 100755
--- a/plugins/extract/detect/mtcnn.py
+++ b/plugins/extract/detect/mtcnn.py
@@ -72,53 +72,58 @@ class Detect(Detector):
 
     def initialize(self, *args, **kwargs):
         """ Create the mtcnn detector """
-        super().initialize(*args, **kwargs)
-        logger.info("Initializing MTCNN Detector...")
-        is_gpu = False
-
-        # Must import tensorflow inside the spawned process
-        # for Windows machines
-        import_tensorflow()
-        _, vram_free, _ = self.get_vram_free()
-        mtcnn_graph = tf.Graph()
-
-        # Windows machines sometimes misreport available vram, and overuse
-        # causing OOM. Allow growth fixes that
-        config = tf.ConfigProto()
-        config.gpu_options.allow_growth = True  # pylint: disable=no-member
-
-        with mtcnn_graph.as_default():  # pylint: disable=not-context-manager
-            sess = tf.Session(config=config)
-            with sess.as_default():  # pylint: disable=not-context-manager
-                pnet, rnet, onet = create_mtcnn(sess, self.model_path)
-
-            if any("gpu" in str(device).lower()
-                   for device in sess.list_devices()):
-                logger.debug("Using GPU")
-                is_gpu = True
-        mtcnn_graph.finalize()
-
-        if not is_gpu:
-            alloc = 2048
-            logger.warning("Using CPU")
-        else:
-            alloc = vram_free
-        logger.debug("Allocated for Tensorflow: %sMB", alloc)
+        try:
+            super().initialize(*args, **kwargs)
+            logger.info("Initializing MTCNN Detector...")
+            is_gpu = False
+
+            # Must import tensorflow inside the spawned process
+            # for Windows machines
+            import_tensorflow()
+            _, vram_free, _ = self.get_vram_free()
+            mtcnn_graph = tf.Graph()
+
+            # Windows machines sometimes misreport available vram, and overuse
+            # causing OOM. Allow growth fixes that
+            config = tf.ConfigProto()
+            config.gpu_options.allow_growth = True  # pylint: disable=no-member
+
+            with mtcnn_graph.as_default():  # pylint: disable=not-context-manager
+                sess = tf.Session(config=config)
+                with sess.as_default():  # pylint: disable=not-context-manager
+                    pnet, rnet, onet = create_mtcnn(sess, self.model_path)
+
+                if any("gpu" in str(device).lower()
+                       for device in sess.list_devices()):
+                    logger.debug("Using GPU")
+                    is_gpu = True
+            mtcnn_graph.finalize()
+
+            if not is_gpu:
+                alloc = 2048
+                logger.warning("Using CPU")
+            else:
+                alloc = vram_free
+            logger.debug("Allocated for Tensorflow: %sMB", alloc)
 
-        self.batch_size = int(alloc / self.vram)
+            self.batch_size = int(alloc / self.vram)
 
-        if self.batch_size < 1:
-            raise ValueError("Insufficient VRAM available to continue "
-                             "({}MB)".format(int(alloc)))
+            if self.batch_size < 1:
+                self.error.set()
+                raise ValueError("Insufficient VRAM available to continue "
+                                 "({}MB)".format(int(alloc)))
 
-        logger.verbose("Processing in %s threads", self.batch_size)
+            logger.verbose("Processing in %s threads", self.batch_size)
 
-        self.kwargs["pnet"] = pnet
-        self.kwargs["rnet"] = rnet
-        self.kwargs["onet"] = onet
+            self.kwargs["pnet"] = pnet
+            self.kwargs["rnet"] = rnet
+            self.kwargs["onet"] = onet
 
-        self.init.set()
-        logger.info("Initialized MTCNN Detector.")
+            self.init.set()
+            logger.info("Initialized MTCNN Detector.")
+        except Exception as err:
+            self.error.set()
+            raise err
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in Multiple Threads """
diff --git a/plugins/extract/detect/s3fd.py b/plugins/extract/detect/s3fd.py
index 7b9f71e..154df4e 100644
--- a/plugins/extract/detect/s3fd.py
+++ b/plugins/extract/detect/s3fd.py
@@ -35,36 +35,41 @@ class Detect(Detector):
 
     def initialize(self, *args, **kwargs):
         """ Create the s3fd detector """
-        super().initialize(*args, **kwargs)
-        logger.info("Initializing S3FD Detector...")
-        card_id, vram_free, vram_total = self.get_vram_free()
-        if vram_free <= self.vram:
-            tf_ratio = 1.0
-        else:
-            tf_ratio = self.vram / vram_total
-        logger.verbose("Reserving %s%% of total VRAM per s3fd thread", round(tf_ratio, 2))
-
-        confidence = self.config["confidence"] / 100
-        self.model = S3fd(self.model_path, self.target, tf_ratio, card_id, confidence)
-
-        if not self.model.is_gpu:
-            alloc = 2048
-            logger.warning("Using CPU")
-        else:
-            logger.debug("Using GPU")
-            alloc = vram_free
-        logger.debug("Allocated for Tensorflow: %sMB", alloc)
-
-        self.batch_size = int(alloc / self.vram)
-
-        if self.batch_size < 1:
-            raise ValueError("Insufficient VRAM available to continue "
-                             "({}MB)".format(int(alloc)))
-
-        logger.verbose("Processing in %s threads", self.batch_size)
-
-        self.init.set()
-        logger.info("Initialized S3FD Detector.")
+        try:
+            super().initialize(*args, **kwargs)
+            logger.info("Initializing S3FD Detector...")
+            card_id, vram_free, vram_total = self.get_vram_free()
+            if vram_free <= self.vram:
+                tf_ratio = 1.0
+            else:
+                tf_ratio = self.vram / vram_total
+            logger.verbose("Reserving %s%% of total VRAM per s3fd thread",
+                           round(tf_ratio * 100, 2))
+
+            confidence = self.config["confidence"] / 100
+            self.model = S3fd(self.model_path, self.target, tf_ratio, card_id, confidence)
+
+            if not self.model.is_gpu:
+                alloc = 2048
+                logger.warning("Using CPU")
+            else:
+                logger.debug("Using GPU")
+                alloc = vram_free
+            logger.debug("Allocated for Tensorflow: %sMB", alloc)
+
+            self.batch_size = int(alloc / self.vram)
+
+            if self.batch_size < 1:
+                raise ValueError("Insufficient VRAM available to continue "
+                                 "({}MB)".format(int(alloc)))
+
+            logger.verbose("Processing in %s threads", self.batch_size)
+
+            self.init.set()
+            logger.info("Initialized S3FD Detector.")
+        except Exception as err:
+            self.error.set()
+            raise err
 
     def detect_faces(self, *args, **kwargs):
         """ Detect faces in Multiple Threads """
diff --git a/scripts/extract.py b/scripts/extract.py
index b88a184..40f5442 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -381,10 +381,8 @@ class Plugins():
         mp_func = PoolProcess if self.detector.parent_is_pool else SpawnProcess
         self.process_detect = mp_func(self.detector.run, **kwargs)
 
-        event = None
-        if hasattr(self.process_detect, "event"):
-            event = self.process_detect.event
-
+        event = self.process_detect.event if hasattr(self.process_detect, "event") else None
+        error = self.process_detect.error if hasattr(self.process_detect, "error") else None
         self.process_detect.start()
 
         if event is None:
@@ -392,10 +390,15 @@ class Plugins():
             return
 
         for mins in reversed(range(5)):
-            event.wait(60)
+            for seconds in range(60):
+                event.wait(seconds)
+                if event.is_set():
+                    break
+                if error and error.is_set():
+                    break
             if event.is_set():
                 break
-            if mins == 0:
+            if mins == 0 or (error and error.is_set()):
                 raise ValueError("Error initializing Detector")
             logger.info("Waiting for Detector... Time out in %s minutes", mins)
 
