commit 232d9313afabc075e28c1a42047b22e0568200e6
Author: iperov <lepersorium@gmail.com>
Date:   Thu Mar 1 14:32:11 2018 +0400

    port 'face_alignment' from PyTorch to Keras. (#228)
    
    * port 'face_alignment' from PyTorch to Keras. It works x2 faster, but initialization takes 20secs.
    
    2DFAN-4.h5 and mmod_human_face_detector.dat included in lib\FaceLandmarksExtractor
    
    fixed dlib vs tensorflow conflict: dlib must do op first, then load keras model, otherwise CUDA OOM error
    
    if face location not found by CNN, its try to find by HOG.
    
    removed this:
    -        if face.landmarks == None:
    -            print("Warning! landmarks not found. Switching to crop!")
    -            return cv2.resize(face.image, (size, size))
    because DetectedFace always has landmarks
    
    * removing DetectedFace.landmarks

diff --git a/lib/FaceLandmarksExtractor/2DFAN-4.h5 b/lib/FaceLandmarksExtractor/2DFAN-4.h5
new file mode 100644
index 0000000..8c5079a
Binary files /dev/null and b/lib/FaceLandmarksExtractor/2DFAN-4.h5 differ
diff --git a/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py b/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py
new file mode 100644
index 0000000..667da06
--- /dev/null
+++ b/lib/FaceLandmarksExtractor/FaceLandmarksExtractor.py
@@ -0,0 +1,171 @@
+import atexit
+import numpy as np
+import os
+import cv2
+import dlib
+import keras
+from keras import backend as K
+
+dlib_cnn_face_detector = None
+dlib_face_detector = None
+keras_model = None
+
+@atexit.register
+def onExit():
+    global dlib_cnn_face_detector
+    global dlib_face_detector
+    global keras_model
+    
+    if keras_model is not None:
+        del keras_model
+        K.clear_session()
+        
+    if dlib_cnn_face_detector is not None:
+        del dlib_cnn_face_detector
+    
+    if dlib_face_detector is not None:
+        del dlib_face_detector
+        
+class TorchBatchNorm2D(keras.engine.topology.Layer):
+    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
+        super(TorchBatchNorm2D, self).__init__(**kwargs)
+        self.supports_masking = True
+        self.axis = axis
+        self.momentum = momentum
+        self.epsilon = epsilon
+
+    def build(self, input_shape):
+        dim = input_shape[self.axis]
+        if dim is None:
+            raise ValueError('Axis ' + str(self.axis) + ' of ' 'input tensor should have a defined dimension ' 'but the layer received an input with shape ' + str(input_shape) + '.')
+        shape = (dim,)
+        self.gamma = self.add_weight(shape=shape, name='gamma', initializer='ones', regularizer=None, constraint=None)
+        self.beta = self.add_weight(shape=shape, name='beta', initializer='zeros', regularizer=None, constraint=None)
+        self.moving_mean = self.add_weight(shape=shape, name='moving_mean', initializer='zeros', trainable=False)            
+        self.moving_variance = self.add_weight(shape=shape, name='moving_variance', initializer='ones', trainable=False)            
+        self.built = True
+
+    def call(self, inputs, training=None):
+        input_shape = K.int_shape(inputs)
+
+        broadcast_shape = [1] * len(input_shape)
+        broadcast_shape[self.axis] = input_shape[self.axis]
+        
+        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
+        broadcast_moving_variance = K.reshape(self.moving_variance, broadcast_shape)
+        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
+        broadcast_beta = K.reshape(self.beta, broadcast_shape)        
+        invstd = K.ones (shape=broadcast_shape, dtype='float32') / K.sqrt(broadcast_moving_variance + K.constant(self.epsilon, dtype='float32'))
+        
+        return (inputs - broadcast_moving_mean) * invstd * broadcast_gamma + broadcast_beta
+       
+    def get_config(self):
+        config = { 'axis': self.axis, 'momentum': self.momentum, 'epsilon': self.epsilon }
+        base_config = super(TorchBatchNorm2D, self).get_config()
+        return dict(list(base_config.items()) + list(config.items()))
+
+def transform(point, center, scale, resolution):
+    pt = np.array ( [point[0], point[1], 1.0] )            
+    h = 200.0 * scale
+    m = np.eye(3)
+    m[0,0] = resolution / h
+    m[1,1] = resolution / h
+    m[0,2] = resolution * ( -center[0] / h + 0.5 )
+    m[1,2] = resolution * ( -center[1] / h + 0.5 )
+    m = np.linalg.inv(m)
+    return np.matmul (m, pt)[0:2]
+    
+def crop(image, center, scale, resolution=256.0):
+    ul = transform([1, 1], center, scale, resolution).astype( np.int )
+    br = transform([resolution, resolution], center, scale, resolution).astype( np.int )
+    if image.ndim > 2:
+        newDim = np.array([br[1] - ul[1], br[0] - ul[0], image.shape[2]], dtype=np.int32)
+        newImg = np.zeros(newDim, dtype=np.uint8)
+    else:
+        newDim = np.array([br[1] - ul[1], br[0] - ul[0]], dtype=np.int)
+        newImg = np.zeros(newDim, dtype=np.uint8)
+    ht = image.shape[0]
+    wd = image.shape[1]
+    newX = np.array([max(1, -ul[0] + 1), min(br[0], wd) - ul[0]], dtype=np.int32)
+    newY = np.array([max(1, -ul[1] + 1), min(br[1], ht) - ul[1]], dtype=np.int32)
+    oldX = np.array([max(1, ul[0] + 1), min(br[0], wd)], dtype=np.int32)
+    oldY = np.array([max(1, ul[1] + 1), min(br[1], ht)], dtype=np.int32)
+    newImg[newY[0] - 1:newY[1], newX[0] - 1:newX[1] ] = image[oldY[0] - 1:oldY[1], oldX[0] - 1:oldX[1], :]
+    newImg = cv2.resize(newImg, dsize=(int(resolution), int(resolution)), interpolation=cv2.INTER_LINEAR)
+    return newImg
+           
+def get_pts_from_predict(a, center, scale):
+    b = a.reshape ( (a.shape[0], a.shape[1]*a.shape[2]) )    
+    c = b.argmax(1).reshape ( (a.shape[0], 1) ).repeat(2, axis=1).astype(np.float)
+    c[:,0] %= a.shape[2]    
+    c[:,1] = np.apply_along_axis ( lambda x: np.floor(x / a.shape[2]), 0, c[:,1] )
+
+    for i in range(a.shape[0]):
+        pX, pY = int(c[i,0]), int(c[i,1])
+        if pX > 0 and pX < 63 and pY > 0 and pY < 63:
+            diff = np.array ( [a[i,pY,pX+1]-a[i,pY,pX-1], a[i,pY+1,pX]-a[i,pY-1,pX]] )
+            c[i] += np.sign(diff)*0.25
+   
+    c += 0.5
+    return [ transform (c[i], center, scale, a.shape[2]) for i in range(a.shape[0]) ]
+        
+def extract(input_image, use_cnn_face_detector=True, all_faces=True):
+    global dlib_cnn_face_detector
+    global dlib_face_detector
+    global keras_model
+    
+    #this execution order (dlib -> keras model) prevents dlib cnn cuda OOM error on Windows
+    if dlib_cnn_face_detector is None:            
+        dlib_cnn_face_detector_path = os.path.join(os.path.dirname(__file__), "mmod_human_face_detector.dat")
+        if not os.path.exists(dlib_cnn_face_detector_path):
+            print ("Error: Unable to find %s, reinstall the lib !" % (dlib_cnn_face_detector_path) )
+        else:
+            dlib_cnn_face_detector = dlib.cnn_face_detection_model_v1(dlib_cnn_face_detector_path)
+    
+    if dlib_face_detector is None:
+        dlib_face_detector = dlib.get_frontal_face_detector()     
+                
+    if use_cnn_face_detector:
+        detected_faces = dlib_cnn_face_detector(input_image, 1)
+        if len(detected_faces) == 0:
+            use_cnn_face_detector = False
+            detected_faces = dlib_face_detector(input_image, 1)
+            if len(detected_faces) != 0:
+                print ('Info: CNN found no faces, but HOG found !')
+    else:        
+        detected_faces = dlib_face_detector(input_image, 1)
+
+    
+    if keras_model is None:        
+        model_path = os.path.join( os.path.dirname(__file__) , "2DFAN-4.h5" )
+        if not os.path.exists(model_path):
+            print ("Error: Unable to find %s, reinstall the lib !" % (model_path) )
+        else:
+            print ("Info: initializing keras model...")
+            keras_model = keras.models.load_model (model_path, custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D} ) 
+    
+    landmarks = []
+    if len(detected_faces) > 0:        
+        for i, d in enumerate(detected_faces):
+            if i > 0 and not all_faces:
+                break
+            
+            d_rect = d.rect if use_cnn_face_detector else d
+        
+            left, top, right, bottom = d_rect.left(), d_rect.top(), d_rect.right(), d_rect.bottom()
+            del d
+    
+            center = np.array( [ (left + right) / 2.0, (top + bottom) / 2.0] )
+            center[1] -= (bottom - top) * 0.12
+            scale = (right - left + bottom - top) / 195.0
+        
+            image = crop(input_image, center, scale).transpose ( (2,0,1) ).astype(np.float32) / 255.0
+            image = np.expand_dims(image, 0)
+            
+            pts_img = get_pts_from_predict ( keras_model.predict (image)[-1][0], center, scale)
+            pts_img = [ ( int(pt[0]), int(pt[1]) ) for pt in pts_img ]             
+            landmarks.append ( ((left, top, right, bottom),pts_img) )
+    else:
+        print("Warning: No faces were detected.")
+        
+    return landmarks
\ No newline at end of file
diff --git a/lib/FaceLandmarksExtractor/__init__.py b/lib/FaceLandmarksExtractor/__init__.py
new file mode 100644
index 0000000..6da6296
--- /dev/null
+++ b/lib/FaceLandmarksExtractor/__init__.py
@@ -0,0 +1 @@
+from .FaceLandmarksExtractor import extract
\ No newline at end of file
diff --git a/lib/FaceLandmarksExtractor/mmod_human_face_detector.dat b/lib/FaceLandmarksExtractor/mmod_human_face_detector.dat
new file mode 100644
index 0000000..f1f73a5
Binary files /dev/null and b/lib/FaceLandmarksExtractor/mmod_human_face_detector.dat differ
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 365f686..e5b10e8 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -1,38 +1,19 @@
-import dlib
-import face_recognition
-import face_recognition_models
+from lib import FaceLandmarksExtractor
 
 def detect_faces(frame, model="hog"):
-    face_locations = face_recognition.face_locations(frame, model=model)
-    landmarks = _raw_face_landmarks(frame, face_locations)
-
-    for ((y, right, bottom, x), landmarks) in zip(face_locations, landmarks):
-        yield DetectedFace(frame[y: bottom, x: right], x, right - x, y, bottom - y, landmarks)
-
-# Copy/Paste (mostly) from private method in face_recognition
-predictor_68_point_model = face_recognition_models.pose_predictor_model_location()
-pose_predictor = dlib.shape_predictor(predictor_68_point_model)
-
-def _raw_face_landmarks(face_image, face_locations):
-    face_locations = [_css_to_rect(face_location) for face_location in face_locations]
-    return [pose_predictor(face_image, face_location) for face_location in face_locations]
-
-def _css_to_rect(css):
-    return dlib.rectangle(css[3], css[0], css[1], css[2])
-# end of Copy/Paste
+    fd = FaceLandmarksExtractor.extract (frame, True if model == "cnn" else False )
+    for face in fd:
+        x, y, right, bottom, landmarks = face[0][0], face[0][1], face[0][2], face[0][3], face[1]
+        yield DetectedFace(frame[y: bottom, x: right], x, right - x, y, bottom - y, landmarksXY=landmarks)
 
 class DetectedFace(object):
-    def __init__(self, image=None, x=None, w=None, y=None, h=None, landmarks=None, landmarksXY=None):
+    def __init__(self, image=None, x=None, w=None, y=None, h=None, landmarksXY=None):
         self.image = image
         self.x = x
         self.w = w
         self.y = y
         self.h = h
-        self.landmarks = landmarks
         self.landmarksXY = landmarksXY
 
     def landmarksAsXY(self):
-        if self.landmarksXY:
-            return self.landmarksXY
-        self.landmarksXY = [(p.x, p.y) for p in self.landmarks.parts()]
-        return self.landmarksXY
+        return self.landmarksXY
\ No newline at end of file
diff --git a/plugins/Extract_Align.py b/plugins/Extract_Align.py
index a28b755..7881983 100644
--- a/plugins/Extract_Align.py
+++ b/plugins/Extract_Align.py
@@ -6,10 +6,6 @@ from lib.aligner import get_align_mat
 
 class Extract(object):
     def extract(self, image, face, size):
-        if face.landmarks == None:
-            print("Warning! landmarks not found. Switching to crop!")
-            return cv2.resize(face.image, (size, size))
-
         alignment = get_align_mat( face )
         return self.transform( image, alignment, size, 48 )
     
