commit 88352b0268efe49b54c9bdfad4846317752991ed
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sun Sep 15 17:07:41 2019 +0100

    De-Multiprocess Extract (#871)
    
    * requirements.txt: - Pin opencv to 4.1.1 (fixes cv2-dnn error)
    
    * lib.face_detect.DetectedFace: change LandmarksXY to landmarks_xy. Add left, right, top, bottom attributes
    
    * lib.model.session: Session manager for loading models into different graphs (for Nvidia + CPU)
    
    * plugins.extract._base: New parent class for all extract plugins
    
    * plugins.extract.pipeline. Remove MultiProcessing. Dynamically limit batchsize for Nvidia cards. Remove loglevel input
    
    * S3FD + FAN plugins. Standardise to Keras version for all backends
    
    * Standardize all extract plugins to new threaded codebase
    
    * Documentation. Start implementing Numpy style docstrings for Sphinx Documentation
    
    * Remove s3fd_amd. Change convert OTF to expect DetectedFace object
    
    * faces_detect - clean up and documentation
    
    * Remove PoolProcess
    
    * Migrate manual tool to new extract workflow
    
    * Remove AMD specific extractor code from cli and plugins
    
    * Sort tool to new extract workflow
    
    * Remove multiprocessing from project
    
    * Remove multiprocessing queues from QueueManager
    
    * Remove multiprocessing support from logger
    
    * Move face_filter to new extraction pipeline
    
    * Alignments landmarksXY > landmarks_xy and legacy handling
    
    * Intercept get_backend for sphinx doc build
    
    # Add Sphinx documentation

diff --git a/.gitignore b/.gitignore
index e2d5ca9..8ce9f7b 100644
--- a/.gitignore
+++ b/.gitignore
@@ -7,12 +7,15 @@
 !*.nsi
 !*.png
 !*.py
+!*.rst
 !*.txt
 !.cache
 !Dockerfile*
 !requirements*
 !.install/
 !.install/windows
+!docs
+!docs/full
 !config/
 !lib/
 !lib/*
diff --git a/docs/conf.py b/docs/conf.py
new file mode 100644
index 0000000..aa8b134
--- /dev/null
+++ b/docs/conf.py
@@ -0,0 +1,54 @@
+# Configuration file for the Sphinx documentation builder.
+#
+# This file only contains a selection of the most common options. For a full
+# list see the documentation:
+# https://www.sphinx-doc.org/en/master/usage/configuration.html
+
+# -- Path setup --------------------------------------------------------------
+
+# If extensions (or modules to document with autodoc) are in another directory,
+# add these directories to sys.path here. If the directory is relative to the
+# documentation root, use os.path.abspath to make it absolute, like shown here.
+#
+import os
+import sys
+sys.path.insert(0, os.path.abspath('../'))
+sys.setrecursionlimit(1500)
+
+# -- Project information -----------------------------------------------------
+
+project = 'faceswap'
+copyright = '2019, faceswap.dev'
+author = 'faceswap.dev'
+
+# The full version, including alpha/beta/rc tags
+release = '0.99'
+
+
+# -- General configuration ---------------------------------------------------
+
+# Add any Sphinx extension module names here, as strings. They can be
+# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
+# ones.
+extensions = ['sphinx.ext.napoleon', ]
+
+# Add any paths that contain templates here, relative to this directory.
+templates_path = ['_templates']
+
+# List of patterns, relative to source directory, that match files and
+# directories to ignore when looking for source files.
+# This pattern also affects html_static_path and html_extra_path.
+exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store']
+
+
+# -- Options for HTML output -------------------------------------------------
+
+# The theme to use for HTML and HTML Help pages.  See the documentation for
+# a list of builtin themes.
+#
+html_theme = 'sphinx_rtd_theme'
+
+# Add any paths that contain custom static files (such as style sheets) here,
+# relative to this directory. They are copied after the builtin static files,
+# so a file named "default.css" will overwrite the builtin "default.css".
+html_static_path = ['_static']
diff --git a/docs/full/lib.faces_detect.rst b/docs/full/lib.faces_detect.rst
new file mode 100644
index 0000000..e246962
--- /dev/null
+++ b/docs/full/lib.faces_detect.rst
@@ -0,0 +1,7 @@
+lib.faces\_detect module
+========================
+
+.. automodule:: lib.faces_detect
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/lib.model.rst b/docs/full/lib.model.rst
new file mode 100644
index 0000000..b75d7a9
--- /dev/null
+++ b/docs/full/lib.model.rst
@@ -0,0 +1,17 @@
+lib.model package
+=================
+
+Submodules
+----------
+
+.. toctree::
+
+   lib.model.session
+
+Module contents
+---------------
+
+.. automodule:: lib.model
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/lib.model.session.rst b/docs/full/lib.model.session.rst
new file mode 100644
index 0000000..e80025a
--- /dev/null
+++ b/docs/full/lib.model.session.rst
@@ -0,0 +1,7 @@
+lib.model.session module
+========================
+
+.. automodule:: lib.model.session
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/lib.rst b/docs/full/lib.rst
new file mode 100644
index 0000000..a4726c5
--- /dev/null
+++ b/docs/full/lib.rst
@@ -0,0 +1,18 @@
+lib package
+===========
+
+Subpackages
+-----------
+
+.. toctree::
+
+   lib.model
+   lib.faces_detect
+
+Module contents
+---------------
+
+.. automodule:: lib
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/modules.rst b/docs/full/modules.rst
new file mode 100644
index 0000000..f632b60
--- /dev/null
+++ b/docs/full/modules.rst
@@ -0,0 +1,8 @@
+faceswap
+========
+
+.. toctree::
+   :maxdepth: 4
+
+   lib
+   plugins
diff --git a/docs/full/plugins.extract._base.rst b/docs/full/plugins.extract._base.rst
new file mode 100644
index 0000000..242ab98
--- /dev/null
+++ b/docs/full/plugins.extract._base.rst
@@ -0,0 +1,7 @@
+plugins.extract._base module
+===============================
+
+.. automodule:: plugins.extract._base
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.align._base.rst b/docs/full/plugins.extract.align._base.rst
new file mode 100644
index 0000000..b8ce7b5
--- /dev/null
+++ b/docs/full/plugins.extract.align._base.rst
@@ -0,0 +1,7 @@
+plugins.extract.align._base module
+======================================
+
+.. automodule:: plugins.extract.align._base
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.align.rst b/docs/full/plugins.extract.align.rst
new file mode 100644
index 0000000..7ae7a06
--- /dev/null
+++ b/docs/full/plugins.extract.align.rst
@@ -0,0 +1,17 @@
+plugins.extract.align package
+=============================
+
+Submodules
+----------
+
+.. toctree::
+
+   plugins.extract.align._base
+   
+Module contents
+---------------
+
+.. automodule:: plugins.extract.align
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.detect._base.rst b/docs/full/plugins.extract.detect._base.rst
new file mode 100644
index 0000000..3ee95a1
--- /dev/null
+++ b/docs/full/plugins.extract.detect._base.rst
@@ -0,0 +1,7 @@
+plugins.extract.detect._base module
+======================================
+
+.. automodule:: plugins.extract.detect._base
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.detect.rst b/docs/full/plugins.extract.detect.rst
new file mode 100644
index 0000000..27f2d9d
--- /dev/null
+++ b/docs/full/plugins.extract.detect.rst
@@ -0,0 +1,17 @@
+plugins.extract.detect package
+==============================
+
+Submodules
+----------
+
+.. toctree::
+
+   plugins.extract.detect._base
+
+Module contents
+---------------
+
+.. automodule:: plugins.extract.detect
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.pipeline.rst b/docs/full/plugins.extract.pipeline.rst
new file mode 100644
index 0000000..f36acf8
--- /dev/null
+++ b/docs/full/plugins.extract.pipeline.rst
@@ -0,0 +1,7 @@
+plugins.extract.pipeline module
+===============================
+
+.. automodule:: plugins.extract.pipeline
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.extract.rst b/docs/full/plugins.extract.rst
new file mode 100644
index 0000000..384a8c5
--- /dev/null
+++ b/docs/full/plugins.extract.rst
@@ -0,0 +1,26 @@
+plugins.extract package
+=======================
+
+Subpackages
+-----------
+
+.. toctree::
+
+   plugins.extract.align
+   plugins.extract.detect
+
+Submodules
+----------
+
+.. toctree::
+
+   plugins.extract._base
+   plugins.extract.pipeline
+
+Module contents
+---------------
+
+.. automodule:: plugins.extract
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/full/plugins.rst b/docs/full/plugins.rst
new file mode 100644
index 0000000..c0659fe
--- /dev/null
+++ b/docs/full/plugins.rst
@@ -0,0 +1,17 @@
+plugins package
+===============
+
+Subpackages
+-----------
+
+.. toctree::
+
+   plugins.extract
+
+Module contents
+---------------
+
+.. automodule:: plugins
+   :members:
+   :undoc-members:
+   :show-inheritance:
diff --git a/docs/index.rst b/docs/index.rst
new file mode 100644
index 0000000..af05d85
--- /dev/null
+++ b/docs/index.rst
@@ -0,0 +1,21 @@
+.. faceswap documentation master file, created by
+   sphinx-quickstart on Fri Sep 13 11:28:50 2019.
+   You can adapt this file completely to your liking, but it should at least
+   contain the root `toctree` directive.
+
+faceswap.dev Developer Documentation
+====================================
+
+.. toctree::
+   :maxdepth: 6
+   :caption: Contents:
+
+   full/modules
+
+
+Indices and tables
+==================
+
+* :ref:`genindex`
+* :ref:`modindex`
+* :ref:`search`
diff --git a/lib/aligner.py b/lib/aligner.py
index 4770f90..4018661 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -139,7 +139,7 @@ def get_matrix_scaling(mat):
 def get_align_mat(face, size, should_align_eyes):
     """ Return the alignment Matrix """
     logger.trace("size: %s, should_align_eyes: %s", size, should_align_eyes)
-    mat_umeyama = umeyama(np.array(face.landmarks_as_xy[17:]), True)[0:2]
+    mat_umeyama = umeyama(np.array(face.landmarks_xy[17:]), True)[0:2]
 
     if should_align_eyes is False:
         return mat_umeyama
@@ -147,7 +147,7 @@ def get_align_mat(face, size, should_align_eyes):
     mat_umeyama = mat_umeyama * size
 
     # Convert to matrix
-    landmarks = np.matrix(face.landmarks_as_xy)
+    landmarks = np.matrix(face.landmarks_xy)
 
     # cv2 expects points to be in the form
     # np.array([ [[x1, y1]], [[x2, y2]], ... ]), we'll expand the dim
diff --git a/lib/alignments.py b/lib/alignments.py
index 0250060..8717947 100644
--- a/lib/alignments.py
+++ b/lib/alignments.py
@@ -34,6 +34,7 @@ class Alignments():
         self.file = self.get_location(folder, filename)
 
         self.data = self.load()
+        self.update_legacy()
         logger.debug("Initialized %s", self.__class__.__name__)
 
     # << PROPERTIES >> #
@@ -272,6 +273,11 @@ class Alignments():
 
     # << LEGACY FUNCTIONS >> #
 
+    def update_legacy(self):
+        """ Update legacy alignments """
+        if self.has_legacy_landmarksxy():
+            logger.info("Updating legacy alignments")
+            self.update_legacy_landmarksxy()
     # < Rotation > #
     # The old rotation method would rotate the image to find a face, then
     # store the rotated landmarks along with a rotation value to tell the
@@ -361,3 +367,25 @@ class Alignments():
                            abs(count_match), msg, frame_name)
         for idx, i_hash in hashes.items():
             faces[idx]["hash"] = i_hash
+
+    # <landmarks> #
+    # Landmarks renamed from landmarksXY to landmarks_xy for PEP compliance
+    def has_legacy_landmarksxy(self):
+        """ check for legacy landmarksXY keys """
+        logger.debug("checking legacy landmarksXY")
+        retval = (any(key == "landmarksXY"
+                      for alignments in self.data.values()
+                      for alignment in alignments
+                      for key in alignment))
+        logger.debug("legacy landmarksXY: %s", retval)
+        return retval
+
+    def update_legacy_landmarksxy(self):
+        """ Update landmarksXY to landmarks_xy and save alignments """
+        update_count = 0
+        for alignments in self.data.values():
+            for alignment in alignments:
+                alignment["landmarks_xy"] = alignment.pop("landmarksXY")
+                update_count += 1
+        logger.debug("Updated landmarks_xy: %s", update_count)
+        self.save()
diff --git a/lib/cli.py b/lib/cli.py
index c51258a..cafe240 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -519,13 +519,19 @@ class ExtractArgs(ExtractConvertArgs):
     @staticmethod
     def get_info():
         """ Return command information """
-        return "Extract faces from image or video sources"
+        return ("Extract faces from image or video sources.\n"
+                "Extraction plugins can be configured in the 'Settings' Menu")
 
     @staticmethod
     def get_optional_arguments():
         """ Put the arguments in a list so that they are accessible from both
         argparse and gui """
-        backend = get_backend()
+        if get_backend() == "cpu":
+            default_detector = default_aligner = "cv2-dnn"
+        else:
+            default_detector = "s3fd"
+            default_aligner = "fan"
+
         argument_list = []
         argument_list.append({"opts": ("--serializer", ),
                               "type": str.lower,
@@ -536,19 +542,6 @@ class ExtractArgs(ExtractConvertArgs):
                               "help": "Serializer for alignments file. If yaml is chosen and not "
                                       "available, then json will be used as the default "
                                       "fallback."})
-        s3fd = "s3fd"
-        fan = "fan"
-        if backend == "cpu":
-            default_detector = default_aligner = "cv2-dnn"
-        else:
-            default_detector = s3fd
-            default_aligner = fan
-        if backend == "amd":
-            default_detector += "-amd"
-            default_aligner += "-amd"
-            s3fd += "-amd"
-            fan += "-amd"
-
         argument_list.append({
             "opts": ("-D", "--detector"),
             "action": Radio,
@@ -558,13 +551,12 @@ class ExtractArgs(ExtractConvertArgs):
             "group": "Plugins",
             "help": "R|Detector to use. Some of these have configurable settings in "
                     "'/config/extract.ini' or 'Edit > Configure Extract Plugins':"
-                    "\nL|'cv2-dnn': A CPU only extractor, is the least reliable, but uses least "
+                    "\nL|cv2-dnn: A CPU only extractor, is the least reliable, but uses least "
                     "resources and runs fast on CPU. Use this if not using a GPU and time is "
                     "important."
-                    "\nL|'mtcnn': Fast on GPU, slow on CPU. Uses fewer resources than other GPU "
-                    "detectors but can often return more false positives. NB: Runs on CPU for AMD "
-                    "cards."
-                    "\nL|'" + s3fd + "': Fast on GPU, slow on CPU. Can detect more faces and "
+                    "\nL|mtcnn: Fast on GPU, slow on CPU. Uses fewer resources than other GPU "
+                    "detectors but can often return more false positives."
+                    "\nL|s3fd: Fast on GPU, slow on CPU. Can detect more faces and "
                     "fewer false positives than other GPU detectors, but is a lot more resource "
                     "intensive."})
         argument_list.append({
@@ -575,10 +567,10 @@ class ExtractArgs(ExtractConvertArgs):
             "default": default_aligner,
             "group": "Plugins",
             "help": "R|Aligner to use."
-                    "\nL|'cv2-dnn': A cpu only CNN based landmark detector. Faster, less "
+                    "\nL|cv2-dnn: A cpu only CNN based landmark detector. Faster, less "
                     "resource intensive, but less accurate. Only use this if not using a gpu "
                     " and time is important."
-                    "\nL|'" + fan + "': Face Alignment Network. Best aligner. GPU "
+                    "\nL|fan: Face Alignment Network. Best aligner. GPU "
                     "heavy, slow when not running on GPU"})
         argument_list.append({"opts": ("-nm", "--normalization"),
                               "action": Radio,
@@ -592,11 +584,11 @@ class ExtractArgs(ExtractConvertArgs):
                                       "extraction speed cost. Different methods will yield "
                                       "different results on different sets. NB: This does not "
                                       "impact the output face, just the input to the aligner."
-                                      "\nL|'none': Don't perform normalization on the face."
-                                      "\nL|'clahe': Perform Contrast Limited Adaptive Histogram "
+                                      "\nL|none: Don't perform normalization on the face."
+                                      "\nL|clahe: Perform Contrast Limited Adaptive Histogram "
                                       "Equalization on the face."
-                                      "\nL|'hist': Equalize the histograms on the RGB channels."
-                                      "\nL|'mean': Normalize the face colors to the mean."})
+                                      "\nL|hist: Equalize the histograms on the RGB channels."
+                                      "\nL|mean: Normalize the face colors to the mean."})
         argument_list.append({"opts": ("-r", "--rotate-images"),
                               "type": str,
                               "dest": "rotate_images",
@@ -752,7 +744,8 @@ class ConvertArgs(ExtractConvertArgs):
     @staticmethod
     def get_info():
         """ Return command information """
-        return "Swap the original faces in a source video/images to your final faces"
+        return ("Swap the original faces in a source video/images to your final faces.\n"
+                "Conversion plugins can be configured in the 'Settings' Menu")
 
     @staticmethod
     def get_optional_arguments():
@@ -985,9 +978,9 @@ class TrainArgs(FaceSwapArgs):
     @staticmethod
     def get_info():
         """ Return command information """
-        return ("Train a model on extracted original (A) and swap (B) faces\n"
-                "Training models can take a long time. Anything from 24hrs to "
-                "over a week")
+        return ("Train a model on extracted original (A) and swap (B) faces.\n"
+                "Training models can take a long time. Anything from 24hrs to over a week\n"
+                "Model plugins can be configured in the 'Settings' Menu")
 
     @staticmethod
     def get_argument_list():
diff --git a/lib/face_filter.py b/lib/face_filter.py
index cd1226f..3191971 100644
--- a/lib/face_filter.py
+++ b/lib/face_filter.py
@@ -3,8 +3,6 @@
 
 import logging
 
-from lib.faces_detect import DetectedFace
-from lib.logger import get_loglevel
 from lib.vgg_face import VGGFace
 from lib.utils import cv2_read_img
 from plugins.extract.pipeline import Extractor
@@ -21,16 +19,25 @@ class FaceFilter():
     """ Face filter for extraction
         NB: we take only first face, so the reference file should only contain one face. """
 
-    def __init__(self, reference_file_paths, nreference_file_paths, detector, aligner, loglevel,
+    def __init__(self, reference_file_paths, nreference_file_paths, detector, aligner,
                  multiprocess=False, threshold=0.4):
         logger.debug("Initializing %s: (reference_file_paths: %s, nreference_file_paths: %s, "
-                     "detector: %s, aligner: %s. loglevel: %s, multiprocess: %s, threshold: %s)",
+                     "detector: %s, aligner: %s, multiprocess: %s, threshold: %s)",
                      self.__class__.__name__, reference_file_paths, nreference_file_paths,
-                     detector, aligner, loglevel, multiprocess, threshold)
-        self.numeric_loglevel = get_loglevel(loglevel)
+                     detector, aligner, multiprocess, threshold)
         self.vgg_face = VGGFace()
         self.filters = self.load_images(reference_file_paths, nreference_file_paths)
-        self.align_faces(detector, aligner, loglevel, multiprocess)
+        # TODO Revert face-filter to use the selected detector and aligner.
+        # Currently Tensorflow does not release vram after it has been allocated
+        # Whilst this vram can still be used, the pipeline for the extraction process can't see
+        # it so thinks there is not enough vram available.
+        # Either the pipeline will need to be changed to be re-usable by face-filter and extraction
+        # Or another vram measurement technique will need to be implemented to for when TF has
+        # already performed allocation. For now we force CPU detectors.
+
+        # self.align_faces(detector, aligner, multiprocess)
+        self.align_faces("cv2-dnn", "cv2-dnn", multiprocess)
+
         self.get_filter_encodings()
         self.threshold = threshold
         logger.debug("Initialized %s", self.__class__.__name__)
@@ -49,38 +56,25 @@ class FaceFilter():
         return retval
 
     # Extraction pipeline
-    def align_faces(self, detector_name, aligner_name, loglevel, multiprocess):
+    def align_faces(self, detector_name, aligner_name, multiprocess):
         """ Use the requested detectors to retrieve landmarks for filter images """
-        extractor = Extractor(detector_name, aligner_name, loglevel, multiprocess=multiprocess)
+        extractor = Extractor(detector_name, aligner_name, multiprocess=multiprocess)
         self.run_extractor(extractor)
         del extractor
         self.load_aligned_face()
 
     def run_extractor(self, extractor):
         """ Run extractor to get faces """
-        exception = False
         for _ in range(extractor.passes):
             self.queue_images(extractor)
-            if exception:
-                break
             extractor.launch()
             for faces in extractor.detected_faces():
-                exception = faces.get("exception", False)
-                if exception:
-                    break
                 filename = faces["filename"]
                 detected_faces = faces["detected_faces"]
-
                 if len(detected_faces) > 1:
                     logger.warning("Multiple faces found in %s file: '%s'. Using first detected "
                                    "face.", self.filters[filename]["type"], filename)
-                    detected_faces = [detected_faces[0]]
-                self.filters[filename]["detected_faces"] = detected_faces
-
-                # Aligner output
-                if extractor.final_pass:
-                    landmarks = faces["landmarks"]
-                    self.filters[filename]["landmarks"] = landmarks
+                self.filters[filename]["detected_face"] = detected_faces[0]
 
     def queue_images(self, extractor):
         """ queue images for detection and alignment """
@@ -100,13 +94,8 @@ class FaceFilter():
         """ Align the faces for vgg_face input """
         for filename, face in self.filters.items():
             logger.debug("Loading aligned face: '%s'", filename)
-            bounding_box = face["detected_faces"][0]
             image = face["image"]
-            landmarks = face["landmarks"][0]
-
-            detected_face = DetectedFace()
-            detected_face.from_bounding_box_dict(bounding_box, image)
-            detected_face.landmarksXY = landmarks
+            detected_face = face["detected_face"]
             detected_face.load_aligned(image, size=224)
             face["face"] = detected_face.aligned_face
             del face["image"]
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 651a107..a4a4b58 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -10,18 +10,49 @@ logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
 class DetectedFace():
-    """ Detected face and landmark information """
-    def __init__(  # pylint: disable=invalid-name
-            self, image=None, x=None, w=None, y=None, h=None,
-            landmarksXY=None):
-        logger.trace("Initializing %s", self.__class__.__name__)
+    """ Detected face and landmark information
+
+    Holds information about a detected face, it's location in a source image
+    and the face's 68 point landmarks.
+
+    Methods for aligning a face are also callable from here.
+
+    Parameters
+    ----------
+    image: np.ndarray, optional
+        This is a generic image placeholder that should not be relied on to be holding a particular
+        image. It may hold the source frame that holds the face, a cropped face or a scaled image
+        depending on the method using this object.
+    x: int
+        The left most point (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    w: int
+        The width (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    y: int
+        The top most point (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    h: int
+        The height (in pixels) of the face's bounding box as discovered in
+        :mod:`plugins.extract.detect`
+    landmarks_xy: list
+        The 68 point landmarks as discovered in :mod:`plugins.extract.align`. Should be a ``list``
+        of 68 `(x, y)` ``tuples`` with each of the landmark co-ordinates.
+    """
+    def __init__(self, image=None, x=None, w=None, y=None, h=None, landmarks_xy=None):
+        logger.trace("Initializing %s: (image: %s, x: %s, w: %s, y: %s, h:%s, landmarks_xy: %s)",
+                     self.__class__.__name__,
+                     image.shape if image is not None and image.any() else image,
+                     x, w, y, h, landmarks_xy)
         self.image = image
         self.x = x
         self.w = w
         self.y = y
         self.h = h
-        self.landmarksXY = landmarksXY
-        self.hash = None  # Hash must be set when the file is saved due to image compression
+        self.landmarks_xy = landmarks_xy
+        self.hash = None
+        """ str: The hash of the face. This cannot be set until the file is saved due to image
+        compression, but will be set if loading data from :func:`from_alignment` """
 
         self.aligned = dict()
         self.feed = dict()
@@ -29,83 +60,124 @@ class DetectedFace():
         logger.trace("Initialized %s", self.__class__.__name__)
 
     @property
-    def extract_ratio(self):
-        """ The ratio of padding to add for training images """
-        return 0.375
+    def left(self):
+        """int: Left point (in pixels) of face detection bounding box within the parent image """
+        return self.x
 
     @property
-    def landmarks_as_xy(self):
-        """ Landmarks as XY """
-        return self.landmarksXY
-
-    def to_bounding_box_dict(self):
-        """ Return Bounding Box as a bounding box dixt """
-        retval = dict(left=self.x, top=self.y, right=self.x + self.w, bottom=self.y + self.h)
-        logger.trace("Returning: %s", retval)
-        return retval
-
-    def from_bounding_box_dict(self, bounding_box_dict, image=None):
-        """ Set Bounding Box from a bounding box dict """
-        logger.trace("Creating from bounding box dict: %s", bounding_box_dict)
-        if not isinstance(bounding_box_dict, dict):
-            raise ValueError("Supplied Bounding Box is not a dictionary.")
-        self.x = bounding_box_dict["left"]
-        self.w = bounding_box_dict["right"] - bounding_box_dict["left"]
-        self.y = bounding_box_dict["top"]
-        self.h = bounding_box_dict["bottom"] - bounding_box_dict["top"]
-        if image is not None and image.any():
-            self.image_to_face(image)
-        logger.trace("Created from bounding box dict: (x: %s, w: %s, y: %s. h: %s)",
-                     self.x, self.w, self.y, self.h)
+    def top(self):
+        """int: Top point (in pixels) of face detection bounding box within the parent image """
+        return self.y
 
-    def image_to_face(self, image):
-        """ Crop an image around bounding box to the face
-            and capture it's dimensions """
-        logger.trace("Cropping face from image")
-        self.image = image[self.y: self.y + self.h,
-                           self.x: self.x + self.w]
+    @property
+    def right(self):
+        """int: Right point (in pixels) of face detection bounding box within the parent image """
+        return self.x + self.w
+
+    @property
+    def bottom(self):
+        """int: Bottom point (in pixels) of face detection bounding box within the parent image """
+        return self.y + self.h
+
+    @property
+    def _extract_ratio(self):
+        """ float: The ratio of padding to add for training images """
+        return 0.375
 
     def to_alignment(self):
-        """ Convert a detected face to alignment dict """
+        """  Return the detected face formatted for an alignments file
+
+        returns
+        -------
+        alignment: dict
+            The alignment dict will be returned with the keys ``x``, ``w``, ``y``, ``h``,
+            ``landmarks_xy``, ``hash``.
+        """
+
         alignment = dict()
         alignment["x"] = self.x
         alignment["w"] = self.w
         alignment["y"] = self.y
         alignment["h"] = self.h
-        alignment["landmarksXY"] = self.landmarksXY
+        alignment["landmarks_xy"] = self.landmarks_xy
         alignment["hash"] = self.hash
         logger.trace("Returning: %s", alignment)
         return alignment
 
     def from_alignment(self, alignment, image=None):
-        """ Convert a face alignment to detected face object """
+        """ Set the attributes of this class from an alignments file and optionally load the face
+        into the ``image`` attribute.
+
+        Parameters
+        ----------
+        alignment: dict
+            A dictionary entry for a face from an alignments file containing the keys
+            ``x``, ``w``, ``y``, ``h``, ``landmarks_xy``. Optionally the key ``hash``
+            will be provided, but not all use cases will know the face hash at this time.
+        image: numpy.ndarray, optional
+            If an image is passed in, then the ``image`` attribute will
+            be set to the cropped face based on the passed in bounding box co-ordinates
+        """
+
         logger.trace("Creating from alignment: (alignment: %s, has_image: %s)",
                      alignment, bool(image is not None))
         self.x = alignment["x"]
         self.w = alignment["w"]
         self.y = alignment["y"]
         self.h = alignment["h"]
-        self.landmarksXY = alignment["landmarksXY"]
+        self.landmarks_xy = alignment["landmarks_xy"]
         # Manual tool does not know the final hash so default to None
         self.hash = alignment.get("hash", None)
         if image is not None and image.any():
-            self.image_to_face(image)
+            self._image_to_face(image)
         logger.trace("Created from alignment: (x: %s, w: %s, y: %s. h: %s, "
                      "landmarks: %s)",
-                     self.x, self.w, self.y, self.h, self.landmarksXY)
+                     self.x, self.w, self.y, self.h, self.landmarks_xy)
+
+    def _image_to_face(self, image):
+        """ set self.image to be the cropped face from detected bounding box """
+        logger.trace("Cropping face from image")
+        self.image = image[self.top: self.bottom,
+                           self.left: self.right]
 
     # <<< Aligned Face methods and properties >>> #
     def load_aligned(self, image, size=256, align_eyes=False, dtype=None):
-        """ No need to load aligned information for all uses of this
-            class, so only call this to load the information for easy
-            reference to aligned properties for this face """
-        # Don't reload an already aligned face:
+        """ Align a face from a given image.
+
+        Aligning a face is a relatively expensive task and is not required for all uses of
+        the :class:`~lib.faces_detect.DetectedFace` object, so call this function explicitly to
+        load an aligned face.
+
+        This method plugs into :mod:`lib.aligner` to perform face alignment based on this face's
+        ``landmarks_xy``. If the face has already been aligned, then this function will return
+        having performed no action.
+
+        Parameters
+        ----------
+        image: numpy.ndarray
+            The image that contains the face to be aligned
+        size: int
+            The size of the output face in pixels
+        align_eyes: bool, optional
+            Optionally perform additional alignment to align eyes. Default: `False`
+        dtype: str, optional
+            Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``
+
+        Notes
+        -----
+        This method must be executed to get access to the following `properties`:
+            - :func:`original_roi`
+            - :func:`aligned_landmarks`
+            - :func:`aligned_face`
+            - :func:`adjusted_interpolators`
+        """
         if self.aligned:
+            # Don't reload an already aligned face
             logger.trace("Skipping alignment calculation for already aligned face")
         else:
             logger.trace("Loading aligned face: (size: %s, align_eyes: %s, dtype: %s)",
                          size, align_eyes, dtype)
-            padding = int(size * self.extract_ratio) // 2
+            padding = int(size * self._extract_ratio) // 2
             self.aligned["size"] = size
             self.aligned["padding"] = padding
             self.aligned["align_eyes"] = align_eyes
@@ -124,24 +196,39 @@ class DetectedFace():
                                                  for key, val in self.aligned.items()
                                                  if key != "face"})
 
-    def padding_from_coverage(self, size, coverage_ratio):
+    def _padding_from_coverage(self, size, coverage_ratio):
         """ Return the image padding for a face from coverage_ratio set against a
             pre-padded training image """
-        adjusted_ratio = coverage_ratio - (1 - self.extract_ratio)
+        adjusted_ratio = coverage_ratio - (1 - self._extract_ratio)
         padding = round((size * adjusted_ratio) / 2)
         logger.trace(padding)
         return padding
 
     def load_feed_face(self, image, size=64, coverage_ratio=0.625, dtype=None):
-        """ Return a face in the correct dimensions for feeding into a NN
-
-            Coverage ratio should be the ratio of the extracted image that was used for
-            training """
+        """ Align a face in the correct dimensions for feeding into a model.
+
+        Parameters
+        ----------
+        image: numpy.ndarray
+            The image that contains the face to be aligned
+        size: int
+            The size of the face in pixels to be fed into the model
+        coverage_ratio: float, optional
+            the ratio of the extracted image that was used for training. Default: `0.625`
+        dtype: str, optional
+            Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``
+
+        Notes
+        -----
+        This method must be executed to get access to the following `properties`:
+            - :func:`feed_face`
+            - :func:`feed_interpolators`
+        """
         logger.trace("Loading feed face: (size: %s, coverage_ratio: %s, dtype: %s)",
                      size, coverage_ratio, dtype)
 
         self.feed["size"] = size
-        self.feed["padding"] = self.padding_from_coverage(size, coverage_ratio)
+        self.feed["padding"] = self._padding_from_coverage(size, coverage_ratio)
         self.feed["matrix"] = get_align_mat(self, size, should_align_eyes=False)
 
         face = np.clip(AlignerExtract().transform(image,
@@ -152,18 +239,35 @@ class DetectedFace():
         self.feed["face"] = face if dtype is None else face.astype(dtype)
 
         logger.trace("Loaded feed face. (face_shape: %s, matrix: %s)",
-                     self.feed_face.shape, self.feed_matrix)
+                     self.feed_face.shape, self._feed_matrix)
 
     def load_reference_face(self, image, size=64, coverage_ratio=0.625, dtype=None):
-        """ Return a face in the correct dimensions for reference to the output from a NN
-
-            Coverage ratio should be the ratio of the extracted image that was used for
-            training """
+        """ Align a face in the correct dimensions for reference against the output from a model.
+
+        Parameters
+        ----------
+        image: numpy.ndarray
+            The image that contains the face to be aligned
+        size: int
+            The size of the face in pixels to be fed into the model
+        coverage_ratio: float, optional
+            the ratio of the extracted image that was used for training. Default: `0.625`
+        dtype: str, optional
+            Optionally set a ``dtype`` for the final face to be formatted in. Default: ``None``
+
+        Notes
+        -----
+        This method must be executed to get access to the following `properties`:
+            - :func:`reference_face`
+            - :func:`reference_landmarks`
+            - :func:`reference_matrix`
+            - :func:`reference_interpolators`
+        """
         logger.trace("Loading reference face: (size: %s, coverage_ratio: %s, dtype: %s)",
                      size, coverage_ratio, dtype)
 
         self.reference["size"] = size
-        self.reference["padding"] = self.padding_from_coverage(size, coverage_ratio)
+        self.reference["padding"] = self._padding_from_coverage(size, coverage_ratio)
         self.reference["matrix"] = get_align_mat(self, size, should_align_eyes=False)
 
         face = np.clip(AlignerExtract().transform(image,
@@ -178,8 +282,10 @@ class DetectedFace():
 
     @property
     def original_roi(self):
-        """ Return the square aligned box location on the original
-            image """
+        """ numpy.ndarray: The location of the extracted face box within the original frame.
+        Only available after :func:`load_aligned` has been called, otherwise returns ``None``"""
+        if not self.aligned:
+            return None
         roi = AlignerExtract().get_original_roi(self.aligned["matrix"],
                                                 self.aligned["size"],
                                                 self.aligned["padding"])
@@ -188,8 +294,11 @@ class DetectedFace():
 
     @property
     def aligned_landmarks(self):
-        """ Return the landmarks location transposed to extracted face """
-        landmarks = AlignerExtract().transform_points(self.landmarksXY,
+        """ numpy.ndarray: The 68 point landmarks location transposed to the extracted face box.
+        Only available after :func:`load_aligned` has been called, otherwise returns ``None``"""
+        if not self.aligned:
+            return None
+        landmarks = AlignerExtract().transform_points(self.landmarks_xy,
                                                       self.aligned["matrix"],
                                                       self.aligned["size"],
                                                       self.aligned["padding"])
@@ -198,12 +307,16 @@ class DetectedFace():
 
     @property
     def aligned_face(self):
-        """ Return aligned detected face """
-        return self.aligned["face"]
+        """ numpy.ndarray: The aligned detected face. Only available after :func:`load_aligned`
+        has been called with an image, otherwise returns ``None`` """
+        return self.aligned.get("face", None)
 
     @property
-    def adjusted_matrix(self):
-        """ Return adjusted matrix for size/padding combination """
+    def _adjusted_matrix(self):
+        """ numpy.ndarray: Adjusted matrix for size/padding combination. Only available after
+        :func:`load_aligned` has been called, otherwise returns ``None``"""
+        if not self.aligned:
+            return None
         mat = AlignerExtract().transform_matrix(self.aligned["matrix"],
                                                 self.aligned["size"],
                                                 self.aligned["padding"])
@@ -212,17 +325,26 @@ class DetectedFace():
 
     @property
     def adjusted_interpolators(self):
-        """ Return the interpolator and reverse interpolator for the adjusted matrix """
-        return get_matrix_scaling(self.adjusted_matrix)
+        """ tuple:  Tuple of (`interpolator` and `reverse interpolator`) for the adjusted matrix.
+        Only available after :func:`load_aligned` has been called, otherwise returns ``None``"""
+        if not self.aligned:
+            return None
+        return get_matrix_scaling(self._adjusted_matrix)
 
     @property
     def feed_face(self):
-        """ Return face for feeding into NN """
+        """ numpy.ndarray: The aligned face sized for feeding into a model. Only available after
+        :func:`load_feed_face` has been called with an image, otherwise returns ``None`` """
+        if not self.feed:
+            return None
         return self.feed["face"]
 
     @property
-    def feed_matrix(self):
-        """ Return matrix for transforming feed face back to image """
+    def _feed_matrix(self):
+        """ numpy.ndarray: The adjusted matrix face sized for feeding into a model. Only available
+        after :func:`load_feed_face` has been called with an image, otherwise returns ``None`` """
+        if not self.feed:
+            return None
         mat = AlignerExtract().transform_matrix(self.feed["matrix"],
                                                 self.feed["size"],
                                                 self.feed["padding"])
@@ -231,18 +353,30 @@ class DetectedFace():
 
     @property
     def feed_interpolators(self):
-        """ Return the interpolators for an input face """
-        return get_matrix_scaling(self.feed_matrix)
+        """ tuple:  Tuple of (`interpolator` and `reverse interpolator`) for the adjusted feed
+        matrix. Only available after :func:`load_feed_face` has been called, otherwise returns
+        ``None``"""
+        if not self.feed:
+            return None
+        return get_matrix_scaling(self._feed_matrix)
 
     @property
     def reference_face(self):
-        """ Return source face at size of output from NN for reference """
+        """ numpy.ndarray: The aligned face sized for reference against a face coming out of a
+        model. Only available after :func:`load_reference_face` has been called, otherwise
+        returns ``None``"""
+        if not self.reference:
+            return None
         return self.reference["face"]
 
     @property
     def reference_landmarks(self):
-        """ Return the landmarks location transposed to reference face """
-        landmarks = AlignerExtract().transform_points(self.landmarksXY,
+        """ numpy.ndarray: The 68 point landmarks location transposed to the reference face box.
+        Only available after :func:`load_reference_face` has been called, otherwise returns
+        ``None``"""
+        if not self.reference:
+            return None
+        landmarks = AlignerExtract().transform_points(self.landmarks_xy,
                                                       self.reference["matrix"],
                                                       self.reference["size"],
                                                       self.reference["padding"])
@@ -251,7 +385,11 @@ class DetectedFace():
 
     @property
     def reference_matrix(self):
-        """ Return matrix for transforming output face back to image """
+        """ numpy.ndarray: The adjusted matrix face sized for refence against a face coming out of
+         a model. Only available after :func:`load_reference_face` has been called, otherwise
+         returns ``None``"""
+        if not self.reference:
+            return None
         mat = AlignerExtract().transform_matrix(self.reference["matrix"],
                                                 self.reference["size"],
                                                 self.reference["padding"])
@@ -260,5 +398,9 @@ class DetectedFace():
 
     @property
     def reference_interpolators(self):
-        """ Return the interpolators for an output face """
+        """ tuple:  Tuple of (`interpolator` and `reverse interpolator`) for the reference
+        matrix. Only available after :func:`load_reference_face` has been called, otherwise
+        returns ``None``"""
+        if not self.reference:
+            return None
         return get_matrix_scaling(self.reference_matrix)
diff --git a/lib/logger.py b/lib/logger.py
index 6dce466..cd01436 100644
--- a/lib/logger.py
+++ b/lib/logger.py
@@ -2,22 +2,17 @@
 """ Logging Setup """
 import collections
 import logging
-from logging.handlers import QueueHandler, QueueListener, RotatingFileHandler
+from logging.handlers import RotatingFileHandler
 import os
 import re
 import sys
 import traceback
 
 from datetime import datetime
-from time import sleep
 from tqdm import tqdm
 
-from lib.queue_manager import queue_manager
 
-LOG_QUEUE = queue_manager._log_queue  # pylint: disable=protected-access
-
-
-class MultiProcessingLogger(logging.Logger):
+class FaceswapLogger(logging.Logger):
     """ Create custom logger  with custom levels """
     def __init__(self, name):
         for new_level in (("VERBOSE", 15), ("TRACE", 5)):
@@ -48,11 +43,14 @@ class FaceswapFormatter(logging.Formatter):
         Messages that begin with "R|" should be handled as is
     """
     def format(self, record):
-        if record.msg.startswith("R|"):
-            record.msg = record.msg[2:]
-            record.strip_spaces = False
-        elif record.strip_spaces:
-            record.msg = re.sub(" +", " ", record.msg.replace("\n", "\\n").replace("\r", "\\r"))
+        if isinstance(record.msg, str):
+            if record.msg.startswith("R|"):
+                record.msg = record.msg[2:]
+                record.strip_spaces = False
+            elif record.strip_spaces:
+                record.msg = re.sub(" +",
+                                    " ",
+                                    record.msg.replace("\n", "\\n").replace("\r", "\\r"))
         return super().format(record)
 
 
@@ -71,31 +69,27 @@ class TqdmHandler(logging.StreamHandler):
         tqdm.write(msg)
 
 
-def set_root_logger(loglevel=logging.INFO, queue=LOG_QUEUE):
-    """ Setup the root logger.
-        Loaded in main process and into any spawned processes
-        Automatically added in multithreading.py"""
+def set_root_logger(loglevel=logging.INFO):
+    """ Setup the root logger. """
     rootlogger = logging.getLogger()
-    q_handler = QueueHandler(queue)
-    rootlogger.addHandler(q_handler)
     rootlogger.setLevel(loglevel)
+    return rootlogger
 
 
 def log_setup(loglevel, logfile, command, is_gui=False):
     """ initial log set up. """
     numeric_loglevel = get_loglevel(loglevel)
     root_loglevel = min(logging.DEBUG, numeric_loglevel)
-    set_root_logger(loglevel=root_loglevel)
+    rootlogger = set_root_logger(loglevel=root_loglevel)
     log_format = FaceswapFormatter("%(asctime)s %(processName)-15s %(threadName)-15s "
                                    "%(module)-15s %(funcName)-25s %(levelname)-8s %(message)s",
                                    datefmt="%m/%d/%Y %H:%M:%S")
     f_handler = file_handler(numeric_loglevel, logfile, log_format, command)
     s_handler = stream_handler(numeric_loglevel, is_gui)
     c_handler = crash_handler(log_format)
-
-    q_listener = QueueListener(LOG_QUEUE, f_handler, s_handler, c_handler,
-                               respect_handler_level=True)
-    q_listener.start()
+    rootlogger.addHandler(f_handler)
+    rootlogger.addHandler(s_handler)
+    rootlogger.addHandler(c_handler)
     logging.info("Log level set to: %s", loglevel.upper())
 
 
@@ -159,10 +153,6 @@ def crash_log():
     path = os.getcwd()
     filename = os.path.join(path, datetime.now().strftime("crash_report.%Y.%m.%d.%H%M%S%f.log"))
 
-    # Wait until all log items have been processed
-    while not LOG_QUEUE.empty():
-        sleep(1)
-
     freeze_log = list(debug_buffer)
     with open(filename, "w") as outfile:
         outfile.writelines(freeze_log)
@@ -184,7 +174,7 @@ def faceswap_logrecord(*args, **kwargs):
 logging.setLogRecordFactory(faceswap_logrecord)
 
 # Set logger class to custom logger
-logging.setLoggerClass(MultiProcessingLogger)
+logging.setLoggerClass(FaceswapLogger)
 
 # Stores the last 100 debug messages
 debug_buffer = RollingBuffer(maxlen=100)  # pylint: disable=invalid-name
diff --git a/lib/model/session.py b/lib/model/session.py
new file mode 100644
index 0000000..6fa2be7
--- /dev/null
+++ b/lib/model/session.py
@@ -0,0 +1,125 @@
+#!/usr/bin python3
+""" Settings manager for Keras Backend """
+
+import logging
+
+import tensorflow as tf
+from keras.models import load_model as k_load_model, Model
+
+from lib.utils import get_backend
+
+logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
+
+
+class KSession():
+    """ Handles the settings of backend sessions.
+
+    This class acts as a wrapper for various :class:`keras.Model()` functions, ensuring that
+    actions performed on a model are handled consistently within the correct graph.
+
+    Currently this only does anything for Nvidia users, making sure a unique graph and session is
+    provided for the given model.
+
+    Parameters
+    ----------
+    name: str
+        The name of the model that is to be loaded
+    model_path: str
+        The path to the keras model file
+    model_kwargs: dict
+        Any kwargs that need to be passed to :func:`keras.models.load_models()`
+    """
+    def __init__(self, name, model_path, model_kwargs=None):
+        logger.trace("Initializing: %s (name: %s, model_path: %s, model_kwargs: %s)",
+                     self.__class__.__name__, name, model_path, model_kwargs)
+        self._name = name
+        self._session = self._set_session()
+        self._model_path = model_path
+        self._model_kwargs = model_kwargs
+        self._model = None
+        logger.trace("Initialized: %s", self.__class__.__name__,)
+
+    def predict(self, feed):
+        """ Get predictions from the model in the correct session.
+
+        This method is a wrapper for :func:`keras.predict()` function.
+
+        Parameters
+        ----------
+        feed: numpy.ndarray or list
+            The feed to be provided to the model as input. This should be a ``numpy.ndarray``
+            for single inputs or a ``list`` of ``numpy.ndarrays`` for multiple inputs.
+        """
+        if self._session is None:
+            return self._model.predict(feed)
+
+        with self._session.as_default():  # pylint: disable=not-context-manager
+            with self._session.graph.as_default():
+                return self._model.predict(feed)
+
+    def _set_session(self):
+        """ Sets the session and graph.
+
+        If the backend is AMD then this does nothing and the global ``Keras`` ``Session``
+        is used
+        """
+        if get_backend() == "amd":
+            return None
+
+        self.graph = tf.Graph()
+        config = tf.ConfigProto()
+        session = tf.Session(graph=tf.Graph(), config=config)
+        logger.debug("Creating tf.session: (graph: %s, session: %s, config: %s)",
+                     session.graph, session, config)
+        return session
+
+    def load_model(self):
+        """ Loads a model within the correct session.
+
+        This method is a wrapper for :func:`keras.models.load_model()`. Loads a model and its
+        weights from :attr:`model_path`. Any additional ``kwargs`` to be passed to
+        :func:`keras.models.load_model()` should also be defined during initialization of the
+        class.
+        """
+        logger.verbose("Initializing plugin model: %s", self._name)
+        if self._session is None:
+            self._model = k_load_model(self._model_path, **self._model_kwargs)
+        else:
+            with self._session.as_default():  # pylint: disable=not-context-manager
+                with self._session.graph.as_default():
+                    self._model = k_load_model(self._model_path, **self._model_kwargs)
+
+    def define_model(self, function):
+        """ Defines a given model in the correct session.
+
+        This method acts as a wrapper for :class:`keras.models.Model()` to ensure that the model
+        is defined within it's own graph.
+
+        Parameters
+        ----------
+        function: function
+            A function that defines a :class:`keras.Model` and returns it's ``inputs`` and
+            ``outputs``. The function that generates these results should be passed in, NOT the
+            results themselves, as the function needs to be executed within the correct context.
+        """
+        if self._session is None:
+            self._model = Model(*function())
+        else:
+            with self._session.as_default():  # pylint: disable=not-context-manager
+                with self._session.graph.as_default():
+                    self._model = Model(*function())
+
+    def load_model_weights(self):
+        """ Load model weights for a defined model inside the correct session.
+
+        This method is a wrapper for :class:`keras.load_weights()`. Once a model has been defined
+        in :func:`define_model()` this method can be called to load its weights in the correct
+        graph from the :attr:`model_path` defined during initialization of this class.
+        """
+        logger.verbose("Initializing plugin model: %s", self._name)
+        if self._session is None:
+            self._model.load_weights(self._model_path)
+        else:
+            with self._session.as_default():  # pylint: disable=not-context-manager
+                with self._session.graph.as_default():
+                    self._model.load_weights(self._model_path)
diff --git a/lib/multithreading.py b/lib/multithreading.py
index abbbda5..62a0251 100644
--- a/lib/multithreading.py
+++ b/lib/multithreading.py
@@ -2,128 +2,18 @@
 """ Multithreading/processing utils for faceswap """
 
 import logging
-import multiprocessing as mp
-from multiprocessing.sharedctypes import RawArray
-from ctypes import c_float
+from multiprocessing import cpu_count
 
 import queue as Queue
 import sys
-import os
 import threading
-from lib.logger import LOG_QUEUE, set_root_logger
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
-_launched_processes = set()  # pylint: disable=invalid-name
 
 
 def total_cpus():
     """ Return total number of cpus """
-    return mp.cpu_count()
-
-
-class PoolProcess():
-    """ Pool multiple processes """
-    def __init__(self, method, in_queue, out_queue, *args, processes=None, **kwargs):
-        self._name = method.__qualname__
-        logger.debug("Initializing %s: (target: '%s', processes: %s)",
-                     self.__class__.__name__, self._name, processes)
-
-        self.procs = self.set_procs(processes)
-        ctx = mp.get_context("spawn")
-        self.pool = ctx.Pool(processes=self.procs,
-                             initializer=set_root_logger,
-                             initargs=(logger.getEffectiveLevel(), LOG_QUEUE))
-        self._method = method
-        self._kwargs = self.build_target_kwargs(in_queue, out_queue, kwargs)
-        self._args = args
-
-        logger.debug("Initialized %s: '%s'", self.__class__.__name__, self._name)
-
-    @staticmethod
-    def build_target_kwargs(in_queue, out_queue, kwargs):
-        """ Add standard kwargs to passed in kwargs list """
-        kwargs["in_queue"] = in_queue
-        kwargs["out_queue"] = out_queue
-        return kwargs
-
-    def set_procs(self, processes):
-        """ Set the number of processes to use """
-        processes = mp.cpu_count() if processes is None else processes
-        running_processes = len(mp.active_children())
-        avail_processes = max(mp.cpu_count() - running_processes, 1)
-        processes = min(avail_processes, processes)
-        logger.verbose("Processing '%s' in %s processes", self._name, processes)
-        return processes
-
-    def start(self):
-        """ Run the processing pool """
-        logging.debug("Pooling Processes: (target: '%s', args: %s, kwargs: %s)",
-                      self._name, self._args, self._kwargs)
-        for idx in range(self.procs):
-            logger.debug("Adding process %s of %s to mp.Pool '%s'",
-                         idx + 1, self.procs, self._name)
-            self.pool.apply_async(self._method, args=self._args, kwds=self._kwargs)
-            _launched_processes.add(self.pool)
-        logging.debug("Pooled Processes: '%s'", self._name)
-
-    def join(self):
-        """ Join the process """
-        logger.debug("Joining Pooled Process: '%s'", self._name)
-        self.pool.close()
-        self.pool.join()
-        _launched_processes.remove(self.pool)
-        logger.debug("Joined Pooled Process: '%s'", self._name)
-
-
-class SpawnProcess(mp.context.SpawnProcess):
-    """ Process in spawnable context
-        Must be spawnable to share CUDA across processes """
-    def __init__(self, target, in_queue, out_queue, *args, **kwargs):
-        name = target.__qualname__
-        logger.debug("Initializing %s: (target: '%s', args: %s, kwargs: %s)",
-                     self.__class__.__name__, name, args, kwargs)
-        ctx = mp.get_context("spawn")
-        self.event = ctx.Event()
-        self.error = ctx.Event()
-        kwargs = self.build_target_kwargs(in_queue, out_queue, kwargs)
-        super().__init__(target=target, name=name, args=args, kwargs=kwargs)
-        self.daemon = True
-        logger.debug("Initialized %s: '%s'", self.__class__.__name__, name)
-
-    def build_target_kwargs(self, in_queue, out_queue, kwargs):
-        """ Add standard kwargs to passed in kwargs list """
-        kwargs["event"] = self.event
-        kwargs["error"] = self.error
-        kwargs["log_init"] = set_root_logger
-        kwargs["log_queue"] = LOG_QUEUE
-        kwargs["log_level"] = logger.getEffectiveLevel()
-        kwargs["in_queue"] = in_queue
-        kwargs["out_queue"] = out_queue
-        return kwargs
-
-    def run(self):
-        """ Add logger to spawned process """
-        logger_init = self._kwargs["log_init"]
-        log_queue = self._kwargs["log_queue"]
-        log_level = self._kwargs["log_level"]
-        logger_init(log_level, log_queue)
-        super().run()
-
-    def start(self):
-        """ Add logging to start function """
-        logger.debug("Spawning Process: (name: '%s', args: %s, kwargs: %s, daemon: %s)",
-                     self._name, self._args, self._kwargs, self.daemon)
-        super().start()
-        _launched_processes.add(self)
-        logger.debug("Spawned Process: (name: '%s', PID: %s)", self._name, self.pid)
-
-    def join(self, timeout=None):
-        """ Add logging to join function """
-        logger.debug("Joining Process: (name: '%s', PID: %s)", self._name, self.pid)
-        super().join(timeout=timeout)
-        if self in _launched_processes:
-            _launched_processes.remove(self)
-        logger.debug("Joined Process: (name: '%s', PID: %s)", self._name, self.pid)
+    return cpu_count()
 
 
 class FSThread(threading.Thread):
@@ -180,6 +70,11 @@ class MultiThread():
         """ Return a list of thread errors """
         return [thread.err for thread in self._threads if thread.err]
 
+    @property
+    def name(self):
+        """ Return thread name """
+        return self._name
+
     def check_and_raise_error(self):
         """ Checks for errors in thread and raises them in caller """
         if not self.has_error:
@@ -223,6 +118,7 @@ class BackgroundGenerator(MultiThread):
     # See below why prefetch count is flawed
     def __init__(self, generator, prefetch=1, thread_count=2,
                  queue=None, args=None, kwargs=None):
+        # pylint:disable=too-many-arguments
         super().__init__(target=self._run, thread_count=thread_count)
         self.queue = queue or Queue.Queue(prefetch)
         self.generator = generator
@@ -252,22 +148,3 @@ class BackgroundGenerator(MultiThread):
                 logger.debug("Got EOF OR NONE in BackgroundGenerator")
                 break
             yield next_item
-
-
-def terminate_processes():
-    """ Join all active processes on unexpected shutdown
-
-        If the process is doing long running work, make sure you
-        have a mechanism in place to terminate this work to avoid
-        long blocks
-    """
-
-    logger.debug("Processes to join: %s", [process
-                                           for process in _launched_processes
-                                           if isinstance(process, mp.pool.Pool)
-                                           or process.is_alive()])
-    for process in list(_launched_processes):
-        if isinstance(process, mp.pool.Pool):
-            process.terminate()
-        if isinstance(process, mp.pool.Pool) or process.is_alive():
-            process.join()
diff --git a/lib/queue_manager.py b/lib/queue_manager.py
index 9baaab2..47b842e 100644
--- a/lib/queue_manager.py
+++ b/lib/queue_manager.py
@@ -5,8 +5,6 @@
     a multiprocess on a Windows System it will break Faceswap"""
 
 import logging
-import multiprocessing as mp
-import sys
 import threading
 
 from queue import Queue, Empty as QueueEmpty  # pylint: disable=unused-import; # noqa
@@ -22,22 +20,11 @@ class QueueManager():
     def __init__(self):
         logger.debug("Initializing %s", self.__class__.__name__)
 
-        # Hacky fix to stop multiprocessing spawning managers in child processes
-        if mp.current_process().name == "MainProcess":
-            # Use a Multiprocessing manager in main process
-            self.manager = mp.Manager()
-        else:
-            # Use a standard mp.queue in child process. NB: This will never be used
-            # but spawned processes will load this module, so we need to dummy in a queue
-            self.manager = mp
-        self.shutdown = self.manager.Event()
+        self.shutdown = threading.Event()
         self.queues = dict()
-        # Despite launching a subprocess, the scripts still want to access the same logging
-        # queue as the GUI, so make sure the GUI gets it's own queue
-        self._log_queue = self.manager.Queue() if "gui" not in sys.argv else mp.Queue()
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def add_queue(self, name, maxsize=0, multiprocessing_queue=True):
+    def add_queue(self, name, maxsize=0):
         """ Add a queue to the manager
 
             Adds an event "shutdown" to the queue that can be used to indicate
@@ -47,10 +34,7 @@ class QueueManager():
         if name in self.queues.keys():
             raise ValueError("Queue '{}' already exists.".format(name))
 
-        if multiprocessing_queue:
-            queue = self.manager.Queue(maxsize=maxsize)
-        else:
-            queue = Queue(maxsize=maxsize)
+        queue = Queue(maxsize=maxsize)
 
         setattr(queue, "shutdown", self.shutdown)
         self.queues[name] = queue
@@ -62,13 +46,13 @@ class QueueManager():
         del self.queues[name]
         logger.debug("QueueManager deleted: '%s'", name)
 
-    def get_queue(self, name, maxsize=0, multiprocessing_queue=True):
+    def get_queue(self, name, maxsize=0):
         """ Return a queue from the manager
             If it doesn't exist, create it """
         logger.debug("QueueManager getting: '%s'", name)
         queue = self.queues.get(name, None)
         if not queue:
-            self.add_queue(name, maxsize, multiprocessing_queue)
+            self.add_queue(name, maxsize)
             queue = self.queues[name]
         logger.debug("QueueManager got: '%s'", name)
         return queue
@@ -109,6 +93,7 @@ class QueueManager():
             logged to INFO so it also displays in console
         """
         while True:
+            logger.info("====================================================")
             for name in sorted(self.queues.keys()):
                 logger.info("%s: %s", name, self.queues[name].qsize())
             sleep(update_secs)
diff --git a/lib/utils.py b/lib/utils.py
index 49143af..f0340f9 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -49,6 +49,9 @@ class Backend():
 
     def get_backend(self):
         """ Return the backend from config/.faceswap """
+        # Intercept for sphinx docs build
+        if sys.argv[0].endswith("sphinx-build"):
+            return "nvidia"
         if not os.path.isfile(self.config_file):
             self.configure_backend()
         while True:
@@ -349,18 +352,18 @@ def rotate_landmarks(face, rotation_matrix):
     # pylint:disable=c-extension-no-member
     """ Rotate the landmarks and bounding box for faces
         found in rotated images.
-        Pass in a DetectedFace object, Alignments dict or bounding box dict
-        (as defined in lib/plugins/extract/detect/_base.py) """
+        Pass in a DetectedFace object or Alignments dict """
     logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
     logger.trace("Rotating landmarks: (rotation_matrix: %s, type(face): %s",
                  rotation_matrix, type(face))
+    rotated_landmarks = None
     # Detected Face Object
     if isinstance(face, DetectedFace):
         bounding_box = [[face.x, face.y],
                         [face.x + face.w, face.y],
                         [face.x + face.w, face.y + face.h],
                         [face.x, face.y + face.h]]
-        landmarks = face.landmarksXY
+        landmarks = face.landmarks_xy
 
     # Alignments Dict
     elif isinstance(face, dict) and "x" in face:
@@ -371,15 +374,7 @@ def rotate_landmarks(face, rotation_matrix):
                          face.get("y", 0) + face.get("h", 0)],
                         [face.get("x", 0),
                          face.get("y", 0) + face.get("h", 0)]]
-        landmarks = face.get("landmarksXY", list())
-
-    # Bounding Box Dict
-    elif isinstance(face, dict) and "left" in face:
-        bounding_box = [[face["left"], face["top"]],
-                        [face["right"], face["top"]],
-                        [face["right"], face["bottom"]],
-                        [face["left"], face["bottom"]]]
-        landmarks = list()
+        landmarks = face.get("landmarks_xy", list())
 
     else:
         raise ValueError("Unsupported face type")
@@ -415,16 +410,7 @@ def rotate_landmarks(face, rotation_matrix):
         face.r = 0
         if len(rotated) > 1:
             rotated_landmarks = [tuple(point) for point in rotated[1].tolist()]
-            face.landmarksXY = rotated_landmarks
-    elif isinstance(face, dict) and "x" in face:
-        face["x"] = int(pt_x)
-        face["y"] = int(pt_y)
-        face["w"] = int(width)
-        face["h"] = int(height)
-        face["r"] = 0
-        if len(rotated) > 1:
-            rotated_landmarks = [tuple(point) for point in rotated[1].tolist()]
-            face["landmarksXY"] = rotated_landmarks
+            face.landmarks_xy = rotated_landmarks
     else:
         face["left"] = int(pt_x)
         face["top"] = int(pt_y)
@@ -450,14 +436,8 @@ def safe_shutdown(got_error=False):
     logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
     logger.debug("Safely shutting down")
     from lib.queue_manager import queue_manager
-    from lib.multithreading import terminate_processes
     queue_manager.terminate_queues()
-    terminate_processes()
     logger.debug("Cleanup complete. Shutting down queue manager and exiting")
-    queue_manager._log_queue.put(None)  # pylint:disable=protected-access
-    while not queue_manager._log_queue.empty():  # pylint:disable=protected-access
-        continue
-    queue_manager.manager.shutdown()
     exit(1 if got_error else 0)
 
 
diff --git a/plugins/extract/_base.py b/plugins/extract/_base.py
new file mode 100644
index 0000000..8274801
--- /dev/null
+++ b/plugins/extract/_base.py
@@ -0,0 +1,436 @@
+#!/usr/bin/env python3
+""" Base class for Faceswap :mod:`~plugins.extract.detect` and :mod:`~plugins.extract.align`
+Plugins
+"""
+import logging
+import os
+import sys
+
+import cv2
+import numpy as np
+
+from lib.multithreading import MultiThread
+from lib.queue_manager import queue_manager
+from lib.utils import GetModel
+from ._config import Config
+
+logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+
+# TODO Cpu mode
+# TODO Run with warnings mode
+
+
+def _get_config(plugin_name, configfile=None):
+    """ Return the config for the requested model
+
+    Parameters
+    ----------
+    plugin_name: str
+        The module name of the child plugin.
+    configfile: str, optional
+        Path to a :file:`./config/<plugin_type>.ini` file for this plugin. Default: use system
+        config.
+
+    Returns
+    -------
+    config_dict, dict
+       A dictionary of configuration items from the config file
+    """
+    return Config(plugin_name, configfile=configfile).config_dict
+
+
+class Extractor():
+    """ Extractor Plugin Object
+
+    All ``_base`` classes for Aligners and Detectors inherit from this class.
+
+    This class sets up a pipeline for working with ML plugins.
+
+    Plugins are split into 3 threads, to utilize Numpy and CV2s parallel processing, as well as
+    allow the predict function of the model to sit in a dedicated thread.
+    A plugin is expected to have 3 core functions, each in their own thread:
+    - :func:`process_input()` - Prepare the data for feeding into a model
+    - :func:`predict` - Feed the data through the model
+    - :func:`process_output()` - Perform any data post-processing
+
+    Parameters
+    ----------
+    git_model_id: int
+        The second digit in the github tag that identifies this model. See
+        https://github.com/deepfakes-models/faceswap-models for more information
+    model_filename: str
+        The name of the model file to be loaded
+
+    Other Parameters
+    ----------------
+    configfile: str, optional
+        Path to a custom configuration ``ini`` file. Default: Use system configfile
+
+
+    The following attributes should be set in the plugin's :func:`__init__` method after
+    initializing the parent.
+
+    Attributes
+    ----------
+    name: str
+        Name of this plugin. Used for display purposes.
+    input_size: int
+        The input size to the model in pixels across one edge. The input size should always be
+        square.
+    colorformat: str
+        Color format for model. Must be ``'BGR'``, ``'RGB'`` or ``'GRAY'``. Defaults to ``'BGR'``
+        if not explicitly set.
+    vram: int
+        Approximate VRAM used by the model at :attr:`input_size`. Used to calculate the
+        :attr:`batchsize`. Be conservative to avoid OOM.
+    vram_warnings: int
+        Approximate VRAM used by the model at :attr:`input_size` that will still run, but generates
+        warnings. Used to calculate the :attr:`batchsize`. Be conservative to avoid OOM.
+    vram_per_batch: int
+        Approximate additional VRAM used by the model for each additional batch. Used to calculate
+        the :attr:`batchsize`. Be conservative to avoid OOM.
+
+    See Also
+    --------
+    plugins.extract.detect._base : Detector parent class for extraction plugins.
+    plugins.extract.align._base : Aligner parent class for extraction plugins.
+    plugins.extract.pipeline : The extract pipeline that configures and calls all plugins
+
+    """
+    def __init__(self, git_model_id=None, model_filename=None, configfile=None):
+        logger.debug("Initializing %s: (git_model_id: %s, model_filename: %s, "
+                     " configfile: %s)", self.__class__.__name__, git_model_id,
+                     model_filename, configfile)
+
+        self.config = _get_config(".".join(self.__module__.split(".")[-2:]), configfile=configfile)
+        """ dict: Config for this plugin, loaded from ``extract.ini`` configfile """
+
+        self.model_path = self._get_model(git_model_id, model_filename)
+        """ str or list: Path to the model file(s) (if required). Multiple model files should
+        be a list of strings """
+
+        # << SET THE FOLLOWING IN PLUGINS __init__ IF DIFFERENT FROM DEFAULT >> #
+        self.name = None
+        self.input_size = None
+        self.colorformat = "BGR"
+        self.vram = None
+        self.vram_warnings = None  # Will run at this with warnings
+        self.vram_per_batch = None
+
+        # << THE FOLLOWING ARE SET IN self.initialize METHOD >> #
+        self.queue_size = 32
+        """ int: Queue size for all internal queues. Set in :func:`initialize()` """
+
+        self.model = None
+        """varies: The model for this plugin.
+        Set in the plugin's :func:`init_model()` method """
+
+        # For detectors that support batching, this should be set to  the calculated batch size
+        # that the amount of available VRAM will support.
+        self.batchsize = 1
+        """ int: Batchsize for feeding this model. The number of images the model should
+        feed through at once. """
+
+        self._queues = dict()
+        """ dict: in + out queues and internal queues for this plugin, """
+
+        self._threads = []
+        """ list: Internal threads for this plugin """
+
+        # << THE FOLLOWING PROTECTED ATTRIBUTES ARE SET IN PLUGIN TYPE _base.py >>> #
+        self._plugin_type = None
+        """ str: Plugin type. ``detect`` or ``align``
+        set in ``<plugin_type>._base`` """
+
+        logger.debug("Initialized _base %s", self.__class__.__name__)
+
+    # <<< OVERIDABLE METHODS >>> #
+    def init_model(self):
+        """ **Override method**
+
+        Override this method to execute the specific model initialization method """
+        raise NotImplementedError
+
+    def process_input(self, batch):
+        """ **Override method**
+
+        Override this method for specific extractor pre-processing of image
+
+        Parameters
+        ----------
+        batch : dict
+            Contains the batch that is currently being passed through the plugin process
+
+        Notes
+        -----
+        When preparing an input to the model a key ``feed`` must be added
+        to the :attr:`batch` ``dict`` which contains this input.
+        """
+        raise NotImplementedError
+
+    def predict(self, batch):
+        """ **Override method**
+
+        Override this method for specific extractor model prediction function
+
+        Parameters
+        ----------
+        batch : dict
+            Contains the batch that is currently being passed through the plugin process
+
+        Notes
+        -----
+        Input for :func:`predict` should have been set in :func:`process_input` with the addition
+        of a ``feed`` key to the :attr:`batch` ``dict``.
+
+        Output from the model should add the key ``prediction`` to the :attr:`batch` ``dict``.
+
+        For Detect:
+            the expected output for the ``prediction`` key of the :attr:`batch` dict should be a
+            ``list`` of :attr:`batchsize` of detected face points. These points should be either
+            a ``list``, ``tuple`` or ``numpy.array`` with the first 4 items being the `left`,
+            `top`, `right`, `bottom` points, in that order
+        """
+        raise NotImplementedError
+
+    def process_output(self, batch):
+        """ **Override method**
+
+        Override this method for specific extractor model post predict function
+
+        Parameters
+        ----------
+        batch : dict
+            Contains the batch that is currently being passed through the plugin process
+
+        Notes
+        -----
+        For Align:
+            The key ``landmarks`` must be returned in the :attr:`batch` ``dict`` from this method.
+            This should be a ``list`` or ``numpy.array`` of :attr:`batchsize` containing a
+            ``list``, ``tuple`` or ``numpy.array`` of `(x, y)` co-ords of the 68 point landmarks
+            as calculated from the :attr:`model`.
+        """
+        raise NotImplementedError
+
+    def _predict(self, batch):
+        """ **Override method** (at `<plugin_type>` level)
+
+        This method is overridable at the `<plugin_type>` level (ie.
+        ``plugins.extract.detect._base`` or ``plugins.extract.align._base``) and should not
+        be overriden within plugins themselves.
+
+        It acts as a wrapper for the plugin's ``self.predict`` method and handles any
+        predict processing that is consistent for all plugins within the `plugin_type`
+
+        Parameters
+        ----------
+        batch : dict
+            Contains the batch that is currently being passed through the plugin process
+        """
+        raise NotImplementedError
+
+    def finalize(self, batch):
+        """ **Override method** (at `<plugin_type>` level)
+
+        This method is overridable at the `<plugin_type>` level (ie.
+        :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
+        be overriden within plugins themselves.
+
+        Handles consistent finalization for all plugins that exist within that plugin type. Its
+        input is always the output from :func:`process_output()`
+
+        Parameters
+        ----------
+        batch : dict
+            Contains the batch that is currently being passed through the plugin process
+
+        """
+
+    def get_batch(self, queue):
+        """ **Override method** (at `<plugin_type>` level)
+
+        This method is overridable at the `<plugin_type>` level (ie.
+        :mod:`plugins.extract.detect._base` or :mod:`plugins.extract.align._base`) and should not
+        be overriden within plugins themselves.
+
+        Get items from the queue in batches of :attr:`batchsize`
+
+        Parameters
+        ----------
+        queue : queue.Queue()
+            The ``queue`` that the batch will be fed from. This will be the input to the plugin.
+        """
+        raise NotImplementedError
+
+    # <<< THREADING METHODS >>> #
+    def start(self):
+        """ Start all threads
+
+        Exposed for :mod:`~plugins.extract.pipeline` to start plugin's threads
+        """
+        for thread in self._threads:
+            thread.start()
+
+    def join(self):
+        """ Join all threads
+
+        Exposed for :mod:`~plugins.extract.pipeline` to join plugin's threads
+        """
+        for thread in self._threads:
+            thread.join()
+            del thread
+
+    def check_and_raise_error(self):
+        """ Check all threads for errors
+
+        Exposed for :mod:`~plugins.extract.pipeline` to check plugin's threads for errors
+        """
+        for thread in self._threads:
+            err = thread.check_and_raise_error()
+            if err is not None:
+                logger.debug("thread_error_detected")
+                return True
+        return False
+
+    # <<< PROTECTED ACCESS METHODS >>> #
+    # <<< INIT METHODS >>> #
+    def _get_model(self, git_model_id, model_filename):
+        """ Check if model is available, if not, download and unzip it """
+        if model_filename is None:
+            logger.debug("No model_filename specified. Returning None")
+            return None
+        if git_model_id is None:
+            logger.debug("No git_model_id specified. Returning None")
+            return None
+        plugin_path = os.path.join(*self.__module__.split(".")[:-1])
+        if os.path.basename(plugin_path) in ("detect", "align"):
+            base_path = os.path.dirname(os.path.realpath(sys.argv[0]))
+            cache_path = os.path.join(base_path, plugin_path, ".cache")
+        else:
+            cache_path = os.path.join(os.path.dirname(__file__), ".cache")
+        model = GetModel(model_filename, cache_path, git_model_id)
+        return model.model_path
+
+    # <<< PLUGIN INITIALIZATION >>> #
+    def initialize(self, *args, **kwargs):
+        """ Inititalize the extractor plugin
+
+            Should be called from :mod:`~plugins.extract.pipeline`
+        """
+        logger.debug("initialize %s: (args: %s, kwargs: %s)",
+                     self.__class__.__name__, args, kwargs)
+        p_type = "Detector" if self._plugin_type == "detect" else "Aligner"
+        logger.info("Initializing %s %s...", self.name, p_type)
+        self.queue_size = kwargs["queue_size"]
+        self._add_queues(kwargs["in_queue"], kwargs["out_queue"], ["predict", "post"])
+        self._compile_threads()
+        self.init_model()
+        logger.info("Initialized %s %s with batchsize of %s", self.name, p_type, self.batchsize)
+
+    def _add_queues(self, in_queue, out_queue, queues):
+        """ Add the queues
+            in_queue and out_queue should be pre-created queue manager queues
+            queues should be a list of queue names """
+        self._queues["in"] = in_queue
+        self._queues["out"] = out_queue
+        for q_name in queues:
+            self._queues[q_name] = queue_manager.get_queue(
+                name="{}_{}".format(self._plugin_type, q_name),
+                maxsize=self.queue_size)
+
+    # <<< THREAD METHODS >>> #
+    def _compile_threads(self):
+        """ Compile the threads into self._threads list """
+        logger.debug("Compiling %s threads", self._plugin_type)
+        self._add_thread("{}_input".format(self._plugin_type),
+                         self.process_input,
+                         self._queues["in"],
+                         self._queues["predict"])
+        self._add_thread("{}_predict".format(self._plugin_type),
+                         self._predict,
+                         self._queues["predict"],
+                         self._queues["post"])
+        self._add_thread("{}_output".format(self._plugin_type),
+                         self.process_output,
+                         self._queues["post"],
+                         self._queues["out"])
+        logger.debug("Compiled %s threads: %s", self._plugin_type, self._threads)
+
+    def _add_thread(self, name, function, in_queue, out_queue):
+        """ Add a MultiThread thread to self._threads """
+        logger.debug("Adding thread: (name: %s, function: %s, in_queue: %s, out_queue: %s)",
+                     name, function, in_queue, out_queue)
+        self._threads.append(MultiThread(target=self._thread_process,
+                                         name=name,
+                                         function=function,
+                                         in_queue=in_queue,
+                                         out_queue=out_queue))
+        logger.debug("Added thread: %s", name)
+
+    def _thread_process(self, function, in_queue, out_queue):
+        """ Perform a plugin function in a thread """
+        func_name = function.__name__
+        logger.debug("threading: (function: '%s')", func_name)
+        while True:
+            if func_name == "process_input":
+                # Process input items to batches
+                exhausted, batch = self.get_batch(in_queue)
+                if exhausted:
+                    if batch:
+                        # Put the final batch
+                        batch = function(batch)
+                        out_queue.put(batch)
+                    break
+            else:
+                batch = self._get_item(in_queue)
+                if batch == "EOF":
+                    break
+            batch = function(batch)
+            if func_name == "process_output":
+                # Process output items to individual items from batch
+                for item in self.finalize(batch):
+                    out_queue.put(item)
+            else:
+                out_queue.put(batch)
+        logger.debug("Putting EOF")
+        out_queue.put("EOF")
+
+    # <<< QUEUE METHODS >>> #
+    @staticmethod
+    def _get_item(queue):
+        """ Yield one item from a queue """
+        item = queue.get()
+        if isinstance(item, dict):
+            logger.trace("item: %s, queue: %s",
+                         {k: v.shape if isinstance(v, np.ndarray) else v
+                          for k, v in item.items()},
+                         queue)
+        else:
+            logger.trace("item: %s, queue: %s", item, queue)
+        return item
+
+    # <<< MISC UTILITY METHODS >>> #
+    def _convert_color(self, image):
+        """ Convert the image to the correct color format """
+        logger.trace("Converting image to color format: %s", self.colorformat)
+        if self.colorformat == "RGB":
+            cvt_image = image[:, :, ::-1].copy()
+        elif self.colorformat == "GRAY":
+            cvt_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)  # pylint:disable=no-member
+        else:
+            cvt_image = image.copy()
+        return cvt_image
+
+    @staticmethod
+    def _dict_lists_to_list_dicts(dictionary):
+        """ Convert a dictionary of lists to a list of dictionaries """
+        return [dict(zip(dictionary, val)) for val in zip(*dictionary.values())]
+
+    @staticmethod
+    def _remove_invalid_keys(dictionary, valid_keys):
+        """ Remove items from dict that are no longer required """
+        for key in list(dictionary.keys()):
+            if key not in valid_keys:
+                logger.trace("Removing from output: '%s'", key)
+                del dictionary[key]
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 3142cf5..c552741 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -1,189 +1,231 @@
 #!/usr/bin/env python3
 """ Base class for Face Aligner plugins
-    Plugins should inherit from this class
 
-    See the override methods for which methods are
-    required.
+All Aligner Plugins should inherit from this class.
+See the override methods for which methods are required.
 
-    The plugin will receive a dict containing:
-    {"filename": <filename of source frame>,
-     "image": <source image>,
-     "detected_faces": <list of bounding box dicts as defined in lib/plugins/extract/detect/_base>}
+The plugin will receive a dict containing:
 
-    For each source item, the plugin must pass a dict to finalize containing:
-    {"filename": <filename of source frame>,
-     "image": <source image>,
-     "detected_faces": <list of bounding box dicts as defined in lib/plugins/extract/detect/_base>,
-     "landmarks": <list of landmarks>}
-    """
+>>> {"filename": [<filename of source frame>],
+>>>  "image": [<source image>],
+>>>  "detected_faces": [<list of DetectedFace objects]}
+
+For each source item, the plugin must pass a dict to finalize containing:
 
-import logging
-import os
-import traceback
+>>> {"filename": [<filename of source frame>],
+>>>  "image": [<source image>],
+>>>  "landmarks": [list of 68 point face landmarks]
+>>>  "detected_faces": [<list of DetectedFace objects>]}
+"""
 
-from io import StringIO
 
 import cv2
+import numpy as np
 
-from lib.aligner import Extract
-from lib.gpu_stats import GPUStats
-from lib.utils import GetModel
+from plugins.extract._base import Extractor, logger
 
-logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
 
+class Aligner(Extractor):
+    """ Aligner plugin _base Object
 
-class Aligner():
-    """ Landmarks Aligner Object """
-    def __init__(self, loglevel, configfile=None, normalize_method=None,
-                 git_model_id=None, model_filename=None, colorspace="BGR", input_size=256):
-        logger.debug("Initializing %s: (loglevel: %s, configfile: %s, normalize_method: %s, "
-                     "git_model_id: %s, model_filename: '%s', colorspace: '%s'. input_size: %s)",
-                     self.__class__.__name__, loglevel, configfile, normalize_method, git_model_id,
-                     model_filename, colorspace, input_size)
-        self.loglevel = loglevel
-        self.normalize_method = normalize_method
-        self.colorspace = colorspace.upper()
-        self.input_size = input_size
-        self.extract = Extract()
-        self.init = None
-        self.error = None
+    All Aligner plugins must inherit from this class
+
+    Parameters
+    ----------
+    git_model_id: int
+        The second digit in the github tag that identifies this model. See
+        https://github.com/deepfakes-models/faceswap-models for more information
+    model_filename: str
+        The name of the model file to be loaded
+    normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional
+        Normalize the images fed to the aligner. Default: ``None``
 
-        # The input and output queues for the plugin.
-        # See lib.queue_manager.QueueManager for getting queues
-        self.queues = {"in": None, "out": None}
+    Other Parameters
+    ----------------
+    configfile: str, optional
+        Path to a custom configuration ``ini`` file. Default: Use system configfile
 
-        #  Get model if required
-        self.model_path = self.get_model(git_model_id, model_filename)
+    See Also
+    --------
+    plugins.extract.align : Aligner plugins
+    plugins.extract._base : Parent class for all extraction plugins
+    plugins.extract.detect._base : Detector parent class for extraction plugins.
 
-        # Approximate VRAM required for aligner. Used to calculate
-        # how many parallel processes / batches can be run.
-        # Be conservative to avoid OOM.
-        self.vram = None
+    """
 
-        # Set to true if the plugin supports PlaidML
-        self.supports_plaidml = False
+    def __init__(self, git_model_id, model_filename,
+                 configfile=None, normalize_method=None):
+        logger.debug("Initializing %s: (normalize_method: %s)", self.__class__.__name__,
+                     normalize_method)
+        super().__init__(git_model_id,
+                         model_filename,
+                         configfile=configfile)
+        self.normalize_method = normalize_method
 
+        self._plugin_type = "align"
+        self._faces_per_filename = dict()  # Tracking for recompiling face batches
+        self._rollover = []  # Items that are rolled over from the previous batch in get_batch
+        self._output_faces = []
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    # <<< OVERRIDE METHODS >>> #
-    # These methods must be overriden when creating a plugin
-    def initialize(self, *args, **kwargs):
-        """ Inititalize the aligner
-            Tasks to be run before any alignments are performed.
-            Override for specific detector """
-        logger.debug("_base initialize %s: (PID: %s, args: %s, kwargs: %s)",
-                     self.__class__.__name__, os.getpid(), args, kwargs)
-        self.init = kwargs["event"]
-        self.error = kwargs["error"]
-        self.queues["in"] = kwargs["in_queue"]
-        self.queues["out"] = kwargs["out_queue"]
-
-    def align_image(self, detected_face, image):
-        """ Align the incoming image for feeding into aligner
-            Override for aligner specific processing """
-        raise NotImplementedError
-
-    def predict_landmarks(self, feed_dict):
-        """ Predict the 68 point landmarks
-            Override for aligner specific landmark prediction """
-        raise NotImplementedError
-
-    # <<< GET MODEL >>> #
-    @staticmethod
-    def get_model(git_model_id, model_filename):
-        """ Check if model is available, if not, download and unzip it """
-        if model_filename is None:
-            logger.debug("No model_filename specified. Returning None")
-            return None
-        if git_model_id is None:
-            logger.debug("No git_model_id specified. Returning None")
-            return None
-        cache_path = os.path.join(os.path.dirname(__file__), ".cache")
-        model = GetModel(model_filename, cache_path, git_model_id)
-        return model.model_path
-
-    # <<< ALIGNMENT WRAPPER >>> #
-    def run(self, *args, **kwargs):
-        """ Parent align process.
-            This should always be called as the entry point so exceptions
-            are passed back to parent.
-            Do not override """
-        try:
-            self.align(*args, **kwargs)
-        except Exception:  # pylint:disable=broad-except
-            logger.error("Caught exception in child process: %s", os.getpid())
-            # Display traceback if in initialization stage
-            if not self.init.is_set():
-                logger.exception("Traceback:")
-            tb_buffer = StringIO()
-            traceback.print_exc(file=tb_buffer)
-            exception = {"exception": (os.getpid(), tb_buffer)}
-            self.queues["out"].put(exception)
-            exit(1)
-
-    def align(self, *args, **kwargs):
-        """ Process landmarks """
-        if not self.init:
-            self.initialize(*args, **kwargs)
-        logger.debug("Launching Align: (args: %s kwargs: %s)", args, kwargs)
-
-        for item in self.get_item():
+    # << QUEUE METHODS >>> #
+    def get_batch(self, queue):
+        """ Get items for inputting into the aligner from the queue in batches
+
+        Items are returned from the ``queue`` in batches of
+        :attr:`~plugins.extract._base.Extractor.batchsize`
+
+        To ensure consistent batchsizes for aligner the items are split into separate items for
+        each :class:`lib.faces_detect.DetectedFace` object.
+
+        Remember to put ``'EOF'`` to the out queue after processing
+        the final batch
+
+        Outputs items in the following format. All lists are of length
+        :attr:`~plugins.extract._base.Extractor.batchsize`:
+
+        >>> {'filename': [<filenames of source frames>],
+        >>>  'image': [<source images>],
+        >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
+
+        Parameters
+        ----------
+        queue : queue.Queue()
+            The ``queue`` that the plugin will be fed from.
+
+        Returns
+        -------
+        exhausted, bool
+            ``True`` if queue is exhausted, ``False`` if not
+        batch, dict
+            A dictionary of lists of :attr:`~plugins.extract._base.Extractor.batchsize`:
+        """
+        exhausted = False
+        batch = dict()
+        idx = 0
+        while idx < self.batchsize:
+            item = self._collect_item(queue)
             if item == "EOF":
-                self.finalize(item)
+                logger.trace("EOF received")
+                exhausted = True
                 break
-            image = self.convert_color(item["image"])
-
-            logger.trace("Aligning faces")
-            try:
-                item["landmarks"] = self.process_landmarks(image, item["detected_faces"])
-                logger.trace("Aligned faces: %s", item["landmarks"])
-            except ValueError as err:
-                logger.warning("Image '%s' could not be processed. This may be due to corrupted "
-                               "data: %s", item["filename"], str(err))
-                item["detected_faces"] = list()
-                item["landmarks"] = list()
-                # UNCOMMENT THIS CODE BLOCK TO PRINT TRACEBACK ERRORS
-                # import sys
-                # exc_info = sys.exc_info()
-                # traceback.print_exception(*exc_info)
-            self.finalize(item)
-        logger.debug("Completed Align")
-
-    def convert_color(self, image):
-        """ Convert the image to the correct colorspace """
-        logger.trace("Converting image to colorspace: %s", self.colorspace)
-        if self.colorspace == "RGB":
-            cvt_image = image[:, :, ::-1].copy()
-        elif self.colorspace == "GRAY":
-            cvt_image = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)  # pylint:disable=no-member
+
+            # Put frames with no faces into the out queue to keep TQDM consistent
+            if not item["detected_faces"]:
+                self._queues["out"].put(item)
+                continue
+
+            for f_idx, face in enumerate(item["detected_faces"]):
+                face.image = self._convert_color(item["image"])
+                batch.setdefault("detected_faces", []).append(face)
+                batch.setdefault("filename", []).append(item["filename"])
+                batch.setdefault("image", []).append(item["image"])
+                idx += 1
+                if idx == self.batchsize:
+                    frame_faces = len(item["detected_faces"])
+                    if f_idx + 1 != frame_faces:
+                        self._rollover = {k: v[f_idx + 1:] if k == "detected_faces" else v
+                                          for k, v in item.items()}
+                        logger.trace("Rolled over %s faces of %s to next batch for '%s'",
+                                     len(self._rollover["detected_faces"]),
+                                     frame_faces, item["filename"])
+                    break
+        if batch:
+            logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
+                                                 for k, v in batch.items()})
         else:
-            cvt_image = image.copy()
-        return cvt_image
-
-    def process_landmarks(self, image, detected_faces):
-        """ Align image and process landmarks """
-        logger.trace("Processing landmarks")
-        retval = list()
-        for detected_face in detected_faces:
-            feed_dict = self.align_image(detected_face, image)
-            self.normalize_face(feed_dict)
-            landmarks = self.predict_landmarks(feed_dict)
-            retval.append(landmarks)
-        logger.trace("Processed landmarks: %s", retval)
-        return retval
+            logger.trace(item)
+        return exhausted, batch
+
+    def _collect_item(self, queue):
+        """ Collect the item from the _rollover dict or from the queue
+            Add face count per frame to self._faces_per_filename for joining
+            batches back up in finalize """
+        if self._rollover:
+            logger.trace("Getting from _rollover: (filename: `%s`, faces: %s)",
+                         self._rollover["filename"], len(self._rollover["detected_faces"]))
+            item = self._rollover
+            self._rollover = dict()
+        else:
+            item = self._get_item(queue)
+            if item != "EOF":
+                logger.trace("Getting from queue: (filename: %s, faces: %s)",
+                             item["filename"], len(item["detected_faces"]))
+                self._faces_per_filename[item["filename"]] = len(item["detected_faces"])
+        return item
+
+    # <<< FINALIZE METHODS >>> #
+    def finalize(self, batch):
+        """ Finalize the output from Aligner
+
+        This should be called as the final task of each `plugin`.
+
+        It strips unneeded items from the :attr:`batch` ``dict`` and pairs the detected faces back
+        up with their original frame before yielding each frame.
+
+        Outputs items in the format:
+
+        >>> {'image': [<original frame>],
+        >>>  'filename': [<frame filename>),
+        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
+
+        Parameters
+        ----------
+        batch : dict
+            The final ``dict`` from the `plugin` process. It must contain the `keys`:
+            ``detected_faces``, ``landmarks``, ``filename``, ``image``
+
+        Yields
+        ------
+        dict
+            A ``dict`` for each frame containing the ``image``, ``filename`` and list of
+            :class:`lib.faces_detect.DetectedFace` objects.
+
+        """
+
+        for face, landmarks in zip(batch["detected_faces"], batch["landmarks"]):
+            face.landmarks_xy = [(int(round(pt[0])), int(round(pt[1]))) for pt in landmarks]
+
+        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
+        logger.trace("Item out: %s", {key: val
+                                      for key, val in batch.items()
+                                      if key != "image"})
+        for filename, image, face in zip(batch["filename"],
+                                         batch["image"],
+                                         batch["detected_faces"]):
+            self._output_faces.append(face)
+            if len(self._output_faces) != self._faces_per_filename[filename]:
+                continue
+            retval = dict(filename=filename, image=image, detected_faces=self._output_faces)
+
+            self._output_faces = []
+            logger.trace("Yielding: (filename: '%s', image: %s, detected_faces: %s)",
+                         retval["filename"], retval["image"].shape, len(retval["detected_faces"]))
+            yield retval
+
+    # <<< PROTECTED METHODS >>> #
+    # <<< PREDICT WRAPPER >>> #
+    def _predict(self, batch):
+        """ Just return the aligner's predict function """
+        return self.predict(batch)
 
     # <<< FACE NORMALIZATION METHODS >>> #
-    def normalize_face(self, feed_dict):
-        """ Normalize the face for feeding into model """
+    def _normalize_faces(self, faces):
+        """ Normalizes the face for feeding into model
+
+        The normalization method is dictated by the cli argument:
+            -nh (--normalization)
+        """
         if self.normalize_method is None:
-            return
-        logger.trace("Normalizing face")
-        meth = getattr(self, "normalize_{}".format(self.normalize_method.lower()))
-        feed_dict["image"] = meth(feed_dict["image"])
-        logger.trace("Normalized face")
+            return faces
+        logger.trace("Normalizing faces")
+        meth = getattr(self, "_normalize_{}".format(self.normalize_method.lower()))
+        faces = [meth(face) for face in faces]
+        logger.trace("Normalized faces")
+        return faces
 
     @staticmethod
-    def normalize_mean(face):
+    def _normalize_mean(face):
         """ Normalize Face to the Mean """
         face = face / 255.0
         for chan in range(3):
@@ -193,59 +235,17 @@ class Aligner():
         return face * 255.0
 
     @staticmethod
-    def normalize_hist(face):
+    def _normalize_hist(face):
         """ Equalize the RGB histogram channels """
         for chan in range(3):
             face[:, :, chan] = cv2.equalizeHist(face[:, :, chan])  # pylint: disable=no-member
         return face
 
     @staticmethod
-    def normalize_clahe(face):
+    def _normalize_clahe(face):
         """ Perform Contrast Limited Adaptive Histogram Equalization """
         clahe = cv2.createCLAHE(clipLimit=2.0,  # pylint: disable=no-member
                                 tileGridSize=(4, 4))
         for chan in range(3):
             face[:, :, chan] = clahe.apply(face[:, :, chan])
         return face
-
-    # <<< FINALIZE METHODS >>> #
-    def finalize(self, output):
-        """ This should be called as the final task of each plugin
-            aligns faces and puts to the out queue """
-        if output == "EOF":
-            logger.trace("Item out: %s", output)
-            self.queues["out"].put("EOF")
-            return
-        logger.trace("Item out: %s", {key: val
-                                      for key, val in output.items()
-                                      if key != "image"})
-        self.queues["out"].put((output))
-
-    # <<< MISC METHODS >>> #
-    def get_vram_free(self):
-        """ Return free and total VRAM on card with most VRAM free"""
-        stats = GPUStats()
-        vram = stats.get_card_most_free(supports_plaidml=self.supports_plaidml)
-        logger.verbose("Using device %s with %sMB free of %sMB",
-                       vram["device"],
-                       int(vram["free"]),
-                       int(vram["total"]))
-        return int(vram["card_id"]), int(vram["free"]), int(vram["total"])
-
-    def get_item(self):
-        """ Yield one item from the queue """
-        while True:
-            item = self.queues["in"].get()
-            if isinstance(item, dict):
-                logger.trace("Item in: %s", {key: val
-                                             for key, val in item.items()
-                                             if key != "image"})
-                # Pass Detector failures straight out and quit
-                if item.get("exception", None):
-                    self.queues["out"].put(item)
-                    exit(1)
-            else:
-                logger.trace("Item in: %s", item)
-            yield item
-            if item == "EOF":
-                break
diff --git a/plugins/extract/align/cv2_dnn.py b/plugins/extract/align/cv2_dnn.py
index ecf3dcf..919e35e 100644
--- a/plugins/extract/align/cv2_dnn.py
+++ b/plugins/extract/align/cv2_dnn.py
@@ -35,60 +35,57 @@ class Align(Aligner):
     def __init__(self, **kwargs):
         git_model_id = 1
         model_filename = "cnn-facial-landmark_v1.pb"
-        super().__init__(git_model_id=git_model_id,
-                         model_filename=model_filename,
-                         colorspace="RGB",
-                         input_size=128,
-                         **kwargs)
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+
+        self.name = "cv2-DNN Aligner"
+        self.input_size = 128
+        self.colorformat = "RGB"
         self.vram = 0  # Doesn't use GPU
-        self.model = None
-
-    def initialize(self, *args, **kwargs):
-        """ Initialization tasks to run prior to alignments """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing cv2 DNN Aligner...")
-            logger.debug("cv2 DNN initialize: (args: %s kwargs: %s)", args, kwargs)
-            logger.verbose("Using CPU for alignment")
-
-            self.model = cv2.dnn.readNetFromTensorflow(  # pylint: disable=no-member
-                self.model_path)
-            self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)  # pylint: disable=no-member
-            self.init.set()
-            logger.info("Initialized cv2 DNN Aligner.")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    def align_image(self, detected_face, image):
+        self.batchsize = 1
+
+    def init_model(self):
+        """ Initialize CV2 DNN Detector Model"""
+        self.model = cv2.dnn.readNetFromTensorflow(self.model_path)  # pylint: disable=no-member
+        self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)  # pylint: disable=no-member
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        faces, batch["roi"] = self.align_image(batch["detected_faces"])
+        faces = self._normalize_faces(faces)
+        batch["feed"] = np.array(faces, dtype="float32").transpose((0, 3, 1, 2))
+        return batch
+
+    def align_image(self, detected_faces):
         """ Align the incoming image for prediction """
         logger.trace("Aligning image around center")
-
-        box = (detected_face["left"],
-               detected_face["top"],
-               detected_face["right"],
-               detected_face["bottom"])
-        height = detected_face["bottom"] - detected_face["top"]
-        width = detected_face["right"] - detected_face["left"]
-        diff_height_width = height - width
-        offset_y = int(abs(diff_height_width / 2))
-        box_moved = self.move_box(box, [0, offset_y])
-
-        # Make box square.
-        roi = self.get_square_box(box_moved)
-        # Pad the image if face is outside of boundaries
-        image = self.pad_image(roi, image)
-        face = image[roi[1]: roi[3], roi[0]: roi[2]]
-
-        if face.shape[0] < self.input_size:
-            interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
-        else:
-            interpolation = cv2.INTER_AREA  # pylint:disable=no-member
-
-        face = cv2.resize(face,  # pylint:disable=no-member
-                          dsize=(int(self.input_size), int(self.input_size)),
-                          interpolation=interpolation)
-        return dict(image=face, roi=roi)
+        rois = []
+        faces = []
+        for face in detected_faces:
+            box = (face.left,
+                   face.top,
+                   face.right,
+                   face.bottom)
+            diff_height_width = face.h - face.w
+            offset_y = int(abs(diff_height_width / 2))
+            box_moved = self.move_box(box, [0, offset_y])
+
+            # Make box square.
+            roi = self.get_square_box(box_moved)
+            # Pad the image if face is outside of boundaries
+            image = self.pad_image(roi, face.image)
+            face = image[roi[1]: roi[3], roi[0]: roi[2]]
+
+            if face.shape[0] < self.input_size:
+                interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
+            else:
+                interpolation = cv2.INTER_AREA  # pylint:disable=no-member
+
+            face = cv2.resize(face,  # pylint:disable=no-member
+                              dsize=(int(self.input_size), int(self.input_size)),
+                              interpolation=interpolation)
+            faces.append(face)
+            rois.append(roi)
+        return faces, rois
 
     @staticmethod
     def move_box(box, offset):
@@ -159,24 +156,27 @@ class Align(Aligner):
         logger.trace("Padded shape: %s", retval.shape)
         return retval
 
-    def predict_landmarks(self, feed_dict):
+    def predict(self, batch):
         """ Predict the 68 point landmarks """
         logger.trace("Predicting Landmarks")
-        image = np.expand_dims(np.transpose(feed_dict["image"], (2, 0, 1)), 0).astype("float32")
-        self.model.setInput(image)
-        prediction = self.model.forward()
-        pts_img = self.get_pts_from_predict(prediction, feed_dict["roi"])
-        return pts_img
+        self.model.setInput(batch["feed"])
+        batch["prediction"] = self.model.forward()
+        return batch
+
+    def process_output(self, batch):
+        """ Process the output from the model """
+        self.get_pts_from_predict(batch)
+        return batch
 
     @staticmethod
-    def get_pts_from_predict(prediction, roi):
+    def get_pts_from_predict(batch):
         """ Get points from predictor """
-        logger.trace("Obtain points from prediction")
-        points = np.array(prediction).flatten()
-        points = np.reshape(points, (-1, 2))
-        points *= (roi[2] - roi[0])
-        points[:, 0] += roi[0]
-        points[:, 1] += roi[1]
-        retval = np.rint(points).astype("uint").tolist()
-        logger.trace("Predicted Landmarks: %s", retval)
-        return retval
+        for prediction, roi in zip(batch["prediction"], batch["roi"]):
+            points = np.array(prediction).flatten()
+            points = np.reshape(points, (-1, 2))
+            points *= (roi[2] - roi[0])
+            points[:, 0] += roi[0]
+            points[:, 1] += roi[1]
+            landmarks = np.rint(points).astype("uint").tolist()
+            batch.setdefault("landmarks", []).append(landmarks)
+        logger.trace("Predicted Landmarks: %s", batch["landmarks"])
diff --git a/plugins/extract/align/fan.py b/plugins/extract/align/fan.py
index f9bc2a4..0c0306e 100644
--- a/plugins/extract/align/fan.py
+++ b/plugins/extract/align/fan.py
@@ -5,122 +5,114 @@
 """
 import cv2
 import numpy as np
+import keras
+from keras import backend as K
 
+from lib.model.session import KSession
 from ._base import Aligner, logger
 
 
 class Align(Aligner):
     """ Perform transformation to align and get landmarks """
     def __init__(self, **kwargs):
-        git_model_id = 0
-        model_filename = "face-alignment-network_2d4_v1.pb"
-        super().__init__(git_model_id=git_model_id,
-                         model_filename=model_filename,
-                         colorspace="RGB",
-                         input_size=256,
-                         **kwargs)
+        git_model_id = 9
+        model_filename = "face-alignment-network_2d4_keras_v1.h5"
+        super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
+        self.name = "FAN"
+        self.input_size = 256
+        self.colorformat = "RGB"
         self.vram = 2240
-        self.model = None
+        self.vram_warnings = 512  # Will run at this with warnings
+        self.vram_per_batch = 64
+        self.batchsize = self.config["batch-size"]
         self.reference_scale = 195
 
-    def initialize(self, *args, **kwargs):
-        """ Initialization tasks to run prior to alignments """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing Face Alignment Network...")
-            logger.debug("fan initialize: (args: %s kwargs: %s)", args, kwargs)
-
-            _, _, vram_total = self.get_vram_free()
-
-            if vram_total <= self.vram:
-                tf_ratio = 1.0
-            else:
-                tf_ratio = self.vram / vram_total
-            logger.verbose("Reserving %sMB for face alignments", self.vram)
-
-            self.model = FAN(self.model_path, ratio=tf_ratio)
-
-            self.init.set()
-            logger.info("Initialized Face Alignment Network.")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    # DETECTED FACE BOUNDING BOX PROCESSING
-    def align_image(self, detected_face, image):
-        """ Get center and scale, crop and align image around center """
-        logger.trace("Aligning image around center")
-        center, scale = self.get_center_scale(detected_face)
-        image = self.crop(image, center, scale)
+    def init_model(self):
+        """ Initialize FAN model """
+        model_kwargs = dict(custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D})
+        self.model = KSession(self.name, self.model_path, model_kwargs=model_kwargs)
+        self.model.load_model()
+        # Feed a placeholder so Aligner is primed for Manual tool
+        placeholder = np.zeros((self.batchsize, 3, self.input_size, self.input_size),
+                               dtype="float32")
+        self.model.predict(placeholder)
+
+    def process_input(self, batch):
+        """ Compile the detected faces for prediction """
+        # TODO Batching
+        logger.trace("Aligning faces around center")
+        batch["center_scale"] = self.get_center_scale(batch["detected_faces"])
+        faces = self.crop(batch)
         logger.trace("Aligned image around center")
-        return dict(image=image, center=center, scale=scale)
+        faces = self._normalize_faces(faces)
+        batch["feed"] = np.array(faces, dtype="float32").transpose((0, 3, 1, 2)) / 255.0
+        return batch
 
-    def get_center_scale(self, detected_face):
+    def get_center_scale(self, detected_faces):
         """ Get the center and set scale of bounding box """
         logger.trace("Calculating center and scale")
-        center = np.array([(detected_face["left"] + detected_face["right"]) / 2.0,
-                           (detected_face["top"] + detected_face["bottom"]) / 2.0])
-
-        height = detected_face["bottom"] - detected_face["top"]
-        width = detected_face["right"] - detected_face["left"]
-
-        center[1] -= height * 0.12
-
-        scale = (width + height) / self.reference_scale
+        l_center = []
+        l_scale = []
+        for face in detected_faces:
+            center = np.array([(face.left + face.right) / 2.0, (face.top + face.bottom) / 2.0])
+            center[1] -= face.h * 0.12
+            l_center.append(center)
+            l_scale.append((face.w + face.h) / self.reference_scale)
+        logger.trace("Calculated center and scale: %s, %s", l_center, l_scale)
+        return l_center, l_scale
+
+    def crop(self, batch):  # pylint:disable=too-many-locals
+        """ Crop image around the center point """
+        logger.trace("Cropping images")
+        new_images = []
+        for face, center, scale in zip(batch["detected_faces"], *batch["center_scale"]):
+            is_color = face.image.ndim > 2
+            v_ul = self.transform([1, 1], center, scale, self.input_size).astype(np.int)
+            v_br = self.transform([self.input_size, self.input_size],
+                                  center,
+                                  scale,
+                                  self.input_size).astype(np.int)
+            if is_color:
+                new_dim = np.array([v_br[1] - v_ul[1],
+                                    v_br[0] - v_ul[0],
+                                    face.image.shape[2]],
+                                   dtype=np.int32)
+                new_img = np.zeros(new_dim, dtype=np.uint8)
+            else:
+                new_dim = np.array([v_br[1] - v_ul[1],
+                                    v_br[0] - v_ul[0]],
+                                   dtype=np.int)
+                new_img = np.zeros(new_dim, dtype=np.uint8)
+            height = face.image.shape[0]
+            width = face.image.shape[1]
+            new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
+                             dtype=np.int32)
+            new_y = np.array([max(1, -v_ul[1] + 1),
+                              min(v_br[1], height) - v_ul[1]],
+                             dtype=np.int32)
+            old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
+                             dtype=np.int32)
+            old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
+                             dtype=np.int32)
+            if is_color:
+                new_img[new_y[0] - 1:new_y[1],
+                        new_x[0] - 1:new_x[1]] = face.image[old_y[0] - 1:old_y[1],
+                                                            old_x[0] - 1:old_x[1], :]
+            else:
+                new_img[new_y[0] - 1:new_y[1],
+                        new_x[0] - 1:new_x[1]] = face.image[old_y[0] - 1:old_y[1],
+                                                            old_x[0] - 1:old_x[1]]
 
-        logger.trace("Calculated center and scale: %s, %s", center, scale)
-        return center, scale
+            if new_img.shape[0] < self.input_size:
+                interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
+            else:
+                interpolation = cv2.INTER_AREA  # pylint:disable=no-member
 
-    def crop(self, image, center, scale):  # pylint:disable=too-many-locals
-        """ Crop image around the center point """
-        logger.trace("Cropping image")
-        is_color = image.ndim > 2
-        v_ul = self.transform([1, 1], center, scale, self.input_size).astype(np.int)
-        v_br = self.transform([self.input_size, self.input_size],
-                              center,
-                              scale,
-                              self.input_size).astype(np.int)
-        if is_color:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0],
-                                image.shape[2]],
-                               dtype=np.int32)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        else:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0]],
-                               dtype=np.int)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        height = image.shape[0]
-        width = image.shape[1]
-        new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
-                         dtype=np.int32)
-        new_y = np.array([max(1, -v_ul[1] + 1),
-                          min(v_br[1], height) - v_ul[1]],
-                         dtype=np.int32)
-        old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
-                         dtype=np.int32)
-        old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
-                         dtype=np.int32)
-        if is_color:
-            new_img[new_y[0] - 1:new_y[1],
-                    new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
-                                                   old_x[0] - 1:old_x[1], :]
-        else:
-            new_img[new_y[0] - 1:new_y[1],
-                    new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
-                                                   old_x[0] - 1:old_x[1]]
-
-        if new_img.shape[0] < self.input_size:
-            interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
-        else:
-            interpolation = cv2.INTER_AREA  # pylint:disable=no-member
-
-        new_img = cv2.resize(new_img,  # pylint:disable=no-member
-                             dsize=(int(self.input_size), int(self.input_size)),
-                             interpolation=interpolation)
-        logger.trace("Cropped image")
-        return new_img
+            new_images.append(cv2.resize(new_img,  # pylint:disable=no-member
+                                         dsize=(int(self.input_size), int(self.input_size)),
+                                         interpolation=interpolation))
+        logger.trace("Cropped images")
+        return new_images
 
     @staticmethod
     def transform(point, center, scale, resolution):
@@ -138,96 +130,120 @@ class Align(Aligner):
         logger.trace("Transformed Points: %s", retval)
         return retval
 
-    def predict_landmarks(self, feed_dict):
+    def predict(self, batch):
         """ Predict the 68 point landmarks """
         logger.trace("Predicting Landmarks")
-        image = np.expand_dims(
-            feed_dict["image"].transpose((2, 0, 1)).astype(np.float32) / 255.0, 0)
-        prediction = self.model.predict(image)[-1]
-        pts_img = self.get_pts_from_predict(prediction, feed_dict["center"], feed_dict["scale"])
-        retval = [(int(pt[0]), int(pt[1])) for pt in pts_img]
-        logger.trace("Predicted Landmarks: %s", retval)
-        return retval
+        batch["prediction"] = self.model.predict(batch["feed"])[-1]
+        logger.trace([pred.shape for pred in batch["prediction"]])
+        return batch
+
+    def process_output(self, batch):
+        """ Process the output from the model """
+        self.get_pts_from_predict(batch)
+        return batch
 
-    def get_pts_from_predict(self, prediction, center, scale):
+    def get_pts_from_predict(self, batch):
         """ Get points from predictor """
         logger.trace("Obtain points from prediction")
-        var_b = prediction.reshape((prediction.shape[0],
-                                    prediction.shape[1] * prediction.shape[2]))
-        var_c = var_b.argmax(1).reshape((prediction.shape[0],
-                                         1)).repeat(2,
-                                                    axis=1).astype(np.float)
-        var_c[:, 0] %= prediction.shape[2]
-        var_c[:, 1] = np.apply_along_axis(
-            lambda x: np.floor(x / prediction.shape[2]),
-            0,
-            var_c[:, 1])
-
-        for i in range(prediction.shape[0]):
-            pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
-            if pt_x > 0 and pt_x < 63 and pt_y > 0 and pt_y < 63:
-                diff = np.array([prediction[i, pt_y, pt_x+1]
-                                 - prediction[i, pt_y, pt_x-1],
-                                 prediction[i, pt_y+1, pt_x]
-                                 - prediction[i, pt_y-1, pt_x]])
-
-                var_c[i] += np.sign(diff)*0.25
-
-        var_c += 0.5
-        retval = [self.transform(var_c[i], center, scale, prediction.shape[2])
-                  for i in range(prediction.shape[0])]
-        logger.trace("Obtained points from prediction: %s", retval)
-
-        return retval
-
-
-class FAN():
-    """The FAN Model.
-    Converted from pyTorch via ONNX from:
-    https://github.com/1adrianb/face-alignment """
-
-    def __init__(self, model_path, ratio=1.0):
-        # Must import tensorflow inside the spawned process
-        # for Windows machines
-        import tensorflow as tf
-        self.tf = tf  # pylint: disable=invalid-name
-
-        self.model_path = model_path
-        self.graph = self.load_graph()
-        self.input = self.graph.get_tensor_by_name("fa/input_1:0")
-        self.output = self.graph.get_tensor_by_name("fa/transpose_647:0")
-        self.session = self.set_session(ratio)
-
-    def load_graph(self):
-        """ Load the tensorflow Model and weights """
-        # pylint: disable=not-context-manager
-        logger.verbose("Initializing Face Alignment Network model...")
-
-        with self.tf.gfile.GFile(self.model_path, "rb") as gfile:
-            graph_def = self.tf.GraphDef()
-            graph_def.ParseFromString(gfile.read())
-        fa_graph = self.tf.Graph()
-        with fa_graph.as_default():
-            self.tf.import_graph_def(graph_def, name="fa")
-        return fa_graph
-
-    def set_session(self, vram_ratio):
-        """ Set the TF Session and initialize """
-        # pylint: disable=not-context-manager, no-member
-        placeholder = np.zeros((1, 3, 256, 256))
-        with self.graph.as_default():
-            config = self.tf.ConfigProto()
-            config.gpu_options.per_process_gpu_memory_fraction = vram_ratio
-            session = self.tf.Session(config=config)
-            with session.as_default():
-                if any("gpu" in str(device).lower() for device in session.list_devices()):
-                    logger.debug("Using GPU")
-                else:
-                    logger.warning("Using CPU")
-                session.run(self.output, feed_dict={self.input: placeholder})
-        return session
-
-    def predict(self, feed_item):
-        """ Predict landmarks in session """
-        return self.session.run(self.output,
-                                feed_dict={self.input: feed_item})
+        landmarks = []
+        for prediction, center, scale in zip(batch["prediction"], *batch["center_scale"]):
+            var_b = prediction.reshape((prediction.shape[0],
+                                        prediction.shape[1] * prediction.shape[2]))
+            var_c = var_b.argmax(1).reshape((prediction.shape[0],
+                                             1)).repeat(2,
+                                                        axis=1).astype(np.float)
+            var_c[:, 0] %= prediction.shape[2]
+            var_c[:, 1] = np.apply_along_axis(
+                lambda x: np.floor(x / prediction.shape[2]),
+                0,
+                var_c[:, 1])
+
+            for i in range(prediction.shape[0]):
+                pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
+                if 63 > pt_x > 0 and 63 > pt_y > 0:
+                    diff = np.array([prediction[i, pt_y, pt_x+1]
+                                     - prediction[i, pt_y, pt_x-1],
+                                     prediction[i, pt_y+1, pt_x]
+                                     - prediction[i, pt_y-1, pt_x]])
+
+                    var_c[i] += np.sign(diff)*0.25
+
+            var_c += 0.5
+            landmarks = [self.transform(var_c[i], center, scale, prediction.shape[2])
+                         for i in range(prediction.shape[0])]
+            batch.setdefault("landmarks", []).append(landmarks)
+        logger.trace("Obtained points from prediction: %s", batch["landmarks"])
+
+
+class TorchBatchNorm2D(keras.engine.base_layer.Layer):
+    # pylint:disable=too-many-instance-attributes
+    """" Required for FAN_keras model """
+    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
+        super(TorchBatchNorm2D, self).__init__(**kwargs)
+        self.supports_masking = True
+        self.axis = axis
+        self.momentum = momentum
+        self.epsilon = epsilon
+        self._epsilon_const = K.constant(self.epsilon, dtype='float32')
+
+        self.built = False
+        self.gamma = None
+        self.beta = None
+        self.moving_mean = None
+        self.moving_variance = None
+
+    def build(self, input_shape):
+        dim = input_shape[self.axis]
+        if dim is None:
+            raise ValueError("Axis {} of input tensor should have a "
+                             "defined dimension but the layer received "
+                             "an input with  shape {}."
+                             .format(str(self.axis), str(input_shape)))
+        shape = (dim,)
+        self.gamma = self.add_weight(shape=shape,
+                                     name='gamma',
+                                     initializer='ones',
+                                     regularizer=None,
+                                     constraint=None)
+        self.beta = self.add_weight(shape=shape,
+                                    name='beta',
+                                    initializer='zeros',
+                                    regularizer=None,
+                                    constraint=None)
+        self.moving_mean = self.add_weight(shape=shape,
+                                           name='moving_mean',
+                                           initializer='zeros',
+                                           trainable=False)
+        self.moving_variance = self.add_weight(shape=shape,
+                                               name='moving_variance',
+                                               initializer='ones',
+                                               trainable=False)
+        self.built = True
+
+    def call(self, inputs, **kwargs):
+        input_shape = K.int_shape(inputs)
+
+        broadcast_shape = [1] * len(input_shape)
+        broadcast_shape[self.axis] = input_shape[self.axis]
+
+        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
+        broadcast_moving_variance = K.reshape(self.moving_variance,
+                                              broadcast_shape)
+        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
+        broadcast_beta = K.reshape(self.beta, broadcast_shape)
+        invstd = (
+            K.ones(shape=broadcast_shape, dtype='float32')
+            / K.sqrt(broadcast_moving_variance + self._epsilon_const)
+        )
+
+        return((inputs - broadcast_moving_mean)
+               * invstd
+               * broadcast_gamma
+               + broadcast_beta)
+
+    def get_config(self):
+        config = {'axis': self.axis,
+                  'momentum': self.momentum,
+                  'epsilon': self.epsilon}
+        base_config = super(TorchBatchNorm2D, self).get_config()
+        return dict(list(base_config.items()) + list(config.items()))
diff --git a/plugins/extract/align/fan_amd.py b/plugins/extract/align/fan_amd.py
deleted file mode 100644
index cc5bce3..0000000
--- a/plugins/extract/align/fan_amd.py
+++ /dev/null
@@ -1,271 +0,0 @@
-#!/usr/bin/env python3
-""" Facial landmarks extractor for faceswap.py
-    Code adapted and modified from:
-    https://github.com/1adrianb/face-alignment
-"""
-import cv2
-import numpy as np
-import keras
-from keras import backend as K
-
-from ._base import Aligner, logger
-
-
-class Align(Aligner):
-    """ Perform transformation to align and get landmarks """
-    def __init__(self, **kwargs):
-        git_model_id = 9
-        model_filename = "face-alignment-network_2d4_keras_v1.h5"
-        super().__init__(git_model_id=git_model_id,
-                         model_filename=model_filename,
-                         colorspace="RGB",
-                         input_size=256,
-                         **kwargs)
-        self.vram = 2240
-        self.model = None
-        self.reference_scale = 195
-        self.supports_plaidml = True
-
-    def initialize(self, *args, **kwargs):
-        """ Initialization tasks to run prior to alignments """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing Face Alignment Network...")
-            logger.debug("fan initialize: (args: %s kwargs: %s)", args, kwargs)
-            self.model = FAN(self.model_path)
-            self.init.set()
-            logger.info("Initialized Face Alignment Network.")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    # DETECTED FACE BOUNDING BOX PROCESSING
-    def align_image(self, detected_face, image):
-        """ Get center and scale, crop and align image around center """
-        logger.trace("Aligning image around center")
-        center, scale = self.get_center_scale(detected_face)
-        image = self.crop(image, center, scale)
-        logger.trace("Aligned image around center")
-        return dict(image=image, center=center, scale=scale)
-
-    def get_center_scale(self, detected_face):
-        """ Get the center and set scale of bounding box """
-        logger.trace("Calculating center and scale")
-        center = np.array([(detected_face["left"] + detected_face["right"]) / 2.0,
-                           (detected_face["top"] + detected_face["bottom"]) / 2.0])
-
-        height = detected_face["bottom"] - detected_face["top"]
-        width = detected_face["right"] - detected_face["left"]
-
-        center[1] -= height * 0.12
-
-        scale = (width + height) / self.reference_scale
-
-        logger.trace("Calculated center and scale: %s, %s", center, scale)
-        return center, scale
-
-    def crop(self, image, center, scale):  # pylint:disable=too-many-locals
-        """ Crop image around the center point """
-        logger.trace("Cropping image")
-        is_color = image.ndim > 2
-        v_ul = self.transform([1, 1], center, scale, self.input_size).astype(np.int)
-        v_br = self.transform([self.input_size, self.input_size],
-                              center,
-                              scale,
-                              self.input_size).astype(np.int)
-        if is_color:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0],
-                                image.shape[2]],
-                               dtype=np.int32)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        else:
-            new_dim = np.array([v_br[1] - v_ul[1],
-                                v_br[0] - v_ul[0]],
-                               dtype=np.int)
-            new_img = np.zeros(new_dim, dtype=np.uint8)
-        height = image.shape[0]
-        width = image.shape[1]
-        new_x = np.array([max(1, -v_ul[0] + 1), min(v_br[0], width) - v_ul[0]],
-                         dtype=np.int32)
-        new_y = np.array([max(1, -v_ul[1] + 1),
-                          min(v_br[1], height) - v_ul[1]],
-                         dtype=np.int32)
-        old_x = np.array([max(1, v_ul[0] + 1), min(v_br[0], width)],
-                         dtype=np.int32)
-        old_y = np.array([max(1, v_ul[1] + 1), min(v_br[1], height)],
-                         dtype=np.int32)
-        if is_color:
-            new_img[new_y[0] - 1:new_y[1],
-                    new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
-                                                   old_x[0] - 1:old_x[1], :]
-        else:
-            new_img[new_y[0] - 1:new_y[1],
-                    new_x[0] - 1:new_x[1]] = image[old_y[0] - 1:old_y[1],
-                                                   old_x[0] - 1:old_x[1]]
-
-        if new_img.shape[0] < self.input_size:
-            interpolation = cv2.INTER_CUBIC  # pylint:disable=no-member
-        else:
-            interpolation = cv2.INTER_AREA  # pylint:disable=no-member
-
-        new_img = cv2.resize(new_img,  # pylint:disable=no-member
-                             dsize=(int(self.input_size), int(self.input_size)),
-                             interpolation=interpolation)
-        logger.trace("Cropped image")
-        return new_img
-
-    @staticmethod
-    def transform(point, center, scale, resolution):
-        """ Transform Image """
-        logger.trace("Transforming Points")
-        pnt = np.array([point[0], point[1], 1.0])
-        hscl = 200.0 * scale
-        eye = np.eye(3)
-        eye[0, 0] = resolution / hscl
-        eye[1, 1] = resolution / hscl
-        eye[0, 2] = resolution * (-center[0] / hscl + 0.5)
-        eye[1, 2] = resolution * (-center[1] / hscl + 0.5)
-        eye = np.linalg.inv(eye)
-        retval = np.matmul(eye, pnt)[0:2]
-        logger.trace("Transformed Points: %s", retval)
-        return retval
-
-    def predict_landmarks(self, feed_dict):
-        """ Predict the 68 point landmarks """
-        logger.trace("Predicting Landmarks")
-        image = np.expand_dims(
-            feed_dict["image"].transpose((2, 0, 1)).astype(np.float32) / 255.0, 0)
-        prediction = self.model.predict(image)[-1]
-        pts_img = self.get_pts_from_predict(prediction, feed_dict["center"], feed_dict["scale"])
-        retval = [(int(pt[0]), int(pt[1])) for pt in pts_img]
-        logger.trace("Predicted Landmarks: %s", retval)
-        return retval
-
-    def get_pts_from_predict(self, prediction, center, scale):
-        """ Get points from predictor """
-        logger.trace("Obtain points from prediction")
-        var_b = prediction.reshape((prediction.shape[0],
-                                    prediction.shape[1] * prediction.shape[2]))
-        var_c = var_b.argmax(1).reshape((prediction.shape[0],
-                                         1)).repeat(2,
-                                                    axis=1).astype(np.float)
-        var_c[:, 0] %= prediction.shape[2]
-        var_c[:, 1] = np.apply_along_axis(
-            lambda x: np.floor(x / prediction.shape[2]),
-            0,
-            var_c[:, 1])
-
-        for i in range(prediction.shape[0]):
-            pt_x, pt_y = int(var_c[i, 0]), int(var_c[i, 1])
-            if pt_x > 0 and pt_x < 63 and pt_y > 0 and pt_y < 63:
-                diff = np.array([prediction[i, pt_y, pt_x+1]
-                                 - prediction[i, pt_y, pt_x-1],
-                                 prediction[i, pt_y+1, pt_x]
-                                 - prediction[i, pt_y-1, pt_x]])
-
-                var_c[i] += np.sign(diff)*0.25
-
-        var_c += 0.5
-        retval = [self.transform(var_c[i], center, scale, prediction.shape[2])
-                  for i in range(prediction.shape[0])]
-        logger.trace("Obtained points from prediction: %s", retval)
-
-        return retval
-
-
-class TorchBatchNorm2D(keras.engine.base_layer.Layer):
-    """" Required for FAN_keras model """
-    def __init__(self, axis=-1, momentum=0.99, epsilon=1e-3, **kwargs):
-        super(TorchBatchNorm2D, self).__init__(**kwargs)
-        self.supports_masking = True
-        self.axis = axis
-        self.momentum = momentum
-        self.epsilon = epsilon
-        self._epsilon_const = K.constant(self.epsilon, dtype='float32')
-
-        self.built = False
-        self.gamma = None
-        self.beta = None
-        self.moving_mean = None
-        self.moving_variance = None
-
-    def build(self, input_shape):
-        dim = input_shape[self.axis]
-        if dim is None:
-            raise ValueError("Axis {} of input tensor should have a "
-                             "defined dimension but the layer received "
-                             "an input with  shape {}."
-                             .format(str(self.axis), str(input_shape)))
-        shape = (dim,)
-        self.gamma = self.add_weight(shape=shape,
-                                     name='gamma',
-                                     initializer='ones',
-                                     regularizer=None,
-                                     constraint=None)
-        self.beta = self.add_weight(shape=shape,
-                                    name='beta',
-                                    initializer='zeros',
-                                    regularizer=None,
-                                    constraint=None)
-        self.moving_mean = self.add_weight(shape=shape,
-                                           name='moving_mean',
-                                           initializer='zeros',
-                                           trainable=False)
-        self.moving_variance = self.add_weight(shape=shape,
-                                               name='moving_variance',
-                                               initializer='ones',
-                                               trainable=False)
-        self.built = True
-
-    def call(self, inputs, **kwargs):
-        input_shape = K.int_shape(inputs)
-
-        broadcast_shape = [1] * len(input_shape)
-        broadcast_shape[self.axis] = input_shape[self.axis]
-
-        broadcast_moving_mean = K.reshape(self.moving_mean, broadcast_shape)
-        broadcast_moving_variance = K.reshape(self.moving_variance,
-                                              broadcast_shape)
-        broadcast_gamma = K.reshape(self.gamma, broadcast_shape)
-        broadcast_beta = K.reshape(self.beta, broadcast_shape)
-        invstd = (
-            K.ones(shape=broadcast_shape, dtype='float32')
-            / K.sqrt(broadcast_moving_variance + self._epsilon_const)
-        )
-
-        return((inputs - broadcast_moving_mean)
-               * invstd
-               * broadcast_gamma
-               + broadcast_beta)
-
-    def get_config(self):
-        config = {'axis': self.axis,
-                  'momentum': self.momentum,
-                  'epsilon': self.epsilon}
-        base_config = super(TorchBatchNorm2D, self).get_config()
-        return dict(list(base_config.items()) + list(config.items()))
-
-
-class FAN():
-    """
-    Converted from pyTorch from
-    https://github.com/1adrianb/face-alignment
-    """
-    def __init__(self, model_path):
-        self.model_path = model_path
-        self.model = None
-        self.load_model()
-
-    def load_model(self):
-        """ Load the Keras Model """
-        logger.verbose("Initializing Face Alignment Network model (Keras version).")
-        self.model = keras.models.load_model(
-            self.model_path,
-            custom_objects={'TorchBatchNorm2D': TorchBatchNorm2D}
-        )
-
-    def predict(self, feed_item):
-        """ Predict landmarks in session """
-        pred = self.model.predict(feed_item)
-        return [pred[-1].reshape((68, 64, 64))]
diff --git a/plugins/extract/detect/s3fd_amd_defaults.py b/plugins/extract/align/fan_defaults.py
similarity index 75%
rename from plugins/extract/detect/s3fd_amd_defaults.py
rename to plugins/extract/align/fan_defaults.py
index e1496ec..da7277a 100644
--- a/plugins/extract/detect/s3fd_amd_defaults.py
+++ b/plugins/extract/align/fan_defaults.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python3
 """
-    The default options for the faceswap S3Fd-AMD Detect plugin.
+    The default options for the faceswap FAN Alignments plugin.
 
     Defaults files should be named <plugin_name>_defaults.py
     Any items placed into this file will automatically get added to the relevant config .ini files
@@ -22,6 +22,8 @@
                    <class 'str'>, <class 'bool'>.
         default:   [required] The default value for this option.
         info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
         choices:   [optional] If this option's datatype is of <class 'str'> then valid
                    selections can be defined here. This validates the option and also enables
                    a combobox / radio option in the GUI.
@@ -42,32 +44,21 @@
 
 
 _HELPTEXT = (
-    "S3FD-AMD Detector options. Uses keras backend to support AMD cards.\n"
-    "Fast on GPU, slow on CPU. Can detect more faces and fewer false "
-    "positives than other GPU detectors, but is a lot more resource intensive."
+    "FAN Aligner options.Fast on GPU, slow on CPU. Best aligner."
     )
 
 
 _DEFAULTS = {
-    "confidence": {
-        "default": 50,
-        "info": "The confidence level at which the detector has succesfully found a face.\n"
-                "Higher levels will be more discriminating, lower levels will have more false "
-                "positives.",
-        "datatype": int,
-        "rounding": 5,
-        "min_max": (25, 100),
-        "choices": [],
-        "gui_radio": False,
-        "fixed": True,
-    },
     "batch-size": {
         "default": 8,
-        "info": "The batch size to use. Normally higher batch sizes equal better performance.\n"
-                "A batchsize of 8 requires about 2 GB vram.",
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about 4 GB vram.",
         "datatype": int,
         "rounding": 1,
-        "min_max": (1, 32),
+        "min_max": (1, 64),
         "choices": [],
         "gui_radio": False,
         "fixed": True,
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index 56f193b..5791159 100755
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -1,249 +1,319 @@
 #!/usr/bin/env python3
 """ Base class for Face Detector plugins
-    Plugins should inherit from this class
 
-    See the override methods for which methods are
-    required.
+All Detector Plugins should inherit from this class.
+See the override methods for which methods are required.
 
-    For each source frame, the plugin must pass a dict to finalize containing:
-    {"filename": <filename of source frame>,
-     "image": <source image>,
-     "detected_faces": <list of dicts containing bounding box points>}}
+For each source frame, the plugin must pass a dict to finalize containing:
 
-    - Use the function self.to_bounding_box_dict(left, right, top, bottom) to define the dict
-    """
-
-import logging
-import os
-import traceback
-from io import StringIO
+>>> {'filename': <filename of source frame>,
+>>>  'image':  <source image>,
+>>>  'detected_faces': <list of DetectedFace objects containing bounding box points}}
 
-import cv2
+To get a :class:`~lib.faces_detect.DetectedFace` object use the function:
 
-from lib.gpu_stats import GPUStats
-from lib.utils import rotate_landmarks, GetModel
-from plugins.extract._config import Config
+>>> face = self.to_detected_face(<face left>, <face top>, <face right>, <face bottom>)
 
-logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
+"""
+import cv2
+import numpy as np
+
+from lib.faces_detect import DetectedFace
+from lib.utils import rotate_landmarks
+from plugins.extract._base import Extractor, logger
+
+
+class Detector(Extractor):
+    """ Detector Object
+
+    Parent class for all Detector plugins
+
+    Parameters
+    ----------
+    git_model_id: int
+        The second digit in the github tag that identifies this model. See
+        https://github.com/deepfakes-models/faceswap-models for more information
+    model_filename: str
+        The name of the model file to be loaded
+    rotation: str, optional
+        Pass in a single number to use increments of that size up to 360, or pass in a ``list`` of
+        ``ints`` to enumerate exactly what angles to check. Can also pass in ``'on'`` to increment
+        at 90 degree intervals. Default: ``None``
+    min_size: int, optional
+        Filters out faces detected below this size. Length, in pixels across the diagonal of the
+        bounding box. Set to ``0`` for off. Default: ``0``
+
+    Other Parameters
+    ----------------
+    configfile: str, optional
+        Path to a custom configuration ``ini`` file. Default: Use system configfile
+
+    See Also
+    --------
+    plugins.extract.pipeline : The extraction pipeline for calling plugins
+    plugins.extract.detect : Detector plugins
+    plugins.extract._base : Parent class for all extraction plugins
+    plugins.extract.align._base : Aligner parent class for extraction plugins.
 
+    """
 
-def get_config(plugin_name, configfile=None):
-    """ Return the config for the requested model """
-    return Config(plugin_name, configfile=configfile).config_dict
+    def __init__(self, git_model_id=None, model_filename=None,
+                 configfile=None, rotation=None, min_size=0):
+        logger.debug("Initializing %s: (rotation: %s, min_size: %s)", self.__class__.__name__,
+                     rotation, min_size)
+        super().__init__(git_model_id,
+                         model_filename,
+                         configfile=configfile)
+        self.rotation = self._get_rotation_angles(rotation)
+        self.min_size = min_size
 
+        self._plugin_type = "detect"
 
-class Detector():
-    """ Detector object """
-    def __init__(self, loglevel, configfile=None,  # pylint:disable=too-many-arguments
-                 git_model_id=None, model_filename=None, rotation=None, min_size=0):
-        logger.debug("Initializing %s: (loglevel: %s, configfile: %s, git_model_id: %s, "
-                     "model_filename: %s, rotation: %s, min_size: %s)",
-                     self.__class__.__name__, loglevel, configfile, git_model_id,
-                     model_filename, rotation, min_size)
-        self.config = get_config(".".join(self.__module__.split(".")[-2:]), configfile=configfile)
-        self.loglevel = loglevel
-        self.rotation = self.get_rotation_angles(rotation)
-        self.min_size = min_size
-        self.parent_is_pool = False
-        self.init = None
-        self.error = None
-
-        # The input and output queues for the plugin.
-        # See lib.queue_manager.QueueManager for getting queues
-        self.queues = {"in": None, "out": None}
-
-        #  Path to model if required
-        self.model_path = self.get_model(git_model_id, model_filename)
-
-        # Target image size for passing images through the detector
-        # Set to tuple of dimensions (x, y) or int of pixel count
-        self.target = None
-
-        # Approximate VRAM used for the set target. Used to calculate
-        # how many parallel processes / batches can be run.
-        # Be conservative to avoid OOM.
-        self.vram = None
-
-        # Set to true if the plugin supports PlaidML
-        self.supports_plaidml = False
-
-        # For detectors that support batching, this should be set to
-        # the calculated batch size that the amount of available VRAM
-        # will support. It is also used for holding the number of threads/
-        # processes for parallel processing plugins
-        self.batch_size = 1
         logger.debug("Initialized _base %s", self.__class__.__name__)
 
-    # <<< OVERRIDE METHODS >>> #
-    def initialize(self, *args, **kwargs):
-        """ Inititalize the detector
-            Tasks to be run before any detection is performed.
-            Override for specific detector """
-        logger.debug("initialize %s (PID: %s, args: %s, kwargs: %s)",
-                     self.__class__.__name__, os.getpid(), args, kwargs)
-        self.init = kwargs.get("event", False)
-        self.error = kwargs.get("error", False)
-        self.queues["in"] = kwargs["in_queue"]
-        self.queues["out"] = kwargs["out_queue"]
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in rgb image
-            Override for specific detector
-            Must return a list of bounding box dicts (See module docstring)"""
-        try:
-            if not self.init:
-                self.initialize(*args, **kwargs)
-        except ValueError as err:
-            logger.error(err)
-            exit(1)
-        logger.debug("Detecting Faces (args: %s, kwargs: %s)", args, kwargs)
-
-    # <<< GET MODEL >>> #
-    @staticmethod
-    def get_model(git_model_id, model_filename):
-        """ Check if model is available, if not, download and unzip it """
-        if model_filename is None:
-            logger.debug("No model_filename specified. Returning None")
-            return None
-        if git_model_id is None:
-            logger.debug("No git_model_id specified. Returning None")
-            return None
-        cache_path = os.path.join(os.path.dirname(__file__), ".cache")
-        model = GetModel(model_filename, cache_path, git_model_id)
-        return model.model_path
-
-    # <<< DETECTION WRAPPER >>> #
-    def run(self, *args, **kwargs):
-        """ Parent detect process.
-            This should always be called as the entry point so exceptions
-            are passed back to parent.
-            Do not override """
-        try:
-            logger.debug("Executing detector run function")
-            self.detect_faces(*args, **kwargs)
-        except Exception as err:  # pylint: disable=broad-except
-            logger.error("Caught exception in child process: %s: %s", os.getpid(), str(err))
-            # Display traceback if in initialization stage
-            if not self.init.is_set():
-                logger.exception("Traceback:")
-            tb_buffer = StringIO()
-            traceback.print_exc(file=tb_buffer)
-            logger.trace(tb_buffer.getvalue())
-            exception = {"exception": (os.getpid(), tb_buffer)}
-            self.queues["out"].put(exception)
-            exit(1)
+    # <<< QUEUE METHODS >>> #
+    def get_batch(self, queue):
+        """ Get items for inputting to the detector plugin in batches
+
+        Items are returned from the ``queue`` in batches of
+        :attr:`~plugins.extract._base.Extractor.batchsize`
+
+        Remember to put ``'EOF'`` to the out queue after processing
+        the final batch
+
+        Outputs items in the following format. All lists are of length
+        :attr:`~plugins.extract._base.Extractor.batchsize`:
+
+        >>> {'filename': [<filenames of source frames>],
+        >>>  'image': [<source images>],
+        >>>  'scaled_image': <np.array of images standardized for prediction>,
+        >>>  'scale': [<scaling factors for each image>],
+        >>>  'pad': [<padding for each image>],
+        >>>  'detected_faces': [[<lib.faces_detect.DetectedFace objects]]}
+
+        Parameters
+        ----------
+        queue : queue.Queue()
+            The ``queue`` that the batch will be fed from. This will be a queue that loads
+            images.
+
+        Returns
+        -------
+        exhausted, bool
+            ``True`` if queue is exhausted, ``False`` if not.
+        batch, dict
+            A dictionary of lists of :attr:`~plugins.extract._base.Extractor.batchsize`.
+        """
+        exhausted = False
+        batch = dict()
+        for _ in range(self.batchsize):
+            item = self._get_item(queue)
+            if item == "EOF":
+                exhausted = True
+                break
+            for key, val in item.items():
+                batch.setdefault(key, []).append(val)
+            scaled_image, scale, pad = self._compile_detection_image(item["image"])
+            batch.setdefault("scaled_image", []).append(scaled_image)
+            batch.setdefault("scale", []).append(scale)
+            batch.setdefault("pad", []).append(pad)
+        if batch:
+            batch["scaled_image"] = np.array(batch["scaled_image"], dtype="float32")
+            logger.trace("Returning batch: %s", {k: v.shape if isinstance(v, np.ndarray) else v
+                                                 for k, v in batch.items() if k != "image"})
+        else:
+            logger.trace(item)
+        return exhausted, batch
 
     # <<< FINALIZE METHODS>>> #
-    def finalize(self, output):
-        """ This should be called as the final task of each plugin
-            Performs fianl processing and puts to the out queue """
-        if isinstance(output, dict):
-            logger.trace("Item out: %s", {key: val
-                                          for key, val in output.items()
-                                          if key != "image"})
-            # Prevent zero size faces
-            iheight, iwidth = output["image"].shape[:2]
-            output["detected_faces"] = [
-                f for f in output.get("detected_faces", list())
-                if f["right"] > 0 and f["left"] < iwidth
-                and f["bottom"] > 0 and f["top"] < iheight
-            ]
-            if self.min_size > 0 and output.get("detected_faces", None):
-                output["detected_faces"] = self.filter_small_faces(output["detected_faces"])
-        else:
-            logger.trace("Item out: %s", output)
-        self.queues["out"].put(output)
+    def finalize(self, batch):
+        """ Finalize the output from Detector
+
+        This should be called as the final task of each ``plugin``.
+
+        It strips unneeded items from the :attr:`batch` ``dict`` and performs standard final
+        processing on each item
+
+        Outputs items in the format:
+
+        >>> {'image': [<original frame>],
+        >>>  'filename': [<frame filename>),
+        >>>  'detected_faces': [<lib.faces_detect.DetectedFace objects>]}
+
+
+        Parameters
+        ----------
+        batch : dict
+            The final ``dict`` from the `plugin` process. It must contain the keys ``image``,
+            ``filename``, ``faces``
+
+        Yields
+        ------
+        dict
+            A ``dict`` for each frame containing the ``image``, ``filename`` and ``list`` of
+            ``detected_faces``
+        """
+        if not isinstance(batch, dict):
+            logger.trace("Item out: %s", batch)
+            return batch
+
+        logger.trace("Item out: %s", {k: v.shape if isinstance(v, np.ndarray) else v
+                                      for k, v in batch.items()})
+
+        batch_faces = [[self.to_detected_face(face[0], face[1], face[2], face[3])
+                        for face in faces]
+                       for faces in batch["prediction"]]
+        # Rotations
+        if any(m.any() for m in batch["rotmat"]) and any(batch_faces):
+            batch_faces = [[self._rotate_rect(face, rotmat) if rotmat.any() else face
+                            for face in faces]
+                           for faces, rotmat in zip(batch_faces, batch["rotmat"])]
+
+        # Scale back out to original frame
+        batch["detected_faces"] = [[self.to_detected_face((face.left - pad[0]) / scale,
+                                                          (face.top - pad[1]) / scale,
+                                                          (face.right - pad[0]) / scale,
+                                                          (face.bottom - pad[1]) / scale)
+                                    for face in faces]
+                                   for scale, pad, faces in zip(batch["scale"],
+                                                                batch["pad"],
+                                                                batch_faces)]
+
+        # Remove zero sized faces
+        self._remove_zero_sized_faces(batch)
+        if self.min_size > 0 and batch.get("detected_faces", None):
+            batch["detected_faces"] = self._filter_small_faces(batch["detected_faces"])
+
+        self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
+        batch = self._dict_lists_to_list_dicts(batch)
+
+        for item in batch:
+            logger.trace("final output: %s", {k: v.shape if isinstance(v, np.ndarray) else v
+                                              for k, v in item.items()})
+            yield item
 
-    def filter_small_faces(self, detected_faces):
-        """ Filter out any faces smaller than the min size threshold """
-        retval = list()
-        for face in detected_faces:
-            width = face["right"] - face["left"]
-            height = face["bottom"] - face["top"]
-            face_size = (width ** 2 + height ** 2) ** 0.5
-            if face_size < self.min_size:
-                logger.debug("Removing detected face: (face_size: %s, min_size: %s",
-                             face_size, self.min_size)
-                continue
-            retval.append(face)
-        return retval
+    @staticmethod
+    def to_detected_face(left, top, right, bottom):
+        """ Return a :class:`~lib.faces_detect.DetectedFace` object for the bounding box """
+        return DetectedFace(x=int(round(left)),
+                            w=int(round(right - left)),
+                            y=int(round(top)),
+                            h=int(round(bottom - top)))
+
+    # <<< PROTECTED ACCESS METHODS >>> #
+    # <<< PREDICT WRAPPER >>> #
+    def _predict(self, batch):
+        """ Wrap models predict function in rotations """
+        batch["rotmat"] = [np.array([]) for _ in range(len(batch["feed"]))]
+        found_faces = [np.array([]) for _ in range(len(batch["feed"]))]
+        for angle in self.rotation:
+            # Rotate the batch and insert placeholders for already found faces
+            self._rotate_batch(batch, angle)
+            batch = self.predict(batch)
+
+            if angle != 0 and any([face.any() for face in batch["prediction"]]):
+                logger.verbose("found face(s) by rotating image %s degrees", angle)
+
+            found_faces = [face if not found.any() else found
+                           for face, found in zip(batch["prediction"], found_faces)]
+
+            if all([face.any() for face in found_faces]):
+                logger.trace("Faces found for all images")
+                break
+
+        batch["prediction"] = found_faces
+        logger.trace("detect_prediction output: (filenames: %s, prediction: %s, rotmat: %s)",
+                     batch["filename"], batch["prediction"], batch["rotmat"])
+        return batch
 
     # <<< DETECTION IMAGE COMPILATION METHODS >>> #
-    def compile_detection_image(self, input_image,  # pylint:disable=too-many-arguments
-                                is_square=False, scale_up=False, to_rgb=False,
-                                to_grayscale=False, pad_to=None):
-        """ Compile the detection image """
-        image = input_image.copy()
-        if to_rgb:
-            image = image[:, :, ::-1]
-        elif to_grayscale:
-            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # pylint: disable=no-member
-        scale = self.set_scale(image, is_square=is_square, scale_up=scale_up)
-        image = self.scale_image(image, scale, pad_to)
-        if pad_to is None:
-            return [image, scale]
-        pad_left = int(pad_to[0] - int(input_image.shape[1] * scale)) // 2
-        pad_top = int(pad_to[1] - int(input_image.shape[0] * scale)) // 2
-        return [image, scale, (pad_left, pad_top)]
-
-    def set_scale(self, image, is_square=False, scale_up=False):
-        """ Set the scale factor for incoming image """
-        height, width = image.shape[:2]
-        if is_square:
-            if isinstance(self.target, int):
-                dims = (self.target ** 0.5, self.target ** 0.5)
-                self.target = dims
-            source = max(height, width)
-            target = max(self.target)
-        else:
-            source = (width * height) ** 0.5
-            if isinstance(self.target, tuple):
-                self.target = self.target[0] * self.target[1]
-            target = self.target ** 0.5
+    def _compile_detection_image(self, input_image):
+        """ Compile the detection image for feeding into the model"""
+        image = self._convert_color(input_image)
 
-        if scale_up or target < source:
-            scale = target / source
-        else:
-            scale = 1.0
-        logger.trace("Detector scale: %s", scale)
+        image_size = image.shape[:2]
+        scale = self._set_scale(image_size)
+        pad = self._set_padding(image_size, scale)
 
+        image = self._scale_image(image, image_size, scale)
+        image = self._pad_image(image)
+        logger.trace("compiled: (images shape: %s, scale: %s, pad: %s)", image.shape, scale, pad)
+        return image, scale, pad
+
+    def _set_scale(self, image_size):
+        """ Set the scale factor for incoming image """
+        scale = self.input_size / max(image_size)
+        logger.trace("Detector scale: %s", scale)
         return scale
 
+    def _set_padding(self, image_size, scale):
+        """ Set the image padding for non-square images """
+        pad_left = int(self.input_size - int(image_size[1] * scale)) // 2
+        pad_top = int(self.input_size - int(image_size[0] * scale)) // 2
+        return pad_left, pad_top
+
     @staticmethod
-    def scale_image(image, scale, pad_to=None):
+    def _scale_image(image, image_size, scale):
         """ Scale the image and optional pad to given size """
-        # pylint: disable=no-member
-        height, width = image.shape[:2]
-        interpln = cv2.INTER_LINEAR if scale > 1.0 else cv2.INTER_AREA
+        interpln = cv2.INTER_CUBIC if scale > 1.0 else cv2.INTER_AREA  # pylint:disable=no-member
         if scale != 1.0:
-            dims = (int(width * scale), int(height * scale))
-            if scale < 1.0:
-                logger.debug("Resizing image from %sx%s to %s. Scale=%s",
-                             width, height, "x".join(str(i) for i in dims), scale)
-            image = cv2.resize(image, dims, interpolation=interpln)
-        if pad_to:
-            image = Detector.pad_image(image, pad_to)
+            dims = (int(image_size[1] * scale), int(image_size[0] * scale))
+            logger.trace("Resizing detection image from %s to %s. Scale=%s",
+                         "x".join(str(i) for i in reversed(image_size)),
+                         "x".join(str(i) for i in dims), scale)
+            image = cv2.resize(image, dims, interpolation=interpln)  # pylint:disable=no-member
+        logger.trace("Resized image shape: %s", image.shape)
         return image
 
-    @staticmethod
-    def pad_image(image, target):
-        """ Pad an image to a square """
+    def _pad_image(self, image):
+        """ Pad a resized image to input size """
         height, width = image.shape[:2]
-        if width < target[0] or height < target[1]:
-            pad_l = (target[0] - width) // 2
-            pad_r = (target[0] - width) - pad_l
-            pad_t = (target[1] - height) // 2
-            pad_b = (target[1] - height) - pad_t
-            img = cv2.copyMakeBorder(  # pylint:disable=no-member
-                image, pad_t, pad_b, pad_l, pad_r,
-                cv2.BORDER_CONSTANT, (0, 0, 0)  # pylint:disable=no-member
-            )
-            return img
+        if width < self.input_size or height < self.input_size:
+            pad_l = (self.input_size - width) // 2
+            pad_r = (self.input_size - width) - pad_l
+            pad_t = (self.input_size - height) // 2
+            pad_b = (self.input_size - height) - pad_t
+            image = cv2.copyMakeBorder(  # pylint:disable=no-member
+                image,
+                pad_t,
+                pad_b,
+                pad_l,
+                pad_r,
+                cv2.BORDER_CONSTANT)  # pylint:disable=no-member
+        logger.trace("Padded image shape: %s", image.shape)
         return image
 
+    # <<< FINALIZE METHODS >>> #
+    @staticmethod
+    def _remove_zero_sized_faces(batch):
+        """ Remove items from dict where detected face is of zero size
+            or face falls entirely outside of image """
+        dims = [img.shape[:2] for img in batch["image"]]
+        logger.trace("image dims: %s", dims)
+        batch["detected_faces"] = [[face for face in faces
+                                    if face.right > 0 and face.left < dim[1]
+                                    and face.bottom > 0 and face.top < dim[0]]
+                                   for dim, faces in zip(dims,
+                                                         batch.get("detected_faces", list()))]
+
+    def _filter_small_faces(self, detected_faces):
+        """ Filter out any faces smaller than the min size threshold """
+        retval = []
+        for faces in detected_faces:
+            this_image = []
+            for face in faces:
+                face_size = (face.w ** 2 + face.h ** 2) ** 0.5
+                if face_size < self.min_size:
+                    logger.debug("Removing detected face: (face_size: %s, min_size: %s",
+                                 face_size, self.min_size)
+                    continue
+                this_image.append(face)
+            retval.append(this_image)
+        return retval
+
     # <<< IMAGE ROTATION METHODS >>> #
     @staticmethod
-    def get_rotation_angles(rotation):
+    def _get_rotation_angles(rotation):
         """ Set the rotation angles. Includes backwards compatibility for the
             'on' and 'off' options:
                 - 'on' - increment 90 degrees
@@ -259,11 +329,9 @@ class Detector():
         if rotation.lower() == "on":
             rotation_angles.extend(range(90, 360, 90))
         else:
-            passed_angles = [
-                int(angle)
-                for angle in rotation.split(",")
-                if int(angle) != 0
-            ]
+            passed_angles = [int(angle)
+                             for angle in rotation.split(",")
+                             if int(angle) != 0]
             if len(passed_angles) == 1:
                 rotation_step_size = passed_angles[0]
                 rotation_angles.extend(range(rotation_step_size,
@@ -275,106 +343,54 @@ class Detector():
         logger.debug("Rotation Angles: %s", rotation_angles)
         return rotation_angles
 
-    def rotate_image(self, image, angle):
-        """ Rotate the image by given angle and return
-            Image with rotation matrix """
+    def _rotate_batch(self, batch, angle):
+        """ Rotate images in a batch by given angle
+            if any faces have already been detected for a batch, store the existing rotation
+            matrix and replace the feed image with a placeholder """
         if angle == 0:
-            return image, None
-        return self.rotate_image_by_angle(image, angle)
+            # Set the initial batch so we always rotate from zero
+            batch["initial_feed"] = batch["feed"].copy()
+            return
+
+        retval = dict()
+        for img, faces, rotmat in zip(batch["initial_feed"], batch["prediction"], batch["rotmat"]):
+            if faces.any():
+                image = np.zeros_like(img)
+                matrix = rotmat
+            else:
+                image, matrix = self._rotate_image_by_angle(img, angle)
+            retval.setdefault("feed", []).append(image)
+            retval.setdefault("rotmat", []).append(matrix)
+        batch["feed"] = np.array(retval["feed"], dtype="float32")
+        batch["rotmat"] = retval["rotmat"]
 
     @staticmethod
-    def rotate_rect(bounding_box, rotation_matrix):
+    def _rotate_rect(bounding_box, rotation_matrix):
         """ Rotate a bounding box dict based on the rotation_matrix"""
         logger.trace("Rotating bounding box")
         bounding_box = rotate_landmarks(bounding_box, rotation_matrix)
         return bounding_box
 
-    @staticmethod
-    def rotate_image_by_angle(image, angle,
-                              rotated_width=None, rotated_height=None):
+    def _rotate_image_by_angle(self, image, angle):
         """ Rotate an image by a given angle.
             From: https://stackoverflow.com/questions/22041699 """
 
-        logger.trace("Rotating image: (angle: %s, rotated_width: %s, rotated_height: %s)",
-                     angle, rotated_width, rotated_height)
+        logger.trace("Rotating image: (image: %s, angle: %s)", image.shape, angle)
+        channels_first = image.shape[0] <= 4
+        if channels_first:
+            image = np.moveaxis(image, 0, 2)
+
         height, width = image.shape[:2]
         image_center = (width/2, height/2)
         rotation_matrix = cv2.getRotationMatrix2D(  # pylint: disable=no-member
             image_center, -1.*angle, 1.)
-        if rotated_width is None or rotated_height is None:
-            abs_cos = abs(rotation_matrix[0, 0])
-            abs_sin = abs(rotation_matrix[0, 1])
-            if rotated_width is None:
-                rotated_width = int(height*abs_sin + width*abs_cos)
-            if rotated_height is None:
-                rotated_height = int(height*abs_cos + width*abs_sin)
-        rotation_matrix[0, 2] += rotated_width/2 - image_center[0]
-        rotation_matrix[1, 2] += rotated_height/2 - image_center[1]
+        rotation_matrix[0, 2] += self.input_size / 2 - image_center[0]
+        rotation_matrix[1, 2] += self.input_size / 2 - image_center[1]
         logger.trace("Rotated image: (rotation_matrix: %s", rotation_matrix)
-        return (cv2.warpAffine(image,  # pylint: disable=no-member
+        image = cv2.warpAffine(image,  # pylint: disable=no-member
                                rotation_matrix,
-                               (rotated_width, rotated_height)),
-                rotation_matrix)
-
-    # << QUEUE METHODS >> #
-    def get_item(self):
-        """ Yield one item from the queue """
-        item = self.queues["in"].get()
-        if isinstance(item, dict):
-            logger.trace("Item in: %s", item["filename"])
-        else:
-            logger.trace("Item in: %s", item)
-        if item == "EOF":
-            logger.debug("In Queue Exhausted")
-            # Re-put EOF into queue for other threads
-            self.queues["in"].put(item)
-        return item
-
-    def get_batch(self):
-        """ Get items from the queue in batches of
-            self.batch_size
-
-            First item in output tuple indicates whether the
-            queue is exhausted.
-            Second item is the batch
-
-            Remember to put "EOF" to the out queue after processing
-            the final batch """
-        exhausted = False
-        batch = list()
-        for _ in range(self.batch_size):
-            item = self.get_item()
-            if item == "EOF":
-                exhausted = True
-                break
-            batch.append(item)
-        logger.trace("Returning batch size: %s", len(batch))
-        return (exhausted, batch)
-
-    # <<< MISC METHODS >>> #
-    def get_vram_free(self):
-        """ Return free and total VRAM on card with most VRAM free"""
-        stats = GPUStats()
-        vram = stats.get_card_most_free(supports_plaidml=self.supports_plaidml)
-        logger.verbose("Using device %s with %sMB free of %sMB",
-                       vram["device"],
-                       int(vram["free"]),
-                       int(vram["total"]))
-        return int(vram["card_id"]), int(vram["free"]), int(vram["total"])
+                               (self.input_size, self.input_size))
+        if channels_first:
+            image = np.moveaxis(image, 2, 0)
 
-    @staticmethod
-    def to_bounding_box_dict(left, top, right, bottom):
-        """ Return a dict for the bounding box """
-        return dict(left=int(round(left)),
-                    right=int(round(right)),
-                    top=int(round(top)),
-                    bottom=int(round(bottom)))
-
-    def set_predetected(self, width, height):
-        """ Set a bounding box dict for predetected faces """
-        # Predetected_face is used for sort tool.
-        # Landmarks should not be extracted again from predetected faces,
-        # because face data is lost, resulting in a large variance
-        # against extract from original image
-        logger.debug("Setting predetected face")
-        return [self.to_bounding_box_dict(0, 0, width, height)]
+        return image, rotation_matrix
diff --git a/plugins/extract/detect/cv2_dnn.py b/plugins/extract/detect/cv2_dnn.py
index 0c39a7f..7e2330b 100755
--- a/plugins/extract/detect/cv2_dnn.py
+++ b/plugins/extract/detect/cv2_dnn.py
@@ -12,84 +12,50 @@ class Detect(Detector):
         git_model_id = 4
         model_filename = ["resnet_ssd_v1.caffemodel", "resnet_ssd_v1.prototxt"]
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
-        self.target = (300, 300)  # Doesn't use VRAM
-        self.vram = 0
-        self.detector = None
+        self.name = "cv2-DNN Detector"
+        self.input_size = 300
+        self.vram = 0  # CPU Only. Doesn't use VRAM
+        self.batchsize = 1
         self.confidence = self.config["confidence"] / 100
 
-    def initialize(self, *args, **kwargs):
-        """ Calculate batch size """
-        super().initialize(*args, **kwargs)
-        logger.info("Initializing cv2 DNN Detector...")
-        logger.verbose("Using CPU for detection")
-        self.detector = cv2.dnn.readNetFromCaffe(self.model_path[1],  # pylint: disable=no-member
-                                                 self.model_path[0])
-        self.detector.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)  # pylint: disable=no-member
-        self.init.set()
-        logger.info("Initialized cv2 DNN Detector.")
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in grayscale image """
-        super().detect_faces(*args, **kwargs)
-        while True:
-            item = self.get_item()
-            if item == "EOF":
-                break
-            logger.trace("Detecting faces: %s", item["filename"])
-            [detect_image, scale] = self.compile_detection_image(item["image"],
-                                                                 is_square=True,
-                                                                 scale_up=True)
-            height, width = detect_image.shape[:2]
-            for angle in self.rotation:
-                current_image, rotmat = self.rotate_image(detect_image, angle)
-                logger.trace("Detecting faces")
-
-                blob = cv2.dnn.blobFromImage(current_image,  # pylint: disable=no-member
-                                             1.0,
-                                             self.target,
-                                             [104, 117, 123],
-                                             False,
-                                             False)
-                self.detector.setInput(blob)
-                detected = self.detector.forward()
-                faces = list()
-                for i in range(detected.shape[2]):
-                    confidence = detected[0, 0, i, 2]
-                    if confidence >= self.confidence:
-                        logger.trace("Accepting due to confidence %s >= %s",
-                                     confidence, self.confidence)
-                        faces.append([(detected[0, 0, i, 3] * width),
-                                      (detected[0, 0, i, 4] * height),
-                                      (detected[0, 0, i, 5] * width),
-                                      (detected[0, 0, i, 6] * height)])
-
-                logger.trace("Detected faces: %s", [face for face in faces])
-
-                if angle != 0 and faces:
-                    logger.verbose("found face(s) by rotating image %s degrees", angle)
-
-                if faces:
-                    break
-
-            detected_faces = self.process_output(faces, rotmat, scale)
-            item["detected_faces"] = detected_faces
-            self.finalize(item)
-
-        self.queues["out"].put("EOF")
-        logger.debug("Detecting Faces Complete")
-
-    def process_output(self, faces, rotation_matrix, scale):
+    def init_model(self):
+        """ Initialize CV2 DNN Detector Model"""
+        self.model = cv2.dnn.readNetFromCaffe(self.model_path[1],  # pylint: disable=no-member
+                                              self.model_path[0])
+        self.model.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)  # pylint: disable=no-member
+
+    def process_input(self, batch):
+        """ Compile the detection image(s) for prediction """
+        batch["feed"] = cv2.dnn.blobFromImages(batch["scaled_image"],  # pylint: disable=no-member
+                                               scalefactor=1.0,
+                                               size=(self.input_size, self.input_size),
+                                               mean=[104, 117, 123],
+                                               swapRB=False,
+                                               crop=False)
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        self.model.setInput(batch["feed"])
+        predictions = self.model.forward()
+        batch["prediction"] = self.finalize_predictions(predictions)
+        return batch
+
+    def finalize_predictions(self, predictions):
+        """ Filter faces based on confidence level """
+        faces = list()
+        for i in range(predictions.shape[2]):
+            confidence = predictions[0, 0, i, 2]
+            if confidence >= self.confidence:
+                logger.trace("Accepting due to confidence %s >= %s",
+                             confidence, self.confidence)
+                faces.append([(predictions[0, 0, i, 3] * self.input_size),
+                              (predictions[0, 0, i, 4] * self.input_size),
+                              (predictions[0, 0, i, 5] * self.input_size),
+                              (predictions[0, 0, i, 6] * self.input_size)])
+        logger.trace("faces: %s", faces)
+        return [np.array(faces)]
+
+    def process_output(self, batch):
         """ Compile found faces for output """
-        logger.trace("Processing Output: (faces: %s, rotation_matrix: %s)",
-                     faces, rotation_matrix)
-
-        faces = [self.to_bounding_box_dict(face[0], face[1], face[2], face[3]) for face in faces]
-        if isinstance(rotation_matrix, np.ndarray):
-            faces = [self.rotate_rect(face, rotation_matrix)
-                     for face in faces]
-        detected = [self.to_bounding_box_dict(face["left"] / scale, face["top"] / scale,
-                                              face["right"] / scale, face["bottom"] / scale)
-                    for face in faces]
-
-        logger.trace("Processed Output: %s", detected)
-        return detected
+        return batch
diff --git a/plugins/extract/detect/cv2_dnn_defaults.py b/plugins/extract/detect/cv2_dnn_defaults.py
index 9bee627..3402385 100755
--- a/plugins/extract/detect/cv2_dnn_defaults.py
+++ b/plugins/extract/detect/cv2_dnn_defaults.py
@@ -22,6 +22,8 @@
                    <class 'str'>, <class 'bool'>.
         default:   [required] The default value for this option.
         info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
         choices:   [optional] If this option's datatype is of <class 'str'> then valid
                    selections can be defined here. This validates the option and also enables
                    a combobox / radio option in the GUI.
@@ -60,5 +62,5 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
-    }
+    },
 }
diff --git a/plugins/extract/detect/manual.py b/plugins/extract/detect/manual.py
index 08f2771..7bd8752 100644
--- a/plugins/extract/detect/manual.py
+++ b/plugins/extract/detect/manual.py
@@ -1,32 +1,39 @@
 #!/usr/bin/env python3
 """ Manual face detection plugin """
 
-from ._base import Detector, logger
+import numpy as np
+from ._base import Detector
 
 
 class Detect(Detector):
     """ Manual Detector """
     def __init__(self, **kwargs):
         super().__init__(**kwargs)
+        self.name = "Manual"
+        self.input_size = 1440  # Arbitrary size for manual tool
+        self.vram = 0
+        self.vram_warnings = 0
+        self.vram_per_batch = 1
+        self.batchsize = 1
 
-    def initialize(self, *args, **kwargs):
-        """ Create the mtcnn detector """
-        super().initialize(*args, **kwargs)
-        logger.info("Initializing Manual Detector...")
-        self.init.set()
-        logger.info("Initialized Manual Detector.")
-
-    def detect_faces(self, *args, **kwargs):
-        """ Return the given bounding box in a bounding box dict """
-        super().detect_faces(*args, **kwargs)
-        while True:
-            item = self.get_item()
-            if item == "EOF":
-                break
-            face = item["face"]
-
-            bounding_box = [self.to_bounding_box_dict(face[0], face[1], face[2], face[3])]
-            item["detected_faces"] = bounding_box
-            self.finalize(item)
-
-        self.queues["out"].put("EOF")
+    def _compile_detection_image(self, input_image):
+        """ Override compile detection image for manual. No face is actually fed into a model """
+        return input_image, 1, (0, 0)
+
+    def init_model(self):
+        """ No model for Manual """
+        return
+
+    def process_input(self, batch):
+        """ No pre-processing for Manual. Just set a dummy feed """
+        batch["feed"] = batch["scaled_image"]
+        return batch
+
+    def predict(self, batch):
+        """ No prediction for Manual """
+        batch["prediction"] = [np.array(batch["manual_face"])]
+        return batch
+
+    def process_output(self, batch):
+        """ Post process the detected faces """
+        return batch
diff --git a/plugins/extract/detect/mtcnn.py b/plugins/extract/detect/mtcnn.py
index efa0e4a..47d036f 100755
--- a/plugins/extract/detect/mtcnn.py
+++ b/plugins/extract/detect/mtcnn.py
@@ -3,39 +3,28 @@
 
 from __future__ import absolute_import, division, print_function
 
-import os
-
-from six import string_types, iteritems
-
 import cv2
+from keras.layers import Conv2D, Dense, Flatten, Input, MaxPool2D, Permute, PReLU
+
 import numpy as np
 
-from lib.multithreading import MultiThread
+from lib.model.session import KSession
 from ._base import Detector, logger
 
 
-# Must import tensorflow inside the spawned process
-# for Windows machines
-tf = None  # pylint: disable = invalid-name
-
-
-def import_tensorflow():
-    """ Import tensorflow from inside spawned process """
-    global tf  # pylint: disable = invalid-name,global-statement
-    import tensorflow as tflow
-    tf = tflow
-
-
 class Detect(Detector):
     """ MTCNN detector for face recognition """
     def __init__(self, **kwargs):
         git_model_id = 2
-        model_filename = ["mtcnn_det_v1.1.npy", "mtcnn_det_v1.2.npy", "mtcnn_det_v1.3.npy"]
+        model_filename = ["mtcnn_det_v2.1.h5", "mtcnn_det_v2.2.h5", "mtcnn_det_v2.3.h5"]
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
-        self.kwargs = self.validate_kwargs()
-        self.name = "mtcnn"
-        self.target = 2073600  # Uses approx 1.30 GB of VRAM
+        self.name = "MTCNN"
+        self.input_size = 1440
         self.vram = 1408
+        self.vram_warnings = 512  # Will run at this with warnings
+        self.vram_per_batch = 1  # TODO implement batch support
+        self.batchsize = 1  # TODO implement batch support
+        self.kwargs = self.validate_kwargs()
 
     def validate_kwargs(self):
         """ Validate that config options are correct. If not reset to default """
@@ -55,163 +44,45 @@ class Detect(Detector):
             valid = False
 
         if not valid:
-            kwargs = {"minsize": 20,                 # minimum size of face
+            kwargs = {"minsize": 20,  # minimum size of face
                       "threshold": [0.6, 0.7, 0.7],  # three steps threshold
                       "factor": 0.709}               # scale factor
             logger.warning("Invalid MTCNN options in config. Running with defaults")
         logger.debug("Using mtcnn kwargs: %s", kwargs)
         return kwargs
 
-    def initialize(self, *args, **kwargs):
-        """ Create the mtcnn detector """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing MTCNN Detector...")
-            is_gpu = False
-
-            # Must import tensorflow inside the spawned process
-            # for Windows machines
-            import_tensorflow()
-            _, vram_free, _ = self.get_vram_free()
-            mtcnn_graph = tf.Graph()
-
-            # Windows machines sometimes misreport available vram, and overuse
-            # causing OOM. Allow growth fixes that
-            config = tf.ConfigProto()
-            config.gpu_options.allow_growth = True  # pylint: disable=no-member
-
-            with mtcnn_graph.as_default():  # pylint: disable=not-context-manager
-                sess = tf.Session(config=config)
-                with sess.as_default():  # pylint: disable=not-context-manager
-                    pnet, rnet, onet = create_mtcnn(sess, self.model_path)
-
-                if any("gpu" in str(device).lower()
-                       for device in sess.list_devices()):
-                    logger.debug("Using GPU")
-                    is_gpu = True
-            mtcnn_graph.finalize()
-
-            if not is_gpu:
-                alloc = 2048
-                logger.warning("Using CPU")
-            else:
-                alloc = vram_free
-            logger.debug("Allocated for Tensorflow: %sMB", alloc)
-
-            self.batch_size = int(alloc / self.vram)
-
-            if self.batch_size < 1:
-                self.error.set()
-                raise ValueError("Insufficient VRAM available to continue "
-                                 "({}MB)".format(int(alloc)))
-
-            logger.verbose("Processing in %s threads", self.batch_size)
-
-            self.kwargs["pnet"] = pnet
-            self.kwargs["rnet"] = rnet
-            self.kwargs["onet"] = onet
-
-            self.init.set()
-            logger.info("Initialized MTCNN Detector.")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in Multiple Threads """
-        super().detect_faces(*args, **kwargs)
-        workers = MultiThread(target=self.detect_thread, thread_count=self.batch_size)
-        workers.start()
-        workers.join()
-        sentinel = self.queues["in"].get()
-        self.queues["out"].put(sentinel)
-        logger.debug("Detecting Faces complete")
-
-    def detect_thread(self):
-        """ Detect faces in rgb image """
-        logger.debug("Launching Detect")
-        while True:
-            item = self.get_item()
-            if item == "EOF":
-                break
-            logger.trace("Detecting faces: '%s'", item["filename"])
-            [detect_image, scale] = self.compile_detection_image(item["image"], to_rgb=True)
-
-            for angle in self.rotation:
-                current_image, rotmat = self.rotate_image(detect_image, angle)
-                faces, points = detect_face(current_image, **self.kwargs)
-                if angle != 0 and faces.any():
-                    logger.verbose("found face(s) by rotating image %s degrees", angle)
-                if faces.any():
-                    break
-
-            detected_faces = self.process_output(faces, points, rotmat, scale)
-            item["detected_faces"] = detected_faces
-            self.finalize(item)
-
-        logger.debug("Thread Completed Detect")
-
-    def process_output(self, faces, points, rotation_matrix, scale):
-        """ Compile found faces for output """
-        logger.trace("Processing Output: (faces: %s, points: %s, rotation_matrix: %s)",
-                     faces, points, rotation_matrix)
-        faces = self.recalculate_bounding_box(faces, points)
-        faces = [self.to_bounding_box_dict(face[0], face[1], face[2], face[3]) for face in faces]
-        if isinstance(rotation_matrix, np.ndarray):
-            faces = [self.rotate_rect(face, rotation_matrix)
-                     for face in faces]
-        detected = [self.to_bounding_box_dict(face["left"] / scale, face["top"] / scale,
-                                              face["right"] / scale, face["bottom"] / scale)
-                    for face in faces]
-        logger.trace("Processed Output: %s", detected)
-        return detected
+    def init_model(self):
+        """ Initialize S3FD Model"""
+        self.model = MTCNN(self.model_path, **self.kwargs)
 
-    @staticmethod
-    def recalculate_bounding_box(faces, landmarks):
-        """ Recalculate the bounding box for Face Alignment.
-
-            Resize the bounding box around features to present
-            a better box to Face Alignment. Helps its chances
-            on edge cases and helps remove 'jitter' """
-        logger.trace("Recalculating Bounding Boxes: (faces: %s, landmarks: %s)",
-                     faces, landmarks)
-        retval = list()
-        no_faces = len(faces)
-        if no_faces == 0:
-            return retval
-        face_landmarks = np.hsplit(landmarks, no_faces)
-        for idx in range(no_faces):
-            pts = np.reshape(face_landmarks[idx], (5, 2), order="F")
-            nose = pts[2]
-
-            minmax = (np.amin(pts, axis=0), np.amax(pts, axis=0))
-            padding = [(minmax[1][0] - minmax[0][0]) / 2,
-                       (minmax[1][1] - minmax[0][1]) / 2]
-
-            center = (minmax[1][0] - padding[0], minmax[1][1] - padding[1])
-            offset = (center[0] - nose[0], nose[1] - center[1])
-            center = (center[0] + offset[0], center[1] + offset[1])
-
-            padding[0] += padding[0]
-            padding[1] += padding[1]
-
-            bounding = [center[0] - padding[0], center[1] - padding[1],
-                        center[0] + padding[0], center[1] + padding[1]]
-            retval.append(bounding)
-        logger.trace("Recalculated Bounding Boxes: %s", retval)
-        return retval
-
-
-# MTCNN Detector for face alignment
-# Code adapted from: https://github.com/davidsandberg/facenet
-
-# Tensorflow implementation of the face detection / alignment algorithm
+    def process_input(self, batch):
+        """ Compile the detection image(s) for prediction """
+        batch["feed"] = (batch["scaled_image"] - 127.5) / 127.5
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        prediction, points = self.model.detect_faces(batch["feed"])
+        logger.trace("filename: %s, prediction: %s, mtcnn_points: %s",
+                     batch["filename"], prediction, points)
+        batch["prediction"], batch["mtcnn_points"] = [prediction], [points]
+        return batch
+
+    def process_output(self, batch):
+        """ Post process the detected faces """
+        return batch
+
+
+# MTCNN Detector
+# Code adapted from: https://github.com/xiangrufan/keras-mtcnn
+#
+# Keras implementation of the face detection / alignment algorithm
 # found at
 # https://github.com/kpzhang93/MTCNN_face_detection_alignment
-
+#
 # MIT License
 #
-# Copyright (c) 2016 David Sandberg
+# Copyright (c) 2016 Kaipeng Zhang
 #
 # Permission is hereby granted, free of charge, to any person obtaining a copy
 # of this software and associated documentation files (the "Software"), to deal
@@ -220,8 +91,8 @@ class Detect(Detector):
 # copies of the Software, and to permit persons to whom the Software is
 # furnished to do so, subject to the following conditions:
 #
-# The above copyright notice and this permission notice shall be included in
-# all copies or substantial portions of the Software.
+# The above copyright notice and this permission notice shall be included in all
+# copies or substantial portions of the Software.
 #
 # THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
 # IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
@@ -232,311 +103,402 @@ class Detect(Detector):
 # SOFTWARE.
 
 
-def layer(operator):
-    """Decorator for composable network layers."""
+class PNet(KSession):
+    """ Keras PNet model for MTCNN """
+    def __init__(self, model_path):
+        super().__init__("MTCNN-PNet", model_path)
+        self.define_model(self.model_definition)
+        self.load_model_weights()
 
-    def layer_decorated(self, *args, **kwargs):
-        # Automatically set a name if not provided.
-        name = kwargs.setdefault('name', self.get_unique_name(operator.__name__))
-        # Figure out the layer inputs.
-        if len(self.terminals) == 0:  # pylint: disable=len-as-condition
-            raise RuntimeError('No input variables found for layer %s.' % name)
-        elif len(self.terminals) == 1:
-            layer_input = self.terminals[0]
-        else:
-            layer_input = list(self.terminals)
-        # Perform the operation and get the output.
-        layer_output = operator(self, layer_input, *args, **kwargs)
-        # Add to layer LUT.
-        self.layers[name] = layer_output
-        # This output is now the input for the next layer.
-        self.feed(layer_output)
-        # Return self for chained calls.
-        return self
-
-    return layer_decorated
-
-
-class Network():
-    """ Tensorflow Network """
-    def __init__(self, inputs, trainable=True):
-        # The input nodes for this network
-        self.inputs = inputs
-        # The current list of terminal nodes
-        self.terminals = []
-        # Mapping from layer names to layers
-        self.layers = dict(inputs)
-        # If true, the resulting variables are set as trainable
-        self.trainable = trainable
-
-        self.setup()
-
-    def setup(self):
-        """Construct the network. """
-        raise NotImplementedError('Must be implemented by the subclass.')
+    @staticmethod
+    def model_definition():
+        """ Keras PNetwork for MTCNN """
+        input_ = Input(shape=(None, None, 3))
+        var_x = Conv2D(10, (3, 3), strides=1, padding='valid', name='conv1')(input_)
+        var_x = PReLU(shared_axes=[1, 2], name='PReLU1')(var_x)
+        var_x = MaxPool2D(pool_size=2)(var_x)
+        var_x = Conv2D(16, (3, 3), strides=1, padding='valid', name='conv2')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='PReLU2')(var_x)
+        var_x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv3')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='PReLU3')(var_x)
+        classifier = Conv2D(2, (1, 1), activation='softmax', name='conv4-1')(var_x)
+        bbox_regress = Conv2D(4, (1, 1), name='conv4-2')(var_x)
+        return [input_], [classifier, bbox_regress]
+
+
+class RNet(KSession):
+    """ Keras RNet model for MTCNN """
+    def __init__(self, model_path):
+        super().__init__("MTCNN-RNet", model_path)
+        self.define_model(self.model_definition)
+        self.load_model_weights()
+
+    @staticmethod
+    def model_definition():
+        """ Keras RNetwork for MTCNN """
+        input_ = Input(shape=(24, 24, 3))
+        var_x = Conv2D(28, (3, 3), strides=1, padding='valid', name='conv1')(input_)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu1')(var_x)
+        var_x = MaxPool2D(pool_size=3, strides=2, padding='same')(var_x)
+
+        var_x = Conv2D(48, (3, 3), strides=1, padding='valid', name='conv2')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu2')(var_x)
+        var_x = MaxPool2D(pool_size=3, strides=2)(var_x)
+
+        var_x = Conv2D(64, (2, 2), strides=1, padding='valid', name='conv3')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu3')(var_x)
+        var_x = Permute((3, 2, 1))(var_x)
+        var_x = Flatten()(var_x)
+        var_x = Dense(128, name='conv4')(var_x)
+        var_x = PReLU(name='prelu4')(var_x)
+        classifier = Dense(2, activation='softmax', name='conv5-1')(var_x)
+        bbox_regress = Dense(4, name='conv5-2')(var_x)
+        return [input_], [classifier, bbox_regress]
+
+
+class ONet(KSession):
+    """ Keras ONet model for MTCNN """
+    def __init__(self, model_path):
+        super().__init__("MTCNN-ONet", model_path)
+        self.define_model(self.model_definition)
+        self.load_model_weights()
 
     @staticmethod
-    def load(model_path, session, ignore_missing=False):
-        """Load network weights.
-        model_path: The path to the numpy-serialized network weights
-        session: The current TensorFlow session
-        ignore_missing: If true, serialized weights for missing layers are
-                        ignored.
+    def model_definition():
+        """ Keras ONetwork for MTCNN """
+        input_ = Input(shape=(48, 48, 3))
+        var_x = Conv2D(32, (3, 3), strides=1, padding='valid', name='conv1')(input_)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu1')(var_x)
+        var_x = MaxPool2D(pool_size=3, strides=2, padding='same')(var_x)
+        var_x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv2')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu2')(var_x)
+        var_x = MaxPool2D(pool_size=3, strides=2)(var_x)
+        var_x = Conv2D(64, (3, 3), strides=1, padding='valid', name='conv3')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu3')(var_x)
+        var_x = MaxPool2D(pool_size=2)(var_x)
+        var_x = Conv2D(128, (2, 2), strides=1, padding='valid', name='conv4')(var_x)
+        var_x = PReLU(shared_axes=[1, 2], name='prelu4')(var_x)
+        var_x = Permute((3, 2, 1))(var_x)
+        var_x = Flatten()(var_x)
+        var_x = Dense(256, name='conv5')(var_x)
+        var_x = PReLU(name='prelu5')(var_x)
+
+        classifier = Dense(2, activation='softmax', name='conv6-1')(var_x)
+        bbox_regress = Dense(4, name='conv6-2')(var_x)
+        landmark_regress = Dense(10, name='conv6-3')(var_x)
+        return [input_], [classifier, bbox_regress, landmark_regress]
+
+
+class MTCNN():
+    """ MTCNN Detector for face alignment """
+    # TODO Batching
+
+    def __init__(self, model_path, minsize, threshold, factor):
         """
-        # pylint: disable=no-member
-        data_dict = np.load(model_path, encoding='latin1').item()
-
-        for op_name in data_dict:
-            with tf.variable_scope(op_name, reuse=True):
-                for param_name, data in iteritems(data_dict[op_name]):
-                    try:
-                        var = tf.get_variable(param_name)
-                        session.run(var.assign(data))
-                    except ValueError:
-                        if not ignore_missing:
-                            raise
-
-    def feed(self, *args):
-        """Set the input(s) for the next operation by replacing the terminal nodes.
-        The arguments can be either layer names or the actual layers.
+        minsize: minimum faces' size
+        threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
+        factor: the factor used to create a scaling pyramid of face sizes to
+                detect in the image.
+        pnet, rnet, onet: caffemodel
         """
-        assert len(args) != 0  # pylint: disable=len-as-condition
-        self.terminals = []
-        for fed_layer in args:
-            if isinstance(fed_layer, string_types):
-                try:
-                    fed_layer = self.layers[fed_layer]
-                except KeyError:
-                    raise KeyError('Unknown layer name fed: %s' % fed_layer)
-            self.terminals.append(fed_layer)
-        return self
-
-    def get_output(self):
-        """Returns the current network output."""
-        return self.terminals[-1]
-
-    def get_unique_name(self, prefix):
-        """Returns an index-suffixed unique name for the given prefix.
-        This is used for auto-generating layer names based on the type-prefix.
+        logger.debug("Initializing: %s: (model_path: '%s')",
+                     self.__class__.__name__, model_path)
+        self.minsize = minsize
+        self.threshold = threshold
+        self.factor = factor
+
+        self.pnet = PNet(model_path[0])
+        self.rnet = RNet(model_path[1])
+        self.onet = ONet(model_path[2])
+        logger.debug("Initialized: %s", self.__class__.__name__)
+
+    def detect_faces(self, batch):
+        """Detects faces in an image, and returns bounding boxes and points for them.
+        batch: input batch
         """
-        ident = sum(t.startswith(prefix) for t, _ in self.layers.items()) + 1
-        return '%s_%d' % (prefix, ident)
-
-    def make_var(self, name, shape):
-        """Creates a new TensorFlow variable."""
-        return tf.get_variable(name, shape, trainable=self.trainable)
-
-    @staticmethod
-    def validate_padding(padding):
-        """Verifies that the padding is one of the supported ones."""
-        assert padding in ('SAME', 'VALID')
-
-    @layer
-    def conv(self,  # pylint: disable=too-many-arguments
-             inp,
-             k_h,
-             k_w,
-             c_o,
-             s_h,
-             s_w,
-             name,
-             relu=True,
-             padding='SAME',
-             group=1,
-             biased=True):
-        """ Conv Layer """
+        total_boxes = np.empty((0, 9))
+        points = np.empty(0)
+        # TODO Implement batch support
+        image = batch[0]
+        origin_h, origin_w = image.shape[:2]
+        rectangles = self.detect_pnet(image, origin_h, origin_w)
+        if not rectangles:
+            return total_boxes, points
+        rectangles = self.detect_rnet(image, rectangles, origin_h, origin_w)
+        if not rectangles:
+            return total_boxes, points
+        rectangles = self.detect_onet(image, rectangles, origin_h, origin_w)
+        if rectangles:
+            total_boxes = np.array([result[:5] for result in rectangles])
+            points = np.array([result[5:] for result in rectangles]).T
+        return total_boxes, points
+
+    def detect_pnet(self, image, height, width):
         # pylint: disable=too-many-locals
-
-        # Verify that the padding is acceptable
-        self.validate_padding(padding)
-        # Get the number of channels in the input
-        c_i = int(inp.get_shape()[-1])
-        # Verify that the grouping parameter is valid
-        assert c_i % group == 0
-        assert c_o % group == 0
-        # Convolution for a given input and kernel
-        convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding) # noqa
-        with tf.variable_scope(name) as scope:
-            kernel = self.make_var('weights',
-                                   shape=[k_h, k_w, c_i // group, c_o])
-            # This is the common-case. Convolve the input without any
-            # further complications.
-            output = convolve(inp, kernel)
-            # Add the biases
-            if biased:
-                biases = self.make_var('biases', [c_o])
-                output = tf.nn.bias_add(output, biases)
-            if relu:
-                # ReLU non-linearity
-                output = tf.nn.relu(output, name=scope.name)
-            return output
-
-    @layer
-    def prelu(self, inp, name):
-        """ Prelu Layer """
-        with tf.variable_scope(name):
-            i = int(inp.get_shape()[-1])
-            alpha = self.make_var('alpha', shape=(i,))
-            output = tf.nn.relu(inp) + tf.multiply(alpha, -tf.nn.relu(-inp))
-        return output
-
-    @layer
-    def max_pool(self, inp, k_h, k_w,  # pylint: disable=too-many-arguments
-                 s_h, s_w, name, padding='SAME'):
-        """ Max Pool Layer """
-        self.validate_padding(padding)
-        return tf.nn.max_pool(inp,
-                              ksize=[1, k_h, k_w, 1],
-                              strides=[1, s_h, s_w, 1],
-                              padding=padding,
-                              name=name)
-
-    @layer
-    def fc(self, inp, num_out, name, relu=True):  # pylint: disable=invalid-name
-        """ FC Layer """
-        with tf.variable_scope(name):
-            input_shape = inp.get_shape()
-            if input_shape.ndims == 4:
-                # The input is spatial. Vectorize it first.
-                dim = 1
-                for this_dim in input_shape[1:].as_list():
-                    dim *= int(this_dim)
-                feed_in = tf.reshape(inp, [-1, dim])
-            else:
-                feed_in, dim = (inp, input_shape[-1].value)
-            weights = self.make_var('weights', shape=[dim, num_out])
-            biases = self.make_var('biases', [num_out])
-            operator = tf.nn.relu_layer if relu else tf.nn.xw_plus_b
-            fc = operator(feed_in, weights, biases, name=name)  # pylint: disable=invalid-name
-            return fc
-
-    @layer
-    def softmax(self, target, axis, name=None):  # pylint: disable=no-self-use
-        """ Multi dimensional softmax,
-            refer to https://github.com/tensorflow/tensorflow/issues/210
-            compute softmax along the dimension of target
-            the native softmax only supports batch_size x dimension """
-
-        max_axis = tf.reduce_max(target, axis, keepdims=True)
-        target_exp = tf.exp(target-max_axis)
-        normalize = tf.reduce_sum(target_exp, axis, keepdims=True)
-        softmax = tf.div(target_exp, normalize, name)
-        return softmax
-
-
-class PNet(Network):
-    """ Tensorflow PNet """
-    def setup(self):
-        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
-         .conv(3, 3, 10, 1, 1, padding='VALID', relu=False, name='conv1')
-         .prelu(name='PReLU1')
-         .max_pool(2, 2, 2, 2, name='pool1')
-         .conv(3, 3, 16, 1, 1, padding='VALID', relu=False, name='conv2')
-         .prelu(name='PReLU2')
-         .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv3')
-         .prelu(name='PReLU3')
-         .conv(1, 1, 2, 1, 1, relu=False, name='conv4-1')
-         .softmax(3, name='prob1'))
-
-        (self.feed('PReLU3')  # pylint: disable=no-value-for-parameter
-         .conv(1, 1, 4, 1, 1, relu=False, name='conv4-2'))
-
-
-class RNet(Network):
-    """ Tensorflow RNet """
-    def setup(self):
-        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
-         .conv(3, 3, 28, 1, 1, padding='VALID', relu=False, name='conv1')
-         .prelu(name='prelu1')
-         .max_pool(3, 3, 2, 2, name='pool1')
-         .conv(3, 3, 48, 1, 1, padding='VALID', relu=False, name='conv2')
-         .prelu(name='prelu2')
-         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
-         .conv(2, 2, 64, 1, 1, padding='VALID', relu=False, name='conv3')
-         .prelu(name='prelu3')
-         .fc(128, relu=False, name='conv4')
-         .prelu(name='prelu4')
-         .fc(2, relu=False, name='conv5-1')
-         .softmax(1, name='prob1'))
-
-        (self.feed('prelu4')  # pylint: disable=no-value-for-parameter
-         .fc(4, relu=False, name='conv5-2'))
-
-
-class ONet(Network):
-    """ Tensorflow ONet """
-    def setup(self):
-        (self.feed('data')  # pylint: disable=no-value-for-parameter, no-member
-         .conv(3, 3, 32, 1, 1, padding='VALID', relu=False, name='conv1')
-         .prelu(name='prelu1')
-         .max_pool(3, 3, 2, 2, name='pool1')
-         .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv2')
-         .prelu(name='prelu2')
-         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')
-         .conv(3, 3, 64, 1, 1, padding='VALID', relu=False, name='conv3')
-         .prelu(name='prelu3')
-         .max_pool(2, 2, 2, 2, name='pool3')
-         .conv(2, 2, 128, 1, 1, padding='VALID', relu=False, name='conv4')
-         .prelu(name='prelu4')
-         .fc(256, relu=False, name='conv5')
-         .prelu(name='prelu5')
-         .fc(2, relu=False, name='conv6-1')
-         .softmax(1, name='prob1'))
-
-        (self.feed('prelu5')  # pylint: disable=no-value-for-parameter
-         .fc(4, relu=False, name='conv6-2'))
-
-        (self.feed('prelu5')  # pylint: disable=no-value-for-parameter
-         .fc(10, relu=False, name='conv6-3'))
-
-
-def create_mtcnn(sess, model_path):
-    """ Create the network """
-    if not model_path:
-        model_path, _ = os.path.split(os.path.realpath(__file__))
-
-    with tf.variable_scope('pnet'):
-        data = tf.placeholder(tf.float32, (None, None, None, 3), 'input')
-        pnet = PNet({'data': data})
-        pnet.load(model_path[0], sess)
-    with tf.variable_scope('rnet'):
-        data = tf.placeholder(tf.float32, (None, 24, 24, 3), 'input')
-        rnet = RNet({'data': data})
-        rnet.load(model_path[1], sess)
-    with tf.variable_scope('onet'):
-        data = tf.placeholder(tf.float32, (None, 48, 48, 3), 'input')
-        onet = ONet({'data': data})
-        onet.load(model_path[2], sess)
-
-    pnet_fun = lambda img: sess.run(('pnet/conv4-2/BiasAdd:0', # noqa
-                                     'pnet/prob1:0'),
-                                    feed_dict={'pnet/input:0': img})
-    rnet_fun = lambda img: sess.run(('rnet/conv5-2/conv5-2:0', # noqa
-                                     'rnet/prob1:0'),
-                                    feed_dict={'rnet/input:0': img})
-    onet_fun = lambda img: sess.run(('onet/conv6-2/conv6-2:0', # noqa
-                                     'onet/conv6-3/conv6-3:0',
-                                     'onet/prob1:0'),
-                                    feed_dict={'onet/input:0': img})
-    return pnet_fun, rnet_fun, onet_fun
-
-
-def detect_face(img, minsize, pnet, rnet,  # pylint: disable=too-many-arguments
-                onet, threshold, factor):
-    """Detects faces in an image, and returns bounding boxes and points for them.
-    img: input image
-    minsize: minimum faces' size
-    pnet, rnet, onet: caffemodel
-    threshold: threshold=[th1, th2, th3], th1-3 are three steps's threshold
-    factor: the factor used to create a scaling pyramid of face sizes to
-            detect in the image.
+        """ first stage - fast proposal network (pnet) to obtain face candidates """
+        scales = calculate_scales(height, width, self.minsize, self.factor)
+        rectangles = []
+        for scale in scales:
+            scale_img = cv2.resize(image,  # pylint:disable=no-member
+                                   (int(width * scale), int(height * scale)))
+            input_ = scale_img.reshape(1, *scale_img.shape)
+            output = self.pnet.predict(input_)
+            # .transpose(0, 2, 1, 3) should be added, but this seems wrong.
+            # first 0 select cls score, second 0 = batchnum, alway=0. 1 one hot repr
+            cls_prob = output[0][0][:, :, 1]
+            roi = output[1][0]
+            out_h, out_w = cls_prob.shape
+            out_side = max(out_h, out_w)
+            cls_prob = np.swapaxes(cls_prob, 0, 1)
+            roi = np.swapaxes(roi, 0, 2)
+            rectangle = detect_face_12net(cls_prob,
+                                          roi,
+                                          out_side,
+                                          1 / scale,
+                                          width,
+                                          height,
+                                          self.threshold[0])
+            rectangles.extend(rectangle)
+        return nms(rectangles, 0.7, 'iou')
+
+    def detect_rnet(self, image, rectangles, height, width):
+        """ second stage - refinement of face candidates with rnet """
+        crop_number = 0
+        predict_24_batch = []
+        for rect in rectangles:
+            crop_img = image[int(rect[1]):int(rect[3]), int(rect[0]):int(rect[2])]
+            scale_img = cv2.resize(crop_img, (24, 24))  # pylint:disable=no-member
+            predict_24_batch.append(scale_img)
+            crop_number += 1
+
+        predict_24_batch = np.array(predict_24_batch)
+        output = self.rnet.predict(predict_24_batch)
+
+        cls_prob = output[0]  # first 0 is to select cls, second batch number, always =0
+        cls_prob = np.array(cls_prob)
+        roi_prob = output[1]  # first 0 is to select roi, second batch number, always =0
+        roi_prob = np.array(roi_prob)
+        return filter_face_24net(cls_prob, roi_prob, rectangles, width, height, self.threshold[1])
+
+    def detect_onet(self, image, rectangles, height, width):
+        """ third stage - further refinement and facial landmarks positions with onet """
+        crop_number = 0
+        predict_batch = []
+        for rect in rectangles:
+            crop_img = image[int(rect[1]):int(rect[3]), int(rect[0]):int(rect[2])]
+            scale_img = cv2.resize(crop_img, (48, 48))  # pylint:disable=no-member
+            predict_batch.append(scale_img)
+            crop_number += 1
+
+        predict_batch = np.array(predict_batch)
+
+        output = self.onet.predict(predict_batch)
+        cls_prob = output[0]
+        roi_prob = output[1]
+        pts_prob = output[2]  # index
+        return filter_face_48net(cls_prob,
+                                 roi_prob,
+                                 pts_prob,
+                                 rectangles,
+                                 width,
+                                 height,
+                                 self.threshold[2])
+
+
+def detect_face_12net(cls_prob, roi, out_side, scale, width, height, threshold):
+    # pylint: disable=too-many-locals, too-many-arguments
+    """ Detect face position and calibrate bounding box on 12net feature map(matrix version)
+    Input:
+        cls_prob : softmax feature map for face classify
+        roi      : feature map for regression
+        out_side : feature map's largest size
+        scale    : current input image scale in multi-scales
+        width    : image's origin width
+        height   : image's origin height
+        threshold: 0.6 can have 99% recall rate
+    """
+    in_side = 2*out_side+11
+    stride = 0
+    if out_side != 1:
+        stride = float(in_side-12)/(out_side-1)
+    (var_x, var_y) = np.where(cls_prob >= threshold)
+    boundingbox = np.array([var_x, var_y]).T
+    bb1 = np.fix((stride * (boundingbox) + 0) * scale)
+    bb2 = np.fix((stride * (boundingbox) + 11) * scale)
+    boundingbox = np.concatenate((bb1, bb2), axis=1)
+    dx_1 = roi[0][var_x, var_y]
+    dx_2 = roi[1][var_x, var_y]
+    dx3 = roi[2][var_x, var_y]
+    dx4 = roi[3][var_x, var_y]
+    score = np.array([cls_prob[var_x, var_y]]).T
+    offset = np.array([dx_1, dx_2, dx3, dx4]).T
+    boundingbox = boundingbox + offset*12.0*scale
+    rectangles = np.concatenate((boundingbox, score), axis=1)
+    rectangles = rect2square(rectangles)
+    pick = []
+    for rect in rectangles:
+        x_1 = int(max(0, rect[0]))
+        y_1 = int(max(0, rect[1]))
+        x_2 = int(min(width, rect[2]))
+        y_2 = int(min(height, rect[3]))
+        sc_ = rect[4]
+        if x_2 > x_1 and y_2 > y_1:
+            pick.append([x_1, y_1, x_2, y_2, sc_])
+    return nms(pick, 0.3, "iou")
+
+
+def filter_face_24net(cls_prob, roi, rectangles, width, height, threshold):
+    # pylint: disable=too-many-locals, too-many-arguments
+    """ Filter face position and calibrate bounding box on 12net's output
+    Input:
+        cls_prob  : softmax feature map for face classify
+        roi_prob  : feature map for regression
+        rectangles: 12net's predict
+        width     : image's origin width
+        height    : image's origin height
+        threshold : 0.6 can have 97% recall rate
+    Output:
+        rectangles: possible face positions
+    """
+    prob = cls_prob[:, 1]
+    pick = np.where(prob >= threshold)
+    rectangles = np.array(rectangles)
+    x_1 = rectangles[pick, 0]
+    y_1 = rectangles[pick, 1]
+    x_2 = rectangles[pick, 2]
+    y_2 = rectangles[pick, 3]
+    sc_ = np.array([prob[pick]]).T
+    dx_1 = roi[pick, 0]
+    dx_2 = roi[pick, 1]
+    dx3 = roi[pick, 2]
+    dx4 = roi[pick, 3]
+    r_width = x_2-x_1
+    r_height = y_2-y_1
+    x_1 = np.array([(x_1 + dx_1 * r_width)[0]]).T
+    y_1 = np.array([(y_1 + dx_2 * r_height)[0]]).T
+    x_2 = np.array([(x_2 + dx3 * r_width)[0]]).T
+    y_2 = np.array([(y_2 + dx4 * r_height)[0]]).T
+    rectangles = np.concatenate((x_1, y_1, x_2, y_2, sc_), axis=1)
+    rectangles = rect2square(rectangles)
+    pick = []
+    for rect in rectangles:
+        x_1 = int(max(0, rect[0]))
+        y_1 = int(max(0, rect[1]))
+        x_2 = int(min(width, rect[2]))
+        y_2 = int(min(height, rect[3]))
+        sc_ = rect[4]
+        if x_2 > x_1 and y_2 > y_1:
+            pick.append([x_1, y_1, x_2, y_2, sc_])
+    return nms(pick, 0.3, 'iou')
+
+
+def filter_face_48net(cls_prob, roi, pts, rectangles, width, height, threshold):
+    # pylint: disable=too-many-locals, too-many-arguments
+    """ Filter face position and calibrate bounding box on 12net's output
+    Input:
+        cls_prob  : cls_prob[1] is face possibility
+        roi       : roi offset
+        pts       : 5 landmark
+        rectangles: 12net's predict, rectangles[i][0:3] is the position, rectangles[i][4] is score
+        width     : image's origin width
+        height    : image's origin height
+        threshold : 0.7 can have 94% recall rate on CelebA-database
+    Output:
+        rectangles: face positions and landmarks
+    """
+    prob = cls_prob[:, 1]
+    pick = np.where(prob >= threshold)
+    rectangles = np.array(rectangles)
+    x_1 = rectangles[pick, 0]
+    y_1 = rectangles[pick, 1]
+    x_2 = rectangles[pick, 2]
+    y_2 = rectangles[pick, 3]
+    sc_ = np.array([prob[pick]]).T
+    dx_1 = roi[pick, 0]
+    dx_2 = roi[pick, 1]
+    dx3 = roi[pick, 2]
+    dx4 = roi[pick, 3]
+    r_width = x_2-x_1
+    r_height = y_2-y_1
+    pts0 = np.array([(r_width * pts[pick, 0] + x_1)[0]]).T
+    pts1 = np.array([(r_height * pts[pick, 5] + y_1)[0]]).T
+    pts2 = np.array([(r_width * pts[pick, 1] + x_1)[0]]).T
+    pts3 = np.array([(r_height * pts[pick, 6] + y_1)[0]]).T
+    pts4 = np.array([(r_width * pts[pick, 2] + x_1)[0]]).T
+    pts5 = np.array([(r_height * pts[pick, 7] + y_1)[0]]).T
+    pts6 = np.array([(r_width * pts[pick, 3] + x_1)[0]]).T
+    pts7 = np.array([(r_height * pts[pick, 8] + y_1)[0]]).T
+    pts8 = np.array([(r_width * pts[pick, 4] + x_1)[0]]).T
+    pts9 = np.array([(r_height * pts[pick, 9] + y_1)[0]]).T
+    x_1 = np.array([(x_1 + dx_1 * r_width)[0]]).T
+    y_1 = np.array([(y_1 + dx_2 * r_height)[0]]).T
+    x_2 = np.array([(x_2 + dx3 * r_width)[0]]).T
+    y_2 = np.array([(y_2 + dx4 * r_height)[0]]).T
+    rectangles = np.concatenate((x_1, y_1, x_2, y_2, sc_,
+                                 pts0, pts1, pts2, pts3, pts4, pts5, pts6, pts7, pts8, pts9),
+                                axis=1)
+    pick = []
+    for rect in rectangles:
+        x_1 = int(max(0, rect[0]))
+        y_1 = int(max(0, rect[1]))
+        x_2 = int(min(width, rect[2]))
+        y_2 = int(min(height, rect[3]))
+        if x_2 > x_1 and y_2 > y_1:
+            pick.append([x_1, y_1, x_2, y_2,
+                         rect[4], rect[5], rect[6], rect[7], rect[8], rect[9],
+                         rect[10], rect[11], rect[12], rect[13], rect[14]])
+    return nms(pick, 0.3, 'iom')
+
+
+def nms(rectangles, threshold, method):
+    # pylint:disable=too-many-locals
+    """ apply NMS(non-maximum suppression) on ROIs in same scale(matrix version)
+    Input:
+        rectangles: rectangles[i][0:3] is the position, rectangles[i][4] is score
+    Output:
+        rectangles: same as input
+    """
+    if not rectangles:
+        return rectangles
+    boxes = np.array(rectangles)
+    x_1 = boxes[:, 0]
+    y_1 = boxes[:, 1]
+    x_2 = boxes[:, 2]
+    y_2 = boxes[:, 3]
+    var_s = boxes[:, 4]
+    area = np.multiply(x_2-x_1+1, y_2-y_1+1)
+    s_sort = np.array(var_s.argsort())
+    pick = []
+    while len(s_sort) > 0:
+        # s_sort[-1] have hightest prob score, s_sort[0:-1]->others
+        xx_1 = np.maximum(x_1[s_sort[-1]], x_1[s_sort[0:-1]])
+        yy_1 = np.maximum(y_1[s_sort[-1]], y_1[s_sort[0:-1]])
+        xx_2 = np.minimum(x_2[s_sort[-1]], x_2[s_sort[0:-1]])
+        yy_2 = np.minimum(y_2[s_sort[-1]], y_2[s_sort[0:-1]])
+        width = np.maximum(0.0, xx_2 - xx_1 + 1)
+        height = np.maximum(0.0, yy_2 - yy_1 + 1)
+        inter = width * height
+        if method == 'iom':
+            var_o = inter / np.minimum(area[s_sort[-1]], area[s_sort[0:-1]])
+        else:
+            var_o = inter / (area[s_sort[-1]] + area[s_sort[0:-1]] - inter)
+        pick.append(s_sort[-1])
+        s_sort = s_sort[np.where(var_o <= threshold)[0]]
+    result_rectangle = boxes[pick].tolist()
+    return result_rectangle
+
+
+def calculate_scales(height, width, minsize, factor):
+    """ Calculate multi-scale
+        Input:
+            height: Original image height
+            width: Original image width
+            minsize: Minimum size for a face to be accepted
+            factor: Scaling factor
+        Output:
+            scales  : Multi-scale
     """
-    # pylint: disable=too-many-locals,too-many-statements,too-many-branches
     factor_count = 0
-    total_boxes = np.empty((0, 9))
-    points = np.empty(0)
-    height = img.shape[0]
-    width = img.shape[1]
     minl = np.amin([height, width])
     var_m = 12.0 / minsize
     minl = minl * var_m
@@ -546,261 +508,21 @@ def detect_face(img, minsize, pnet, rnet,  # pylint: disable=too-many-arguments
         scales += [var_m * np.power(factor, factor_count)]
         minl = minl * factor
         factor_count += 1
+    logger.trace(scales)
+    return scales
 
-    # # # # # # # # # # # # #
-    # first stage - fast proposal network (pnet) to obtain face candidates
-    # # # # # # # # # # # # #
-    for scale in scales:
-        height_scale = int(np.ceil(height * scale))
-        width_scale = int(np.ceil(width * scale))
-        im_data = imresample(img, (height_scale, width_scale))
-        im_data = (im_data - 127.5) * 0.0078125
-        img_x = np.expand_dims(im_data, 0)
-        img_y = np.transpose(img_x, (0, 2, 1, 3))
-        out = pnet(img_y)
-        out0 = np.transpose(out[0], (0, 2, 1, 3))
-        out1 = np.transpose(out[1], (0, 2, 1, 3))
-
-        boxes, _ = generate_bounding_box(out1[0, :, :, 1].copy(),
-                                         out0[0, :, :, :].copy(),
-                                         scale, threshold[0])
-
-        # inter-scale nms
-        pick = nms(boxes.copy(), 0.5, 'Union')
-        if boxes.size > 0 and pick.size > 0:
-            boxes = boxes[pick, :]
-            total_boxes = np.append(total_boxes, boxes, axis=0)
-
-    numbox = total_boxes.shape[0]
-    if numbox > 0:
-        pick = nms(total_boxes.copy(), 0.7, 'Union')
-        total_boxes = total_boxes[pick, :]
-        regw = total_boxes[:, 2]-total_boxes[:, 0]
-        regh = total_boxes[:, 3]-total_boxes[:, 1]
-        qq_1 = total_boxes[:, 0]+total_boxes[:, 5] * regw
-        qq_2 = total_boxes[:, 1]+total_boxes[:, 6] * regh
-        qq_3 = total_boxes[:, 2]+total_boxes[:, 7] * regw
-        qq_4 = total_boxes[:, 3]+total_boxes[:, 8] * regh
-        total_boxes = np.transpose(np.vstack([qq_1, qq_2, qq_3, qq_4, total_boxes[:, 4]]))
-        total_boxes = rerec(total_boxes.copy())
-        total_boxes[:, 0:4] = np.fix(total_boxes[:, 0:4]).astype(np.int32)
-        d_y, ed_y, d_x, ed_x, var_y, e_y, var_x, e_x, tmpw, tmph = pad(total_boxes.copy(),
-                                                                       width, height)
-
-    numbox = total_boxes.shape[0]
-
-    # # # # # # # # # # # # #
-    # second stage - refinement of face candidates with rnet
-    # # # # # # # # # # # # #
-
-    if numbox > 0:
-        tempimg = np.zeros((24, 24, 3, numbox))
-        for k in range(0, numbox):
-            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))
-            tmp[d_y[k] - 1:ed_y[k], d_x[k] - 1:ed_x[k], :] = img[var_y[k] - 1:e_y[k],
-                                                                 var_x[k]-1:e_x[k], :]
-            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:
-                tempimg[:, :, :, k] = imresample(tmp, (24, 24))
-            else:
-                return np.empty()
-        tempimg = (tempimg-127.5)*0.0078125
-        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))
-        out = rnet(tempimg1)
-        out0 = np.transpose(out[0])
-        out1 = np.transpose(out[1])
-        score = out1[1, :]
-        ipass = np.where(score > threshold[1])
-        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(),
-                                 np.expand_dims(score[ipass].copy(), 1)])
-        m_v = out0[:, ipass[0]]
-        if total_boxes.shape[0] > 0:
-            pick = nms(total_boxes, 0.7, 'Union')
-            total_boxes = total_boxes[pick, :]
-            total_boxes = bbreg(total_boxes.copy(), np.transpose(m_v[:, pick]))
-            total_boxes = rerec(total_boxes.copy())
-
-    numbox = total_boxes.shape[0]
-
-    # # # # # # # # # # # # #
-    # third stage - further refinement and facial landmarks positions with onet
-    # NB: Facial landmarks code commented out for faceswap
-    # # # # # # # # # # # # #
-
-    if numbox > 0:
-        # third stage
-        total_boxes = np.fix(total_boxes).astype(np.int32)
-        d_y, ed_y, d_x, ed_x, var_y, e_y, var_x, e_x, tmpw, tmph = pad(total_boxes.copy(),
-                                                                       width, height)
-        tempimg = np.zeros((48, 48, 3, numbox))
-        for k in range(0, numbox):
-            tmp = np.zeros((int(tmph[k]), int(tmpw[k]), 3))
-            tmp[d_y[k] - 1:ed_y[k], d_x[k] - 1:ed_x[k], :] = img[var_y[k] - 1:e_y[k],
-                                                                 var_x[k] - 1:e_x[k], :]
-            if tmp.shape[0] > 0 and tmp.shape[1] > 0 or tmp.shape[0] == 0 and tmp.shape[1] == 0:
-                tempimg[:, :, :, k] = imresample(tmp, (48, 48))
-            else:
-                return np.empty()
-        tempimg = (tempimg-127.5)*0.0078125
-        tempimg1 = np.transpose(tempimg, (3, 1, 0, 2))
-        out = onet(tempimg1)
-        out0 = np.transpose(out[0])
-        out1 = np.transpose(out[1])
-        out2 = np.transpose(out[2])
-        score = out2[1, :]
-        points = out1
-        ipass = np.where(score > threshold[2])
-        points = points[:, ipass[0]]
-        total_boxes = np.hstack([total_boxes[ipass[0], 0:4].copy(),
-                                 np.expand_dims(score[ipass].copy(), 1)])
-        m_v = out0[:, ipass[0]]
-
-        width = total_boxes[:, 2] - total_boxes[:, 0] + 1
-        height = total_boxes[:, 3] - total_boxes[:, 1] + 1
-        points[0:5, :] = (np.tile(width, (5, 1)) * points[0:5, :] +
-                          np.tile(total_boxes[:, 0], (5, 1)) - 1)
-        points[5:10, :] = (np.tile(height, (5, 1)) * points[5:10, :] +
-                           np.tile(total_boxes[:, 1], (5, 1)) - 1)
-        if total_boxes.shape[0] > 0:
-            total_boxes = bbreg(total_boxes.copy(), np.transpose(m_v))
-            pick = nms(total_boxes.copy(), 0.7, 'Min')
-            total_boxes = total_boxes[pick, :]
-            points = points[:, pick]
-
-    return total_boxes, points
-
-
-# function [boundingbox] = bbreg(boundingbox,reg)
-def bbreg(boundingbox, reg):
-    """Calibrate bounding boxes"""
-    if reg.shape[1] == 1:
-        reg = np.reshape(reg, (reg.shape[2], reg.shape[3]))
-
-    width = boundingbox[:, 2] - boundingbox[:, 0] + 1
-    height = boundingbox[:, 3] - boundingbox[:, 1] + 1
-    b_1 = boundingbox[:, 0] + reg[:, 0] * width
-    b_2 = boundingbox[:, 1] + reg[:, 1] * height
-    b_3 = boundingbox[:, 2] + reg[:, 2] * width
-    b_4 = boundingbox[:, 3] + reg[:, 3] * height
-    boundingbox[:, 0:4] = np.transpose(np.vstack([b_1, b_2, b_3, b_4]))
-    return boundingbox
-
-
-def generate_bounding_box(imap, reg, scale, threshold):
-    """Use heatmap to generate bounding boxes"""
-    # pylint: disable=too-many-locals
-    stride = 2
-    cellsize = 12
-
-    imap = np.transpose(imap)
-    d_x1 = np.transpose(reg[:, :, 0])
-    d_y1 = np.transpose(reg[:, :, 1])
-    d_x2 = np.transpose(reg[:, :, 2])
-    d_y2 = np.transpose(reg[:, :, 3])
-    dim_y, dim_x = np.where(imap >= threshold)
-    if dim_y.shape[0] == 1:
-        d_x1 = np.flipud(d_x1)
-        d_y1 = np.flipud(d_y1)
-        d_x2 = np.flipud(d_x2)
-        d_y2 = np.flipud(d_y2)
-    score = imap[(dim_y, dim_x)]
-    reg = np.transpose(np.vstack([d_x1[(dim_y, dim_x)], d_y1[(dim_y, dim_x)],
-                                  d_x2[(dim_y, dim_x)], d_y2[(dim_y, dim_x)]]))
-    if reg.size == 0:
-        reg = np.empty((0, 3))
-    bbox = np.transpose(np.vstack([dim_y, dim_x]))
-    q_1 = np.fix((stride * bbox + 1) / scale)
-    q_2 = np.fix((stride * bbox + cellsize - 1 + 1) / scale)
-    boundingbox = np.hstack([q_1, q_2, np.expand_dims(score, 1), reg])
-    return boundingbox, reg
-
-
-# function pick = nms(boxes,threshold,type)
-def nms(boxes, threshold, method):
-    """ Non_Max Suppression """
-    # pylint: disable=too-many-locals
-    if boxes.size == 0:
-        return np.empty((0, 3))
-    x_1 = boxes[:, 0]
-    y_1 = boxes[:, 1]
-    x_2 = boxes[:, 2]
-    y_2 = boxes[:, 3]
-    var_s = boxes[:, 4]
-    area = (x_2 - x_1 + 1) * (y_2 - y_1 + 1)
-    s_sort = np.argsort(var_s)
-    pick = np.zeros_like(var_s, dtype=np.int16)
-    counter = 0
-    while s_sort.size > 0:
-        i = s_sort[-1]
-        pick[counter] = i
-        counter += 1
-        idx = s_sort[0:-1]
-        xx_1 = np.maximum(x_1[i], x_1[idx])
-        yy_1 = np.maximum(y_1[i], y_1[idx])
-        xx_2 = np.minimum(x_2[i], x_2[idx])
-        yy_2 = np.minimum(y_2[i], y_2[idx])
-        width = np.maximum(0.0, xx_2-xx_1+1)
-        height = np.maximum(0.0, yy_2-yy_1+1)
-        inter = width * height
-        if method == 'Min':
-            var_o = inter / np.minimum(area[i], area[idx])
-        else:
-            var_o = inter / (area[i] + area[idx] - inter)
-        s_sort = s_sort[np.where(var_o <= threshold)]
-    pick = pick[0:counter]
-    return pick
-
-
-# function [d_y ed_y d_x ed_x y e_y x e_x tmp_width tmp_height] = pad(total_boxes,width,height)
-def pad(total_boxes, width, height):
-    """Compute the padding coordinates (pad the bounding boxes to square)"""
-    tmp_width = (total_boxes[:, 2] - total_boxes[:, 0] + 1).astype(np.int32)
-    tmp_height = (total_boxes[:, 3] - total_boxes[:, 1] + 1).astype(np.int32)
-    numbox = total_boxes.shape[0]
-
-    d_x = np.ones((numbox), dtype=np.int32)
-    d_y = np.ones((numbox), dtype=np.int32)
-    ed_x = tmp_width.copy().astype(np.int32)
-    ed_y = tmp_height.copy().astype(np.int32)
-
-    dim_x = total_boxes[:, 0].copy().astype(np.int32)
-    dim_y = total_boxes[:, 1].copy().astype(np.int32)
-    e_x = total_boxes[:, 2].copy().astype(np.int32)
-    e_y = total_boxes[:, 3].copy().astype(np.int32)
-
-    tmp = np.where(e_x > width)
-    ed_x.flat[tmp] = np.expand_dims(-e_x[tmp] + width + tmp_width[tmp], 1)
-    e_x[tmp] = width
-
-    tmp = np.where(e_y > height)
-    ed_y.flat[tmp] = np.expand_dims(-e_y[tmp] + height + tmp_height[tmp], 1)
-    e_y[tmp] = height
-
-    tmp = np.where(dim_x < 1)
-    d_x.flat[tmp] = np.expand_dims(2 - dim_x[tmp], 1)
-    dim_x[tmp] = 1
-
-    tmp = np.where(dim_y < 1)
-    d_y.flat[tmp] = np.expand_dims(2 - dim_y[tmp], 1)
-    dim_y[tmp] = 1
-
-    return d_y, ed_y, d_x, ed_x, dim_y, e_y, dim_x, e_x, tmp_width, tmp_height
-
-
-# function [bbox_a] = rerec(bbox_a)
-def rerec(bbox_a):
-    """Convert bbox_a to square."""
-    height = bbox_a[:, 3]-bbox_a[:, 1]
-    width = bbox_a[:, 2]-bbox_a[:, 0]
-    length = np.maximum(width, height)
-    bbox_a[:, 0] = bbox_a[:, 0] + width * 0.5 - length * 0.5
-    bbox_a[:, 1] = bbox_a[:, 1] + height * 0.5 - length * 0.5
-    bbox_a[:, 2:4] = bbox_a[:, 0:2] + np.transpose(np.tile(length, (2, 1)))
-    return bbox_a
-
-
-def imresample(img, size):
-    """ Resample image """
-    # pylint: disable=no-member
-    im_data = cv2.resize(img, (size[1], size[0]),
-                         interpolation=cv2.INTER_AREA)  # @UndefinedVariable
-    return im_data
+
+def rect2square(rectangles):
+    """ change rectangles into squares (matrix version)
+    Input:
+        rectangles: rectangles[i][0:3] is the position, rectangles[i][4] is score
+    Output:
+        squares: same as input
+    """
+    width = rectangles[:, 2] - rectangles[:, 0]
+    height = rectangles[:, 3] - rectangles[:, 1]
+    length = np.maximum(width, height).T
+    rectangles[:, 0] = rectangles[:, 0] + width * 0.5 - length * 0.5
+    rectangles[:, 1] = rectangles[:, 1] + height * 0.5 - length * 0.5
+    rectangles[:, 2:4] = rectangles[:, 0:2] + np.repeat([length], 2, axis=0).T
+    return rectangles
diff --git a/plugins/extract/detect/mtcnn_defaults.py b/plugins/extract/detect/mtcnn_defaults.py
index 6692527..e50778d 100755
--- a/plugins/extract/detect/mtcnn_defaults.py
+++ b/plugins/extract/detect/mtcnn_defaults.py
@@ -22,6 +22,8 @@
                    <class 'str'>, <class 'bool'>.
         default:   [required] The default value for this option.
         info:      [required] A string describing what this option does.
+        group:     [optional]. A group for grouping options together in the GUI. If not
+                   provided this will not group this option with any others.
         choices:   [optional] If this option's datatype is of <class 'str'> then valid
                    selections can be defined here. This validates the option and also enables
                    a combobox / radio option in the GUI.
diff --git a/plugins/extract/detect/s3fd.py b/plugins/extract/detect/s3fd.py
index 88b2f7d..238a11a 100644
--- a/plugins/extract/detect/s3fd.py
+++ b/plugins/extract/detect/s3fd.py
@@ -7,189 +7,244 @@ https://github.com/1adrianb/face-alignment
 """
 
 from scipy.special import logsumexp
-
 import numpy as np
+import keras
+import keras.backend as K
 
-from lib.multithreading import MultiThread
+from lib.model.session import KSession
 from ._base import Detector, logger
 
 
 class Detect(Detector):
     """ S3FD detector for face recognition """
     def __init__(self, **kwargs):
-        git_model_id = 3
-        model_filename = "s3fd_v1.pb"
+        git_model_id = 11
+        model_filename = "s3fd_keras_v1.h5"
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
-        self.name = "s3fd"
-        self.target = (640, 640)  # Uses approx 4 GB of VRAM
-        self.vram = 4224
-        self.min_vram = 1024  # Will run at this with warnings
-        self.model = None
-
-    def initialize(self, *args, **kwargs):
-        """ Create the s3fd detector """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing S3FD Detector...")
-            card_id, vram_free, vram_total = self.get_vram_free()
-            if vram_free <= self.vram:
-                tf_ratio = 1.0
+        self.name = "S3FD"
+        self.input_size = 640
+        self.vram = 4096
+        self.vram_warnings = 1024  # Will run at this with warnings
+        self.vram_per_batch = 128
+        self.batchsize = self.config["batch-size"]
+
+    def init_model(self):
+        """ Initialize S3FD Model"""
+        confidence = self.config["confidence"] / 100
+        model_kwargs = dict(custom_objects=dict(O2K_Add=O2K_Add,
+                                                O2K_Slice=O2K_Slice,
+                                                O2K_Sum=O2K_Sum,
+                                                O2K_Sqrt=O2K_Sqrt,
+                                                O2K_Pow=O2K_Pow,
+                                                O2K_ConstantLayer=O2K_ConstantLayer,
+                                                O2K_Div=O2K_Div))
+        self.model = S3fd(self.model_path, model_kwargs, confidence)
+
+    def process_input(self, batch):
+        """ Compile the detection image(s) for prediction """
+        batch["feed"] = self.model.prepare_batch(batch["scaled_image"])
+        return batch
+
+    def predict(self, batch):
+        """ Run model to get predictions """
+        predictions = self.model.predict(batch["feed"])
+        batch["prediction"] = self.model.finalize_predictions(predictions)
+        logger.trace("filename: %s, prediction: %s", batch["filename"], batch["prediction"])
+        return batch
+
+    def process_output(self, batch):
+        """ Compile found faces for output """
+        return batch
+
+
+################################################################################
+# CUSTOM KERAS LAYERS
+# generated by onnx2keras
+################################################################################
+class O2K_ElementwiseLayer(keras.engine.Layer):
+    def __init__(self, **kwargs):
+        super(O2K_ElementwiseLayer, self).__init__(**kwargs)
+
+    def call(self, *args):
+        raise NotImplementedError()
+
+    def compute_output_shape(self, input_shape):
+        # TODO: do this nicer
+        ldims = len(input_shape[0])
+        rdims = len(input_shape[1])
+        if ldims > rdims:
+            return input_shape[0]
+        if rdims > ldims:
+            return input_shape[1]
+        lprod = np.prod(list(filter(bool, input_shape[0])))
+        rprod = np.prod(list(filter(bool, input_shape[1])))
+        return input_shape[0 if lprod > rprod else 1]
+
+
+class O2K_Add(O2K_ElementwiseLayer):
+    def call(self, x, *args):
+        return x[0] + x[1]
+
+
+class O2K_Slice(keras.engine.Layer):
+    def __init__(self, starts, ends, axes=None, steps=None, **kwargs):
+        self._starts = starts
+        self._ends = ends
+        self._axes = axes
+        self._steps = steps
+        super(O2K_Slice, self).__init__(**kwargs)
+
+    def get_config(self):
+        config = super(O2K_Slice, self).get_config()
+        config.update({
+            'starts': self._starts, 'ends': self._ends,
+            'axes': self._axes, 'steps': self._steps
+        })
+        return config
+
+    def get_slices(self, ndims):
+        axes = self._axes
+        steps = self._steps
+        if axes is None:
+            axes = tuple(range(ndims))
+        if steps is None:
+            steps = (1,) * len(axes)
+        assert len(axes) == len(steps) == len(self._starts) == len(self._ends)
+        return list(zip(axes, self._starts, self._ends, steps))
+
+    def compute_output_shape(self, input_shape):
+        input_shape = list(input_shape)
+        for ax, start, end, steps in self.get_slices(len(input_shape)):
+            size = input_shape[ax]
+            if ax == 0:
+                raise AttributeError("Can not slice batch axis.")
+            if size is None:
+                if start < 0 or end < 0:
+                    raise AttributeError("Negative slices not supported on symbolic axes")
+                logger.warning("Slicing symbolic axis might lead to problems.")
+                input_shape[ax] = (end - start) // steps
+                continue
+            if start < 0:
+                start = size - start
+            if end < 0:
+                end = size - end
+            input_shape[ax] = (min(size, end) - start) // steps
+        return tuple(input_shape)
+
+    def call(self, x, *args):
+        ax_map = dict((x[0], slice(*x[1:])) for x in self.get_slices(K.ndim(x)))
+        shape = K.int_shape(x)
+        slices = [(ax_map[a] if a in ax_map else slice(None)) for a in range(len(shape))]
+        x = x[tuple(slices)]
+        return x
+
+
+class O2K_ReduceLayer(keras.engine.Layer):
+    def __init__(self, axes=None, keepdims=True, **kwargs):
+        self._axes = [axes] if isinstance(axes, int) else axes
+        self._keepdims = bool(keepdims)
+        super(O2K_ReduceLayer, self).__init__(**kwargs)
+
+    def get_config(self):
+        config = super(O2K_ReduceLayer, self).get_config()
+        config.update({
+            'axes': self._axes,
+            'keepdims': self._keepdims
+        })
+        return config
+
+    def compute_output_shape(self, input_shape):
+        if self._axes is None:
+            return (1,)*len(input_shape) if self._keepdims else tuple()
+        ret = list(input_shape)
+        for i in sorted(self._axes, reverse=True):
+            if self._keepdims:
+                ret[i] = 1
             else:
-                tf_ratio = self.vram / vram_total
+                ret.pop(i)
+        return tuple(ret)
 
-            logger.verbose("Reserving %s%% of total VRAM per s3fd thread",
-                           round(tf_ratio * 100, 2))
+    def call(self, x, *args):
+        raise NotImplementedError()
 
-            confidence = self.config["confidence"] / 100
-            self.model = S3fd(self.model_path, self.target, tf_ratio, card_id, confidence)
 
-            if not self.model.is_gpu:
-                alloc = 2048
-                logger.warning("Using CPU")
-            else:
-                logger.debug("Using GPU")
-                alloc = vram_free
-            logger.debug("Allocated for Tensorflow: %sMB", alloc)
-
-            if self.min_vram < alloc < self.vram:
-                self.batch_size = 1
-                logger.warning("You are running s3fd with %sMB VRAM. The model is optimized for "
-                               "%sMB VRAM. Detection should still run but you may get "
-                               "warnings/errors", int(alloc), self.vram)
-            else:
-                self.batch_size = int(alloc / self.vram)
-            if self.batch_size < 1:
-                raise ValueError("Insufficient VRAM available to continue "
-                                 "({}MB)".format(int(alloc)))
-
-            logger.verbose("Processing in %s threads", self.batch_size)
-
-            self.init.set()
-            logger.info("Initialized S3FD Detector.")
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in Multiple Threads """
-        super().detect_faces(*args, **kwargs)
-        workers = MultiThread(target=self.detect_thread, thread_count=self.batch_size)
-        workers.start()
-        workers.join()
-        sentinel = self.queues["in"].get()
-        self.queues["out"].put(sentinel)
-        logger.debug("Detecting Faces complete")
-
-    def detect_thread(self):
-        """ Detect faces in rgb image """
-        logger.debug("Launching Detect")
-        while True:
-            item = self.get_item()
-            if item == "EOF":
-                break
-            logger.trace("Detecting faces: '%s'", item["filename"])
-            detect_image, scale = self.compile_detection_image(item["image"], is_square=True)
-            for angle in self.rotation:
-                current_image, rotmat = self.rotate_image(detect_image, angle)
-                faces = self.model.detect_face(current_image)
-                if angle != 0 and faces.any():
-                    logger.verbose("found face(s) by rotating image %s degrees", angle)
-                if faces.any():
-                    break
-
-            detected_faces = self.process_output(faces, rotmat, scale)
-            item["detected_faces"] = detected_faces
-            self.finalize(item)
-
-        logger.debug("Thread Completed Detect")
-
-    def process_output(self, faces, rotation_matrix, scale):
-        """ Compile found faces for output """
-        logger.trace("Processing Output: (faces: %s, rotation_matrix: %s)", faces, rotation_matrix)
-        faces = [self.to_bounding_box_dict(face[0], face[1], face[2], face[3]) for face in faces]
-        if isinstance(rotation_matrix, np.ndarray):
-            faces = [self.rotate_rect(face, rotation_matrix)
-                     for face in faces]
-        detected = [self.to_bounding_box_dict(face["left"] / scale, face["top"] / scale,
-                                              face["right"] / scale, face["bottom"] / scale)
-                    for face in faces]
-        logger.trace("Processed Output: %s", detected)
-        return detected
-
-
-class S3fd():
-    """ Tensorflow Network """
-    def __init__(self, model_path, target_size, vram_ratio, card_id, confidence):
-        logger.debug("Initializing: %s: (model_path: '%s', target_size: %s, vram_ratio: %s, "
-                     "card_id: %s)",
-                     self.__class__.__name__, model_path, target_size, vram_ratio, card_id)
-        # Must import tensorflow inside the spawned process for Windows machines
-        import tensorflow as tf
-        self.is_gpu = False
-        self.tf = tf  # pylint: disable=invalid-name
-        self.model_path = model_path
+class O2K_Sum(O2K_ReduceLayer):
+    def call(self, x, *args):
+        return K.sum(x, self._axes, self._keepdims)
+
+
+class O2K_Sqrt(keras.engine.Layer):
+    def call(self, x, *args):
+        return K.sqrt(x)
+
+
+class O2K_Pow(keras.engine.Layer):
+    def call(self, x, *args):
+        return K.pow(*x)
+
+
+class O2K_ConstantLayer(keras.engine.Layer):
+    def __init__(self, constant_obj, dtype, **kwargs):
+        self._dtype = np.dtype(dtype).name
+        self._constant = np.array(constant_obj, dtype=self._dtype)
+        super(O2K_ConstantLayer, self).__init__(**kwargs)
+
+    def call(self, *args):
+        # pylint:disable=arguments-differ
+        data = K.constant(self._constant, dtype=self._dtype)
+        return data
+
+    def compute_output_shape(self, input_shape):
+        return self._constant.shape
+
+    def get_config(self):
+        config = super(O2K_ConstantLayer, self).get_config()
+        config.update({
+            'constant_obj': self._constant,
+            'dtype': self._dtype
+        })
+        return config
+
+
+class O2K_Div(O2K_ElementwiseLayer):
+    # pylint:disable=arguments-differ
+    def call(self, x, *args):
+        return x[0] / x[1]
+
+
+class S3fd(KSession):
+    """ Keras Network """
+    def __init__(self, model_path, model_kwargs, confidence):
+        logger.debug("Initializing: %s: (model_path: '%s')",
+                     self.__class__.__name__, model_path)
+        super().__init__("S3FD", model_path, model_kwargs)
+        self.load_model()
         self.confidence = confidence
-        self.graph = self.load_graph()
-        self.input = self.graph.get_tensor_by_name("s3fd/input_1:0")
-        self.output = self.get_outputs()
-        self.session = self.set_session(target_size, vram_ratio, card_id)
         logger.debug("Initialized: %s", self.__class__.__name__)
 
-    def load_graph(self):
-        """ Load the tensorflow Model and weights """
-        # pylint: disable=not-context-manager
-        logger.verbose("Initializing S3FD Network model...")
-        with self.tf.gfile.GFile(self.model_path, "rb") as gfile:
-            graph_def = self.tf.GraphDef()
-            graph_def.ParseFromString(gfile.read())
-        fa_graph = self.tf.Graph()
-        with fa_graph.as_default():
-            self.tf.import_graph_def(graph_def, name="s3fd")
-        return fa_graph
-
-    def get_outputs(self):
-        """ Return the output tensors """
-        tensor_names = ["concat_31", "transpose_72", "transpose_75", "transpose_78",
-                        "transpose_81", "transpose_84", "transpose_87", "transpose_90",
-                        "transpose_93", "transpose_96", "transpose_99", "transpose_102"]
-        logger.debug("tensor_names: %s", tensor_names)
-        tensors = [self.graph.get_tensor_by_name("s3fd/{}:0".format(t_name))
-                   for t_name in tensor_names]
-        logger.debug("tensors: %s", tensors)
-        return tensors
-
-    def set_session(self, target_size, vram_ratio, card_id):
-        """ Set the TF Session and initialize """
-        # pylint: disable=not-context-manager, no-member
-        placeholder = np.zeros((1, 3, target_size[0], target_size[1]))
-        config = self.tf.ConfigProto()
-        if card_id != -1:
-            config.gpu_options.visible_device_list = str(card_id)
-        if vram_ratio != 1.0:
-            config.gpu_options.per_process_gpu_memory_fraction = vram_ratio
-
-        with self.graph.as_default():
-            session = self.tf.Session(config=config)
-            self.is_gpu = any("gpu" in str(device).lower() for device in session.list_devices())
-            session.run(self.output, feed_dict={self.input: placeholder})
-        return session
-
-    def detect_face(self, feed_item):
-        """ Detect faces """
-        feed_item = feed_item - np.array([104.0, 117.0, 123.0])
-        feed_item = feed_item.transpose(2, 0, 1)
-        feed_item = feed_item.reshape((1,) + feed_item.shape).astype('float32')
-        bboxlist = self.session.run(self.output, feed_dict={self.input: feed_item})
-        bboxlist = self.post_process(bboxlist)
-
-        keep = self.nms(bboxlist, 0.3)
-        bboxlist = bboxlist[keep, :]
-        bboxlist = [x for x in bboxlist if x[-1] >= self.confidence]
+    @staticmethod
+    def prepare_batch(batch):
+        """ Prepare a batch for prediction """
+        batch = batch - np.array([104.0, 117.0, 123.0])
+        batch = batch.transpose(0, 3, 1, 2)
+        return batch
 
-        return np.array(bboxlist)
+    def finalize_predictions(self, bboxlists):
+        """ Detect faces """
+        ret = list()
+        for i in range(bboxlists[0].shape[0]):
+            bboxlist = [x[i:i+1, ...] for x in bboxlists]
+            bboxlist = self.post_process(bboxlist)
+            keep = self.nms(bboxlist, 0.3)
+            bboxlist = bboxlist[keep, :]
+            bboxlist = [x for x in bboxlist if x[-1] >= self.confidence]
+            ret.append(np.array(bboxlist))
+        return ret
 
     def post_process(self, bboxlist):
-        """ Perform post processing on output """
+        """ Perform post processing on output
+            TODO: do this on the batch.
+        """
         retval = list()
         for i in range(len(bboxlist) // 2):
             bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=1)
@@ -238,6 +293,7 @@ class S3fd():
 
     @staticmethod
     def nms(dets, thresh):
+        # pylint:disable=too-many-locals
         """ Perform Non-Maximum Suppression """
         keep = list()
         if len(dets) == 0:
diff --git a/plugins/extract/detect/s3fd_amd.py b/plugins/extract/detect/s3fd_amd.py
deleted file mode 100644
index 4dbcc06..0000000
--- a/plugins/extract/detect/s3fd_amd.py
+++ /dev/null
@@ -1,492 +0,0 @@
-#!/usr/bin/env python3
-""" S3FD Face detection plugin
-https://arxiv.org/abs/1708.05237
-
-Adapted from S3FD Port in FAN:
-https://github.com/1adrianb/face-alignment
-"""
-
-from scipy.special import logsumexp
-import numpy as np
-from ._base import Detector, logger
-import keras
-import keras.backend as K
-from lib.multithreading import FSThread
-from lib.queue_manager import queue_manager
-import queue
-from os.path import basename
-
-
-class Detect(Detector):
-    """ S3FD detector for face recognition """
-    def __init__(self, **kwargs):
-        git_model_id = 11
-        model_filename = "s3fd_keras_v1.h5"
-        super().__init__(
-            git_model_id=git_model_id, model_filename=model_filename,
-            **kwargs
-        )
-        self.name = "s3fd_amd"
-        self.target = (640, 640)  # Uses approx 4 GB of VRAM
-        self.vram = 4096
-        self.min_vram = 1024  # Will run at this with warnings
-        self.model = None
-        self.got_input_eof = False
-        self.rotate_queue = None  # set in the detect_faces method
-        self.supports_plaidml = True
-
-    def initialize(self, *args, **kwargs):
-        """ Create the s3fd detector """
-        try:
-            super().initialize(*args, **kwargs)
-            logger.info("Initializing S3FD-AMD Detector...")
-            confidence = self.config["confidence"] / 100
-            self.batch_size = self.config["batch-size"]
-            self.model = S3fd_amd(self.model_path, self.target, confidence)
-            self.init.set()
-            logger.info(
-                "Initialized S3FD-AMD Detector with batchsize of %i.", self.batch_size
-            )
-        except Exception as err:
-            self.error.set()
-            raise err
-
-    def post_processing_thread(self, in_queue, again_queue):
-        # If -r is set we move images without found faces and remaining
-        # rotations to a queue which is "merged" with the intial input queue.
-        # This also means it is possible that we get data after an EOF.
-        # This is handled by counting open rotation jobs and propagating
-        # a second EOF as soon as we are really done through
-        # the preprocsessing thread (detect_faces) and the prediction thread.
-        open_rot_jobs = 0
-        got_first_eof = False
-        while True:
-            job = in_queue.get()
-            if job == "EOF":
-                logger.debug("S3fd-amd post processing got EOF")
-                got_first_eof = True
-            else:
-                predictions, items = job
-                bboxes = self.model.finalize_predictions(predictions)
-                for bbox, item in zip(bboxes, items):
-                    s3fd_opts = item["_s3fd"]
-                    detected_faces = self.process_output(bbox, s3fd_opts)
-                    did_rotation = s3fd_opts["rotations"].pop(0) != 0
-                    if detected_faces:
-                        item["detected_faces"] = detected_faces
-                        del item["_s3fd"]
-                        self.finalize(item)
-                        if did_rotation:
-                            open_rot_jobs -= 1
-                            logger.trace("Found face after rotation.")
-                    elif s3fd_opts["rotations"]:  # we have remaining rotations
-                        logger.trace("No face detected, remaining rotations: %s", s3fd_opts["rotations"])
-                        if not did_rotation:
-                            open_rot_jobs += 1
-                        logger.trace("Rotate face %s and try again.", item["filename"])
-                        again_queue.put(item)
-                    else:
-                        logger.debug("No face detected for %s.", item["filename"])
-                        open_rot_jobs -= 1
-                        item["detected_faces"] = []
-                        del item["_s3fd"]
-                        self.finalize(item)
-            if got_first_eof and open_rot_jobs <= 0:
-                logger.debug("Sending second EOF")
-                again_queue.put("EOF")
-                self.finalize("EOF")
-                break
-
-    def prediction_thread(self, in_queue, out_queue):
-        got_first_eof = False
-        while True:
-            job = in_queue.get()
-            if job == "EOF":
-                logger.debug("S3fd-amd prediction processing got EOF")
-                if got_first_eof:
-                    break
-                out_queue.put(job)
-                got_first_eof = True
-                continue
-            batch, items = job
-            predictions = self.model.predict(batch)
-            out_queue.put((predictions, items))
-
-    def detect_faces(self, *args, **kwargs):
-        """ Detect faces in rgb image """
-        super().detect_faces(*args, **kwargs)
-        self.rotate_queue = queue_manager.get_queue("s3fd_rotate", 8, False)
-        prediction_queue = queue_manager.get_queue("s3fd_pred", 8, False)
-        post_queue = queue_manager.get_queue("s3fd_post", 8, False)
-        worker = FSThread(
-            target=self.prediction_thread, args=(prediction_queue, post_queue)
-        )
-        post_worker = FSThread(
-            target=self.post_processing_thread, args=(post_queue, self.rotate_queue)
-        )
-        worker.start()
-        post_worker.start()
-
-        got_first_eof = False
-        while True:
-            worker.check_and_raise_error()
-            post_worker.check_and_raise_error()
-            got_eof, in_batch = self.get_batch()
-            batch = list()
-            for item in in_batch:
-                s3fd_opts = item.setdefault("_s3fd", {})
-                if "scaled_img" not in s3fd_opts:
-                    logger.trace("Resizing %s" % basename(item["filename"]))
-                    detect_image, scale, pads = self.compile_detection_image(
-                        item["image"], is_square=True, pad_to=self.target
-                    )
-                    s3fd_opts["scale"] = scale
-                    s3fd_opts["pads"] = pads
-                    s3fd_opts["rotations"] = list(self.rotation)
-                    s3fd_opts["rotmatrix"] = None  # the first "rotation" is always 0
-                    img = s3fd_opts["scaled_img"] = detect_image
-                else:
-                    logger.trace("Rotating %s" % basename(item["filename"]))
-                    angle = s3fd_opts["rotations"][0]
-                    img, rotmat = self.rotate_image_by_angle(
-                        s3fd_opts["scaled_img"], angle, *self.target
-                    )
-                    s3fd_opts["rotmatrix"] = rotmat
-                batch.append((img, item))
-
-            if batch:
-                batch_data = np.array([x[0] for x in batch], dtype="float32")
-                batch_data = self.model.prepare_batch(batch_data)
-                batch_items = [x[1] for x in batch]
-                prediction_queue.put((batch_data, batch_items))
-
-            if got_eof:
-                logger.debug("S3fd-amd main worker got EOF")
-                prediction_queue.put("EOF")
-                # Required to prevent hanging when less then BS items are in the
-                # again queue and we won't receive new images.
-                self.batch_size = 1
-                if got_first_eof:
-                    break
-                got_first_eof = True
-
-        logger.debug("Joining s3fd-amd worker")
-        worker.join()
-        post_worker.join()
-        for qname in ():
-            queue_manager.del_queue(qname)
-        logger.debug("Detecting Faces complete")
-
-    def process_output(self, faces, opts):
-        """ Compile found faces for output """
-        logger.trace(
-            "Processing Output: (faces: %s, rotation_matrix: %s)",
-            faces, opts["rotmatrix"]
-        )
-        detected = []
-        scale = opts["scale"]
-        pad_l, pad_t = opts["pads"]
-        rot = opts["rotmatrix"]
-        for face in faces:
-            face = self.to_bounding_box_dict(face[0], face[1], face[2], face[3])
-            if isinstance(rot, np.ndarray):
-                face = self.rotate_rect(face, rot)
-            face = self.to_bounding_box_dict(
-                (face["left"] - pad_l) / scale,
-                (face["top"] - pad_t) / scale,
-                (face["right"] - pad_l) / scale,
-                (face["bottom"] - pad_t) / scale
-            )
-            detected.append(face)
-        logger.trace("Processed Output: %s", detected)
-        return detected
-
-    def get_item(self):
-        """
-        Yield one item from the input or rotation
-        queue while prioritizing rotation queue to
-        prevent deadlocks.
-        """
-        try:
-            item = self.rotate_queue.get(block=self.got_input_eof)
-            return item
-        except queue.Empty:
-            pass
-        item = super(Detect, self).get_item()
-        if not isinstance(item, dict) and item == "EOF":
-            self.got_input_eof = True
-        return item
-
-
-################################################################################
-# CUSTOM KERAS LAYERS
-# generated by onnx2keras
-################################################################################
-class O2K_ElementwiseLayer(keras.engine.Layer):
-    def __init__(self, **kwargs):
-        super(O2K_ElementwiseLayer, self).__init__(**kwargs)
-
-    def call(self, *args):
-        raise NotImplementedError()
-
-    def compute_output_shape(self, input_shape):
-        # TODO: do this nicer
-        ldims = len(input_shape[0])
-        rdims = len(input_shape[1])
-        if ldims > rdims:
-            return input_shape[0]
-        if rdims > ldims:
-            return input_shape[1]
-        lprod = np.prod(list(filter(bool, input_shape[0])))
-        rprod = np.prod(list(filter(bool, input_shape[1])))
-        return input_shape[0 if lprod > rprod else 1]
-
-
-class O2K_Add(O2K_ElementwiseLayer):
-    def call(self, x, *args):
-        return x[0] + x[1]
-
-
-class O2K_Slice(keras.engine.Layer):
-    def __init__(self, starts, ends, axes=None, steps=None, **kwargs):
-        self._starts = starts
-        self._ends = ends
-        self._axes = axes
-        self._steps = steps
-        super(O2K_Slice, self).__init__(**kwargs)
-
-    def get_config(self):
-        config = super(O2K_Slice, self).get_config()
-        config.update({
-            'starts': self._starts, 'ends': self._ends,
-            'axes': self._axes, 'steps': self._steps
-        })
-        return config
-
-    def get_slices(self, ndims):
-        axes = self._axes
-        steps = self._steps
-        if axes is None:
-            axes = tuple(range(ndims))
-        if steps is None:
-            steps = (1,) * len(axes)
-        assert len(axes) == len(steps) == len(self._starts) == len(self._ends)
-        return list(zip(axes, self._starts, self._ends, steps))
-
-    def compute_output_shape(self, input_shape):
-        input_shape = list(input_shape)
-        for ax, start, end, steps in self.get_slices(len(input_shape)):
-            size = input_shape[ax]
-            if ax == 0:
-                raise AttributeError("Can not slice batch axis.")
-            if size is None:
-                if start < 0 or end < 0:
-                    raise AttributeError("Negative slices not supported on symbolic axes")
-                logger.warning("Slicing symbolic axis might lead to problems.")
-                input_shape[ax] = (end - start) // steps
-                continue
-            if start < 0:
-                start = size - start
-            if end < 0:
-                end = size - end
-            input_shape[ax] = (min(size, end) - start) // steps
-        return tuple(input_shape)
-
-    def call(self, x, *args):
-        ax_map = dict((x[0], slice(*x[1:])) for x in self.get_slices(K.ndim(x)))
-        shape = K.int_shape(x)
-        slices = [(ax_map[a] if a in ax_map else slice(None)) for a in range(len(shape))]
-        x = x[tuple(slices)]
-        return x
-
-
-class O2K_ReduceLayer(keras.engine.Layer):
-    def __init__(self, axes=None, keepdims=True, **kwargs):
-        self._axes = [axes] if isinstance(axes, int) else axes
-        self._keepdims = bool(keepdims)
-        super(O2K_ReduceLayer, self).__init__(**kwargs)
-
-    def get_config(self):
-        config = super(O2K_ReduceLayer, self).get_config()
-        config.update({
-            'axes': self._axes,
-            'keepdims': self._keepdims
-        })
-        return config
-
-    def compute_output_shape(self, input_shape):
-        if self._axes is None:
-            return (1,)*len(input_shape) if self._keepdims else tuple()
-        ret = list(input_shape)
-        for i in sorted(self._axes, reverse=True):
-            if self._keepdims:
-                ret[i] = 1
-            else:
-                ret.pop(i)
-        return tuple(ret)
-
-    def call(self, x, *args):
-        raise NotImplementedError()
-
-
-class O2K_Sum(O2K_ReduceLayer):
-    def call(self, x, *args):
-        return K.sum(x, self._axes, self._keepdims)
-
-
-class O2K_Sqrt(keras.engine.Layer):
-    def call(self, x, *args):
-        return K.sqrt(x)
-
-
-class O2K_Pow(keras.engine.Layer):
-    def call(self, x, *args):
-        return K.pow(*x)
-
-
-class O2K_ConstantLayer(keras.engine.Layer):
-    def __init__(self, constant_obj, dtype, **kwargs):
-        self._dtype = np.dtype(dtype).name
-        self._constant = np.array(constant_obj, dtype=self._dtype)
-        super(O2K_ConstantLayer, self).__init__(**kwargs)
-
-    def call(self, *args):
-        data = K.constant(self._constant, dtype=self._dtype)
-        return data
-
-    def compute_output_shape(self, input_shape):
-        return self._constant.shape
-
-    def get_config(self):
-        config = super(O2K_ConstantLayer, self).get_config()
-        config.update({
-            'constant_obj': self._constant,
-            'dtype': self._dtype
-        })
-        return config
-
-
-class O2K_Div(O2K_ElementwiseLayer):
-    def call(self, x, *args):
-        return x[0] / x[1]
-
-
-class S3fd_amd():
-    """ Keras Network """
-    def __init__(self, model_path, target_size, confidence):
-        logger.debug("Initializing: %s: (model_path: '%s')",
-                     self.__class__.__name__, model_path)
-        self.model_path = model_path
-        self.confidence = confidence
-        self.model = self.load_model()
-        logger.debug("Initialized: %s", self.__class__.__name__)
-
-    def load_model(self):
-        """ Load the keras Model and weights """
-        logger.verbose("Initializing S3FD_amd Network model...")
-        layers = {
-            'O2K_Add': O2K_Add, 'O2K_Slice': O2K_Slice,
-            'O2K_Sum': O2K_Sum, 'O2K_Sqrt': O2K_Sqrt,
-            'O2K_Pow': O2K_Pow, 'O2K_ConstantLayer': O2K_ConstantLayer,
-            'O2K_Div': O2K_Div
-        }
-        model = keras.models.load_model(self.model_path, custom_objects=layers)
-        model._make_predict_function()  # pylint: disable=protected-access
-        return model
-
-    def prepare_batch(self, batch):
-        batch = batch - np.array([104.0, 117.0, 123.0])
-        batch = batch.transpose(0, 3, 1, 2)
-        return batch
-
-    def predict(self, batch):
-        bboxlists = self.model.predict(batch)
-        return bboxlists
-
-    def finalize_predictions(self, bboxlists):
-        """ Detect faces """
-        ret = list()
-        for i in range(bboxlists[0].shape[0]):
-            bboxlist = [x[i:i+1, ...] for x in bboxlists]
-            bboxlist = self.post_process(bboxlist)
-            keep = self.nms(bboxlist, 0.3)
-            bboxlist = bboxlist[keep, :]
-            bboxlist = [x for x in bboxlist if x[-1] >= self.confidence]
-            ret.append(np.array(bboxlist))
-        return ret
-
-    def post_process(self, bboxlist):
-        """ Perform post processing on output
-            TODO: do this on the batch.
-        """
-        retval = list()
-        for i in range(len(bboxlist) // 2):
-            bboxlist[i * 2] = self.softmax(bboxlist[i * 2], axis=1)
-        for i in range(len(bboxlist) // 2):
-            ocls, oreg = bboxlist[i * 2], bboxlist[i * 2 + 1]
-            stride = 2 ** (i + 2)    # 4,8,16,32,64,128
-            poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
-            for _, hindex, windex in poss:
-                axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride
-                score = ocls[0, 1, hindex, windex]
-                loc = np.ascontiguousarray(oreg[0, :, hindex, windex]).reshape((1, 4))
-                priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])
-                variances = [0.1, 0.2]
-                box = self.decode(loc, priors, variances)
-                x_1, y_1, x_2, y_2 = box[0] * 1.0
-                retval.append([x_1, y_1, x_2, y_2, score])
-        retval = np.array(retval)
-        if len(retval) == 0:
-            retval = np.zeros((1, 5))
-        return retval
-
-    @staticmethod
-    def softmax(inp, axis):
-        """Compute softmax values for each sets of scores in x."""
-        return np.exp(inp - logsumexp(inp, axis=axis, keepdims=True))
-
-    @staticmethod
-    def decode(loc, priors, variances):
-        """Decode locations from predictions using priors to undo
-        the encoding we did for offset regression at train time.
-        Args:
-            loc (tensor): location predictions for loc layers,
-                Shape: [num_priors,4]
-            priors (tensor): Prior boxes in center-offset form.
-                Shape: [num_priors,4].
-            variances: (list[float]) Variances of priorboxes
-        Return:
-            decoded bounding box predictions
-        """
-        boxes = np.concatenate((priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],
-                                priors[:, 2:] * np.exp(loc[:, 2:] * variances[1])),
-                               1)
-        boxes[:, :2] -= boxes[:, 2:] / 2
-        boxes[:, 2:] += boxes[:, :2]
-        return boxes
-
-    @staticmethod
-    def nms(dets, thresh):
-        """ Perform Non-Maximum Suppression """
-        keep = list()
-        if len(dets) == 0:
-            return keep
-
-        x_1, y_1, x_2, y_2, scores = dets[:, 0], dets[:, 1], dets[:, 2], dets[:, 3], dets[:, 4]
-        areas = (x_2 - x_1 + 1) * (y_2 - y_1 + 1)
-        order = scores.argsort()[::-1]
-
-        keep = []
-        while order.size > 0:
-            i = order[0]
-            keep.append(i)
-            xx_1, yy_1 = np.maximum(x_1[i], x_1[order[1:]]), np.maximum(y_1[i], y_1[order[1:]])
-            xx_2, yy_2 = np.minimum(x_2[i], x_2[order[1:]]), np.minimum(y_2[i], y_2[order[1:]])
-
-            width, height = np.maximum(0.0, xx_2 - xx_1 + 1), np.maximum(0.0, yy_2 - yy_1 + 1)
-            ovr = width * height / (areas[i] + areas[order[1:]] - width * height)
-
-            inds = np.where(ovr <= thresh)[0]
-            order = order[inds + 1]
-
-        return keep
diff --git a/plugins/extract/detect/s3fd_defaults.py b/plugins/extract/detect/s3fd_defaults.py
index 0f55892..59fcb07 100755
--- a/plugins/extract/detect/s3fd_defaults.py
+++ b/plugins/extract/detect/s3fd_defaults.py
@@ -16,28 +16,30 @@
                    <metadata> dictionary requirements are listed below.
 
     The following keys are expected for the _DEFAULTS <metadata> dict:
-        datatype:  [required] A python type class. This limits the type of data that can be
-                   provided in the .ini file and ensures that the value is returned in the
-                   correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
-                   <class 'str'>, <class 'bool'>.
-        default:   [required] The default value for this option.
-        info:      [required] A string describing what this option does.
-        choices:   [optional] If this option's datatype is of <class 'str'> then valid
-                   selections can be defined here. This validates the option and also enables
-                   a combobox / radio option in the GUI.
-        gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
-                   radio buttons rather than a combobox to display this option.
-        min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
-                   otherwise it is ignored. Should be a tuple of min and max accepted values.
-                   This is used for controlling the GUI slider range. Values are not enforced.
-        rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
-                   required otherwise it is ignored. Used for the GUI slider. For floats, this
-                   is the number of decimal places to display. For ints this is the step size.
-        fixed:     [optional] [train only]. Training configurations are fixed when the model is
-                   created, and then reloaded from the state file. Marking an item as fixed=False
-                   indicates that this value can be changed for existing models, and will override
-                   the value saved in the state file with the updated value in config. If not
-                   provided this will default to True.
+    datatype:  [required] A python type class. This limits the type of data that can be
+                provided in the .ini file and ensures that the value is returned in the
+                correct type to faceswap. Valid datatypes are: <class 'int'>, <class 'float'>,
+                <class 'str'>, <class 'bool'>.
+    default:   [required] The default value for this option.
+    info:      [required] A string describing what this option does.
+    group:     [optional]. A group for grouping options together in the GUI. If not
+                provided this will not group this option with any others.
+    choices:   [optional] If this option's datatype is of <class 'str'> then valid
+                selections can be defined here. This validates the option and also enables
+                a combobox / radio option in the GUI.
+    gui_radio: [optional] If <choices> are defined, this indicates that the GUI should use
+                radio buttons rather than a combobox to display this option.
+    min_max:   [partial] For <class 'int'> and <class 'float'> datatypes this is required
+                otherwise it is ignored. Should be a tuple of min and max accepted values.
+                This is used for controlling the GUI slider range. Values are not enforced.
+    rounding:  [partial] For <class 'int'> and <class 'float'> datatypes this is
+                required otherwise it is ignored. Used for the GUI slider. For floats, this
+                is the number of decimal places to display. For ints this is the step size.
+    fixed:     [optional] [train only]. Training configurations are fixed when the model is
+                created, and then reloaded from the state file. Marking an item as fixed=False
+                indicates that this value can be changed for existing models, and will override
+                the value saved in the state file with the updated value in config. If not
+                provided this will default to True.
 """
 
 
@@ -59,5 +61,19 @@ _DEFAULTS = {
         "choices": [],
         "gui_radio": False,
         "fixed": True,
+    },
+    "batch-size": {
+        "default": 8,
+        "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
+                "but setting it too high can harm performance.\n"
+                "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
+                "accomodate then this will automatically be lowered."
+                "\n\tAMD users: A batchsize of 8 requires about 2 GB vram.",
+        "datatype": int,
+        "rounding": 1,
+        "min_max": (1, 64),
+        "choices": [],
+        "gui_radio": False,
+        "fixed": True,
     }
 }
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index fbfeaa5..632ed4d 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -2,102 +2,271 @@
 """
 Return a requested detector/aligner pipeline
 
-Tensorflow does not like to release GPU VRAM, so these are launched in subprocesses
-so that the vram is released on subprocess exit """
+Tensorflow does not like to release GPU VRAM, so parallel plugins need to be managed to work
+together.
+
+This module sets up a pipeline for the extraction workflow, loading align and detect plugins
+either in parallal or in series, giving easy access to input and output.
+
+ """
 
 import logging
 
 from lib.gpu_stats import GPUStats
-from lib.multithreading import PoolProcess, SpawnProcess
 from lib.queue_manager import queue_manager, QueueEmpty
+from lib.utils import get_backend
 from plugins.plugin_loader import PluginLoader
 
 logger = logging.getLogger(__name__)  # pylint:disable=invalid-name
 
 
 class Extractor():
-    """ Creates a detect/align pipeline and returns results from a generator
-
-        Input queue is dynamically set depending on the current phase of extraction
-        and can be accessed from:
-            Extractor.input_queue
+    """ Creates a :mod:`~plugins.extract.detect`/:mod:`~plugins.extract.align` pipeline and yields
+    results frame by frame from the :attr:`detected_faces` generator
+
+    :attr:`input_queue` is dynamically set depending on the current :attr:`phase` of extraction
+
+    Parameters
+    ----------
+    detector: str
+        The name of a detector plugin as exists in :mod:`plugins.extract.detect`
+    aligner: str
+        The name of an aligner plugin as exists in :mod:`plugins.extract.align`
+    configfile: str, optional
+        The path to a custom ``extract.ini`` configfile. If ``None`` then the system
+        :file:`config/extract.ini` file will be used.
+    multiprocess: bool, optional
+        Whether to attempt processing the plugins in parallel. This may get overridden
+        internally depending on the plugin combination. Default: ``False``
+    rotate_images: str, optional
+        Used to set the :attr:`~plugins.extract.detect.rotation` attribute. Pass in a single number
+        to use increments of that size up to 360, or pass in a ``list`` of ``ints`` to enumerate
+        exactly what angles to check. Can also pass in ``'on'`` to increment at 90 degree
+        intervals. Default: ``None``
+    min_size: int, optional
+        Used to set the :attr:`~plugins.extract.detect.min_size` attribute Filters out faces
+        detected below this size. Length, in pixels across the diagonal of the bounding box. Set
+        to ``0`` for off. Default: ``0``
+    normalize_method: {`None`, 'clahe', 'hist', 'mean'}, optional
+        Used to set the :attr:`~plugins.extract.align.normalize_method` attribute. Normalize the
+        images fed to the aligner.Default: ``None``
+
+    Attributes
+    ----------
+    phase: str
+        The current phase that the pipeline is running. Used in conjunction with :attr:`passes` and
+        :attr:`final_pass` to indicate to the caller which phase is being processed
     """
-    def __init__(self, detector, aligner, loglevel,
+    def __init__(self, detector, aligner,
                  configfile=None, multiprocess=False, rotate_images=None, min_size=20,
                  normalize_method=None):
-        logger.debug("Initializing %s: (detector: %s, aligner: %s, loglevel: %s, configfile: %s, "
+        logger.debug("Initializing %s: (detector: %s, aligner: %s, configfile: %s, "
                      "multiprocess: %s, rotate_images: %s, min_size: %s, "
                      "normalize_method: %s)", self.__class__.__name__, detector, aligner,
-                     loglevel, configfile, multiprocess, rotate_images, min_size,
-                     normalize_method)
+                     configfile, multiprocess, rotate_images, min_size, normalize_method)
         self.phase = "detect"
-        self.detector = self.load_detector(detector, loglevel, rotate_images, min_size, configfile)
-        self.aligner = self.load_aligner(aligner, loglevel, configfile, normalize_method)
-        self.is_parallel = self.set_parallel_processing(multiprocess)
-        self.processes = list()
-        self.queues = self.add_queues()
+        self._queue_size = 32
+        self._vram_buffer = 320  # Leave a buffer for VRAM allocation
+        self._detector = self._load_detector(detector, rotate_images, min_size, configfile)
+        self._aligner = self._load_aligner(aligner, configfile, normalize_method)
+        self._is_parallel = self._set_parallel_processing(multiprocess)
+        self._queues = self._add_queues()
         logger.debug("Initialized %s", self.__class__.__name__)
 
     @property
     def input_queue(self):
-        """ Return the correct input queue depending on the current phase """
-        if self.is_parallel or self.phase == "detect":
+        """ queue: Return the correct input queue depending on the current phase
+
+        The input queue is the entry point into the extraction pipeline. A ``dict`` should
+        be put to the queue in the following format(s):
+
+        For detect/single phase operations:
+
+        >>> {'filename': <path to the source image that is to be extracted from>,
+        >>>  'image': <the source image as a numpy.array in BGR color format>}
+
+        For align (2nd pass operations):
+
+        >>> {'filename': <path to the source image that is to be extracted from>,
+        >>>  'image': <the source image as a numpy.array in BGR color format>,
+        >>>  'detected_faces: [<list of DetectedFace objects as generated from detect>]}
+
+        """
+        if self._is_parallel or self.phase == "detect":
             qname = "extract_detect_in"
         else:
             qname = "extract_align_in"
-        retval = self.queues[qname]
-        logger.trace("%s: %s", qname, retval)
-        return retval
-
-    @property
-    def output_queue(self):
-        """ Return the correct output queue depending on the current phase """
-        qname = "extract_align_out" if self.final_pass else "extract_align_in"
-        retval = self.queues[qname]
+        retval = self._queues[qname]
         logger.trace("%s: %s", qname, retval)
         return retval
 
     @property
     def passes(self):
-        """ Return the number of passes the extractor needs to make """
-        retval = 1 if self.is_parallel else 2
+        """ int: Returns the total number of passes the extractor needs to make.
+
+        This is calculated on several factors (vram available, plugin choice,
+        :attr:`multiprocess` etc.). It is useful for iterating over the pipeline
+        and handling accordingly.
+
+        Example
+        -------
+        >>> for phase in extractor.passes:
+        >>>     if phase == 1:
+        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
+        >>>                                    "image": np.array(image)})
+        >>>     else:
+        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
+        >>>                                    "image": np.array(image),
+        >>>                                    "detected_faces": [<DetectedFace objects]})
+        """
+        retval = 1 if self._is_parallel else 2
         logger.trace(retval)
         return retval
 
     @property
     def final_pass(self):
-        """ Return true if this is the final extractor pass """
-        retval = self.is_parallel or self.phase == "align"
+        """ bool, Return ``True`` if this is the final extractor pass otherwise ``False``
+
+        Useful for iterating over the pipeline :attr:`passes` or :func:`detected_faces` and
+        handling accordingly.
+
+        Example
+        -------
+        >>> for face in extractor.detected_faces():
+        >>>     if extractor.final_pass:
+        >>>         <do final processing>
+        >>>     else:
+        >>>         <do intermediate processing>
+        >>>         extractor.input_queue.put({"filename": "path/to/image/file",
+        >>>                                    "image": np.array(image),
+        >>>                                    "detected_faces": [<DetectedFace objects]})
+        """
+        retval = self._is_parallel or self.phase == "align"
         logger.trace(retval)
         return retval
 
-    @staticmethod
-    def load_detector(detector, loglevel, rotation, min_size, configfile):
-        """ Set global arguments and load detector plugin """
-        detector_name = detector.replace("-", "_").lower()
-        logger.debug("Loading Detector: '%s'", detector_name)
-        detector = PluginLoader.get_detector(detector_name)(loglevel=loglevel,
-                                                            rotation=rotation,
-                                                            min_size=min_size,
-                                                            configfile=configfile)
-        return detector
+    def set_batchsize(self, plugin_type, batchsize):
+        """ Set the batchsize of a given :attr:`plugin_type` to the given :attr:`batchsize`.
 
-    @staticmethod
-    def load_aligner(aligner, loglevel, configfile, normalize_method):
-        """ Set global arguments and load aligner plugin """
-        aligner_name = aligner.replace("-", "_").lower()
-        logger.debug("Loading Aligner: '%s'", aligner_name)
-        aligner = PluginLoader.get_aligner(aligner_name)(loglevel=loglevel,
-                                                         configfile=configfile,
-                                                         normalize_method=normalize_method)
-        return aligner
+        This should be set prior to :func:`launch` if the batchsize is to be manually overriden
 
-    def set_parallel_processing(self, multiprocess):
-        """ Set whether to run detect and align together or separately """
-        detector_vram = self.detector.vram
-        aligner_vram = self.aligner.vram
+        Parameters
+        ----------
+        plugin_type: {'aligner', 'detector'}
+            The plugin_type to be overriden
+        batchsize: int
+            The batchsize to use for this plugin type
+        """
+        logger.debug("Overriding batchsize for plugin_type: %s to: %s", plugin_type, batchsize)
+        plugin = getattr(self, "_{}".format(plugin_type))
+        plugin.batchsize = batchsize
+
+    def launch(self):
+        """ Launches the plugin(s)
+
+        This launches the plugins held in the pipeline, and should be called at the beginning
+        of each :attr:`phase`. To ensure VRAM is conserved, It will only launch the plugin(s)
+        required for the currently running phase
+
+        Example
+        -------
+        >>> for phase in extractor.passes:
+        >>>     extractor.launch():
+        >>>         <do processing>
+        """
+
+        if self._is_parallel:
+            self._launch_aligner()
+            self._launch_detector()
+        elif self.phase == "detect":
+            self._launch_detector()
+        else:
+            self._launch_aligner()
+
+    def detected_faces(self):
+        """ Generator that returns results, frame by frame from the extraction pipeline
+
+        This is the exit point for the extraction pipeline and is used to obtain the output
+        of any pipeline :attr:`phase`
+
+        Yields
+        ------
+        faces: dict
+            regardless of phase, the returned dictinary will contain, exclusively, ``filename``:
+            the filename of the source image, ``image``: the ``numpy.array`` of the source image
+            in BGR color format, ``detected_faces``: a list of
+            :class:`~lib.faces_detect.Detected_Face` objects.
+
+        Example
+        -------
+        >>> for face in extractor.detected_faces():
+        >>>     filename = face["filename"]
+        >>>     image = face["image"]
+        >>>     detected_faces = face["detected_faces"]
+        """
+        logger.debug("Running Detection. Phase: '%s'", self.phase)
+        # If not multiprocessing, intercept the align in queue for
+        # detection phase
+        out_queue = self._output_queue
+        while True:
+            try:
+                if self._check_and_raise_error():
+                    break
+                faces = out_queue.get(True, 1)
+                if faces == "EOF":
+                    break
+            except QueueEmpty:
+                continue
+
+            yield faces
+        self._join_threads()
+        if self.final_pass:
+            # Cleanup queues
+            for q_name in self._queues.keys():
+                queue_manager.del_queue(q_name)
+            logger.debug("Detection Complete")
+        else:
+            logger.debug("Switching to align phase")
+            self.phase = "align"
+
+    # <<< INTERNAL METHODS >>> #
+    @property
+    def _output_queue(self):
+        """ Return the correct output queue depending on the current phase """
+        qname = "extract_align_out" if self.final_pass else "extract_align_in"
+        retval = self._queues[qname]
+        logger.trace("%s: %s", qname, retval)
+        return retval
+
+    @property
+    def _active_plugins(self):
+        """ Return the plugins that are currently active based on pass """
+        if self.passes == 1:
+            retval = [self._detector, self._aligner]
+        elif self.passes == 2 and not self.final_pass:
+            retval = [self._detector]
+        else:
+            retval = [self._aligner]
+        logger.trace("Active plugins: %s", retval)
+        return retval
+
+    def _add_queues(self):
+        """ Add the required processing queues to Queue Manager """
+        queues = dict()
+        for task in ("extract_detect_in", "extract_align_in", "extract_align_out"):
+            # Limit queue size to avoid stacking ram
+            self._queue_size = 32
+            if task == "extract_detect_in" or (not self._is_parallel
+                                               and task == "extract_align_in"):
+                self._queue_size = 64
+            queue_manager.add_queue(task, maxsize=self._queue_size)
+            queues[task] = queue_manager.get_queue(task)
+        logger.debug("Queues: %s", queues)
+        return queues
 
-        if detector_vram == 0 or aligner_vram == 0:
+    def _set_parallel_processing(self, multiprocess):
+        """ Set whether to run detect and align together or separately """
+        if self._detector.vram == 0 or self._aligner.vram == 0:
             logger.debug("At least one of aligner or detector have no VRAM requirement. "
                          "Enabling parallel processing.")
             return True
@@ -107,168 +276,94 @@ class Extractor():
             return False
 
         gpu_stats = GPUStats()
-        if gpu_stats.is_plaidml and (not self.detector.supports_plaidml or
-                                     not self.aligner.supports_plaidml):
-            logger.debug("At least one of aligner or detector does not support plaidML. "
-                         "Enabling parallel processing.")
-            return True
-
-        if not gpu_stats.is_plaidml and (
-                (self.detector.supports_plaidml and aligner_vram != 0) or
-                (self.aligner.supports_plaidml and detector_vram != 0)):
-            logger.warning("Keras + non-Keras aligner/detector combination does not support "
-                           "parallel processing. Switching to serial.")
-            return False
-
-        if self.detector.supports_plaidml and self.aligner.supports_plaidml:
-            logger.debug("Both aligner and detector support plaidML. Disabling parallel "
-                         "processing.")
-            return False
-
         if gpu_stats.device_count == 0:
             logger.debug("No GPU detected. Enabling parallel processing.")
             return True
 
-        required_vram = detector_vram + aligner_vram + 320  # 320MB buffer
+        if get_backend() == "amd":
+            logger.debug("Parallel processing discabled by amd")
+            return False
+
+        vram_required = self._detector.vram + self._aligner.vram + self._vram_buffer
         stats = gpu_stats.get_card_most_free()
-        free_vram = int(stats["free"])
+        vram_free = int(stats["free"])
         logger.verbose("%s - %sMB free of %sMB",
                        stats["device"],
-                       free_vram,
+                       vram_free,
                        int(stats["total"]))
-        if free_vram <= required_vram:
+        if vram_free <= vram_required:
             logger.warning("Not enough free VRAM for parallel processing. "
                            "Switching to serial")
             return False
+
+        self._set_extractor_batchsize(vram_required, vram_free)
         return True
 
-    def add_queues(self):
-        """ Add the required processing queues to Queue Manager """
-        queues = dict()
-        for task in ("extract_detect_in", "extract_align_in", "extract_align_out"):
-            # Limit queue size to avoid stacking ram
-            size = 32
-            if task == "extract_detect_in" or (not self.is_parallel
-                                               and task == "extract_align_in"):
-                size = 64
-            queue_manager.add_queue(task, maxsize=size)
-            queues[task] = queue_manager.get_queue(task)
-        logger.debug("Queues: %s", queues)
-        return queues
+    # << INTERNAL PLUGIN HANDLING >> #
+    @staticmethod
+    def _load_detector(detector, rotation, min_size, configfile):
+        """ Set global arguments and load detector plugin """
+        detector_name = detector.replace("-", "_").lower()
+        logger.debug("Loading Detector: '%s'", detector_name)
+        detector = PluginLoader.get_detector(detector_name)(rotation=rotation,
+                                                            min_size=min_size,
+                                                            configfile=configfile)
+        return detector
 
-    def launch(self):
-        """ Launches the plugins
-            This can be called multiple times depending on the phase/whether multiprocessing
-            is enabled.
-
-            If multiprocessing:
-                launches both plugins, but aligner first so that it's VRAM can be allocated
-                prior to giving the remaining to the detector
-            If not multiprocessing:
-                Launches the relevant plugin for the current phase """
-        if self.is_parallel:
-            logger.debug("Launching aligner and detector")
-            self.launch_aligner()
-            self.launch_detector()
-        elif self.phase == "detect":
-            logger.debug("Launching detector")
-            self.launch_detector()
-        else:
-            logger.debug("Launching aligner")
-            self.launch_aligner()
+    @staticmethod
+    def _load_aligner(aligner, configfile, normalize_method):
+        """ Set global arguments and load aligner plugin """
+        aligner_name = aligner.replace("-", "_").lower()
+        logger.debug("Loading Aligner: '%s'", aligner_name)
+        aligner = PluginLoader.get_aligner(aligner_name)(configfile=configfile,
+                                                         normalize_method=normalize_method)
+        return aligner
 
-    def launch_aligner(self):
+    def _launch_aligner(self):
         """ Launch the face aligner """
         logger.debug("Launching Aligner")
-        kwargs = {"in_queue": self.queues["extract_align_in"],
-                  "out_queue": self.queues["extract_align_out"]}
-
-        process = SpawnProcess(self.aligner.run, **kwargs)
-        event = process.event
-        error = process.error
-        process.start()
-        self.processes.append(process)
-
-        # Wait for Aligner to take it's VRAM
-        # The first ever load of the model for FAN has reportedly taken
-        # up to 3-4 minutes, hence high timeout.
-        # TODO investigate why this is and fix if possible
-        for mins in reversed(range(5)):
-            for seconds in range(60):
-                event.wait(seconds)
-                if event.is_set():
-                    break
-                if error.is_set():
-                    break
-            if event.is_set():
-                break
-            if mins == 0 or error.is_set():
-                raise ValueError("Error initializing Aligner")
-            logger.info("Waiting for Aligner... Time out in %s minutes", mins)
-
+        kwargs = dict(in_queue=self._queues["extract_align_in"],
+                      out_queue=self._queues["extract_align_out"],
+                      queue_size=self._queue_size)
+        self._aligner.initialize(**kwargs)
+        self._aligner.start()
         logger.debug("Launched Aligner")
 
-    def launch_detector(self):
+    def _launch_detector(self):
         """ Launch the face detector """
         logger.debug("Launching Detector")
-        kwargs = {"in_queue": self.queues["extract_detect_in"],
-                  "out_queue": self.queues["extract_align_in"]}
-        mp_func = PoolProcess if self.detector.parent_is_pool else SpawnProcess
-        process = mp_func(self.detector.run, **kwargs)
-
-        event = process.event if hasattr(process, "event") else None
-        error = process.error if hasattr(process, "error") else None
-        process.start()
-        self.processes.append(process)
-
-        if event is None:
-            logger.debug("Launched Detector")
-            return
-
-        for mins in reversed(range(5)):
-            for seconds in range(60):
-                event.wait(seconds)
-                if event.is_set():
-                    break
-                if error and error.is_set():
-                    break
-            if event.is_set():
-                break
-            if mins == 0 or (error and error.is_set()):
-                raise ValueError("Error initializing Detector")
-            logger.info("Waiting for Detector... Time out in %s minutes", mins)
-
+        kwargs = dict(in_queue=self._queues["extract_detect_in"],
+                      out_queue=self._queues["extract_align_in"],
+                      queue_size=self._queue_size)
+        self._detector.initialize(**kwargs)
+        self._detector.start()
         logger.debug("Launched Detector")
 
-    def detected_faces(self):
-        """ Detect faces from in an image """
-        logger.debug("Running Detection. Phase: '%s'", self.phase)
-        # If not multiprocessing, intercept the align in queue for
-        # detection phase
-        out_queue = self.output_queue
-        while True:
-            try:
-                faces = out_queue.get(True, 1)
-                if faces == "EOF":
-                    break
-                if isinstance(faces, dict) and faces.get("exception"):
-                    pid = faces["exception"][0]
-                    t_back = faces["exception"][1].getvalue()
-                    err = "Error in child process {}. {}".format(pid, t_back)
-                    raise Exception(err)
-            except QueueEmpty:
-                continue
-
-            yield faces
-        for process in self.processes:
-            logger.trace("Joining process: %s", process)
-            process.join()
-            del process
-        if self.final_pass:
-            # Cleanup queues
-            for q_name in self.queues.keys():
-                queue_manager.del_queue(q_name)
-            logger.debug("Detection Complete")
-        else:
-            logger.debug("Switching to align phase")
-            self.phase = "align"
+    def _set_extractor_batchsize(self, vram_required, vram_free):
+        """ Sets the batchsize of the used plugins based on their vram and
+            vram_per_batch_requirements """
+        batch_required = ((self._aligner.vram_per_batch * self._aligner.batchsize) +
+                          (self._detector.vram_per_batch * self._detector.batchsize))
+        plugin_required = vram_required + batch_required
+        if plugin_required <= vram_free:
+            logger.verbose("Plugin requirements within threshold: (plugin_required: %sMB, "
+                           "vram_free: %sMB)", plugin_required, vram_free)
+            return
+        # Hacky split across 2 plugins
+        available_for_batching = (vram_free - vram_required) // 2
+        self._aligner.batchsize = max(1, available_for_batching // self._aligner.vram_per_batch)
+        self._detector.batchsize = max(1, available_for_batching // self._detector.vram_per_batch)
+        logger.verbose("Reset batchsizes: (aligner: %s, detector: %s)",
+                       self._aligner.batchsize, self._detector.batchsize)
+
+    def _join_threads(self):
+        """ Join threads for current pass """
+        for plugin in self._active_plugins:
+            plugin.join()
+
+    def _check_and_raise_error(self):
+        """ Check all threads for errors and raise if one occurs """
+        for plugin in self._active_plugins:
+            if plugin.check_and_raise_error():
+                return True
+        return False
diff --git a/plugins/plugin_loader.py b/plugins/plugin_loader.py
index 610e113..75d8655 100644
--- a/plugins/plugin_loader.py
+++ b/plugins/plugin_loader.py
@@ -5,8 +5,6 @@ import logging
 import os
 from importlib import import_module
 
-from lib.utils import get_backend
-
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
 
 
@@ -61,15 +59,6 @@ class PluginLoader():
                             and not item.name.endswith("defaults.py")
                             and item.name.endswith(".py")
                             and item.name != "manual.py")
-        # TODO Remove this hacky fix when we move them to the same models
-        multi_versions = [extractor.replace("-amd", "")
-                          for extractor in extractors if extractor.endswith("-amd")]
-        if get_backend() == "amd":
-            for extractor in multi_versions:
-                extractors.remove(extractor)
-        else:
-            for extractor in multi_versions:
-                extractors.remove("{}-amd".format(extractor))
         return extractors
 
     @staticmethod
diff --git a/requirements.txt b/requirements.txt
index 8ab14be..8399dd0 100755
--- a/requirements.txt
+++ b/requirements.txt
@@ -2,7 +2,7 @@ tqdm
 psutil
 pathlib
 numpy==1.16.2
-opencv-python>=4.0
+opencv-python==4.1.1.26
 scikit-image
 Pillow==6.1.0
 scikit-learn
diff --git a/scripts/convert.py b/scripts/convert.py
index cf51e73..c184b1f 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -94,7 +94,7 @@ class Convert():
         """ Add the queues for convert """
         logger.debug("Adding queues. Queue size: %s", self.queue_size)
         for qname in ("convert_in", "convert_out", "patch"):
-            queue_manager.add_queue(qname, self.queue_size, multiprocessing_queue=False)
+            queue_manager.add_queue(qname, self.queue_size)
 
     def process(self):
         """ Process the conversion """
@@ -256,7 +256,6 @@ class DiskIO():
                        "superior results")
         extractor = Extractor(detector="cv2-dnn",
                               aligner="cv2-dnn",
-                              loglevel=self.args.loglevel,
                               multiprocess=False,
                               rotate_images=None,
                               min_size=20)
@@ -283,7 +282,7 @@ class DiskIO():
             q_name = task
         setattr(self,
                 "{}_queue".format(task),
-                queue_manager.get_queue(q_name, multiprocessing_queue=False))
+                queue_manager.get_queue(q_name))
         logger.debug("Added queue for task: '%s'", task)
 
     def start_thread(self, task):
@@ -381,15 +380,7 @@ class DiskIO():
         self.extractor.input_queue.put(inp)
         faces = next(self.extractor.detected_faces())
 
-        landmarks = faces["landmarks"]
-        detected_faces = faces["detected_faces"]
-        final_faces = list()
-
-        for idx, face in enumerate(detected_faces):
-            detected_face = DetectedFace()
-            detected_face.from_bounding_box_dict(face)
-            detected_face.landmarksXY = landmarks[idx]
-            final_faces.append(detected_face)
+        final_faces = [face for face in faces["detected_faces"]]
         return final_faces
 
     # Saving tasks
diff --git a/scripts/extract.py b/scripts/extract.py
index 3d814b1..7192464 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -8,7 +8,6 @@ from pathlib import Path
 
 from tqdm import tqdm
 
-from lib.faces_detect import DetectedFace
 from lib.multithreading import MultiThread
 from lib.queue_manager import queue_manager
 from lib.utils import get_folder, hash_encode_image
@@ -34,7 +33,6 @@ class Extract():
         normalization = None if self.args.normalization == "none" else self.args.normalization
         self.extractor = Extractor(self.args.detector,
                                    self.args.aligner,
-                                   self.args.loglevel,
                                    configfile=configfile,
                                    multiprocess=not self.args.singleprocess,
                                    rotate_images=self.args.rotate_images,
@@ -239,15 +237,11 @@ class Extract():
         """ Align the detected face and add the destination file path """
         final_faces = list()
         image = faces["image"]
-        landmarks = faces["landmarks"]
         detected_faces = faces["detected_faces"]
-        for idx, face in enumerate(detected_faces):
-            detected_face = DetectedFace()
-            detected_face.from_bounding_box_dict(face, image)
-            detected_face.landmarksXY = landmarks[idx]
-            detected_face.load_aligned(image, size=size, align_eyes=align_eyes)
+        for face in detected_faces:
+            face.load_aligned(image, size=size, align_eyes=align_eyes)
             final_faces.append({"file_location": self.output_dir / Path(filename).stem,
-                                "face": detected_face})
+                                "face": face})
         faces["detected_faces"] = final_faces
 
     def output_faces(self, filename, faces):
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index 5a3b1ae..5a763cd 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -283,7 +283,6 @@ class PostProcess():
 
             face_filter = dict(detector=detector,
                                aligner=aligner,
-                               loglevel=self.args.loglevel,
                                multiprocess=not self.args.singleprocess)
             filter_lists = dict()
             if hasattr(self.args, "ref_threshold"):
@@ -405,7 +404,7 @@ class FaceFilter(PostProcessAction):
         self.filter = self.load_face_filter(**kwargs)
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def load_face_filter(self, filter_lists, ref_threshold, aligner, detector, loglevel,
+    def load_face_filter(self, filter_lists, ref_threshold, aligner, detector,
                          multiprocess):
         """ Load faces to filter out of images """
         if not any(val for val in filter_lists.values()):
@@ -420,7 +419,6 @@ class FaceFilter(PostProcessAction):
                                     filter_files[1],
                                     detector,
                                     aligner,
-                                    loglevel,
                                     multiprocess,
                                     ref_threshold)
             logger.debug("Face filter: %s", facefilter)
diff --git a/tools/lib_alignments/annotate.py b/tools/lib_alignments/annotate.py
index 274c860..6c27b4d 100644
--- a/tools/lib_alignments/annotate.py
+++ b/tools/lib_alignments/annotate.py
@@ -70,7 +70,7 @@ class Annotate():
         """ Draw the facial landmarks """
         color = self.colors[color_id]
         for alignment in self.alignments:
-            landmarks = alignment["landmarksXY"]
+            landmarks = alignment["landmarks_xy"]
             logger.trace("Drawing Landmarks: (landmarks: %s, color: %s, radius: %s)",
                          landmarks, color, radius)
             for (pos_x, pos_y) in landmarks:
@@ -84,7 +84,7 @@ class Annotate():
         """ Draw the facial landmarks """
         color = self.colors[color_id]
         for alignment in self.alignments:
-            landmarks = alignment["landmarksXY"]
+            landmarks = alignment["landmarks_xy"]
             logger.trace("Drawing Landmarks Mesh: (landmarks: %s, color: %s, thickness: %s)",
                          landmarks, color, thickness)
             for key, val in FACIAL_LANDMARKS_IDXS.items():
diff --git a/tools/lib_alignments/jobs.py b/tools/lib_alignments/jobs.py
index 2ca6ac6..55b6925 100644
--- a/tools/lib_alignments/jobs.py
+++ b/tools/lib_alignments/jobs.py
@@ -667,7 +667,7 @@ class Reformat():
                      "y": top,
                      "h": bottom - top,
                      "hash": f_hash,
-                     "landmarksXY": dfl_alignments["source_landmarks"]}
+                     "landmarks_xy": dfl_alignments["source_landmarks"]}
         logger.trace("Adding alignment: (frame: '%s', alignment: %s", sourcefile, alignment)
         alignments.setdefault(sourcefile, list()).append(alignment)
 
@@ -974,7 +974,7 @@ class Spatial():
                 continue
             # We should only be normalizing a single face, so just take
             # the first landmarks found
-            landmarks = np.array(val[0]["landmarksXY"]).reshape(68, 2, 1)
+            landmarks = np.array(val[0]["landmarks_xy"]).reshape(68, 2, 1)
             start = end
             end = start + landmarks.shape[2]
             # Store in one big array
@@ -1047,7 +1047,7 @@ class Spatial():
             logger.trace("Updating: (frame: %s)", frame)
             landmarks_update = landmarks[:, :, idx].astype(int)
             landmarks_xy = landmarks_update.reshape(68, 2).tolist()
-            self.alignments.data[frame][0]["landmarksXY"] = landmarks_xy
+            self.alignments.data[frame][0]["landmarks_xy"] = landmarks_xy
             logger.trace("Updated: (frame: '%s', landmarks: %s)", frame, landmarks_xy)
         logger.debug("Updated alignments")
 
diff --git a/tools/lib_alignments/jobs_manual.py b/tools/lib_alignments/jobs_manual.py
index af261c7..f5d2810 100644
--- a/tools/lib_alignments/jobs_manual.py
+++ b/tools/lib_alignments/jobs_manual.py
@@ -7,10 +7,8 @@ import sys
 import cv2
 import numpy as np
 
-from lib.multithreading import SpawnProcess
-from lib.queue_manager import queue_manager, QueueEmpty
-from lib.utils import get_backend
-from plugins.plugin_loader import PluginLoader
+from lib.queue_manager import queue_manager
+from plugins.extract.pipeline import Extractor
 from . import Annotate, ExtractedFaces, Frames, Legacy
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -617,7 +615,7 @@ class Manual():
     def set_faces(self, frame):
         """ Pass the current frame faces to faces window """
         faces = self.extracted_faces.get_faces_in_frame(frame)
-        landmarks = [{"landmarksXY": face.aligned_landmarks}
+        landmarks = [{"landmarks_xy": face.aligned_landmarks}
                      for face in self.extracted_faces.faces]
         return FacesDisplay(faces, landmarks, self.extracted_faces.size, self.interface)
 
@@ -772,8 +770,8 @@ class MouseHandler():
         self.alignments = interface.alignments
         self.frames = interface.frames
 
-        self.extractor = dict()
-        self.init_extractor(loglevel)
+        self.queues = dict()
+        self.extractor = self.init_extractor()
 
         self.mouse_state = None
         self.last_move = None
@@ -786,61 +784,17 @@ class MouseHandler():
                       "bounding_box_orig": list()}
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def init_extractor(self, loglevel):
+    def init_extractor(self):
         """ Initialize Aligner """
         logger.debug("Initialize Extractor")
-        out_queue = queue_manager.get_queue("out")
-
-        d_kwargs = {"in_queue": queue_manager.get_queue("in"),
-                    "out_queue": queue_manager.get_queue("align")}
-        a_kwargs = {"in_queue": queue_manager.get_queue("align"),
-                    "out_queue": out_queue}
-
-        detector = PluginLoader.get_detector("manual")(loglevel=loglevel)
-        detect_process = SpawnProcess(detector.run, **d_kwargs)
-        d_event = detect_process.event
-        detect_process.start()
-
-        plugins = ["fan_amd"] if get_backend() == "amd" else ["fan"]
-        plugins.append("cv2_dnn")
-        for plugin in plugins:
-            aligner = PluginLoader.get_aligner(plugin)(loglevel=loglevel,
-                                                       normalize_method="hist")
-            align_process = SpawnProcess(aligner.run, **a_kwargs)
-            a_event = align_process.event
-            align_process.start()
-
-            # Wait for Aligner to initialize
-            # The first ever load of the model for FAN has reportedly taken
-            # up to 3-4 minutes, hence high timeout.
-            a_event.wait(300)
-            if not a_event.is_set():
-                if plugin.startswith("fan"):
-                    align_process.join()
-                    logger.error("Error initializing FAN. Trying CV2-DNN")
-                    continue
-                else:
-                    raise ValueError("Error inititalizing Aligner")
-            if plugin == "cv2_dnn":
-                break
-
-            try:
-                err = None
-                err = out_queue.get(True, 1)
-            except QueueEmpty:
-                pass
-            if not err:
-                break
-            align_process.join()
-            logger.error("Error initializing FAN. Trying CV2-DNN")
-
-        d_event.wait(10)
-        if not d_event.is_set():
-            raise ValueError("Error inititalizing Detector")
-
-        self.extractor["detect"] = detector
-        self.extractor["align"] = aligner
+        extractor = Extractor("manual", "fan", multiprocess=True, normalize_method="hist")
+        self.queues["in"] = extractor.input_queue
+        # Set the batchsizes to 1
+        extractor.set_batchsize("detector", 1)
+        extractor.set_batchsize("aligner", 1)
+        extractor.launch()
         logger.debug("Initialized Extractor")
+        return extractor
 
     def on_event(self, event, x, y, flags, param):  # pylint: disable=unused-argument,invalid-name
         """ Handle the mouse events """
@@ -970,22 +924,12 @@ class MouseHandler():
 
     def update_landmarks(self):
         """ Update the landmarks """
-        queue_manager.get_queue("in").put({"image": self.media["image"],
-                                           "filename": self.media["frame_id"],
-                                           "face": self.media["bounding_box"]})
-        landmarks = queue_manager.get_queue("out").get()
-
-        if isinstance(landmarks, dict) and landmarks.get("exception"):
-            cv2.destroyAllWindows()  # pylint: disable=no-member
-            pid = landmarks["exception"][0]
-            t_back = landmarks["exception"][1].getvalue()
-            err = "Error in child process {}. {}".format(pid, t_back)
-            raise Exception(err)
-        if landmarks == "EOF":
-            exit(0)
-
-        alignment = self.extracted_to_alignment((landmarks["detected_faces"][0],
-                                                 landmarks["landmarks"][0]))
+        self.queues["in"].put({"image": self.media["image"],
+                               "filename": self.media["frame_id"],
+                               "manual_face": self.media["bounding_box"]})
+        detected_face = next(self.extractor.detected_faces())["detected_faces"][0]
+        alignment = detected_face.to_alignment()
+
         frame = self.media["frame_id"]
 
         if self.interface.get_selected_face_id() is None:
@@ -999,15 +943,3 @@ class MouseHandler():
 
         self.interface.state["edit"]["updated"] = True
         self.interface.state["edit"]["update_faces"] = True
-
-    @staticmethod
-    def extracted_to_alignment(extract_data):
-        """ Convert Extracted Tuple to Alignments data """
-        alignment = dict()
-        bbox, landmarks = extract_data
-        alignment["x"] = bbox["left"]
-        alignment["w"] = bbox["right"] - bbox["left"]
-        alignment["y"] = bbox["top"]
-        alignment["h"] = bbox["bottom"] - bbox["top"]
-        alignment["landmarksXY"] = landmarks
-        return alignment
diff --git a/tools/sort.py b/tools/sort.py
index 0391164..b7c3f4e 100644
--- a/tools/sort.py
+++ b/tools/sort.py
@@ -16,8 +16,7 @@ from tqdm import tqdm
 from lib.cli import FullHelpArgumentParser
 from lib import Serializer
 from lib.faces_detect import DetectedFace
-from lib.multithreading import SpawnProcess
-from lib.queue_manager import queue_manager, QueueEmpty
+from lib.queue_manager import queue_manager
 from lib.utils import cv2_read_img
 from lib.vgg_face2_keras import VGGFace2 as VGGFace
 from plugins.plugin_loader import PluginLoader
@@ -85,48 +84,22 @@ class Sort():
 
         self.sort_process()
 
-    def launch_aligner(self):
+    @staticmethod
+    def launch_aligner():
         """ Load the aligner plugin to retrieve landmarks """
-        out_queue = queue_manager.get_queue("out")
-        kwargs = {"in_queue": queue_manager.get_queue("in"),
-                  "out_queue": out_queue}
-
-        for plugin in ("fan", "cv2_dnn"):
-            aligner = PluginLoader.get_aligner(plugin)(loglevel=self.args.loglevel)
-            process = SpawnProcess(aligner.run, **kwargs)
-            event = process.event
-            process.start()
-            # Wait for Aligner to take init
-            # The first ever load of the model for FAN has reportedly taken
-            # up to 3-4 minutes, hence high timeout.
-            event.wait(300)
-
-            if not event.is_set():
-                if plugin == "fan":
-                    process.join()
-                    logger.error("Error initializing FAN. Trying CV2-DNN")
-                    continue
-                else:
-                    raise ValueError("Error inititalizing Aligner")
-            if plugin == "cv2_dnn":
-                return
-
-            try:
-                err = None
-                err = out_queue.get(True, 1)
-            except QueueEmpty:
-                pass
-            if not err:
-                break
-            process.join()
-            logger.error("Error initializing FAN. Trying CV2-DNN")
+        kwargs = dict(in_queue=queue_manager.get_queue("in"),
+                      out_queue=queue_manager.get_queue("out"),
+                      queue_size=8)
+        aligner = PluginLoader.get_aligner("fan")(normalize_method="hist")
+        aligner.batchsize = 1
+        aligner.initialize(**kwargs)
+        aligner.start()
 
     @staticmethod
     def alignment_dict(image):
         """ Set the image to a dict for alignment """
         height, width = image.shape[:2]
         face = DetectedFace(x=0, w=width, y=0, h=height)
-        face = face.to_bounding_box_dict()
         return {"image": image,
                 "detected_faces": [face]}
 
@@ -134,9 +107,11 @@ class Sort():
     def get_landmarks(filename):
         """ Extract the face from a frame (If not alignments file found) """
         image = cv2_read_img(filename, raise_error=True)
-        queue_manager.get_queue("in").put(Sort.alignment_dict(image))
+        feed = Sort.alignment_dict(image)
+        feed["filename"] = filename
+        queue_manager.get_queue("in").put(feed)
         face = queue_manager.get_queue("out").get()
-        landmarks = face["landmarks"][0]
+        landmarks = face["detected_faces"][0].landmarks_xy
         return landmarks
 
     def sort_process(self):
