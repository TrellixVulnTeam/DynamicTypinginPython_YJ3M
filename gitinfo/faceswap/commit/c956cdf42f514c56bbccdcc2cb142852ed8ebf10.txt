commit c956cdf42f514c56bbccdcc2cb142852ed8ebf10
Author: Hidde Jansen <hidde@hiddejansen.com>
Date:   Sat Dec 23 00:16:27 2017 +0100

    Add command line options for train.py

diff --git a/lib/cli.py b/lib/cli.py
index 482a175..62357b4 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -1,15 +1,146 @@
 import argparse
 import os
 import cv2
+import numpy
 
-from lib.utils import get_image_paths, get_folder
+from lib.utils import get_image_paths, get_folder, load_images, stack_images
 from lib.faces_detect import crop_faces
+from lib.training_data import get_training_data
+
+from lib.model import autoencoder_A, autoencoder_B
+from lib.model import encoder, decoder_A, decoder_B
 
 
 class FullPaths(argparse.Action):
     """Expand user- and relative-paths"""
+
     def __call__(self, parser, namespace, values, option_string=None):
-        setattr(namespace, self.dest, os.path.abspath(os.path.expanduser(values)))
+        setattr(namespace, self.dest, os.path.abspath(
+            os.path.expanduser(values)))
+
+
+class TrainingProcessor(object):
+    arguments = None
+
+    def __init__(self, description='default'):
+        print('Initializing')
+        self.parse_arguments(description)
+
+        print("Model A Directory: {}".format(self.arguments.input_A))
+        print("Model B Directory: {}".format(self.arguments.input_B))
+        print("Training data directory: {}".format(self.arguments.model_dir))
+        print('Starting, this may take a while...')
+
+        try:
+            encoder.load_weights(self.arguments.model_dir + '/encoder.h5')
+            decoder_A.load_weights(self.arguments.model_dir + '/decoder_A.h5')
+            decoder_B.load_weights(self.arguments.model_dir + '/decoder_B.h5')
+        except Exception as e:
+            print('Not loading existing training data.')
+
+        self.process()
+
+    def parse_arguments(self, description):
+        parser = argparse.ArgumentParser(
+            description=description,
+            epilog="Questions and feedback: \
+            https://github.com/deepfakes/faceswap-playground"
+        )
+
+        parser.add_argument('-A', '--input-A',
+                            action=FullPaths,
+                            dest="input_A",
+                            default="input_A",
+                            help="Input directory. A directory containing training images for face A.\
+                             Defaults to 'input'")
+        parser.add_argument('-B', '--input-B',
+                            action=FullPaths,
+                            dest="input_B",
+                            default="input_B",
+                            help="Input directory. A directory containing training images for face B.\
+                             Defaults to 'input'")
+        parser.add_argument('-m', '--model-dir',
+                            action=FullPaths,
+                            dest="model_dir",
+                            default="model",
+                            help="Model directory. This is where the training data will \
+                                be stored. Defaults to 'model'")
+        parser.add_argument('-p', '--preview',
+                            action="store_true",
+                            dest="preview",
+                            default=False,
+                            help="Show preview output. If not specified, write progress \
+                            to file.")
+        parser.add_argument('-v', '--verbose',
+                            action="store_true",
+                            dest="verbose",
+                            default=False,
+                            help="Show verbose output")
+        parser = self.add_optional_arguments(parser)
+        self.arguments = parser.parse_args()
+
+    def add_optional_arguments(self, parser):
+        # Override this for custom arguments
+        return parser
+
+    def save_model_weights(self):
+        encoder.save_weights(self.arguments.model_dir + '/encoder.h5')
+        decoder_A.save_weights(self.arguments.model_dir + '/decoder_A.h5')
+        decoder_B.save_weights(self.arguments.model_dir + '/decoder_B.h5')
+        print('save model weights')
+
+    def show_sample(self, test_A, test_B):
+        figure_A = numpy.stack([
+            test_A,
+            autoencoder_A.predict(test_A),
+            autoencoder_B.predict(test_A),
+        ], axis=1)
+        figure_B = numpy.stack([
+            test_B,
+            autoencoder_B.predict(test_B),
+            autoencoder_A.predict(test_B),
+        ], axis=1)
+
+        figure = numpy.concatenate([figure_A, figure_B], axis=0)
+        figure = figure.reshape((4, 7) + figure.shape[1:])
+        figure = stack_images(figure)
+
+        figure = numpy.clip(figure * 255, 0, 255).astype('uint8')
+
+        if self.arguments.preview is True:
+            cv2.imshow('', figure)
+        else:
+            cv2.imwrite('_sample.jpg', figure)
+
+    def process(self):
+        images_A = get_image_paths(self.arguments.images_A)
+        images_B = get_image_paths(self.arguments.images_B)
+        images_A = load_images(images_A) / 255.0
+        images_B = load_images(images_B) / 255.0
+
+        images_A += images_B.mean(axis=(0, 1, 2)) - \
+            images_A.mean(axis=(0, 1, 2))
+
+        print('press "q" to stop training and save model')
+
+        BATCH_SIZE = 64
+
+        for epoch in range(1000000):
+            warped_A, target_A = get_training_data(images_A, BATCH_SIZE)
+            warped_B, target_B = get_training_data(images_B, BATCH_SIZE)
+
+            loss_A = autoencoder_A.train_on_batch(warped_A, target_A)
+            loss_B = autoencoder_B.train_on_batch(warped_B, target_B)
+            print(loss_A, loss_B)
+
+            if epoch % 100 == 0:
+                self.save_model_weights()
+                self.show_sample(target_A[0:14], target_B[0:14])
+
+            key = cv2.waitKey(1)
+            if key == ord('q'):
+                self.save_model_weights()
+                exit()
 
 
 class DirectoryProcessor(object):
diff --git a/train.py b/train.py
index 1fa32f4..74ac82c 100755
--- a/train.py
+++ b/train.py
@@ -1,74 +1,4 @@
-import cv2
-import numpy
-
-from lib.utils import get_image_paths, load_images, stack_images
-from lib.training_data import get_training_data
-
-from lib.model import autoencoder_A
-from lib.model import autoencoder_B
-from lib.model import encoder, decoder_A, decoder_B
-
-try:
-    encoder.load_weights('models/encoder.h5')
-    decoder_A.load_weights('models/decoder_A.h5')
-    decoder_B.load_weights('models/decoder_B.h5')
-except:
-    pass
-
-
-def save_model_weights():
-    encoder.save_weights('models/encoder.h5')
-    decoder_A.save_weights('models/decoder_A.h5')
-    decoder_B.save_weights('models/decoder_B.h5')
-    print('save model weights')
-
-
-def show_sample(test_A, test_B):
-    figure_A = numpy.stack([
-        test_A,
-        autoencoder_A.predict(test_A),
-        autoencoder_B.predict(test_A),
-        ], axis=1)
-    figure_B = numpy.stack([
-        test_B,
-        autoencoder_B.predict(test_B),
-        autoencoder_A.predict(test_B),
-        ], axis=1)
-
-    figure = numpy.concatenate([figure_A, figure_B], axis=0)
-    figure = figure.reshape((4, 7) + figure.shape[1:])
-    figure = stack_images(figure)
-
-    figure = numpy.clip(figure * 255, 0, 255).astype('uint8')
-
-    cv2.imwrite('_sample.jpg', figure)
-
-
-images_A = get_image_paths('data/trump')
-images_B = get_image_paths('data/cage')
-images_A = load_images(images_A) / 255.0
-images_B = load_images(images_B) / 255.0
-
-images_A += images_B.mean(axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2))
-
-print('press "q" to stop training and save model')
-
-BATCH_SIZE = 64
-
-for epoch in range(1000000):
-    warped_A, target_A = get_training_data(images_A, BATCH_SIZE)
-    warped_B, target_B = get_training_data(images_B, BATCH_SIZE)
-
-    loss_A = autoencoder_A.train_on_batch(warped_A, target_A)
-    loss_B = autoencoder_B.train_on_batch(warped_B, target_B)
-    print(loss_A, loss_B)
-
-    if epoch % 100 == 0:
-        save_model_weights()
-        show_sample(target_A[0:14], target_B[0:14])
-
-    key = cv2.waitKey(1)
-    if key == ord('q'):
-        save_model_weights()
-        exit()
+from lib.cli import TrainingProcessor
 
+training_processor = TrainingProcessor(description='Processes a collection of pictures \
+    to train a machine learning model')
