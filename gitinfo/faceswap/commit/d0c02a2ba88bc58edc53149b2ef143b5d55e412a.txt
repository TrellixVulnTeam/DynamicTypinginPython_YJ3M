commit d0c02a2ba88bc58edc53149b2ef143b5d55e412a
Author: Hidde Jansen <Ganonmaster@users.noreply.github.com>
Date:   Wed Dec 20 21:27:14 2017 +0100

    PEP8 / Codestyle (#11)

diff --git a/Dockerfile b/Dockerfile
index b265a6a..24376aa 100644
--- a/Dockerfile
+++ b/Dockerfile
@@ -10,14 +10,14 @@ COPY requirements.txt .
 # install dependencies from python packages
 RUN pip install --upgrade setuptools
 
-RUN pip --no-cache-dir install -r ./requirements.txt
+RUN pip3 --no-cache-dir install -r ./requirements.txt
 
 RUN apt-get install -y \
     cmake \
     libboost-all-dev
 
 # Add these to requirements.txt
-RUN pip --no-cache-dir install \
+RUN pip3 --no-cache-dir install \
     scikit-image \
     # boost \
     dlib
diff --git a/convert_photo.py b/convert_photo.py
index 83b6354..060a14e 100644
--- a/convert_photo.py
+++ b/convert_photo.py
@@ -6,15 +6,15 @@ from lib.utils import get_image_paths, get_folder
 from lib.faces_detect import crop_faces
 from lib.faces_process import convert_one_image
 
-output_dir = get_folder( 'modified' )
+output_dir = get_folder('modified')
 
-images_SRC = get_image_paths( 'original' )
+images_SRC = get_image_paths('original')
 
 for fn in images_SRC:
     image = cv2.imread(fn)
-    for ((x,w),(y,h),face) in crop_faces( image ):
-        new_face = convert_one_image( cv2.resize(face, (256,256)) )
-        image[slice(y,y+h),slice(x,x+w)] = cv2.resize(new_face, (w,h))
-    
+    for ((x, w), (y, h), face) in crop_faces(image):
+        new_face = convert_one_image(cv2.resize(face, (256, 256)))
+        image[slice(y, y + h), slice(x, x + w)] = cv2.resize(new_face, (w, h))
+
     output_file = output_dir / Path(fn).name
-    cv2.imwrite( str(output_file) , image )
+    cv2.imwrite(str(output_file), image)
diff --git a/convert_trump_cage.py b/convert_trump_cage.py
index fdd9813..022b61d 100644
--- a/convert_trump_cage.py
+++ b/convert_trump_cage.py
@@ -1,17 +1,16 @@
 import cv2
-import numpy
 from pathlib import Path
 
 from lib.utils import get_image_paths, get_folder
 from lib.faces_process import convert_one_image
 
-output_dir = get_folder( 'output' )
+output_dir = get_folder('output')
 
-images_A = get_image_paths( 'data/trump' )
-images_B = get_image_paths( 'data/cage' )
+images_A = get_image_paths('data/trump')
+images_B = get_image_paths('data/cage')
 
 for fn in images_A:
     image = cv2.imread(fn)
-    new_image = convert_one_image( image )
+    new_image = convert_one_image(image)
     output_file = output_dir / Path(fn).name
-    cv2.imwrite( str(output_file), new_image )
\ No newline at end of file
+    cv2.imwrite(str(output_file), new_image)
diff --git a/extract.py b/extract.py
index 4b991bf..fc3fec6 100644
--- a/extract.py
+++ b/extract.py
@@ -1,17 +1,16 @@
 import cv2
-import numpy
 from pathlib import Path
 
 from lib.utils import get_image_paths, get_folder
 from lib.faces_detect import crop_faces
 
-output_dir = get_folder( 'extract' )
+output_dir = get_folder('extract')
 
-images_SRC = get_image_paths( 'src' )
+images_SRC = get_image_paths('src')
 
 for fn in images_SRC:
     image = cv2.imread(fn)
-    for (idx, (p1,p2,img)) in enumerate(crop_faces( image )):
-        final = cv2.resize(img, (256,256))
+    for (idx, (p1, p2, img)) in enumerate(crop_faces(image)):
+        final = cv2.resize(img, (256, 256))
         output_file = output_dir / Path(fn).stem
-        cv2.imwrite( str(output_file) + str(idx) + Path(fn).suffix, final )
+        cv2.imwrite(str(output_file) + str(idx) + Path(fn).suffix, final)
diff --git a/lib/aligner.py b/lib/aligner.py
index 19a7d1f..1fbae5b 100644
--- a/lib/aligner.py
+++ b/lib/aligner.py
@@ -13,7 +13,7 @@ class Aligner:
 
     def get_rects(self, img):
         rects = self.detector(img, 1)
-        #print("[+] Number of faces found:", len(rects))
+        # print("[+] Number of faces found:", len(rects))
         return rects
 
     def get_first_rect(self, img):
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index f66d43e..3adc65a 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -1,15 +1,14 @@
 import cv2
-import numpy
 
 # Give right path to the xml file or put it directly in current folder
 face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
 
-def crop_faces( image ):
-#Add : cv.EqualizeHist(image, image) ?
+
+def crop_faces(image):
+    # Add : cv.EqualizeHist(image, image) ?
     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
 
     faces = face_cascade.detectMultiScale(gray, 1.3, 5)
 
-    for (x,y,w,h) in faces:
-        yield ((x,w), (y,h), image[y: y + h, x: x + w])
-
+    for (x, y, w, h) in faces:
+        yield ((x, w), (y, h), image[y: y + h, x: x + w])
diff --git a/lib/faces_process.py b/lib/faces_process.py
index 7c3c45d..0a9da49 100644
--- a/lib/faces_process.py
+++ b/lib/faces_process.py
@@ -1,37 +1,39 @@
 import cv2
 import numpy
 
-from aligner import Aligner
-from model import autoencoder_A
-from model import autoencoder_B
-from model import encoder, decoder_A, decoder_B
+from .aligner import Aligner
+from .model import autoencoder_A
+from .model import autoencoder_B
+from .model import encoder, decoder_A, decoder_B
 
-encoder  .load_weights( "models/encoder.h5"   )
-decoder_A.load_weights( "models/decoder_A.h5" )
-decoder_B.load_weights( "models/decoder_B.h5" )
+encoder.load_weights("models/encoder.h5")
+decoder_A.load_weights("models/decoder_A.h5")
+decoder_B.load_weights("models/decoder_B.h5")
 
 autoencoder = autoencoder_B
 
 # landmark file can be found in http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2
 # unzip it in the same folder as the main scripts
-aligner = Aligner("shape_predictor_68_face_landmarks.dat","mmod_human_face_detector.dat")
-
-def convert_one_image( image ):
-    assert image.shape == (256,256,3)
-    crop = slice(48,208)
-    face = image[crop,crop]
-    face = cv2.resize( face, (64,64) )
-    face = numpy.expand_dims( face, 0 )
-    new_face = autoencoder.predict( face / 255.0 )[0]
-    new_face = numpy.clip( new_face * 255, 0, 255 ).astype( image.dtype )
-    new_face = cv2.resize( new_face, (160,160) )
+aligner = Aligner("shape_predictor_68_face_landmarks.dat", "mmod_human_face_detector.dat")
+
+
+def convert_one_image(image):
+    assert image.shape == (256, 256, 3)
+    crop = slice(48, 208)
+    face = image[crop, crop]
+    face = cv2.resize(face, (64, 64))
+    face = numpy.expand_dims(face, 0)
+    new_face = autoencoder.predict(face / 255.0)[0]
+    new_face = numpy.clip(new_face * 255, 0, 255).astype(image.dtype)
+    new_face = cv2.resize(new_face, (160, 160))
     result = aligner.align(image.copy(), new_face)
     if result is None:
         return superpose(image, new_face, crop)
     else:
         return result
 
+
 def superpose(image, new_face, crop):
     new_image = image.copy()
-    new_image[crop,crop] = new_face
+    new_image[crop, crop] = new_face
     return new_image
diff --git a/lib/image_augmentation.py b/lib/image_augmentation.py
index 6fd6dc1..25f812d 100644
--- a/lib/image_augmentation.py
+++ b/lib/image_augmentation.py
@@ -1,41 +1,42 @@
 import cv2
 import numpy
 
-from umeyama import umeyama
+from .umeyama import umeyama
 
-def random_transform( image, rotation_range, zoom_range, shift_range, random_flip ):
+
+def random_transform(image, rotation_range, zoom_range, shift_range, random_flip):
     h,w = image.shape[0:2]
-    rotation = numpy.random.uniform( -rotation_range, rotation_range )
-    scale = numpy.random.uniform( 1 - zoom_range, 1 + zoom_range )
-    tx = numpy.random.uniform( -shift_range, shift_range ) * w
-    ty = numpy.random.uniform( -shift_range, shift_range ) * h
-    mat = cv2.getRotationMatrix2D( (w//2,h//2), rotation, scale )
-    mat[:,2] += (tx,ty)
-    result = cv2.warpAffine( image, mat, (w,h), borderMode=cv2.BORDER_REPLICATE )
+    rotation = numpy.random.uniform(-rotation_range, rotation_range)
+    scale = numpy.random.uniform(1 - zoom_range, 1 + zoom_range)
+    tx = numpy.random.uniform(-shift_range, shift_range) * w
+    ty = numpy.random.uniform(-shift_range, shift_range) * h
+    mat = cv2.getRotationMatrix2D((w // 2, h // 2), rotation, scale)
+    mat[:, 2] += (tx, ty)
+    result = cv2.warpAffine(image, mat, (w, h), borderMode=cv2.BORDER_REPLICATE)
     if numpy.random.random() < random_flip:
-        result = result[:,::-1]
+        result = result[:, ::-1]
     return result
 
 # get pair of random warped images from aligened face image
-def random_warp( image ):
-    assert image.shape == (256,256,3)
-    range_ = numpy.linspace( 128-80, 128+80, 5 )
-    mapx = numpy.broadcast_to( range_, (5,5) )
+def random_warp(image):
+    assert image.shape == (256, 256, 3)
+    range_ = numpy.linspace(128 - 80, 128 + 80, 5)
+    mapx = numpy.broadcast_to(range_, (5, 5))
     mapy = mapx.T
 
-    mapx = mapx + numpy.random.normal( size=(5,5), scale=5 )
-    mapy = mapy + numpy.random.normal( size=(5,5), scale=5 )
+    mapx = mapx + numpy.random.normal(size=(5, 5), scale=5)
+    mapy = mapy + numpy.random.normal(size=(5, 5), scale=5)
 
-    interp_mapx = cv2.resize( mapx, (80,80) )[8:72,8:72].astype('float32')
-    interp_mapy = cv2.resize( mapy, (80,80) )[8:72,8:72].astype('float32')
+    interp_mapx = cv2.resize(mapx, (80, 80))[8:72, 8:72].astype('float32')
+    interp_mapy = cv2.resize(mapy, (80, 80))[8:72, 8:72].astype('float32')
 
-    warped_image = cv2.remap( image, interp_mapx, interp_mapy, cv2.INTER_LINEAR )
+    warped_image = cv2.remap(image, interp_mapx, interp_mapy, cv2.INTER_LINEAR)
 
-    src_points = numpy.stack( [ mapx.ravel(), mapy.ravel() ], axis=-1 )
-    dst_points = numpy.mgrid[0:65:16,0:65:16].T.reshape(-1,2)
-    mat = umeyama( src_points, dst_points, True )[0:2]
+    src_points = numpy.stack([mapx.ravel(), mapy.ravel()], axis=-1)
+    dst_points = numpy.mgrid[0:65:16, 0:65:16].T.reshape(-1, 2)
+    mat = umeyama(src_points, dst_points, True)[0:2]
 
-    target_image = cv2.warpAffine( image, mat, (64,64) )
+    target_image = cv2.warpAffine(image, mat, (64, 64))
 
     return warped_image, target_image
 
diff --git a/lib/model.py b/lib/model.py
index 8c3b003..86813ed 100644
--- a/lib/model.py
+++ b/lib/model.py
@@ -4,58 +4,63 @@ from keras.layers.advanced_activations import LeakyReLU
 from keras.layers.convolutional import Conv2D
 from keras.optimizers import Adam
 
-from pixel_shuffler import PixelShuffler
+from .pixel_shuffler import PixelShuffler
 
-optimizer = Adam( lr=5e-5, beta_1=0.5, beta_2=0.999 )
+optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)
 
-IMAGE_SHAPE = (64,64,3)
+IMAGE_SHAPE = (64, 64, 3)
 ENCODER_DIM = 1024
 
-def conv( filters ):
+
+def conv(filters):
     def block(x):
-        x = Conv2D( filters, kernel_size=5, strides=2, padding='same' )(x)
+        x = Conv2D(filters, kernel_size=5, strides=2, padding='same')(x)
         x = LeakyReLU(0.1)(x)
         return x
     return block
 
-def upscale( filters ):
+
+def upscale(filters):
     def block(x):
-        x = Conv2D( filters*4, kernel_size=3, padding='same' )(x)
+        x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)
         x = LeakyReLU(0.1)(x)
         x = PixelShuffler()(x)
         return x
     return block
 
+
 def Encoder():
-    input_ = Input( shape=IMAGE_SHAPE )
+    input_ = Input(shape=IMAGE_SHAPE)
     x = input_
-    x = conv( 128)(x)
-    x = conv( 256)(x)
-    x = conv( 512)(x)
+    x = conv(128)(x)
+    x = conv(256)(x)
+    x = conv(512)(x)
     x = conv(1024)(x)
-    x = Dense( ENCODER_DIM )( Flatten()(x) )
-    x = Dense(4*4*1024)(x)
-    x = Reshape((4,4,1024))(x)
+    x = Dense(ENCODER_DIM)(Flatten()(x))
+    x = Dense(4 * 4 * 1024)(x)
+    x = Reshape((4, 4, 1024))(x)
     x = upscale(512)(x)
-    return Model( input_, x )
+    return Model(input_, x)
+
 
 def Decoder():
-    input_ = Input( shape=(8,8,512) )
+    input_ = Input(shape=(8, 8, 512))
     x = input_
     x = upscale(256)(x)
     x = upscale(128)(x)
-    x = upscale( 64)(x)
-    x = Conv2D( 3, kernel_size=5, padding='same', activation='sigmoid' )(x)
-    return Model( input_, x )
+    x = upscale(64)(x)
+    x = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+    return Model(input_, x)
+
 
 encoder = Encoder()
 decoder_A = Decoder()
 decoder_B = Decoder()
 
-x = Input( shape=IMAGE_SHAPE )
+x = Input(shape=IMAGE_SHAPE)
 
-autoencoder_A = Model( x, decoder_A( encoder(x) ) )
-autoencoder_B = Model( x, decoder_B( encoder(x) ) )
-autoencoder_A.compile( optimizer=optimizer, loss='mean_absolute_error' )
-autoencoder_B.compile( optimizer=optimizer, loss='mean_absolute_error' )
+autoencoder_A = Model(x, decoder_A(encoder(x)))
+autoencoder_B = Model(x, decoder_B(encoder(x)))
+autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')
+autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')
 
diff --git a/lib/pixel_shuffler.py b/lib/pixel_shuffler.py
index 2f4fc68..9904191 100644
--- a/lib/pixel_shuffler.py
+++ b/lib/pixel_shuffler.py
@@ -6,6 +6,7 @@ from keras.utils import conv_utils
 from keras.engine.topology import Layer
 import keras.backend as K
 
+
 class PixelShuffler(Layer):
     def __init__(self, size=(2, 2), data_format=None, **kwargs):
         super(PixelShuffler, self).__init__(**kwargs)
diff --git a/lib/training_data.py b/lib/training_data.py
index c5c56c5..d9be3fc 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -1,6 +1,6 @@
 import numpy
-from image_augmentation import random_transform
-from image_augmentation import random_warp
+from .image_augmentation import random_transform
+from .image_augmentation import random_warp
 
 random_transform_args = {
     'rotation_range': 10,
@@ -9,16 +9,17 @@ random_transform_args = {
     'random_flip': 0.4,
     }
 
-def get_training_data( images, batch_size ):
-    indices = numpy.random.randint( len(images), size=batch_size )
-    for i,index in enumerate(indices):
+
+def get_training_data(images, batch_size):
+    indices = numpy.random.randint(len(images), size=batch_size)
+    for i, index in enumerate(indices):
         image = images[index]
-        image = random_transform( image, **random_transform_args )
-        warped_img, target_img = random_warp( image )
+        image = random_transform(image, **random_transform_args)
+        warped_img, target_img = random_warp(image)
 
         if i == 0:
-            warped_images = numpy.empty( (batch_size,) + warped_img.shape, warped_img.dtype )
-            target_images = numpy.empty( (batch_size,) + target_img.shape, warped_img.dtype )
+            warped_images = numpy.empty((batch_size,) + warped_img.shape, warped_img.dtype)
+            target_images = numpy.empty((batch_size,) + target_img.shape, warped_img.dtype)
 
         warped_images[i] = warped_img
         target_images[i] = target_img
diff --git a/lib/umeyama.py b/lib/umeyama.py
index b5ac96c..a835484 100644
--- a/lib/umeyama.py
+++ b/lib/umeyama.py
@@ -12,7 +12,8 @@
 
 import numpy as np
 
-def umeyama( src, dst, estimate_scale ):
+
+def umeyama(src, dst, estimate_scale):
     """Estimate N-D similarity transformation with or without scaling.
     Parameters
     ----------
@@ -81,4 +82,3 @@ def umeyama( src, dst, estimate_scale ):
     T[:dim, :dim] *= scale
 
     return T
-
diff --git a/lib/utils.py b/lib/utils.py
index 2bd23bd..f2e3a71 100644
--- a/lib/utils.py
+++ b/lib/utils.py
@@ -5,38 +5,43 @@ import os
 from pathlib import Path
 from scandir import scandir
 
-def get_folder( path ):
-    output_dir = Path( path )
-    #output_dir.mkdir( parents=True, exist_ok=True )
+
+def get_folder(path):
+    output_dir = Path(path)
+    # output_dir.mkdir(parents=True, exist_ok=True)
     return output_dir
 
-def get_image_paths( directory ):
-    return [ x.path for x in scandir( directory ) if x.name.endswith('.jpg') or x.name.endswith('.png') ]
 
-def load_images( image_paths, convert=None ):
-    iter_all_images = ( cv2.imread(fn) for fn in image_paths )
+def get_image_paths(directory):
+    return [x.path for x in scandir(directory) if x.name.endswith('.jpg') or x.name.endswith('.png')]
+
+
+def load_images(image_paths, convert=None):
+    iter_all_images = (cv2.imread(fn) for fn in image_paths)
     if convert:
-        iter_all_images = ( convert(img) for img in iter_all_images )
-    for i,image in enumerate( iter_all_images ):
+        iter_all_images = (convert(img) for img in iter_all_images)
+    for i, image in enumerate(iter_all_images):
         if i == 0:
-            all_images = numpy.empty( ( len(image_paths), ) + image.shape, dtype=image.dtype )
+            all_images = numpy.empty((len(image_paths), ) + image.shape, dtype=image.dtype)
         all_images[i] = image
     return all_images
 
-def get_transpose_axes( n ):
+
+def get_transpose_axes(n):
     if n % 2 == 0:
-        y_axes = list( range( 1, n-1, 2 ) )
-        x_axes = list( range( 0, n-1, 2 ) )
+        y_axes = list(range(1, n - 1, 2))
+        x_axes = list(range(0, n - 1, 2))
     else:
-        y_axes = list( range( 0, n-1, 2 ) )
-        x_axes = list( range( 1, n-1, 2 ) )
-    return y_axes, x_axes, [n-1]
-
-def stack_images( images ):
-    images_shape = numpy.array( images.shape )
-    new_axes = get_transpose_axes( len( images_shape ) )
-    new_shape = [ numpy.prod( images_shape[x] ) for x in new_axes ]
+        y_axes = list(range(0, n - 1, 2))
+        x_axes = list(range(1, n - 1, 2))
+    return y_axes, x_axes, [n - 1]
+
+
+def stack_images(images):
+    images_shape = numpy.array(images.shape)
+    new_axes = get_transpose_axes(len(images_shape))
+    new_shape = [numpy.prod(images_shape[x]) for x in new_axes]
     return numpy.transpose(
         images,
-        axes = numpy.concatenate( new_axes )
-        ).reshape( new_shape )
+        axes=numpy.concatenate(new_axes)
+        ).reshape(new_shape)
diff --git a/train.py b/train.py
index b808fe1..1fa32f4 100755
--- a/train.py
+++ b/train.py
@@ -9,56 +9,59 @@ from lib.model import autoencoder_B
 from lib.model import encoder, decoder_A, decoder_B
 
 try:
-    encoder  .load_weights( 'models/encoder.h5'   )
-    decoder_A.load_weights( 'models/decoder_A.h5' )
-    decoder_B.load_weights( 'models/decoder_B.h5' )
+    encoder.load_weights('models/encoder.h5')
+    decoder_A.load_weights('models/decoder_A.h5')
+    decoder_B.load_weights('models/decoder_B.h5')
 except:
     pass
 
+
 def save_model_weights():
-    encoder  .save_weights( 'models/encoder.h5'   )
-    decoder_A.save_weights( 'models/decoder_A.h5' )
-    decoder_B.save_weights( 'models/decoder_B.h5' )
-    print( 'save model weights' )
+    encoder.save_weights('models/encoder.h5')
+    decoder_A.save_weights('models/decoder_A.h5')
+    decoder_B.save_weights('models/decoder_B.h5')
+    print('save model weights')
+
 
-def show_sample( test_A, test_B ):
+def show_sample(test_A, test_B):
     figure_A = numpy.stack([
         test_A,
-        autoencoder_A.predict( test_A ),
-        autoencoder_B.predict( test_A ),
-        ], axis=1 )
+        autoencoder_A.predict(test_A),
+        autoencoder_B.predict(test_A),
+        ], axis=1)
     figure_B = numpy.stack([
         test_B,
-        autoencoder_B.predict( test_B ),
-        autoencoder_A.predict( test_B ),
-        ], axis=1 )
+        autoencoder_B.predict(test_B),
+        autoencoder_A.predict(test_B),
+        ], axis=1)
+
+    figure = numpy.concatenate([figure_A, figure_B], axis=0)
+    figure = figure.reshape((4, 7) + figure.shape[1:])
+    figure = stack_images(figure)
 
-    figure = numpy.concatenate( [ figure_A, figure_B ], axis=0 )
-    figure = figure.reshape( (4,7) + figure.shape[1:] )
-    figure = stack_images( figure )
+    figure = numpy.clip(figure * 255, 0, 255).astype('uint8')
 
-    figure = numpy.clip( figure * 255, 0, 255 ).astype('uint8')
+    cv2.imwrite('_sample.jpg', figure)
 
-    cv2.imwrite( '_sample.jpg', figure )
 
-images_A = get_image_paths( 'data/trump' )
-images_B = get_image_paths( 'data/cage'  )
-images_A = load_images( images_A ) / 255.0
-images_B = load_images( images_B ) / 255.0
+images_A = get_image_paths('data/trump')
+images_B = get_image_paths('data/cage')
+images_A = load_images(images_A) / 255.0
+images_B = load_images(images_B) / 255.0
 
-images_A += images_B.mean( axis=(0,1,2) ) - images_A.mean( axis=(0,1,2) )
+images_A += images_B.mean(axis=(0, 1, 2)) - images_A.mean(axis=(0, 1, 2))
 
-print( 'press "q" to stop training and save model' )
+print('press "q" to stop training and save model')
 
 BATCH_SIZE = 64
 
 for epoch in range(1000000):
-    warped_A, target_A = get_training_data( images_A, BATCH_SIZE )
-    warped_B, target_B = get_training_data( images_B, BATCH_SIZE )
+    warped_A, target_A = get_training_data(images_A, BATCH_SIZE)
+    warped_B, target_B = get_training_data(images_B, BATCH_SIZE)
 
-    loss_A = autoencoder_A.train_on_batch( warped_A, target_A )
-    loss_B = autoencoder_B.train_on_batch( warped_B, target_B )
-    print( loss_A, loss_B )
+    loss_A = autoencoder_A.train_on_batch(warped_A, target_A)
+    loss_B = autoencoder_B.train_on_batch(warped_B, target_B)
+    print(loss_A, loss_B)
 
     if epoch % 100 == 0:
         save_model_weights()
