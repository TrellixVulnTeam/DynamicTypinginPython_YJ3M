commit a14bb9d6369ff5bc0fd4e8f956bc581bb03450d0
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Wed Sep 18 23:00:55 2019 +0000

    extract: Autoset batchsize if it is too large in singleprocess mode

diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index 45a50f1..7e22292 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -70,6 +70,7 @@ class Extractor():
         self._detector = self._load_detector(detector, rotate_images, min_size, configfile)
         self._aligner = self._load_aligner(aligner, configfile, normalize_method)
         self._is_parallel = self._set_parallel_processing(multiprocess)
+        self._set_extractor_batchsize()
         self._queues = self._add_queues()
         logger.debug("Initialized %s", self.__class__.__name__)
 
@@ -295,8 +296,6 @@ class Extractor():
             logger.warning("Not enough free VRAM for parallel processing. "
                            "Switching to serial")
             return False
-
-        self._set_extractor_batchsize(vram_required, vram_free)
         return True
 
     # << INTERNAL PLUGIN HANDLING >> #
@@ -337,22 +336,46 @@ class Extractor():
         self._detector.start()
         logger.debug("Launched Detector")
 
-    def _set_extractor_batchsize(self, vram_required, vram_free):
-        """ Sets the batchsize of the used plugins based on their vram and
-            vram_per_batch_requirements """
-        batch_required = ((self._aligner.vram_per_batch * self._aligner.batchsize) +
-                          (self._detector.vram_per_batch * self._detector.batchsize))
-        plugin_required = vram_required + batch_required
-        if plugin_required <= vram_free:
-            logger.verbose("Plugin requirements within threshold: (plugin_required: %sMB, "
-                           "vram_free: %sMB)", plugin_required, vram_free)
+    def _set_extractor_batchsize(self):
+        """ Sets the batchsize of the requested plugins based on their vram and
+            vram_per_batch_requirements if the the configured batchsize requires more
+            vram than is available. Nvidia only. """
+        if (self._detector.vram == 0 and self._aligner.vram == 0) or get_backend() != "nvidia":
+            logger.debug("Either detector and aligner have no VRAM requirements or not running "
+                         "on Nvidia. Not updating batchsize requirements/")
             return
-        # Hacky split across 2 plugins
-        available_for_batching = (vram_free - vram_required) // 2
-        self._aligner.batchsize = max(1, available_for_batching // self._aligner.vram_per_batch)
-        self._detector.batchsize = max(1, available_for_batching // self._detector.vram_per_batch)
-        logger.verbose("Reset batchsizes: (aligner: %s, detector: %s)",
-                       self._aligner.batchsize, self._detector.batchsize)
+        stats = GPUStats().get_card_most_free()
+        vram_free = int(stats["free"])
+        if self._is_parallel:
+            vram_required = self._detector.vram + self._aligner.vram + self._vram_buffer
+            batch_required = ((self._aligner.vram_per_batch * self._aligner.batchsize) +
+                              (self._detector.vram_per_batch * self._detector.batchsize))
+            plugin_required = vram_required + batch_required
+            if plugin_required <= vram_free:
+                logger.debug("Plugin requirements within threshold: (plugin_required: %sMB, "
+                             "vram_free: %sMB)", plugin_required, vram_free)
+                return
+            # Hacky split across 2 plugins
+            available_vram = (vram_free - vram_required) // 2
+            for plugin in (self._aligner, self._detector):
+                self._set_plugin_batchsize(plugin, available_vram)
+        else:
+            for plugin in (self._aligner, self._detector):
+                vram_required = plugin.vram + self._vram_buffer
+                batch_required = plugin.vram_per_batch * plugin.batchsize
+                plugin_required = vram_required + batch_required
+                if plugin_required <= vram_free:
+                    logger.debug("%s requirements within threshold: (plugin_required: %sMB, "
+                                 "vram_free: %sMB)", plugin.name, plugin_required, vram_free)
+                    continue
+                available_vram = vram_free - vram_required
+                self._set_plugin_batchsize(plugin, available_vram)
+
+    @staticmethod
+    def _set_plugin_batchsize(plugin, available_vram):
+        """ Set the batchsize for the given plugin based on given available vram """
+        plugin.batchsize = max(1, available_vram // plugin.vram_per_batch)
+        logger.verbose("Reset batchsize for %s to %s", plugin.name, plugin.batchsize)
 
     def _join_threads(self):
         """ Join threads for current pass """
