commit 95f8362c73258ea99c7ea50d1f9271adc63794c1
Author: andenixa <37909402+andenixa@users.noreply.github.com>
Date:   Mon May 14 17:37:13 2018 +0300

    Added OriginalHighRes model converter (#389)
    
    * Adjusted converter to work with OriginalHighRes model vector dim
    
    * fixed field name to args
    
    * New OriginalHighRes with lighter Encoder and decoder with more clarity

diff --git a/plugins/Model_OriginalHighRes/Model.py b/plugins/Model_OriginalHighRes/Model.py
new file mode 100644
index 0000000..a40f170
--- /dev/null
+++ b/plugins/Model_OriginalHighRes/Model.py
@@ -0,0 +1,284 @@
+# Based on the original https://www.reddit.com/r/deepfakes/ code sample + contribs
+# Based on https://github.com/iperov/OpenDeepFaceSwap for Decoder multiple res block chain
+# Based on the https://github.com/shaoanlu/faceswap-GAN repo
+# source : https://github.com/shaoanlu/faceswap-GAN/blob/master/FaceSwap_GAN_v2_sz128_train.ipynbtemp/faceswap_GAN_keras.ipynb
+
+
+import os
+import sys
+
+from keras.initializers import RandomNormal
+from keras.layers import Input, Dense, Flatten, Reshape
+from keras.layers import SeparableConv2D, add
+from keras.layers.advanced_activations import LeakyReLU
+from keras.layers.convolutional import Conv2D
+from keras.models import Model as KerasModel
+from keras.optimizers import Adam
+from keras.utils import multi_gpu_model
+
+from lib.PixelShuffler import PixelShuffler
+
+from . import __version__
+from keras.layers.core import Activation
+
+
+if isinstance(__version__, (list, tuple)):
+    version_str = ".".join([str(n) for n in __version__[1:]])
+else: 
+    version_str = __version__
+
+
+mswindows = sys.platform=="win32"
+
+
+try:
+    from lib.utils import backup_file
+except ImportError:
+    pass
+
+
+class Encoders():
+    REGULAR = 'v2' # high memory consumption encoder
+    NEW_SLIM = 'v3' # slightly lighter on resources and taining speed is faster
+
+
+ENCODER = Encoders.NEW_SLIM
+
+
+hdf = {'encoderH5': f'encoder_{version_str}{ENCODER}.h5',
+       'decoder_AH5': f'decoder_A_{version_str}{ENCODER}.h5',
+       'decoder_BH5': f'decoder_B_{version_str}{ENCODER}.h5'}
+
+
+class Model():
+
+    # still playing with dims
+    ENCODER_DIM = 2048
+        
+    IMAGE_SIZE = 128, 128
+    IMAGE_DEPTH = len('RGB') # good to let ppl know what these are...
+    IMAGE_SHAPE = *IMAGE_SIZE, IMAGE_DEPTH
+    
+    def __init__(self, model_dir, gpus):
+        
+        if mswindows:  
+            from ctypes import cdll    
+            mydll = cdll.LoadLibrary("user32.dll")
+            mydll.SetProcessDPIAware(True)        
+        
+        self.model_dir = model_dir
+        
+        # can't chnage gpu's when the model is initialized no point in making it r/w
+        self._gpus = gpus 
+        
+        Encoder = getattr(self, "Encoder") if not ENCODER else getattr(self, "Encoder_{}".format(ENCODER))
+        
+        self.encoder = Encoder()
+        self.decoder_A = self.Decoder()
+        self.decoder_B = self.Decoder()
+        
+        self.initModel()        
+
+    
+    def initModel(self):
+        optimizer = Adam(lr=5e-5, beta_1=0.5, beta_2=0.999)
+        
+        x = Input(shape=self.IMAGE_SHAPE)
+        
+        self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))
+        self.autoencoder_B = KerasModel(x, self.decoder_B(self.encoder(x)))
+        
+        if self.gpus > 1:
+            self.autoencoder_A = multi_gpu_model( self.autoencoder_A , self.gpus)
+            self.autoencoder_B = multi_gpu_model( self.autoencoder_B , self.gpus)
+        
+        self.autoencoder_A.compile(optimizer=optimizer, loss='mean_absolute_error')
+        self.autoencoder_B.compile(optimizer=optimizer, loss='mean_absolute_error')
+        
+        
+    def load(self, swapped):
+        
+        face_A, face_B = (hdf['decoder_AH5'], hdf['decoder_BH5']) if not swapped else (hdf['decoder_BH5'], hdf['decoder_AH5'])
+
+        try:            
+            self.encoder.load_weights(os.path.join(self.model_dir, hdf['encoderH5']))
+            self.decoder_A.load_weights(os.path.join(self.model_dir, face_A))
+            self.decoder_B.load_weights(os.path.join(self.model_dir, face_B))
+            print('loaded model weights')
+            return True
+        except Exception as e:
+            print('Failed loading existing training data.')
+            print(e)
+            return False        
+
+    def converter(self, swap):
+        autoencoder = self.autoencoder_B if not swap else self.autoencoder_A
+        return autoencoder.predict
+    
+    def conv(self, filters, kernel_size=4, strides=2):
+        def block(x):
+            x = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding='same')(x)         
+            x = LeakyReLU(0.1)(x)
+            return x
+        return block    
+        
+    def conv_sep(self, filters):
+        def block(x):
+            x = SeparableConv2D(filters, kernel_size=4, strides=2, padding='same')(x)        
+            x = LeakyReLU(0.1)(x)
+            return x
+        return block
+    
+    def conv_sep_v3(self, filters, kernel_size=4, strides=2):
+        def block(x):
+            x = SeparableConv2D(filters, kernel_size=kernel_size, strides=strides, padding="same")(x)
+            x = Activation("relu")(x)
+            return x
+        return block       
+    
+    
+    def upscale(self, filters):
+        def block(x):
+            x = Conv2D(filters * 4, kernel_size=3, padding='same')(x)
+            x = LeakyReLU(0.1)(x)
+            x = PixelShuffler()(x)
+            return x
+        return block  
+    
+    def upscale_sep(self, filters):
+        def block(x):
+            x = SeparableConv2D(filters * 4, kernel_size=3, padding='same')(x)
+            x = LeakyReLU(0.1)(x)
+            x = PixelShuffler()(x)
+            return x
+        return block      
+    
+    def res(self, filters, dilation_rate=1):
+        def block(x):
+            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
+            rb = LeakyReLU(alpha=0.2)(rb)
+            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
+            x = add([rb, x])
+            x = LeakyReLU(alpha=0.2)(x)
+            return x
+        return block    
+  
+    
+    def res_block(self, filters, dilation_rate=1):
+        def block(x):
+            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(x)
+            rb = LeakyReLU(alpha=0.2)(rb)
+            rb = Conv2D(filters, kernel_size=3, padding="same", dilation_rate=dilation_rate, use_bias=False)(rb)
+            x = add([rb, x])
+            x = LeakyReLU(alpha=0.2)(x)
+            return x
+        return block       
+    
+    
+    def Encoder_v3(self):
+        """Lighter on resources encoder with bigger first conv layer"""
+        retina = Input(shape=self.IMAGE_SHAPE)
+        x = self.conv_sep_v3(192)(retina)         
+        x = self.conv(256)(x)
+        x = self.conv(384)(x)        
+        x = self.conv_sep_v3(512)(x)
+        x = self.conv(768)(x)
+        x = self.conv_sep_v3(1024)(x)
+        x = Dense(self.ENCODER_DIM)(Flatten()(x))
+        x = Dense(4 * 4 * 1024)(x)
+        x = Reshape((4, 4, 1024))(x)
+        out = self.upscale(512)(x)
+        return KerasModel(retina, out)    
+    
+    
+    def Encoder_v2(self):
+        """Old algorithm; pretty good but slow"""
+        retina = Input(shape=self.IMAGE_SHAPE)
+        x = self.conv(128)(retina)         
+        x = self.conv(144)(x)              
+        x = self.conv_sep(256)(x)
+        x = self.conv(448)(x)        
+        x = self.conv_sep(512)(x)        
+        x = self.conv(768)(x)
+        x = self.conv_sep(1024)(x)
+        x = Dense(self.ENCODER_DIM)(Flatten()(x))
+        x = Dense(4 * 4 * 1024)(x)
+        x = Reshape((4, 4, 1024))(x)
+        out = self.upscale(512)(x)
+        return KerasModel(retina, out)
+    
+
+    def Decoder(self):
+        inp = Input(shape=(8, 8, 512))
+        x = self.upscale(384)(inp)
+        x = self.res_block(384)(x)
+        x = self.upscale_sep(192)(x)
+        x = self.res_block(192)(x)
+        x = self.upscale(128)(x)
+        x = self.res_block(128)(x)
+        x = self.upscale(64)(x)
+        x = self.res_block(64)(x)                    
+        
+#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(x)
+#         rb = LeakyReLU(alpha=0.2)(rb)
+#         rb = Conv2D(64, kernel_size=3, padding="same", dilation_rate=2)(rb)
+#         x = add([rb, x])
+# 
+        #x = self.upscale(32)(x)        
+                        
+        out = Conv2D(3, kernel_size=5, padding='same', activation='sigmoid')(x)
+                
+        return KerasModel(inp, out)
+        
+    
+    def save_weights(self):
+        from threading import Thread
+        from time import sleep
+        
+        model_dir = str(self.model_dir)
+        
+        try:
+            for model in hdf.values():            
+                backup_file(model_dir, model)
+        except NameError:
+            print('backup functionality not available')   
+            pass
+        
+        #thought maybe I/O bound, sometimes saving in parallel is faster
+        threads = []
+        t = Thread(target=self.encoder.save_weights, args=(os.path.join(model_dir, hdf['encoderH5']),))
+        threads.append(t)         
+        t = Thread(target=self.decoder_A.save_weights, args=(os.path.join(model_dir, hdf['decoder_AH5']),))
+        threads.append(t)
+        t = Thread(target=self.decoder_B.save_weights, args=(os.path.join(model_dir, hdf['decoder_BH5']),))
+        threads.append(t)
+        
+        for thread in threads:
+            thread.start()            
+        
+        while any([t.is_alive() for t in threads]):
+            sleep(0.1)
+            
+        print('saved model weights')              
+    
+        
+                
+    @property
+    def gpus(self):
+        return self._gpus
+    
+    @property
+    def model_name(self):
+        try:
+            return self._model_nomen
+        except AttributeError:
+            self._model_nomen = self._model_nomen = os.path.split(os.path.dirname(__file__))[1].replace("Model_", "")            
+        return self._model_nomen
+             
+    
+    def __str__(self):
+        return "<{}: ver={}, nn_dims={}, img_size={}>".format(self.model_name, 
+                                                              version_str, 
+                                                              self.ENCODER_DIM, 
+                                                              "x".join([str(n) for n in self.IMAGE_SHAPE[:2]]))        
+        
diff --git a/plugins/Model_OriginalHighRes/Trainer.py b/plugins/Model_OriginalHighRes/Trainer.py
new file mode 100644
index 0000000..e05f90a
--- /dev/null
+++ b/plugins/Model_OriginalHighRes/Trainer.py
@@ -0,0 +1,82 @@
+
+import time
+
+import numpy
+
+from lib.training_data import TrainingDataGenerator, stack_images
+
+
+TRANSFORM_PRC = 115.
+#TRANSFORM_PRC = 150.
+
+
+class Trainer():
+#     
+    _random_transform_args = {
+        'rotation_range': 10 * (TRANSFORM_PRC * .01),
+        'zoom_range': 0.05 * (TRANSFORM_PRC * .01),
+        'shift_range': 0.05 * (TRANSFORM_PRC * .01),
+        'random_flip': 0.4 * (TRANSFORM_PRC * .01),
+    }
+    
+    def __init__(self, model, fn_A, fn_B, batch_size, *args):
+        self.batch_size = batch_size
+        self.model = model
+
+        #generator = TrainingDataGenerator(self.random_transform_args, 160)
+                
+        # make sre to keep zoom=2 or you won't get 128x128 vectors as input
+        #generator = TrainingDataGenerator(self.random_transform_args, 220, 5, zoom=2)
+        generator = TrainingDataGenerator(self.random_transform_args, 160, 6, zoom=2)
+        #generator = TrainingDataGenerator(self.random_transform_args, 180, 7, zoom=2)
+        
+        self.images_A = generator.minibatchAB(fn_A, self.batch_size)
+        self.images_B = generator.minibatchAB(fn_B, self.batch_size)
+                
+        self.generator = generator        
+        
+
+    def train_one_step(self, iter_no, viewer):
+  
+        _, warped_A, target_A = next(self.images_A)
+        _, warped_B, target_B = next(self.images_B)
+
+        loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)
+        loss_B = self.model.autoencoder_B.train_on_batch(warped_B, target_B)        
+                        
+        print("[{0}] [#{1:05d}] loss_A: {2:.5f}, loss_B: {3:.5f}".format(
+            time.strftime("%H:%M:%S"), iter_no, loss_A, loss_B),
+            end='\r')
+
+        if viewer is not None:
+            viewer(self.show_sample(target_A[0:24], target_B[0:24]), "training using {}, bs={}".format(self.model, self.batch_size))
+            
+
+    def show_sample(self, test_A, test_B):
+        figure_A = numpy.stack([
+            test_A,
+            self.model.autoencoder_A.predict(test_A),
+            self.model.autoencoder_B.predict(test_A),
+        ], axis=1)
+        figure_B = numpy.stack([
+            test_B,
+            self.model.autoencoder_B.predict(test_B),
+            self.model.autoencoder_A.predict(test_B),
+        ], axis=1)
+
+        if test_A.shape[0] % 2 == 1:
+            figure_A = numpy.concatenate ([figure_A, numpy.expand_dims(figure_A[0],0) ])
+            figure_B = numpy.concatenate ([figure_B, numpy.expand_dims(figure_B[0],0) ])
+
+        figure = numpy.concatenate([figure_A, figure_B], axis=0)
+        w = 4
+        h = int( figure.shape[0] / w)
+        figure = figure.reshape((w, h) + figure.shape[1:])
+        figure = stack_images(figure)
+
+        return numpy.clip(figure * 255, 0, 255).astype('uint8')
+    
+    
+    @property
+    def random_transform_args(self):
+        return self._random_transform_args
diff --git a/plugins/Model_OriginalHighRes/__init__.py b/plugins/Model_OriginalHighRes/__init__.py
new file mode 100644
index 0000000..f1322c9
--- /dev/null
+++ b/plugins/Model_OriginalHighRes/__init__.py
@@ -0,0 +1,7 @@
+# -*- coding: utf-8 -*-
+
+__author__ = """Based on https://reddit.com/u/deepfakes/"""
+
+from ._version import __version__
+from .Model import Model
+from .Trainer import Trainer
diff --git a/plugins/Model_OriginalHighRes/_version.py b/plugins/Model_OriginalHighRes/_version.py
new file mode 100644
index 0000000..96c4bb6
--- /dev/null
+++ b/plugins/Model_OriginalHighRes/_version.py
@@ -0,0 +1 @@
+__version__ = 0, 2, 7
\ No newline at end of file
diff --git a/scripts/convert.py b/scripts/convert.py
index d7c6c6b..63d31f0 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -134,10 +134,12 @@ class Convert(object):
 
         image = self.images.rotate_image(image, face.r)
         # TODO: This switch between 64 and 128 is a hack for now.
-        # We should have a separate cli option for size
+        # We should have a separate cli option for size        
+        size = 128 if (self.args.trainer.strip().lower() in ('gan128', 'originalhighres')) else 64        
+        
         image = converter.patch_image(image,
                                       face,
-                                      64 if "128" not in self.args.trainer else 128)
+                                      size)
         image = self.images.rotate_image(image, face.r, reverse=True)
         return image
 
