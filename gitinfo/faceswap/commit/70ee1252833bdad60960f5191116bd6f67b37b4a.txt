commit 70ee1252833bdad60960f5191116bd6f67b37b4a
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sun Oct 13 22:50:28 2019 +0000

    Serialize masks to alignments file
    
    - Add new serializers (npy + compressed)
    - Remove Serializer option from cli
    - Revert get_aligned call in scripts/extract
    - Default alignments to compressed
    - Size masks to 128px and compress
    - Remove mask thresholding/blur from generation code
    - Add Mask class to lib/faces_detect
    - Revert debug landmarks to aligned face
    - Revert non-extraction code to staging version

diff --git a/lib/alignments.py b/lib/alignments.py
index fffb50f..294dd1a 100644
--- a/lib/alignments.py
+++ b/lib/alignments.py
@@ -18,19 +18,15 @@ class Alignments():
     """ Holds processes pertaining to the alignments file.
 
         folder:     folder alignments file is stored in
-        filename:   Filename of alignments file excluding extension. If a
+        filename:   Filename of alignments file. If a
                     valid extension is provided, then it will be used to
-                    decide the serializer, and the serializer argument will
-                    be ignored.
-        serializer: If provided, this will be the format that the data is
-                    saved in (if data is to be saved). Can be 'json', 'pickle'
-                    or 'yaml'
+                    decide the serializer otherwise compressed pickle is used.
     """
     # pylint: disable=too-many-public-methods
-    def __init__(self, folder, filename="alignments", serializer="json"):
-        logger.debug("Initializing %s: (folder: '%s', filename: '%s', serializer: '%s')",
-                     self.__class__.__name__, folder, filename, serializer)
-        self.serializer = self.get_serializer(filename, serializer)
+    def __init__(self, folder, filename="alignments"):
+        logger.debug("Initializing %s: (folder: '%s', filename: '%s')",
+                     self.__class__.__name__, folder, filename)
+        self.serializer = self.get_serializer(filename)
         self.file = self.get_location(folder, filename)
 
         self.data = self.load()
@@ -74,25 +70,21 @@ class Alignments():
     # << INIT FUNCTIONS >> #
 
     @staticmethod
-    def get_serializer(filename, serializer):
+    def get_serializer(filename):
         """ Set the serializer to be used for loading and
             saving alignments
 
             If a filename with a valid extension is passed in
             this will be used as the serializer, otherwise the
-            specified serializer will be used """
-        logger.debug("Getting serializer: (filename: '%s', serializer: '%s')",
-                     filename, serializer)
+            compressed pickle will be used """
+        logger.debug("Getting serializer: (filename: '%s')", filename)
         extension = os.path.splitext(filename)[1]
         if extension in (".json", ".p", ".yaml", ".yml"):
             logger.debug("Serializer set from filename extension: '%s'", extension)
             retval = get_serializer_from_filename(filename)
-        elif serializer not in ("json", "pickle", "yaml"):
-            raise ValueError("Error: {} is not a valid serializer. Use "
-                             "'json', 'pickle' or 'yaml'")
         else:
-            logger.debug("Serializer set from argument: '%s'", serializer)
-            retval = get_serializer(serializer)
+            logger.debug("Returning default Pickle serializer")
+            retval = get_serializer("compressed")
         logger.verbose("Using '%s' serializer for alignments", retval.file_extension)
         return retval
 
diff --git a/lib/cli.py b/lib/cli.py
index 4b9b7d2..30bbea4 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -532,15 +532,6 @@ class ExtractArgs(ExtractConvertArgs):
             default_aligner = "fan"
 
         argument_list = []
-        argument_list.append({"opts": ("--serializer", ),
-                              "type": str.lower,
-                              "dest": "serializer",
-                              "default": "json",
-                              "choices": ("json", "pickle", "yaml"),
-                              "group": "Data",
-                              "help": "Serializer for alignments file. If yaml is chosen and not "
-                                      "available, then json will be used as the default "
-                                      "fallback."})
         argument_list.append({"opts": ("-D", "--detector"),
                               "action": Radio,
                               "type": str.lower,
diff --git a/lib/convert.py b/lib/convert.py
index bd7d6cd..3f782f2 100644
--- a/lib/convert.py
+++ b/lib/convert.py
@@ -141,7 +141,7 @@ class Converter():
                                            predicted["detected_faces"]):
             predicted_mask = new_face[:, :, -1] if new_face.shape[2] == 4 else None
             new_face = new_face[:, :, :3]
-            src_face = detected_face.reference_face / np.array(255.0, dtype="float32")
+            src_face = detected_face.reference_face
             interpolator = detected_face.reference_interpolators[1]
 
             new_face = self.pre_warp_adjustments(src_face, new_face, detected_face, predicted_mask)
diff --git a/lib/faces_detect.py b/lib/faces_detect.py
index 3361473..2e94bb0 100644
--- a/lib/faces_detect.py
+++ b/lib/faces_detect.py
@@ -2,6 +2,8 @@
 """ Face and landmarks detection for faceswap.py """
 import logging
 
+from zlib import compress, decompress
+
 import cv2
 import numpy as np
 
@@ -41,12 +43,12 @@ class DetectedFace():
         of 68 `(x, y)` ``tuples`` with each of the landmark co-ordinates.
     mask: dict
         The generated mask(s) for the face as generated in :mod:`plugins.extract.mask`. Must be a
-        dict of `{name (str): mask (numpy.ndarray)}
+        dict of {**name** (`str`): :class:`Mask`}.
     """
     def __init__(self, image=None, x=None, w=None, y=None, h=None,
                  landmarks_xy=None, mask=None, filename=None):
         logger.trace("Initializing %s: (image: %s, x: %s, w: %s, y: %s, h:%s, "
-                     "landmarks_xy: %s, filename: %s)",
+                     "landmarks_xy: %s, mask: %s, filename: %s)",
                      self.__class__.__name__,
                      image.shape if image is not None and image.any() else image,
                      x, w, y, h, landmarks_xy,
@@ -95,6 +97,34 @@ class DetectedFace():
         """ The coverage ratio to add for training images """
         return 1.0
 
+    def add_mask(self, name, mask, affine_matrix, frame_dims, interpolator):
+        """ Add a :class:`Mask` to this detected face
+
+        The mask should be the original output from  :mod:`plugins.extract.mask`
+        If a mask with this name already exists it will be overwritten by the given
+        mask.
+
+        Parameters
+        ----------
+        name: str
+            The name of the mask as defined by the :attr:`plugins.extract.mask._base.name`
+            parameter.
+        mask: numpy.ndarray
+            The mask that is to be added as output from :mod:`plugins.extract.mask`
+            It should be in the range 0.0 - 1.0 ideally with a ``dtype`` of ``float32``
+        affine_matrix: numpy.ndarray
+            The transformation matrix required to transform the mask to the original frame.
+        frame_dims: tuple
+            The `(height, width)` dimensions of the original frame that this mask was created from.
+        interpolator:
+            The CV2 interpolator required to transform this mask to it's original frame
+        """
+        logger.trace("name: '%s', mask shape: %s, affine_matrix: %s, frame_dims: %s, "
+                     "interpolator: %s", name, mask.shape, affine_matrix, frame_dims, interpolator)
+        fsmask = Mask()
+        fsmask.add(mask, affine_matrix, frame_dims, interpolator)
+        self.mask[name] = fsmask
+
     def to_alignment(self):
         """  Return the detected face formatted for an alignments file
 
@@ -436,7 +466,7 @@ class DetectedFace():
 def rotate_landmarks(face, rotation_matrix):
     """ Rotates the 68 point landmarks and detection bounding box around the given rotation matrix.
 
-    Paramaters
+    Parameters
     ----------
     face: DetectedFace or dict
         A :class:`DetectedFace` or an `alignments file` ``dict`` containing the 68 point landmarks
@@ -517,3 +547,80 @@ def rotate_landmarks(face, rotation_matrix):
 
     logger.trace("Rotated landmarks: %s", rotated_landmarks)
     return face
+
+
+class Mask():
+    """ Face Mask information and convenience methods
+
+    Holds a Faceswap mask as generated from :mod:`plugins.extract.mask` and the information
+    required to transform it to its original frame.
+
+    Holds convenience methods to handle the warping, storing and retrieval of the mask.
+
+    Parameters
+    ----------
+    storage_size: int, optional
+        The size (in pixels) that the mask should be stored at. Default: 128.
+
+    Attributes
+    ----------
+    storage_dims: tuple
+        The `(height, width)` of the stored mask.
+    """
+
+    def __init__(self, storage_size=128):
+        self.storage_dims = (storage_size, storage_size)
+
+        self._mask = None
+        self._original_dims = None
+        self._affine_matrix = None
+        self._frame_dims = None
+        self._intepolator = None
+
+    @property
+    def mask(self):
+        """ numpy.ndarray: The mask at the size of :attr:`storage_dims` """
+        return decompress(self._mask)
+
+    @property
+    def full_frame_mask(self):
+        """ numpy.ndarray: The mask affined to the original full frame """
+        mask = np.zeros(self._frame_dims + (1, ), dtype="uint8")
+        mask = cv2.warpAffine(cv2.resize(self.mask, self._original_dims, cv2.INTER_CUBIC),
+                              self._affine_matrix,
+                              self._frame_dims,
+                              mask,
+                              flags=cv2.WARP_INVERSE_MAP | self._intepolator,
+                              borderMode=cv2.BORDER_TRANSPARENT)
+        logger.trace("mask shape: %s, mask dtype: %s, mask min: %s, mask max: %s",
+                     mask.shape, mask.dtype, mask.min(), mask.max())
+        return mask
+
+    def add(self, mask, affine_matrix, frame_dims, interpolator):
+        """ Add a Faceswap mask to this :class:`Mask`.
+
+        The mask should be the original output from  :mod:`plugins.extract.mask`
+
+        Parameters
+        ----------
+        mask: numpy.ndarray
+            The mask that is to be added as output from :mod:`plugins.extract.mask`
+            It should be in the range 0.0 - 1.0 ideally with a ``dtype`` of ``float32``
+        affine_matrix: numpy.ndarray
+            The transformation matrix required to transform the mask to the original frame.
+        frame_dims: tuple
+            The `(height, width)` dimensions of the original frame that this mask was created from.
+        interpolator:
+            The CV2 interpolator required to transform this mask to it's original frame
+        """
+        logger.trace("mask shape: %s, mask dtype: %s, mask min: %s, mask max: %s, "
+                     "affine_matrix: %s, frame_dims: %s, interpolator: %s", mask.shape, mask.dtype,
+                     mask.min(), mask.max(), affine_matrix, frame_dims, interpolator)
+        self._original_dims = mask.shape[:2]
+        self._affine_matrix = affine_matrix
+        self._frame_dims = frame_dims
+        self._intepolator = interpolator
+        mask = (cv2.resize(mask,
+                           self.storage_dims,
+                           interpolation=cv2.INTER_AREA) * 255.0).astype("uint8")
+        self._mask = compress(mask)
diff --git a/lib/serializer.py b/lib/serializer.py
index dad5fc6..1b5b7e4 100644
--- a/lib/serializer.py
+++ b/lib/serializer.py
@@ -2,10 +2,16 @@
 """
 Library for serializing python objects to and from various different serializer formats
 """
-import logging
+
 import json
+import logging
 import os
 import pickle
+import zlib
+
+from io import BytesIO
+
+import numpy as np
 
 from lib.utils import FaceswapError
 
@@ -101,6 +107,7 @@ class Serializer():
                 data = s_file.read()
                 logger.debug("stored data type: %s", type(data))
                 retval = self.unmarshal(data)
+
         except IOError as err:
             msg = "Error reading from '{}': {}".format(filename, err.strerror)
             raise FaceswapError(msg) from err
@@ -187,7 +194,7 @@ class _YAMLSerializer(Serializer):
 
     @classmethod
     def _unmarshal(cls, data):
-        return yaml.load(data.decode("utf-8"))
+        return yaml.load(data.decode("utf-8"), Loader=yaml.FullLoader)
 
 
 class _JSONSerializer(Serializer):
@@ -209,7 +216,7 @@ class _PickleSerializer(Serializer):
     """ Pickle Serializer """
     def __init__(self):
         super().__init__()
-        self._file_extension = "p"
+        self._file_extension = "pickle"
 
     @classmethod
     def _marshal(cls, data):
@@ -220,12 +227,54 @@ class _PickleSerializer(Serializer):
         return pickle.loads(data)
 
 
+class _NPYSerializer(Serializer):  # pylint:disable=abstract-method
+    """ NPY Serializer """
+    def __init__(self):
+        super().__init__()
+        self._file_extension = "npy"
+        self._bytes = BytesIO()
+
+    def _marshal(self, data):
+        """ NPY Marshal to bytesIO so standard bytes writer can write out """
+        b_handler = BytesIO()
+        np.save(b_handler, data)
+        b_handler.seek(0)
+        return b_handler.read()
+
+    def _unmarshal(self, data):
+        """ NPY Unmarshal to bytesIO so we can use numpy loader """
+        b_handler = BytesIO(data)
+        retval = np.load(b_handler)
+        del b_handler
+        if retval.dtype == "object":
+            retval = retval[()]
+        return retval
+
+
+class _CompressedSerializer(Serializer):
+    """ A compressed pickle serializer for Faceswap """
+    def __init__(self):
+        super().__init__()
+        self._file_extension = "fsc"
+        self._child = get_serializer("pickle")
+
+    def _marshal(self, data):
+        """ Pickle and compress data """
+        data = self._child._marshal(data)  # pylint: disable=protected-access
+        return zlib.compress(data)
+
+    def _unmarshal(self, data):
+        """ Decompress and unpicke data """
+        data = zlib.decompress(data)
+        return self._child._unmarshal(data)  # pylint: disable=protected-access
+
+
 def get_serializer(serializer):
     """ Obtain a serializer object
 
     Parameters
     ----------
-    serializer: {'json', 'pickle', yaml'}
+    serializer: {'json', 'pickle', yaml', 'npy', 'compressed'}
         The required serializer format
 
     Returns
@@ -237,17 +286,24 @@ def get_serializer(serializer):
     -------
     >>> serializer = get_serializer('json')
     """
-    if serializer.lower() == "json":
-        return _JSONSerializer()
-    if serializer.lower() == "pickle":
-        return _PickleSerializer()
-    if serializer.lower() == "yaml" and yaml is not None:
-        return _YAMLSerializer()
-    if serializer.lower() == "yaml" and yaml is None:
+    if serializer.lower() == "npy":
+        retval = _NPYSerializer()
+    elif serializer.lower() == "compressed":
+        retval = _CompressedSerializer()
+    elif serializer.lower() == "json":
+        retval = _JSONSerializer()
+    elif serializer.lower() == "pickle":
+        retval = _PickleSerializer()
+    elif serializer.lower() == "yaml" and yaml is not None:
+        retval = _YAMLSerializer()
+    elif serializer.lower() == "yaml" and yaml is None:
         logger.warning("You must have PyYAML installed to use YAML as the serializer."
                        "Switching to JSON as the serializer.")
-    logger.warning("Unrecognized serializer: '%s'. Returning json serializer", serializer)
-    return _JSONSerializer()
+        retval = _JSONSerializer
+    else:
+        logger.warning("Unrecognized serializer: '%s'. Returning json serializer", serializer)
+    logger.debug(retval)
+    return retval
 
 
 def get_serializer_from_filename(filename):
@@ -273,13 +329,21 @@ def get_serializer_from_filename(filename):
     logger.debug("extension: '%s'", extension)
 
     if extension == ".json":
-        return _JSONSerializer()
-    if extension == ".p":
-        return _PickleSerializer()
-    if extension in (".yaml", ".yml") and yaml is not None:
-        return _YAMLSerializer()
-    if extension in (".yaml", ".yml") and yaml is None:
+        retval = _JSONSerializer()
+    elif extension == ".p":
+        retval = _PickleSerializer()
+    elif extension == ".npy":
+        retval = _NPYSerializer()
+    elif extension == ".fsc":
+        retval = _CompressedSerializer()
+    elif extension in (".yaml", ".yml") and yaml is not None:
+        retval = _YAMLSerializer()
+    elif extension in (".yaml", ".yml") and yaml is None:
         logger.warning("You must have PyYAML installed to use YAML as the serializer.\n"
                        "Switching to JSON as the serializer.")
-    logger.warning("Unrecognized extension: '%s'. Returning json serializer", extension)
-    return _JSONSerializer()
+        retval = _JSONSerializer()
+    else:
+        logger.warning("Unrecognized extension: '%s'. Returning json serializer", extension)
+        retval = _JSONSerializer()
+    logger.debug(retval)
+    return retval
diff --git a/lib/training_data.py b/lib/training_data.py
index dce6367..52886b6 100644
--- a/lib/training_data.py
+++ b/lib/training_data.py
@@ -11,6 +11,7 @@ import cv2
 from scipy.interpolate import griddata
 
 from lib.image import batch_convert_color, read_image_batch
+from lib.model import masks
 from lib.multithreading import BackgroundGenerator
 from lib.utils import FaceswapError
 
@@ -47,12 +48,17 @@ class TrainingDataGenerator():
         * **no_flip** (`bool`) - ``True`` if the image shouldn't be randomly flipped as part of \
         augmentation, otherwise ``False``
 
+        * **mask_type** (`str`) - The mask type to be used (as defined in \
+        :mod:`lib.model.masks`). If not ``None`` then the additional key ``landmarks`` must be \
+        provided.
+
         * **warp_to_landmarks** (`bool`) - ``True`` if the random warp method should warp to \
         similar landmarks from the other side, ``False`` if the standard random warp method \
         should be used. If ``True`` then the additional key ``landmarks`` must be provided.
 
-        * **landmarks** (`numpy.ndarray`, `optional`). Required if :attr:`warp_to_landmarks` is \
-        ``True``. The 68 point face landmarks from an alignments file.
+        * **landmarks** (`numpy.ndarray`, `optional`). Required if using a :attr:`mask_type` is \
+        not ``None`` or :attr:`warp_to_landmarks` is ``True``. The 68 point face landmarks from \
+        an alignments file.
 
     config: dict
         The configuration ``dict`` generated from :file:`config.train.ini` containing the trainer \
@@ -68,6 +74,7 @@ class TrainingDataGenerator():
         self._model_input_size = model_input_size
         self._model_output_shapes = model_output_shapes
         self._training_opts = training_opts
+        self._mask_class = self._set__mask_class()
         self._landmarks = self._training_opts.get("landmarks", None)
         self._nearest_landmarks = {}
 
@@ -123,7 +130,8 @@ class TrainingDataGenerator():
             :mod:`plugins.train.trainer._base` from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`).
+            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
+            in the ``dict`` if the :attr:`mask_type` is not ``None``
 
             * **samples** (`numpy.ndarray`) - A 4-dimensional array containg the samples for \
             feeding to the model's predict function for generating preview and timelapse samples. \
@@ -146,6 +154,18 @@ class TrainingDataGenerator():
         return batcher.iterator()
 
     # << INTERNAL METHODS >> #
+    def _set__mask_class(self):
+        """ Returns the correct mask class from :mod:`lib`.model.masks` as defined in the
+        :attr:`mask_type` parameter. """
+        mask_type = self._training_opts.get("mask_type", None)
+        if mask_type:
+            logger.debug("Mask type: '%s'", mask_type)
+            _mask_class = getattr(masks, mask_type)
+        else:
+            _mask_class = None
+        logger.debug("Mask class: %s", _mask_class)
+        return _mask_class
+
     def _validate_samples(self, data):
         """ Ensures that the total number of images within :attr:`images` is greater or equal to
         the selected :attr:`batchsize`. Raises an exception if this is not the case. """
@@ -187,23 +207,24 @@ class TrainingDataGenerator():
         logger.trace("Process batch: (filenames: '%s', side: '%s')", filenames, side)
         batch = read_image_batch(filenames)
         processed = dict()
+        to_landmarks = self._training_opts["warp_to_landmarks"]
 
         # Initialize processing training size on first image
         if not self._processing.initialized:
             self._processing.initialize(batch.shape[1])
 
         # Get Landmarks prior to manipulating the image
-        if self._training_opts["warp_to_landmarks"]:
+        if self._mask_class or to_landmarks:
             batch_src_pts = self._get_landmarks(filenames, batch, side)
-            batch_dst_pts = self._get_closest_match(filenames, side, batch_src_pts)
-            warp_kwargs = dict(batch_src_points=batch_src_pts,
-                               batch_dst_points=batch_dst_pts)
-        else:
-            warp_kwargs = dict()
 
-        # Color Augmentation of the image only
+        # Color augmentation before mask is added
         if self._training_opts["augment_color"]:
-            batch[..., :3] = self._processing.color_adjust(batch[..., :3])
+            batch = self._processing.color_adjust(batch)
+
+        # Add mask to batch prior to transforms and warps
+        if self._mask_class:
+            batch = np.array([self._mask_class(src_pts, image, channels=4).mask
+                              for src_pts, image in zip(batch_src_pts, batch)])
 
         # Random Transform and flip
         batch = self._processing.transform(batch)
@@ -217,10 +238,15 @@ class TrainingDataGenerator():
         # Get Targets
         processed.update(self._processing.get_targets(batch))
 
-        # Random Warp # TODO change masks to have a input mask and a warped target mask
-        processed["feed"] = [self._processing.warp(batch[..., :3],
-                                                   self._training_opts["warp_to_landmarks"],
-                                                   **warp_kwargs)]
+        # Random Warp
+        if to_landmarks:
+            warp_kwargs = dict(batch_src_points=batch_src_pts,
+                               batch_dst_points=self._get_closest_match(filenames,
+                                                                        side,
+                                                                        batch_src_pts))
+        else:
+            warp_kwargs = dict()
+        processed["feed"] = self._processing.warp(batch[..., :3], to_landmarks, **warp_kwargs)
 
         logger.trace("Processed batch: (filenames: %s, side: '%s', processed: %s)",
                      filenames,
@@ -232,8 +258,8 @@ class TrainingDataGenerator():
 
     def _get_landmarks(self, filenames, batch, side):
         """ Obtains the 68 Point Landmarks for the images in this batch. This is only called if
-        config item ``warp_to_landmarks`` is ``True``. If the landmarks for an image cannot be
-        found, then an error is raised. """
+        config item ``warp_to_landmarks`` is ``True`` or if :attr:`mask_type` is not ``None``. If
+        the landmarks for an image cannot be found, then an error is raised. """
         logger.trace("Retrieving landmarks: (filenames: %s, side: '%s')", filenames, side)
         src_points = [self._landmarks[side].get(sha1(face).hexdigest(), None) for face in batch]
 
@@ -244,7 +270,7 @@ class TrainingDataGenerator():
             msg = ("Files missing alignments for this batch: {}"
                    "\nAt least one of your images does not have a matching entry in your "
                    "alignments file."
-                   "\nIf you are using 'warp to landmarks' then every "
+                   "\nIf you are training with a mask or using 'warp to landmarks' then every "
                    "face you intend to train on must exist within the alignments file."
                    "\nThe specific files that caused this failure are listed above."
                    "\nMost likely there will be more than just these files missing from the "
@@ -423,17 +449,18 @@ class ImageAugmentation():
             output they will be returned as their own item from the ``masks`` key.
 
             * **masks** (`numpy.ndarray`) - A 4-dimensional array containing the target masks in \
-            the format (`batchsize`, `height`, `width`, `1`).
+            the format (`batchsize`, `height`, `width`, `1`). **NB:** This item will only exist \
+            in the ``dict`` if a batch of 4 channel images has been passed in :attr:`batch`
         """
         logger.trace("Compiling targets")
         slices = self._constants["tgt_slices"]
         target_batch = [np.array([cv2.resize(image[slices, slices, :],
                                              (size, size),
                                              cv2.INTER_AREA)
-                                  for image in batch], dtype='float32') / 255.
+                                  for image in batch])
                         for size in self._output_sizes]
         logger.trace("Target image shapes: %s",
-                     [tgt_images.shape[1:] for tgt_images in target_batch])
+                     [tgt.shape for tgt_images in target_batch for tgt in tgt_images])
 
         retval = self._separate_target_mask(target_batch)
         logger.trace("Final targets: %s",
@@ -442,19 +469,25 @@ class ImageAugmentation():
         return retval
 
     @staticmethod
-    def _separate_target_mask(size_list_of_batches):
+    def _separate_target_mask(batch):
         """ Return the batch and the batch of final masks
 
         Returns the targets as a list of 4-dimensional ``numpy.ndarray`` s of shape (`batchsize`,
         `height`, `width`, 3). If the :attr:`batch` is 4 channels, then the masks will be split
         from the batch, with the largest output masks being returned in their own item.
         """
-        targets = [batch[..., :3] for batch in size_list_of_batches]
-        if size_list_of_batches[-1].shape[-1] == 4:
-            masks = [size_list_of_batches[-1][..., 3:]]
+        batch = [tgt.astype("float32") / 255.0 for tgt in batch]
+        if all(tgt.shape[-1] == 4 for tgt in batch):
+            logger.trace("Batch contains mask")
+            sizes = [item.shape[1] for item in batch]
+            mask_batch = np.expand_dims(batch[sizes.index(max(sizes))][..., -1], axis=-1)
+            batch = [item[..., :3] for item in batch]
+            logger.trace("batch shapes: %s, mask_batch shape: %s",
+                         [tgt.shape for tgt in batch], mask_batch.shape)
+            retval = dict(targets=batch, masks=mask_batch)
         else:
-            masks = [np.ones((size_list_of_batches[-1].shape[:-1] + (1,)), dtype='float32')]
-        retval = dict(targets=targets, masks=masks)
+            logger.trace("Batch has no mask")
+            retval = dict(targets=batch)
         return retval
 
     # <<< COLOR AUGMENTATION >>> #
diff --git a/plugins/extract/mask/_base.py b/plugins/extract/mask/_base.py
index ee866fb..3582869 100644
--- a/plugins/extract/mask/_base.py
+++ b/plugins/extract/mask/_base.py
@@ -18,8 +18,6 @@ For each source item, the plugin must pass a dict to finalize containing:
 >>>  "detected_faces": <list of bounding box dicts from lib/plugins/extract/detect/_base>}
 """
 
-import base64
-import zlib
 import cv2
 import numpy as np
 
@@ -189,26 +187,23 @@ class Masker(Extractor):  # pylint:disable=abstract-method
             :class:`lib.faces_detect.DetectedFace` objects.
 
         """
-        if self.blur_kernel is not None:
-            predicted = np.array([cv2.GaussianBlur(mask, (self.blur_kernel, self.blur_kernel), 0)
-                                  for mask in batch["prediction"]])
-        else:
-            predicted = batch["prediction"]
-        predicted[predicted < 0.04] = 0.0
-        predicted[predicted > 0.96] = 1.0
-        # TODO Convert this and landmarks_xy to numpy arrays once serialization
-        # decision is made, Hacky temp fix as can't serialize numpy arrays in json
-        # and tolist is hugely slow and gobbles ram
+        # TODO Migrate these settings to retrieval rather than storage
+        # if self.blur_kernel is not None:
+        #    predicted = np.array([cv2.GaussianBlur(mask, (self.blur_kernel, self.blur_kernel), 0)
+        #                          for mask in batch["prediction"]])
+        # else:
+        #    predicted = batch["prediction"]
+        # predicted[predicted < 0.04] = 0.0
+        # predicted[predicted > 0.96] = 1.0
+        # TODO Convert landmarks_xy to numpy arrays
         for mask, face in zip(batch["prediction"], batch["detected_faces"]):
-            placeholder = np.zeros(face.image.shape[:2] + (1, ), dtype="float32")
-            placeholder = (cv2.warpAffine(
-                mask,
-                face.feed_matrix,
-                (face.image.shape[1], face.image.shape[0]),
-                placeholder,
-                flags=cv2.WARP_INVERSE_MAP | face.feed_interpolators[1],
-                borderMode=cv2.BORDER_TRANSPARENT) * 255.0).astype("uint8")
-            face.mask[self.name] = base64.b64encode(zlib.compress(placeholder)).decode()
+            face.add_mask(self.name,
+                          mask,
+                          face.feed_matrix,
+                          (face.image.shape[1], face.image.shape[0]),
+                          face.feed_interpolators[1])
+            face.feed = None
+
         self._remove_invalid_keys(batch, ("detected_faces", "filename", "image"))
         logger.trace("Item out: %s", {key: val
                                       for key, val in batch.items()
diff --git a/plugins/train/_config.py b/plugins/train/_config.py
index 14db591..31d1dd5 100644
--- a/plugins/train/_config.py
+++ b/plugins/train/_config.py
@@ -8,6 +8,7 @@ import sys
 from importlib import import_module
 
 from lib.config import FaceswapConfig
+from lib.model.masks import get_available_masks
 from lib.utils import full_path_split
 
 logger = logging.getLogger(__name__)  # pylint: disable=invalid-name
@@ -68,11 +69,15 @@ class Config(FaceswapConfig):
                  "\n\t87.5%% spans from ear to ear."
                  "\n\t100.0%% is a mugshot.")
         self.add_item(
-            section=section, title="replicate_input_mask", datatype=bool,
-            default=False, group="mask",
-            info="Dedicate portions of the model to learning how to duplicate the input "
-                 "mask. Increases VRAM usage in exchange for a learning a quick ability "
-                 "to try to replicate more complex mask models.")
+            section=section, title="mask_type", datatype=str, default="none",
+            choices=get_available_masks(), group="mask",
+            info="The mask to be used for training:"
+                 "\n\t none: Doesn't use any mask."
+                 "\n\t components: An improved face hull mask using a facehull of 8 facial parts"
+                 "\n\t dfl_full: An improved face hull mask using a facehull of 3 facial parts"
+                 "\n\t extended: Based on components mask. Extends the eyebrow points to further "
+                 "up the forehead. May perform badly on difficult angles."
+                 "\n\t facehull: Face cutout based on landmarks")
         self.add_item(
             section=section, title="mask_blur", datatype=bool, default=False, group="mask",
             info="Apply gaussian blur to the mask input. This has the effect of smoothing the "
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index b495c08..df561c3 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -105,10 +105,7 @@ class ModelBase():
                               "augment_color": augment_color,
                               "no_flip": no_flip,
                               "pingpong": self.vram_savings.pingpong,
-                              "snapshot_interval": snapshot_interval,
-                              "replicate_input_mask": self.config["replicate_input_mask"],
-                              "penalized_mask_loss": self.config["penalized_mask_loss"]}
-
+                              "snapshot_interval": snapshot_interval}
 
         if self.multiple_models_in_folder:
             deprecation_warning("Support for multiple model types within the same folder",
@@ -225,6 +222,7 @@ class ModelBase():
         # Force number of preview images to between 2 and 16
         self.training_opts["training_size"] = self.state.training_size
         self.training_opts["no_logs"] = self.state.current_session["no_logs"]
+        self.training_opts["mask_type"] = self.config.get("mask_type", None)
         self.training_opts["coverage_ratio"] = self.calculate_coverage_ratio()
         logger.debug("Set training data: %s", self.training_opts)
 
@@ -262,11 +260,12 @@ class ModelBase():
         logger.debug("Getting inputs")
         inputs = [Input(shape=self.input_shape, name="face_in")]
         output_network = [network for network in self.networks.values() if network.is_output][0]
-        if self.config["replicate_input_mask"] or self.config["penalized_mask_loss"]:
-            # penalized mask doesn't have a mask ouput, so we can't use output shapes
-            # mask should always be last output..this needs to be a rule
-            mask_shape = output_network.output_shapes[-1]
-            inputs.append(Input(shape=(mask_shape[1:-1] + (1,)), name="mask_in"))
+        mask_idx = [idx for idx, name in enumerate(output_network.output_names)
+                    if name.startswith("mask")]
+        if mask_idx:
+            # Add the final mask shape as input
+            mask_shape = output_network.output_shapes[mask_idx[0]]
+            inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
         logger.debug("Got inputs: %s", inputs)
         return inputs
 
@@ -445,7 +444,7 @@ class ModelBase():
             logger.error("Model could not be found in folder '%s'. Exiting", self.model_dir)
             exit(0)
 
-        if not self.is_legacy or not self.predict:
+        if not self.is_legacy:
             K.clear_session()
         model_mapping = self.map_models(swapped)
         for network in self.networks.values():
@@ -579,7 +578,7 @@ class ModelBase():
         self.state.config["coverage"] = 62.5
         self.state.config["subpixel_upscaling"] = False
         self.state.config["reflect_padding"] = False
-        self.state.config["replicate_input_mask"] = False
+        self.state.config["mask_type"] = None
         self.state.config["lowmem"] = False
         self.encoder_dim = 1024
 
@@ -744,7 +743,7 @@ class Loss():
         for idx, loss_name in enumerate(self.names):
             if loss_name.startswith("mask"):
                 loss_funcs.append(self.selected_mask_loss)
-            elif self.config["penalized_mask_loss"]:
+            elif self.mask_input is not None and self.config.get("penalized_mask_loss", False):
                 face_size = self.output_shapes[idx][1]
                 mask_size = self.mask_shape[1]
                 scaling = face_size / mask_size
diff --git a/plugins/train/model/dfaker.py b/plugins/train/model/dfaker.py
index 9f41e6d..758c43b 100644
--- a/plugins/train/model/dfaker.py
+++ b/plugins/train/model/dfaker.py
@@ -40,7 +40,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/dfl_h128.py b/plugins/train/model/dfl_h128.py
index 78e140b..6bf9a6f 100644
--- a/plugins/train/model/dfl_h128.py
+++ b/plugins/train/model/dfl_h128.py
@@ -50,8 +50,8 @@ class Model(OriginalModel):
                                    activation="sigmoid",
                                    name="face_out")
         outputs = [var_x]
-
-        if self.config.get("replicate_input_mask", False):
+        # Mask
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, self.encoder_dim)
             var_y = self.blocks.upscale(var_y, self.encoder_dim // 2)
diff --git a/plugins/train/model/dfl_sae.py b/plugins/train/model/dfl_sae.py
index fd9ad53..e79822c 100644
--- a/plugins/train/model/dfl_sae.py
+++ b/plugins/train/model/dfl_sae.py
@@ -31,7 +31,7 @@ class Model(ModelBase):
     @property
     def use_mask(self):
         """ Return True if a mask has been set else false """
-        return self.config.get("replicate_input_mask", False)
+        return self.config.get("mask_type", None) is not None
 
     @property
     def ae_dims(self):
diff --git a/plugins/train/model/iae.py b/plugins/train/model/iae.py
index 4f1c1e8..b164fef 100644
--- a/plugins/train/model/iae.py
+++ b/plugins/train/model/iae.py
@@ -77,7 +77,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/lightweight.py b/plugins/train/model/lightweight.py
index 44f2092..1963c8c 100644
--- a/plugins/train/model/lightweight.py
+++ b/plugins/train/model/lightweight.py
@@ -47,7 +47,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/model/original.py b/plugins/train/model/original.py
index 09bedff..55d3bea 100644
--- a/plugins/train/model/original.py
+++ b/plugins/train/model/original.py
@@ -73,7 +73,7 @@ class Model(ModelBase):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 256)
             var_y = self.blocks.upscale(var_y, 128)
diff --git a/plugins/train/model/realface.py b/plugins/train/model/realface.py
index d950451..10562b8 100644
--- a/plugins/train/model/realface.py
+++ b/plugins/train/model/realface.py
@@ -134,7 +134,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None) is not None:
             var_y = var_xy
             mask_b_complexity = 384
             for idx in range(self.upscalers_no-2):
@@ -184,7 +184,7 @@ class Model(ModelBase):
 
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None) is not None:
             var_y = var_xy
             mask_a_complexity = 384
             for idx in range(self.upscalers_no-2):
diff --git a/plugins/train/model/unbalanced.py b/plugins/train/model/unbalanced.py
index 323639c..b8c2a08 100644
--- a/plugins/train/model/unbalanced.py
+++ b/plugins/train/model/unbalanced.py
@@ -80,7 +80,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             var_y = self.blocks.upscale(var_y, decoder_complexity)
@@ -129,7 +129,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, decoder_complexity)
             if not self.lowmem:
diff --git a/plugins/train/model/villain.py b/plugins/train/model/villain.py
index c55b293..4a0a67a 100644
--- a/plugins/train/model/villain.py
+++ b/plugins/train/model/villain.py
@@ -78,7 +78,7 @@ class Model(OriginalModel):
                                    name="face_out")
         outputs = [var_x]
 
-        if self.config.get("replicate_input_mask", False):
+        if self.config.get("mask_type", None):
             var_y = input_
             var_y = self.blocks.upscale(var_y, 512)
             var_y = self.blocks.upscale(var_y, 256)
diff --git a/plugins/train/trainer/_base.py b/plugins/train/trainer/_base.py
index 1247d68..e4aef16 100644
--- a/plugins/train/trainer/_base.py
+++ b/plugins/train/trainer/_base.py
@@ -7,17 +7,18 @@
 
     A training_opts dictionary can be set in the corresponding model.
     Accepted values:
-        alignments:             dict containing paths to alignments files for keys 'a' and 'b'
-        preview_scaling:        How much to scale the preview out by
-        training_size:          Size of the training images
-        coverage_ratio:         Ratio of face to be cropped out for training
-        replicate_input_mask:   Replicate input masks with additional model dedicated layers
-        no_logs:                Disable tensorboard logging
-        snapshot_interval:      Interval for saving model snapshots
-        warp_to_landmarks:      Use random_warp_landmarks instead of random_warp
-        augment_color:          Perform random shifting of L*a*b* colors
-        no_flip:                Don't perform a random flip on the image
-        pingpong:               Train each side seperately per save iteration rather than together
+        alignments:         dict containing paths to alignments files for keys 'a' and 'b'
+        preview_scaling:    How much to scale the preview out by
+        training_size:      Size of the training images
+        coverage_ratio:     Ratio of face to be cropped out for training
+        mask_type:          Type of mask to use. See lib.model.masks for valid mask names.
+                            Set to None for not used
+        no_logs:            Disable tensorboard logging
+        snapshot_interval:  Interval for saving model snapshots
+        warp_to_landmarks:  Use random_warp_landmarks instead of random_warp
+        augment_color:      Perform random shifting of L*a*b* colors
+        no_flip:            Don't perform a random flip on the image
+        pingpong:           Train each side seperately per save iteration rather than together
 """
 
 import logging
@@ -89,15 +90,14 @@ class TrainerBase():
     def landmarks_required(self):
         """ Return True if Landmarks are required """
         opts = self.model.training_opts
-        retval = opts["warp_to_landmarks"]
+        retval = bool(opts.get("mask_type", None) or opts["warp_to_landmarks"])
         logger.debug(retval)
         return retval
 
     @property
     def use_mask(self):
         """ Return True if a mask is requested """
-        retval = (self.model.training_opts.get("replicate_input_mask", False) or
-                  self.model.training_opts.get("penalized_mask_loss", True))
+        retval = bool(self.model.training_opts.get("mask_type", None))
         logger.debug(retval)
         return retval
 
@@ -176,11 +176,10 @@ class TrainerBase():
             for side, batcher in self.batchers.items():
                 if self.pingpong.active and side != self.pingpong.side:
                     continue
-                loss[side] = batcher.train_one_batch()
+                loss[side] = batcher.train_one_batch(do_preview)
                 if not do_preview and not do_timelapse:
                     continue
                 if do_preview:
-                    batcher.generate_preview(do_preview)
                     self.samples.images[side] = batcher.compile_sample(None)
                 if do_timelapse:
                     self.timelapse.get_sample(side, timelapse_kwargs)
@@ -248,14 +247,13 @@ class Batcher():
         self.config = config
         self.target = None
         self.samples = None
-        self.masks = None
+        self.mask = None
 
         generator = self.load_generator()
         self.feed = generator.minibatch_ab(images, batch_size, self.side)
 
         self.preview_feed = None
         self.timelapse_feed = None
-        self.set_preview_feed()
 
     def load_generator(self):
         """ Pass arguments to TrainingDataGenerator and return object """
@@ -269,12 +267,12 @@ class Batcher():
                                           self.config)
         return generator
 
-    def train_one_batch(self):
+    def train_one_batch(self, do_preview):
         """ Train a batch """
         logger.trace("Training one step: (side: %s)", self.side)
-        model_inputs, model_targets = self.get_next()
+        batch = self.get_next(do_preview)
         try:
-            loss = self.model.predictors[self.side].train_on_batch(x=model_inputs, y=model_targets)
+            loss = self.model.predictors[self.side].train_on_batch(*batch)
         except tf_errors.ResourceExhaustedError as err:
             msg = ("You do not have enough GPU memory available to train the selected model at "
                    "the selected settings. You can try a number of things:"
@@ -290,28 +288,31 @@ class Batcher():
         loss = loss if isinstance(loss, list) else [loss]
         return loss
 
-    def get_next(self):
+    def get_next(self, do_preview):
         """ Return the next batch from the generator
-            Items should come out as: (sample, warped, targets, [mask]) """
-        logger.debug("Generating targets")
+            Items should come out as: (warped, target [, mask]) """
         batch = next(self.feed)
-        targets_use_mask = self.model.training_opts["replicate_input_mask"]
-        model_inputs = batch["feed"] + batch["masks"] if self.use_mask else batch["feed"]
-        model_targets = batch["targets"] + batch["masks"] if targets_use_mask else batch["targets"]
-        return model_inputs, model_targets
+        if self.use_mask:
+            batch = [[batch["feed"], batch["masks"]], batch["targets"] + [batch["masks"]]]
+        else:
+            batch = [batch["feed"], batch["targets"]]
+        self.generate_preview(do_preview)
+        return batch
 
     def generate_preview(self, do_preview):
         """ Generate the preview if a preview iteration """
         if not do_preview:
             self.samples = None
             self.target = None
-            self.masks = None
             return
         logger.debug("Generating preview")
+        if self.preview_feed is None:
+            self.set_preview_feed()
         batch = next(self.preview_feed)
         self.samples = batch["samples"]
-        self.target = batch["targets"][self.model.largest_face_index]
-        self.masks = batch["masks"][0]
+        self.target = [batch["targets"][self.model.largest_face_index]]
+        if self.use_mask:
+            self.target += [batch["masks"]]
 
     def set_preview_feed(self):
         """ Set the preview dictionary """
@@ -326,27 +327,27 @@ class Batcher():
                                                                is_preview=True)
         logger.debug("Set preview feed. Batchsize: %s", batchsize)
 
-    def compile_sample(self, batch_size, samples=None, images=None, masks=None):
+    def compile_sample(self, batch_size, samples=None, images=None):
         """ Training samples to display in the viewer """
         num_images = self.config.get("preview_images", 14)
         num_images = min(batch_size, num_images) if batch_size is not None else num_images
         logger.debug("Compiling samples: (side: '%s', samples: %s)", self.side, num_images)
         images = images if images is not None else self.target
-        masks = masks if masks is not None else self.masks
-        samples = samples if samples is not None else self.samples
-        retval = [samples[0:num_images], images[0:num_images], masks[0:num_images]]
+        retval = [samples[0:num_images]] if samples is not None else [self.samples[0:num_images]]
+        if self.use_mask:
+            retval.extend(tgt[0:num_images] for tgt in images)
+        else:
+            retval.extend(images[0:num_images])
         return retval
 
     def compile_timelapse_sample(self):
         """ Timelapse samples """
         batch = next(self.timelapse_feed)
         batchsize = len(batch["samples"])
-        images = batch["targets"][self.model.largest_face_index]
-        masks = batch["masks"][0]
-        sample = self.compile_sample(batchsize,
-                                     samples=batch["samples"],
-                                     images=images,
-                                     masks=masks)
+        images = [batch["targets"][self.model.largest_face_index]]
+        if self.use_mask:
+            images = images + [batch["masks"]]
+        sample = self.compile_sample(batchsize, samples=batch["samples"], images=images)
         return sample
 
     def set_timelapse_feed(self, images, batchsize):
@@ -415,7 +416,7 @@ class Samples():
         height = int(figure.shape[0] / width)
         figure = figure.reshape((width, height) + figure.shape[1:])
         figure = stack_images(figure)
-        figure = np.concatenate((header, figure), axis=0)
+        figure = np.vstack((header, figure))
 
         logger.debug("Compiled sample")
         return np.clip(figure * 255, 0, 255).astype('uint8')
@@ -517,8 +518,9 @@ class Samples():
         for mask in masks3:
             mask[np.where((mask == [1., 1., 1.]).all(axis=2))] = [0., 0., 1.]
         for previews in faces:
-            images = np.array([cv2.addWeighted(img,  # pylint: disable=no-member
-                                               1.0, masks3[idx], 0.3, 0)
+            images = np.array([cv2.addWeighted(img, 1.0,  # pylint: disable=no-member
+                                               masks3[idx], 0.3,
+                                               0)
                                for idx, img in enumerate(previews)])
             retval.append(images)
         logger.debug("masked shapes: %s", [faces.shape for faces in retval])
@@ -531,7 +533,7 @@ class Samples():
         new_images = list()
         for idx, img in enumerate(backgrounds):
             img[offset:offset + foregrounds[idx].shape[0],
-                offset:offset + foregrounds[idx].shape[1], :3] = foregrounds[idx]
+                offset:offset + foregrounds[idx].shape[1]] = foregrounds[idx]
             new_images.append(img)
         retval = np.array(new_images)
         logger.debug("Overlayed foreground. Shape: %s", retval.shape)
diff --git a/scripts/convert.py b/scripts/convert.py
index 1bd8e99..8caa6dd 100644
--- a/scripts/convert.py
+++ b/scripts/convert.py
@@ -586,7 +586,7 @@ class Predict():
     def compile_feed_faces(detected_faces):
         """ Compile the faces for feeding into the predictor """
         logger.trace("Compiling feed face. Batchsize: %s", len(detected_faces))
-        feed_faces = np.stack([detected_face.feed_face / 255. for detected_face in detected_faces])
+        feed_faces = np.stack([detected_face.feed_face for detected_face in detected_faces])
         logger.trace("Compiled Feed faces. Shape: %s", feed_faces.shape)
         return feed_faces
 
diff --git a/scripts/extract.py b/scripts/extract.py
index b07d853..8979b1d 100644
--- a/scripts/extract.py
+++ b/scripts/extract.py
@@ -11,7 +11,7 @@ from tqdm import tqdm
 from lib.image import encode_image_with_hash
 from lib.multithreading import MultiThread
 from lib.queue_manager import queue_manager
-from lib.utils import get_folder, deprecation_warning
+from lib.utils import get_folder
 from plugins.extract.pipeline import Extractor
 from scripts.fsmedia import Alignments, Images, PostProcess, Utils
 
@@ -225,12 +225,7 @@ class Extract():
 
     def output_processing(self, faces, size, filename):
         """ Prepare faces for output """
-        final_faces = list()
-        for detected_face in faces["detected_faces"]:
-            filename = self.output_dir / Path(detected_face.filename).stem
-            final_faces.append({"file_location": filename,
-                                "face": detected_face})
-        faces["detected_faces"] = final_faces
+        self.align_face(faces, size, filename)
         self.post_process.do_actions(faces)
 
         faces_count = len(faces["detected_faces"])
@@ -240,16 +235,27 @@ class Extract():
         if not self.verify_output and faces_count > 1:
             self.verify_output = True
 
+    def align_face(self, faces, size, filename):
+        """ Align the detected face and add the destination file path """
+        final_faces = list()
+        image = faces["image"]
+        detected_faces = faces["detected_faces"]
+        for face in detected_faces:
+            face.load_aligned(image, size=size)
+            final_faces.append({"file_location": self.output_dir / Path(filename).stem,
+                                "face": face})
+        faces["detected_faces"] = final_faces
+
     def output_faces(self, filename, faces):
         """ Output faces to save thread """
         final_faces = list()
         for idx, detected_face in enumerate(faces["detected_faces"]):
             output_file = detected_face["file_location"]
-            extension = '.png'
+            extension = Path(filename).suffix
             out_filename = "{}_{}{}".format(str(output_file), str(idx), extension)
 
             face = detected_face["face"]
-            resized_face = face.feed_face
+            resized_face = face.aligned_face
             face.hash, img = encode_image_with_hash(resized_face, extension)
             self.save_queue.put((out_filename, img))
             final_faces.append(face.to_alignment())
diff --git a/scripts/fsmedia.py b/scripts/fsmedia.py
index dadbe64..31a0bb0 100644
--- a/scripts/fsmedia.py
+++ b/scripts/fsmedia.py
@@ -57,10 +57,8 @@ class Alignments(AlignmentsBase):
         self.args = arguments
         self.is_extract = is_extract
         folder, filename = self.set_folder_filename(input_is_video)
-        serializer = self.set_serializer()
         super().__init__(folder,
-                         filename=filename,
-                         serializer=serializer)
+                         filename=filename)
         logger.debug("Initialized %s", self.__class__.__name__)
 
     def set_folder_filename(self, input_is_video):
@@ -79,19 +77,6 @@ class Alignments(AlignmentsBase):
         logger.debug("Setting Alignments: (folder: '%s' filename: '%s')", folder, filename)
         return folder, filename
 
-    def set_serializer(self):
-        """ Set the serializer to be used for loading and
-            saving alignments """
-        if hasattr(self.args, "serializer") and self.args.serializer:
-            logger.debug("Serializer provided: '%s'", self.args.serializer)
-            serializer = self.args.serializer
-        else:
-            # If there is a full filename then this will be overriden
-            # by filename extension
-            serializer = "json"
-            logger.debug("No Serializer defaulting to: '%s'", serializer)
-        return serializer
-
     def load(self):
         """ Override  parent loader to handle skip existing on extract """
         data = dict()
@@ -383,9 +368,9 @@ class DebugLandmarks(PostProcessAction):  # pylint: disable=too-few-public-metho
             face = detected_face["face"]
             logger.trace("Drawing Landmarks. Frame: '%s'. Face: %s",
                          detected_face["file_location"].parts[-1], idx)
-            aligned_landmarks = face.feed_landmarks
+            aligned_landmarks = face.aligned_landmarks
             for (pos_x, pos_y) in aligned_landmarks:
-                cv2.circle(face.feed_face,  # pylint: disable=no-member
+                cv2.circle(face.feed_landmarks,  # pylint: disable=no-member
                            (pos_x, pos_y), 2, (0, 0, 255, 255), -1)
 
 
diff --git a/tools/preview.py b/tools/preview.py
index ef803ec..87595c0 100644
--- a/tools/preview.py
+++ b/tools/preview.py
@@ -21,6 +21,7 @@ from lib.gui.tooltip import Tooltip
 from lib.gui.control_helper import set_slider_rounding
 from lib.convert import Converter
 from lib.faces_detect import DetectedFace
+from lib.model.masks import get_available_masks
 from lib.multithreading import MultiThread
 from lib.utils import FaceswapError, set_system_verbosity
 from lib.queue_manager import queue_manager
@@ -732,7 +733,7 @@ class ActionFrame(ttk.Frame):  # pylint: disable=too-many-ancestors
         """ Add the comboboxes to the Action Frame """
         for opt in self.options:
             if opt == "mask_type":
-                choices = ["dfl_full", "components", "extended", "predicted"]
+                choices = get_available_masks() + ["predicted"]
             else:
                 choices = PluginLoader.get_available_convert_plugins(opt, True)
             choices = [self.format_to_display(choice) for choice in choices]
