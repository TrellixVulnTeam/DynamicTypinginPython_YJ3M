commit ad035f2a243368b7f1d16903a544854b6d05a177
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sat Oct 19 23:53:42 2019 +0000

    Optimize Extract parallel VRAM allocation

diff --git a/plugins/extract/align/fan_defaults.py b/plugins/extract/align/fan_defaults.py
index da7277a..1c08acd 100644
--- a/plugins/extract/align/fan_defaults.py
+++ b/plugins/extract/align/fan_defaults.py
@@ -50,7 +50,7 @@ _HELPTEXT = (
 
 _DEFAULTS = {
     "batch-size": {
-        "default": 8,
+        "default": 12,
         "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
                 "but setting it too high can harm performance.\n"
                 "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
diff --git a/plugins/extract/detect/s3fd.py b/plugins/extract/detect/s3fd.py
index 469db17..70456f2 100644
--- a/plugins/extract/detect/s3fd.py
+++ b/plugins/extract/detect/s3fd.py
@@ -23,9 +23,9 @@ class Detect(Detector):
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
         self.name = "S3FD"
         self.input_size = 640
-        self.vram = 4096
+        self.vram = 4112
         self.vram_warnings = 1024  # Will run at this with warnings
-        self.vram_per_batch = 128
+        self.vram_per_batch = 208
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/detect/s3fd_defaults.py b/plugins/extract/detect/s3fd_defaults.py
index 59fcb07..a0f78a3 100755
--- a/plugins/extract/detect/s3fd_defaults.py
+++ b/plugins/extract/detect/s3fd_defaults.py
@@ -63,7 +63,7 @@ _DEFAULTS = {
         "fixed": True,
     },
     "batch-size": {
-        "default": 8,
+        "default": 4,
         "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
                 "but setting it too high can harm performance.\n"
                 "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
diff --git a/plugins/extract/mask/unet_dfl.py b/plugins/extract/mask/unet_dfl.py
index 74206e3..993b1c6 100644
--- a/plugins/extract/mask/unet_dfl.py
+++ b/plugins/extract/mask/unet_dfl.py
@@ -26,9 +26,9 @@ class Mask(Masker):
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
         self.name = "U-Net"
         self.input_size = 256
-        self.vram = 3440
+        self.vram = 3424
         self.vram_warnings = 256
-        self.vram_per_batch = 48
+        self.vram_per_batch = 80
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/mask/vgg_clear.py b/plugins/extract/mask/vgg_clear.py
index c8828b8..55a2174 100644
--- a/plugins/extract/mask/vgg_clear.py
+++ b/plugins/extract/mask/vgg_clear.py
@@ -27,9 +27,9 @@ class Mask(Masker):
         super().__init__(git_model_id=git_model_id, model_filename=model_filename, **kwargs)
         self.name = "VGG Clear"
         self.input_size = 300
-        self.vram = 3104
+        self.vram = 2944
         self.vram_warnings = 1088  # at BS 1. OOMs at higher batchsizes
-        self.vram_per_batch = 96
+        self.vram_per_batch = 400
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/mask/vgg_clear_defaults.py b/plugins/extract/mask/vgg_clear_defaults.py
index 003a943..6ce28a9 100644
--- a/plugins/extract/mask/vgg_clear_defaults.py
+++ b/plugins/extract/mask/vgg_clear_defaults.py
@@ -51,7 +51,7 @@ _HELPTEXT = (
 
 _DEFAULTS = {
     "batch-size": {
-        "default": 8,
+        "default": 6,
         "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
                 "but setting it too high can harm performance.\n"
                 "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
diff --git a/plugins/extract/mask/vgg_obstructed.py b/plugins/extract/mask/vgg_obstructed.py
index 8731975..480dd4a 100644
--- a/plugins/extract/mask/vgg_obstructed.py
+++ b/plugins/extract/mask/vgg_obstructed.py
@@ -29,7 +29,7 @@ class Mask(Masker):
         self.input_size = 500
         self.vram = 3936
         self.vram_warnings = 1088  # at BS 1. OOMs at higher batchsizes
-        self.vram_per_batch = 208
+        self.vram_per_batch = 304
         self.batchsize = self.config["batch-size"]
 
     def init_model(self):
diff --git a/plugins/extract/mask/vgg_obstructed_defaults.py b/plugins/extract/mask/vgg_obstructed_defaults.py
index 89e42ce..9a21d76 100644
--- a/plugins/extract/mask/vgg_obstructed_defaults.py
+++ b/plugins/extract/mask/vgg_obstructed_defaults.py
@@ -52,7 +52,7 @@ _HELPTEXT = (
 
 _DEFAULTS = {
     "batch-size": {
-        "default": 8,
+        "default": 2,
         "info": "The batch size to use. To a point, higher batch sizes equal better performance, "
                 "but setting it too high can harm performance.\n"
                 "\n\tNvidia users: If the batchsize is set higher than the your GPU can "
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index 13e33cf..badb90b 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -68,7 +68,7 @@ class Extractor():
         self._flow = ["detect", "align", "mask"]
         self.phase = self._flow[0]
         self._queue_size = 32
-        self._vram_buffer = 320  # Leave a buffer for VRAM allocation
+        self._vram_buffer = 256  # Leave a buffer for VRAM allocation
         self._detect = self._load_detect(detector, rotate_images, min_size, configfile)
         self._align = self._load_align(aligner, configfile, normalize_method)
         self._mask = self._load_mask(masker, configfile)
@@ -229,6 +229,37 @@ class Extractor():
             logger.debug("Switching to %s phase", self.phase)
 
     # <<< INTERNAL METHODS >>> #
+    @property
+    def _parallel_scaling(self):
+        """ dict: key is number of parallel plugins being loaded, value is the scaling factor that
+        the total base vram for those plugins should be scaled by
+
+        Notes
+        -----
+        VRAM for parallel plugins does not stack in a linear manner. Calculating the precise
+        scaling for any given plugin combination is non trivial, however the following are
+        calculations based on running 2-5 plugins in parallel using s3fd, fan, unet, vgg-clear
+        and vgg-obstructed. The worst ratio is selected for each combination, plus a litle extra
+        to ensure that vram is not used up.
+
+        If OOM errors are being reported, then these ratios should be relaxed some more
+        """
+        retval = {2: 0.7,
+                  3: 0.55,
+                  4: 0.5,
+                  5: 0.4}
+        logger.trace(retval)
+        return retval
+
+    @property
+    def _total_vram_required(self):
+        """ Return vram required for all phases plus the buffer """
+        vrams = [getattr(self, "_{}".format(p)).vram for p in self._flow]
+        vram_required_count = sum(1 for p in vrams if p > 0)
+        retval = (sum(vrams) * self._parallel_scaling[vram_required_count]) + self._vram_buffer
+        logger.trace(retval)
+        return retval
+
     @property
     def _next_phase(self):
         """ Return the next phase from the flow list """
@@ -243,13 +274,6 @@ class Extractor():
         logger.trace(retval)
         return retval
 
-    @property
-    def _total_vram_required(self):
-        """ Return vram required for all phases plus the buffer """
-        retval = sum([getattr(self, "_{}".format(p)).vram for p in self._flow]) + self._vram_buffer
-        logger.trace(retval)
-        return retval
-
     @property
     def _output_queue(self):
         """ Return the correct output queue depending on the current phase """
