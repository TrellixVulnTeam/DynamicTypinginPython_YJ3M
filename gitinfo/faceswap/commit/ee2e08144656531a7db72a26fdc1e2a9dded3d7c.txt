commit ee2e08144656531a7db72a26fdc1e2a9dded3d7c
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Sun Aug 4 13:22:30 2019 +0000

    Disable VRAM saving features for plaidML users
    
    - Disable non-compatible vram saving features for plaidML users
    - Move VRAMSavings to own class in /model/_base.py

diff --git a/lib/cli.py b/lib/cli.py
index e45df87..819145b 100644
--- a/lib/cli.py
+++ b/lib/cli.py
@@ -1001,29 +1001,29 @@ class TrainArgs(FaceSwapArgs):
                               "action": "store_true",
                               "dest": "memory_saving_gradients",
                               "default": False,
-                              "help": "Trades off VRAM usage against computation time. Can fit "
-                                      "larger models into memory at a cost of slower training "
-                                      "speed. 50%%-150%% batch size increase for 20%%-50%% longer "
-                                      "training time. NB: Launch time will be significantly "
-                                      "delayed. Switching sides using ping-pong training will "
-                                      "take longer."})
+                              "help": "[Nvidia only] Trades off VRAM usage against computation "
+                                      "time. Can fit larger models into memory at a cost of "
+                                      "slower training speed. 50%%-150%% batch size increase for "
+                                      "20%%-50%% longer training time. NB: Launch time will be "
+                                      "significantly delayed. Switching sides using ping-pong "
+                                      "training will take longer."})
         argument_list.append({"opts": ("-o", "--optimizer-savings"),
                               "dest": "optimizer_savings",
                               "action": "store_true",
                               "default": False,
-                              "help": "To save VRAM some optimizer gradient calculations can be "
-                                      "performed on the CPU rather than the GPU. This allows you "
-                                      "to increase batchsize at a training speed cost. Nvidia "
-                                      "only. This option will have no effect for plaidML users."})
+                              "help": "[Nvidia only] To save VRAM some optimizer gradient "
+                                      "calculations can be performed on the CPU rather than the "
+                                      "GPU. This allows you to increase batchsize at a training "
+                                      "speed cost."})
         argument_list.append({"opts": ("-pp", "--ping-pong"),
                               "action": "store_true",
                               "dest": "pingpong",
                               "default": False,
-                              "help": "Enable ping pong training. Trains one side at a time, "
-                                      "switching sides at each save iteration. Training will take "
-                                      "2 to 4 times longer, with about a 30%%-50%% reduction in "
-                                      "VRAM useage. NB: Preview won't show until both sides have "
-                                      "been trained once."})
+                              "help": "[Nvidia only] Enable ping pong training. Trains one side "
+                                      "at a time, switching sides at each save iteration. "
+                                      "Training will take 2 to 4 times longer, with about a "
+                                      "30%%-50%% reduction in VRAM useage. NB: Preview won't show "
+                                      "until both sides have been trained once."})
         argument_list.append({"opts": ("-wl", "--warp-to-landmarks"),
                               "action": "store_true",
                               "dest": "warp_to_landmarks",
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 1dcd5d8..21ea8be 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -50,7 +50,7 @@ class ModelBase():
                  trainer="original",
                  pingpong=False,
                  memory_saving_gradients=False,
-                 optimizer_savings="none",
+                 optimizer_savings=False,
                  predict=False):
         logger.debug("Initializing ModelBase (%s): (model_dir: '%s', gpus: %s, configfile: %s, "
                      "snapshot_interval: %s, no_logs: %s, warp_to_landmarks: %s, augment_color: "
@@ -65,6 +65,7 @@ class ModelBase():
 
         self.predict = predict
         self.model_dir = model_dir
+        self.vram_savings = VRAMSavings(pingpong, optimizer_savings, memory_saving_gradients)
 
         self.backup = Backup(self.model_dir, self.name)
         self.gpus = gpus
@@ -74,11 +75,12 @@ class ModelBase():
         self.trainer = trainer
 
         self.load_config()  # Load config if plugin has not already referenced it
+
         self.state = State(self.model_dir,
                            self.name,
                            self.config_changeable_items,
                            no_logs,
-                           pingpong,
+                           self.vram_savings.pingpong,
                            training_image_size)
 
         self.blocks = NNBlocks(use_subpixel=self.config["subpixel_upscaling"],
@@ -102,11 +104,9 @@ class ModelBase():
                               "warp_to_landmarks": warp_to_landmarks,
                               "augment_color": augment_color,
                               "no_flip": no_flip,
-                              "pingpong": pingpong,
+                              "pingpong": self.vram_savings.pingpong,
                               "snapshot_interval": snapshot_interval}
 
-        self.optimizer_savings = optimizer_savings
-        self.set_gradient_type(memory_saving_gradients)
         if self.multiple_models_in_folder:
             deprecation_warning("Support for multiple model types within the same folder",
                                 additional_info="Please split each model into separate folders to "
@@ -207,15 +207,6 @@ class ModelBase():
         logger.debug(retval)
         return retval
 
-    @staticmethod
-    def set_gradient_type(memory_saving_gradients):
-        """ Monkeypatch Memory Saving Gradients if requested """
-        if not memory_saving_gradients:
-            return
-        logger.info("Using Memory Saving Gradients")
-        from lib.model import memory_saving_gradients
-        K.__dict__["gradients"] = memory_saving_gradients.gradients_memory
-
     def load_config(self):
         """ Load the global config for reference in self.config """
         global _CONFIG  # pylint: disable=global-statement
@@ -392,7 +383,7 @@ class ModelBase():
             # TODO: Remove this as soon it is fixed in PlaidML.
             opt_kwargs["clipnorm"] = 1.0
         logger.debug("Optimizer kwargs: %s", opt_kwargs)
-        return Adam(**opt_kwargs, cpu_mode=self.optimizer_savings)
+        return Adam(**opt_kwargs, cpu_mode=self.vram_savings.optimizer_savings)
 
     def converter(self, swap):
         """ Converter for autoencoder models """
@@ -611,6 +602,51 @@ class ModelBase():
         self.state.save()
 
 
+class VRAMSavings():
+    """ VRAM Saving training methods """
+    def __init__(self, pingpong, optimizer_savings, memory_saving_gradients):
+        logger.debug("Initializing %s: (pingpong: %s, optimizer_savings: %s, "
+                     "memory_saving_gradients: %s)", self.__class__.__name__,
+                     pingpong, optimizer_savings, memory_saving_gradients)
+        self.is_plaidml = keras.backend.backend() == "plaidml.keras.backend"
+        self.pingpong = self.set_pingpong(pingpong)
+        self.optimizer_savings = self.set_optimizer_savings(optimizer_savings)
+        self.memory_saving_gradients = self.set_gradient_type(memory_saving_gradients)
+        logger.debug("Initialized: %s", self.__class__.__name__)
+
+    def set_pingpong(self, pingpong):
+        """ Disable pingpong for plaidML users """
+        if pingpong and self.is_plaidml:
+            logger.warning("Pingpong training not supported on plaidML. Disabling")
+            pingpong = False
+        logger.debug("pingpong: %s", pingpong)
+        if pingpong:
+            logger.info("Using Pingpong Training")
+        return pingpong
+
+    def set_optimizer_savings(self, optimizer_savings):
+        """ Disable optimizer savings for plaidML users """
+        if optimizer_savings and self.is_plaidml == "plaidml.keras.backend":
+            logger.warning("Optimizer Savings not supported on plaidML. Disabling")
+            optimizer_savings = False
+        logger.debug("optimizer_savings: %s", optimizer_savings)
+        if optimizer_savings:
+            logger.info("Using Optimizer Savings")
+        return optimizer_savings
+
+    def set_gradient_type(self, memory_saving_gradients):
+        """ Monkeypatch Memory Saving Gradients if requested """
+        if memory_saving_gradients and self.is_plaidml:
+            logger.warning("Memory Saving Gradients not supported on plaidML. Disabling")
+            memory_saving_gradients = False
+        logger.debug("memory_saving_gradients: %s", memory_saving_gradients)
+        if memory_saving_gradients:
+            logger.info("Using Memory Saving Gradients")
+            from lib.model import memory_saving_gradients
+            K.__dict__["gradients"] = memory_saving_gradients.gradients_memory
+        return memory_saving_gradients
+
+
 class Loss():
     """ Holds loss names and functions for an Autoencoder """
     def __init__(self, side, outputs, mask_input, predict):
