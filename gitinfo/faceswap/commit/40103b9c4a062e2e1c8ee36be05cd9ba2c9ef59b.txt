commit 40103b9c4a062e2e1c8ee36be05cd9ba2c9ef59b
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Thu Jul 18 17:04:47 2019 +0000

    Model Structure Changes
    
    Standardize inputs for AutoEncoders
    Name outputs from AutoEncoders
    Name layers in nn_blocks.py

diff --git a/lib/model/nn_blocks.py b/lib/model/nn_blocks.py
index 27a2618..89e72ec 100644
--- a/lib/model/nn_blocks.py
+++ b/lib/model/nn_blocks.py
@@ -30,6 +30,7 @@ class NNBlocks():
                      "%s, use_reflect_padding: %s, first_run: %s)",
                      self.__class__.__name__, use_subpixel, use_icnr_init, use_convaware_init,
                      use_reflect_padding, first_run)
+        self.names = dict()
         self.first_run = first_run
         self.use_subpixel = use_subpixel
         self.use_icnr_init = use_icnr_init
@@ -40,6 +41,13 @@ class NNBlocks():
                         "few minutes...")
         logger.debug("Initialized %s", self.__class__.__name__)
 
+    def get_name(self, name):
+        """ Return unique layer name for requested block """
+        self.names[name] = self.names.setdefault(name, -1) + 1
+        name = "{}_{}".format(name, self.names[name])
+        logger.debug("Generating block name: %s", name)
+        return name
+
     def update_kwargs(self, kwargs):
         """ Update Kwargs for conv2D and Seperable conv2D layers.
             Set the default kernel initializer to conv_aware or he_uniform()
@@ -86,18 +94,22 @@ class NNBlocks():
         """ Convolution Layer"""
         logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, use_instance_norm: %s, "
                      "kwargs: %s)", inp, filters, kernel_size, strides, use_instance_norm, kwargs)
+        name = self.get_name("conv")
         if self.use_reflect_padding:
-            inp = ReflectionPadding2D(stride=strides, kernel_size=kernel_size)(inp)
+            inp = ReflectionPadding2D(stride=strides,
+                                      kernel_size=kernel_size,
+                                      name="{}_reflectionpadding2d".format(name))(inp)
             padding = "valid"
         var_x = self.conv2d(inp, filters,
                             kernel_size=kernel_size,
                             strides=strides,
                             padding=padding,
+                            name="{}_conv2d".format(name),
                             **kwargs)
         if use_instance_norm:
-            var_x = InstanceNormalization()(var_x)
+            var_x = InstanceNormalization(name="{}_instancenorm".format(name))(var_x)
         if not res_block_follows:
-            var_x = LeakyReLU(0.1)(var_x)
+            var_x = LeakyReLU(0.1, name="{}_leakyrelu".format(name))(var_x)
         return var_x
 
     def upscale(self, inp, filters, kernel_size=3, padding="same",
@@ -105,8 +117,11 @@ class NNBlocks():
         """ Upscale Layer """
         logger.debug("inp: %s, filters: %s, kernel_size: %s, use_instance_norm: %s, kwargs: %s)",
                      inp, filters, kernel_size, use_instance_norm, kwargs)
+        name = self.get_name("upscale")
         if self.use_reflect_padding:
-            inp = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(inp)
+            inp = ReflectionPadding2D(stride=1,
+                                      kernel_size=kernel_size,
+                                      name="{}_reflectionpadding2d".format(name))(inp)
             padding = "valid"
         kwargs = self.update_kwargs(kwargs)
         if self.use_icnr_init:
@@ -117,17 +132,18 @@ class NNBlocks():
                             kernel_size=kernel_size,
                             padding=padding,
                             force_initializer=True,
+                            name="{}_conv2d".format(name),
                             **kwargs)
         if self.use_icnr_init:
             self.switch_kernel_initializer(kwargs, original_init)
         if use_instance_norm:
-            var_x = InstanceNormalization()(var_x)
+            var_x = InstanceNormalization(name="{}_instancenorm".format(name))(var_x)
         if not res_block_follows:
-            var_x = LeakyReLU(0.1)(var_x)
+            var_x = LeakyReLU(0.1, name="{}_leakyrelu".format(name))(var_x)
         if self.use_subpixel:
-            var_x = SubPixelUpscaling()(var_x)
+            var_x = SubPixelUpscaling(name="{}_subpixel".format(name))(var_x)
         else:
-            var_x = PixelShuffler()(var_x)
+            var_x = PixelShuffler(name="{}_pixelshuffler".format(name))(var_x)
         return var_x
 
     # <<< DFaker Model Blocks >>> #
@@ -135,24 +151,31 @@ class NNBlocks():
         """ Residual block """
         logger.debug("inp: %s, filters: %s, kernel_size: %s, kwargs: %s)",
                      inp, filters, kernel_size, kwargs)
-        var_x = LeakyReLU(alpha=0.2)(inp)
+        name = self.get_name("residual")
+        var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_0".format(name))(inp)
         if self.use_reflect_padding:
-            var_x = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(var_x)
+            var_x = ReflectionPadding2D(stride=1,
+                                        kernel_size=kernel_size,
+                                        name="{}_reflectionpadding2d_0".format(name))(var_x)
             padding = "valid"
         var_x = self.conv2d(var_x, filters,
                             kernel_size=kernel_size,
                             padding=padding,
+                            name="{}_conv2d_0".format(name),
                             **kwargs)
-        var_x = LeakyReLU(alpha=0.2)(var_x)
+        var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_1".format(name))(var_x)
         if self.use_reflect_padding:
-            var_x = ReflectionPadding2D(stride=1, kernel_size=kernel_size)(var_x)
+            var_x = ReflectionPadding2D(stride=1,
+                                        kernel_size=kernel_size,
+                                        name="{}_reflectionpadding2d_1".format(name))(var_x)
             padding = "valid"
         var_x = self.conv2d(var_x, filters,
                             kernel_size=kernel_size,
                             padding=padding,
+                            name="{}_conv2d_1".format(name),
                             **kwargs)
         var_x = Add()([var_x, inp])
-        var_x = LeakyReLU(alpha=0.2)(var_x)
+        var_x = LeakyReLU(alpha=0.2, name="{}_leakyrelu_3".format(name))(var_x)
         return var_x
 
     # <<< Unbalanced Model Blocks >>> #
@@ -160,13 +183,15 @@ class NNBlocks():
         """ Seperable Convolution Layer """
         logger.debug("inp: %s, filters: %s, kernel_size: %s, strides: %s, kwargs: %s)",
                      inp, filters, kernel_size, strides, kwargs)
+        name = self.get_name("separableconv2d")
         kwargs = self.update_kwargs(kwargs)
         var_x = SeparableConv2D(filters,
                                 kernel_size=kernel_size,
                                 strides=strides,
                                 padding="same",
+                                name="{}_seperableconv2d".format(name),
                                 **kwargs)(inp)
-        var_x = Activation("relu")(var_x)
+        var_x = Activation("relu", name="{}_relu".format(name))(var_x)
         return var_x
 
 # <<< GAN V2.2 Blocks >>> #
diff --git a/plugins/train/model/_base.py b/plugins/train/model/_base.py
index 257cf13..61d94dd 100644
--- a/plugins/train/model/_base.py
+++ b/plugins/train/model/_base.py
@@ -14,6 +14,7 @@ from json import JSONDecodeError
 import keras
 from keras import losses
 from keras import backend as K
+from keras.layers import Input
 from keras.models import load_model, Model
 from keras.utils import get_custom_objects, multi_gpu_model
 
@@ -69,7 +70,6 @@ class ModelBase():
         self.gpus = gpus
         self.configfile = configfile
         self.input_shape = input_shape
-        self.output_shape = None  # set after model is compiled
         self.encoder_dim = encoder_dim
         self.trainer = trainer
 
@@ -162,6 +162,16 @@ class ModelBase():
         logger.debug("model_files: %s, retval: %s", model_files, retval)
         return retval
 
+    @property
+    def output_shape(self):
+        """ Return the output shapes from the main AutoEncoder """
+        out = list()
+        for predictor in self.predictors.values():
+            out.extend([K.int_shape(output)[-3:] for output in predictor.outputs])
+            break  # Only get output from one autoencoder. Shapes are the same
+        # Only return the output shape of the face
+        return tuple(out[0])
+
     @staticmethod
     def set_gradient_type(memory_saving_gradients):
         """ Monkeypatch Memory Saving Gradients if requested """
@@ -204,8 +214,9 @@ class ModelBase():
         """ Build the model. Override for custom build methods """
         self.add_networks()
         self.load_models(swapped=False)
+        inputs = self.get_inputs()
         try:
-            self.build_autoencoders()
+            self.build_autoencoders(inputs)
         except ValueError as err:
             if "must be from the same graph" in str(err).lower():
                 msg = ("There was an error loading saved weights. This is most likely due to "
@@ -219,14 +230,25 @@ class ModelBase():
         self.log_summary()
         self.compile_predictors(initialize=True)
 
-    def build_autoencoders(self):
+    def get_inputs(self):
+        """ Return the inputs for the model """
+        logger.debug("Getting inputs")
+        inputs = [Input(shape=self.input_shape, name="face_in")]
+        output_network = [network for network in self.networks.values() if network.is_output][0]
+        if "mask_out" in output_network.output_names:
+            mask_idx = output_network.output_names.index("mask_out")
+            mask_shape = output_network.output_shapes[mask_idx]
+            inputs.append(Input(shape=mask_shape[1:], name="mask_in"))
+        logger.debug("Got inputs: %s", inputs)
+        return inputs
+
+    def build_autoencoders(self, inputs):
         """ Override for Model Specific autoencoder builds
 
-            NB! ENSURE YOU NAME YOUR INPUTS. At least the following input names
-            are expected:
-                face (the input for image)
-                mask (the input for mask if it is used)
-
+            Inputs is defined in self.get_inputs() and is standardized for all models
+                if will generally be in the order:
+                [face (the input for image),
+                 mask (the input for mask if it is used)]
         """
         raise NotImplementedError
 
@@ -275,8 +297,6 @@ class ModelBase():
         self.predictors[side] = model
         if not self.state.inputs:
             self.store_input_shapes(model)
-        if not self.output_shape:
-            self.set_output_shape(model)
 
     def store_input_shapes(self, model):
         """ Store the input and output shapes to state """
@@ -288,15 +308,6 @@ class ModelBase():
         self.state.inputs = inputs
         logger.debug("Added input shapes: %s", self.state.inputs)
 
-    def set_output_shape(self, model):
-        """ Set the output shape for use in training and convert """
-        logger.debug("Setting output shape")
-        out = [K.int_shape(tensor)[-3:] for tensor in model.outputs]
-        if not out:
-            raise ValueError("No outputs found! Check your model.")
-        self.output_shape = tuple(out[0])
-        logger.debug("Added output shape: %s", self.output_shape)
-
     def reset_pingpong(self):
         """ Reset the models for pingpong training """
         logger.debug("Resetting models")
@@ -310,7 +321,8 @@ class ModelBase():
             model.network = Model.from_config(model.config)
             model.network.set_weights(model.weights)
 
-        self.build_autoencoders()
+        inputs = self.get_inputs()
+        self.build_autoencoders(inputs)
         self.compile_predictors(initialize=False)
         logger.debug("Reset models")
 
@@ -636,6 +648,26 @@ class NNMeta():
             name += "_{}".format(self.side)
         return name
 
+    @property
+    def output_names(self):
+        """ Return output node names """
+        output_names = [output.name for output in self.network.outputs]
+        if self.is_output and not any(name == "face_out" for name in output_names):
+            # Saved models break if their layer names are changed, so dummy
+            # in correct output names for legacy models
+            output_names = self.get_output_names()
+        return output_names
+
+    def get_output_names(self):
+        """ Return the output names based on number of channels and instances """
+        output_types = ["mask_out" if K.int_shape(output)[-1] == 1 else "face_out"
+                        for output in self.network.outputs]
+        output_names = ["{}{}".format(name,
+                                      "" if output_types.count(name) == 1 else "_{}".format(idx))
+                        for idx, name in enumerate(output_types)]
+        logger.debug(output_names)
+        return output_names
+
     def load(self, fullpath=None):
         """ Load model """
         fullpath = fullpath if fullpath else self.filename
diff --git a/plugins/train/model/dfaker.py b/plugins/train/model/dfaker.py
index fc4a650..758c43b 100644
--- a/plugins/train/model/dfaker.py
+++ b/plugins/train/model/dfaker.py
@@ -21,21 +21,6 @@ class Model(OriginalModel):
         super().__init__(*args, **kwargs)
         logger.debug("Initialized %s", self.__class__.__name__)
 
-    def build_autoencoders(self):
-        """ Initialize Dfaker model """
-        logger.debug("Initializing model")
-        inputs = [Input(shape=self.input_shape, name="face")]
-        if self.config.get("mask_type", None):
-            mask_shape = (self.input_shape[0] * 2, self.input_shape[1] * 2, 1)
-            inputs.append(Input(shape=mask_shape, name="mask"))
-
-        for side in ("a", "b"):
-            decoder = self.networks["decoder_{}".format(side)].network
-            output = decoder(self.networks["encoder"].network(inputs[0]))
-            autoencoder = KerasModel(inputs, output)
-            self.add_predictor(side, autoencoder)
-        logger.debug("Initialized model")
-
     def decoder(self):
         """ Decoder Network """
         input_ = Input(shape=(8, 8, 512))
@@ -48,7 +33,11 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, 128, res_block_follows=True)
         var_x = self.blocks.res_block(var_x, 128, kernel_initializer=self.kernel_initializer)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -60,6 +49,7 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel([input_], outputs=outputs)
diff --git a/plugins/train/model/dfl_h128.py b/plugins/train/model/dfl_h128.py
index 1153fd0..6bf9a6f 100644
--- a/plugins/train/model/dfl_h128.py
+++ b/plugins/train/model/dfl_h128.py
@@ -44,7 +44,11 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, self.encoder_dim)
         var_x = self.blocks.upscale(var_x, self.encoder_dim // 2)
         var_x = self.blocks.upscale(var_x, self.encoder_dim // 4)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
         # Mask
         if self.config.get("mask_type", None):
@@ -55,6 +59,7 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/iae.py b/plugins/train/model/iae.py
index e3d5a79..b164fef 100644
--- a/plugins/train/model/iae.py
+++ b/plugins/train/model/iae.py
@@ -27,14 +27,9 @@ class Model(ModelBase):
         self.add_network("inter", None, self.intermediate())
         logger.debug("Added networks")
 
-    def build_autoencoders(self):
+    def build_autoencoders(self, inputs):
         """ Initialize IAE model """
         logger.debug("Initializing model")
-        inputs = [Input(shape=self.input_shape, name="face")]
-        if self.config.get("mask_type", None):
-            mask_shape = (self.input_shape[:2] + (1, ))
-            inputs.append(Input(shape=mask_shape, name="mask"))
-
         decoder = self.networks["decoder"].network
         encoder = self.networks["encoder"].network
         inter_both = self.networks["inter"].network
@@ -75,7 +70,11 @@ class Model(ModelBase):
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -87,6 +86,7 @@ class Model(ModelBase):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/lightweight.py b/plugins/train/model/lightweight.py
index 703d8cc..1963c8c 100644
--- a/plugins/train/model/lightweight.py
+++ b/plugins/train/model/lightweight.py
@@ -40,7 +40,11 @@ class Model(OriginalModel):
         var_x = self.blocks.upscale(var_x, 512)
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -51,6 +55,7 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/original.py b/plugins/train/model/original.py
index 8fbf40e..55d3bea 100644
--- a/plugins/train/model/original.py
+++ b/plugins/train/model/original.py
@@ -33,14 +33,9 @@ class Model(ModelBase):
         self.add_network("encoder", None, self.encoder())
         logger.debug("Added networks")
 
-    def build_autoencoders(self):
+    def build_autoencoders(self, inputs):
         """ Initialize original model """
         logger.debug("Initializing model")
-        inputs = [Input(shape=self.input_shape, name="face")]
-        if self.config.get("mask_type", None):
-            mask_shape = (self.input_shape[:2] + (1, ))
-            inputs.append(Input(shape=mask_shape, name="mask"))
-
         for side in ("a", "b"):
             logger.debug("Adding Autoencoder. Side: %s", side)
             decoder = self.networks["decoder_{}".format(side)].network
@@ -71,7 +66,11 @@ class Model(ModelBase):
         var_x = self.blocks.upscale(var_x, 256)
         var_x = self.blocks.upscale(var_x, 128)
         var_x = self.blocks.upscale(var_x, 64)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -82,6 +81,7 @@ class Model(ModelBase):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/realface.py b/plugins/train/model/realface.py
index 5e268f9..819b510 100644
--- a/plugins/train/model/realface.py
+++ b/plugins/train/model/realface.py
@@ -69,21 +69,16 @@ class Model(ModelBase):
         return dense_width, upscalers_no
 
     def add_networks(self):
-        """ Add the original model weights """
+        """ Add the realface model weights """
         logger.debug("Adding networks")
         self.add_network("decoder", "a", self.decoder_a(), is_output=True)
         self.add_network("decoder", "b", self.decoder_b(), is_output=True)
         self.add_network("encoder", None, self.encoder())
         logger.debug("Added networks")
 
-    def build_autoencoders(self):
-        """ Initialize original model """
+    def build_autoencoders(self, inputs):
+        """ Initialize realface model """
         logger.debug("Initializing model")
-        inputs = [Input(shape=self.input_shape, name="face")]
-        if self.config.get("mask_type", None):
-            mask_shape = self.config["output_size"], self.config["output_size"], 1
-            inputs.append(Input(shape=mask_shape, name="mask"))
-
         for side in "a", "b":
             logger.debug("Adding Autoencoder. Side: %s", side)
             decoder = self.networks["decoder_{}".format(side)].network
@@ -131,7 +126,11 @@ class Model(ModelBase):
             var_x = self.blocks.res_block(var_x, decoder_b_complexity // 2**idx, use_bias=True)
         var_x = self.blocks.upscale(var_x, decoder_b_complexity // 2**(idx + 1))
 
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
 
         outputs = [var_x]
 
@@ -145,7 +144,8 @@ class Model(ModelBase):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
 
             outputs += [var_y]
 
@@ -176,7 +176,11 @@ class Model(ModelBase):
             var_x = self.blocks.upscale(var_x, decoder_a_complexity // 2**idx)
         var_x = self.blocks.upscale(var_x, decoder_a_complexity // 2**(idx + 1))
 
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
 
         outputs = [var_x]
 
@@ -190,7 +194,8 @@ class Model(ModelBase):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
 
             outputs += [var_y]
 
diff --git a/plugins/train/model/unbalanced.py b/plugins/train/model/unbalanced.py
index 0d4709f..b8c2a08 100644
--- a/plugins/train/model/unbalanced.py
+++ b/plugins/train/model/unbalanced.py
@@ -73,7 +73,11 @@ class Model(OriginalModel):
             var_x = SpatialDropout2D(0.25)(var_x)
         var_x = self.blocks.upscale(var_x, decoder_complexity // 2, **kwargs)
         var_x = self.blocks.upscale(var_x, decoder_complexity // 4, **kwargs)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -85,7 +89,8 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
 
@@ -117,7 +122,11 @@ class Model(OriginalModel):
             var_x = self.blocks.res_block(var_x, decoder_complexity // 2,
                                           kernel_initializer=self.kernel_initializer)
             var_x = self.blocks.upscale(var_x, decoder_complexity // 4, **kwargs)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -132,6 +141,7 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
diff --git a/plugins/train/model/villain.py b/plugins/train/model/villain.py
index f43af37..4a0a67a 100644
--- a/plugins/train/model/villain.py
+++ b/plugins/train/model/villain.py
@@ -71,7 +71,11 @@ class Model(OriginalModel):
         var_x = self.blocks.res_block(var_x, 256, **kwargs)
         var_x = self.blocks.upscale(var_x, self.input_shape[0], res_block_follows=True, **kwargs)
         var_x = self.blocks.res_block(var_x, self.input_shape[0], **kwargs)
-        var_x = self.blocks.conv2d(var_x, 3, kernel_size=5, padding="same", activation="sigmoid")
+        var_x = self.blocks.conv2d(var_x, 3,
+                                   kernel_size=5,
+                                   padding="same",
+                                   activation="sigmoid",
+                                   name="face_out")
         outputs = [var_x]
 
         if self.config.get("mask_type", None):
@@ -82,6 +86,7 @@ class Model(OriginalModel):
             var_y = self.blocks.conv2d(var_y, 1,
                                        kernel_size=5,
                                        padding="same",
-                                       activation="sigmoid")
+                                       activation="sigmoid",
+                                       name="mask_out")
             outputs.append(var_y)
         return KerasModel(input_, outputs=outputs)
