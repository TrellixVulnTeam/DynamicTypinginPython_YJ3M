commit 11f35fa7986a83e8dda87018b3826e85a7299e75
Author: torzdf <36920800+torzdf@users.noreply.github.com>
Date:   Thu Jun 20 18:25:28 2019 +0100

    Fix Active Devices for plaidML. Add Supports PlaidML to Extractors

diff --git a/lib/gpu_stats.py b/lib/gpu_stats.py
index 8cf2cbf..6b13121 100644
--- a/lib/gpu_stats.py
+++ b/lib/gpu_stats.py
@@ -56,6 +56,11 @@ class GPUStats():
         if self.logger:
             self.logger.debug("Initialized %s", self.__class__.__name__)
 
+    @property
+    def is_plaidml(self):
+        """ Return whether running on plaidML backend """
+        return self.plaid is not None
+
     def initialize(self, log=False):
         """ Initialize pynvml """
         if not self.initialized:
@@ -192,10 +197,9 @@ class GPUStats():
         self.initialize()
         if self.plaid:
             # NB There is no useful way to get allocated VRAM on PlaidML.
-            # OpenCL loads and unloads VRAM as required, so this returns the global memory size
-            # less the maximum allowed allocation size. It's not particularly useful
-            vram = [self.plaid.vram[idx] - self.plaid.max_alloc[idx]
-                    for idx in range(self.device_count)]
+            # OpenCL loads and unloads VRAM as required, so this returns 0
+            # It's not particularly useful
+            vram = [0 for idx in range(self.device_count)]
 
         elif IS_MACOS:
             vram = [pynvx.cudaGetMemUsed(handle, ignore=True) / (1024 * 1024)
@@ -214,9 +218,9 @@ class GPUStats():
         self.initialize()
         if self.plaid:
             # NB There is no useful way to get free VRAM on PlaidML.
-            # OpenCL loads and unloads VRAM as required, so this returns the maximum allowed
-            # allocation size. It's not particularly useful
-            vram = self.plaid.max_alloc
+            # OpenCL loads and unloads VRAM as required, so this returns the total memory
+            # It's not particularly useful
+            vram = self.plaid.vram
         elif IS_MACOS:
             vram = [pynvx.cudaGetMemFree(handle, ignore=True) / (1024 * 1024)
                     for handle in self.handles]
@@ -228,10 +232,10 @@ class GPUStats():
             self.logger.debug("GPU VRAM free: %s", vram)
         return vram
 
-    def get_card_most_free(self):
+    def get_card_most_free(self, supports_plaidml=True):
         """ Return the card and available VRAM for active card with
             most VRAM free """
-        if self.device_count == 0:
+        if self.device_count == 0 or (self.is_plaidml and not supports_plaidml):
             return {"card_id": -1,
                     "device": "No Nvidia devices found",
                     "free": 2048,
diff --git a/lib/plaidml_tools.py b/lib/plaidml_tools.py
index 9cc178e..972ae8a 100644
--- a/lib/plaidml_tools.py
+++ b/lib/plaidml_tools.py
@@ -37,7 +37,7 @@ class PlaidMLStats():
     @property
     def active_devices(self):
         """ Return the active device IDs """
-        return plaidml.settings.device_ids
+        return [idx for idx, d_id in enumerate(self.ids) if d_id in plaidml.settings.device_ids]
 
     @property
     def device_count(self):
diff --git a/plugins/extract/align/_base.py b/plugins/extract/align/_base.py
index 2f4f902..b20f353 100644
--- a/plugins/extract/align/_base.py
+++ b/plugins/extract/align/_base.py
@@ -59,6 +59,10 @@ class Aligner():
         # how many parallel processes / batches can be run.
         # Be conservative to avoid OOM.
         self.vram = None
+
+        # Set to true if the plugin supports PlaidML
+        self.supports_plaidml = False
+
         logger.debug("Initialized %s", self.__class__.__name__)
 
     # <<< OVERRIDE METHODS >>> #
@@ -218,11 +222,10 @@ class Aligner():
         self.queues["out"].put((output))
 
     # <<< MISC METHODS >>> #
-    @staticmethod
-    def get_vram_free():
+    def get_vram_free(self):
         """ Return free and total VRAM on card with most VRAM free"""
         stats = GPUStats()
-        vram = stats.get_card_most_free()
+        vram = stats.get_card_most_free(supports_plaidml=self.supports_plaidml)
         logger.verbose("Using device %s with %sMB free of %sMB",
                        vram["device"],
                        int(vram["free"]),
diff --git a/plugins/extract/detect/_base.py b/plugins/extract/detect/_base.py
index f75626a..7917a9c 100755
--- a/plugins/extract/detect/_base.py
+++ b/plugins/extract/detect/_base.py
@@ -63,6 +63,9 @@ class Detector():
         # Be conservative to avoid OOM.
         self.vram = None
 
+        # Set to true if the plugin supports PlaidML
+        self.supports_plaidml = False
+
         # For detectors that support batching, this should be set to
         # the calculated batch size that the amount of available VRAM
         # will support. It is also used for holding the number of threads/
@@ -317,11 +320,10 @@ class Detector():
         return (exhausted, batch)
 
     # <<< MISC METHODS >>> #
-    @staticmethod
-    def get_vram_free():
+    def get_vram_free(self):
         """ Return free and total VRAM on card with most VRAM free"""
         stats = GPUStats()
-        vram = stats.get_card_most_free()
+        vram = stats.get_card_most_free(supports_plaidml=self.supports_plaidml)
         logger.verbose("Using device %s with %sMB free of %sMB",
                        vram["device"],
                        int(vram["free"]),
diff --git a/plugins/extract/pipeline.py b/plugins/extract/pipeline.py
index ea920bf..d7ee84b 100644
--- a/plugins/extract/pipeline.py
+++ b/plugins/extract/pipeline.py
@@ -96,20 +96,27 @@ class Extractor():
         """ Set whether to run detect and align together or separately """
         detector_vram = self.detector.vram
         aligner_vram = self.aligner.vram
+
         if detector_vram == 0 or aligner_vram == 0:
             logger.debug("At least one of aligner or detector have no VRAM requirement. "
                          "Enabling parallel processing.")
             return True
 
+        if not multiprocess:
+            logger.debug("Parallel processing disabled by cli.")
+            return False
+
         gpu_stats = GPUStats()
+        if gpu_stats.is_plaidml and (not self.detector.supports_plaidml or
+                                     not self.aligner.supports_plaidml):
+            logger.debug("At least one of aligner or detector does not support plaidML. "
+                         "Enabling parallel processing.")
+            return True
+
         if gpu_stats.device_count == 0:
             logger.debug("No GPU detected. Enabling parallel processing.")
             return True
 
-        if not multiprocess:
-            logger.debug("Parallel processing disabled by cli.")
-            return False
-
         required_vram = detector_vram + aligner_vram + 320  # 320MB buffer
         stats = gpu_stats.get_card_most_free()
         free_vram = int(stats["free"])
