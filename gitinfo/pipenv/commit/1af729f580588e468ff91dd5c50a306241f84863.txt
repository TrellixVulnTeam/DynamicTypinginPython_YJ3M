commit 1af729f580588e468ff91dd5c50a306241f84863
Author: Dan Ryan <dan@danryan.co>
Date:   Fri Sep 7 01:58:24 2018 -0400

    Vendor passa, update piptools
    
    Signed-off-by: Dan Ryan <dan@danryan.co>

diff --git a/pipenv/patched/patched.txt b/pipenv/patched/patched.txt
index f8bdb9e1..d12555de 100644
--- a/pipenv/patched/patched.txt
+++ b/pipenv/patched/patched.txt
@@ -2,6 +2,6 @@ safety
 git+https://github.com/jumpscale7/python-consistent-toml.git#egg=contoml
 crayons==0.1.2
 pipfile==0.0.2
-git+https://github.com/jazzband/pip-tools.git@9cb41d828fcb0967a32cc140c1dcaca94e5f4daa#egg=piptools
+git+https://github.com/jazzband/pip-tools.git@19a3b1f11d941b01209bb4fad4a2a16d15f67171#egg=piptools
 prettytoml==0.3
-pip==10.0.1
+pip==18.0
diff --git a/pipenv/patched/piptools/_compat/pip_compat.py b/pipenv/patched/piptools/_compat/pip_compat.py
index 0a0d27dc..3ea08267 100644
--- a/pipenv/patched/piptools/_compat/pip_compat.py
+++ b/pipenv/patched/piptools/_compat/pip_compat.py
@@ -1,42 +1,37 @@
 # -*- coding=utf-8 -*-
 import importlib
 
-
 def do_import(module_path, subimport=None, old_path=None, vendored_name=None):
-    internal = 'pip._internal.{0}'.format(module_path)
     old_path = old_path or module_path
-    pip9 = 'pip.{0}'.format(old_path)
-    _tmp = None
-    if vendored_name:
-        vendor = '{0}._internal'.format(vendored_name)
-        vendor = '{0}.{1}'.format(vendor, old_path if old_path else module_path)
-        try:
-            _tmp = importlib.import_module(vendor)
-        except ImportError:
-            pass
-    if not _tmp:
+    prefix = vendored_name if vendored_name else "pip"
+    prefixes = ["{0}._internal".format(prefix), "{0}".format(prefix)]
+    paths = [module_path, old_path]
+    search_order = ["{0}.{1}".format(p, pth) for p in prefixes for pth in paths if pth is not None]
+    package = subimport if subimport else None
+    for to_import in search_order:
+        if not subimport:
+            to_import, _, package = to_import.rpartition(".")
         try:
-            _tmp = importlib.import_module(internal)
+            imported = importlib.import_module(to_import)
         except ImportError:
-            _tmp = importlib.import_module(pip9)
-    if subimport:
-        return getattr(_tmp, subimport, _tmp)
-    return _tmp
+            continue
+        else:
+            return getattr(imported, package)
 
 
-InstallRequirement = do_import('req.req_install', 'InstallRequirement', vendored_name='notpip')
-parse_requirements = do_import('req.req_file', 'parse_requirements', vendored_name='notpip')
-RequirementSet = do_import('req.req_set', 'RequirementSet', vendored_name='notpip')
-user_cache_dir = do_import('utils.appdirs', 'user_cache_dir', vendored_name='notpip')
-FAVORITE_HASH = do_import('utils.hashes', 'FAVORITE_HASH', vendored_name='notpip')
-is_file_url = do_import('download', 'is_file_url', vendored_name='notpip')
-url_to_path = do_import('download', 'url_to_path', vendored_name='notpip')
-PackageFinder = do_import('index', 'PackageFinder', vendored_name='notpip')
-FormatControl = do_import('index', 'FormatControl', vendored_name='notpip')
-Wheel = do_import('wheel', 'Wheel', vendored_name='notpip')
-Command = do_import('basecommand', 'Command', vendored_name='notpip')
-cmdoptions = do_import('cmdoptions', vendored_name='notpip')
-get_installed_distributions = do_import('utils.misc', 'get_installed_distributions', old_path='utils', vendored_name='notpip')
+InstallRequirement = do_import('req.req_install', 'InstallRequirement', vendored_name="notpip")
+parse_requirements = do_import('req.req_file', 'parse_requirements', vendored_name="notpip")
+RequirementSet = do_import('req.req_set', 'RequirementSet', vendored_name="notpip")
+user_cache_dir = do_import('utils.appdirs', 'user_cache_dir', vendored_name="notpip")
+FAVORITE_HASH = do_import('utils.hashes', 'FAVORITE_HASH', vendored_name="notpip")
+is_file_url = do_import('download', 'is_file_url', vendored_name="notpip")
+url_to_path = do_import('download', 'url_to_path', vendored_name="notpip")
+PackageFinder = do_import('index', 'PackageFinder', vendored_name="notpip")
+FormatControl = do_import('index', 'FormatControl', vendored_name="notpip")
+Wheel = do_import('wheel', 'Wheel', vendored_name="notpip")
+Command = do_import('cli.base_command', 'Command', old_path='basecommand', vendored_name="notpip")
+cmdoptions = do_import('cli.cmdoptions', old_path='cmdoptions', vendored_name="notpip")
+get_installed_distributions = do_import('utils.misc', 'get_installed_distributions', old_path='utils', vendored_name="notpip")
 PyPI = do_import('models.index', 'PyPI', vendored_name='notpip')
 SafeFileCache = do_import('download', 'SafeFileCache', vendored_name='notpip')
 InstallationError = do_import('exceptions', 'InstallationError', vendored_name='notpip')
diff --git a/pipenv/patched/piptools/cache.py b/pipenv/patched/piptools/cache.py
index 610a4f37..7595b964 100644
--- a/pipenv/patched/piptools/cache.py
+++ b/pipenv/patched/piptools/cache.py
@@ -6,7 +6,7 @@ import json
 import os
 import sys
 
-from pipenv.patched.notpip._vendor.packaging.requirements import Requirement
+from pip._vendor.packaging.requirements import Requirement
 
 from .exceptions import PipToolsError
 from .locations import CACHE_DIR
diff --git a/pipenv/patched/piptools/io.py b/pipenv/patched/piptools/io.py
index e9526832..b6bca675 100644
--- a/pipenv/patched/piptools/io.py
+++ b/pipenv/patched/piptools/io.py
@@ -347,7 +347,7 @@ class AtomicSaver(object):
         writer at a time.
       * Optional recovery of partial data in failure cases.
 
-    .. _context manager: https://docs.python.org/2/reference/compound_stmts.html#with
+    .. _context manager: https://docs.python.org/3/reference/compound_stmts.html#with
     .. _umask: https://en.wikipedia.org/wiki/Umask
 
     """
diff --git a/pipenv/patched/piptools/locations.py b/pipenv/patched/piptools/locations.py
index fbc820ab..4e6174c5 100644
--- a/pipenv/patched/piptools/locations.py
+++ b/pipenv/patched/piptools/locations.py
@@ -2,13 +2,10 @@ import os
 from shutil import rmtree
 
 from .click import secho
-# Patch by vphilippon 2017-11-22: Use pipenv cache path.
-# from ._compat import user_cache_dir
-from pipenv.environments import PIPENV_CACHE_DIR
+from ._compat import user_cache_dir
 
-# The user_cache_dir helper comes straight from pipenv.patched.notpip itself
-# CACHE_DIR = user_cache_dir(os.path.join('pip-tools'))
-CACHE_DIR = PIPENV_CACHE_DIR
+# The user_cache_dir helper comes straight from pip itself
+CACHE_DIR = user_cache_dir('pip-tools')
 
 # NOTE
 # We used to store the cache dir under ~/.pip-tools, which is not the
diff --git a/pipenv/patched/piptools/repositories/pypi.py b/pipenv/patched/piptools/repositories/pypi.py
index 9e741560..eb20560d 100644
--- a/pipenv/patched/piptools/repositories/pypi.py
+++ b/pipenv/patched/piptools/repositories/pypi.py
@@ -4,7 +4,6 @@ from __future__ import (absolute_import, division, print_function,
 import copy
 import hashlib
 import os
-import sys
 from contextlib import contextmanager
 from shutil import rmtree
 
@@ -18,33 +17,29 @@ from .._compat import (
     TemporaryDirectory,
     PyPI,
     InstallRequirement,
-    SafeFileCache,
+    SafeFileCache
 )
+os.environ["PIP_SHIMS_BASE_MODULE"] = "notpip"
+from pip_shims.shims import pip_import, VcsSupport, WheelCache
+from packaging.requirements import Requirement
+from packaging.specifiers import SpecifierSet, Specifier
+from packaging.markers import Op, Value, Variable, Marker
+InstallationError = pip_import("InstallationError", "exceptions.InstallationError", "7.0", "9999")
+from notpip._internal.resolve import Resolver as PipResolver
 
-from pipenv.patched.notpip._vendor.packaging.requirements import Requirement
-from pipenv.patched.notpip._vendor.packaging.specifiers import SpecifierSet, Specifier
-from pipenv.patched.notpip._vendor.packaging.markers import Op, Value, Variable
-from pipenv.patched.notpip._internal.exceptions import InstallationError
-from pipenv.patched.notpip._internal.vcs import VcsSupport
 
-from pipenv.environments import PIPENV_CACHE_DIR
+from pipenv.environments import PIPENV_CACHE_DIR as CACHE_DIR
 from ..exceptions import NoCandidateFound
-from ..utils import (fs_str, is_pinned_requirement, lookup_table,
+from ..utils import (fs_str, is_pinned_requirement, lookup_table, dedup,
                      make_install_requirement, clean_requires_python)
-
 from .base import BaseRepository
 
-
-try:
-    from pipenv.patched.notpip._internal.operations.prepare import RequirementPreparer
-    from pipenv.patched.notpip._internal.resolve import Resolver as PipResolver
-except ImportError:
-    pass
-
 try:
-    from pipenv.patched.notpip._internal.cache import WheelCache
+    from notpip._internal.req.req_tracker import RequirementTracker
 except ImportError:
-    from pipenv.patched.notpip.wheel import WheelCache
+    @contextmanager
+    def RequirementTracker():
+        yield
 
 
 class HashCache(SafeFileCache):
@@ -56,7 +51,7 @@ class HashCache(SafeFileCache):
     def __init__(self, *args, **kwargs):
         session = kwargs.pop('session')
         self.session = session
-        kwargs.setdefault('directory', os.path.join(PIPENV_CACHE_DIR, 'hash-cache'))
+        kwargs.setdefault('directory', os.path.join(CACHE_DIR, 'hash-cache'))
         super(HashCache, self).__init__(*args, **kwargs)
 
     def get_hash(self, location):
@@ -99,7 +94,6 @@ class PyPIRepository(BaseRepository):
         self.session = session
         self.use_json = use_json
         self.pip_options = pip_options
-        self.wheel_cache = WheelCache(PIPENV_CACHE_DIR, pip_options.format_control)
 
         index_urls = [pip_options.index_url] + pip_options.extra_index_urls
         if pip_options.no_index:
@@ -131,8 +125,8 @@ class PyPIRepository(BaseRepository):
 
         # Setup file paths
         self.freshen_build_caches()
-        self._download_dir = fs_str(os.path.join(PIPENV_CACHE_DIR, 'pkgs'))
-        self._wheel_download_dir = fs_str(os.path.join(PIPENV_CACHE_DIR, 'wheels'))
+        self._download_dir = fs_str(os.path.join(CACHE_DIR, 'pkgs'))
+        self._wheel_download_dir = fs_str(os.path.join(CACHE_DIR, 'wheels'))
 
     def freshen_build_caches(self):
         """
@@ -169,7 +163,6 @@ class PyPIRepository(BaseRepository):
             return ireq  # return itself as the best match
 
         all_candidates = clean_requires_python(self.find_all_candidates(ireq.name))
-
         candidates_by_version = lookup_table(all_candidates, key=lambda c: c.version, unique=True)
         try:
             matching_versions = ireq.specifier.filter((candidate.version for candidate in all_candidates),
@@ -184,14 +177,10 @@ class PyPIRepository(BaseRepository):
         best_candidate = max(matching_candidates, key=self.finder._candidate_sort_key)
 
         # Turn the candidate into a pinned InstallRequirement
-        new_req = make_install_requirement(
-            best_candidate.project, best_candidate.version, ireq.extras, ireq.markers, constraint=ireq.constraint
+        return make_install_requirement(
+            best_candidate.project, best_candidate.version, ireq.extras, ireq.markers,  constraint=ireq.constraint
          )
 
-        # KR TODO: Marker here?
-
-        return new_req
-
     def get_json_dependencies(self, ireq):
 
         if not (is_pinned_requirement(ireq)):
@@ -248,116 +237,86 @@ class PyPIRepository(BaseRepository):
 
         return json_results
 
-    def get_legacy_dependencies(self, ireq):
-        """
-        Given a pinned or an editable InstallRequirement, returns a set of
-        dependencies (also InstallRequirements, but not necessarily pinned).
-        They indicate the secondary dependencies for the given requirement.
-        """
-        if not (ireq.editable or is_pinned_requirement(ireq)):
-            raise TypeError('Expected pinned or editable InstallRequirement, got {}'.format(ireq))
+    def resolve_reqs(self, download_dir, ireq, wheel_cache, setup_requires={}, dist=None):
+        results = None
+        setup_requires = {}
+        dist = None
+        try:
+            from notpip._internal.operations.prepare import RequirementPreparer
+        except ImportError:
+                # Pip 9 and below
+            reqset = RequirementSet(
+                self.build_dir,
+                self.source_dir,
+                download_dir=download_dir,
+                wheel_download_dir=self._wheel_download_dir,
+                session=self.session,
+                ignore_installed=True,
+                ignore_compatibility=False,
+                wheel_cache=wheel_cache
+            )
+            results = reqset._prepare_file(self.finder, ireq, ignore_requires_python=True)
+        else:
+            # pip >= 10
+            preparer_kwargs = {
+                'build_dir': self.build_dir,
+                'src_dir': self.source_dir,
+                'download_dir': download_dir,
+                'wheel_download_dir': self._wheel_download_dir,
+                'progress_bar': 'off',
+                'build_isolation': True
+            }
+            resolver_kwargs = {
+                'finder': self.finder,
+                'session': self.session,
+                'upgrade_strategy': "to-satisfy-only",
+                'force_reinstall': True,
+                'ignore_dependencies': False,
+                'ignore_requires_python': True,
+                'ignore_installed': True,
+                'isolated': False,
+                'wheel_cache': wheel_cache,
+                'use_user_site': False,
+                'ignore_compatibility': True
+            }
+            resolver = None
+            preparer = None
+            with RequirementTracker() as req_tracker:
+                # Pip 18 uses a requirement tracker to prevent fork bombs
+                if req_tracker:
+                    preparer_kwargs['req_tracker'] = req_tracker
+                preparer = RequirementPreparer(**preparer_kwargs)
+                resolver_kwargs['preparer'] = preparer
+                reqset = RequirementSet()
+                ireq.is_direct = True
+                # reqset.add_requirement(ireq)
+                resolver = PipResolver(**resolver_kwargs)
+                resolver.require_hashes = False
+                results = resolver._resolve_one(reqset, ireq)
+                reqset.cleanup_files()
 
-        if ireq not in self._dependencies_cache:
-            if ireq.editable and (ireq.source_dir and os.path.exists(ireq.source_dir)):
-                # No download_dir for locally available editable requirements.
-                # If a download_dir is passed, pip will  unnecessarely
-                # archive the entire source directory
-                download_dir = None
-            elif ireq.link and not ireq.link.is_artifact:
-                # No download_dir for VCS sources.  This also works around pip
-                # using git-checkout-index, which gets rid of the .git dir.
-                download_dir = None
-            else:
-                download_dir = self._download_dir
-                if not os.path.isdir(download_dir):
-                    os.makedirs(download_dir)
-            if not os.path.isdir(self._wheel_download_dir):
-                os.makedirs(self._wheel_download_dir)
+        if ireq.editable and (ireq.source_dir and os.path.exists(ireq.source_dir)):
             # Collect setup_requires info from local eggs.
             # Do this after we call the preparer on these reqs to make sure their
             # egg info has been created
-            setup_requires = {}
-            dist = None
-            if ireq.editable:
-                try:
-                    from pipenv.utils import chdir
-                    with chdir(ireq.setup_py_dir):
-                        from setuptools.dist import distutils
-                        dist = distutils.core.run_setup(ireq.setup_py)
-                except (ImportError, InstallationError, TypeError, AttributeError):
-                    pass
+            from pipenv.utils import chdir
+            with chdir(ireq.setup_py_dir):
                 try:
-                    dist = ireq.get_dist() if not dist else dist
+                    from setuptools.dist import distutils
+                    dist = distutils.core.run_setup(ireq.setup_py)
                 except InstallationError:
                     ireq.run_egg_info()
-                    dist = ireq.get_dist()
                 except (TypeError, ValueError, AttributeError):
                     pass
-                else:
-                    setup_requires = getattr(dist, "extras_require", None)
-                    if not setup_requires:
-                        setup_requires = {"setup_requires": getattr(dist, "setup_requires", None)}
-            try:
-                # Pip 9 and below
-                reqset = RequirementSet(
-                    self.build_dir,
-                    self.source_dir,
-                    download_dir=download_dir,
-                    wheel_download_dir=self._wheel_download_dir,
-                    session=self.session,
-                    ignore_installed=True,
-                    ignore_compatibility=False,
-                    wheel_cache=self.wheel_cache,
-                )
-                result = reqset._prepare_file(
-                    self.finder,
-                    ireq,
-                    ignore_requires_python=True
-                )
-            except TypeError:
-                # Pip >= 10 (new resolver!)
-                preparer = RequirementPreparer(
-                    build_dir=self.build_dir,
-                    src_dir=self.source_dir,
-                    download_dir=download_dir,
-                    wheel_download_dir=self._wheel_download_dir,
-                    progress_bar='off',
-                    build_isolation=False
-                )
-                reqset = RequirementSet()
-                ireq.is_direct = True
-                reqset.add_requirement(ireq)
-                self.resolver = PipResolver(
-                    preparer=preparer,
-                    finder=self.finder,
-                    session=self.session,
-                    upgrade_strategy="to-satisfy-only",
-                    force_reinstall=True,
-                    ignore_dependencies=False,
-                    ignore_requires_python=True,
-                    ignore_installed=True,
-                    isolated=False,
-                    wheel_cache=self.wheel_cache,
-                    use_user_site=False,
-                    ignore_compatibility=False
-                )
-                self.resolver.resolve(reqset)
-                result = set(reqset.requirements.values())
-
-            # HACK: Sometimes the InstallRequirement doesn't properly get
-            # these values set on it during the resolution process. It's
-            # difficult to pin down what is going wrong. This fixes things.
-            if not getattr(ireq, 'version', None):
-                try:
-                    dist = ireq.get_dist() if not dist else None
-                    ireq.version = ireq.get_dist().version
-                except (ValueError, OSError, TypeError, AttributeError) as e:
-                    pass
-            if not getattr(ireq, 'project_name', None):
-                try:
-                    ireq.project_name = dist.project_name if dist else None
-                except (ValueError, TypeError) as e:
-                    pass
+                if not dist:
+                    try:
+                        dist = ireq.get_dist()
+                    except (ImportError, ValueError, TypeError, AttributeError):
+                        pass
+        if ireq.editable and dist:
+            setup_requires = getattr(dist, "extras_require", None)
+            if not setup_requires:
+                setup_requires = {"setup_requires": getattr(dist, "setup_requires", None)}
             if not getattr(ireq, 'req', None):
                 try:
                     ireq.req = dist.as_requirement() if dist else None
@@ -385,14 +344,14 @@ class PyPIRepository(BaseRepository):
                         if ':' not in value:
                             try:
                                 if not not_python:
-                                    result = result + [InstallRequirement.from_line("{0}{1}".format(value, python_version).replace(':', ';'))]
+                                    results.add(InstallRequirement.from_line("{0}{1}".format(value, python_version).replace(':', ';')))
                             # Anything could go wrong here -- can't be too careful.
                             except Exception:
                                 pass
 
             # this section properly creates 'python_version' markers for cross-python
             # virtualenv creation and for multi-python compatibility.
-            requires_python = reqset.requires_python if hasattr(reqset, 'requires_python') else self.resolver.requires_python
+            requires_python = reqset.requires_python if hasattr(reqset, 'requires_python') else resolver.requires_python
             if requires_python:
                 marker_str = ''
                 # This corrects a logic error from the previous code which said that if
@@ -402,28 +361,68 @@ class PyPIRepository(BaseRepository):
                 if any(requires_python.startswith(op) for op in Specifier._operators.keys()):
                     # We are checking first if we have  leading specifier operator
                     # if not, we can assume we should be doing a == comparison
-                    specifierset = list(SpecifierSet(requires_python))
+                    specifierset = SpecifierSet(requires_python)
                     # for multiple specifiers, the correct way to represent that in
                     # a specifierset is `Requirement('fakepkg; python_version<"3.0,>=2.6"')`
-                    marker_key = Variable('python_version')
-                    markers = []
-                    for spec in specifierset:
-                        operator, val = spec._spec
-                        operator = Op(operator)
-                        val = Value(val)
-                        markers.append(''.join([marker_key.serialize(), operator.serialize(), val.serialize()]))
-                    marker_str = ' and '.join(markers)
+                    from passa.internals.specifiers import cleanup_pyspecs
+                    marker_str = str(Marker(" and ".join(dedup([
+                        "python_version {0[0]} '{0[1]}'".format(spec)
+                        for spec in cleanup_pyspecs(specifierset)
+                    ]))))
                 # The best way to add markers to a requirement is to make a separate requirement
                 # with only markers on it, and then to transfer the object istelf
                 marker_to_add = Requirement('fakepkg; {0}'.format(marker_str)).marker
-                result.remove(ireq)
+                if ireq in results:
+                    results.remove(ireq)
+                print(marker_to_add)
                 ireq.req.marker = marker_to_add
-                result.add(ireq)
 
-            self._dependencies_cache[ireq] = result
-            reqset.cleanup_files()
+        results = set(results) if results else set()
+        return results, ireq
+
+    def get_legacy_dependencies(self, ireq):
+        """
+        Given a pinned or an editable InstallRequirement, returns a set of
+        dependencies (also InstallRequirements, but not necessarily pinned).
+        They indicate the secondary dependencies for the given requirement.
+        """
+        if not (ireq.editable or is_pinned_requirement(ireq)):
+            raise TypeError('Expected pinned or editable InstallRequirement, got {}'.format(ireq))
+
+        if ireq not in self._dependencies_cache:
+            if ireq.editable and (ireq.source_dir and os.path.exists(ireq.source_dir)):
+                # No download_dir for locally available editable requirements.
+                # If a download_dir is passed, pip will  unnecessarely
+                # archive the entire source directory
+                download_dir = None
+
+            elif ireq.link and not ireq.link.is_artifact:
+                # No download_dir for VCS sources.  This also works around pip
+                # using git-checkout-index, which gets rid of the .git dir.
+                download_dir = None
+            else:
+                download_dir = self._download_dir
+                if not os.path.isdir(download_dir):
+                    os.makedirs(download_dir)
+            if not os.path.isdir(self._wheel_download_dir):
+                os.makedirs(self._wheel_download_dir)
 
-        return set(self._dependencies_cache[ireq])
+            wheel_cache = WheelCache(CACHE_DIR, self.pip_options.format_control)
+            prev_tracker = os.environ.get('PIP_REQ_TRACKER')
+            try:
+                results, ireq = self.resolve_reqs(download_dir, ireq, wheel_cache)
+                self._dependencies_cache[ireq] = results
+            finally:
+                if 'PIP_REQ_TRACKER' in os.environ:
+                    if prev_tracker:
+                        os.environ['PIP_REQ_TRACKER'] = prev_tracker
+                    else:
+                        del os.environ['PIP_REQ_TRACKER']
+                try:
+                    self.wheel_cache.cleanup()
+                except AttributeError:
+                    pass
+        return self._dependencies_cache[ireq]
 
     def get_hashes(self, ireq):
         """
diff --git a/pipenv/patched/piptools/resolver.py b/pipenv/patched/piptools/resolver.py
index 807cf518..d5a471d4 100644
--- a/pipenv/patched/piptools/resolver.py
+++ b/pipenv/patched/piptools/resolver.py
@@ -7,8 +7,6 @@ from functools import partial
 from itertools import chain, count
 import os
 
-from first import first
-from pipenv.patched.notpip._vendor.packaging.markers import default_environment
 from ._compat import InstallRequirement
 
 from . import click
@@ -73,7 +71,7 @@ class Resolver(object):
         with self.repository.allow_all_wheels():
             return {ireq: self.repository.get_hashes(ireq) for ireq in ireqs}
 
-    def resolve(self, max_rounds=12):
+    def resolve(self, max_rounds=10):
         """
         Finds concrete package versions for all the given InstallRequirements
         and their recursive dependencies.  The end result is a flat list of
@@ -145,20 +143,18 @@ class Resolver(object):
         """
         for _, ireqs in full_groupby(constraints, key=key_from_ireq):
             ireqs = list(ireqs)
-            editable_ireq = first(ireqs, key=lambda ireq: ireq.editable)
+            editable_ireq = next((ireq for ireq in ireqs if ireq.editable), None)
             if editable_ireq:
                 yield editable_ireq  # ignore all the other specs: the editable one is the one that counts
                 continue
+
             ireqs = iter(ireqs)
             # deepcopy the accumulator so as to not modify the self.our_constraints invariant
             combined_ireq = copy.deepcopy(next(ireqs))
+            combined_ireq.comes_from = None
             for ireq in ireqs:
                 # NOTE we may be losing some info on dropped reqs here
-                try:
-                    combined_ireq.req.specifier &= ireq.req.specifier
-                except TypeError:
-                    if ireq.req.specifier._specs and not combined_ireq.req.specifier._specs:
-                        combined_ireq.req.specifier._specs = ireq.req.specifier._specs
+                combined_ireq.req.specifier &= ireq.req.specifier
                 combined_ireq.constraint &= ireq.constraint
                 if not combined_ireq.markers:
                     combined_ireq.markers = ireq.markers
@@ -166,6 +162,7 @@ class Resolver(object):
                     _markers = combined_ireq.markers._markers
                     if not isinstance(_markers[0], (tuple, list)):
                         combined_ireq.markers._markers = [_markers, 'and', ireq.markers._markers]
+
                 # Return a sorted, de-duped tuple of extras
                 combined_ireq.extras = tuple(sorted(set(tuple(combined_ireq.extras) + tuple(ireq.extras))))
             yield combined_ireq
@@ -286,11 +283,12 @@ class Resolver(object):
 
         # fix our malformed extras
         if ireq.extras:
-            if hasattr(ireq, 'extra'):
+            if getattr(ireq, "extra", None):
                 if ireq.extras:
                     ireq.extras.extend(ireq.extra)
                 else:
                     ireq.extras = ireq.extra
+
         elif not is_pinned_requirement(ireq):
             raise TypeError('Expected pinned or editable requirement, got {}'.format(ireq))
 
@@ -301,14 +299,14 @@ class Resolver(object):
         if ireq not in self.dependency_cache:
             log.debug('  {} not in cache, need to check index'.format(format_requirement(ireq)), fg='yellow')
             dependencies = self.repository.get_dependencies(ireq)
-            self.dependency_cache[ireq] = sorted(format_requirement(_ireq) for _ireq in dependencies)
+            self.dependency_cache[ireq] = sorted(set(format_requirement(ireq) for ireq in dependencies))
 
         # Example: ['Werkzeug>=0.9', 'Jinja2>=2.4']
         dependency_strings = self.dependency_cache[ireq]
         log.debug('  {:25} requires {}'.format(format_requirement(ireq),
                                                ', '.join(sorted(dependency_strings, key=lambda s: s.lower())) or '-'))
         for dependency_string in dependency_strings:
-                yield InstallRequirement.from_line(dependency_string, constraint=ireq.constraint)
+            yield InstallRequirement.from_line(dependency_string, constraint=ireq.constraint)
 
     def reverse_dependencies(self, ireqs):
         non_editable = [ireq for ireq in ireqs if not ireq.editable]
diff --git a/pipenv/patched/piptools/utils.py b/pipenv/patched/piptools/utils.py
index 2f389eec..6225d7e2 100644
--- a/pipenv/patched/piptools/utils.py
+++ b/pipenv/patched/piptools/utils.py
@@ -2,20 +2,19 @@
 from __future__ import (absolute_import, division, print_function,
                         unicode_literals)
 
-import six
 import os
 import sys
+import six
 from itertools import chain, groupby
 from collections import OrderedDict
 from contextlib import contextmanager
 
 from ._compat import InstallRequirement
 
-from first import first
-from pipenv.patched.notpip._vendor.packaging.specifiers import SpecifierSet, InvalidSpecifier
-from pipenv.patched.notpip._vendor.packaging.version import Version, InvalidVersion, parse as parse_version
-from pipenv.patched.notpip._vendor.packaging.markers import Marker, Op, Value, Variable
 from .click import style
+from pip._vendor.packaging.specifiers import SpecifierSet, InvalidSpecifier
+from pip._vendor.packaging.version import Version, InvalidVersion, parse as parse_version
+from pip._vendor.packaging.markers import Marker, Op, Value, Variable
 
 
 UNSAFE_PACKAGES = {'setuptools', 'distribute', 'pip'}
@@ -158,10 +157,6 @@ def _requirement_to_str_lowercase_name(requirement):
 
 
 def format_requirement(ireq, marker=None):
-    """
-    Generic formatter for pretty printing InstallRequirements to the terminal
-    in a less verbose way than using its `__str__` method.
-    """
     if ireq.editable:
         line = '-e {}'.format(ireq.link)
     else:
@@ -207,7 +202,7 @@ def is_pinned_requirement(ireq):
     if len(ireq.specifier._specs) != 1:
         return False
 
-    op, version = first(ireq.specifier._specs)._spec
+    op, version = next(iter(ireq.specifier._specs))._spec
     return (op == '==' or op == '===') and not version.endswith('.*')
 
 
@@ -219,7 +214,7 @@ def as_tuple(ireq):
         raise TypeError('Expected a pinned InstallRequirement, got {}'.format(ireq))
 
     name = key_from_req(ireq.req)
-    version = first(ireq.specifier._specs)._spec[1]
+    version = next(iter(ireq.specifier._specs))._spec[1]
     extras = tuple(sorted(ireq.extras))
     return name, version, extras
 
diff --git a/pipenv/vendor/passa/__init__.py b/pipenv/vendor/passa/__init__.py
new file mode 100644
index 00000000..6f92267d
--- /dev/null
+++ b/pipenv/vendor/passa/__init__.py
@@ -0,0 +1,7 @@
+# -*- coding=utf-8 -*-
+
+__all__ = [
+    '__version__'
+]
+
+__version__ = '0.3.0'
diff --git a/pipenv/vendor/passa/__main__.py b/pipenv/vendor/passa/__main__.py
new file mode 100644
index 00000000..76c2e6a6
--- /dev/null
+++ b/pipenv/vendor/passa/__main__.py
@@ -0,0 +1,6 @@
+# -*- coding=utf-8 -*-
+
+from .cli import main
+
+if __name__ == '__main__':
+    main()
diff --git a/pipenv/vendor/passa/_pip.py b/pipenv/vendor/passa/_pip.py
new file mode 100644
index 00000000..5cf1cea8
--- /dev/null
+++ b/pipenv/vendor/passa/_pip.py
@@ -0,0 +1,315 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import contextlib
+import distutils.log
+import os
+
+import setuptools.dist
+
+import distlib.scripts
+import distlib.wheel
+import pip_shims
+import six
+import vistir
+
+from ._pip_shims import VCS_SUPPORT, build_wheel as _build_wheel, unpack_url
+from .caches import CACHE_DIR
+from .utils import filter_sources
+
+
+@vistir.path.ensure_mkdir_p(mode=0o775)
+def _get_src_dir():
+    src = os.environ.get("PIP_SRC")
+    if src:
+        return src
+    virtual_env = os.environ.get("VIRTUAL_ENV")
+    if virtual_env:
+        return os.path.join(virtual_env, "src")
+    temp_src = vistir.path.create_tracked_tempdir(prefix='passa-src')
+    return temp_src
+
+
+def _prepare_wheel_building_kwargs(ireq):
+    download_dir = os.path.join(CACHE_DIR, "pkgs")
+    vistir.mkdir_p(download_dir)
+
+    wheel_download_dir = os.path.join(CACHE_DIR, "wheels")
+    vistir.mkdir_p(wheel_download_dir)
+
+    if ireq.source_dir is None:
+        src_dir = _get_src_dir()
+    else:
+        src_dir = ireq.source_dir
+
+    # This logic matches pip's behavior, although I don't fully understand the
+    # intention. I guess the idea is to build editables in-place, otherwise out
+    # of the source tree?
+    if ireq.editable:
+        build_dir = src_dir
+    else:
+        build_dir = vistir.path.create_tracked_tempdir(prefix="passa-build")
+
+    return {
+        "build_dir": build_dir,
+        "src_dir": src_dir,
+        "download_dir": download_dir,
+        "wheel_download_dir": wheel_download_dir,
+    }
+
+
+def _get_pip_index_urls(sources):
+    index_urls = []
+    trusted_hosts = []
+    for source in sources:
+        url = source.get("url")
+        if not url:
+            continue
+        index_urls.append(url)
+        if source.get("verify_ssl", True):
+            continue
+        host = six.moves.urllib.parse.urlparse(source["url"]).hostname
+        trusted_hosts.append(host)
+    return index_urls, trusted_hosts
+
+
+class _PipCommand(pip_shims.Command):
+    name = "PipCommand"
+
+
+def _get_pip_session(trusted_hosts):
+    cmd = _PipCommand()
+    options, _ = cmd.parser.parse_args([])
+    options.cache_dir = CACHE_DIR
+    options.trusted_hosts = trusted_hosts
+    session = cmd._build_session(options)
+    return session
+
+
+def _get_finder(sources):
+    index_urls, trusted_hosts = _get_pip_index_urls(sources)
+    session = _get_pip_session(trusted_hosts)
+    finder = pip_shims.PackageFinder(
+        find_links=[],
+        index_urls=index_urls,
+        trusted_hosts=trusted_hosts,
+        allow_all_prereleases=True,
+        session=session,
+    )
+    return finder
+
+
+def _get_wheel_cache():
+    format_control = pip_shims.FormatControl(set(), set())
+    wheel_cache = pip_shims.WheelCache(CACHE_DIR, format_control)
+    return wheel_cache
+
+
+def _convert_hashes(values):
+    """Convert Pipfile.lock hash lines into InstallRequirement option format.
+
+    The option format uses a str-list mapping. Keys are hash algorithms, and
+    the list contains all values of that algorithm.
+    """
+    hashes = {}
+    if not values:
+        return hashes
+    for value in values:
+        try:
+            name, value = value.split(":", 1)
+        except ValueError:
+            name = "sha256"
+        if name not in hashes:
+            hashes[name] = []
+        hashes[name].append(value)
+    return hashes
+
+
+def build_wheel(ireq, sources, hashes=None):
+    """Build a wheel file for the InstallRequirement object.
+
+    An artifact is downloaded (or read from cache). If the artifact is not a
+    wheel, build one out of it. The dynamically built wheel is ephemeral; do
+    not depend on its existence after the returned wheel goes out of scope.
+
+    If `hashes` is truthy, it is assumed to be a list of hashes (as formatted
+    in Pipfile.lock) to be checked against the download.
+
+    Returns a `distlib.wheel.Wheel` instance. Raises a `RuntimeError` if the
+    wheel cannot be built.
+    """
+    kwargs = _prepare_wheel_building_kwargs(ireq)
+    finder = _get_finder(sources)
+
+    # Not for upgrade, hash not required. Hashes are not required here even
+    # when we provide them, because pip skips local wheel cache if we set it
+    # to True. Hashes are checked later if we need to download the file.
+    ireq.populate_link(finder, False, False)
+
+    # Ensure ireq.source_dir is set.
+    # This is intentionally set to build_dir, not src_dir. Comments from pip:
+    #   [...] if filesystem packages are not marked editable in a req, a non
+    #   deterministic error occurs when the script attempts to unpack the
+    #   build directory.
+    # Also see comments in `_prepare_wheel_building_kwargs()` -- If the ireq
+    # is editable, build_dir is actually src_dir, making the build in-place.
+    ireq.ensure_has_source_dir(kwargs["build_dir"])
+
+    # Ensure the remote artifact is downloaded locally. For wheels, it is
+    # enough to just download because we'll use them directly. For an sdist,
+    # we need to unpack so we can build it.
+    if not pip_shims.is_file_url(ireq.link):
+        if ireq.is_wheel:
+            only_download = True
+            download_dir = kwargs["wheel_download_dir"]
+        else:
+            only_download = False
+            download_dir = kwargs["download_dir"]
+        ireq.options["hashes"] = _convert_hashes(hashes)
+        unpack_url(
+            ireq.link, ireq.source_dir, download_dir,
+            only_download=only_download, session=finder.session,
+            hashes=ireq.hashes(False), progress_bar=False,
+        )
+
+    if ireq.is_wheel:
+        # If this is a wheel, use the downloaded thing.
+        output_dir = kwargs["wheel_download_dir"]
+        wheel_path = os.path.join(output_dir, ireq.link.filename)
+    else:
+        # Othereise we need to build an ephemeral wheel.
+        wheel_path = _build_wheel(
+            ireq, vistir.path.create_tracked_tempdir(prefix="ephem"),
+            finder, _get_wheel_cache(), kwargs,
+        )
+        if wheel_path is None or not os.path.exists(wheel_path):
+            raise RuntimeError("failed to build wheel from {}".format(ireq))
+    return distlib.wheel.Wheel(wheel_path)
+
+
+def _obtrain_ref(vcs_obj, src_dir, name, rev=None):
+    target_dir = os.path.join(src_dir, name)
+    target_rev = vcs_obj.make_rev_options(rev)
+    if not os.path.exists(target_dir):
+        vcs_obj.obtain(target_dir)
+    if (not vcs_obj.is_commit_id_equal(target_dir, rev) and
+            not vcs_obj.is_commit_id_equal(target_dir, target_rev)):
+        vcs_obj.update(target_dir, target_rev)
+    return vcs_obj.get_revision(target_dir)
+
+
+def get_vcs_ref(requirement):
+    backend = VCS_SUPPORT._registry.get(requirement.vcs)
+    vcs = backend(url=requirement.req.vcs_uri)
+    src = _get_src_dir()
+    name = requirement.normalized_name
+    ref = _obtrain_ref(vcs, src, name, rev=requirement.req.ref)
+    return ref
+
+
+def find_installation_candidates(ireq, sources):
+    finder = _get_finder(sources)
+    return finder.find_all_candidates(ireq.name)
+
+
+class RequirementUninstallation(object):
+    """A context manager to remove a package for the inner block.
+
+    This uses `UninstallPathSet` to control the workflow. If the inner block
+    exits correctly, the uninstallation is committed, otherwise rolled back.
+    """
+    def __init__(self, ireq, auto_confirm, verbose):
+        self.ireq = ireq
+        self.pathset = None
+        self.auto_confirm = auto_confirm
+        self.verbose = verbose
+
+    def __enter__(self):
+        self.pathset = self.ireq.uninstall(
+            auto_confirm=self.auto_confirm,
+            verbose=self.verbose,
+        )
+        return self.pathset
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        if self.pathset is None:
+            return
+        if exc_type is None:
+            self.pathset.commit()
+        else:
+            self.pathset.rollback()
+
+
+def uninstall_requirement(ireq, **kwargs):
+    return RequirementUninstallation(ireq, **kwargs)
+
+
+@contextlib.contextmanager
+def _suppress_distutils_logs():
+    """Hack to hide noise generated by `setup.py develop`.
+
+    There isn't a good way to suppress them now, so let's monky-patch.
+    See https://bugs.python.org/issue25392.
+    """
+    f = distutils.log.Log._log
+
+    def _log(log, level, msg, args):
+        if level >= distutils.log.ERROR:
+            f(log, level, msg, args)
+
+    distutils.log.Log._log = _log
+    yield
+    distutils.log.Log._log = f
+
+
+class NoopInstaller(object):
+    """An installer.
+
+    This class is not designed to be instantiated by itself, but used as a
+    common interface for subclassing.
+
+    An installer has two methods, `prepare()` and `install()`. Neither takes
+    arguments, and should be called in that order to prepare an installation
+    operation, and to actually install things.
+    """
+    def prepare(self):
+        pass
+
+    def install(self):
+        pass
+
+
+class EditableInstaller(NoopInstaller):
+    """Installer to handle editable.
+    """
+    def __init__(self, requirement):
+        ireq = requirement.as_ireq()
+        self.working_directory = ireq.setup_py_dir
+        self.setup_py = ireq.setup_py
+
+    def install(self):
+        with vistir.cd(self.working_directory), _suppress_distutils_logs():
+            # Access from Setuptools to ensure things are patched correctly.
+            setuptools.dist.distutils.core.run_setup(
+                self.setup_py, ["develop", "--no-deps"],
+            )
+
+
+class WheelInstaller(NoopInstaller):
+    """Installer by building a wheel.
+
+    The wheel is built during `prepare()`, and installed in `install()`.
+    """
+    def __init__(self, requirement, sources, paths):
+        self.ireq = requirement.as_ireq()
+        self.sources = filter_sources(requirement, sources)
+        self.hashes = requirement.hashes or None
+        self.paths = paths
+        self.wheel = None
+
+    def prepare(self):
+        self.wheel = build_wheel(self.ireq, self.sources, self.hashes)
+
+    def install(self):
+        self.wheel.install(self.paths, distlib.scripts.ScriptMaker(None, None))
diff --git a/pipenv/vendor/passa/_pip_shims.py b/pipenv/vendor/passa/_pip_shims.py
new file mode 100644
index 00000000..b2c7b6ea
--- /dev/null
+++ b/pipenv/vendor/passa/_pip_shims.py
@@ -0,0 +1,61 @@
+# -*- coding=utf-8 -*-
+
+"""Shims to make the pip interface more consistent accross versions.
+
+There are currently two members:
+
+* VCS_SUPPORT is an instance of VcsSupport.
+* build_wheel abstracts the process to build a wheel out of a bunch parameters.
+* unpack_url wraps the actual function in pip to accept modern parameters.
+"""
+
+from __future__ import absolute_import, unicode_literals
+
+import pip_shims
+
+
+def _build_wheel_pre10(ireq, output_dir, finder, wheel_cache, kwargs):
+    kwargs.update({"wheel_cache": wheel_cache, "session": finder.session})
+    reqset = pip_shims.RequirementSet(**kwargs)
+    builder = pip_shims.WheelBuilder(reqset, finder)
+    return builder._build_one(ireq, output_dir)
+
+
+def _build_wheel_modern(ireq, output_dir, finder, wheel_cache, kwargs):
+    """Build a wheel.
+
+    * ireq: The InstallRequirement object to build
+    * output_dir: The directory to build the wheel in.
+    * finder: pip's internal Finder object to find the source out of ireq.
+    * kwargs: Various keyword arguments from `_prepare_wheel_building_kwargs`.
+    """
+    kwargs.update({"progress_bar": "off", "build_isolation": False})
+    with pip_shims.RequirementTracker() as req_tracker:
+        if req_tracker:
+            kwargs["req_tracker"] = req_tracker
+        preparer = pip_shims.RequirementPreparer(**kwargs)
+        builder = pip_shims.WheelBuilder(finder, preparer, wheel_cache)
+        return builder._build_one(ireq, output_dir)
+
+
+def _unpack_url_pre10(*args, **kwargs):
+    """Shim for unpack_url in various pip versions.
+
+    pip before 10.0 does not accept `progress_bar` here. Simply drop it.
+    """
+    kwargs.pop("progress_bar", None)
+    return pip_shims.unpack_url(*args, **kwargs)
+
+
+PIP_VERSION = pip_shims.utils._parse(pip_shims.pip_version)
+VERSION_10 = pip_shims.utils._parse("10")
+
+
+VCS_SUPPORT = pip_shims.VcsSupport()
+
+build_wheel = _build_wheel_modern
+unpack_url = pip_shims.unpack_url
+
+if PIP_VERSION < VERSION_10:
+    build_wheel = _build_wheel_pre10
+    unpack_url = _unpack_url_pre10
diff --git a/pipenv/vendor/passa/caches.py b/pipenv/vendor/passa/caches.py
new file mode 100644
index 00000000..6d3131fa
--- /dev/null
+++ b/pipenv/vendor/passa/caches.py
@@ -0,0 +1,214 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import copy
+import hashlib
+import json
+import os
+import sys
+
+import appdirs
+import pip_shims
+import requests
+import vistir
+
+from ._pip_shims import VCS_SUPPORT
+from .utils import get_pinned_version
+
+
+CACHE_DIR = os.environ.get("PASSA_CACHE_DIR", appdirs.user_cache_dir("passa"))
+
+
+class HashCache(pip_shims.SafeFileCache):
+    """Caches hashes of PyPI artifacts so we do not need to re-download them.
+
+    Hashes are only cached when the URL appears to contain a hash in it and the
+    cache key includes the hash value returned from the server). This ought to
+    avoid ssues where the location on the server changes.
+    """
+    def __init__(self, *args, **kwargs):
+        session = kwargs.pop('session', requests.session())
+        self.session = session
+        kwargs.setdefault('directory', os.path.join(CACHE_DIR, 'hash-cache'))
+        super(HashCache, self).__init__(*args, **kwargs)
+
+    def get_hash(self, location):
+        # If there is no location hash (i.e., md5, sha256, etc.), we don't want
+        # to store it.
+        hash_value = None
+        orig_scheme = location.scheme
+        new_location = copy.deepcopy(location)
+        if orig_scheme in VCS_SUPPORT.all_schemes:
+            new_location.url = new_location.url.split("+", 1)[-1]
+        can_hash = new_location.hash
+        if can_hash:
+            # hash url WITH fragment
+            hash_value = self.get(new_location.url)
+        if not hash_value:
+            hash_value = self._get_file_hash(new_location)
+            hash_value = hash_value.encode('utf8')
+        if can_hash:
+            self.set(new_location.url, hash_value)
+        return hash_value.decode('utf8')
+
+    def _get_file_hash(self, location):
+        h = hashlib.new(pip_shims.FAVORITE_HASH)
+        with vistir.open_file(location, self.session) as fp:
+            for chunk in iter(lambda: fp.read(8096), b""):
+                h.update(chunk)
+        return ":".join([h.name, h.hexdigest()])
+
+
+# pip-tools's dependency cache implementation.
+class CorruptCacheError(Exception):
+    def __init__(self, path):
+        self.path = path
+
+    def __str__(self):
+        lines = [
+            'The dependency cache seems to have been corrupted.',
+            'Inspect, or delete, the following file:',
+            '  {}'.format(self.path),
+        ]
+        return os.linesep.join(lines)
+
+
+def _key_from_req(req):
+    """Get an all-lowercase version of the requirement's name."""
+    if hasattr(req, 'key'):
+        # from pkg_resources, such as installed dists for pip-sync
+        key = req.key
+    else:
+        # from packaging, such as install requirements from requirements.txt
+        key = req.name
+
+    key = key.replace('_', '-').lower()
+    return key
+
+
+def _read_cache_file(cache_file_path):
+    with open(cache_file_path, 'r') as cache_file:
+        try:
+            doc = json.load(cache_file)
+        except ValueError:
+            raise CorruptCacheError(cache_file_path)
+
+        # Check version and load the contents
+        assert doc['__format__'] == 1, 'Unknown cache file format'
+        return doc['dependencies']
+
+
+class _JSONCache(object):
+    """A persistent cache backed by a JSON file.
+
+    The cache file is written to the appropriate user cache dir for the
+    current platform, i.e.
+
+        ~/.cache/pip-tools/depcache-pyX.Y.json
+
+    Where X.Y indicates the Python version.
+    """
+    filename_format = None
+
+    def __init__(self, cache_dir=CACHE_DIR):
+        vistir.mkdir_p(cache_dir)
+        python_version = ".".join(str(digit) for digit in sys.version_info[:2])
+        cache_filename = self.filename_format.format(
+            python_version=python_version,
+        )
+        self._cache_file = os.path.join(cache_dir, cache_filename)
+        self._cache = None
+
+    @property
+    def cache(self):
+        """The dictionary that is the actual in-memory cache.
+
+        This property lazily loads the cache from disk.
+        """
+        if self._cache is None:
+            self.read_cache()
+        return self._cache
+
+    def as_cache_key(self, ireq):
+        """Given a requirement, return its cache key.
+
+        This behavior is a little weird in order to allow backwards
+        compatibility with cache files. For a requirement without extras, this
+        will return, for example::
+
+            ("ipython", "2.1.0")
+
+        For a requirement with extras, the extras will be comma-separated and
+        appended to the version, inside brackets, like so::
+
+            ("ipython", "2.1.0[nbconvert,notebook]")
+        """
+        extras = tuple(sorted(ireq.extras))
+        if not extras:
+            extras_string = ""
+        else:
+            extras_string = "[{}]".format(",".join(extras))
+        name = _key_from_req(ireq.req)
+        version = get_pinned_version(ireq)
+        return name, "{}{}".format(version, extras_string)
+
+    def read_cache(self):
+        """Reads the cached contents into memory.
+        """
+        if os.path.exists(self._cache_file):
+            self._cache = _read_cache_file(self._cache_file)
+        else:
+            self._cache = {}
+
+    def write_cache(self):
+        """Writes the cache to disk as JSON.
+        """
+        doc = {
+            '__format__': 1,
+            'dependencies': self._cache,
+        }
+        with open(self._cache_file, 'w') as f:
+            json.dump(doc, f, sort_keys=True)
+
+    def clear(self):
+        self._cache = {}
+        self.write_cache()
+
+    def __contains__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return pkgversion_and_extras in self.cache.get(pkgname, {})
+
+    def __getitem__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return self.cache[pkgname][pkgversion_and_extras]
+
+    def __setitem__(self, ireq, values):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        self.cache.setdefault(pkgname, {})
+        self.cache[pkgname][pkgversion_and_extras] = values
+        self.write_cache()
+
+    def __delitem__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        try:
+            del self.cache[pkgname][pkgversion_and_extras]
+        except KeyError:
+            return
+        self.write_cache()
+
+    def get(self, ireq, default=None):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return self.cache.get(pkgname, {}).get(pkgversion_and_extras, default)
+
+
+class DependencyCache(_JSONCache):
+    """Cache the dependency of cancidates.
+    """
+    filename_format = "depcache-py{python_version}.json"
+
+
+class RequiresPythonCache(_JSONCache):
+    """Cache a candidate's Requires-Python information.
+    """
+    filename_format = "pyreqcache-py{python_version}.json"
diff --git a/pipenv/vendor/passa/candidates.py b/pipenv/vendor/passa/candidates.py
new file mode 100644
index 00000000..d5390d65
--- /dev/null
+++ b/pipenv/vendor/passa/candidates.py
@@ -0,0 +1,81 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import os
+import sys
+
+import packaging.specifiers
+import packaging.version
+import requirementslib
+
+from ._pip import find_installation_candidates, get_vcs_ref
+
+
+def _filter_matching_python_requirement(candidates, python_version):
+    for c in candidates:
+        try:
+            requires_python = c.requires_python
+        except AttributeError:
+            requires_python = c.location.requires_python
+        if python_version and requires_python:
+            # Old specifications had people setting this to single digits
+            # which is effectively the same as '>=digit,<digit+1'
+            if requires_python.isdigit():
+                requires_python = '>={0},<{1}'.format(
+                    requires_python, int(requires_python) + 1,
+                )
+            try:
+                specset = packaging.specifiers.SpecifierSet(requires_python)
+            except packaging.specifiers.InvalidSpecifier:
+                continue
+            if not specset.contains(python_version):
+                continue
+        yield c
+
+
+def _copy_requirement(requirement):
+    return requirement.copy()
+
+
+def _requirement_from_metadata(name, version, extras, index):
+    # Markers are intentionally dropped here. They will be added to candidates
+    # after resolution, so we can perform marker aggregation.
+    r = requirementslib.Requirement.from_metadata(name, version, extras, None)
+    r.index = index
+    return r
+
+
+def find_candidates(requirement, sources, allow_pre):
+    # A non-named requirement has exactly one candidate that is itself. For
+    # VCS, we also lock the requirement to an exact ref.
+    if not requirement.is_named:
+        candidate = _copy_requirement(requirement)
+        if candidate.is_vcs:
+            candidate.req.ref = get_vcs_ref(candidate)
+        return [candidate]
+
+    ireq = requirement.as_ireq()
+    icans = find_installation_candidates(ireq, sources)
+
+    python_version = os.environ.get(
+        "PASSA_PYTHON_VERSION",
+        "{0[0]}.{0[1]}".format(sys.version_info),
+    )
+    if python_version != ":all:":
+        matching_icans = list(_filter_matching_python_requirement(
+            icans, packaging.version.parse(python_version),
+        ))
+        icans = matching_icans or icans
+
+    versions = ireq.specifier.filter((c.version for c in icans), allow_pre)
+    if not allow_pre and not versions:
+        versions = ireq.specifier.filter((c.version for c in icans), True)
+
+    name = requirement.normalized_name
+    extras = requirement.extras
+    index = requirement.index
+    return [
+        _requirement_from_metadata(name, version, extras, index)
+        for version in sorted(versions)
+    ]
diff --git a/pipenv/vendor/passa/cli.py b/pipenv/vendor/passa/cli.py
new file mode 100644
index 00000000..c750b2f6
--- /dev/null
+++ b/pipenv/vendor/passa/cli.py
@@ -0,0 +1,115 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import argparse
+import collections
+import functools
+import io
+import os
+
+import plette
+import requirementslib
+import six
+
+from . import operations
+
+
+_DEFAULT_NEWLINES = "\n"
+
+
+def _preferred_newlines(f):
+    if isinstance(f.newlines, six.text_type):
+        return f.newlines
+    return _DEFAULT_NEWLINES
+
+
+FileModel = collections.namedtuple("FileModel", "model location newline")
+Project = collections.namedtuple("Project", "root pipfile lockfile")
+
+
+def _build_project(root):
+    root = os.path.abspath(root)
+    pipfile_location = os.path.join(root, "Pipfile")
+    with io.open(pipfile_location, encoding="utf-8") as f:
+        pipfile = plette.Pipfile.load(f)
+        pipfile_le = _preferred_newlines(f)
+
+    lockfile_location = os.path.join(root, "Pipfile.lock")
+    if os.path.exists(lockfile_location):
+        with io.open(lockfile_location, encoding="utf-8") as f:
+            lockfile = plette.Lockfile.load(f)
+            lockfile_le = _preferred_newlines(f)
+    else:
+        lockfile = None
+        lockfile_le = _DEFAULT_NEWLINES
+
+    return Project(
+        root=root,
+        pipfile=FileModel(pipfile, pipfile_location, pipfile_le),
+        lockfile=FileModel(lockfile, lockfile_location, lockfile_le),
+    )
+
+
+def locking_subparser(name):
+
+    def decorator(f):
+
+        @functools.wraps(f)
+        def wrapped(subs):
+            parser = subs.add_parser(name)
+            parser.set_defaults(_cmdkey=name)
+            parser.add_argument(
+                "project",
+                type=_build_project,
+            )
+            parser.add_argument(
+                "-o", "--output",
+                choices=["write", "print", "none"],
+                default="print",
+                help="How to output the lockfile",
+            )
+            f(parser)
+
+        return wrapped
+
+    return decorator
+
+
+@locking_subparser("add")
+def add_parser(parser):
+    parser.add_argument(
+        "requirement",
+        nargs="+", type=requirementslib.Requirement.from_line,
+        help="Requirement(s) to add",
+    )
+
+
+@locking_subparser("lock")
+def lock_parser(parser):
+    parser.add_argument(
+        "--force",
+        action="store_true", default=False,
+        help="Always re-generate lock file",
+    )
+
+
+def get_parser():
+    parser = argparse.ArgumentParser(prog="passa")
+    subs = parser.add_subparsers()
+    lock_parser(subs)
+    return parser
+
+
+def parse_arguments(argv):
+    parser = get_parser()
+    return parser.parse_args(argv)
+
+
+def main(argv=None):
+    options = parse_arguments(argv)
+    operations.main(options)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/pipenv/vendor/passa/cli/__init__.py b/pipenv/vendor/passa/cli/__init__.py
new file mode 100644
index 00000000..cb503e81
--- /dev/null
+++ b/pipenv/vendor/passa/cli/__init__.py
@@ -0,0 +1,49 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import argparse
+import importlib
+import pkgutil
+import sys
+
+from passa import __version__
+
+
+CURRENT_MODULE_PATH = sys.modules[__name__].__path__
+
+
+def main(argv=None):
+    root_parser = argparse.ArgumentParser(
+        prog="passa",
+        description="Pipfile project management tool.",
+    )
+    root_parser.add_argument(
+        "--version",
+        action="version",
+        version="%(prog)s, version {}".format(__version__),
+        help="show the version and exit",
+    )
+
+    subparsers = root_parser.add_subparsers()
+    for _, name, _ in pkgutil.iter_modules(CURRENT_MODULE_PATH, "."):
+        module = importlib.import_module(name, __name__)
+        try:
+            klass = module.Command
+        except AttributeError:
+            continue
+        parser = subparsers.add_parser(klass.name, help=klass.description)
+        command = klass(parser)
+        parser.set_defaults(func=command.main)
+
+    options = root_parser.parse_args(argv)
+
+    try:
+        f = options.func
+    except AttributeError:
+        root_parser.print_help()
+        result = -1
+    else:
+        result = f(options)
+    if result is not None:
+        sys.exit(result)
diff --git a/pipenv/vendor/passa/cli/_base.py b/pipenv/vendor/passa/cli/_base.py
new file mode 100644
index 00000000..6000afee
--- /dev/null
+++ b/pipenv/vendor/passa/cli/_base.py
@@ -0,0 +1,54 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import argparse
+import os
+import sys
+
+
+def build_project(root):
+    # This is imported lazily to reduce import overhead. Not evey command
+    # needs the project instance.
+    from passa.projects import Project
+    return Project(os.path.abspath(root))
+
+
+class BaseCommand(object):
+    """A CLI command.
+    """
+    name = None
+    description = None
+    parsed_main = None
+
+    def __init__(self, parser):
+        self.parser = parser
+        self.add_arguments()
+
+    @classmethod
+    def run_current_module(cls):
+        parser = argparse.ArgumentParser(
+            prog="passa {}".format(cls.name),
+            description=cls.description,
+        )
+        cls(parser)()
+
+    def __call__(self, argv=None):
+        options = self.parser.parse_args(argv)
+        result = self.main(options)
+        if result is not None:
+            sys.exit(result)
+
+    def add_arguments(self):
+        self.parser.add_argument(
+            "--project",
+            metavar="project",
+            default=os.getcwd(),
+            type=build_project,
+            help="path to project root (directory containing Pipfile)",
+        )
+
+    def main(self, options):
+        # This __dict__ access is needed for Python 2 to prevent Python from
+        # wrapping parsed_main into an unbounded method.
+        return type(self).__dict__["parsed_main"](options)
diff --git a/pipenv/vendor/passa/cli/add.py b/pipenv/vendor/passa/cli/add.py
new file mode 100644
index 00000000..1ea0f09a
--- /dev/null
+++ b/pipenv/vendor/passa/cli/add.py
@@ -0,0 +1,99 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import itertools
+import sys
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.lockers import PinReuseLocker
+    from passa.operations.lock import lock
+
+    lines = list(itertools.chain(
+        options.requirement_lines,
+        ("-e {}".format(e) for e in options.editable_lines),
+    ))
+
+    project = options.project
+    for line in lines:
+        try:
+            project.add_line_to_pipfile(line, develop=options.dev)
+        except (TypeError, ValueError) as e:
+            print("Cannot add {line!r} to Pipfile: {error}".format(
+                line=line, error=str(e),
+            ), file=sys.stderr)
+            return 2
+
+    prev_lockfile = project.lockfile
+
+    locker = PinReuseLocker(project)
+    success = lock(locker)
+    if not success:
+        return 1
+
+    project._p.write()
+    project._l.write()
+    print("Written to project at", project.root)
+
+    if not options.sync:
+        return
+
+    from passa.operations.sync import sync
+    from passa.synchronizers import Synchronizer
+
+    lockfile_diff = project.difference_lockfile(prev_lockfile)
+    default = bool(any(lockfile_diff.default))
+    develop = bool(any(lockfile_diff.develop))
+
+    syncer = Synchronizer(
+        project, default=default, develop=develop,
+        clean_unneeded=False,
+    )
+    success = sync(syncer)
+    if not success:
+        return 1
+
+    print("Synchronized project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "add"
+    description = "Add packages to project."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "requirement_lines", metavar="requirement",
+            nargs="*",
+            help="requirement to add (can be used multiple times)",
+        )
+        self.parser.add_argument(
+            "-e", "--editable",
+            metavar="requirement", dest="editable_lines",
+            action="append", default=[],
+            help="editable requirement to add (can be used multiple times)",
+        )
+        self.parser.add_argument(
+            "--dev",
+            action="store_true",
+            help="add packages to [dev-packages]",
+        )
+        self.parser.add_argument(
+            "--no-sync", dest="sync",
+            action="store_false", default=True,
+            help="do not synchronize the environment",
+        )
+
+    def main(self, options):
+        if not options.editable_lines and not options.requirement_lines:
+            self.parser.error("Must supply either a requirement or --editable")
+        return super(Command, self).main(options)
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/clean.py b/pipenv/vendor/passa/cli/clean.py
new file mode 100644
index 00000000..90dbe733
--- /dev/null
+++ b/pipenv/vendor/passa/cli/clean.py
@@ -0,0 +1,38 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.operations.sync import clean
+    from passa.synchronizers import Cleaner
+
+    project = options.project
+    cleaner = Cleaner(project, default=True, develop=options.dev)
+
+    success = clean(cleaner)
+    if not success:
+        return 1
+
+    print("Cleaned project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "clean"
+    description = "Uninstall unlisted packages from the current environment."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "--no-dev", dest="dev",
+            action="store_false", default=True,
+            help="uninstall develop packages, only keep default ones",
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/freeze.py b/pipenv/vendor/passa/cli/freeze.py
new file mode 100644
index 00000000..6ca57170
--- /dev/null
+++ b/pipenv/vendor/passa/cli/freeze.py
@@ -0,0 +1,138 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import contextlib
+import io
+import itertools
+import sys
+
+import six
+import vistir.misc
+
+from ._base import BaseCommand
+
+
+def _source_as_lines(source, extra):
+    url = source["url"]
+    if extra:
+        lines = ["--extra-index-url {}".format(url)]
+    else:
+        lines = ["--index-url {}".format(url)]
+    if not source.get("verify_ssl", True):
+        lines = ["--trusted-host {}".format(url)]
+    return lines
+
+
+def _requirement_as_line(requirement, sources, include_hashes):
+    if requirement.index:
+        sources = sources
+    else:
+        sources = None
+    line = requirement.as_line(sources=sources, include_hashes=include_hashes)
+    if not isinstance(line, six.text_type):
+        line = line.decode("utf-8")
+    return line
+
+
+@contextlib.contextmanager
+def open_for_output(filename):
+    if filename is None:
+        yield sys.stdout
+        return
+    with io.open(filename, "w", encoding="utf-8", newline="\n") as f:
+        yield f
+
+
+def main(options):
+    from requirementslib import Requirement
+
+    lockfile = options.project.lockfile
+    if not lockfile:
+        print("Pipfile.lock is required to export.", file=sys.stderr)
+        return 1
+
+    section_names = []
+    if options.default:
+        section_names.append("default")
+    if options.dev:
+        section_names.append("develop")
+    requirements = [
+        Requirement.from_pipfile(key, entry._data)
+        for key, entry in itertools.chain.from_iterable(
+            lockfile.get(name, {}).items()
+            for name in section_names
+        )
+    ]
+
+    include_hashes = options.include_hashes
+    if include_hashes is None:
+        include_hashes = all(r.is_named for r in requirements)
+
+    sources = lockfile.meta.sources._data
+
+    source_lines = list(vistir.misc.dedup(itertools.chain(
+        itertools.chain.from_iterable(
+            _source_as_lines(source, False)
+            for source in sources[:1]
+        ),
+        itertools.chain.from_iterable(
+            _source_as_lines(source, True)
+            for source in sources[1:]
+        ),
+    )))
+
+    requirement_lines = sorted(vistir.misc.dedup(
+        _requirement_as_line(requirement, sources, include_hashes)
+        for requirement in requirements
+    ))
+
+    with open_for_output(options.target) as f:
+        for line in source_lines:
+            f.write(line)
+            f.write("\n")
+        f.write("\n")
+        for line in requirement_lines:
+            f.write(line)
+            f.write("\n\n")
+
+
+class Command(BaseCommand):
+
+    name = "freeze"
+    description = "Export project depenencies to requirements.txt."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "--target",
+            default=None,
+            help="file to export into (default is to print to stdout)",
+        )
+        self.parser.add_argument(
+            "--dev",
+            action="store_true", default=False,
+            help="include development packages in requirements.txt",
+        )
+        self.parser.add_argument(
+            "--no-default", dest="default",
+            action="store_false", default=True,
+            help="do not include default packages in requirements.txt",
+        )
+        include_hashes_group = self.parser.add_mutually_exclusive_group()
+        include_hashes_group.add_argument(
+            "--include-hashes", dest="include_hashes",
+            action="store_true",
+            help="output hashes in requirements.txt (default is to guess)",
+        )
+        include_hashes_group.add_argument(
+            "--no-include-hashes", dest="include_hashes",
+            action="store_false",
+            help=("do not output hashes in requirements.txt "
+                  "(default is to guess)"),
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/install.py b/pipenv/vendor/passa/cli/install.py
new file mode 100644
index 00000000..47667e45
--- /dev/null
+++ b/pipenv/vendor/passa/cli/install.py
@@ -0,0 +1,63 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.lockers import BasicLocker
+    from passa.operations.lock import lock
+
+    project = options.project
+
+    if not options.check or not project.is_synced():
+        locker = BasicLocker(project)
+        success = lock(locker)
+        if not success:
+            return 1
+        project._l.write()
+        print("Written to project at", project.root)
+
+    from passa.operations.sync import sync
+    from passa.synchronizers import Synchronizer
+
+    syncer = Synchronizer(
+        project, default=True, develop=options.dev,
+        clean_unneeded=options.clean,
+    )
+
+    success = sync(syncer)
+    if not success:
+        return 1
+
+    print("Synchronized project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "install"
+    description = "Generate Pipfile.lock to synchronize the environment."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "--no-check", dest="check",
+            action="store_false", default=True,
+            help="do not check if Pipfile.lock is update, always resolve",
+        )
+        self.parser.add_argument(
+            "--dev",
+            action="store_true",
+            help="install develop packages",
+        )
+        self.parser.add_argument(
+            "--no-clean", dest="clean",
+            action="store_false", default=True,
+            help="do not uninstall packages not specified in Pipfile.lock",
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/lock.py b/pipenv/vendor/passa/cli/lock.py
new file mode 100644
index 00000000..42dfcc06
--- /dev/null
+++ b/pipenv/vendor/passa/cli/lock.py
@@ -0,0 +1,29 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.lockers import BasicLocker
+    from passa.operations.lock import lock
+
+    project = options.project
+    locker = BasicLocker(project)
+    success = lock(locker)
+    if not success:
+        return
+
+    project._l.write()
+    print("Written to project at", project.root)
+
+
+class Command(BaseCommand):
+    name = "lock"
+    description = "Generate Pipfile.lock."
+    parsed_main = main
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/remove.py b/pipenv/vendor/passa/cli/remove.py
new file mode 100644
index 00000000..b2e03998
--- /dev/null
+++ b/pipenv/vendor/passa/cli/remove.py
@@ -0,0 +1,75 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.lockers import PinReuseLocker
+    from passa.operations.lock import lock
+
+    default = (options.only != "dev")
+    develop = (options.only != "default")
+
+    project = options.project
+    project.remove_keys_from_pipfile(
+        options.packages, default=default, develop=develop,
+    )
+
+    locker = PinReuseLocker(project)
+    success = lock(locker)
+    if not success:
+        return 1
+
+    project._p.write()
+    project._l.write()
+    print("Written to project at", project.root)
+
+    if not options.clean:
+        return
+
+    from passa.operations.sync import clean
+    from passa.synchronizers import Cleaner
+
+    cleaner = Cleaner(project, default=True, develop=True)
+    success = clean(cleaner)
+    if not success:
+        return 1
+
+    print("Cleaned project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "remove"
+    description = "Remove packages from project."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "packages", metavar="package",
+            nargs="+",
+            help="package to remove (can be used multiple times)",
+        )
+        dev_group = self.parser.add_mutually_exclusive_group()
+        dev_group.add_argument(
+            "--dev", dest="only",
+            action="store_const", const="dev",
+            help="only try to remove from [dev-packages]",
+        )
+        dev_group.add_argument(
+            "--default", dest="only",
+            action="store_const", const="default",
+            help="only try to remove from [packages]",
+        )
+        self.parser.add_argument(
+            "--no-clean", dest="clean",
+            action="store_false", default=True,
+            help="do not uninstall packages not specified in Pipfile.lock",
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/sync.py b/pipenv/vendor/passa/cli/sync.py
new file mode 100644
index 00000000..d2d9dcc7
--- /dev/null
+++ b/pipenv/vendor/passa/cli/sync.py
@@ -0,0 +1,46 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.operations.sync import sync
+    from passa.synchronizers import Synchronizer
+
+    project = options.project
+    syncer = Synchronizer(
+        project, default=True, develop=options.dev,
+        clean_unneeded=options.clean,
+    )
+
+    success = sync(syncer)
+    if not success:
+        return 1
+
+    print("Synchronized project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "sync"
+    description = "Install Pipfile.lock into the current environment."
+    parsed_main = main
+
+    def add_arguments(self):
+        super(Command, self).add_arguments()
+        self.parser.add_argument(
+            "--dev",
+            action="store_true",
+            help="install develop packages",
+        )
+        self.parser.add_argument(
+            "--no-clean", dest="clean",
+            action="store_false", default=True,
+            help="do not uninstall packages not specified in Pipfile.lock",
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/cli/upgrade.py b/pipenv/vendor/passa/cli/upgrade.py
new file mode 100644
index 00000000..bfa342f4
--- /dev/null
+++ b/pipenv/vendor/passa/cli/upgrade.py
@@ -0,0 +1,90 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import sys
+
+from ._base import BaseCommand
+
+
+def main(options):
+    from passa.lockers import EagerUpgradeLocker, PinReuseLocker
+    from passa.operations.lock import lock
+
+    project = options.project
+    packages = options.packages
+    for package in packages:
+        if not project.contains_key_in_pipfile(package):
+            print("{package!r} not found in Pipfile".format(
+                package=package,
+            ), file=sys.stderr)
+            return 2
+
+    project.remove_entries_from_lockfile(packages)
+
+    prev_lockfile = project.lockfile
+
+    if options.strategy == "eager":
+        locker = EagerUpgradeLocker(project, packages)
+    else:
+        locker = PinReuseLocker(project)
+    success = lock(locker)
+    if not success:
+        return 1
+
+    project._l.write()
+    print("Written to project at", project.root)
+
+    if not options.sync:
+        return
+
+    from passa.operations.sync import sync
+    from passa.synchronizers import Synchronizer
+
+    lockfile_diff = project.difference_lockfile(prev_lockfile)
+    default = bool(any(lockfile_diff.default))
+    develop = bool(any(lockfile_diff.develop))
+
+    syncer = Synchronizer(
+        project, default=default, develop=develop,
+        clean_unneeded=False,
+    )
+    success = sync(syncer)
+    if not success:
+        return 1
+
+    print("Synchronized project at", project.root)
+
+
+class Command(BaseCommand):
+
+    name = "upgrade"
+    description = "Upgrade packages in project."
+    parsed_main = main
+
+    def add_arguments(self):
+        self.parser.add_argument(
+            "packages", metavar="package",
+            nargs="+",
+            help="package to upgrade (can be used multiple times)",
+        )
+        self.parser.add_argument(
+            "--strategy",
+            choices=["eager", "only-if-needed"],
+            default="only-if-needed",
+            help="how dependency upgrading is handled",
+        )
+        self.parser.add_argument(
+            "--no-sync", dest="sync",
+            action="store_false", default=True,
+            help="do not synchronize the environment",
+        )
+        self.parser.add_argument(
+            "--no-clean", dest="clean",
+            action="store_false", default=True,
+            help="do not uninstall packages not specified in Pipfile.lock",
+        )
+
+
+if __name__ == "__main__":
+    Command.run_current_module()
diff --git a/pipenv/vendor/passa/dependencies.py b/pipenv/vendor/passa/dependencies.py
new file mode 100644
index 00000000..8edf2fd7
--- /dev/null
+++ b/pipenv/vendor/passa/dependencies.py
@@ -0,0 +1,253 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import functools
+import os
+import sys
+
+import packaging.specifiers
+import packaging.utils
+import packaging.version
+import requests
+import requirementslib
+import six
+
+from ._pip import build_wheel
+from .caches import DependencyCache, RequiresPythonCache
+from .markers import contains_extra, get_contained_extras, get_without_extra
+from .utils import is_pinned
+
+
+DEPENDENCY_CACHE = DependencyCache()
+REQUIRES_PYTHON_CACHE = RequiresPythonCache()
+
+
+def _cached(f, **kwargs):
+
+    @functools.wraps(f)
+    def wrapped(ireq):
+        result = f(ireq, **kwargs)
+        if result is not None and is_pinned(ireq):
+            deps, requires_python = result
+            DEPENDENCY_CACHE[ireq] = deps
+            REQUIRES_PYTHON_CACHE[ireq] = requires_python
+        return result
+
+    return wrapped
+
+
+def _is_cache_broken(line, parent_name):
+    dep_req = requirementslib.Requirement.from_line(line)
+    if contains_extra(dep_req.markers):
+        return True     # The "extra =" marker breaks everything.
+    elif dep_req.normalized_name == parent_name:
+        return True     # A package cannot depend on itself.
+    return False
+
+
+def _get_dependencies_from_cache(ireq):
+    """Retrieves dependencies for the requirement from the dependency cache.
+    """
+    if os.environ.get("PASSA_IGNORE_LOCAL_CACHE"):
+        return
+    if ireq.editable:
+        return
+    try:
+        deps = DEPENDENCY_CACHE[ireq]
+        pyrq = REQUIRES_PYTHON_CACHE[ireq]
+    except KeyError:
+        return
+
+    # Preserving sanity: Run through the cache and make sure every entry if
+    # valid. If this fails, something is wrong with the cache. Drop it.
+    try:
+        packaging.specifiers.SpecifierSet(pyrq)
+        ireq_name = packaging.utils.canonicalize_name(ireq.name)
+        if any(_is_cache_broken(line, ireq_name) for line in deps):
+            broken = True
+        else:
+            broken = False
+    except Exception:
+        broken = True
+
+    if broken:
+        print("dropping broken cache for {0}".format(ireq.name))
+        del DEPENDENCY_CACHE[ireq]
+        del REQUIRES_PYTHON_CACHE[ireq]
+        return
+
+    return deps, pyrq
+
+
+def _get_dependencies_from_json_url(url, session):
+    response = session.get(url)
+    response.raise_for_status()
+    info = response.json()["info"]
+
+    requires_python = info["requires_python"] or ""
+    try:
+        requirement_lines = info["requires_dist"]
+    except KeyError:
+        requirement_lines = info["requires"]
+
+    # The JSON API return null for empty requirements, for some reason, so we
+    # can't just pass it into the comprehension.
+    if not requirement_lines:
+        return [], requires_python
+
+    dependencies = [
+        dep_req.as_line(include_hashes=False) for dep_req in (
+            requirementslib.Requirement.from_line(line)
+            for line in requirement_lines
+        )
+        if not contains_extra(dep_req.markers)
+    ]
+    return dependencies, requires_python
+
+
+def _get_dependencies_from_json(ireq, sources):
+    """Retrieves dependencies for the install requirement from the JSON API.
+
+    :param ireq: A single InstallRequirement
+    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`
+    :return: A set of dependency lines for generating new InstallRequirements.
+    :rtype: set(str) or None
+    """
+    if os.environ.get("PASSA_IGNORE_JSON_API"):
+        return
+    if ireq.editable:
+        return
+
+    # It is technically possible to parse extras out of the JSON API's
+    # requirement format, but it is such a chore let's just use the simple API.
+    if ireq.extras:
+        return
+
+    url_prefixes = [
+        proc_url[:-7]   # Strip "/simple".
+        for proc_url in (
+            raw_url.rstrip("/")
+            for raw_url in (source.get("url", "") for source in sources)
+        )
+        if proc_url.endswith("/simple")
+    ]
+
+    session = requests.session()
+    version = str(ireq.specifier).lstrip("=")
+
+    for prefix in url_prefixes:
+        url = "{prefix}/pypi/{name}/{version}/json".format(
+            prefix=prefix,
+            name=packaging.utils.canonicalize_name(ireq.name),
+            version=version,
+        )
+        try:
+            dependencies = _get_dependencies_from_json_url(url, session)
+            if dependencies is not None:
+                return dependencies
+        except Exception as e:
+            pass
+        print("unable to read dependencies via {0}".format(url))
+    return
+
+
+def _read_requirements(metadata, extras):
+    """Read wheel metadata to know what it depends on.
+
+    The `run_requires` attribute contains a list of dict or str specifying
+    requirements. For dicts, it may contain an "extra" key to specify these
+    requirements are for a specific extra. Unfortunately, not all fields are
+    specificed like this (I don't know why); some are specified with markers.
+    So we jump though these terrible hoops to know exactly what we need.
+
+    The extra extraction is not comprehensive. Tt assumes the marker is NEVER
+    something like `extra == "foo" and extra == "bar"`. I guess this never
+    makes sense anyway? Markers are just terrible.
+    """
+    extras = extras or ()
+    requirements = []
+    for entry in metadata.run_requires:
+        if isinstance(entry, six.text_type):
+            entry = {"requires": [entry]}
+            extra = None
+        else:
+            extra = entry.get("extra")
+        if extra is not None and extra not in extras:
+            continue
+        for line in entry.get("requires", []):
+            r = requirementslib.Requirement.from_line(line)
+            if r.markers:
+                contained = get_contained_extras(r.markers)
+                if (contained and not any(e in contained for e in extras)):
+                    continue
+                marker = get_without_extra(r.markers)
+                r.markers = str(marker) if marker else None
+                line = r.as_line(include_hashes=False)
+            requirements.append(line)
+    return requirements
+
+
+def _read_requires_python(metadata):
+    """Read wheel metadata to know the value of Requires-Python.
+
+    This is surprisingly poorly supported in Distlib. This function tries
+    several ways to get this information:
+
+    * Metadata 2.0: metadata.dictionary.get("requires_python") is not None
+    * Metadata 2.1: metadata._legacy.get("Requires-Python") is not None
+    * Metadata 1.2: metadata._legacy.get("Requires-Python") != "UNKNOWN"
+    """
+    # TODO: Support more metadata formats.
+    value = metadata.dictionary.get("requires_python")
+    if value is not None:
+        return value
+    if metadata._legacy:
+        value = metadata._legacy.get("Requires-Python")
+        if value is not None and value != "UNKNOWN":
+            return value
+    return ""
+
+
+def _get_dependencies_from_pip(ireq, sources):
+    """Retrieves dependencies for the requirement from pip internals.
+
+    The current strategy is to build a wheel out of the ireq, and read metadata
+    out of it.
+    """
+    wheel = build_wheel(ireq, sources)
+    extras = ireq.extras or ()
+    requirements = _read_requirements(wheel.metadata, extras)
+    requires_python = _read_requires_python(wheel.metadata)
+    return requirements, requires_python
+
+
+def get_dependencies(requirement, sources):
+    """Get all dependencies for a given install requirement.
+
+    :param requirement: A requirement
+    :param sources: Pipfile-formatted sources
+    :type sources: list[dict]
+    """
+    getters = [
+        _get_dependencies_from_cache,
+        _cached(_get_dependencies_from_json, sources=sources),
+        _cached(_get_dependencies_from_pip, sources=sources),
+    ]
+    ireq = requirement.as_ireq()
+    last_exc = None
+    for getter in getters:
+        try:
+            result = getter(ireq)
+        except Exception as e:
+            last_exc = sys.exc_info()
+            continue
+        if result is not None:
+            deps, pyreq = result
+            reqs = [requirementslib.Requirement.from_line(d) for d in deps]
+            return reqs, pyreq
+    if last_exc:
+        six.reraise(*last_exc)
+    raise RuntimeError("failed to get dependencies for {}".format(
+        requirement.as_line(),
+    ))
diff --git a/pipenv/vendor/passa/dependencies_pip.py b/pipenv/vendor/passa/dependencies_pip.py
new file mode 100644
index 00000000..8335b211
--- /dev/null
+++ b/pipenv/vendor/passa/dependencies_pip.py
@@ -0,0 +1,187 @@
+import importlib
+import os
+
+import distlib.wheel
+import packaging.version
+import pip_shims
+import requirementslib
+import six
+
+from .caches import CACHE_DIR
+from .markers import get_contained_extras, get_without_extra
+from .utils import cheesy_temporary_directory, mkdir_p
+
+
+# HACK: Can we get pip_shims to support these in time?
+def _import_module_of(obj):
+    return importlib.import_module(obj.__module__)
+
+
+WheelBuilder = _import_module_of(pip_shims.Wheel).WheelBuilder
+unpack_url = _import_module_of(pip_shims.is_file_url).unpack_url
+
+
+def _prepare_wheel_building_kwargs():
+    format_control = pip_shims.FormatControl(set(), set())
+    wheel_cache = pip_shims.WheelCache(CACHE_DIR, format_control)
+
+    download_dir = os.path.join(CACHE_DIR, "pkgs")
+    mkdir_p(download_dir)
+
+    build_dir = cheesy_temporary_directory(prefix="build")
+    src_dir = cheesy_temporary_directory(prefix="source")
+
+    return {
+        "wheel_cache": wheel_cache,
+        "build_dir": build_dir,
+        "src_dir": src_dir,
+        "download_dir": download_dir,
+        "wheel_download_dir": download_dir,
+    }
+
+
+def _get_pip_index_urls(sources):
+    index_urls = []
+    trusted_hosts = []
+    for source in sources:
+        url = source.get("url")
+        if not url:
+            continue
+        index_urls.append(url)
+        if source.get("verify_ssl", True):
+            continue
+        host = six.moves.urllib.parse.urlparse(source["url"]).hostname
+        trusted_hosts.append(host)
+    return index_urls, trusted_hosts
+
+
+class _PipCommand(pip_shims.Command):
+    name = 'PipCommand'
+
+
+def _get_pip_session(trusted_hosts):
+    cmd = _PipCommand()
+    options, _ = cmd.parser.parse_args([])
+    options.cache_dir = CACHE_DIR
+    options.trusted_hosts = trusted_hosts
+    session = cmd._build_session(options)
+    return session
+
+
+def _get_internal_objects(sources):
+    index_urls, trusted_hosts = _get_pip_index_urls(sources)
+    session = _get_pip_session(trusted_hosts)
+    finder = pip_shims.PackageFinder(
+        find_links=[],
+        index_urls=index_urls,
+        trusted_hosts=trusted_hosts,
+        allow_all_prereleases=True,
+        session=session,
+    )
+    return finder, session
+
+
+def _build_wheel_pre10(ireq, output_dir, finder, session, kwargs):
+    reqset = pip_shims.RequirementSet(**kwargs)
+    builder = WheelBuilder(reqset, finder)
+    return builder._build_one(ireq, output_dir)
+
+
+def _build_wheel_10x(ireq, output_dir, finder, session, kwargs):
+    kwargs.update({"progress_bar": "off", "build_isolation": False})
+    wheel_cache = kwargs.pop("wheel_cache")
+    preparer = pip_shims.RequirementPreparer(**kwargs)
+    builder = WheelBuilder(finder, preparer, wheel_cache)
+    return builder._build_one(ireq, output_dir)
+
+
+def _build_wheel_modern(ireq, output_dir, finder, session, kwargs):
+    kwargs.update({"progress_bar": "off", "build_isolation": False})
+    wheel_cache = kwargs.pop("wheel_cache")
+    with pip_shims.RequirementTracker() as req_tracker:
+        kwargs["req_tracker"] = req_tracker
+        preparer = pip_shims.RequirementPreparer(**kwargs)
+        builder = WheelBuilder(finder, preparer, wheel_cache)
+        return builder._build_one(ireq, output_dir)
+
+
+def _build_wheel(*args):
+    pip_version = packaging.version.parse(pip_shims.pip_version)
+    if pip_version < packaging.version.parse("10"):
+        return _build_wheel_pre10(*args)
+    elif pip_version < packaging.version.parse("18"):
+        return _build_wheel_10x(*args)
+    return _build_wheel_modern(*args)
+
+
+def _read_requirements(wheel_path, extras):
+    """Read wheel metadata to know what it depends on.
+
+    The `run_requires` attribute contains a list of dict or str specifying
+    requirements. For dicts, it may contain an "extra" key to specify these
+    requirements are for a specific extra. Unfortunately, not all fields are
+    specificed like this (I don't know why); some are specified with markers.
+    So we jump though these terrible hoops to know exactly what we need.
+
+    The extra extraction is not comprehensive. Tt assumes the marker is NEVER
+    something like `extra == "foo" and extra == "bar"`. I guess this never
+    makes sense anyway? Markers are just terrible.
+    """
+    extras = extras or ()
+    wheel = distlib.wheel.Wheel(wheel_path)
+    requirements = []
+    for entry in wheel.metadata.run_requires:
+        if isinstance(entry, six.text_type):
+            entry = {"requires": [entry]}
+            extra = None
+        else:
+            extra = entry.get("extra")
+        if extra is not None and extra not in extras:
+            continue
+        for line in entry.get("requires", []):
+            r = requirementslib.Requirement.from_line(line)
+            if r.markers:
+                contained_extras = get_contained_extras(r.markers)
+                if (contained_extras and
+                        not any(e in contained_extras for e in extras)):
+                    continue
+                marker = get_without_extra(r.markers)
+                r.markers = str(marker) if marker else None
+                line = r.as_line(include_hashes=False)
+            requirements.append(line)
+    return requirements
+
+
+def _get_dependencies_from_pip(ireq, sources):
+    """Retrieves dependencies for the requirement from pip internals.
+
+    The current strategy is to build a wheel out of the ireq, and read metadata
+    out of it.
+    """
+    kwargs = _prepare_wheel_building_kwargs()
+    finder, session = _get_internal_objects(sources)
+
+    # Not for upgrade, hash not required.
+    ireq.populate_link(finder, False, False)
+    ireq.ensure_has_source_dir(kwargs["src_dir"])
+    if not pip_shims.is_file_url(ireq.link):
+        # This makes sure the remote artifact is downloaded locally. For
+        # wheels, it is enough to just download because we'll use them
+        # directly. For an sdist, we need to unpack so we can build it.
+        unpack_url(
+            ireq.link, ireq.source_dir, kwargs["download_dir"],
+            only_download=ireq.is_wheel, session=session,
+            hashes=ireq.hashes(True), progress_bar=False,
+        )
+
+    if ireq.is_wheel:   # If this is a wheel, use the downloaded thing.
+        output_dir = kwargs["download_dir"]
+        wheel_path = os.path.join(output_dir, ireq.link.filename)
+    else:               # Othereise we need to build an ephemeral wheel.
+        output_dir = cheesy_temporary_directory(prefix="ephem")
+        wheel_path = _build_wheel(ireq, output_dir, finder, session, kwargs)
+
+    if not wheel_path or not os.path.exists(wheel_path):
+        raise RuntimeError("failed to build wheel from {}".format(ireq))
+    requirements = _read_requirements(wheel_path, ireq.extras)
+    return requirements
diff --git a/pipenv/vendor/passa/hashes.py b/pipenv/vendor/passa/hashes.py
new file mode 100644
index 00000000..fe049274
--- /dev/null
+++ b/pipenv/vendor/passa/hashes.py
@@ -0,0 +1,61 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import contextlib
+
+from pip_shims import Wheel
+
+
+def _wheel_supported(self, tags=None):
+    # Ignore current platform. Support everything.
+    return True
+
+
+def _wheel_support_index_min(self, tags=None):
+    # All wheels are equal priority for sorting.
+    return 0
+
+
+@contextlib.contextmanager
+def _allow_all_wheels():
+    """Monkey patch pip.Wheel to allow all wheels
+
+    The usual checks against platforms and Python versions are ignored to allow
+    fetching all available entries in PyPI. This also saves the candidate cache
+    and set a new one, or else the results from the previous non-patched calls
+    will interfere.
+    """
+    original_wheel_supported = Wheel.supported
+    original_support_index_min = Wheel.support_index_min
+
+    Wheel.supported = _wheel_supported
+    Wheel.support_index_min = _wheel_support_index_min
+    yield
+    Wheel.supported = original_wheel_supported
+    Wheel.support_index_min = original_support_index_min
+
+
+def get_hashes(cache, req):
+    if req.is_vcs:
+        return set()
+
+    ireq = req.as_ireq()
+
+    if ireq.editable:
+        return set()
+
+    if req.is_file_or_url:
+        # TODO: Get the hash of the linked artifact?
+        return set()
+
+    if not ireq.is_pinned:
+        return set()
+
+    with _allow_all_wheels():
+        matching_candidates = req.find_all_matches()
+
+    return {
+        cache.get_hash(candidate.location)
+        for candidate in matching_candidates
+    }
diff --git a/pipenv/vendor/passa/internals/__init__.py b/pipenv/vendor/passa/internals/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/pipenv/vendor/passa/internals/_pip.py b/pipenv/vendor/passa/internals/_pip.py
new file mode 100644
index 00000000..b7629713
--- /dev/null
+++ b/pipenv/vendor/passa/internals/_pip.py
@@ -0,0 +1,317 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import contextlib
+import distutils.log
+import os
+
+import setuptools.dist
+
+import distlib.scripts
+import distlib.wheel
+import pip_shims
+import six
+import vistir
+
+from ._pip_shims import VCS_SUPPORT, build_wheel as _build_wheel, unpack_url
+from .caches import CACHE_DIR
+from .utils import filter_sources
+
+
+@vistir.path.ensure_mkdir_p(mode=0o775)
+def _get_src_dir():
+    src = os.environ.get("PIP_SRC")
+    if src:
+        return src
+    virtual_env = os.environ.get("VIRTUAL_ENV")
+    if virtual_env:
+        return os.path.join(virtual_env, "src")
+    return os.path.join(os.getcwd(), "src")     # Match pip's behavior.
+
+
+def _prepare_wheel_building_kwargs(ireq):
+    download_dir = os.path.join(CACHE_DIR, "pkgs")
+    vistir.mkdir_p(download_dir)
+
+    wheel_download_dir = os.path.join(CACHE_DIR, "wheels")
+    vistir.mkdir_p(wheel_download_dir)
+
+    if ireq.source_dir is not None:
+        src_dir = ireq.source_dir
+    elif ireq.editable:
+        src_dir = _get_src_dir()
+    else:
+        src_dir = vistir.path.create_tracked_tempdir(prefix='passa-src')
+
+
+    # This logic matches pip's behavior, although I don't fully understand the
+    # intention. I guess the idea is to build editables in-place, otherwise out
+    # of the source tree?
+    if ireq.editable:
+        build_dir = src_dir
+    else:
+        build_dir = vistir.path.create_tracked_tempdir(prefix="passa-build")
+
+    return {
+        "build_dir": build_dir,
+        "src_dir": src_dir,
+        "download_dir": download_dir,
+        "wheel_download_dir": wheel_download_dir,
+    }
+
+
+def _get_pip_index_urls(sources):
+    index_urls = []
+    trusted_hosts = []
+    for source in sources:
+        url = source.get("url")
+        if not url:
+            continue
+        index_urls.append(url)
+        if source.get("verify_ssl", True):
+            continue
+        host = six.moves.urllib.parse.urlparse(source["url"]).hostname
+        trusted_hosts.append(host)
+    return index_urls, trusted_hosts
+
+
+class _PipCommand(pip_shims.Command):
+    name = "PipCommand"
+
+
+def _get_pip_session(trusted_hosts):
+    cmd = _PipCommand()
+    options, _ = cmd.parser.parse_args([])
+    options.cache_dir = CACHE_DIR
+    options.trusted_hosts = trusted_hosts
+    session = cmd._build_session(options)
+    return session
+
+
+def _get_finder(sources):
+    index_urls, trusted_hosts = _get_pip_index_urls(sources)
+    session = _get_pip_session(trusted_hosts)
+    finder = pip_shims.PackageFinder(
+        find_links=[],
+        index_urls=index_urls,
+        trusted_hosts=trusted_hosts,
+        allow_all_prereleases=True,
+        session=session,
+    )
+    return finder
+
+
+def _get_wheel_cache():
+    format_control = pip_shims.FormatControl(set(), set())
+    wheel_cache = pip_shims.WheelCache(CACHE_DIR, format_control)
+    return wheel_cache
+
+
+def _convert_hashes(values):
+    """Convert Pipfile.lock hash lines into InstallRequirement option format.
+
+    The option format uses a str-list mapping. Keys are hash algorithms, and
+    the list contains all values of that algorithm.
+    """
+    hashes = {}
+    if not values:
+        return hashes
+    for value in values:
+        try:
+            name, value = value.split(":", 1)
+        except ValueError:
+            name = "sha256"
+        if name not in hashes:
+            hashes[name] = []
+        hashes[name].append(value)
+    return hashes
+
+
+def build_wheel(ireq, sources, hashes=None):
+    """Build a wheel file for the InstallRequirement object.
+
+    An artifact is downloaded (or read from cache). If the artifact is not a
+    wheel, build one out of it. The dynamically built wheel is ephemeral; do
+    not depend on its existence after the returned wheel goes out of scope.
+
+    If `hashes` is truthy, it is assumed to be a list of hashes (as formatted
+    in Pipfile.lock) to be checked against the download.
+
+    Returns a `distlib.wheel.Wheel` instance. Raises a `RuntimeError` if the
+    wheel cannot be built.
+    """
+    kwargs = _prepare_wheel_building_kwargs(ireq)
+    finder = _get_finder(sources)
+
+    # Not for upgrade, hash not required. Hashes are not required here even
+    # when we provide them, because pip skips local wheel cache if we set it
+    # to True. Hashes are checked later if we need to download the file.
+    ireq.populate_link(finder, False, False)
+
+    # Ensure ireq.source_dir is set.
+    # This is intentionally set to build_dir, not src_dir. Comments from pip:
+    #   [...] if filesystem packages are not marked editable in a req, a non
+    #   deterministic error occurs when the script attempts to unpack the
+    #   build directory.
+    # Also see comments in `_prepare_wheel_building_kwargs()` -- If the ireq
+    # is editable, build_dir is actually src_dir, making the build in-place.
+    ireq.ensure_has_source_dir(kwargs["build_dir"])
+
+    # Ensure the source is fetched. For wheels, it is enough to just download
+    # because we'll use them directly. For an sdist, we need to unpack so we
+    # can build it.
+    if not ireq.editable or not pip_shims.is_file_url(ireq.link):
+        if ireq.is_wheel:
+            only_download = True
+            download_dir = kwargs["wheel_download_dir"]
+        else:
+            only_download = False
+            download_dir = kwargs["download_dir"]
+        ireq.options["hashes"] = _convert_hashes(hashes)
+        unpack_url(
+            ireq.link, ireq.source_dir, download_dir,
+            only_download=only_download, session=finder.session,
+            hashes=ireq.hashes(False), progress_bar=False,
+        )
+
+    if ireq.is_wheel:
+        # If this is a wheel, use the downloaded thing.
+        output_dir = kwargs["wheel_download_dir"]
+        wheel_path = os.path.join(output_dir, ireq.link.filename)
+    else:
+        # Othereise we need to build an ephemeral wheel.
+        wheel_path = _build_wheel(
+            ireq, vistir.path.create_tracked_tempdir(prefix="ephem"),
+            finder, _get_wheel_cache(), kwargs,
+        )
+        if wheel_path is None or not os.path.exists(wheel_path):
+            raise RuntimeError("failed to build wheel from {}".format(ireq))
+    return distlib.wheel.Wheel(wheel_path)
+
+
+def _obtrain_ref(vcs_obj, src_dir, name, rev=None):
+    target_dir = os.path.join(src_dir, name)
+    target_rev = vcs_obj.make_rev_options(rev)
+    if not os.path.exists(target_dir):
+        vcs_obj.obtain(target_dir)
+    if (not vcs_obj.is_commit_id_equal(target_dir, rev) and
+            not vcs_obj.is_commit_id_equal(target_dir, target_rev)):
+        vcs_obj.update(target_dir, target_rev)
+    return vcs_obj.get_revision(target_dir)
+
+
+def get_vcs_ref(requirement):
+    backend = VCS_SUPPORT._registry.get(requirement.vcs)
+    vcs = backend(url=requirement.req.vcs_uri)
+    src = _get_src_dir()
+    name = requirement.normalized_name
+    ref = _obtrain_ref(vcs, src, name, rev=requirement.req.ref)
+    return ref
+
+
+def find_installation_candidates(ireq, sources):
+    finder = _get_finder(sources)
+    return finder.find_all_candidates(ireq.name)
+
+
+class RequirementUninstallation(object):
+    """A context manager to remove a package for the inner block.
+
+    This uses `UninstallPathSet` to control the workflow. If the inner block
+    exits correctly, the uninstallation is committed, otherwise rolled back.
+    """
+    def __init__(self, ireq, auto_confirm, verbose):
+        self.ireq = ireq
+        self.pathset = None
+        self.auto_confirm = auto_confirm
+        self.verbose = verbose
+
+    def __enter__(self):
+        self.pathset = self.ireq.uninstall(
+            auto_confirm=self.auto_confirm,
+            verbose=self.verbose,
+        )
+        return self.pathset
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        if self.pathset is None:
+            return
+        if exc_type is None:
+            self.pathset.commit()
+        else:
+            self.pathset.rollback()
+
+
+def uninstall_requirement(ireq, **kwargs):
+    return RequirementUninstallation(ireq, **kwargs)
+
+
+@contextlib.contextmanager
+def _suppress_distutils_logs():
+    """Hack to hide noise generated by `setup.py develop`.
+
+    There isn't a good way to suppress them now, so let's monky-patch.
+    See https://bugs.python.org/issue25392.
+    """
+    f = distutils.log.Log._log
+
+    def _log(log, level, msg, args):
+        if level >= distutils.log.ERROR:
+            f(log, level, msg, args)
+
+    distutils.log.Log._log = _log
+    yield
+    distutils.log.Log._log = f
+
+
+class NoopInstaller(object):
+    """An installer.
+
+    This class is not designed to be instantiated by itself, but used as a
+    common interface for subclassing.
+
+    An installer has two methods, `prepare()` and `install()`. Neither takes
+    arguments, and should be called in that order to prepare an installation
+    operation, and to actually install things.
+    """
+    def prepare(self):
+        pass
+
+    def install(self):
+        pass
+
+
+class EditableInstaller(NoopInstaller):
+    """Installer to handle editable.
+    """
+    def __init__(self, requirement):
+        ireq = requirement.as_ireq()
+        self.working_directory = ireq.setup_py_dir
+        self.setup_py = ireq.setup_py
+
+    def install(self):
+        with vistir.cd(self.working_directory), _suppress_distutils_logs():
+            # Access from Setuptools to ensure things are patched correctly.
+            setuptools.dist.distutils.core.run_setup(
+                self.setup_py, ["develop", "--no-deps"],
+            )
+
+
+class WheelInstaller(NoopInstaller):
+    """Installer by building a wheel.
+
+    The wheel is built during `prepare()`, and installed in `install()`.
+    """
+    def __init__(self, requirement, sources, paths):
+        self.ireq = requirement.as_ireq()
+        self.sources = filter_sources(requirement, sources)
+        self.hashes = requirement.hashes or None
+        self.paths = paths
+        self.wheel = None
+
+    def prepare(self):
+        self.wheel = build_wheel(self.ireq, self.sources, self.hashes)
+
+    def install(self):
+        self.wheel.install(self.paths, distlib.scripts.ScriptMaker(None, None))
diff --git a/pipenv/vendor/passa/internals/_pip_shims.py b/pipenv/vendor/passa/internals/_pip_shims.py
new file mode 100644
index 00000000..b2c7b6ea
--- /dev/null
+++ b/pipenv/vendor/passa/internals/_pip_shims.py
@@ -0,0 +1,61 @@
+# -*- coding=utf-8 -*-
+
+"""Shims to make the pip interface more consistent accross versions.
+
+There are currently two members:
+
+* VCS_SUPPORT is an instance of VcsSupport.
+* build_wheel abstracts the process to build a wheel out of a bunch parameters.
+* unpack_url wraps the actual function in pip to accept modern parameters.
+"""
+
+from __future__ import absolute_import, unicode_literals
+
+import pip_shims
+
+
+def _build_wheel_pre10(ireq, output_dir, finder, wheel_cache, kwargs):
+    kwargs.update({"wheel_cache": wheel_cache, "session": finder.session})
+    reqset = pip_shims.RequirementSet(**kwargs)
+    builder = pip_shims.WheelBuilder(reqset, finder)
+    return builder._build_one(ireq, output_dir)
+
+
+def _build_wheel_modern(ireq, output_dir, finder, wheel_cache, kwargs):
+    """Build a wheel.
+
+    * ireq: The InstallRequirement object to build
+    * output_dir: The directory to build the wheel in.
+    * finder: pip's internal Finder object to find the source out of ireq.
+    * kwargs: Various keyword arguments from `_prepare_wheel_building_kwargs`.
+    """
+    kwargs.update({"progress_bar": "off", "build_isolation": False})
+    with pip_shims.RequirementTracker() as req_tracker:
+        if req_tracker:
+            kwargs["req_tracker"] = req_tracker
+        preparer = pip_shims.RequirementPreparer(**kwargs)
+        builder = pip_shims.WheelBuilder(finder, preparer, wheel_cache)
+        return builder._build_one(ireq, output_dir)
+
+
+def _unpack_url_pre10(*args, **kwargs):
+    """Shim for unpack_url in various pip versions.
+
+    pip before 10.0 does not accept `progress_bar` here. Simply drop it.
+    """
+    kwargs.pop("progress_bar", None)
+    return pip_shims.unpack_url(*args, **kwargs)
+
+
+PIP_VERSION = pip_shims.utils._parse(pip_shims.pip_version)
+VERSION_10 = pip_shims.utils._parse("10")
+
+
+VCS_SUPPORT = pip_shims.VcsSupport()
+
+build_wheel = _build_wheel_modern
+unpack_url = pip_shims.unpack_url
+
+if PIP_VERSION < VERSION_10:
+    build_wheel = _build_wheel_pre10
+    unpack_url = _unpack_url_pre10
diff --git a/pipenv/vendor/passa/internals/caches.py b/pipenv/vendor/passa/internals/caches.py
new file mode 100644
index 00000000..6d3131fa
--- /dev/null
+++ b/pipenv/vendor/passa/internals/caches.py
@@ -0,0 +1,214 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import copy
+import hashlib
+import json
+import os
+import sys
+
+import appdirs
+import pip_shims
+import requests
+import vistir
+
+from ._pip_shims import VCS_SUPPORT
+from .utils import get_pinned_version
+
+
+CACHE_DIR = os.environ.get("PASSA_CACHE_DIR", appdirs.user_cache_dir("passa"))
+
+
+class HashCache(pip_shims.SafeFileCache):
+    """Caches hashes of PyPI artifacts so we do not need to re-download them.
+
+    Hashes are only cached when the URL appears to contain a hash in it and the
+    cache key includes the hash value returned from the server). This ought to
+    avoid ssues where the location on the server changes.
+    """
+    def __init__(self, *args, **kwargs):
+        session = kwargs.pop('session', requests.session())
+        self.session = session
+        kwargs.setdefault('directory', os.path.join(CACHE_DIR, 'hash-cache'))
+        super(HashCache, self).__init__(*args, **kwargs)
+
+    def get_hash(self, location):
+        # If there is no location hash (i.e., md5, sha256, etc.), we don't want
+        # to store it.
+        hash_value = None
+        orig_scheme = location.scheme
+        new_location = copy.deepcopy(location)
+        if orig_scheme in VCS_SUPPORT.all_schemes:
+            new_location.url = new_location.url.split("+", 1)[-1]
+        can_hash = new_location.hash
+        if can_hash:
+            # hash url WITH fragment
+            hash_value = self.get(new_location.url)
+        if not hash_value:
+            hash_value = self._get_file_hash(new_location)
+            hash_value = hash_value.encode('utf8')
+        if can_hash:
+            self.set(new_location.url, hash_value)
+        return hash_value.decode('utf8')
+
+    def _get_file_hash(self, location):
+        h = hashlib.new(pip_shims.FAVORITE_HASH)
+        with vistir.open_file(location, self.session) as fp:
+            for chunk in iter(lambda: fp.read(8096), b""):
+                h.update(chunk)
+        return ":".join([h.name, h.hexdigest()])
+
+
+# pip-tools's dependency cache implementation.
+class CorruptCacheError(Exception):
+    def __init__(self, path):
+        self.path = path
+
+    def __str__(self):
+        lines = [
+            'The dependency cache seems to have been corrupted.',
+            'Inspect, or delete, the following file:',
+            '  {}'.format(self.path),
+        ]
+        return os.linesep.join(lines)
+
+
+def _key_from_req(req):
+    """Get an all-lowercase version of the requirement's name."""
+    if hasattr(req, 'key'):
+        # from pkg_resources, such as installed dists for pip-sync
+        key = req.key
+    else:
+        # from packaging, such as install requirements from requirements.txt
+        key = req.name
+
+    key = key.replace('_', '-').lower()
+    return key
+
+
+def _read_cache_file(cache_file_path):
+    with open(cache_file_path, 'r') as cache_file:
+        try:
+            doc = json.load(cache_file)
+        except ValueError:
+            raise CorruptCacheError(cache_file_path)
+
+        # Check version and load the contents
+        assert doc['__format__'] == 1, 'Unknown cache file format'
+        return doc['dependencies']
+
+
+class _JSONCache(object):
+    """A persistent cache backed by a JSON file.
+
+    The cache file is written to the appropriate user cache dir for the
+    current platform, i.e.
+
+        ~/.cache/pip-tools/depcache-pyX.Y.json
+
+    Where X.Y indicates the Python version.
+    """
+    filename_format = None
+
+    def __init__(self, cache_dir=CACHE_DIR):
+        vistir.mkdir_p(cache_dir)
+        python_version = ".".join(str(digit) for digit in sys.version_info[:2])
+        cache_filename = self.filename_format.format(
+            python_version=python_version,
+        )
+        self._cache_file = os.path.join(cache_dir, cache_filename)
+        self._cache = None
+
+    @property
+    def cache(self):
+        """The dictionary that is the actual in-memory cache.
+
+        This property lazily loads the cache from disk.
+        """
+        if self._cache is None:
+            self.read_cache()
+        return self._cache
+
+    def as_cache_key(self, ireq):
+        """Given a requirement, return its cache key.
+
+        This behavior is a little weird in order to allow backwards
+        compatibility with cache files. For a requirement without extras, this
+        will return, for example::
+
+            ("ipython", "2.1.0")
+
+        For a requirement with extras, the extras will be comma-separated and
+        appended to the version, inside brackets, like so::
+
+            ("ipython", "2.1.0[nbconvert,notebook]")
+        """
+        extras = tuple(sorted(ireq.extras))
+        if not extras:
+            extras_string = ""
+        else:
+            extras_string = "[{}]".format(",".join(extras))
+        name = _key_from_req(ireq.req)
+        version = get_pinned_version(ireq)
+        return name, "{}{}".format(version, extras_string)
+
+    def read_cache(self):
+        """Reads the cached contents into memory.
+        """
+        if os.path.exists(self._cache_file):
+            self._cache = _read_cache_file(self._cache_file)
+        else:
+            self._cache = {}
+
+    def write_cache(self):
+        """Writes the cache to disk as JSON.
+        """
+        doc = {
+            '__format__': 1,
+            'dependencies': self._cache,
+        }
+        with open(self._cache_file, 'w') as f:
+            json.dump(doc, f, sort_keys=True)
+
+    def clear(self):
+        self._cache = {}
+        self.write_cache()
+
+    def __contains__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return pkgversion_and_extras in self.cache.get(pkgname, {})
+
+    def __getitem__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return self.cache[pkgname][pkgversion_and_extras]
+
+    def __setitem__(self, ireq, values):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        self.cache.setdefault(pkgname, {})
+        self.cache[pkgname][pkgversion_and_extras] = values
+        self.write_cache()
+
+    def __delitem__(self, ireq):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        try:
+            del self.cache[pkgname][pkgversion_and_extras]
+        except KeyError:
+            return
+        self.write_cache()
+
+    def get(self, ireq, default=None):
+        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
+        return self.cache.get(pkgname, {}).get(pkgversion_and_extras, default)
+
+
+class DependencyCache(_JSONCache):
+    """Cache the dependency of cancidates.
+    """
+    filename_format = "depcache-py{python_version}.json"
+
+
+class RequiresPythonCache(_JSONCache):
+    """Cache a candidate's Requires-Python information.
+    """
+    filename_format = "pyreqcache-py{python_version}.json"
diff --git a/pipenv/vendor/passa/internals/candidates.py b/pipenv/vendor/passa/internals/candidates.py
new file mode 100644
index 00000000..d5390d65
--- /dev/null
+++ b/pipenv/vendor/passa/internals/candidates.py
@@ -0,0 +1,81 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import os
+import sys
+
+import packaging.specifiers
+import packaging.version
+import requirementslib
+
+from ._pip import find_installation_candidates, get_vcs_ref
+
+
+def _filter_matching_python_requirement(candidates, python_version):
+    for c in candidates:
+        try:
+            requires_python = c.requires_python
+        except AttributeError:
+            requires_python = c.location.requires_python
+        if python_version and requires_python:
+            # Old specifications had people setting this to single digits
+            # which is effectively the same as '>=digit,<digit+1'
+            if requires_python.isdigit():
+                requires_python = '>={0},<{1}'.format(
+                    requires_python, int(requires_python) + 1,
+                )
+            try:
+                specset = packaging.specifiers.SpecifierSet(requires_python)
+            except packaging.specifiers.InvalidSpecifier:
+                continue
+            if not specset.contains(python_version):
+                continue
+        yield c
+
+
+def _copy_requirement(requirement):
+    return requirement.copy()
+
+
+def _requirement_from_metadata(name, version, extras, index):
+    # Markers are intentionally dropped here. They will be added to candidates
+    # after resolution, so we can perform marker aggregation.
+    r = requirementslib.Requirement.from_metadata(name, version, extras, None)
+    r.index = index
+    return r
+
+
+def find_candidates(requirement, sources, allow_pre):
+    # A non-named requirement has exactly one candidate that is itself. For
+    # VCS, we also lock the requirement to an exact ref.
+    if not requirement.is_named:
+        candidate = _copy_requirement(requirement)
+        if candidate.is_vcs:
+            candidate.req.ref = get_vcs_ref(candidate)
+        return [candidate]
+
+    ireq = requirement.as_ireq()
+    icans = find_installation_candidates(ireq, sources)
+
+    python_version = os.environ.get(
+        "PASSA_PYTHON_VERSION",
+        "{0[0]}.{0[1]}".format(sys.version_info),
+    )
+    if python_version != ":all:":
+        matching_icans = list(_filter_matching_python_requirement(
+            icans, packaging.version.parse(python_version),
+        ))
+        icans = matching_icans or icans
+
+    versions = ireq.specifier.filter((c.version for c in icans), allow_pre)
+    if not allow_pre and not versions:
+        versions = ireq.specifier.filter((c.version for c in icans), True)
+
+    name = requirement.normalized_name
+    extras = requirement.extras
+    index = requirement.index
+    return [
+        _requirement_from_metadata(name, version, extras, index)
+        for version in sorted(versions)
+    ]
diff --git a/pipenv/vendor/passa/internals/dependencies.py b/pipenv/vendor/passa/internals/dependencies.py
new file mode 100644
index 00000000..8edf2fd7
--- /dev/null
+++ b/pipenv/vendor/passa/internals/dependencies.py
@@ -0,0 +1,253 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import functools
+import os
+import sys
+
+import packaging.specifiers
+import packaging.utils
+import packaging.version
+import requests
+import requirementslib
+import six
+
+from ._pip import build_wheel
+from .caches import DependencyCache, RequiresPythonCache
+from .markers import contains_extra, get_contained_extras, get_without_extra
+from .utils import is_pinned
+
+
+DEPENDENCY_CACHE = DependencyCache()
+REQUIRES_PYTHON_CACHE = RequiresPythonCache()
+
+
+def _cached(f, **kwargs):
+
+    @functools.wraps(f)
+    def wrapped(ireq):
+        result = f(ireq, **kwargs)
+        if result is not None and is_pinned(ireq):
+            deps, requires_python = result
+            DEPENDENCY_CACHE[ireq] = deps
+            REQUIRES_PYTHON_CACHE[ireq] = requires_python
+        return result
+
+    return wrapped
+
+
+def _is_cache_broken(line, parent_name):
+    dep_req = requirementslib.Requirement.from_line(line)
+    if contains_extra(dep_req.markers):
+        return True     # The "extra =" marker breaks everything.
+    elif dep_req.normalized_name == parent_name:
+        return True     # A package cannot depend on itself.
+    return False
+
+
+def _get_dependencies_from_cache(ireq):
+    """Retrieves dependencies for the requirement from the dependency cache.
+    """
+    if os.environ.get("PASSA_IGNORE_LOCAL_CACHE"):
+        return
+    if ireq.editable:
+        return
+    try:
+        deps = DEPENDENCY_CACHE[ireq]
+        pyrq = REQUIRES_PYTHON_CACHE[ireq]
+    except KeyError:
+        return
+
+    # Preserving sanity: Run through the cache and make sure every entry if
+    # valid. If this fails, something is wrong with the cache. Drop it.
+    try:
+        packaging.specifiers.SpecifierSet(pyrq)
+        ireq_name = packaging.utils.canonicalize_name(ireq.name)
+        if any(_is_cache_broken(line, ireq_name) for line in deps):
+            broken = True
+        else:
+            broken = False
+    except Exception:
+        broken = True
+
+    if broken:
+        print("dropping broken cache for {0}".format(ireq.name))
+        del DEPENDENCY_CACHE[ireq]
+        del REQUIRES_PYTHON_CACHE[ireq]
+        return
+
+    return deps, pyrq
+
+
+def _get_dependencies_from_json_url(url, session):
+    response = session.get(url)
+    response.raise_for_status()
+    info = response.json()["info"]
+
+    requires_python = info["requires_python"] or ""
+    try:
+        requirement_lines = info["requires_dist"]
+    except KeyError:
+        requirement_lines = info["requires"]
+
+    # The JSON API return null for empty requirements, for some reason, so we
+    # can't just pass it into the comprehension.
+    if not requirement_lines:
+        return [], requires_python
+
+    dependencies = [
+        dep_req.as_line(include_hashes=False) for dep_req in (
+            requirementslib.Requirement.from_line(line)
+            for line in requirement_lines
+        )
+        if not contains_extra(dep_req.markers)
+    ]
+    return dependencies, requires_python
+
+
+def _get_dependencies_from_json(ireq, sources):
+    """Retrieves dependencies for the install requirement from the JSON API.
+
+    :param ireq: A single InstallRequirement
+    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`
+    :return: A set of dependency lines for generating new InstallRequirements.
+    :rtype: set(str) or None
+    """
+    if os.environ.get("PASSA_IGNORE_JSON_API"):
+        return
+    if ireq.editable:
+        return
+
+    # It is technically possible to parse extras out of the JSON API's
+    # requirement format, but it is such a chore let's just use the simple API.
+    if ireq.extras:
+        return
+
+    url_prefixes = [
+        proc_url[:-7]   # Strip "/simple".
+        for proc_url in (
+            raw_url.rstrip("/")
+            for raw_url in (source.get("url", "") for source in sources)
+        )
+        if proc_url.endswith("/simple")
+    ]
+
+    session = requests.session()
+    version = str(ireq.specifier).lstrip("=")
+
+    for prefix in url_prefixes:
+        url = "{prefix}/pypi/{name}/{version}/json".format(
+            prefix=prefix,
+            name=packaging.utils.canonicalize_name(ireq.name),
+            version=version,
+        )
+        try:
+            dependencies = _get_dependencies_from_json_url(url, session)
+            if dependencies is not None:
+                return dependencies
+        except Exception as e:
+            pass
+        print("unable to read dependencies via {0}".format(url))
+    return
+
+
+def _read_requirements(metadata, extras):
+    """Read wheel metadata to know what it depends on.
+
+    The `run_requires` attribute contains a list of dict or str specifying
+    requirements. For dicts, it may contain an "extra" key to specify these
+    requirements are for a specific extra. Unfortunately, not all fields are
+    specificed like this (I don't know why); some are specified with markers.
+    So we jump though these terrible hoops to know exactly what we need.
+
+    The extra extraction is not comprehensive. Tt assumes the marker is NEVER
+    something like `extra == "foo" and extra == "bar"`. I guess this never
+    makes sense anyway? Markers are just terrible.
+    """
+    extras = extras or ()
+    requirements = []
+    for entry in metadata.run_requires:
+        if isinstance(entry, six.text_type):
+            entry = {"requires": [entry]}
+            extra = None
+        else:
+            extra = entry.get("extra")
+        if extra is not None and extra not in extras:
+            continue
+        for line in entry.get("requires", []):
+            r = requirementslib.Requirement.from_line(line)
+            if r.markers:
+                contained = get_contained_extras(r.markers)
+                if (contained and not any(e in contained for e in extras)):
+                    continue
+                marker = get_without_extra(r.markers)
+                r.markers = str(marker) if marker else None
+                line = r.as_line(include_hashes=False)
+            requirements.append(line)
+    return requirements
+
+
+def _read_requires_python(metadata):
+    """Read wheel metadata to know the value of Requires-Python.
+
+    This is surprisingly poorly supported in Distlib. This function tries
+    several ways to get this information:
+
+    * Metadata 2.0: metadata.dictionary.get("requires_python") is not None
+    * Metadata 2.1: metadata._legacy.get("Requires-Python") is not None
+    * Metadata 1.2: metadata._legacy.get("Requires-Python") != "UNKNOWN"
+    """
+    # TODO: Support more metadata formats.
+    value = metadata.dictionary.get("requires_python")
+    if value is not None:
+        return value
+    if metadata._legacy:
+        value = metadata._legacy.get("Requires-Python")
+        if value is not None and value != "UNKNOWN":
+            return value
+    return ""
+
+
+def _get_dependencies_from_pip(ireq, sources):
+    """Retrieves dependencies for the requirement from pip internals.
+
+    The current strategy is to build a wheel out of the ireq, and read metadata
+    out of it.
+    """
+    wheel = build_wheel(ireq, sources)
+    extras = ireq.extras or ()
+    requirements = _read_requirements(wheel.metadata, extras)
+    requires_python = _read_requires_python(wheel.metadata)
+    return requirements, requires_python
+
+
+def get_dependencies(requirement, sources):
+    """Get all dependencies for a given install requirement.
+
+    :param requirement: A requirement
+    :param sources: Pipfile-formatted sources
+    :type sources: list[dict]
+    """
+    getters = [
+        _get_dependencies_from_cache,
+        _cached(_get_dependencies_from_json, sources=sources),
+        _cached(_get_dependencies_from_pip, sources=sources),
+    ]
+    ireq = requirement.as_ireq()
+    last_exc = None
+    for getter in getters:
+        try:
+            result = getter(ireq)
+        except Exception as e:
+            last_exc = sys.exc_info()
+            continue
+        if result is not None:
+            deps, pyreq = result
+            reqs = [requirementslib.Requirement.from_line(d) for d in deps]
+            return reqs, pyreq
+    if last_exc:
+        six.reraise(*last_exc)
+    raise RuntimeError("failed to get dependencies for {}".format(
+        requirement.as_line(),
+    ))
diff --git a/pipenv/vendor/passa/internals/hashes.py b/pipenv/vendor/passa/internals/hashes.py
new file mode 100644
index 00000000..fe049274
--- /dev/null
+++ b/pipenv/vendor/passa/internals/hashes.py
@@ -0,0 +1,61 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import contextlib
+
+from pip_shims import Wheel
+
+
+def _wheel_supported(self, tags=None):
+    # Ignore current platform. Support everything.
+    return True
+
+
+def _wheel_support_index_min(self, tags=None):
+    # All wheels are equal priority for sorting.
+    return 0
+
+
+@contextlib.contextmanager
+def _allow_all_wheels():
+    """Monkey patch pip.Wheel to allow all wheels
+
+    The usual checks against platforms and Python versions are ignored to allow
+    fetching all available entries in PyPI. This also saves the candidate cache
+    and set a new one, or else the results from the previous non-patched calls
+    will interfere.
+    """
+    original_wheel_supported = Wheel.supported
+    original_support_index_min = Wheel.support_index_min
+
+    Wheel.supported = _wheel_supported
+    Wheel.support_index_min = _wheel_support_index_min
+    yield
+    Wheel.supported = original_wheel_supported
+    Wheel.support_index_min = original_support_index_min
+
+
+def get_hashes(cache, req):
+    if req.is_vcs:
+        return set()
+
+    ireq = req.as_ireq()
+
+    if ireq.editable:
+        return set()
+
+    if req.is_file_or_url:
+        # TODO: Get the hash of the linked artifact?
+        return set()
+
+    if not ireq.is_pinned:
+        return set()
+
+    with _allow_all_wheels():
+        matching_candidates = req.find_all_matches()
+
+    return {
+        cache.get_hash(candidate.location)
+        for candidate in matching_candidates
+    }
diff --git a/pipenv/vendor/passa/internals/lockers.py b/pipenv/vendor/passa/internals/lockers.py
new file mode 100644
index 00000000..b1b842f4
--- /dev/null
+++ b/pipenv/vendor/passa/internals/lockers.py
@@ -0,0 +1,197 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import itertools
+
+import plette
+import requirementslib
+import resolvelib
+import vistir
+
+from passa import reporters
+
+from .caches import HashCache
+from .hashes import get_hashes
+from .metadata import set_metadata
+from .providers import BasicProvider, EagerUpgradeProvider, PinReuseProvider
+from .traces import trace_graph
+from .utils import identify_requirment
+
+
+def _get_requirements(model, section_name):
+    """Produce a mapping of identifier: requirement from the section.
+    """
+    if not model:
+        return {}
+    return {identify_requirment(r): r for r in (
+        requirementslib.Requirement.from_pipfile(name, package._data)
+        for name, package in model.get(section_name, {}).items()
+    )}
+
+
+def _collect_derived_entries(state, traces, identifiers):
+    """Produce a mapping containing all candidates derived from `identifiers`.
+
+    `identifiers` should provide a collection of requirement identifications
+    from a section (i.e. `packages` or `dev-packages`). This function uses
+    `trace` to filter out candidates in the state that are present because of
+    an entry in that collection.
+    """
+    identifiers = set(identifiers)
+    if not identifiers:
+        return {}
+
+    entries = {}
+    extras = {}
+    for identifier, requirement in state.mapping.items():
+        routes = {trace[1] for trace in traces[identifier] if len(trace) > 1}
+        if identifier not in identifiers and not (identifiers & routes):
+            continue
+        name = requirement.normalized_name
+        if requirement.extras:
+            # Aggregate extras from multiple routes so we can produce their
+            # union in the lock file. (sarugaku/passa#24)
+            try:
+                extras[name].extend(requirement.extras)
+            except KeyError:
+                extras[name] = list(requirement.extras)
+        entries[name] = next(iter(requirement.as_pipfile().values()))
+    for name, ext in extras.items():
+        entries[name]["extras"] = ext
+
+    return entries
+
+
+class AbstractLocker(object):
+    """Helper class to produce a new lock file for a project.
+
+    This is not intended for instantiation. You should use one of its concrete
+    subclasses instead. The class contains logic to:
+
+    * Prepare a project for locking
+    * Perform the actually resolver invocation
+    * Convert resolver output into lock file format
+    * Update the project to have the new lock file
+    """
+    def __init__(self, project):
+        self.project = project
+        self.default_requirements = _get_requirements(
+            project.pipfile, "packages",
+        )
+        self.develop_requirements = _get_requirements(
+            project.pipfile, "dev-packages",
+        )
+
+        # This comprehension dance ensures we merge packages from both
+        # sections, and definitions in the default section win.
+        self.requirements = {k: r for k, r in itertools.chain(
+            self.develop_requirements.items(),
+            self.default_requirements.items(),
+        )}.values()
+
+        self.sources = [s._data.copy() for s in project.pipfile.sources]
+        self.allow_prereleases = bool(
+            project.pipfile.get("pipenv", {}).get("allow_prereleases", False),
+        )
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self.project.root)
+
+    def get_provider(self):
+        raise NotImplementedError
+
+    def lock(self):
+        """Lock specified (abstract) requirements into (concrete) candidates.
+
+        The locking procedure consists of four stages:
+
+        * Resolve versions and dependency graph (powered by ResolveLib).
+        * Walk the graph to determine "why" each candidate came to be, i.e.
+          what top-level requirements result in a given candidate.
+        * Populate hashes for resolved candidates.
+        * Populate markers based on dependency specifications of each
+          candidate, and the dependency graph.
+        """
+        reporters.report("lock-starting", {"requirements": self.requirements})
+
+        provider = self.get_provider()
+        resolver = resolvelib.Resolver(
+            provider, reporters.get_reporter().build_for_resolvelib(),
+        )
+
+        with vistir.cd(self.project.root):
+            state = resolver.resolve(self.requirements)
+
+        traces = trace_graph(state.graph)
+        reporters.report("lock-trace-ended", {
+            "state": state, "traces": traces,
+        })
+
+        hash_cache = HashCache()
+        for r in state.mapping.values():
+            if not r.hashes:
+                r.hashes = get_hashes(hash_cache, r)
+
+        set_metadata(
+            state.mapping, traces,
+            provider.fetched_dependencies, provider.requires_pythons,
+        )
+
+        lockfile = plette.Lockfile.with_meta_from(self.project.pipfile)
+        lockfile["default"] = _collect_derived_entries(
+            state, traces, self.default_requirements,
+        )
+        lockfile["develop"] = _collect_derived_entries(
+            state, traces, self.develop_requirements,
+        )
+        self.project.lockfile = lockfile
+
+
+class BasicLocker(AbstractLocker):
+    """Basic concrete locker.
+
+    This takes a project, generates a lock file from its Pipfile, and sets
+    the lock file property to the project.
+    """
+    def get_provider(self):
+        return BasicProvider(
+            self.requirements, self.sources, self.allow_prereleases,
+        )
+
+
+class PinReuseLocker(AbstractLocker):
+    """A specialized locker to handle re-locking based on existing pins.
+
+    See :class:`.providers.PinReuseProvider` for more information.
+    """
+    def __init__(self, project):
+        super(PinReuseLocker, self).__init__(project)
+        pins = _get_requirements(project.lockfile, "develop")
+        pins.update(_get_requirements(project.lockfile, "default"))
+        for pin in pins.values():
+            pin.markers = None
+        self.preferred_pins = pins
+
+    def get_provider(self):
+        return PinReuseProvider(
+            self.preferred_pins,
+            self.requirements, self.sources, self.allow_prereleases,
+        )
+
+
+class EagerUpgradeLocker(PinReuseLocker):
+    """A specialized locker to handle the "eager" upgrade strategy.
+
+    See :class:`.providers.EagerUpgradeProvider` for more
+    information.
+    """
+    def __init__(self, tracked_names, *args, **kwargs):
+        super(EagerUpgradeLocker, self).__init__(*args, **kwargs)
+        self.tracked_names = tracked_names
+
+    def get_provider(self):
+        return EagerUpgradeProvider(
+            self.tracked_names, self.preferred_pins,
+            self.requirements, self.sources, self.allow_prereleases,
+        )
diff --git a/pipenv/vendor/passa/internals/markers.py b/pipenv/vendor/passa/internals/markers.py
new file mode 100644
index 00000000..95efab95
--- /dev/null
+++ b/pipenv/vendor/passa/internals/markers.py
@@ -0,0 +1,101 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+from packaging.markers import Marker
+
+
+def _strip_extra(elements):
+    """Remove the "extra == ..." operands from the list.
+
+    This is not a comprehensive implementation, but relies on an important
+    characteristic of metadata generation: The "extra == ..." operand is always
+    associated with an "and" operator. This means that we can simply remove the
+    operand and the "and" operator associated with it.
+    """
+    extra_indexes = []
+    for i, element in enumerate(elements):
+        if isinstance(element, list):
+            cancelled = _strip_extra(element)
+            if cancelled:
+                extra_indexes.append(i)
+        elif isinstance(element, tuple) and element[0].value == "extra":
+            extra_indexes.append(i)
+    for i in reversed(extra_indexes):
+        del elements[i]
+        if i > 0 and elements[i - 1] == "and":
+            # Remove the "and" before it.
+            del elements[i - 1]
+        elif elements:
+            # This shouldn't ever happen, but is included for completeness.
+            # If there is not an "and" before this element, try to remove the
+            # operator after it.
+            del elements[0]
+    return (not elements)
+
+
+def get_without_extra(marker):
+    """Build a new marker without the `extra == ...` part.
+
+    The implementation relies very deep into packaging's internals, but I don't
+    have a better way now (except implementing the whole thing myself).
+
+    This could return `None` if the `extra == ...` part is the only one in the
+    input marker.
+    """
+    # TODO: Why is this very deep in the internals? Why is a better solution
+    # implementing it yourself when someone is already maintaining a codebase
+    # for this? It's literally a grammar implementation that is required to
+    # meet the demands of a pep... -d
+    if not marker:
+        return None
+    marker = Marker(str(marker))
+    elements = marker._markers
+    _strip_extra(elements)
+    if elements:
+        return marker
+    return None
+
+
+def _markers_collect_extras(markers, collection):
+    # Optimization: the marker element is usually appended at the end.
+    for el in reversed(markers):
+        if (isinstance(el, tuple) and
+                el[0].value == "extra" and
+                el[1].value == "=="):
+            collection.add(el[2].value)
+        elif isinstance(el, list):
+            _markers_collect_extras(el, collection)
+
+
+def get_contained_extras(marker):
+    """Collect "extra == ..." operands from a marker.
+
+    Returns a list of str. Each str is a speficied extra in this marker.
+    """
+    if not marker:
+        return set()
+    marker = Marker(str(marker))
+    extras = set()
+    _markers_collect_extras(marker._markers, extras)
+    return extras
+
+
+def _markers_contains_extra(markers):
+    # Optimization: the marker element is usually appended at the end.
+    for element in reversed(markers):
+        if isinstance(element, tuple) and element[0].value == "extra":
+            return True
+        elif isinstance(element, list):
+            if _markers_contains_extra(element):
+                return True
+    return False
+
+
+def contains_extra(marker):
+    """Check whehter a marker contains an "extra == ..." operand.
+    """
+    if not marker:
+        return False
+    marker = Marker(str(marker))
+    return _markers_contains_extra(marker._markers)
diff --git a/pipenv/vendor/passa/internals/metadata.py b/pipenv/vendor/passa/internals/metadata.py
new file mode 100644
index 00000000..9709c535
--- /dev/null
+++ b/pipenv/vendor/passa/internals/metadata.py
@@ -0,0 +1,169 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import copy
+import itertools
+
+import packaging.markers
+import packaging.specifiers
+import vistir
+import vistir.misc
+
+from .markers import get_without_extra
+from .specifiers import cleanup_pyspecs, pyspec_from_markers
+
+
+def dedup_markers(s):
+    # TODO: Implement better logic.
+    deduped = sorted(vistir.misc.dedup(s))
+    return deduped
+
+
+class MetaSet(object):
+    """Representation of a "metadata set".
+
+    This holds multiple metadata representaions. Each metadata representation
+    includes a marker, and a specifier set of Python versions required.
+    """
+    def __init__(self):
+        self.markerset = frozenset()
+        self.pyspecset = packaging.specifiers.SpecifierSet()
+
+    def __repr__(self):
+        return "MetaSet(markerset={0!r}, pyspecset={1!r})".format(
+            ",".join(sorted(self.markerset)), str(self.pyspecset),
+        )
+
+    def __str__(self):
+        pyspecs = set()
+        markerset = set()
+        for m in self.markerset:
+            marker_specs = pyspec_from_markers(packaging.markers.Marker(m))
+            if marker_specs:
+                pyspecs.add(marker_specs)
+            else:
+                markerset.add(m)
+        if pyspecs:
+            self.pyspecset._specs &= pyspecs
+            self.markerset = frozenset(markerset)
+        return " and ".join(dedup_markers(itertools.chain(
+            # Make sure to always use the same quotes so we can dedup properly.
+            (
+                "{0}".format(ms) if " or " in ms else ms
+                for ms in (str(m).replace('"', "'") for m in self.markerset)
+            ),
+            (
+                "python_version {0[0]} '{0[1]}'".format(spec)
+                for spec in cleanup_pyspecs(self.pyspecset)
+            ),
+        )))
+
+    def __bool__(self):
+        return bool(self.markerset or self.pyspecset)
+
+    def __nonzero__(self):  # Python 2.
+        return self.__bool__()
+
+    def __or__(self, pair):
+        marker, specset = pair
+        markerset = set(self.markerset)
+        if marker:
+            marker_specs = pyspec_from_markers(marker)
+            if not marker_specs:
+                markerset.add(str(marker))
+            else:
+                specset._specs &= marker_specs
+        metaset = MetaSet()
+        metaset.markerset = frozenset(markerset)
+        # TODO: Implement some logic to clean up dups like '3.0.*' and '3.0'.
+        metaset.pyspecset &= self.pyspecset & specset
+        return metaset
+
+
+def _build_metasets(dependencies, pythons, key, trace, all_metasets):
+    all_parent_metasets = []
+    for route in trace:
+        parent = route[-1]
+        try:
+            parent_metasets = all_metasets[parent]
+        except KeyError:    # Parent not calculated yet. Wait for it.
+            return
+        all_parent_metasets.append((parent, parent_metasets))
+
+    metaset_iters = []
+    for parent, parent_metasets in all_parent_metasets:
+        r = dependencies[parent][key]
+        python = pythons[key]
+        metaset = (
+            get_without_extra(r.markers),
+            packaging.specifiers.SpecifierSet(python),
+        )
+        metaset_iters.append(
+            parent_metaset | metaset
+            for parent_metaset in parent_metasets
+        )
+    return list(itertools.chain.from_iterable(metaset_iters))
+
+
+def _calculate_metasets_mapping(dependencies, pythons, traces):
+    all_metasets = {None: [MetaSet()]}
+
+    del traces[None]
+    while traces:
+        new_metasets = {}
+        for key, trace in traces.items():
+            assert key not in all_metasets, key     # Sanity check for debug.
+            metasets = _build_metasets(
+                dependencies, pythons, key, trace, all_metasets,
+            )
+            if metasets is None:
+                continue
+            new_metasets[key] = metasets
+        if not new_metasets:
+            break   # No progress? Deadlocked. Give up.
+        all_metasets.update(new_metasets)
+        for key in new_metasets:
+            del traces[key]
+
+    return all_metasets
+
+
+def _format_metasets(metasets):
+    # If there is an unconditional route, this needs to be unconditional.
+    if not metasets or not all(metasets):
+        return None
+
+    # This extra str(Marker()) call helps simplify the expression.
+    return str(packaging.markers.Marker(" or ".join(
+        "{0}".format(s) if " and " in s else s
+        for s in dedup_markers(str(metaset) for metaset in metasets
+        if metaset)
+    )))
+
+
+def set_metadata(candidates, traces, dependencies, pythons):
+    """Add "metadata" to candidates based on the dependency tree.
+
+    Metadata for a candidate includes markers and a specifier for Python
+    version requirements.
+
+    :param candidates: A key-candidate mapping. Candidates in the mapping will
+        have their markers set.
+    :param traces: A graph trace (produced by `traces.trace_graph`) providing
+        information about dependency relationships between candidates.
+    :param dependencies: A key-collection mapping containing what dependencies
+        each candidate in `candidates` requested.
+    :param pythons: A key-str mapping containing Requires-Python information
+        of each candidate.
+
+    Keys in mappings and entries in the trace are identifiers of a package, as
+    implemented by the `identify` method of the resolver's provider.
+
+    The candidates are modified in-place.
+    """
+    metasets_mapping = _calculate_metasets_mapping(
+        dependencies, pythons, copy.deepcopy(traces),
+    )
+    for key, candidate in candidates.items():
+        candidate.markers = _format_metasets(metasets_mapping[key])
diff --git a/pipenv/vendor/passa/internals/providers.py b/pipenv/vendor/passa/internals/providers.py
new file mode 100644
index 00000000..73418392
--- /dev/null
+++ b/pipenv/vendor/passa/internals/providers.py
@@ -0,0 +1,184 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import os
+
+import resolvelib
+
+from .candidates import find_candidates
+from .dependencies import get_dependencies
+from .utils import filter_sources, identify_requirment, strip_extras
+
+
+PROTECTED_PACKAGE_NAMES = {"pip", "setuptools"}
+
+
+class BasicProvider(resolvelib.AbstractProvider):
+    """Provider implementation to interface with `requirementslib.Requirement`.
+    """
+    def __init__(self, root_requirements, sources, allow_prereleases):
+        self.sources = sources
+        self.allow_prereleases = bool(allow_prereleases)
+        self.invalid_candidates = set()
+
+        # Remember requirements of each pinned candidate. The resolver calls
+        # `get_dependencies()` only when it wants to repin, so the last time
+        # the dependencies we got when it is last called on a package, are
+        # the set used by the resolver. We use this later to trace how a given
+        # dependency is specified by a package.
+        self.fetched_dependencies = {None: {
+            self.identify(r): r for r in root_requirements
+        }}
+        # TODO: Find a way to resolve with multiple versions (by tricking
+        # runtime) Include multiple keys in pipfiles?
+        self.requires_pythons = {None: ""}  # TODO: Don't use any value
+
+    def identify(self, dependency):
+        return identify_requirment(dependency)
+
+    def get_preference(self, resolution, candidates, information):
+        # TODO: Provide better sorting logic. This simply resolve the ones with
+        # less choices first. Not sophisticated, but sounds reasonable?
+        return len(candidates)
+
+    def find_matches(self, requirement):
+        # TODO: Implement per-package prereleases flag. (pypa/pipenv#1696)
+        allow_prereleases = self.allow_prereleases
+        sources = filter_sources(requirement, self.sources)
+        candidates = find_candidates(requirement, sources, allow_prereleases)
+        return candidates
+
+    def is_satisfied_by(self, requirement, candidate):
+        # A non-named requirement has exactly one candidate, as implemented in
+        # `find_matches()`. It must match.
+        if not requirement.is_named:
+            return True
+
+        # Optimization: Everything matches if there are no specifiers.
+        if not requirement.specifiers:
+            return True
+
+        # We can't handle old version strings before PEP 440. Drop them all.
+        # Practically this shouldn't be a problem if the user is specifying a
+        # remotely reasonable dependency not from before 2013.
+        candidate_line = candidate.as_line()
+        if candidate_line in self.invalid_candidates:
+            return False
+        try:
+            version = candidate.get_specifier().version
+        except ValueError:
+            print('ignoring invalid version {}'.format(candidate_line))
+            self.invalid_candidates.add(candidate_line)
+            return False
+
+        return requirement.as_ireq().specifier.contains(version)
+
+    def get_dependencies(self, candidate):
+        sources = filter_sources(candidate, self.sources)
+        try:
+            dependencies, requires_python = get_dependencies(
+                candidate, sources=sources,
+            )
+        except Exception as e:
+            if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                raise
+            print("failed to get dependencies for {0!r}: {1}".format(
+                candidate.as_line(include_hashes=False), e,
+            ))
+            dependencies = []
+            requires_python = ""
+        # Exclude protected packages from the list. This prevents those
+        # packages from being locked, unless the user is actually working on
+        # them, and explicitly lists them as top-level requirements -- those
+        # packages are not added via this code path. (sarugaku/passa#15)
+        dependencies = [
+            dependency for dependency in dependencies
+            if dependency.normalized_name not in PROTECTED_PACKAGE_NAMES
+        ]
+        if candidate.extras:
+            # HACK: If this candidate has extras, add the original candidate
+            # (same pinned version, no extras) as its dependency. This ensures
+            # the same package with different extras (treated as distinct by
+            # the resolver) have the same version. (sarugaku/passa#4)
+            dependencies.append(strip_extras(candidate))
+        candidate_key = self.identify(candidate)
+        self.fetched_dependencies[candidate_key] = {
+            self.identify(r): r for r in dependencies
+        }
+        self.requires_pythons[candidate_key] = requires_python
+        return dependencies
+
+
+class PinReuseProvider(BasicProvider):
+    """A provider that reuses preferred pins if possible.
+
+    This is used to implement "add", "remove", and "only-if-needed upgrade",
+    where already-pinned candidates in Pipfile.lock should be preferred.
+    """
+    def __init__(self, preferred_pins, *args, **kwargs):
+        super(PinReuseProvider, self).__init__(*args, **kwargs)
+        self.preferred_pins = preferred_pins
+
+    def find_matches(self, requirement):
+        candidates = super(PinReuseProvider, self).find_matches(requirement)
+        try:
+            # Add the preferred pin. Remember the resolve prefer candidates
+            # at the end of the list, so the most preferred should be last.
+            candidates.append(self.preferred_pins[self.identify(requirement)])
+        except KeyError:
+            pass
+        return candidates
+
+
+class EagerUpgradeProvider(PinReuseProvider):
+    """A specialized provider to handle an "eager" upgrade strategy.
+
+    An eager upgrade tries to upgrade not only packages specified, but also
+    their dependencies (recursively). This contrasts to the "only-if-needed"
+    default, which only promises to upgrade the specified package, and
+    prevents touching anything else if at all possible.
+
+    The provider is implemented as to keep track of all dependencies of the
+    specified packages to upgrade, and free their pins when it has a chance.
+    """
+    def __init__(self, tracked_names, *args, **kwargs):
+        super(EagerUpgradeProvider, self).__init__(*args, **kwargs)
+        self.tracked_names = set(tracked_names)
+        for name in tracked_names:
+            self.preferred_pins.pop(name, None)
+
+        # HACK: Set this special flag to distinguish preferred pins from
+        # regular, to tell the resolver to NOT use them for tracked packages.
+        for pin in self.preferred_pins.values():
+            pin._preferred_by_provider = True
+
+    def is_satisfied_by(self, requirement, candidate):
+        # If this is a tracking package, tell the resolver out of using the
+        # preferred pin, and into a "normal" candidate selection process.
+        if (self.identify(requirement) in self.tracked_names and
+                getattr(candidate, "_preferred_by_provider", False)):
+            return False
+        return super(EagerUpgradeProvider, self).is_satisfied_by(
+            requirement, candidate,
+        )
+
+    def get_dependencies(self, candidate):
+        # If this package is being tracked for upgrade, remove pins of its
+        # dependencies, and start tracking these new packages.
+        dependencies = super(EagerUpgradeProvider, self).get_dependencies(
+            candidate,
+        )
+        if self.identify(candidate) in self.tracked_names:
+            for dependency in dependencies:
+                name = self.identify(dependency)
+                self.tracked_names.add(name)
+                self.preferred_pins.pop(name, None)
+        return dependencies
+
+    def get_preference(self, resolution, candidates, information):
+        # Resolve tracking packages so we have a chance to unpin them first.
+        name = self.identify(candidates[0])
+        if name in self.tracked_names:
+            return -1
+        return len(candidates)
diff --git a/pipenv/vendor/passa/internals/specifiers.py b/pipenv/vendor/passa/internals/specifiers.py
new file mode 100644
index 00000000..75afb6ad
--- /dev/null
+++ b/pipenv/vendor/passa/internals/specifiers.py
@@ -0,0 +1,136 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import itertools
+import operator
+
+from packaging.specifiers import SpecifierSet, Specifier
+from vistir.misc import dedup
+
+
+def _tuplize_version(version):
+    return tuple(int(x) for x in version.split("."))
+
+
+def _format_version(version):
+    return ".".join(str(i) for i in version)
+
+
+# Prefer [x,y) ranges.
+REPLACE_RANGES = {">": ">=", "<=": "<"}
+
+
+def _format_pyspec(specifier):
+    if isinstance(specifier, str):
+        if not any(op in specifier for op in Specifier._operators.keys()):
+            specifier = "=={0}".format(specifier)
+        specifier = Specifier(specifier)
+    if specifier.operator == "==" and specifier.version.endswith(".*"):
+        specifier = Specifier("=={0}".format(specifier.version[:-2]))
+    try:
+        op = REPLACE_RANGES[specifier.operator]
+    except KeyError:
+        return specifier
+    version = specifier.version.replace(".*", "")
+    curr_tuple = _tuplize_version(version)
+    try:
+        next_tuple = (curr_tuple[0], curr_tuple[1] + 1)
+    except IndexError:
+        next_tuple = (curr_tuple[0], 1)
+    specifier = Specifier("{0}{1}".format(op, _format_version(next_tuple)))
+    return specifier
+
+
+def _get_specs(specset):
+    if isinstance(specset, Specifier):
+        specset = str(specset)
+    if isinstance(specset, str):
+        specset = SpecifierSet(specset.replace(".*", ""))
+    return [
+        (spec._spec[0], _tuplize_version(spec._spec[1]))
+        for spec in getattr(specset, "_specs", [])
+    ]
+
+
+def _group_by_op(specs):
+    specs = [_get_specs(x) for x in list(specs)]
+    flattened = [(op, version) for spec in specs for op, version in spec]
+    specs = sorted(flattened, key=operator.itemgetter(1))
+    grouping = itertools.groupby(specs, key=operator.itemgetter(0))
+    return grouping
+
+
+def cleanup_pyspecs(specs, joiner="or"):
+    specs = {_format_pyspec(spec) for spec in specs}
+    # for != operator we want to group by version
+    # if all are consecutive, join as a list
+    results = set()
+    for op, versions in _group_by_op(specs):
+        versions = [version[1] for version in versions]
+        versions = sorted(dedup(versions))
+        # if we are doing an or operation, we need to use the min for >=
+        # this way OR(>=2.6, >=2.7, >=3.6) picks >=2.6
+        # if we do an AND operation we need to use MAX to be more selective
+        if op in (">", ">="):
+            if joiner == "or":
+                results.add((op, _format_version(min(versions))))
+            else:
+                results.add((op, _format_version(max(versions))))
+        # we use inverse logic here so we will take the max value if we are
+        # using OR but the min value if we are using AND
+        elif op in ("<=", "<"):
+            if joiner == "or":
+                results.add((op, _format_version(max(versions))))
+            else:
+                results.add((op, _format_version(min(versions))))
+        # leave these the same no matter what operator we use
+        elif op in ("!=", "==", "~="):
+            version_list = sorted(
+                "{0}".format(_format_version(version))
+                for version in versions
+            )
+            version = ", ".join(version_list)
+            if len(version_list) == 1:
+                results.add((op, version))
+            elif op == "!=":
+                results.add(("not in", version))
+            elif op == "==":
+                results.add(("in", version))
+            else:
+                specifier = SpecifierSet(",".join(sorted(
+                    "{0}".format(op, v) for v in version_list
+                )))._specs
+                for s in specifier:
+                    results &= (specifier._spec[0], specifier._spec[1])
+        else:
+            if len(version) == 1:
+                results.add((op, version))
+            else:
+                specifier = SpecifierSet("{0}".format(version))._specs
+                for s in specifier:
+                    results |= (specifier._spec[0], specifier._spec[1])
+    return results
+
+
+def pyspec_from_markers(marker):
+    if marker._markers[0][0] != 'python_version':
+        return
+    op = marker._markers[0][1].value
+    version = marker._markers[0][2].value
+    specset = set()
+    if op == "in":
+        specset.update(
+            Specifier("=={0}".format(v.strip()))
+            for v in version.split(",")
+        )
+    elif op == "not in":
+        specset.update(
+            Specifier("!={0}".format(v.strip()))
+            for v in version.split(",")
+        )
+    else:
+        specset.add(Specifier("".join([op, version])))
+    if specset:
+        return specset
+    return None
diff --git a/pipenv/vendor/passa/internals/synchronizers.py b/pipenv/vendor/passa/internals/synchronizers.py
new file mode 100644
index 00000000..2a1f1d18
--- /dev/null
+++ b/pipenv/vendor/passa/internals/synchronizers.py
@@ -0,0 +1,214 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import collections
+import contextlib
+import os
+import sys
+import sysconfig
+
+import pkg_resources
+
+import packaging.markers
+import packaging.version
+import requirementslib
+
+from ._pip import uninstall_requirement, EditableInstaller, WheelInstaller
+
+
+def _is_installation_local(name):
+    """Check whether the distribution is in the current Python installation.
+
+    This is used to distinguish packages seen by a virtual environment. A venv
+    may be able to see global packages, but we don't want to mess with them.
+    """
+    location = pkg_resources.working_set.by_key[name].location
+    return os.path.commonprefix([location, sys.prefix]) == sys.prefix
+
+
+def _is_up_to_date(distro, version):
+    # This is done in strings to avoid type mismatches caused by vendering.
+    return str(version) == str(packaging.version.parse(distro.version))
+
+
+GroupCollection = collections.namedtuple("GroupCollection", [
+    "uptodate", "outdated", "noremove", "unneeded",
+])
+
+
+def _group_installed_names(packages):
+    """Group locally installed packages based on given specifications.
+
+    `packages` is a name-package mapping that are used as baseline to
+    determine how the installed package should be grouped.
+
+    Returns a 3-tuple of disjoint sets, all containing names of installed
+    packages:
+
+    * `uptodate`: These match the specifications.
+    * `outdated`: These installations are specified, but don't match the
+        specifications in `packages`.
+    * `unneeded`: These are installed, but not specified in `packages`.
+    """
+    groupcoll = GroupCollection(set(), set(), set(), set())
+
+    for distro in pkg_resources.working_set:
+        name = distro.key
+        try:
+            package = packages[name]
+        except KeyError:
+            groupcoll.unneeded.add(name)
+            continue
+
+        r = requirementslib.Requirement.from_pipfile(name, package)
+        if not r.is_named:
+            # Always mark non-named. I think pip does something similar?
+            groupcoll.outdated.add(name)
+        elif not _is_up_to_date(distro, r.get_version()):
+            groupcoll.outdated.add(name)
+        else:
+            groupcoll.uptodate.add(name)
+
+    return groupcoll
+
+
+@contextlib.contextmanager
+def _remove_package(name):
+    if name is None or not _is_installation_local(name):
+        yield
+        return
+    r = requirementslib.Requirement.from_line(name)
+    with uninstall_requirement(r.as_ireq(), auto_confirm=True, verbose=False):
+        yield
+
+
+def _get_packages(lockfile, default, develop):
+    # Don't need to worry about duplicates because only extras can differ.
+    # Extras don't matter because they only affect dependencies, and we
+    # don't install dependencies anyway!
+    packages = {}
+    if default:
+        packages.update(lockfile.default._data)
+    if develop:
+        packages.update(lockfile.develop._data)
+    return packages
+
+
+def _build_paths():
+    """Prepare paths for distlib.wheel.Wheel to install into.
+    """
+    paths = sysconfig.get_paths()
+    return {
+        "prefix": sys.prefix,
+        "data": paths["data"],
+        "scripts": paths["scripts"],
+        "headers": paths["include"],
+        "purelib": paths["purelib"],
+        "platlib": paths["platlib"],
+    }
+
+
+PROTECTED_FROM_CLEAN = {"setuptools", "pip"}
+
+
+def _clean(names):
+    cleaned = set()
+    for name in names:
+        if name in PROTECTED_FROM_CLEAN:
+            continue
+        with _remove_package(name):
+            pass
+        cleaned.add(name)
+    return cleaned
+
+
+class Synchronizer(object):
+    """Helper class to install packages from a project's lock file.
+    """
+    def __init__(self, project, default, develop, clean_unneeded):
+        self._root = project.root   # Only for repr.
+        self.packages = _get_packages(project.lockfile, default, develop)
+        self.sources = project.lockfile.meta.sources._data
+        self.paths = _build_paths()
+        self.clean_unneeded = clean_unneeded
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
+
+    def sync(self):
+        groupcoll = _group_installed_names(self.packages)
+
+        installed = set()
+        updated = set()
+        cleaned = set()
+
+        # TODO: Show a prompt to confirm cleaning. We will need to implement a
+        # reporter pattern for this as well.
+        if self.clean_unneeded:
+            names = _clean(groupcoll.unneeded)
+            cleaned.update(names)
+
+        # TODO: Specify installation order? (pypa/pipenv#2274)
+        installers = []
+        for name, package in self.packages.items():
+            r = requirementslib.Requirement.from_pipfile(name, package)
+            name = r.normalized_name
+            if name in groupcoll.uptodate:
+                continue
+            markers = r.markers
+            if markers and not packaging.markers.Marker(markers).evaluate():
+                continue
+            r.markers = None
+            if r.editable:
+                installer = EditableInstaller(r)
+            else:
+                installer = WheelInstaller(r, self.sources, self.paths)
+            try:
+                installer.prepare()
+            except Exception as e:
+                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                    raise
+                print("failed to prepare {0!r}: {1}".format(
+                    r.as_line(include_hashes=False), e,
+                ))
+            else:
+                installers.append((name, installer))
+
+        for name, installer in installers:
+            if name in groupcoll.outdated:
+                name_to_remove = name
+            else:
+                name_to_remove = None
+            try:
+                with _remove_package(name_to_remove):
+                    installer.install()
+            except Exception as e:
+                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                    raise
+                print("failed to install {0!r}: {1}".format(
+                    r.as_line(include_hashes=False), e,
+                ))
+                continue
+            if name in groupcoll.outdated or name in groupcoll.noremove:
+                updated.add(name)
+            else:
+                installed.add(name)
+
+        return installed, updated, cleaned
+
+
+class Cleaner(object):
+    """Helper class to clean packages not in a project's lock file.
+    """
+    def __init__(self, project, default, develop):
+        self._root = project.root   # Only for repr.
+        self.packages = _get_packages(project.lockfile, default, develop)
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
+
+    def clean(self):
+        groupcoll = _group_installed_names(self.packages)
+        _clean(groupcoll.unneeded)
+        return groupcoll.unneeded
diff --git a/pipenv/vendor/passa/internals/traces.py b/pipenv/vendor/passa/internals/traces.py
new file mode 100644
index 00000000..9715db97
--- /dev/null
+++ b/pipenv/vendor/passa/internals/traces.py
@@ -0,0 +1,40 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+
+def _trace_visit_vertex(graph, current, target, visited, path, paths):
+    if current == target:
+        paths.append(path)
+        return
+    for v in graph.iter_children(current):
+        if v == current or v in visited:
+            continue
+        next_path = path + [current]
+        next_visited = visited | {current}
+        _trace_visit_vertex(graph, v, target, next_visited, next_path, paths)
+
+
+def trace_graph(graph):
+    """Build a collection of "traces" for each package.
+
+    A trace is a list of names that eventually leads to the package. For
+    example, if A and B are root dependencies, A depends on C and D, B
+    depends on C, and C depends on D, the return value would be like::
+
+        {
+            None: [],
+            "A": [None],
+            "B": [None],
+            "C": [[None, "A"], [None, "B"]],
+            "D": [[None, "B", "C"], [None, "A"]],
+        }
+    """
+    result = {None: []}
+    for vertex in graph:
+        result[vertex] = []
+        for root in graph.iter_children(None):
+            paths = []
+            _trace_visit_vertex(graph, root, vertex, {None}, [None], paths)
+            result[vertex].extend(paths)
+    return result
diff --git a/pipenv/vendor/passa/internals/utils.py b/pipenv/vendor/passa/internals/utils.py
new file mode 100644
index 00000000..d23a10c7
--- /dev/null
+++ b/pipenv/vendor/passa/internals/utils.py
@@ -0,0 +1,106 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+
+def identify_requirment(r):
+    """Produce an identifier for a requirement to use in the resolver.
+
+    Note that we are treating the same package with different extras as
+    distinct. This allows semantics like "I only want this extra in
+    development, not production".
+
+    This also makes the resolver's implementation much simpler, with the minor
+    costs of possibly needing a few extra resolution steps if we happen to have
+    the same package apprearing multiple times.
+    """
+    return "{0}{1}".format(r.normalized_name, r.extras_as_pip)
+
+
+def get_pinned_version(ireq):
+    """Get the pinned version of an InstallRequirement.
+
+    An InstallRequirement is considered pinned if:
+
+    - Is not editable
+    - It has exactly one specifier
+    - That specifier is "=="
+    - The version does not contain a wildcard
+
+    Examples:
+        django==1.8   # pinned
+        django>1.8    # NOT pinned
+        django~=1.8   # NOT pinned
+        django==1.*   # NOT pinned
+
+    Raises `TypeError` if the input is not a valid InstallRequirement, or
+    `ValueError` if the InstallRequirement is not pinned.
+    """
+    try:
+        specifier = ireq.specifier
+    except AttributeError:
+        raise TypeError("Expected InstallRequirement, not {}".format(
+            type(ireq).__name__,
+        ))
+
+    if ireq.editable:
+        raise ValueError("InstallRequirement is editable")
+    if not specifier:
+        raise ValueError("InstallRequirement has no version specification")
+    if len(specifier._specs) != 1:
+        raise ValueError("InstallRequirement has multiple specifications")
+
+    op, version = next(iter(specifier._specs))._spec
+    if op not in ('==', '===') or version.endswith('.*'):
+        raise ValueError("InstallRequirement not pinned (is {0!r})".format(
+            op + version,
+        ))
+
+    return version
+
+
+def is_pinned(ireq):
+    """Returns whether an InstallRequirement is a "pinned" requirement.
+
+    An InstallRequirement is considered pinned if:
+
+    - Is not editable
+    - It has exactly one specifier
+    - That specifier is "=="
+    - The version does not contain a wildcard
+
+    Examples:
+        django==1.8   # pinned
+        django>1.8    # NOT pinned
+        django~=1.8   # NOT pinned
+        django==1.*   # NOT pinned
+    """
+    try:
+        get_pinned_version(ireq)
+    except (TypeError, ValueError):
+        return False
+    return True
+
+
+def filter_sources(requirement, sources):
+    """Returns a filtered list of sources for this requirement.
+
+    This considers the index specified by the requirement, and returns only
+    matching source entries if there is at least one.
+    """
+    if not sources or not requirement.index:
+        return sources
+    filtered_sources = [
+        source for source in sources
+        if source.get("name") == requirement.index
+    ]
+    return filtered_sources or sources
+
+
+def strip_extras(requirement):
+    """Returns a new requirement object with extras removed.
+    """
+    line = requirement.as_line()
+    new = type(requirement).from_line(line)
+    new.extras = None
+    return new
diff --git a/pipenv/vendor/passa/lockers.py b/pipenv/vendor/passa/lockers.py
new file mode 100644
index 00000000..4ab4cc3b
--- /dev/null
+++ b/pipenv/vendor/passa/lockers.py
@@ -0,0 +1,182 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import itertools
+
+import plette
+import requirementslib
+import resolvelib
+import vistir
+
+from .caches import HashCache
+from .hashes import get_hashes
+from .metadata import set_metadata
+from .providers import BasicProvider, EagerUpgradeProvider, PinReuseProvider
+from .reporters import StdOutReporter
+from .traces import trace_graph
+from .utils import identify_requirment
+
+
+def _get_requirements(model, section_name):
+    """Produce a mapping of identifier: requirement from the section.
+    """
+    if not model:
+        return {}
+    return {identify_requirment(r): r for r in (
+        requirementslib.Requirement.from_pipfile(name, package._data)
+        for name, package in model.get(section_name, {}).items()
+    )}
+
+
+def _iter_derived_entries(state, traces, names):
+    """Produce a mapping containing all candidates derived from `names`.
+
+    `name` should provide a collection of requirement identifications from
+    a section (i.e. `packages` or `dev-packages`). This function uses `trace`
+    to filter out candidates in the state that are present because of an entry
+    in that collection.
+    """
+    if not names:
+        return
+    names = set(names)
+    for name, requirement in state.mapping.items():
+        routes = {trace[1] for trace in traces[name] if len(trace) > 1}
+        if name not in names and not (names & routes):
+            continue
+        yield (
+            requirement.normalized_name,
+            next(iter(requirement.as_pipfile().values()))
+        )
+
+
+class AbstractLocker(object):
+    """Helper class to produce a new lock file for a project.
+
+    This is not intended for instantiation. You should use one of its concrete
+    subclasses instead. The class contains logic to:
+
+    * Prepare a project for locking
+    * Perform the actually resolver invocation
+    * Convert resolver output into lock file format
+    * Update the project to have the new lock file
+    """
+    def __init__(self, project):
+        self.project = project
+        self.default_requirements = _get_requirements(
+            project.pipfile, "packages",
+        )
+        self.develop_requirements = _get_requirements(
+            project.pipfile, "dev-packages",
+        )
+
+        # This comprehension dance ensures we merge packages from both
+        # sections, and definitions in the default section win.
+        self.requirements = {k: r for k, r in itertools.chain(
+            self.develop_requirements.items(),
+            self.default_requirements.items(),
+        )}.values()
+
+        self.sources = [s._data.copy() for s in project.pipfile.sources]
+        self.allow_prereleases = bool(
+            project.pipfile.get("pipenv", {}).get("allow_prereleases", False),
+        )
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self.project.root)
+
+    def get_provider(self):
+        raise NotImplementedError
+
+    def get_reporter(self):
+        # TODO: Build SpinnerReporter, and use this only in verbose mode.
+        return StdOutReporter(self.requirements)
+
+    def lock(self):
+        """Lock specified (abstract) requirements into (concrete) candidates.
+
+        The locking procedure consists of four stages:
+
+        * Resolve versions and dependency graph (powered by ResolveLib).
+        * Walk the graph to determine "why" each candidate came to be, i.e.
+          what top-level requirements result in a given candidate.
+        * Populate hashes for resolved candidates.
+        * Populate markers based on dependency specifications of each
+          candidate, and the dependency graph.
+        """
+        provider = self.get_provider()
+        reporter = self.get_reporter()
+        resolver = resolvelib.Resolver(provider, reporter)
+
+        with vistir.cd(self.project.root):
+            state = resolver.resolve(self.requirements)
+
+        traces = trace_graph(state.graph)
+
+        hash_cache = HashCache()
+        for r in state.mapping.values():
+            if not r.hashes:
+                r.hashes = get_hashes(hash_cache, r)
+
+        set_metadata(
+            state.mapping, traces,
+            provider.fetched_dependencies, provider.requires_pythons,
+        )
+
+        lockfile = plette.Lockfile.with_meta_from(self.project.pipfile)
+        lockfile["default"] = dict(_iter_derived_entries(
+            state, traces, self.default_requirements,
+        ))
+        lockfile["develop"] = dict(_iter_derived_entries(
+            state, traces, self.develop_requirements,
+        ))
+        self.project.lockfile = lockfile
+
+
+class BasicLocker(AbstractLocker):
+    """Basic concrete locker.
+
+    This takes a project, generates a lock file from its Pipfile, and sets
+    the lock file property to the project.
+    """
+    def get_provider(self):
+        return BasicProvider(
+            self.requirements, self.sources, self.allow_prereleases,
+        )
+
+
+class PinReuseLocker(AbstractLocker):
+    """A specialized locker to handle re-locking based on existing pins.
+
+    See :class:`passa.providers.PinReuseProvider` for more information.
+    """
+    def __init__(self, project):
+        super(PinReuseLocker, self).__init__(project)
+        pins = _get_requirements(project.lockfile, "develop")
+        pins.update(_get_requirements(project.lockfile, "default"))
+        for pin in pins.values():
+            pin.markers = None
+        self.preferred_pins = pins
+
+    def get_provider(self):
+        return PinReuseProvider(
+            self.preferred_pins,
+            self.requirements, self.sources, self.allow_prereleases,
+        )
+
+
+class EagerUpgradeLocker(PinReuseLocker):
+    """A specialized locker to handle the "eager" upgrade strategy.
+
+    See :class:`passa.providers.EagerUpgradeProvider` for more
+    information.
+    """
+    def __init__(self, tracked_names, *args, **kwargs):
+        super(EagerUpgradeLocker, self).__init__(*args, **kwargs)
+        self.tracked_names = tracked_names
+
+    def get_provider(self):
+        return EagerUpgradeProvider(
+            self.tracked_names, self.preferred_pins,
+            self.requirements, self.sources, self.allow_prereleases,
+        )
diff --git a/pipenv/vendor/passa/locking.py b/pipenv/vendor/passa/locking.py
new file mode 100644
index 00000000..e4b6ced5
--- /dev/null
+++ b/pipenv/vendor/passa/locking.py
@@ -0,0 +1,105 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import itertools
+
+from plette import Lockfile
+from requirementslib import Requirement
+from resolvelib import Resolver
+
+from .caches import HashCache
+from .hashes import get_hashes
+from .metadata import set_metadata
+from .providers import RequirementsLibProvider
+from .reporters import StdOutReporter
+from .traces import trace_graph
+from .utils import identify_requirment
+
+
+def resolve_requirements(requirements, sources, pins, allow_pre):
+    """Lock specified (abstract) requirements into (concrete) candidates.
+
+    The locking procedure consists of four stages:
+
+    * Resolve versions and dependency graph (powered by ResolveLib).
+    * Walk the graph to determine "why" each candidate came to be, i.e. what
+      top-level requirements result in a given candidate.
+    * Populate hashes for resolved candidates.
+    * Populate markers based on dependency specifications of each candidate,
+      and the dependency graph.
+    """
+    provider = RequirementsLibProvider(requirements, sources, pins, allow_pre)
+    reporter = StdOutReporter(requirements)
+    resolver = Resolver(provider, reporter)
+
+    state = resolver.resolve(requirements)
+    traces = trace_graph(state.graph)
+
+    hash_cache = HashCache()
+    for r in state.mapping.values():
+        if not r.hashes:
+            r.hashes = get_hashes(hash_cache, r)
+
+    set_metadata(
+        state.mapping, traces,
+        provider.fetched_dependencies, provider.requires_pythons,
+    )
+    return state, traces
+
+
+def _get_requirements(pipfile, section_name):
+    """Produce a mapping of identifier: requirement from the section.
+    """
+    return {identify_requirment(r): r for r in (
+        Requirement.from_pipfile(name, package._data)
+        for name, package in pipfile.get(section_name, {}).items()
+    )}
+
+
+def _get_derived_entries(state, traces, names):
+    """Produce a mapping containing all candidates derived from `names`.
+
+    `name` should provide a collection of requirement identifications from
+    a section (i.e. `packages` or `dev-packages`). This function uses `trace`
+    to filter out candidates in the state that are present because of an entry
+    in that collection.
+    """
+    if not names:
+        return {}
+    return_map = {}
+    for req_name_from_state, req in state.mapping.items():
+        req_traces = [trace[1] for trace in traces[req_name_from_state] if len(trace) > 1]
+        if req_name_from_state in names or len(set(names) & set(req_traces)):
+            return_map[req.normalized_name] = next(iter(req.as_pipfile().values()))
+    return return_map
+
+
+def build_lockfile(pipfile, lockfile):
+    default_reqs = _get_requirements(pipfile, "packages")
+    develop_reqs = _get_requirements(pipfile, "dev-packages")
+
+    pins = {}
+    if lockfile:
+        pins = _get_requirements(lockfile, "develop")
+        pins.update(_get_requirements(lockfile, "default"))
+
+    # This comprehension dance ensures we merge packages from both
+    # sections, and definitions in the default section win.
+    requirements = {k: r for k, r in itertools.chain(
+        develop_reqs.items(), default_reqs.items(),
+    )}.values()
+
+    sources = [s._data.copy() for s in pipfile.sources]
+    try:
+        allow_prereleases = bool(pipfile["pipenv"]["allow_prereleases"])
+    except (KeyError, TypeError):
+        allow_prereleases = False
+    state, traces = resolve_requirements(
+        requirements, sources, pins, allow_prereleases,
+    )
+
+    new_lock = Lockfile.with_meta_from(pipfile)
+    new_lock["default"] = _get_derived_entries(state, traces, default_reqs)
+    new_lock["develop"] = _get_derived_entries(state, traces, develop_reqs)
+    return new_lock
diff --git a/pipenv/vendor/passa/markers.py b/pipenv/vendor/passa/markers.py
new file mode 100644
index 00000000..5f6f37d6
--- /dev/null
+++ b/pipenv/vendor/passa/markers.py
@@ -0,0 +1,228 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import itertools
+import operator
+
+import vistir
+
+from packaging.specifiers import SpecifierSet, Specifier
+from packaging.markers import Marker
+
+
+PYTHON_BOUNDARIES = {2: 7, 3: 9}
+
+
+def _strip_extra(elements):
+    """Remove the "extra == ..." operands from the list.
+
+    This is not a comprehensive implementation, but relies on an important
+    characteristic of metadata generation: The "extra == ..." operand is always
+    associated with an "and" operator. This means that we can simply remove the
+    operand and the "and" operator associated with it.
+    """
+    extra_indexes = []
+    for i, element in enumerate(elements):
+        if isinstance(element, list):
+            cancelled = _strip_extra(element)
+            if cancelled:
+                extra_indexes.append(i)
+        elif isinstance(element, tuple) and element[0].value == "extra":
+            extra_indexes.append(i)
+    for i in reversed(extra_indexes):
+        del elements[i]
+        if i > 0 and elements[i - 1] == "and":
+            # Remove the "and" before it.
+            del elements[i - 1]
+        elif elements:
+            # This shouldn't ever happen, but is included for completeness.
+            # If there is not an "and" before this element, try to remove the
+            # operator after it.
+            del elements[0]
+    return (not elements)
+
+
+def get_without_extra(marker):
+    """Build a new marker without the `extra == ...` part.
+
+    The implementation relies very deep into packaging's internals, but I don't
+    have a better way now (except implementing the whole thing myself).
+
+    This could return `None` if the `extra == ...` part is the only one in the
+    input marker.
+    """
+    # TODO: Why is this very deep in the internals? Why is a better solution
+    # implementing it yourself when someone is already maintaining a codebase
+    # for this? It's literally a grammar implementation that is required to
+    # meet the demands of a pep... -d
+    if not marker:
+        return None
+    marker = Marker(str(marker))
+    elements = marker._markers
+    _strip_extra(elements)
+    if elements:
+        return marker
+    return None
+
+
+def _markers_collect_extras(markers, collection):
+    # Optimization: the marker element is usually appended at the end.
+    for el in reversed(markers):
+        if (isinstance(el, tuple) and
+                el[0].value == "extra" and
+                el[1].value == "=="):
+            collection.add(el[2].value)
+        elif isinstance(el, list):
+            _markers_collect_extras(el, collection)
+
+
+def get_contained_extras(marker):
+    """Collect "extra == ..." operands from a marker.
+
+    Returns a list of str. Each str is a speficied extra in this marker.
+    """
+    if not marker:
+        return set()
+    marker = Marker(str(marker))
+    extras = set()
+    _markers_collect_extras(marker._markers, extras)
+    return extras
+
+
+def _markers_contains_extra(markers):
+    # Optimization: the marker element is usually appended at the end.
+    for element in reversed(markers):
+        if isinstance(element, tuple) and element[0].value == "extra":
+            return True
+        elif isinstance(element, list):
+            if _markers_contains_extra(element):
+                return True
+    return False
+
+
+def contains_extra(marker):
+    """Check whehter a marker contains an "extra == ..." operand.
+    """
+    if not marker:
+        return False
+    marker = Marker(str(marker))
+    return _markers_contains_extra(marker._markers)
+
+
+def format_pyspec(specifier):
+    if isinstance(specifier, str):
+        if not any(operator in specifier for operator in Specifier._operators.keys()):
+            new_op = "=="
+            new_version = specifier
+            return Specifier("{0}{1}".format(new_op, new_version))
+    version = specifier._coerce_version(specifier.version.replace(".*", ""))
+    version_tuple = version._version.release
+    if specifier.operator in (">", "<="):
+        # Prefer to always pick the operator for version n+1
+        if version_tuple[1] < PYTHON_BOUNDARIES.get(version_tuple[0], 0):
+            if specifier.operator == ">":
+                new_op = ">="
+            else:
+                new_op = "<"
+            new_version = (version_tuple[0], version_tuple[1] + 1)
+            specifier = Specifier("{0}{1}".format(new_op, version_to_str(new_version)))
+    return specifier
+
+
+def make_version_tuple(version):
+    return tuple([int(x) for x in version.split(".")])
+
+
+def version_to_str(version):
+    return ".".join([str(i) for i in version])
+
+
+def get_specs(specset):
+    if isinstance(specset, Specifier):
+        specset = str(specset)
+    if isinstance(specset, str):
+        specset = SpecifierSet(specset.replace(".*", ""))
+
+    specs = getattr(specset, "_specs", None)
+    return [(spec._spec[0], make_version_tuple(spec._spec[1])) for spec in list(specs)]
+
+
+def group_by_version(versions):
+    versions = sorted(map(lambda x: make_version_tuple(x)))
+    grouping = itertools.groupby(versions, key=operator.itemgetter(0))
+    return grouping
+
+
+def group_by_op(specs):
+    specs = [get_specs(x) for x in list(specs)]
+    flattened = [(op, version) for spec in specs for op, version in spec]
+    specs = sorted(flattened, key=operator.itemgetter(1))
+    grouping = itertools.groupby(specs, key=operator.itemgetter(0))
+    return grouping
+
+
+def marker_to_spec(marker):
+    if marker._markers[0][0] != 'python_version':
+        return
+    operator = marker._markers[0][1].value
+    version = marker._markers[0][2].value
+    specset = set()
+    if operator in ("in", "not in"):
+        op = "==" if operator == "in" else "!="
+        specset |= set([Specifier("{0}{1}".format(op, v.strip())) for v in version.split(",")])
+    else:
+        spec = Specifier("".join([operator, version]))
+        specset.add(spec)
+    if specset:
+        return specset
+    return None
+
+
+def cleanup_specs(specs, operator="or"):
+    specs = {format_pyspec(spec) for spec in specs}
+    # for != operator we want to group by version
+    # if all are consecutive, join as a list
+    results = set()
+    for op, versions in group_by_op(specs):
+        versions = [version[1] for version in versions]
+        versions = sorted(vistir.misc.dedup(versions))
+        # if we are doing an or operation, we need to use the min for >=
+        # this way OR(>=2.6, >=2.7, >=3.6) picks >=2.6
+        # if we do an AND operation we need to use MAX to be more selective
+        if op in (">", ">="):
+            if operator == "or":
+                results.add((op, version_to_str(min(versions))))
+            else:
+                results.add((op, version_to_str(max(versions))))
+        # we use inverse logic here so we will take the max value if we are using OR
+        # but the min value if we are using AND
+        elif op in ("<=", "<"):
+            if operator == "or":
+                results.add((op, version_to_str(max(versions))))
+            else:
+                results.add((op, version_to_str(min(versions))))
+        # leave these the same no matter what operator we use
+        elif op in ("!=", "==", "~="):
+            version_list = sorted(["{0}".format(version_to_str(version)) for version in versions])
+            version = ", ".join(version_list)
+            if len(version_list) == 1:
+                results.add((op, version))
+            else:
+                if op == "!=":
+                    results.add(("not in", version))
+                elif op == "==":
+                    results.add(("in", version))
+                else:
+                    version = ", ".join(sorted(["{0}".format(op, v) for v in version_list]))
+                    specifier = SpecifierSet(version)._specs
+                    for s in specifier:
+                        results &= (specifier._spec[0], specifier._spec[1])
+        else:
+            if len(version) == 1:
+                results.add((op, version))
+            else:
+                specifier = SpecifierSet("{0}".format(version))._specs
+                for s in specifier:
+                    results |= (specifier._spec[0], specifier._spec[1])
+    return results
diff --git a/pipenv/vendor/passa/metadata.py b/pipenv/vendor/passa/metadata.py
new file mode 100644
index 00000000..312691a1
--- /dev/null
+++ b/pipenv/vendor/passa/metadata.py
@@ -0,0 +1,169 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import copy
+import itertools
+
+import packaging.markers
+import packaging.specifiers
+import vistir
+import vistir.misc
+
+from .markers import get_without_extra, cleanup_specs, marker_to_spec
+
+
+def dedup_markers(s):
+    # TODO: Implement better logic.
+    deduped = sorted(vistir.misc.dedup(s))
+    return deduped
+
+
+class MetaSet(object):
+    """Representation of a "metadata set".
+
+    This holds multiple metadata representaions. Each metadata representation
+    includes a marker, and a specifier set of Python versions required.
+    """
+    def __init__(self):
+        self.markerset = frozenset()
+        self.pyspecset = packaging.specifiers.SpecifierSet()
+
+    def __repr__(self):
+        return "MetaSet(markerset={0!r}, pyspecset={1!r})".format(
+            ",".join(sorted(self.markerset)), str(self.pyspecset),
+        )
+
+    def __str__(self):
+        pyspecs = set()
+        markerset = set()
+        for m in self.markerset:
+            py_marker = marker_to_spec(packaging.markers.Marker(m))
+            if py_marker:
+                pyspecs.add(py_marker)
+            else:
+                markerset.add(m)
+        if pyspecs:
+            self.pyspecset._specs &= pyspecs
+            self.markerset = frozenset(markerset)
+        return " and ".join(dedup_markers(itertools.chain(
+            # Make sure to always use the same quotes so we can dedup properly.
+            (
+                "{0}".format(ms) if " or " in ms else ms
+                for ms in (str(m).replace('"', "'") for m in self.markerset)
+            ),
+            (
+                "python_version {0[0]} '{0[1]}'".format(spec)
+                for spec in cleanup_specs(self.pyspecset)
+            ),
+        )))
+
+    def __bool__(self):
+        return bool(self.markerset or self.pyspecset)
+
+    def __nonzero__(self):  # Python 2.
+        return self.__bool__()
+
+    def __or__(self, pair):
+        marker, specset = pair
+        markerset = set(self.markerset)
+        pyspec_markers = set()
+        if marker:
+            pyspec_markers = marker_to_spec(marker)
+            if not pyspec_markers:
+                markerset.add(str(marker))
+            else:
+                specset._specs &= pyspec_markers
+        metaset = MetaSet()
+        metaset.markerset = frozenset(markerset)
+        # TODO: Implement some logic to clean up dups like '3.0.*' and '3.0'.
+        metaset.pyspecset &= self.pyspecset & specset
+        return metaset
+
+
+def _build_metasets(dependencies, pythons, key, trace, all_metasets):
+    all_parent_metasets = []
+    for route in trace:
+        parent = route[-1]
+        try:
+            parent_metasets = all_metasets[parent]
+        except KeyError:    # Parent not calculated yet. Wait for it.
+            return
+        all_parent_metasets.append((parent, parent_metasets))
+
+    metaset_iters = []
+    for parent, parent_metasets in all_parent_metasets:
+        r = dependencies[parent][key]
+        python = pythons[key]
+        metaset = (
+            get_without_extra(r.markers),
+            packaging.specifiers.SpecifierSet(python),
+        )
+        metaset_iters.append(
+            parent_metaset | metaset
+            for parent_metaset in parent_metasets
+        )
+    return list(itertools.chain.from_iterable(metaset_iters))
+
+
+def _calculate_metasets_mapping(dependencies, pythons, traces):
+    all_metasets = {None: [MetaSet()]}
+
+    del traces[None]
+    while traces:
+        new_metasets = {}
+        for key, trace in traces.items():
+            assert key not in all_metasets, key     # Sanity check for debug.
+            metasets = _build_metasets(
+                dependencies, pythons, key, trace, all_metasets,
+            )
+            if metasets is None:
+                continue
+            new_metasets[key] = metasets
+        if not new_metasets:
+            break   # No progress? Deadlocked. Give up.
+        all_metasets.update(new_metasets)
+        for key in new_metasets:
+            del traces[key]
+
+    return all_metasets
+
+
+def _format_metasets(metasets):
+    # If there is an unconditional route, this needs to be unconditional.
+    if not metasets or not all(metasets):
+        return None
+
+    # This extra str(Marker()) call helps simplify the expression.
+    return str(packaging.markers.Marker(" or ".join(
+        "{0}".format(s) if " and " in s else s
+        for s in dedup_markers(str(metaset) for metaset in metasets
+        if metaset)
+    )))
+
+
+def set_metadata(candidates, traces, dependencies, pythons):
+    """Add "metadata" to candidates based on the dependency tree.
+
+    Metadata for a candidate includes markers and a specifier for Python
+    version requirements.
+
+    :param candidates: A key-candidate mapping. Candidates in the mapping will
+        have their markers set.
+    :param traces: A graph trace (produced by `traces.trace_graph`) providing
+        information about dependency relationships between candidates.
+    :param dependencies: A key-collection mapping containing what dependencies
+        each candidate in `candidates` requested.
+    :param pythons: A key-str mapping containing Requires-Python information
+        of each candidate.
+
+    Keys in mappings and entries in the trace are identifiers of a package, as
+    implemented by the `identify` method of the resolver's provider.
+
+    The candidates are modified in-place.
+    """
+    metasets_mapping = _calculate_metasets_mapping(
+        dependencies, pythons, copy.deepcopy(traces),
+    )
+    for key, candidate in candidates.items():
+        candidate.markers = _format_metasets(metasets_mapping[key])
diff --git a/pipenv/vendor/passa/operations/__init__.py b/pipenv/vendor/passa/operations/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/pipenv/vendor/passa/operations/_utils.py b/pipenv/vendor/passa/operations/_utils.py
new file mode 100644
index 00000000..e69de29b
diff --git a/pipenv/vendor/passa/operations/lock.py b/pipenv/vendor/passa/operations/lock.py
new file mode 100644
index 00000000..a68d0b7d
--- /dev/null
+++ b/pipenv/vendor/passa/operations/lock.py
@@ -0,0 +1,28 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from resolvelib import NoVersionsAvailable, ResolutionImpossible
+
+from passa.reporters import print_requirement
+
+
+def lock(locker):
+    success = False
+    try:
+        locker.lock()
+    except NoVersionsAvailable as e:
+        print("\nCANNOT RESOLVE. NO CANDIDATES FOUND FOR:")
+        print("{:>40}".format(e.requirement.as_line(include_hashes=False)))
+        if e.parent:
+            line = e.parent.as_line(include_hashes=False)
+            print("{:>41}".format("(from {})".format(line)))
+        else:
+            print("{:>41}".format("(user)"))
+    except ResolutionImpossible as e:
+        print("\nCANNOT RESOLVE.\nOFFENDING REQUIREMENTS:")
+        for r in e.requirements:
+            print_requirement(r)
+    else:
+        success = True
+    return success
diff --git a/pipenv/vendor/passa/operations/sync.py b/pipenv/vendor/passa/operations/sync.py
new file mode 100644
index 00000000..3014e8d9
--- /dev/null
+++ b/pipenv/vendor/passa/operations/sync.py
@@ -0,0 +1,23 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+
+def sync(syncer):
+    print("Starting synchronization")
+    installed, updated, cleaned = syncer.sync()
+    if cleaned:
+        print("Uninstalled: {}".format(", ".join(sorted(cleaned))))
+    if installed:
+        print("Installed: {}".format(", ".join(sorted(installed))))
+    if updated:
+        print("Updated: {}".format(", ".join(sorted(updated))))
+    return True
+
+
+def clean(cleaner):
+    print("Cleaning")
+    cleaned = cleaner.clean()
+    if cleaned:
+        print("Uninstalled: {}".format(", ".join(sorted(cleaned))))
+    return True
diff --git a/pipenv/vendor/passa/projects.py b/pipenv/vendor/passa/projects.py
new file mode 100644
index 00000000..79e71bb1
--- /dev/null
+++ b/pipenv/vendor/passa/projects.py
@@ -0,0 +1,235 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import collections
+import io
+import os
+
+import attr
+import packaging.markers
+import packaging.utils
+import plette
+import plette.models
+import six
+import tomlkit
+
+
+SectionDifference = collections.namedtuple("SectionDifference", [
+    "inthis", "inthat",
+])
+FileDifference = collections.namedtuple("FileDifference", [
+    "default", "develop",
+])
+
+
+def _are_pipfile_entries_equal(a, b):
+    a = {k: v for k, v in a.items() if k not in ("markers", "hashes", "hash")}
+    b = {k: v for k, v in b.items() if k not in ("markers", "hashes", "hash")}
+    if a != b:
+        return False
+    try:
+        marker_eval_a = packaging.markers.Marker(a["markers"]).evaluate()
+    except (AttributeError, KeyError, TypeError, ValueError):
+        marker_eval_a = True
+    try:
+        marker_eval_b = packaging.markers.Marker(b["markers"]).evaluate()
+    except (AttributeError, KeyError, TypeError, ValueError):
+        marker_eval_b = True
+    return marker_eval_a == marker_eval_b
+
+
+DEFAULT_NEWLINES = "\n"
+
+
+def preferred_newlines(f):
+    if isinstance(f.newlines, six.text_type):
+        return f.newlines
+    return DEFAULT_NEWLINES
+
+
+@attr.s
+class ProjectFile(object):
+    """A file in the Pipfile project.
+    """
+    location = attr.ib()
+    line_ending = attr.ib()
+    model = attr.ib()
+
+    @classmethod
+    def read(cls, location, model_cls, invalid_ok=False):
+        try:
+            with io.open(location, encoding="utf-8") as f:
+                model = model_cls.load(f)
+                line_ending = preferred_newlines(f)
+        except Exception:
+            if not invalid_ok:
+                raise
+            model = None
+            line_ending = DEFAULT_NEWLINES
+        return cls(location=location, line_ending=line_ending, model=model)
+
+    def write(self):
+        kwargs = {"encoding": "utf-8", "newline": self.line_ending}
+        with io.open(self.location, "w", **kwargs) as f:
+            self.model.dump(f)
+
+    def dumps(self):
+        strio = six.StringIO()
+        self.model.dump(strio)
+        return strio.getvalue()
+
+
+@attr.s
+class Project(object):
+
+    root = attr.ib()
+    _p = attr.ib(init=False)
+    _l = attr.ib(init=False)
+
+    def __attrs_post_init__(self):
+        self.root = root = os.path.abspath(self.root)
+        self._p = ProjectFile.read(
+            os.path.join(root, "Pipfile"),
+            plette.Pipfile,
+        )
+        self._l = ProjectFile.read(
+            os.path.join(root, "Pipfile.lock"),
+            plette.Lockfile,
+            invalid_ok=True,
+        )
+
+    @property
+    def pipfile(self):
+        return self._p.model
+
+    @property
+    def pipfile_location(self):
+        return self._p.location
+
+    @property
+    def lockfile(self):
+        return self._l.model
+
+    @property
+    def lockfile_location(self):
+        return self._l.location
+
+    @lockfile.setter
+    def lockfile(self, new):
+        self._l.model = new
+
+    def is_synced(self):
+        return self.lockfile and self.lockfile.is_up_to_date(self.pipfile)
+
+    def _get_pipfile_section(self, develop, insert=True):
+        name = "dev-packages" if develop else "packages"
+        try:
+            section = self.pipfile[name]
+        except KeyError:
+            section = plette.models.PackageCollection(tomlkit.table())
+            if insert:
+                self.pipfile[name] = section
+        return section
+
+    def contains_key_in_pipfile(self, key):
+        sections = [
+            self._get_pipfile_section(develop=False, insert=False),
+            self._get_pipfile_section(develop=True, insert=False),
+        ]
+        return any(
+            (packaging.utils.canonicalize_name(name) ==
+             packaging.utils.canonicalize_name(key))
+            for section in sections
+            for name in section
+        )
+
+    def add_line_to_pipfile(self, line, develop):
+        from requirementslib import Requirement
+        requirement = Requirement.from_line(line)
+        section = self._get_pipfile_section(develop=develop)
+        key = requirement.normalized_name
+        entry = next(iter(requirement.as_pipfile().values()))
+        if isinstance(entry, dict):
+            # HACK: TOMLKit prefers to expand tables by default, but we
+            # always want inline tables here. Also tomlkit.inline_table
+            # does not have `update()`.
+            table = tomlkit.inline_table()
+            for k, v in entry.items():
+                table[k] = v
+            entry = table
+        section[key] = entry
+
+    def remove_keys_from_pipfile(self, keys, default, develop):
+        keys = {packaging.utils.canonicalize_name(key) for key in keys}
+        sections = []
+        if default:
+            sections.append(self._get_pipfile_section(
+                develop=False, insert=False,
+            ))
+        if develop:
+            sections.append(self._get_pipfile_section(
+                develop=True, insert=False,
+            ))
+        for section in sections:
+            removals = set()
+            for name in section:
+                if packaging.utils.canonicalize_name(name) in keys:
+                    removals.add(name)
+            for key in removals:
+                del section._data[key]
+
+    def remove_keys_from_lockfile(self, keys):
+        keys = {packaging.utils.canonicalize_name(key) for key in keys}
+        removed = False
+        for section_name in ("default", "develop"):
+            try:
+                section = self.lockfile[section_name]
+            except KeyError:
+                continue
+            removals = {}
+            for name in section:
+                if packaging.utils.canonicalize_name(name) in keys:
+                    removals.add(name)
+            removed = removed or bool(removals)
+            for key in removals:
+                del section._data[key]
+
+        if removed:
+            # HACK: The lock file no longer represents the Pipfile at this
+            # point. Set the hash to an arbitrary invalid value.
+            self.lockfile.meta.hash = plette.models.Hash({"__invalid__": ""})
+
+    def difference_lockfile(self, lockfile):
+        """Generate a difference between the current and given lockfiles.
+
+        Returns a 2-tuple containing differences in default in develop
+        sections.
+
+        Each element is a 2-tuple of dicts. The first, `inthis`, contains
+        entries only present in the current lockfile; the second, `inthat`,
+        contains entries only present in the given one.
+
+        If a key exists in both this and that, but the values differ, the key
+        is present in both dicts, pointing to values from each file.
+        """
+        diff_data = {
+            "default": SectionDifference({}, {}),
+            "develop": SectionDifference({}, {}),
+        }
+        for section_name, section_diff in diff_data.items():
+            this = self.lockfile[section_name]._data
+            that = lockfile[section_name]._data
+            for key, this_value in this.items():
+                try:
+                    that_value = that[key]
+                except KeyError:
+                    section_diff.inthis[key] = this_value
+                    continue
+                if not _are_pipfile_entries_equal(this_value, that_value):
+                    section_diff.inthis[key] = this_value
+                    section_diff.inthat[key] = that_value
+            for key, that_value in that.items():
+                if key not in this:
+                    section_diff.inthat[key] = that_value
+        return FileDifference(**diff_data)
diff --git a/pipenv/vendor/passa/providers.py b/pipenv/vendor/passa/providers.py
new file mode 100644
index 00000000..7dfa3306
--- /dev/null
+++ b/pipenv/vendor/passa/providers.py
@@ -0,0 +1,167 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import os
+
+import resolvelib
+
+from .candidates import find_candidates
+from .dependencies import get_dependencies
+from .utils import filter_sources, identify_requirment
+
+
+class BasicProvider(resolvelib.AbstractProvider):
+    """Provider implementation to interface with `requirementslib.Requirement`.
+    """
+    def __init__(self, root_requirements, sources, allow_prereleases):
+        self.sources = sources
+        self.allow_prereleases = bool(allow_prereleases)
+        self.invalid_candidates = set()
+
+        # Remember requirements of each pinned candidate. The resolver calls
+        # `get_dependencies()` only when it wants to repin, so the last time
+        # the dependencies we got when it is last called on a package, are
+        # the set used by the resolver. We use this later to trace how a given
+        # dependency is specified by a package.
+        self.fetched_dependencies = {None: {
+            self.identify(r): r for r in root_requirements
+        }}
+        # TODO: Find a way to resolve with multiple versions (by tricking
+        # runtime) Include multiple keys in pipfiles?
+        self.requires_pythons = {None: ""}  # TODO: Don't use any value
+
+    def identify(self, dependency):
+        return identify_requirment(dependency)
+
+    def get_preference(self, resolution, candidates, information):
+        # TODO: Provide better sorting logic. This simply resolve the ones with
+        # less choices first. Not sophisticated, but sounds reasonable?
+        return len(candidates)
+
+    def find_matches(self, requirement):
+        # TODO: Implement per-package prereleases flag. (pypa/pipenv#1696)
+        allow_prereleases = self.allow_prereleases
+        sources = filter_sources(requirement, self.sources)
+        candidates = find_candidates(requirement, sources, allow_prereleases)
+        return candidates
+
+    def is_satisfied_by(self, requirement, candidate):
+        # A non-named requirement has exactly one candidate, as implemented in
+        # `find_matches()`. It must match.
+        if not requirement.is_named:
+            return True
+
+        # Optimization: Everything matches if there are no specifiers.
+        if not requirement.specifiers:
+            return True
+
+        # We can't handle old version strings before PEP 440. Drop them all.
+        # Practically this shouldn't be a problem if the user is specifying a
+        # remotely reasonable dependency not from before 2013.
+        candidate_line = candidate.as_line()
+        if candidate_line in self.invalid_candidates:
+            return False
+        try:
+            version = candidate.get_specifier().version
+        except ValueError:
+            print('ignoring invalid version {}'.format(candidate_line))
+            self.invalid_candidates.add(candidate_line)
+            return False
+
+        return requirement.as_ireq().specifier.contains(version)
+
+    def get_dependencies(self, candidate):
+        sources = filter_sources(candidate, self.sources)
+        try:
+            dependencies, requires_python = get_dependencies(
+                candidate, sources=sources,
+            )
+        except Exception as e:
+            if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                raise
+            print("failed to get dependencies for {0!r}: {1}".format(
+                candidate.as_line(include_hashes=False), e,
+            ))
+            dependencies = []
+            requires_python = ""
+        candidate_key = self.identify(candidate)
+        self.fetched_dependencies[candidate_key] = {
+            self.identify(r): r for r in dependencies
+        }
+        self.requires_pythons[candidate_key] = requires_python
+        return dependencies
+
+
+class PinReuseProvider(BasicProvider):
+    """A provider that reuses preferred pins if possible.
+
+    This is used to implement "add", "remove", and "only-if-needed upgrade",
+    where already-pinned candidates in Pipfile.lock should be preferred.
+    """
+    def __init__(self, preferred_pins, *args, **kwargs):
+        super(PinReuseProvider, self).__init__(*args, **kwargs)
+        self.preferred_pins = preferred_pins
+
+    def find_matches(self, requirement):
+        candidates = super(PinReuseProvider, self).find_matches(requirement)
+        try:
+            # Add the preferred pin. Remember the resolve prefer candidates
+            # at the end of the list, so the most preferred should be last.
+            candidates.append(self.preferred_pins[self.identify(requirement)])
+        except KeyError:
+            pass
+        return candidates
+
+
+class EagerUpgradeProvider(PinReuseProvider):
+    """A specialized provider to handle an "eager" upgrade strategy.
+
+    An eager upgrade tries to upgrade not only packages specified, but also
+    their dependeices (recursively). This contrasts to the "only-if-needed"
+    default, which only promises to upgrade the specified package, and
+    prevents touching anything else if at all possible.
+
+    The provider is implemented as to keep track of all dependencies of the
+    specified packages to upgrade, and free their pins when it has a chance.
+    """
+    def __init__(self, tracked_names, *args, **kwargs):
+        super(EagerUpgradeProvider, self).__init__(*args, **kwargs)
+        self.tracked_names = set(tracked_names)
+        for name in tracked_names:
+            self.preferred_pins.pop(name, None)
+
+        # HACK: Set this special flag to distinguish preferred pins from
+        # regular, to tell the resolver to NOT use them for tracked packages.
+        for pin in self.preferred_pins.values():
+            pin._preferred_by_provider = True
+
+    def is_satisfied_by(self, requirement, candidate):
+        # If this is a tracking package, tell the resolver out of using the
+        # preferred pin, and into a "normal" candidate selection process.
+        if (self.identify(requirement) in self.tracked_names and
+                getattr(candidate, "_preferred_by_provider", False)):
+            return False
+        return super(EagerUpgradeProvider, self).is_satisfied_by(
+            requirement, candidate,
+        )
+
+    def get_dependencies(self, candidate):
+        # If this package is being tracked for upgrade, remove pins of its
+        # dependencies, and start tracking these new packages.
+        dependencies = super(EagerUpgradeProvider, self).get_dependencies(
+            candidate,
+        )
+        if self.identify(candidate) in self.tracked_names:
+            for dependency in dependencies:
+                name = self.identify(dependency)
+                self.tracked_names.add(name)
+                self.preferred_pins.pop(name, None)
+        return dependencies
+
+    def get_preference(self, resolution, candidates, information):
+        # Resolve tracking packages so we have a chance to unpin them first.
+        name = self.identify(candidates[0])
+        if name in self.tracked_names:
+            return -1
+        return len(candidates)
diff --git a/pipenv/vendor/passa/reporters.py b/pipenv/vendor/passa/reporters.py
new file mode 100644
index 00000000..4fe6c0b8
--- /dev/null
+++ b/pipenv/vendor/passa/reporters.py
@@ -0,0 +1,90 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import resolvelib
+
+from .traces import trace_graph
+
+
+def print_title(text):
+    print('\n{:=^84}\n'.format(text))
+
+
+def print_requirement(r, end='\n'):
+    print('{:>40}'.format(r.as_line(include_hashes=False)), end=end)
+
+
+def print_dependency(state, key):
+    print_requirement(state.mapping[key], end='')
+    parents = sorted(
+        state.graph.iter_parents(key),
+        key=lambda n: (-1, '') if n is None else (ord(n[0].lower()), n),
+    )
+    for i, p in enumerate(parents):
+        if p is None:
+            line = '(user)'
+        else:
+            line = state.mapping[p].as_line(include_hashes=False)
+        if i == 0:
+            padding = ' <= '
+        else:
+            padding = ' ' * 44
+        print('{pad}{line}'.format(pad=padding, line=line))
+
+
+class StdOutReporter(resolvelib.BaseReporter):
+    """Simple reporter that prints things to stdout.
+    """
+    def __init__(self, requirements):
+        super(StdOutReporter, self).__init__()
+        self.requirements = requirements
+
+    def starting(self):
+        self._prev = None
+        print_title(' User requirements ')
+        for r in self.requirements:
+            print_requirement(r)
+
+    def ending_round(self, index, state):
+        print_title(' Round {} '.format(index))
+        mapping = state.mapping
+        if self._prev is None:
+            difference = set(mapping.keys())
+            changed = set()
+        else:
+            difference = set(mapping.keys()) - set(self._prev.keys())
+            changed = set(
+                k for k, v in mapping.items()
+                if k in self._prev and self._prev[k] != v
+            )
+        self._prev = mapping
+
+        if difference:
+            print('New pins: ')
+            for k in difference:
+                print_dependency(state, k)
+        print()
+
+        if changed:
+            print('Changed pins:')
+            for k in changed:
+                print_dependency(state, k)
+        print()
+
+    def ending(self, state):
+        print_title(" STABLE PINS ")
+        path_lists = trace_graph(state.graph)
+        for k in sorted(state.mapping):
+            print(state.mapping[k].as_line(include_hashes=False))
+            paths = path_lists[k]
+            for path in paths:
+                if path == [None]:
+                    print('    User requirement')
+                    continue
+                print('   ', end='')
+                for v in reversed(path[1:]):
+                    line = state.mapping[v].as_line(include_hashes=False)
+                    print(' <=', line, end='')
+                print()
+        print()
diff --git a/pipenv/vendor/passa/reporters/__init__.py b/pipenv/vendor/passa/reporters/__init__.py
new file mode 100644
index 00000000..dffe5327
--- /dev/null
+++ b/pipenv/vendor/passa/reporters/__init__.py
@@ -0,0 +1,31 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from .base import BaseReporter
+
+
+_REPORTER = BaseReporter()
+
+
+def _get_stdout_reporter():
+    from .stdout import Reporter
+    return Reporter()
+
+
+def configure_reporter(name):
+    global _REPORTER
+    _REPORTER = {
+        None: BaseReporter,
+        "stdout": _get_stdout_reporter,
+    }[name]()
+
+
+def get_reporter():
+    return _REPORTER
+
+
+def report(event, context=None):
+    if context is None:
+        context = {}
+    _REPORTER.report(event, context)
diff --git a/pipenv/vendor/passa/reporters/base.py b/pipenv/vendor/passa/reporters/base.py
new file mode 100644
index 00000000..66a432cb
--- /dev/null
+++ b/pipenv/vendor/passa/reporters/base.py
@@ -0,0 +1,52 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+import resolvelib
+
+
+class ResolveLibReporter(resolvelib.BaseReporter):
+    """Implementation of a ResolveLib reporter that bridge messages.
+    """
+    def __init__(self, parent):
+        super(ResolveLibReporter, self).__init__()
+        self.parent = parent
+
+    def starting(self):
+        self.parent.report("resolvelib-starting", {"child": self})
+
+    def ending_round(self, index, state):
+        self.parent.report("resolvelib-ending-round", {
+            "child": self, "index": index, "state": state,
+        })
+
+    def ending(self, state):
+        self.parent.report("resolvelib-ending", {
+            "child": self, "state": state,
+        })
+
+
+class BaseReporter(object):
+    """Basic reporter that does nothing.
+    """
+    def build_for_resolvelib(self):
+        """Build a reporter for ResolveLib.
+        """
+        return ResolveLibReporter(self)
+
+    def report(self, event, context):
+        """Report an event.
+
+        The default behavior is to look for a "handle_EVENT" method on the
+        class to execute, or do nothing if there is no such method.
+
+        :param event: A string to indicate the event.
+        :param context: A mapping containing appropriate data for the handling
+            function.
+        """
+        handler_name = "handle_{}".format(event.replace("-", "_"))
+        try:
+            handler = getattr(self, handler_name)
+        except AttributeError:
+            return
+        handler(context or {})
diff --git a/pipenv/vendor/passa/reporters/stdout.py b/pipenv/vendor/passa/reporters/stdout.py
new file mode 100644
index 00000000..20423377
--- /dev/null
+++ b/pipenv/vendor/passa/reporters/stdout.py
@@ -0,0 +1,106 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, print_function, unicode_literals
+
+from resolvelib import NoVersionsAvailable, ResolutionImpossible
+
+from .base import BaseReporter
+
+
+def _print_title(text):
+    print('\n{:=^84}\n'.format(text))
+
+
+def _print_requirement(r, end='\n'):
+    print('{:>40}'.format(r.as_line(include_hashes=False)), end=end)
+
+
+def _print_dependency(state, key):
+    _print_requirement(state.mapping[key], end='')
+    parents = sorted(
+        state.graph.iter_parents(key),
+        key=lambda n: (-1, '') if n is None else (ord(n[0].lower()), n),
+    )
+    for i, p in enumerate(parents):
+        if p is None:
+            line = '(user)'
+        else:
+            line = state.mapping[p].as_line(include_hashes=False)
+        if i == 0:
+            padding = ' <= '
+        else:
+            padding = ' ' * 44
+        print('{pad}{line}'.format(pad=padding, line=line))
+
+
+class Reporter(BaseReporter):
+    """A reporter implementation that prints messages to stdout.
+    """
+    def handle_resolvelib_starting(self, context):
+        context["child"]._prev_mapping = None
+
+    def handle_resolvelib_ending_round(self, context):
+        _print_title(' Round {} '.format(context["index"]))
+        mapping = context["state"].mapping
+        if context["child"]._prev_mapping is None:
+            difference = set(mapping.keys())
+            changed = set()
+        else:
+            prev = context["child"]._prev_mapping
+            difference = set(mapping.keys()) - set(prev.keys())
+            changed = set(
+                k for k, v in mapping.items()
+                if k in prev and prev[k] != v
+            )
+        context["child"]._prev_mapping = mapping
+
+        if difference:
+            print('New pins: ')
+            for k in difference:
+                _print_dependency(context["state"], k)
+        print()
+
+        if changed:
+            print('Changed pins:')
+            for k in changed:
+                _print_dependency(context["state"], k)
+        print()
+
+    def handle_lock_starting(self, context):
+        _print_title(' User requirements ')
+        for r in context["requirements"]:
+            _print_requirement(r)
+
+    def handle_lock_trace_ended(self, context):
+        _print_title(" STABLE PINS ")
+        mapping = context["state"].mapping
+        for k in sorted(mapping):
+            print(mapping[k].as_line(include_hashes=False))
+            paths = context["traces"][k]
+            for path in paths:
+                if path == [None]:
+                    print('    User requirement')
+                    continue
+                print('   ', end='')
+                for v in reversed(path[1:]):
+                    line = mapping[v].as_line(include_hashes=False)
+                    print(' <=', line, end='')
+                print()
+        print()
+
+    def handle_lock_failed(self, context):
+        e = context["exception"]
+        if isinstance(e, ResolutionImpossible):
+            print("\nCANNOT RESOLVE.\nOFFENDING REQUIREMENTS:")
+            for r in e.requirements:
+                _print_requirement(r)
+        elif isinstance(e, NoVersionsAvailable):
+            print("\nCANNOT RESOLVE. NO CANDIDATES FOUND FOR:")
+            print("{:>40}".format(e.requirement.as_line(include_hashes=False)))
+            if e.parent:
+                line = e.parent.as_line(include_hashes=False)
+                print("{:>41}".format("(from {})".format(line)))
+            else:
+                print("{:>41}".format("(user)"))
+        else:
+            raise
diff --git a/pipenv/vendor/passa/synchronizers.py b/pipenv/vendor/passa/synchronizers.py
new file mode 100644
index 00000000..30fc4492
--- /dev/null
+++ b/pipenv/vendor/passa/synchronizers.py
@@ -0,0 +1,211 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+import collections
+import contextlib
+import os
+import sys
+import sysconfig
+
+import pkg_resources
+
+import packaging.markers
+import packaging.version
+import requirementslib
+
+from ._pip import uninstall_requirement, EditableInstaller, WheelInstaller
+
+
+def _is_installation_local(name):
+    """Check whether the distribution is in the current Python installation.
+
+    This is used to distinguish packages seen by a virtual environment. A venv
+    may be able to see global packages, but we don't want to mess with them.
+    """
+    location = pkg_resources.working_set.by_key[name].location
+    return os.path.commonprefix([location, sys.prefix]) == sys.prefix
+
+
+def _is_up_to_date(distro, version):
+    # This is done in strings to avoid type mismatches caused by vendering.
+    return str(version) == str(packaging.version.parse(distro.version))
+
+
+GroupCollection = collections.namedtuple("GroupCollection", [
+    "uptodate", "outdated", "noremove", "unneeded",
+])
+
+
+def _group_installed_names(packages):
+    """Group locally installed packages based on given specifications.
+
+    `packages` is a name-package mapping that are used as baseline to
+    determine how the installed package should be grouped.
+
+    Returns a 3-tuple of disjoint sets, all containing names of installed
+    packages:
+
+    * `uptodate`: These match the specifications.
+    * `outdated`: These installations are specified, but don't match the
+        specifications in `packages`.
+    * `unneeded`: These are installed, but not specified in `packages`.
+    """
+    groupcoll = GroupCollection(set(), set(), set(), set())
+
+    for distro in pkg_resources.working_set:
+        name = distro.key
+        try:
+            package = packages[name]
+        except KeyError:
+            groupcoll.unneeded.add(name)
+            continue
+
+        r = requirementslib.Requirement.from_pipfile(name, package)
+        if not r.is_named:
+            # Always mark non-named. I think pip does something similar?
+            groupcoll.outdated.add(name)
+        elif not _is_up_to_date(distro, r.get_version()):
+            groupcoll.outdated.add(name)
+        else:
+            groupcoll.uptodate.add(name)
+
+    return groupcoll
+
+
+@contextlib.contextmanager
+def _remove_package(name):
+    if name is None or not _is_installation_local(name):
+        yield
+        return
+    r = requirementslib.Requirement.from_line(name)
+    with uninstall_requirement(r.as_ireq(), auto_confirm=True, verbose=False):
+        yield
+
+
+def _get_packages(lockfile, default, develop):
+    # Don't need to worry about duplicates because only extras can differ.
+    # Extras don't matter because they only affect dependencies, and we
+    # don't install dependencies anyway!
+    packages = {}
+    if default:
+        packages.update(lockfile.default._data)
+    if develop:
+        packages.update(lockfile.develop._data)
+    return packages
+
+
+def _build_paths():
+    """Prepare paths for distlib.wheel.Wheel to install into.
+    """
+    paths = sysconfig.get_paths()
+    return {
+        "prefix": sys.prefix,
+        "data": paths["data"],
+        "scripts": paths["scripts"],
+        "headers": paths["include"],
+        "purelib": paths["purelib"],
+        "platlib": paths["platlib"],
+    }
+
+
+PROTECTED_FROM_CLEAN = {"setuptools", "pip"}
+
+
+def _clean(names):
+    for name in names:
+        if name in PROTECTED_FROM_CLEAN:
+            continue
+        with _remove_package(name):
+            pass
+
+
+class Synchronizer(object):
+    """Helper class to install packages from a project's lock file.
+    """
+    def __init__(self, project, default, develop, clean_unneeded):
+        self._root = project.root   # Only for repr.
+        self.packages = _get_packages(project.lockfile, default, develop)
+        self.sources = project.lockfile.meta.sources._data
+        self.paths = _build_paths()
+        self.clean_unneeded = clean_unneeded
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
+
+    def sync(self):
+        groupcoll = _group_installed_names(self.packages)
+
+        installed = set()
+        updated = set()
+        cleaned = set()
+
+        # TODO: Show a prompt to confirm cleaning. We will need to implement a
+        # reporter pattern for this as well.
+        if self.clean_unneeded:
+            cleaned.update(groupcoll.unneeded)
+            _clean(cleaned)
+
+        # TODO: Specify installation order? (pypa/pipenv#2274)
+        installers = []
+        for name, package in self.packages.items():
+            r = requirementslib.Requirement.from_pipfile(name, package)
+            name = r.normalized_name
+            if name in groupcoll.uptodate:
+                continue
+            markers = r.markers
+            if markers and not packaging.markers.Marker(markers).evaluate():
+                continue
+            r.markers = None
+            if r.editable:
+                installer = EditableInstaller(r)
+            else:
+                installer = WheelInstaller(r, self.sources, self.paths)
+            try:
+                installer.prepare()
+            except Exception as e:
+                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                    raise
+                print("failed to prepare {0!r}: {1}".format(
+                    r.as_line(include_hashes=False), e,
+                ))
+            else:
+                installers.append((name, installer))
+
+        for name, installer in installers:
+            if name in groupcoll.outdated:
+                name_to_remove = name
+            else:
+                name_to_remove = None
+            try:
+                with _remove_package(name_to_remove):
+                    installer.install()
+            except Exception as e:
+                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
+                    raise
+                print("failed to install {0!r}: {1}".format(
+                    r.as_line(include_hashes=False), e,
+                ))
+                continue
+            if name in groupcoll.outdated or name in groupcoll.noremove:
+                updated.add(name)
+            else:
+                installed.add(name)
+
+        return installed, updated, cleaned
+
+
+class Cleaner(object):
+    """Helper class to clean packages not in a project's lock file.
+    """
+    def __init__(self, project, default, develop):
+        self._root = project.root   # Only for repr.
+        self.packages = _get_packages(project.lockfile, default, develop)
+
+    def __repr__(self):
+        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
+
+    def clean(self):
+        groupcoll = _group_installed_names(self.packages)
+        _clean(groupcoll.unneeded)
+        return groupcoll.unneeded
diff --git a/pipenv/vendor/passa/traces.py b/pipenv/vendor/passa/traces.py
new file mode 100644
index 00000000..9715db97
--- /dev/null
+++ b/pipenv/vendor/passa/traces.py
@@ -0,0 +1,40 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+
+def _trace_visit_vertex(graph, current, target, visited, path, paths):
+    if current == target:
+        paths.append(path)
+        return
+    for v in graph.iter_children(current):
+        if v == current or v in visited:
+            continue
+        next_path = path + [current]
+        next_visited = visited | {current}
+        _trace_visit_vertex(graph, v, target, next_visited, next_path, paths)
+
+
+def trace_graph(graph):
+    """Build a collection of "traces" for each package.
+
+    A trace is a list of names that eventually leads to the package. For
+    example, if A and B are root dependencies, A depends on C and D, B
+    depends on C, and C depends on D, the return value would be like::
+
+        {
+            None: [],
+            "A": [None],
+            "B": [None],
+            "C": [[None, "A"], [None, "B"]],
+            "D": [[None, "B", "C"], [None, "A"]],
+        }
+    """
+    result = {None: []}
+    for vertex in graph:
+        result[vertex] = []
+        for root in graph.iter_children(None):
+            paths = []
+            _trace_visit_vertex(graph, root, vertex, {None}, [None], paths)
+            result[vertex].extend(paths)
+    return result
diff --git a/pipenv/vendor/passa/utils.py b/pipenv/vendor/passa/utils.py
new file mode 100644
index 00000000..1028db10
--- /dev/null
+++ b/pipenv/vendor/passa/utils.py
@@ -0,0 +1,97 @@
+# -*- coding=utf-8 -*-
+
+from __future__ import absolute_import, unicode_literals
+
+
+def identify_requirment(r):
+    """Produce an identifier for a requirement to use in the resolver.
+
+    Note that we are treating the same package with different extras as
+    distinct. This allows semantics like "I only want this extra in
+    development, not production".
+
+    This also makes the resolver's implementation much simpler, with the minor
+    costs of possibly needing a few extra resolution steps if we happen to have
+    the same package apprearing multiple times.
+    """
+    return "{0}{1}".format(r.normalized_name, r.extras_as_pip)
+
+
+def get_pinned_version(ireq):
+    """Get the pinned version of an InstallRequirement.
+
+    An InstallRequirement is considered pinned if:
+
+    - Is not editable
+    - It has exactly one specifier
+    - That specifier is "=="
+    - The version does not contain a wildcard
+
+    Examples:
+        django==1.8   # pinned
+        django>1.8    # NOT pinned
+        django~=1.8   # NOT pinned
+        django==1.*   # NOT pinned
+
+    Raises `TypeError` if the input is not a valid InstallRequirement, or
+    `ValueError` if the InstallRequirement is not pinned.
+    """
+    try:
+        specifier = ireq.specifier
+    except AttributeError:
+        raise TypeError("Expected InstallRequirement, not {}".format(
+            type(ireq).__name__,
+        ))
+
+    if ireq.editable:
+        raise ValueError("InstallRequirement is editable")
+    if not specifier:
+        raise ValueError("InstallRequirement has no version specification")
+    if len(specifier._specs) != 1:
+        raise ValueError("InstallRequirement has multiple specifications")
+
+    op, version = next(iter(specifier._specs))._spec
+    if op not in ('==', '===') or version.endswith('.*'):
+        raise ValueError("InstallRequirement not pinned (is {0!r})".format(
+            op + version,
+        ))
+
+    return version
+
+
+def is_pinned(ireq):
+    """Returns whether an InstallRequirement is a "pinned" requirement.
+
+    An InstallRequirement is considered pinned if:
+
+    - Is not editable
+    - It has exactly one specifier
+    - That specifier is "=="
+    - The version does not contain a wildcard
+
+    Examples:
+        django==1.8   # pinned
+        django>1.8    # NOT pinned
+        django~=1.8   # NOT pinned
+        django==1.*   # NOT pinned
+    """
+    try:
+        get_pinned_version(ireq)
+    except (TypeError, ValueError):
+        return False
+    return True
+
+
+def filter_sources(requirement, sources):
+    """Return a filtered list of sources for this requirement.
+
+    This considers the index specified by the requirement, and returns only
+    matching source entries if there is at least one.
+    """
+    if not sources or not requirement.index:
+        return sources
+    filtered_sources = [
+        source for source in sources
+        if source.get("name") == requirement.index
+    ]
+    return filtered_sources or sources
diff --git a/pipenv/vendor/passa/vcs.py b/pipenv/vendor/passa/vcs.py
new file mode 100644
index 00000000..23d033d3
--- /dev/null
+++ b/pipenv/vendor/passa/vcs.py
@@ -0,0 +1,37 @@
+import os
+
+from pip_shims import VcsSupport
+
+from .utils import cheesy_temporary_directory, mkdir_p
+
+
+def _obtrain_ref(vcs_obj, src_dir, name, rev=None):
+    target_dir = os.path.join(src_dir, name)
+    target_rev = vcs_obj.make_rev_options(rev)
+    if not os.path.exists(target_dir):
+        vcs_obj.obtain(target_dir)
+    if (not vcs_obj.is_commit_id_equal(target_dir, rev) and
+            not vcs_obj.is_commit_id_equal(target_dir, target_rev)):
+        vcs_obj.update(target_dir, target_rev)
+    return vcs_obj.get_revision(target_dir)
+
+
+def _get_src():
+    src = os.environ.get("PIP_SRC")
+    if src:
+        return src
+    virtual_env = os.environ.get("VIRTUAL_ENV")
+    if virtual_env:
+        return os.path.join(virtual_env, "src")
+    temp_src = cheesy_temporary_directory(prefix='passa-src')
+    return temp_src
+
+
+def set_ref(requirement):
+    backend = VcsSupport()._registry.get(requirement.vcs)
+    vcs = backend(url=requirement.req.vcs_uri)
+    src = _get_src()
+    mkdir_p(src, mode=0o775)
+    name = requirement.normalized_name
+    ref = _obtrain_ref(vcs, src, name, rev=requirement.req.ref)
+    requirement.req.ref = ref
diff --git a/pipenv/vendor/yaspin/__init__.py b/pipenv/vendor/yaspin/__init__.py
new file mode 100644
index 00000000..57853a13
--- /dev/null
+++ b/pipenv/vendor/yaspin/__init__.py
@@ -0,0 +1,10 @@
+# -*- coding: utf-8 -*-
+
+from __future__ import absolute_import
+
+from .__version__ import __version__  # noqa
+from .api import kbi_safe_yaspin, yaspin
+from .base_spinner import Spinner
+
+
+__all__ = ("yaspin", "kbi_safe_yaspin", "Spinner")
diff --git a/pipenv/vendor/yaspin/__version__.py b/pipenv/vendor/yaspin/__version__.py
new file mode 100644
index 00000000..9e78220f
--- /dev/null
+++ b/pipenv/vendor/yaspin/__version__.py
@@ -0,0 +1 @@
+__version__ = "0.14.0"
diff --git a/pipenv/vendor/yaspin/api.py b/pipenv/vendor/yaspin/api.py
new file mode 100644
index 00000000..156630db
--- /dev/null
+++ b/pipenv/vendor/yaspin/api.py
@@ -0,0 +1,88 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.api
+~~~~~~~~~~
+
+This module implements the Yaspin API.
+
+:copyright: (c) 2018 by Pavlo Dmytrenko.
+:license: MIT, see LICENSE for more details.
+"""
+
+import signal
+
+from .core import Yaspin
+from .signal_handlers import default_handler
+
+
+def yaspin(*args, **kwargs):
+    """Display spinner in stdout.
+
+    Can be used as a context manager or as a function decorator.
+
+    Arguments:
+        spinner (base_spinner.Spinner, optional): Spinner object to use.
+        text (str, optional): Text to show along with spinner.
+        color (str, optional): Spinner color.
+        on_color (str, optional): Color highlight for the spinner.
+        attrs (list, optional): Color attributes for the spinner.
+        reversal (bool, optional): Reverse spin direction.
+        side (str, optional): Place spinner to the right or left end
+            of the text string.
+        sigmap (dict, optional): Maps POSIX signals to their respective
+            handlers.
+
+    Returns:
+        core.Yaspin: instance of the Yaspin class.
+
+    Raises:
+        ValueError: If unsupported ``color`` is specified.
+        ValueError: If unsupported ``on_color`` is specified.
+        ValueError: If unsupported color attribute in ``attrs``
+            is specified.
+        ValueError: If trying to register handler for SIGKILL signal.
+        ValueError: If unsupported ``side`` is specified.
+
+    Available text colors:
+        red, green, yellow, blue, magenta, cyan, white.
+
+    Available text highlights:
+        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan,
+        on_white, on_grey.
+
+    Available attributes:
+        bold, dark, underline, blink, reverse, concealed.
+
+    Example::
+
+        # Use as a context manager
+        with yaspin():
+            some_operations()
+
+        # Context manager with text
+        with yaspin(text="Processing..."):
+            some_operations()
+
+        # Context manager with custom sequence
+        with yaspin(Spinner('-\\|/', 150)):
+            some_operations()
+
+        # As decorator
+        @yaspin(text="Loading...")
+        def foo():
+            time.sleep(5)
+
+        foo()
+
+    """
+    return Yaspin(*args, **kwargs)
+
+
+def kbi_safe_yaspin(*args, **kwargs):
+    kwargs["sigmap"] = {signal.SIGINT: default_handler}
+    return Yaspin(*args, **kwargs)
+
+
+_kbi_safe_doc = yaspin.__doc__.replace("yaspin", "kbi_safe_yaspin")
+kbi_safe_yaspin.__doc__ = _kbi_safe_doc
diff --git a/pipenv/vendor/yaspin/base_spinner.py b/pipenv/vendor/yaspin/base_spinner.py
new file mode 100644
index 00000000..537ff799
--- /dev/null
+++ b/pipenv/vendor/yaspin/base_spinner.py
@@ -0,0 +1,16 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.base_spinner
+~~~~~~~~~~~~~~~~~~~
+
+Spinner class, used to construct other spinners.
+"""
+
+from __future__ import absolute_import
+
+from collections import namedtuple
+
+
+Spinner = namedtuple("Spinner", "frames interval")
+default_spinner = Spinner("⠋⠙⠹⠸⠼⠴⠦⠧⠇⠏", 80)
diff --git a/pipenv/vendor/yaspin/compat.py b/pipenv/vendor/yaspin/compat.py
new file mode 100644
index 00000000..744de5a1
--- /dev/null
+++ b/pipenv/vendor/yaspin/compat.py
@@ -0,0 +1,33 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.compat
+~~~~~~~~~~~~~
+
+Compatibility layer.
+"""
+
+import sys
+
+
+PY2 = sys.version_info[0] == 2
+
+
+if PY2:
+    builtin_str = str
+    bytes = str
+    str = unicode  # noqa
+    basestring = basestring  # noqa
+
+    def iteritems(dct):
+        return dct.iteritems()
+
+
+else:
+    builtin_str = str
+    bytes = bytes
+    str = str
+    basestring = (str, bytes)
+
+    def iteritems(dct):
+        return dct.items()
diff --git a/pipenv/vendor/yaspin/constants.py b/pipenv/vendor/yaspin/constants.py
new file mode 100644
index 00000000..b26baabe
--- /dev/null
+++ b/pipenv/vendor/yaspin/constants.py
@@ -0,0 +1,110 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.constants
+~~~~~~~~~~~~~~~~
+
+Some setups.
+"""
+
+
+ENCODING = "utf-8"
+COLOR_MAP = {
+    # name: type
+    "blink": "attrs",
+    "bold": "attrs",
+    "concealed": "attrs",
+    "dark": "attrs",
+    "reverse": "attrs",
+    "underline": "attrs",
+    "blue": "color",
+    "cyan": "color",
+    "green": "color",
+    "magenta": "color",
+    "red": "color",
+    "white": "color",
+    "yellow": "color",
+    "on_blue": "on_color",
+    "on_cyan": "on_color",
+    "on_green": "on_color",
+    "on_grey": "on_color",
+    "on_magenta": "on_color",
+    "on_red": "on_color",
+    "on_white": "on_color",
+    "on_yellow": "on_color",
+}
+COLOR_ATTRS = COLOR_MAP.keys()
+
+# Get spinner names:
+# $ < yaspin/data/spinners.json | jq '. | keys'
+SPINNER_ATTRS = [
+    "arc",
+    "arrow",
+    "arrow2",
+    "arrow3",
+    "balloon",
+    "balloon2",
+    "bounce",
+    "bouncingBall",
+    "bouncingBar",
+    "boxBounce",
+    "boxBounce2",
+    "christmas",
+    "circle",
+    "circleHalves",
+    "circleQuarters",
+    "clock",
+    "dots",
+    "dots10",
+    "dots11",
+    "dots12",
+    "dots2",
+    "dots3",
+    "dots4",
+    "dots5",
+    "dots6",
+    "dots7",
+    "dots8",
+    "dots9",
+    "dqpb",
+    "earth",
+    "flip",
+    "grenade",
+    "growHorizontal",
+    "growVertical",
+    "hamburger",
+    "hearts",
+    "layer",
+    "line",
+    "line2",
+    "monkey",
+    "moon",
+    "noise",
+    "pipe",
+    "point",
+    "pong",
+    "runner",
+    "shark",
+    "simpleDots",
+    "simpleDotsScrolling",
+    "smiley",
+    "squareCorners",
+    "squish",
+    "star",
+    "star2",
+    "toggle",
+    "toggle10",
+    "toggle11",
+    "toggle12",
+    "toggle13",
+    "toggle2",
+    "toggle3",
+    "toggle4",
+    "toggle5",
+    "toggle6",
+    "toggle7",
+    "toggle8",
+    "toggle9",
+    "triangle",
+    "weather",
+]
diff --git a/pipenv/vendor/yaspin/core.py b/pipenv/vendor/yaspin/core.py
new file mode 100644
index 00000000..d01fb98e
--- /dev/null
+++ b/pipenv/vendor/yaspin/core.py
@@ -0,0 +1,534 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.yaspin
+~~~~~~~~~~~~~
+
+A lightweight terminal spinner.
+"""
+
+from __future__ import absolute_import
+
+import functools
+import itertools
+import signal
+import sys
+import threading
+import time
+
+from .base_spinner import default_spinner
+from .compat import PY2, basestring, builtin_str, bytes, iteritems, str
+from .constants import COLOR_ATTRS, COLOR_MAP, ENCODING, SPINNER_ATTRS
+from .helpers import to_unicode
+from .termcolor import colored
+
+
+class Yaspin(object):
+    """Implements a context manager that spawns a thread
+    to write spinner frames into a tty (stdout) during
+    context execution.
+    """
+
+    # When Python finds its output attached to a terminal,
+    # it sets the sys.stdout.encoding attribute to the terminal's encoding.
+    # The print statement's handler will automatically encode unicode
+    # arguments into bytes.
+    #
+    # In Py2 when piping or redirecting output, Python does not detect
+    # the desired character set of the output, it sets sys.stdout.encoding
+    # to None, and print will invoke the default "ascii" codec.
+    #
+    # Py3 invokes "UTF-8" codec by default.
+    #
+    # Thats why in Py2, output should be encoded manually with desired
+    # encoding in order to support pipes and redirects.
+
+    def __init__(
+        self,
+        spinner=None,
+        text="",
+        color=None,
+        on_color=None,
+        attrs=None,
+        reversal=False,
+        side="left",
+        sigmap=None,
+    ):
+        # Spinner
+        self._spinner = self._set_spinner(spinner)
+        self._frames = self._set_frames(self._spinner, reversal)
+        self._interval = self._set_interval(self._spinner)
+        self._cycle = self._set_cycle(self._frames)
+
+        # Color Specification
+        self._color = self._set_color(color) if color else color
+        self._on_color = self._set_on_color(on_color) if on_color else on_color
+        self._attrs = self._set_attrs(attrs) if attrs else set()
+        self._color_func = self._compose_color_func()
+
+        # Other
+        self._text = self._set_text(text)
+        self._side = self._set_side(side)
+        self._reversal = reversal
+
+        # Helper flags
+        self._stop_spin = None
+        self._hide_spin = None
+        self._spin_thread = None
+        self._last_frame = None
+
+        # Signals
+
+        # In Python 2 signal.SIG* are of type int.
+        # In Python 3 signal.SIG* are enums.
+        #
+        # Signal     = Union[enum.Enum, int]
+        # SigHandler = Union[enum.Enum, Callable]
+        self._sigmap = sigmap if sigmap else {}  # Dict[Signal, SigHandler]
+        # Maps signals to their default handlers in order to reset
+        # custom handlers set by ``sigmap`` at the cleanup phase.
+        self._dfl_sigmap = {}  # Dict[Signal, SigHandler]
+
+    #
+    # Dunders
+    #
+    def __repr__(self):
+        repr_ = u"<Yaspin frames={0!s}>".format(self._frames)
+        if PY2:
+            return repr_.encode(ENCODING)
+        return repr_
+
+    def __enter__(self):
+        self.start()
+        return self
+
+    def __exit__(self, exc_type, exc_val, traceback):
+        # Avoid stop() execution for the 2nd time
+        if self._spin_thread.is_alive():
+            self.stop()
+        return False  # nothing is handled
+
+    def __call__(self, fn):
+        @functools.wraps(fn)
+        def inner(*args, **kwargs):
+            with self:
+                return fn(*args, **kwargs)
+
+        return inner
+
+    def __getattr__(self, name):
+        # CLI spinners
+        if name in SPINNER_ATTRS:
+            from .spinners import Spinners
+
+            sp = getattr(Spinners, name)
+            self.spinner = sp
+        # Color Attributes: "color", "on_color", "attrs"
+        elif name in COLOR_ATTRS:
+            attr_type = COLOR_MAP[name]
+            # Call appropriate property setters;
+            # _color_func is updated automatically by setters.
+            if attr_type == "attrs":
+                self.attrs = [name]  # calls property setter
+            if attr_type in ("color", "on_color"):
+                setattr(self, attr_type, name)  # calls property setter
+        # Side: "left" or "right"
+        elif name in ("left", "right"):
+            self.side = name  # calls property setter
+        # Common error for unsupported attributes
+        else:
+            raise AttributeError(
+                "'{0}' object has no attribute: '{1}'".format(
+                    self.__class__.__name__, name
+                )
+            )
+        return self
+
+    #
+    # Properties
+    #
+    @property
+    def spinner(self):
+        return self._spinner
+
+    @spinner.setter
+    def spinner(self, sp):
+        self._spinner = self._set_spinner(sp)
+        self._frames = self._set_frames(self._spinner, self._reversal)
+        self._interval = self._set_interval(self._spinner)
+        self._cycle = self._set_cycle(self._frames)
+
+    @property
+    def text(self):
+        return self._text
+
+    @text.setter
+    def text(self, txt):
+        self._text = self._set_text(txt)
+
+    @property
+    def color(self):
+        return self._color
+
+    @color.setter
+    def color(self, value):
+        self._color = self._set_color(value) if value else value
+        self._color_func = self._compose_color_func()  # update
+
+    @property
+    def on_color(self):
+        return self._on_color
+
+    @on_color.setter
+    def on_color(self, value):
+        self._on_color = self._set_on_color(value) if value else value
+        self._color_func = self._compose_color_func()  # update
+
+    @property
+    def attrs(self):
+        return list(self._attrs)
+
+    @attrs.setter
+    def attrs(self, value):
+        new_attrs = self._set_attrs(value) if value else set()
+        self._attrs = self._attrs.union(new_attrs)
+        self._color_func = self._compose_color_func()  # update
+
+    @property
+    def side(self):
+        return self._side
+
+    @side.setter
+    def side(self, value):
+        self._side = self._set_side(value)
+
+    @property
+    def reversal(self):
+        return self._reversal
+
+    @reversal.setter
+    def reversal(self, value):
+        self._reversal = value
+        self._frames = self._set_frames(self._spinner, self._reversal)
+        self._cycle = self._set_cycle(self._frames)
+
+    #
+    # Public
+    #
+    def start(self):
+        if self._sigmap:
+            self._register_signal_handlers()
+
+        if sys.stdout.isatty():
+            self._hide_cursor()
+
+        self._stop_spin = threading.Event()
+        self._hide_spin = threading.Event()
+        self._spin_thread = threading.Thread(target=self._spin)
+        self._spin_thread.start()
+
+    def stop(self):
+        if self._dfl_sigmap:
+            # Reset registered signal handlers to default ones
+            self._reset_signal_handlers()
+
+        if self._spin_thread:
+            self._stop_spin.set()
+            self._spin_thread.join()
+
+        sys.stdout.write("\r")
+        self._clear_line()
+
+        if sys.stdout.isatty():
+            self._show_cursor()
+
+    def hide(self):
+        """Hide the spinner to allow for custom writing to the terminal."""
+        thr_is_alive = self._spin_thread and self._spin_thread.is_alive()
+
+        if thr_is_alive and not self._hide_spin.is_set():
+            # set the hidden spinner flag
+            self._hide_spin.set()
+
+            # clear the current line
+            sys.stdout.write("\r")
+            self._clear_line()
+
+            # flush the stdout buffer so the current line can be rewritten to
+            sys.stdout.flush()
+
+    def show(self):
+        """Show the hidden spinner."""
+        thr_is_alive = self._spin_thread and self._spin_thread.is_alive()
+
+        if thr_is_alive and self._hide_spin.is_set():
+            # clear the hidden spinner flag
+            self._hide_spin.clear()
+
+            # clear the current line so the spinner is not appended to it
+            sys.stdout.write("\r")
+            self._clear_line()
+
+    def write(self, text):
+        """Write text in the terminal without breaking the spinner."""
+        # similar to tqdm.write()
+        # https://pypi.python.org/pypi/tqdm#writing-messages
+        sys.stdout.write("\r")
+        self._clear_line()
+
+        _text = to_unicode(text)
+        if PY2:
+            _text = _text.encode(ENCODING)
+
+        # Ensure output is bytes for Py2 and Unicode for Py3
+        assert isinstance(_text, builtin_str)
+
+        sys.stdout.write("{0}\n".format(_text))
+
+    def ok(self, text="OK"):
+        """Set Ok (success) finalizer to a spinner."""
+        _text = text if text else "OK"
+        self._freeze(_text)
+
+    def fail(self, text="FAIL"):
+        """Set fail finalizer to a spinner."""
+        _text = text if text else "FAIL"
+        self._freeze(_text)
+
+    #
+    # Protected
+    #
+    def _freeze(self, final_text):
+        """Stop spinner, compose last frame and 'freeze' it."""
+        text = to_unicode(final_text)
+        self._last_frame = self._compose_out(text, mode="last")
+
+        # Should be stopped here, otherwise prints after
+        # self._freeze call will mess up the spinner
+        self.stop()
+        sys.stdout.write(self._last_frame)
+
+    def _spin(self):
+        while not self._stop_spin.is_set():
+
+            if self._hide_spin.is_set():
+                # Wait a bit to avoid wasting cycles
+                time.sleep(self._interval)
+                continue
+
+            # Compose output
+            spin_phase = next(self._cycle)
+            out = self._compose_out(spin_phase)
+
+            # Write
+            sys.stdout.write(out)
+            self._clear_line()
+            sys.stdout.flush()
+
+            # Wait
+            time.sleep(self._interval)
+            sys.stdout.write("\b")
+
+    def _compose_color_func(self):
+        fn = functools.partial(
+            colored,
+            color=self._color,
+            on_color=self._on_color,
+            attrs=list(self._attrs),
+        )
+        return fn
+
+    def _compose_out(self, frame, mode=None):
+        # Ensure Unicode input
+        assert isinstance(frame, str)
+        assert isinstance(self._text, str)
+
+        frame = frame.encode(ENCODING) if PY2 else frame
+        text = self._text.encode(ENCODING) if PY2 else self._text
+
+        # Colors
+        if self._color_func is not None:
+            frame = self._color_func(frame)
+
+        # Position
+        if self._side == "right":
+            frame, text = text, frame
+
+        # Mode
+        if not mode:
+            out = "\r{0} {1}".format(frame, text)
+        else:
+            out = "{0} {1}\n".format(frame, text)
+
+        # Ensure output is bytes for Py2 and Unicode for Py3
+        assert isinstance(out, builtin_str)
+
+        return out
+
+    def _register_signal_handlers(self):
+        # SIGKILL cannot be caught or ignored, and the receiving
+        # process cannot perform any clean-up upon receiving this
+        # signal.
+        if signal.SIGKILL in self._sigmap.keys():
+            raise ValueError(
+                "Trying to set handler for SIGKILL signal. "
+                "SIGKILL cannot be cought or ignored in POSIX systems."
+            )
+
+        for sig, sig_handler in iteritems(self._sigmap):
+            # A handler for a particular signal, once set, remains
+            # installed until it is explicitly reset. Store default
+            # signal handlers for subsequent reset at cleanup phase.
+            dfl_handler = signal.getsignal(sig)
+            self._dfl_sigmap[sig] = dfl_handler
+
+            # ``signal.SIG_DFL`` and ``signal.SIG_IGN`` are also valid
+            # signal handlers and are not callables.
+            if callable(sig_handler):
+                # ``signal.signal`` accepts handler function which is
+                # called with two arguments: signal number and the
+                # interrupted stack frame. ``functools.partial`` solves
+                # the problem of passing spinner instance into the handler
+                # function.
+                sig_handler = functools.partial(sig_handler, spinner=self)
+
+            signal.signal(sig, sig_handler)
+
+    def _reset_signal_handlers(self):
+        for sig, sig_handler in iteritems(self._dfl_sigmap):
+            signal.signal(sig, sig_handler)
+
+    #
+    # Static
+    #
+    @staticmethod
+    def _set_color(value):
+        # type: (str) -> str
+        available_values = [k for k, v in iteritems(COLOR_MAP) if v == "color"]
+
+        if value not in available_values:
+            raise ValueError(
+                "'{0}': unsupported color value. Use one of the: {1}".format(
+                    value, ", ".join(available_values)
+                )
+            )
+        return value
+
+    @staticmethod
+    def _set_on_color(value):
+        # type: (str) -> str
+        available_values = [
+            k for k, v in iteritems(COLOR_MAP) if v == "on_color"
+        ]
+        if value not in available_values:
+            raise ValueError(
+                "'{0}': unsupported on_color value. "
+                "Use one of the: {1}".format(
+                    value, ", ".join(available_values)
+                )
+            )
+        return value
+
+    @staticmethod
+    def _set_attrs(attrs):
+        # type: (List[str]) -> Set[str]
+        available_values = [k for k, v in iteritems(COLOR_MAP) if v == "attrs"]
+
+        for attr in attrs:
+            if attr not in available_values:
+                raise ValueError(
+                    "'{0}': unsupported attribute value. "
+                    "Use one of the: {1}".format(
+                        attr, ", ".join(available_values)
+                    )
+                )
+        return set(attrs)
+
+    @staticmethod
+    def _set_spinner(spinner):
+        if not spinner:
+            sp = default_spinner
+
+        if hasattr(spinner, "frames") and hasattr(spinner, "interval"):
+            if not spinner.frames or not spinner.interval:
+                sp = default_spinner
+            else:
+                sp = spinner
+        else:
+            sp = default_spinner
+
+        return sp
+
+    @staticmethod
+    def _set_side(side):
+        # type: (str) -> str
+        if side not in ("left", "right"):
+            raise ValueError(
+                "'{0}': unsupported side value. "
+                "Use either 'left' or 'right'."
+            )
+        return side
+
+    @staticmethod
+    def _set_frames(spinner, reversal):
+        # type: (base_spinner.Spinner, bool) -> Union[str, List]
+        uframes = None  # unicode frames
+        uframes_seq = None  # sequence of unicode frames
+
+        if isinstance(spinner.frames, basestring):
+            uframes = to_unicode(spinner.frames) if PY2 else spinner.frames
+
+        # TODO (pavdmyt): support any type that implements iterable
+        if isinstance(spinner.frames, (list, tuple)):
+
+            # Empty ``spinner.frames`` is handled by ``Yaspin._set_spinner``
+            if spinner.frames and isinstance(spinner.frames[0], bytes):
+                uframes_seq = [to_unicode(frame) for frame in spinner.frames]
+            else:
+                uframes_seq = spinner.frames
+
+        _frames = uframes or uframes_seq
+        if not _frames:
+            # Empty ``spinner.frames`` is handled by ``Yaspin._set_spinner``.
+            # This code is very unlikely to be executed. However, it's still
+            # here to be on a safe side.
+            raise ValueError(
+                "{0!r}: no frames found in spinner".format(spinner)
+            )
+
+        # Builtin ``reversed`` returns reverse iterator,
+        # which adds unnecessary difficulty for returning
+        # unicode value;
+        # Hence using [::-1] syntax
+        frames = _frames[::-1] if reversal else _frames
+
+        return frames
+
+    @staticmethod
+    def _set_interval(spinner):
+        # Milliseconds to Seconds
+        return spinner.interval * 0.001
+
+    @staticmethod
+    def _set_cycle(frames):
+        return itertools.cycle(frames)
+
+    @staticmethod
+    def _set_text(text):
+        if PY2:
+            return to_unicode(text)
+        return text
+
+    @staticmethod
+    def _hide_cursor():
+        sys.stdout.write("\033[?25l")
+        sys.stdout.flush()
+
+    @staticmethod
+    def _show_cursor():
+        sys.stdout.write("\033[?25h")
+        sys.stdout.flush()
+
+    @staticmethod
+    def _clear_line():
+        sys.stdout.write("\033[K")
diff --git a/pipenv/vendor/yaspin/data/spinners.json b/pipenv/vendor/yaspin/data/spinners.json
new file mode 100644
index 00000000..b388b2a5
--- /dev/null
+++ b/pipenv/vendor/yaspin/data/spinners.json
@@ -0,0 +1,912 @@
+{
+	"dots": {
+		"interval": 80,
+		"frames": [
+			"⠋",
+			"⠙",
+			"⠹",
+			"⠸",
+			"⠼",
+			"⠴",
+			"⠦",
+			"⠧",
+			"⠇",
+			"⠏"
+		]
+	},
+	"dots2": {
+		"interval": 80,
+		"frames": [
+			"⣾",
+			"⣽",
+			"⣻",
+			"⢿",
+			"⡿",
+			"⣟",
+			"⣯",
+			"⣷"
+		]
+	},
+	"dots3": {
+		"interval": 80,
+		"frames": [
+			"⠋",
+			"⠙",
+			"⠚",
+			"⠞",
+			"⠖",
+			"⠦",
+			"⠴",
+			"⠲",
+			"⠳",
+			"⠓"
+		]
+	},
+	"dots4": {
+		"interval": 80,
+		"frames": [
+			"⠄",
+			"⠆",
+			"⠇",
+			"⠋",
+			"⠙",
+			"⠸",
+			"⠰",
+			"⠠",
+			"⠰",
+			"⠸",
+			"⠙",
+			"⠋",
+			"⠇",
+			"⠆"
+		]
+	},
+	"dots5": {
+		"interval": 80,
+		"frames": [
+			"⠋",
+			"⠙",
+			"⠚",
+			"⠒",
+			"⠂",
+			"⠂",
+			"⠒",
+			"⠲",
+			"⠴",
+			"⠦",
+			"⠖",
+			"⠒",
+			"⠐",
+			"⠐",
+			"⠒",
+			"⠓",
+			"⠋"
+		]
+	},
+	"dots6": {
+		"interval": 80,
+		"frames": [
+			"⠁",
+			"⠉",
+			"⠙",
+			"⠚",
+			"⠒",
+			"⠂",
+			"⠂",
+			"⠒",
+			"⠲",
+			"⠴",
+			"⠤",
+			"⠄",
+			"⠄",
+			"⠤",
+			"⠴",
+			"⠲",
+			"⠒",
+			"⠂",
+			"⠂",
+			"⠒",
+			"⠚",
+			"⠙",
+			"⠉",
+			"⠁"
+		]
+	},
+	"dots7": {
+		"interval": 80,
+		"frames": [
+			"⠈",
+			"⠉",
+			"⠋",
+			"⠓",
+			"⠒",
+			"⠐",
+			"⠐",
+			"⠒",
+			"⠖",
+			"⠦",
+			"⠤",
+			"⠠",
+			"⠠",
+			"⠤",
+			"⠦",
+			"⠖",
+			"⠒",
+			"⠐",
+			"⠐",
+			"⠒",
+			"⠓",
+			"⠋",
+			"⠉",
+			"⠈"
+		]
+	},
+	"dots8": {
+		"interval": 80,
+		"frames": [
+			"⠁",
+			"⠁",
+			"⠉",
+			"⠙",
+			"⠚",
+			"⠒",
+			"⠂",
+			"⠂",
+			"⠒",
+			"⠲",
+			"⠴",
+			"⠤",
+			"⠄",
+			"⠄",
+			"⠤",
+			"⠠",
+			"⠠",
+			"⠤",
+			"⠦",
+			"⠖",
+			"⠒",
+			"⠐",
+			"⠐",
+			"⠒",
+			"⠓",
+			"⠋",
+			"⠉",
+			"⠈",
+			"⠈"
+		]
+	},
+	"dots9": {
+		"interval": 80,
+		"frames": [
+			"⢹",
+			"⢺",
+			"⢼",
+			"⣸",
+			"⣇",
+			"⡧",
+			"⡗",
+			"⡏"
+		]
+	},
+	"dots10": {
+		"interval": 80,
+		"frames": [
+			"⢄",
+			"⢂",
+			"⢁",
+			"⡁",
+			"⡈",
+			"⡐",
+			"⡠"
+		]
+	},
+	"dots11": {
+		"interval": 100,
+		"frames": [
+			"⠁",
+			"⠂",
+			"⠄",
+			"⡀",
+			"⢀",
+			"⠠",
+			"⠐",
+			"⠈"
+		]
+	},
+	"dots12": {
+		"interval": 80,
+		"frames": [
+			"⢀⠀",
+			"⡀⠀",
+			"⠄⠀",
+			"⢂⠀",
+			"⡂⠀",
+			"⠅⠀",
+			"⢃⠀",
+			"⡃⠀",
+			"⠍⠀",
+			"⢋⠀",
+			"⡋⠀",
+			"⠍⠁",
+			"⢋⠁",
+			"⡋⠁",
+			"⠍⠉",
+			"⠋⠉",
+			"⠋⠉",
+			"⠉⠙",
+			"⠉⠙",
+			"⠉⠩",
+			"⠈⢙",
+			"⠈⡙",
+			"⢈⠩",
+			"⡀⢙",
+			"⠄⡙",
+			"⢂⠩",
+			"⡂⢘",
+			"⠅⡘",
+			"⢃⠨",
+			"⡃⢐",
+			"⠍⡐",
+			"⢋⠠",
+			"⡋⢀",
+			"⠍⡁",
+			"⢋⠁",
+			"⡋⠁",
+			"⠍⠉",
+			"⠋⠉",
+			"⠋⠉",
+			"⠉⠙",
+			"⠉⠙",
+			"⠉⠩",
+			"⠈⢙",
+			"⠈⡙",
+			"⠈⠩",
+			"⠀⢙",
+			"⠀⡙",
+			"⠀⠩",
+			"⠀⢘",
+			"⠀⡘",
+			"⠀⠨",
+			"⠀⢐",
+			"⠀⡐",
+			"⠀⠠",
+			"⠀⢀",
+			"⠀⡀"
+		]
+	},
+	"line": {
+		"interval": 130,
+		"frames": [
+			"-",
+			"\\",
+			"|",
+			"/"
+		]
+	},
+	"line2": {
+		"interval": 100,
+		"frames": [
+			"⠂",
+			"-",
+			"–",
+			"—",
+			"–",
+			"-"
+		]
+	},
+	"pipe": {
+		"interval": 100,
+		"frames": [
+			"┤",
+			"┘",
+			"┴",
+			"└",
+			"├",
+			"┌",
+			"┬",
+			"┐"
+		]
+	},
+	"simpleDots": {
+		"interval": 400,
+		"frames": [
+			".  ",
+			".. ",
+			"...",
+			"   "
+		]
+	},
+	"simpleDotsScrolling": {
+		"interval": 200,
+		"frames": [
+			".  ",
+			".. ",
+			"...",
+			" ..",
+			"  .",
+			"   "
+		]
+	},
+	"star": {
+		"interval": 70,
+		"frames": [
+			"✶",
+			"✸",
+			"✹",
+			"✺",
+			"✹",
+			"✷"
+		]
+	},
+	"star2": {
+		"interval": 80,
+		"frames": [
+			"+",
+			"x",
+			"*"
+		]
+	},
+	"flip": {
+		"interval": 70,
+		"frames": [
+			"_",
+			"_",
+			"_",
+			"-",
+			"`",
+			"`",
+			"'",
+			"´",
+			"-",
+			"_",
+			"_",
+			"_"
+		]
+	},
+	"hamburger": {
+		"interval": 100,
+		"frames": [
+			"☱",
+			"☲",
+			"☴"
+		]
+	},
+	"growVertical": {
+		"interval": 120,
+		"frames": [
+			"▁",
+			"▃",
+			"▄",
+			"▅",
+			"▆",
+			"▇",
+			"▆",
+			"▅",
+			"▄",
+			"▃"
+		]
+	},
+	"growHorizontal": {
+		"interval": 120,
+		"frames": [
+			"▏",
+			"▎",
+			"▍",
+			"▌",
+			"▋",
+			"▊",
+			"▉",
+			"▊",
+			"▋",
+			"▌",
+			"▍",
+			"▎"
+		]
+	},
+	"balloon": {
+		"interval": 140,
+		"frames": [
+			" ",
+			".",
+			"o",
+			"O",
+			"@",
+			"*",
+			" "
+		]
+	},
+	"balloon2": {
+		"interval": 120,
+		"frames": [
+			".",
+			"o",
+			"O",
+			"°",
+			"O",
+			"o",
+			"."
+		]
+	},
+	"noise": {
+		"interval": 100,
+		"frames": [
+			"▓",
+			"▒",
+			"░"
+		]
+	},
+	"bounce": {
+		"interval": 120,
+		"frames": [
+			"⠁",
+			"⠂",
+			"⠄",
+			"⠂"
+		]
+	},
+	"boxBounce": {
+		"interval": 120,
+		"frames": [
+			"▖",
+			"▘",
+			"▝",
+			"▗"
+		]
+	},
+	"boxBounce2": {
+		"interval": 100,
+		"frames": [
+			"▌",
+			"▀",
+			"▐",
+			"▄"
+		]
+	},
+	"triangle": {
+		"interval": 50,
+		"frames": [
+			"◢",
+			"◣",
+			"◤",
+			"◥"
+		]
+	},
+	"arc": {
+		"interval": 100,
+		"frames": [
+			"◜",
+			"◠",
+			"◝",
+			"◞",
+			"◡",
+			"◟"
+		]
+	},
+	"circle": {
+		"interval": 120,
+		"frames": [
+			"◡",
+			"⊙",
+			"◠"
+		]
+	},
+	"squareCorners": {
+		"interval": 180,
+		"frames": [
+			"◰",
+			"◳",
+			"◲",
+			"◱"
+		]
+	},
+	"circleQuarters": {
+		"interval": 120,
+		"frames": [
+			"◴",
+			"◷",
+			"◶",
+			"◵"
+		]
+	},
+	"circleHalves": {
+		"interval": 50,
+		"frames": [
+			"◐",
+			"◓",
+			"◑",
+			"◒"
+		]
+	},
+	"squish": {
+		"interval": 100,
+		"frames": [
+			"╫",
+			"╪"
+		]
+	},
+	"toggle": {
+		"interval": 250,
+		"frames": [
+			"⊶",
+			"⊷"
+		]
+	},
+	"toggle2": {
+		"interval": 80,
+		"frames": [
+			"▫",
+			"▪"
+		]
+	},
+	"toggle3": {
+		"interval": 120,
+		"frames": [
+			"□",
+			"■"
+		]
+	},
+	"toggle4": {
+		"interval": 100,
+		"frames": [
+			"■",
+			"□",
+			"▪",
+			"▫"
+		]
+	},
+	"toggle5": {
+		"interval": 100,
+		"frames": [
+			"▮",
+			"▯"
+		]
+	},
+	"toggle6": {
+		"interval": 300,
+		"frames": [
+			"ဝ",
+			"၀"
+		]
+	},
+	"toggle7": {
+		"interval": 80,
+		"frames": [
+			"⦾",
+			"⦿"
+		]
+	},
+	"toggle8": {
+		"interval": 100,
+		"frames": [
+			"◍",
+			"◌"
+		]
+	},
+	"toggle9": {
+		"interval": 100,
+		"frames": [
+			"◉",
+			"◎"
+		]
+	},
+	"toggle10": {
+		"interval": 100,
+		"frames": [
+			"㊂",
+			"㊀",
+			"㊁"
+		]
+	},
+	"toggle11": {
+		"interval": 50,
+		"frames": [
+			"⧇",
+			"⧆"
+		]
+	},
+	"toggle12": {
+		"interval": 120,
+		"frames": [
+			"☗",
+			"☖"
+		]
+	},
+	"toggle13": {
+		"interval": 80,
+		"frames": [
+			"=",
+			"*",
+			"-"
+		]
+	},
+	"arrow": {
+		"interval": 100,
+		"frames": [
+			"←",
+			"↖",
+			"↑",
+			"↗",
+			"→",
+			"↘",
+			"↓",
+			"↙"
+		]
+	},
+	"arrow2": {
+		"interval": 80,
+		"frames": [
+			"⬆️ ",
+			"↗️ ",
+			"➡️ ",
+			"↘️ ",
+			"⬇️ ",
+			"↙️ ",
+			"⬅️ ",
+			"↖️ "
+		]
+	},
+	"arrow3": {
+		"interval": 120,
+		"frames": [
+			"▹▹▹▹▹",
+			"▸▹▹▹▹",
+			"▹▸▹▹▹",
+			"▹▹▸▹▹",
+			"▹▹▹▸▹",
+			"▹▹▹▹▸"
+		]
+	},
+	"bouncingBar": {
+		"interval": 80,
+		"frames": [
+			"[    ]",
+			"[=   ]",
+			"[==  ]",
+			"[=== ]",
+			"[ ===]",
+			"[  ==]",
+			"[   =]",
+			"[    ]",
+			"[   =]",
+			"[  ==]",
+			"[ ===]",
+			"[====]",
+			"[=== ]",
+			"[==  ]",
+			"[=   ]"
+		]
+	},
+	"bouncingBall": {
+		"interval": 80,
+		"frames": [
+			"( ●    )",
+			"(  ●   )",
+			"(   ●  )",
+			"(    ● )",
+			"(     ●)",
+			"(    ● )",
+			"(   ●  )",
+			"(  ●   )",
+			"( ●    )",
+			"(●     )"
+		]
+	},
+	"smiley": {
+		"interval": 200,
+		"frames": [
+			"😄 ",
+			"😝 "
+		]
+	},
+	"monkey": {
+		"interval": 300,
+		"frames": [
+			"🙈 ",
+			"🙈 ",
+			"🙉 ",
+			"🙊 "
+		]
+	},
+	"hearts": {
+		"interval": 100,
+		"frames": [
+			"💛 ",
+			"💙 ",
+			"💜 ",
+			"💚 ",
+			"❤️ "
+		]
+	},
+	"clock": {
+		"interval": 100,
+		"frames": [
+			"🕛 ",
+			"🕐 ",
+			"🕑 ",
+			"🕒 ",
+			"🕓 ",
+			"🕔 ",
+			"🕕 ",
+			"🕖 ",
+			"🕗 ",
+			"🕘 ",
+			"🕙 ",
+			"🕚 "
+		]
+	},
+	"earth": {
+		"interval": 180,
+		"frames": [
+			"🌍 ",
+			"🌎 ",
+			"🌏 "
+		]
+	},
+	"moon": {
+		"interval": 80,
+		"frames": [
+			"🌑 ",
+			"🌒 ",
+			"🌓 ",
+			"🌔 ",
+			"🌕 ",
+			"🌖 ",
+			"🌗 ",
+			"🌘 "
+		]
+	},
+	"runner": {
+		"interval": 140,
+		"frames": [
+			"🚶 ",
+			"🏃 "
+		]
+	},
+	"pong": {
+		"interval": 80,
+		"frames": [
+			"▐⠂       ▌",
+			"▐⠈       ▌",
+			"▐ ⠂      ▌",
+			"▐ ⠠      ▌",
+			"▐  ⡀     ▌",
+			"▐  ⠠     ▌",
+			"▐   ⠂    ▌",
+			"▐   ⠈    ▌",
+			"▐    ⠂   ▌",
+			"▐    ⠠   ▌",
+			"▐     ⡀  ▌",
+			"▐     ⠠  ▌",
+			"▐      ⠂ ▌",
+			"▐      ⠈ ▌",
+			"▐       ⠂▌",
+			"▐       ⠠▌",
+			"▐       ⡀▌",
+			"▐      ⠠ ▌",
+			"▐      ⠂ ▌",
+			"▐     ⠈  ▌",
+			"▐     ⠂  ▌",
+			"▐    ⠠   ▌",
+			"▐    ⡀   ▌",
+			"▐   ⠠    ▌",
+			"▐   ⠂    ▌",
+			"▐  ⠈     ▌",
+			"▐  ⠂     ▌",
+			"▐ ⠠      ▌",
+			"▐ ⡀      ▌",
+			"▐⠠       ▌"
+		]
+	},
+	"shark": {
+		"interval": 120,
+		"frames": [
+			"▐|\\____________▌",
+			"▐_|\\___________▌",
+			"▐__|\\__________▌",
+			"▐___|\\_________▌",
+			"▐____|\\________▌",
+			"▐_____|\\_______▌",
+			"▐______|\\______▌",
+			"▐_______|\\_____▌",
+			"▐________|\\____▌",
+			"▐_________|\\___▌",
+			"▐__________|\\__▌",
+			"▐___________|\\_▌",
+			"▐____________|\\▌",
+			"▐____________/|▌",
+			"▐___________/|_▌",
+			"▐__________/|__▌",
+			"▐_________/|___▌",
+			"▐________/|____▌",
+			"▐_______/|_____▌",
+			"▐______/|______▌",
+			"▐_____/|_______▌",
+			"▐____/|________▌",
+			"▐___/|_________▌",
+			"▐__/|__________▌",
+			"▐_/|___________▌",
+			"▐/|____________▌"
+		]
+	},
+	"dqpb": {
+		"interval": 100,
+		"frames": [
+			"d",
+			"q",
+			"p",
+			"b"
+		]
+	},
+	"weather": {
+		"interval": 100,
+		"frames": [
+			"☀️ ",
+			"☀️ ",
+			"☀️ ",
+			"🌤 ",
+			"⛅️ ",
+			"🌥 ",
+			"☁️ ",
+			"🌧 ",
+			"🌨 ",
+			"🌧 ",
+			"🌨 ",
+			"🌧 ",
+			"🌨 ",
+			"⛈ ",
+			"🌨 ",
+			"🌧 ",
+			"🌨 ",
+			"☁️ ",
+			"🌥 ",
+			"⛅️ ",
+			"🌤 ",
+			"☀️ ",
+			"☀️ "
+		]
+	},
+	"christmas": {
+		"interval": 400,
+		"frames": [
+			"🌲",
+			"🎄"
+		]
+	},
+	"grenade": {
+		"interval": 80,
+		"frames": [
+			"،   ",
+			"′   ",
+			" ´ ",
+			" ‾ ",
+			"  ⸌",
+			"  ⸊",
+			"  |",
+			"  ⁎",
+			"  ⁕",
+			" ෴ ",
+			"  ⁓",
+			"   ",
+			"   ",
+			"   "
+		]
+	},
+	"point": {
+		"interval": 125,
+		"frames": [
+			"∙∙∙",
+			"●∙∙",
+			"∙●∙",
+			"∙∙●",
+			"∙∙∙"
+		]
+	},
+	"layer": {
+		"interval": 150,
+		"frames": [
+			"-",
+			"=",
+			"≡"
+		]
+	}
+}
diff --git a/pipenv/vendor/yaspin/helpers.py b/pipenv/vendor/yaspin/helpers.py
new file mode 100644
index 00000000..49ce0d06
--- /dev/null
+++ b/pipenv/vendor/yaspin/helpers.py
@@ -0,0 +1,19 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.helpers
+~~~~~~~~~~~~~~
+
+Helper functions.
+"""
+
+from __future__ import absolute_import
+
+from .compat import bytes
+from .constants import ENCODING
+
+
+def to_unicode(text_type, encoding=ENCODING):
+    if isinstance(text_type, bytes):
+        return text_type.decode(encoding)
+    return text_type
diff --git a/pipenv/vendor/yaspin/signal_handlers.py b/pipenv/vendor/yaspin/signal_handlers.py
new file mode 100644
index 00000000..f38f5d6b
--- /dev/null
+++ b/pipenv/vendor/yaspin/signal_handlers.py
@@ -0,0 +1,35 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.signal_handlers
+~~~~~~~~~~~~~~~~~~~~~~
+
+Callback functions or "signal handlers", that are invoked
+when the signal occurs.
+"""
+
+import sys
+
+
+def default_handler(signum, frame, spinner):
+    """Signal handler, used to gracefully shut down the ``spinner`` instance
+    when specified signal is received by the process running the ``spinner``.
+
+    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``
+    function for more details.
+    """
+    spinner.fail()
+    spinner.stop()
+    sys.exit(0)
+
+
+def fancy_handler(signum, frame, spinner):
+    """Signal handler, used to gracefully shut down the ``spinner`` instance
+    when specified signal is received by the process running the ``spinner``.
+
+    ``signum`` and ``frame`` are mandatory arguments. Check ``signal.signal``
+    function for more details.
+    """
+    spinner.red.fail("✘")
+    spinner.stop()
+    sys.exit(0)
diff --git a/pipenv/vendor/yaspin/spinners.py b/pipenv/vendor/yaspin/spinners.py
new file mode 100644
index 00000000..9c3fa7b8
--- /dev/null
+++ b/pipenv/vendor/yaspin/spinners.py
@@ -0,0 +1,29 @@
+# -*- coding: utf-8 -*-
+
+"""
+yaspin.spinners
+~~~~~~~~~~~~~~~
+
+A collection of cli spinners.
+"""
+
+import codecs
+import os
+from collections import namedtuple
+
+try:
+    import simplejson as json
+except ImportError:
+    import json
+
+
+THIS_DIR = os.path.dirname(os.path.realpath(__file__))
+SPINNERS_PATH = os.path.join(THIS_DIR, "data/spinners.json")
+
+
+def _hook(dct):
+    return namedtuple("Spinner", dct.keys())(*dct.values())
+
+
+with codecs.open(SPINNERS_PATH, encoding="utf-8") as f:
+    Spinners = json.load(f, object_hook=_hook)
diff --git a/pipenv/vendor/yaspin/termcolor.py b/pipenv/vendor/yaspin/termcolor.py
new file mode 100644
index 00000000..f11b824b
--- /dev/null
+++ b/pipenv/vendor/yaspin/termcolor.py
@@ -0,0 +1,168 @@
+# coding: utf-8
+# Copyright (c) 2008-2011 Volvox Development Team
+#
+# Permission is hereby granted, free of charge, to any person obtaining a copy
+# of this software and associated documentation files (the "Software"), to deal
+# in the Software without restriction, including without limitation the rights
+# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+# copies of the Software, and to permit persons to whom the Software is
+# furnished to do so, subject to the following conditions:
+#
+# The above copyright notice and this permission notice shall be included in
+# all copies or substantial portions of the Software.
+#
+# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+# THE SOFTWARE.
+#
+# Author: Konstantin Lepa <konstantin.lepa@gmail.com>
+
+"""ANSII Color formatting for output in terminal."""
+
+from __future__ import print_function
+import os
+
+
+__ALL__ = [ 'colored', 'cprint' ]
+
+VERSION = (1, 1, 0)
+
+ATTRIBUTES = dict(
+        list(zip([
+            'bold',
+            'dark',
+            '',
+            'underline',
+            'blink',
+            '',
+            'reverse',
+            'concealed'
+            ],
+            list(range(1, 9))
+            ))
+        )
+del ATTRIBUTES['']
+
+
+HIGHLIGHTS = dict(
+        list(zip([
+            'on_grey',
+            'on_red',
+            'on_green',
+            'on_yellow',
+            'on_blue',
+            'on_magenta',
+            'on_cyan',
+            'on_white'
+            ],
+            list(range(40, 48))
+            ))
+        )
+
+
+COLORS = dict(
+        list(zip([
+            'grey',
+            'red',
+            'green',
+            'yellow',
+            'blue',
+            'magenta',
+            'cyan',
+            'white',
+            ],
+            list(range(30, 38))
+            ))
+        )
+
+
+RESET = '\033[0m'
+
+
+def colored(text, color=None, on_color=None, attrs=None):
+    """Colorize text.
+
+    Available text colors:
+        red, green, yellow, blue, magenta, cyan, white.
+
+    Available text highlights:
+        on_red, on_green, on_yellow, on_blue, on_magenta, on_cyan, on_white.
+
+    Available attributes:
+        bold, dark, underline, blink, reverse, concealed.
+
+    Example:
+        colored('Hello, World!', 'red', 'on_grey', ['blue', 'blink'])
+        colored('Hello, World!', 'green')
+    """
+    if os.getenv('ANSI_COLORS_DISABLED') is None:
+        fmt_str = '\033[%dm%s'
+        if color is not None:
+            text = fmt_str % (COLORS[color], text)
+
+        if on_color is not None:
+            text = fmt_str % (HIGHLIGHTS[on_color], text)
+
+        if attrs is not None:
+            for attr in attrs:
+                text = fmt_str % (ATTRIBUTES[attr], text)
+
+        text += RESET
+    return text
+
+
+def cprint(text, color=None, on_color=None, attrs=None, **kwargs):
+    """Print colorize text.
+
+    It accepts arguments of print function.
+    """
+
+    print((colored(text, color, on_color, attrs)), **kwargs)
+
+
+if __name__ == '__main__':
+    print('Current terminal type: %s' % os.getenv('TERM'))
+    print('Test basic colors:')
+    cprint('Grey color', 'grey')
+    cprint('Red color', 'red')
+    cprint('Green color', 'green')
+    cprint('Yellow color', 'yellow')
+    cprint('Blue color', 'blue')
+    cprint('Magenta color', 'magenta')
+    cprint('Cyan color', 'cyan')
+    cprint('White color', 'white')
+    print(('-' * 78))
+
+    print('Test highlights:')
+    cprint('On grey color', on_color='on_grey')
+    cprint('On red color', on_color='on_red')
+    cprint('On green color', on_color='on_green')
+    cprint('On yellow color', on_color='on_yellow')
+    cprint('On blue color', on_color='on_blue')
+    cprint('On magenta color', on_color='on_magenta')
+    cprint('On cyan color', on_color='on_cyan')
+    cprint('On white color', color='grey', on_color='on_white')
+    print('-' * 78)
+
+    print('Test attributes:')
+    cprint('Bold grey color', 'grey', attrs=['bold'])
+    cprint('Dark red color', 'red', attrs=['dark'])
+    cprint('Underline green color', 'green', attrs=['underline'])
+    cprint('Blink yellow color', 'yellow', attrs=['blink'])
+    cprint('Reversed blue color', 'blue', attrs=['reverse'])
+    cprint('Concealed Magenta color', 'magenta', attrs=['concealed'])
+    cprint('Bold underline reverse cyan color', 'cyan',
+            attrs=['bold', 'underline', 'reverse'])
+    cprint('Dark blink concealed white color', 'white',
+            attrs=['dark', 'blink', 'concealed'])
+    print(('-' * 78))
+
+    print('Test mixing:')
+    cprint('Underline red on grey color', 'red', 'on_grey',
+            ['underline'])
+    cprint('Reversed green on red color', 'green', 'on_red', ['reverse'])
+
diff --git a/tasks/vendoring/patches/patched/piptools.patch b/tasks/vendoring/patches/patched/piptools.patch
index 6dff468a..80b3f9e5 100644
--- a/tasks/vendoring/patches/patched/piptools.patch
+++ b/tasks/vendoring/patches/patched/piptools.patch
@@ -19,10 +19,10 @@ index 4e6174c..75f9b49 100644
  # NOTE
  # We used to store the cache dir under ~/.pip-tools, which is not the
 diff --git a/pipenv/patched/piptools/repositories/pypi.py b/pipenv/patched/piptools/repositories/pypi.py
-index 1c4b943..84077f0 100644
+index bf69803..eb20560 100644
 --- a/pipenv/patched/piptools/repositories/pypi.py
 +++ b/pipenv/patched/piptools/repositories/pypi.py
-@@ -1,9 +1,10 @@
+@@ -1,7 +1,7 @@
  # coding: utf-8
  from __future__ import (absolute_import, division, print_function,
                          unicode_literals)
@@ -30,40 +30,47 @@ index 1c4b943..84077f0 100644
 +import copy
  import hashlib
  import os
-+import sys
  from contextlib import contextmanager
- from shutil import rmtree
- 
-@@ -15,13 +16,22 @@ from .._compat import (
+@@ -15,26 +15,70 @@ from .._compat import (
      Wheel,
      FAVORITE_HASH,
      TemporaryDirectory,
 -    PyPI
 +    PyPI,
 +    InstallRequirement,
-+    SafeFileCache,
++    SafeFileCache
  )
++os.environ["PIP_SHIMS_BASE_MODULE"] = "notpip"
++from pip_shims.shims import pip_import, VcsSupport, WheelCache
++from packaging.requirements import Requirement
++from packaging.specifiers import SpecifierSet, Specifier
++from packaging.markers import Op, Value, Variable, Marker
++InstallationError = pip_import("InstallationError", "exceptions.InstallationError", "7.0", "9999")
++from notpip._internal.resolve import Resolver as PipResolver
++
  
 -from ..cache import CACHE_DIR
-+from pip._vendor.packaging.requirements import Requirement
-+from pip._vendor.packaging.specifiers import SpecifierSet, Specifier
-+from pip._vendor.packaging.markers import Op, Value, Variable
-+from pip._internal.exceptions import InstallationError
-+from pip._internal.vcs import VcsSupport
-+
-+from pipenv.environments import PIPENV_CACHE_DIR
++from pipenv.environments import PIPENV_CACHE_DIR as CACHE_DIR
  from ..exceptions import NoCandidateFound
- from ..utils import (fs_str, is_pinned_requirement, lookup_table,
+-from ..utils import (fs_str, is_pinned_requirement, lookup_table,
 -                     make_install_requirement)
++from ..utils import (fs_str, is_pinned_requirement, lookup_table, dedup,
 +                     make_install_requirement, clean_requires_python)
-+
  from .base import BaseRepository
  
+ try:
+-    from pip._internal.req.req_tracker import RequirementTracker
++    from notpip._internal.req.req_tracker import RequirementTracker
+ except ImportError:
+     @contextmanager
+     def RequirementTracker():
+         yield
  
-@@ -37,6 +47,45 @@ except ImportError:
-     from pip.wheel import WheelCache
- 
- 
+-try:
+-    from pip._internal.cache import WheelCache
+-except ImportError:
+-    from pip.wheel import WheelCache
++
 +class HashCache(SafeFileCache):
 +    """Caches hashes of PyPI artifacts so we do not need to re-download them
 +
@@ -73,7 +80,7 @@ index 1c4b943..84077f0 100644
 +    def __init__(self, *args, **kwargs):
 +        session = kwargs.pop('session')
 +        self.session = session
-+        kwargs.setdefault('directory', os.path.join(PIPENV_CACHE_DIR, 'hash-cache'))
++        kwargs.setdefault('directory', os.path.join(CACHE_DIR, 'hash-cache'))
 +        super(HashCache, self).__init__(*args, **kwargs)
 +
 +    def get_hash(self, location):
@@ -101,12 +108,10 @@ index 1c4b943..84077f0 100644
 +            for chunk in iter(lambda: fp.read(8096), b""):
 +                h.update(chunk)
 +        return ":".join([FAVORITE_HASH, h.hexdigest()])
-+
-+
- class PyPIRepository(BaseRepository):
-     DEFAULT_INDEX_URL = PyPI.simple_url
  
-@@ -46,10 +95,11 @@ class PyPIRepository(BaseRepository):
+ 
+ class PyPIRepository(BaseRepository):
+@@ -46,8 +90,9 @@ class PyPIRepository(BaseRepository):
      config), but any other PyPI mirror can be used if index_urls is
      changed/configured on the Finder.
      """
@@ -115,12 +120,9 @@ index 1c4b943..84077f0 100644
          self.session = session
 +        self.use_json = use_json
          self.pip_options = pip_options
--        self.wheel_cache = WheelCache(CACHE_DIR, pip_options.format_control)
-+        self.wheel_cache = WheelCache(PIPENV_CACHE_DIR, pip_options.format_control)
  
          index_urls = [pip_options.index_url] + pip_options.extra_index_urls
-         if pip_options.no_index:
-@@ -74,11 +124,15 @@ class PyPIRepository(BaseRepository):
+@@ -73,6 +118,10 @@ class PyPIRepository(BaseRepository):
          # of all secondary dependencies for the given requirement, so we
          # only have to go to disk once for each requirement
          self._dependencies_cache = {}
@@ -131,20 +133,12 @@ index 1c4b943..84077f0 100644
  
          # Setup file paths
          self.freshen_build_caches()
--        self._download_dir = fs_str(os.path.join(CACHE_DIR, 'pkgs'))
--        self._wheel_download_dir = fs_str(os.path.join(CACHE_DIR, 'wheels'))
-+        self._download_dir = fs_str(os.path.join(PIPENV_CACHE_DIR, 'pkgs'))
-+        self._wheel_download_dir = fs_str(os.path.join(PIPENV_CACHE_DIR, 'wheels'))
- 
-     def freshen_build_caches(self):
-         """
-@@ -114,10 +168,14 @@ class PyPIRepository(BaseRepository):
+@@ -113,10 +162,13 @@ class PyPIRepository(BaseRepository):
          if ireq.editable:
              return ireq  # return itself as the best match
  
 -        all_candidates = self.find_all_candidates(ireq.name)
 +        all_candidates = clean_requires_python(self.find_all_candidates(ireq.name))
-+
          candidates_by_version = lookup_table(all_candidates, key=lambda c: c.version, unique=True)
 -        matching_versions = ireq.specifier.filter((candidate.version for candidate in all_candidates),
 +        try:
@@ -155,21 +149,15 @@ index 1c4b943..84077f0 100644
  
          # Reuses pip's internal candidate sort key to sort
          matching_candidates = [candidates_by_version[ver] for ver in matching_versions]
-@@ -126,11 +184,71 @@ class PyPIRepository(BaseRepository):
-         best_candidate = max(matching_candidates, key=self.finder._candidate_sort_key)
+@@ -126,25 +178,84 @@ class PyPIRepository(BaseRepository):
  
          # Turn the candidate into a pinned InstallRequirement
--        return make_install_requirement(
+         return make_install_requirement(
 -            best_candidate.project, best_candidate.version, ireq.extras, constraint=ireq.constraint
 -        )
-+        new_req = make_install_requirement(
-+            best_candidate.project, best_candidate.version, ireq.extras, ireq.markers, constraint=ireq.constraint
++            best_candidate.project, best_candidate.version, ireq.extras, ireq.markers,  constraint=ireq.constraint
 +         )
 +
-+        # KR TODO: Marker here?
-+
-+        return new_req
-+
 +    def get_json_dependencies(self, ireq):
 +
 +        if not (is_pinned_requirement(ireq)):
@@ -178,7 +166,8 @@ index 1c4b943..84077f0 100644
 +        def gen(ireq):
 +            if self.DEFAULT_INDEX_URL not in self.finder.index_urls:
 +                return
-+
+ 
+-    def resolve_reqs(self, download_dir, ireq, wheel_cache):
 +            url = 'https://pypi.org/pypi/{0}/json'.format(ireq.req.name)
 +            releases = self.session.get(url).json()['releases']
 +
@@ -211,8 +200,8 @@ index 1c4b943..84077f0 100644
 +            return set(self._json_dep_cache[ireq])
 +        except Exception:
 +            return set()
- 
-     def get_dependencies(self, ireq):
++
++    def get_dependencies(self, ireq):
 +        json_results = set()
 +
 +        if self.use_json:
@@ -226,100 +215,98 @@ index 1c4b943..84077f0 100644
 +
 +        return json_results
 +
-+    def get_legacy_dependencies(self, ireq):
-         """
-         Given a pinned or an editable InstallRequirement, returns a set of
-         dependencies (also InstallRequirements, but not necessarily pinned).
-@@ -155,20 +273,46 @@ class PyPIRepository(BaseRepository):
-                     os.makedirs(download_dir)
-             if not os.path.isdir(self._wheel_download_dir):
-                 os.makedirs(self._wheel_download_dir)
--
++    def resolve_reqs(self, download_dir, ireq, wheel_cache, setup_requires={}, dist=None):
+         results = None
++        setup_requires = {}
++        dist = None
+         try:
+-            from pip._internal.operations.prepare import RequirementPreparer
+-            from pip._internal.resolve import Resolver as PipResolver
++            from notpip._internal.operations.prepare import RequirementPreparer
+         except ImportError:
+-            # Pip 9 and below
++                # Pip 9 and below
+             reqset = RequirementSet(
+                 self.build_dir,
+                 self.source_dir,
+                 download_dir=download_dir,
+                 wheel_download_dir=self._wheel_download_dir,
+                 session=self.session,
++                ignore_installed=True,
++                ignore_compatibility=False,
+                 wheel_cache=wheel_cache
+             )
+-            results = reqset._prepare_file(self.finder, ireq)
++            results = reqset._prepare_file(self.finder, ireq, ignore_requires_python=True)
+         else:
+             # pip >= 10
+             preparer_kwargs = {
+@@ -153,19 +264,20 @@ class PyPIRepository(BaseRepository):
+                 'download_dir': download_dir,
+                 'wheel_download_dir': self._wheel_download_dir,
+                 'progress_bar': 'off',
+-                'build_isolation': False
++                'build_isolation': True
+             }
+             resolver_kwargs = {
+                 'finder': self.finder,
+                 'session': self.session,
+                 'upgrade_strategy': "to-satisfy-only",
+-                'force_reinstall': False,
++                'force_reinstall': True,
+                 'ignore_dependencies': False,
+-                'ignore_requires_python': False,
++                'ignore_requires_python': True,
+                 'ignore_installed': True,
+                 'isolated': False,
+                 'wheel_cache': wheel_cache,
+-                'use_user_site': False
++                'use_user_site': False,
++                'ignore_compatibility': True
+             }
+             resolver = None
+             preparer = None
+@@ -177,15 +289,98 @@ class PyPIRepository(BaseRepository):
+                 resolver_kwargs['preparer'] = preparer
+                 reqset = RequirementSet()
+                 ireq.is_direct = True
+-                reqset.add_requirement(ireq)
++                # reqset.add_requirement(ireq)
+                 resolver = PipResolver(**resolver_kwargs)
+                 resolver.require_hashes = False
+                 results = resolver._resolve_one(reqset, ireq)
+                 reqset.cleanup_files()
+ 
+-        return set(results)
++        if ireq.editable and (ireq.source_dir and os.path.exists(ireq.source_dir)):
 +            # Collect setup_requires info from local eggs.
 +            # Do this after we call the preparer on these reqs to make sure their
 +            # egg info has been created
-+            setup_requires = {}
-+            dist = None
-+            if ireq.editable:
-+                try:
-+                    from pipenv.utils import chdir
-+                    with chdir(ireq.setup_py_dir):
-+                        from setuptools.dist import distutils
-+                        dist = distutils.core.run_setup(ireq.setup_py)
-+                except (ImportError, InstallationError, TypeError, AttributeError):
-+                    pass
++            from pipenv.utils import chdir
++            with chdir(ireq.setup_py_dir):
 +                try:
-+                    dist = ireq.get_dist() if not dist else dist
++                    from setuptools.dist import distutils
++                    dist = distutils.core.run_setup(ireq.setup_py)
 +                except InstallationError:
 +                    ireq.run_egg_info()
-+                    dist = ireq.get_dist()
 +                except (TypeError, ValueError, AttributeError):
 +                    pass
-+                else:
-+                    setup_requires = getattr(dist, "extras_require", None)
-+                    if not setup_requires:
-+                        setup_requires = {"setup_requires": getattr(dist, "setup_requires", None)}
-             try:
--                # Pip < 9 and below
-+                # Pip 9 and below
-                 reqset = RequirementSet(
-                     self.build_dir,
-                     self.source_dir,
-                     download_dir=download_dir,
-                     wheel_download_dir=self._wheel_download_dir,
-                     session=self.session,
-+                    ignore_installed=True,
-+                    ignore_compatibility=False,
-                     wheel_cache=self.wheel_cache,
-                 )
--                self._dependencies_cache[ireq] = reqset._prepare_file(
-+                result = reqset._prepare_file(
-                     self.finder,
--                    ireq
-+                    ireq,
-+                    ignore_requires_python=True
-                 )
-             except TypeError:
-                 # Pip >= 10 (new resolver!)
-@@ -188,17 +332,97 @@ class PyPIRepository(BaseRepository):
-                     finder=self.finder,
-                     session=self.session,
-                     upgrade_strategy="to-satisfy-only",
--                    force_reinstall=False,
-+                    force_reinstall=True,
-                     ignore_dependencies=False,
--                    ignore_requires_python=False,
-+                    ignore_requires_python=True,
-                     ignore_installed=True,
-                     isolated=False,
-                     wheel_cache=self.wheel_cache,
-                     use_user_site=False,
-+                    ignore_compatibility=False
-                 )
-                 self.resolver.resolve(reqset)
--                self._dependencies_cache[ireq] = reqset.requirements.values()
-+                result = set(reqset.requirements.values())
-+
-+            # HACK: Sometimes the InstallRequirement doesn't properly get
-+            # these values set on it during the resolution process. It's
-+            # difficult to pin down what is going wrong. This fixes things.
-+            if not getattr(ireq, 'version', None):
-+                try:
-+                    dist = ireq.get_dist() if not dist else None
-+                    ireq.version = ireq.get_dist().version
-+                except (ValueError, OSError, TypeError, AttributeError) as e:
-+                    pass
-+            if not getattr(ireq, 'project_name', None):
-+                try:
-+                    ireq.project_name = dist.project_name if dist else None
-+                except (ValueError, TypeError) as e:
-+                    pass
++                if not dist:
++                    try:
++                        dist = ireq.get_dist()
++                    except (ImportError, ValueError, TypeError, AttributeError):
++                        pass
++        if ireq.editable and dist:
++            setup_requires = getattr(dist, "extras_require", None)
++            if not setup_requires:
++                setup_requires = {"setup_requires": getattr(dist, "setup_requires", None)}
 +            if not getattr(ireq, 'req', None):
 +                try:
 +                    ireq.req = dist.as_requirement() if dist else None
 +                except (ValueError, TypeError) as e:
 +                    pass
-+
+ 
+-    def get_dependencies(self, ireq):
 +            # Convert setup_requires dict into a somewhat usable form.
 +            if setup_requires:
 +                for section in setup_requires:
@@ -341,14 +328,14 @@ index 1c4b943..84077f0 100644
 +                        if ':' not in value:
 +                            try:
 +                                if not not_python:
-+                                    result = result + [InstallRequirement.from_line("{0}{1}".format(value, python_version).replace(':', ';'))]
++                                    results.add(InstallRequirement.from_line("{0}{1}".format(value, python_version).replace(':', ';')))
 +                            # Anything could go wrong here -- can't be too careful.
 +                            except Exception:
 +                                pass
 +
 +            # this section properly creates 'python_version' markers for cross-python
 +            # virtualenv creation and for multi-python compatibility.
-+            requires_python = reqset.requires_python if hasattr(reqset, 'requires_python') else self.resolver.requires_python
++            requires_python = reqset.requires_python if hasattr(reqset, 'requires_python') else resolver.requires_python
 +            if requires_python:
 +                marker_str = ''
 +                # This corrects a logic error from the previous code which said that if
@@ -358,31 +345,48 @@ index 1c4b943..84077f0 100644
 +                if any(requires_python.startswith(op) for op in Specifier._operators.keys()):
 +                    # We are checking first if we have  leading specifier operator
 +                    # if not, we can assume we should be doing a == comparison
-+                    specifierset = list(SpecifierSet(requires_python))
++                    specifierset = SpecifierSet(requires_python)
 +                    # for multiple specifiers, the correct way to represent that in
 +                    # a specifierset is `Requirement('fakepkg; python_version<"3.0,>=2.6"')`
-+                    marker_key = Variable('python_version')
-+                    markers = []
-+                    for spec in specifierset:
-+                        operator, val = spec._spec
-+                        operator = Op(operator)
-+                        val = Value(val)
-+                        markers.append(''.join([marker_key.serialize(), operator.serialize(), val.serialize()]))
-+                    marker_str = ' and '.join(markers)
++                    from passa.internals.specifiers import cleanup_pyspecs
++                    marker_str = str(Marker(" and ".join(dedup([
++                        "python_version {0[0]} '{0[1]}'".format(spec)
++                        for spec in cleanup_pyspecs(specifierset)
++                    ]))))
 +                # The best way to add markers to a requirement is to make a separate requirement
 +                # with only markers on it, and then to transfer the object istelf
 +                marker_to_add = Requirement('fakepkg; {0}'.format(marker_str)).marker
-+                result.remove(ireq)
++                if ireq in results:
++                    results.remove(ireq)
++                print(marker_to_add)
 +                ireq.req.marker = marker_to_add
-+                result.add(ireq)
 +
-+            self._dependencies_cache[ireq] = result
-             reqset.cleanup_files()
++        results = set(results) if results else set()
++        return results, ireq
 +
-         return set(self._dependencies_cache[ireq])
- 
-     def get_hashes(self, ireq):
-@@ -210,6 +434,10 @@ class PyPIRepository(BaseRepository):
++    def get_legacy_dependencies(self, ireq):
+         """
+         Given a pinned or an editable InstallRequirement, returns a set of
+         dependencies (also InstallRequirements, but not necessarily pinned).
+@@ -200,6 +395,7 @@ class PyPIRepository(BaseRepository):
+                 # If a download_dir is passed, pip will  unnecessarely
+                 # archive the entire source directory
+                 download_dir = None
++
+             elif ireq.link and not ireq.link.is_artifact:
+                 # No download_dir for VCS sources.  This also works around pip
+                 # using git-checkout-index, which gets rid of the .git dir.
+@@ -214,7 +410,8 @@ class PyPIRepository(BaseRepository):
+             wheel_cache = WheelCache(CACHE_DIR, self.pip_options.format_control)
+             prev_tracker = os.environ.get('PIP_REQ_TRACKER')
+             try:
+-                self._dependencies_cache[ireq] = self.resolve_reqs(download_dir, ireq, wheel_cache)
++                results, ireq = self.resolve_reqs(download_dir, ireq, wheel_cache)
++                self._dependencies_cache[ireq] = results
+             finally:
+                 if 'PIP_REQ_TRACKER' in os.environ:
+                     if prev_tracker:
+@@ -236,6 +433,10 @@ class PyPIRepository(BaseRepository):
          if ireq.editable:
              return set()
  
@@ -393,7 +397,7 @@ index 1c4b943..84077f0 100644
          if not is_pinned_requirement(ireq):
              raise TypeError(
                  "Expected pinned requirement, got {}".format(ireq))
-@@ -217,24 +445,22 @@ class PyPIRepository(BaseRepository):
+@@ -243,24 +444,22 @@ class PyPIRepository(BaseRepository):
          # We need to get all of the candidates that match our current version
          # pin, these will represent all of the files that could possibly
          # satisfy this constraint.
