commit b2957af35ce5aff3f774dad1a1d413f2237d7106
Author: Dan Ryan <dan@danryan.co>
Date:   Fri Sep 7 02:29:56 2018 -0400

    Update passa
    
    Signed-off-by: Dan Ryan <dan@danryan.co>

diff --git a/pipenv/vendor/passa/LICENSE b/pipenv/vendor/passa/LICENSE
new file mode 100644
index 00000000..e1a278e7
--- /dev/null
+++ b/pipenv/vendor/passa/LICENSE
@@ -0,0 +1,13 @@
+Copyright (c) 2018, Dan Ryan <dan@danryan.co>
+
+Permission to use, copy, modify, and distribute this software for any
+purpose with or without fee is hereby granted, provided that the above
+copyright notice and this permission notice appear in all copies.
+
+THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
diff --git a/pipenv/vendor/passa/__init__.py b/pipenv/vendor/passa/__init__.py
index 6f92267d..ea633f0a 100644
--- a/pipenv/vendor/passa/__init__.py
+++ b/pipenv/vendor/passa/__init__.py
@@ -4,4 +4,4 @@ __all__ = [
     '__version__'
 ]
 
-__version__ = '0.3.0'
+__version__ = '0.3.1.dev0'
diff --git a/pipenv/vendor/passa/_pip.py b/pipenv/vendor/passa/_pip.py
deleted file mode 100644
index 5cf1cea8..00000000
--- a/pipenv/vendor/passa/_pip.py
+++ /dev/null
@@ -1,315 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import contextlib
-import distutils.log
-import os
-
-import setuptools.dist
-
-import distlib.scripts
-import distlib.wheel
-import pip_shims
-import six
-import vistir
-
-from ._pip_shims import VCS_SUPPORT, build_wheel as _build_wheel, unpack_url
-from .caches import CACHE_DIR
-from .utils import filter_sources
-
-
-@vistir.path.ensure_mkdir_p(mode=0o775)
-def _get_src_dir():
-    src = os.environ.get("PIP_SRC")
-    if src:
-        return src
-    virtual_env = os.environ.get("VIRTUAL_ENV")
-    if virtual_env:
-        return os.path.join(virtual_env, "src")
-    temp_src = vistir.path.create_tracked_tempdir(prefix='passa-src')
-    return temp_src
-
-
-def _prepare_wheel_building_kwargs(ireq):
-    download_dir = os.path.join(CACHE_DIR, "pkgs")
-    vistir.mkdir_p(download_dir)
-
-    wheel_download_dir = os.path.join(CACHE_DIR, "wheels")
-    vistir.mkdir_p(wheel_download_dir)
-
-    if ireq.source_dir is None:
-        src_dir = _get_src_dir()
-    else:
-        src_dir = ireq.source_dir
-
-    # This logic matches pip's behavior, although I don't fully understand the
-    # intention. I guess the idea is to build editables in-place, otherwise out
-    # of the source tree?
-    if ireq.editable:
-        build_dir = src_dir
-    else:
-        build_dir = vistir.path.create_tracked_tempdir(prefix="passa-build")
-
-    return {
-        "build_dir": build_dir,
-        "src_dir": src_dir,
-        "download_dir": download_dir,
-        "wheel_download_dir": wheel_download_dir,
-    }
-
-
-def _get_pip_index_urls(sources):
-    index_urls = []
-    trusted_hosts = []
-    for source in sources:
-        url = source.get("url")
-        if not url:
-            continue
-        index_urls.append(url)
-        if source.get("verify_ssl", True):
-            continue
-        host = six.moves.urllib.parse.urlparse(source["url"]).hostname
-        trusted_hosts.append(host)
-    return index_urls, trusted_hosts
-
-
-class _PipCommand(pip_shims.Command):
-    name = "PipCommand"
-
-
-def _get_pip_session(trusted_hosts):
-    cmd = _PipCommand()
-    options, _ = cmd.parser.parse_args([])
-    options.cache_dir = CACHE_DIR
-    options.trusted_hosts = trusted_hosts
-    session = cmd._build_session(options)
-    return session
-
-
-def _get_finder(sources):
-    index_urls, trusted_hosts = _get_pip_index_urls(sources)
-    session = _get_pip_session(trusted_hosts)
-    finder = pip_shims.PackageFinder(
-        find_links=[],
-        index_urls=index_urls,
-        trusted_hosts=trusted_hosts,
-        allow_all_prereleases=True,
-        session=session,
-    )
-    return finder
-
-
-def _get_wheel_cache():
-    format_control = pip_shims.FormatControl(set(), set())
-    wheel_cache = pip_shims.WheelCache(CACHE_DIR, format_control)
-    return wheel_cache
-
-
-def _convert_hashes(values):
-    """Convert Pipfile.lock hash lines into InstallRequirement option format.
-
-    The option format uses a str-list mapping. Keys are hash algorithms, and
-    the list contains all values of that algorithm.
-    """
-    hashes = {}
-    if not values:
-        return hashes
-    for value in values:
-        try:
-            name, value = value.split(":", 1)
-        except ValueError:
-            name = "sha256"
-        if name not in hashes:
-            hashes[name] = []
-        hashes[name].append(value)
-    return hashes
-
-
-def build_wheel(ireq, sources, hashes=None):
-    """Build a wheel file for the InstallRequirement object.
-
-    An artifact is downloaded (or read from cache). If the artifact is not a
-    wheel, build one out of it. The dynamically built wheel is ephemeral; do
-    not depend on its existence after the returned wheel goes out of scope.
-
-    If `hashes` is truthy, it is assumed to be a list of hashes (as formatted
-    in Pipfile.lock) to be checked against the download.
-
-    Returns a `distlib.wheel.Wheel` instance. Raises a `RuntimeError` if the
-    wheel cannot be built.
-    """
-    kwargs = _prepare_wheel_building_kwargs(ireq)
-    finder = _get_finder(sources)
-
-    # Not for upgrade, hash not required. Hashes are not required here even
-    # when we provide them, because pip skips local wheel cache if we set it
-    # to True. Hashes are checked later if we need to download the file.
-    ireq.populate_link(finder, False, False)
-
-    # Ensure ireq.source_dir is set.
-    # This is intentionally set to build_dir, not src_dir. Comments from pip:
-    #   [...] if filesystem packages are not marked editable in a req, a non
-    #   deterministic error occurs when the script attempts to unpack the
-    #   build directory.
-    # Also see comments in `_prepare_wheel_building_kwargs()` -- If the ireq
-    # is editable, build_dir is actually src_dir, making the build in-place.
-    ireq.ensure_has_source_dir(kwargs["build_dir"])
-
-    # Ensure the remote artifact is downloaded locally. For wheels, it is
-    # enough to just download because we'll use them directly. For an sdist,
-    # we need to unpack so we can build it.
-    if not pip_shims.is_file_url(ireq.link):
-        if ireq.is_wheel:
-            only_download = True
-            download_dir = kwargs["wheel_download_dir"]
-        else:
-            only_download = False
-            download_dir = kwargs["download_dir"]
-        ireq.options["hashes"] = _convert_hashes(hashes)
-        unpack_url(
-            ireq.link, ireq.source_dir, download_dir,
-            only_download=only_download, session=finder.session,
-            hashes=ireq.hashes(False), progress_bar=False,
-        )
-
-    if ireq.is_wheel:
-        # If this is a wheel, use the downloaded thing.
-        output_dir = kwargs["wheel_download_dir"]
-        wheel_path = os.path.join(output_dir, ireq.link.filename)
-    else:
-        # Othereise we need to build an ephemeral wheel.
-        wheel_path = _build_wheel(
-            ireq, vistir.path.create_tracked_tempdir(prefix="ephem"),
-            finder, _get_wheel_cache(), kwargs,
-        )
-        if wheel_path is None or not os.path.exists(wheel_path):
-            raise RuntimeError("failed to build wheel from {}".format(ireq))
-    return distlib.wheel.Wheel(wheel_path)
-
-
-def _obtrain_ref(vcs_obj, src_dir, name, rev=None):
-    target_dir = os.path.join(src_dir, name)
-    target_rev = vcs_obj.make_rev_options(rev)
-    if not os.path.exists(target_dir):
-        vcs_obj.obtain(target_dir)
-    if (not vcs_obj.is_commit_id_equal(target_dir, rev) and
-            not vcs_obj.is_commit_id_equal(target_dir, target_rev)):
-        vcs_obj.update(target_dir, target_rev)
-    return vcs_obj.get_revision(target_dir)
-
-
-def get_vcs_ref(requirement):
-    backend = VCS_SUPPORT._registry.get(requirement.vcs)
-    vcs = backend(url=requirement.req.vcs_uri)
-    src = _get_src_dir()
-    name = requirement.normalized_name
-    ref = _obtrain_ref(vcs, src, name, rev=requirement.req.ref)
-    return ref
-
-
-def find_installation_candidates(ireq, sources):
-    finder = _get_finder(sources)
-    return finder.find_all_candidates(ireq.name)
-
-
-class RequirementUninstallation(object):
-    """A context manager to remove a package for the inner block.
-
-    This uses `UninstallPathSet` to control the workflow. If the inner block
-    exits correctly, the uninstallation is committed, otherwise rolled back.
-    """
-    def __init__(self, ireq, auto_confirm, verbose):
-        self.ireq = ireq
-        self.pathset = None
-        self.auto_confirm = auto_confirm
-        self.verbose = verbose
-
-    def __enter__(self):
-        self.pathset = self.ireq.uninstall(
-            auto_confirm=self.auto_confirm,
-            verbose=self.verbose,
-        )
-        return self.pathset
-
-    def __exit__(self, exc_type, exc_value, traceback):
-        if self.pathset is None:
-            return
-        if exc_type is None:
-            self.pathset.commit()
-        else:
-            self.pathset.rollback()
-
-
-def uninstall_requirement(ireq, **kwargs):
-    return RequirementUninstallation(ireq, **kwargs)
-
-
-@contextlib.contextmanager
-def _suppress_distutils_logs():
-    """Hack to hide noise generated by `setup.py develop`.
-
-    There isn't a good way to suppress them now, so let's monky-patch.
-    See https://bugs.python.org/issue25392.
-    """
-    f = distutils.log.Log._log
-
-    def _log(log, level, msg, args):
-        if level >= distutils.log.ERROR:
-            f(log, level, msg, args)
-
-    distutils.log.Log._log = _log
-    yield
-    distutils.log.Log._log = f
-
-
-class NoopInstaller(object):
-    """An installer.
-
-    This class is not designed to be instantiated by itself, but used as a
-    common interface for subclassing.
-
-    An installer has two methods, `prepare()` and `install()`. Neither takes
-    arguments, and should be called in that order to prepare an installation
-    operation, and to actually install things.
-    """
-    def prepare(self):
-        pass
-
-    def install(self):
-        pass
-
-
-class EditableInstaller(NoopInstaller):
-    """Installer to handle editable.
-    """
-    def __init__(self, requirement):
-        ireq = requirement.as_ireq()
-        self.working_directory = ireq.setup_py_dir
-        self.setup_py = ireq.setup_py
-
-    def install(self):
-        with vistir.cd(self.working_directory), _suppress_distutils_logs():
-            # Access from Setuptools to ensure things are patched correctly.
-            setuptools.dist.distutils.core.run_setup(
-                self.setup_py, ["develop", "--no-deps"],
-            )
-
-
-class WheelInstaller(NoopInstaller):
-    """Installer by building a wheel.
-
-    The wheel is built during `prepare()`, and installed in `install()`.
-    """
-    def __init__(self, requirement, sources, paths):
-        self.ireq = requirement.as_ireq()
-        self.sources = filter_sources(requirement, sources)
-        self.hashes = requirement.hashes or None
-        self.paths = paths
-        self.wheel = None
-
-    def prepare(self):
-        self.wheel = build_wheel(self.ireq, self.sources, self.hashes)
-
-    def install(self):
-        self.wheel.install(self.paths, distlib.scripts.ScriptMaker(None, None))
diff --git a/pipenv/vendor/passa/_pip_shims.py b/pipenv/vendor/passa/_pip_shims.py
deleted file mode 100644
index b2c7b6ea..00000000
--- a/pipenv/vendor/passa/_pip_shims.py
+++ /dev/null
@@ -1,61 +0,0 @@
-# -*- coding=utf-8 -*-
-
-"""Shims to make the pip interface more consistent accross versions.
-
-There are currently two members:
-
-* VCS_SUPPORT is an instance of VcsSupport.
-* build_wheel abstracts the process to build a wheel out of a bunch parameters.
-* unpack_url wraps the actual function in pip to accept modern parameters.
-"""
-
-from __future__ import absolute_import, unicode_literals
-
-import pip_shims
-
-
-def _build_wheel_pre10(ireq, output_dir, finder, wheel_cache, kwargs):
-    kwargs.update({"wheel_cache": wheel_cache, "session": finder.session})
-    reqset = pip_shims.RequirementSet(**kwargs)
-    builder = pip_shims.WheelBuilder(reqset, finder)
-    return builder._build_one(ireq, output_dir)
-
-
-def _build_wheel_modern(ireq, output_dir, finder, wheel_cache, kwargs):
-    """Build a wheel.
-
-    * ireq: The InstallRequirement object to build
-    * output_dir: The directory to build the wheel in.
-    * finder: pip's internal Finder object to find the source out of ireq.
-    * kwargs: Various keyword arguments from `_prepare_wheel_building_kwargs`.
-    """
-    kwargs.update({"progress_bar": "off", "build_isolation": False})
-    with pip_shims.RequirementTracker() as req_tracker:
-        if req_tracker:
-            kwargs["req_tracker"] = req_tracker
-        preparer = pip_shims.RequirementPreparer(**kwargs)
-        builder = pip_shims.WheelBuilder(finder, preparer, wheel_cache)
-        return builder._build_one(ireq, output_dir)
-
-
-def _unpack_url_pre10(*args, **kwargs):
-    """Shim for unpack_url in various pip versions.
-
-    pip before 10.0 does not accept `progress_bar` here. Simply drop it.
-    """
-    kwargs.pop("progress_bar", None)
-    return pip_shims.unpack_url(*args, **kwargs)
-
-
-PIP_VERSION = pip_shims.utils._parse(pip_shims.pip_version)
-VERSION_10 = pip_shims.utils._parse("10")
-
-
-VCS_SUPPORT = pip_shims.VcsSupport()
-
-build_wheel = _build_wheel_modern
-unpack_url = pip_shims.unpack_url
-
-if PIP_VERSION < VERSION_10:
-    build_wheel = _build_wheel_pre10
-    unpack_url = _unpack_url_pre10
diff --git a/pipenv/vendor/passa/caches.py b/pipenv/vendor/passa/caches.py
deleted file mode 100644
index 6d3131fa..00000000
--- a/pipenv/vendor/passa/caches.py
+++ /dev/null
@@ -1,214 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import copy
-import hashlib
-import json
-import os
-import sys
-
-import appdirs
-import pip_shims
-import requests
-import vistir
-
-from ._pip_shims import VCS_SUPPORT
-from .utils import get_pinned_version
-
-
-CACHE_DIR = os.environ.get("PASSA_CACHE_DIR", appdirs.user_cache_dir("passa"))
-
-
-class HashCache(pip_shims.SafeFileCache):
-    """Caches hashes of PyPI artifacts so we do not need to re-download them.
-
-    Hashes are only cached when the URL appears to contain a hash in it and the
-    cache key includes the hash value returned from the server). This ought to
-    avoid ssues where the location on the server changes.
-    """
-    def __init__(self, *args, **kwargs):
-        session = kwargs.pop('session', requests.session())
-        self.session = session
-        kwargs.setdefault('directory', os.path.join(CACHE_DIR, 'hash-cache'))
-        super(HashCache, self).__init__(*args, **kwargs)
-
-    def get_hash(self, location):
-        # If there is no location hash (i.e., md5, sha256, etc.), we don't want
-        # to store it.
-        hash_value = None
-        orig_scheme = location.scheme
-        new_location = copy.deepcopy(location)
-        if orig_scheme in VCS_SUPPORT.all_schemes:
-            new_location.url = new_location.url.split("+", 1)[-1]
-        can_hash = new_location.hash
-        if can_hash:
-            # hash url WITH fragment
-            hash_value = self.get(new_location.url)
-        if not hash_value:
-            hash_value = self._get_file_hash(new_location)
-            hash_value = hash_value.encode('utf8')
-        if can_hash:
-            self.set(new_location.url, hash_value)
-        return hash_value.decode('utf8')
-
-    def _get_file_hash(self, location):
-        h = hashlib.new(pip_shims.FAVORITE_HASH)
-        with vistir.open_file(location, self.session) as fp:
-            for chunk in iter(lambda: fp.read(8096), b""):
-                h.update(chunk)
-        return ":".join([h.name, h.hexdigest()])
-
-
-# pip-tools's dependency cache implementation.
-class CorruptCacheError(Exception):
-    def __init__(self, path):
-        self.path = path
-
-    def __str__(self):
-        lines = [
-            'The dependency cache seems to have been corrupted.',
-            'Inspect, or delete, the following file:',
-            '  {}'.format(self.path),
-        ]
-        return os.linesep.join(lines)
-
-
-def _key_from_req(req):
-    """Get an all-lowercase version of the requirement's name."""
-    if hasattr(req, 'key'):
-        # from pkg_resources, such as installed dists for pip-sync
-        key = req.key
-    else:
-        # from packaging, such as install requirements from requirements.txt
-        key = req.name
-
-    key = key.replace('_', '-').lower()
-    return key
-
-
-def _read_cache_file(cache_file_path):
-    with open(cache_file_path, 'r') as cache_file:
-        try:
-            doc = json.load(cache_file)
-        except ValueError:
-            raise CorruptCacheError(cache_file_path)
-
-        # Check version and load the contents
-        assert doc['__format__'] == 1, 'Unknown cache file format'
-        return doc['dependencies']
-
-
-class _JSONCache(object):
-    """A persistent cache backed by a JSON file.
-
-    The cache file is written to the appropriate user cache dir for the
-    current platform, i.e.
-
-        ~/.cache/pip-tools/depcache-pyX.Y.json
-
-    Where X.Y indicates the Python version.
-    """
-    filename_format = None
-
-    def __init__(self, cache_dir=CACHE_DIR):
-        vistir.mkdir_p(cache_dir)
-        python_version = ".".join(str(digit) for digit in sys.version_info[:2])
-        cache_filename = self.filename_format.format(
-            python_version=python_version,
-        )
-        self._cache_file = os.path.join(cache_dir, cache_filename)
-        self._cache = None
-
-    @property
-    def cache(self):
-        """The dictionary that is the actual in-memory cache.
-
-        This property lazily loads the cache from disk.
-        """
-        if self._cache is None:
-            self.read_cache()
-        return self._cache
-
-    def as_cache_key(self, ireq):
-        """Given a requirement, return its cache key.
-
-        This behavior is a little weird in order to allow backwards
-        compatibility with cache files. For a requirement without extras, this
-        will return, for example::
-
-            ("ipython", "2.1.0")
-
-        For a requirement with extras, the extras will be comma-separated and
-        appended to the version, inside brackets, like so::
-
-            ("ipython", "2.1.0[nbconvert,notebook]")
-        """
-        extras = tuple(sorted(ireq.extras))
-        if not extras:
-            extras_string = ""
-        else:
-            extras_string = "[{}]".format(",".join(extras))
-        name = _key_from_req(ireq.req)
-        version = get_pinned_version(ireq)
-        return name, "{}{}".format(version, extras_string)
-
-    def read_cache(self):
-        """Reads the cached contents into memory.
-        """
-        if os.path.exists(self._cache_file):
-            self._cache = _read_cache_file(self._cache_file)
-        else:
-            self._cache = {}
-
-    def write_cache(self):
-        """Writes the cache to disk as JSON.
-        """
-        doc = {
-            '__format__': 1,
-            'dependencies': self._cache,
-        }
-        with open(self._cache_file, 'w') as f:
-            json.dump(doc, f, sort_keys=True)
-
-    def clear(self):
-        self._cache = {}
-        self.write_cache()
-
-    def __contains__(self, ireq):
-        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
-        return pkgversion_and_extras in self.cache.get(pkgname, {})
-
-    def __getitem__(self, ireq):
-        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
-        return self.cache[pkgname][pkgversion_and_extras]
-
-    def __setitem__(self, ireq, values):
-        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
-        self.cache.setdefault(pkgname, {})
-        self.cache[pkgname][pkgversion_and_extras] = values
-        self.write_cache()
-
-    def __delitem__(self, ireq):
-        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
-        try:
-            del self.cache[pkgname][pkgversion_and_extras]
-        except KeyError:
-            return
-        self.write_cache()
-
-    def get(self, ireq, default=None):
-        pkgname, pkgversion_and_extras = self.as_cache_key(ireq)
-        return self.cache.get(pkgname, {}).get(pkgversion_and_extras, default)
-
-
-class DependencyCache(_JSONCache):
-    """Cache the dependency of cancidates.
-    """
-    filename_format = "depcache-py{python_version}.json"
-
-
-class RequiresPythonCache(_JSONCache):
-    """Cache a candidate's Requires-Python information.
-    """
-    filename_format = "pyreqcache-py{python_version}.json"
diff --git a/pipenv/vendor/passa/candidates.py b/pipenv/vendor/passa/candidates.py
deleted file mode 100644
index d5390d65..00000000
--- a/pipenv/vendor/passa/candidates.py
+++ /dev/null
@@ -1,81 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import os
-import sys
-
-import packaging.specifiers
-import packaging.version
-import requirementslib
-
-from ._pip import find_installation_candidates, get_vcs_ref
-
-
-def _filter_matching_python_requirement(candidates, python_version):
-    for c in candidates:
-        try:
-            requires_python = c.requires_python
-        except AttributeError:
-            requires_python = c.location.requires_python
-        if python_version and requires_python:
-            # Old specifications had people setting this to single digits
-            # which is effectively the same as '>=digit,<digit+1'
-            if requires_python.isdigit():
-                requires_python = '>={0},<{1}'.format(
-                    requires_python, int(requires_python) + 1,
-                )
-            try:
-                specset = packaging.specifiers.SpecifierSet(requires_python)
-            except packaging.specifiers.InvalidSpecifier:
-                continue
-            if not specset.contains(python_version):
-                continue
-        yield c
-
-
-def _copy_requirement(requirement):
-    return requirement.copy()
-
-
-def _requirement_from_metadata(name, version, extras, index):
-    # Markers are intentionally dropped here. They will be added to candidates
-    # after resolution, so we can perform marker aggregation.
-    r = requirementslib.Requirement.from_metadata(name, version, extras, None)
-    r.index = index
-    return r
-
-
-def find_candidates(requirement, sources, allow_pre):
-    # A non-named requirement has exactly one candidate that is itself. For
-    # VCS, we also lock the requirement to an exact ref.
-    if not requirement.is_named:
-        candidate = _copy_requirement(requirement)
-        if candidate.is_vcs:
-            candidate.req.ref = get_vcs_ref(candidate)
-        return [candidate]
-
-    ireq = requirement.as_ireq()
-    icans = find_installation_candidates(ireq, sources)
-
-    python_version = os.environ.get(
-        "PASSA_PYTHON_VERSION",
-        "{0[0]}.{0[1]}".format(sys.version_info),
-    )
-    if python_version != ":all:":
-        matching_icans = list(_filter_matching_python_requirement(
-            icans, packaging.version.parse(python_version),
-        ))
-        icans = matching_icans or icans
-
-    versions = ireq.specifier.filter((c.version for c in icans), allow_pre)
-    if not allow_pre and not versions:
-        versions = ireq.specifier.filter((c.version for c in icans), True)
-
-    name = requirement.normalized_name
-    extras = requirement.extras
-    index = requirement.index
-    return [
-        _requirement_from_metadata(name, version, extras, index)
-        for version in sorted(versions)
-    ]
diff --git a/pipenv/vendor/passa/cli.py b/pipenv/vendor/passa/cli.py
deleted file mode 100644
index c750b2f6..00000000
--- a/pipenv/vendor/passa/cli.py
+++ /dev/null
@@ -1,115 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import argparse
-import collections
-import functools
-import io
-import os
-
-import plette
-import requirementslib
-import six
-
-from . import operations
-
-
-_DEFAULT_NEWLINES = "\n"
-
-
-def _preferred_newlines(f):
-    if isinstance(f.newlines, six.text_type):
-        return f.newlines
-    return _DEFAULT_NEWLINES
-
-
-FileModel = collections.namedtuple("FileModel", "model location newline")
-Project = collections.namedtuple("Project", "root pipfile lockfile")
-
-
-def _build_project(root):
-    root = os.path.abspath(root)
-    pipfile_location = os.path.join(root, "Pipfile")
-    with io.open(pipfile_location, encoding="utf-8") as f:
-        pipfile = plette.Pipfile.load(f)
-        pipfile_le = _preferred_newlines(f)
-
-    lockfile_location = os.path.join(root, "Pipfile.lock")
-    if os.path.exists(lockfile_location):
-        with io.open(lockfile_location, encoding="utf-8") as f:
-            lockfile = plette.Lockfile.load(f)
-            lockfile_le = _preferred_newlines(f)
-    else:
-        lockfile = None
-        lockfile_le = _DEFAULT_NEWLINES
-
-    return Project(
-        root=root,
-        pipfile=FileModel(pipfile, pipfile_location, pipfile_le),
-        lockfile=FileModel(lockfile, lockfile_location, lockfile_le),
-    )
-
-
-def locking_subparser(name):
-
-    def decorator(f):
-
-        @functools.wraps(f)
-        def wrapped(subs):
-            parser = subs.add_parser(name)
-            parser.set_defaults(_cmdkey=name)
-            parser.add_argument(
-                "project",
-                type=_build_project,
-            )
-            parser.add_argument(
-                "-o", "--output",
-                choices=["write", "print", "none"],
-                default="print",
-                help="How to output the lockfile",
-            )
-            f(parser)
-
-        return wrapped
-
-    return decorator
-
-
-@locking_subparser("add")
-def add_parser(parser):
-    parser.add_argument(
-        "requirement",
-        nargs="+", type=requirementslib.Requirement.from_line,
-        help="Requirement(s) to add",
-    )
-
-
-@locking_subparser("lock")
-def lock_parser(parser):
-    parser.add_argument(
-        "--force",
-        action="store_true", default=False,
-        help="Always re-generate lock file",
-    )
-
-
-def get_parser():
-    parser = argparse.ArgumentParser(prog="passa")
-    subs = parser.add_subparsers()
-    lock_parser(subs)
-    return parser
-
-
-def parse_arguments(argv):
-    parser = get_parser()
-    return parser.parse_args(argv)
-
-
-def main(argv=None):
-    options = parse_arguments(argv)
-    operations.main(options)
-
-
-if __name__ == "__main__":
-    main()
diff --git a/pipenv/vendor/passa/cli/_base.py b/pipenv/vendor/passa/cli/_base.py
index 6000afee..68e0e34d 100644
--- a/pipenv/vendor/passa/cli/_base.py
+++ b/pipenv/vendor/passa/cli/_base.py
@@ -6,12 +6,31 @@ import argparse
 import os
 import sys
 
+import tomlkit.exceptions
+
 
 def build_project(root):
     # This is imported lazily to reduce import overhead. Not evey command
     # needs the project instance.
-    from passa.projects import Project
-    return Project(os.path.abspath(root))
+    from passa.internals.projects import Project
+    root = os.path.abspath(root)
+    if not os.path.isfile(os.path.join(root, "Pipfile")):
+        raise argparse.ArgumentError(
+            "{0!r} is not a Pipfile project".format(root),
+        )
+    try:
+        project = Project(root)
+    except tomlkit.exceptions.ParseError as e:
+        raise argparse.ArgumentError(
+            "failed to parse Pipfile: {0!r}".format(str(e)),
+        )
+    return project
+
+
+# Better error reporting. Recent argparse would emit something like
+# "invalid project root value: 'xxxxxx'". The str() wrapper is needed to
+# keep Python 2 happy :(
+build_project.__name__ = str("project root")
 
 
 class BaseCommand(object):
diff --git a/pipenv/vendor/passa/cli/add.py b/pipenv/vendor/passa/cli/add.py
index 1ea0f09a..26ce0ed8 100644
--- a/pipenv/vendor/passa/cli/add.py
+++ b/pipenv/vendor/passa/cli/add.py
@@ -9,7 +9,7 @@ from ._base import BaseCommand
 
 
 def main(options):
-    from passa.lockers import PinReuseLocker
+    from passa.internals.lockers import PinReuseLocker
     from passa.operations.lock import lock
 
     lines = list(itertools.chain(
@@ -41,8 +41,8 @@ def main(options):
     if not options.sync:
         return
 
+    from passa.internals.synchronizers import Synchronizer
     from passa.operations.sync import sync
-    from passa.synchronizers import Synchronizer
 
     lockfile_diff = project.difference_lockfile(prev_lockfile)
     default = bool(any(lockfile_diff.default))
diff --git a/pipenv/vendor/passa/cli/clean.py b/pipenv/vendor/passa/cli/clean.py
index 90dbe733..cd1b679b 100644
--- a/pipenv/vendor/passa/cli/clean.py
+++ b/pipenv/vendor/passa/cli/clean.py
@@ -6,8 +6,8 @@ from ._base import BaseCommand
 
 
 def main(options):
+    from passa.internals.synchronizers import Cleaner
     from passa.operations.sync import clean
-    from passa.synchronizers import Cleaner
 
     project = options.project
     cleaner = Cleaner(project, default=True, develop=options.dev)
@@ -22,7 +22,7 @@ def main(options):
 class Command(BaseCommand):
 
     name = "clean"
-    description = "Uninstall unlisted packages from the current environment."
+    description = "Uninstall unlisted packages from the environment."
     parsed_main = main
 
     def add_arguments(self):
diff --git a/pipenv/vendor/passa/cli/install.py b/pipenv/vendor/passa/cli/install.py
index 47667e45..f47377b1 100644
--- a/pipenv/vendor/passa/cli/install.py
+++ b/pipenv/vendor/passa/cli/install.py
@@ -6,7 +6,7 @@ from ._base import BaseCommand
 
 
 def main(options):
-    from passa.lockers import BasicLocker
+    from passa.internals.lockers import BasicLocker
     from passa.operations.lock import lock
 
     project = options.project
@@ -19,8 +19,8 @@ def main(options):
         project._l.write()
         print("Written to project at", project.root)
 
+    from passa.internals.synchronizers import Synchronizer
     from passa.operations.sync import sync
-    from passa.synchronizers import Synchronizer
 
     syncer = Synchronizer(
         project, default=True, develop=options.dev,
diff --git a/pipenv/vendor/passa/cli/lock.py b/pipenv/vendor/passa/cli/lock.py
index 42dfcc06..67b1d118 100644
--- a/pipenv/vendor/passa/cli/lock.py
+++ b/pipenv/vendor/passa/cli/lock.py
@@ -6,7 +6,7 @@ from ._base import BaseCommand
 
 
 def main(options):
-    from passa.lockers import BasicLocker
+    from passa.internals.lockers import BasicLocker
     from passa.operations.lock import lock
 
     project = options.project
diff --git a/pipenv/vendor/passa/cli/remove.py b/pipenv/vendor/passa/cli/remove.py
index b2e03998..b1dbfd7c 100644
--- a/pipenv/vendor/passa/cli/remove.py
+++ b/pipenv/vendor/passa/cli/remove.py
@@ -6,7 +6,7 @@ from ._base import BaseCommand
 
 
 def main(options):
-    from passa.lockers import PinReuseLocker
+    from passa.internals.lockers import PinReuseLocker
     from passa.operations.lock import lock
 
     default = (options.only != "dev")
@@ -29,8 +29,8 @@ def main(options):
     if not options.clean:
         return
 
+    from passa.internals.synchronizers import Cleaner
     from passa.operations.sync import clean
-    from passa.synchronizers import Cleaner
 
     cleaner = Cleaner(project, default=True, develop=True)
     success = clean(cleaner)
diff --git a/pipenv/vendor/passa/cli/sync.py b/pipenv/vendor/passa/cli/sync.py
index d2d9dcc7..ade81e0a 100644
--- a/pipenv/vendor/passa/cli/sync.py
+++ b/pipenv/vendor/passa/cli/sync.py
@@ -6,8 +6,8 @@ from ._base import BaseCommand
 
 
 def main(options):
+    from passa.internals.synchronizers import Synchronizer
     from passa.operations.sync import sync
-    from passa.synchronizers import Synchronizer
 
     project = options.project
     syncer = Synchronizer(
@@ -25,7 +25,7 @@ def main(options):
 class Command(BaseCommand):
 
     name = "sync"
-    description = "Install Pipfile.lock into the current environment."
+    description = "Install Pipfile.lock into the environment."
     parsed_main = main
 
     def add_arguments(self):
diff --git a/pipenv/vendor/passa/cli/upgrade.py b/pipenv/vendor/passa/cli/upgrade.py
index bfa342f4..011fff6b 100644
--- a/pipenv/vendor/passa/cli/upgrade.py
+++ b/pipenv/vendor/passa/cli/upgrade.py
@@ -8,7 +8,7 @@ from ._base import BaseCommand
 
 
 def main(options):
-    from passa.lockers import EagerUpgradeLocker, PinReuseLocker
+    from passa.internals.lockers import EagerUpgradeLocker, PinReuseLocker
     from passa.operations.lock import lock
 
     project = options.project
@@ -20,7 +20,7 @@ def main(options):
             ), file=sys.stderr)
             return 2
 
-    project.remove_entries_from_lockfile(packages)
+    project.remove_keys_from_lockfile(packages)
 
     prev_lockfile = project.lockfile
 
@@ -39,7 +39,7 @@ def main(options):
         return
 
     from passa.operations.sync import sync
-    from passa.synchronizers import Synchronizer
+    from passa.internals.synchronizers import Synchronizer
 
     lockfile_diff = project.difference_lockfile(prev_lockfile)
     default = bool(any(lockfile_diff.default))
@@ -63,6 +63,7 @@ class Command(BaseCommand):
     parsed_main = main
 
     def add_arguments(self):
+        super(Command, self).add_arguments()
         self.parser.add_argument(
             "packages", metavar="package",
             nargs="+",
diff --git a/pipenv/vendor/passa/dependencies.py b/pipenv/vendor/passa/dependencies.py
deleted file mode 100644
index 8edf2fd7..00000000
--- a/pipenv/vendor/passa/dependencies.py
+++ /dev/null
@@ -1,253 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import functools
-import os
-import sys
-
-import packaging.specifiers
-import packaging.utils
-import packaging.version
-import requests
-import requirementslib
-import six
-
-from ._pip import build_wheel
-from .caches import DependencyCache, RequiresPythonCache
-from .markers import contains_extra, get_contained_extras, get_without_extra
-from .utils import is_pinned
-
-
-DEPENDENCY_CACHE = DependencyCache()
-REQUIRES_PYTHON_CACHE = RequiresPythonCache()
-
-
-def _cached(f, **kwargs):
-
-    @functools.wraps(f)
-    def wrapped(ireq):
-        result = f(ireq, **kwargs)
-        if result is not None and is_pinned(ireq):
-            deps, requires_python = result
-            DEPENDENCY_CACHE[ireq] = deps
-            REQUIRES_PYTHON_CACHE[ireq] = requires_python
-        return result
-
-    return wrapped
-
-
-def _is_cache_broken(line, parent_name):
-    dep_req = requirementslib.Requirement.from_line(line)
-    if contains_extra(dep_req.markers):
-        return True     # The "extra =" marker breaks everything.
-    elif dep_req.normalized_name == parent_name:
-        return True     # A package cannot depend on itself.
-    return False
-
-
-def _get_dependencies_from_cache(ireq):
-    """Retrieves dependencies for the requirement from the dependency cache.
-    """
-    if os.environ.get("PASSA_IGNORE_LOCAL_CACHE"):
-        return
-    if ireq.editable:
-        return
-    try:
-        deps = DEPENDENCY_CACHE[ireq]
-        pyrq = REQUIRES_PYTHON_CACHE[ireq]
-    except KeyError:
-        return
-
-    # Preserving sanity: Run through the cache and make sure every entry if
-    # valid. If this fails, something is wrong with the cache. Drop it.
-    try:
-        packaging.specifiers.SpecifierSet(pyrq)
-        ireq_name = packaging.utils.canonicalize_name(ireq.name)
-        if any(_is_cache_broken(line, ireq_name) for line in deps):
-            broken = True
-        else:
-            broken = False
-    except Exception:
-        broken = True
-
-    if broken:
-        print("dropping broken cache for {0}".format(ireq.name))
-        del DEPENDENCY_CACHE[ireq]
-        del REQUIRES_PYTHON_CACHE[ireq]
-        return
-
-    return deps, pyrq
-
-
-def _get_dependencies_from_json_url(url, session):
-    response = session.get(url)
-    response.raise_for_status()
-    info = response.json()["info"]
-
-    requires_python = info["requires_python"] or ""
-    try:
-        requirement_lines = info["requires_dist"]
-    except KeyError:
-        requirement_lines = info["requires"]
-
-    # The JSON API return null for empty requirements, for some reason, so we
-    # can't just pass it into the comprehension.
-    if not requirement_lines:
-        return [], requires_python
-
-    dependencies = [
-        dep_req.as_line(include_hashes=False) for dep_req in (
-            requirementslib.Requirement.from_line(line)
-            for line in requirement_lines
-        )
-        if not contains_extra(dep_req.markers)
-    ]
-    return dependencies, requires_python
-
-
-def _get_dependencies_from_json(ireq, sources):
-    """Retrieves dependencies for the install requirement from the JSON API.
-
-    :param ireq: A single InstallRequirement
-    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`
-    :return: A set of dependency lines for generating new InstallRequirements.
-    :rtype: set(str) or None
-    """
-    if os.environ.get("PASSA_IGNORE_JSON_API"):
-        return
-    if ireq.editable:
-        return
-
-    # It is technically possible to parse extras out of the JSON API's
-    # requirement format, but it is such a chore let's just use the simple API.
-    if ireq.extras:
-        return
-
-    url_prefixes = [
-        proc_url[:-7]   # Strip "/simple".
-        for proc_url in (
-            raw_url.rstrip("/")
-            for raw_url in (source.get("url", "") for source in sources)
-        )
-        if proc_url.endswith("/simple")
-    ]
-
-    session = requests.session()
-    version = str(ireq.specifier).lstrip("=")
-
-    for prefix in url_prefixes:
-        url = "{prefix}/pypi/{name}/{version}/json".format(
-            prefix=prefix,
-            name=packaging.utils.canonicalize_name(ireq.name),
-            version=version,
-        )
-        try:
-            dependencies = _get_dependencies_from_json_url(url, session)
-            if dependencies is not None:
-                return dependencies
-        except Exception as e:
-            pass
-        print("unable to read dependencies via {0}".format(url))
-    return
-
-
-def _read_requirements(metadata, extras):
-    """Read wheel metadata to know what it depends on.
-
-    The `run_requires` attribute contains a list of dict or str specifying
-    requirements. For dicts, it may contain an "extra" key to specify these
-    requirements are for a specific extra. Unfortunately, not all fields are
-    specificed like this (I don't know why); some are specified with markers.
-    So we jump though these terrible hoops to know exactly what we need.
-
-    The extra extraction is not comprehensive. Tt assumes the marker is NEVER
-    something like `extra == "foo" and extra == "bar"`. I guess this never
-    makes sense anyway? Markers are just terrible.
-    """
-    extras = extras or ()
-    requirements = []
-    for entry in metadata.run_requires:
-        if isinstance(entry, six.text_type):
-            entry = {"requires": [entry]}
-            extra = None
-        else:
-            extra = entry.get("extra")
-        if extra is not None and extra not in extras:
-            continue
-        for line in entry.get("requires", []):
-            r = requirementslib.Requirement.from_line(line)
-            if r.markers:
-                contained = get_contained_extras(r.markers)
-                if (contained and not any(e in contained for e in extras)):
-                    continue
-                marker = get_without_extra(r.markers)
-                r.markers = str(marker) if marker else None
-                line = r.as_line(include_hashes=False)
-            requirements.append(line)
-    return requirements
-
-
-def _read_requires_python(metadata):
-    """Read wheel metadata to know the value of Requires-Python.
-
-    This is surprisingly poorly supported in Distlib. This function tries
-    several ways to get this information:
-
-    * Metadata 2.0: metadata.dictionary.get("requires_python") is not None
-    * Metadata 2.1: metadata._legacy.get("Requires-Python") is not None
-    * Metadata 1.2: metadata._legacy.get("Requires-Python") != "UNKNOWN"
-    """
-    # TODO: Support more metadata formats.
-    value = metadata.dictionary.get("requires_python")
-    if value is not None:
-        return value
-    if metadata._legacy:
-        value = metadata._legacy.get("Requires-Python")
-        if value is not None and value != "UNKNOWN":
-            return value
-    return ""
-
-
-def _get_dependencies_from_pip(ireq, sources):
-    """Retrieves dependencies for the requirement from pip internals.
-
-    The current strategy is to build a wheel out of the ireq, and read metadata
-    out of it.
-    """
-    wheel = build_wheel(ireq, sources)
-    extras = ireq.extras or ()
-    requirements = _read_requirements(wheel.metadata, extras)
-    requires_python = _read_requires_python(wheel.metadata)
-    return requirements, requires_python
-
-
-def get_dependencies(requirement, sources):
-    """Get all dependencies for a given install requirement.
-
-    :param requirement: A requirement
-    :param sources: Pipfile-formatted sources
-    :type sources: list[dict]
-    """
-    getters = [
-        _get_dependencies_from_cache,
-        _cached(_get_dependencies_from_json, sources=sources),
-        _cached(_get_dependencies_from_pip, sources=sources),
-    ]
-    ireq = requirement.as_ireq()
-    last_exc = None
-    for getter in getters:
-        try:
-            result = getter(ireq)
-        except Exception as e:
-            last_exc = sys.exc_info()
-            continue
-        if result is not None:
-            deps, pyreq = result
-            reqs = [requirementslib.Requirement.from_line(d) for d in deps]
-            return reqs, pyreq
-    if last_exc:
-        six.reraise(*last_exc)
-    raise RuntimeError("failed to get dependencies for {}".format(
-        requirement.as_line(),
-    ))
diff --git a/pipenv/vendor/passa/dependencies_pip.py b/pipenv/vendor/passa/dependencies_pip.py
deleted file mode 100644
index 8335b211..00000000
--- a/pipenv/vendor/passa/dependencies_pip.py
+++ /dev/null
@@ -1,187 +0,0 @@
-import importlib
-import os
-
-import distlib.wheel
-import packaging.version
-import pip_shims
-import requirementslib
-import six
-
-from .caches import CACHE_DIR
-from .markers import get_contained_extras, get_without_extra
-from .utils import cheesy_temporary_directory, mkdir_p
-
-
-# HACK: Can we get pip_shims to support these in time?
-def _import_module_of(obj):
-    return importlib.import_module(obj.__module__)
-
-
-WheelBuilder = _import_module_of(pip_shims.Wheel).WheelBuilder
-unpack_url = _import_module_of(pip_shims.is_file_url).unpack_url
-
-
-def _prepare_wheel_building_kwargs():
-    format_control = pip_shims.FormatControl(set(), set())
-    wheel_cache = pip_shims.WheelCache(CACHE_DIR, format_control)
-
-    download_dir = os.path.join(CACHE_DIR, "pkgs")
-    mkdir_p(download_dir)
-
-    build_dir = cheesy_temporary_directory(prefix="build")
-    src_dir = cheesy_temporary_directory(prefix="source")
-
-    return {
-        "wheel_cache": wheel_cache,
-        "build_dir": build_dir,
-        "src_dir": src_dir,
-        "download_dir": download_dir,
-        "wheel_download_dir": download_dir,
-    }
-
-
-def _get_pip_index_urls(sources):
-    index_urls = []
-    trusted_hosts = []
-    for source in sources:
-        url = source.get("url")
-        if not url:
-            continue
-        index_urls.append(url)
-        if source.get("verify_ssl", True):
-            continue
-        host = six.moves.urllib.parse.urlparse(source["url"]).hostname
-        trusted_hosts.append(host)
-    return index_urls, trusted_hosts
-
-
-class _PipCommand(pip_shims.Command):
-    name = 'PipCommand'
-
-
-def _get_pip_session(trusted_hosts):
-    cmd = _PipCommand()
-    options, _ = cmd.parser.parse_args([])
-    options.cache_dir = CACHE_DIR
-    options.trusted_hosts = trusted_hosts
-    session = cmd._build_session(options)
-    return session
-
-
-def _get_internal_objects(sources):
-    index_urls, trusted_hosts = _get_pip_index_urls(sources)
-    session = _get_pip_session(trusted_hosts)
-    finder = pip_shims.PackageFinder(
-        find_links=[],
-        index_urls=index_urls,
-        trusted_hosts=trusted_hosts,
-        allow_all_prereleases=True,
-        session=session,
-    )
-    return finder, session
-
-
-def _build_wheel_pre10(ireq, output_dir, finder, session, kwargs):
-    reqset = pip_shims.RequirementSet(**kwargs)
-    builder = WheelBuilder(reqset, finder)
-    return builder._build_one(ireq, output_dir)
-
-
-def _build_wheel_10x(ireq, output_dir, finder, session, kwargs):
-    kwargs.update({"progress_bar": "off", "build_isolation": False})
-    wheel_cache = kwargs.pop("wheel_cache")
-    preparer = pip_shims.RequirementPreparer(**kwargs)
-    builder = WheelBuilder(finder, preparer, wheel_cache)
-    return builder._build_one(ireq, output_dir)
-
-
-def _build_wheel_modern(ireq, output_dir, finder, session, kwargs):
-    kwargs.update({"progress_bar": "off", "build_isolation": False})
-    wheel_cache = kwargs.pop("wheel_cache")
-    with pip_shims.RequirementTracker() as req_tracker:
-        kwargs["req_tracker"] = req_tracker
-        preparer = pip_shims.RequirementPreparer(**kwargs)
-        builder = WheelBuilder(finder, preparer, wheel_cache)
-        return builder._build_one(ireq, output_dir)
-
-
-def _build_wheel(*args):
-    pip_version = packaging.version.parse(pip_shims.pip_version)
-    if pip_version < packaging.version.parse("10"):
-        return _build_wheel_pre10(*args)
-    elif pip_version < packaging.version.parse("18"):
-        return _build_wheel_10x(*args)
-    return _build_wheel_modern(*args)
-
-
-def _read_requirements(wheel_path, extras):
-    """Read wheel metadata to know what it depends on.
-
-    The `run_requires` attribute contains a list of dict or str specifying
-    requirements. For dicts, it may contain an "extra" key to specify these
-    requirements are for a specific extra. Unfortunately, not all fields are
-    specificed like this (I don't know why); some are specified with markers.
-    So we jump though these terrible hoops to know exactly what we need.
-
-    The extra extraction is not comprehensive. Tt assumes the marker is NEVER
-    something like `extra == "foo" and extra == "bar"`. I guess this never
-    makes sense anyway? Markers are just terrible.
-    """
-    extras = extras or ()
-    wheel = distlib.wheel.Wheel(wheel_path)
-    requirements = []
-    for entry in wheel.metadata.run_requires:
-        if isinstance(entry, six.text_type):
-            entry = {"requires": [entry]}
-            extra = None
-        else:
-            extra = entry.get("extra")
-        if extra is not None and extra not in extras:
-            continue
-        for line in entry.get("requires", []):
-            r = requirementslib.Requirement.from_line(line)
-            if r.markers:
-                contained_extras = get_contained_extras(r.markers)
-                if (contained_extras and
-                        not any(e in contained_extras for e in extras)):
-                    continue
-                marker = get_without_extra(r.markers)
-                r.markers = str(marker) if marker else None
-                line = r.as_line(include_hashes=False)
-            requirements.append(line)
-    return requirements
-
-
-def _get_dependencies_from_pip(ireq, sources):
-    """Retrieves dependencies for the requirement from pip internals.
-
-    The current strategy is to build a wheel out of the ireq, and read metadata
-    out of it.
-    """
-    kwargs = _prepare_wheel_building_kwargs()
-    finder, session = _get_internal_objects(sources)
-
-    # Not for upgrade, hash not required.
-    ireq.populate_link(finder, False, False)
-    ireq.ensure_has_source_dir(kwargs["src_dir"])
-    if not pip_shims.is_file_url(ireq.link):
-        # This makes sure the remote artifact is downloaded locally. For
-        # wheels, it is enough to just download because we'll use them
-        # directly. For an sdist, we need to unpack so we can build it.
-        unpack_url(
-            ireq.link, ireq.source_dir, kwargs["download_dir"],
-            only_download=ireq.is_wheel, session=session,
-            hashes=ireq.hashes(True), progress_bar=False,
-        )
-
-    if ireq.is_wheel:   # If this is a wheel, use the downloaded thing.
-        output_dir = kwargs["download_dir"]
-        wheel_path = os.path.join(output_dir, ireq.link.filename)
-    else:               # Othereise we need to build an ephemeral wheel.
-        output_dir = cheesy_temporary_directory(prefix="ephem")
-        wheel_path = _build_wheel(ireq, output_dir, finder, session, kwargs)
-
-    if not wheel_path or not os.path.exists(wheel_path):
-        raise RuntimeError("failed to build wheel from {}".format(ireq))
-    requirements = _read_requirements(wheel_path, ireq.extras)
-    return requirements
diff --git a/pipenv/vendor/passa/hashes.py b/pipenv/vendor/passa/hashes.py
deleted file mode 100644
index fe049274..00000000
--- a/pipenv/vendor/passa/hashes.py
+++ /dev/null
@@ -1,61 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import contextlib
-
-from pip_shims import Wheel
-
-
-def _wheel_supported(self, tags=None):
-    # Ignore current platform. Support everything.
-    return True
-
-
-def _wheel_support_index_min(self, tags=None):
-    # All wheels are equal priority for sorting.
-    return 0
-
-
-@contextlib.contextmanager
-def _allow_all_wheels():
-    """Monkey patch pip.Wheel to allow all wheels
-
-    The usual checks against platforms and Python versions are ignored to allow
-    fetching all available entries in PyPI. This also saves the candidate cache
-    and set a new one, or else the results from the previous non-patched calls
-    will interfere.
-    """
-    original_wheel_supported = Wheel.supported
-    original_support_index_min = Wheel.support_index_min
-
-    Wheel.supported = _wheel_supported
-    Wheel.support_index_min = _wheel_support_index_min
-    yield
-    Wheel.supported = original_wheel_supported
-    Wheel.support_index_min = original_support_index_min
-
-
-def get_hashes(cache, req):
-    if req.is_vcs:
-        return set()
-
-    ireq = req.as_ireq()
-
-    if ireq.editable:
-        return set()
-
-    if req.is_file_or_url:
-        # TODO: Get the hash of the linked artifact?
-        return set()
-
-    if not ireq.is_pinned:
-        return set()
-
-    with _allow_all_wheels():
-        matching_candidates = req.find_all_matches()
-
-    return {
-        cache.get_hash(candidate.location)
-        for candidate in matching_candidates
-    }
diff --git a/pipenv/vendor/passa/internals/candidates.py b/pipenv/vendor/passa/internals/candidates.py
index d5390d65..1b154bbf 100644
--- a/pipenv/vendor/passa/internals/candidates.py
+++ b/pipenv/vendor/passa/internals/candidates.py
@@ -35,7 +35,11 @@ def _filter_matching_python_requirement(candidates, python_version):
 
 
 def _copy_requirement(requirement):
-    return requirement.copy()
+    # Markers are intentionally dropped here. They will be added to candidates
+    # after resolution, so we can perform marker aggregation.
+    new = requirement.copy()
+    new.markers = None
+    return new
 
 
 def _requirement_from_metadata(name, version, extras, index):
@@ -46,7 +50,7 @@ def _requirement_from_metadata(name, version, extras, index):
     return r
 
 
-def find_candidates(requirement, sources, allow_pre):
+def find_candidates(requirement, sources, allow_prereleases):
     # A non-named requirement has exactly one candidate that is itself. For
     # VCS, we also lock the requirement to an exact ref.
     if not requirement.is_named:
@@ -68,14 +72,18 @@ def find_candidates(requirement, sources, allow_pre):
         ))
         icans = matching_icans or icans
 
-    versions = ireq.specifier.filter((c.version for c in icans), allow_pre)
-    if not allow_pre and not versions:
-        versions = ireq.specifier.filter((c.version for c in icans), True)
+    versions = sorted(ireq.specifier.filter(
+        (c.version for c in icans), allow_prereleases,
+    ))
+    if not allow_prereleases and not versions:
+        versions = sorted(ireq.specifier.filter(
+            (c.version for c in icans), True,
+        ))
 
     name = requirement.normalized_name
     extras = requirement.extras
     index = requirement.index
     return [
         _requirement_from_metadata(name, version, extras, index)
-        for version in sorted(versions)
+        for version in versions
     ]
diff --git a/pipenv/vendor/passa/internals/dependencies.py b/pipenv/vendor/passa/internals/dependencies.py
index 8edf2fd7..60087ec5 100644
--- a/pipenv/vendor/passa/internals/dependencies.py
+++ b/pipenv/vendor/passa/internals/dependencies.py
@@ -16,7 +16,7 @@ import six
 from ._pip import build_wheel
 from .caches import DependencyCache, RequiresPythonCache
 from .markers import contains_extra, get_contained_extras, get_without_extra
-from .utils import is_pinned
+from .utils import get_pinned_version, is_pinned
 
 
 DEPENDENCY_CACHE = DependencyCache()
@@ -116,14 +116,17 @@ def _get_dependencies_from_json(ireq, sources):
     """
     if os.environ.get("PASSA_IGNORE_JSON_API"):
         return
-    if ireq.editable:
-        return
 
     # It is technically possible to parse extras out of the JSON API's
     # requirement format, but it is such a chore let's just use the simple API.
     if ireq.extras:
         return
 
+    try:
+        version = get_pinned_version(ireq)
+    except ValueError:
+        return
+
     url_prefixes = [
         proc_url[:-7]   # Strip "/simple".
         for proc_url in (
@@ -134,7 +137,6 @@ def _get_dependencies_from_json(ireq, sources):
     ]
 
     session = requests.session()
-    version = str(ireq.specifier).lstrip("=")
 
     for prefix in url_prefixes:
         url = "{prefix}/pypi/{name}/{version}/json".format(
@@ -210,7 +212,7 @@ def _read_requires_python(metadata):
 
 
 def _get_dependencies_from_pip(ireq, sources):
-    """Retrieves dependencies for the requirement from pip internals.
+    """Retrieves dependencies for the requirement from pipenv.patched.notpip internals.
 
     The current strategy is to build a wheel out of the ireq, and read metadata
     out of it.
diff --git a/pipenv/vendor/passa/internals/lockers.py b/pipenv/vendor/passa/internals/lockers.py
index b1b842f4..4f7a722b 100644
--- a/pipenv/vendor/passa/internals/lockers.py
+++ b/pipenv/vendor/passa/internals/lockers.py
@@ -9,12 +9,11 @@ import requirementslib
 import resolvelib
 import vistir
 
-from passa import reporters
-
 from .caches import HashCache
 from .hashes import get_hashes
 from .metadata import set_metadata
 from .providers import BasicProvider, EagerUpgradeProvider, PinReuseProvider
+from .reporters import StdOutReporter
 from .traces import trace_graph
 from .utils import identify_requirment
 
@@ -101,6 +100,10 @@ class AbstractLocker(object):
     def get_provider(self):
         raise NotImplementedError
 
+    def get_reporter(self):
+        # TODO: Build SpinnerReporter, and use this only in verbose mode.
+        return StdOutReporter(self.requirements)
+
     def lock(self):
         """Lock specified (abstract) requirements into (concrete) candidates.
 
@@ -113,20 +116,14 @@ class AbstractLocker(object):
         * Populate markers based on dependency specifications of each
           candidate, and the dependency graph.
         """
-        reporters.report("lock-starting", {"requirements": self.requirements})
-
         provider = self.get_provider()
-        resolver = resolvelib.Resolver(
-            provider, reporters.get_reporter().build_for_resolvelib(),
-        )
+        reporter = self.get_reporter()
+        resolver = resolvelib.Resolver(provider, reporter)
 
         with vistir.cd(self.project.root):
             state = resolver.resolve(self.requirements)
 
         traces = trace_graph(state.graph)
-        reporters.report("lock-trace-ended", {
-            "state": state, "traces": traces,
-        })
 
         hash_cache = HashCache()
         for r in state.mapping.values():
diff --git a/pipenv/vendor/passa/projects.py b/pipenv/vendor/passa/internals/projects.py
similarity index 99%
rename from pipenv/vendor/passa/projects.py
rename to pipenv/vendor/passa/internals/projects.py
index 79e71bb1..6a9fcce5 100644
--- a/pipenv/vendor/passa/projects.py
+++ b/pipenv/vendor/passa/internals/projects.py
@@ -187,7 +187,7 @@ class Project(object):
                 section = self.lockfile[section_name]
             except KeyError:
                 continue
-            removals = {}
+            removals = set()
             for name in section:
                 if packaging.utils.canonicalize_name(name) in keys:
                     removals.add(name)
diff --git a/pipenv/vendor/passa/internals/providers.py b/pipenv/vendor/passa/internals/providers.py
index 73418392..1c062af7 100644
--- a/pipenv/vendor/passa/internals/providers.py
+++ b/pipenv/vendor/passa/internals/providers.py
@@ -8,7 +8,10 @@ import resolvelib
 
 from .candidates import find_candidates
 from .dependencies import get_dependencies
-from .utils import filter_sources, identify_requirment, strip_extras
+from .utils import (
+    filter_sources, get_allow_prereleases,
+    are_requirements_equal, identify_requirment, strip_extras,
+)
 
 
 PROTECTED_PACKAGE_NAMES = {"pip", "setuptools"}
@@ -43,18 +46,27 @@ class BasicProvider(resolvelib.AbstractProvider):
         return len(candidates)
 
     def find_matches(self, requirement):
-        # TODO: Implement per-package prereleases flag. (pypa/pipenv#1696)
-        allow_prereleases = self.allow_prereleases
-        sources = filter_sources(requirement, self.sources)
-        candidates = find_candidates(requirement, sources, allow_prereleases)
+        candidates = find_candidates(
+            requirement, filter_sources(requirement, self.sources),
+            get_allow_prereleases(requirement, self.allow_prereleases),
+        )
         return candidates
 
     def is_satisfied_by(self, requirement, candidate):
         # A non-named requirement has exactly one candidate, as implemented in
-        # `find_matches()`. It must match.
+        # `find_matches()`. Since pip does not yet implement URL based lookup
+        # (PEP 508) yet, it must match unless there are duplicated entries in
+        # Pipfile. If there is, the user takes the blame. (sarugaku/passa#34)
         if not requirement.is_named:
             return True
 
+        # A non-named candidate can only come from a non-named requirement,
+        # which, since pip does not implement URL based lookup (PEP 508) yet,
+        # can only come from Pipfile. Assume the user knows what they're doing,
+        # and use it without checking. (sarugaku/passa#34)
+        if not candidate.is_named:
+            return True
+
         # Optimization: Everything matches if there are no specifiers.
         if not requirement.specifiers:
             return True
@@ -62,13 +74,13 @@ class BasicProvider(resolvelib.AbstractProvider):
         # We can't handle old version strings before PEP 440. Drop them all.
         # Practically this shouldn't be a problem if the user is specifying a
         # remotely reasonable dependency not from before 2013.
-        candidate_line = candidate.as_line()
+        candidate_line = candidate.as_line(include_hashes=False)
         if candidate_line in self.invalid_candidates:
             return False
         try:
             version = candidate.get_specifier().version
-        except ValueError:
-            print('ignoring invalid version {}'.format(candidate_line))
+        except (TypeError, ValueError):
+            print('ignoring invalid version from {!r}'.format(candidate_line))
             self.invalid_candidates.add(candidate_line)
             return False
 
diff --git a/pipenv/vendor/passa/reporters.py b/pipenv/vendor/passa/internals/reporters.py
similarity index 100%
rename from pipenv/vendor/passa/reporters.py
rename to pipenv/vendor/passa/internals/reporters.py
diff --git a/pipenv/vendor/passa/internals/utils.py b/pipenv/vendor/passa/internals/utils.py
index d23a10c7..8f8e6fd0 100644
--- a/pipenv/vendor/passa/internals/utils.py
+++ b/pipenv/vendor/passa/internals/utils.py
@@ -97,6 +97,18 @@ def filter_sources(requirement, sources):
     return filtered_sources or sources
 
 
+def get_allow_prereleases(requirement, global_setting):
+    # TODO: Implement per-package prereleases flag. (pypa/pipenv#1696)
+    return global_setting
+
+
+def are_requirements_equal(this, that):
+    return (
+        this.as_line(include_hashes=False) ==
+        that.as_line(include_hashes=False)
+    )
+
+
 def strip_extras(requirement):
     """Returns a new requirement object with extras removed.
     """
diff --git a/pipenv/vendor/passa/lockers.py b/pipenv/vendor/passa/lockers.py
deleted file mode 100644
index 4ab4cc3b..00000000
--- a/pipenv/vendor/passa/lockers.py
+++ /dev/null
@@ -1,182 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import itertools
-
-import plette
-import requirementslib
-import resolvelib
-import vistir
-
-from .caches import HashCache
-from .hashes import get_hashes
-from .metadata import set_metadata
-from .providers import BasicProvider, EagerUpgradeProvider, PinReuseProvider
-from .reporters import StdOutReporter
-from .traces import trace_graph
-from .utils import identify_requirment
-
-
-def _get_requirements(model, section_name):
-    """Produce a mapping of identifier: requirement from the section.
-    """
-    if not model:
-        return {}
-    return {identify_requirment(r): r for r in (
-        requirementslib.Requirement.from_pipfile(name, package._data)
-        for name, package in model.get(section_name, {}).items()
-    )}
-
-
-def _iter_derived_entries(state, traces, names):
-    """Produce a mapping containing all candidates derived from `names`.
-
-    `name` should provide a collection of requirement identifications from
-    a section (i.e. `packages` or `dev-packages`). This function uses `trace`
-    to filter out candidates in the state that are present because of an entry
-    in that collection.
-    """
-    if not names:
-        return
-    names = set(names)
-    for name, requirement in state.mapping.items():
-        routes = {trace[1] for trace in traces[name] if len(trace) > 1}
-        if name not in names and not (names & routes):
-            continue
-        yield (
-            requirement.normalized_name,
-            next(iter(requirement.as_pipfile().values()))
-        )
-
-
-class AbstractLocker(object):
-    """Helper class to produce a new lock file for a project.
-
-    This is not intended for instantiation. You should use one of its concrete
-    subclasses instead. The class contains logic to:
-
-    * Prepare a project for locking
-    * Perform the actually resolver invocation
-    * Convert resolver output into lock file format
-    * Update the project to have the new lock file
-    """
-    def __init__(self, project):
-        self.project = project
-        self.default_requirements = _get_requirements(
-            project.pipfile, "packages",
-        )
-        self.develop_requirements = _get_requirements(
-            project.pipfile, "dev-packages",
-        )
-
-        # This comprehension dance ensures we merge packages from both
-        # sections, and definitions in the default section win.
-        self.requirements = {k: r for k, r in itertools.chain(
-            self.develop_requirements.items(),
-            self.default_requirements.items(),
-        )}.values()
-
-        self.sources = [s._data.copy() for s in project.pipfile.sources]
-        self.allow_prereleases = bool(
-            project.pipfile.get("pipenv", {}).get("allow_prereleases", False),
-        )
-
-    def __repr__(self):
-        return "<{0} @ {1!r}>".format(type(self).__name__, self.project.root)
-
-    def get_provider(self):
-        raise NotImplementedError
-
-    def get_reporter(self):
-        # TODO: Build SpinnerReporter, and use this only in verbose mode.
-        return StdOutReporter(self.requirements)
-
-    def lock(self):
-        """Lock specified (abstract) requirements into (concrete) candidates.
-
-        The locking procedure consists of four stages:
-
-        * Resolve versions and dependency graph (powered by ResolveLib).
-        * Walk the graph to determine "why" each candidate came to be, i.e.
-          what top-level requirements result in a given candidate.
-        * Populate hashes for resolved candidates.
-        * Populate markers based on dependency specifications of each
-          candidate, and the dependency graph.
-        """
-        provider = self.get_provider()
-        reporter = self.get_reporter()
-        resolver = resolvelib.Resolver(provider, reporter)
-
-        with vistir.cd(self.project.root):
-            state = resolver.resolve(self.requirements)
-
-        traces = trace_graph(state.graph)
-
-        hash_cache = HashCache()
-        for r in state.mapping.values():
-            if not r.hashes:
-                r.hashes = get_hashes(hash_cache, r)
-
-        set_metadata(
-            state.mapping, traces,
-            provider.fetched_dependencies, provider.requires_pythons,
-        )
-
-        lockfile = plette.Lockfile.with_meta_from(self.project.pipfile)
-        lockfile["default"] = dict(_iter_derived_entries(
-            state, traces, self.default_requirements,
-        ))
-        lockfile["develop"] = dict(_iter_derived_entries(
-            state, traces, self.develop_requirements,
-        ))
-        self.project.lockfile = lockfile
-
-
-class BasicLocker(AbstractLocker):
-    """Basic concrete locker.
-
-    This takes a project, generates a lock file from its Pipfile, and sets
-    the lock file property to the project.
-    """
-    def get_provider(self):
-        return BasicProvider(
-            self.requirements, self.sources, self.allow_prereleases,
-        )
-
-
-class PinReuseLocker(AbstractLocker):
-    """A specialized locker to handle re-locking based on existing pins.
-
-    See :class:`passa.providers.PinReuseProvider` for more information.
-    """
-    def __init__(self, project):
-        super(PinReuseLocker, self).__init__(project)
-        pins = _get_requirements(project.lockfile, "develop")
-        pins.update(_get_requirements(project.lockfile, "default"))
-        for pin in pins.values():
-            pin.markers = None
-        self.preferred_pins = pins
-
-    def get_provider(self):
-        return PinReuseProvider(
-            self.preferred_pins,
-            self.requirements, self.sources, self.allow_prereleases,
-        )
-
-
-class EagerUpgradeLocker(PinReuseLocker):
-    """A specialized locker to handle the "eager" upgrade strategy.
-
-    See :class:`passa.providers.EagerUpgradeProvider` for more
-    information.
-    """
-    def __init__(self, tracked_names, *args, **kwargs):
-        super(EagerUpgradeLocker, self).__init__(*args, **kwargs)
-        self.tracked_names = tracked_names
-
-    def get_provider(self):
-        return EagerUpgradeProvider(
-            self.tracked_names, self.preferred_pins,
-            self.requirements, self.sources, self.allow_prereleases,
-        )
diff --git a/pipenv/vendor/passa/locking.py b/pipenv/vendor/passa/locking.py
deleted file mode 100644
index e4b6ced5..00000000
--- a/pipenv/vendor/passa/locking.py
+++ /dev/null
@@ -1,105 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import itertools
-
-from plette import Lockfile
-from requirementslib import Requirement
-from resolvelib import Resolver
-
-from .caches import HashCache
-from .hashes import get_hashes
-from .metadata import set_metadata
-from .providers import RequirementsLibProvider
-from .reporters import StdOutReporter
-from .traces import trace_graph
-from .utils import identify_requirment
-
-
-def resolve_requirements(requirements, sources, pins, allow_pre):
-    """Lock specified (abstract) requirements into (concrete) candidates.
-
-    The locking procedure consists of four stages:
-
-    * Resolve versions and dependency graph (powered by ResolveLib).
-    * Walk the graph to determine "why" each candidate came to be, i.e. what
-      top-level requirements result in a given candidate.
-    * Populate hashes for resolved candidates.
-    * Populate markers based on dependency specifications of each candidate,
-      and the dependency graph.
-    """
-    provider = RequirementsLibProvider(requirements, sources, pins, allow_pre)
-    reporter = StdOutReporter(requirements)
-    resolver = Resolver(provider, reporter)
-
-    state = resolver.resolve(requirements)
-    traces = trace_graph(state.graph)
-
-    hash_cache = HashCache()
-    for r in state.mapping.values():
-        if not r.hashes:
-            r.hashes = get_hashes(hash_cache, r)
-
-    set_metadata(
-        state.mapping, traces,
-        provider.fetched_dependencies, provider.requires_pythons,
-    )
-    return state, traces
-
-
-def _get_requirements(pipfile, section_name):
-    """Produce a mapping of identifier: requirement from the section.
-    """
-    return {identify_requirment(r): r for r in (
-        Requirement.from_pipfile(name, package._data)
-        for name, package in pipfile.get(section_name, {}).items()
-    )}
-
-
-def _get_derived_entries(state, traces, names):
-    """Produce a mapping containing all candidates derived from `names`.
-
-    `name` should provide a collection of requirement identifications from
-    a section (i.e. `packages` or `dev-packages`). This function uses `trace`
-    to filter out candidates in the state that are present because of an entry
-    in that collection.
-    """
-    if not names:
-        return {}
-    return_map = {}
-    for req_name_from_state, req in state.mapping.items():
-        req_traces = [trace[1] for trace in traces[req_name_from_state] if len(trace) > 1]
-        if req_name_from_state in names or len(set(names) & set(req_traces)):
-            return_map[req.normalized_name] = next(iter(req.as_pipfile().values()))
-    return return_map
-
-
-def build_lockfile(pipfile, lockfile):
-    default_reqs = _get_requirements(pipfile, "packages")
-    develop_reqs = _get_requirements(pipfile, "dev-packages")
-
-    pins = {}
-    if lockfile:
-        pins = _get_requirements(lockfile, "develop")
-        pins.update(_get_requirements(lockfile, "default"))
-
-    # This comprehension dance ensures we merge packages from both
-    # sections, and definitions in the default section win.
-    requirements = {k: r for k, r in itertools.chain(
-        develop_reqs.items(), default_reqs.items(),
-    )}.values()
-
-    sources = [s._data.copy() for s in pipfile.sources]
-    try:
-        allow_prereleases = bool(pipfile["pipenv"]["allow_prereleases"])
-    except (KeyError, TypeError):
-        allow_prereleases = False
-    state, traces = resolve_requirements(
-        requirements, sources, pins, allow_prereleases,
-    )
-
-    new_lock = Lockfile.with_meta_from(pipfile)
-    new_lock["default"] = _get_derived_entries(state, traces, default_reqs)
-    new_lock["develop"] = _get_derived_entries(state, traces, develop_reqs)
-    return new_lock
diff --git a/pipenv/vendor/passa/markers.py b/pipenv/vendor/passa/markers.py
deleted file mode 100644
index 5f6f37d6..00000000
--- a/pipenv/vendor/passa/markers.py
+++ /dev/null
@@ -1,228 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import itertools
-import operator
-
-import vistir
-
-from packaging.specifiers import SpecifierSet, Specifier
-from packaging.markers import Marker
-
-
-PYTHON_BOUNDARIES = {2: 7, 3: 9}
-
-
-def _strip_extra(elements):
-    """Remove the "extra == ..." operands from the list.
-
-    This is not a comprehensive implementation, but relies on an important
-    characteristic of metadata generation: The "extra == ..." operand is always
-    associated with an "and" operator. This means that we can simply remove the
-    operand and the "and" operator associated with it.
-    """
-    extra_indexes = []
-    for i, element in enumerate(elements):
-        if isinstance(element, list):
-            cancelled = _strip_extra(element)
-            if cancelled:
-                extra_indexes.append(i)
-        elif isinstance(element, tuple) and element[0].value == "extra":
-            extra_indexes.append(i)
-    for i in reversed(extra_indexes):
-        del elements[i]
-        if i > 0 and elements[i - 1] == "and":
-            # Remove the "and" before it.
-            del elements[i - 1]
-        elif elements:
-            # This shouldn't ever happen, but is included for completeness.
-            # If there is not an "and" before this element, try to remove the
-            # operator after it.
-            del elements[0]
-    return (not elements)
-
-
-def get_without_extra(marker):
-    """Build a new marker without the `extra == ...` part.
-
-    The implementation relies very deep into packaging's internals, but I don't
-    have a better way now (except implementing the whole thing myself).
-
-    This could return `None` if the `extra == ...` part is the only one in the
-    input marker.
-    """
-    # TODO: Why is this very deep in the internals? Why is a better solution
-    # implementing it yourself when someone is already maintaining a codebase
-    # for this? It's literally a grammar implementation that is required to
-    # meet the demands of a pep... -d
-    if not marker:
-        return None
-    marker = Marker(str(marker))
-    elements = marker._markers
-    _strip_extra(elements)
-    if elements:
-        return marker
-    return None
-
-
-def _markers_collect_extras(markers, collection):
-    # Optimization: the marker element is usually appended at the end.
-    for el in reversed(markers):
-        if (isinstance(el, tuple) and
-                el[0].value == "extra" and
-                el[1].value == "=="):
-            collection.add(el[2].value)
-        elif isinstance(el, list):
-            _markers_collect_extras(el, collection)
-
-
-def get_contained_extras(marker):
-    """Collect "extra == ..." operands from a marker.
-
-    Returns a list of str. Each str is a speficied extra in this marker.
-    """
-    if not marker:
-        return set()
-    marker = Marker(str(marker))
-    extras = set()
-    _markers_collect_extras(marker._markers, extras)
-    return extras
-
-
-def _markers_contains_extra(markers):
-    # Optimization: the marker element is usually appended at the end.
-    for element in reversed(markers):
-        if isinstance(element, tuple) and element[0].value == "extra":
-            return True
-        elif isinstance(element, list):
-            if _markers_contains_extra(element):
-                return True
-    return False
-
-
-def contains_extra(marker):
-    """Check whehter a marker contains an "extra == ..." operand.
-    """
-    if not marker:
-        return False
-    marker = Marker(str(marker))
-    return _markers_contains_extra(marker._markers)
-
-
-def format_pyspec(specifier):
-    if isinstance(specifier, str):
-        if not any(operator in specifier for operator in Specifier._operators.keys()):
-            new_op = "=="
-            new_version = specifier
-            return Specifier("{0}{1}".format(new_op, new_version))
-    version = specifier._coerce_version(specifier.version.replace(".*", ""))
-    version_tuple = version._version.release
-    if specifier.operator in (">", "<="):
-        # Prefer to always pick the operator for version n+1
-        if version_tuple[1] < PYTHON_BOUNDARIES.get(version_tuple[0], 0):
-            if specifier.operator == ">":
-                new_op = ">="
-            else:
-                new_op = "<"
-            new_version = (version_tuple[0], version_tuple[1] + 1)
-            specifier = Specifier("{0}{1}".format(new_op, version_to_str(new_version)))
-    return specifier
-
-
-def make_version_tuple(version):
-    return tuple([int(x) for x in version.split(".")])
-
-
-def version_to_str(version):
-    return ".".join([str(i) for i in version])
-
-
-def get_specs(specset):
-    if isinstance(specset, Specifier):
-        specset = str(specset)
-    if isinstance(specset, str):
-        specset = SpecifierSet(specset.replace(".*", ""))
-
-    specs = getattr(specset, "_specs", None)
-    return [(spec._spec[0], make_version_tuple(spec._spec[1])) for spec in list(specs)]
-
-
-def group_by_version(versions):
-    versions = sorted(map(lambda x: make_version_tuple(x)))
-    grouping = itertools.groupby(versions, key=operator.itemgetter(0))
-    return grouping
-
-
-def group_by_op(specs):
-    specs = [get_specs(x) for x in list(specs)]
-    flattened = [(op, version) for spec in specs for op, version in spec]
-    specs = sorted(flattened, key=operator.itemgetter(1))
-    grouping = itertools.groupby(specs, key=operator.itemgetter(0))
-    return grouping
-
-
-def marker_to_spec(marker):
-    if marker._markers[0][0] != 'python_version':
-        return
-    operator = marker._markers[0][1].value
-    version = marker._markers[0][2].value
-    specset = set()
-    if operator in ("in", "not in"):
-        op = "==" if operator == "in" else "!="
-        specset |= set([Specifier("{0}{1}".format(op, v.strip())) for v in version.split(",")])
-    else:
-        spec = Specifier("".join([operator, version]))
-        specset.add(spec)
-    if specset:
-        return specset
-    return None
-
-
-def cleanup_specs(specs, operator="or"):
-    specs = {format_pyspec(spec) for spec in specs}
-    # for != operator we want to group by version
-    # if all are consecutive, join as a list
-    results = set()
-    for op, versions in group_by_op(specs):
-        versions = [version[1] for version in versions]
-        versions = sorted(vistir.misc.dedup(versions))
-        # if we are doing an or operation, we need to use the min for >=
-        # this way OR(>=2.6, >=2.7, >=3.6) picks >=2.6
-        # if we do an AND operation we need to use MAX to be more selective
-        if op in (">", ">="):
-            if operator == "or":
-                results.add((op, version_to_str(min(versions))))
-            else:
-                results.add((op, version_to_str(max(versions))))
-        # we use inverse logic here so we will take the max value if we are using OR
-        # but the min value if we are using AND
-        elif op in ("<=", "<"):
-            if operator == "or":
-                results.add((op, version_to_str(max(versions))))
-            else:
-                results.add((op, version_to_str(min(versions))))
-        # leave these the same no matter what operator we use
-        elif op in ("!=", "==", "~="):
-            version_list = sorted(["{0}".format(version_to_str(version)) for version in versions])
-            version = ", ".join(version_list)
-            if len(version_list) == 1:
-                results.add((op, version))
-            else:
-                if op == "!=":
-                    results.add(("not in", version))
-                elif op == "==":
-                    results.add(("in", version))
-                else:
-                    version = ", ".join(sorted(["{0}".format(op, v) for v in version_list]))
-                    specifier = SpecifierSet(version)._specs
-                    for s in specifier:
-                        results &= (specifier._spec[0], specifier._spec[1])
-        else:
-            if len(version) == 1:
-                results.add((op, version))
-            else:
-                specifier = SpecifierSet("{0}".format(version))._specs
-                for s in specifier:
-                    results |= (specifier._spec[0], specifier._spec[1])
-    return results
diff --git a/pipenv/vendor/passa/metadata.py b/pipenv/vendor/passa/metadata.py
deleted file mode 100644
index 312691a1..00000000
--- a/pipenv/vendor/passa/metadata.py
+++ /dev/null
@@ -1,169 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import copy
-import itertools
-
-import packaging.markers
-import packaging.specifiers
-import vistir
-import vistir.misc
-
-from .markers import get_without_extra, cleanup_specs, marker_to_spec
-
-
-def dedup_markers(s):
-    # TODO: Implement better logic.
-    deduped = sorted(vistir.misc.dedup(s))
-    return deduped
-
-
-class MetaSet(object):
-    """Representation of a "metadata set".
-
-    This holds multiple metadata representaions. Each metadata representation
-    includes a marker, and a specifier set of Python versions required.
-    """
-    def __init__(self):
-        self.markerset = frozenset()
-        self.pyspecset = packaging.specifiers.SpecifierSet()
-
-    def __repr__(self):
-        return "MetaSet(markerset={0!r}, pyspecset={1!r})".format(
-            ",".join(sorted(self.markerset)), str(self.pyspecset),
-        )
-
-    def __str__(self):
-        pyspecs = set()
-        markerset = set()
-        for m in self.markerset:
-            py_marker = marker_to_spec(packaging.markers.Marker(m))
-            if py_marker:
-                pyspecs.add(py_marker)
-            else:
-                markerset.add(m)
-        if pyspecs:
-            self.pyspecset._specs &= pyspecs
-            self.markerset = frozenset(markerset)
-        return " and ".join(dedup_markers(itertools.chain(
-            # Make sure to always use the same quotes so we can dedup properly.
-            (
-                "{0}".format(ms) if " or " in ms else ms
-                for ms in (str(m).replace('"', "'") for m in self.markerset)
-            ),
-            (
-                "python_version {0[0]} '{0[1]}'".format(spec)
-                for spec in cleanup_specs(self.pyspecset)
-            ),
-        )))
-
-    def __bool__(self):
-        return bool(self.markerset or self.pyspecset)
-
-    def __nonzero__(self):  # Python 2.
-        return self.__bool__()
-
-    def __or__(self, pair):
-        marker, specset = pair
-        markerset = set(self.markerset)
-        pyspec_markers = set()
-        if marker:
-            pyspec_markers = marker_to_spec(marker)
-            if not pyspec_markers:
-                markerset.add(str(marker))
-            else:
-                specset._specs &= pyspec_markers
-        metaset = MetaSet()
-        metaset.markerset = frozenset(markerset)
-        # TODO: Implement some logic to clean up dups like '3.0.*' and '3.0'.
-        metaset.pyspecset &= self.pyspecset & specset
-        return metaset
-
-
-def _build_metasets(dependencies, pythons, key, trace, all_metasets):
-    all_parent_metasets = []
-    for route in trace:
-        parent = route[-1]
-        try:
-            parent_metasets = all_metasets[parent]
-        except KeyError:    # Parent not calculated yet. Wait for it.
-            return
-        all_parent_metasets.append((parent, parent_metasets))
-
-    metaset_iters = []
-    for parent, parent_metasets in all_parent_metasets:
-        r = dependencies[parent][key]
-        python = pythons[key]
-        metaset = (
-            get_without_extra(r.markers),
-            packaging.specifiers.SpecifierSet(python),
-        )
-        metaset_iters.append(
-            parent_metaset | metaset
-            for parent_metaset in parent_metasets
-        )
-    return list(itertools.chain.from_iterable(metaset_iters))
-
-
-def _calculate_metasets_mapping(dependencies, pythons, traces):
-    all_metasets = {None: [MetaSet()]}
-
-    del traces[None]
-    while traces:
-        new_metasets = {}
-        for key, trace in traces.items():
-            assert key not in all_metasets, key     # Sanity check for debug.
-            metasets = _build_metasets(
-                dependencies, pythons, key, trace, all_metasets,
-            )
-            if metasets is None:
-                continue
-            new_metasets[key] = metasets
-        if not new_metasets:
-            break   # No progress? Deadlocked. Give up.
-        all_metasets.update(new_metasets)
-        for key in new_metasets:
-            del traces[key]
-
-    return all_metasets
-
-
-def _format_metasets(metasets):
-    # If there is an unconditional route, this needs to be unconditional.
-    if not metasets or not all(metasets):
-        return None
-
-    # This extra str(Marker()) call helps simplify the expression.
-    return str(packaging.markers.Marker(" or ".join(
-        "{0}".format(s) if " and " in s else s
-        for s in dedup_markers(str(metaset) for metaset in metasets
-        if metaset)
-    )))
-
-
-def set_metadata(candidates, traces, dependencies, pythons):
-    """Add "metadata" to candidates based on the dependency tree.
-
-    Metadata for a candidate includes markers and a specifier for Python
-    version requirements.
-
-    :param candidates: A key-candidate mapping. Candidates in the mapping will
-        have their markers set.
-    :param traces: A graph trace (produced by `traces.trace_graph`) providing
-        information about dependency relationships between candidates.
-    :param dependencies: A key-collection mapping containing what dependencies
-        each candidate in `candidates` requested.
-    :param pythons: A key-str mapping containing Requires-Python information
-        of each candidate.
-
-    Keys in mappings and entries in the trace are identifiers of a package, as
-    implemented by the `identify` method of the resolver's provider.
-
-    The candidates are modified in-place.
-    """
-    metasets_mapping = _calculate_metasets_mapping(
-        dependencies, pythons, copy.deepcopy(traces),
-    )
-    for key, candidate in candidates.items():
-        candidate.markers = _format_metasets(metasets_mapping[key])
diff --git a/pipenv/vendor/passa/operations/_utils.py b/pipenv/vendor/passa/operations/_utils.py
deleted file mode 100644
index e69de29b..00000000
diff --git a/pipenv/vendor/passa/operations/lock.py b/pipenv/vendor/passa/operations/lock.py
index a68d0b7d..200735ac 100644
--- a/pipenv/vendor/passa/operations/lock.py
+++ b/pipenv/vendor/passa/operations/lock.py
@@ -4,7 +4,7 @@ from __future__ import absolute_import, print_function, unicode_literals
 
 from resolvelib import NoVersionsAvailable, ResolutionImpossible
 
-from passa.reporters import print_requirement
+from passa.internals.reporters import print_requirement
 
 
 def lock(locker):
diff --git a/pipenv/vendor/passa/providers.py b/pipenv/vendor/passa/providers.py
deleted file mode 100644
index 7dfa3306..00000000
--- a/pipenv/vendor/passa/providers.py
+++ /dev/null
@@ -1,167 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, print_function, unicode_literals
-
-import os
-
-import resolvelib
-
-from .candidates import find_candidates
-from .dependencies import get_dependencies
-from .utils import filter_sources, identify_requirment
-
-
-class BasicProvider(resolvelib.AbstractProvider):
-    """Provider implementation to interface with `requirementslib.Requirement`.
-    """
-    def __init__(self, root_requirements, sources, allow_prereleases):
-        self.sources = sources
-        self.allow_prereleases = bool(allow_prereleases)
-        self.invalid_candidates = set()
-
-        # Remember requirements of each pinned candidate. The resolver calls
-        # `get_dependencies()` only when it wants to repin, so the last time
-        # the dependencies we got when it is last called on a package, are
-        # the set used by the resolver. We use this later to trace how a given
-        # dependency is specified by a package.
-        self.fetched_dependencies = {None: {
-            self.identify(r): r for r in root_requirements
-        }}
-        # TODO: Find a way to resolve with multiple versions (by tricking
-        # runtime) Include multiple keys in pipfiles?
-        self.requires_pythons = {None: ""}  # TODO: Don't use any value
-
-    def identify(self, dependency):
-        return identify_requirment(dependency)
-
-    def get_preference(self, resolution, candidates, information):
-        # TODO: Provide better sorting logic. This simply resolve the ones with
-        # less choices first. Not sophisticated, but sounds reasonable?
-        return len(candidates)
-
-    def find_matches(self, requirement):
-        # TODO: Implement per-package prereleases flag. (pypa/pipenv#1696)
-        allow_prereleases = self.allow_prereleases
-        sources = filter_sources(requirement, self.sources)
-        candidates = find_candidates(requirement, sources, allow_prereleases)
-        return candidates
-
-    def is_satisfied_by(self, requirement, candidate):
-        # A non-named requirement has exactly one candidate, as implemented in
-        # `find_matches()`. It must match.
-        if not requirement.is_named:
-            return True
-
-        # Optimization: Everything matches if there are no specifiers.
-        if not requirement.specifiers:
-            return True
-
-        # We can't handle old version strings before PEP 440. Drop them all.
-        # Practically this shouldn't be a problem if the user is specifying a
-        # remotely reasonable dependency not from before 2013.
-        candidate_line = candidate.as_line()
-        if candidate_line in self.invalid_candidates:
-            return False
-        try:
-            version = candidate.get_specifier().version
-        except ValueError:
-            print('ignoring invalid version {}'.format(candidate_line))
-            self.invalid_candidates.add(candidate_line)
-            return False
-
-        return requirement.as_ireq().specifier.contains(version)
-
-    def get_dependencies(self, candidate):
-        sources = filter_sources(candidate, self.sources)
-        try:
-            dependencies, requires_python = get_dependencies(
-                candidate, sources=sources,
-            )
-        except Exception as e:
-            if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
-                raise
-            print("failed to get dependencies for {0!r}: {1}".format(
-                candidate.as_line(include_hashes=False), e,
-            ))
-            dependencies = []
-            requires_python = ""
-        candidate_key = self.identify(candidate)
-        self.fetched_dependencies[candidate_key] = {
-            self.identify(r): r for r in dependencies
-        }
-        self.requires_pythons[candidate_key] = requires_python
-        return dependencies
-
-
-class PinReuseProvider(BasicProvider):
-    """A provider that reuses preferred pins if possible.
-
-    This is used to implement "add", "remove", and "only-if-needed upgrade",
-    where already-pinned candidates in Pipfile.lock should be preferred.
-    """
-    def __init__(self, preferred_pins, *args, **kwargs):
-        super(PinReuseProvider, self).__init__(*args, **kwargs)
-        self.preferred_pins = preferred_pins
-
-    def find_matches(self, requirement):
-        candidates = super(PinReuseProvider, self).find_matches(requirement)
-        try:
-            # Add the preferred pin. Remember the resolve prefer candidates
-            # at the end of the list, so the most preferred should be last.
-            candidates.append(self.preferred_pins[self.identify(requirement)])
-        except KeyError:
-            pass
-        return candidates
-
-
-class EagerUpgradeProvider(PinReuseProvider):
-    """A specialized provider to handle an "eager" upgrade strategy.
-
-    An eager upgrade tries to upgrade not only packages specified, but also
-    their dependeices (recursively). This contrasts to the "only-if-needed"
-    default, which only promises to upgrade the specified package, and
-    prevents touching anything else if at all possible.
-
-    The provider is implemented as to keep track of all dependencies of the
-    specified packages to upgrade, and free their pins when it has a chance.
-    """
-    def __init__(self, tracked_names, *args, **kwargs):
-        super(EagerUpgradeProvider, self).__init__(*args, **kwargs)
-        self.tracked_names = set(tracked_names)
-        for name in tracked_names:
-            self.preferred_pins.pop(name, None)
-
-        # HACK: Set this special flag to distinguish preferred pins from
-        # regular, to tell the resolver to NOT use them for tracked packages.
-        for pin in self.preferred_pins.values():
-            pin._preferred_by_provider = True
-
-    def is_satisfied_by(self, requirement, candidate):
-        # If this is a tracking package, tell the resolver out of using the
-        # preferred pin, and into a "normal" candidate selection process.
-        if (self.identify(requirement) in self.tracked_names and
-                getattr(candidate, "_preferred_by_provider", False)):
-            return False
-        return super(EagerUpgradeProvider, self).is_satisfied_by(
-            requirement, candidate,
-        )
-
-    def get_dependencies(self, candidate):
-        # If this package is being tracked for upgrade, remove pins of its
-        # dependencies, and start tracking these new packages.
-        dependencies = super(EagerUpgradeProvider, self).get_dependencies(
-            candidate,
-        )
-        if self.identify(candidate) in self.tracked_names:
-            for dependency in dependencies:
-                name = self.identify(dependency)
-                self.tracked_names.add(name)
-                self.preferred_pins.pop(name, None)
-        return dependencies
-
-    def get_preference(self, resolution, candidates, information):
-        # Resolve tracking packages so we have a chance to unpin them first.
-        name = self.identify(candidates[0])
-        if name in self.tracked_names:
-            return -1
-        return len(candidates)
diff --git a/pipenv/vendor/passa/reporters/__init__.py b/pipenv/vendor/passa/reporters/__init__.py
deleted file mode 100644
index dffe5327..00000000
--- a/pipenv/vendor/passa/reporters/__init__.py
+++ /dev/null
@@ -1,31 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, print_function, unicode_literals
-
-from .base import BaseReporter
-
-
-_REPORTER = BaseReporter()
-
-
-def _get_stdout_reporter():
-    from .stdout import Reporter
-    return Reporter()
-
-
-def configure_reporter(name):
-    global _REPORTER
-    _REPORTER = {
-        None: BaseReporter,
-        "stdout": _get_stdout_reporter,
-    }[name]()
-
-
-def get_reporter():
-    return _REPORTER
-
-
-def report(event, context=None):
-    if context is None:
-        context = {}
-    _REPORTER.report(event, context)
diff --git a/pipenv/vendor/passa/reporters/base.py b/pipenv/vendor/passa/reporters/base.py
deleted file mode 100644
index 66a432cb..00000000
--- a/pipenv/vendor/passa/reporters/base.py
+++ /dev/null
@@ -1,52 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, print_function, unicode_literals
-
-import resolvelib
-
-
-class ResolveLibReporter(resolvelib.BaseReporter):
-    """Implementation of a ResolveLib reporter that bridge messages.
-    """
-    def __init__(self, parent):
-        super(ResolveLibReporter, self).__init__()
-        self.parent = parent
-
-    def starting(self):
-        self.parent.report("resolvelib-starting", {"child": self})
-
-    def ending_round(self, index, state):
-        self.parent.report("resolvelib-ending-round", {
-            "child": self, "index": index, "state": state,
-        })
-
-    def ending(self, state):
-        self.parent.report("resolvelib-ending", {
-            "child": self, "state": state,
-        })
-
-
-class BaseReporter(object):
-    """Basic reporter that does nothing.
-    """
-    def build_for_resolvelib(self):
-        """Build a reporter for ResolveLib.
-        """
-        return ResolveLibReporter(self)
-
-    def report(self, event, context):
-        """Report an event.
-
-        The default behavior is to look for a "handle_EVENT" method on the
-        class to execute, or do nothing if there is no such method.
-
-        :param event: A string to indicate the event.
-        :param context: A mapping containing appropriate data for the handling
-            function.
-        """
-        handler_name = "handle_{}".format(event.replace("-", "_"))
-        try:
-            handler = getattr(self, handler_name)
-        except AttributeError:
-            return
-        handler(context or {})
diff --git a/pipenv/vendor/passa/reporters/stdout.py b/pipenv/vendor/passa/reporters/stdout.py
deleted file mode 100644
index 20423377..00000000
--- a/pipenv/vendor/passa/reporters/stdout.py
+++ /dev/null
@@ -1,106 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, print_function, unicode_literals
-
-from resolvelib import NoVersionsAvailable, ResolutionImpossible
-
-from .base import BaseReporter
-
-
-def _print_title(text):
-    print('\n{:=^84}\n'.format(text))
-
-
-def _print_requirement(r, end='\n'):
-    print('{:>40}'.format(r.as_line(include_hashes=False)), end=end)
-
-
-def _print_dependency(state, key):
-    _print_requirement(state.mapping[key], end='')
-    parents = sorted(
-        state.graph.iter_parents(key),
-        key=lambda n: (-1, '') if n is None else (ord(n[0].lower()), n),
-    )
-    for i, p in enumerate(parents):
-        if p is None:
-            line = '(user)'
-        else:
-            line = state.mapping[p].as_line(include_hashes=False)
-        if i == 0:
-            padding = ' <= '
-        else:
-            padding = ' ' * 44
-        print('{pad}{line}'.format(pad=padding, line=line))
-
-
-class Reporter(BaseReporter):
-    """A reporter implementation that prints messages to stdout.
-    """
-    def handle_resolvelib_starting(self, context):
-        context["child"]._prev_mapping = None
-
-    def handle_resolvelib_ending_round(self, context):
-        _print_title(' Round {} '.format(context["index"]))
-        mapping = context["state"].mapping
-        if context["child"]._prev_mapping is None:
-            difference = set(mapping.keys())
-            changed = set()
-        else:
-            prev = context["child"]._prev_mapping
-            difference = set(mapping.keys()) - set(prev.keys())
-            changed = set(
-                k for k, v in mapping.items()
-                if k in prev and prev[k] != v
-            )
-        context["child"]._prev_mapping = mapping
-
-        if difference:
-            print('New pins: ')
-            for k in difference:
-                _print_dependency(context["state"], k)
-        print()
-
-        if changed:
-            print('Changed pins:')
-            for k in changed:
-                _print_dependency(context["state"], k)
-        print()
-
-    def handle_lock_starting(self, context):
-        _print_title(' User requirements ')
-        for r in context["requirements"]:
-            _print_requirement(r)
-
-    def handle_lock_trace_ended(self, context):
-        _print_title(" STABLE PINS ")
-        mapping = context["state"].mapping
-        for k in sorted(mapping):
-            print(mapping[k].as_line(include_hashes=False))
-            paths = context["traces"][k]
-            for path in paths:
-                if path == [None]:
-                    print('    User requirement')
-                    continue
-                print('   ', end='')
-                for v in reversed(path[1:]):
-                    line = mapping[v].as_line(include_hashes=False)
-                    print(' <=', line, end='')
-                print()
-        print()
-
-    def handle_lock_failed(self, context):
-        e = context["exception"]
-        if isinstance(e, ResolutionImpossible):
-            print("\nCANNOT RESOLVE.\nOFFENDING REQUIREMENTS:")
-            for r in e.requirements:
-                _print_requirement(r)
-        elif isinstance(e, NoVersionsAvailable):
-            print("\nCANNOT RESOLVE. NO CANDIDATES FOUND FOR:")
-            print("{:>40}".format(e.requirement.as_line(include_hashes=False)))
-            if e.parent:
-                line = e.parent.as_line(include_hashes=False)
-                print("{:>41}".format("(from {})".format(line)))
-            else:
-                print("{:>41}".format("(user)"))
-        else:
-            raise
diff --git a/pipenv/vendor/passa/synchronizers.py b/pipenv/vendor/passa/synchronizers.py
deleted file mode 100644
index 30fc4492..00000000
--- a/pipenv/vendor/passa/synchronizers.py
+++ /dev/null
@@ -1,211 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-import collections
-import contextlib
-import os
-import sys
-import sysconfig
-
-import pkg_resources
-
-import packaging.markers
-import packaging.version
-import requirementslib
-
-from ._pip import uninstall_requirement, EditableInstaller, WheelInstaller
-
-
-def _is_installation_local(name):
-    """Check whether the distribution is in the current Python installation.
-
-    This is used to distinguish packages seen by a virtual environment. A venv
-    may be able to see global packages, but we don't want to mess with them.
-    """
-    location = pkg_resources.working_set.by_key[name].location
-    return os.path.commonprefix([location, sys.prefix]) == sys.prefix
-
-
-def _is_up_to_date(distro, version):
-    # This is done in strings to avoid type mismatches caused by vendering.
-    return str(version) == str(packaging.version.parse(distro.version))
-
-
-GroupCollection = collections.namedtuple("GroupCollection", [
-    "uptodate", "outdated", "noremove", "unneeded",
-])
-
-
-def _group_installed_names(packages):
-    """Group locally installed packages based on given specifications.
-
-    `packages` is a name-package mapping that are used as baseline to
-    determine how the installed package should be grouped.
-
-    Returns a 3-tuple of disjoint sets, all containing names of installed
-    packages:
-
-    * `uptodate`: These match the specifications.
-    * `outdated`: These installations are specified, but don't match the
-        specifications in `packages`.
-    * `unneeded`: These are installed, but not specified in `packages`.
-    """
-    groupcoll = GroupCollection(set(), set(), set(), set())
-
-    for distro in pkg_resources.working_set:
-        name = distro.key
-        try:
-            package = packages[name]
-        except KeyError:
-            groupcoll.unneeded.add(name)
-            continue
-
-        r = requirementslib.Requirement.from_pipfile(name, package)
-        if not r.is_named:
-            # Always mark non-named. I think pip does something similar?
-            groupcoll.outdated.add(name)
-        elif not _is_up_to_date(distro, r.get_version()):
-            groupcoll.outdated.add(name)
-        else:
-            groupcoll.uptodate.add(name)
-
-    return groupcoll
-
-
-@contextlib.contextmanager
-def _remove_package(name):
-    if name is None or not _is_installation_local(name):
-        yield
-        return
-    r = requirementslib.Requirement.from_line(name)
-    with uninstall_requirement(r.as_ireq(), auto_confirm=True, verbose=False):
-        yield
-
-
-def _get_packages(lockfile, default, develop):
-    # Don't need to worry about duplicates because only extras can differ.
-    # Extras don't matter because they only affect dependencies, and we
-    # don't install dependencies anyway!
-    packages = {}
-    if default:
-        packages.update(lockfile.default._data)
-    if develop:
-        packages.update(lockfile.develop._data)
-    return packages
-
-
-def _build_paths():
-    """Prepare paths for distlib.wheel.Wheel to install into.
-    """
-    paths = sysconfig.get_paths()
-    return {
-        "prefix": sys.prefix,
-        "data": paths["data"],
-        "scripts": paths["scripts"],
-        "headers": paths["include"],
-        "purelib": paths["purelib"],
-        "platlib": paths["platlib"],
-    }
-
-
-PROTECTED_FROM_CLEAN = {"setuptools", "pip"}
-
-
-def _clean(names):
-    for name in names:
-        if name in PROTECTED_FROM_CLEAN:
-            continue
-        with _remove_package(name):
-            pass
-
-
-class Synchronizer(object):
-    """Helper class to install packages from a project's lock file.
-    """
-    def __init__(self, project, default, develop, clean_unneeded):
-        self._root = project.root   # Only for repr.
-        self.packages = _get_packages(project.lockfile, default, develop)
-        self.sources = project.lockfile.meta.sources._data
-        self.paths = _build_paths()
-        self.clean_unneeded = clean_unneeded
-
-    def __repr__(self):
-        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
-
-    def sync(self):
-        groupcoll = _group_installed_names(self.packages)
-
-        installed = set()
-        updated = set()
-        cleaned = set()
-
-        # TODO: Show a prompt to confirm cleaning. We will need to implement a
-        # reporter pattern for this as well.
-        if self.clean_unneeded:
-            cleaned.update(groupcoll.unneeded)
-            _clean(cleaned)
-
-        # TODO: Specify installation order? (pypa/pipenv#2274)
-        installers = []
-        for name, package in self.packages.items():
-            r = requirementslib.Requirement.from_pipfile(name, package)
-            name = r.normalized_name
-            if name in groupcoll.uptodate:
-                continue
-            markers = r.markers
-            if markers and not packaging.markers.Marker(markers).evaluate():
-                continue
-            r.markers = None
-            if r.editable:
-                installer = EditableInstaller(r)
-            else:
-                installer = WheelInstaller(r, self.sources, self.paths)
-            try:
-                installer.prepare()
-            except Exception as e:
-                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
-                    raise
-                print("failed to prepare {0!r}: {1}".format(
-                    r.as_line(include_hashes=False), e,
-                ))
-            else:
-                installers.append((name, installer))
-
-        for name, installer in installers:
-            if name in groupcoll.outdated:
-                name_to_remove = name
-            else:
-                name_to_remove = None
-            try:
-                with _remove_package(name_to_remove):
-                    installer.install()
-            except Exception as e:
-                if os.environ.get("PASSA_NO_SUPPRESS_EXCEPTIONS"):
-                    raise
-                print("failed to install {0!r}: {1}".format(
-                    r.as_line(include_hashes=False), e,
-                ))
-                continue
-            if name in groupcoll.outdated or name in groupcoll.noremove:
-                updated.add(name)
-            else:
-                installed.add(name)
-
-        return installed, updated, cleaned
-
-
-class Cleaner(object):
-    """Helper class to clean packages not in a project's lock file.
-    """
-    def __init__(self, project, default, develop):
-        self._root = project.root   # Only for repr.
-        self.packages = _get_packages(project.lockfile, default, develop)
-
-    def __repr__(self):
-        return "<{0} @ {1!r}>".format(type(self).__name__, self._root)
-
-    def clean(self):
-        groupcoll = _group_installed_names(self.packages)
-        _clean(groupcoll.unneeded)
-        return groupcoll.unneeded
diff --git a/pipenv/vendor/passa/traces.py b/pipenv/vendor/passa/traces.py
deleted file mode 100644
index 9715db97..00000000
--- a/pipenv/vendor/passa/traces.py
+++ /dev/null
@@ -1,40 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-
-def _trace_visit_vertex(graph, current, target, visited, path, paths):
-    if current == target:
-        paths.append(path)
-        return
-    for v in graph.iter_children(current):
-        if v == current or v in visited:
-            continue
-        next_path = path + [current]
-        next_visited = visited | {current}
-        _trace_visit_vertex(graph, v, target, next_visited, next_path, paths)
-
-
-def trace_graph(graph):
-    """Build a collection of "traces" for each package.
-
-    A trace is a list of names that eventually leads to the package. For
-    example, if A and B are root dependencies, A depends on C and D, B
-    depends on C, and C depends on D, the return value would be like::
-
-        {
-            None: [],
-            "A": [None],
-            "B": [None],
-            "C": [[None, "A"], [None, "B"]],
-            "D": [[None, "B", "C"], [None, "A"]],
-        }
-    """
-    result = {None: []}
-    for vertex in graph:
-        result[vertex] = []
-        for root in graph.iter_children(None):
-            paths = []
-            _trace_visit_vertex(graph, root, vertex, {None}, [None], paths)
-            result[vertex].extend(paths)
-    return result
diff --git a/pipenv/vendor/passa/utils.py b/pipenv/vendor/passa/utils.py
deleted file mode 100644
index 1028db10..00000000
--- a/pipenv/vendor/passa/utils.py
+++ /dev/null
@@ -1,97 +0,0 @@
-# -*- coding=utf-8 -*-
-
-from __future__ import absolute_import, unicode_literals
-
-
-def identify_requirment(r):
-    """Produce an identifier for a requirement to use in the resolver.
-
-    Note that we are treating the same package with different extras as
-    distinct. This allows semantics like "I only want this extra in
-    development, not production".
-
-    This also makes the resolver's implementation much simpler, with the minor
-    costs of possibly needing a few extra resolution steps if we happen to have
-    the same package apprearing multiple times.
-    """
-    return "{0}{1}".format(r.normalized_name, r.extras_as_pip)
-
-
-def get_pinned_version(ireq):
-    """Get the pinned version of an InstallRequirement.
-
-    An InstallRequirement is considered pinned if:
-
-    - Is not editable
-    - It has exactly one specifier
-    - That specifier is "=="
-    - The version does not contain a wildcard
-
-    Examples:
-        django==1.8   # pinned
-        django>1.8    # NOT pinned
-        django~=1.8   # NOT pinned
-        django==1.*   # NOT pinned
-
-    Raises `TypeError` if the input is not a valid InstallRequirement, or
-    `ValueError` if the InstallRequirement is not pinned.
-    """
-    try:
-        specifier = ireq.specifier
-    except AttributeError:
-        raise TypeError("Expected InstallRequirement, not {}".format(
-            type(ireq).__name__,
-        ))
-
-    if ireq.editable:
-        raise ValueError("InstallRequirement is editable")
-    if not specifier:
-        raise ValueError("InstallRequirement has no version specification")
-    if len(specifier._specs) != 1:
-        raise ValueError("InstallRequirement has multiple specifications")
-
-    op, version = next(iter(specifier._specs))._spec
-    if op not in ('==', '===') or version.endswith('.*'):
-        raise ValueError("InstallRequirement not pinned (is {0!r})".format(
-            op + version,
-        ))
-
-    return version
-
-
-def is_pinned(ireq):
-    """Returns whether an InstallRequirement is a "pinned" requirement.
-
-    An InstallRequirement is considered pinned if:
-
-    - Is not editable
-    - It has exactly one specifier
-    - That specifier is "=="
-    - The version does not contain a wildcard
-
-    Examples:
-        django==1.8   # pinned
-        django>1.8    # NOT pinned
-        django~=1.8   # NOT pinned
-        django==1.*   # NOT pinned
-    """
-    try:
-        get_pinned_version(ireq)
-    except (TypeError, ValueError):
-        return False
-    return True
-
-
-def filter_sources(requirement, sources):
-    """Return a filtered list of sources for this requirement.
-
-    This considers the index specified by the requirement, and returns only
-    matching source entries if there is at least one.
-    """
-    if not sources or not requirement.index:
-        return sources
-    filtered_sources = [
-        source for source in sources
-        if source.get("name") == requirement.index
-    ]
-    return filtered_sources or sources
diff --git a/pipenv/vendor/passa/vcs.py b/pipenv/vendor/passa/vcs.py
deleted file mode 100644
index 23d033d3..00000000
--- a/pipenv/vendor/passa/vcs.py
+++ /dev/null
@@ -1,37 +0,0 @@
-import os
-
-from pip_shims import VcsSupport
-
-from .utils import cheesy_temporary_directory, mkdir_p
-
-
-def _obtrain_ref(vcs_obj, src_dir, name, rev=None):
-    target_dir = os.path.join(src_dir, name)
-    target_rev = vcs_obj.make_rev_options(rev)
-    if not os.path.exists(target_dir):
-        vcs_obj.obtain(target_dir)
-    if (not vcs_obj.is_commit_id_equal(target_dir, rev) and
-            not vcs_obj.is_commit_id_equal(target_dir, target_rev)):
-        vcs_obj.update(target_dir, target_rev)
-    return vcs_obj.get_revision(target_dir)
-
-
-def _get_src():
-    src = os.environ.get("PIP_SRC")
-    if src:
-        return src
-    virtual_env = os.environ.get("VIRTUAL_ENV")
-    if virtual_env:
-        return os.path.join(virtual_env, "src")
-    temp_src = cheesy_temporary_directory(prefix='passa-src')
-    return temp_src
-
-
-def set_ref(requirement):
-    backend = VcsSupport()._registry.get(requirement.vcs)
-    vcs = backend(url=requirement.req.vcs_uri)
-    src = _get_src()
-    mkdir_p(src, mode=0o775)
-    name = requirement.normalized_name
-    ref = _obtrain_ref(vcs, src, name, rev=requirement.req.ref)
-    requirement.req.ref = ref
