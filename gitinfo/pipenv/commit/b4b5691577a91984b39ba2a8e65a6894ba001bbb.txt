commit b4b5691577a91984b39ba2a8e65a6894ba001bbb
Author: Dan Ryan <dan.ryan@canonical.com>
Date:   Thu Apr 16 21:23:37 2020 -0400

    Fix failing pyup API key
    
    - Update vendored safety package
    - Swap to blank pyup API key by default (but allow custom key)
    - Slightly improve safety revendoring
    - Fixes #4188
    
    Signed-off-by: Dan Ryan <dan.ryan@canonical.com>

diff --git a/news/4188.bugfix.rst b/news/4188.bugfix.rst
new file mode 100644
index 00000000..0ea2c943
--- /dev/null
+++ b/news/4188.bugfix.rst
@@ -0,0 +1 @@
+Fixed an issue with ``pipenv check`` failing due to an invalid API key from ``pyup.io``.
diff --git a/pipenv/core.py b/pipenv/core.py
index fffbfb6b..2d1e17a9 100644
--- a/pipenv/core.py
+++ b/pipenv/core.py
@@ -2644,9 +2644,10 @@ def do_check(
             err=True,
         )
     else:
-        ignored = ""
-    key = "--key={0}".format(PIPENV_PYUP_API_KEY)
-    cmd = _cmd + [safety_path, "check", "--json", key]
+        ignored = []
+    cmd = _cmd + [safety_path, "check", "--json"]
+    if PIPENV_PYUP_API_KEY:
+        cmd = cmd + ["--key={0}".format(PIPENV_PYUP_API_KEY)]
     if ignored:
         for cve in ignored:
             cmd += cve
diff --git a/pipenv/environments.py b/pipenv/environments.py
index 622b76ff..79f66019 100644
--- a/pipenv/environments.py
+++ b/pipenv/environments.py
@@ -256,7 +256,7 @@ approach, you may set this to '0', 'off', or 'false'.
 """
 
 PIPENV_PYUP_API_KEY = os.environ.get(
-    "PIPENV_PYUP_API_KEY", "1ab8d58f-5122e025-83674263-bc1e79e0"
+    "PIPENV_PYUP_API_KEY", None
 )
 
 # Internal, support running in a different Python from sys.executable.
diff --git a/pipenv/patched/safety/LICENSE b/pipenv/patched/safety.LICENSE
similarity index 99%
rename from pipenv/patched/safety/LICENSE
rename to pipenv/patched/safety.LICENSE
index 55a1eb03..c5fda558 100644
--- a/pipenv/patched/safety/LICENSE
+++ b/pipenv/patched/safety.LICENSE
@@ -1,4 +1,3 @@
-
 MIT License
 
 Copyright (c) 2016, pyup.io
diff --git a/pipenv/patched/safety.zip b/pipenv/patched/safety.zip
index b839971a..ca31f5b4 100644
Binary files a/pipenv/patched/safety.zip and b/pipenv/patched/safety.zip differ
diff --git a/pipenv/patched/safety/__init__.py b/pipenv/patched/safety/__init__.py
deleted file mode 100644
index e9a6e965..00000000
--- a/pipenv/patched/safety/__init__.py
+++ /dev/null
@@ -1,5 +0,0 @@
-# -*- coding: utf-8 -*-
-
-__author__ = """pyup.io"""
-__email__ = 'support@pyup.io'
-__version__ = '1.8.7'
diff --git a/pipenv/patched/safety/__main__.py b/pipenv/patched/safety/__main__.py
deleted file mode 100644
index d9a0bdab..00000000
--- a/pipenv/patched/safety/__main__.py
+++ /dev/null
@@ -1,8 +0,0 @@
-"""Allow safety to be executable through `python -m safety`."""
-from __future__ import absolute_import
-
-from .cli import cli
-
-
-if __name__ == "__main__":  # pragma: no cover
-    cli(prog_name="safety")
diff --git a/pipenv/patched/safety/cli.py b/pipenv/patched/safety/cli.py
deleted file mode 100644
index 2e8f88df..00000000
--- a/pipenv/patched/safety/cli.py
+++ /dev/null
@@ -1,131 +0,0 @@
-# -*- coding: utf-8 -*-
-from __future__ import absolute_import
-import sys
-import click
-from safety import __version__
-from safety import safety
-from safety.formatter import report
-import itertools
-from safety.util import read_requirements, read_vulnerabilities
-from safety.errors import DatabaseFetchError, DatabaseFileNotFoundError, InvalidKeyError
-
-try:
-    from json.decoder import JSONDecodeError
-except ImportError:
-    JSONDecodeError = ValueError
-
-@click.group()
-@click.version_option(version=__version__)
-def cli():
-    pass
-
-
-@cli.command()
-@click.option("--key", default="",
-              help="API Key for pyup.io's vulnerability database. Can be set as SAFETY_API_KEY "
-                   "environment variable. Default: empty")
-@click.option("--db", default="",
-              help="Path to a local vulnerability database. Default: empty")
-@click.option("--json/--no-json", default=False,
-              help="Output vulnerabilities in JSON format. Default: --no-json")
-@click.option("--full-report/--short-report", default=False,
-              help='Full reports include a security advisory (if available). Default: '
-                   '--short-report')
-@click.option("--bare/--not-bare", default=False,
-              help='Output vulnerable packages only. Useful in combination with other tools.'
-                   'Default: --not-bare')
-@click.option("--cache/--no-cache", default=False,
-              help="Cache requests to the vulnerability database locally. Default: --no-cache")
-@click.option("--stdin/--no-stdin", default=False,
-              help="Read input from stdin. Default: --no-stdin")
-@click.option("files", "--file", "-r", multiple=True, type=click.File(),
-              help="Read input from one (or multiple) requirement files. Default: empty")
-@click.option("ignore", "--ignore", "-i", multiple=True, type=str, default=[],
-              help="Ignore one (or multiple) vulnerabilities by ID. Default: empty")
-@click.option("--output", "-o", default="",
-              help="Path to where output file will be placed. Default: empty")
-@click.option("proxyhost", "--proxy-host", "-ph", multiple=False, type=str, default=None,
-              help="Proxy host IP or DNS --proxy-host")
-@click.option("proxyport", "--proxy-port", "-pp", multiple=False, type=int, default=80,
-              help="Proxy port number --proxy-port")
-@click.option("proxyprotocol", "--proxy-protocol", "-pr", multiple=False, type=str, default='http',
-              help="Proxy protocol (https or http) --proxy-protocol")
-def check(key, db, json, full_report, bare, stdin, files, cache, ignore, output, proxyprotocol, proxyhost, proxyport):
-    if files and stdin:
-        click.secho("Can't read from --stdin and --file at the same time, exiting", fg="red", file=sys.stderr)
-        sys.exit(-1)
-
-    if files:
-        packages = list(itertools.chain.from_iterable(read_requirements(f, resolve=True) for f in files))
-    elif stdin:
-        packages = list(read_requirements(sys.stdin))
-    else:
-        import pkg_resources
-        packages = [
-            d for d in pkg_resources.working_set
-            if d.key not in {"python", "wsgiref", "argparse"}
-        ]    
-    proxy_dictionary = {}
-    if proxyhost is not None:
-        if proxyprotocol in ["http", "https"]:
-            proxy_dictionary = {proxyprotocol: "{0}://{1}:{2}".format(proxyprotocol, proxyhost, str(proxyport))}
-        else:
-            click.secho("Proxy Protocol should be http or https only.", fg="red")
-            sys.exit(-1)
-    try:
-        vulns = safety.check(packages=packages, key=key, db_mirror=db, cached=cache, ignore_ids=ignore, proxy=proxy_dictionary)
-        output_report = report(vulns=vulns, 
-                               full=full_report, 
-                               json_report=json, 
-                               bare_report=bare,
-                               checked_packages=len(packages), 
-                               db=db, 
-                               key=key)
-
-        if output:
-            with open(output, 'w+') as output_file:
-                output_file.write(output_report)
-        else:
-            click.secho(output_report, nl=False if bare and not vulns else True)
-        sys.exit(-1 if vulns else 0)
-    except InvalidKeyError:
-        click.secho("Your API Key '{key}' is invalid. See {link}".format(
-            key=key, link='https://goo.gl/O7Y1rS'),
-            fg="red",
-            file=sys.stderr)
-        sys.exit(-1)
-    except DatabaseFileNotFoundError:
-        click.secho("Unable to load vulnerability database from {db}".format(db=db), fg="red", file=sys.stderr)
-        sys.exit(-1)
-    except DatabaseFetchError:
-        click.secho("Unable to load vulnerability database", fg="red", file=sys.stderr)
-        sys.exit(-1)
-
-
-@cli.command()
-@click.option("--full-report/--short-report", default=False,
-              help='Full reports include a security advisory (if available). Default: '
-                   '--short-report')
-@click.option("--bare/--not-bare", default=False,
-              help='Output vulnerable packages only. Useful in combination with other tools.'
-                   'Default: --not-bare')
-@click.option("file", "--file", "-f", type=click.File(), required=True,
-              help="Read input from an insecure report file. Default: empty")
-def review(full_report, bare, file):
-    if full_report and bare:
-        click.secho("Can't choose both --bare and --full-report/--short-report", fg="red")
-        sys.exit(-1)
-
-    try:
-        input_vulns = read_vulnerabilities(file)
-    except JSONDecodeError:
-        click.secho("Not a valid JSON file", fg="red")
-        sys.exit(-1)
-
-    vulns = safety.review(input_vulns)
-    output_report = report(vulns=vulns, full=full_report, bare_report=bare)
-    click.secho(output_report, nl=False if bare and not vulns else True)
-
-
-if __name__ == "__main__":
-    cli()
diff --git a/pipenv/patched/safety/constants.py b/pipenv/patched/safety/constants.py
deleted file mode 100644
index 71cf6c47..00000000
--- a/pipenv/patched/safety/constants.py
+++ /dev/null
@@ -1,20 +0,0 @@
-# -*- coding: utf-8 -*-
-import os
-
-OPEN_MIRRORS = [
-    "https://raw.githubusercontent.com/pyupio/safety-db/master/data/",
-]
-
-API_MIRRORS = [
-    "https://pyup.io/api/v1/safety/"
-]
-
-REQUEST_TIMEOUT = 5
-
-CACHE_VALID_SECONDS = 60 * 60 * 2  # 2 hours
-
-CACHE_FILE = os.path.join(
-    os.path.expanduser("~"),
-    ".safety",
-    "cache.json"
-)
diff --git a/pipenv/patched/safety/errors.py b/pipenv/patched/safety/errors.py
deleted file mode 100644
index 346adba7..00000000
--- a/pipenv/patched/safety/errors.py
+++ /dev/null
@@ -1,10 +0,0 @@
-class DatabaseFetchError(Exception):
-    pass
-
-
-class DatabaseFileNotFoundError(DatabaseFetchError):
-    pass
-
-
-class InvalidKeyError(DatabaseFetchError):
-    pass
diff --git a/pipenv/patched/safety/formatter.py b/pipenv/patched/safety/formatter.py
deleted file mode 100644
index c19bff1b..00000000
--- a/pipenv/patched/safety/formatter.py
+++ /dev/null
@@ -1,202 +0,0 @@
-# -*- coding: utf-8 -*-
-import platform
-import sys
-import json
-import os
-import textwrap
-
-# python 2.7 compat
-try:
-    FileNotFoundError
-except NameError:
-    FileNotFoundError = IOError
-
-try:
-    system = platform.system()
-    python_version = ".".join([str(i) for i in sys.version_info[0:2]])
-    # get_terminal_size exists on Python 3.4 but isn't working on windows
-    if system == "Windows" and python_version in ["3.4"]:
-        raise ImportError
-    from shutil import get_terminal_size
-except ImportError:
-    # fallback for python < 3
-    import subprocess
-    from collections import namedtuple
-
-    def get_terminal_size():
-        size = namedtuple("_", ["rows", "columns"])
-        try:
-            rows, columns = subprocess.check_output(
-                ['stty', 'size'],
-                stderr=subprocess.STDOUT
-            ).split()
-            return size(rows=int(rows), columns=int(columns))
-        # this won't work
-        # - on windows (FileNotFoundError/OSError)
-        # - python 2.6 (AttributeError)
-        # - if the output is somehow mangled (ValueError)
-        except (ValueError, FileNotFoundError, OSError,
-                AttributeError, subprocess.CalledProcessError):
-            return size(rows=0, columns=0)
-
-
-def get_advisory(vuln):
-    return vuln.advisory if vuln.advisory else "No advisory found for this vulnerability."
-
-
-class SheetReport(object):
-    REPORT_BANNER = """
-╒══════════════════════════════════════════════════════════════════════════════╕
-│                                                                              │
-│                               /$$$$$$            /$$                         │
-│                              /$$__  $$          | $$                         │
-│           /$$$$$$$  /$$$$$$ | $$  \__//$$$$$$  /$$$$$$   /$$   /$$           │
-│          /$$_____/ |____  $$| $$$$   /$$__  $$|_  $$_/  | $$  | $$           │
-│         |  $$$$$$   /$$$$$$$| $$_/  | $$$$$$$$  | $$    | $$  | $$           │
-│          \____  $$ /$$__  $$| $$    | $$_____/  | $$ /$$| $$  | $$           │
-│          /$$$$$$$/|  $$$$$$$| $$    |  $$$$$$$  |  $$$$/|  $$$$$$$           │
-│         |_______/  \_______/|__/     \_______/   \___/   \____  $$           │
-│                                                          /$$  | $$           │
-│                                                         |  $$$$$$/           │
-│  by pyup.io                                              \______/            │
-│                                                                              │
-╞══════════════════════════════════════════════════════════════════════════════╡
-    """.strip()
-
-    TABLE_HEADING = """
-╞════════════════════════════╤═══════════╤══════════════════════════╤══════════╡
-│ package                    │ installed │ affected                 │ ID       │
-╞════════════════════════════╧═══════════╧══════════════════════════╧══════════╡
-    """.strip()
-
-    TABLE_FOOTER = """
-╘════════════════════════════╧═══════════╧══════════════════════════╧══════════╛
-    """.strip()
-
-    TABLE_BREAK = """
-╞════════════════════════════╡═══════════╡══════════════════════════╡══════════╡
-    """.strip()
-
-    REPORT_HEADING = """
-│ REPORT                                                                       │
-    """.strip()
-
-    REPORT_SECTION = """
-╞══════════════════════════════════════════════════════════════════════════════╡
-    """.strip()
-
-    REPORT_FOOTER = """
-╘══════════════════════════════════════════════════════════════════════════════╛
-    """.strip()
-
-    @staticmethod
-    def render(vulns, full, checked_packages, used_db):
-        db_format_str = '{: <' + str(51 - len(str(checked_packages))) + '}'
-        status = "│ checked {packages} packages, using {db} │".format(
-            packages=checked_packages,
-            db=db_format_str.format(used_db),
-            section=SheetReport.REPORT_SECTION
-        )
-        if vulns:
-            table = []
-            for n, vuln in enumerate(vulns):
-                table.append("│ {:26} │ {:9} │ {:24} │ {:8} │".format(
-                    vuln.name[:26],
-                    vuln.version[:9],
-                    vuln.spec[:24],
-                    vuln.vuln_id
-                ))
-                if full:
-                    table.append(SheetReport.REPORT_SECTION)
-
-                    descr = get_advisory(vuln)
-
-                    for pn, paragraph in enumerate(descr.replace('\r', '').split('\n\n')):
-                        if pn:
-                            table.append("│ {:76} │".format(''))
-                        for line in textwrap.wrap(paragraph, width=76):
-                            try:
-                                table.append("│ {:76} │".format(line.encode('utf-8')))
-                            except TypeError:
-                                table.append("│ {:76} │".format(line))
-                    # append the REPORT_SECTION only if this isn't the last entry
-                    if n + 1 < len(vulns):
-                        table.append(SheetReport.REPORT_SECTION)
-            return "\n".join(
-                [SheetReport.REPORT_BANNER, SheetReport.REPORT_HEADING, status, SheetReport.TABLE_HEADING,
-                 "\n".join(table), SheetReport.REPORT_FOOTER]
-            )
-        else:
-            content = "│ {:76} │".format("No known security vulnerabilities found.")
-            return "\n".join(
-                    [SheetReport.REPORT_BANNER, SheetReport.REPORT_HEADING, status, SheetReport.REPORT_SECTION,
-                     content, SheetReport.REPORT_FOOTER]
-                )
-
-
-class BasicReport(object):
-    """Basic report, intented to be used for terminals with < 80 columns"""
-
-    @staticmethod
-    def render(vulns, full, checked_packages, used_db):
-        table = [
-            "safety report",
-            "checked {packages} packages, using {db}".format(
-                packages=checked_packages,
-                db=used_db
-            ),
-            "---"
-        ]
-        if vulns:
-
-            for vuln in vulns:
-                table.append("-> {}, installed {}, affected {}, id {}".format(
-                    vuln.name,
-                    vuln.version[:13],
-                    vuln.spec[:27],
-                    vuln.vuln_id
-                ))
-                if full:
-                    table.append(get_advisory(vuln))
-                    table.append("--")
-        else:
-            table.append("No known security vulnerabilities found.")
-        return "\n".join(
-            table
-        )
-
-
-class JsonReport(object):
-    """Json report, for when the output is input for something else"""
-
-    @staticmethod
-    def render(vulns, full):
-        return json.dumps(vulns, indent=4, sort_keys=True)
-
-
-class BareReport(object):
-    """Bare report, for command line tools"""
-    @staticmethod
-    def render(vulns, full):
-        return " ".join(set([v.name for v in vulns]))
-
-
-def get_used_db(key, db):
-    key = key if key else os.environ.get("SAFETY_API_KEY", False)
-    if key:
-        return "pyup.io's DB"
-    if db == '':
-        return 'default DB'
-    return "local DB"
-
-
-def report(vulns, full=False, json_report=False, bare_report=False, checked_packages=0, db=None, key=None):
-    if bare_report:
-        return BareReport.render(vulns, full=full)
-    if json_report:
-        return JsonReport.render(vulns, full=full)
-    size = get_terminal_size()
-    used_db = get_used_db(key=key, db=db)
-    if size.columns >= 80:
-        return SheetReport.render(vulns, full=full, checked_packages=checked_packages, used_db=used_db)
-    return BasicReport.render(vulns, full=full, checked_packages=checked_packages, used_db=used_db)
diff --git a/pipenv/patched/safety/safety.py b/pipenv/patched/safety/safety.py
deleted file mode 100644
index 2fca3eb2..00000000
--- a/pipenv/patched/safety/safety.py
+++ /dev/null
@@ -1,169 +0,0 @@
-# -*- coding: utf-8 -*-
-import requests
-from packaging.specifiers import SpecifierSet
-from .errors import DatabaseFetchError, InvalidKeyError, DatabaseFileNotFoundError
-from .constants import OPEN_MIRRORS, API_MIRRORS, REQUEST_TIMEOUT, CACHE_VALID_SECONDS, CACHE_FILE
-from collections import namedtuple
-import os
-import json
-import time
-import errno
-
-
-class Vulnerability(namedtuple("Vulnerability",
-                               ["name", "spec", "version", "advisory", "vuln_id"])):
-    pass
-
-
-def get_from_cache(db_name):
-    if os.path.exists(CACHE_FILE):
-        with open(CACHE_FILE) as f:
-            try:
-                data = json.loads(f.read())
-                if db_name in data:
-                    if "cached_at" in data[db_name]:
-                        if data[db_name]["cached_at"] + CACHE_VALID_SECONDS > time.time():
-                            return data[db_name]["db"]
-            except json.JSONDecodeError:
-                pass
-    return False
-
-
-def write_to_cache(db_name, data):
-    # cache is in: ~/safety/cache.json
-    # and has the following form:
-    # {
-    #   "insecure.json": {
-    #       "cached_at": 12345678
-    #       "db": {}
-    #   },
-    #   "insecure_full.json": {
-    #       "cached_at": 12345678
-    #       "db": {}
-    #   },
-    # }
-    if not os.path.exists(os.path.dirname(CACHE_FILE)):
-        try:
-            os.makedirs(os.path.dirname(CACHE_FILE))
-            with open(CACHE_FILE, "w") as _:
-                _.write(json.dumps({}))
-        except OSError as exc:  # Guard against race condition
-            if exc.errno != errno.EEXIST:
-                raise
-
-    with open(CACHE_FILE, "r") as f:
-        try:
-            cache = json.loads(f.read())
-        except json.JSONDecodeError:
-            cache = {}
-
-    with open(CACHE_FILE, "w") as f:
-        cache[db_name] = {
-            "cached_at": time.time(),
-            "db": data
-        }
-        f.write(json.dumps(cache))
-
-
-def fetch_database_url(mirror, db_name, key, cached, proxy):
-
-    headers = {}
-    if key:
-        headers["X-Api-Key"] = key
-
-    if cached:
-        cached_data = get_from_cache(db_name=db_name)
-        if cached_data:
-            return cached_data
-    url = mirror + db_name
-    r = requests.get(url=url, timeout=REQUEST_TIMEOUT, headers=headers, proxies=proxy)
-    if r.status_code == 200:
-        data = r.json()
-        if cached:
-            write_to_cache(db_name, data)
-        return data
-    elif r.status_code == 403:
-        raise InvalidKeyError()
-
-
-def fetch_database_file(path, db_name):
-    full_path = os.path.join(path, db_name)
-    if not os.path.exists(full_path):
-        raise DatabaseFileNotFoundError()
-    with open(full_path) as f:
-        return json.loads(f.read())
-
-
-def fetch_database(full=False, key=False, db=False, cached=False, proxy={}):
-
-    if db:
-        mirrors = [db]
-    else:
-        mirrors = API_MIRRORS if key else OPEN_MIRRORS
-
-    db_name = "insecure_full.json" if full else "insecure.json"
-    for mirror in mirrors:
-        # mirror can either be a local path or a URL
-        if mirror.startswith("http://") or mirror.startswith("https://"):
-            data = fetch_database_url(mirror, db_name=db_name, key=key, cached=cached, proxy=proxy)
-        else:
-            data = fetch_database_file(mirror, db_name=db_name)
-        if data:
-            return data
-    raise DatabaseFetchError()
-
-
-def get_vulnerabilities(pkg, spec, db):
-    for entry in db[pkg]:
-        for entry_spec in entry["specs"]:
-            if entry_spec == spec:
-                yield entry
-
-
-def check(packages, key, db_mirror, cached, ignore_ids, proxy):
-    key = key if key else os.environ.get("SAFETY_API_KEY", False)
-    db = fetch_database(key=key, db=db_mirror, cached=cached, proxy=proxy)
-    db_full = None
-    vulnerable_packages = frozenset(db.keys())
-    vulnerable = []
-    for pkg in packages:
-        # normalize the package name, the safety-db is converting underscores to dashes and uses
-        # lowercase
-        name = pkg.key.replace("_", "-").lower()
-
-        if name in vulnerable_packages:
-            # we have a candidate here, build the spec set
-            for specifier in db[name]:
-                spec_set = SpecifierSet(specifiers=specifier)
-                if spec_set.contains(pkg.version):
-                    if not db_full:
-                        db_full = fetch_database(full=True, key=key, db=db_mirror, cached=cached, proxy=proxy)
-                    for data in get_vulnerabilities(pkg=name, spec=specifier, db=db_full):
-                        vuln_id = data.get("id").replace("pyup.io-", "")
-                        if vuln_id and vuln_id not in ignore_ids:
-                            vulnerable.append(
-                                Vulnerability(
-                                    name=name,
-                                    spec=specifier,
-                                    version=pkg.version,
-                                    advisory=data.get("advisory"),
-                                    vuln_id=vuln_id
-                                )
-                            )
-    return vulnerable
-
-
-def review(vulnerabilities):
-    vulnerable = []
-    for vuln in vulnerabilities:
-        current_vuln = {
-            "name": vuln[0],
-            "spec": vuln[1],
-            "version": vuln[2],
-            "advisory": vuln[3],
-            "vuln_id": vuln[4],
-        }
-        vulnerable.append(
-            Vulnerability(**current_vuln)
-        )
-    return vulnerable
diff --git a/pipenv/patched/safety/util.py b/pipenv/patched/safety/util.py
deleted file mode 100644
index 16062f41..00000000
--- a/pipenv/patched/safety/util.py
+++ /dev/null
@@ -1,98 +0,0 @@
-from dparse.parser import setuptools_parse_requirements_backport as _parse_requirements
-from collections import namedtuple
-import click
-import sys
-import json
-import os
-Package = namedtuple("Package", ["key", "version"])
-RequirementFile = namedtuple("RequirementFile", ["path"])
-
-
-def read_vulnerabilities(fh):
-    return json.load(fh)
-
-
-def iter_lines(fh, lineno=0):
-    for line in fh.readlines()[lineno:]:
-        yield line
-
-
-def parse_line(line):
-    if line.startswith('-e') or line.startswith('http://') or line.startswith('https://'):
-        if "#egg=" in line:
-            line = line.split("#egg=")[-1]
-    return _parse_requirements(line)
-
-
-def read_requirements(fh, resolve=False):
-    """
-    Reads requirements from a file like object and (optionally) from referenced files.
-    :param fh: file like object to read from
-    :param resolve: boolean. resolves referenced files.
-    :return: generator
-    """
-    is_temp_file = not hasattr(fh, 'name')
-    for num, line in enumerate(iter_lines(fh)):
-        line = line.strip()
-        if not line:
-            # skip empty lines
-            continue
-        if line.startswith('#') or \
-            line.startswith('-i') or \
-            line.startswith('--index-url') or \
-            line.startswith('--extra-index-url') or \
-            line.startswith('-f') or line.startswith('--find-links') or \
-            line.startswith('--no-index') or line.startswith('--allow-external') or \
-            line.startswith('--allow-unverified') or line.startswith('-Z') or \
-            line.startswith('--always-unzip'):
-            # skip unsupported lines
-            continue
-        elif line.startswith('-r') or line.startswith('--requirement'):
-            # got a referenced file here, try to resolve the path
-            # if this is a tempfile, skip
-            if is_temp_file:
-                continue
-            filename = line.strip("-r ").strip("--requirement").strip()
-            # if there is a comment, remove it
-            if " #" in filename:
-                filename = filename.split(" #")[0].strip()
-            req_file_path = os.path.join(os.path.dirname(fh.name), filename)
-            if resolve:
-                # recursively yield the resolved requirements
-                if os.path.exists(req_file_path):
-                    with open(req_file_path) as _fh:
-                        for req in read_requirements(_fh, resolve=True):
-                            yield req
-            else:
-                yield RequirementFile(path=req_file_path)
-        else:
-            try:
-                parseable_line = line
-                # multiline requirements are not parseable
-                if "\\" in line:
-                    parseable_line = line.replace("\\", "")
-                    for next_line in iter_lines(fh, num + 1):
-                        parseable_line += next_line.strip().replace("\\", "")
-                        line += "\n" + next_line
-                        if "\\" in next_line:
-                            continue
-                        break
-                req, = parse_line(parseable_line)
-                if len(req.specifier._specs) == 1 and \
-                        next(iter(req.specifier._specs))._spec[0] == "==":
-                    yield Package(key=req.name, version=next(iter(req.specifier._specs))._spec[1])
-                else:
-                    try:
-                        fname = fh.name
-                    except AttributeError:
-                        fname = line
-
-                    click.secho(
-                        "Warning: unpinned requirement '{req}' found in {fname}, "
-                        "unable to check.".format(req=req.name,
-                                                  fname=fname),
-                        fg="yellow",
-                        file=sys.stderr
-                    )
-            except ValueError:
-                continue
diff --git a/tasks/vendoring/__init__.py b/tasks/vendoring/__init__.py
index dbcdac0b..32375682 100644
--- a/tasks/vendoring/__init__.py
+++ b/tasks/vendoring/__init__.py
@@ -8,6 +8,7 @@ import itertools
 import re
 import shutil
 import sys
+
 # from tempfile import TemporaryDirectory
 import tarfile
 import zipfile
@@ -26,72 +27,68 @@ from pipenv.vendor.vistir.contextmanagers import open_file
 import pipenv.vendor.parse as parse
 
 
-TASK_NAME = 'update'
+TASK_NAME = "update"
 
 LIBRARY_DIRNAMES = {
-    'requirements-parser': 'requirements',
-    'backports.shutil_get_terminal_size': 'backports/shutil_get_terminal_size',
-    'backports.weakref': 'backports/weakref',
-    'backports.functools_lru_cache': 'backports/functools_lru_cache',
-    'python-dotenv': 'dotenv',
-    'pip-tools': 'piptools',
-    'setuptools': 'pkg_resources',
-    'msgpack-python': 'msgpack',
-    'attrs': 'attr',
-    'enum': 'backports/enum'
+    "requirements-parser": "requirements",
+    "backports.shutil_get_terminal_size": "backports/shutil_get_terminal_size",
+    "backports.weakref": "backports/weakref",
+    "backports.functools_lru_cache": "backports/functools_lru_cache",
+    "python-dotenv": "dotenv",
+    "pip-tools": "piptools",
+    "setuptools": "pkg_resources",
+    "msgpack-python": "msgpack",
+    "attrs": "attr",
+    "enum": "backports/enum",
 }
 
-PY2_DOWNLOAD = ['enum34']
+PY2_DOWNLOAD = ["enum34"]
 
 # from time to time, remove the no longer needed ones
 HARDCODED_LICENSE_URLS = {
-    'pytoml': 'https://github.com/avakar/pytoml/raw/master/LICENSE',
-    'cursor': 'https://raw.githubusercontent.com/GijsTimmers/cursor/master/LICENSE',
-    'delegator.py': 'https://raw.githubusercontent.com/kennethreitz/delegator.py/master/LICENSE',
-    'click-didyoumean': 'https://raw.githubusercontent.com/click-contrib/click-didyoumean/master/LICENSE',
-    'click-completion': 'https://raw.githubusercontent.com/click-contrib/click-completion/master/LICENSE',
-    'parse': 'https://raw.githubusercontent.com/techalchemy/parse/master/LICENSE',
-    'semver': 'https://raw.githubusercontent.com/k-bx/python-semver/master/LICENSE.txt',
-    'crayons': 'https://raw.githubusercontent.com/kennethreitz/crayons/master/LICENSE',
-    'pip-tools': 'https://raw.githubusercontent.com/jazzband/pip-tools/master/LICENSE',
-    'pytoml': 'https://github.com/avakar/pytoml/raw/master/LICENSE',
-    'webencodings': 'https://github.com/SimonSapin/python-webencodings/raw/'
-                    'master/LICENSE',
-    'requirementslib': 'https://github.com/techalchemy/requirementslib/raw/master/LICENSE',
-    'distlib': 'https://github.com/vsajip/distlib/raw/master/LICENSE.txt',
-    'pythonfinder': 'https://raw.githubusercontent.com/techalchemy/pythonfinder/master/LICENSE.txt',
-    'pyparsing': 'https://raw.githubusercontent.com/pyparsing/pyparsing/master/LICENSE',
-    'resolvelib': 'https://raw.githubusercontent.com/sarugaku/resolvelib/master/LICENSE',
-    'funcsigs': 'https://raw.githubusercontent.com/aliles/funcsigs/master/LICENSE'
+    "pytoml": "https://github.com/avakar/pytoml/raw/master/LICENSE",
+    "cursor": "https://raw.githubusercontent.com/GijsTimmers/cursor/master/LICENSE",
+    "delegator.py": "https://raw.githubusercontent.com/kennethreitz/delegator.py/master/LICENSE",
+    "click-didyoumean": "https://raw.githubusercontent.com/click-contrib/click-didyoumean/master/LICENSE",
+    "click-completion": "https://raw.githubusercontent.com/click-contrib/click-completion/master/LICENSE",
+    "parse": "https://raw.githubusercontent.com/techalchemy/parse/master/LICENSE",
+    "semver": "https://raw.githubusercontent.com/k-bx/python-semver/master/LICENSE.txt",
+    "crayons": "https://raw.githubusercontent.com/kennethreitz/crayons/master/LICENSE",
+    "pip-tools": "https://raw.githubusercontent.com/jazzband/pip-tools/master/LICENSE",
+    "pytoml": "https://github.com/avakar/pytoml/raw/master/LICENSE",
+    "webencodings": "https://github.com/SimonSapin/python-webencodings/raw/"
+    "master/LICENSE",
+    "requirementslib": "https://github.com/techalchemy/requirementslib/raw/master/LICENSE",
+    "distlib": "https://github.com/vsajip/distlib/raw/master/LICENSE.txt",
+    "pythonfinder": "https://raw.githubusercontent.com/techalchemy/pythonfinder/master/LICENSE.txt",
+    "pyparsing": "https://raw.githubusercontent.com/pyparsing/pyparsing/master/LICENSE",
+    "resolvelib": "https://raw.githubusercontent.com/sarugaku/resolvelib/master/LICENSE",
+    "funcsigs": "https://raw.githubusercontent.com/aliles/funcsigs/master/LICENSE",
 }
 
 FILE_WHITE_LIST = (
-    'Makefile',
-    'vendor.txt',
-    'patched.txt',
-    '__init__.py',
-    'README.rst',
-    'README.md',
-    'appdirs.py',
-    'safety.zip',
-    'cacert.pem',
-    'vendor_pip.txt',
+    "Makefile",
+    "vendor.txt",
+    "patched.txt",
+    "__init__.py",
+    "README.rst",
+    "README.md",
+    "appdirs.py",
+    "safety.zip",
+    "cacert.pem",
+    "vendor_pip.txt",
 )
 
-PATCHED_RENAMES = {
-    'pip': 'notpip'
-}
+PATCHED_RENAMES = {"pip": "notpip"}
 
 LIBRARY_RENAMES = {
-    'pip': 'pipenv.patched.notpip',
+    "pip": "pipenv.patched.notpip",
     "functools32": "pipenv.vendor.backports.functools_lru_cache",
-    'enum34': 'enum',
+    "enum34": "enum",
 }
 
 
-LICENSE_RENAMES = {
-    "pythonfinder/LICENSE": "pythonfinder/pep514tools.LICENSE"
-}
+LICENSE_RENAMES = {"pythonfinder/LICENSE": "pythonfinder/pep514tools.LICENSE"}
 
 
 def drop_dir(path):
@@ -108,32 +105,32 @@ def remove_all(paths):
 
 
 def log(msg):
-    print('[vendoring.%s] %s' % (TASK_NAME, msg))
+    print("[vendoring.%s] %s" % (TASK_NAME, msg))
 
 
 def _get_git_root(ctx):
-    return Path(ctx.run('git rev-parse --show-toplevel', hide=True).stdout.strip())
+    return Path(ctx.run("git rev-parse --show-toplevel", hide=True).stdout.strip())
 
 
 def _get_vendor_dir(ctx):
-    return _get_git_root(ctx) / 'pipenv' / 'vendor'
+    return _get_git_root(ctx) / "pipenv" / "vendor"
 
 
 def _get_patched_dir(ctx):
-    return _get_git_root(ctx) / 'pipenv' / 'patched'
+    return _get_git_root(ctx) / "pipenv" / "patched"
 
 
 def clean_vendor(ctx, vendor_dir):
     # Old _vendor cleanup
-    remove_all(vendor_dir.glob('*.pyc'))
-    log('Cleaning %s' % vendor_dir)
+    remove_all(vendor_dir.glob("*.pyc"))
+    log("Cleaning %s" % vendor_dir)
     for item in vendor_dir.iterdir():
         if item.is_dir():
             shutil.rmtree(str(item))
         elif item.name not in FILE_WHITE_LIST:
             item.unlink()
         else:
-            log('Skipping %s' % item)
+            log("Skipping %s" % item)
 
 
 def detect_vendored_libs(vendor_dir):
@@ -154,7 +151,7 @@ def rewrite_imports(package_dir, vendored_libs, vendor_dir):
     for item in package_dir.iterdir():
         if item.is_dir():
             rewrite_imports(item, vendored_libs, vendor_dir)
-        elif item.name.endswith('.py'):
+        elif item.name.endswith(".py"):
             rewrite_file_imports(item, vendored_libs, vendor_dir)
 
 
@@ -162,9 +159,9 @@ def rewrite_file_imports(item, vendored_libs, vendor_dir):
     """Rewrite 'import xxx' and 'from xxx import' for vendored_libs"""
     # log('Reading file: %s' % item)
     try:
-        text = item.read_text(encoding='utf-8')
+        text = item.read_text(encoding="utf-8")
     except UnicodeDecodeError:
-        text = item.read_text(encoding='cp1252')
+        text = item.read_text(encoding="cp1252")
     renames = LIBRARY_RENAMES
     for k in LIBRARY_RENAMES.keys():
         if k not in vendored_libs:
@@ -174,111 +171,124 @@ def rewrite_file_imports(item, vendored_libs, vendor_dir):
         if lib in renames:
             to_lib = renames[lib]
         text = re.sub(
-            r'([\n\s]*)import %s([\n\s\.]+)' % lib,
-            r'\1import %s\2' % to_lib,
-            text,
-        )
-        text = re.sub(
-            r'([\n\s]*)from %s([\s\.])+' % lib,
-            r'\1from %s\2' % to_lib,
-            text,
+            r"([\n\s]*)import %s([\n\s\.]+)" % lib, r"\1import %s\2" % to_lib, text,
         )
+        text = re.sub(r"([\n\s]*)from %s([\s\.])+" % lib, r"\1from %s\2" % to_lib, text,)
         text = re.sub(
             r"(\n\s*)__import__\('%s([\s'\.])+" % lib,
             r"\1__import__('%s\2" % to_lib,
             text,
         )
-    item.write_text(text, encoding='utf-8')
+    item.write_text(text, encoding="utf-8")
 
 
 def apply_patch(ctx, patch_file_path):
-    log('Applying patch %s' % patch_file_path.name)
-    ctx.run('git apply --ignore-whitespace --verbose %s' % patch_file_path)
+    log("Applying patch %s" % patch_file_path.name)
+    ctx.run("git apply --ignore-whitespace --verbose %s" % patch_file_path)
+
+
+def _recursive_write_to_zip(zf, path, root=None):
+    if path == Path(zf.filename):
+        return
+    if root is None:
+        if not path.is_dir():
+            raise ValueError('root is required for non-directory path')
+        root = path
+    if not path.is_dir():
+        zf.write(str(path), str(path.relative_to(root)))
+        return
+    for c in path.iterdir():
+        _recursive_write_to_zip(zf, c, root)
 
 
 @invoke.task
 def update_safety(ctx):
-    ignore_subdeps = ['pip', 'pip-egg-info', 'bin']
-    ignore_files = ['pip-delete-this-directory.txt', 'PKG-INFO']
-    vendor_dir = _get_patched_dir(ctx)
-    log('Using vendor dir: %s' % vendor_dir)
-    log('Downloading safety package files...')
-    build_dir = vendor_dir / 'build'
-    download_dir = TemporaryDirectory(prefix='pipenv-', suffix='-safety')
-    if build_dir.exists() and build_dir.is_dir():
-        drop_dir(build_dir)
-
-    ctx.run(
-        'pip download -b {0} --no-binary=:all: --no-clean -d {1} safety pyyaml'.format(
-            str(build_dir), str(download_dir.name),
+    ignore_subdeps = ["pip", "pip-egg-info", "bin", "pipenv", "virtualenv", "virtualenv-clone"]
+    ignore_files = ["pip-delete-this-directory.txt", "PKG-INFO"]
+    ignore_patterns = ["*.pyd", "*.so"]
+    patched_dir = _get_patched_dir(ctx)
+    vendor_dir = _get_vendor_dir(ctx)
+    log("Using vendor dir: %s" % patched_dir)
+    log("Downloading safety package files...")
+    build_dir = patched_dir / "build"
+    with TemporaryDirectory(prefix="pipenv-", suffix="-safety") as download_dir:
+        if build_dir.exists() and build_dir.is_dir():
+            log("dropping pre-existing build dir at {0}".format(build_dir.as_posix()))
+            drop_dir(build_dir)
+        pip_command = "pip download -b {0} --no-binary=:all: --no-clean -d {1} pyyaml safety".format(
+            build_dir.absolute().as_posix(), str(download_dir.name),
         )
-    )
-    safety_dir = build_dir / 'safety'
-    yaml_build_dir = build_dir / 'pyyaml'
-    main_file = safety_dir / '__main__.py'
-    main_content = """
-import sys
-yaml_lib = 'yaml{0}'.format(sys.version_info[0])
-locals()[yaml_lib] = __import__(yaml_lib)
-sys.modules['yaml'] = sys.modules[yaml_lib]
-from safety.cli import cli
-
-# Disable insecure warnings.
-import urllib3
-from urllib3.exceptions import InsecureRequestWarning
-urllib3.disable_warnings(InsecureRequestWarning)
-
-cli(prog_name="safety")
-    """.strip()
-    with open(str(main_file), 'w') as fh:
-        fh.write(main_content)
-
-    with ctx.cd(str(safety_dir)):
-        ctx.run('pip install --no-compile --no-binary=:all: -t . .')
-        safety_dir = safety_dir.absolute()
-        yaml_dir = safety_dir / 'yaml'
-        if yaml_dir.exists():
-            version_choices = ['2', '3']
-            version_choices.remove(str(sys.version_info[0]))
-            mkdir_p(str(safety_dir / 'yaml{0}'.format(sys.version_info[0])))
-            for fn in yaml_dir.glob('*.py'):
-                fn.rename(str(safety_dir.joinpath('yaml{0}'.format(sys.version_info[0]), fn.name)))
-            if version_choices[0] == '2':
-                lib = yaml_build_dir / 'lib' / 'yaml'
-            else:
-                lib = yaml_build_dir / 'lib3' / 'yaml'
-            shutil.copytree(str(lib.absolute()), str(safety_dir / 'yaml{0}'.format(version_choices[0])))
-        requests_dir = safety_dir / 'requests'
-        cacert = vendor_dir / 'requests' / 'cacert.pem'
-        if not cacert.exists():
-            from pipenv.vendor import requests
-            cacert = Path(requests.certs.where())
-        target_cert = requests_dir / 'cacert.pem'
-        target_cert.write_bytes(cacert.read_bytes())
-        ctx.run("sed -i 's/r = requests.get(url=url, timeout=REQUEST_TIMEOUT, headers=headers)/r = requests.get(url=url, timeout=REQUEST_TIMEOUT, headers=headers, verify=False)/g' {0}".format(str(safety_dir / 'safety' / 'safety.py')))
-        for egg in safety_dir.glob('*.egg-info'):
-            drop_dir(egg.absolute())
-        for dep in ignore_subdeps:
-            dep_dir = safety_dir / dep
-            if dep_dir.exists():
-                drop_dir(dep_dir)
-        for dep in ignore_files:
-            fn = safety_dir / dep
-            if fn.exists():
-                fn.unlink()
-    zip_name = '{0}/safety'.format(str(vendor_dir))
-    shutil.make_archive(zip_name, format='zip', root_dir=str(safety_dir), base_dir='./')
-    drop_dir(build_dir)
-    download_dir.cleanup()
+        log("downloading deps via pip: {0}".format(pip_command))
+        ctx.run(pip_command)
+        safety_dir = Path(__file__).absolute().parent.joinpath("safety")
+        safety_build_dir = build_dir / "safety"
+        yaml_build_dir = build_dir / "pyyaml"
+        lib_dir = safety_dir.joinpath("lib")
+
+        with ctx.cd(str(safety_dir)):
+            lib_dir.mkdir(exist_ok=True)
+            install_cmd = "pip install --no-compile --no-binary=:all: -t {0} {1}".format(lib_dir.as_posix(), safety_build_dir.as_posix())
+            log("installing dependencies: {0}".format(install_cmd))
+            ctx.run(install_cmd)
+            safety_dir = safety_dir.absolute()
+            yaml_dir = lib_dir / "yaml"
+            yaml_lib_dir_map = {
+                "2": {
+                    "current_path": yaml_build_dir / "lib/yaml",
+                    "destination": lib_dir / "yaml2",
+                },
+                "3": {
+                    "current_path": yaml_build_dir / "lib3/yaml",
+                    "destination": lib_dir / "yaml3",
+                },
+            }
+            if yaml_dir.exists():
+                drop_dir(yaml_dir)
+            log("Mapping yaml paths for python 2 and 3...")
+            for py_version, path_dict in yaml_lib_dir_map.items():
+                path_dict["current_path"].rename(path_dict["destination"])
+            log("Ensuring certificates are available...")
+            requests_dir = lib_dir / "requests"
+            cacert = vendor_dir / "certifi" / "cacert.pem"
+            if not cacert.exists():
+                from pipenv.vendor import requests
+                cacert = Path(requests.certs.where())
+            target_cert = requests_dir / "cacert.pem"
+            target_cert.write_bytes(cacert.read_bytes())
+            log("dropping ignored files...")
+            for pattern in ignore_patterns:
+                for path in lib_dir.rglob(pattern):
+                    log("removing {0!s}".format(path))
+                    path.unlink()
+            for dep in ignore_subdeps:
+                if lib_dir.joinpath(dep).exists():
+                    log("cleaning up {0}".format(dep))
+                    drop_dir(lib_dir.joinpath(dep))
+                for path in itertools.chain.from_iterable((
+                    lib_dir.rglob("{0}*.egg-info".format(dep)),
+                    lib_dir.rglob("{0}*.dist-info".format(dep))
+                )):
+                    log("cleaning up {0}".format(path))
+                    drop_dir(path)
+            for fn in ignore_files:
+                for path in lib_dir.rglob(fn):
+                    log("cleaning up {0}".format(path))
+                    path.unlink()
+        zip_name = "{0}/safety.zip".format(str(patched_dir))
+        log("writing zipfile...")
+        with zipfile.ZipFile(zip_name, 'w', compression=zipfile.ZIP_DEFLATED, compresslevel=6) as zf:
+            _recursive_write_to_zip(zf, safety_dir)
+        drop_dir(build_dir)
+        drop_dir(lib_dir)
 
 
 def rename_if_needed(ctx, vendor_dir, item):
-    rename_dict = LIBRARY_RENAMES if vendor_dir.name != 'patched' else PATCHED_RENAMES
+    rename_dict = LIBRARY_RENAMES if vendor_dir.name != "patched" else PATCHED_RENAMES
     new_path = None
     if item.name in rename_dict or item.name in LIBRARY_DIRNAMES:
         new_name = rename_dict.get(item.name, LIBRARY_DIRNAMES.get(item.name))
         new_path = item.parent / new_name
-        log('Renaming %s => %s' % (item.name, new_path))
+        log("Renaming %s => %s" % (item.name, new_path))
         # handle existing directories
         try:
             item.rename(str(new_path))
@@ -288,38 +298,40 @@ def rename_if_needed(ctx, vendor_dir, item):
 
 
 def write_backport_imports(ctx, vendor_dir):
-    backport_dir = vendor_dir / 'backports'
+    backport_dir = vendor_dir / "backports"
     if not backport_dir.exists():
         return
-    backport_init = backport_dir / '__init__.py'
+    backport_init = backport_dir / "__init__.py"
     backport_libs = detect_vendored_libs(backport_dir)
     init_py_lines = backport_init.read_text().splitlines()
     for lib in backport_libs:
-        lib_line = 'from . import {0}'.format(lib)
+        lib_line = "from . import {0}".format(lib)
         if lib_line not in init_py_lines:
-            log('Adding backport %s to __init__.py exports' % lib)
+            log("Adding backport %s to __init__.py exports" % lib)
             init_py_lines.append(lib_line)
-    backport_init.write_text('\n'.join(init_py_lines) + '\n')
+    backport_init.write_text("\n".join(init_py_lines) + "\n")
 
 
 def _ensure_package_in_requirements(ctx, requirements_file, package):
     requirement = None
-    log('using requirements file: %s' % requirements_file)
+    log("using requirements file: %s" % requirements_file)
     req_file_lines = [l for l in requirements_file.read_text().splitlines()]
     if package:
         match = [r for r in req_file_lines if r.strip().lower().startswith(package)]
         matched_req = None
         if match:
             for m in match:
-                specifiers = [m.index(s) for s in ['>', '<', '=', '~'] if s in m]
-                if m.lower() == package or (specifiers and m[:min(specifiers)].lower() == package):
+                specifiers = [m.index(s) for s in [">", "<", "=", "~"] if s in m]
+                if m.lower() == package or (
+                    specifiers and m[: min(specifiers)].lower() == package
+                ):
                     matched_req = "{0}".format(m)
                     requirement = matched_req
                     log("Matched req: %r" % matched_req)
         if not matched_req:
             req_file_lines.append("{0}".format(package))
             log("Writing requirements file: %s" % requirements_file)
-            requirements_file.write_text('\n'.join(req_file_lines))
+            requirements_file.write_text("\n".join(req_file_lines))
             requirement = "{0}".format(package)
     return requirement
 
@@ -327,16 +339,15 @@ def _ensure_package_in_requirements(ctx, requirements_file, package):
 def install(ctx, vendor_dir, package=None):
     requirements_file = vendor_dir / "{0}.txt".format(vendor_dir.name)
     requirement = "-r {0}".format(requirements_file.as_posix())
-    log('Using requirements file: %s' % requirement)
+    log("Using requirements file: %s" % requirement)
     if package:
         requirement = _ensure_package_in_requirements(ctx, requirements_file, package)
     # We use --no-deps because we want to ensure that all of our dependencies
     # are added to vendor.txt, this includes all dependencies recursively up
     # the chain.
     ctx.run(
-        'pip install -t {0} --no-compile --no-deps --upgrade {1}'.format(
-            vendor_dir.as_posix(),
-            requirement,
+        "pip install -t {0} --no-compile --no-deps --upgrade {1}".format(
+            vendor_dir.as_posix(), requirement,
         )
     )
     # read licenses from distinfo files if possible
@@ -346,24 +357,32 @@ def install(ctx, vendor_dir, package=None):
         if not license_file.exists():
             continue
         if vendor_dir.joinpath(pkg).exists():
-            vendor_dir.joinpath(pkg).joinpath("LICENSE").write_text(license_file.read_text())
+            vendor_dir.joinpath(pkg).joinpath("LICENSE").write_text(
+                license_file.read_text()
+            )
         elif vendor_dir.joinpath("{0}.py".format(pkg)).exists():
-            vendor_dir.joinpath("{0}.py.LICENSE".format(pkg)).write_text(license_file.read_text())
+            vendor_dir.joinpath("{0}.py.LICENSE".format(pkg)).write_text(
+                license_file.read_text()
+            )
         else:
-            matched_path = next(iter(pth for pth in vendor_dir.glob("{0}*".format(pkg))), None)
+            matched_path = next(
+                iter(pth for pth in vendor_dir.glob("{0}*".format(pkg))), None
+            )
             if matched_path is not None:
-                vendor_dir.joinpath("{0}.LICENSE".format(matched_path)).write_text(license_file.read_text())
+                vendor_dir.joinpath("{0}.LICENSE".format(matched_path)).write_text(
+                    license_file.read_text()
+                )
 
 
 def post_install_cleanup(ctx, vendor_dir):
-    remove_all(vendor_dir.glob('*.dist-info'))
-    remove_all(vendor_dir.glob('*.egg-info'))
+    remove_all(vendor_dir.glob("*.dist-info"))
+    remove_all(vendor_dir.glob("*.egg-info"))
 
     # Cleanup setuptools unneeded parts
-    drop_dir(vendor_dir / 'bin')
-    drop_dir(vendor_dir / 'tests')
-    drop_dir(vendor_dir / 'shutil_backports')
-    remove_all(vendor_dir.glob('toml.py'))
+    drop_dir(vendor_dir / "bin")
+    drop_dir(vendor_dir / "tests")
+    drop_dir(vendor_dir / "shutil_backports")
+    remove_all(vendor_dir.glob("toml.py"))
 
 
 @invoke.task
@@ -373,24 +392,24 @@ def apply_patches(ctx, patched=False, pre=False):
     else:
         vendor_dir = _get_vendor_dir(ctx)
     log("Applying pre-patches...")
-    patch_dir = Path(__file__).parent / 'patches' / vendor_dir.name
+    patch_dir = Path(__file__).parent / "patches" / vendor_dir.name
     if pre:
         if not patched:
             pass
-        for patch in patch_dir.glob('*.patch'):
-            if not patch.name.startswith('_post'):
+        for patch in patch_dir.glob("*.patch"):
+            if not patch.name.startswith("_post"):
                 apply_patch(ctx, patch)
     else:
-        patches = patch_dir.glob('*.patch' if not patched else '_post*.patch')
+        patches = patch_dir.glob("*.patch" if not patched else "_post*.patch")
         for patch in patches:
             apply_patch(ctx, patch)
 
 
 def vendor(ctx, vendor_dir, package=None, rewrite=True):
-    log('Reinstalling vendored libraries')
-    is_patched = vendor_dir.name == 'patched'
+    log("Reinstalling vendored libraries")
+    is_patched = vendor_dir.name == "patched"
     install(ctx, vendor_dir, package=package)
-    log('Running post-install cleanup...')
+    log("Running post-install cleanup...")
     post_install_cleanup(ctx, vendor_dir)
     # Detect the vendored packages/modules
     vendored_libs = detect_vendored_libs(_get_vendor_dir(ctx))
@@ -401,18 +420,18 @@ def vendor(ctx, vendor_dir, package=None, rewrite=True):
     if is_patched:
         apply_patches(ctx, patched=is_patched, pre=True)
     log("Removing scandir library files...")
-    remove_all(vendor_dir.glob('*.so'))
-    drop_dir(vendor_dir / 'setuptools')
-    drop_dir(vendor_dir / 'pkg_resources' / '_vendor')
-    drop_dir(vendor_dir / 'pkg_resources' / 'extern')
-    drop_dir(vendor_dir / 'bin')
+    remove_all(vendor_dir.glob("*.so"))
+    drop_dir(vendor_dir / "setuptools")
+    drop_dir(vendor_dir / "pkg_resources" / "_vendor")
+    drop_dir(vendor_dir / "pkg_resources" / "extern")
+    drop_dir(vendor_dir / "bin")
 
     # Global import rewrites
-    log('Renaming specified libs...')
+    log("Renaming specified libs...")
     for item in vendor_dir.iterdir():
         if item.is_dir():
             if rewrite and not package or (package and item.name.lower() in package):
-                log('Rewriting imports for %s...' % item)
+                log("Rewriting imports for %s..." % item)
                 rewrite_imports(item, vendored_libs, vendor_dir)
             rename_if_needed(ctx, vendor_dir, item)
         elif item.name not in FILE_WHITE_LIST:
@@ -422,23 +441,23 @@ def vendor(ctx, vendor_dir, package=None, rewrite=True):
     if not package:
         apply_patches(ctx, patched=is_patched, pre=False)
         if is_patched:
-            piptools_vendor = vendor_dir / 'piptools' / '_vendored'
+            piptools_vendor = vendor_dir / "piptools" / "_vendored"
             if piptools_vendor.exists():
                 drop_dir(piptools_vendor)
-            msgpack = vendor_dir / 'notpip' / '_vendor' / 'msgpack'
+            msgpack = vendor_dir / "notpip" / "_vendor" / "msgpack"
             if msgpack.exists():
-                remove_all(msgpack.glob('*.so'))
+                remove_all(msgpack.glob("*.so"))
 
 
 @invoke.task
 def redo_imports(ctx, library):
     vendor_dir = _get_vendor_dir(ctx)
-    log('Using vendor dir: %s' % vendor_dir)
+    log("Using vendor dir: %s" % vendor_dir)
     vendored_libs = detect_vendored_libs(vendor_dir)
     item = vendor_dir / library
-    library_name = vendor_dir / '{0}.py'.format(library)
+    library_name = vendor_dir / "{0}.py".format(library)
     log("Detected vendored libraries: %s" % ", ".join(vendored_libs))
-    log('Rewriting imports for %s...' % item)
+    log("Rewriting imports for %s..." % item)
     if item.is_dir():
         rewrite_imports(item, vendored_libs, vendor_dir)
     else:
@@ -448,7 +467,7 @@ def redo_imports(ctx, library):
 @invoke.task
 def rewrite_all_imports(ctx):
     vendor_dir = _get_vendor_dir(ctx)
-    log('Using vendor dir: %s' % vendor_dir)
+    log("Using vendor dir: %s" % vendor_dir)
     vendored_libs = detect_vendored_libs(vendor_dir)
     log("Detected vendored libraries: %s" % ", ".join(vendored_libs))
     log("Rewriting all imports related to vendored libs")
@@ -460,16 +479,21 @@ def rewrite_all_imports(ctx):
 
 
 @invoke.task
-def packages_missing_licenses(ctx, vendor_dir=None, requirements_file='vendor.txt', package=None):
+def packages_missing_licenses(
+    ctx, vendor_dir=None, requirements_file="vendor.txt", package=None
+):
     if not vendor_dir:
         vendor_dir = _get_vendor_dir(ctx)
     requirements = vendor_dir.joinpath(requirements_file).read_text().splitlines()
     new_requirements = []
     LICENSE_EXTS = ("rst", "txt", "APACHE", "BSD", "md")
-    LICENSES = [".".join(lic) for lic in itertools.product(("LICENSE", "LICENSE-MIT"), LICENSE_EXTS)]
+    LICENSES = [
+        ".".join(lic)
+        for lic in itertools.product(("LICENSE", "LICENSE-MIT"), LICENSE_EXTS)
+    ]
     for i, req in enumerate(requirements):
         pkg = req.strip().split("=")[0]
-        possible_pkgs = [pkg, pkg.replace('-', '_')]
+        possible_pkgs = [pkg, pkg.replace("-", "_")]
         match_found = False
         if pkg in PY2_DOWNLOAD:
             match_found = True
@@ -506,29 +530,37 @@ def packages_missing_licenses(ctx, vendor_dir=None, requirements_file='vendor.tx
 
 @invoke.task
 def download_licenses(
-    ctx, vendor_dir=None, requirements_file='vendor.txt', package=None, only=False,
-    patched=False
+    ctx,
+    vendor_dir=None,
+    requirements_file="vendor.txt",
+    package=None,
+    only=False,
+    patched=False,
 ):
-    log('Downloading licenses')
+    log("Downloading licenses")
     if not vendor_dir:
         if patched:
             vendor_dir = _get_patched_dir(ctx)
-            requirements_file = 'patched.txt'
+            requirements_file = "patched.txt"
         else:
             vendor_dir = _get_vendor_dir(ctx)
     requirements_file = vendor_dir / requirements_file
-    requirements = packages_missing_licenses(ctx, vendor_dir, requirements_file, package=package)
+    requirements = packages_missing_licenses(
+        ctx, vendor_dir, requirements_file, package=package
+    )
 
-    with NamedTemporaryFile(prefix="pipenv", suffix="vendor-reqs", delete=False, mode="w") as fh:
+    with NamedTemporaryFile(
+        prefix="pipenv", suffix="vendor-reqs", delete=False, mode="w"
+    ) as fh:
         fh.write("\n".join(requirements))
         new_requirements_file = fh.name
     new_requirements_file = Path(new_requirements_file)
     log(requirements)
-    tmp_dir = vendor_dir / '__tmp__'
+    tmp_dir = vendor_dir / "__tmp__"
     # TODO: Fix this whenever it gets sorted out (see https://github.com/pypa/pip/issues/5739)
     cmd = "pip download --no-binary :all: --only-binary requests_download --no-deps"
     enum_cmd = "pip download --no-deps"
-    ctx.run('pip install flit')  # needed for the next step
+    ctx.run("pip install flit")  # needed for the next step
     for req in requirements_file.read_text().splitlines():
         if req.startswith("enum34"):
             exe_cmd = "{0} -d {1} {2}".format(enum_cmd, tmp_dir.as_posix(), req)
@@ -553,9 +585,7 @@ def download_licenses(
                     backend, _, _ = backend.partition(".")
                 ctx.run("pip install {0}".format(backend))
             ctx.run(
-                "{0} --no-build-isolation -d {1} {2}".format(
-                    cmd, tmp_dir.as_posix(), req
-                )
+                "{0} --no-build-isolation -d {1} {2}".format(cmd, tmp_dir.as_posix(), req)
             )
     for sdist in tmp_dir.iterdir():
         extract_license(vendor_dir, sdist)
@@ -564,18 +594,18 @@ def download_licenses(
 
 
 def extract_license(vendor_dir, sdist):
-    if sdist.stem.endswith('.tar'):
+    if sdist.stem.endswith(".tar"):
         ext = sdist.suffix[1:]
-        with tarfile.open(sdist, mode='r:{}'.format(ext)) as tar:
+        with tarfile.open(sdist, mode="r:{}".format(ext)) as tar:
             found = find_and_extract_license(vendor_dir, tar, tar.getmembers())
-    elif sdist.suffix in ('.zip', '.whl'):
+    elif sdist.suffix in (".zip", ".whl"):
         with zipfile.ZipFile(sdist) as zip:
             found = find_and_extract_license(vendor_dir, zip, zip.infolist())
     else:
-        raise NotImplementedError('new sdist type!')
+        raise NotImplementedError("new sdist type!")
 
     if not found:
-        log('License not found in {}, will download'.format(sdist.name))
+        log("License not found in {}, will download".format(sdist.name))
         license_fallback(vendor_dir, sdist.name)
 
 
@@ -586,10 +616,10 @@ def find_and_extract_license(vendor_dir, tar, members):
             name = member.name
         except AttributeError:  # zipfile
             name = member.filename
-        if 'LICENSE' in name or 'COPYING' in name:
-            if '/test' in name:
+        if "LICENSE" in name or "COPYING" in name:
+            if "/test" in name:
                 # some testing licenses in hml5lib and distlib
-                log('Ignoring {}'.format(name))
+                log("Ignoring {}".format(name))
                 continue
             found = True
             extract_license_member(vendor_dir, tar, member, name)
@@ -600,13 +630,13 @@ def license_fallback(vendor_dir, sdist_name):
     """Hardcoded license URLs. Check when updating if those are still needed"""
     libname = libname_from_dir(sdist_name)
     if libname not in HARDCODED_LICENSE_URLS:
-        raise ValueError('No hardcoded URL for {} license'.format(libname))
+        raise ValueError("No hardcoded URL for {} license".format(libname))
 
     url = HARDCODED_LICENSE_URLS[libname]
-    _, _, name = url.rpartition('/')
+    _, _, name = url.rpartition("/")
     dest = license_destination(vendor_dir, libname, name)
     r = requests.get(url, allow_redirects=True)
-    log('Downloading {}'.format(url))
+    log("Downloading {}".format(url))
     r.raise_for_status()
     dest.write_bytes(r.content)
 
@@ -614,11 +644,11 @@ def license_fallback(vendor_dir, sdist_name):
 def libname_from_dir(dirname):
     """Reconstruct the library name without it's version"""
     parts = []
-    for part in dirname.split('-'):
+    for part in dirname.split("-"):
         if part[0].isdigit():
             break
         parts.append(part)
-    return '-'.join(parts)
+    return "-".join(parts)
 
 
 def license_destination(vendor_dir, libname, filename):
@@ -626,10 +656,10 @@ def license_destination(vendor_dir, libname, filename):
     normal = vendor_dir / libname
     if normal.is_dir():
         return normal / filename
-    lowercase = vendor_dir / libname.lower().replace('-', '_')
+    lowercase = vendor_dir / libname.lower().replace("-", "_")
     if lowercase.is_dir():
         return lowercase / filename
-    rename_dict = LIBRARY_RENAMES if vendor_dir.name != 'patched' else PATCHED_RENAMES
+    rename_dict = LIBRARY_RENAMES if vendor_dir.name != "patched" else PATCHED_RENAMES
     # Short circuit all logic if we are renaming the whole library
     if libname in rename_dict:
         return vendor_dir / rename_dict[libname] / filename
@@ -637,15 +667,15 @@ def license_destination(vendor_dir, libname, filename):
         override = vendor_dir / LIBRARY_DIRNAMES[libname]
         if not override.exists() and override.parent.exists():
             # for flattened subdeps, specifically backports/weakref.py
-            return (
-                vendor_dir / override.parent
-            ) / '{0}.{1}'.format(override.name, filename)
+            return (vendor_dir / override.parent) / "{0}.{1}".format(
+                override.name, filename
+            )
         license_path = Path(LIBRARY_DIRNAMES[libname]) / filename
         if license_path.as_posix() in LICENSE_RENAMES:
             return vendor_dir / LICENSE_RENAMES[license_path.as_posix()]
         return vendor_dir / LIBRARY_DIRNAMES[libname] / filename
     # fallback to libname.LICENSE (used for nondirs)
-    return vendor_dir / '{}.{}'.format(libname, filename)
+    return vendor_dir / "{}.{}".format(libname, filename)
 
 
 def extract_license_member(vendor_dir, tar, member, name):
@@ -653,7 +683,7 @@ def extract_license_member(vendor_dir, tar, member, name):
     dirname = list(mpath.parents)[-2].name  # -1 is .
     libname = libname_from_dir(dirname)
     dest = license_destination(vendor_dir, libname, mpath.name)
-    log('Extracting {} into {}'.format(name, dest))
+    log("Extracting {} into {}".format(name, dest))
     try:
         fileobj = tar.extractfile(member)
         dest.write_bytes(fileobj.read())
@@ -662,18 +692,20 @@ def extract_license_member(vendor_dir, tar, member, name):
 
 
 @invoke.task()
-def generate_patch(ctx, package_path, patch_description, base='HEAD'):
+def generate_patch(ctx, package_path, patch_description, base="HEAD"):
     pkg = Path(package_path)
-    if len(pkg.parts) != 2 or pkg.parts[0] not in ('vendor', 'patched'):
-        raise ValueError('example usage: generate-patch patched/piptools some-description')
+    if len(pkg.parts) != 2 or pkg.parts[0] not in ("vendor", "patched"):
+        raise ValueError(
+            "example usage: generate-patch patched/piptools some-description"
+        )
     if patch_description:
-        patch_fn = '{0}-{1}.patch'.format(pkg.parts[1], patch_description)
+        patch_fn = "{0}-{1}.patch".format(pkg.parts[1], patch_description)
     else:
-        patch_fn = '{0}.patch'.format(pkg.parts[1])
-    command = 'git diff {base} -p {root} > {out}'.format(
+        patch_fn = "{0}.patch".format(pkg.parts[1])
+    command = "git diff {base} -p {root} > {out}".format(
         base=base,
-        root=Path('pipenv').joinpath(pkg),
-        out=Path(__file__).parent.joinpath('patches', pkg.parts[0], patch_fn),
+        root=Path("pipenv").joinpath(pkg),
+        out=Path(__file__).parent.joinpath("patches", pkg.parts[0], patch_fn),
     )
     with ctx.cd(str(_get_git_root(ctx))):
         log(command)
@@ -717,14 +749,20 @@ def unpin_and_copy_requirements(ctx, requirement_file, name="requirements.txt"):
         target = Path(tempdir.name).joinpath("requirements.txt")
         contents = unpin_file(requirement_file.read_text())
         target.write_text(contents)
-        env = {"PIPENV_IGNORE_VIRTUALENVS": "1", "PIPENV_NOSPIN": "1", "PIPENV_PYTHON": "2.7"}
+        env = {
+            "PIPENV_IGNORE_VIRTUALENVS": "1",
+            "PIPENV_NOSPIN": "1",
+            "PIPENV_PYTHON": "2.7",
+        }
         with ctx.cd(tempdir.name):
             ctx.run("pipenv install -r {0}".format(target.as_posix()), env=env, hide=True)
             result = ctx.run("pipenv lock -r", env=env, hide=True).stdout.strip()
             ctx.run("pipenv --rm", env=env, hide=True)
             result = list(sorted([line.strip() for line in result.splitlines()[1:]]))
             new_requirements = requirement_file.parent.joinpath(name)
-            requirement_file.rename(requirement_file.parent.joinpath("{}.bak".format(name)))
+            requirement_file.rename(
+                requirement_file.parent.joinpath("{}.bak".format(name))
+            )
             new_requirements.write_text("\n".join(result))
     return result
 
@@ -743,7 +781,7 @@ def unpin_and_update_vendored(ctx, vendor=True, patched=False):
 def main(ctx, package=None):
     vendor_dir = _get_vendor_dir(ctx)
     patched_dir = _get_patched_dir(ctx)
-    log('Using vendor dir: %s' % vendor_dir)
+    log("Using vendor dir: %s" % vendor_dir)
     if package:
         vendor(ctx, vendor_dir, package=package)
         download_licenses(ctx, vendor_dir, package=package)
@@ -758,7 +796,7 @@ def main(ctx, package=None):
     # log("Vendoring passa...")
     # vendor_passa(ctx)
     # update_safety(ctx)
-    log('Revendoring complete')
+    log("Revendoring complete")
 
 
 @invoke.task
diff --git a/tasks/vendoring/safety/__main__.py b/tasks/vendoring/safety/__main__.py
new file mode 100644
index 00000000..6df76720
--- /dev/null
+++ b/tasks/vendoring/safety/__main__.py
@@ -0,0 +1,46 @@
+"""Allow safety to be executable through `python -m safety`."""
+from __future__ import absolute_import
+
+import sys
+import sysconfig
+import os
+
+LIBPATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "lib")
+
+
+def get_site_packages():
+    prefixes = {sys.prefix, sysconfig.get_config_var('prefix')}
+    try:
+        prefixes.add(sys.real_prefix)
+    except AttributeError:
+        pass
+    form = sysconfig.get_path('purelib', expand=False)
+    py_version_short = '{0[0]}.{0[1]}'.format(sys.version_info)
+    return {
+        form.format(base=prefix, py_version_short=py_version_short)
+        for prefix in prefixes
+    }
+
+
+def insert_before_site_packages(*paths):
+    site_packages = get_site_packages()
+    index = None
+    for i, path in enumerate(sys.path):
+        if path in site_packages:
+            index = i
+            break
+    if index is None:
+        sys.path += list(paths)
+    else:
+        sys.path = sys.path[:index] + list(paths) + sys.path[index:]
+
+
+if __name__ == "__main__":
+    import certifi
+    os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
+    insert_before_site_packages(LIBPATH)
+    yaml_lib = 'yaml{0}'.format(sys.version_info[0])
+    locals()[yaml_lib] = __import__(yaml_lib)
+    sys.modules['yaml'] = sys.modules[yaml_lib]
+    from safety.cli import cli
+    cli(prog_name="safety")
