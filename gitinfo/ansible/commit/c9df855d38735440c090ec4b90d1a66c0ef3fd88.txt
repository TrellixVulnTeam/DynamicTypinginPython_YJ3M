commit c9df855d38735440c090ec4b90d1a66c0ef3fd88
Author: James Cammarata <jimi@sngx.net>
Date:   Mon Apr 28 20:37:08 2014 -0500

    Adding an integration test for the ec2_elb module

diff --git a/test/integration/Makefile b/test/integration/Makefile
index 8d01ae2a4a..5e79f6e80c 100644
--- a/test/integration/Makefile
+++ b/test/integration/Makefile
@@ -48,7 +48,7 @@ $(CREDENTIALS_FILE):
 	@exit 1
 
 amazon: $(CREDENTIALS_FILE)
-	BOTO_CONFIG=/dev/null ansible-playbook amazon.yml -i $(INVENTORY) -e @$(VARS_FILE) $(CREDENTIALS_ARG) -e "resource_prefix=$(CLOUD_RESOURCE_PREFIX)" -v $(TEST_FLAGS) ; \
+	ANSIBLE_SSH_PIPELINING=no BOTO_CONFIG=/dev/null ansible-playbook amazon.yml -i $(INVENTORY) -e @$(VARS_FILE) $(CREDENTIALS_ARG) -e "resource_prefix=$(CLOUD_RESOURCE_PREFIX)" -v $(TEST_FLAGS) ; \
     RC=$$? ; \
     CLOUD_RESOURCE_PREFIX="$(CLOUD_RESOURCE_PREFIX)" make amazon_cleanup ; \
     exit $$RC;
diff --git a/test/integration/amazon.yml b/test/integration/amazon.yml
index 867b4dcf5d..73c4aacaf7 100644
--- a/test/integration/amazon.yml
+++ b/test/integration/amazon.yml
@@ -1,4 +1,4 @@
-- hosts: testhost
+- hosts: amazon
   gather_facts: true
   roles:
     - { role: test_ec2_key, tags: test_ec2_key }
@@ -9,6 +9,24 @@
     #- { role: test_ec2_facts, tags: test_ec2_facts }
     - { role: test_ec2_elb_lb, tags: test_ec2_elb_lb }
     #- { role: test_ec2_eip, tags: test_ec2_eip }
-    #- { role: test_ec2_elb, tags: test_ec2_elb }
     #- { role: test_ec2_ami, tags: test_ec2_ami }
     #- { role: test_ec2, tags: test_ec2 }
+
+# complex test for ec2_elb, split up over multiple plays
+# since there is a setup component as well as the test which
+# runs on a different set of hosts (ec2 instances)
+
+- hosts: amazon
+  roles:
+    - { role: ec2_provision_instances, tags: test_ec2_elb, count: 5 }
+
+- hosts: ec2
+  gather_facts: no
+  remote_user: ec2-user
+  sudo: true
+  roles:
+    - { role: ec2_elb_instance_setup, tags: test_ec2_elb }
+
+- hosts: amazon
+  roles:
+    - { role: test_ec2_elb, tags: test_ec2_elb }
diff --git a/test/integration/inventory b/test/integration/inventory
index d89301f866..6a64a919db 100644
--- a/test/integration/inventory
+++ b/test/integration/inventory
@@ -25,5 +25,5 @@ groups_tree_var=3000
 grandparent_var=2000
 overridden_in_parent=2000
 
-[amazon:children]
-local
+[amazon]
+localhost ansible_ssh_host=127.0.0.1 ansible_connection=local
diff --git a/test/integration/roles/ec2_elb_instance_setup/defaults/main.yml b/test/integration/roles/ec2_elb_instance_setup/defaults/main.yml
new file mode 100644
index 0000000000..04ebaf9f45
--- /dev/null
+++ b/test/integration/roles/ec2_elb_instance_setup/defaults/main.yml
@@ -0,0 +1,2 @@
+---
+# defaults for ec2_elb_setup
diff --git a/test/integration/roles/ec2_elb_instance_setup/files/index.html b/test/integration/roles/ec2_elb_instance_setup/files/index.html
new file mode 100644
index 0000000000..937fbdd7c7
--- /dev/null
+++ b/test/integration/roles/ec2_elb_instance_setup/files/index.html
@@ -0,0 +1,6 @@
+<html>
+<head>Hi!</head>
+<body>
+Hello!
+</body>
+</html>
diff --git a/test/integration/roles/ec2_elb_instance_setup/meta/main.yml b/test/integration/roles/ec2_elb_instance_setup/meta/main.yml
new file mode 100644
index 0000000000..32cf5dda7e
--- /dev/null
+++ b/test/integration/roles/ec2_elb_instance_setup/meta/main.yml
@@ -0,0 +1 @@
+dependencies: []
diff --git a/test/integration/roles/ec2_elb_instance_setup/tasks/main.yml b/test/integration/roles/ec2_elb_instance_setup/tasks/main.yml
new file mode 100644
index 0000000000..341392b00c
--- /dev/null
+++ b/test/integration/roles/ec2_elb_instance_setup/tasks/main.yml
@@ -0,0 +1,14 @@
+---
+# tasks file for ec2_elb_setup
+
+# ============================================================
+# install apache on the ec2 instances
+
+- name: install apache on new ec2 instances
+  yum: name=httpd
+
+- name: start and enable apache
+  service: name=httpd state=started enabled=yes
+
+- name: create an index.html landing page
+  copy: dest=/var/www/html/index.html src=index.html owner=root group=root mode=0644
diff --git a/test/integration/roles/ec2_provision_instances/defaults/main.yml b/test/integration/roles/ec2_provision_instances/defaults/main.yml
new file mode 100644
index 0000000000..b58153ecde
--- /dev/null
+++ b/test/integration/roles/ec2_provision_instances/defaults/main.yml
@@ -0,0 +1,3 @@
+---
+# defaults file for ec2_provision_isntances
+count: 1
diff --git a/test/integration/roles/ec2_provision_instances/meta/main.yml b/test/integration/roles/ec2_provision_instances/meta/main.yml
new file mode 100644
index 0000000000..1f64f1169a
--- /dev/null
+++ b/test/integration/roles/ec2_provision_instances/meta/main.yml
@@ -0,0 +1,3 @@
+dependencies:
+  - prepare_tests
+  - setup_ec2
diff --git a/test/integration/roles/ec2_provision_instances/tasks/main.yml b/test/integration/roles/ec2_provision_instances/tasks/main.yml
new file mode 100644
index 0000000000..2f3c283e85
--- /dev/null
+++ b/test/integration/roles/ec2_provision_instances/tasks/main.yml
@@ -0,0 +1,49 @@
+---
+# tasks file for ec2_provision_instances
+
+# ============================================================
+# create a keypair using the ssh key
+
+- name: create the keypair for ec2
+  ec2_key:
+    name: "{{ resource_prefix }}"
+    region: "{{ ec2_region }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    key_material: "{{ key_material }}"
+    wait: yes
+    state: present
+
+# ============================================================
+# create some instances for testing, and add them to a new
+# group ("ec2") for use later
+
+- name: create ec2 instances for testing
+  ec2:
+    instance_type: t1.micro
+    image: ami-fb8e9292
+    group: default
+    region: "{{ ec2_region }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    key_name: "{{ resource_prefix }}"
+    wait: yes
+    instance_tags:
+        Name: "{{ resource_prefix }}"
+    exact_count: "{{ count }}"
+    count_tag:
+        Name: "{{ resource_prefix }}"
+  register: ec2_provision_result
+
+- name: add ec2 instances to a new group
+  add_host:
+    hostname: "{{ item.public_ip }}"
+    groups: "ec2"
+    ansible_ssh_private_key_file: "{{ sshkey }}"
+  with_items: ec2_provision_result.instances
+
+- name: wait for the instances to become available
+  wait_for:
+    port: 22
+    host: "{{ item.public_ip }}"
+  with_items: ec2_provision_result.instances
diff --git a/test/integration/roles/test_ec2_elb/meta/main.yml b/test/integration/roles/test_ec2_elb/meta/main.yml
index 1f64f1169a..32cf5dda7e 100644
--- a/test/integration/roles/test_ec2_elb/meta/main.yml
+++ b/test/integration/roles/test_ec2_elb/meta/main.yml
@@ -1,3 +1 @@
-dependencies:
-  - prepare_tests
-  - setup_ec2
+dependencies: []
diff --git a/test/integration/roles/test_ec2_elb/tasks/main.yml b/test/integration/roles/test_ec2_elb/tasks/main.yml
index 0e68d3698e..30ada1955b 100644
--- a/test/integration/roles/test_ec2_elb/tasks/main.yml
+++ b/test/integration/roles/test_ec2_elb/tasks/main.yml
@@ -1,2 +1,186 @@
 ---
 # tasks file for test_ec2_elb
+
+# ============================================================
+# create an ELB for testing
+
+- name: create the test load balancer
+  ec2_elb_lb:
+    name: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    state: present
+    zones:
+      - "{{ ec2_region }}b"
+      - "{{ ec2_region }}c"
+    listeners:
+      - protocol: http
+        load_balancer_port: 80
+        instance_port: 80
+    health_check:
+        ping_protocol: http
+        ping_port: 80
+        ping_path: "/index.html"
+        response_timeout: 5
+        interval: 10
+        unhealthy_threshold: 3
+        healthy_threshold: 2
+  register: result
+
+- name: assert the test load balancer was created correctly
+  assert:
+    that:
+      - 'result.changed'
+      - '"failed" not in result'
+      - 'result.elb.status == "created"'
+      - '"{{ ec2_region }}b" in result.elb.zones'
+      - '"{{ ec2_region }}c" in result.elb.zones'
+      - 'result.elb.health_check.healthy_threshold == 2'
+      - 'result.elb.health_check.interval == 10'
+      - 'result.elb.health_check.target == "HTTP:80/index.html"'
+      - 'result.elb.health_check.timeout == 5'
+      - 'result.elb.health_check.unhealthy_threshold == 3'
+      - '[80, 80, "HTTP", "HTTP"] in result.elb.listeners'
+
+
+# ============================================================
+# add one of the instances to the LB
+
+- name: add first instance to the load balancer
+  ec2_elb:
+    ec2_elbs: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    instance_id: "{{ ec2_provision_result.instance_ids[0] }}"
+    state: present
+    wait_timeout: 300
+  register: result
+
+- name: assert the first instance was added ok
+  assert:
+    that:
+      - 'result.changed == True'
+      - '"{{resource_prefix}}" in result.ansible_facts.ec2_elbs'
+
+# ============================================================
+# add all other instances to the LB
+
+- name: add other instances to the load balancer
+  ec2_elb:
+    ec2_elbs: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    instance_id: "{{ item }}"
+    state: present
+    wait_timeout: 300
+  with_items: "ec2_provision_result.instance_ids[1:]"
+  register: result
+
+- name: assert the other instances were added ok
+  assert:
+    that:
+      - 'item.changed == True'
+      - '"{{resource_prefix}}" in item.ansible_facts.ec2_elbs'
+  with_items: result.results
+
+# ============================================================
+# shutdown http first instance so it goes out of service
+
+#- name: terminate first instance
+#  ec2:
+#    ec2_access_key: "{{ ec2_access_key }}"
+#    ec2_secret_key: "{{ ec2_secret_key }}"
+#    region: "{{ ec2_region }}"
+#    state: 'absent'
+#    instance_ids: "{{ ec2_provision_result.instance_ids[0] }}"
+#  register: result
+#
+#- name: assert the instance was terminated
+#  assert:
+#    that:
+#      - 'result.changed == True'
+#      - 'result.instance_ids[0] == ec2_provision_result.instance_ids[0]'
+#
+#- name: wait for the instance to die
+#  wait_for: port=80 host="{{ec2_provision_result.instances[0].public_ip}}" state=absent
+
+- name: "shutdown the apache service on the first instance ({{ec2_provision_result.instances[0].public_ip}})"
+  service: name=httpd state=stopped
+  remote_user: "ec2-user"
+  sudo: yes
+  sudo_user: root
+  delegate_to: "{{ec2_provision_result.instances[0].public_ip}}"
+
+- name: assert that the httpd service was stopped
+  assert:
+    that:
+      - 'result.changed == True'
+
+- name: pause long enough for the instance to go out of service
+  pause: seconds=60
+
+# ============================================================
+# remove the out of service instance
+
+- name: remove the out of service instance
+  ec2_elb:
+    ec2_elbs: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    instance_id: "{{ ec2_provision_result.instance_ids[0] }}"
+    state: absent
+    wait_timeout: 300
+  register: result
+
+- name: assert that the out of service instance was removed
+  assert:
+    that:
+      - 'result.changed == True'
+      - '"{{resource_prefix}}" in result.ansible_facts.ec2_elbs'
+
+# ============================================================
+# remove another instance that is still in service
+
+- name: remove the second instance
+  ec2_elb:
+    ec2_elbs: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    instance_id: "{{ ec2_provision_result.instance_ids[1] }}"
+    state: absent
+    wait_timeout: 300
+  register: result
+
+- name: assert that the second instance was removed
+  assert:
+    that:
+      - 'result.changed == True'
+      - '"{{resource_prefix}}" in result.ansible_facts.ec2_elbs'
+
+# ============================================================
+# remove all other instances
+
+- name: remove the rest of the instances
+  ec2_elb:
+    ec2_elbs: "{{ resource_prefix }}"
+    ec2_access_key: "{{ ec2_access_key }}"
+    ec2_secret_key: "{{ ec2_secret_key }}"
+    region: "{{ ec2_region }}"
+    instance_id: "{{ item }}"
+    state: absent
+    wait_timeout: 300
+  with_items: "ec2_provision_result.instance_ids[2:]"
+  register: result
+
+- name: assert the other instances were removed
+  assert:
+    that:
+      - 'item.changed == True'
+      - '"{{resource_prefix}}" in item.ansible_facts.ec2_elbs'
+  with_items: result.results
+
diff --git a/test/integration/roles/test_ec2_elb/vars/main.yml b/test/integration/roles/test_ec2_elb/vars/main.yml
deleted file mode 100644
index 415390f64a..0000000000
--- a/test/integration/roles/test_ec2_elb/vars/main.yml
+++ /dev/null
@@ -1,2 +0,0 @@
----
-# vars file for test_ec2_elb
