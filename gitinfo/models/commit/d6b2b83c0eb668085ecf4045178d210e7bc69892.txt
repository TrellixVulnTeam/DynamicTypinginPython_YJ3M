commit d6b2b83c0eb668085ecf4045178d210e7bc69892
Author: Goldie Gadde <ggadde@google.com>
Date:   Tue Feb 5 07:54:04 2019 -0800

    tf_upgrade_v2 on resnet and utils folders. (#6154)
    
    * Add resnet56 short tests. (#6101)
    
    * Add resnet56 short tests.
    - created base benchmark module
    - renamed accuracy test class to contain the word Accuracy
    which will result in a need to update all the jobs
    and a loss of history but is worth it.
    - short tests are mostly copied from shining with oss refactor
    
    * Address feedback.
    
    * Move flag_methods to init
    - Address setting default flags repeatedly.
    
    * Rename accuracy tests.
    
    * Lint errors resolved.
    
    * fix model_dir set to flags.data_dir.
    
    * fixed not fulling pulling out flag_methods.
    
    * Use core mirrored strategy in official models (#6126)
    
    * Imagenet short tests (#6132)
    
    * Add short imagenet tests (taken from seemuch)
    - also rename to match go forward naming
    
    * fix method name
    
    * Update doc strings.
    
    * Fixe gpu number.
    
    * points default data_dir to child folder. (#6131)
    
    Failed test is python2  and was a kokoro failure
    
    * Imagenet short tests (#6136)
    
    * Add short imagenet tests (taken from seemuch)
    - also rename to match go forward naming
    
    * fix method name
    
    * Update doc strings.
    
    * Fixe gpu number.
    
    * Add fill_objects
    
    * fixed calling wrong class in super.
    
    * fix lint issue.
    
    * Flag (#6121)
    
    * Fix the turn_off_ds flag problem
    
    * add param names to all args
    
    * Export benchmark stats using tf.test.Benchmark.report_benchmark() (#6103)
    
    * Export benchmark stats using tf.test.Benchmark.report_benchmark()
    
    * Fix python style using pyformat
    
    * Typos. (#6120)
    
    * log verbosity=2 logs every epoch no progress bars (#6142)
    
    * tf_upgrade_v2 on resnet and utils folder.
    
    * tf_upgrade_v2 on resnet and utils folder.

diff --git a/official/resnet/cifar10_download_and_extract.py b/official/resnet/cifar10_download_and_extract.py
index ee587efc..a44d042e 100644
--- a/official/resnet/cifar10_download_and_extract.py
+++ b/official/resnet/cifar10_download_and_extract.py
@@ -60,4 +60,4 @@ def main(_):
 
 if __name__ == '__main__':
   FLAGS, unparsed = parser.parse_known_args()
-  tf.app.run(argv=[sys.argv[0]] + unparsed)
+  tf.compat.v1.app.run(argv=[sys.argv[0]] + unparsed)
diff --git a/official/resnet/cifar10_main.py b/official/resnet/cifar10_main.py
index e5e5008f..053816aa 100644
--- a/official/resnet/cifar10_main.py
+++ b/official/resnet/cifar10_main.py
@@ -52,7 +52,7 @@ DATASET_NAME = 'CIFAR-10'
 ###############################################################################
 def get_filenames(is_training, data_dir):
   """Returns a list of filenames."""
-  assert tf.gfile.Exists(data_dir), (
+  assert tf.io.gfile.exists(data_dir), (
       'Run cifar10_download_and_extract.py first to download and extract the '
       'CIFAR-10 data.')
 
@@ -68,7 +68,7 @@ def get_filenames(is_training, data_dir):
 def parse_record(raw_record, is_training, dtype):
   """Parse CIFAR-10 image and label from a raw record."""
   # Convert bytes to a vector of uint8 that is record_bytes long.
-  record_vector = tf.decode_raw(raw_record, tf.uint8)
+  record_vector = tf.io.decode_raw(raw_record, tf.uint8)
 
   # The first byte represents the label, which we convert from uint8 to int32
   # and then to one-hot.
@@ -81,7 +81,7 @@ def parse_record(raw_record, is_training, dtype):
 
   # Convert from [depth, height, width] to [height, width, depth], and cast as
   # float32.
-  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)
+  image = tf.cast(tf.transpose(a=depth_major, perm=[1, 2, 0]), tf.float32)
 
   image = preprocess_image(image, is_training)
   image = tf.cast(image, dtype)
@@ -97,7 +97,7 @@ def preprocess_image(image, is_training):
         image, HEIGHT + 8, WIDTH + 8)
 
     # Randomly crop a [HEIGHT, WIDTH] section of the image.
-    image = tf.random_crop(image, [HEIGHT, WIDTH, NUM_CHANNELS])
+    image = tf.image.random_crop(image, [HEIGHT, WIDTH, NUM_CHANNELS])
 
     # Randomly flip the image horizontally.
     image = tf.image.random_flip_left_right(image)
@@ -253,8 +253,9 @@ def run_cifar(flags_obj):
     Dictionary of results. Including final accuracy.
   """
   if flags_obj.image_bytes_as_serving_input:
-    tf.logging.fatal('--image_bytes_as_serving_input cannot be set to True '
-                     'for CIFAR. This flag is only applicable to ImageNet.')
+    tf.compat.v1.logging.fatal(
+        '--image_bytes_as_serving_input cannot be set to True for CIFAR. '
+        'This flag is only applicable to ImageNet.')
     return
 
   input_function = (flags_obj.use_synthetic_data and
@@ -273,6 +274,6 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
   define_cifar_flags()
   absl_app.run(main)
diff --git a/official/resnet/cifar10_test.py b/official/resnet/cifar10_test.py
index a736e10d..e4f514ce 100644
--- a/official/resnet/cifar10_test.py
+++ b/official/resnet/cifar10_test.py
@@ -25,7 +25,7 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 from official.resnet import cifar10_main
 from official.utils.testing import integration
 
-tf.logging.set_verbosity(tf.logging.ERROR)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
 
 _BATCH_SIZE = 128
 _HEIGHT = 32
@@ -44,7 +44,7 @@ class BaseTest(tf.test.TestCase):
 
   def tearDown(self):
     super(BaseTest, self).tearDown()
-    tf.gfile.DeleteRecursively(self.get_temp_dir())
+    tf.io.gfile.rmtree(self.get_temp_dir())
 
   def test_dataset_input_fn(self):
     fake_data = bytearray()
@@ -62,7 +62,8 @@ class BaseTest(tf.test.TestCase):
         filename, cifar10_main._RECORD_BYTES)  # pylint: disable=protected-access
     fake_dataset = fake_dataset.map(
         lambda val: cifar10_main.parse_record(val, False, tf.float32))
-    image, label = fake_dataset.make_one_shot_iterator().get_next()
+    image, label = tf.compat.v1.data.make_one_shot_iterator(
+        fake_dataset).get_next()
 
     self.assertAllEqual(label.shape, ())
     self.assertAllEqual(image.shape, (_HEIGHT, _WIDTH, _NUM_CHANNELS))
@@ -79,7 +80,7 @@ class BaseTest(tf.test.TestCase):
   def cifar10_model_fn_helper(self, mode, resnet_version, dtype):
     input_fn = cifar10_main.get_synth_input_fn(dtype)
     dataset = input_fn(True, '', _BATCH_SIZE)
-    iterator = dataset.make_initializable_iterator()
+    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
     features, labels = iterator.get_next()
     spec = cifar10_main.cifar10_model_fn(
         features, labels, mode, {
@@ -142,7 +143,7 @@ class BaseTest(tf.test.TestCase):
     model = cifar10_main.Cifar10Model(32, data_format='channels_last',
                                       num_classes=num_classes,
                                       resnet_version=resnet_version)
-    fake_input = tf.random_uniform([batch_size, _HEIGHT, _WIDTH, _NUM_CHANNELS])
+    fake_input = tf.random.uniform([batch_size, _HEIGHT, _WIDTH, _NUM_CHANNELS])
     output = model(fake_input, training=True)
 
     self.assertAllEqual(output.shape, (batch_size, num_classes))
diff --git a/official/resnet/estimator_cifar_benchmark.py b/official/resnet/estimator_cifar_benchmark.py
index 46d46891..bb296874 100644
--- a/official/resnet/estimator_cifar_benchmark.py
+++ b/official/resnet/estimator_cifar_benchmark.py
@@ -144,7 +144,7 @@ class EstimatorCifar10BenchmarkTests(tf.test.Benchmark):
     return os.path.join(self.output_dir, folder_name)
 
   def _setup(self):
-    tf.logging.set_verbosity(tf.logging.DEBUG)
+    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)
     if EstimatorCifar10BenchmarkTests.local_flags is None:
       cifar_main.define_cifar_flags()
       # Loads flags to get defaults to then override.
diff --git a/official/resnet/imagenet_main.py b/official/resnet/imagenet_main.py
index 01e7f0c0..3392e166 100644
--- a/official/resnet/imagenet_main.py
+++ b/official/resnet/imagenet_main.py
@@ -95,14 +95,14 @@ def _parse_example_proto(example_serialized):
   """
   # Dense features in Example proto.
   feature_map = {
-      'image/encoded': tf.FixedLenFeature([], dtype=tf.string,
-                                          default_value=''),
-      'image/class/label': tf.FixedLenFeature([], dtype=tf.int64,
-                                              default_value=-1),
-      'image/class/text': tf.FixedLenFeature([], dtype=tf.string,
+      'image/encoded': tf.io.FixedLenFeature([], dtype=tf.string,
                                              default_value=''),
+      'image/class/label': tf.io.FixedLenFeature([], dtype=tf.int64,
+                                                 default_value=-1),
+      'image/class/text': tf.io.FixedLenFeature([], dtype=tf.string,
+                                                default_value=''),
   }
-  sparse_float32 = tf.VarLenFeature(dtype=tf.float32)
+  sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)
   # Sparse features in Example proto.
   feature_map.update(
       {k: sparse_float32 for k in ['image/object/bbox/xmin',
@@ -110,7 +110,8 @@ def _parse_example_proto(example_serialized):
                                    'image/object/bbox/xmax',
                                    'image/object/bbox/ymax']})
 
-  features = tf.parse_single_example(example_serialized, feature_map)
+  features = tf.io.parse_single_example(serialized=example_serialized,
+                                        features=feature_map)
   label = tf.cast(features['image/class/label'], dtype=tf.int32)
 
   xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)
@@ -124,7 +125,7 @@ def _parse_example_proto(example_serialized):
   # Force the variable number of bounding boxes into the shape
   # [1, num_boxes, coords].
   bbox = tf.expand_dims(bbox, 0)
-  bbox = tf.transpose(bbox, [0, 2, 1])
+  bbox = tf.transpose(a=bbox, perm=[0, 2, 1])
 
   return features['image/encoded'], label, bbox
 
@@ -188,7 +189,7 @@ def input_fn(is_training, data_dir, batch_size, num_epochs=1,
   # This number is low enough to not cause too much contention on small systems
   # but high enough to provide the benefits of parallelization. You may want
   # to increase this number if you have a large number of CPU cores.
-  dataset = dataset.apply(tf.contrib.data.parallel_interleave(
+  dataset = dataset.apply(tf.data.experimental.parallel_interleave(
       tf.data.TFRecordDataset, cycle_length=10))
 
   return resnet_run_loop.process_record_dataset(
@@ -352,6 +353,6 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
   define_imagenet_flags()
   absl_app.run(main)
diff --git a/official/resnet/imagenet_preprocessing.py b/official/resnet/imagenet_preprocessing.py
index 01bcea44..a8a7ad5f 100644
--- a/official/resnet/imagenet_preprocessing.py
+++ b/official/resnet/imagenet_preprocessing.py
@@ -108,7 +108,7 @@ def _central_crop(image, crop_height, crop_width):
   Returns:
     3-D tensor with cropped image.
   """
-  shape = tf.shape(image)
+  shape = tf.shape(input=image)
   height, width = shape[0], shape[1]
 
   amount_to_be_cropped_h = (height - crop_height)
@@ -195,7 +195,7 @@ def _aspect_preserving_resize(image, resize_min):
   Returns:
     resized_image: A 3-D tensor containing the resized image.
   """
-  shape = tf.shape(image)
+  shape = tf.shape(input=image)
   height, width = shape[0], shape[1]
 
   new_height, new_width = _smallest_size_at_least(height, width, resize_min)
@@ -218,7 +218,7 @@ def _resize_image(image, height, width):
     resized_image: A 3-D tensor containing the resized image. The first two
       dimensions have the shape [height, width].
   """
-  return tf.image.resize_images(
+  return tf.image.resize(
       image, [height, width], method=tf.image.ResizeMethod.BILINEAR,
       align_corners=False)
 
diff --git a/official/resnet/imagenet_test.py b/official/resnet/imagenet_test.py
index 1eb2a992..f845e9c1 100644
--- a/official/resnet/imagenet_test.py
+++ b/official/resnet/imagenet_test.py
@@ -24,7 +24,7 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 from official.resnet import imagenet_main
 from official.utils.testing import integration
 
-tf.logging.set_verbosity(tf.logging.ERROR)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
 
 _BATCH_SIZE = 32
 _LABEL_CLASSES = 1001
@@ -39,7 +39,7 @@ class BaseTest(tf.test.TestCase):
 
   def tearDown(self):
     super(BaseTest, self).tearDown()
-    tf.gfile.DeleteRecursively(self.get_temp_dir())
+    tf.io.gfile.rmtree(self.get_temp_dir())
 
   def _tensor_shapes_helper(self, resnet_size, resnet_version, dtype, with_gpu):
     """Checks the tensor shapes after each phase of the ResNet model."""
@@ -62,7 +62,7 @@ class BaseTest(tf.test.TestCase):
           resnet_version=resnet_version,
           dtype=dtype
       )
-      inputs = tf.random_uniform([1, 224, 224, 3])
+      inputs = tf.random.uniform([1, 224, 224, 3])
       output = model(inputs, training=True)
 
       initial_conv = graph.get_tensor_by_name('resnet_model/initial_conv:0')
@@ -189,11 +189,11 @@ class BaseTest(tf.test.TestCase):
 
   def resnet_model_fn_helper(self, mode, resnet_version, dtype):
     """Tests that the EstimatorSpec is given the appropriate arguments."""
-    tf.train.create_global_step()
+    tf.compat.v1.train.create_global_step()
 
     input_fn = imagenet_main.get_synth_input_fn(dtype)
     dataset = input_fn(True, '', _BATCH_SIZE)
-    iterator = dataset.make_initializable_iterator()
+    iterator = tf.compat.v1.data.make_initializable_iterator(dataset)
     features, labels = iterator.get_next()
     spec = imagenet_main.imagenet_model_fn(
         features, labels, mode, {
@@ -257,7 +257,7 @@ class BaseTest(tf.test.TestCase):
         50, data_format='channels_last', num_classes=num_classes,
         resnet_version=resnet_version)
 
-    fake_input = tf.random_uniform([batch_size, 224, 224, 3])
+    fake_input = tf.random.uniform([batch_size, 224, 224, 3])
     output = model(fake_input, training=True)
 
     self.assertAllEqual(output.shape, (batch_size, num_classes))
diff --git a/official/resnet/keras/keras_benchmark.py b/official/resnet/keras/keras_benchmark.py
index 66a70ed2..6b2d26e1 100644
--- a/official/resnet/keras/keras_benchmark.py
+++ b/official/resnet/keras/keras_benchmark.py
@@ -43,7 +43,7 @@ class KerasBenchmark(tf.test.Benchmark):
 
   def _setup(self):
     """Sets up and resets flags before each test."""
-    tf.logging.set_verbosity(tf.logging.DEBUG)
+    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)
     if KerasBenchmark.local_flags is None:
       for flag_method in self.flag_methods:
         flag_method()
diff --git a/official/resnet/keras/keras_cifar_main.py b/official/resnet/keras/keras_cifar_main.py
index 48c28312..427da3f8 100644
--- a/official/resnet/keras/keras_cifar_main.py
+++ b/official/resnet/keras/keras_cifar_main.py
@@ -81,7 +81,7 @@ def parse_record_keras(raw_record, is_training, dtype):
     Tuple with processed image tensor and one-hot-encoded label tensor.
   """
   image, label = cifar_main.parse_record(raw_record, is_training, dtype)
-  label = tf.sparse_to_dense(label, (cifar_main.NUM_CLASSES,), 1)
+  label = tf.compat.v1.sparse_to_dense(label, (cifar_main.NUM_CLASSES,), 1)
   return image, label
 
 
@@ -98,7 +98,7 @@ def run(flags_obj):
     Dictionary of training and eval stats.
   """
   if flags_obj.enable_eager:
-    tf.enable_eager_execution()
+    tf.compat.v1.enable_eager_execution()
 
   dtype = flags_core.get_tf_dtype(flags_obj)
   if dtype == 'fp16':
@@ -194,7 +194,7 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
   cifar_main.define_cifar_flags()
   keras_common.define_keras_flags()
   absl_app.run(main)
diff --git a/official/resnet/keras/keras_common.py b/official/resnet/keras/keras_common.py
index ac9dad78..05c6c46f 100644
--- a/official/resnet/keras/keras_common.py
+++ b/official/resnet/keras/keras_common.py
@@ -80,9 +80,10 @@ class TimeHistory(tf.keras.callbacks.Callback):
       if batch != 0:
         self.record_batch = True
         self.timestamp_log.append(BatchTimestamp(batch, timestamp))
-        tf.logging.info("BenchmarkMetric: {'num_batches':%d, 'time_taken': %f,"
-                        "'images_per_second': %f}" %
-                        (batch, elapsed_time, examples_per_second))
+        tf.compat.v1.logging.info(
+            "BenchmarkMetric: {'num_batches':%d, 'time_taken': %f,"
+            "'images_per_second': %f}" %
+            (batch, elapsed_time, examples_per_second))
 
 
 class LearningRateBatchScheduler(tf.keras.callbacks.Callback):
@@ -120,8 +121,9 @@ class LearningRateBatchScheduler(tf.keras.callbacks.Callback):
     if lr != self.prev_lr:
       self.model.optimizer.learning_rate = lr  # lr should be a float here
       self.prev_lr = lr
-      tf.logging.debug('Epoch %05d Batch %05d: LearningRateBatchScheduler '
-                       'change learning rate to %s.', self.epochs, batch, lr)
+      tf.compat.v1.logging.debug(
+          'Epoch %05d Batch %05d: LearningRateBatchScheduler '
+          'change learning rate to %s.', self.epochs, batch, lr)
 
 
 def get_optimizer():
@@ -226,22 +228,20 @@ def get_synth_input_fn(height, width, num_channels, num_classes,
   def input_fn(is_training, data_dir, batch_size, *args, **kwargs):
     """Returns dataset filled with random data."""
     # Synthetic input should be within [0, 255].
-    inputs = tf.truncated_normal(
-        [height, width, num_channels],
-        dtype=dtype,
-        mean=127,
-        stddev=60,
-        name='synthetic_inputs')
-
-    labels = tf.random_uniform(
-        [1],
-        minval=0,
-        maxval=num_classes - 1,
-        dtype=tf.int32,
-        name='synthetic_labels')
+    inputs = tf.random.truncated_normal([height, width, num_channels],
+                                        dtype=dtype,
+                                        mean=127,
+                                        stddev=60,
+                                        name='synthetic_inputs')
+
+    labels = tf.random.uniform([1],
+                               minval=0,
+                               maxval=num_classes - 1,
+                               dtype=tf.int32,
+                               name='synthetic_labels')
     data = tf.data.Dataset.from_tensors((inputs, labels)).repeat()
     data = data.batch(batch_size)
-    data = data.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)
+    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
     return data
 
   return input_fn
diff --git a/official/resnet/keras/keras_common_test.py b/official/resnet/keras/keras_common_test.py
index 9d84d7b8..2444a72a 100644
--- a/official/resnet/keras/keras_common_test.py
+++ b/official/resnet/keras/keras_common_test.py
@@ -22,7 +22,7 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 
 from official.resnet.keras import keras_common
 
-tf.logging.set_verbosity(tf.logging.ERROR)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
 
 
 class KerasCommonTests(tf.test.TestCase):
diff --git a/official/resnet/keras/keras_imagenet_main.py b/official/resnet/keras/keras_imagenet_main.py
index 513cd209..293ddab9 100644
--- a/official/resnet/keras/keras_imagenet_main.py
+++ b/official/resnet/keras/keras_imagenet_main.py
@@ -88,7 +88,7 @@ def run(flags_obj):
     ValueError: If fp16 is passed as it is not currently supported.
   """
   if flags_obj.enable_eager:
-    tf.enable_eager_execution()
+    tf.compat.v1.enable_eager_execution()
 
   dtype = flags_core.get_tf_dtype(flags_obj)
   if dtype == 'fp16':
@@ -187,7 +187,7 @@ def main(_):
 
 
 if __name__ == '__main__':
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
   imagenet_main.define_imagenet_flags()
   keras_common.define_keras_flags()
   absl_app.run(main)
diff --git a/official/resnet/layer_test.py b/official/resnet/layer_test.py
index bbef600e..26c04294 100644
--- a/official/resnet/layer_test.py
+++ b/official/resnet/layer_test.py
@@ -72,10 +72,10 @@ class BaseTest(reference_data.BaseTest):
 
     g = tf.Graph()
     with g.as_default():
-      tf.set_random_seed(self.name_to_seed(name))
-      input_tensor = tf.get_variable(
+      tf.compat.v1.set_random_seed(self.name_to_seed(name))
+      input_tensor = tf.compat.v1.get_variable(
           "input_tensor", dtype=tf.float32,
-          initializer=tf.random_uniform((32, 16, 16, 3), maxval=1)
+          initializer=tf.random.uniform((32, 16, 16, 3), maxval=1)
       )
       layer = resnet_model.batch_norm(
           inputs=input_tensor, data_format=DATA_FORMAT, training=True)
@@ -137,7 +137,7 @@ class BaseTest(reference_data.BaseTest):
 
     g = tf.Graph()
     with g.as_default():
-      tf.set_random_seed(self.name_to_seed(name))
+      tf.compat.v1.set_random_seed(self.name_to_seed(name))
       strides = 1
       channels_out = channels
       projection_shortcut = None
@@ -151,9 +151,9 @@ class BaseTest(reference_data.BaseTest):
       if bottleneck:
         filters = channels_out // 4
 
-      input_tensor = tf.get_variable(
+      input_tensor = tf.compat.v1.get_variable(
           "input_tensor", dtype=tf.float32,
-          initializer=tf.random_uniform((batch_size, width, width, channels),
+          initializer=tf.random.uniform((batch_size, width, width, channels),
                                         maxval=1)
       )
 
diff --git a/official/resnet/resnet_model.py b/official/resnet/resnet_model.py
index e17aab5f..02d654cd 100644
--- a/official/resnet/resnet_model.py
+++ b/official/resnet/resnet_model.py
@@ -48,7 +48,7 @@ def batch_norm(inputs, training, data_format):
   """Performs a batch normalization using a standard set of parameters."""
   # We set fused=True for a significant performance boost. See
   # https://www.tensorflow.org/performance/performance_guide#common_fused_ops
-  return tf.layers.batch_normalization(
+  return tf.compat.v1.layers.batch_normalization(
       inputs=inputs, axis=1 if data_format == 'channels_first' else 3,
       momentum=_BATCH_NORM_DECAY, epsilon=_BATCH_NORM_EPSILON, center=True,
       scale=True, training=training, fused=True)
@@ -73,11 +73,13 @@ def fixed_padding(inputs, kernel_size, data_format):
   pad_end = pad_total - pad_beg
 
   if data_format == 'channels_first':
-    padded_inputs = tf.pad(inputs, [[0, 0], [0, 0],
-                                    [pad_beg, pad_end], [pad_beg, pad_end]])
+    padded_inputs = tf.pad(tensor=inputs,
+                           paddings=[[0, 0], [0, 0], [pad_beg, pad_end],
+                                     [pad_beg, pad_end]])
   else:
-    padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
-                                    [pad_beg, pad_end], [0, 0]])
+    padded_inputs = tf.pad(tensor=inputs,
+                           paddings=[[0, 0], [pad_beg, pad_end],
+                                     [pad_beg, pad_end], [0, 0]])
   return padded_inputs
 
 
@@ -88,10 +90,10 @@ def conv2d_fixed_padding(inputs, filters, kernel_size, strides, data_format):
   if strides > 1:
     inputs = fixed_padding(inputs, kernel_size, data_format)
 
-  return tf.layers.conv2d(
+  return tf.compat.v1.layers.conv2d(
       inputs=inputs, filters=filters, kernel_size=kernel_size, strides=strides,
       padding=('SAME' if strides == 1 else 'VALID'), use_bias=False,
-      kernel_initializer=tf.variance_scaling_initializer(),
+      kernel_initializer=tf.compat.v1.variance_scaling_initializer(),
       data_format=data_format)
 
 
@@ -475,8 +477,8 @@ class Model(object):
       A variable scope for the model.
     """
 
-    return tf.variable_scope('resnet_model',
-                             custom_getter=self._custom_dtype_getter)
+    return tf.compat.v1.variable_scope('resnet_model',
+                                       custom_getter=self._custom_dtype_getter)
 
   def __call__(self, inputs, training):
     """Add operations to classify a batch of input images.
@@ -495,7 +497,7 @@ class Model(object):
         # Convert the inputs from channels_last (NHWC) to channels_first (NCHW).
         # This provides a large performance boost on GPU. See
         # https://www.tensorflow.org/performance/performance_guide#data_formats
-        inputs = tf.transpose(inputs, [0, 3, 1, 2])
+        inputs = tf.transpose(a=inputs, perm=[0, 3, 1, 2])
 
       inputs = conv2d_fixed_padding(
           inputs=inputs, filters=self.num_filters, kernel_size=self.kernel_size,
@@ -511,7 +513,7 @@ class Model(object):
         inputs = tf.nn.relu(inputs)
 
       if self.first_pool_size:
-        inputs = tf.layers.max_pooling2d(
+        inputs = tf.compat.v1.layers.max_pooling2d(
             inputs=inputs, pool_size=self.first_pool_size,
             strides=self.first_pool_stride, padding='SAME',
             data_format=self.data_format)
@@ -537,10 +539,10 @@ class Model(object):
       # but that is the same as doing a reduce_mean. We do a reduce_mean
       # here because it performs better than AveragePooling2D.
       axes = [2, 3] if self.data_format == 'channels_first' else [1, 2]
-      inputs = tf.reduce_mean(inputs, axes, keepdims=True)
+      inputs = tf.reduce_mean(input_tensor=inputs, axis=axes, keepdims=True)
       inputs = tf.identity(inputs, 'final_reduce_mean')
 
       inputs = tf.squeeze(inputs, axes)
-      inputs = tf.layers.dense(inputs=inputs, units=self.num_classes)
+      inputs = tf.compat.v1.layers.dense(inputs=inputs, units=self.num_classes)
       inputs = tf.identity(inputs, 'final_dense')
       return inputs
diff --git a/official/resnet/resnet_run_loop.py b/official/resnet/resnet_run_loop.py
index 3b3721ee..ffc4ffb1 100644
--- a/official/resnet/resnet_run_loop.py
+++ b/official/resnet/resnet_run_loop.py
@@ -88,7 +88,7 @@ def process_record_dataset(dataset,
 
   # Parses the raw records into images and labels.
   dataset = dataset.apply(
-      tf.contrib.data.map_and_batch(
+      tf.data.experimental.map_and_batch(
           lambda value: parse_record_fn(value, is_training, dtype),
           batch_size=batch_size,
           num_parallel_batches=num_parallel_batches,
@@ -100,12 +100,12 @@ def process_record_dataset(dataset,
   # critical training path. Setting buffer_size to tf.contrib.data.AUTOTUNE
   # allows DistributionStrategies to adjust how many batches to fetch based
   # on how many devices are present.
-  dataset = dataset.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)
+  dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
 
   # Defines a specific size thread pool for tf.data operations.
   if datasets_num_private_threads:
-    tf.logging.info('datasets_num_private_threads: %s',
-                    datasets_num_private_threads)
+    tf.compat.v1.logging.info('datasets_num_private_threads: %s',
+                              datasets_num_private_threads)
     dataset = threadpool.override_threadpool(
         dataset,
         threadpool.PrivateThreadPool(
@@ -140,21 +140,21 @@ def get_synth_input_fn(height, width, num_channels, num_classes,
   def input_fn(is_training, data_dir, batch_size, *args, **kwargs):
     """Returns dataset filled with random data."""
     # Synthetic input should be within [0, 255].
-    inputs = tf.truncated_normal(
+    inputs = tf.random.truncated_normal(
         [batch_size] + [height, width, num_channels],
         dtype=dtype,
         mean=127,
         stddev=60,
         name='synthetic_inputs')
 
-    labels = tf.random_uniform(
+    labels = tf.random.uniform(
         [batch_size],
         minval=0,
         maxval=num_classes - 1,
         dtype=tf.int32,
         name='synthetic_labels')
     data = tf.data.Dataset.from_tensors((inputs, labels)).repeat()
-    data = data.prefetch(buffer_size=tf.contrib.data.AUTOTUNE)
+    data = data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
     return data
 
   return input_fn
@@ -172,7 +172,7 @@ def image_bytes_serving_input_fn(image_shape, dtype=tf.float32):
         image_bytes, bbox, height, width, num_channels, is_training=False)
     return image
 
-  image_bytes_list = tf.placeholder(
+  image_bytes_list = tf.compat.v1.placeholder(
       shape=[None], dtype=tf.string, name='input_tensor')
   images = tf.map_fn(
       _preprocess_image, image_bytes_list, back_prop=False, dtype=dtype)
@@ -197,15 +197,17 @@ def override_flags_and_set_envars_for_gpu_thread_pool(flags_obj):
     what has been set by the user on the command-line.
   """
   cpu_count = multiprocessing.cpu_count()
-  tf.logging.info('Logical CPU cores: %s', cpu_count)
+  tf.compat.v1.logging.info('Logical CPU cores: %s', cpu_count)
 
   # Sets up thread pool for each GPU for op scheduling.
   per_gpu_thread_count = 1
   total_gpu_thread_count = per_gpu_thread_count * flags_obj.num_gpus
   os.environ['TF_GPU_THREAD_MODE'] = flags_obj.tf_gpu_thread_mode
   os.environ['TF_GPU_THREAD_COUNT'] = str(per_gpu_thread_count)
-  tf.logging.info('TF_GPU_THREAD_COUNT: %s', os.environ['TF_GPU_THREAD_COUNT'])
-  tf.logging.info('TF_GPU_THREAD_MODE: %s', os.environ['TF_GPU_THREAD_MODE'])
+  tf.compat.v1.logging.info('TF_GPU_THREAD_COUNT: %s',
+                            os.environ['TF_GPU_THREAD_COUNT'])
+  tf.compat.v1.logging.info('TF_GPU_THREAD_MODE: %s',
+                            os.environ['TF_GPU_THREAD_MODE'])
 
   # Reduces general thread pool by number of threads used for GPU pool.
   main_thread_count = cpu_count - total_gpu_thread_count
@@ -256,13 +258,15 @@ def learning_rate_with_decay(
 
   def learning_rate_fn(global_step):
     """Builds scaled learning rate function with 5 epoch warm up."""
-    lr = tf.train.piecewise_constant(global_step, boundaries, vals)
+    lr = tf.compat.v1.train.piecewise_constant(global_step, boundaries, vals)
     if warmup:
       warmup_steps = int(batches_per_epoch * 5)
       warmup_lr = (
           initial_learning_rate * tf.cast(global_step, tf.float32) / tf.cast(
               warmup_steps, tf.float32))
-      return tf.cond(global_step < warmup_steps, lambda: warmup_lr, lambda: lr)
+      return tf.cond(pred=global_step < warmup_steps,
+                     true_fn=lambda: warmup_lr,
+                     false_fn=lambda: lr)
     return lr
 
   return learning_rate_fn
@@ -313,7 +317,7 @@ def resnet_model_fn(features, labels, mode, model_class,
   """
 
   # Generate a summary node for the images
-  tf.summary.image('images', features, max_outputs=6)
+  tf.compat.v1.summary.image('images', features, max_outputs=6)
   # Checks that features/images have same data type being used for calculations.
   assert features.dtype == dtype
 
@@ -328,7 +332,7 @@ def resnet_model_fn(features, labels, mode, model_class,
   logits = tf.cast(logits, tf.float32)
 
   predictions = {
-      'classes': tf.argmax(logits, axis=1),
+      'classes': tf.argmax(input=logits, axis=1),
       'probabilities': tf.nn.softmax(logits, name='softmax_tensor')
   }
 
@@ -342,12 +346,12 @@ def resnet_model_fn(features, labels, mode, model_class,
         })
 
   # Calculate loss, which includes softmax cross entropy and L2 regularization.
-  cross_entropy = tf.losses.sparse_softmax_cross_entropy(
+  cross_entropy = tf.compat.v1.losses.sparse_softmax_cross_entropy(
       logits=logits, labels=labels)
 
   # Create a tensor named cross_entropy for logging purposes.
   tf.identity(cross_entropy, name='cross_entropy')
-  tf.summary.scalar('cross_entropy', cross_entropy)
+  tf.compat.v1.summary.scalar('cross_entropy', cross_entropy)
 
   # If no loss_filter_fn is passed, assume we want the default behavior,
   # which is that batch_normalization variables are excluded from loss.
@@ -358,21 +362,21 @@ def resnet_model_fn(features, labels, mode, model_class,
   # Add weight decay to the loss.
   l2_loss = weight_decay * tf.add_n(
       # loss is computed using fp32 for numerical stability.
-      [tf.nn.l2_loss(tf.cast(v, tf.float32)) for v in tf.trainable_variables()
-       if loss_filter_fn(v.name)])
-  tf.summary.scalar('l2_loss', l2_loss)
+      [tf.nn.l2_loss(tf.cast(v, tf.float32))
+       for v in tf.compat.v1.trainable_variables() if loss_filter_fn(v.name)])
+  tf.compat.v1.summary.scalar('l2_loss', l2_loss)
   loss = cross_entropy + l2_loss
 
   if mode == tf.estimator.ModeKeys.TRAIN:
-    global_step = tf.train.get_or_create_global_step()
+    global_step = tf.compat.v1.train.get_or_create_global_step()
 
     learning_rate = learning_rate_fn(global_step)
 
     # Create a tensor named learning_rate for logging purposes
     tf.identity(learning_rate, name='learning_rate')
-    tf.summary.scalar('learning_rate', learning_rate)
+    tf.compat.v1.summary.scalar('learning_rate', learning_rate)
 
-    optimizer = tf.train.MomentumOptimizer(
+    optimizer = tf.compat.v1.train.MomentumOptimizer(
         learning_rate=learning_rate,
         momentum=momentum
     )
@@ -409,24 +413,22 @@ def resnet_model_fn(features, labels, mode, model_class,
         grad_vars = _dense_grad_filter(grad_vars)
       minimize_op = optimizer.apply_gradients(grad_vars, global_step)
 
-    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)
     train_op = tf.group(minimize_op, update_ops)
   else:
     train_op = None
 
-  accuracy = tf.metrics.accuracy(labels, predictions['classes'])
-  accuracy_top_5 = tf.metrics.mean(tf.nn.in_top_k(predictions=logits,
-                                                  targets=labels,
-                                                  k=5,
-                                                  name='top_5_op'))
+  accuracy = tf.compat.v1.metrics.accuracy(labels, predictions['classes'])
+  accuracy_top_5 = tf.compat.v1.metrics.mean(
+      tf.nn.in_top_k(predictions=logits, targets=labels, k=5, name='top_5_op'))
   metrics = {'accuracy': accuracy,
              'accuracy_top_5': accuracy_top_5}
 
   # Create a tensor named train_accuracy for logging purposes
   tf.identity(accuracy[1], name='train_accuracy')
   tf.identity(accuracy_top_5[1], name='train_accuracy_top_5')
-  tf.summary.scalar('train_accuracy', accuracy[1])
-  tf.summary.scalar('train_accuracy_top_5', accuracy_top_5[1])
+  tf.compat.v1.summary.scalar('train_accuracy', accuracy[1])
+  tf.compat.v1.summary.scalar('train_accuracy_top_5', accuracy_top_5[1])
 
   return tf.estimator.EstimatorSpec(
       mode=mode,
@@ -465,7 +467,7 @@ def resnet_main(
 
   # Creates session config. allow_soft_placement = True, is required for
   # multi-GPU and is not harmful for other modes.
-  session_config = tf.ConfigProto(
+  session_config = tf.compat.v1.ConfigProto(
       inter_op_parallelism_threads=flags_obj.inter_op_parallelism_threads,
       intra_op_parallelism_threads=flags_obj.intra_op_parallelism_threads,
       allow_soft_placement=True)
@@ -557,13 +559,14 @@ def resnet_main(
     schedule[-1] = flags_obj.train_epochs - sum(schedule[:-1])  # over counting.
 
   for cycle_index, num_train_epochs in enumerate(schedule):
-    tf.logging.info('Starting cycle: %d/%d', cycle_index, int(n_loops))
+    tf.compat.v1.logging.info('Starting cycle: %d/%d', cycle_index,
+                              int(n_loops))
 
     if num_train_epochs:
       classifier.train(input_fn=lambda: input_fn_train(num_train_epochs),
                        hooks=train_hooks, max_steps=flags_obj.max_train_steps)
 
-    tf.logging.info('Starting to evaluate.')
+    tf.compat.v1.logging.info('Starting to evaluate.')
 
     # flags_obj.max_train_steps is generally associated with testing and
     # profiling. As a result it is frequently called with synthetic data, which
diff --git a/official/utils/accelerator/tpu.py b/official/utils/accelerator/tpu.py
index 117accea..3817b704 100644
--- a/official/utils/accelerator/tpu.py
+++ b/official/utils/accelerator/tpu.py
@@ -71,7 +71,7 @@ def construct_scalar_host_call(metric_dict, model_dir, prefix=""):
   # expects [batch_size, ...] Tensors, thus reshape to introduce a batch
   # dimension. These Tensors are implicitly concatenated to
   # [params['batch_size']].
-  global_step_tensor = tf.reshape(tf.train.get_or_create_global_step(), [1])
+  global_step_tensor = tf.reshape(tf.compat.v1.train.get_or_create_global_step(), [1])
   other_tensors = [tf.reshape(metric_dict[key], [1]) for key in metric_names]
 
   return host_call_fn, [global_step_tensor] + other_tensors
diff --git a/official/utils/accelerator/tpu_test.py b/official/utils/accelerator/tpu_test.py
index 64ff0516..110ad9d0 100644
--- a/official/utils/accelerator/tpu_test.py
+++ b/official/utils/accelerator/tpu_test.py
@@ -36,14 +36,14 @@ class TPUBaseTester(tf.test.TestCase):
     np.random.seed(seed)
 
     embeddings = np.random.random(size=(vocab_size, embedding_dim))
-    embedding_table = tf.convert_to_tensor(embeddings, dtype=tf.float32)
+    embedding_table = tf.convert_to_tensor(value=embeddings, dtype=tf.float32)
 
     tokens = np.random.randint(low=1, high=vocab_size-1,
                                size=(batch_size, sequence_length))
     for i in range(batch_size):
       tokens[i, np.random.randint(low=0, high=sequence_length-1):] = 0
-    values = tf.convert_to_tensor(tokens, dtype=tf.int32)
-    mask = tf.to_float(tf.not_equal(values, 0))
+    values = tf.convert_to_tensor(value=tokens, dtype=tf.int32)
+    mask = tf.cast(tf.not_equal(values, 0), dtype=tf.float32)
     return embedding_table, values, mask
 
   def _test_embedding(self, embedding_dim, vocab_size,
diff --git a/official/utils/data/file_io.py b/official/utils/data/file_io.py
index b8180bce..b46bcaf2 100644
--- a/official/utils/data/file_io.py
+++ b/official/utils/data/file_io.py
@@ -47,11 +47,11 @@ class _GarbageCollector(object):
   def purge(self):
     try:
       for i in self.temp_buffers:
-        if tf.gfile.Exists(i):
-          tf.gfile.Remove(i)
-          tf.logging.info("Buffer file {} removed".format(i))
+        if tf.io.gfile.exists(i):
+          tf.io.gfile.remove(i)
+          tf.compat.v1.logging.info("Buffer file {} removed".format(i))
     except Exception as e:
-      tf.logging.error("Failed to cleanup buffer files: {}".format(e))
+      tf.compat.v1.logging.error("Failed to cleanup buffer files: {}".format(e))
 
 
 _GARBAGE_COLLECTOR = _GarbageCollector()
@@ -64,7 +64,7 @@ def write_to_temp_buffer(dataframe, buffer_folder, columns):
   if buffer_folder is None:
     _, buffer_path = tempfile.mkstemp()
   else:
-    tf.gfile.MakeDirs(buffer_folder)
+    tf.io.gfile.makedirs(buffer_folder)
     buffer_path = os.path.join(buffer_folder, str(uuid.uuid4()))
   _GARBAGE_COLLECTOR.register(buffer_path)
 
@@ -169,35 +169,35 @@ def write_to_buffer(dataframe, buffer_path, columns, expected_size=None):
   Returns:
     The path of the buffer.
   """
-  if tf.gfile.Exists(buffer_path) and tf.gfile.Stat(buffer_path).length > 0:
-    actual_size = tf.gfile.Stat(buffer_path).length
+  if tf.io.gfile.exists(buffer_path) and tf.io.gfile.stat(buffer_path).length > 0:
+    actual_size = tf.io.gfile.stat(buffer_path).length
     if expected_size == actual_size:
       return buffer_path
-    tf.logging.warning(
+    tf.compat.v1.logging.warning(
         "Existing buffer {} has size {}. Expected size {}. Deleting and "
         "rebuilding buffer.".format(buffer_path, actual_size, expected_size))
-    tf.gfile.Remove(buffer_path)
+    tf.io.gfile.remove(buffer_path)
 
   if dataframe is None:
     raise ValueError(
         "dataframe was None but a valid existing buffer was not found.")
 
-  tf.gfile.MakeDirs(os.path.split(buffer_path)[0])
+  tf.io.gfile.makedirs(os.path.split(buffer_path)[0])
 
-  tf.logging.info("Constructing TFRecordDataset buffer: {}".format(buffer_path))
+  tf.compat.v1.logging.info("Constructing TFRecordDataset buffer: {}".format(buffer_path))
 
   count = 0
   pool = multiprocessing.Pool(multiprocessing.cpu_count())
   try:
-    with tf.python_io.TFRecordWriter(buffer_path) as writer:
+    with tf.io.TFRecordWriter(buffer_path) as writer:
       for df_shards in iter_shard_dataframe(df=dataframe,
                                             rows_per_core=_ROWS_PER_CORE):
         _serialize_shards(df_shards, columns, pool, writer)
         count += sum([len(s) for s in df_shards])
-        tf.logging.info("{}/{} examples written."
+        tf.compat.v1.logging.info("{}/{} examples written."
                         .format(str(count).ljust(8), len(dataframe)))
   finally:
     pool.terminate()
 
-  tf.logging.info("Buffer write complete.")
+  tf.compat.v1.logging.info("Buffer write complete.")
   return buffer_path
diff --git a/official/utils/data/file_io_test.py b/official/utils/data/file_io_test.py
index 8996b396..a0e35507 100644
--- a/official/utils/data/file_io_test.py
+++ b/official/utils/data/file_io_test.py
@@ -77,9 +77,9 @@ _TEST_CASES = [
 ]
 
 _FEATURE_MAP = {
-    _RAW_ROW: tf.FixedLenFeature([1], dtype=tf.int64),
-    _DUMMY_COL: tf.FixedLenFeature([1], dtype=tf.int64),
-    _DUMMY_VEC_COL: tf.FixedLenFeature([_DUMMY_VEC_LEN], dtype=tf.float32)
+    _RAW_ROW: tf.io.FixedLenFeature([1], dtype=tf.int64),
+    _DUMMY_COL: tf.io.FixedLenFeature([1], dtype=tf.int64),
+    _DUMMY_VEC_COL: tf.io.FixedLenFeature([_DUMMY_VEC_LEN], dtype=tf.float32)
 }
 
 
@@ -156,9 +156,9 @@ class BaseTest(tf.test.TestCase):
     with self.test_session(graph=tf.Graph()) as sess:
       dataset = tf.data.TFRecordDataset(buffer_path)
       dataset = dataset.batch(1).map(
-          lambda x: tf.parse_example(x, _FEATURE_MAP))
+          lambda x: tf.io.parse_example(serialized=x, features=_FEATURE_MAP))
 
-      data_iter = dataset.make_one_shot_iterator()
+      data_iter = tf.compat.v1.data.make_one_shot_iterator(dataset)
       seen_rows = set()
       for i in range(num_rows+5):
         row = data_iter.get_next()
@@ -177,7 +177,7 @@ class BaseTest(tf.test.TestCase):
           self.assertGreaterEqual(i, num_rows, msg="Too few rows.")
 
     file_io._GARBAGE_COLLECTOR.purge()
-    assert not tf.gfile.Exists(buffer_path)
+    assert not tf.io.gfile.exists(buffer_path)
 
   def test_serialize_deserialize_0(self):
     self._serialize_deserialize(num_cores=1)
diff --git a/official/utils/export/export.py b/official/utils/export/export.py
index 9ae7bca9..8061c288 100644
--- a/official/utils/export/export.py
+++ b/official/utils/export/export.py
@@ -40,7 +40,7 @@ def build_tensor_serving_input_receiver_fn(shape, dtype=tf.float32,
   """
   def serving_input_receiver_fn():
     # Prep a placeholder where the input example will be fed in
-    features = tf.placeholder(
+    features = tf.compat.v1.placeholder(
         dtype=dtype, shape=[batch_size] + shape, name='input_tensor')
 
     return tf.estimator.export.TensorServingInputReceiver(
diff --git a/official/utils/flags/_device.py b/official/utils/flags/_device.py
index 937e9d3e..edaf2f9a 100644
--- a/official/utils/flags/_device.py
+++ b/official/utils/flags/_device.py
@@ -39,7 +39,7 @@ def require_cloud_storage(flag_names):
     valid_flags = True
     for key in flag_names:
       if not flag_values[key].startswith("gs://"):
-        tf.logging.error("{} must be a GCS path.".format(key))
+        tf.compat.v1.logging.error("{} must be a GCS path.".format(key))
         valid_flags = False
 
     return valid_flags
diff --git a/official/utils/logs/hooks.py b/official/utils/logs/hooks.py
index 8e66f512..046b388b 100644
--- a/official/utils/logs/hooks.py
+++ b/official/utils/logs/hooks.py
@@ -25,7 +25,7 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 from official.utils.logs import logger
 
 
-class ExamplesPerSecondHook(tf.train.SessionRunHook):
+class ExamplesPerSecondHook(tf.estimator.SessionRunHook):
   """Hook to print out examples per second.
 
   Total time is tracked and then divided by the total number of steps
@@ -66,7 +66,7 @@ class ExamplesPerSecondHook(tf.train.SessionRunHook):
 
     self._logger = metric_logger or logger.BaseBenchmarkLogger()
 
-    self._timer = tf.train.SecondOrStepTimer(
+    self._timer = tf.estimator.SecondOrStepTimer(
         every_steps=every_n_steps, every_secs=every_n_secs)
 
     self._step_train_time = 0
@@ -76,7 +76,7 @@ class ExamplesPerSecondHook(tf.train.SessionRunHook):
 
   def begin(self):
     """Called once before using the session to check global step."""
-    self._global_step_tensor = tf.train.get_global_step()
+    self._global_step_tensor = tf.compat.v1.train.get_global_step()
     if self._global_step_tensor is None:
       raise RuntimeError(
           "Global step should be created to use StepCounterHook.")
@@ -90,7 +90,7 @@ class ExamplesPerSecondHook(tf.train.SessionRunHook):
     Returns:
       A SessionRunArgs object or None if never triggered.
     """
-    return tf.train.SessionRunArgs(self._global_step_tensor)
+    return tf.estimator.SessionRunArgs(self._global_step_tensor)
 
   def after_run(self, run_context, run_values):  # pylint: disable=unused-argument
     """Called after each call to run().
diff --git a/official/utils/logs/hooks_helper.py b/official/utils/logs/hooks_helper.py
index b037798a..020ce7a8 100644
--- a/official/utils/logs/hooks_helper.py
+++ b/official/utils/logs/hooks_helper.py
@@ -57,7 +57,7 @@ def get_train_hooks(name_list, use_tpu=False, **kwargs):
     return []
 
   if use_tpu:
-    tf.logging.warning("hooks_helper received name_list `{}`, but a TPU is "
+    tf.compat.v1.logging.warning("hooks_helper received name_list `{}`, but a TPU is "
                        "specified. No hooks will be used.".format(name_list))
     return []
 
@@ -89,7 +89,7 @@ def get_logging_tensor_hook(every_n_iter=100, tensors_to_log=None, **kwargs):  #
   if tensors_to_log is None:
     tensors_to_log = _TENSORS_TO_LOG
 
-  return tf.train.LoggingTensorHook(
+  return tf.estimator.LoggingTensorHook(
       tensors=tensors_to_log,
       every_n_iter=every_n_iter)
 
@@ -106,7 +106,7 @@ def get_profiler_hook(model_dir, save_steps=1000, **kwargs):  # pylint: disable=
     Returns a ProfilerHook that writes out timelines that can be loaded into
     profiling tools like chrome://tracing.
   """
-  return tf.train.ProfilerHook(save_steps=save_steps, output_dir=model_dir)
+  return tf.estimator.ProfilerHook(save_steps=save_steps, output_dir=model_dir)
 
 
 def get_examples_per_second_hook(every_n_steps=100,
diff --git a/official/utils/logs/hooks_helper_test.py b/official/utils/logs/hooks_helper_test.py
index e6bc4df7..c090f86e 100644
--- a/official/utils/logs/hooks_helper_test.py
+++ b/official/utils/logs/hooks_helper_test.py
@@ -45,7 +45,7 @@ class BaseTest(unittest.TestCase):
     returned_hook = hooks_helper.get_train_hooks(
         [test_hook_name], model_dir="", **kwargs)
     self.assertEqual(len(returned_hook), 1)
-    self.assertIsInstance(returned_hook[0], tf.train.SessionRunHook)
+    self.assertIsInstance(returned_hook[0], tf.estimator.SessionRunHook)
     self.assertEqual(returned_hook[0].__class__.__name__.lower(),
                      expected_hook_name)
 
diff --git a/official/utils/logs/hooks_test.py b/official/utils/logs/hooks_test.py
index 61813453..70697797 100644
--- a/official/utils/logs/hooks_test.py
+++ b/official/utils/logs/hooks_test.py
@@ -26,7 +26,7 @@ import tensorflow as tf  # pylint: disable=g-bad-import-order
 from official.utils.logs import hooks
 from official.utils.testing import mock_lib
 
-tf.logging.set_verbosity(tf.logging.DEBUG)
+tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.DEBUG)
 
 
 class ExamplesPerSecondHookTest(tf.test.TestCase):
@@ -44,9 +44,10 @@ class ExamplesPerSecondHookTest(tf.test.TestCase):
 
     self.graph = tf.Graph()
     with self.graph.as_default():
-      tf.train.create_global_step()
-      self.train_op = tf.assign_add(tf.train.get_global_step(), 1)
-      self.global_step = tf.train.get_global_step()
+      tf.compat.v1.train.create_global_step()
+      self.train_op = tf.compat.v1.assign_add(
+          tf.compat.v1.train.get_global_step(), 1)
+      self.global_step = tf.compat.v1.train.get_global_step()
 
   def test_raise_in_both_secs_and_steps(self):
     with self.assertRaises(ValueError):
@@ -71,8 +72,8 @@ class ExamplesPerSecondHookTest(tf.test.TestCase):
         warm_steps=warm_steps,
         metric_logger=self._logger)
 
-    with tf.train.MonitoredSession(
-        tf.train.ChiefSessionCreator(), [hook]) as mon_sess:
+    with tf.compat.v1.train.MonitoredSession(
+        tf.compat.v1.train.ChiefSessionCreator(), [hook]) as mon_sess:
       for _ in range(every_n_steps):
         # Explicitly run global_step after train_op to get the accurate
         # global_step value
@@ -125,8 +126,8 @@ class ExamplesPerSecondHookTest(tf.test.TestCase):
         every_n_secs=every_n_secs,
         metric_logger=self._logger)
 
-    with tf.train.MonitoredSession(
-        tf.train.ChiefSessionCreator(), [hook]) as mon_sess:
+    with tf.compat.v1.train.MonitoredSession(
+        tf.compat.v1.train.ChiefSessionCreator(), [hook]) as mon_sess:
       # Explicitly run global_step after train_op to get the accurate
       # global_step value
       mon_sess.run(self.train_op)
diff --git a/official/utils/logs/logger.py b/official/utils/logs/logger.py
index 856ce62a..d76292ae 100644
--- a/official/utils/logs/logger.py
+++ b/official/utils/logs/logger.py
@@ -119,12 +119,13 @@ class BaseBenchmarkLogger(object):
       eval_results: dict, the result of evaluate.
     """
     if not isinstance(eval_results, dict):
-      tf.logging.warning("eval_results should be dictionary for logging. "
-                         "Got %s", type(eval_results))
+      tf.compat.v1.logging.warning(
+          "eval_results should be dictionary for logging. Got %s",
+          type(eval_results))
       return
-    global_step = eval_results[tf.GraphKeys.GLOBAL_STEP]
+    global_step = eval_results[tf.compat.v1.GraphKeys.GLOBAL_STEP]
     for key in sorted(eval_results):
-      if key != tf.GraphKeys.GLOBAL_STEP:
+      if key != tf.compat.v1.GraphKeys.GLOBAL_STEP:
         self.log_metric(key, eval_results[key], global_step=global_step)
 
   def log_metric(self, name, value, unit=None, global_step=None, extras=None):
@@ -143,12 +144,12 @@ class BaseBenchmarkLogger(object):
     """
     metric = _process_metric_to_json(name, value, unit, global_step, extras)
     if metric:
-      tf.logging.info("Benchmark metric: %s", metric)
+      tf.compat.v1.logging.info("Benchmark metric: %s", metric)
 
   def log_run_info(self, model_name, dataset_name, run_params, test_id=None):
-    tf.logging.info("Benchmark run: %s",
-                    _gather_run_info(model_name, dataset_name, run_params,
-                                     test_id))
+    tf.compat.v1.logging.info(
+        "Benchmark run: %s", _gather_run_info(model_name, dataset_name,
+                                              run_params, test_id))
 
   def on_finish(self, status):
     pass
@@ -160,9 +161,9 @@ class BenchmarkFileLogger(BaseBenchmarkLogger):
   def __init__(self, logging_dir):
     super(BenchmarkFileLogger, self).__init__()
     self._logging_dir = logging_dir
-    if not tf.gfile.IsDirectory(self._logging_dir):
-      tf.gfile.MakeDirs(self._logging_dir)
-    self._metric_file_handler = tf.gfile.GFile(
+    if not tf.io.gfile.isdir(self._logging_dir):
+      tf.io.gfile.makedirs(self._logging_dir)
+    self._metric_file_handler = tf.io.gfile.GFile(
         os.path.join(self._logging_dir, METRIC_LOG_FILE_NAME), "a")
 
   def log_metric(self, name, value, unit=None, global_step=None, extras=None):
@@ -186,8 +187,9 @@ class BenchmarkFileLogger(BaseBenchmarkLogger):
         self._metric_file_handler.write("\n")
         self._metric_file_handler.flush()
       except (TypeError, ValueError) as e:
-        tf.logging.warning("Failed to dump metric to log file: "
-                           "name %s, value %s, error %s", name, value, e)
+        tf.compat.v1.logging.warning(
+            "Failed to dump metric to log file: name %s, value %s, error %s",
+            name, value, e)
 
   def log_run_info(self, model_name, dataset_name, run_params, test_id=None):
     """Collect most of the TF runtime information for the local env.
@@ -204,14 +206,14 @@ class BenchmarkFileLogger(BaseBenchmarkLogger):
     """
     run_info = _gather_run_info(model_name, dataset_name, run_params, test_id)
 
-    with tf.gfile.GFile(os.path.join(
+    with tf.io.gfile.GFile(os.path.join(
         self._logging_dir, BENCHMARK_RUN_LOG_FILE_NAME), "w") as f:
       try:
         json.dump(run_info, f)
         f.write("\n")
       except (TypeError, ValueError) as e:
-        tf.logging.warning("Failed to dump benchmark run info to log file: %s",
-                           e)
+        tf.compat.v1.logging.warning(
+            "Failed to dump benchmark run info to log file: %s", e)
 
   def on_finish(self, status):
     self._metric_file_handler.flush()
@@ -324,7 +326,7 @@ def _process_metric_to_json(
     name, value, unit=None, global_step=None, extras=None):
   """Validate the metric data and generate JSON for insert."""
   if not isinstance(value, numbers.Number):
-    tf.logging.warning(
+    tf.compat.v1.logging.warning(
         "Metric value to log should be a number. Got %s", type(value))
     return None
 
@@ -341,7 +343,7 @@ def _process_metric_to_json(
 
 def _collect_tensorflow_info(run_info):
   run_info["tensorflow_version"] = {
-      "version": tf.VERSION, "git_hash": tf.GIT_VERSION}
+      "version": tf.version.VERSION, "git_hash": tf.version.GIT_VERSION}
 
 
 def _collect_run_params(run_info, run_params):
@@ -385,7 +387,8 @@ def _collect_cpu_info(run_info):
 
     run_info["machine_config"]["cpu_info"] = cpu_info
   except ImportError:
-    tf.logging.warn("'cpuinfo' not imported. CPU info will not be logged.")
+    tf.compat.v1.logging.warn(
+        "'cpuinfo' not imported. CPU info will not be logged.")
 
 
 def _collect_gpu_info(run_info, session_config=None):
@@ -415,7 +418,8 @@ def _collect_memory_info(run_info):
     run_info["machine_config"]["memory_total"] = vmem.total
     run_info["machine_config"]["memory_available"] = vmem.available
   except ImportError:
-    tf.logging.warn("'psutil' not imported. Memory info will not be logged.")
+    tf.compat.v1.logging.warn(
+        "'psutil' not imported. Memory info will not be logged.")
 
 
 def _collect_test_environment(run_info):
diff --git a/official/utils/logs/logger_test.py b/official/utils/logs/logger_test.py
index f02388d9..24f456f0 100644
--- a/official/utils/logs/logger_test.py
+++ b/official/utils/logs/logger_test.py
@@ -78,7 +78,7 @@ class BenchmarkLoggerTest(tf.test.TestCase):
     mock_logger = mock.MagicMock()
     mock_config_benchmark_logger.return_value = mock_logger
     with logger.benchmark_context(None):
-      tf.logging.info("start benchmarking")
+      tf.compat.v1.logging.info("start benchmarking")
     mock_logger.on_finish.assert_called_once_with(logger.RUN_STATUS_SUCCESS)
 
   @mock.patch("official.utils.logs.logger.config_benchmark_logger")
@@ -95,18 +95,18 @@ class BaseBenchmarkLoggerTest(tf.test.TestCase):
 
   def setUp(self):
     super(BaseBenchmarkLoggerTest, self).setUp()
-    self._actual_log = tf.logging.info
+    self._actual_log = tf.compat.v1.logging.info
     self.logged_message = None
 
     def mock_log(*args, **kwargs):
       self.logged_message = args
       self._actual_log(*args, **kwargs)
 
-    tf.logging.info = mock_log
+    tf.compat.v1.logging.info = mock_log
 
   def tearDown(self):
     super(BaseBenchmarkLoggerTest, self).tearDown()
-    tf.logging.info = self._actual_log
+    tf.compat.v1.logging.info = self._actual_log
 
   def test_log_metric(self):
     log = logger.BaseBenchmarkLogger()
@@ -128,16 +128,16 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
 
   def tearDown(self):
     super(BenchmarkFileLoggerTest, self).tearDown()
-    tf.gfile.DeleteRecursively(self.get_temp_dir())
+    tf.io.gfile.rmtree(self.get_temp_dir())
     os.environ.clear()
     os.environ.update(self.original_environ)
 
   def test_create_logging_dir(self):
     non_exist_temp_dir = os.path.join(self.get_temp_dir(), "unknown_dir")
-    self.assertFalse(tf.gfile.IsDirectory(non_exist_temp_dir))
+    self.assertFalse(tf.io.gfile.isdir(non_exist_temp_dir))
 
     logger.BenchmarkFileLogger(non_exist_temp_dir)
-    self.assertTrue(tf.gfile.IsDirectory(non_exist_temp_dir))
+    self.assertTrue(tf.io.gfile.isdir(non_exist_temp_dir))
 
   def test_log_metric(self):
     log_dir = tempfile.mkdtemp(dir=self.get_temp_dir())
@@ -145,8 +145,8 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_metric("accuracy", 0.999, global_step=1e4, extras={"name": "value"})
 
     metric_log = os.path.join(log_dir, "metric.log")
-    self.assertTrue(tf.gfile.Exists(metric_log))
-    with tf.gfile.GFile(metric_log) as f:
+    self.assertTrue(tf.io.gfile.exists(metric_log))
+    with tf.io.gfile.GFile(metric_log) as f:
       metric = json.loads(f.readline())
       self.assertEqual(metric["name"], "accuracy")
       self.assertEqual(metric["value"], 0.999)
@@ -161,8 +161,8 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_metric("loss", 0.02, global_step=1e4)
 
     metric_log = os.path.join(log_dir, "metric.log")
-    self.assertTrue(tf.gfile.Exists(metric_log))
-    with tf.gfile.GFile(metric_log) as f:
+    self.assertTrue(tf.io.gfile.exists(metric_log))
+    with tf.io.gfile.GFile(metric_log) as f:
       accuracy = json.loads(f.readline())
       self.assertEqual(accuracy["name"], "accuracy")
       self.assertEqual(accuracy["value"], 0.999)
@@ -184,7 +184,7 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_metric("accuracy", const)
 
     metric_log = os.path.join(log_dir, "metric.log")
-    self.assertFalse(tf.gfile.Exists(metric_log))
+    self.assertFalse(tf.io.gfile.exists(metric_log))
 
   def test_log_evaluation_result(self):
     eval_result = {"loss": 0.46237424,
@@ -195,8 +195,8 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_evaluation_result(eval_result)
 
     metric_log = os.path.join(log_dir, "metric.log")
-    self.assertTrue(tf.gfile.Exists(metric_log))
-    with tf.gfile.GFile(metric_log) as f:
+    self.assertTrue(tf.io.gfile.exists(metric_log))
+    with tf.io.gfile.GFile(metric_log) as f:
       accuracy = json.loads(f.readline())
       self.assertEqual(accuracy["name"], "accuracy")
       self.assertEqual(accuracy["value"], 0.9285)
@@ -216,7 +216,7 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_evaluation_result(eval_result)
 
     metric_log = os.path.join(log_dir, "metric.log")
-    self.assertFalse(tf.gfile.Exists(metric_log))
+    self.assertFalse(tf.io.gfile.exists(metric_log))
 
   @mock.patch("official.utils.logs.logger._gather_run_info")
   def test_log_run_info(self, mock_gather_run_info):
@@ -229,8 +229,8 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     log.log_run_info("model_name", "dataset_name", {})
 
     run_log = os.path.join(log_dir, "benchmark_run.log")
-    self.assertTrue(tf.gfile.Exists(run_log))
-    with tf.gfile.GFile(run_log) as f:
+    self.assertTrue(tf.io.gfile.exists(run_log))
+    with tf.io.gfile.GFile(run_log) as f:
       run_info = json.loads(f.readline())
       self.assertEqual(run_info["model_name"], "model_name")
       self.assertEqual(run_info["dataset"], "dataset_name")
@@ -240,8 +240,10 @@ class BenchmarkFileLoggerTest(tf.test.TestCase):
     run_info = {}
     logger._collect_tensorflow_info(run_info)
     self.assertNotEqual(run_info["tensorflow_version"], {})
-    self.assertEqual(run_info["tensorflow_version"]["version"], tf.VERSION)
-    self.assertEqual(run_info["tensorflow_version"]["git_hash"], tf.GIT_VERSION)
+    self.assertEqual(run_info["tensorflow_version"]["version"],
+                     tf.version.VERSION)
+    self.assertEqual(run_info["tensorflow_version"]["git_hash"],
+                     tf.version.GIT_VERSION)
 
   def test_collect_run_params(self):
     run_info = {}
@@ -315,7 +317,7 @@ class BenchmarkBigQueryLoggerTest(tf.test.TestCase):
 
   def tearDown(self):
     super(BenchmarkBigQueryLoggerTest, self).tearDown()
-    tf.gfile.DeleteRecursively(self.get_temp_dir())
+    tf.io.gfile.rmtree(self.get_temp_dir())
     os.environ.clear()
     os.environ.update(self.original_environ)
 
diff --git a/official/utils/logs/metric_hook.py b/official/utils/logs/metric_hook.py
index 31b98282..f408e3e9 100644
--- a/official/utils/logs/metric_hook.py
+++ b/official/utils/logs/metric_hook.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import tensorflow as tf  # pylint: disable=g-bad-import-order
 
 
-class LoggingMetricHook(tf.train.LoggingTensorHook):
+class LoggingMetricHook(tf.estimator.LoggingTensorHook):
   """Hook to log benchmark metric information.
 
   This hook is very similar as tf.train.LoggingTensorHook, which logs given
@@ -68,7 +68,7 @@ class LoggingMetricHook(tf.train.LoggingTensorHook):
 
   def begin(self):
     super(LoggingMetricHook, self).begin()
-    self._global_step_tensor = tf.train.get_global_step()
+    self._global_step_tensor = tf.compat.v1.train.get_global_step()
     if self._global_step_tensor is None:
       raise RuntimeError(
           "Global step should be created to use LoggingMetricHook.")
diff --git a/official/utils/logs/metric_hook_test.py b/official/utils/logs/metric_hook_test.py
index e7bfd07b..870ed6eb 100644
--- a/official/utils/logs/metric_hook_test.py
+++ b/official/utils/logs/metric_hook_test.py
@@ -39,7 +39,7 @@ class LoggingMetricHookTest(tf.test.TestCase):
 
   def tearDown(self):
     super(LoggingMetricHookTest, self).tearDown()
-    tf.gfile.DeleteRecursively(self.get_temp_dir())
+    tf.io.gfile.rmtree(self.get_temp_dir())
 
   def test_illegal_args(self):
     with self.assertRaisesRegexp(ValueError, "nvalid every_n_iter"):
@@ -55,15 +55,15 @@ class LoggingMetricHookTest(tf.test.TestCase):
       metric_hook.LoggingMetricHook(tensors=["t"], every_n_iter=5)
 
   def test_print_at_end_only(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       t = tf.constant(42.0, name="foo")
       train_op = tf.constant(3)
       hook = metric_hook.LoggingMetricHook(
           tensors=[t.name], at_end=True, metric_logger=self._logger)
       hook.begin()
       mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
 
       for _ in range(3):
         mon_sess.run(train_op)
@@ -88,8 +88,8 @@ class LoggingMetricHookTest(tf.test.TestCase):
         hook.begin()
 
   def test_log_tensors(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       t1 = tf.constant(42.0, name="foo")
       t2 = tf.constant(43.0, name="bar")
       train_op = tf.constant(3)
@@ -97,7 +97,7 @@ class LoggingMetricHookTest(tf.test.TestCase):
           tensors=[t1, t2], at_end=True, metric_logger=self._logger)
       hook.begin()
       mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
 
       for _ in range(3):
         mon_sess.run(train_op)
@@ -126,7 +126,7 @@ class LoggingMetricHookTest(tf.test.TestCase):
         metric_logger=self._logger)
     hook.begin()
     mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-    sess.run(tf.global_variables_initializer())
+    sess.run(tf.compat.v1.global_variables_initializer())
     mon_sess.run(train_op)
     self.assertRegexpMatches(str(self._logger.logged_metric), t.name)
     for _ in range(3):
@@ -153,15 +153,15 @@ class LoggingMetricHookTest(tf.test.TestCase):
       self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)
 
   def test_print_every_n_steps(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       self._validate_print_every_n_steps(sess, at_end=False)
       # Verify proper reset.
       self._validate_print_every_n_steps(sess, at_end=False)
 
   def test_print_every_n_steps_and_end(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       self._validate_print_every_n_steps(sess, at_end=True)
       # Verify proper reset.
       self._validate_print_every_n_steps(sess, at_end=True)
@@ -175,7 +175,7 @@ class LoggingMetricHookTest(tf.test.TestCase):
         metric_logger=self._logger)
     hook.begin()
     mon_sess = monitored_session._HookedSession(sess, [hook])  # pylint: disable=protected-access
-    sess.run(tf.global_variables_initializer())
+    sess.run(tf.compat.v1.global_variables_initializer())
 
     mon_sess.run(train_op)
     self.assertRegexpMatches(str(self._logger.logged_metric), t.name)
@@ -199,15 +199,15 @@ class LoggingMetricHookTest(tf.test.TestCase):
       self.assertEqual(str(self._logger.logged_metric).find(t.name), -1)
 
   def test_print_every_n_secs(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       self._validate_print_every_n_secs(sess, at_end=False)
       # Verify proper reset.
       self._validate_print_every_n_secs(sess, at_end=False)
 
   def test_print_every_n_secs_and_end(self):
-    with tf.Graph().as_default(), tf.Session() as sess:
-      tf.train.get_or_create_global_step()
+    with tf.Graph().as_default(), tf.compat.v1.Session() as sess:
+      tf.compat.v1.train.get_or_create_global_step()
       self._validate_print_every_n_secs(sess, at_end=True)
       # Verify proper reset.
       self._validate_print_every_n_secs(sess, at_end=True)
diff --git a/official/utils/logs/mlperf_helper.py b/official/utils/logs/mlperf_helper.py
index 65a7435e..c9c04344 100644
--- a/official/utils/logs/mlperf_helper.py
+++ b/official/utils/logs/mlperf_helper.py
@@ -94,7 +94,7 @@ def get_mlperf_log():
       version = pkg_resources.get_distribution("mlperf_compliance")
       version = tuple(int(i) for i in version.version.split("."))
       if version < _MIN_VERSION:
-        tf.logging.warning(
+        tf.compat.v1.logging.warning(
             "mlperf_compliance is version {}, must be >= {}".format(
                 ".".join([str(i) for i in version]),
                 ".".join([str(i) for i in _MIN_VERSION])))
@@ -187,6 +187,6 @@ def clear_system_caches():
 
 
 if __name__ == "__main__":
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
   with LOGGER(True):
     ncf_print(key=TAGS.RUN_START)
diff --git a/official/utils/misc/model_helpers.py b/official/utils/misc/model_helpers.py
index 9c452235..3637508b 100644
--- a/official/utils/misc/model_helpers.py
+++ b/official/utils/misc/model_helpers.py
@@ -48,7 +48,7 @@ def past_stop_threshold(stop_threshold, eval_metric):
                      "must be a number.")
 
   if eval_metric >= stop_threshold:
-    tf.logging.info(
+    tf.compat.v1.logging.info(
         "Stop threshold of {} was passed with metric value {}.".format(
             stop_threshold, eval_metric))
     return True
@@ -87,7 +87,7 @@ def generate_synthetic_data(
 
 
 def apply_clean(flags_obj):
-  if flags_obj.clean and tf.gfile.Exists(flags_obj.model_dir):
-    tf.logging.info("--clean flag set. Removing existing model dir: {}".format(
+  if flags_obj.clean and tf.io.gfile.exists(flags_obj.model_dir):
+    tf.compat.v1.logging.info("--clean flag set. Removing existing model dir: {}".format(
         flags_obj.model_dir))
-    tf.gfile.DeleteRecursively(flags_obj.model_dir)
+    tf.io.gfile.rmtree(flags_obj.model_dir)
diff --git a/official/utils/misc/model_helpers_test.py b/official/utils/misc/model_helpers_test.py
index ff7f9b4b..b259f74f 100644
--- a/official/utils/misc/model_helpers_test.py
+++ b/official/utils/misc/model_helpers_test.py
@@ -69,13 +69,13 @@ class SyntheticDataTest(tf.test.TestCase):
   """Tests for generate_synthetic_data."""
 
   def test_generate_synethetic_data(self):
-    input_element, label_element = model_helpers.generate_synthetic_data(
-        input_shape=tf.TensorShape([5]),
-        input_value=123,
-        input_dtype=tf.float32,
-        label_shape=tf.TensorShape([]),
-        label_value=456,
-        label_dtype=tf.int32).make_one_shot_iterator().get_next()
+    input_element, label_element = tf.compat.v1.data.make_one_shot_iterator(
+        model_helpers.generate_synthetic_data(input_shape=tf.TensorShape([5]),
+                                              input_value=123,
+                                              input_dtype=tf.float32,
+                                              label_shape=tf.TensorShape([]),
+                                              label_value=456,
+                                              label_dtype=tf.int32)).get_next()
 
     with self.test_session() as sess:
       for n in range(5):
@@ -89,7 +89,7 @@ class SyntheticDataTest(tf.test.TestCase):
         input_value=43.5,
         input_dtype=tf.float32)
 
-    element = d.make_one_shot_iterator().get_next()
+    element = tf.compat.v1.data.make_one_shot_iterator(d).get_next()
     self.assertFalse(isinstance(element, tuple))
 
     with self.test_session() as sess:
@@ -102,7 +102,7 @@ class SyntheticDataTest(tf.test.TestCase):
                      'b': {'c': tf.TensorShape([3]), 'd': tf.TensorShape([])}},
         input_value=1.1)
 
-    element = d.make_one_shot_iterator().get_next()
+    element = tf.compat.v1.data.make_one_shot_iterator(d).get_next()
     self.assertIn('a', element)
     self.assertIn('b', element)
     self.assertEquals(len(element['b']), 2)
diff --git a/official/utils/testing/reference_data.py b/official/utils/testing/reference_data.py
index aaae9116..266af6a3 100644
--- a/official/utils/testing/reference_data.py
+++ b/official/utils/testing/reference_data.py
@@ -170,12 +170,12 @@ class BaseTest(tf.test.TestCase):
     # Serialize graph for comparison.
     graph_bytes = graph.as_graph_def().SerializeToString()
     expected_file = os.path.join(data_dir, "expected_graph")
-    with tf.gfile.Open(expected_file, "wb") as f:
+    with tf.io.gfile.GFile(expected_file, "wb") as f:
       f.write(graph_bytes)
 
     with graph.as_default():
-      init = tf.global_variables_initializer()
-      saver = tf.train.Saver()
+      init = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
 
     with self.test_session(graph=graph) as sess:
       sess.run(init)
@@ -191,11 +191,11 @@ class BaseTest(tf.test.TestCase):
 
       if correctness_function is not None:
         results = correctness_function(*eval_results)
-        with tf.gfile.Open(os.path.join(data_dir, "results.json"), "w") as f:
+        with tf.io.gfile.GFile(os.path.join(data_dir, "results.json"), "w") as f:
           json.dump(results, f)
 
-      with tf.gfile.Open(os.path.join(data_dir, "tf_version.json"), "w") as f:
-        json.dump([tf.VERSION, tf.GIT_VERSION], f)
+      with tf.io.gfile.GFile(os.path.join(data_dir, "tf_version.json"), "w") as f:
+        json.dump([tf.version.VERSION, tf.version.GIT_VERSION], f)
 
   def _evaluate_test_case(self, name, graph, ops_to_eval, correctness_function):
     """Determine if a graph agrees with the reference data.
@@ -216,7 +216,7 @@ class BaseTest(tf.test.TestCase):
     # Serialize graph for comparison.
     graph_bytes = graph.as_graph_def().SerializeToString()
     expected_file = os.path.join(data_dir, "expected_graph")
-    with tf.gfile.Open(expected_file, "rb") as f:
+    with tf.io.gfile.GFile(expected_file, "rb") as f:
       expected_graph_bytes = f.read()
       # The serialization is non-deterministic byte-for-byte. Instead there is
       # a utility which evaluates the semantics of the two graphs to test for
@@ -228,19 +228,19 @@ class BaseTest(tf.test.TestCase):
         graph_bytes, expected_graph_bytes).decode("utf-8")
 
     with graph.as_default():
-      init = tf.global_variables_initializer()
-      saver = tf.train.Saver()
+      init = tf.compat.v1.global_variables_initializer()
+      saver = tf.compat.v1.train.Saver()
 
-    with tf.gfile.Open(os.path.join(data_dir, "tf_version.json"), "r") as f:
+    with tf.io.gfile.GFile(os.path.join(data_dir, "tf_version.json"), "r") as f:
       tf_version_reference, tf_git_version_reference = json.load(f)  # pylint: disable=unpacking-non-sequence
 
     tf_version_comparison = ""
-    if tf.GIT_VERSION != tf_git_version_reference:
+    if tf.version.GIT_VERSION != tf_git_version_reference:
       tf_version_comparison = (
           "Test was built using:     {} (git = {})\n"
           "Local TensorFlow version: {} (git = {})"
           .format(tf_version_reference, tf_git_version_reference,
-                  tf.VERSION, tf.GIT_VERSION)
+                  tf.version.VERSION, tf.version.GIT_VERSION)
       )
 
     with self.test_session(graph=graph) as sess:
@@ -249,7 +249,7 @@ class BaseTest(tf.test.TestCase):
         saver.restore(sess=sess, save_path=os.path.join(
             data_dir, self.ckpt_prefix))
         if differences:
-          tf.logging.warn(
+          tf.compat.v1.logging.warn(
               "The provided graph is different than expected:\n  {}\n"
               "However the weights were still able to be loaded.\n{}".format(
                   differences, tf_version_comparison)
@@ -262,7 +262,7 @@ class BaseTest(tf.test.TestCase):
       eval_results = [op.eval() for op in ops_to_eval]
       if correctness_function is not None:
         results = correctness_function(*eval_results)
-        with tf.gfile.Open(os.path.join(data_dir, "results.json"), "r") as f:
+        with tf.io.gfile.GFile(os.path.join(data_dir, "results.json"), "r") as f:
           expected_results = json.load(f)
         self.assertAllClose(results, expected_results)
 
@@ -298,7 +298,7 @@ class BaseTest(tf.test.TestCase):
             correctness_function=correctness_function
         )
       except:
-        tf.logging.error("Failed unittest {}".format(name))
+        tf.compat.v1.logging.error("Failed unittest {}".format(name))
         raise
     else:
       self._construct_and_save_reference_files(
diff --git a/official/utils/testing/reference_data_test.py b/official/utils/testing/reference_data_test.py
index c5c18344..4db5d95f 100644
--- a/official/utils/testing/reference_data_test.py
+++ b/official/utils/testing/reference_data_test.py
@@ -63,12 +63,12 @@ class GoldenBaseTest(reference_data.BaseTest):
     with g.as_default():
       seed = self.name_to_seed(name)
       seed = seed + 1 if bad_seed else seed
-      tf.set_random_seed(seed)
+      tf.compat.v1.set_random_seed(seed)
       tensor_name = "wrong_tensor" if wrong_name else "input_tensor"
       tensor_shape = (1, 2) if wrong_shape else (1, 1)
-      input_tensor = tf.get_variable(
+      input_tensor = tf.compat.v1.get_variable(
           tensor_name, dtype=tf.float32,
-          initializer=tf.random_uniform(tensor_shape, maxval=1)
+          initializer=tf.random.uniform(tensor_shape, maxval=1)
       )
 
     def correctness_function(tensor_result):
@@ -86,13 +86,13 @@ class GoldenBaseTest(reference_data.BaseTest):
 
     g = tf.Graph()
     with g.as_default():
-      tf.set_random_seed(self.name_to_seed(name))
-      input_tensor = tf.get_variable(
+      tf.compat.v1.set_random_seed(self.name_to_seed(name))
+      input_tensor = tf.compat.v1.get_variable(
           "input_tensor", dtype=tf.float32,
-          initializer=tf.random_uniform((1, 2), maxval=1)
+          initializer=tf.random.uniform((1, 2), maxval=1)
       )
-      layer = tf.layers.dense(inputs=input_tensor, units=4)
-      layer = tf.layers.dense(inputs=layer, units=1)
+      layer = tf.compat.v1.layers.dense(inputs=input_tensor, units=4)
+      layer = tf.compat.v1.layers.dense(inputs=layer, units=1)
 
     self._save_or_test_ops(
         name=name, graph=g, ops_to_eval=[layer], test=test,
