commit 23b5b4227dfa1b23d7c21f0dfaf0951b16671f43
Author: Aman Gupta <4409685+aman2930@users.noreply.github.com>
Date:   Thu Aug 30 10:51:38 2018 -0700

    Bypassing Export model step, if training on TPU's. As this need inference to be supported on TPU's. Remove this check once inference is supported. (#5209)

diff --git a/official/transformer/transformer_main.py b/official/transformer/transformer_main.py
index b22c1693..d0c5bd0d 100644
--- a/official/transformer/transformer_main.py
+++ b/official/transformer/transformer_main.py
@@ -610,7 +610,7 @@ def run_transformer(flags_obj):
       bleu_threshold=flags_obj.stop_threshold,
       vocab_file=flags_obj.vocab_file)
 
-  if flags_obj.export_dir:
+  if flags_obj.export_dir and not params["use_tpu"]:
     serving_input_fn = export.build_tensor_serving_input_receiver_fn(
         shape=[None], dtype=tf.int64, batch_size=None)
     # Export saved model, and save the vocab file as an extra asset. The vocab
