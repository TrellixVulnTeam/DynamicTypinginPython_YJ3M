commit 99f9442b0f1f5225c7ef725c32e55afb41186e66
Author: Neal Wu <neal@nealwu.com>
Date:   Mon May 22 18:01:56 2017 -0700

    Remove barrier, add tf.identity where appropriate, and make sure tests pass

diff --git a/inception/inception/slim/ops_test.py b/inception/inception/slim/ops_test.py
index cf5afbba..13dc5d9a 100644
--- a/inception/inception/slim/ops_test.py
+++ b/inception/inception/slim/ops_test.py
@@ -418,7 +418,7 @@ class DropoutTest(tf.test.TestCase):
     with self.test_session():
       images = tf.random_uniform((5, height, width, 3), seed=1)
       output = ops.dropout(images)
-      self.assertEquals(output.op.name, 'Dropout/dropout/mul_1')
+      self.assertEquals(output.op.name, 'Dropout/dropout/mul')
       output.get_shape().assert_is_compatible_with(images.get_shape())
 
   def testCreateDropoutNoTraining(self):
@@ -599,9 +599,7 @@ class BatchNormTest(tf.test.TestCase):
       output = ops.batch_norm(images, decay=0.1)
       update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)
       with tf.control_dependencies(update_ops):
-        barrier = tf.no_op(name='gradient_barrier')
-        with tf.control_dependencies([barrier]):
-          output = tf.identity(output)
+        output = tf.identity(output)
       # Initialize all variables
       sess.run(tf.global_variables_initializer())
       moving_mean = variables.get_variables('BatchNorm/moving_mean')[0]
@@ -630,9 +628,7 @@ class BatchNormTest(tf.test.TestCase):
       output = ops.batch_norm(images, decay=0.1, is_training=False)
       update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)
       with tf.control_dependencies(update_ops):
-        barrier = tf.no_op(name='gradient_barrier')
-        with tf.control_dependencies([barrier]):
-          output = tf.identity(output)
+        output = tf.identity(output)
       # Initialize all variables
       sess.run(tf.global_variables_initializer())
       moving_mean = variables.get_variables('BatchNorm/moving_mean')[0]
@@ -665,9 +661,7 @@ class BatchNormTest(tf.test.TestCase):
       output = ops.batch_norm(images, decay=0.1, is_training=False)
       update_ops = tf.get_collection(ops.UPDATE_OPS_COLLECTION)
       with tf.control_dependencies(update_ops):
-        barrier = tf.no_op(name='gradient_barrier')
-        with tf.control_dependencies([barrier]):
-          output = tf.identity(output)
+        output = tf.identity(output)
       # Initialize all variables
       sess.run(tf.global_variables_initializer())
       moving_mean = variables.get_variables('BatchNorm/moving_mean')[0]
diff --git a/slim/deployment/model_deploy.py b/slim/deployment/model_deploy.py
index 24dd5c34..96b762ba 100644
--- a/slim/deployment/model_deploy.py
+++ b/slim/deployment/model_deploy.py
@@ -379,7 +379,7 @@ def deploy(config,
 
         update_op = tf.group(*update_ops)
         with tf.control_dependencies([update_op]):
-          train_op = total_loss
+          train_op = tf.identity(total_loss, name='train_op')
     else:
       clones_losses = []
       regularization_losses = tf.get_collection(
diff --git a/slim/train_image_classifier.py b/slim/train_image_classifier.py
index 5aa674f4..57049a1a 100755
--- a/slim/train_image_classifier.py
+++ b/slim/train_image_classifier.py
@@ -540,7 +540,7 @@ def main(_):
 
     update_op = tf.group(*update_ops)
     with tf.control_dependencies([update_op]):
-      train_tensor = total_loss
+      train_tensor = tf.identity(total_loss, name='train_op')
 
     # Add the summaries from the first clone. These contain the summaries
     # created by model_fn and either optimize_clones() or _gather_clone_loss().
