commit bf3c9d8b62d100364f811be8c3aedb497b14c1ea
Author: Theo <theo.leffyr@gmail.com>
Date:   Wed May 30 16:49:15 2018 +0200

    [Diff Privacy] Update README and fix code exec wrt Python 3.6

diff --git a/research/differential_privacy/dp_sgd/README.md b/research/differential_privacy/dp_sgd/README.md
index 6c084674..e131ee17 100644
--- a/research/differential_privacy/dp_sgd/README.md
+++ b/research/differential_privacy/dp_sgd/README.md
@@ -27,25 +27,23 @@ paper: https://arxiv.org/abs/1607.00133
 
 Note: r0.11 might experience some problems
 
-2. Bazel 0.3.1
+2. Bazel 0.3.1 (<em>Optional</em>)
 
-3. Download MNIST data
-
-TODO(xpan): Complete the link:
-[train](http://download.tensorflow.org/models/)
-[test](http://download.tensorflow.org/models/)
-
-Alternatively, download the tfrecord format MNIST from:
-https://github.com/panyx0718/models/tree/master/slim
+3. Download MNIST data (tfrecord format) <br>
+   ```shell
+   cd models/research/slim
+   DATA_DIR=/tmp/mnist/
+   mkdir /tmp/mnist
+   python download_and_convert_data.py --dataset_name=mnist --dataset_dir="${DATA_DIR}"
+   ```
 
 <b>How to run:</b>
 
 ```shell
 # Clone the codes under differential_privacy.
 # Create an empty WORKSPACE file.
-# Download the data to the data/ directory.
 
-# List the codes.
+# List the codes (Optional).
 $ ls -R differential_privacy/
 differential_privacy/:
 dp_sgd  __init__.py  privacy_accountant  README.md
@@ -71,21 +69,27 @@ BUILD  gaussian_moments.py
 differential_privacy/privacy_accountant/tf:
 accountant.py  accountant_test.py  BUILD
 
-# List the data.
+# List the data (optional).
+$ mv /tmp/mnist/mnist_train.tfrecord data
+$ mv /tmp/mnist/mnist_test.tfrecord data
 $ ls -R data/
 
 ./data:
 mnist_test.tfrecord  mnist_train.tfrecord
 
-# Build the codes.
+# Build the codes (optional).
 $ bazel build -c opt differential_privacy/...
 
-# Run the mnist differntial privacy training codes.
+# Run the mnist differential privacy training codes.
+# 1. With bazel
 $ bazel-bin/differential_privacy/dp_sgd/dp_mnist/dp_mnist \
     --training_data_path=data/mnist_train.tfrecord \
     --eval_data_path=data/mnist_test.tfrecord \
     --save_path=/tmp/mnist_dir
 
+# 2. Or without (by default data is in /tmp/mnist)
+python dp_sgd/dp_mnist/dp_mnist.py  
+
 ...
 step: 1
 step: 2
diff --git a/research/differential_privacy/dp_sgd/dp_mnist/dp_mnist.py b/research/differential_privacy/dp_sgd/dp_mnist/dp_mnist.py
index d9343974..52ec8c5c 100644
--- a/research/differential_privacy/dp_sgd/dp_mnist/dp_mnist.py
+++ b/research/differential_privacy/dp_sgd/dp_mnist/dp_mnist.py
@@ -37,7 +37,7 @@ tf.flags.DEFINE_integer("batch_size", 600,
 tf.flags.DEFINE_integer("batches_per_lot", 1,
                         "Number of batches per lot.")
 # Together, batch_size and batches_per_lot determine lot_size.
-tf.flags.DEFINE_integer("num_training_steps", 50000,
+tf.flags.DEFINE_integer("num_training_steps", 100,
                         "The number of training steps."
                         "This counts number of lots.")
 
diff --git a/research/differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py b/research/differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py
index 7e46f9ba..8a65de96 100644
--- a/research/differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py
+++ b/research/differential_privacy/dp_sgd/dp_optimizer/dp_optimizer.py
@@ -138,7 +138,7 @@ class DPGradientDescentOptimizer(tf.train.GradientDescentOptimizer):
       sanitized_grads = self.compute_sanitized_gradients(
           loss, var_list=var_list)
 
-      grads_and_vars = zip(sanitized_grads, var_list)
+      grads_and_vars = list(zip(sanitized_grads, var_list))
       self._assert_valid_dtypes([v for g, v in grads_and_vars if g is not None])
 
       apply_grads = self.apply_gradients(grads_and_vars,
