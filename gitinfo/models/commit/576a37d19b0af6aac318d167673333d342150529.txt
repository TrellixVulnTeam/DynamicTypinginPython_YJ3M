commit 576a37d19b0af6aac318d167673333d342150529
Author: lcchen <lcchen@google.com>
Date:   Wed Mar 14 17:23:43 2018 -0700

    update dataset examples

diff --git a/research/deeplab/g3doc/cityscapes.md b/research/deeplab/g3doc/cityscapes.md
index 43b3226e..5f302689 100644
--- a/research/deeplab/g3doc/cityscapes.md
+++ b/research/deeplab/g3doc/cityscapes.md
@@ -43,6 +43,10 @@ A local training job using `xception_65` can be run with the following command:
 python deeplab/train.py \
     --logtostderr \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --training_number_of_steps=90000 \
+>>>>>>> origin/master
 =======
     --training_number_of_steps=90000 \
 >>>>>>> origin/master
@@ -57,6 +61,11 @@ python deeplab/train.py \
     --train_crop_size=769 \
     --train_batch_size=1 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="cityscapes" \
+    --train_split="train" \
+>>>>>>> origin/master
 =======
     --dataset="cityscapes" \
     --train_split="train" \
@@ -71,6 +80,7 @@ where ${PATH_TO_INITIAL_CHECKPOINT} is the path to the initial checkpoint
 directory in which training checkpoints and events will be written to, and
 ${PATH_TO_DATASET} is the directory in which the Cityscapes dataset resides.
 
+<<<<<<< HEAD
 <<<<<<< HEAD
 Note that for {train,eval,vis}.py:
 
@@ -78,6 +88,8 @@ Note that for {train,eval,vis}.py:
     the available GPU memory and also set `fine_tune_batch_norm` to be False or
     True depending on the use case.
 =======
+=======
+>>>>>>> origin/master
 **Note that for {train,eval,vis}.py**:
 
 1.  In order to reproduce our results, one needs to use large batch size (> 8),
@@ -86,6 +98,9 @@ Note that for {train,eval,vis}.py:
     GPU memory at hand, please fine-tune from our provided checkpoints whose
     batch norm parameters have been trained, and use smaller learning rate with
     fine_tune_batch_norm = False.
+<<<<<<< HEAD
+>>>>>>> origin/master
+=======
 >>>>>>> origin/master
 
 2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
@@ -111,6 +126,11 @@ python deeplab/eval.py \
     --eval_crop_size=1025 \
     --eval_crop_size=2049 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="cityscapes" \
+    --eval_split="val" \
+>>>>>>> origin/master
 =======
     --dataset="cityscapes" \
     --eval_split="val" \
@@ -142,6 +162,11 @@ python deeplab/vis.py \
     --vis_crop_size=1025 \
     --vis_crop_size=2049 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="cityscapes" \
+    --vis_split="val" \
+>>>>>>> origin/master
 =======
     --dataset="cityscapes" \
     --vis_split="val" \
diff --git a/research/deeplab/g3doc/pascal.md b/research/deeplab/g3doc/pascal.md
index 20a7b1ba..7314c693 100644
--- a/research/deeplab/g3doc/pascal.md
+++ b/research/deeplab/g3doc/pascal.md
@@ -45,6 +45,10 @@ A local training job using `xception_65` can be run with the following command:
 python deeplab/train.py \
     --logtostderr \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --training_number_of_steps=30000 \
+>>>>>>> origin/master
 =======
     --training_number_of_steps=30000 \
 >>>>>>> origin/master
@@ -59,6 +63,11 @@ python deeplab/train.py \
     --train_crop_size=513 \
     --train_batch_size=1 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="pascal_voc_seg" \
+    --train_split="train" \
+>>>>>>> origin/master
 =======
     --dataset="pascal_voc_seg" \
     --train_split="train" \
@@ -74,6 +83,7 @@ directory in which training checkpoints and events will be written to, and
 ${PATH_TO_DATASET} is the directory in which the PASCAL VOC 2012 dataset
 resides.
 
+<<<<<<< HEAD
 <<<<<<< HEAD
 Note that for {train,eval,vis}.py:
 
@@ -81,6 +91,8 @@ Note that for {train,eval,vis}.py:
     the available GPU memory and also set `fine_tune_batch_norm` to be False or
     True depending on the use case.
 =======
+=======
+>>>>>>> origin/master
 **Note that for {train,eval,vis}.py:**
 
 1.  In order to reproduce our results, one needs to use large batch size (> 12),
@@ -89,6 +101,9 @@ Note that for {train,eval,vis}.py:
     GPU memory at hand, please fine-tune from our provided checkpoints whose
     batch norm parameters have been trained, and use smaller learning rate with
     fine_tune_batch_norm = False.
+<<<<<<< HEAD
+>>>>>>> origin/master
+=======
 >>>>>>> origin/master
 
 2.  The users should change atrous_rates from [6, 12, 18] to [12, 24, 36] if
@@ -114,6 +129,11 @@ python deeplab/eval.py \
     --eval_crop_size=513 \
     --eval_crop_size=513 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="pascal_voc_seg" \
+    --eval_split="val" \
+>>>>>>> origin/master
 =======
     --dataset="pascal_voc_seg" \
     --eval_split="val" \
@@ -145,6 +165,11 @@ python deeplab/vis.py \
     --vis_crop_size=513 \
     --vis_crop_size=513 \
 <<<<<<< HEAD
+<<<<<<< HEAD
+=======
+    --dataset="pascal_voc_seg" \
+    --vis_split="val" \
+>>>>>>> origin/master
 =======
     --dataset="pascal_voc_seg" \
     --vis_split="val" \
