commit 9302933b93f573ac92026ccc48b3b0a4df7b1fda
Author: Skirmantas Kligys <skligys@google.com>
Date:   Tue Jan 7 21:31:23 2020 -0800

    Update for tensorflow.contrib removal. (#8010)
    
    * Push conflicting earlier changes in files touched.
    
    PiperOrigin-RevId: 268414407
    PiperOrigin-RevId: 278006953
    PiperOrigin-RevId: 281567252
    PiperOrigin-RevId: 288363770
    
    * Removes tf.contrib usage and replaces them with direct imports for lstm object detection.
    
    PiperOrigin-RevId: 286065032
    
    * Internal changes.
    
    PiperOrigin-RevId: 288363770

diff --git a/research/lstm_object_detection/lstm/lstm_cells.py b/research/lstm_object_detection/lstm/lstm_cells.py
index ee8a2b75..0d6a2bd1 100644
--- a/research/lstm_object_detection/lstm/lstm_cells.py
+++ b/research/lstm_object_detection/lstm/lstm_cells.py
@@ -12,19 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """BottleneckConvLSTMCell implementation."""
 
 import tensorflow as tf
-from tensorflow.contrib.framework.python.ops import variables
-import lstm_object_detection.lstm.utils as lstm_utils
 
-slim = tf.contrib.slim
-
-_batch_norm = tf.contrib.layers.batch_norm
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import rnn as contrib_rnn
+from tensorflow.contrib import slim
+from tensorflow.contrib.framework.python.ops import variables as contrib_variables
+import lstm_object_detection.lstm.utils as lstm_utils
 
 
-class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
+class BottleneckConvLSTMCell(contrib_rnn.RNNCell):
   """Basic LSTM recurrent network cell using separable convolutions.
 
   The implementation is based on:
@@ -58,15 +57,15 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       num_units: int, The number of channels in the LSTM cell.
       forget_bias: float, The bias added to forget gates (see above).
       activation: Activation function of the inner states.
-      flatten_state: if True, state tensor will be flattened and stored as
-        a 2-d tensor. Use for exporting the model to tfmini.
+      flatten_state: if True, state tensor will be flattened and stored as a 2-d
+        tensor. Use for exporting the model to tfmini.
       clip_state: if True, clip state between [-6, 6].
-      output_bottleneck: if True, the cell bottleneck will be concatenated
-        to the cell output.
+      output_bottleneck: if True, the cell bottleneck will be concatenated to
+        the cell output.
       pre_bottleneck: if True, cell assumes that bottlenecking was performing
         before the function was called.
-      visualize_gates: if True, add histogram summaries of all gates
-        and outputs to tensorboard.
+      visualize_gates: if True, add histogram summaries of all gates and outputs
+        to tensorboard.
     """
     self._filter_size = list(filter_size)
     self._output_size = list(output_size)
@@ -84,13 +83,12 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
 
   @property
   def state_size(self):
-    return tf.contrib.rnn.LSTMStateTuple(self._output_size + [self._num_units],
-                                         self._output_size + [self._num_units])
+    return contrib_rnn.LSTMStateTuple(self._output_size + [self._num_units],
+                                      self._output_size + [self._num_units])
 
   @property
   def state_size_flat(self):
-    return tf.contrib.rnn.LSTMStateTuple([self._param_count],
-                                         [self._param_count])
+    return contrib_rnn.LSTMStateTuple([self._param_count], [self._param_count])
 
   @property
   def output_size(self):
@@ -103,6 +101,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       inputs: Input tensor at the current timestep.
       state: Tuple of tensors, the state and output at the previous timestep.
       scope: Optional scope.
+
     Returns:
       A tuple where the first element is the LSTM output and the second is
       a LSTMStateTuple of the state at the current timestep.
@@ -122,7 +121,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       if self._pre_bottleneck:
         bottleneck = inputs
       else:
-        bottleneck = tf.contrib.layers.separable_conv2d(
+        bottleneck = contrib_layers.separable_conv2d(
             tf.concat([inputs, h], 3),
             self._num_units,
             self._filter_size,
@@ -134,7 +133,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
         if self._viz_gates:
           slim.summaries.add_histogram_summary(bottleneck, 'bottleneck')
 
-      concat = tf.contrib.layers.separable_conv2d(
+      concat = contrib_layers.separable_conv2d(
           bottleneck,
           4 * self._num_units,
           self._filter_size,
@@ -165,7 +164,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
         new_c = tf.reshape(new_c, [-1, self._param_count])
         new_h = tf.reshape(new_h, [-1, self._param_count])
 
-      return output, tf.contrib.rnn.LSTMStateTuple(new_c, new_h)
+      return output, contrib_rnn.LSTMStateTuple(new_c, new_h)
 
   def init_state(self, state_name, batch_size, dtype, learned_state=False):
     """Creates an initial state compatible with this cell.
@@ -184,7 +183,8 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
         self.state_size_flat if self._flatten_state else self.state_size)
     # list of 2 zero tensors or variables tensors, depending on if
     # learned_state is true
-    ret_flat = [(variables.model_variable(
+    # pylint: disable=g-long-ternary,g-complex-comprehension
+    ret_flat = [(contrib_variables.model_variable(
         state_name + str(i),
         shape=s,
         dtype=dtype,
@@ -202,8 +202,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       ]
     for s, r in zip(state_size, ret_flat):
       r.set_shape([None] + s)
-    return tf.contrib.framework.nest.pack_sequence_as(
-        structure=[1, 1], flat_sequence=ret_flat)
+    return tf.nest.pack_sequence_as(structure=[1, 1], flat_sequence=ret_flat)
 
   def pre_bottleneck(self, inputs, state, input_index):
     """Apply pre-bottleneck projection to inputs.
@@ -217,6 +216,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       state: 4D Tensor with shape [batch_size x width x height x state_size].
       input_index: integer index indicating which base features the inputs
         correspoding to.
+
     Returns:
       inputs: pre-bottlenecked inputs.
     Raises:
@@ -227,12 +227,13 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
     # returns the state as a tuple. This should not be an issue since we
     # only need to modify state[1] during export, when state should be a
     # list.
-    if not len(inputs.shape) == 4:
+    if len(inputs.shape) != 4:
       raise ValueError('Expect rank 4 feature tensor.')
-    if not self._flatten_state and not len(state.shape) == 4:
+    if not self._flatten_state and len(state.shape) != 4:
       raise ValueError('Expect rank 4 state tensor.')
-    if self._flatten_state and not len(state.shape) == 2:
+    if self._flatten_state and len(state.shape) != 2:
       raise ValueError('Expect rank 2 state tensor when flatten_state is set.')
+
     with tf.name_scope(None):
       state = tf.identity(state, name='raw_inputs/init_lstm_h')
     if self._flatten_state:
@@ -242,7 +243,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       state = tf.reshape(state, [batch_size, height, width, -1])
     with tf.variable_scope('conv_lstm_cell', reuse=tf.AUTO_REUSE):
       scope_name = 'bottleneck_%d' % input_index
-      inputs = tf.contrib.layers.separable_conv2d(
+      inputs = contrib_layers.separable_conv2d(
           tf.concat([inputs, state], 3),
           self.output_size[-1],
           self._filter_size,
@@ -257,7 +258,7 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
     return inputs
 
 
-class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
+class GroupedConvLSTMCell(contrib_rnn.RNNCell):
   """Basic LSTM recurrent network cell using separable convolutions.
 
   The implementation is based on: https://arxiv.org/abs/1903.10172.
@@ -297,21 +298,21 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
       forget_bias: float, The bias added to forget gates (see above).
       activation: Activation function of the inner states.
       use_batch_norm: if True, use batch norm after convolution
-      flatten_state: if True, state tensor will be flattened and stored as
-        a 2-d tensor. Use for exporting the model to tfmini
+      flatten_state: if True, state tensor will be flattened and stored as a 2-d
+        tensor. Use for exporting the model to tfmini
       groups: Number of groups to split the state into. Must evenly divide
         num_units.
       clip_state: if True, clips state between [-6, 6].
       scale_state: if True, scales state so that all values are under 6 at all
         times.
-      output_bottleneck: if True, the cell bottleneck will be concatenated
-        to the cell output.
+      output_bottleneck: if True, the cell bottleneck will be concatenated to
+        the cell output.
       pre_bottleneck: if True, cell assumes that bottlenecking was performing
         before the function was called.
       is_quantized: if True, the model is in quantize mode, which requires
         quantization friendly concat and separable_conv2d ops.
-      visualize_gates: if True, add histogram summaries of all gates
-        and outputs to tensorboard
+      visualize_gates: if True, add histogram summaries of all gates and outputs
+        to tensorboard
 
     Raises:
       ValueError: when both clip_state and scale_state are enabled.
@@ -340,13 +341,12 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
 
   @property
   def state_size(self):
-    return tf.contrib.rnn.LSTMStateTuple(self._output_size + [self._num_units],
-                                         self._output_size + [self._num_units])
+    return contrib_rnn.LSTMStateTuple(self._output_size + [self._num_units],
+                                      self._output_size + [self._num_units])
 
   @property
   def state_size_flat(self):
-    return tf.contrib.rnn.LSTMStateTuple([self._param_count],
-                                         [self._param_count])
+    return contrib_rnn.LSTMStateTuple([self._param_count], [self._param_count])
 
   @property
   def output_size(self):
@@ -370,6 +370,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
       inputs: Input tensor at the current timestep.
       state: Tuple of tensors, the state at the previous timestep.
       scope: Optional scope.
+
     Returns:
       A tuple where the first element is the LSTM output and the second is
       a LSTMStateTuple of the state at the current timestep.
@@ -409,7 +410,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
           if self._use_batch_norm:
             b_x = lstm_utils.quantizable_separable_conv2d(
                 inputs,
-                self._num_units / self._groups,
+                self._num_units // self._groups,
                 self._filter_size,
                 is_quantized=self._is_quantized,
                 depth_multiplier=1,
@@ -418,7 +419,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
                 scope='bottleneck_%d_x' % k)
             b_h = lstm_utils.quantizable_separable_conv2d(
                 h_list[k],
-                self._num_units / self._groups,
+                self._num_units // self._groups,
                 self._filter_size,
                 is_quantized=self._is_quantized,
                 depth_multiplier=1,
@@ -426,10 +427,14 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
                 normalizer_fn=None,
                 scope='bottleneck_%d_h' % k)
             b_x = slim.batch_norm(
-                b_x, scale=True, is_training=self._is_training,
+                b_x,
+                scale=True,
+                is_training=self._is_training,
                 scope='BatchNorm_%d_X' % k)
             b_h = slim.batch_norm(
-                b_h, scale=True, is_training=self._is_training,
+                b_h,
+                scale=True,
+                is_training=self._is_training,
                 scope='BatchNorm_%d_H' % k)
             bottleneck = b_x + b_h
           else:
@@ -439,13 +444,13 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
             bottleneck_concat = lstm_utils.quantizable_concat(
                 [inputs, h_list[k]],
                 axis=3,
-                is_training=self._is_training,
+                is_training=False,
                 is_quantized=self._is_quantized,
                 scope='bottleneck_%d/quantized_concat' % k)
 
             bottleneck = lstm_utils.quantizable_separable_conv2d(
                 bottleneck_concat,
-                self._num_units / self._groups,
+                self._num_units // self._groups,
                 self._filter_size,
                 is_quantized=self._is_quantized,
                 depth_multiplier=1,
@@ -455,7 +460,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
 
         concat = lstm_utils.quantizable_separable_conv2d(
             bottleneck,
-            4 * self._num_units / self._groups,
+            4 * self._num_units // self._groups,
             self._filter_size,
             is_quantized=self._is_quantized,
             depth_multiplier=1,
@@ -632,7 +637,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
       with tf.name_scope(None):
         new_c = tf.identity(new_c, name='raw_outputs/lstm_c')
         new_h = tf.identity(new_h, name='raw_outputs/lstm_h')
-      states_and_output = tf.contrib.rnn.LSTMStateTuple(new_c, new_h)
+      states_and_output = contrib_rnn.LSTMStateTuple(new_c, new_h)
 
       return output, states_and_output
 
@@ -649,11 +654,12 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
     Returns:
       ret: the created initial state
     """
-    state_size = (self.state_size_flat if self._flatten_state
-                  else self.state_size)
+    state_size = (
+        self.state_size_flat if self._flatten_state else self.state_size)
     # list of 2 zero tensors or variables tensors,
     # depending on if learned_state is true
-    ret_flat = [(variables.model_variable(
+    # pylint: disable=g-long-ternary,g-complex-comprehension
+    ret_flat = [(contrib_variables.model_variable(
         state_name + str(i),
         shape=s,
         dtype=dtype,
@@ -668,8 +674,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
                   for tensor in ret_flat]
     for s, r in zip(state_size, ret_flat):
       r = tf.reshape(r, [-1] + s)
-    ret = tf.contrib.framework.nest.pack_sequence_as(
-        structure=[1, 1], flat_sequence=ret_flat)
+    ret = tf.nest.pack_sequence_as(structure=[1, 1], flat_sequence=ret_flat)
     return ret
 
   def pre_bottleneck(self, inputs, state, input_index):
@@ -684,6 +689,7 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
       state: 4D Tensor with shape [batch_size x width x height x state_size].
       input_index: integer index indicating which base features the inputs
         correspoding to.
+
     Returns:
       inputs: pre-bottlenecked inputs.
     Raises:
@@ -696,12 +702,13 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
     # list.
     if not self._pre_bottleneck:
       raise ValueError('Only applied when pre_bottleneck is set to true.')
-    if not len(inputs.shape) == 4:
+    if len(inputs.shape) != 4:
       raise ValueError('Expect a rank 4 feature tensor.')
-    if not self._flatten_state and not len(state.shape) == 4:
+    if not self._flatten_state and len(state.shape) != 4:
       raise ValueError('Expect rank 4 state tensor.')
-    if self._flatten_state and not len(state.shape) == 2:
+    if self._flatten_state and len(state.shape) != 2:
       raise ValueError('Expect rank 2 state tensor when flatten_state is set.')
+
     with tf.name_scope(None):
       state = tf.identity(state, name='raw_inputs/init_lstm_h')
     if self._flatten_state:
@@ -739,6 +746,5 @@ class GroupedConvLSTMCell(tf.contrib.rnn.RNNCell):
       # For exporting inference graph, we only mark the first timestep.
       with tf.name_scope(None):
         inputs = tf.identity(
-            inputs,
-            name='raw_outputs/base_endpoint_%d' % (input_index + 1))
+            inputs, name='raw_outputs/base_endpoint_%d' % (input_index + 1))
     return inputs
diff --git a/research/lstm_object_detection/lstm/rnn_decoder_test.py b/research/lstm_object_detection/lstm/rnn_decoder_test.py
index b2e229cb..07c6b6b8 100644
--- a/research/lstm_object_detection/lstm/rnn_decoder_test.py
+++ b/research/lstm_object_detection/lstm/rnn_decoder_test.py
@@ -19,13 +19,15 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
 import numpy as np
+import tensorflow as tf
 
+from tensorflow.contrib import layers as contrib_layers
+from tensorflow.contrib import rnn as contrib_rnn
 from lstm_object_detection.lstm import rnn_decoder
 
 
-class MockRnnCell(tf.contrib.rnn.RNNCell):
+class MockRnnCell(contrib_rnn.RNNCell):
 
   def __init__(self, input_size, num_units):
     self._input_size = input_size
@@ -45,7 +47,7 @@ class MockRnnCell(tf.contrib.rnn.RNNCell):
 
   def pre_bottleneck(self, inputs, state, input_index):
     with tf.variable_scope('bottleneck_%d' % input_index, reuse=tf.AUTO_REUSE):
-      inputs = tf.contrib.layers.separable_conv2d(
+      inputs = contrib_layers.separable_conv2d(
           tf.concat([inputs, state], 3),
           self._input_size,
           self._filter_size,
@@ -177,8 +179,6 @@ class MultiInputRnnDecoderTest(tf.test.TestCase):
           (outputs, states, inputs_large, inputs_small, initial_state))
       outputs_results = results[0]
       states_results = results[1]
-      inputs_large_results = results[2]
-      inputs_small_results = results[3]
       initial_states_results = results[4]
       self.assertEqual(
           outputs_results[0].shape,
@@ -189,7 +189,7 @@ class MultiInputRnnDecoderTest(tf.test.TestCase):
                        (batch_size, width, height, num_units))
       # The first step should always update state.
       self.assertAllEqual(states_results[0][0],
-                              np.multiply(initial_states_results[0], 2))
+                          np.multiply(initial_states_results[0], 2))
       self.assertAllEqual(states_results[0][1], initial_states_results[1])
 
   def test_rnn_decoder_multiple_unroll(self):
@@ -227,8 +227,6 @@ class MultiInputRnnDecoderTest(tf.test.TestCase):
           (outputs, states, inputs_large, inputs_small, initial_state))
       outputs_results = results[0]
       states_results = results[1]
-      inputs_large_results = results[2]
-      inputs_small_results = results[3]
       initial_states_results = results[4]
 
       # The first step should always update state.
@@ -281,8 +279,6 @@ class MultiInputRnnDecoderTest(tf.test.TestCase):
           (outputs, states, inputs_large, inputs_small, initial_state))
       outputs_results = results[0]
       states_results = results[1]
-      inputs_large_results = results[2]
-      inputs_small_results = results[3]
       initial_states_results = results[4]
 
       for i in range(num_unroll):
diff --git a/research/lstm_object_detection/lstm/utils.py b/research/lstm_object_detection/lstm/utils.py
index ef2cf27f..b6a1651f 100644
--- a/research/lstm_object_detection/lstm/utils.py
+++ b/research/lstm_object_detection/lstm/utils.py
@@ -19,6 +19,8 @@ from __future__ import absolute_import
 from __future__ import division
 
 import tensorflow as tf
+from tensorflow.contrib import framework as contrib_framework
+from tensorflow.contrib import layers as contrib_layers
 from tensorflow.python.training import moving_averages
 
 
@@ -28,7 +30,7 @@ def _quant_var(
     vars_collection=tf.GraphKeys.MOVING_AVERAGE_VARIABLES,
 ):
   """Create an var for storing the min/max quantization range."""
-  return tf.contrib.framework.model_variable(
+  return contrib_framework.model_variable(
       name,
       shape=[],
       initializer=tf.constant_initializer(initializer_val),
@@ -80,16 +82,21 @@ def quantizable_concat(inputs,
       else:
         concat_tensors = tf.concat(inputs, axis=axis)
         tf.logging.info('concat_tensors: {}'.format(concat_tensors))
+        # TFLite requires that 0.0 is always in the [min; max] range.
+        range_min = tf.minimum(
+            tf.reduce_min(concat_tensors), 0.0, name='SafeQuantRangeMin')
+        range_max = tf.maximum(
+            tf.reduce_max(concat_tensors), 0.0, name='SafeQuantRangeMax')
         # Otherwise we need to keep track of the moving averages of the min and
         # of the elements of the input tensor max.
         min_val = moving_averages.assign_moving_average(
             min_var,
-            tf.reduce_min(concat_tensors),
+            range_min,
             ema_decay,
             name='AssignMinEma')
         max_val = moving_averages.assign_moving_average(
             max_var,
-            tf.reduce_max(concat_tensors),
+            range_max,
             ema_decay,
             name='AssignMaxEma')
         tf.logging.info('min_val: {}'.format(min_val))
@@ -144,7 +151,7 @@ def quantizable_separable_conv2d(inputs,
     Tensor resulting from concatenation of input tensors
   """
   if is_quantized:
-    outputs = tf.contrib.layers.separable_conv2d(
+    outputs = contrib_layers.separable_conv2d(
         inputs,
         None,
         kernel_size,
@@ -154,9 +161,9 @@ def quantizable_separable_conv2d(inputs,
         normalizer_fn=None,
         biases_initializer=None,
         scope=scope)
-    outputs = tf.contrib.layers.bias_add(
+    outputs = contrib_layers.bias_add(
         outputs, trainable=True, scope='%s_bias' % scope)
-    outputs = tf.contrib.layers.conv2d(
+    outputs = contrib_layers.conv2d(
         outputs,
         num_outputs, [1, 1],
         activation_fn=activation_fn,
@@ -164,7 +171,7 @@ def quantizable_separable_conv2d(inputs,
         normalizer_fn=normalizer_fn,
         scope=scope)
   else:
-    outputs = tf.contrib.layers.separable_conv2d(
+    outputs = contrib_layers.separable_conv2d(
         inputs,
         num_outputs,
         kernel_size,
@@ -202,10 +209,13 @@ def quantize_op(inputs,
       min_var = _quant_var('min', default_min)
       max_var = _quant_var('max', default_max)
       if is_training:
+        # TFLite requires that 0.0 is always in the [min; max] range.
+        range_min = tf.minimum(tf.reduce_min(inputs), 0.0, 'SafeQuantRangeMin')
+        range_max = tf.maximum(tf.reduce_max(inputs), 0.0, 'SafeQuantRangeMax')
         min_val = moving_averages.assign_moving_average(
-            min_var, tf.reduce_min(inputs), ema_decay, name='AssignMinEma')
+            min_var, range_min, ema_decay, name='AssignMinEma')
         max_val = moving_averages.assign_moving_average(
-            max_var, tf.reduce_max(inputs), ema_decay, name='AssignMaxEma')
+            max_var, range_max, ema_decay, name='AssignMaxEma')
         inputs = tf.fake_quant_with_min_max_vars(inputs, min_val, max_val)
       else:
         inputs = tf.fake_quant_with_min_max_vars(inputs, min_var, max_var)
diff --git a/research/lstm_object_detection/lstm/utils_test.py b/research/lstm_object_detection/lstm/utils_test.py
index ccdc4c5d..e0a1a0ca 100644
--- a/research/lstm_object_detection/lstm/utils_test.py
+++ b/research/lstm_object_detection/lstm/utils_test.py
@@ -89,6 +89,8 @@ class QuantizableUtilsTest(tf.test.TestCase):
     op_names = [op.name for op in graph.get_operations()]
     self.assertTrue(any('AssignMinEma' in name for name in op_names))
     self.assertTrue(any('AssignMaxEma' in name for name in op_names))
+    self.assertTrue(any('SafeQuantRangeMin' in name for name in op_names))
+    self.assertTrue(any('SafeQuantRangeMax' in name for name in op_names))
 
   def _check_no_min_max_vars(self, graph):
     op_types = [op.type for op in graph.get_operations()]
@@ -99,6 +101,8 @@ class QuantizableUtilsTest(tf.test.TestCase):
     op_names = [op.name for op in graph.get_operations()]
     self.assertFalse(any('AssignMinEma' in name for name in op_names))
     self.assertFalse(any('AssignMaxEma' in name for name in op_names))
+    self.assertFalse(any('SafeQuantRangeMin' in name for name in op_names))
+    self.assertFalse(any('SafeQuantRangeMax' in name for name in op_names))
 
 
 class QuantizableSeparableConv2dTest(tf.test.TestCase):
diff --git a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
index 1c88ad6e..dc85401e 100644
--- a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
+++ b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
@@ -16,6 +16,7 @@
 """LSTDInterleavedFeatureExtractor which interleaves multiple MobileNet V2."""
 
 import tensorflow as tf
+from tensorflow.contrib import slim
 
 from tensorflow.python.framework import ops as tf_ops
 from lstm_object_detection.lstm import lstm_cells
@@ -28,8 +29,6 @@ from object_detection.utils import shape_utils
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
-slim = tf.contrib.slim
-
 
 class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
     lstm_ssd_meta_arch.LSTMSSDInterleavedFeatureExtractor):
diff --git a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
index 12fe8a5c..3d987ae8 100644
--- a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
+++ b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
@@ -18,12 +18,12 @@
 import itertools
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim
+from tensorflow.contrib import training as contrib_training
 
 from lstm_object_detection.models import lstm_ssd_interleaved_mobilenet_v2_feature_extractor
 from object_detection.models import ssd_feature_extractor_test
 
-slim = tf.contrib.slim
-
 
 class LSTMSSDInterleavedMobilenetV2FeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
@@ -233,13 +233,23 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractorTest(
         'lstm_state_step': tf.zeros([1])
     }
     seq = {'dummy': tf.random_uniform([2, 1, 1, 1])}
-    stateful_reader1 = tf.contrib.training.SequenceQueueingStateSaver(
-        batch_size=1, num_unroll=1, input_length=2, input_key='',
-        input_sequences=seq, input_context={}, initial_states=init_state1,
+    stateful_reader1 = contrib_training.SequenceQueueingStateSaver(
+        batch_size=1,
+        num_unroll=1,
+        input_length=2,
+        input_key='',
+        input_sequences=seq,
+        input_context={},
+        initial_states=init_state1,
         capacity=1)
-    stateful_reader2 = tf.contrib.training.SequenceQueueingStateSaver(
-        batch_size=1, num_unroll=1, input_length=2, input_key='',
-        input_sequences=seq, input_context={}, initial_states=init_state2,
+    stateful_reader2 = contrib_training.SequenceQueueingStateSaver(
+        batch_size=1,
+        num_unroll=1,
+        input_length=2,
+        input_key='',
+        input_sequences=seq,
+        input_context={},
+        initial_states=init_state2,
         capacity=1)
     image = tf.random_uniform([1, image_height, image_width, 3])
     feature_extractor = self._create_feature_extractor(depth_multiplier,
diff --git a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
index 32c72ccb..2f097dee 100644
--- a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
@@ -16,6 +16,7 @@
 """LSTMSSDFeatureExtractor for MobilenetV1 features."""
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 from tensorflow.python.framework import ops as tf_ops
 from lstm_object_detection.lstm import lstm_cells
 from lstm_object_detection.lstm import rnn_decoder
@@ -26,7 +27,7 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class LSTMSSDMobileNetV1FeatureExtractor(
diff --git a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
index 088993c3..dfb67f12 100644
--- a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
@@ -17,11 +17,13 @@
 
 import numpy as np
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
+from tensorflow.contrib import training as contrib_training
 
 from lstm_object_detection.models import lstm_ssd_mobilenet_v1_feature_extractor as feature_extactor
 from object_detection.models import ssd_feature_extractor_test
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 class LstmSsdMobilenetV1FeatureExtractorTest(
@@ -111,7 +113,7 @@ class LstmSsdMobilenetV1FeatureExtractorTest(
         'lstm_state_step': tf.zeros([1])
     }
     seq = {'test': tf.random_uniform([3, 1, 1, 1])}
-    stateful_reader = tf.contrib.training.SequenceQueueingStateSaver(
+    stateful_reader = contrib_training.SequenceQueueingStateSaver(
         batch_size=1,
         num_unroll=1,
         input_length=2,
diff --git a/research/lstm_object_detection/models/mobilenet_defs.py b/research/lstm_object_detection/models/mobilenet_defs.py
index 34d7138d..6b9e963a 100644
--- a/research/lstm_object_detection/models/mobilenet_defs.py
+++ b/research/lstm_object_detection/models/mobilenet_defs.py
@@ -15,12 +15,13 @@
 """Definitions for modified MobileNet models used in LSTD."""
 
 import tensorflow as tf
+from tensorflow.contrib import slim as contrib_slim
 
 from nets import mobilenet_v1
 from nets.mobilenet import conv_blocks as mobilenet_convs
 from nets.mobilenet import mobilenet
 
-slim = tf.contrib.slim
+slim = contrib_slim
 
 
 def mobilenet_v1_lite_def(depth_multiplier, low_res=False):
