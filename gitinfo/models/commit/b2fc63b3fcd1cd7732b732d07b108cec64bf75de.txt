commit b2fc63b3fcd1cd7732b732d07b108cec64bf75de
Author: Edouard Fouch√© <edouard.fouche@hotmail.fr>
Date:   Sat May 6 17:26:55 2017 +0200

    use six.moves.range instead of range

diff --git a/lm_1b/lm_1b_eval.py b/lm_1b/lm_1b_eval.py
index 4d1a7c20..150ab6ca 100644
--- a/lm_1b/lm_1b_eval.py
+++ b/lm_1b/lm_1b_eval.py
@@ -17,6 +17,7 @@
 """
 import os
 import sys
+import six
 
 import numpy as np
 import tensorflow as tf
@@ -177,7 +178,7 @@ def _SampleModel(prefix_words, vocab):
 
   prefix = [vocab.word_to_id(w) for w in prefix_words.split()]
   prefix_char_ids = [vocab.word_to_char_ids(w) for w in prefix_words.split()]
-  for _ in range(FLAGS.num_samples):
+  for _ in six.moves.range(FLAGS.num_samples):
     inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)
     char_ids_inputs = np.zeros(
         [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)
@@ -230,7 +231,7 @@ def _DumpEmb(vocab):
   sys.stderr.write('Finished softmax weights\n')
 
   all_embs = np.zeros([vocab.size, 1024])
-  for i in range(vocab.size):
+  for i in six.moves.range(vocab.size):
     input_dict = {t['inputs_in']: inputs,
                   t['targets_in']: targets,
                   t['target_weights_in']: weights}
@@ -269,7 +270,7 @@ def _DumpSentenceEmbedding(sentence, vocab):
   inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)
   char_ids_inputs = np.zeros(
       [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)
-  for i in range(len(word_ids)):
+  for i in six.moves.range(len(word_ids)):
     inputs[0, 0] = word_ids[i]
     char_ids_inputs[0, 0, :] = char_ids[i]
 
