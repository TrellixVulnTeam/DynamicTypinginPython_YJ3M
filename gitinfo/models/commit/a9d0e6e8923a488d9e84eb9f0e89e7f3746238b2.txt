commit a9d0e6e8923a488d9e84eb9f0e89e7f3746238b2
Author: Neal Wu <neal@nealwu.com>
Date:   Tue Feb 21 17:56:43 2017 -0800

    Converted slim models to TF 1.0 and fixed a few issues

diff --git a/slim/deployment/model_deploy.py b/slim/deployment/model_deploy.py
index 8e56e24c..3b45a573 100644
--- a/slim/deployment/model_deploy.py
+++ b/slim/deployment/model_deploy.py
@@ -232,11 +232,9 @@ def _gather_clone_loss(clone, num_clones, regularization_losses):
       sum_loss = tf.add_n(all_losses)
   # Add the summaries out of the clone device block.
   if clone_loss is not None:
-    tf.scalar_summary(clone.scope + '/clone_loss', clone_loss,
-                      name='clone_loss')
+    tf.summary.scalar('clone_loss', clone_loss)
   if regularization_loss is not None:
-    tf.scalar_summary('regularization_loss', regularization_loss,
-                      name='regularization_loss')
+    tf.summary.scalar('regularization_loss', regularization_loss)
   return sum_loss
 
 
@@ -404,12 +402,11 @@ def deploy(config,
 
     if total_loss is not None:
       # Add total_loss to summary.
-      summaries.add(tf.scalar_summary('total_loss', total_loss,
-                                      name='total_loss'))
+      summaries.add(tf.summary.scalar('total_loss', total_loss))
 
     if summaries:
       # Merge all summaries together.
-      summary_op = tf.merge_summary(list(summaries), name='summary_op')
+      summary_op = tf.summary.merge(list(summaries), name='summary_op')
     else:
       summary_op = None
 
@@ -467,9 +464,9 @@ def _add_gradients_summaries(grads_and_vars):
         grad_values = grad.values
       else:
         grad_values = grad
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient',
                                             grad_values))
-      summaries.append(tf.histogram_summary(var.op.name + ':gradient_norm',
+      summaries.append(tf.summary.histogram(var.op.name + ':gradient_norm',
                                             tf.global_norm([grad_values])))
     else:
       tf.logging.info('Var %s has no gradient', var.op.name)
diff --git a/slim/eval_image_classifier.py b/slim/eval_image_classifier.py
index e5b923e1..f4d3f6ad 100644
--- a/slim/eval_image_classifier.py
+++ b/slim/eval_image_classifier.py
@@ -160,7 +160,7 @@ def main(_):
     # Print the summaries to screen.
     for name, value in names_to_values.iteritems():
       summary_name = 'eval/%s' % name
-      op = tf.scalar_summary(summary_name, value, collections=[])
+      op = tf.summary.scalar(summary_name, value, collections=[])
       op = tf.Print(op, [value], summary_name)
       tf.add_to_collection(tf.GraphKeys.SUMMARIES, op)
 
diff --git a/slim/nets/alexnet.py b/slim/nets/alexnet.py
index a6b93de0..4e7e563c 100644
--- a/slim/nets/alexnet.py
+++ b/slim/nets/alexnet.py
@@ -113,7 +113,7 @@ def alexnet_v2(inputs,
         net = slim.conv2d(net, num_classes, [1, 1],
                           activation_fn=None,
                           normalizer_fn=None,
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
                           scope='fc8')
 
       # Convert end_points_collection into a end_point dict.
diff --git a/slim/nets/cifarnet.py b/slim/nets/cifarnet.py
index 371a9cbf..44ca0fed 100644
--- a/slim/nets/cifarnet.py
+++ b/slim/nets/cifarnet.py
@@ -77,7 +77,7 @@ def cifarnet(images, num_classes=10, is_training=False,
     net = slim.fully_connected(net, 192, scope='fc4')
     end_points['fc4'] = net
     logits = slim.fully_connected(net, num_classes,
-                                  biases_initializer=tf.zeros_initializer,
+                                  biases_initializer=tf.zeros_initializer(),
                                   weights_initializer=trunc_normal(1/192.0),
                                   weights_regularizer=None,
                                   activation_fn=None,
diff --git a/slim/nets/inception_v1.py b/slim/nets/inception_v1.py
index 8f644796..e1ed3147 100644
--- a/slim/nets/inception_v1.py
+++ b/slim/nets/inception_v1.py
@@ -93,7 +93,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -110,7 +110,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -132,7 +132,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -149,7 +149,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -166,7 +166,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -183,7 +183,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -200,7 +200,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -222,7 +222,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
 
@@ -239,7 +239,7 @@ def inception_v1_base(inputs,
           with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
-          net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+          net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if final_endpoint == end_point: return net, end_points
     raise ValueError('Unknown final endpoint %s' % final_endpoint)
diff --git a/slim/nets/inception_v2.py b/slim/nets/inception_v2.py
index 6c9f1009..2d75ba97 100644
--- a/slim/nets/inception_v2.py
+++ b/slim/nets/inception_v2.py
@@ -145,7 +145,7 @@ def inception_v2_base(inputs,
               branch_3, depth(32), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 256
@@ -175,7 +175,7 @@ def inception_v2_base(inputs,
               branch_3, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 320
@@ -200,7 +200,7 @@ def inception_v2_base(inputs,
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(
               net, [3, 3], stride=2, scope='MaxPool_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
@@ -230,7 +230,7 @@ def inception_v2_base(inputs,
               branch_3, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
@@ -260,7 +260,7 @@ def inception_v2_base(inputs,
               branch_3, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
@@ -290,7 +290,7 @@ def inception_v2_base(inputs,
               branch_3, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
 
@@ -321,7 +321,7 @@ def inception_v2_base(inputs,
               branch_3, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
@@ -346,7 +346,7 @@ def inception_v2_base(inputs,
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2,
                                      scope='MaxPool_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
       # 7 x 7 x 1024
@@ -376,7 +376,7 @@ def inception_v2_base(inputs,
               branch_3, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
 
@@ -407,7 +407,7 @@ def inception_v2_base(inputs,
               branch_3, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.1),
               scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
         end_points[end_point] = net
         if end_point == final_endpoint: return net, end_points
     raise ValueError('Unknown final endpoint %s' % final_endpoint)
diff --git a/slim/nets/inception_v3.py b/slim/nets/inception_v3.py
index 5c5f9651..2e5e5f3e 100644
--- a/slim/nets/inception_v3.py
+++ b/slim/nets/inception_v3.py
@@ -158,7 +158,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -182,7 +182,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -205,7 +205,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -224,7 +224,7 @@ def inception_v3_base(inputs,
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -253,7 +253,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -282,7 +282,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
       # mixed_6: 17 x 17 x 768.
@@ -310,7 +310,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -339,7 +339,7 @@ def inception_v3_base(inputs,
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -361,7 +361,7 @@ def inception_v3_base(inputs,
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1, branch_2])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
       # mixed_9: 8 x 8 x 2048.
@@ -371,21 +371,21 @@ def inception_v3_base(inputs,
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
         with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0b_3x1')])
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
         with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
 
@@ -396,21 +396,21 @@ def inception_v3_base(inputs,
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
         with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
-          branch_1 = tf.concat(3, [
+          branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0c_3x1')])
         with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
-          branch_2 = tf.concat(3, [
+          branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
         with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
-        net = tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+        net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
       end_points[end_point] = net
       if end_point == final_endpoint: return net, end_points
     raise ValueError('Unknown final endpoint %s' % final_endpoint)
diff --git a/slim/nets/inception_v4.py b/slim/nets/inception_v4.py
index 0c581f7c..b706d5b7 100644
--- a/slim/nets/inception_v4.py
+++ b/slim/nets/inception_v4.py
@@ -49,7 +49,7 @@ def block_inception_a(inputs, scope=None, reuse=None):
       with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
 
 
 def block_reduction_a(inputs, scope=None, reuse=None):
@@ -69,7 +69,7 @@ def block_reduction_a(inputs, scope=None, reuse=None):
       with tf.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
 
 
 def block_inception_b(inputs, scope=None, reuse=None):
@@ -93,7 +93,7 @@ def block_inception_b(inputs, scope=None, reuse=None):
       with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
 
 
 def block_reduction_b(inputs, scope=None, reuse=None):
@@ -115,7 +115,7 @@ def block_reduction_b(inputs, scope=None, reuse=None):
       with tf.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
-      return tf.concat(3, [branch_0, branch_1, branch_2])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
 
 
 def block_inception_c(inputs, scope=None, reuse=None):
@@ -128,20 +128,20 @@ def block_inception_c(inputs, scope=None, reuse=None):
         branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
       with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
-        branch_1 = tf.concat(3, [
+        branch_1 = tf.concat(axis=3, values=[
             slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'),
             slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])
       with tf.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')
         branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')
-        branch_2 = tf.concat(3, [
+        branch_2 = tf.concat(axis=3, values=[
             slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'),
             slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])
       with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')
-      return tf.concat(3, [branch_0, branch_1, branch_2, branch_3])
+      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
 
 
 def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
@@ -192,7 +192,7 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
         with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID',
                                  scope='Conv2d_0a_3x3')
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
         if add_and_check_final('Mixed_3a', net): return net, end_points
 
       # 73 x 73 x 160
@@ -207,7 +207,7 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
           branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')
           branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding='VALID',
                                  scope='Conv2d_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
         if add_and_check_final('Mixed_4a', net): return net, end_points
 
       # 71 x 71 x 192
@@ -218,7 +218,7 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
         with tf.variable_scope('Branch_1'):
           branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
-        net = tf.concat(3, [branch_0, branch_1])
+        net = tf.concat(axis=3, values=[branch_0, branch_1])
         if add_and_check_final('Mixed_5a', net): return net, end_points
 
       # 35 x 35 x 384
diff --git a/slim/nets/overfeat.py b/slim/nets/overfeat.py
index 0c8f45ce..64a54252 100644
--- a/slim/nets/overfeat.py
+++ b/slim/nets/overfeat.py
@@ -41,7 +41,7 @@ def overfeat_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME'):
       with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
         return arg_sc
@@ -107,7 +107,7 @@ def overfeat(inputs,
         net = slim.conv2d(net, num_classes, [1, 1],
                           activation_fn=None,
                           normalizer_fn=None,
-                          biases_initializer=tf.zeros_initializer,
+                          biases_initializer=tf.zeros_initializer(),
                           scope='fc8')
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
diff --git a/slim/nets/resnet_v2.py b/slim/nets/resnet_v2.py
index 9476db24..b9d28c22 100644
--- a/slim/nets/resnet_v2.py
+++ b/slim/nets/resnet_v2.py
@@ -234,7 +234,7 @@ def resnet_v2_50(inputs,
   return resnet_v2(inputs, blocks, num_classes, is_training=is_training,
                    global_pool=global_pool, output_stride=output_stride,
                    include_root_block=True, reuse=reuse, scope=scope)
-
+resnet_v2_50.default_image_size = 224
 
 def resnet_v2_101(inputs,
                   num_classes=None,
@@ -256,6 +256,7 @@ def resnet_v2_101(inputs,
   return resnet_v2(inputs, blocks, num_classes, is_training=is_training,
                    global_pool=global_pool, output_stride=output_stride,
                    include_root_block=True, reuse=reuse, scope=scope)
+resnet_v2_101.default_image_size = 224
 
 
 def resnet_v2_152(inputs,
@@ -278,6 +279,7 @@ def resnet_v2_152(inputs,
   return resnet_v2(inputs, blocks, num_classes, is_training=is_training,
                    global_pool=global_pool, output_stride=output_stride,
                    include_root_block=True, reuse=reuse, scope=scope)
+resnet_v2_152.default_image_size = 224
 
 
 def resnet_v2_200(inputs,
@@ -300,3 +302,4 @@ def resnet_v2_200(inputs,
   return resnet_v2(inputs, blocks, num_classes, is_training=is_training,
                    global_pool=global_pool, output_stride=output_stride,
                    include_root_block=True, reuse=reuse, scope=scope)
+resnet_v2_200.default_image_size = 224
diff --git a/slim/nets/vgg.py b/slim/nets/vgg.py
index c9a66e1b..7de28062 100644
--- a/slim/nets/vgg.py
+++ b/slim/nets/vgg.py
@@ -58,7 +58,7 @@ def vgg_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.zeros_initializer):
+                      biases_initializer=tf.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:
       return arg_sc
 
diff --git a/slim/preprocessing/cifarnet_preprocessing.py b/slim/preprocessing/cifarnet_preprocessing.py
index 1057d462..195a5c7d 100644
--- a/slim/preprocessing/cifarnet_preprocessing.py
+++ b/slim/preprocessing/cifarnet_preprocessing.py
@@ -45,7 +45,7 @@ def preprocess_for_train(image,
   Returns:
     A preprocessed image.
   """
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
 
   # Transform the image to floats.
   image = tf.to_float(image)
@@ -58,7 +58,7 @@ def preprocess_for_train(image,
   # Randomly flip the image horizontally.
   distorted_image = tf.image.random_flip_left_right(distorted_image)
 
-  tf.image_summary('distorted_image', tf.expand_dims(distorted_image, 0))
+  tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
 
   # Because these operations are not commutative, consider randomizing
   # the order their operation.
@@ -67,7 +67,7 @@ def preprocess_for_train(image,
   distorted_image = tf.image.random_contrast(distorted_image,
                                              lower=0.2, upper=1.8)
   # Subtract off the mean and divide by the variance of the pixels.
-  return tf.image.per_image_whitening(distorted_image)
+  return tf.image.per_image_standardization(distorted_image)
 
 
 def preprocess_for_eval(image, output_height, output_width):
@@ -81,7 +81,7 @@ def preprocess_for_eval(image, output_height, output_width):
   Returns:
     A preprocessed image.
   """
-  tf.image_summary('image', tf.expand_dims(image, 0))
+  tf.summary.image('image', tf.expand_dims(image, 0))
   # Transform the image to floats.
   image = tf.to_float(image)
 
@@ -89,10 +89,10 @@ def preprocess_for_eval(image, output_height, output_width):
   resized_image = tf.image.resize_image_with_crop_or_pad(image,
                                                          output_width,
                                                          output_height)
-  tf.image_summary('resized_image', tf.expand_dims(resized_image, 0))
+  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
 
   # Subtract off the mean and divide by the variance of the pixels.
-  return tf.image.per_image_whitening(resized_image)
+  return tf.image.per_image_standardization(resized_image)
 
 
 def preprocess_image(image, output_height, output_width, is_training=False):
diff --git a/slim/preprocessing/inception_preprocessing.py b/slim/preprocessing/inception_preprocessing.py
index 133264b6..ca3eba0b 100644
--- a/slim/preprocessing/inception_preprocessing.py
+++ b/slim/preprocessing/inception_preprocessing.py
@@ -192,7 +192,7 @@ def preprocess_for_train(image, height, width, bbox,
     # the coordinates are ordered [ymin, xmin, ymax, xmax].
     image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                   bbox)
-    tf.image_summary('image_with_bounding_boxes', image_with_box)
+    tf.summary.image('image_with_bounding_boxes', image_with_box)
 
     distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)
     # Restore the shape since the dynamic slice based upon the bbox_size loses
@@ -200,7 +200,7 @@ def preprocess_for_train(image, height, width, bbox,
     distorted_image.set_shape([None, None, 3])
     image_with_distorted_box = tf.image.draw_bounding_boxes(
         tf.expand_dims(image, 0), distorted_bbox)
-    tf.image_summary('images_with_distorted_bounding_box',
+    tf.summary.image('images_with_distorted_bounding_box',
                      image_with_distorted_box)
 
     # This resizing operation may distort the images because the aspect
@@ -215,7 +215,7 @@ def preprocess_for_train(image, height, width, bbox,
         lambda x, method: tf.image.resize_images(x, [height, width], method=method),
         num_cases=num_resize_cases)
 
-    tf.image_summary('cropped_resized_image',
+    tf.summary.image('cropped_resized_image',
                      tf.expand_dims(distorted_image, 0))
 
     # Randomly flip the image horizontally.
@@ -227,10 +227,10 @@ def preprocess_for_train(image, height, width, bbox,
         lambda x, ordering: distort_color(x, ordering, fast_mode),
         num_cases=4)
 
-    tf.image_summary('final_distorted_image',
+    tf.summary.image('final_distorted_image',
                      tf.expand_dims(distorted_image, 0))
-    distorted_image = tf.sub(distorted_image, 0.5)
-    distorted_image = tf.mul(distorted_image, 2.0)
+    distorted_image = tf.subtract(distorted_image, 0.5)
+    distorted_image = tf.multiply(distorted_image, 2.0)
     return distorted_image
 
 
@@ -270,8 +270,8 @@ def preprocess_for_eval(image, height, width,
       image = tf.image.resize_bilinear(image, [height, width],
                                        align_corners=False)
       image = tf.squeeze(image, [0])
-    image = tf.sub(image, 0.5)
-    image = tf.mul(image, 2.0)
+    image = tf.subtract(image, 0.5)
+    image = tf.multiply(image, 2.0)
     return image
 
 
diff --git a/slim/preprocessing/lenet_preprocessing.py b/slim/preprocessing/lenet_preprocessing.py
index 22c352a2..ac5e71af 100644
--- a/slim/preprocessing/lenet_preprocessing.py
+++ b/slim/preprocessing/lenet_preprocessing.py
@@ -39,6 +39,6 @@ def preprocess_image(image, output_height, output_width, is_training):
   image = tf.to_float(image)
   image = tf.image.resize_image_with_crop_or_pad(
       image, output_width, output_height)
-  image = tf.sub(image, 128.0)
+  image = tf.subtract(image, 128.0)
   image = tf.div(image, 128.0)
   return image
diff --git a/slim/preprocessing/preprocessing_factory.py b/slim/preprocessing/preprocessing_factory.py
index 89d3d962..35f8645e 100644
--- a/slim/preprocessing/preprocessing_factory.py
+++ b/slim/preprocessing/preprocessing_factory.py
@@ -56,6 +56,9 @@ def get_preprocessing(name, is_training=False):
       'resnet_v1_50': vgg_preprocessing,
       'resnet_v1_101': vgg_preprocessing,
       'resnet_v1_152': vgg_preprocessing,
+      'resnet_v2_50': vgg_preprocessing,
+      'resnet_v2_101': vgg_preprocessing,
+      'resnet_v2_152': vgg_preprocessing,
       'vgg': vgg_preprocessing,
       'vgg_a': vgg_preprocessing,
       'vgg_16': vgg_preprocessing,
diff --git a/slim/preprocessing/vgg_preprocessing.py b/slim/preprocessing/vgg_preprocessing.py
index 672c7408..1900cae2 100644
--- a/slim/preprocessing/vgg_preprocessing.py
+++ b/slim/preprocessing/vgg_preprocessing.py
@@ -73,7 +73,7 @@ def _crop(image, offset_height, offset_width, crop_height, crop_width):
       ['Rank of image must be equal to 3.'])
   cropped_shape = control_flow_ops.with_dependencies(
       [rank_assertion],
-      tf.pack([crop_height, crop_width, original_shape[2]]))
+      tf.stack([crop_height, crop_width, original_shape[2]]))
 
   size_assertion = tf.Assert(
       tf.logical_and(
@@ -81,7 +81,7 @@ def _crop(image, offset_height, offset_width, crop_height, crop_width):
           tf.greater_equal(original_shape[1], crop_width)),
       ['Crop size greater than the image size.'])
 
-  offsets = tf.to_int32(tf.pack([offset_height, offset_width, 0]))
+  offsets = tf.to_int32(tf.stack([offset_height, offset_width, 0]))
 
   # Use tf.slice instead of crop_to_bounding box as it accepts tensors to
   # define the crop size.
@@ -227,10 +227,10 @@ def _mean_image_subtraction(image, means):
   if len(means) != num_channels:
     raise ValueError('len(means) must match the number of channels')
 
-  channels = tf.split(2, num_channels, image)
+  channels = tf.split(axis=2, num_or_size_splits=num_channels, value=image)
   for i in range(num_channels):
     channels[i] -= means[i]
-  return tf.concat(2, channels)
+  return tf.concat(axis=2, values=channels)
 
 
 def _smallest_size_at_least(height, width, smallest_side):
diff --git a/slim/slim_walkthough.ipynb b/slim/slim_walkthough.ipynb
index 06809b81..cd52c482 100644
--- a/slim/slim_walkthough.ipynb
+++ b/slim/slim_walkthough.ipynb
@@ -676,7 +676,7 @@
     "    total_loss = slim.losses.get_total_loss()\n",
     "\n",
     "    # Create some summaries to visualize the training process:\n",
-    "    tf.scalar_summary('losses/Total Loss', total_loss)\n",
+    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
     "  \n",
     "    # Specify the optimizer and create the train op:\n",
     "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
@@ -1015,7 +1015,7 @@
     "    total_loss = slim.losses.get_total_loss()\n",
     "\n",
     "    # Create some summaries to visualize the training process:\n",
-    "    tf.scalar_summary('losses/Total Loss', total_loss)\n",
+    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
     "  \n",
     "    # Specify the optimizer and create the train op:\n",
     "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
diff --git a/slim/train_image_classifier.py b/slim/train_image_classifier.py
index 0e95c602..278463d3 100644
--- a/slim/train_image_classifier.py
+++ b/slim/train_image_classifier.py
@@ -316,8 +316,8 @@ def _configure_optimizer(learning_rate):
 def _add_variables_summaries(learning_rate):
   summaries = []
   for variable in slim.get_model_variables():
-    summaries.append(tf.histogram_summary(variable.op.name, variable))
-  summaries.append(tf.scalar_summary('training/Learning Rate', learning_rate))
+    summaries.append(tf.summary.histogram(variable.op.name, variable))
+  summaries.append(tf.summary.scalar('training/Learning Rate', learning_rate))
   return summaries
 
 
@@ -469,11 +469,11 @@ def main(_):
       # Specify the loss function #
       #############################
       if 'AuxLogits' in end_points:
-        slim.losses.softmax_cross_entropy(
+        tf.losses.softmax_cross_entropy(
             end_points['AuxLogits'], labels,
-            label_smoothing=FLAGS.label_smoothing, weight=0.4, scope='aux_loss')
-      slim.losses.softmax_cross_entropy(
-          logits, labels, label_smoothing=FLAGS.label_smoothing, weight=1.0)
+            label_smoothing=FLAGS.label_smoothing, weights=0.4, scope='aux_loss')
+      tf.losses.softmax_cross_entropy(
+          tf.squeeze(logits), labels, label_smoothing=FLAGS.label_smoothing, weights=1.0)
       return end_points
 
     # Gather initial summaries.
@@ -489,17 +489,17 @@ def main(_):
     end_points = clones[0].outputs
     for end_point in end_points:
       x = end_points[end_point]
-      summaries.add(tf.histogram_summary('activations/' + end_point, x))
-      summaries.add(tf.scalar_summary('sparsity/' + end_point,
+      summaries.add(tf.summary.histogram('activations/' + end_point, x))
+      summaries.add(tf.summary.scalar('sparsity/' + end_point,
                                       tf.nn.zero_fraction(x)))
 
     # Add summaries for losses.
     for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):
-      summaries.add(tf.scalar_summary('losses/%s' % loss.op.name, loss))
+      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))
 
     # Add summaries for variables.
     for variable in slim.get_model_variables():
-      summaries.add(tf.histogram_summary(variable.op.name, variable))
+      summaries.add(tf.summary.histogram(variable.op.name, variable))
 
     #################################
     # Configure the moving averages #
@@ -517,8 +517,7 @@ def main(_):
     with tf.device(deploy_config.optimizer_device()):
       learning_rate = _configure_learning_rate(dataset.num_samples, global_step)
       optimizer = _configure_optimizer(learning_rate)
-      summaries.add(tf.scalar_summary('learning_rate', learning_rate,
-                                      name='learning_rate'))
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
 
     if FLAGS.sync_replicas:
       # If sync_replicas is enabled, the averaging will be done in the chief
@@ -543,8 +542,7 @@ def main(_):
         optimizer,
         var_list=variables_to_train)
     # Add total_loss to summary.
-    summaries.add(tf.scalar_summary('total_loss', total_loss,
-                                    name='total_loss'))
+    summaries.add(tf.summary.scalar('total_loss', total_loss))
 
     # Create gradient updates.
     grad_updates = optimizer.apply_gradients(clones_gradients,
@@ -561,7 +559,7 @@ def main(_):
                                        first_clone_scope))
 
     # Merge all summaries together.
-    summary_op = tf.merge_summary(list(summaries), name='summary_op')
+    summary_op = tf.summary.merge(list(summaries), name='summary_op')
 
 
     ###########################
