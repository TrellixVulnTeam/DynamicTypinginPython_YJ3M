commit cbb624791068590026f50f01d370d9328fe8ebf2
Author: Sergio Guadarrama <sguada@google.com>
Date:   Sat Oct 28 08:04:10 2017 -0700

    Bring tensorflow/models slim up to date.
    
    Includes new gan nets.
    
    Inclues new NasNet models.
    
    PiperOrigin-RevId: 173772097

diff --git a/research/slim/BUILD b/research/slim/BUILD
index ef89c247..79b1e6ed 100644
--- a/research/slim/BUILD
+++ b/research/slim/BUILD
@@ -1,14 +1,14 @@
 # Description:
 #   Contains files for loading, training and evaluating TF-Slim-based models.
 
-package(default_visibility = ["//visibility:public"])
+package(
+    default_visibility = ["//visibility:public"],
+)
 
 licenses(["notice"])  # Apache 2.0
 
 exports_files(["LICENSE"])
 
-package_group(name = "internal")
-
 py_library(
     name = "dataset_utils",
     srcs = ["datasets/dataset_utils.py"],
@@ -35,6 +35,7 @@ py_binary(
     name = "build_imagenet_data",
     srcs = ["datasets/build_imagenet_data.py"],
     deps = [
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -44,6 +45,7 @@ py_library(
     srcs = ["datasets/download_and_convert_cifar10.py"],
     deps = [
         ":dataset_utils",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -62,6 +64,7 @@ py_library(
     srcs = ["datasets/download_and_convert_mnist.py"],
     deps = [
         ":dataset_utils",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -73,6 +76,7 @@ py_binary(
         ":download_and_convert_cifar10",
         ":download_and_convert_flowers",
         ":download_and_convert_mnist",
+        "//tensorflow",
     ],
 )
 
@@ -137,6 +141,7 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":model_deploy",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -154,6 +159,7 @@ py_library(
     srcs = ["preprocessing/inception_preprocessing.py"],
     deps = [
         "//tensorflow",
+        "//tensorflow/python:control_flow_ops",
     ],
 )
 
@@ -192,10 +198,13 @@ py_library(
     deps = [
         ":alexnet",
         ":cifarnet",
+        ":cyclegan",
         ":inception",
         ":lenet",
         ":mobilenet_v1",
+        ":nasnet",
         ":overfeat",
+        ":pix2pix",
         ":resnet_v1",
         ":resnet_v2",
         ":vgg",
@@ -206,6 +215,7 @@ py_library(
     name = "alexnet",
     srcs = ["nets/alexnet.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_test(
@@ -227,6 +237,45 @@ py_library(
     ],
 )
 
+py_library(
+    name = "cyclegan",
+    srcs = ["nets/cyclegan.py"],
+    deps = [
+        # "//numpy",
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "cyclegan_test",
+    srcs = ["nets/cyclegan_test.py"],
+    shard_count = 3,
+    srcs_version = "PY2AND3",
+    deps = [
+        ":cyclegan",
+        "//tensorflow",
+    ],
+)
+
+py_library(
+    name = "dcgan",
+    srcs = ["nets/dcgan.py"],
+    deps = [
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "dcgan_test",
+    srcs = ["nets/dcgan_test.py"],
+    shard_count = 3,
+    srcs_version = "PY2AND3",
+    deps = [
+        ":dcgan",
+        "//tensorflow",
+    ],
+)
+
 py_library(
     name = "inception",
     srcs = ["nets/inception.py"],
@@ -244,6 +293,7 @@ py_library(
     name = "inception_utils",
     srcs = ["nets/inception_utils.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_library(
@@ -252,6 +302,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":inception_utils",
+        "//tensorflow",
     ],
 )
 
@@ -261,6 +312,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":inception_utils",
+        "//tensorflow",
     ],
 )
 
@@ -270,6 +322,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":inception_utils",
+        "//tensorflow",
     ],
 )
 
@@ -279,6 +332,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":inception_utils",
+        "//tensorflow",
     ],
 )
 
@@ -286,6 +340,7 @@ py_library(
     name = "inception_resnet_v2",
     srcs = ["nets/inception_resnet_v2.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_test(
@@ -296,6 +351,7 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -308,6 +364,7 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -320,6 +377,7 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -360,6 +418,7 @@ py_library(
     name = "mobilenet_v1",
     srcs = ["nets/mobilenet_v1.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_test(
@@ -370,6 +429,49 @@ py_test(
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet_v1",
+        # "//numpy",
+        "//tensorflow",
+    ],
+)
+
+py_library(
+    name = "nasnet_utils",
+    srcs = ["nets/nasnet/nasnet_utils.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        "//tensorflow",
+    ],
+)
+
+py_library(
+    name = "nasnet",
+    srcs = ["nets/nasnet/nasnet.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":nasnet_utils",
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "nasnet_utils_test",
+    size = "medium",
+    srcs = ["nets/nasnet/nasnet_utils_test.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":nasnet_utils",
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "nasnet_test",
+    size = "large",
+    srcs = ["nets/nasnet/nasnet_test.py"],
+    shard_count = 10,
+    srcs_version = "PY2AND3",
+    deps = [
+        ":nasnet",
         "//tensorflow",
     ],
 )
@@ -378,6 +480,7 @@ py_library(
     name = "overfeat",
     srcs = ["nets/overfeat.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_test(
@@ -391,10 +494,28 @@ py_test(
     ],
 )
 
+py_library(
+    name = "pix2pix",
+    srcs = ["nets/pix2pix.py"],
+    srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
+)
+
+py_test(
+    name = "pix2pix_test",
+    srcs = ["nets/pix2pix_test.py"],
+    srcs_version = "PY2AND3",
+    deps = [
+        ":pix2pix",
+        "//tensorflow",
+    ],
+)
+
 py_library(
     name = "resnet_utils",
     srcs = ["nets/resnet_utils.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_library(
@@ -403,6 +524,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":resnet_utils",
+        "//tensorflow",
     ],
 )
 
@@ -413,7 +535,9 @@ py_test(
     shard_count = 2,
     srcs_version = "PY2AND3",
     deps = [
+        ":resnet_utils",
         ":resnet_v1",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -424,6 +548,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         ":resnet_utils",
+        "//tensorflow",
     ],
 )
 
@@ -434,7 +559,9 @@ py_test(
     shard_count = 2,
     srcs_version = "PY2AND3",
     deps = [
+        ":resnet_utils",
         ":resnet_v2",
+        # "//numpy",
         "//tensorflow",
     ],
 )
@@ -443,6 +570,7 @@ py_library(
     name = "vgg",
     srcs = ["nets/vgg.py"],
     srcs_version = "PY2AND3",
+    deps = ["//tensorflow"],
 )
 
 py_test(
@@ -494,7 +622,6 @@ py_binary(
     srcs = ["eval_image_classifier.py"],
     deps = [
         ":dataset_factory",
-        ":model_deploy",
         ":nets_factory",
         ":preprocessing_factory",
         "//tensorflow",
@@ -508,6 +635,7 @@ py_binary(
         ":dataset_factory",
         ":nets_factory",
         "//tensorflow",
+        "//tensorflow/python:platform",
     ],
 )
 
@@ -521,7 +649,7 @@ py_test(
     ],
     deps = [
         ":export_inference_graph",
-        ":nets_factory",
         "//tensorflow",
+        "//tensorflow/python:platform",
     ],
 )
diff --git a/research/slim/README.md b/research/slim/README.md
index d0c095f0..c1f99898 100644
--- a/research/slim/README.md
+++ b/research/slim/README.md
@@ -259,12 +259,16 @@ Model | TF-Slim File | Checkpoint | Top-1 Accuracy| Top-5 Accuracy |
 [MobileNet_v1_1.0_224](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py)|[mobilenet_v1_1.0_224_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)|70.7|89.5|
 [MobileNet_v1_0.50_160](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py)|[mobilenet_v1_0.50_160_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.50_160_2017_06_14.tar.gz)|59.9|82.5|
 [MobileNet_v1_0.25_128](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.py)|[mobilenet_v1_0.25_128_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_2017_06_14.tar.gz)|41.3|66.2|
+[NASNet-A_Mobile_224](https://arxiv.org/abs/1707.07012)#|[Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/nasnet.py)|[nasnet-a_mobile_04_10_2017.tar.gz](https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_mobile_04_10_2017.tar.gz)|74.0|91.6|
+[NASNet-A_Large_331](https://arxiv.org/abs/1707.07012)#|[Code](https://github.com/tensorflow/models/blob/master/research/slim/nets/nasnet/nasnet.py)|[nasnet-a_large_04_10_2017.tar.gz](https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_large_04_10_2017.tar.gz)|82.7|96.2|
 
 ^ ResNet V2 models use Inception pre-processing and input image size of 299 (use
 `--preprocessing_name inception --eval_image_size 299` when using
 `eval_image_classifier.py`). Performance numbers for ResNet V2 models are
 reported on the ImageNet validation set.
 
+(#) More information and details about the NASNet architectures are available at this [README](nets/nasnet/README.md)
+
 All 16 MobileNet Models reported in the [MobileNet Paper](https://arxiv.org/abs/1704.04861) can be found [here](https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet_v1.md).
 
 (\*): Results quoted from the [paper](https://arxiv.org/abs/1603.05027).
@@ -393,7 +397,8 @@ $ python eval_image_classifier.py \
     --model_name=inception_v3
 ```
 
-See the [evaluation module example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim#evaluation-loop) for an example of how to evaluate a model at multiple checkpoints during or after the training.
+See the [evaluation module example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim#evaluation-loop)
+for an example of how to evaluate a model at multiple checkpoints during or after the training.
 
 # Exporting the Inference Graph
 <a id='Export'></a>
diff --git a/research/slim/datasets/download_and_convert_imagenet.sh b/research/slim/datasets/download_and_convert_imagenet.sh
index 5a401264..170a2bb0 100755
--- a/research/slim/datasets/download_and_convert_imagenet.sh
+++ b/research/slim/datasets/download_and_convert_imagenet.sh
@@ -45,11 +45,11 @@
 # downloading the raw images.
 #
 # usage:
-#  ./download_and_preprocess_imagenet.sh [data-dir]
+#  ./download_and_convert_imagenet.sh [data-dir]
 set -e
 
 if [ -z "$1" ]; then
-  echo "usage download_and_preprocess_imagenet.sh [data dir]"
+  echo "usage download_and_convert_imagenet.sh [data dir]"
   exit
 fi
 
diff --git a/research/slim/deployment/model_deploy.py b/research/slim/deployment/model_deploy.py
index c6820769..a1245578 100644
--- a/research/slim/deployment/model_deploy.py
+++ b/research/slim/deployment/model_deploy.py
@@ -103,6 +103,8 @@ import collections
 
 import tensorflow as tf
 
+from tensorflow.python.eager import context
+
 slim = tf.contrib.slim
 
 
@@ -342,7 +344,13 @@ def deploy(config,
   Returns:
     A `DeployedModel` namedtuple.
 
+  Raises:
+    RuntimeError: If eager execution is enabled.
   """
+  if context.in_eager_mode():
+    raise RuntimeError(
+        'slim.deploy is not supported when eager execution is enabled.')
+
   # Gather initial summaries.
   summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
 
diff --git a/research/slim/deployment/model_deploy_test.py b/research/slim/deployment/model_deploy_test.py
index 48982eda..780b691d 100644
--- a/research/slim/deployment/model_deploy_test.py
+++ b/research/slim/deployment/model_deploy_test.py
@@ -158,7 +158,7 @@ def LogisticClassifier(inputs, labels, scope=None, reuse=None):
 def BatchNormClassifier(inputs, labels, scope=None, reuse=None):
   with tf.variable_scope(scope, 'BatchNormClassifier', [inputs, labels],
                          reuse=reuse):
-    inputs = slim.batch_norm(inputs, decay=0.1)
+    inputs = slim.batch_norm(inputs, decay=0.1, fused=True)
     predictions = slim.fully_connected(inputs, 1,
                                        activation_fn=tf.sigmoid,
                                        scope='fully_connected')
@@ -476,6 +476,11 @@ class DeployTest(tf.test.TestCase):
       j = int(2 * self._labels[i] + np.random.randint(0, 2))
       self._inputs[i, j] = 1
 
+  def _addBesselsCorrection(self, sample_size, expected_var):
+    correction_factor = sample_size / (sample_size - 1)
+    expected_var *= correction_factor
+    return expected_var
+
   def testLocalTrainOp(self):
     g = tf.Graph()
     with g.as_default():
@@ -519,9 +524,11 @@ class DeployTest(tf.test.TestCase):
 
         final_mean, final_variance = sess.run([moving_mean,
                                                moving_variance])
-        self.assertAllClose(final_mean, [0.125, 0.25, 0.375, 0.25])
-        self.assertAllClose(final_variance, [0.109375, 0.1875,
-                                             0.234375, 0.1875])
+        expected_mean = np.array([0.125, 0.25, 0.375, 0.25])
+        expected_var = np.array([0.109375, 0.1875, 0.234375, 0.1875])
+        expected_var = self._addBesselsCorrection(16, expected_var)
+        self.assertAllClose(final_mean, expected_mean)
+        self.assertAllClose(final_variance, expected_var)
 
   def testNoSummariesOnGPU(self):
     with tf.Graph().as_default():
diff --git a/research/slim/nets/alexnet.py b/research/slim/nets/alexnet.py
index 4e7e563c..e860a6cb 100644
--- a/research/slim/nets/alexnet.py
+++ b/research/slim/nets/alexnet.py
@@ -57,7 +57,8 @@ def alexnet_v2(inputs,
                is_training=True,
                dropout_keep_prob=0.5,
                spatial_squeeze=True,
-               scope='alexnet_v2'):
+               scope='alexnet_v2',
+               global_pool=False):
   """AlexNet version 2.
 
   Described in: http://arxiv.org/pdf/1404.5997v2.pdf
@@ -66,26 +67,34 @@ def alexnet_v2(inputs,
   layers-imagenet-1gpu.cfg
 
   Note: All the fully_connected layers have been transformed to conv2d layers.
-        To use in classification mode, resize input to 224x224. To use in fully
-        convolutional mode, set spatial_squeeze to false.
+        To use in classification mode, resize input to 224x224 or set
+        global_pool=True. To use in fully convolutional mode, set
+        spatial_squeeze to false.
         The LRN layers have been removed and change the initializers from
         random_normal_initializer to xavier_initializer.
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: the number of predicted classes. If 0 or None, the logits layer
+    is omitted and the input features to the logits layer are returned instead.
     is_training: whether or not the model is being trained.
     dropout_keep_prob: the probability that activations are kept in the dropout
       layers during training.
     spatial_squeeze: whether or not should squeeze the spatial dimensions of the
-      outputs. Useful to remove unnecessary dimensions for classification.
+      logits. Useful to remove unnecessary dimensions for classification.
     scope: Optional scope for the variables.
+    global_pool: Optional boolean flag. If True, the input to the classification
+      layer is avgpooled to size 1x1, for any input size. (This is not part
+      of the original AlexNet.)
 
   Returns:
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the non-dropped-out input to the logits layer (if num_classes is 0
+      or None).
+    end_points: a dict of tensors with intermediate activations.
   """
   with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                         outputs_collections=[end_points_collection]):
@@ -108,18 +117,22 @@ def alexnet_v2(inputs,
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
         net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
-        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                           scope='dropout7')
-        net = slim.conv2d(net, num_classes, [1, 1],
-                          activation_fn=None,
-                          normalizer_fn=None,
-                          biases_initializer=tf.zeros_initializer(),
-                          scope='fc8')
-
-      # Convert end_points_collection into a end_point dict.
-      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      if spatial_squeeze:
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
-        end_points[sc.name + '/fc8'] = net
+        # Convert end_points_collection into a end_point dict.
+        end_points = slim.utils.convert_collection_to_dict(
+            end_points_collection)
+        if global_pool:
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          end_points['global_pool'] = net
+        if num_classes:
+          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
+                             scope='dropout7')
+          net = slim.conv2d(net, num_classes, [1, 1],
+                            activation_fn=None,
+                            normalizer_fn=None,
+                            biases_initializer=tf.zeros_initializer(),
+                            scope='fc8')
+          if spatial_squeeze:
+            net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+          end_points[sc.name + '/fc8'] = net
       return net, end_points
 alexnet_v2.default_image_size = 224
diff --git a/research/slim/nets/alexnet_test.py b/research/slim/nets/alexnet_test.py
index 0e562fbb..9a063842 100644
--- a/research/slim/nets/alexnet_test.py
+++ b/research/slim/nets/alexnet_test.py
@@ -48,6 +48,18 @@ class AlexnetV2Test(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, 4, 7, num_classes])
 
+  def testGlobalPool(self):
+    batch_size = 1
+    height, width = 300, 400
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False,
+                                     global_pool=True)
+      self.assertEquals(logits.op.name, 'alexnet_v2/fc8/BiasAdd')
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, 1, 1, num_classes])
+
   def testEndPoints(self):
     batch_size = 5
     height, width = 224, 224
@@ -69,6 +81,29 @@ class AlexnetV2Test(tf.test.TestCase):
                        ]
       self.assertSetEqual(set(end_points.keys()), set(expected_names))
 
+  def testNoClasses(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, end_points = alexnet.alexnet_v2(inputs, num_classes)
+      expected_names = ['alexnet_v2/conv1',
+                        'alexnet_v2/pool1',
+                        'alexnet_v2/conv2',
+                        'alexnet_v2/pool2',
+                        'alexnet_v2/conv3',
+                        'alexnet_v2/conv4',
+                        'alexnet_v2/conv5',
+                        'alexnet_v2/pool5',
+                        'alexnet_v2/fc6',
+                        'alexnet_v2/fc7'
+                       ]
+      self.assertSetEqual(set(end_points.keys()), set(expected_names))
+      self.assertTrue(net.op.name.startswith('alexnet_v2/fc7'))
+      self.assertListEqual(net.get_shape().as_list(),
+                           [batch_size, 1, 1, 4096])
+
   def testModelVariables(self):
     batch_size = 5
     height, width = 224, 224
diff --git a/research/slim/nets/cifarnet.py b/research/slim/nets/cifarnet.py
index 44ca0fed..97ed944b 100644
--- a/research/slim/nets/cifarnet.py
+++ b/research/slim/nets/cifarnet.py
@@ -42,7 +42,9 @@ def cifarnet(images, num_classes=10, is_training=False,
 
   Args:
     images: A batch of `Tensors` of size [batch_size, height, width, channels].
-    num_classes: the number of classes in the dataset.
+    num_classes: the number of classes in the dataset. If 0 or None, the logits
+      layer is omitted and the input features to the logits layer are returned
+      instead.
     is_training: specifies whether or not we're currently training the model.
       This variable will determine the behaviour of the dropout layer.
     dropout_keep_prob: the percentage of activation values that are retained.
@@ -50,14 +52,15 @@ def cifarnet(images, num_classes=10, is_training=False,
     scope: Optional variable_scope.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, `num_classes`]
+    net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the input to the logits layer if num_classes
+      is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'CifarNet', [images, num_classes]):
+  with tf.variable_scope(scope, 'CifarNet', [images]):
     net = slim.conv2d(images, 64, [5, 5], scope='conv1')
     end_points['conv1'] = net
     net = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
@@ -76,6 +79,8 @@ def cifarnet(images, num_classes=10, is_training=False,
                        scope='dropout3')
     net = slim.fully_connected(net, 192, scope='fc4')
     end_points['fc4'] = net
+    if not num_classes:
+      return net, end_points
     logits = slim.fully_connected(net, num_classes,
                                   biases_initializer=tf.zeros_initializer(),
                                   weights_initializer=trunc_normal(1/192.0),
diff --git a/research/slim/nets/cyclegan.py b/research/slim/nets/cyclegan.py
new file mode 100644
index 00000000..bfa9a443
--- /dev/null
+++ b/research/slim/nets/cyclegan.py
@@ -0,0 +1,269 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Defines the CycleGAN generator and discriminator networks."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import google3
+import numpy as np
+
+import tensorflow as tf
+
+layers = tf.contrib.layers
+
+
+def cyclegan_arg_scope(instance_norm_center=True,
+                       instance_norm_scale=True,
+                       instance_norm_epsilon=0.001,
+                       weights_init_stddev=0.02,
+                       weight_decay=0.0):
+  """Returns a default argument scope for all generators and discriminators.
+
+  Args:
+    instance_norm_center: Whether instance normalization applies centering.
+    instance_norm_scale: Whether instance normalization applies scaling.
+    instance_norm_epsilon: Small float added to the variance in the instance
+      normalization to avoid dividing by zero.
+    weights_init_stddev: Standard deviation of the random values to initialize
+      the convolution kernels with.
+    weight_decay: Magnitude of weight decay applied to all convolution kernel
+      variables of the generator.
+
+  Returns:
+    An arg-scope.
+  """
+  instance_norm_params = {
+      'center': instance_norm_center,
+      'scale': instance_norm_scale,
+      'epsilon': instance_norm_epsilon,
+  }
+
+  weights_regularizer = None
+  if weight_decay and weight_decay > 0.0:
+    weights_regularizer = layers.l2_regularizer(weight_decay)
+
+  with tf.contrib.framework.arg_scope(
+      [layers.conv2d],
+      normalizer_fn=layers.instance_norm,
+      normalizer_params=instance_norm_params,
+      weights_initializer=tf.random_normal_initializer(0, weights_init_stddev),
+      weights_regularizer=weights_regularizer) as sc:
+    return sc
+
+
+def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose'):
+  """Upsamples the given inputs.
+
+  Args:
+    net: A Tensor of size [batch_size, height, width, filters].
+    num_outputs: The number of output filters.
+    stride: A list of 2 scalars or a 1x2 Tensor indicating the scale,
+      relative to the inputs, of the output dimensions. For example, if kernel
+      size is [2, 3], then the output height and width will be twice and three
+      times the input size.
+    method: The upsampling method: 'nn_upsample_conv', 'bilinear_upsample_conv',
+      or 'conv2d_transpose'.
+
+  Returns:
+    A Tensor which was upsampled using the specified method.
+
+  Raises:
+    ValueError: if `method` is not recognized.
+  """
+  with tf.variable_scope('upconv'):
+    net_shape = tf.shape(net)
+    height = net_shape[1]
+    width = net_shape[2]
+
+    # Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3
+    # 'valid' convolution produce an output with the same dimension as the
+    # input.
+    spatial_pad_1 = np.array([[0, 0], [1, 1], [1, 1], [0, 0]])
+
+    if method == 'nn_upsample_conv':
+      net = tf.image.resize_nearest_neighbor(
+          net, [stride[0] * height, stride[1] * width])
+      net = tf.pad(net, spatial_pad_1, 'REFLECT')
+      net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
+    if method == 'bilinear_upsample_conv':
+      net = tf.image.resize_bilinear(
+          net, [stride[0] * height, stride[1] * width])
+      net = tf.pad(net, spatial_pad_1, 'REFLECT')
+      net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
+    elif method == 'conv2d_transpose':
+      net = layers.conv2d_transpose(
+          net, num_outputs, kernel_size=[3, 3], stride=stride, padding='same')
+    else:
+      raise ValueError('Unknown method: [%s]', method)
+
+    return net
+
+
+def _dynamic_or_static_shape(tensor):
+  shape = tf.shape(tensor)
+  static_shape = tf.contrib.util.constant_value(shape)
+  return static_shape if static_shape is not None else shape
+
+
+def cyclegan_generator_resnet(images,
+                              arg_scope_fn=cyclegan_arg_scope,
+                              num_resnet_blocks=6,
+                              num_filters=64,
+                              upsample_fn=cyclegan_upsample,
+                              kernel_size=3,
+                              num_outputs=3,
+                              tanh_linear_slope=0.0,
+                              is_training=False):
+  """Defines the cyclegan resnet network architecture.
+
+  As closely as possible following
+  https://github.com/junyanz/CycleGAN/blob/master/models/architectures.lua#L232
+
+  FYI: This network requires input height and width to be divisible by 4 in
+  order to generate an output with shape equal to input shape. Assertions will
+  catch this if input dimensions are known at graph construction time, but
+  there's no protection if unknown at graph construction time (you'll see an
+  error).
+
+  Args:
+    images: Input image tensor of shape [batch_size, h, w, 3].
+    arg_scope_fn: Function to create the global arg_scope for the network.
+    num_resnet_blocks: Number of ResNet blocks in the middle of the generator.
+    num_filters: Number of filters of the first hidden layer.
+    upsample_fn: Upsampling function for the decoder part of the generator.
+    kernel_size: Size w or list/tuple [h, w] of the filter kernels for all inner
+      layers.
+    num_outputs: Number of output layers. Defaults to 3 for RGB.
+    tanh_linear_slope: Slope of the linear function to add to the tanh over the
+      logits.
+    is_training: Whether the network is created in training mode or inference
+      only mode. Not actually needed, just for compliance with other generator
+      network functions.
+
+  Returns:
+    A `Tensor` representing the model output and a dictionary of model end
+      points.
+
+  Raises:
+    ValueError: If the input height or width is known at graph construction time
+      and not a multiple of 4.
+  """
+  # Neither dropout nor batch norm -> dont need is_training
+  del is_training
+
+  end_points = {}
+
+  input_size = images.shape.as_list()
+  height, width = input_size[1], input_size[2]
+  if height and height % 4 != 0:
+    raise ValueError('The input height must be a multiple of 4.')
+  if width and width % 4 != 0:
+    raise ValueError('The input width must be a multiple of 4.')
+
+  if not isinstance(kernel_size, (list, tuple)):
+    kernel_size = [kernel_size, kernel_size]
+
+  kernel_height = kernel_size[0]
+  kernel_width = kernel_size[1]
+  pad_top = (kernel_height - 1) // 2
+  pad_bottom = kernel_height // 2
+  pad_left = (kernel_width - 1) // 2
+  pad_right = kernel_width // 2
+  paddings = np.array(
+      [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right], [0, 0]],
+      dtype=np.int32)
+  spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])
+
+  with tf.contrib.framework.arg_scope(arg_scope_fn()):
+
+    ###########
+    # Encoder #
+    ###########
+    with tf.variable_scope('input'):
+      # 7x7 input stage
+      net = tf.pad(images, spatial_pad_3, 'REFLECT')
+      net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')
+      end_points['encoder_0'] = net
+
+    with tf.variable_scope('encoder'):
+      with tf.contrib.framework.arg_scope(
+          [layers.conv2d],
+          kernel_size=kernel_size,
+          stride=2,
+          activation_fn=tf.nn.relu,
+          padding='VALID'):
+
+        net = tf.pad(net, paddings, 'REFLECT')
+        net = layers.conv2d(net, num_filters * 2)
+        end_points['encoder_1'] = net
+        net = tf.pad(net, paddings, 'REFLECT')
+        net = layers.conv2d(net, num_filters * 4)
+        end_points['encoder_2'] = net
+
+    ###################
+    # Residual Blocks #
+    ###################
+    with tf.variable_scope('residual_blocks'):
+      with tf.contrib.framework.arg_scope(
+          [layers.conv2d],
+          kernel_size=kernel_size,
+          stride=1,
+          activation_fn=tf.nn.relu,
+          padding='VALID'):
+        for block_id in xrange(num_resnet_blocks):
+          with tf.variable_scope('block_{}'.format(block_id)):
+            res_net = tf.pad(net, paddings, 'REFLECT')
+            res_net = layers.conv2d(res_net, num_filters * 4)
+            res_net = tf.pad(res_net, paddings, 'REFLECT')
+            res_net = layers.conv2d(res_net, num_filters * 4,
+                                    activation_fn=None)
+            net += res_net
+
+            end_points['resnet_block_%d' % block_id] = net
+
+    ###########
+    # Decoder #
+    ###########
+    with tf.variable_scope('decoder'):
+
+      with tf.contrib.framework.arg_scope(
+          [layers.conv2d],
+          kernel_size=kernel_size,
+          stride=1,
+          activation_fn=tf.nn.relu):
+
+        with tf.variable_scope('decoder1'):
+          net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])
+        end_points['decoder1'] = net
+
+        with tf.variable_scope('decoder2'):
+          net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])
+        end_points['decoder2'] = net
+
+    with tf.variable_scope('output'):
+      net = tf.pad(net, spatial_pad_3, 'REFLECT')
+      logits = layers.conv2d(
+          net,
+          num_outputs, [7, 7],
+          activation_fn=None,
+          normalizer_fn=None,
+          padding='valid')
+      logits = tf.reshape(logits, _dynamic_or_static_shape(images))
+
+      end_points['logits'] = logits
+      end_points['predictions'] = tf.tanh(logits) + logits * tanh_linear_slope
+
+  return end_points['predictions'], end_points
diff --git a/research/slim/nets/cyclegan_test.py b/research/slim/nets/cyclegan_test.py
new file mode 100644
index 00000000..395773ea
--- /dev/null
+++ b/research/slim/nets/cyclegan_test.py
@@ -0,0 +1,112 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for tensorflow.contrib.slim.nets.cyclegan."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from nets import cyclegan
+
+
+# TODO(joelshor): Add a test to check generator endpoints.
+class CycleganTest(tf.test.TestCase):
+
+  def test_generator_inference(self):
+    """Check one inference step."""
+    img_batch = tf.zeros([2, 32, 32, 3])
+    model_output, _ = cyclegan.cyclegan_generator_resnet(img_batch)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      sess.run(model_output)
+
+  def _test_generator_graph_helper(self, shape):
+    """Check that generator can take small and non-square inputs."""
+    output_imgs, _ = cyclegan.cyclegan_generator_resnet(tf.ones(shape))
+    self.assertAllEqual(shape, output_imgs.shape.as_list())
+
+  def test_generator_graph_small(self):
+    self._test_generator_graph_helper([4, 32, 32, 3])
+
+  def test_generator_graph_medium(self):
+    self._test_generator_graph_helper([3, 128, 128, 3])
+
+  def test_generator_graph_nonsquare(self):
+    self._test_generator_graph_helper([2, 80, 400, 3])
+
+  def test_generator_unknown_batch_dim(self):
+    """Check that generator can take unknown batch dimension inputs."""
+    img = tf.placeholder(tf.float32, shape=[None, 32, None, 3])
+    output_imgs, _ = cyclegan.cyclegan_generator_resnet(img)
+
+    self.assertAllEqual([None, 32, None, 3], output_imgs.shape.as_list())
+
+  def _input_and_output_same_shape_helper(self, kernel_size):
+    img_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])
+    output_img_batch, _ = cyclegan.cyclegan_generator_resnet(
+        img_batch, kernel_size=kernel_size)
+
+    self.assertAllEqual(img_batch.shape.as_list(),
+                        output_img_batch.shape.as_list())
+
+  def input_and_output_same_shape_kernel3(self):
+    self._input_and_output_same_shape_helper(3)
+
+  def input_and_output_same_shape_kernel4(self):
+    self._input_and_output_same_shape_helper(4)
+
+  def input_and_output_same_shape_kernel5(self):
+    self._input_and_output_same_shape_helper(5)
+
+  def input_and_output_same_shape_kernel6(self):
+    self._input_and_output_same_shape_helper(6)
+
+  def _error_if_height_not_multiple_of_four_helper(self, height):
+    self.assertRaisesRegexp(
+        ValueError,
+        'The input height must be a multiple of 4.',
+        cyclegan.cyclegan_generator_resnet,
+        tf.placeholder(tf.float32, shape=[None, height, 32, 3]))
+
+  def test_error_if_height_not_multiple_of_four_height29(self):
+    self._error_if_height_not_multiple_of_four_helper(29)
+
+  def test_error_if_height_not_multiple_of_four_height30(self):
+    self._error_if_height_not_multiple_of_four_helper(30)
+
+  def test_error_if_height_not_multiple_of_four_height31(self):
+    self._error_if_height_not_multiple_of_four_helper(31)
+
+  def _error_if_width_not_multiple_of_four_helper(self, width):
+    self.assertRaisesRegexp(
+        ValueError,
+        'The input width must be a multiple of 4.',
+        cyclegan.cyclegan_generator_resnet,
+        tf.placeholder(tf.float32, shape=[None, 32, width, 3]))
+
+  def test_error_if_width_not_multiple_of_four_width29(self):
+    self._error_if_width_not_multiple_of_four_helper(29)
+
+  def test_error_if_width_not_multiple_of_four_width30(self):
+    self._error_if_width_not_multiple_of_four_helper(30)
+
+  def test_error_if_width_not_multiple_of_four_width31(self):
+    self._error_if_width_not_multiple_of_four_helper(31)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/slim/nets/dcgan.py b/research/slim/nets/dcgan.py
new file mode 100644
index 00000000..5dee2d90
--- /dev/null
+++ b/research/slim/nets/dcgan.py
@@ -0,0 +1,202 @@
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""DCGAN generator and discriminator from https://arxiv.org/abs/1511.06434."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from math import log
+
+import google3
+
+import tensorflow as tf
+slim = tf.contrib.slim
+
+
+def _validate_image_inputs(inputs):
+  inputs.get_shape().assert_has_rank(4)
+  inputs.get_shape()[1:3].assert_is_fully_defined()
+  if inputs.get_shape()[1] != inputs.get_shape()[2]:
+    raise ValueError('Input tensor does not have equal width and height: ',
+                     inputs.get_shape()[1:3])
+  width = inputs.get_shape().as_list()[1]
+  if log(width, 2) != int(log(width, 2)):
+    raise ValueError('Input tensor `width` is not a power of 2: ', width)
+
+
+# TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
+# setups need the gradient of gradient FusedBatchNormGrad.
+def discriminator(inputs,
+                  depth=64,
+                  is_training=True,
+                  reuse=None,
+                  scope='Discriminator',
+                  fused_batch_norm=False):
+  """Discriminator network for DCGAN.
+
+  Construct discriminator network from inputs to the final endpoint.
+
+  Args:
+    inputs: A tensor of size [batch_size, height, width, channels]. Must be
+      floating point.
+    depth: Number of channels in first convolution layer.
+    is_training: Whether the network is for training or not.
+    reuse: Whether or not the network variables should be reused. `scope`
+      must be given to be reused.
+    scope: Optional variable_scope.
+    fused_batch_norm: If `True`, use a faster, fused implementation of
+      batch norm.
+
+  Returns:
+    logits: The pre-softmax activations, a tensor of size [batch_size, 1]
+    end_points: a dictionary from components of the network to their activation.
+
+  Raises:
+    ValueError: If the input image shape is not 4-dimensional, if the spatial
+      dimensions aren't defined at graph construction time, if the spatial
+      dimensions aren't square, or if the spatial dimensions aren't a power of
+      two.
+  """
+
+  normalizer_fn = slim.batch_norm
+  normalizer_fn_args = {
+      'is_training': is_training,
+      'zero_debias_moving_mean': True,
+      'fused': fused_batch_norm,
+  }
+
+  _validate_image_inputs(inputs)
+  inp_shape = inputs.get_shape().as_list()[1]
+
+  end_points = {}
+  with tf.variable_scope(scope, values=[inputs], reuse=reuse) as scope:
+    with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
+      with slim.arg_scope([slim.conv2d],
+                          stride=2,
+                          kernel_size=4,
+                          activation_fn=tf.nn.leaky_relu):
+        net = inputs
+        for i in xrange(int(log(inp_shape, 2))):
+          scope = 'conv%i' % (i + 1)
+          current_depth = depth * 2**i
+          normalizer_fn_ = None if i == 0 else normalizer_fn
+          net = slim.conv2d(
+              net, current_depth, normalizer_fn=normalizer_fn_, scope=scope)
+          end_points[scope] = net
+
+        logits = slim.conv2d(net, 1, kernel_size=1, stride=1, padding='VALID',
+                             normalizer_fn=None, activation_fn=None)
+        logits = tf.reshape(logits, [-1, 1])
+        end_points['logits'] = logits
+
+        return logits, end_points
+
+
+# TODO(joelshor): Use fused batch norm by default. Investigate why some GAN
+# setups need the gradient of gradient FusedBatchNormGrad.
+def generator(inputs,
+              depth=64,
+              final_size=32,
+              num_outputs=3,
+              is_training=True,
+              reuse=None,
+              scope='Generator',
+              fused_batch_norm=False):
+  """Generator network for DCGAN.
+
+  Construct generator network from inputs to the final endpoint.
+
+  Args:
+    inputs: A tensor with any size N. [batch_size, N]
+    depth: Number of channels in last deconvolution layer.
+    final_size: The shape of the final output.
+    num_outputs: Number of output features. For images, this is the number of
+      channels.
+    is_training: whether is training or not.
+    reuse: Whether or not the network has its variables should be reused. scope
+      must be given to be reused.
+    scope: Optional variable_scope.
+    fused_batch_norm: If `True`, use a faster, fused implementation of
+      batch norm.
+
+  Returns:
+    logits: the pre-softmax activations, a tensor of size
+      [batch_size, 32, 32, channels]
+    end_points: a dictionary from components of the network to their activation.
+
+  Raises:
+    ValueError: If `inputs` is not 2-dimensional.
+    ValueError: If `final_size` isn't a power of 2 or is less than 8.
+  """
+  normalizer_fn = slim.batch_norm
+  normalizer_fn_args = {
+      'is_training': is_training,
+      'zero_debias_moving_mean': True,
+      'fused': fused_batch_norm,
+  }
+
+  inputs.get_shape().assert_has_rank(2)
+  if log(final_size, 2) != int(log(final_size, 2)):
+    raise ValueError('`final_size` (%i) must be a power of 2.' % final_size)
+  if final_size < 8:
+    raise ValueError('`final_size` (%i) must be greater than 8.' % final_size)
+
+  end_points = {}
+  num_layers = int(log(final_size, 2)) - 1
+  with tf.variable_scope(scope, values=[inputs], reuse=reuse) as scope:
+    with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
+      with slim.arg_scope([slim.conv2d_transpose],
+                          normalizer_fn=normalizer_fn,
+                          stride=2,
+                          kernel_size=4):
+        net = tf.expand_dims(tf.expand_dims(inputs, 1), 1)
+
+        # First upscaling is different because it takes the input vector.
+        current_depth = depth * 2 ** (num_layers - 1)
+        scope = 'deconv1'
+        net = slim.conv2d_transpose(
+            net, current_depth, stride=1, padding='VALID', scope=scope)
+        end_points[scope] = net
+
+        for i in xrange(2, num_layers):
+          scope = 'deconv%i' % (i)
+          current_depth = depth * 2 ** (num_layers - i)
+          net = slim.conv2d_transpose(net, current_depth, scope=scope)
+          end_points[scope] = net
+
+        # Last layer has different normalizer and activation.
+        scope = 'deconv%i' % (num_layers)
+        net = slim.conv2d_transpose(
+            net, depth, normalizer_fn=None, activation_fn=None, scope=scope)
+        end_points[scope] = net
+
+        # Convert to proper channels.
+        scope = 'logits'
+        logits = slim.conv2d(
+            net,
+            num_outputs,
+            normalizer_fn=None,
+            activation_fn=None,
+            kernel_size=1,
+            stride=1,
+            padding='VALID',
+            scope=scope)
+        end_points[scope] = logits
+
+        logits.get_shape().assert_has_rank(4)
+        logits.get_shape().assert_is_compatible_with(
+            [None, final_size, final_size, num_outputs])
+
+        return logits, end_points
diff --git a/research/slim/nets/dcgan_test.py b/research/slim/nets/dcgan_test.py
new file mode 100644
index 00000000..124342c6
--- /dev/null
+++ b/research/slim/nets/dcgan_test.py
@@ -0,0 +1,118 @@
+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for dcgan."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from nets import dcgan
+
+
+class DCGANTest(tf.test.TestCase):
+
+  def test_generator_run(self):
+    tf.set_random_seed(1234)
+    noise = tf.random_normal([100, 64])
+    image, _ = dcgan.generator(noise)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      image.eval()
+
+  def test_generator_graph(self):
+    tf.set_random_seed(1234)
+    # Check graph construction for a number of image size/depths and batch
+    # sizes.
+    for i, batch_size in zip(xrange(3, 7), xrange(3, 8)):
+      tf.reset_default_graph()
+      final_size = 2 ** i
+      noise = tf.random_normal([batch_size, 64])
+      image, end_points = dcgan.generator(
+          noise,
+          depth=32,
+          final_size=final_size)
+
+      self.assertAllEqual([batch_size, final_size, final_size, 3],
+                          image.shape.as_list())
+
+      expected_names = ['deconv%i' % j for j in xrange(1, i)] + ['logits']
+      self.assertSetEqual(set(expected_names), set(end_points.keys()))
+
+      # Check layer depths.
+      for j in range(1, i):
+        layer = end_points['deconv%i' % j]
+        self.assertEqual(32 * 2**(i-j-1), layer.get_shape().as_list()[-1])
+
+  def test_generator_invalid_input(self):
+    wrong_dim_input = tf.zeros([5, 32, 32])
+    with self.assertRaises(ValueError):
+      dcgan.generator(wrong_dim_input)
+
+    correct_input = tf.zeros([3, 2])
+    with self.assertRaisesRegexp(ValueError, 'must be a power of 2'):
+      dcgan.generator(correct_input, final_size=30)
+
+    with self.assertRaisesRegexp(ValueError, 'must be greater than 8'):
+      dcgan.generator(correct_input, final_size=4)
+
+  def test_discriminator_run(self):
+    image = tf.random_uniform([5, 32, 32, 3], -1, 1)
+    output, _ = dcgan.discriminator(image)
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output.eval()
+
+  def test_discriminator_graph(self):
+    # Check graph construction for a number of image size/depths and batch
+    # sizes.
+    for i, batch_size in zip(xrange(1, 6), xrange(3, 8)):
+      tf.reset_default_graph()
+      img_w = 2 ** i
+      image = tf.random_uniform([batch_size, img_w, img_w, 3], -1, 1)
+      output, end_points = dcgan.discriminator(
+          image,
+          depth=32)
+
+      self.assertAllEqual([batch_size, 1], output.get_shape().as_list())
+
+      expected_names = ['conv%i' % j for j in xrange(1, i+1)] + ['logits']
+      self.assertSetEqual(set(expected_names), set(end_points.keys()))
+
+      # Check layer depths.
+      for j in range(1, i+1):
+        layer = end_points['conv%i' % j]
+        self.assertEqual(32 * 2**(j-1), layer.get_shape().as_list()[-1])
+
+  def test_discriminator_invalid_input(self):
+    wrong_dim_img = tf.zeros([5, 32, 32])
+    with self.assertRaises(ValueError):
+      dcgan.discriminator(wrong_dim_img)
+
+    spatially_undefined_shape = tf.placeholder(tf.float32, [5, 32, None, 3])
+    with self.assertRaises(ValueError):
+      dcgan.discriminator(spatially_undefined_shape)
+
+    not_square = tf.zeros([5, 32, 16, 3])
+    with self.assertRaisesRegexp(ValueError, 'not have equal width and height'):
+      dcgan.discriminator(not_square)
+
+    not_power_2 = tf.zeros([5, 30, 30, 3])
+    with self.assertRaisesRegexp(ValueError, 'not a power of 2'):
+      dcgan.discriminator(not_power_2)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/slim/nets/inception_resnet_v2.py b/research/slim/nets/inception_resnet_v2.py
index 37ad8b86..0205340e 100644
--- a/research/slim/nets/inception_resnet_v2.py
+++ b/research/slim/nets/inception_resnet_v2.py
@@ -45,7 +45,12 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
     mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_1, tower_conv2_2])
     up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,
                      activation_fn=None, scope='Conv2d_1x1')
-    net += scale * up
+    scaled_up = up * scale
+    if activation_fn == tf.nn.relu6:
+      # Use clip_by_value to simulate bandpass activation.
+      scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0)
+
+    net += scaled_up
     if activation_fn:
       net = activation_fn(net)
   return net
@@ -65,7 +70,13 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
     mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])
     up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,
                      activation_fn=None, scope='Conv2d_1x1')
-    net += scale * up
+
+    scaled_up = up * scale
+    if activation_fn == tf.nn.relu6:
+      # Use clip_by_value to simulate bandpass activation.
+      scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0)
+
+    net += scaled_up
     if activation_fn:
       net = activation_fn(net)
   return net
@@ -85,7 +96,13 @@ def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
     mixed = tf.concat(axis=3, values=[tower_conv, tower_conv1_2])
     up = slim.conv2d(mixed, net.get_shape()[3], 1, normalizer_fn=None,
                      activation_fn=None, scope='Conv2d_1x1')
-    net += scale * up
+
+    scaled_up = up * scale
+    if activation_fn == tf.nn.relu6:
+      # Use clip_by_value to simulate bandpass activation.
+      scaled_up = tf.clip_by_value(scaled_up, -6.0, 6.0)
+
+    net += scaled_up
     if activation_fn:
       net = activation_fn(net)
   return net
@@ -95,7 +112,8 @@ def inception_resnet_v2_base(inputs,
                              final_endpoint='Conv2d_7b_1x1',
                              output_stride=16,
                              align_feature_maps=False,
-                             scope=None):
+                             scope=None,
+                             activation_fn=tf.nn.relu):
   """Inception model from  http://arxiv.org/abs/1602.07261.
 
   Constructs an Inception Resnet v2 network from inputs to the given final
@@ -113,6 +131,7 @@ def inception_resnet_v2_base(inputs,
     align_feature_maps: When true, changes all the VALID paddings in the network
       to SAME padding so that the feature maps are aligned.
     scope: Optional variable_scope.
+    activation_fn: Activation function for block scopes.
 
   Returns:
     tensor_out: output tensor corresponding to the final_endpoint.
@@ -191,7 +210,8 @@ def inception_resnet_v2_base(inputs,
 
       if add_and_check_final('Mixed_5b', net): return net, end_points
       # TODO(alemi): Register intermediate endpoints
-      net = slim.repeat(net, 10, block35, scale=0.17)
+      net = slim.repeat(net, 10, block35, scale=0.17,
+                        activation_fn=activation_fn)
 
       # 17 x 17 x 1088 if output_stride == 8,
       # 33 x 33 x 1088 if output_stride == 16
@@ -220,7 +240,8 @@ def inception_resnet_v2_base(inputs,
 
       # TODO(alemi): register intermediate endpoints
       with slim.arg_scope([slim.conv2d], rate=2 if use_atrous else 1):
-        net = slim.repeat(net, 20, block17, scale=0.10)
+        net = slim.repeat(net, 20, block17, scale=0.10,
+                          activation_fn=activation_fn)
       if add_and_check_final('PreAuxLogits', net): return net, end_points
 
       if output_stride == 8:
@@ -257,7 +278,7 @@ def inception_resnet_v2_base(inputs,
       if add_and_check_final('Mixed_7a', net): return net, end_points
 
       # TODO(alemi): register intermediate endpoints
-      net = slim.repeat(net, 9, block8, scale=0.20)
+      net = slim.repeat(net, 9, block8, scale=0.20, activation_fn=activation_fn)
       net = block8(net, activation_fn=None)
 
       # 8 x 8 x 1536
@@ -271,33 +292,42 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
                         dropout_keep_prob=0.8,
                         reuse=None,
                         scope='InceptionResnetV2',
-                        create_aux_logits=True):
+                        create_aux_logits=True,
+                        activation_fn=tf.nn.relu):
   """Creates the Inception Resnet V2 model.
 
   Args:
     inputs: a 4-D tensor of size [batch_size, height, width, 3].
-    num_classes: number of predicted classes.
+      Dimension batch_size may be undefined. If create_aux_logits is false,
+      also height and width may be undefined.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before  dropout)
+      are returned instead.
     is_training: whether is training or not.
     dropout_keep_prob: float, the fraction to keep before final layer.
     reuse: whether or not the network and its variables should be reused. To be
       able to reuse 'scope' must be given.
     scope: Optional variable_scope.
     create_aux_logits: Whether to include the auxilliary logits.
+    activation_fn: Activation function for conv2d.
 
   Returns:
-    logits: the logits outputs of the model.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the non-dropped-out input to the logits layer (if num_classes is 0 or
+      None).
     end_points: the set of end_points from the inception model.
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs, num_classes],
+  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs],
                          reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
 
-      net, end_points = inception_resnet_v2_base(inputs, scope=scope)
+      net, end_points = inception_resnet_v2_base(inputs, scope=scope,
+                                                 activation_fn=activation_fn)
 
-      if create_aux_logits:
+      if create_aux_logits and num_classes:
         with tf.variable_scope('AuxLogits'):
           aux = end_points['PreAuxLogits']
           aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',
@@ -311,13 +341,20 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
           end_points['AuxLogits'] = aux
 
       with tf.variable_scope('Logits'):
-        net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',
-                              scope='AvgPool_1a_8x8')
+        # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
+        # can be set to False to disable pooling here (as in resnet_*()).
+        kernel_size = net.get_shape()[1:3]
+        if kernel_size.is_fully_defined():
+          net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                                scope='AvgPool_1a_8x8')
+        else:
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        end_points['global_pool'] = net
+        if not num_classes:
+          return net, end_points
         net = slim.flatten(net)
-
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='Dropout')
-
         end_points['PreLogitsFlatten'] = net
         logits = slim.fully_connected(net, num_classes, activation_fn=None,
                                       scope='Logits')
@@ -330,13 +367,15 @@ inception_resnet_v2.default_image_size = 299
 
 def inception_resnet_v2_arg_scope(weight_decay=0.00004,
                                   batch_norm_decay=0.9997,
-                                  batch_norm_epsilon=0.001):
+                                  batch_norm_epsilon=0.001,
+                                  activation_fn=tf.nn.relu):
   """Returns the scope with the default parameters for inception_resnet_v2.
 
   Args:
     weight_decay: the weight decay for weights variables.
     batch_norm_decay: decay for the moving average of batch_norm momentums.
     batch_norm_epsilon: small float added to variance to avoid dividing by zero.
+    activation_fn: Activation function for conv2d.
 
   Returns:
     a arg_scope with the parameters needed for inception_resnet_v2.
@@ -349,9 +388,10 @@ def inception_resnet_v2_arg_scope(weight_decay=0.00004,
     batch_norm_params = {
         'decay': batch_norm_decay,
         'epsilon': batch_norm_epsilon,
+        'fused': None,  # Use fused batch norm if possible.
     }
     # Set activation_fn and parameters for batch_norm.
-    with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu,
+    with slim.arg_scope([slim.conv2d], activation_fn=activation_fn,
                         normalizer_fn=slim.batch_norm,
                         normalizer_params=batch_norm_params) as scope:
       return scope
diff --git a/research/slim/nets/inception_resnet_v2_test.py b/research/slim/nets/inception_resnet_v2_test.py
index c369ed9f..a9d881bc 100644
--- a/research/slim/nets/inception_resnet_v2_test.py
+++ b/research/slim/nets/inception_resnet_v2_test.py
@@ -54,6 +54,19 @@ class InceptionTest(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
 
+  def testBuildNoClasses(self):
+    batch_size = 5
+    height, width = 299, 299
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, endpoints = inception.inception_resnet_v2(inputs, num_classes)
+      self.assertTrue('AuxLogits' not in endpoints)
+      self.assertTrue('Logits' not in endpoints)
+      self.assertTrue(
+          net.op.name.startswith('InceptionResnetV2/Logits/AvgPool'))
+      self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1536])
+
   def testBuildEndPoints(self):
     batch_size = 5
     height, width = 299, 299
@@ -213,6 +226,39 @@ class InceptionTest(tf.test.TestCase):
       self.assertListEqual(pre_pool.get_shape().as_list(),
                            [batch_size, 3, 3, 1536])
 
+  def testGlobalPool(self):
+    batch_size = 2
+    height, width = 400, 600
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, end_points = inception.inception_resnet_v2(inputs, num_classes)
+      self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Conv2d_7b_1x1']
+      self.assertListEqual(pre_pool.get_shape().as_list(),
+                           [batch_size, 11, 17, 1536])
+
+  def testGlobalPoolUnknownImageShape(self):
+    batch_size = 2
+    height, width = 400, 600
+    num_classes = 1000
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
+      logits, end_points = inception.inception_resnet_v2(
+          inputs, num_classes, create_aux_logits=False)
+      self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Conv2d_7b_1x1']
+      images = tf.random_uniform((batch_size, height, width, 3))
+      sess.run(tf.global_variables_initializer())
+      logits_out, pre_pool_out = sess.run([logits, pre_pool],
+                                          {inputs: images.eval()})
+      self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
+      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 11, 17, 1536))
+
   def testUnknownBatchSize(self):
     batch_size = 1
     height, width = 299, 299
diff --git a/research/slim/nets/inception_utils.py b/research/slim/nets/inception_utils.py
index 66ee41fb..5ff46e0b 100644
--- a/research/slim/nets/inception_utils.py
+++ b/research/slim/nets/inception_utils.py
@@ -32,7 +32,8 @@ slim = tf.contrib.slim
 def inception_arg_scope(weight_decay=0.00004,
                         use_batch_norm=True,
                         batch_norm_decay=0.9997,
-                        batch_norm_epsilon=0.001):
+                        batch_norm_epsilon=0.001,
+                        activation_fn=tf.nn.relu):
   """Defines the default arg scope for inception models.
 
   Args:
@@ -41,6 +42,7 @@ def inception_arg_scope(weight_decay=0.00004,
     batch_norm_decay: Decay for batch norm moving average.
     batch_norm_epsilon: Small float added to variance to avoid dividing by zero
       in batch norm.
+    activation_fn: Activation function for conv2d.
 
   Returns:
     An `arg_scope` to use for the inception models.
@@ -52,6 +54,8 @@ def inception_arg_scope(weight_decay=0.00004,
       'epsilon': batch_norm_epsilon,
       # collection containing update_ops.
       'updates_collections': tf.GraphKeys.UPDATE_OPS,
+      # use fused batch norm if possible.
+      'fused': None,
   }
   if use_batch_norm:
     normalizer_fn = slim.batch_norm
@@ -65,7 +69,7 @@ def inception_arg_scope(weight_decay=0.00004,
     with slim.arg_scope(
         [slim.conv2d],
         weights_initializer=slim.variance_scaling_initializer(),
-        activation_fn=tf.nn.relu,
+        activation_fn=activation_fn,
         normalizer_fn=normalizer_fn,
         normalizer_params=normalizer_params) as sc:
       return sc
diff --git a/research/slim/nets/inception_v1.py b/research/slim/nets/inception_v1.py
index c2c7be28..d9871659 100644
--- a/research/slim/nets/inception_v1.py
+++ b/research/slim/nets/inception_v1.py
@@ -261,7 +261,8 @@ def inception_v1(inputs,
                  prediction_fn=slim.softmax,
                  spatial_squeeze=True,
                  reuse=None,
-                 scope='InceptionV1'):
+                 scope='InceptionV1',
+                 global_pool=False):
   """Defines the Inception V1 architecture.
 
   This architecture is defined in:
@@ -275,7 +276,9 @@ def inception_v1(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before dropout)
+      are returned instead.
     is_training: whether is training or not.
     dropout_keep_prob: the percentage of activation values that are retained.
     prediction_fn: a function to get predictions out of logits.
@@ -284,23 +287,35 @@ def inception_v1(inputs,
     reuse: whether or not the network and its variables should be reused. To be
       able to reuse 'scope' must be given.
     scope: Optional variable_scope.
+    global_pool: Optional boolean flag to control the avgpooling before the
+      logits layer. If false or unset, pooling is done with a fixed window
+      that reduces default-sized inputs to 1x1, while larger inputs lead to
+      larger outputs. If true, any input size is pooled down to 1x1.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the non-dropped-out input to the logits layer
+      if num_classes is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
   """
   # Final pooling and prediction
-  with tf.variable_scope(scope, 'InceptionV1', [inputs, num_classes],
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v1_base(inputs, scope=scope)
       with tf.variable_scope('Logits'):
-        net = slim.avg_pool2d(net, [7, 7], stride=1, scope='AvgPool_0a_7x7')
-        net = slim.dropout(net,
-                           dropout_keep_prob, scope='Dropout_0b')
+        if global_pool:
+          # Global average pooling.
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          end_points['global_pool'] = net
+        else:
+          # Pooling with a fixed kernel size.
+          net = slim.avg_pool2d(net, [7, 7], stride=1, scope='AvgPool_0a_7x7')
+          end_points['AvgPool_0a_7x7'] = net
+        if not num_classes:
+          return net, end_points
+        net = slim.dropout(net, dropout_keep_prob, scope='Dropout_0b')
         logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                              normalizer_fn=None, scope='Conv2d_0c_1x1')
         if spatial_squeeze:
diff --git a/research/slim/nets/inception_v1_test.py b/research/slim/nets/inception_v1_test.py
index b9271814..34ed96bb 100644
--- a/research/slim/nets/inception_v1_test.py
+++ b/research/slim/nets/inception_v1_test.py
@@ -35,13 +35,26 @@ class InceptionV1Test(tf.test.TestCase):
 
     inputs = tf.random_uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v1(inputs, num_classes)
-    self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
+    self.assertTrue(logits.op.name.startswith(
+        'InceptionV1/Logits/SpatialSqueeze'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [batch_size, num_classes])
     self.assertTrue('Predictions' in end_points)
     self.assertListEqual(end_points['Predictions'].get_shape().as_list(),
                          [batch_size, num_classes])
 
+  def testBuildPreLogitsNetwork(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = inception.inception_v1(inputs, num_classes)
+    self.assertTrue(net.op.name.startswith('InceptionV1/Logits/AvgPool'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
+    self.assertFalse('Logits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+
   def testBuildBaseNetwork(self):
     batch_size = 5
     height, width = 224, 224
@@ -144,6 +157,25 @@ class InceptionV1Test(tf.test.TestCase):
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
+  def testGlobalPoolUnknownImageShape(self):
+    tf.reset_default_graph()
+    batch_size = 2
+    height, width = 300, 400
+    num_classes = 1000
+    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      logits, end_points = inception.inception_v1(inputs, num_classes,
+                                                  global_pool=True)
+      self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Mixed_5c']
+      feed_dict = {inputs: input_np}
+      tf.global_variables_initializer().run()
+      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+
   def testUnknowBatchSize(self):
     batch_size = 1
     height, width = 224, 224
diff --git a/research/slim/nets/inception_v2.py b/research/slim/nets/inception_v2.py
index 46a5e72d..66290b4d 100644
--- a/research/slim/nets/inception_v2.py
+++ b/research/slim/nets/inception_v2.py
@@ -458,7 +458,8 @@ def inception_v2(inputs,
                  prediction_fn=slim.softmax,
                  spatial_squeeze=True,
                  reuse=None,
-                 scope='InceptionV2'):
+                 scope='InceptionV2',
+                 global_pool=False):
   """Inception v2 model for classification.
 
   Constructs an Inception v2 network for classification as described in
@@ -468,7 +469,9 @@ def inception_v2(inputs,
 
   Args:
     inputs: a tensor of shape [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before dropout)
+      are returned instead.
     is_training: whether is training or not.
     dropout_keep_prob: the percentage of activation values that are retained.
     min_depth: Minimum depth value (number of channels) for all convolution ops.
@@ -484,10 +487,15 @@ def inception_v2(inputs,
     reuse: whether or not the network and its variables should be reused. To be
       able to reuse 'scope' must be given.
     scope: Optional variable_scope.
+    global_pool: Optional boolean flag to control the avgpooling before the
+      logits layer. If false or unset, pooling is done with a fixed window
+      that reduces default-sized inputs to 1x1, while larger inputs lead to
+      larger outputs. If true, any input size is pooled down to 1x1.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the non-dropped-out input to the logits layer
+      if num_classes is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
 
@@ -499,17 +507,25 @@ def inception_v2(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
 
   # Final pooling and prediction
-  with tf.variable_scope(scope, 'InceptionV2', [inputs, num_classes],
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV2', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v2_base(
           inputs, scope=scope, min_depth=min_depth,
           depth_multiplier=depth_multiplier)
       with tf.variable_scope('Logits'):
-        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])
-        net = slim.avg_pool2d(net, kernel_size, padding='VALID',
-                              scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+        if global_pool:
+          # Global average pooling.
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          end_points['global_pool'] = net
+        else:
+          # Pooling with a fixed kernel size.
+          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])
+          net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                                scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+          end_points['AvgPool_1a'] = net
+        if not num_classes:
+          return net, end_points
         # 1 x 1 x 1024
         net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')
         logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
diff --git a/research/slim/nets/inception_v2_test.py b/research/slim/nets/inception_v2_test.py
index 8389c786..ac8912ad 100644
--- a/research/slim/nets/inception_v2_test.py
+++ b/research/slim/nets/inception_v2_test.py
@@ -35,13 +35,26 @@ class InceptionV2Test(tf.test.TestCase):
 
     inputs = tf.random_uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v2(inputs, num_classes)
-    self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
+    self.assertTrue(logits.op.name.startswith(
+        'InceptionV2/Logits/SpatialSqueeze'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [batch_size, num_classes])
     self.assertTrue('Predictions' in end_points)
     self.assertListEqual(end_points['Predictions'].get_shape().as_list(),
                          [batch_size, num_classes])
 
+  def testBuildPreLogitsNetwork(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = inception.inception_v2(inputs, num_classes)
+    self.assertTrue(net.op.name.startswith('InceptionV2/Logits/AvgPool'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
+    self.assertFalse('Logits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+
   def testBuildBaseNetwork(self):
     batch_size = 5
     height, width = 224, 224
@@ -258,6 +271,25 @@ class InceptionV2Test(tf.test.TestCase):
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
+  def testGlobalPoolUnknownImageShape(self):
+    tf.reset_default_graph()
+    batch_size = 2
+    height, width = 300, 400
+    num_classes = 1000
+    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      logits, end_points = inception.inception_v2(inputs, num_classes,
+                                                  global_pool=True)
+      self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Mixed_5c']
+      feed_dict = {inputs: input_np}
+      tf.global_variables_initializer().run()
+      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+
   def testUnknowBatchSize(self):
     batch_size = 1
     height, width = 224, 224
diff --git a/research/slim/nets/inception_v3.py b/research/slim/nets/inception_v3.py
index 372d4016..12217791 100644
--- a/research/slim/nets/inception_v3.py
+++ b/research/slim/nets/inception_v3.py
@@ -426,7 +426,8 @@ def inception_v3(inputs,
                  spatial_squeeze=True,
                  reuse=None,
                  create_aux_logits=True,
-                 scope='InceptionV3'):
+                 scope='InceptionV3',
+                 global_pool=False):
   """Inception model from http://arxiv.org/abs/1512.00567.
 
   "Rethinking the Inception Architecture for Computer Vision"
@@ -443,7 +444,9 @@ def inception_v3(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before dropout)
+      are returned instead.
     is_training: whether is training or not.
     dropout_keep_prob: the percentage of activation values that are retained.
     min_depth: Minimum depth value (number of channels) for all convolution ops.
@@ -460,10 +463,15 @@ def inception_v3(inputs,
       able to reuse 'scope' must be given.
     create_aux_logits: Whether to create the auxiliary logits.
     scope: Optional variable_scope.
+    global_pool: Optional boolean flag to control the avgpooling before the
+      logits layer. If false or unset, pooling is done with a fixed window
+      that reduces default-sized inputs to 1x1, while larger inputs lead to
+      larger outputs. If true, any input size is pooled down to 1x1.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, num_classes]
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the non-dropped-out input to the logits layer
+      if num_classes is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
 
@@ -474,8 +482,7 @@ def inception_v3(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes],
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'InceptionV3', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v3_base(
@@ -483,7 +490,7 @@ def inception_v3(inputs,
           depth_multiplier=depth_multiplier)
 
       # Auxiliary Head logits
-      if create_aux_logits:
+      if create_aux_logits and num_classes:
         with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                             stride=1, padding='SAME'):
           aux_logits = end_points['Mixed_6e']
@@ -511,9 +518,18 @@ def inception_v3(inputs,
 
       # Final pooling and prediction
       with tf.variable_scope('Logits'):
-        kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])
-        net = slim.avg_pool2d(net, kernel_size, padding='VALID',
-                              scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+        if global_pool:
+          # Global average pooling.
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='GlobalPool')
+          end_points['global_pool'] = net
+        else:
+          # Pooling with a fixed kernel size.
+          kernel_size = _reduced_kernel_size_for_small_input(net, [8, 8])
+          net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                                scope='AvgPool_1a_{}x{}'.format(*kernel_size))
+          end_points['AvgPool_1a'] = net
+        if not num_classes:
+          return net, end_points
         # 1 x 1 x 2048
         net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')
         end_points['PreLogits'] = net
diff --git a/research/slim/nets/inception_v3_test.py b/research/slim/nets/inception_v3_test.py
index a1e870d3..feae315c 100644
--- a/research/slim/nets/inception_v3_test.py
+++ b/research/slim/nets/inception_v3_test.py
@@ -35,13 +35,26 @@ class InceptionV3Test(tf.test.TestCase):
 
     inputs = tf.random_uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v3(inputs, num_classes)
-    self.assertTrue(logits.op.name.startswith('InceptionV3/Logits'))
+    self.assertTrue(logits.op.name.startswith(
+        'InceptionV3/Logits/SpatialSqueeze'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [batch_size, num_classes])
     self.assertTrue('Predictions' in end_points)
     self.assertListEqual(end_points['Predictions'].get_shape().as_list(),
                          [batch_size, num_classes])
 
+  def testBuildPreLogitsNetwork(self):
+    batch_size = 5
+    height, width = 299, 299
+    num_classes = None
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = inception.inception_v3(inputs, num_classes)
+    self.assertTrue(net.op.name.startswith('InceptionV3/Logits/AvgPool'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 2048])
+    self.assertFalse('Logits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+
   def testBuildBaseNetwork(self):
     batch_size = 5
     height, width = 299, 299
@@ -225,6 +238,24 @@ class InceptionV3Test(tf.test.TestCase):
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])
 
+  def testGlobalPoolUnknownImageShape(self):
+    tf.reset_default_graph()
+    batch_size = 2
+    height, width = 400, 600
+    num_classes = 1000
+    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      logits, end_points = inception.inception_v3(inputs, num_classes,
+                                                  global_pool=True)
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Mixed_7c']
+      feed_dict = {inputs: input_np}
+      tf.global_variables_initializer().run()
+      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 11, 17, 2048])
+
   def testUnknowBatchSize(self):
     batch_size = 1
     height, width = 299, 299
diff --git a/research/slim/nets/inception_v4.py b/research/slim/nets/inception_v4.py
index b4f07ea7..bab406a8 100644
--- a/research/slim/nets/inception_v4.py
+++ b/research/slim/nets/inception_v4.py
@@ -263,7 +263,9 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
 
   Args:
     inputs: a 4-D tensor of size [batch_size, height, width, 3].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before dropout)
+      are returned instead.
     is_training: whether is training or not.
     dropout_keep_prob: float, the fraction to keep before final layer.
     reuse: whether or not the network and its variables should be reused. To be
@@ -272,7 +274,9 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
     create_aux_logits: Whether to include the auxiliary logits.
 
   Returns:
-    logits: the logits outputs of the model.
+    net: a Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the non-dropped input to the logits layer
+      if num_classes is 0 or None.
     end_points: the set of end_points from the inception model.
   """
   end_points = {}
@@ -284,7 +288,7 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
       with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                           stride=1, padding='SAME'):
         # Auxiliary Head logits
-        if create_aux_logits:
+        if create_aux_logits and num_classes:
           with tf.variable_scope('AuxLogits'):
             # 17 x 17 x 1024
             aux_logits = end_points['Mixed_6h']
@@ -303,10 +307,20 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
             end_points['AuxLogits'] = aux_logits
 
         # Final pooling and prediction
+        # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
+        # can be set to False to disable pooling here (as in resnet_*()).
         with tf.variable_scope('Logits'):
           # 8 x 8 x 1536
-          net = slim.avg_pool2d(net, net.get_shape()[1:3], padding='VALID',
-                                scope='AvgPool_1a')
+          kernel_size = net.get_shape()[1:3]
+          if kernel_size.is_fully_defined():
+            net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                                  scope='AvgPool_1a')
+          else:
+            net = tf.reduce_mean(net, [1, 2], keep_dims=True,
+                                 name='global_pool')
+          end_points['global_pool'] = net
+          if not num_classes:
+            return net, end_points
           # 1 x 1 x 1536
           net = slim.dropout(net, dropout_keep_prob, scope='Dropout_1b')
           net = slim.flatten(net, scope='PreLogitsFlatten')
diff --git a/research/slim/nets/inception_v4_test.py b/research/slim/nets/inception_v4_test.py
index 11cffb63..815f9ec9 100644
--- a/research/slim/nets/inception_v4_test.py
+++ b/research/slim/nets/inception_v4_test.py
@@ -43,6 +43,17 @@ class InceptionTest(tf.test.TestCase):
     self.assertListEqual(predictions.get_shape().as_list(),
                          [batch_size, num_classes])
 
+  def testBuildPreLogitsNetwork(self):
+    batch_size = 5
+    height, width = 299, 299
+    num_classes = None
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = inception.inception_v4(inputs, num_classes)
+    self.assertTrue(net.op.name.startswith('InceptionV4/Logits/AvgPool'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1536])
+    self.assertFalse('Logits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+
   def testBuildWithoutAuxLogits(self):
     batch_size = 5
     height, width = 299, 299
@@ -90,6 +101,7 @@ class InceptionTest(tf.test.TestCase):
                         'Mixed_7d': [batch_size, 8, 8, 1536],
                         # Logits and predictions
                         'AuxLogits': [batch_size, num_classes],
+                        'global_pool': [batch_size, 1, 1, 1536],
                         'PreLogitsFlatten': [batch_size, 1536],
                         'Logits': [batch_size, num_classes],
                         'Predictions': [batch_size, num_classes]}
@@ -164,6 +176,38 @@ class InceptionTest(tf.test.TestCase):
     self.assertListEqual(pre_pool.get_shape().as_list(),
                          [batch_size, 3, 3, 1536])
 
+  def testGlobalPool(self):
+    batch_size = 2
+    height, width = 400, 600
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    logits, end_points = inception.inception_v4(inputs, num_classes)
+    self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    pre_pool = end_points['Mixed_7d']
+    self.assertListEqual(pre_pool.get_shape().as_list(),
+                         [batch_size, 11, 17, 1536])
+
+  def testGlobalPoolUnknownImageShape(self):
+    batch_size = 2
+    height, width = 400, 600
+    num_classes = 1000
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
+      logits, end_points = inception.inception_v4(
+          inputs, num_classes, create_aux_logits=False)
+      self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Mixed_7d']
+      images = tf.random_uniform((batch_size, height, width, 3))
+      sess.run(tf.global_variables_initializer())
+      logits_out, pre_pool_out = sess.run([logits, pre_pool],
+                                          {inputs: images.eval()})
+      self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
+      self.assertTupleEqual(pre_pool_out.shape, (batch_size, 11, 17, 1536))
+
   def testUnknownBatchSize(self):
     batch_size = 1
     height, width = 299, 299
diff --git a/research/slim/nets/lenet.py b/research/slim/nets/lenet.py
index 789d2bdc..c79dbfac 100644
--- a/research/slim/nets/lenet.py
+++ b/research/slim/nets/lenet.py
@@ -40,7 +40,9 @@ def lenet(images, num_classes=10, is_training=False,
 
   Args:
     images: A batch of `Tensors` of size [batch_size, height, width, channels].
-    num_classes: the number of classes in the dataset.
+    num_classes: the number of classes in the dataset. If 0 or None, the logits
+      layer is omitted and the input features to the logits layer are returned
+      instead.
     is_training: specifies whether or not we're currently training the model.
       This variable will determine the behaviour of the dropout layer.
     dropout_keep_prob: the percentage of activation values that are retained.
@@ -48,28 +50,30 @@ def lenet(images, num_classes=10, is_training=False,
     scope: Optional variable_scope.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, `num_classes`]
+     net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the inon-dropped-out nput to the logits layer
+      if num_classes is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'LeNet', [images, num_classes]):
-    net = slim.conv2d(images, 32, [5, 5], scope='conv1')
-    net = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
-    net = slim.conv2d(net, 64, [5, 5], scope='conv2')
-    net = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
+  with tf.variable_scope(scope, 'LeNet', [images]):
+    net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')
+    net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
+    net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')
+    net = end_points['pool2'] = slim.max_pool2d(net, [2, 2], 2, scope='pool2')
     net = slim.flatten(net)
     end_points['Flatten'] = net
 
-    net = slim.fully_connected(net, 1024, scope='fc3')
-    net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                       scope='dropout3')
-    logits = slim.fully_connected(net, num_classes, activation_fn=None,
-                                  scope='fc4')
+    net = end_points['fc3'] = slim.fully_connected(net, 1024, scope='fc3')
+    if not num_classes:
+      return net, end_points
+    net = end_points['dropout3'] = slim.dropout(
+        net, dropout_keep_prob, is_training=is_training, scope='dropout3')
+    logits = end_points['Logits'] = slim.fully_connected(
+        net, num_classes, activation_fn=None, scope='fc4')
 
-  end_points['Logits'] = logits
   end_points['Predictions'] = prediction_fn(logits, scope='Predictions')
 
   return logits, end_points
diff --git a/research/slim/nets/mobilenet_v1.py b/research/slim/nets/mobilenet_v1.py
index 32eaf91c..d2dcd3d3 100644
--- a/research/slim/nets/mobilenet_v1.py
+++ b/research/slim/nets/mobilenet_v1.py
@@ -154,7 +154,7 @@ def mobilenet_v1_base(inputs,
     inputs: a tensor of shape [batch_size, height, width, channels].
     final_endpoint: specifies the endpoint to construct the network up to. It
       can be one of ['Conv2d_0', 'Conv2d_1_pointwise', 'Conv2d_2_pointwise',
-      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5_pointwise',
+      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5'_pointwise,
       'Conv2d_6_pointwise', 'Conv2d_7_pointwise', 'Conv2d_8_pointwise',
       'Conv2d_9_pointwise', 'Conv2d_10_pointwise', 'Conv2d_11_pointwise',
       'Conv2d_12_pointwise', 'Conv2d_13_pointwise'].
@@ -276,12 +276,15 @@ def mobilenet_v1(inputs,
                  prediction_fn=tf.contrib.layers.softmax,
                  spatial_squeeze=True,
                  reuse=None,
-                 scope='MobilenetV1'):
+                 scope='MobilenetV1',
+                 global_pool=False):
   """Mobilenet v1 model for classification.
 
   Args:
     inputs: a tensor of shape [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer
+      is omitted and the input features to the logits layer (before dropout)
+      are returned instead.
     dropout_keep_prob: the percentage of activation values that are retained.
     is_training: whether is training or not.
     min_depth: Minimum depth value (number of channels) for all convolution ops.
@@ -298,10 +301,15 @@ def mobilenet_v1(inputs,
     reuse: whether or not the network and its variables should be reused. To be
       able to reuse 'scope' must be given.
     scope: Optional variable_scope.
+    global_pool: Optional boolean flag to control the avgpooling before the
+      logits layer. If false or unset, pooling is done with a fixed window
+      that reduces default-sized inputs to 1x1, while larger inputs lead to
+      larger outputs. If true, any input size is pooled down to 1x1.
 
   Returns:
-    logits: the pre-softmax activations, a tensor of size
-      [batch_size, num_classes]
+    net: a 2D Tensor with the logits (pre-softmax activations) if num_classes
+      is a non-zero integer, or the non-dropped-out input to the logits layer
+      if num_classes is 0 or None.
     end_points: a dictionary from components of the network to the corresponding
       activation.
 
@@ -313,8 +321,7 @@ def mobilenet_v1(inputs,
     raise ValueError('Invalid input tensor rank, expected 4, was: %d' %
                      len(input_shape))
 
-  with tf.variable_scope(scope, 'MobilenetV1', [inputs, num_classes],
-                         reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'MobilenetV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = mobilenet_v1_base(inputs, scope=scope,
@@ -322,10 +329,18 @@ def mobilenet_v1(inputs,
                                           depth_multiplier=depth_multiplier,
                                           conv_defs=conv_defs)
       with tf.variable_scope('Logits'):
-        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])
-        net = slim.avg_pool2d(net, kernel_size, padding='VALID',
-                              scope='AvgPool_1a')
-        end_points['AvgPool_1a'] = net
+        if global_pool:
+          # Global average pooling.
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          end_points['global_pool'] = net
+        else:
+          # Pooling with a fixed kernel size.
+          kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])
+          net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                                scope='AvgPool_1a')
+          end_points['AvgPool_1a'] = net
+        if not num_classes:
+          return net, end_points
         # 1 x 1 x 1024
         net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')
         logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
diff --git a/research/slim/nets/mobilenet_v1_test.py b/research/slim/nets/mobilenet_v1_test.py
index 44e66446..716aa02a 100644
--- a/research/slim/nets/mobilenet_v1_test.py
+++ b/research/slim/nets/mobilenet_v1_test.py
@@ -35,13 +35,26 @@ class MobilenetV1Test(tf.test.TestCase):
 
     inputs = tf.random_uniform((batch_size, height, width, 3))
     logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
-    self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+    self.assertTrue(logits.op.name.startswith(
+        'MobilenetV1/Logits/SpatialSqueeze'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [batch_size, num_classes])
     self.assertTrue('Predictions' in end_points)
     self.assertListEqual(end_points['Predictions'].get_shape().as_list(),
                          [batch_size, num_classes])
 
+  def testBuildPreLogitsNetwork(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+    self.assertTrue(net.op.name.startswith('MobilenetV1/Logits/AvgPool'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
+    self.assertFalse('Logits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+
   def testBuildBaseNetwork(self):
     batch_size = 5
     height, width = 224, 224
@@ -383,6 +396,25 @@ class MobilenetV1Test(tf.test.TestCase):
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
+  def testGlobalPoolUnknownImageShape(self):
+    tf.reset_default_graph()
+    batch_size = 2
+    height, width = 300, 400
+    num_classes = 1000
+    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes,
+                                                     global_pool=True)
+      self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Conv2d_13_pointwise']
+      feed_dict = {inputs: input_np}
+      tf.global_variables_initializer().run()
+      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 10, 13, 1024])
+
   def testUnknowBatchSize(self):
     batch_size = 1
     height, width = 224, 224
diff --git a/research/slim/nets/nasnet/README.md b/research/slim/nets/nasnet/README.md
new file mode 100644
index 00000000..ea9e88c2
--- /dev/null
+++ b/research/slim/nets/nasnet/README.md
@@ -0,0 +1,66 @@
+# TensorFlow-Slim NASNet-A Implementation/Checkpoints
+This directory contains the code for the NASNet-A model from the paper
+[Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/abs/1707.07012) by Zoph et al.
+In nasnet.py there are three different configurations of NASNet-A that are implementented. One of the models is the NASNet-A built for CIFAR-10 and the
+other two are variants of NASNet-A trained on ImageNet, which are listed below.
+
+# Pre-Trained Models
+Two NASNet-A checkpoints are available that have been trained on the
+[ILSVRC-2012-CLS](http://www.image-net.org/challenges/LSVRC/2012/)
+image classification dataset. Accuracies were computed by evaluating using a single image crop.
+
+Model Checkpoint | Million MACs | Million Parameters | Top-1 Accuracy| Top-5 Accuracy |
+:----:|:------------:|:----------:|:-------:|:-------:|
+[NASNet-A_Mobile_224](https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_mobile_04_10_2017.tar.gz)|564|5.3|74.0|91.6|
+[NASNet-A_Large_331](https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_large_04_10_2017.tar.gz)|23800|88.9|82.7|96.2|
+
+
+Here is an example of how to download the NASNet-A_Mobile_224 checkpoint. The way to download the NASNet-A_Large_331 is the same.
+
+```shell
+CHECKPOINT_DIR=/tmp/checkpoints
+mkdir ${CHECKPOINT_DIR}
+cd ${CHECKPOINT_DIR}
+wget https://storage.googleapis.com/download.tensorflow.org/models/nasnet-a_mobile_04_10_2017.tar.gz
+tar -xvf nasnet-a_mobile_04_10_2017.tar.gz
+rm nasnet-a_mobile_04_10_2017.tar.gz
+```
+More information on integrating NASNet Models into your project can be found at the [TF-Slim Image Classification Library](https://github.com/tensorflow/models/blob/master/research/slim/README.md).
+
+To get started running models on-device go to [TensorFlow Mobile](https://www.tensorflow.org/mobile/).
+
+###Sample Commands for using NASNet-A Mobile and Large Checkpoints for Inference
+-------
+Run eval with the NASNet-A mobile ImageNet model
+
+```shell
+DATASET_DIR=/tmp/imagenet
+EVAL_DIR=/tmp/tfmodel/eval
+CHECKPOINT_DIR=/tmp/checkpoints/model.ckpt
+python tensorflow_models/research/slim/eval_image_classifier \
+--checkpoint_path=${CHECKPOINT_DIR} \
+--eval_dir=${EVAL_DIR} \
+--dataset_dir=${DATASET_DIR} \
+--dataset_name=imagenet \
+--dataset_split_name=validation \
+--model_name=nasnet_mobile \
+--eval_image_size=224 \
+--moving_average_decay=0.9999 \
+```
+
+Run eval with the NASNet-A large ImageNet model
+
+```shell
+DATASET_DIR=/tmp/imagenet
+EVAL_DIR=/tmp/tfmodel/eval
+CHECKPOINT_DIR=/tmp/checkpoints/model.ckpt
+python tensorflow_models/research/slim/eval_image_classifier \
+--checkpoint_path=${CHECKPOINT_DIR} \
+--eval_dir=${EVAL_DIR} \
+--dataset_dir=${DATASET_DIR} \
+--dataset_name=imagenet \
+--dataset_split_name=validation \
+--model_name=nasnet_large \
+--eval_image_size=331 \
+--moving_average_decay=0.9999 \
+```
diff --git a/research/slim/nets/nasnet/nasnet.py b/research/slim/nets/nasnet/nasnet.py
new file mode 100644
index 00000000..7e5d79f0
--- /dev/null
+++ b/research/slim/nets/nasnet/nasnet.py
@@ -0,0 +1,515 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Contains the definition for the NASNet classification networks.
+
+Paper: https://arxiv.org/abs/1707.07012
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from nets.nasnet import nasnet_utils
+
+arg_scope = tf.contrib.framework.arg_scope
+slim = tf.contrib.slim
+
+
+# Notes for training NASNet Cifar Model
+# -------------------------------------
+# batch_size: 32
+# learning rate: 0.025
+# cosine (single period) learning rate decay
+# auxiliary head loss weighting: 0.4
+# clip global norm of all gradients by 5
+def _cifar_config(is_training=True):
+  drop_path_keep_prob = 1.0 if not is_training else 0.6
+  return tf.contrib.training.HParams(
+      stem_multiplier=3.0,
+      drop_path_keep_prob=drop_path_keep_prob,
+      num_cells=18,
+      use_aux_head=1,
+      num_conv_filters=32,
+      dense_dropout_keep_prob=1.0,
+      filter_scaling_rate=2.0,
+      num_reduction_layers=2,
+      data_format='NHWC',
+      skip_reduction_layer_input=0,
+      # 600 epochs with a batch size of 32
+      # This is used for the drop path probabilities since it needs to increase
+      # the drop out probability over the course of training.
+      total_training_steps=937500,
+  )
+
+
+# Notes for training large NASNet model on ImageNet
+# -------------------------------------
+# batch size (per replica): 16
+# learning rate: 0.015 * 100
+# learning rate decay factor: 0.97
+# num epochs per decay: 2.4
+# sync sgd with 100 replicas
+# auxiliary head loss weighting: 0.4
+# label smoothing: 0.1
+# clip global norm of all gradients by 10
+def _large_imagenet_config(is_training=True):
+  drop_path_keep_prob = 1.0 if not is_training else 0.7
+  return tf.contrib.training.HParams(
+      stem_multiplier=3.0,
+      dense_dropout_keep_prob=0.5,
+      num_cells=18,
+      filter_scaling_rate=2.0,
+      num_conv_filters=168,
+      drop_path_keep_prob=drop_path_keep_prob,
+      use_aux_head=1,
+      num_reduction_layers=2,
+      data_format='NHWC',
+      skip_reduction_layer_input=1,
+      total_training_steps=250000,
+  )
+
+
+# Notes for training the mobile NASNet ImageNet model
+# -------------------------------------
+# batch size (per replica): 32
+# learning rate: 0.04 * 50
+# learning rate scaling factor: 0.97
+# num epochs per decay: 2.4
+# sync sgd with 50 replicas
+# auxiliary head weighting: 0.4
+# label smoothing: 0.1
+# clip global norm of all gradients by 10
+def _mobile_imagenet_config():
+  return tf.contrib.training.HParams(
+      stem_multiplier=1.0,
+      dense_dropout_keep_prob=0.5,
+      num_cells=12,
+      filter_scaling_rate=2.0,
+      drop_path_keep_prob=1.0,
+      num_conv_filters=44,
+      use_aux_head=1,
+      num_reduction_layers=2,
+      data_format='NHWC',
+      skip_reduction_layer_input=0,
+      total_training_steps=250000,
+  )
+
+
+def nasnet_cifar_arg_scope(weight_decay=5e-4,
+                           batch_norm_decay=0.9,
+                           batch_norm_epsilon=1e-5):
+  """Defines the default arg scope for the NASNet-A Cifar model.
+
+  Args:
+    weight_decay: The weight decay to use for regularizing the model.
+    batch_norm_decay: Decay for batch norm moving average.
+    batch_norm_epsilon: Small float added to variance to avoid dividing by zero
+      in batch norm.
+
+  Returns:
+    An `arg_scope` to use for the NASNet Cifar Model.
+  """
+  batch_norm_params = {
+      # Decay for the moving averages.
+      'decay': batch_norm_decay,
+      # epsilon to prevent 0s in variance.
+      'epsilon': batch_norm_epsilon,
+      'scale': True,
+      'fused': True,
+  }
+  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+      mode='FAN_OUT')
+  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
+                 weights_regularizer=weights_regularizer,
+                 weights_initializer=weights_initializer):
+    with arg_scope([slim.fully_connected],
+                   activation_fn=None, scope='FC'):
+      with arg_scope([slim.conv2d, slim.separable_conv2d],
+                     activation_fn=None, biases_initializer=None):
+        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:
+          return sc
+
+
+def nasnet_mobile_arg_scope(weight_decay=4e-5,
+                            batch_norm_decay=0.9997,
+                            batch_norm_epsilon=1e-3):
+  """Defines the default arg scope for the NASNet-A Mobile ImageNet model.
+
+  Args:
+    weight_decay: The weight decay to use for regularizing the model.
+    batch_norm_decay: Decay for batch norm moving average.
+    batch_norm_epsilon: Small float added to variance to avoid dividing by zero
+      in batch norm.
+
+  Returns:
+    An `arg_scope` to use for the NASNet Mobile Model.
+  """
+  batch_norm_params = {
+      # Decay for the moving averages.
+      'decay': batch_norm_decay,
+      # epsilon to prevent 0s in variance.
+      'epsilon': batch_norm_epsilon,
+      'scale': True,
+      'fused': True,
+  }
+  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+      mode='FAN_OUT')
+  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
+                 weights_regularizer=weights_regularizer,
+                 weights_initializer=weights_initializer):
+    with arg_scope([slim.fully_connected],
+                   activation_fn=None, scope='FC'):
+      with arg_scope([slim.conv2d, slim.separable_conv2d],
+                     activation_fn=None, biases_initializer=None):
+        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:
+          return sc
+
+
+def nasnet_large_arg_scope(weight_decay=5e-5,
+                           batch_norm_decay=0.9997,
+                           batch_norm_epsilon=1e-3):
+  """Defines the default arg scope for the NASNet-A Large ImageNet model.
+
+  Args:
+    weight_decay: The weight decay to use for regularizing the model.
+    batch_norm_decay: Decay for batch norm moving average.
+    batch_norm_epsilon: Small float added to variance to avoid dividing by zero
+      in batch norm.
+
+  Returns:
+    An `arg_scope` to use for the NASNet Large Model.
+  """
+  batch_norm_params = {
+      # Decay for the moving averages.
+      'decay': batch_norm_decay,
+      # epsilon to prevent 0s in variance.
+      'epsilon': batch_norm_epsilon,
+      'scale': True,
+      'fused': True,
+  }
+  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+      mode='FAN_OUT')
+  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
+                 weights_regularizer=weights_regularizer,
+                 weights_initializer=weights_initializer):
+    with arg_scope([slim.fully_connected],
+                   activation_fn=None, scope='FC'):
+      with arg_scope([slim.conv2d, slim.separable_conv2d],
+                     activation_fn=None, biases_initializer=None):
+        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:
+          return sc
+
+
+def _build_aux_head(net, end_points, num_classes, hparams, scope):
+  """Auxiliary head used for all models across all datasets."""
+  with tf.variable_scope(scope):
+    aux_logits = tf.identity(net)
+    with tf.variable_scope('aux_logits'):
+      aux_logits = slim.avg_pool2d(
+          aux_logits, [5, 5], stride=3, padding='VALID')
+      aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='proj')
+      aux_logits = slim.batch_norm(aux_logits, scope='aux_bn0')
+      aux_logits = tf.nn.relu(aux_logits)
+      # Shape of feature map before the final layer.
+      shape = aux_logits.shape
+      if hparams.data_format == 'NHWC':
+        shape = shape[1:3]
+      else:
+        shape = shape[2:4]
+      aux_logits = slim.conv2d(aux_logits, 768, shape, padding='VALID')
+      aux_logits = slim.batch_norm(aux_logits, scope='aux_bn1')
+      aux_logits = tf.nn.relu(aux_logits)
+      aux_logits = tf.contrib.layers.flatten(aux_logits)
+      aux_logits = slim.fully_connected(aux_logits, num_classes)
+      end_points['AuxLogits'] = aux_logits
+
+
+def _imagenet_stem(inputs, hparams, stem_cell):
+  """Stem used for models trained on ImageNet."""
+  num_stem_cells = 2
+
+  # 149 x 149 x 32
+  num_stem_filters = int(32 * hparams.stem_multiplier)
+  net = slim.conv2d(
+      inputs, num_stem_filters, [3, 3], stride=2, scope='conv0',
+      padding='VALID')
+  net = slim.batch_norm(net, scope='conv0_bn')
+
+  # Run the reduction cells
+  cell_outputs = [None, net]
+  filter_scaling = 1.0 / (hparams.filter_scaling_rate**num_stem_cells)
+  for cell_num in range(num_stem_cells):
+    net = stem_cell(
+        net,
+        scope='cell_stem_{}'.format(cell_num),
+        filter_scaling=filter_scaling,
+        stride=2,
+        prev_layer=cell_outputs[-2],
+        cell_num=cell_num)
+    cell_outputs.append(net)
+    filter_scaling *= hparams.filter_scaling_rate
+  return net, cell_outputs
+
+
+def _cifar_stem(inputs, hparams):
+  """Stem used for models trained on Cifar."""
+  num_stem_filters = int(hparams.num_conv_filters * hparams.stem_multiplier)
+  net = slim.conv2d(
+      inputs,
+      num_stem_filters,
+      3,
+      scope='l1_stem_3x3')
+  net = slim.batch_norm(net, scope='l1_stem_bn')
+  return net, [None, net]
+
+
+def build_nasnet_cifar(
+    images, num_classes, is_training=True):
+  """Build NASNet model for the Cifar Dataset."""
+  hparams = _cifar_config(is_training=is_training)
+
+  if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
+    tf.logging.info('A GPU is available on the machine, consider using NCHW '
+                    'data format for increased speed on GPU.')
+
+  if hparams.data_format == 'NCHW':
+    images = tf.transpose(images, [0, 3, 1, 2])
+
+  # Calculate the total number of cells in the network
+  # Add 2 for the reduction cells
+  total_num_cells = hparams.num_cells + 2
+
+  normal_cell = nasnet_utils.NasNetANormalCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  reduction_cell = nasnet_utils.NasNetAReductionCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  with arg_scope([slim.dropout, nasnet_utils.drop_path, slim.batch_norm],
+                 is_training=is_training):
+    with arg_scope([slim.avg_pool2d,
+                    slim.max_pool2d,
+                    slim.conv2d,
+                    slim.batch_norm,
+                    slim.separable_conv2d,
+                    nasnet_utils.factorized_reduction,
+                    nasnet_utils.global_avg_pool,
+                    nasnet_utils.get_channel_index,
+                    nasnet_utils.get_channel_dim],
+                   data_format=hparams.data_format):
+      return _build_nasnet_base(images,
+                                normal_cell=normal_cell,
+                                reduction_cell=reduction_cell,
+                                num_classes=num_classes,
+                                hparams=hparams,
+                                is_training=is_training,
+                                stem_type='cifar')
+build_nasnet_cifar.default_image_size = 32
+
+
+def build_nasnet_mobile(images, num_classes,
+                        is_training=True, is_batchnorm_training=True,
+                        final_endpoint=None):
+  """Build NASNet Mobile model for the ImageNet Dataset."""
+  hparams = _mobile_imagenet_config()
+
+  if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
+    tf.logging.info('A GPU is available on the machine, consider using NCHW '
+                    'data format for increased speed on GPU.')
+
+  if hparams.data_format == 'NCHW':
+    images = tf.transpose(images, [0, 3, 1, 2])
+
+  # Calculate the total number of cells in the network
+  # Add 2 for the reduction cells
+  total_num_cells = hparams.num_cells + 2
+  # If ImageNet, then add an additional two for the stem cells
+  total_num_cells += 2
+
+  normal_cell = nasnet_utils.NasNetANormalCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  reduction_cell = nasnet_utils.NasNetAReductionCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  with arg_scope([slim.dropout, nasnet_utils.drop_path],
+                 is_training=is_training):
+    with arg_scope([slim.batch_norm], is_training=is_batchnorm_training):
+      with arg_scope([slim.avg_pool2d,
+                      slim.max_pool2d,
+                      slim.conv2d,
+                      slim.batch_norm,
+                      slim.separable_conv2d,
+                      nasnet_utils.factorized_reduction,
+                      nasnet_utils.global_avg_pool,
+                      nasnet_utils.get_channel_index,
+                      nasnet_utils.get_channel_dim],
+                     data_format=hparams.data_format):
+        return _build_nasnet_base(images,
+                                  normal_cell=normal_cell,
+                                  reduction_cell=reduction_cell,
+                                  num_classes=num_classes,
+                                  hparams=hparams,
+                                  is_training=is_training,
+                                  stem_type='imagenet',
+                                  final_endpoint=final_endpoint)
+build_nasnet_mobile.default_image_size = 224
+
+
+def build_nasnet_large(images, num_classes,
+                       is_training=True, is_batchnorm_training=True,
+                       final_endpoint=None):
+  """Build NASNet Large model for the ImageNet Dataset."""
+  hparams = _large_imagenet_config(is_training=is_training)
+
+  if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
+    tf.logging.info('A GPU is available on the machine, consider using NCHW '
+                    'data format for increased speed on GPU.')
+
+  if hparams.data_format == 'NCHW':
+    images = tf.transpose(images, [0, 3, 1, 2])
+
+  # Calculate the total number of cells in the network
+  # Add 2 for the reduction cells
+  total_num_cells = hparams.num_cells + 2
+  # If ImageNet, then add an additional two for the stem cells
+  total_num_cells += 2
+
+  normal_cell = nasnet_utils.NasNetANormalCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  reduction_cell = nasnet_utils.NasNetAReductionCell(
+      hparams.num_conv_filters, hparams.drop_path_keep_prob,
+      total_num_cells, hparams.total_training_steps)
+  with arg_scope([slim.dropout, nasnet_utils.drop_path],
+                 is_training=is_training):
+    with arg_scope([slim.batch_norm], is_training=is_batchnorm_training):
+      with arg_scope([slim.avg_pool2d,
+                      slim.max_pool2d,
+                      slim.conv2d,
+                      slim.batch_norm,
+                      slim.separable_conv2d,
+                      nasnet_utils.factorized_reduction,
+                      nasnet_utils.global_avg_pool,
+                      nasnet_utils.get_channel_index,
+                      nasnet_utils.get_channel_dim],
+                     data_format=hparams.data_format):
+        return _build_nasnet_base(images,
+                                  normal_cell=normal_cell,
+                                  reduction_cell=reduction_cell,
+                                  num_classes=num_classes,
+                                  hparams=hparams,
+                                  is_training=is_training,
+                                  stem_type='imagenet',
+                                  final_endpoint=final_endpoint)
+build_nasnet_large.default_image_size = 331
+
+
+def _build_nasnet_base(images,
+                       normal_cell,
+                       reduction_cell,
+                       num_classes,
+                       hparams,
+                       is_training,
+                       stem_type,
+                       final_endpoint=None):
+  """Constructs a NASNet image model."""
+
+  end_points = {}
+  def add_and_check_endpoint(endpoint_name, net):
+    end_points[endpoint_name] = net
+    return final_endpoint and (endpoint_name == final_endpoint)
+
+  # Find where to place the reduction cells or stride normal cells
+  reduction_indices = nasnet_utils.calc_reduction_layers(
+      hparams.num_cells, hparams.num_reduction_layers)
+  stem_cell = reduction_cell
+
+  if stem_type == 'imagenet':
+    stem = lambda: _imagenet_stem(images, hparams, stem_cell)
+  elif stem_type == 'cifar':
+    stem = lambda: _cifar_stem(images, hparams)
+  else:
+    raise ValueError('Unknown stem_type: ', stem_type)
+  net, cell_outputs = stem()
+  if add_and_check_endpoint('Stem', net): return net, end_points
+
+  # Setup for building in the auxiliary head.
+  aux_head_cell_idxes = []
+  if len(reduction_indices) >= 2:
+    aux_head_cell_idxes.append(reduction_indices[1] - 1)
+
+  # Run the cells
+  filter_scaling = 1.0
+  # true_cell_num accounts for the stem cells
+  true_cell_num = 2 if stem_type == 'imagenet' else 0
+  for cell_num in range(hparams.num_cells):
+    stride = 1
+    if hparams.skip_reduction_layer_input:
+      prev_layer = cell_outputs[-2]
+    if cell_num in reduction_indices:
+      filter_scaling *= hparams.filter_scaling_rate
+      net = reduction_cell(
+          net,
+          scope='reduction_cell_{}'.format(reduction_indices.index(cell_num)),
+          filter_scaling=filter_scaling,
+          stride=2,
+          prev_layer=cell_outputs[-2],
+          cell_num=true_cell_num)
+      if add_and_check_endpoint(
+          'Reduction_Cell_{}'.format(reduction_indices.index(cell_num)), net):
+        return net, end_points
+      true_cell_num += 1
+      cell_outputs.append(net)
+    if not hparams.skip_reduction_layer_input:
+      prev_layer = cell_outputs[-2]
+    net = normal_cell(
+        net,
+        scope='cell_{}'.format(cell_num),
+        filter_scaling=filter_scaling,
+        stride=stride,
+        prev_layer=prev_layer,
+        cell_num=true_cell_num)
+
+    if add_and_check_endpoint('Cell_{}'.format(cell_num), net):
+      return net, end_points
+    true_cell_num += 1
+    if (hparams.use_aux_head and cell_num in aux_head_cell_idxes and
+        num_classes and is_training):
+      aux_net = tf.nn.relu(net)
+      _build_aux_head(aux_net, end_points, num_classes, hparams,
+                      scope='aux_{}'.format(cell_num))
+    cell_outputs.append(net)
+
+  # Final softmax layer
+  with tf.variable_scope('final_layer'):
+    net = tf.nn.relu(net)
+    net = nasnet_utils.global_avg_pool(net)
+    if add_and_check_endpoint('global_pool', net) or num_classes is None:
+      return net, end_points
+    net = slim.dropout(net, hparams.dense_dropout_keep_prob, scope='dropout')
+    logits = slim.fully_connected(net, num_classes)
+
+    if add_and_check_endpoint('Logits', logits):
+      return net, end_points
+
+    predictions = tf.nn.softmax(logits, name='predictions')
+    if add_and_check_endpoint('Predictions', predictions):
+      return net, end_points
+  return logits, end_points
diff --git a/research/slim/nets/nasnet/nasnet_test.py b/research/slim/nets/nasnet/nasnet_test.py
new file mode 100644
index 00000000..d04d1f68
--- /dev/null
+++ b/research/slim/nets/nasnet/nasnet_test.py
@@ -0,0 +1,289 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for slim.nasnet."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from nets.nasnet import nasnet
+
+slim = tf.contrib.slim
+
+
+class NASNetTest(tf.test.TestCase):
+
+  def testBuildLogitsCifarModel(self):
+    batch_size = 5
+    height, width = 32, 32
+    num_classes = 10
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
+      logits, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
+    auxlogits = end_points['AuxLogits']
+    predictions = end_points['Predictions']
+    self.assertListEqual(auxlogits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(predictions.get_shape().as_list(),
+                         [batch_size, num_classes])
+
+  def testBuildLogitsMobileModel(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+      logits, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
+    auxlogits = end_points['AuxLogits']
+    predictions = end_points['Predictions']
+    self.assertListEqual(auxlogits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(predictions.get_shape().as_list(),
+                         [batch_size, num_classes])
+
+  def testBuildLogitsLargeModel(self):
+    batch_size = 5
+    height, width = 331, 331
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
+      logits, end_points = nasnet.build_nasnet_large(inputs, num_classes)
+    auxlogits = end_points['AuxLogits']
+    predictions = end_points['Predictions']
+    self.assertListEqual(auxlogits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertListEqual(predictions.get_shape().as_list(),
+                         [batch_size, num_classes])
+
+  def testBuildPreLogitsCifarModel(self):
+    batch_size = 5
+    height, width = 32, 32
+    num_classes = None
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
+      net, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
+    self.assertFalse('AuxLogits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+    self.assertTrue(net.op.name.startswith('final_layer/Mean'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 768])
+
+  def testBuildPreLogitsMobileModel(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+      net, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
+    self.assertFalse('AuxLogits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+    self.assertTrue(net.op.name.startswith('final_layer/Mean'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 1056])
+
+  def testBuildPreLogitsLargeModel(self):
+    batch_size = 5
+    height, width = 331, 331
+    num_classes = None
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
+      net, end_points = nasnet.build_nasnet_large(inputs, num_classes)
+    self.assertFalse('AuxLogits' in end_points)
+    self.assertFalse('Predictions' in end_points)
+    self.assertTrue(net.op.name.startswith('final_layer/Mean'))
+    self.assertListEqual(net.get_shape().as_list(), [batch_size, 4032])
+
+  def testAllEndPointsShapesCifarModel(self):
+    batch_size = 5
+    height, width = 32, 32
+    num_classes = 10
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
+      _, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
+    endpoints_shapes = {'Stem': [batch_size, 32, 32, 96],
+                        'Cell_0': [batch_size, 32, 32, 192],
+                        'Cell_1': [batch_size, 32, 32, 192],
+                        'Cell_2': [batch_size, 32, 32, 192],
+                        'Cell_3': [batch_size, 32, 32, 192],
+                        'Cell_4': [batch_size, 32, 32, 192],
+                        'Cell_5': [batch_size, 32, 32, 192],
+                        'Cell_6': [batch_size, 16, 16, 384],
+                        'Cell_7': [batch_size, 16, 16, 384],
+                        'Cell_8': [batch_size, 16, 16, 384],
+                        'Cell_9': [batch_size, 16, 16, 384],
+                        'Cell_10': [batch_size, 16, 16, 384],
+                        'Cell_11': [batch_size, 16, 16, 384],
+                        'Cell_12': [batch_size, 8, 8, 768],
+                        'Cell_13': [batch_size, 8, 8, 768],
+                        'Cell_14': [batch_size, 8, 8, 768],
+                        'Cell_15': [batch_size, 8, 8, 768],
+                        'Cell_16': [batch_size, 8, 8, 768],
+                        'Cell_17': [batch_size, 8, 8, 768],
+                        'Reduction_Cell_0': [batch_size, 16, 16, 256],
+                        'Reduction_Cell_1': [batch_size, 8, 8, 512],
+                        'global_pool': [batch_size, 768],
+                        # Logits and predictions
+                        'AuxLogits': [batch_size, num_classes],
+                        'Logits': [batch_size, num_classes],
+                        'Predictions': [batch_size, num_classes]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name in endpoints_shapes:
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      expected_shape = endpoints_shapes[endpoint_name]
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testAllEndPointsShapesMobileModel(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+      _, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
+    endpoints_shapes = {'Stem': [batch_size, 28, 28, 88],
+                        'Cell_0': [batch_size, 28, 28, 264],
+                        'Cell_1': [batch_size, 28, 28, 264],
+                        'Cell_2': [batch_size, 28, 28, 264],
+                        'Cell_3': [batch_size, 28, 28, 264],
+                        'Cell_4': [batch_size, 14, 14, 528],
+                        'Cell_5': [batch_size, 14, 14, 528],
+                        'Cell_6': [batch_size, 14, 14, 528],
+                        'Cell_7': [batch_size, 14, 14, 528],
+                        'Cell_8': [batch_size, 7, 7, 1056],
+                        'Cell_9': [batch_size, 7, 7, 1056],
+                        'Cell_10': [batch_size, 7, 7, 1056],
+                        'Cell_11': [batch_size, 7, 7, 1056],
+                        'Reduction_Cell_0': [batch_size, 14, 14, 352],
+                        'Reduction_Cell_1': [batch_size, 7, 7, 704],
+                        'global_pool': [batch_size, 1056],
+                        # Logits and predictions
+                        'AuxLogits': [batch_size, num_classes],
+                        'Logits': [batch_size, num_classes],
+                        'Predictions': [batch_size, num_classes]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name in endpoints_shapes:
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      expected_shape = endpoints_shapes[endpoint_name]
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testAllEndPointsShapesLargeModel(self):
+    batch_size = 5
+    height, width = 331, 331
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
+      _, end_points = nasnet.build_nasnet_large(inputs, num_classes)
+    endpoints_shapes = {'Stem': [batch_size, 42, 42, 336],
+                        'Cell_0': [batch_size, 42, 42, 1008],
+                        'Cell_1': [batch_size, 42, 42, 1008],
+                        'Cell_2': [batch_size, 42, 42, 1008],
+                        'Cell_3': [batch_size, 42, 42, 1008],
+                        'Cell_4': [batch_size, 42, 42, 1008],
+                        'Cell_5': [batch_size, 42, 42, 1008],
+                        'Cell_6': [batch_size, 21, 21, 2016],
+                        'Cell_7': [batch_size, 21, 21, 2016],
+                        'Cell_8': [batch_size, 21, 21, 2016],
+                        'Cell_9': [batch_size, 21, 21, 2016],
+                        'Cell_10': [batch_size, 21, 21, 2016],
+                        'Cell_11': [batch_size, 21, 21, 2016],
+                        'Cell_12': [batch_size, 11, 11, 4032],
+                        'Cell_13': [batch_size, 11, 11, 4032],
+                        'Cell_14': [batch_size, 11, 11, 4032],
+                        'Cell_15': [batch_size, 11, 11, 4032],
+                        'Cell_16': [batch_size, 11, 11, 4032],
+                        'Cell_17': [batch_size, 11, 11, 4032],
+                        'Reduction_Cell_0': [batch_size, 21, 21, 1344],
+                        'Reduction_Cell_1': [batch_size, 11, 11, 2688],
+                        'global_pool': [batch_size, 4032],
+                        # Logits and predictions
+                        'AuxLogits': [batch_size, num_classes],
+                        'Logits': [batch_size, num_classes],
+                        'Predictions': [batch_size, num_classes]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name in endpoints_shapes:
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      expected_shape = endpoints_shapes[endpoint_name]
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testVariablesSetDeviceMobileModel(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    tf.train.create_global_step()
+    # Force all Variables to reside on the device.
+    with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
+      with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+        nasnet.build_nasnet_mobile(inputs, num_classes)
+    with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
+      with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+        nasnet.build_nasnet_mobile(inputs, num_classes)
+    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+      self.assertDeviceEqual(v.device, '/cpu:0')
+    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+      self.assertDeviceEqual(v.device, '/gpu:0')
+
+  def testUnknownBatchSizeMobileModel(self):
+    batch_size = 1
+    height, width = 224, 224
+    num_classes = 1000
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+      with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+        logits, _ = nasnet.build_nasnet_mobile(inputs, num_classes)
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [None, num_classes])
+      images = tf.random_uniform((batch_size, height, width, 3))
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(logits, {inputs: images.eval()})
+      self.assertEquals(output.shape, (batch_size, num_classes))
+
+  def testEvaluationMobileModel(self):
+    batch_size = 2
+    height, width = 224, 224
+    num_classes = 1000
+    with self.test_session() as sess:
+      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
+        logits, _ = nasnet.build_nasnet_mobile(eval_inputs,
+                                               num_classes,
+                                               is_training=False)
+      predictions = tf.argmax(logits, 1)
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(predictions)
+      self.assertEquals(output.shape, (batch_size,))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/slim/nets/nasnet/nasnet_utils.py b/research/slim/nets/nasnet/nasnet_utils.py
new file mode 100644
index 00000000..c3c1a498
--- /dev/null
+++ b/research/slim/nets/nasnet/nasnet_utils.py
@@ -0,0 +1,479 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""A custom module for some common operations used by NASNet.
+
+Functions exposed in this file:
+- calc_reduction_layers
+- get_channel_index
+- get_channel_dim
+- global_avg_pool
+- factorized_reduction
+- drop_path
+
+Classes exposed in this file:
+- NasNetABaseCell
+- NasNetANormalCell
+- NasNetAReductionCell
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import google3
+
+import tensorflow as tf
+
+
+arg_scope = tf.contrib.framework.arg_scope
+slim = tf.contrib.slim
+
+DATA_FORMAT_NCHW = 'NCHW'
+DATA_FORMAT_NHWC = 'NHWC'
+INVALID = 'null'
+
+
+def calc_reduction_layers(num_cells, num_reduction_layers):
+  """Figure out what layers should have reductions."""
+  reduction_layers = []
+  for pool_num in range(1, num_reduction_layers + 1):
+    layer_num = (float(pool_num) / (num_reduction_layers + 1)) * num_cells
+    layer_num = int(layer_num)
+    reduction_layers.append(layer_num)
+  return reduction_layers
+
+
+@tf.contrib.framework.add_arg_scope
+def get_channel_index(data_format=INVALID):
+  assert data_format != INVALID
+  axis = 3 if data_format == 'NHWC' else 1
+  return axis
+
+
+@tf.contrib.framework.add_arg_scope
+def get_channel_dim(shape, data_format=INVALID):
+  assert data_format != INVALID
+  assert len(shape) == 4
+  if data_format == 'NHWC':
+    return int(shape[3])
+  elif data_format == 'NCHW':
+    return int(shape[1])
+  else:
+    raise ValueError('Not a valid data_format', data_format)
+
+
+@tf.contrib.framework.add_arg_scope
+def global_avg_pool(x, data_format=INVALID):
+  """Average pool away the height and width spatial dimensions of x."""
+  assert data_format != INVALID
+  assert data_format in ['NHWC', 'NCHW']
+  assert x.shape.ndims == 4
+  if data_format == 'NHWC':
+    return tf.reduce_mean(x, [1, 2])
+  else:
+    return tf.reduce_mean(x, [2, 3])
+
+
+@tf.contrib.framework.add_arg_scope
+def factorized_reduction(net, output_filters, stride, data_format=INVALID):
+  """Reduces the shape of net without information loss due to striding."""
+  assert output_filters % 2 == 0, (
+      'Need even number of filters when using this factorized reduction.')
+  assert data_format != INVALID
+  if stride == 1:
+    net = slim.conv2d(net, output_filters, 1, scope='path_conv')
+    net = slim.batch_norm(net, scope='path_bn')
+    return net
+  if data_format == 'NHWC':
+    stride_spec = [1, stride, stride, 1]
+  else:
+    stride_spec = [1, 1, stride, stride]
+
+  # Skip path 1
+  path1 = tf.nn.avg_pool(
+      net, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)
+  path1 = slim.conv2d(path1, int(output_filters / 2), 1, scope='path1_conv')
+
+  # Skip path 2
+  # First pad with 0's on the right and bottom, then shift the filter to
+  # include those 0's that were added.
+  if data_format == 'NHWC':
+    pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]
+    path2 = tf.pad(net, pad_arr)[:, 1:, 1:, :]
+    concat_axis = 3
+  else:
+    pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]
+    path2 = tf.pad(net, pad_arr)[:, :, 1:, 1:]
+    concat_axis = 1
+
+  path2 = tf.nn.avg_pool(
+      path2, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)
+  path2 = slim.conv2d(path2, int(output_filters / 2), 1, scope='path2_conv')
+
+  # Concat and apply BN
+  final_path = tf.concat(values=[path1, path2], axis=concat_axis)
+  final_path = slim.batch_norm(final_path, scope='final_path_bn')
+  return final_path
+
+
+@tf.contrib.framework.add_arg_scope
+def drop_path(net, keep_prob, is_training=True):
+  """Drops out a whole example hiddenstate with the specified probability."""
+  if is_training:
+    batch_size = tf.shape(net)[0]
+    noise_shape = [batch_size, 1, 1, 1]
+    random_tensor = keep_prob
+    random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)
+    binary_tensor = tf.floor(random_tensor)
+    net = tf.div(net, keep_prob) * binary_tensor
+  return net
+
+
+def _operation_to_filter_shape(operation):
+  splitted_operation = operation.split('x')
+  filter_shape = int(splitted_operation[0][-1])
+  assert filter_shape == int(
+      splitted_operation[1][0]), 'Rectangular filters not supported.'
+  return filter_shape
+
+
+def _operation_to_num_layers(operation):
+  splitted_operation = operation.split('_')
+  if 'x' in splitted_operation[-1]:
+    return 1
+  return int(splitted_operation[-1])
+
+
+def _operation_to_info(operation):
+  """Takes in operation name and returns meta information.
+
+  An example would be 'separable_3x3_4' -> (3, 4).
+
+  Args:
+    operation: String that corresponds to convolution operation.
+
+  Returns:
+    Tuple of (filter shape, num layers).
+  """
+  num_layers = _operation_to_num_layers(operation)
+  filter_shape = _operation_to_filter_shape(operation)
+  return num_layers, filter_shape
+
+
+def _stacked_separable_conv(net, stride, operation, filter_size):
+  """Takes in an operations and parses it to the correct sep operation."""
+  num_layers, kernel_size = _operation_to_info(operation)
+  for layer_num in range(num_layers - 1):
+    net = tf.nn.relu(net)
+    net = slim.separable_conv2d(
+        net,
+        filter_size,
+        kernel_size,
+        depth_multiplier=1,
+        scope='separable_{0}x{0}_{1}'.format(kernel_size, layer_num + 1),
+        stride=stride)
+    net = slim.batch_norm(
+        net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, layer_num + 1))
+    stride = 1
+  net = tf.nn.relu(net)
+  net = slim.separable_conv2d(
+      net,
+      filter_size,
+      kernel_size,
+      depth_multiplier=1,
+      scope='separable_{0}x{0}_{1}'.format(kernel_size, num_layers),
+      stride=stride)
+  net = slim.batch_norm(
+      net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, num_layers))
+  return net
+
+
+def _operation_to_pooling_type(operation):
+  """Takes in the operation string and returns the pooling type."""
+  splitted_operation = operation.split('_')
+  return splitted_operation[0]
+
+
+def _operation_to_pooling_shape(operation):
+  """Takes in the operation string and returns the pooling kernel shape."""
+  splitted_operation = operation.split('_')
+  shape = splitted_operation[-1]
+  assert 'x' in shape
+  filter_height, filter_width = shape.split('x')
+  assert filter_height == filter_width
+  return int(filter_height)
+
+
+def _operation_to_pooling_info(operation):
+  """Parses the pooling operation string to return its type and shape."""
+  pooling_type = _operation_to_pooling_type(operation)
+  pooling_shape = _operation_to_pooling_shape(operation)
+  return pooling_type, pooling_shape
+
+
+def _pooling(net, stride, operation):
+  """Parses operation and performs the correct pooling operation on net."""
+  padding = 'SAME'
+  pooling_type, pooling_shape = _operation_to_pooling_info(operation)
+  if pooling_type == 'avg':
+    net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding=padding)
+  elif pooling_type == 'max':
+    net = slim.max_pool2d(net, pooling_shape, stride=stride, padding=padding)
+  else:
+    raise NotImplementedError('Unimplemented pooling type: ', pooling_type)
+  return net
+
+
+class NasNetABaseCell(object):
+  """NASNet Cell class that is used as a 'layer' in image architectures.
+
+  Args:
+    num_conv_filters: The number of filters for each convolution operation.
+    operations: List of operations that are performed in the NASNet Cell in
+      order.
+    used_hiddenstates: Binary array that signals if the hiddenstate was used
+      within the cell. This is used to determine what outputs of the cell
+      should be concatenated together.
+    hiddenstate_indices: Determines what hiddenstates should be combined
+      together with the specified operations to create the NASNet cell.
+  """
+
+  def __init__(self, num_conv_filters, operations, used_hiddenstates,
+               hiddenstate_indices, drop_path_keep_prob, total_num_cells,
+               total_training_steps):
+    self._num_conv_filters = num_conv_filters
+    self._operations = operations
+    self._used_hiddenstates = used_hiddenstates
+    self._hiddenstate_indices = hiddenstate_indices
+    self._drop_path_keep_prob = drop_path_keep_prob
+    self._total_num_cells = total_num_cells
+    self._total_training_steps = total_training_steps
+
+  def _reduce_prev_layer(self, prev_layer, curr_layer):
+    """Matches dimension of prev_layer to the curr_layer."""
+    # Set the prev layer to the current layer if it is none
+    if prev_layer is None:
+      return curr_layer
+    curr_num_filters = self._filter_size
+    prev_num_filters = get_channel_dim(prev_layer.shape)
+    curr_filter_shape = int(curr_layer.shape[2])
+    prev_filter_shape = int(prev_layer.shape[2])
+    if curr_filter_shape != prev_filter_shape:
+      prev_layer = tf.nn.relu(prev_layer)
+      prev_layer = factorized_reduction(
+          prev_layer, curr_num_filters, stride=2)
+    elif curr_num_filters != prev_num_filters:
+      prev_layer = tf.nn.relu(prev_layer)
+      prev_layer = slim.conv2d(
+          prev_layer, curr_num_filters, 1, scope='prev_1x1')
+      prev_layer = slim.batch_norm(prev_layer, scope='prev_bn')
+    return prev_layer
+
+  def _cell_base(self, net, prev_layer):
+    """Runs the beginning of the conv cell before the predicted ops are run."""
+    num_filters = self._filter_size
+
+    # Check to be sure prev layer stuff is setup correctly
+    prev_layer = self._reduce_prev_layer(prev_layer, net)
+
+    net = tf.nn.relu(net)
+    net = slim.conv2d(net, num_filters, 1, scope='1x1')
+    net = slim.batch_norm(net, scope='beginning_bn')
+    split_axis = get_channel_index()
+    net = tf.split(
+        axis=split_axis, num_or_size_splits=1, value=net)
+    for split in net:
+      assert int(split.shape[split_axis] == int(self._num_conv_filters *
+                                                self._filter_scaling))
+    net.append(prev_layer)
+    return net
+
+  def __call__(self, net, scope=None, filter_scaling=1, stride=1,
+               prev_layer=None, cell_num=-1):
+    """Runs the conv cell."""
+    self._cell_num = cell_num
+    self._filter_scaling = filter_scaling
+    self._filter_size = int(self._num_conv_filters * filter_scaling)
+
+    i = 0
+    with tf.variable_scope(scope):
+      net = self._cell_base(net, prev_layer)
+      for iteration in range(5):
+        with tf.variable_scope('comb_iter_{}'.format(iteration)):
+          left_hiddenstate_idx, right_hiddenstate_idx = (
+              self._hiddenstate_indices[i],
+              self._hiddenstate_indices[i + 1])
+          original_input_left = left_hiddenstate_idx < 2
+          original_input_right = right_hiddenstate_idx < 2
+          h1 = net[left_hiddenstate_idx]
+          h2 = net[right_hiddenstate_idx]
+
+          operation_left = self._operations[i]
+          operation_right = self._operations[i+1]
+          i += 2
+          # Apply conv operations
+          with tf.variable_scope('left'):
+            h1 = self._apply_conv_operation(h1, operation_left,
+                                            stride, original_input_left)
+          with tf.variable_scope('right'):
+            h2 = self._apply_conv_operation(h2, operation_right,
+                                            stride, original_input_right)
+
+          # Combine hidden states using 'add'.
+          with tf.variable_scope('combine'):
+            h = h1 + h2
+
+          # Add hiddenstate to the list of hiddenstates we can choose from
+          net.append(h)
+
+      with tf.variable_scope('cell_output'):
+        net = self._combine_unused_states(net)
+
+      return net
+
+  def _apply_conv_operation(self, net, operation,
+                            stride, is_from_original_input):
+    """Applies the predicted conv operation to net."""
+    # Dont stride if this is not one of the original hiddenstates
+    if stride > 1 and not is_from_original_input:
+      stride = 1
+    input_filters = get_channel_dim(net.shape)
+    filter_size = self._filter_size
+    if 'separable' in operation:
+      net = _stacked_separable_conv(net, stride, operation, filter_size)
+    elif operation in ['none']:
+      # Check if a stride is needed, then use a strided 1x1 here
+      if stride > 1 or (input_filters != filter_size):
+        net = tf.nn.relu(net)
+        net = slim.conv2d(net, filter_size, 1, stride=stride, scope='1x1')
+        net = slim.batch_norm(net, scope='bn_1')
+    elif 'pool' in operation:
+      net = _pooling(net, stride, operation)
+      if input_filters != filter_size:
+        net = slim.conv2d(net, filter_size, 1, stride=1, scope='1x1')
+        net = slim.batch_norm(net, scope='bn_1')
+    else:
+      raise ValueError('Unimplemented operation', operation)
+
+    if operation != 'none':
+      net = self._apply_drop_path(net)
+    return net
+
+  def _combine_unused_states(self, net):
+    """Concatenate the unused hidden states of the cell."""
+    used_hiddenstates = self._used_hiddenstates
+
+    final_height = int(net[-1].shape[2])
+    final_num_filters = get_channel_dim(net[-1].shape)
+    assert len(used_hiddenstates) == len(net)
+    for idx, used_h in enumerate(used_hiddenstates):
+      curr_height = int(net[idx].shape[2])
+      curr_num_filters = get_channel_dim(net[idx].shape)
+
+      # Determine if a reduction should be applied to make the number of
+      # filters match.
+      should_reduce = final_num_filters != curr_num_filters
+      should_reduce = (final_height != curr_height) or should_reduce
+      should_reduce = should_reduce and not used_h
+      if should_reduce:
+        stride = 2 if final_height != curr_height else 1
+        with tf.variable_scope('reduction_{}'.format(idx)):
+          net[idx] = factorized_reduction(
+              net[idx], final_num_filters, stride)
+
+    states_to_combine = (
+        [h for h, is_used in zip(net, used_hiddenstates) if not is_used])
+
+    # Return the concat of all the states
+    concat_axis = get_channel_index()
+    net = tf.concat(values=states_to_combine, axis=concat_axis)
+    return net
+
+  def _apply_drop_path(self, net):
+    """Apply drop_path regularization to net."""
+    drop_path_keep_prob = self._drop_path_keep_prob
+    if drop_path_keep_prob < 1.0:
+      # Scale keep prob by layer number
+      assert self._cell_num != -1
+      # The added 2 is for the reduction cells
+      num_cells = self._total_num_cells
+      layer_ratio = (self._cell_num + 1)/float(num_cells)
+      with tf.device('/cpu:0'):
+        tf.summary.scalar('layer_ratio', layer_ratio)
+      drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)
+      # Decrease the keep probability over time
+      current_step = tf.cast(tf.contrib.framework.get_or_create_global_step(),
+                             tf.float32)
+      drop_path_burn_in_steps = self._total_training_steps
+      current_ratio = (
+          current_step / drop_path_burn_in_steps)
+      current_ratio = tf.minimum(1.0, current_ratio)
+      with tf.device('/cpu:0'):
+        tf.summary.scalar('current_ratio', current_ratio)
+      drop_path_keep_prob = (
+          1 - current_ratio * (1 - drop_path_keep_prob))
+      with tf.device('/cpu:0'):
+        tf.summary.scalar('drop_path_keep_prob', drop_path_keep_prob)
+      net = drop_path(net, drop_path_keep_prob)
+    return net
+
+
+class NasNetANormalCell(NasNetABaseCell):
+  """NASNetA Normal Cell."""
+
+  def __init__(self, num_conv_filters, drop_path_keep_prob, total_num_cells,
+               total_training_steps):
+    operations = ['separable_5x5_2',
+                  'separable_3x3_2',
+                  'separable_5x5_2',
+                  'separable_3x3_2',
+                  'avg_pool_3x3',
+                  'none',
+                  'avg_pool_3x3',
+                  'avg_pool_3x3',
+                  'separable_3x3_2',
+                  'none']
+    used_hiddenstates = [1, 0, 0, 0, 0, 0, 0]
+    hiddenstate_indices = [0, 1, 1, 1, 0, 1, 1, 1, 0, 0]
+    super(NasNetANormalCell, self).__init__(num_conv_filters, operations,
+                                            used_hiddenstates,
+                                            hiddenstate_indices,
+                                            drop_path_keep_prob,
+                                            total_num_cells,
+                                            total_training_steps)
+
+
+class NasNetAReductionCell(NasNetABaseCell):
+  """NASNetA Reduction Cell."""
+
+  def __init__(self, num_conv_filters, drop_path_keep_prob, total_num_cells,
+               total_training_steps):
+    operations = ['separable_5x5_2',
+                  'separable_7x7_2',
+                  'max_pool_3x3',
+                  'separable_7x7_2',
+                  'avg_pool_3x3',
+                  'separable_5x5_2',
+                  'none',
+                  'avg_pool_3x3',
+                  'separable_3x3_2',
+                  'max_pool_3x3']
+    used_hiddenstates = [1, 1, 1, 0, 0, 0, 0]
+    hiddenstate_indices = [0, 1, 0, 1, 0, 1, 3, 2, 2, 0]
+    super(NasNetAReductionCell, self).__init__(num_conv_filters, operations,
+                                               used_hiddenstates,
+                                               hiddenstate_indices,
+                                               drop_path_keep_prob,
+                                               total_num_cells,
+                                               total_training_steps)
diff --git a/research/slim/nets/nasnet/nasnet_utils_test.py b/research/slim/nets/nasnet/nasnet_utils_test.py
new file mode 100644
index 00000000..60bf9029
--- /dev/null
+++ b/research/slim/nets/nasnet/nasnet_utils_test.py
@@ -0,0 +1,62 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for slim.nets.nasnet.nasnet_utils."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from nets.nasnet import nasnet_utils
+
+
+class NasnetUtilsTest(tf.test.TestCase):
+
+  def testCalcReductionLayers(self):
+    num_cells = 18
+    num_reduction_layers = 2
+    reduction_layers = nasnet_utils.calc_reduction_layers(
+        num_cells, num_reduction_layers)
+    self.assertEqual(len(reduction_layers), 2)
+    self.assertEqual(reduction_layers[0], 6)
+    self.assertEqual(reduction_layers[1], 12)
+
+  def testGetChannelIndex(self):
+    data_formats = ['NHWC', 'NCHW']
+    for data_format in data_formats:
+      index = nasnet_utils.get_channel_index(data_format)
+      correct_index = 3 if data_format == 'NHWC' else 1
+      self.assertEqual(index, correct_index)
+
+  def testGetChannelDim(self):
+    data_formats = ['NHWC', 'NCHW']
+    shape = [10, 20, 30, 40]
+    for data_format in data_formats:
+      dim = nasnet_utils.get_channel_dim(shape, data_format)
+      correct_dim = shape[3] if data_format == 'NHWC' else shape[1]
+      self.assertEqual(dim, correct_dim)
+
+  def testGlobalAvgPool(self):
+    data_formats = ['NHWC', 'NCHW']
+    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))
+    for data_format in data_formats:
+      output = nasnet_utils.global_avg_pool(
+          inputs, data_format)
+      self.assertEqual(output.shape, [5, 10])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/slim/nets/nets_factory.py b/research/slim/nets/nets_factory.py
index 8edc9796..4862a2c7 100644
--- a/research/slim/nets/nets_factory.py
+++ b/research/slim/nets/nets_factory.py
@@ -1,4 +1,4 @@
-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -30,6 +30,7 @@ from nets import overfeat
 from nets import resnet_v1
 from nets import resnet_v2
 from nets import vgg
+from nets.nasnet import nasnet
 
 slim = tf.contrib.slim
 
@@ -57,6 +58,9 @@ networks_map = {'alexnet_v2': alexnet.alexnet_v2,
                 'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_075,
                 'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_050,
                 'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_025,
+                'nasnet_cifar': nasnet.build_nasnet_cifar,
+                'nasnet_mobile': nasnet.build_nasnet_mobile,
+                'nasnet_large': nasnet.build_nasnet_large,
                }
 
 arg_scopes_map = {'alexnet_v2': alexnet.alexnet_v2_arg_scope,
@@ -84,6 +88,9 @@ arg_scopes_map = {'alexnet_v2': alexnet.alexnet_v2_arg_scope,
                   'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_arg_scope,
                   'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_arg_scope,
                   'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_arg_scope,
+                  'nasnet_cifar': nasnet.nasnet_cifar_arg_scope,
+                  'nasnet_mobile': nasnet.nasnet_mobile_arg_scope,
+                  'nasnet_large': nasnet.nasnet_large_arg_scope,
                  }
 
 
@@ -92,7 +99,8 @@ def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):
 
   Args:
     name: The name of the network.
-    num_classes: The number of classes to use for classification.
+    num_classes: The number of classes to use for classification. If 0 or None,
+      the logits layer is omitted and its input features are returned instead.
     weight_decay: The l2 coefficient for the model weights.
     is_training: `True` if the model is being used for training and `False`
       otherwise.
@@ -100,7 +108,20 @@ def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):
   Returns:
     network_fn: A function that applies the model to a batch of images. It has
       the following signature:
-        logits, end_points = network_fn(images)
+          net, end_points = network_fn(images)
+      The `images` input is a tensor of shape [batch_size, height, width, 3]
+      with height = width = network_fn.default_image_size. (The permissibility
+      and treatment of other sizes depends on the network_fn.)
+      The returned `end_points` are a dictionary of intermediate activations.
+      The returned `net` is the topmost layer, depending on `num_classes`:
+      If `num_classes` was a non-zero integer, `net` is a logits tensor
+      of shape [batch_size, num_classes].
+      If `num_classes` was 0 or `None`, `net` is a tensor with the input
+      to the logits layer of shape [batch_size, 1, 1, num_features] or
+      [batch_size, num_features]. Dropout has not been applied to this
+      (even if the network's original classification does); it remains for
+      the caller to do this or not.
+
   Raises:
     ValueError: If network `name` is not recognized.
   """
@@ -108,10 +129,10 @@ def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):
     raise ValueError('Name of network unknown %s' % name)
   func = networks_map[name]
   @functools.wraps(func)
-  def network_fn(images):
+  def network_fn(images, **kwargs):
     arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
     with slim.arg_scope(arg_scope):
-      return func(images, num_classes, is_training=is_training)
+      return func(images, num_classes, is_training=is_training, **kwargs)
   if hasattr(func, 'default_image_size'):
     network_fn.default_image_size = func.default_image_size
 
diff --git a/research/slim/nets/overfeat.py b/research/slim/nets/overfeat.py
index 64a54252..069f550e 100644
--- a/research/slim/nets/overfeat.py
+++ b/research/slim/nets/overfeat.py
@@ -52,7 +52,8 @@ def overfeat(inputs,
              is_training=True,
              dropout_keep_prob=0.5,
              spatial_squeeze=True,
-             scope='overfeat'):
+             scope='overfeat',
+             global_pool=False):
   """Contains the model definition for the OverFeat network.
 
   The definition for the network was obtained from:
@@ -68,20 +69,26 @@ def overfeat(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
+      omitted and the input features to the logits layer are returned instead.
     is_training: whether or not the model is being trained.
     dropout_keep_prob: the probability that activations are kept in the dropout
       layers during training.
     spatial_squeeze: whether or not should squeeze the spatial dimensions of the
       outputs. Useful to remove unnecessary dimensions for classification.
     scope: Optional scope for the variables.
+    global_pool: Optional boolean flag. If True, the input to the classification
+      layer is avgpooled to size 1x1, for any input size. (This is not part
+      of the original OverFeat.)
 
   Returns:
-    the last op containing the log predictions and end_points dict.
-
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the non-dropped-out input to the logits layer (if num_classes is 0 or
+      None).
+    end_points: a dict of tensors with intermediate activations.
   """
   with tf.variable_scope(scope, 'overfeat', [inputs]) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                         outputs_collections=end_points_collection):
@@ -94,25 +101,31 @@ def overfeat(inputs,
       net = slim.conv2d(net, 1024, [3, 3], scope='conv4')
       net = slim.conv2d(net, 1024, [3, 3], scope='conv5')
       net = slim.max_pool2d(net, [2, 2], scope='pool5')
+
+      # Use conv2d instead of fully_connected layers.
       with slim.arg_scope([slim.conv2d],
                           weights_initializer=trunc_normal(0.005),
                           biases_initializer=tf.constant_initializer(0.1)):
-        # Use conv2d instead of fully_connected layers.
         net = slim.conv2d(net, 3072, [6, 6], padding='VALID', scope='fc6')
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
         net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
-        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                           scope='dropout7')
-        net = slim.conv2d(net, num_classes, [1, 1],
-                          activation_fn=None,
-                          normalizer_fn=None,
-                          biases_initializer=tf.zeros_initializer(),
-                          scope='fc8')
-      # Convert end_points_collection into a end_point dict.
-      end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      if spatial_squeeze:
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
-        end_points[sc.name + '/fc8'] = net
+        # Convert end_points_collection into a end_point dict.
+        end_points = slim.utils.convert_collection_to_dict(
+            end_points_collection)
+        if global_pool:
+          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          end_points['global_pool'] = net
+        if num_classes:
+          net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
+                             scope='dropout7')
+          net = slim.conv2d(net, num_classes, [1, 1],
+                            activation_fn=None,
+                            normalizer_fn=None,
+                            biases_initializer=tf.zeros_initializer(),
+                            scope='fc8')
+          if spatial_squeeze:
+            net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+          end_points[sc.name + '/fc8'] = net
       return net, end_points
 overfeat.default_image_size = 231
diff --git a/research/slim/nets/overfeat_test.py b/research/slim/nets/overfeat_test.py
index c6314e3f..dab0039e 100644
--- a/research/slim/nets/overfeat_test.py
+++ b/research/slim/nets/overfeat_test.py
@@ -48,6 +48,18 @@ class OverFeatTest(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, 2, 2, num_classes])
 
+  def testGlobalPool(self):
+    batch_size = 1
+    height, width = 281, 281
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False,
+                                    global_pool=True)
+      self.assertEquals(logits.op.name, 'overfeat/fc8/BiasAdd')
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, 1, 1, num_classes])
+
   def testEndPoints(self):
     batch_size = 5
     height, width = 231, 231
@@ -69,6 +81,27 @@ class OverFeatTest(tf.test.TestCase):
                        ]
       self.assertSetEqual(set(end_points.keys()), set(expected_names))
 
+  def testNoClasses(self):
+    batch_size = 5
+    height, width = 231, 231
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, end_points = overfeat.overfeat(inputs, num_classes)
+      expected_names = ['overfeat/conv1',
+                        'overfeat/pool1',
+                        'overfeat/conv2',
+                        'overfeat/pool2',
+                        'overfeat/conv3',
+                        'overfeat/conv4',
+                        'overfeat/conv5',
+                        'overfeat/pool5',
+                        'overfeat/fc6',
+                        'overfeat/fc7'
+                       ]
+      self.assertSetEqual(set(end_points.keys()), set(expected_names))
+      self.assertTrue(net.op.name.startswith('overfeat/fc7'))
+
   def testModelVariables(self):
     batch_size = 5
     height, width = 231, 231
diff --git a/research/slim/nets/pix2pix.py b/research/slim/nets/pix2pix.py
new file mode 100644
index 00000000..3b241eba
--- /dev/null
+++ b/research/slim/nets/pix2pix.py
@@ -0,0 +1,292 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# =============================================================================
+"""Implementation of the Image-to-Image Translation model.
+
+This network represents a port of the following work:
+
+  Image-to-Image Translation with Conditional Adversarial Networks
+  Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros
+  Arxiv, 2017
+  https://phillipi.github.io/pix2pix/
+
+A reference implementation written in Lua can be found at:
+https://github.com/phillipi/pix2pix/blob/master/models.lua
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import functools
+
+import tensorflow as tf
+
+layers = tf.contrib.layers
+
+
+def pix2pix_arg_scope():
+  """Returns a default argument scope for isola_net.
+
+  Returns:
+    An arg scope.
+  """
+  # These parameters come from the online port, which don't necessarily match
+  # those in the paper.
+  # TODO(nsilberman): confirm these values with Philip.
+  instance_norm_params = {
+      'center': True,
+      'scale': True,
+      'epsilon': 0.00001,
+  }
+
+  with tf.contrib.framework.arg_scope(
+      [layers.conv2d, layers.conv2d_transpose],
+      normalizer_fn=layers.instance_norm,
+      normalizer_params=instance_norm_params,
+      weights_initializer=tf.random_normal_initializer(0, 0.02)) as sc:
+    return sc
+
+
+def upsample(net, num_outputs, kernel_size, method='nn_upsample_conv'):
+  """Upsamples the given inputs.
+
+  Args:
+    net: A `Tensor` of size [batch_size, height, width, filters].
+    num_outputs: The number of output filters.
+    kernel_size: A list of 2 scalars or a 1x2 `Tensor` indicating the scale,
+      relative to the inputs, of the output dimensions. For example, if kernel
+      size is [2, 3], then the output height and width will be twice and three
+      times the input size.
+    method: The upsampling method.
+
+  Returns:
+    An `Tensor` which was upsampled using the specified method.
+
+  Raises:
+    ValueError: if `method` is not recognized.
+  """
+  net_shape = tf.shape(net)
+  height = net_shape[1]
+  width = net_shape[2]
+
+  if method == 'nn_upsample_conv':
+    net = tf.image.resize_nearest_neighbor(
+        net, [kernel_size[0] * height, kernel_size[1] * width])
+    net = layers.conv2d(net, num_outputs, [4, 4], activation_fn=None)
+  elif method == 'conv2d_transpose':
+    net = layers.conv2d_transpose(
+        net, num_outputs, [4, 4], stride=kernel_size, activation_fn=None)
+  else:
+    raise ValueError('Unknown method: [%s]', method)
+
+  return net
+
+
+class Block(
+    collections.namedtuple('Block', ['num_filters', 'decoder_keep_prob'])):
+  """Represents a single block of encoder and decoder processing.
+
+  The Image-to-Image translation paper works a bit differently than the original
+  U-Net model. In particular, each block represents a single operation in the
+  encoder which is concatenated with the corresponding decoder representation.
+  A dropout layer follows the concatenation and convolution of the concatenated
+  features.
+  """
+  pass
+
+
+def _default_generator_blocks():
+  """Returns the default generator block definitions.
+
+  Returns:
+    A list of generator blocks.
+  """
+  return [
+      Block(64, 0.5),
+      Block(128, 0.5),
+      Block(256, 0.5),
+      Block(512, 0),
+      Block(512, 0),
+      Block(512, 0),
+      Block(512, 0),
+  ]
+
+
+def pix2pix_generator(net,
+                      num_outputs,
+                      blocks=None,
+                      upsample_method='nn_upsample_conv',
+                      is_training=False):  # pylint: disable=unused-argument
+  """Defines the network architecture.
+
+  Args:
+    net: A `Tensor` of size [batch, height, width, channels]. Note that the
+      generator currently requires square inputs (e.g. height=width).
+    num_outputs: The number of (per-pixel) outputs.
+    blocks: A list of generator blocks or `None` to use the default generator
+      definition.
+    upsample_method: The method of upsampling images, one of 'nn_upsample_conv'
+      or 'conv2d_transpose'
+    is_training: Whether or not we're in training or testing mode.
+
+  Returns:
+    A `Tensor` representing the model output and a dictionary of model end
+      points.
+
+  Raises:
+    ValueError: if the input heights do not match their widths.
+  """
+  end_points = {}
+
+  blocks = blocks or _default_generator_blocks()
+
+  input_size = net.get_shape().as_list()
+  height, width = input_size[1], input_size[2]
+  if height != width:
+    raise ValueError('The input height must match the input width.')
+
+  input_size[3] = num_outputs
+
+  upsample_fn = functools.partial(upsample, method=upsample_method)
+
+  encoder_activations = []
+
+  ###########
+  # Encoder #
+  ###########
+  with tf.variable_scope('encoder'):
+    with tf.contrib.framework.arg_scope(
+        [layers.conv2d],
+        kernel_size=[4, 4],
+        stride=2,
+        activation_fn=tf.nn.leaky_relu):
+
+      for block_id, block in enumerate(blocks):
+        # No normalizer for the first encoder layers as per 'Image-to-Image',
+        # Section 5.1.1
+        if block_id == 0:
+          # First layer doesn't use normalizer_fn
+          net = layers.conv2d(net, block.num_filters, normalizer_fn=None)
+        elif block_id < len(blocks) - 1:
+          net = layers.conv2d(net, block.num_filters)
+        else:
+          # Last layer doesn't use activation_fn nor normalizer_fn
+          net = layers.conv2d(
+              net, block.num_filters, activation_fn=None, normalizer_fn=None)
+
+        encoder_activations.append(net)
+        end_points['encoder%d' % block_id] = net
+
+  ###########
+  # Decoder #
+  ###########
+  reversed_blocks = list(blocks)
+  reversed_blocks.reverse()
+
+  with tf.variable_scope('decoder'):
+    # Dropout is used at both train and test time as per 'Image-to-Image',
+    # Section 2.1 (last paragraph).
+    with tf.contrib.framework.arg_scope([layers.dropout], is_training=True):
+
+      for block_id, block in enumerate(reversed_blocks):
+        if block_id > 0:
+          net = tf.concat([net, encoder_activations[-block_id - 1]], axis=3)
+
+        # The Relu comes BEFORE the upsample op:
+        net = tf.nn.relu(net)
+        net = upsample_fn(net, block.num_filters, [2, 2])
+        if block.decoder_keep_prob > 0:
+          net = layers.dropout(net, keep_prob=block.decoder_keep_prob)
+        end_points['decoder%d' % block_id] = net
+
+  with tf.variable_scope('output'):
+    logits = layers.conv2d(net, num_outputs, [4, 4], activation_fn=None)
+    logits = tf.reshape(logits, input_size)
+
+    end_points['logits'] = logits
+    end_points['predictions'] = tf.tanh(logits)
+
+  return logits, end_points
+
+
+def pix2pix_discriminator(net, num_filters, padding=2, is_training=False):
+  """Creates the Image2Image Translation Discriminator.
+
+  Args:
+    net: A `Tensor` of size [batch_size, height, width, channels] representing
+      the input.
+    num_filters: A list of the filters in the discriminator. The length of the
+      list determines the number of layers in the discriminator.
+    padding: Amount of reflection padding applied before each convolution.
+    is_training: Whether or not the model is training or testing.
+
+  Returns:
+    A logits `Tensor` of size [batch_size, N, N, 1] where N is the number of
+    'patches' we're attempting to discriminate and a dictionary of model end
+    points.
+  """
+  del is_training
+  end_points = {}
+
+  num_layers = len(num_filters)
+
+  def padded(net, scope):
+    if padding:
+      with tf.variable_scope(scope):
+        spatial_pad = tf.constant(
+            [[0, 0], [padding, padding], [padding, padding], [0, 0]],
+            dtype=tf.int32)
+        return tf.pad(net, spatial_pad, 'REFLECT')
+    else:
+      return net
+
+  with tf.contrib.framework.arg_scope(
+      [layers.conv2d],
+      kernel_size=[4, 4],
+      stride=2,
+      padding='valid',
+      activation_fn=tf.nn.leaky_relu):
+
+    # No normalization on the input layer.
+    net = layers.conv2d(
+        padded(net, 'conv0'), num_filters[0], normalizer_fn=None, scope='conv0')
+
+    end_points['conv0'] = net
+
+    for i in range(1, num_layers - 1):
+      net = layers.conv2d(
+          padded(net, 'conv%d' % i), num_filters[i], scope='conv%d' % i)
+      end_points['conv%d' % i] = net
+
+    # Stride 1 on the last layer.
+    net = layers.conv2d(
+        padded(net, 'conv%d' % (num_layers - 1)),
+        num_filters[-1],
+        stride=1,
+        scope='conv%d' % (num_layers - 1))
+    end_points['conv%d' % (num_layers - 1)] = net
+
+    # 1-dim logits, stride 1, no activation, no normalization.
+    logits = layers.conv2d(
+        padded(net, 'conv%d' % num_layers),
+        1,
+        stride=1,
+        activation_fn=None,
+        normalizer_fn=None,
+        scope='conv%d' % num_layers)
+    end_points['logits'] = logits
+    end_points['predictions'] = tf.sigmoid(logits)
+  return logits, end_points
diff --git a/research/slim/nets/pix2pix_test.py b/research/slim/nets/pix2pix_test.py
new file mode 100644
index 00000000..4c60c068
--- /dev/null
+++ b/research/slim/nets/pix2pix_test.py
@@ -0,0 +1,168 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# =============================================================================
+"""Tests for pix2pix."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from nets import pix2pix
+
+
+class GeneratorTest(tf.test.TestCase):
+
+  def test_nonsquare_inputs_raise_exception(self):
+    batch_size = 2
+    height, width = 240, 320
+    num_outputs = 4
+
+    images = tf.ones((batch_size, height, width, 3))
+
+    with self.assertRaises(ValueError):
+      with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+        pix2pix.pix2pix_generator(
+            images, num_outputs, upsample_method='nn_upsample_conv')
+
+  def _reduced_default_blocks(self):
+    """Returns the default blocks, scaled down to make test run faster."""
+    return [pix2pix.Block(b.num_filters // 32, b.decoder_keep_prob)
+            for b in pix2pix._default_generator_blocks()]
+
+  def test_output_size_nn_upsample_conv(self):
+    batch_size = 2
+    height, width = 256, 256
+    num_outputs = 4
+
+    images = tf.ones((batch_size, height, width, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      logits, _ = pix2pix.pix2pix_generator(
+          images, num_outputs, blocks=self._reduced_default_blocks(),
+          upsample_method='nn_upsample_conv')
+
+    with self.test_session() as session:
+      session.run(tf.global_variables_initializer())
+      np_outputs = session.run(logits)
+      self.assertListEqual([batch_size, height, width, num_outputs],
+                           list(np_outputs.shape))
+
+  def test_output_size_conv2d_transpose(self):
+    batch_size = 2
+    height, width = 256, 256
+    num_outputs = 4
+
+    images = tf.ones((batch_size, height, width, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      logits, _ = pix2pix.pix2pix_generator(
+          images, num_outputs, blocks=self._reduced_default_blocks(),
+          upsample_method='conv2d_transpose')
+
+    with self.test_session() as session:
+      session.run(tf.global_variables_initializer())
+      np_outputs = session.run(logits)
+      self.assertListEqual([batch_size, height, width, num_outputs],
+                           list(np_outputs.shape))
+
+  def test_block_number_dictates_number_of_layers(self):
+    batch_size = 2
+    height, width = 256, 256
+    num_outputs = 4
+
+    images = tf.ones((batch_size, height, width, 3))
+    blocks = [
+        pix2pix.Block(64, 0.5),
+        pix2pix.Block(128, 0),
+    ]
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      _, end_points = pix2pix.pix2pix_generator(
+          images, num_outputs, blocks)
+
+    num_encoder_layers = 0
+    num_decoder_layers = 0
+    for end_point in end_points:
+      if end_point.startswith('encoder'):
+        num_encoder_layers += 1
+      elif end_point.startswith('decoder'):
+        num_decoder_layers += 1
+
+    self.assertEqual(num_encoder_layers, len(blocks))
+    self.assertEqual(num_decoder_layers, len(blocks))
+
+
+class DiscriminatorTest(tf.test.TestCase):
+
+  def _layer_output_size(self, input_size, kernel_size=4, stride=2, pad=2):
+    return (input_size + pad * 2 - kernel_size) // stride + 1
+
+  def test_four_layers(self):
+    batch_size = 2
+    input_size = 256
+
+    output_size = self._layer_output_size(input_size)
+    output_size = self._layer_output_size(output_size)
+    output_size = self._layer_output_size(output_size)
+    output_size = self._layer_output_size(output_size, stride=1)
+    output_size = self._layer_output_size(output_size, stride=1)
+
+    images = tf.ones((batch_size, input_size, input_size, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      logits, end_points = pix2pix.pix2pix_discriminator(
+          images, num_filters=[64, 128, 256, 512])
+    self.assertListEqual([batch_size, output_size, output_size, 1],
+                         logits.shape.as_list())
+    self.assertListEqual([batch_size, output_size, output_size, 1],
+                         end_points['predictions'].shape.as_list())
+
+  def test_four_layers_no_padding(self):
+    batch_size = 2
+    input_size = 256
+
+    output_size = self._layer_output_size(input_size, pad=0)
+    output_size = self._layer_output_size(output_size, pad=0)
+    output_size = self._layer_output_size(output_size, pad=0)
+    output_size = self._layer_output_size(output_size, stride=1, pad=0)
+    output_size = self._layer_output_size(output_size, stride=1, pad=0)
+
+    images = tf.ones((batch_size, input_size, input_size, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      logits, end_points = pix2pix.pix2pix_discriminator(
+          images, num_filters=[64, 128, 256, 512], padding=0)
+    self.assertListEqual([batch_size, output_size, output_size, 1],
+                         logits.shape.as_list())
+    self.assertListEqual([batch_size, output_size, output_size, 1],
+                         end_points['predictions'].shape.as_list())
+
+  def test_four_layers_wrog_paddig(self):
+    batch_size = 2
+    input_size = 256
+
+    images = tf.ones((batch_size, input_size, input_size, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      with self.assertRaises(TypeError):
+        pix2pix.pix2pix_discriminator(
+            images, num_filters=[64, 128, 256, 512], padding=1.5)
+
+  def test_four_layers_negative_padding(self):
+    batch_size = 2
+    input_size = 256
+
+    images = tf.ones((batch_size, input_size, input_size, 3))
+    with tf.contrib.framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+      with self.assertRaises(ValueError):
+        pix2pix.pix2pix_discriminator(
+            images, num_filters=[64, 128, 256, 512], padding=-1)
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/slim/nets/resnet_utils.py b/research/slim/nets/resnet_utils.py
index 92d7fd14..a16a1ba6 100644
--- a/research/slim/nets/resnet_utils.py
+++ b/research/slim/nets/resnet_utils.py
@@ -228,6 +228,7 @@ def resnet_arg_scope(weight_decay=0.0001,
       'epsilon': batch_norm_epsilon,
       'scale': batch_norm_scale,
       'updates_collections': tf.GraphKeys.UPDATE_OPS,
+      'fused': None,  # Use fused batch norm if possible.
   }
 
   with slim.arg_scope(
diff --git a/research/slim/nets/resnet_v1.py b/research/slim/nets/resnet_v1.py
index 548bee04..32ff1fbf 100644
--- a/research/slim/nets/resnet_v1.py
+++ b/research/slim/nets/resnet_v1.py
@@ -125,7 +125,7 @@ def bottleneck(inputs,
       output = tf.nn.relu(shortcut + residual)
 
     return slim.utils.collect_named_outputs(outputs_collections,
-                                            sc.original_name_scope,
+                                            sc.name,
                                             output)
 
 
@@ -166,9 +166,9 @@ def resnet_v1(inputs,
     inputs: A tensor of size [batch, height_in, width_in, channels].
     blocks: A list of length equal to the number of ResNet blocks. Each element
       is a resnet_utils.Block object describing the units in the block.
-    num_classes: Number of predicted classes for classification tasks. If None
-      we return the features before the logit layer.
-    is_training: whether is training or not.
+    num_classes: Number of predicted classes for classification tasks.
+      If 0 or None, we return the features before the logit layer.
+    is_training: whether batch_norm layers are in training mode.
     global_pool: If True, we perform global average pooling before computing the
       logits. Set to True for image classification, False for dense prediction.
     output_stride: If None, then the output will be computed at the nominal
@@ -189,10 +189,10 @@ def resnet_v1(inputs,
     net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].
       If global_pool is False, then height_out and width_out are reduced by a
       factor of output_stride compared to the respective height_in and width_in,
-      else both height_out and width_out equal one. If num_classes is None, then
-      net is the output of the last ResNet block, potentially after global
-      average pooling. If num_classes is not None, net contains the pre-softmax
-      activations.
+      else both height_out and width_out equal one. If num_classes is 0 or None,
+      then net is the output of the last ResNet block, potentially after global
+      average pooling. If num_classes a non-zero integer, net contains the
+      pre-softmax activations.
     end_points: A dictionary from components of the network to the corresponding
       activation.
 
@@ -200,7 +200,7 @@ def resnet_v1(inputs,
     ValueError: If the target output_stride is not valid.
   """
   with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
                          resnet_utils.stack_blocks_dense],
                         outputs_collections=end_points_collection):
@@ -214,18 +214,21 @@ def resnet_v1(inputs,
           net = resnet_utils.conv2d_same(net, 64, 7, stride=2, scope='conv1')
           net = slim.max_pool2d(net, [3, 3], stride=2, scope='pool1')
         net = resnet_utils.stack_blocks_dense(net, blocks, output_stride)
+        # Convert end_points_collection into a dictionary of end_points.
+        end_points = slim.utils.convert_collection_to_dict(
+            end_points_collection)
+
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)
-        if num_classes is not None:
+          end_points['global_pool'] = net
+        if num_classes:
           net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                             normalizer_fn=None, scope='logits')
+          end_points[sc.name + '/logits'] = net
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='SpatialSqueeze')
-        # Convert end_points_collection into a dictionary of end_points.
-        end_points = slim.utils.convert_collection_to_dict(
-            end_points_collection)
-        if num_classes is not None:
+            end_points[sc.name + '/spatial_squeeze'] = net
           end_points['predictions'] = slim.softmax(net, scope='predictions')
         return net, end_points
 resnet_v1.default_image_size = 224
diff --git a/research/slim/nets/resnet_v1_test.py b/research/slim/nets/resnet_v1_test.py
index 0f5f7a75..10c56082 100644
--- a/research/slim/nets/resnet_v1_test.py
+++ b/research/slim/nets/resnet_v1_test.py
@@ -285,6 +285,31 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
     self.assertTrue('predictions' in end_points)
     self.assertListEqual(end_points['predictions'].get_shape().as_list(),
                          [2, 1, 1, num_classes])
+    self.assertTrue('global_pool' in end_points)
+    self.assertListEqual(end_points['global_pool'].get_shape().as_list(),
+                         [2, 1, 1, 32])
+
+  def testEndpointNames(self):
+    # Like ResnetUtilsTest.testEndPointsV1(), but for the public API.
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(2, 224, 224, 3)
+    with slim.arg_scope(resnet_utils.resnet_arg_scope()):
+      _, end_points = self._resnet_small(inputs, num_classes,
+                                         global_pool=global_pool,
+                                         scope='resnet')
+    expected = ['resnet/conv1']
+    for block in range(1, 5):
+      for unit in range(1, 4 if block < 4 else 3):
+        for conv in range(1, 4):
+          expected.append('resnet/block%d/unit_%d/bottleneck_v1/conv%d' %
+                          (block, unit, conv))
+        expected.append('resnet/block%d/unit_%d/bottleneck_v1' % (block, unit))
+      expected.append('resnet/block%d/unit_1/bottleneck_v1/shortcut' % block)
+      expected.append('resnet/block%d' % block)
+    expected.extend(['global_pool', 'resnet/logits', 'resnet/spatial_squeeze',
+                     'predictions'])
+    self.assertItemsEqual(end_points.keys(), expected)
 
   def testClassificationShapes(self):
     global_pool = True
diff --git a/research/slim/nets/resnet_v2.py b/research/slim/nets/resnet_v2.py
index 10f0c491..6455e4cd 100644
--- a/research/slim/nets/resnet_v2.py
+++ b/research/slim/nets/resnet_v2.py
@@ -39,7 +39,7 @@ ResNet-101 for image classification into 1000 classes:
 ResNet-101 for semantic segmentation into 21 classes:
 
    # inputs has shape [batch, 513, 513, 3]
-   with slim.arg_scope(resnet_v2.resnet_arg_scope(is_training)):
+   with slim.arg_scope(resnet_v2.resnet_arg_scope()):
       net, end_points = resnet_v2.resnet_v2_101(inputs,
                                                 21,
                                                 is_training=False,
@@ -104,7 +104,7 @@ def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,
     output = shortcut + residual
 
     return slim.utils.collect_named_outputs(outputs_collections,
-                                            sc.original_name_scope,
+                                            sc.name,
                                             output)
 
 
@@ -145,9 +145,9 @@ def resnet_v2(inputs,
     inputs: A tensor of size [batch, height_in, width_in, channels].
     blocks: A list of length equal to the number of ResNet blocks. Each element
       is a resnet_utils.Block object describing the units in the block.
-    num_classes: Number of predicted classes for classification tasks. If None
-      we return the features before the logit layer.
-    is_training: whether is training or not.
+    num_classes: Number of predicted classes for classification tasks.
+      If 0 or None, we return the features before the logit layer.
+    is_training: whether batch_norm layers are in training mode.
     global_pool: If True, we perform global average pooling before computing the
       logits. Set to True for image classification, False for dense prediction.
     output_stride: If None, then the output will be computed at the nominal
@@ -170,10 +170,10 @@ def resnet_v2(inputs,
     net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].
       If global_pool is False, then height_out and width_out are reduced by a
       factor of output_stride compared to the respective height_in and width_in,
-      else both height_out and width_out equal one. If num_classes is None, then
-      net is the output of the last ResNet block, potentially after global
-      average pooling. If num_classes is not None, net contains the pre-softmax
-      activations.
+      else both height_out and width_out equal one. If num_classes is 0 or None,
+      then net is the output of the last ResNet block, potentially after global
+      average pooling. If num_classes is a non-zero integer, net contains the
+      pre-softmax activations.
     end_points: A dictionary from components of the network to the corresponding
       activation.
 
@@ -181,7 +181,7 @@ def resnet_v2(inputs,
     ValueError: If the target output_stride is not valid.
   """
   with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
                          resnet_utils.stack_blocks_dense],
                         outputs_collections=end_points_collection):
@@ -204,18 +204,21 @@ def resnet_v2(inputs,
         # normalization or activation functions in the residual unit output. See
         # Appendix of [2].
         net = slim.batch_norm(net, activation_fn=tf.nn.relu, scope='postnorm')
+        # Convert end_points_collection into a dictionary of end_points.
+        end_points = slim.utils.convert_collection_to_dict(
+            end_points_collection)
+
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)
+          end_points['global_pool'] = net
         if num_classes is not None:
           net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
                             normalizer_fn=None, scope='logits')
+          end_points[sc.name + '/logits'] = net
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='SpatialSqueeze')
-        # Convert end_points_collection into a dictionary of end_points.
-        end_points = slim.utils.convert_collection_to_dict(
-            end_points_collection)
-        if num_classes is not None:
+            end_points[sc.name + '/spatial_squeeze'] = net
           end_points['predictions'] = slim.softmax(net, scope='predictions')
         return net, end_points
 resnet_v2.default_image_size = 224
diff --git a/research/slim/nets/resnet_v2_test.py b/research/slim/nets/resnet_v2_test.py
index 392c4590..017d3944 100644
--- a/research/slim/nets/resnet_v2_test.py
+++ b/research/slim/nets/resnet_v2_test.py
@@ -285,6 +285,31 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
     self.assertTrue('predictions' in end_points)
     self.assertListEqual(end_points['predictions'].get_shape().as_list(),
                          [2, 1, 1, num_classes])
+    self.assertTrue('global_pool' in end_points)
+    self.assertListEqual(end_points['global_pool'].get_shape().as_list(),
+                         [2, 1, 1, 32])
+
+  def testEndpointNames(self):
+    # Like ResnetUtilsTest.testEndPointsV2(), but for the public API.
+    global_pool = True
+    num_classes = 10
+    inputs = create_test_input(2, 224, 224, 3)
+    with slim.arg_scope(resnet_utils.resnet_arg_scope()):
+      _, end_points = self._resnet_small(inputs, num_classes,
+                                         global_pool=global_pool,
+                                         scope='resnet')
+    expected = ['resnet/conv1']
+    for block in range(1, 5):
+      for unit in range(1, 4 if block < 4 else 3):
+        for conv in range(1, 4):
+          expected.append('resnet/block%d/unit_%d/bottleneck_v2/conv%d' %
+                          (block, unit, conv))
+        expected.append('resnet/block%d/unit_%d/bottleneck_v2' % (block, unit))
+      expected.append('resnet/block%d/unit_1/bottleneck_v2/shortcut' % block)
+      expected.append('resnet/block%d' % block)
+    expected.extend(['global_pool', 'resnet/logits', 'resnet/spatial_squeeze',
+                     'predictions'])
+    self.assertItemsEqual(end_points.keys(), expected)
 
   def testClassificationShapes(self):
     global_pool = True
diff --git a/research/slim/nets/vgg.py b/research/slim/nets/vgg.py
index cf598422..23b9031c 100644
--- a/research/slim/nets/vgg.py
+++ b/research/slim/nets/vgg.py
@@ -69,7 +69,8 @@ def vgg_a(inputs,
           dropout_keep_prob=0.5,
           spatial_squeeze=True,
           scope='vgg_a',
-          fc_conv_padding='VALID'):
+          fc_conv_padding='VALID',
+          global_pool=False):
   """Oxford Net VGG 11-Layers version A Example.
 
   Note: All the fully_connected layers have been transformed to conv2d layers.
@@ -77,7 +78,8 @@ def vgg_a(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
+      omitted and the input features to the logits layer are returned instead.
     is_training: whether or not the model is being trained.
     dropout_keep_prob: the probability that activations are kept in the dropout
       layers during training.
@@ -90,12 +92,17 @@ def vgg_a(inputs,
       get a prediction map downsampled by a factor of 32 as an output.
       Otherwise, the output prediction map will be (input / 32) - 6 in case of
       'VALID' padding.
+    global_pool: Optional boolean flag. If True, the input to the classification
+      layer is avgpooled to size 1x1, for any input size. (This is not part
+      of the original VGG architecture.)
 
   Returns:
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the input to the logits layer (if num_classes is 0 or None).
+    end_points: a dict of tensors with intermediate activations.
   """
   with tf.variable_scope(scope, 'vgg_a', [inputs]) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.max_pool2d],
                         outputs_collections=end_points_collection):
@@ -109,21 +116,26 @@ def vgg_a(inputs,
       net = slim.max_pool2d(net, [2, 2], scope='pool4')
       net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv5')
       net = slim.max_pool2d(net, [2, 2], scope='pool5')
+
       # Use conv2d instead of fully_connected layers.
       net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
       net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                          scope='dropout6')
       net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
-      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                         scope='dropout7')
-      net = slim.conv2d(net, num_classes, [1, 1],
-                        activation_fn=None,
-                        normalizer_fn=None,
-                        scope='fc8')
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      if spatial_squeeze:
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
+        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        end_points['global_pool'] = net
+      if num_classes:
+        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
+                           scope='dropout7')
+        net = slim.conv2d(net, num_classes, [1, 1],
+                          activation_fn=None,
+                          normalizer_fn=None,
+                          scope='fc8')
+        if spatial_squeeze:
+          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
         end_points[sc.name + '/fc8'] = net
       return net, end_points
 vgg_a.default_image_size = 224
@@ -135,7 +147,8 @@ def vgg_16(inputs,
            dropout_keep_prob=0.5,
            spatial_squeeze=True,
            scope='vgg_16',
-           fc_conv_padding='VALID'):
+           fc_conv_padding='VALID',
+           global_pool=False):
   """Oxford Net VGG 16-Layers version D Example.
 
   Note: All the fully_connected layers have been transformed to conv2d layers.
@@ -143,7 +156,8 @@ def vgg_16(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
+      omitted and the input features to the logits layer are returned instead.
     is_training: whether or not the model is being trained.
     dropout_keep_prob: the probability that activations are kept in the dropout
       layers during training.
@@ -156,12 +170,17 @@ def vgg_16(inputs,
       get a prediction map downsampled by a factor of 32 as an output.
       Otherwise, the output prediction map will be (input / 32) - 6 in case of
       'VALID' padding.
+    global_pool: Optional boolean flag. If True, the input to the classification
+      layer is avgpooled to size 1x1, for any input size. (This is not part
+      of the original VGG architecture.)
 
   Returns:
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the input to the logits layer (if num_classes is 0 or None).
+    end_points: a dict of tensors with intermediate activations.
   """
   with tf.variable_scope(scope, 'vgg_16', [inputs]) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                         outputs_collections=end_points_collection):
@@ -175,21 +194,26 @@ def vgg_16(inputs,
       net = slim.max_pool2d(net, [2, 2], scope='pool4')
       net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')
       net = slim.max_pool2d(net, [2, 2], scope='pool5')
+
       # Use conv2d instead of fully_connected layers.
       net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
       net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                          scope='dropout6')
       net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
-      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                         scope='dropout7')
-      net = slim.conv2d(net, num_classes, [1, 1],
-                        activation_fn=None,
-                        normalizer_fn=None,
-                        scope='fc8')
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      if spatial_squeeze:
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
+        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        end_points['global_pool'] = net
+      if num_classes:
+        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
+                           scope='dropout7')
+        net = slim.conv2d(net, num_classes, [1, 1],
+                          activation_fn=None,
+                          normalizer_fn=None,
+                          scope='fc8')
+        if spatial_squeeze and num_classes is not None:
+          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
         end_points[sc.name + '/fc8'] = net
       return net, end_points
 vgg_16.default_image_size = 224
@@ -201,7 +225,8 @@ def vgg_19(inputs,
            dropout_keep_prob=0.5,
            spatial_squeeze=True,
            scope='vgg_19',
-           fc_conv_padding='VALID'):
+           fc_conv_padding='VALID',
+           global_pool=False):
   """Oxford Net VGG 19-Layers version E Example.
 
   Note: All the fully_connected layers have been transformed to conv2d layers.
@@ -209,7 +234,8 @@ def vgg_19(inputs,
 
   Args:
     inputs: a tensor of size [batch_size, height, width, channels].
-    num_classes: number of predicted classes.
+    num_classes: number of predicted classes. If 0 or None, the logits layer is
+      omitted and the input features to the logits layer are returned instead.
     is_training: whether or not the model is being trained.
     dropout_keep_prob: the probability that activations are kept in the dropout
       layers during training.
@@ -222,13 +248,18 @@ def vgg_19(inputs,
       get a prediction map downsampled by a factor of 32 as an output.
       Otherwise, the output prediction map will be (input / 32) - 6 in case of
       'VALID' padding.
-
+    global_pool: Optional boolean flag. If True, the input to the classification
+      layer is avgpooled to size 1x1, for any input size. (This is not part
+      of the original VGG architecture.)
 
   Returns:
-    the last op containing the log predictions and end_points dict.
+    net: the output of the logits layer (if num_classes is a non-zero integer),
+      or the non-dropped-out input to the logits layer (if num_classes is 0 or
+      None).
+    end_points: a dict of tensors with intermediate activations.
   """
   with tf.variable_scope(scope, 'vgg_19', [inputs]) as sc:
-    end_points_collection = sc.name + '_end_points'
+    end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
                         outputs_collections=end_points_collection):
@@ -242,21 +273,26 @@ def vgg_19(inputs,
       net = slim.max_pool2d(net, [2, 2], scope='pool4')
       net = slim.repeat(net, 4, slim.conv2d, 512, [3, 3], scope='conv5')
       net = slim.max_pool2d(net, [2, 2], scope='pool5')
+
       # Use conv2d instead of fully_connected layers.
       net = slim.conv2d(net, 4096, [7, 7], padding=fc_conv_padding, scope='fc6')
       net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                          scope='dropout6')
       net = slim.conv2d(net, 4096, [1, 1], scope='fc7')
-      net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
-                         scope='dropout7')
-      net = slim.conv2d(net, num_classes, [1, 1],
-                        activation_fn=None,
-                        normalizer_fn=None,
-                        scope='fc8')
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
-      if spatial_squeeze:
-        net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
+      if global_pool:
+        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        end_points['global_pool'] = net
+      if num_classes:
+        net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
+                           scope='dropout7')
+        net = slim.conv2d(net, num_classes, [1, 1],
+                          activation_fn=None,
+                          normalizer_fn=None,
+                          scope='fc8')
+        if spatial_squeeze:
+          net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
         end_points[sc.name + '/fc8'] = net
       return net, end_points
 vgg_19.default_image_size = 224
diff --git a/research/slim/nets/vgg_test.py b/research/slim/nets/vgg_test.py
index e4ff8def..6760c368 100644
--- a/research/slim/nets/vgg_test.py
+++ b/research/slim/nets/vgg_test.py
@@ -48,6 +48,18 @@ class VGGATest(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, 2, 2, num_classes])
 
+  def testGlobalPool(self):
+    batch_size = 1
+    height, width = 256, 256
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False,
+                            global_pool=True)
+      self.assertEquals(logits.op.name, 'vgg_a/fc8/BiasAdd')
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, 1, 1, num_classes])
+
   def testEndPoints(self):
     batch_size = 5
     height, width = 224, 224
@@ -74,6 +86,32 @@ class VGGATest(tf.test.TestCase):
                        ]
       self.assertSetEqual(set(end_points.keys()), set(expected_names))
 
+  def testNoClasses(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, end_points = vgg.vgg_a(inputs, num_classes)
+      expected_names = ['vgg_a/conv1/conv1_1',
+                        'vgg_a/pool1',
+                        'vgg_a/conv2/conv2_1',
+                        'vgg_a/pool2',
+                        'vgg_a/conv3/conv3_1',
+                        'vgg_a/conv3/conv3_2',
+                        'vgg_a/pool3',
+                        'vgg_a/conv4/conv4_1',
+                        'vgg_a/conv4/conv4_2',
+                        'vgg_a/pool4',
+                        'vgg_a/conv5/conv5_1',
+                        'vgg_a/conv5/conv5_2',
+                        'vgg_a/pool5',
+                        'vgg_a/fc6',
+                        'vgg_a/fc7',
+                       ]
+      self.assertSetEqual(set(end_points.keys()), set(expected_names))
+      self.assertTrue(net.op.name.startswith('vgg_a/fc7'))
+
   def testModelVariables(self):
     batch_size = 5
     height, width = 224, 224
@@ -177,6 +215,18 @@ class VGG16Test(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, 2, 2, num_classes])
 
+  def testGlobalPool(self):
+    batch_size = 1
+    height, width = 256, 256
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False,
+                             global_pool=True)
+      self.assertEquals(logits.op.name, 'vgg_16/fc8/BiasAdd')
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, 1, 1, num_classes])
+
   def testEndPoints(self):
     batch_size = 5
     height, width = 224, 224
@@ -208,6 +258,37 @@ class VGG16Test(tf.test.TestCase):
                        ]
       self.assertSetEqual(set(end_points.keys()), set(expected_names))
 
+  def testNoClasses(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, end_points = vgg.vgg_16(inputs, num_classes)
+      expected_names = ['vgg_16/conv1/conv1_1',
+                        'vgg_16/conv1/conv1_2',
+                        'vgg_16/pool1',
+                        'vgg_16/conv2/conv2_1',
+                        'vgg_16/conv2/conv2_2',
+                        'vgg_16/pool2',
+                        'vgg_16/conv3/conv3_1',
+                        'vgg_16/conv3/conv3_2',
+                        'vgg_16/conv3/conv3_3',
+                        'vgg_16/pool3',
+                        'vgg_16/conv4/conv4_1',
+                        'vgg_16/conv4/conv4_2',
+                        'vgg_16/conv4/conv4_3',
+                        'vgg_16/pool4',
+                        'vgg_16/conv5/conv5_1',
+                        'vgg_16/conv5/conv5_2',
+                        'vgg_16/conv5/conv5_3',
+                        'vgg_16/pool5',
+                        'vgg_16/fc6',
+                        'vgg_16/fc7',
+                       ]
+      self.assertSetEqual(set(end_points.keys()), set(expected_names))
+      self.assertTrue(net.op.name.startswith('vgg_16/fc7'))
+
   def testModelVariables(self):
     batch_size = 5
     height, width = 224, 224
@@ -321,6 +402,18 @@ class VGG19Test(tf.test.TestCase):
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, 2, 2, num_classes])
 
+  def testGlobalPool(self):
+    batch_size = 1
+    height, width = 256, 256
+    num_classes = 1000
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False,
+                             global_pool=True)
+      self.assertEquals(logits.op.name, 'vgg_19/fc8/BiasAdd')
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, 1, 1, num_classes])
+
   def testEndPoints(self):
     batch_size = 5
     height, width = 224, 224
@@ -356,6 +449,41 @@ class VGG19Test(tf.test.TestCase):
       ]
       self.assertSetEqual(set(end_points.keys()), set(expected_names))
 
+  def testNoClasses(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = None
+    with self.test_session():
+      inputs = tf.random_uniform((batch_size, height, width, 3))
+      net, end_points = vgg.vgg_19(inputs, num_classes)
+      expected_names = [
+          'vgg_19/conv1/conv1_1',
+          'vgg_19/conv1/conv1_2',
+          'vgg_19/pool1',
+          'vgg_19/conv2/conv2_1',
+          'vgg_19/conv2/conv2_2',
+          'vgg_19/pool2',
+          'vgg_19/conv3/conv3_1',
+          'vgg_19/conv3/conv3_2',
+          'vgg_19/conv3/conv3_3',
+          'vgg_19/conv3/conv3_4',
+          'vgg_19/pool3',
+          'vgg_19/conv4/conv4_1',
+          'vgg_19/conv4/conv4_2',
+          'vgg_19/conv4/conv4_3',
+          'vgg_19/conv4/conv4_4',
+          'vgg_19/pool4',
+          'vgg_19/conv5/conv5_1',
+          'vgg_19/conv5/conv5_2',
+          'vgg_19/conv5/conv5_3',
+          'vgg_19/conv5/conv5_4',
+          'vgg_19/pool5',
+          'vgg_19/fc6',
+          'vgg_19/fc7',
+      ]
+      self.assertSetEqual(set(end_points.keys()), set(expected_names))
+      self.assertTrue(net.op.name.startswith('vgg_19/fc7'))
+
   def testModelVariables(self):
     batch_size = 5
     height, width = 224, 224
diff --git a/research/slim/preprocessing/cifarnet_preprocessing.py b/research/slim/preprocessing/cifarnet_preprocessing.py
index 195a5c7d..0b5a88fa 100644
--- a/research/slim/preprocessing/cifarnet_preprocessing.py
+++ b/research/slim/preprocessing/cifarnet_preprocessing.py
@@ -30,7 +30,8 @@ slim = tf.contrib.slim
 def preprocess_for_train(image,
                          output_height,
                          output_width,
-                         padding=_PADDING):
+                         padding=_PADDING,
+                         add_image_summaries=True):
   """Preprocesses the given image for training.
 
   Note that the actual resizing scale is sampled from
@@ -41,11 +42,13 @@ def preprocess_for_train(image,
     output_height: The height of the image after preprocessing.
     output_width: The width of the image after preprocessing.
     padding: The amound of padding before and after each dimension of the image.
+    add_image_summaries: Enable image summaries.
 
   Returns:
     A preprocessed image.
   """
-  tf.summary.image('image', tf.expand_dims(image, 0))
+  if add_image_summaries:
+    tf.summary.image('image', tf.expand_dims(image, 0))
 
   # Transform the image to floats.
   image = tf.to_float(image)
@@ -58,7 +61,8 @@ def preprocess_for_train(image,
   # Randomly flip the image horizontally.
   distorted_image = tf.image.random_flip_left_right(distorted_image)
 
-  tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
+  if add_image_summaries:
+    tf.summary.image('distorted_image', tf.expand_dims(distorted_image, 0))
 
   # Because these operations are not commutative, consider randomizing
   # the order their operation.
@@ -70,18 +74,21 @@ def preprocess_for_train(image,
   return tf.image.per_image_standardization(distorted_image)
 
 
-def preprocess_for_eval(image, output_height, output_width):
+def preprocess_for_eval(image, output_height, output_width,
+                        add_image_summaries=True):
   """Preprocesses the given image for evaluation.
 
   Args:
     image: A `Tensor` representing an image of arbitrary size.
     output_height: The height of the image after preprocessing.
     output_width: The width of the image after preprocessing.
+    add_image_summaries: Enable image summaries.
 
   Returns:
     A preprocessed image.
   """
-  tf.summary.image('image', tf.expand_dims(image, 0))
+  if add_image_summaries:
+    tf.summary.image('image', tf.expand_dims(image, 0))
   # Transform the image to floats.
   image = tf.to_float(image)
 
@@ -89,13 +96,15 @@ def preprocess_for_eval(image, output_height, output_width):
   resized_image = tf.image.resize_image_with_crop_or_pad(image,
                                                          output_width,
                                                          output_height)
-  tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
+  if add_image_summaries:
+    tf.summary.image('resized_image', tf.expand_dims(resized_image, 0))
 
   # Subtract off the mean and divide by the variance of the pixels.
   return tf.image.per_image_standardization(resized_image)
 
 
-def preprocess_image(image, output_height, output_width, is_training=False):
+def preprocess_image(image, output_height, output_width, is_training=False,
+                     add_image_summaries=True):
   """Preprocesses the given image.
 
   Args:
@@ -104,11 +113,16 @@ def preprocess_image(image, output_height, output_width, is_training=False):
     output_width: The width of the image after preprocessing.
     is_training: `True` if we're preprocessing the image for training and
       `False` otherwise.
+    add_image_summaries: Enable image summaries.
 
   Returns:
     A preprocessed image.
   """
   if is_training:
-    return preprocess_for_train(image, output_height, output_width)
+    return preprocess_for_train(
+        image, output_height, output_width,
+        add_image_summaries=add_image_summaries)
   else:
-    return preprocess_for_eval(image, output_height, output_width)
+    return preprocess_for_eval(
+        image, output_height, output_width,
+        add_image_summaries=add_image_summaries)
diff --git a/research/slim/preprocessing/inception_preprocessing.py b/research/slim/preprocessing/inception_preprocessing.py
index 45e81fd8..aa77842a 100644
--- a/research/slim/preprocessing/inception_preprocessing.py
+++ b/research/slim/preprocessing/inception_preprocessing.py
@@ -155,7 +155,8 @@ def distorted_bounding_box_crop(image,
 
 def preprocess_for_train(image, height, width, bbox,
                          fast_mode=True,
-                         scope=None):
+                         scope=None,
+                         add_image_summaries=True):
   """Distort one image for training a network.
 
   Distorting images provides a useful technique for augmenting the data
@@ -178,6 +179,7 @@ def preprocess_for_train(image, height, width, bbox,
     fast_mode: Optional boolean, if True avoids slower transformations (i.e.
       bi-cubic resizing, random_hue or random_contrast).
     scope: Optional scope for name_scope.
+    add_image_summaries: Enable image summaries.
   Returns:
     3-D float Tensor of distorted image used for training with range [-1, 1].
   """
@@ -192,7 +194,8 @@ def preprocess_for_train(image, height, width, bbox,
     # the coordinates are ordered [ymin, xmin, ymax, xmax].
     image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                   bbox)
-    tf.summary.image('image_with_bounding_boxes', image_with_box)
+    if add_image_summaries:
+      tf.summary.image('image_with_bounding_boxes', image_with_box)
 
     distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)
     # Restore the shape since the dynamic slice based upon the bbox_size loses
@@ -200,8 +203,9 @@ def preprocess_for_train(image, height, width, bbox,
     distorted_image.set_shape([None, None, 3])
     image_with_distorted_box = tf.image.draw_bounding_boxes(
         tf.expand_dims(image, 0), distorted_bbox)
-    tf.summary.image('images_with_distorted_bounding_box',
-                     image_with_distorted_box)
+    if add_image_summaries:
+      tf.summary.image('images_with_distorted_bounding_box',
+                       image_with_distorted_box)
 
     # This resizing operation may distort the images because the aspect
     # ratio is not respected. We select a resize method in a round robin
@@ -215,8 +219,9 @@ def preprocess_for_train(image, height, width, bbox,
         lambda x, method: tf.image.resize_images(x, [height, width], method),
         num_cases=num_resize_cases)
 
-    tf.summary.image('cropped_resized_image',
-                     tf.expand_dims(distorted_image, 0))
+    if add_image_summaries:
+      tf.summary.image('cropped_resized_image',
+                       tf.expand_dims(distorted_image, 0))
 
     # Randomly flip the image horizontally.
     distorted_image = tf.image.random_flip_left_right(distorted_image)
@@ -227,8 +232,9 @@ def preprocess_for_train(image, height, width, bbox,
         lambda x, ordering: distort_color(x, ordering, fast_mode),
         num_cases=4)
 
-    tf.summary.image('final_distorted_image',
-                     tf.expand_dims(distorted_image, 0))
+    if add_image_summaries:
+      tf.summary.image('final_distorted_image',
+                       tf.expand_dims(distorted_image, 0))
     distorted_image = tf.subtract(distorted_image, 0.5)
     distorted_image = tf.multiply(distorted_image, 2.0)
     return distorted_image
@@ -278,7 +284,8 @@ def preprocess_for_eval(image, height, width,
 def preprocess_image(image, height, width,
                      is_training=False,
                      bbox=None,
-                     fast_mode=True):
+                     fast_mode=True,
+                     add_image_summaries=True):
   """Pre-process one image for training or evaluation.
 
   Args:
@@ -295,6 +302,7 @@ def preprocess_image(image, height, width,
       where each coordinate is [0, 1) and the coordinates are arranged as
       [ymin, xmin, ymax, xmax].
     fast_mode: Optional boolean, if True avoids slower transformations.
+    add_image_summaries: Enable image summaries.
 
   Returns:
     3-D float Tensor containing an appropriately scaled image
@@ -303,6 +311,7 @@ def preprocess_image(image, height, width,
     ValueError: if user does not provide bounding box
   """
   if is_training:
-    return preprocess_for_train(image, height, width, bbox, fast_mode)
+    return preprocess_for_train(image, height, width, bbox, fast_mode,
+                                add_image_summaries=add_image_summaries)
   else:
     return preprocess_for_eval(image, height, width)
diff --git a/research/slim/preprocessing/preprocessing_factory.py b/research/slim/preprocessing/preprocessing_factory.py
index ea3f01b7..af82640d 100644
--- a/research/slim/preprocessing/preprocessing_factory.py
+++ b/research/slim/preprocessing/preprocessing_factory.py
@@ -54,6 +54,8 @@ def get_preprocessing(name, is_training=False):
       'inception_resnet_v2': inception_preprocessing,
       'lenet': lenet_preprocessing,
       'mobilenet_v1': inception_preprocessing,
+      'nasnet_mobile': inception_preprocessing,
+      'nasnet_large': inception_preprocessing,
       'resnet_v1_50': vgg_preprocessing,
       'resnet_v1_101': vgg_preprocessing,
       'resnet_v1_152': vgg_preprocessing,
diff --git a/research/slim/scripts/export_mobilenet.sh b/research/slim/scripts/export_mobilenet.sh
index c81ee6d9..bddabe15 100755
--- a/research/slim/scripts/export_mobilenet.sh
+++ b/research/slim/scripts/export_mobilenet.sh
@@ -1,4 +1,19 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
 # This script prepares the various different versions of MobileNet models for
 # use in a mobile application. If you don't specify your own trained checkpoint
 # file, it will download pretrained checkpoints for ImageNet. You'll also need
diff --git a/research/slim/scripts/finetune_inception_resnet_v2_on_flowers.sh b/research/slim/scripts/finetune_inception_resnet_v2_on_flowers.sh
index 166f8c71..ad003ba3 100644
--- a/research/slim/scripts/finetune_inception_resnet_v2_on_flowers.sh
+++ b/research/slim/scripts/finetune_inception_resnet_v2_on_flowers.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the Flowers dataset
diff --git a/research/slim/scripts/finetune_inception_v1_on_flowers.sh b/research/slim/scripts/finetune_inception_v1_on_flowers.sh
index d152e367..fee482ab 100644
--- a/research/slim/scripts/finetune_inception_v1_on_flowers.sh
+++ b/research/slim/scripts/finetune_inception_v1_on_flowers.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the Flowers dataset
diff --git a/research/slim/scripts/finetune_inception_v3_on_flowers.sh b/research/slim/scripts/finetune_inception_v3_on_flowers.sh
index 8d8a5d4b..39986638 100644
--- a/research/slim/scripts/finetune_inception_v3_on_flowers.sh
+++ b/research/slim/scripts/finetune_inception_v3_on_flowers.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the Flowers dataset
diff --git a/research/slim/scripts/finetune_resnet_v1_50_on_flowers.sh b/research/slim/scripts/finetune_resnet_v1_50_on_flowers.sh
index 8134dfc3..00d9043c 100644
--- a/research/slim/scripts/finetune_resnet_v1_50_on_flowers.sh
+++ b/research/slim/scripts/finetune_resnet_v1_50_on_flowers.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the Flowers dataset
diff --git a/research/slim/scripts/train_cifarnet_on_cifar10.sh b/research/slim/scripts/train_cifarnet_on_cifar10.sh
index 13a9bc52..4613ad18 100644
--- a/research/slim/scripts/train_cifarnet_on_cifar10.sh
+++ b/research/slim/scripts/train_cifarnet_on_cifar10.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the Cifar10 dataset
diff --git a/research/slim/scripts/train_lenet_on_mnist.sh b/research/slim/scripts/train_lenet_on_mnist.sh
index e5371eba..1d588eb5 100644
--- a/research/slim/scripts/train_lenet_on_mnist.sh
+++ b/research/slim/scripts/train_lenet_on_mnist.sh
@@ -1,4 +1,18 @@
 #!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 #
 # This script performs the following operations:
 # 1. Downloads the MNIST dataset
diff --git a/research/slim/setup.py b/research/slim/setup.py
index 4262a4ee..3ec7ecd6 100644
--- a/research/slim/setup.py
+++ b/research/slim/setup.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """Setup script for slim."""
 
 from setuptools import find_packages
