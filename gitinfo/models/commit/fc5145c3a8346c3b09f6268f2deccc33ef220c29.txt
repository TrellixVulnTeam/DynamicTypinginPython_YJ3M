commit fc5145c3a8346c3b09f6268f2deccc33ef220c29
Author: Vivek Rathod <rathodv@google.com>
Date:   Sun Oct 29 10:54:50 2017 -0700

    nasnet config, model and release info.

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index f9c352b0..5d526ed5 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -31,43 +31,58 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:l291WsrB-hQJ:scholar.go
 
 ## Table of contents
 
-Before You Start:
-* <a href='g3doc/installation.md'>Installation</a><br>
-
 Quick Start:
-* <a href='object_detection_tutorial.ipynb'>
+
+  * <a href='object_detection_tutorial.ipynb'>
       Quick Start: Jupyter notebook for off-the-shelf inference</a><br>
-* <a href="g3doc/running_pets.md">Quick Start: Training a pet detector</a><br>
+  * <a href="g3doc/running_pets.md">Quick Start: Training a pet detector</a><br>
 
 Setup:
-* <a href='g3doc/configuring_jobs.md'>
+
+  * <a href='g3doc/installation.md'>Installation</a><br>
+  * <a href='g3doc/configuring_jobs.md'>
       Configuring an object detection pipeline</a><br>
-* <a href='g3doc/preparing_inputs.md'>Preparing inputs</a><br>
+  * <a href='g3doc/preparing_inputs.md'>Preparing inputs</a><br>
 
 Running:
-* <a href='g3doc/running_locally.md'>Running locally</a><br>
-* <a href='g3doc/running_on_cloud.md'>Running on the cloud</a><br>
+
+  * <a href='g3doc/running_locally.md'>Running locally</a><br>
+  * <a href='g3doc/running_on_cloud.md'>Running on the cloud</a><br>
 
 Extras:
-* <a href='g3doc/detection_model_zoo.md'>Tensorflow detection model zoo</a><br>
-* <a href='g3doc/exporting_models.md'>
+
+  * <a href='g3doc/detection_model_zoo.md'>Tensorflow detection model zoo</a><br>
+  * <a href='g3doc/exporting_models.md'>
       Exporting a trained model for inference</a><br>
-* <a href='g3doc/defining_your_own_model.md'>
+  * <a href='g3doc/defining_your_own_model.md'>
       Defining your own model architecture</a><br>
-* <a href='g3doc/using_your_own_dataset.md'>
+  * <a href='g3doc/using_your_own_dataset.md'>
       Bringing in your own dataset</a><br>
 
 ## Getting Help
 
-Please report bugs to the tensorflow/models/ Github
+To get help with issues you may encounter using the Tensorflow Object Detection
+API, create a new question on [StackOverflow](https://stackoverflow.com/) with
+the tags "tensorflow" and "object-detection".
+
+Please report bugs (actually broken code, not usage questions) to the
+tensorflow/models Github
 [issue tracker](https://github.com/tensorflow/models/issues), prefixing the
-issue name with "object_detection". To get help with issues you may encounter
-using the Tensorflow Object Detection API, create a new question on
-[StackOverflow](https://stackoverflow.com/) with the tags "tensorflow" and
-"object-detection".
+issue name with "object_detection".
 
 ## Release information
 
+### October 31, 2017
+
+We have released a new state-of-the-art model for object detection using
+the Faster-RCNN with the
+[NASNet-A image featurization](https://arxiv.org/abs/1707.07012). This
+model achieves mAP of 43.1% on the test-dev validation dataset for COCO,
+improving on the best available model in the zoo by 6% in terms
+of absolute mAP.
+
+<b>Thanks to contributors</b>: Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc Le
+
 ### August 11, 2017
 
 We have released an update to the [Android Detect
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index ba656bae..387285f4 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -40,3 +40,4 @@ Inside the un-tar'ed directory, you will find:
 | [rfcn_resnet101_coco](http://download.tensorflow.org/models/object_detection/rfcn_resnet101_coco_11_06_2017.tar.gz)  | medium | 30 | Boxes |
 | [faster_rcnn_resnet101_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_11_06_2017.tar.gz) | medium | 32 | Boxes |
 | [faster_rcnn_inception_resnet_v2_atrous_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_11_06_2017.tar.gz) | slow | 37 | Boxes |
+| [faster_rcnn_nas](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_17_10_2017.tar.gz) | slow | 43 | Boxes |
diff --git a/research/object_detection/g3doc/running_pets.md b/research/object_detection/g3doc/running_pets.md
index 439ed6c9..f2c8df80 100644
--- a/research/object_detection/g3doc/running_pets.md
+++ b/research/object_detection/g3doc/running_pets.md
@@ -298,10 +298,11 @@ python object_detection/export_inference_graph.py \
     --input_type image_tensor \
     --pipeline_config_path object_detection/samples/configs/faster_rcnn_resnet101_pets.config \
     --trained_checkpoint_prefix model.ckpt-${CHECKPOINT_NUMBER} \
-    --output_directory output_inference_graph.pb
+    --output_directory exported_graphs
 ```
 
-Afterwards, you should see a graph named `output_inference_graph.pb`.
+Afterwards, you should see a directory named `exported_graphs` containing the
+SavedModel and frozen graph.
 
 ## What's Next
 
diff --git a/research/object_detection/samples/configs/faster_rcnn_nas_coco.config b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
new file mode 100644
index 00000000..eb9dc9b4
--- /dev/null
+++ b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
@@ -0,0 +1,148 @@
+# Faster R-CNN with NASNet-A featurization
+# Configured for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      # TODO(shlens): Only fixed_shape_resizer is currently supported for NASNet
+      # featurization. The reason for this is that nasnet.py only supports
+      # inputs with fully known shapes. We need to update nasnet.py to handle
+      # shapes not known at compile time.
+      fixed_shape_resizer {
+        height: 1200
+        width: 1200
+      }
+    }
+    feature_extractor {
+      type: 'faster_rcnn_nas'
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 16
+        width_stride: 16
+      }
+    }
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 50
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 17
+    maxpool_kernel_size: 1
+    maxpool_stride: 1
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0003
+          schedule {
+            step: 0
+            learning_rate: .0003
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00003
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000003
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "pascal_voc_metrics"
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+  num_epochs: 1
+}
