commit c34c439bb65c30e128bdd8608802e21219b6dc19
Author: Dmitry Murygin <murygin.dmitry@huawei.com>
Date:   Tue Oct 22 12:41:13 2019 +0000

    Fix EmbeddingPostprocessor

diff --git a/official/nlp/bert_modeling.py b/official/nlp/bert_modeling.py
index 2a32f140..d4fde633 100644
--- a/official/nlp/bert_modeling.py
+++ b/official/nlp/bert_modeling.py
@@ -341,11 +341,7 @@ class EmbeddingPostprocessor(tf.keras.layers.Layer):
     output = word_embeddings
     if self.use_type_embeddings:
       flat_token_type_ids = tf.reshape(token_type_ids, [-1])
-      one_hot_ids = tf.one_hot(
-          flat_token_type_ids,
-          depth=self.token_type_vocab_size,
-          dtype=self.dtype)
-      token_type_embeddings = tf.matmul(one_hot_ids, self.type_embeddings)
+      token_type_embeddings = tf.gather(self.type_embeddings, flat_token_type_ids)
       token_type_embeddings = tf.reshape(token_type_embeddings,
                                          [batch_size, seq_length, width])
       output += token_type_embeddings
