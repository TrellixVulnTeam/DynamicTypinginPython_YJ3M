commit 7212436440eaa11293ca84befcc5d8327109ea76
Author: Hongkun Yu <hongkuny@google.com>
Date:   Fri Aug 16 23:23:02 2019 -0700

    Delete keras_application_models which is no longer needed.
    
    PiperOrigin-RevId: 263910123

diff --git a/official/keras_application_models/README.md b/official/keras_application_models/README.md
deleted file mode 100644
index b3cd525e..00000000
--- a/official/keras_application_models/README.md
+++ /dev/null
@@ -1,39 +0,0 @@
-# Keras Application Models Benchmark
-## Overview
-This provides a single scaffold to benchmark the Keras built-in application [models](https://keras.io/applications/). All the models are for image classification applications, and include:
-
- - Xception
- - VGG16
- - VGG19
- - ResNet50
- - InceptionV3
- - InceptionResNetV2
- - MobileNet
- - DenseNet
- - NASNet
-
-## Dataset
-Synthetic dataset is used for the benchmark.
-
-## Callbacks
-Two custom callbacks are provided for model benchmarking: ExamplesPerSecondCallback and LoggingMetricCallback. For each callback, `epoch_based` and `batch_based` options are available to set the benchmark level. Check [model_callbacks.py](model_callbacks.py) for more details.
-
-## Running Code
-To benchmark a model, use `--model` to specify the model name. To perform the benchmark with eager execution, issue the following command:
-```
-python benchmark_main.py --model resnet50 --eager
-```
-Note that, if eager execution is enabled, only one GPU is utilized even if multiple GPUs are provided and multi_gpu_model is used.
-
-
-To use distribution strategy in the benchmark, run the following:
-```
-python benchmark_main.py --model resnet50 --dist_strat
-```
-Currently, only one of the --eager and --dist_strat arguments can be defined, as DistributionStrategy is not supported in Eager execution now.
-
-Arguments:
-  * `--model`: Which model to be benchmarked. The model name is defined as the keys of `MODELS` in [benchmark_main.py](benchmark_main.py).
-  * `--callbacks`: To specify a list of callbacks.
-
-Use the `--help` or `-h` flag to get a full list of possible arguments.
diff --git a/official/keras_application_models/__init__.py b/official/keras_application_models/__init__.py
deleted file mode 100644
index e69de29b..00000000
diff --git a/official/keras_application_models/benchmark_main.py b/official/keras_application_models/benchmark_main.py
deleted file mode 100644
index 3511371d..00000000
--- a/official/keras_application_models/benchmark_main.py
+++ /dev/null
@@ -1,228 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Benchmark on the keras built-in application models."""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-# pylint: disable=g-bad-import-order
-import numpy as np
-from absl import app as absl_app
-from absl import flags
-import tensorflow as tf
-# pylint: enable=g-bad-import-order
-
-from official.keras_application_models import dataset
-from official.keras_application_models import model_callbacks
-from official.utils.flags import core as flags_core
-from official.utils.logs import logger
-from official.utils.misc import distribution_utils
-
-# Define a dictionary that maps model names to their model classes inside Keras
-MODELS = {
-    "vgg16": tf.keras.applications.VGG16,
-    "vgg19": tf.keras.applications.VGG19,
-    "inceptionv3": tf.keras.applications.InceptionV3,
-    "xception": tf.keras.applications.Xception,
-    "resnet50": tf.keras.applications.ResNet50,
-    "inceptionresnetv2": tf.keras.applications.InceptionResNetV2,
-    "mobilenet": tf.keras.applications.MobileNet,
-    "densenet121": tf.keras.applications.DenseNet121,
-    "densenet169": tf.keras.applications.DenseNet169,
-    "densenet201": tf.keras.applications.DenseNet201,
-    "nasnetlarge": tf.keras.applications.NASNetLarge,
-    "nasnetmobile": tf.keras.applications.NASNetMobile,
-}
-
-
-def run_keras_model_benchmark(_):
-  """Run the benchmark on keras model."""
-  # Ensure a valid model name was supplied via command line argument
-  if FLAGS.model not in MODELS.keys():
-    raise AssertionError("The --model command line argument should "
-                         "be a key in the `MODELS` dictionary.")
-
-  # Check if eager execution is enabled
-  if FLAGS.eager:
-    tf.logging.info("Eager execution is enabled...")
-    tf.enable_eager_execution()
-
-  # Load the model
-  tf.logging.info("Benchmark on {} model...".format(FLAGS.model))
-  keras_model = MODELS[FLAGS.model]
-
-  # Get dataset
-  dataset_name = "ImageNet"
-  if FLAGS.use_synthetic_data:
-    tf.logging.info("Using synthetic dataset...")
-    dataset_name += "_Synthetic"
-    train_dataset = dataset.generate_synthetic_input_dataset(
-        FLAGS.model, FLAGS.batch_size)
-    val_dataset = dataset.generate_synthetic_input_dataset(
-        FLAGS.model, FLAGS.batch_size)
-    model = keras_model(weights=None)
-  else:
-    tf.logging.info("Using CIFAR-10 dataset...")
-    dataset_name = "CIFAR-10"
-    ds = dataset.Cifar10Dataset(FLAGS.batch_size)
-    train_dataset = ds.train_dataset
-    val_dataset = ds.test_dataset
-    model = keras_model(
-        weights=None, input_shape=ds.input_shape, classes=ds.num_classes)
-
-  num_gpus = flags_core.get_num_gpus(FLAGS)
-
-  distribution = None
-  # Use distribution strategy
-  if FLAGS.dist_strat:
-    distribution = distribution_utils.get_distribution_strategy(
-        distribution_strategy=FLAGS.distribution_strategy,
-        num_gpus=num_gpus)
-  elif num_gpus > 1:
-    # Run with multi_gpu_model
-    # If eager execution is enabled, only one GPU is utilized even if multiple
-    # GPUs are provided.
-    if FLAGS.eager:
-      tf.logging.warning(
-          "{} GPUs are provided, but only one GPU is utilized as "
-          "eager execution is enabled.".format(num_gpus))
-    model = tf.keras.utils.multi_gpu_model(model, gpus=num_gpus)
-
-  # Adam optimizer and some other optimizers doesn't work well with
-  # distribution strategy (b/113076709)
-  # Use GradientDescentOptimizer here
-  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
-  model.compile(loss="categorical_crossentropy",
-                optimizer=optimizer,
-                metrics=["accuracy"],
-                distribute=distribution)
-
-  # Create benchmark logger for benchmark logging
-  run_params = {
-      "batch_size": FLAGS.batch_size,
-      "synthetic_data": FLAGS.use_synthetic_data,
-      "train_epochs": FLAGS.train_epochs,
-      "num_train_images": FLAGS.num_train_images,
-      "num_eval_images": FLAGS.num_eval_images,
-  }
-
-  benchmark_logger = logger.get_benchmark_logger()
-  benchmark_logger.log_run_info(
-      model_name=FLAGS.model,
-      dataset_name=dataset_name,
-      run_params=run_params,
-      test_id=FLAGS.benchmark_test_id)
-
-  # Create callbacks that log metric values about the training and evaluation
-  callbacks = model_callbacks.get_model_callbacks(
-      FLAGS.callbacks,
-      batch_size=FLAGS.batch_size,
-      metric_logger=benchmark_logger)
-  # Train and evaluate the model
-  history = model.fit(
-      train_dataset,
-      epochs=FLAGS.train_epochs,
-      callbacks=callbacks,
-      validation_data=val_dataset,
-      steps_per_epoch=int(np.ceil(FLAGS.num_train_images / FLAGS.batch_size)),
-      validation_steps=int(np.ceil(FLAGS.num_eval_images / FLAGS.batch_size))
-  )
-
-  tf.logging.info("Logging the evaluation results...")
-  for epoch in range(FLAGS.train_epochs):
-    eval_results = {
-        "accuracy": history.history["val_acc"][epoch],
-        "loss": history.history["val_loss"][epoch],
-        tf.GraphKeys.GLOBAL_STEP: (epoch + 1) * np.ceil(
-            FLAGS.num_eval_images/FLAGS.batch_size)
-    }
-    benchmark_logger.log_evaluation_result(eval_results)
-
-  # Clear the session explicitly to avoid session delete error
-  tf.keras.backend.clear_session()
-
-
-def define_keras_benchmark_flags():
-  """Add flags for keras built-in application models."""
-  flags_core.define_base(hooks=False)
-  flags_core.define_performance()
-  flags_core.define_image()
-  flags_core.define_benchmark()
-  flags.adopt_module_key_flags(flags_core)
-
-  flags_core.set_defaults(
-      data_format="channels_last",
-      use_synthetic_data=True,
-      batch_size=32,
-      train_epochs=2)
-
-  flags.DEFINE_enum(
-      name="model", default=None,
-      enum_values=MODELS.keys(), case_sensitive=False,
-      help=flags_core.help_wrap(
-          "Model to be benchmarked."))
-
-  flags.DEFINE_integer(
-      name="num_train_images", default=1000,
-      help=flags_core.help_wrap(
-          "The number of synthetic images for training. The default value is "
-          "1000."))
-
-  flags.DEFINE_integer(
-      name="num_eval_images", default=50,
-      help=flags_core.help_wrap(
-          "The number of synthetic images for evaluation. The default value is "
-          "50."))
-
-  flags.DEFINE_boolean(
-      name="eager", default=False, help=flags_core.help_wrap(
-          "To enable eager execution. Note that if eager execution is enabled, "
-          "only one GPU is utilized even if multiple GPUs are provided and "
-          "multi_gpu_model is used."))
-
-  flags.DEFINE_boolean(
-      name="dist_strat", default=False, help=flags_core.help_wrap(
-          "To enable distribution strategy for model training and evaluation. "
-          "Number of GPUs used for distribution strategy can be set by the "
-          "argument --num_gpus."))
-
-  flags.DEFINE_list(
-      name="callbacks",
-      default=["ExamplesPerSecondCallback", "LoggingMetricCallback"],
-      help=flags_core.help_wrap(
-          "A list of (case insensitive) strings to specify the names of "
-          "callbacks. For example: `--callbacks ExamplesPerSecondCallback,"
-          "LoggingMetricCallback`"))
-
-  @flags.multi_flags_validator(
-      ["eager", "dist_strat"],
-      message="Both --eager and --dist_strat were set. Only one can be "
-              "defined, as DistributionStrategy is not supported in Eager "
-              "execution currently.")
-  # pylint: disable=unused-variable
-  def _check_eager_dist_strat(flag_dict):
-    return not(flag_dict["eager"] and flag_dict["dist_strat"])
-
-
-def main(_):
-  with logger.benchmark_context(FLAGS):
-    run_keras_model_benchmark(FLAGS)
-
-
-if __name__ == "__main__":
-  tf.logging.set_verbosity(tf.logging.INFO)
-  define_keras_benchmark_flags()
-  FLAGS = flags.FLAGS
-  absl_app.run(main)
diff --git a/official/keras_application_models/dataset.py b/official/keras_application_models/dataset.py
deleted file mode 100644
index 1a35caeb..00000000
--- a/official/keras_application_models/dataset.py
+++ /dev/null
@@ -1,74 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Prepare dataset for keras model benchmark."""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import tensorflow as tf
-from official.utils.misc import model_helpers  # pylint: disable=g-bad-import-order
-
-# Default values for dataset.
-_NUM_CHANNELS = 3
-_NUM_CLASSES = 1000
-
-
-def _get_default_image_size(model):
-  """Provide default image size for each model."""
-  image_size = (224, 224)
-  if model in ["inceptionv3", "xception", "inceptionresnetv2"]:
-    image_size = (299, 299)
-  elif model in ["nasnetlarge"]:
-    image_size = (331, 331)
-  return image_size
-
-
-def generate_synthetic_input_dataset(model, batch_size):
-  """Generate synthetic dataset."""
-  image_size = _get_default_image_size(model)
-  image_shape = (batch_size,) + image_size + (_NUM_CHANNELS,)
-  label_shape = (batch_size, _NUM_CLASSES)
-
-  dataset = model_helpers.generate_synthetic_data(
-      input_shape=tf.TensorShape(image_shape),
-      label_shape=tf.TensorShape(label_shape),
-  )
-  return dataset
-
-
-class Cifar10Dataset(object):
-  """CIFAR10 dataset, including train and test set.
-
-  Each sample consists of a 32x32 color image, and label is from 10 classes.
-  """
-
-  def __init__(self, batch_size):
-    """Initializes train/test datasets.
-
-    Args:
-      batch_size: int, the number of batch size.
-    """
-    self.input_shape = (32, 32, 3)
-    self.num_classes = 10
-    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
-    x_train, x_test = x_train / 255.0, x_test / 255.0
-    y_train, y_test = y_train.astype(np.int64), y_test.astype(np.int64)
-    y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)
-    y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)
-    self.train_dataset = tf.data.Dataset.from_tensor_slices(
-        (x_train, y_train)).shuffle(2000).batch(batch_size).repeat()
-    self.test_dataset = tf.data.Dataset.from_tensor_slices(
-        (x_test, y_test)).shuffle(2000).batch(batch_size).repeat()
diff --git a/official/keras_application_models/model_callbacks.py b/official/keras_application_models/model_callbacks.py
deleted file mode 100644
index 8d75fc12..00000000
--- a/official/keras_application_models/model_callbacks.py
+++ /dev/null
@@ -1,165 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Callbacks for Keras built-in application models.
-
-Note that, in the callbacks, the global_step is initialized in the __init__ of
-each callback rather than on_train_begin. As on_train_begin gets called in
-the fit_loop, and it will be reset with each call to fit(). To keep the
-global_step persistent across all training sessions, it should be initialized in
-the __init__.
-"""
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import time
-
-import tensorflow as tf  # pylint: disable=g-bad-import-order
-
-from official.utils.logs import logger
-
-# Metrics to log after each batch and epoch
-_PER_BATCH_METRICS = {
-    "loss": "train_loss",
-    "acc": "train_accuracy",
-}
-_PER_EPOCH_METRICS = {
-    "loss": "train_loss",
-    "acc": "train_accuracy",
-    "val_loss": "loss",
-    "val_acc": "accuracy"
-}
-
-
-class ExamplesPerSecondCallback(tf.keras.callbacks.Callback):
-  """ExamplesPerSecond callback.
-
-  This callback records the average_examples_per_sec and
-  current_examples_per_sec during training.
-  """
-
-  def __init__(self, batch_size, every_n_steps=1, metric_logger=None):
-    self._batch_size = batch_size
-    self._every_n_steps = every_n_steps
-    self._logger = metric_logger or logger.BaseBenchmarkLogger()
-    self._global_step = 0  # Initialize it in __init__
-    super(ExamplesPerSecondCallback, self).__init__()
-
-  def on_train_begin(self, logs=None):
-    self._train_start_time = time.time()
-    self._last_recorded_time = time.time()
-
-  def on_batch_end(self, batch, logs=None):
-    """Log the examples_per_sec metric every_n_steps."""
-    self._global_step += 1
-    current_time = time.time()
-
-    if self._global_step % self._every_n_steps == 0:
-      average_examples_per_sec = self._batch_size * (
-          self._global_step / (current_time - self._train_start_time))
-      self._logger.log_metric(
-          "average_examples_per_sec", average_examples_per_sec,
-          global_step=self._global_step)
-
-      current_examples_per_sec = self._batch_size * (
-          self._every_n_steps / (current_time - self._last_recorded_time))
-      self._logger.log_metric(
-          "current_examples_per_sec", current_examples_per_sec,
-          global_step=self._global_step)
-      self._last_recorded_time = current_time  # Update last_recorded_time
-
-
-class LoggingMetricCallback(tf.keras.callbacks.Callback):
-  """LoggingMetric callback.
-
-  Log the predefined _PER_BATCH_METRICS after each batch, and log the predefined
-  _PER_EPOCH_METRICS after each epoch.
-  """
-
-  def __init__(self, metric_logger=None):
-    self._logger = metric_logger or logger.BaseBenchmarkLogger()
-    self._per_batch_metrics = _PER_BATCH_METRICS
-    self._per_epoch_metrics = _PER_EPOCH_METRICS
-    self._global_step = 0  # Initialize it in __init__
-    super(LoggingMetricCallback, self).__init__()
-
-  def on_batch_end(self, batch, logs=None):
-    """Log metrics after each batch."""
-    self._global_step += 1
-    for metric in _PER_BATCH_METRICS:
-      self._logger.log_metric(
-          _PER_BATCH_METRICS[metric],
-          logs.get(metric),
-          global_step=self._global_step)
-
-  def on_epoch_end(self, epoch, logs=None):
-    """Log metrics after each epoch."""
-    for metric in _PER_EPOCH_METRICS:
-      self._logger.log_metric(
-          _PER_EPOCH_METRICS[metric],
-          logs.get(metric),
-          global_step=self._global_step)
-
-
-def get_model_callbacks(name_list, **kwargs):
-  """Factory for getting a list of TensorFlow hooks for training by name.
-
-  Args:
-    name_list: a list of strings to name desired callback classes. Allowed:
-      ExamplesPerSecondCallback, LoggingMetricCallback, which are defined
-      as keys in CALLBACKS.
-    **kwargs: a dictionary of arguments to the callbacks.
-
-  Returns:
-    list of instantiated callbacks, ready to be used in a classifier.train call.
-
-  Raises:
-    ValueError: if an unrecognized name is passed.
-  """
-
-  if not name_list:
-    return []
-
-  callbacks = []
-  for name in name_list:
-    callback_name = CALLBACKS.get(name.strip().lower())
-    if callback_name is None:
-      raise ValueError(
-          "Unrecognized training callback requested: {}".format(name))
-    else:
-      callbacks.append(callback_name(**kwargs))
-
-  return callbacks
-
-
-def get_examples_per_second_callback(
-    every_n_steps=1, batch_size=32, metric_logger=None, **kwargs):  # pylint: disable=unused-argument
-  """Function to get ExamplesPerSecondCallback."""
-  return ExamplesPerSecondCallback(
-      batch_size=batch_size, every_n_steps=every_n_steps,
-      metric_logger=metric_logger or logger.get_benchmark_logger())
-
-
-def get_logging_metric_callback(metric_logger=None, **kwargs):  # pylint: disable=unused-argument
-  """Function to get LoggingMetricCallback."""
-  return LoggingMetricCallback(
-      metric_logger=metric_logger or logger.get_benchmark_logger())
-
-
-# A dictionary to map the callback name and its corresponding function
-CALLBACKS = {
-    "examplespersecondcallback": get_examples_per_second_callback,
-    "loggingmetriccallback": get_logging_metric_callback,
-}
