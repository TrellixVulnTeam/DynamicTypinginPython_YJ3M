commit a9684184ee62ce593e46a9821b390dd058936051
Author: Abdullah Rashwan <arashwan@google.com>
Date:   Mon Jun 8 12:43:59 2020 -0700

    Internal change
    
    PiperOrigin-RevId: 315331850

diff --git a/official/core/base_task.py b/official/core/base_task.py
index 14fa30d4..eed53f91 100644
--- a/official/core/base_task.py
+++ b/official/core/base_task.py
@@ -191,8 +191,19 @@ class Task(tf.Module):
       # Scales loss as the default gradients allreduce performs sum inside the
       # optimizer.
       scaled_loss = loss / tf.distribute.get_strategy().num_replicas_in_sync
+
+      # For mixed precision, when a LossScaleOptimizer is used, the loss is
+      # scaled to avoid numeric underflow.
+      if isinstance(optimizer,
+                    tf.keras.mixed_precision.experimental.LossScaleOptimizer):
+        scaled_loss = optimizer.get_scaled_loss(scaled_loss)
+
     tvars = model.trainable_variables
     grads = tape.gradient(scaled_loss, tvars)
+
+    if isinstance(optimizer,
+                  tf.keras.mixed_precision.experimental.LossScaleOptimizer):
+      grads = optimizer.get_unscaled_gradients(grads)
     optimizer.apply_gradients(list(zip(grads, tvars)))
     logs = {self.loss: loss}
     if metrics:
