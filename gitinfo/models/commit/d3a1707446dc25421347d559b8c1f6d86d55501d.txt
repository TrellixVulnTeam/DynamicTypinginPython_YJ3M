commit d3a1707446dc25421347d559b8c1f6d86d55501d
Author: Alan Yee <alyee@ucsd.edu>
Date:   Fri Aug 18 14:33:16 2017 -0700

    Update MaskingNoiseAutoencoderRunner.py
    
    -Fixed styling of print
    -Fixed code according to PEP 8

diff --git a/autoencoder/MaskingNoiseAutoencoderRunner.py b/autoencoder/MaskingNoiseAutoencoderRunner.py
index 4f5db168..73a17a4a 100644
--- a/autoencoder/MaskingNoiseAutoencoderRunner.py
+++ b/autoencoder/MaskingNoiseAutoencoderRunner.py
@@ -1,5 +1,8 @@
-import numpy as np
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
+import numpy as np
 import sklearn.preprocessing as prep
 import tensorflow as tf
 from tensorflow.examples.tutorials.mnist import input_data
@@ -8,29 +11,32 @@ from autoencoder_models.DenoisingAutoencoder import MaskingNoiseAutoencoder
 
 mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
 
+
 def standard_scale(X_train, X_test):
     preprocessor = prep.StandardScaler().fit(X_train)
     X_train = preprocessor.transform(X_train)
     X_test = preprocessor.transform(X_test)
     return X_train, X_test
 
+
 def get_random_block_from_data(data, batch_size):
     start_index = np.random.randint(0, len(data) - batch_size)
     return data[start_index:(start_index + batch_size)]
 
-X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
 
+X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
 
 n_samples = int(mnist.train.num_examples)
 training_epochs = 100
 batch_size = 128
 display_step = 1
 
-autoencoder = MaskingNoiseAutoencoder(n_input = 784,
-                                      n_hidden = 200,
-                                      transfer_function = tf.nn.softplus,
-                                      optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),
-                                      dropout_probability = 0.95)
+autoencoder = MaskingNoiseAutoencoder(
+    n_input = 784,
+    n_hidden = 200,
+    transfer_function = tf.nn.softplus,
+    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),
+    dropout_probability = 0.95)
 
 for epoch in range(training_epochs):
     avg_cost = 0.
@@ -43,6 +49,7 @@ for epoch in range(training_epochs):
         avg_cost += cost / n_samples * batch_size
 
     if epoch % display_step == 0:
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1),
+              "Cost: ", "{:.9f}".format(avg_cost))
 
 print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
