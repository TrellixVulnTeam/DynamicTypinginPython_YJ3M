commit c627506ff35030eb1f64d4e926b7e5a177718b6e
Author: Andr√© Araujo <6316419+andrefaraujo@users.noreply.github.com>
Date:   Fri May 1 20:51:51 2020 -0700

    DELF open-source library v2.0 (#8454)
    
    * Merged commit includes the following changes:
    253126424  by Andre Araujo:
    
        Scripts to compute metrics for Google Landmarks dataset.
    
        Also, a small fix to metric in retrieval case: avoids duplicate predicted images.
    
    --
    253118971  by Andre Araujo:
    
        Metrics for Google Landmarks dataset.
    
    --
    253106953  by Andre Araujo:
    
        Library to read files from Google Landmarks challenges.
    
    --
    250700636  by Andre Araujo:
    
        Handle case of aggregation extraction with empty set of input features.
    
    --
    250516819  by Andre Araujo:
    
        Add minimum size for DELF extractor.
    
    --
    250435822  by Andre Araujo:
    
        Add max_image_size/min_image_size for open-source DELF proto / module.
    
    --
    250414606  by Andre Araujo:
    
        Refactor extract_aggregation to allow reuse with different datasets.
    
    --
    250356863  by Andre Araujo:
    
        Remove unnecessary cmd_args variable from boxes_and_features_extraction.
    
    --
    249783379  by Andre Araujo:
    
        Create directory for writing mapping file if it does not exist.
    
    --
    249581591  by Andre Araujo:
    
        Refactor scripts to extract boxes and features from images in Revisited datasets.
        Also, change tf.logging.info --> print for easier logging in open source code.
    
    --
    249511821  by Andre Araujo:
    
        Small change to function for file/directory handling.
    
    --
    249289499  by Andre Araujo:
    
        Internal change.
    
    --
    
    PiperOrigin-RevId: 253126424
    
    * Updating DELF init to adjust to latest changes
    
    * Editing init files for python packages
    
    * Edit D2R dataset reader to work with py3.
    
    PiperOrigin-RevId: 253135576
    
    * DELF package: fix import ordering
    
    * Adding new requirements to setup.py
    
    * Adding init file for training dir
    
    * Merged commit includes the following changes:
    
    FolderOrigin-RevId: /google/src/cloud/andrearaujo/delf_oss/google3/..
    
    * Adding init file for training subdirs
    
    * Working version of DELF training
    
    * Internal change.
    
    PiperOrigin-RevId: 253248648
    
    * Fix variance loading in open-source code.
    
    PiperOrigin-RevId: 260619120
    
    * Separate image re-ranking as a standalone library, and add metric writing to dataset library.
    
    PiperOrigin-RevId: 260998608
    
    * Tool to read written D2R Revisited datasets metrics file. Test is added.
    
    Also adds a unit test for previously-existing SaveMetricsFile function.
    
    PiperOrigin-RevId: 263361410
    
    * Add optional resize factor for feature extraction.
    
    PiperOrigin-RevId: 264437080
    
    * Fix NumPy's new version spacing changes.
    
    PiperOrigin-RevId: 265127245
    
    * Maker image matching function visible, and add support for RANSAC seed.
    
    PiperOrigin-RevId: 277177468
    
    * Avoid matplotlib failure due to missing display backend.
    
    PiperOrigin-RevId: 287316435
    
    * Removes tf.contrib dependency.
    
    PiperOrigin-RevId: 288842237
    
    * Fix tf contrib removal for feature_aggregation_extractor.
    
    PiperOrigin-RevId: 289487669
    
    * Merged commit includes the following changes:
    309118395  by Andre Araujo:
    
        Make DELF open-source code compatible with TF2.
    
    --
    309067582  by Andre Araujo:
    
        Handle image resizing rounding properly for python extraction.
    
        New behavior is tested with unit tests.
    
    --
    308690144  by Andre Araujo:
    
        Several changes to improve DELF model/training code and make it work in TF 2.1.0:
        - Rename some files for better clarity
        - Using compat.v1 versions of functions
        - Formatting changes
        - Using more appropriate TF function names
    
    --
    308689397  by Andre Araujo:
    
        Internal change.
    
    --
    308341315  by Andre Araujo:
    
        Remove old slim dependency in DELF open-source model.
    
        This avoids issues with requiring old TF-v1, making it compatible with latest TF.
    
    --
    306777559  by Andre Araujo:
    
        Internal change
    
    --
    304505811  by Andre Araujo:
    
        Raise error during geometric verification if local features have different dimensionalities.
    
    --
    301739992  by Andre Araujo:
    
        Transform some geometric verification constants into arguments, to allow custom matching.
    
    --
    301300324  by Andre Araujo:
    
        Apply name change(experimental_run_v2 -> run) for all callers in Tensorflow.
    
    --
    299919057  by Andre Araujo:
    
        Automated refactoring to make code Python 3 compatible.
    
    --
    297953698  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297521242  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297278247  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297270405  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297238741  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297108605  by Andre Araujo:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    294676131  by Andre Araujo:
    
        Add option to resize images to square resolutions without aspect ratio preservation.
    
    --
    293849641  by Andre Araujo:
    
        Internal change.
    
    --
    293840896  by Andre Araujo:
    
        Changing Slim import to tf_slim codebase.
    
    --
    293661660  by Andre Araujo:
    
        Allow the delf training script to read from TFRecords dataset.
    
    --
    291755295  by Andre Araujo:
    
        Internal change.
    
    --
    291448508  by Andre Araujo:
    
        Internal change.
    
    --
    291414459  by Andre Araujo:
    
        Adding train script.
    
    --
    291384336  by Andre Araujo:
    
        Adding model export script and test.
    
    --
    291260565  by Andre Araujo:
    
        Adding placeholder for Google Landmarks dataset.
    
    --
    291205548  by Andre Araujo:
    
        Definition of DELF model using Keras ResNet50 as backbone.
    
    --
    289500793  by Andre Araujo:
    
        Add TFRecord building script for delf.
    
    --
    
    PiperOrigin-RevId: 309118395
    
    * Updating README, dependency versions
    
    * Updating training README
    
    * Fixing init import of export_model
    
    * Fixing init import of export_model_utils
    
    * tkinter in INSTALL_INSTRUCTIONS
    
    * Merged commit includes the following changes:
    
    FolderOrigin-RevId: /google/src/cloud/andrearaujo/delf_oss/google3/..
    
    * INSTALL_INSTRUCTIONS mentioning different cloning options

diff --git a/research/delf/INSTALL_INSTRUCTIONS.md b/research/delf/INSTALL_INSTRUCTIONS.md
index 4d3f04db..5c0a851b 100644
--- a/research/delf/INSTALL_INSTRUCTIONS.md
+++ b/research/delf/INSTALL_INSTRUCTIONS.md
@@ -2,17 +2,34 @@
 
 ### Tensorflow
 
-For detailed steps to install Tensorflow, follow the [Tensorflow installation
-instructions](https://www.tensorflow.org/install/). A typical user can install
-Tensorflow using one of the following commands:
+For detailed steps to install Tensorflow, follow the
+[Tensorflow installation instructions](https://www.tensorflow.org/install/). A
+typical user can install Tensorflow using one of the following commands:
 
 ```bash
 # For CPU:
-pip install 'tensorflow==1.14'
+pip install 'tensorflow'
 # For GPU:
-pip install 'tensorflow-gpu==1.14'
+pip install 'tensorflow-gpu'
 ```
 
+### TF-Slim
+
+Note: currently, we need to install the latest version from source, to avoid
+using previous versions which relied on tf.contrib (which is now deprecated).
+
+```bash
+git clone git@github.com:google-research/tf-slim.git
+cd tf-slim
+pip install .
+```
+
+Note that these commands assume you are cloning using SSH. If you are using
+HTTPS instead, use `git clone https://github.com/google-research/tf-slim.git`
+instead. See
+[this link](https://help.github.com/en/github/using-git/which-remote-url-should-i-use)
+for more information.
+
 ### Protobuf
 
 The DELF library uses [protobuf](https://github.com/google/protobuf) (the python
@@ -33,6 +50,7 @@ Install python library dependencies:
 
 ```bash
 pip install matplotlib numpy scikit-image scipy
+sudo apt-get install python-tk
 ```
 
 ### `tensorflow/models`
@@ -43,18 +61,20 @@ your `PYTHONPATH`, as instructed
 [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md))
 
 ```bash
-git clone https://github.com/tensorflow/models
-
-# First, install slim's "nets" package.
-cd models/research/slim/
-pip install -e .
+git clone git@github.com:tensorflow/models.git
 
-# Second, setup the object_detection module by editing PYTHONPATH.
+# Setup the object_detection module by editing PYTHONPATH.
 cd ..
 # From tensorflow/models/research/
 export PYTHONPATH=$PYTHONPATH:`pwd`
 ```
 
+Note that these commands assume you are cloning using SSH. If you are using
+HTTPS instead, use `git clone https://github.com/tensorflow/models.git` instead.
+See
+[this link](https://help.github.com/en/github/using-git/which-remote-url-should-i-use)
+for more information.
+
 Then, compile DELF's protobufs. Use `PATH_TO_PROTOC` as the directory where you
 downloaded the `protoc` compiler.
 
@@ -63,7 +83,8 @@ downloaded the `protoc` compiler.
 ${PATH_TO_PROTOC?}/bin/protoc delf/protos/*.proto --python_out=.
 ```
 
-Finally, install the DELF package.
+Finally, install the DELF package. This may also install some other dependencies
+under the hood.
 
 ```bash
 # From tensorflow/models/research/delf/
@@ -85,11 +106,21 @@ loaded successfully.
 
 Installation issues may happen if multiple python versions are mixed. The
 instructions above assume python2.7 version is used; if using python3.X, be sure
-to use `pip3` instead of `pip`, and all should work.
+to use `pip3` instead of `pip`, `python3-tk` instead of `python-tk`, and all
+should work.
 
 #### `pip install`
 
 Issues might be observed if using `pip install` with `-e` option (editable
 mode). You may try out to simply remove the `-e` from the commands above. Also,
-depending on your machine setup, you might need to run the `sudo pip install` command,
-that is with a `sudo` at the beginning.
+depending on your machine setup, you might need to run the `sudo pip install`
+command, that is with a `sudo` at the beginning.
+
+#### Cloning github repositories
+
+The default commands above assume you are cloning using SSH. If you are using
+HTTPS instead, use for example `git clone
+https://github.com/tensorflow/models.git` instead of `git clone
+git@github.com:tensorflow/models.git`. See
+[this link](https://help.github.com/en/github/using-git/which-remote-url-should-i-use)
+for more information.
diff --git a/research/delf/README.md b/research/delf/README.md
index 3b8a3d55..ee605f64 100644
--- a/research/delf/README.md
+++ b/research/delf/README.md
@@ -1,6 +1,3 @@
-![TensorFlow Requirement: 1.x](https://img.shields.io/badge/TensorFlow%20Requirement-1.x-brightgreen)
-![TensorFlow 2 Not Supported](https://img.shields.io/badge/TensorFlow%202%20Not%20Supported-%E2%9C%95-red.svg)
-
 # DELF: DEep Local Features
 
 This project presents code for extracting DELF features, which were introduced
@@ -17,7 +14,7 @@ detects and describes semantic local features which can be geometrically
 verified between images showing the same object instance. The pre-trained models
 released here have been optimized for landmark recognition, so expect it to work
 well in this area. We also provide tensorflow code for building the DELF model,
-which could then be used to train models for other types of objects.
+and [NEW] code for model training.
 
 If you make use of this code, please consider citing the following papers:
 
@@ -37,6 +34,10 @@ Proc. CVPR'19
 
 ## News
 
+-   [Jun'19] DELF achieved 2nd place in
+    [CVPR Visual Localization challenge (Local Features track)](https://sites.google.com/corp/view/ltvl2019).
+    See our slides
+    [here](https://docs.google.com/presentation/d/e/2PACX-1vTswzoXelqFqI_pCEIVl2uazeyGr7aKNklWHQCX-CbQ7MB17gaycqIaDTguuUCRm6_lXHwCdrkP7n1x/pub?start=false&loop=false&delayms=3000).
 -   [Apr'19] Check out our CVPR'19 paper:
     ["Detect-to-Retrieve: Efficient Regional Aggregation for Image Search"](https://arxiv.org/abs/1812.01584)
 -   [Jun'18] DELF achieved state-of-the-art results in a CVPR'18 image retrieval
@@ -59,7 +60,9 @@ We have two Google-Landmarks dataset versions:
     [Landmark Recognition](https://www.kaggle.com/c/landmark-recognition-2019)
     and [Landmark Retrieval](https://www.kaggle.com/c/landmark-retrieval-2019).
     It can be downloaded from CVDF
-    [here](https://github.com/cvdfoundation/google-landmark).
+    [here](https://github.com/cvdfoundation/google-landmark). See also
+    [the CVPR'20 paper](https://arxiv.org/abs/2004.01804) on this new dataset
+    version.
 
 If you make use of these datasets in your research, please consider citing the
 papers mentioned above.
@@ -109,6 +112,10 @@ should obtain a nice figure showing local feature matches, as:
 
 ![MatchedImagesExample](delf/python/examples/matched_images_example.jpg)
 
+### DELF training
+
+Please follow [these instructions](delf/python/training/README.md).
+
 ### Landmark detection
 
 Please follow [these instructions](DETECTION.md). At the end, you should obtain
@@ -145,7 +152,7 @@ This directory contains files for several different purposes:
 
 -   `box_io.py`, `datum_io.py`, `feature_io.py` are helper files for reading and
     writing tensors and features.
--   `delf_v1.py` contains the code to create DELF models.
+-   `delf_v1.py` contains code to create DELF models.
 -   `feature_aggregation_extractor.py` contains a module to perform local
     feature aggregation.
 -   `feature_aggregation_similarity.py` contains a module to perform similarity
@@ -160,29 +167,62 @@ feature extraction/matching, and object detection:
 
 -   `delf_config_example.pbtxt` shows an example instantiation of the DelfConfig
     proto, used for DELF feature extraction.
+-   `detector.py` is a module to construct an object detector function.
 -   `extract_boxes.py` enables object detection from a list of images.
 -   `extract_features.py` enables DELF extraction from a list of images.
+-   `extractor.py` is a module to construct a DELF local feature extraction
+    function.
 -   `match_images.py` supports image matching using DELF features extracted
     using `extract_features.py`.
 
 The subdirectory `delf/python/detect_to_retrieve` contains sample
 scripts/configs related to the Detect-to-Retrieve paper:
 
+-   `aggregation_extraction.py` is a library to extract/save feature
+    aggregation.
+-   `boxes_and_features_extraction.py` is a library to extract/save boxes and
+    DELF features.
 -   `cluster_delf_features.py` for local feature clustering.
 -   `dataset.py` for parsing/evaluating results on Revisited Oxford/Paris
     datasets.
+-   `delf_gld_config.pbtxt` gives the DelfConfig used in Detect-to-Retrieve
+    paper.
 -   `extract_aggregation.py` for aggregated local feature extraction.
 -   `extract_index_boxes_and_features.py` for index image local feature
     extraction / bounding box detection on Revisited datasets.
 -   `extract_query_features.py` for query image local feature extraction on
     Revisited datasets.
+-   `image_reranking.py` is a module to re-rank images with geometric
+    verification.
 -   `perform_retrieval.py` for performing retrieval/evaluating methods using
     aggregated local features on Revisited datasets.
--   `delf_gld_config.pbtxt` gives the DelfConfig used in Detect-to-Retrieve
-    paper.
 -   `index_aggregation_config.pbtxt`, `query_aggregation_config.pbtxt` give
     AggregationConfig's for Detect-to-Retrieve experiments.
 
+The subdirectory `delf/python/google_landmarks_dataset` contains sample
+scripts/modules for computing GLD metrics:
+
+-   `compute_recognition_metrics.py` performs recognition metric computation
+    given input predictions and solution files.
+-   `compute_retrieval_metrics.py` performs retrieval metric computation given
+    input predictions and solution files.
+-   `dataset_file_io.py` is a module for dataset-related file IO.
+-   `metrics.py` is a module for GLD metric computation.
+
+The subdirectory `delf/python/training` contains sample scripts/modules for
+performing DELF training:
+
+-   `datasets/googlelandmarks.py` is the dataset module used for training.
+-   `model/delf_model.py` is the model module used for training.
+-   `model/export_model.py` is a script for exporting trained models in the
+    format used by the inference code.
+-   `model/export_model_utils.py` is a module with utilities for model
+    exporting.
+-   `model/resnet50.py` is a module with a backbone RN50 implementation.
+-   `build_image_dataset.py` converts downloaded dataset into TFRecords format
+    for training.
+-   `train.py` is the main training script.
+
 Besides these, other files in the different subdirectories contain tests for the
 various modules.
 
@@ -192,6 +232,13 @@ Andr&eacute; Araujo (@andrefaraujo)
 
 ## Release history
 
+### April, 2020 (version 2.0)
+
+-   Initial DELF training code released.
+-   Codebase is now fully compatible with TF 2.1.
+
+**Thanks to contributors**: Arun Mukundan, Yuewei Na and Andr&eacute; Araujo.
+
 ### April, 2019
 
 Detect-to-Retrieve code released.
diff --git a/research/delf/delf/__init__.py b/research/delf/delf/__init__.py
index 78a084b0..bfbca8db 100644
--- a/research/delf/delf/__init__.py
+++ b/research/delf/delf/__init__.py
@@ -33,5 +33,7 @@ from delf.python import feature_io
 from delf.python.examples import detector
 from delf.python.examples import extractor
 from delf.python import detect_to_retrieve
-from delf.python import google_landmarks_dataset
+from delf.python import training
+from delf.python.training import model
+from delf.python.training import datasets
 # pylint: enable=unused-import
diff --git a/research/delf/delf/python/box_io.py b/research/delf/delf/python/box_io.py
index 0286c671..8b0f0d2c 100644
--- a/research/delf/delf/python/box_io.py
+++ b/research/delf/delf/python/box_io.py
@@ -132,7 +132,7 @@ def ReadFromFile(file_path):
     scores: [N] float array with detection scores.
     class_indices: [N] int array with class indices.
   """
-  with tf.gfile.GFile(file_path, 'rb') as f:
+  with tf.io.gfile.GFile(file_path, 'rb') as f:
     return ParseFromString(f.read())
 
 
@@ -147,5 +147,5 @@ def WriteToFile(file_path, boxes, scores, class_indices):
     class_indices: [N] int array with class indices.
   """
   serialized_data = SerializeToString(boxes, scores, class_indices)
-  with tf.gfile.GFile(file_path, 'w') as f:
+  with tf.io.gfile.GFile(file_path, 'w') as f:
     f.write(serialized_data)
diff --git a/research/delf/delf/python/box_io_test.py b/research/delf/delf/python/box_io_test.py
index c54e3648..b733fc9c 100644
--- a/research/delf/delf/python/box_io_test.py
+++ b/research/delf/delf/python/box_io_test.py
@@ -57,7 +57,7 @@ class BoxesIoTest(tf.test.TestCase):
   def testWriteAndReadToFile(self):
     boxes, scores, class_indices = self._create_data()
 
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.boxes')
     box_io.WriteToFile(filename, boxes, scores, class_indices)
     data_read = box_io.ReadFromFile(filename)
@@ -67,7 +67,7 @@ class BoxesIoTest(tf.test.TestCase):
     self.assertAllEqual(class_indices, data_read[2])
 
   def testWriteAndReadToFileEmptyFile(self):
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.box')
     box_io.WriteToFile(filename, np.array([]), np.array([]), np.array([]))
     data_read = box_io.ReadFromFile(filename)
diff --git a/research/delf/delf/python/datum_io.py b/research/delf/delf/python/datum_io.py
index d101141a..f0d4cbfd 100644
--- a/research/delf/delf/python/datum_io.py
+++ b/research/delf/delf/python/datum_io.py
@@ -179,7 +179,7 @@ def ReadFromFile(file_path):
   Returns:
     data: NumPy array.
   """
-  with tf.gfile.GFile(file_path, 'rb') as f:
+  with tf.io.gfile.GFile(file_path, 'rb') as f:
     return ParseFromString(f.read())
 
 
@@ -192,7 +192,7 @@ def ReadPairFromFile(file_path):
   Returns:
     Two NumPy arrays.
   """
-  with tf.gfile.GFile(file_path, 'rb') as f:
+  with tf.io.gfile.GFile(file_path, 'rb') as f:
     return ParsePairFromString(f.read())
 
 
@@ -204,7 +204,7 @@ def WriteToFile(data, file_path):
     file_path: Path to file that will be written.
   """
   serialized_data = SerializeToString(data)
-  with tf.gfile.GFile(file_path, 'w') as f:
+  with tf.io.gfile.GFile(file_path, 'w') as f:
     f.write(serialized_data)
 
 
@@ -217,5 +217,5 @@ def WritePairToFile(arr_1, arr_2, file_path):
     file_path: Path to file that will be written.
   """
   serialized_data = SerializePairToString(arr_1, arr_2)
-  with tf.gfile.GFile(file_path, 'w') as f:
+  with tf.io.gfile.GFile(file_path, 'w') as f:
     f.write(serialized_data)
diff --git a/research/delf/delf/python/datum_io_test.py b/research/delf/delf/python/datum_io_test.py
index f13fd73d..00a94936 100644
--- a/research/delf/delf/python/datum_io_test.py
+++ b/research/delf/delf/python/datum_io_test.py
@@ -69,7 +69,7 @@ class DatumIoTest(tf.test.TestCase):
   def testWriteAndReadToFile(self):
     data = np.array([[[-1.0, 125.0, -2.5], [14.5, 3.5, 0.0]],
                      [[20.0, 0.0, 30.0], [25.5, 36.0, 42.0]]])
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.datum')
     datum_io.WriteToFile(data, filename)
     data_read = datum_io.ReadFromFile(filename)
@@ -84,7 +84,7 @@ class DatumIoTest(tf.test.TestCase):
     data_2 = np.array(
         [[[255, 0, 5], [10, 300, 0]], [[20, 1, 100], [255, 360, 420]]],
         dtype='uint32')
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.datum_pair')
     datum_io.WritePairToFile(data_1, data_2, filename)
     data_read_1, data_read_2 = datum_io.ReadPairFromFile(filename)
diff --git a/research/delf/delf/python/delf_v1.py b/research/delf/delf/python/delf_v1.py
index 7fcf44fe..27772a00 100644
--- a/research/delf/delf/python/delf_v1.py
+++ b/research/delf/delf/python/delf_v1.py
@@ -26,10 +26,9 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
-
-from nets import resnet_v1
-
-slim = tf.contrib.slim
+from tf_slim import layers
+from tf_slim.nets import resnet_v1
+from tf_slim.ops.arg_scope import arg_scope
 
 _SUPPORTED_TARGET_LAYER = ['resnet_v1_50/block3', 'resnet_v1_50/block4']
 
@@ -68,7 +67,7 @@ class DelfV1(object):
   """
 
   def __init__(self, target_layer_type=_SUPPORTED_TARGET_LAYER[0]):
-    tf.logging.info('Creating model %s ', target_layer_type)
+    tf.compat.v1.logging.info('Creating model %s ', target_layer_type)
 
     self._target_layer_type = target_layer_type
     if self._target_layer_type not in _SUPPORTED_TARGET_LAYER:
@@ -89,8 +88,8 @@ class DelfV1(object):
     the attention score map.
 
     Args:
-      attention_feature_map: Potentially normalized feature map that will
-        be aggregated with attention score map.
+      attention_feature_map: Potentially normalized feature map that will be
+        aggregated with attention score map.
       feature_map: Unnormalized feature map that will be used to compute
         attention score map.
       attention_nonlinear: Type of non-linearity that will be applied to
@@ -105,11 +104,11 @@ class DelfV1(object):
     Raises:
       ValueError: If unknown attention non-linearity type is provided.
     """
-    with tf.variable_scope(
+    with tf.compat.v1.variable_scope(
         'attention', values=[attention_feature_map, feature_map]):
-      with tf.variable_scope('compute', values=[feature_map]):
+      with tf.compat.v1.variable_scope('compute', values=[feature_map]):
         activation_fn_conv1 = tf.nn.relu
-        feature_map_conv1 = slim.conv2d(
+        feature_map_conv1 = layers.conv2d(
             feature_map,
             512,
             kernel,
@@ -117,7 +116,7 @@ class DelfV1(object):
             activation_fn=activation_fn_conv1,
             scope='conv1')
 
-        attention_score = slim.conv2d(
+        attention_score = layers.conv2d(
             feature_map_conv1,
             1,
             kernel,
@@ -127,12 +126,12 @@ class DelfV1(object):
             scope='conv2')
 
       # Set activation of conv2 layer of attention model.
-      with tf.variable_scope(
+      with tf.compat.v1.variable_scope(
           'merge', values=[attention_feature_map, attention_score]):
         if attention_nonlinear not in _SUPPORTED_ATTENTION_NONLINEARITY:
           raise ValueError('Unknown attention non-linearity.')
         if attention_nonlinear == 'softplus':
-          with tf.variable_scope(
+          with tf.compat.v1.variable_scope(
               'softplus_attention',
               values=[attention_feature_map, attention_score]):
             attention_prob = tf.nn.softplus(attention_score)
@@ -169,7 +168,7 @@ class DelfV1(object):
     Raises:
       ValueError: If unknown attention_type is provided.
     """
-    with tf.variable_scope(
+    with tf.compat.v1.variable_scope(
         _ATTENTION_VARIABLE_SCOPE,
         values=[feature_map, end_points],
         reuse=reuse):
@@ -182,8 +181,9 @@ class DelfV1(object):
         attention_feature_map = feature_map
       end_points['attention_feature_map'] = attention_feature_map
 
-      attention_outputs = self._PerformAttention(
-          attention_feature_map, feature_map, attention_nonlinear, kernel)
+      attention_outputs = self._PerformAttention(attention_feature_map,
+                                                 feature_map,
+                                                 attention_nonlinear, kernel)
       prelogits, attention_prob, attention_score = attention_outputs
       end_points['prelogits'] = prelogits
       end_points['attention_prob'] = attention_prob
@@ -248,8 +248,8 @@ class DelfV1(object):
       kernel: Convolutional kernel to use in attention layers (eg, [3, 3]).
       training_resnet: Whether or not the Resnet blocks from the model are in
         training mode.
-      training_attention: Whether or not the attention part of the model is
-        in training mode.
+      training_attention: Whether or not the attention part of the model is in
+        training mode.
       reuse: Whether or not the layer and its variables should be reused.
       use_batch_norm: Whether or not to use batch normalization.
 
@@ -262,18 +262,17 @@ class DelfV1(object):
       end_points: Set of activations for external use.
     """
     # Construct Resnet50 features.
-    with slim.arg_scope(
-        resnet_v1.resnet_arg_scope(use_batch_norm=use_batch_norm)):
+    with arg_scope(resnet_v1.resnet_arg_scope(use_batch_norm=use_batch_norm)):
       _, end_points = self.GetResnet50Subnetwork(
           images, is_training=training_resnet, reuse=reuse)
 
     feature_map = end_points[self._target_layer_type]
 
     # Construct attention subnetwork on top of features.
-    with slim.arg_scope(
+    with arg_scope(
         resnet_v1.resnet_arg_scope(
             weight_decay=weight_decay, use_batch_norm=use_batch_norm)):
-      with slim.arg_scope([slim.batch_norm], is_training=training_attention):
+      with arg_scope([layers.batch_norm], is_training=training_attention):
         (prelogits, attention_prob, attention_score,
          end_points) = self._GetAttentionSubnetwork(
              feature_map,
@@ -330,13 +329,13 @@ class DelfV1(object):
             training_resnet=training_resnet,
             training_attention=training_attention,
             reuse=reuse))
-    with slim.arg_scope(
+    with arg_scope(
         resnet_v1.resnet_arg_scope(
             weight_decay=weight_decay, batch_norm_scale=True)):
-      with slim.arg_scope([slim.batch_norm], is_training=training_attention):
-        with tf.variable_scope(
+      with arg_scope([layers.batch_norm], is_training=training_attention):
+        with tf.compat.v1.variable_scope(
             _ATTENTION_VARIABLE_SCOPE, values=[attention_feat], reuse=reuse):
-          logits = slim.conv2d(
+          logits = layers.conv2d(
               attention_feat,
               num_classes, [1, 1],
               activation_fn=None,
diff --git a/research/delf/delf/python/detect_to_retrieve/aggregation_extraction.py b/research/delf/delf/python/detect_to_retrieve/aggregation_extraction.py
index 70c32df0..af85e27d 100644
--- a/research/delf/delf/python/detect_to_retrieve/aggregation_extraction.py
+++ b/research/delf/delf/python/detect_to_retrieve/aggregation_extraction.py
@@ -59,7 +59,7 @@ def _ReadMappingBasenameToBoxNames(input_path, index_image_names):
       strings (file names containing DELF features for boxes).
   """
   images_to_box_feature_files = {}
-  with tf.gfile.GFile(input_path, 'r') as f:
+  with tf.io.gfile.GFile(input_path, 'r') as f:
     reader = csv.DictReader(f)
     for row in reader:
       index_image_name = index_image_names[int(row['index_image_id'])]
@@ -101,7 +101,7 @@ def ExtractAggregatedRepresentationsToFiles(image_names, features_dir,
 
   # Parse AggregationConfig proto, and select output extension.
   config = aggregation_config_pb2.AggregationConfig()
-  with tf.gfile.GFile(aggregation_config_path, 'r') as f:
+  with tf.io.gfile.GFile(aggregation_config_path, 'r') as f:
     text_format.Merge(f.read(), config)
   output_extension = '.'
   if config.use_regional_aggregation:
@@ -121,10 +121,10 @@ def ExtractAggregatedRepresentationsToFiles(image_names, features_dir,
         mapping_path, image_names)
 
   # Create output directory if necessary.
-  if not tf.gfile.Exists(output_aggregation_dir):
-    tf.gfile.MakeDirs(output_aggregation_dir)
+  if not tf.io.gfile.exists(output_aggregation_dir):
+    tf.io.gfile.makedirs(output_aggregation_dir)
 
-  with tf.Session() as sess:
+  with tf.compat.v1.Session() as sess:
     extractor = feature_aggregation_extractor.ExtractAggregatedRepresentation(
         sess, config)
 
diff --git a/research/delf/delf/python/detect_to_retrieve/boxes_and_features_extraction.py b/research/delf/delf/python/detect_to_retrieve/boxes_and_features_extraction.py
index 22cbce75..ddc977b1 100644
--- a/research/delf/delf/python/detect_to_retrieve/boxes_and_features_extraction.py
+++ b/research/delf/delf/python/detect_to_retrieve/boxes_and_features_extraction.py
@@ -55,7 +55,7 @@ def _PilLoader(path):
   Returns:
     PIL image in RGB format.
   """
-  with tf.gfile.GFile(path, 'rb') as f:
+  with tf.io.gfile.GFile(path, 'rb') as f:
     img = Image.open(f)
     return img.convert('RGB')
 
@@ -68,7 +68,7 @@ def _WriteMappingBasenameToIds(index_names_ids_and_boxes, output_path):
       ID and box ID.
     output_path: Output CSV path.
   """
-  with tf.gfile.GFile(output_path, 'w') as f:
+  with tf.io.gfile.GFile(output_path, 'w') as f:
     csv_writer = csv.DictWriter(
         f, fieldnames=['name', 'index_image_id', 'box_id'])
     csv_writer.writeheader()
@@ -118,22 +118,22 @@ def ExtractBoxesAndFeaturesToFiles(image_names, image_paths, delf_config_path,
 
   # Parse DelfConfig proto.
   config = delf_config_pb2.DelfConfig()
-  with tf.gfile.GFile(delf_config_path, 'r') as f:
+  with tf.io.gfile.GFile(delf_config_path, 'r') as f:
     text_format.Merge(f.read(), config)
 
   # Create output directories if necessary.
-  if not tf.gfile.Exists(output_features_dir):
-    tf.gfile.MakeDirs(output_features_dir)
-  if not tf.gfile.Exists(output_boxes_dir):
-    tf.gfile.MakeDirs(output_boxes_dir)
-  if not tf.gfile.Exists(os.path.dirname(output_mapping)):
-    tf.gfile.MakeDirs(os.path.dirname(output_mapping))
+  if not tf.io.gfile.exists(output_features_dir):
+    tf.io.gfile.makedirs(output_features_dir)
+  if not tf.io.gfile.exists(output_boxes_dir):
+    tf.io.gfile.makedirs(output_boxes_dir)
+  if not tf.io.gfile.exists(os.path.dirname(output_mapping)):
+    tf.io.gfile.makedirs(os.path.dirname(output_mapping))
 
   names_ids_and_boxes = []
   with tf.Graph().as_default():
-    with tf.Session() as sess:
+    with tf.compat.v1.Session() as sess:
       # Initialize variables, construct detector and DELF extractor.
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       sess.run(init_op)
       detector_fn = detector.MakeDetector(
           sess, detector_model_dir, import_scope='detector')
@@ -161,7 +161,7 @@ def ExtractBoxesAndFeaturesToFiles(image_names, image_paths, delf_config_path,
         width, height = pil_im.size
 
         # Extract and save boxes.
-        if tf.gfile.Exists(output_box_filename):
+        if tf.io.gfile.exists(output_box_filename):
           print('Skipping box computation for %s' % image_name)
           (boxes_out, scores_out,
            class_indices_out) = box_io.ReadFromFile(output_box_filename)
@@ -197,7 +197,7 @@ def ExtractBoxesAndFeaturesToFiles(image_names, image_paths, delf_config_path,
 
           names_ids_and_boxes.append([box_name, i, delf_file_ind - 1])
 
-          if tf.gfile.Exists(output_feature_filename):
+          if tf.io.gfile.exists(output_feature_filename):
             print('Skipping DELF computation for %s' % box_name)
             continue
 
diff --git a/research/delf/delf/python/detect_to_retrieve/cluster_delf_features.py b/research/delf/delf/python/detect_to_retrieve/cluster_delf_features.py
index dc3f6e67..9ddda8e4 100644
--- a/research/delf/delf/python/detect_to_retrieve/cluster_delf_features.py
+++ b/research/delf/delf/python/detect_to_retrieve/cluster_delf_features.py
@@ -52,7 +52,7 @@ _DELF_DIM = 128
 _STATUS_CHECK_ITERATIONS = 100
 
 
-class _IteratorInitHook(tf.train.SessionRunHook):
+class _IteratorInitHook(tf.estimator.SessionRunHook):
   """Hook to initialize data iterator after session is created."""
 
   def __init__(self):
@@ -70,14 +70,14 @@ def main(argv):
     raise RuntimeError('Too many command-line arguments.')
 
   # Process output directory.
-  if tf.gfile.Exists(cmd_args.output_cluster_dir):
+  if tf.io.gfile.exists(cmd_args.output_cluster_dir):
     raise RuntimeError(
         'output_cluster_dir = %s already exists. This may indicate that a '
         'previous run already wrote checkpoints in this directory, which would '
         'lead to incorrect training. Please re-run this script by specifying an'
         ' inexisting directory.' % cmd_args.output_cluster_dir)
   else:
-    tf.gfile.MakeDirs(cmd_args.output_cluster_dir)
+    tf.io.gfile.makedirs(cmd_args.output_cluster_dir)
 
   # Read list of index images from dataset file.
   print('Reading list of index images from dataset file...')
@@ -126,8 +126,8 @@ def main(argv):
       Returns:
         Tensor with the data for training.
       """
-      features_placeholder = tf.placeholder(tf.float32,
-                                            features_for_clustering.shape)
+      features_placeholder = tf.compat.v1.placeholder(
+          tf.float32, features_for_clustering.shape)
       delf_dataset = tf.data.Dataset.from_tensor_slices((features_placeholder))
       delf_dataset = delf_dataset.shuffle(1000).batch(
           features_for_clustering.shape[0])
@@ -146,7 +146,7 @@ def main(argv):
 
   input_fn, init_hook = _get_input_fn()
 
-  kmeans = tf.estimator.experimental.KMeans(
+  kmeans = tf.compat.v1.estimator.experimental.KMeans(
       num_clusters=cmd_args.num_clusters,
       model_dir=cmd_args.output_cluster_dir,
       use_mini_batch=False,
diff --git a/research/delf/delf/python/detect_to_retrieve/dataset.py b/research/delf/delf/python/detect_to_retrieve/dataset.py
index 4e6af5c2..9a1e6b24 100644
--- a/research/delf/delf/python/detect_to_retrieve/dataset.py
+++ b/research/delf/delf/python/detect_to_retrieve/dataset.py
@@ -40,7 +40,7 @@ def ReadDatasetFile(dataset_file_path):
       array of integers; additionally, it has a key 'bbx' mapping to a NumPy
       array of floats with bounding box coordinates.
   """
-  with tf.gfile.GFile(dataset_file_path, 'rb') as f:
+  with tf.io.gfile.GFile(dataset_file_path, 'rb') as f:
     cfg = matlab.loadmat(f)
 
   # Parse outputs according to the specificities of the dataset file.
@@ -314,3 +314,156 @@ def ComputeMetrics(sorted_index_ids, ground_truth, desired_pr_ranks):
 
   return (mean_average_precision, mean_precisions, mean_recalls,
           average_precisions, precisions, recalls)
+
+
+def SaveMetricsFile(mean_average_precision, mean_precisions, mean_recalls,
+                    pr_ranks, output_path):
+  """Saves aggregated retrieval metrics to text file.
+
+  Args:
+    mean_average_precision: Dict mapping each dataset protocol to a float.
+    mean_precisions: Dict mapping each dataset protocol to a NumPy array of
+      floats with shape [len(pr_ranks)].
+    mean_recalls: Dict mapping each dataset protocol to a NumPy array of floats
+      with shape [len(pr_ranks)].
+    pr_ranks: List of integers.
+    output_path: Full file path.
+  """
+  with tf.io.gfile.GFile(output_path, 'w') as f:
+    for k in sorted(mean_average_precision.keys()):
+      f.write('{}\n  mAP={}\n  mP@k{} {}\n  mR@k{} {}\n'.format(
+          k, np.around(mean_average_precision[k] * 100, decimals=2),
+          np.array(pr_ranks), np.around(mean_precisions[k] * 100, decimals=2),
+          np.array(pr_ranks), np.around(mean_recalls[k] * 100, decimals=2)))
+
+
+def _ParseSpaceSeparatedStringsInBrackets(line, prefixes, ind):
+  """Parses line containing space-separated strings in brackets.
+
+  Args:
+    line: String, containing line in metrics file with mP@k or mR@k figures.
+    prefixes: Tuple/list of strings, containing valid prefixes.
+    ind: Integer indicating which field within brackets is parsed.
+
+  Yields:
+    entry: String format entry.
+
+  Raises:
+    ValueError: If input line does not contain a valid prefix.
+  """
+  for prefix in prefixes:
+    if line.startswith(prefix):
+      line = line[len(prefix):]
+      break
+  else:
+    raise ValueError('Line %s is malformed, cannot find valid prefixes' % line)
+
+  for entry in line.split('[')[ind].split(']')[0].split():
+    yield entry
+
+
+def _ParsePrRanks(line):
+  """Parses PR ranks from mP@k line in metrics file.
+
+  Args:
+    line: String, containing line in metrics file with mP@k figures.
+
+  Returns:
+    pr_ranks: List of integers, containing used ranks.
+
+  Raises:
+    ValueError: If input line is malformed.
+  """
+  return [
+      int(pr_rank) for pr_rank in _ParseSpaceSeparatedStringsInBrackets(
+          line, ['  mP@k['], 0) if pr_rank
+  ]
+
+
+def _ParsePrScores(line, num_pr_ranks):
+  """Parses PR scores from line in metrics file.
+
+  Args:
+    line: String, containing line in metrics file with mP@k or mR@k figures.
+    num_pr_ranks: Integer, number of scores that should be in output list.
+
+  Returns:
+    pr_scores: List of floats, containing scores.
+
+  Raises:
+    ValueError: If input line is malformed.
+  """
+  pr_scores = [
+      float(pr_score) for pr_score in _ParseSpaceSeparatedStringsInBrackets(
+          line, ('  mP@k[', '  mR@k['), 1) if pr_score
+  ]
+
+  if len(pr_scores) != num_pr_ranks:
+    raise ValueError('Line %s is malformed, expected %d scores but found %d' %
+                     (line, num_pr_ranks, len(pr_scores)))
+
+  return pr_scores
+
+
+def ReadMetricsFile(metrics_path):
+  """Reads aggregated retrieval metrics from text file.
+
+  Args:
+    metrics_path: Full file path, containing aggregated retrieval metrics.
+
+  Returns:
+    mean_average_precision: Dict mapping each dataset protocol to a float.
+    pr_ranks: List of integer ranks used in aggregated recall/precision metrics.
+    mean_precisions: Dict mapping each dataset protocol to a NumPy array of
+      floats with shape [len(`pr_ranks`)].
+    mean_recalls: Dict mapping each dataset protocol to a NumPy array of floats
+      with shape [len(`pr_ranks`)].
+
+  Raises:
+    ValueError: If input file is malformed.
+  """
+  with tf.io.gfile.GFile(metrics_path, 'r') as f:
+    file_contents_stripped = [l.rstrip() for l in f]
+
+  if len(file_contents_stripped) % 4:
+    raise ValueError(
+        'Malformed input %s: number of lines must be a multiple of 4, '
+        'but it is %d' % (metrics_path, len(file_contents_stripped)))
+
+  mean_average_precision = {}
+  pr_ranks = []
+  mean_precisions = {}
+  mean_recalls = {}
+  protocols = set()
+  for i in range(0, len(file_contents_stripped), 4):
+    protocol = file_contents_stripped[i]
+    if protocol in protocols:
+      raise ValueError(
+          'Malformed input %s: protocol %s is found a second time' %
+          (metrics_path, protocol))
+    protocols.add(protocol)
+
+    # Parse mAP.
+    mean_average_precision[protocol] = float(
+        file_contents_stripped[i + 1].split('=')[1]) / 100.0
+
+    # Parse (or check consistency of) pr_ranks.
+    parsed_pr_ranks = _ParsePrRanks(file_contents_stripped[i + 2])
+    if not pr_ranks:
+      pr_ranks = parsed_pr_ranks
+    else:
+      if parsed_pr_ranks != pr_ranks:
+        raise ValueError('Malformed input %s: inconsistent PR ranks' %
+                         metrics_path)
+
+    # Parse mean precisions.
+    mean_precisions[protocol] = np.array(
+        _ParsePrScores(file_contents_stripped[i + 2], len(pr_ranks)),
+        dtype=float) / 100.0
+
+    # Parse mean recalls.
+    mean_recalls[protocol] = np.array(
+        _ParsePrScores(file_contents_stripped[i + 3], len(pr_ranks)),
+        dtype=float) / 100.0
+
+  return mean_average_precision, pr_ranks, mean_precisions, mean_recalls
diff --git a/research/delf/delf/python/detect_to_retrieve/dataset_test.py b/research/delf/delf/python/detect_to_retrieve/dataset_test.py
index ec8d27d4..d7b3ac2d 100644
--- a/research/delf/delf/python/detect_to_retrieve/dataset_test.py
+++ b/research/delf/delf/python/detect_to_retrieve/dataset_test.py
@@ -18,6 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import os
+
 import numpy as np
 import tensorflow as tf
 
@@ -192,6 +194,92 @@ class DatasetTest(tf.test.TestCase):
     self.assertAllClose(precisions, expected_precisions)
     self.assertAllClose(recalls, expected_recalls)
 
+  def testSaveMetricsFileWorks(self):
+    # Define inputs.
+    mean_average_precision = {'hard': 0.7, 'medium': 0.9}
+    mean_precisions = {
+        'hard': np.array([1.0, 0.8]),
+        'medium': np.array([1.0, 1.0])
+    }
+    mean_recalls = {
+        'hard': np.array([0.5, 0.8]),
+        'medium': np.array([0.5, 1.0])
+    }
+    pr_ranks = [1, 5]
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'metrics.txt')
+
+    # Run tested function.
+    dataset.SaveMetricsFile(mean_average_precision, mean_precisions,
+                            mean_recalls, pr_ranks, output_path)
+
+    # Define expected results.
+    expected_metrics = ('hard\n'
+                        '  mAP=70.0\n'
+                        '  mP@k[1 5] [100.  80.]\n'
+                        '  mR@k[1 5] [50. 80.]\n'
+                        'medium\n'
+                        '  mAP=90.0\n'
+                        '  mP@k[1 5] [100. 100.]\n'
+                        '  mR@k[1 5] [ 50. 100.]\n')
+
+    # Parse actual results, and compare to expected.
+    with tf.io.gfile.GFile(output_path) as f:
+      metrics = f.read()
+
+    self.assertEqual(metrics, expected_metrics)
+
+  def testSaveAndReadMetricsWorks(self):
+    # Define inputs.
+    mean_average_precision = {'hard': 0.7, 'medium': 0.9}
+    mean_precisions = {
+        'hard': np.array([1.0, 0.8]),
+        'medium': np.array([1.0, 1.0])
+    }
+    mean_recalls = {
+        'hard': np.array([0.5, 0.8]),
+        'medium': np.array([0.5, 1.0])
+    }
+    pr_ranks = [1, 5]
+    output_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'metrics.txt')
+
+    # Run tested functions.
+    dataset.SaveMetricsFile(mean_average_precision, mean_precisions,
+                            mean_recalls, pr_ranks, output_path)
+    (read_mean_average_precision, read_pr_ranks, read_mean_precisions,
+     read_mean_recalls) = dataset.ReadMetricsFile(output_path)
+
+    # Compares actual and expected metrics.
+    self.assertEqual(read_mean_average_precision, mean_average_precision)
+    self.assertEqual(read_pr_ranks, pr_ranks)
+    self.assertEqual(read_mean_precisions.keys(), mean_precisions.keys())
+    self.assertAllEqual(read_mean_precisions['hard'], mean_precisions['hard'])
+    self.assertAllEqual(read_mean_precisions['medium'],
+                        mean_precisions['medium'])
+    self.assertEqual(read_mean_recalls.keys(), mean_recalls.keys())
+    self.assertAllEqual(read_mean_recalls['hard'], mean_recalls['hard'])
+    self.assertAllEqual(read_mean_recalls['medium'], mean_recalls['medium'])
+
+  def testReadMetricsWithRepeatedProtocolFails(self):
+    # Define inputs.
+    input_path = os.path.join(tf.compat.v1.test.get_temp_dir(), 'metrics.txt')
+    with tf.io.gfile.GFile(input_path, 'w') as f:
+      f.write('hard\n'
+              '  mAP=70.0\n'
+              '  mP@k[1 5] [ 100.   80.]\n'
+              '  mR@k[1 5] [ 50.  80.]\n'
+              'medium\n'
+              '  mAP=90.0\n'
+              '  mP@k[1 5] [ 100.  100.]\n'
+              '  mR@k[1 5] [  50.  100.]\n'
+              'medium\n'
+              '  mAP=90.0\n'
+              '  mP@k[1 5] [ 100.  100.]\n'
+              '  mR@k[1 5] [  50.  100.]\n')
+
+    # Run tested functions.
+    with self.assertRaisesRegex(ValueError, 'Malformed input'):
+      dataset.ReadMetricsFile(input_path)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/delf/delf/python/detect_to_retrieve/extract_query_features.py b/research/delf/delf/python/detect_to_retrieve/extract_query_features.py
index d7a174aa..7bb9d509 100644
--- a/research/delf/delf/python/detect_to_retrieve/extract_query_features.py
+++ b/research/delf/delf/python/detect_to_retrieve/extract_query_features.py
@@ -61,7 +61,7 @@ def _PilLoader(path):
   Returns:
     PIL image in RGB format.
   """
-  with tf.gfile.GFile(path, 'rb') as f:
+  with tf.io.gfile.GFile(path, 'rb') as f:
     img = Image.open(f)
     return img.convert('RGB')
 
@@ -70,28 +70,29 @@ def main(argv):
   if len(argv) > 1:
     raise RuntimeError('Too many command-line arguments.')
 
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   # Read list of query images from dataset file.
-  tf.logging.info('Reading list of query images and boxes from dataset file...')
+  tf.compat.v1.logging.info(
+      'Reading list of query images and boxes from dataset file...')
   query_list, _, ground_truth = dataset.ReadDatasetFile(
       cmd_args.dataset_file_path)
   num_images = len(query_list)
-  tf.logging.info('done! Found %d images', num_images)
+  tf.compat.v1.logging.info('done! Found %d images', num_images)
 
   # Parse DelfConfig proto.
   config = delf_config_pb2.DelfConfig()
-  with tf.gfile.GFile(cmd_args.delf_config_path, 'r') as f:
+  with tf.io.gfile.GFile(cmd_args.delf_config_path, 'r') as f:
     text_format.Merge(f.read(), config)
 
   # Create output directory if necessary.
-  if not tf.gfile.Exists(cmd_args.output_features_dir):
-    tf.gfile.MakeDirs(cmd_args.output_features_dir)
+  if not tf.io.gfile.exists(cmd_args.output_features_dir):
+    tf.io.gfile.makedirs(cmd_args.output_features_dir)
 
   with tf.Graph().as_default():
-    with tf.Session() as sess:
+    with tf.compat.v1.Session() as sess:
       # Initialize variables, construct DELF extractor.
-      init_op = tf.global_variables_initializer()
+      init_op = tf.compat.v1.global_variables_initializer()
       sess.run(init_op)
       extractor_fn = extractor.MakeExtractor(sess, config)
 
@@ -102,8 +103,8 @@ def main(argv):
                                             query_image_name + _IMAGE_EXTENSION)
         output_feature_filename = os.path.join(
             cmd_args.output_features_dir, query_image_name + _DELF_EXTENSION)
-        if tf.gfile.Exists(output_feature_filename):
-          tf.logging.info('Skipping %s', query_image_name)
+        if tf.io.gfile.exists(output_feature_filename):
+          tf.compat.v1.logging.info('Skipping %s', query_image_name)
           continue
 
         # Crop query image according to bounding box.
diff --git a/research/delf/delf/python/detect_to_retrieve/image_reranking.py b/research/delf/delf/python/detect_to_retrieve/image_reranking.py
new file mode 100644
index 00000000..9460e429
--- /dev/null
+++ b/research/delf/delf/python/detect_to_retrieve/image_reranking.py
@@ -0,0 +1,213 @@
+# Copyright 2019 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Library to re-rank images based on geometric verification."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+import numpy as np
+from scipy import spatial
+from skimage import measure
+from skimage import transform
+
+from delf import feature_io
+
+# Extensions.
+_DELF_EXTENSION = '.delf'
+
+# Pace to log.
+_STATUS_CHECK_GV_ITERATIONS = 10
+
+# Re-ranking / geometric verification parameters.
+_NUM_TO_RERANK = 100
+_NUM_RANSAC_TRIALS = 1000
+_MIN_RANSAC_SAMPLES = 3
+
+
+def MatchFeatures(query_locations,
+                  query_descriptors,
+                  index_image_locations,
+                  index_image_descriptors,
+                  ransac_seed=None,
+                  feature_distance_threshold=0.9,
+                  ransac_residual_threshold=10.0):
+  """Matches local features using geometric verification.
+
+  First, finds putative local feature matches by matching `query_descriptors`
+  against a KD-tree from the `index_image_descriptors`. Then, attempts to fit an
+  affine transformation between the putative feature corresponces using their
+  locations.
+
+  Args:
+    query_locations: Locations of local features for query image. NumPy array of
+      shape [#query_features, 2].
+    query_descriptors: Descriptors of local features for query image. NumPy
+      array of shape [#query_features, depth].
+    index_image_locations: Locations of local features for index image. NumPy
+      array of shape [#index_image_features, 2].
+    index_image_descriptors: Descriptors of local features for index image.
+      NumPy array of shape [#index_image_features, depth].
+    ransac_seed: Seed used by RANSAC. If None (default), no seed is provided.
+    feature_distance_threshold: Distance threshold below which a pair of
+      features is considered a potential match, and will be fed into RANSAC.
+    ransac_residual_threshold: Residual error threshold for considering matches
+      as inliers, used in RANSAC algorithm.
+
+  Returns:
+    score: Number of inliers of match. If no match is found, returns 0.
+
+  Raises:
+    ValueError: If local descriptors from query and index images have different
+      dimensionalities.
+  """
+  num_features_query = query_locations.shape[0]
+  num_features_index_image = index_image_locations.shape[0]
+  if not num_features_query or not num_features_index_image:
+    return 0
+
+  local_feature_dim = query_descriptors.shape[1]
+  if index_image_descriptors.shape[1] != local_feature_dim:
+    raise ValueError(
+        'Local feature dimensionality is not consistent for query and index '
+        'images.')
+
+  # Find nearest-neighbor matches using a KD tree.
+  index_image_tree = spatial.cKDTree(index_image_descriptors)
+  _, indices = index_image_tree.query(
+      query_descriptors, distance_upper_bound=feature_distance_threshold)
+
+  # Select feature locations for putative matches.
+  query_locations_to_use = np.array([
+      query_locations[i,]
+      for i in range(num_features_query)
+      if indices[i] != num_features_index_image
+  ])
+  index_image_locations_to_use = np.array([
+      index_image_locations[indices[i],]
+      for i in range(num_features_query)
+      if indices[i] != num_features_index_image
+  ])
+
+  # If there are no putative matches, early return 0.
+  if not query_locations_to_use.shape[0]:
+    return 0
+
+  # Perform geometric verification using RANSAC.
+  _, inliers = measure.ransac(
+      (index_image_locations_to_use, query_locations_to_use),
+      transform.AffineTransform,
+      min_samples=_MIN_RANSAC_SAMPLES,
+      residual_threshold=ransac_residual_threshold,
+      max_trials=_NUM_RANSAC_TRIALS,
+      random_state=ransac_seed)
+  if inliers is None:
+    inliers = []
+
+  return sum(inliers)
+
+
+def RerankByGeometricVerification(input_ranks, initial_scores, query_name,
+                                  index_names, query_features_dir,
+                                  index_features_dir, junk_ids):
+  """Re-ranks retrieval results using geometric verification.
+
+  Args:
+    input_ranks: 1D NumPy array with indices of top-ranked index images, sorted
+      from the most to the least similar.
+    initial_scores: 1D NumPy array with initial similarity scores between query
+      and index images. Entry i corresponds to score for image i.
+    query_name: Name for query image (string).
+    index_names: List of names for index images (strings).
+    query_features_dir: Directory where query local feature file is located
+      (string).
+    index_features_dir: Directory where index local feature files are located
+      (string).
+    junk_ids: Set with indices of junk images which should not be considered
+      during re-ranking.
+
+  Returns:
+    output_ranks: 1D NumPy array with index image indices, sorted from the most
+      to the least similar according to the geometric verification and initial
+      scores.
+
+  Raises:
+    ValueError: If `input_ranks`, `initial_scores` and `index_names` do not have
+      the same number of entries.
+  """
+  num_index_images = len(index_names)
+  if len(input_ranks) != num_index_images:
+    raise ValueError('input_ranks and index_names have different number of '
+                     'elements: %d vs %d' %
+                     (len(input_ranks), len(index_names)))
+  if len(initial_scores) != num_index_images:
+    raise ValueError('initial_scores and index_names have different number of '
+                     'elements: %d vs %d' %
+                     (len(initial_scores), len(index_names)))
+
+  # Filter out junk images from list that will be re-ranked.
+  input_ranks_for_gv = []
+  for ind in input_ranks:
+    if ind not in junk_ids:
+      input_ranks_for_gv.append(ind)
+  num_to_rerank = min(_NUM_TO_RERANK, len(input_ranks_for_gv))
+
+  # Load query image features.
+  query_features_path = os.path.join(query_features_dir,
+                                     query_name + _DELF_EXTENSION)
+  query_locations, _, query_descriptors, _, _ = feature_io.ReadFromFile(
+      query_features_path)
+
+  # Initialize list containing number of inliers and initial similarity scores.
+  inliers_and_initial_scores = []
+  for i in range(num_index_images):
+    inliers_and_initial_scores.append([0, initial_scores[i]])
+
+  # Loop over top-ranked images and get results.
+  print('Starting to re-rank')
+  for i in range(num_to_rerank):
+    if i > 0 and i % _STATUS_CHECK_GV_ITERATIONS == 0:
+      print('Re-ranking: i = %d out of %d' % (i, num_to_rerank))
+
+    index_image_id = input_ranks_for_gv[i]
+
+    # Load index image features.
+    index_image_features_path = os.path.join(
+        index_features_dir, index_names[index_image_id] + _DELF_EXTENSION)
+    (index_image_locations, _, index_image_descriptors, _,
+     _) = feature_io.ReadFromFile(index_image_features_path)
+
+    inliers_and_initial_scores[index_image_id][0] = MatchFeatures(
+        query_locations, query_descriptors, index_image_locations,
+        index_image_descriptors)
+
+  # Sort based on (inliers_score, initial_score).
+  def _InliersInitialScoresSorting(k):
+    """Helper function to sort list based on two entries.
+
+    Args:
+      k: Index into `inliers_and_initial_scores`.
+
+    Returns:
+      Tuple containing inlier score and initial score.
+    """
+    return (inliers_and_initial_scores[k][0], inliers_and_initial_scores[k][1])
+
+  output_ranks = sorted(
+      range(num_index_images), key=_InliersInitialScoresSorting, reverse=True)
+
+  return output_ranks
diff --git a/research/delf/delf/python/detect_to_retrieve/perform_retrieval.py b/research/delf/delf/python/detect_to_retrieve/perform_retrieval.py
index e7a37d34..c2034dfb 100644
--- a/research/delf/delf/python/detect_to_retrieve/perform_retrieval.py
+++ b/research/delf/delf/python/detect_to_retrieve/perform_retrieval.py
@@ -24,9 +24,6 @@ import sys
 import time
 
 import numpy as np
-from scipy import spatial
-from skimage import measure
-from skimage import transform
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -34,8 +31,8 @@ from tensorflow.python.platform import app
 from delf import aggregation_config_pb2
 from delf import datum_io
 from delf import feature_aggregation_similarity
-from delf import feature_io
 from delf.python.detect_to_retrieve import dataset
+from delf.python.detect_to_retrieve import image_reranking
 
 cmd_args = None
 
@@ -45,7 +42,6 @@ _ASMK = aggregation_config_pb2.AggregationConfig.ASMK
 _ASMK_STAR = aggregation_config_pb2.AggregationConfig.ASMK_STAR
 
 # Extensions.
-_DELF_EXTENSION = '.delf'
 _VLAD_EXTENSION_SUFFIX = 'vlad'
 _ASMK_EXTENSION_SUFFIX = 'asmk'
 _ASMK_STAR_EXTENSION_SUFFIX = 'asmk_star'
@@ -55,18 +51,10 @@ _PR_RANKS = (1, 5, 10)
 
 # Pace to log.
 _STATUS_CHECK_LOAD_ITERATIONS = 50
-_STATUS_CHECK_GV_ITERATIONS = 10
 
 # Output file names.
 _METRICS_FILENAME = 'metrics.txt'
 
-# Re-ranking / geometric verification parameters.
-_NUM_TO_RERANK = 100
-_FEATURE_DISTANCE_THRESHOLD = 0.9
-_NUM_RANSAC_TRIALS = 1000
-_MIN_RANSAC_SAMPLES = 3
-_RANSAC_RESIDUAL_THRESHOLD = 10
-
 
 def _ReadAggregatedDescriptors(input_dir, image_list, config):
   """Reads aggregated descriptors.
@@ -123,180 +111,6 @@ def _ReadAggregatedDescriptors(input_dir, image_list, config):
   return aggregated_descriptors, visual_words
 
 
-def _MatchFeatures(query_locations, query_descriptors, index_image_locations,
-                   index_image_descriptors):
-  """Matches local features using geometric verification.
-
-  First, finds putative local feature matches by matching `query_descriptors`
-  against a KD-tree from the `index_image_descriptors`. Then, attempts to fit an
-  affine transformation between the putative feature corresponces using their
-  locations.
-
-  Args:
-    query_locations: Locations of local features for query image. NumPy array of
-      shape [#query_features, 2].
-    query_descriptors: Descriptors of local features for query image. NumPy
-      array of shape [#query_features, depth].
-    index_image_locations: Locations of local features for index image. NumPy
-      array of shape [#index_image_features, 2].
-    index_image_descriptors: Descriptors of local features for index image.
-      NumPy array of shape [#index_image_features, depth].
-
-  Returns:
-    score: Number of inliers of match. If no match is found, returns 0.
-  """
-  num_features_query = query_locations.shape[0]
-  num_features_index_image = index_image_locations.shape[0]
-  if not num_features_query or not num_features_index_image:
-    return 0
-
-  # Find nearest-neighbor matches using a KD tree.
-  index_image_tree = spatial.cKDTree(index_image_descriptors)
-  _, indices = index_image_tree.query(
-      query_descriptors, distance_upper_bound=_FEATURE_DISTANCE_THRESHOLD)
-
-  # Select feature locations for putative matches.
-  query_locations_to_use = np.array([
-      query_locations[i,]
-      for i in range(num_features_query)
-      if indices[i] != num_features_index_image
-  ])
-  index_image_locations_to_use = np.array([
-      index_image_locations[indices[i],]
-      for i in range(num_features_query)
-      if indices[i] != num_features_index_image
-  ])
-
-  # If there are no putative matches, early return 0.
-  if not query_locations_to_use.shape[0]:
-    return 0
-
-  # Perform geometric verification using RANSAC.
-  _, inliers = measure.ransac(
-      (index_image_locations_to_use, query_locations_to_use),
-      transform.AffineTransform,
-      min_samples=_MIN_RANSAC_SAMPLES,
-      residual_threshold=_RANSAC_RESIDUAL_THRESHOLD,
-      max_trials=_NUM_RANSAC_TRIALS)
-  if inliers is None:
-    inliers = []
-
-  return sum(inliers)
-
-
-def _RerankByGeometricVerification(input_ranks, initial_scores, query_name,
-                                   index_names, query_features_dir,
-                                   index_features_dir, junk_ids):
-  """Re-ranks retrieval results using geometric verification.
-
-  Args:
-    input_ranks: 1D NumPy array with indices of top-ranked index images, sorted
-      from the most to the least similar.
-    initial_scores: 1D NumPy array with initial similarity scores between query
-      and index images. Entry i corresponds to score for image i.
-    query_name: Name for query image (string).
-    index_names: List of names for index images (strings).
-    query_features_dir: Directory where query local feature file is located
-      (string).
-    index_features_dir: Directory where index local feature files are located
-      (string).
-    junk_ids: Set with indices of junk images which should not be considered
-      during re-ranking.
-
-  Returns:
-    output_ranks: 1D NumPy array with index image indices, sorted from the most
-      to the least similar according to the geometric verification and initial
-      scores.
-
-  Raises:
-    ValueError: If `input_ranks`, `initial_scores` and `index_names` do not have
-      the same number of entries.
-  """
-  num_index_images = len(index_names)
-  if len(input_ranks) != num_index_images:
-    raise ValueError('input_ranks and index_names have different number of '
-                     'elements: %d vs %d' %
-                     (len(input_ranks), len(index_names)))
-  if len(initial_scores) != num_index_images:
-    raise ValueError('initial_scores and index_names have different number of '
-                     'elements: %d vs %d' %
-                     (len(initial_scores), len(index_names)))
-
-  # Filter out junk images from list that will be re-ranked.
-  input_ranks_for_gv = []
-  for ind in input_ranks:
-    if ind not in junk_ids:
-      input_ranks_for_gv.append(ind)
-  num_to_rerank = min(_NUM_TO_RERANK, len(input_ranks_for_gv))
-
-  # Load query image features.
-  query_features_path = os.path.join(query_features_dir,
-                                     query_name + _DELF_EXTENSION)
-  query_locations, _, query_descriptors, _, _ = feature_io.ReadFromFile(
-      query_features_path)
-
-  # Initialize list containing number of inliers and initial similarity scores.
-  inliers_and_initial_scores = []
-  for i in range(num_index_images):
-    inliers_and_initial_scores.append([0, initial_scores[i]])
-
-  # Loop over top-ranked images and get results.
-  print('Starting to re-rank')
-  for i in range(num_to_rerank):
-    if i > 0 and i % _STATUS_CHECK_GV_ITERATIONS == 0:
-      print('Re-ranking: i = %d out of %d' % (i, num_to_rerank))
-
-    index_image_id = input_ranks_for_gv[i]
-
-    # Load index image features.
-    index_image_features_path = os.path.join(
-        index_features_dir, index_names[index_image_id] + _DELF_EXTENSION)
-    (index_image_locations, _, index_image_descriptors, _,
-     _) = feature_io.ReadFromFile(index_image_features_path)
-
-    inliers_and_initial_scores[index_image_id][0] = _MatchFeatures(
-        query_locations, query_descriptors, index_image_locations,
-        index_image_descriptors)
-
-  # Sort based on (inliers_score, initial_score).
-  def _InliersInitialScoresSorting(k):
-    """Helper function to sort list based on two entries.
-
-    Args:
-      k: Index into `inliers_and_initial_scores`.
-
-    Returns:
-      Tuple containing inlier score and initial score.
-    """
-    return (inliers_and_initial_scores[k][0], inliers_and_initial_scores[k][1])
-
-  output_ranks = sorted(
-      range(num_index_images), key=_InliersInitialScoresSorting, reverse=True)
-
-  return output_ranks
-
-
-def _SaveMetricsFile(mean_average_precision, mean_precisions, mean_recalls,
-                     pr_ranks, output_path):
-  """Saves aggregated retrieval metrics to text file.
-
-  Args:
-    mean_average_precision: Dict mapping each dataset protocol to a float.
-    mean_precisions: Dict mapping each dataset protocol to a NumPy array of
-      floats with shape [len(pr_ranks)].
-    mean_recalls: Dict mapping each dataset protocol to a NumPy array of floats
-      with shape [len(pr_ranks)].
-    pr_ranks: List of integers.
-    output_path: Full file path.
-  """
-  with tf.gfile.GFile(output_path, 'w') as f:
-    for k in sorted(mean_average_precision.keys()):
-      f.write('{}\n  mAP={}\n  mP@k{} {}\n  mR@k{} {}\n'.format(
-          k, np.around(mean_average_precision[k] * 100, decimals=2),
-          np.array(pr_ranks), np.around(mean_precisions[k] * 100, decimals=2),
-          np.array(pr_ranks), np.around(mean_recalls[k] * 100, decimals=2)))
-
-
 def main(argv):
   if len(argv) > 1:
     raise RuntimeError('Too many command-line arguments.')
@@ -314,10 +128,10 @@ def main(argv):
 
   # Parse AggregationConfig protos.
   query_config = aggregation_config_pb2.AggregationConfig()
-  with tf.gfile.GFile(cmd_args.query_aggregation_config_path, 'r') as f:
+  with tf.io.gfile.GFile(cmd_args.query_aggregation_config_path, 'r') as f:
     text_format.Merge(f.read(), query_config)
   index_config = aggregation_config_pb2.AggregationConfig()
-  with tf.gfile.GFile(cmd_args.index_aggregation_config_path, 'r') as f:
+  with tf.io.gfile.GFile(cmd_args.index_aggregation_config_path, 'r') as f:
     text_format.Merge(f.read(), index_config)
 
   # Read aggregated descriptors.
@@ -355,11 +169,11 @@ def main(argv):
 
     # Re-rank using geometric verification.
     if cmd_args.use_geometric_verification:
-      medium_ranks_after_gv[i] = _RerankByGeometricVerification(
+      medium_ranks_after_gv[i] = image_reranking.RerankByGeometricVerification(
           ranks_before_gv[i], similarities, query_list[i], index_list,
           cmd_args.query_features_dir, cmd_args.index_features_dir,
           set(medium_ground_truth[i]['junk']))
-      hard_ranks_after_gv[i] = _RerankByGeometricVerification(
+      hard_ranks_after_gv[i] = image_reranking.RerankByGeometricVerification(
           ranks_before_gv[i], similarities, query_list[i], index_list,
           cmd_args.query_features_dir, cmd_args.index_features_dir,
           set(hard_ground_truth[i]['junk']))
@@ -368,8 +182,8 @@ def main(argv):
     print('done! Retrieval for query %d took %f seconds' % (i, elapsed))
 
   # Create output directory if necessary.
-  if not tf.gfile.Exists(cmd_args.output_dir):
-    tf.gfile.MakeDirs(cmd_args.output_dir)
+  if not tf.io.gfile.exists(cmd_args.output_dir):
+    tf.io.gfile.makedirs(cmd_args.output_dir)
 
   # Compute metrics.
   medium_metrics = dataset.ComputeMetrics(ranks_before_gv, medium_ground_truth,
@@ -403,9 +217,9 @@ def main(argv):
         'medium_after_gv': medium_metrics_after_gv[2],
         'hard_after_gv': hard_metrics_after_gv[2]
     })
-  _SaveMetricsFile(mean_average_precision_dict, mean_precisions_dict,
-                   mean_recalls_dict, _PR_RANKS,
-                   os.path.join(cmd_args.output_dir, _METRICS_FILENAME))
+  dataset.SaveMetricsFile(mean_average_precision_dict, mean_precisions_dict,
+                          mean_recalls_dict, _PR_RANKS,
+                          os.path.join(cmd_args.output_dir, _METRICS_FILENAME))
 
 
 if __name__ == '__main__':
diff --git a/research/delf/delf/python/examples/detector.py b/research/delf/delf/python/examples/detector.py
index f72c1d80..080c0bfb 100644
--- a/research/delf/delf/python/examples/detector.py
+++ b/research/delf/delf/python/examples/detector.py
@@ -32,8 +32,8 @@ def MakeDetector(sess, model_dir, import_scope=None):
   Returns:
     Function that receives an image and returns detection results.
   """
-  tf.saved_model.loader.load(
-      sess, [tf.saved_model.tag_constants.SERVING],
+  tf.compat.v1.saved_model.loader.load(
+      sess, [tf.compat.v1.saved_model.tag_constants.SERVING],
       model_dir,
       import_scope=import_scope)
   import_scope_prefix = import_scope + '/' if import_scope is not None else ''
diff --git a/research/delf/delf/python/examples/extract_boxes.py b/research/delf/delf/python/examples/extract_boxes.py
index 6e315d17..5eda40d1 100644
--- a/research/delf/delf/python/examples/extract_boxes.py
+++ b/research/delf/delf/python/examples/extract_boxes.py
@@ -58,7 +58,7 @@ def _ReadImageList(list_path):
   Returns:
     image_paths: List of image paths.
   """
-  with tf.gfile.GFile(list_path, 'r') as f:
+  with tf.io.gfile.GFile(list_path, 'r') as f:
     image_paths = f.readlines()
   image_paths = [entry.rstrip() for entry in image_paths]
   return image_paths
@@ -130,46 +130,48 @@ def main(argv):
   if len(argv) > 1:
     raise RuntimeError('Too many command-line arguments.')
 
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   # Read list of images.
-  tf.logging.info('Reading list of images...')
+  tf.compat.v1.logging.info('Reading list of images...')
   image_paths = _ReadImageList(cmd_args.list_images_path)
   num_images = len(image_paths)
-  tf.logging.info('done! Found %d images', num_images)
+  tf.compat.v1.logging.info('done! Found %d images', num_images)
 
   # Create output directories if necessary.
-  if not tf.gfile.Exists(cmd_args.output_dir):
-    tf.gfile.MakeDirs(cmd_args.output_dir)
-  if cmd_args.output_viz_dir and not tf.gfile.Exists(cmd_args.output_viz_dir):
-    tf.gfile.MakeDirs(cmd_args.output_viz_dir)
+  if not tf.io.gfile.exists(cmd_args.output_dir):
+    tf.io.gfile.makedirs(cmd_args.output_dir)
+  if cmd_args.output_viz_dir and not tf.io.gfile.exists(
+      cmd_args.output_viz_dir):
+    tf.io.gfile.makedirs(cmd_args.output_viz_dir)
 
   # Tell TensorFlow that the model will be built into the default Graph.
   with tf.Graph().as_default():
     # Reading list of images.
-    filename_queue = tf.train.string_input_producer(image_paths, shuffle=False)
-    reader = tf.WholeFileReader()
+    filename_queue = tf.compat.v1.train.string_input_producer(
+        image_paths, shuffle=False)
+    reader = tf.compat.v1.WholeFileReader()
     _, value = reader.read(filename_queue)
-    image_tf = tf.image.decode_jpeg(value, channels=3)
+    image_tf = tf.io.decode_jpeg(value, channels=3)
     image_tf = tf.expand_dims(image_tf, 0)
 
-    with tf.Session() as sess:
-      init_op = tf.global_variables_initializer()
+    with tf.compat.v1.Session() as sess:
+      init_op = tf.compat.v1.global_variables_initializer()
       sess.run(init_op)
 
       detector_fn = detector.MakeDetector(sess, cmd_args.detector_path)
 
       # Start input enqueue threads.
       coord = tf.train.Coordinator()
-      threads = tf.train.start_queue_runners(sess=sess, coord=coord)
+      threads = tf.compat.v1.train.start_queue_runners(sess=sess, coord=coord)
       start = time.clock()
       for i, image_path in enumerate(image_paths):
         # Write to log-info once in a while.
         if i == 0:
-          tf.logging.info('Starting to detect objects in images...')
+          tf.compat.v1.logging.info('Starting to detect objects in images...')
         elif i % _STATUS_CHECK_ITERATIONS == 0:
           elapsed = (time.clock() - start)
-          tf.logging.info(
+          tf.compat.v1.logging.info(
               'Processing image %d out of %d, last %d '
               'images took %f seconds', i, num_images, _STATUS_CHECK_ITERATIONS,
               elapsed)
@@ -183,8 +185,8 @@ def main(argv):
         out_boxes_filename = base_boxes_filename + _BOX_EXT
         out_boxes_fullpath = os.path.join(cmd_args.output_dir,
                                           out_boxes_filename)
-        if tf.gfile.Exists(out_boxes_fullpath):
-          tf.logging.info('Skipping %s', image_path)
+        if tf.io.gfile.exists(out_boxes_fullpath):
+          tf.compat.v1.logging.info('Skipping %s', image_path)
           continue
 
         # Extract and save boxes.
diff --git a/research/delf/delf/python/examples/extract_features.py b/research/delf/delf/python/examples/extract_features.py
index 405020b6..d182c101 100644
--- a/research/delf/delf/python/examples/extract_features.py
+++ b/research/delf/delf/python/examples/extract_features.py
@@ -27,6 +27,7 @@ import os
 import sys
 import time
 
+from six.moves import range
 import tensorflow as tf
 
 from google.protobuf import text_format
@@ -53,55 +54,57 @@ def _ReadImageList(list_path):
   Returns:
     image_paths: List of image paths.
   """
-  with tf.gfile.GFile(list_path, 'r') as f:
+  with tf.io.gfile.GFile(list_path, 'r') as f:
     image_paths = f.readlines()
   image_paths = [entry.rstrip() for entry in image_paths]
   return image_paths
 
 
 def main(unused_argv):
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   # Read list of images.
-  tf.logging.info('Reading list of images...')
+  tf.compat.v1.logging.info('Reading list of images...')
   image_paths = _ReadImageList(cmd_args.list_images_path)
   num_images = len(image_paths)
-  tf.logging.info('done! Found %d images', num_images)
+  tf.compat.v1.logging.info('done! Found %d images', num_images)
 
   # Parse DelfConfig proto.
   config = delf_config_pb2.DelfConfig()
-  with tf.gfile.FastGFile(cmd_args.config_path, 'r') as f:
+  with tf.io.gfile.GFile(cmd_args.config_path, 'r') as f:
     text_format.Merge(f.read(), config)
 
   # Create output directory if necessary.
-  if not tf.gfile.Exists(cmd_args.output_dir):
-    tf.gfile.MakeDirs(cmd_args.output_dir)
+  if not tf.io.gfile.exists(cmd_args.output_dir):
+    tf.io.gfile.makedirs(cmd_args.output_dir)
 
   # Tell TensorFlow that the model will be built into the default Graph.
   with tf.Graph().as_default():
     # Reading list of images.
-    filename_queue = tf.train.string_input_producer(image_paths, shuffle=False)
-    reader = tf.WholeFileReader()
+    filename_queue = tf.compat.v1.train.string_input_producer(
+        image_paths, shuffle=False)
+    reader = tf.compat.v1.WholeFileReader()
     _, value = reader.read(filename_queue)
-    image_tf = tf.image.decode_jpeg(value, channels=3)
+    image_tf = tf.io.decode_jpeg(value, channels=3)
 
-    with tf.Session() as sess:
-      init_op = tf.global_variables_initializer()
+    with tf.compat.v1.Session() as sess:
+      init_op = tf.compat.v1.global_variables_initializer()
       sess.run(init_op)
 
       extractor_fn = extractor.MakeExtractor(sess, config)
 
       # Start input enqueue threads.
       coord = tf.train.Coordinator()
-      threads = tf.train.start_queue_runners(sess=sess, coord=coord)
+      threads = tf.compat.v1.train.start_queue_runners(sess=sess, coord=coord)
       start = time.clock()
       for i in range(num_images):
         # Write to log-info once in a while.
         if i == 0:
-          tf.logging.info('Starting to extract DELF features from images...')
+          tf.compat.v1.logging.info(
+              'Starting to extract DELF features from images...')
         elif i % _STATUS_CHECK_ITERATIONS == 0:
           elapsed = (time.clock() - start)
-          tf.logging.info(
+          tf.compat.v1.logging.info(
               'Processing image %d out of %d, last %d '
               'images took %f seconds', i, num_images, _STATUS_CHECK_ITERATIONS,
               elapsed)
@@ -114,8 +117,8 @@ def main(unused_argv):
         out_desc_filename = os.path.splitext(os.path.basename(
             image_paths[i]))[0] + _DELF_EXT
         out_desc_fullpath = os.path.join(cmd_args.output_dir, out_desc_filename)
-        if tf.gfile.Exists(out_desc_fullpath):
-          tf.logging.info('Skipping %s', image_paths[i])
+        if tf.io.gfile.exists(out_desc_fullpath):
+          tf.compat.v1.logging.info('Skipping %s', image_paths[i])
           continue
 
         # Extract and save features.
diff --git a/research/delf/delf/python/examples/extractor.py b/research/delf/delf/python/examples/extractor.py
index 782dbd93..93b01772 100644
--- a/research/delf/delf/python/examples/extractor.py
+++ b/research/delf/delf/python/examples/extractor.py
@@ -30,44 +30,70 @@ _MIN_HEIGHT = 10
 _MIN_WIDTH = 10
 
 
-def ResizeImage(image, config):
+def ResizeImage(image, config, resize_factor=1.0, square_output=False):
   """Resizes image according to config.
 
   Args:
     image: Uint8 array with shape (height, width, 3).
     config: DelfConfig proto containing the model configuration.
+    resize_factor: Optional float resize factor for the input image. If given,
+      the maximum and minimum allowed image sizes in `config` are scaled by this
+      factor. Must be non-negative.
+    square_output: If True, the output image's aspect ratio is potentially
+      distorted and a square image (ie, height=width) is returned. The image is
+      resized such that the largest image side is used in both dimensions.
 
   Returns:
     resized_image: Uint8 array with resized image.
-    scale_factor: Float with factor used for resizing (If upscaling, larger than
-      1; if downscaling, smaller than 1).
+    scale_factors: 2D float array, with factors used for resizing along height
+      and width (If upscaling, larger than 1; if downscaling, smaller than 1).
 
   Raises:
     ValueError: If `image` has incorrect number of dimensions/channels.
   """
+  if resize_factor < 0.0:
+    raise ValueError('negative resize_factor is not allowed: %f' %
+                     resize_factor)
   if image.ndim != 3:
     raise ValueError('image has incorrect number of dimensions: %d' %
                      image.ndims)
   height, width, channels = image.shape
 
+  # Take into account resize factor.
+  max_image_size = resize_factor * config.max_image_size
+  min_image_size = resize_factor * config.min_image_size
+
   if channels != 3:
     raise ValueError('image has incorrect number of channels: %d' % channels)
 
-  if config.max_image_size != -1 and (width > config.max_image_size or
-                                      height > config.max_image_size):
-    scale_factor = config.max_image_size / max(width, height)
-  elif config.min_image_size != -1 and (width < config.min_image_size and
-                                        height < config.min_image_size):
-    scale_factor = config.min_image_size / max(width, height)
+  largest_side = max(width, height)
+
+  if max_image_size >= 0 and largest_side > max_image_size:
+    scale_factor = max_image_size / largest_side
+  elif min_image_size >= 0 and largest_side < min_image_size:
+    scale_factor = min_image_size / largest_side
+  elif square_output and (height != width):
+    scale_factor = 1.0
   else:
     # No resizing needed, early return.
-    return image, 1.0
+    return image, np.ones(2, dtype=float)
+
+  # Note that new_shape is in (width, height) format (PIL convention), while
+  # scale_factors are in (height, width) convention (NumPy convention).
+  if square_output:
+    new_shape = (int(round(largest_side * scale_factor)),
+                 int(round(largest_side * scale_factor)))
+  else:
+    new_shape = (int(round(width * scale_factor)),
+                 int(round(height * scale_factor)))
+
+  scale_factors = np.array([new_shape[1] / height, new_shape[0] / width],
+                           dtype=float)
 
-  new_shape = (int(width * scale_factor), int(height * scale_factor))
   pil_image = Image.fromarray(image)
   resized_image = np.array(pil_image.resize(new_shape, resample=Image.BILINEAR))
 
-  return resized_image, scale_factor
+  return resized_image, scale_factors
 
 
 def MakeExtractor(sess, config, import_scope=None):
@@ -81,8 +107,8 @@ def MakeExtractor(sess, config, import_scope=None):
   Returns:
     Function that receives an image and returns features.
   """
-  tf.saved_model.loader.load(
-      sess, [tf.saved_model.tag_constants.SERVING],
+  tf.compat.v1.saved_model.loader.load(
+      sess, [tf.compat.v1.saved_model.tag_constants.SERVING],
       config.model_path,
       import_scope=import_scope)
   import_scope_prefix = import_scope + '/' if import_scope is not None else ''
@@ -118,7 +144,7 @@ def MakeExtractor(sess, config, import_scope=None):
     Returns:
       Tuple (locations, descriptors, feature_scales, attention)
     """
-    resized_image, scale_factor = ResizeImage(image, config)
+    resized_image, scale_factors = ResizeImage(image, config)
 
     # If the image is too small, returns empty features.
     if resized_image.shape[0] < _MIN_HEIGHT or resized_image.shape[
@@ -134,7 +160,7 @@ def MakeExtractor(sess, config, import_scope=None):
              input_image_scales: list(config.image_scales),
              input_max_feature_num: config.delf_local_config.max_feature_num
          })
-    rescaled_locations_out = locations_out / scale_factor
+    rescaled_locations_out = locations_out / scale_factors
 
     return (rescaled_locations_out, descriptors_out, feature_scales_out,
             attention_out)
diff --git a/research/delf/delf/python/examples/extractor_test.py b/research/delf/delf/python/examples/extractor_test.py
index 7f5a8a58..2ecdfe93 100644
--- a/research/delf/delf/python/examples/extractor_test.py
+++ b/research/delf/delf/python/examples/extractor_test.py
@@ -29,16 +29,36 @@ from delf import extractor
 class ExtractorTest(tf.test.TestCase, parameterized.TestCase):
 
   @parameterized.named_parameters(
-      ('Max-1Min-1', -1, -1, [4, 2, 3], 1.0),
-      ('Max2Min-1', 2, -1, [2, 1, 3], 0.5),
-      ('Max8Min-1', 8, -1, [4, 2, 3], 1.0),
-      ('Max-1Min1', -1, 1, [4, 2, 3], 1.0),
-      ('Max-1Min8', -1, 8, [8, 4, 3], 2.0),
-      ('Max16Min8', 16, 8, [8, 4, 3], 2.0),
-      ('Max2Min2', 2, 2, [2, 1, 3], 0.5),
+      ('Max-1Min-1', -1, -1, 1.0, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max-1Min-1Square', -1, -1, 1.0, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max2Min-1', 2, -1, 1.0, False, [2, 1, 3], [0.5, 0.5]),
+      ('Max2Min-1Square', 2, -1, 1.0, True, [2, 2, 3], [0.5, 1.0]),
+      ('Max8Min-1', 8, -1, 1.0, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max8Min-1Square', 8, -1, 1.0, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max-1Min1', -1, 1, 1.0, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max-1Min1Square', -1, 1, 1.0, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max-1Min8', -1, 8, 1.0, False, [8, 4, 3], [2.0, 2.0]),
+      ('Max-1Min8Square', -1, 8, 1.0, True, [8, 8, 3], [2.0, 4.0]),
+      ('Max16Min8', 16, 8, 1.0, False, [8, 4, 3], [2.0, 2.0]),
+      ('Max16Min8Square', 16, 8, 1.0, True, [8, 8, 3], [2.0, 4.0]),
+      ('Max2Min2', 2, 2, 1.0, False, [2, 1, 3], [0.5, 0.5]),
+      ('Max2Min2Square', 2, 2, 1.0, True, [2, 2, 3], [0.5, 1.0]),
+      ('Max-1Min-1Factor0.5', -1, -1, 0.5, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max-1Min-1Factor0.5Square', -1, -1, 0.5, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max2Min-1Factor2.0', 2, -1, 2.0, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max2Min-1Factor2.0Square', 2, -1, 2.0, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max-1Min8Factor0.5', -1, 8, 0.5, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max-1Min8Factor0.5Square', -1, 8, 0.5, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max-1Min8Factor0.25', -1, 8, 0.25, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max-1Min8Factor0.25Square', -1, 8, 0.25, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max2Min2Factor2.0', 2, 2, 2.0, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max2Min2Factor2.0Square', 2, 2, 2.0, True, [4, 4, 3], [1.0, 2.0]),
+      ('Max16Min8Factor0.5', 16, 8, 0.5, False, [4, 2, 3], [1.0, 1.0]),
+      ('Max16Min8Factor0.5Square', 16, 8, 0.5, True, [4, 4, 3], [1.0, 2.0]),
   )
-  def testResizeImageWorks(self, max_image_size, min_image_size, expected_shape,
-                           expected_scale_factor):
+  def testResizeImageWorks(self, max_image_size, min_image_size, resize_factor,
+                           square_output, expected_shape,
+                           expected_scale_factors):
     # Construct image of size 4x2x3.
     image = np.array([[[0, 0, 0], [1, 1, 1]], [[2, 2, 2], [3, 3, 3]],
                       [[4, 4, 4], [5, 5, 5]], [[6, 6, 6], [7, 7, 7]]],
@@ -48,9 +68,31 @@ class ExtractorTest(tf.test.TestCase, parameterized.TestCase):
     config = delf_config_pb2.DelfConfig(
         max_image_size=max_image_size, min_image_size=min_image_size)
 
-    resized_image, scale_factor = extractor.ResizeImage(image, config)
+    resized_image, scale_factors = extractor.ResizeImage(
+        image, config, resize_factor, square_output)
     self.assertAllEqual(resized_image.shape, expected_shape)
-    self.assertAllClose(scale_factor, expected_scale_factor)
+    self.assertAllClose(scale_factors, expected_scale_factors)
+
+  @parameterized.named_parameters(
+      ('Max2Min2', 2, 2, 1.0, False, [2, 1, 3], [0.666666, 0.5]),
+      ('Max2Min2Square', 2, 2, 1.0, True, [2, 2, 3], [0.666666, 1.0]),
+  )
+  def testResizeImageRoundingWorks(self, max_image_size, min_image_size,
+                                   resize_factor, square_output, expected_shape,
+                                   expected_scale_factors):
+    # Construct image of size 3x2x3.
+    image = np.array([[[0, 0, 0], [1, 1, 1]], [[2, 2, 2], [3, 3, 3]],
+                      [[4, 4, 4], [5, 5, 5]]],
+                     dtype='uint8')
+
+    # Set up config.
+    config = delf_config_pb2.DelfConfig(
+        max_image_size=max_image_size, min_image_size=min_image_size)
+
+    resized_image, scale_factors = extractor.ResizeImage(
+        image, config, resize_factor, square_output)
+    self.assertAllEqual(resized_image.shape, expected_shape)
+    self.assertAllClose(scale_factors, expected_scale_factors)
 
 
 if __name__ == '__main__':
diff --git a/research/delf/delf/python/examples/match_images.py b/research/delf/delf/python/examples/match_images.py
index a5abc58e..ec97314d 100644
--- a/research/delf/delf/python/examples/match_images.py
+++ b/research/delf/delf/python/examples/match_images.py
@@ -27,7 +27,10 @@ from __future__ import print_function
 import argparse
 import sys
 
-import matplotlib.image as mpimg
+import matplotlib
+# Needed before pyplot import for matplotlib to work properly.
+matplotlib.use('Agg')
+import matplotlib.image as mpimg  # pylint: disable=g-import-not-at-top
 import matplotlib.pyplot as plt
 import numpy as np
 from scipy import spatial
@@ -45,17 +48,17 @@ _DISTANCE_THRESHOLD = 0.8
 
 
 def main(unused_argv):
-  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
 
   # Read features.
   locations_1, _, descriptors_1, _, _ = feature_io.ReadFromFile(
       cmd_args.features_1_path)
   num_features_1 = locations_1.shape[0]
-  tf.logging.info("Loaded image 1's %d features" % num_features_1)
+  tf.compat.v1.logging.info("Loaded image 1's %d features" % num_features_1)
   locations_2, _, descriptors_2, _, _ = feature_io.ReadFromFile(
       cmd_args.features_2_path)
   num_features_2 = locations_2.shape[0]
-  tf.logging.info("Loaded image 2's %d features" % num_features_2)
+  tf.compat.v1.logging.info("Loaded image 2's %d features" % num_features_2)
 
   # Find nearest-neighbor matches using a KD tree.
   d1_tree = spatial.cKDTree(descriptors_1)
@@ -81,7 +84,7 @@ def main(unused_argv):
                               residual_threshold=20,
                               max_trials=1000)
 
-  tf.logging.info('Found %d inliers' % sum(inliers))
+  tf.compat.v1.logging.info('Found %d inliers' % sum(inliers))
 
   # Visualize correspondences, and save to file.
   _, ax = plt.subplots()
diff --git a/research/delf/delf/python/feature_aggregation_extractor.py b/research/delf/delf/python/feature_aggregation_extractor.py
index 887de4f6..36794f64 100644
--- a/research/delf/delf/python/feature_aggregation_extractor.py
+++ b/research/delf/delf/python/feature_aggregation_extractor.py
@@ -27,6 +27,7 @@ import tensorflow as tf
 
 from delf import aggregation_config_pb2
 
+_CLUSTER_CENTERS_VAR_NAME = "clusters"
 _NORM_SQUARED_TOLERANCE = 1e-12
 
 # Aliases for aggregation types.
@@ -66,10 +67,7 @@ class ExtractAggregatedRepresentation(object):
             aggregation_config.feature_dimensionality
         ])
     tf.compat.v1.train.init_from_checkpoint(
-        aggregation_config.codebook_path, {
-            tf.contrib.factorization.KMeansClustering.CLUSTER_CENTERS_VAR_NAME:
-                codebook
-        })
+        aggregation_config.codebook_path, {_CLUSTER_CENTERS_VAR_NAME: codebook})
 
     # Construct extraction graph based on desired options.
     if self._aggregation_type == _VLAD:
@@ -270,7 +268,7 @@ class ExtractAggregatedRepresentation(object):
           output_vlad: VLAD descriptor updated to take into account contribution
             from ind-th feature.
         """
-        return ind + 1, tf.compat.v1.tensor_scatter_add(
+        return ind + 1, tf.tensor_scatter_nd_add(
             vlad, tf.expand_dims(selected_visual_words[ind], axis=1),
             tf.tile(
                 tf.expand_dims(features[ind], axis=0), [num_assignments, 1]) -
diff --git a/research/delf/delf/python/feature_extractor.py b/research/delf/delf/python/feature_extractor.py
index 6eca9c6c..0e16670f 100644
--- a/research/delf/delf/python/feature_extractor.py
+++ b/research/delf/delf/python/feature_extractor.py
@@ -12,8 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""DELF feature extractor.
-"""
+"""DELF feature extractor."""
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
@@ -39,8 +38,8 @@ def NormalizePixelValues(image,
   Returns:
     image: a float32 tensor of the same shape as the input image.
   """
-  image = tf.to_float(image)
-  image = tf.div(tf.subtract(image, pixel_value_offset), pixel_value_scale)
+  image = tf.cast(image, dtype=tf.float32)
+  image = tf.truediv(tf.subtract(image, pixel_value_offset), pixel_value_scale)
   return image
 
 
@@ -53,6 +52,7 @@ def CalculateReceptiveBoxes(height, width, rf, stride, padding):
     rf: The receptive field size.
     stride: The effective stride between two adjacent feature points.
     padding: The effective padding size.
+
   Returns:
     rf_boxes: [N, 4] receptive boxes tensor. Here N equals to height x width.
     Each box is represented by [ymin, xmin, ymax, xmax].
@@ -60,7 +60,8 @@ def CalculateReceptiveBoxes(height, width, rf, stride, padding):
   x, y = tf.meshgrid(tf.range(width), tf.range(height))
   coordinates = tf.reshape(tf.stack([y, x], axis=2), [-1, 2])
   # [y,x,y,x]
-  point_boxes = tf.to_float(tf.concat([coordinates, coordinates], 1))
+  point_boxes = tf.cast(
+      tf.concat([coordinates, coordinates], 1), dtype=tf.float32)
   bias = [-padding, -padding, -padding + rf - 1, -padding + rf - 1]
   rf_boxes = stride * point_boxes + bias
   return rf_boxes
@@ -94,12 +95,10 @@ def ExtractKeypointDescriptor(image, layer_name, image_scales, iou,
     abs_thres: A float tensor denoting the score threshold for feature
       selection.
     model_fn: Model function. Follows the signature:
-
       * Args:
         * `images`: Image tensor which is re-scaled.
         * `normalized_image`: Whether or not the images are normalized.
         * `reuse`: Whether or not the layer and its variables should be reused.
-
       * Returns:
         * `attention`: Attention score after the non-linearity.
         * `feature_map`: Feature map obtained from the ResNet model.
@@ -117,7 +116,8 @@ def ExtractKeypointDescriptor(image, layer_name, image_scales, iou,
   Raises:
     ValueError: If the layer_name is unsupported.
   """
-  original_image_shape_float = tf.gather(tf.to_float(tf.shape(image)), [0, 1])
+  original_image_shape_float = tf.gather(
+      tf.cast(tf.shape(image), dtype=tf.float32), [0, 1])
   image_tensor = NormalizePixelValues(image)
   image_tensor = tf.expand_dims(image_tensor, 0, name='image/expand_dims')
 
@@ -163,8 +163,10 @@ def ExtractKeypointDescriptor(image, layer_name, image_scales, iou,
       scores: Concatenated attention score tensor with the shape of [K].
     """
     scale = tf.gather(image_scales, scale_index)
-    new_image_size = tf.to_int32(tf.round(original_image_shape_float * scale))
-    resized_image = tf.image.resize_bilinear(image_tensor, new_image_size)
+    new_image_size = tf.cast(
+        tf.round(original_image_shape_float * scale), dtype=tf.int32)
+    resized_image = tf.compat.v1.image.resize_bilinear(image_tensor,
+                                                       new_image_size)
 
     attention, feature_map = model_fn(
         resized_image, normalized_image=True, reuse=reuse)
@@ -254,7 +256,7 @@ def BuildModel(layer_name, attention_nonlinear, attention_type,
       Currently, only 'softplus' is supported.
     attention_type: Type of the attention used. Options are:
       'use_l2_normalized_feature' and 'use_default_input_feature'. Note that
-       this is irrelevant during inference time.
+      this is irrelevant during inference time.
     attention_kernel_size: Size of attention kernel (kernel is square).
 
   Returns:
@@ -268,6 +270,7 @@ def BuildModel(layer_name, attention_nonlinear, attention_type,
       images: Image tensor.
       normalized_image: Whether or not the images are normalized.
       reuse: Whether or not the layer and its variables should be reused.
+
     Returns:
       attention: Attention score after the non-linearity.
       feature_map: Feature map after ResNet convolution.
@@ -328,57 +331,72 @@ def ApplyPcaAndWhitening(data,
   return output
 
 
-def DelfFeaturePostProcessing(boxes, descriptors, config):
-  """Extract DELF features from input image.
+def PostProcessDescriptors(descriptors, use_pca, pca_parameters):
+  """Post-process descriptors.
 
   Args:
-    boxes: [N, 4] float tensor which denotes the selected receptive box. N is
-      the number of final feature points which pass through keypoint selection
-      and NMS steps.
     descriptors: [N, input_dim] float tensor.
-    config: DelfConfig proto with DELF extraction options.
+    use_pca: Whether to use PCA.
+    pca_parameters: DelfPcaParameters proto.
 
   Returns:
-    locations: [N, 2] float tensor which denotes the selected keypoint
-      locations.
-    final_descriptors: [N, output_dim] float tensor with DELF descriptors after
+    final_descriptors: [N, output_dim] float tensor with descriptors after
       normalization and (possibly) PCA/whitening.
   """
-
-  # Get center of descriptor boxes, corresponding to feature locations.
-  locations = CalculateKeypointCenters(boxes)
-
-  # Post-process descriptors: L2-normalize, and if desired apply PCA (followed
-  # by L2-normalization).
-  with tf.variable_scope('postprocess'):
+  # L2-normalize, and if desired apply PCA (followed by L2-normalization).
+  with tf.compat.v1.variable_scope('postprocess'):
     final_descriptors = tf.nn.l2_normalize(
-        descriptors, dim=1, name='l2_normalization')
+        descriptors, axis=1, name='l2_normalization')
 
-    if config.delf_local_config.use_pca:
+    if use_pca:
       # Load PCA parameters.
       pca_mean = tf.constant(
-          datum_io.ReadFromFile(
-              config.delf_local_config.pca_parameters.mean_path),
-          dtype=tf.float32)
+          datum_io.ReadFromFile(pca_parameters.mean_path), dtype=tf.float32)
       pca_matrix = tf.constant(
-          datum_io.ReadFromFile(
-              config.delf_local_config.pca_parameters.projection_matrix_path),
+          datum_io.ReadFromFile(pca_parameters.projection_matrix_path),
           dtype=tf.float32)
-      pca_dim = config.delf_local_config.pca_parameters.pca_dim
+      pca_dim = pca_parameters.pca_dim
       pca_variances = None
-      if config.delf_local_config.pca_parameters.use_whitening:
-        pca_variances = tf.constant(
-            datum_io.ReadFromFile(
-                config.delf_local_config.pca_parameters.pca_variances_path),
-            dtype=tf.float32)
+      if pca_parameters.use_whitening:
+        pca_variances = tf.squeeze(
+            tf.constant(
+                datum_io.ReadFromFile(pca_parameters.pca_variances_path),
+                dtype=tf.float32))
 
       # Apply PCA, and whitening if desired.
-      final_descriptors = ApplyPcaAndWhitening(
-          final_descriptors, pca_matrix, pca_mean, pca_dim,
-          config.delf_local_config.pca_parameters.use_whitening, pca_variances)
+      final_descriptors = ApplyPcaAndWhitening(final_descriptors, pca_matrix,
+                                               pca_mean, pca_dim,
+                                               pca_parameters.use_whitening,
+                                               pca_variances)
 
       # Re-normalize.
       final_descriptors = tf.nn.l2_normalize(
-          final_descriptors, dim=1, name='pca_l2_normalization')
+          final_descriptors, axis=1, name='pca_l2_normalization')
+
+  return final_descriptors
+
+
+def DelfFeaturePostProcessing(boxes, descriptors, config):
+  """Extract DELF features from input image.
+
+  Args:
+    boxes: [N, 4] float tensor which denotes the selected receptive box. N is
+      the number of final feature points which pass through keypoint selection
+      and NMS steps.
+    descriptors: [N, input_dim] float tensor.
+    config: DelfConfig proto with DELF extraction options.
+
+  Returns:
+    locations: [N, 2] float tensor which denotes the selected keypoint
+      locations.
+    final_descriptors: [N, output_dim] float tensor with DELF descriptors after
+      normalization and (possibly) PCA/whitening.
+  """
+
+  # Get center of descriptor boxes, corresponding to feature locations.
+  locations = CalculateKeypointCenters(boxes)
+  final_descriptors = PostProcessDescriptors(
+      descriptors, config.delf_local_config.use_pca,
+      config.delf_local_config.pca_parameters)
 
   return locations, final_descriptors
diff --git a/research/delf/delf/python/feature_extractor_test.py b/research/delf/delf/python/feature_extractor_test.py
index 882dc9e1..675ecb8f 100644
--- a/research/delf/delf/python/feature_extractor_test.py
+++ b/research/delf/delf/python/feature_extractor_test.py
@@ -34,7 +34,7 @@ class FeatureExtractorTest(tf.test.TestCase):
         image, pixel_value_offset=5.0, pixel_value_scale=2.0)
     exp_normalized_image = [[[-1.0, 125.0, -2.5], [14.5, 3.5, 0.0]],
                             [[20.0, 0.0, 30.0], [25.5, 36.0, 42.0]]]
-    with self.test_session() as sess:
+    with self.session() as sess:
       normalized_image_out = sess.run(normalized_image)
 
     self.assertAllEqual(normalized_image_out, exp_normalized_image)
@@ -43,7 +43,7 @@ class FeatureExtractorTest(tf.test.TestCase):
     boxes = feature_extractor.CalculateReceptiveBoxes(
         height=1, width=2, rf=291, stride=32, padding=145)
     exp_boxes = [[-145., -145., 145., 145.], [-145., -113., 145., 177.]]
-    with self.test_session() as sess:
+    with self.session() as sess:
       boxes_out = sess.run(boxes)
 
     self.assertAllEqual(exp_boxes, boxes_out)
@@ -52,7 +52,7 @@ class FeatureExtractorTest(tf.test.TestCase):
     boxes = [[-10.0, 0.0, 11.0, 21.0], [-2.5, 5.0, 18.5, 26.0],
              [45.0, -2.5, 66.0, 18.5]]
     centers = feature_extractor.CalculateKeypointCenters(boxes)
-    with self.test_session() as sess:
+    with self.session() as sess:
       centers_out = sess.run(centers)
 
     exp_centers = [[0.5, 10.5], [8.0, 15.5], [55.5, 8.0]]
@@ -72,12 +72,11 @@ class FeatureExtractorTest(tf.test.TestCase):
       del normalized_image, reuse  # Unused variables in the test.
       image_shape = tf.shape(image)
       attention = tf.squeeze(tf.norm(image, axis=3))
-      feature_map = tf.concat(
-          [
-              tf.tile(image, [1, 1, 1, 341]),
-              tf.zeros([1, image_shape[1], image_shape[2], 1])
-          ],
-          axis=3)
+      feature_map = tf.concat([
+          tf.tile(image, [1, 1, 1, 341]),
+          tf.zeros([1, image_shape[1], image_shape[2], 1])
+      ],
+                              axis=3)
       return attention, feature_map
 
     boxes, feature_scales, features, scores = (
@@ -99,7 +98,7 @@ class FeatureExtractorTest(tf.test.TestCase):
             axis=1))
     exp_scores = [[1.723042], [1.600781]]
 
-    with self.test_session() as sess:
+    with self.session() as sess:
       boxes_out, feature_scales_out, features_out, scores_out = sess.run(
           [boxes, feature_scales, features, scores])
 
@@ -118,16 +117,18 @@ class FeatureExtractorTest(tf.test.TestCase):
     use_whitening = True
     pca_variances = tf.constant([4.0, 1.0])
 
-    output = feature_extractor.ApplyPcaAndWhitening(
-        data, pca_matrix, pca_mean, output_dim, use_whitening, pca_variances)
+    output = feature_extractor.ApplyPcaAndWhitening(data, pca_matrix, pca_mean,
+                                                    output_dim, use_whitening,
+                                                    pca_variances)
 
     exp_output = [[2.5, -5.0], [-6.0, -2.0], [-0.5, -3.0], [1.0, -2.0]]
 
-    with self.test_session() as sess:
+    with self.session() as sess:
       output_out = sess.run(output)
 
     self.assertAllEqual(exp_output, output_out)
 
 
 if __name__ == '__main__':
+  tf.compat.v1.disable_eager_execution()
   tf.test.main()
diff --git a/research/delf/delf/python/feature_io.py b/research/delf/delf/python/feature_io.py
index fdc0e7b4..9b68586b 100644
--- a/research/delf/delf/python/feature_io.py
+++ b/research/delf/delf/python/feature_io.py
@@ -168,7 +168,7 @@ def ReadFromFile(file_path):
     attention: [N] float array with attention scores.
     orientations: [N] float array with orientations.
   """
-  with tf.gfile.FastGFile(file_path, 'rb') as f:
+  with tf.io.gfile.GFile(file_path, 'rb') as f:
     return ParseFromString(f.read())
 
 
@@ -192,5 +192,5 @@ def WriteToFile(file_path,
   """
   serialized_data = SerializeToString(locations, scales, descriptors, attention,
                                       orientations)
-  with tf.gfile.FastGFile(file_path, 'w') as f:
+  with tf.io.gfile.GFile(file_path, 'w') as f:
     f.write(serialized_data)
diff --git a/research/delf/delf/python/feature_io_test.py b/research/delf/delf/python/feature_io_test.py
index 8dbe77bb..dc8ad75d 100644
--- a/research/delf/delf/python/feature_io_test.py
+++ b/research/delf/delf/python/feature_io_test.py
@@ -81,7 +81,7 @@ class DelfFeaturesIoTest(tf.test.TestCase):
   def testWriteAndReadToFile(self):
     locations, scales, descriptors, attention, orientations = create_data()
 
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.delf')
     feature_io.WriteToFile(filename, locations, scales, descriptors, attention,
                            orientations)
@@ -94,7 +94,7 @@ class DelfFeaturesIoTest(tf.test.TestCase):
     self.assertAllEqual(orientations, data_read[4])
 
   def testWriteAndReadToFileEmptyFile(self):
-    tmpdir = tf.test.get_temp_dir()
+    tmpdir = tf.compat.v1.test.get_temp_dir()
     filename = os.path.join(tmpdir, 'test.delf')
     feature_io.WriteToFile(filename, np.array([]), np.array([]), np.array([]),
                            np.array([]), np.array([]))
diff --git a/research/delf/delf/python/google_landmarks_dataset/dataset_file_io.py b/research/delf/delf/python/google_landmarks_dataset/dataset_file_io.py
index 7f544bf2..93f2785d 100644
--- a/research/delf/delf/python/google_landmarks_dataset/dataset_file_io.py
+++ b/research/delf/delf/python/google_landmarks_dataset/dataset_file_io.py
@@ -49,7 +49,7 @@ def ReadSolution(file_path, task):
   public_solution = {}
   private_solution = {}
   ignored_ids = []
-  with tf.gfile.GFile(file_path, 'r') as csv_file:
+  with tf.io.gfile.GFile(file_path, 'r') as csv_file:
     reader = csv.reader(csv_file)
     next(reader, None)  # Skip header.
     for row in reader:
@@ -108,7 +108,7 @@ def ReadPredictions(file_path, public_ids, private_ids, ignored_ids, task):
   """
   public_predictions = {}
   private_predictions = {}
-  with tf.gfile.GFile(file_path, 'r') as csv_file:
+  with tf.io.gfile.GFile(file_path, 'r') as csv_file:
     reader = csv.reader(csv_file)
     next(reader, None)  # Skip header.
     for row in reader:
diff --git a/research/delf/delf/python/google_landmarks_dataset/dataset_file_io_test.py b/research/delf/delf/python/google_landmarks_dataset/dataset_file_io_test.py
index c4934959..bf5166e4 100644
--- a/research/delf/delf/python/google_landmarks_dataset/dataset_file_io_test.py
+++ b/research/delf/delf/python/google_landmarks_dataset/dataset_file_io_test.py
@@ -29,8 +29,9 @@ class DatasetFileIoTest(tf.test.TestCase):
 
   def testReadRecognitionSolutionWorks(self):
     # Define inputs.
-    file_path = os.path.join(tf.test.get_temp_dir(), 'recognition_solution.csv')
-    with tf.gfile.GFile(file_path, 'w') as f:
+    file_path = os.path.join(tf.compat.v1.test.get_temp_dir(),
+                             'recognition_solution.csv')
+    with tf.io.gfile.GFile(file_path, 'w') as f:
       f.write('id,landmarks,Usage\n')
       f.write('0123456789abcdef,0 12,Public\n')
       f.write('0223456789abcdef,,Public\n')
@@ -60,8 +61,9 @@ class DatasetFileIoTest(tf.test.TestCase):
 
   def testReadRetrievalSolutionWorks(self):
     # Define inputs.
-    file_path = os.path.join(tf.test.get_temp_dir(), 'retrieval_solution.csv')
-    with tf.gfile.GFile(file_path, 'w') as f:
+    file_path = os.path.join(tf.compat.v1.test.get_temp_dir(),
+                             'retrieval_solution.csv')
+    with tf.io.gfile.GFile(file_path, 'w') as f:
       f.write('id,images,Usage\n')
       f.write('0123456789abcdef,None,Ignored\n')
       f.write('0223456789abcdef,fedcba9876543210 fedcba9876543200,Public\n')
@@ -91,9 +93,9 @@ class DatasetFileIoTest(tf.test.TestCase):
 
   def testReadRecognitionPredictionsWorks(self):
     # Define inputs.
-    file_path = os.path.join(tf.test.get_temp_dir(),
+    file_path = os.path.join(tf.compat.v1.test.get_temp_dir(),
                              'recognition_predictions.csv')
-    with tf.gfile.GFile(file_path, 'w') as f:
+    with tf.io.gfile.GFile(file_path, 'w') as f:
       f.write('id,landmarks\n')
       f.write('0123456789abcdef,12 0.1 \n')
       f.write('0423456789abcdef,0 19.0\n')
@@ -129,9 +131,9 @@ class DatasetFileIoTest(tf.test.TestCase):
 
   def testReadRetrievalPredictionsWorks(self):
     # Define inputs.
-    file_path = os.path.join(tf.test.get_temp_dir(),
+    file_path = os.path.join(tf.compat.v1.test.get_temp_dir(),
                              'retrieval_predictions.csv')
-    with tf.gfile.GFile(file_path, 'w') as f:
+    with tf.io.gfile.GFile(file_path, 'w') as f:
       f.write('id,images\n')
       f.write('0123456789abcdef,fedcba9876543250 \n')
       f.write('0423456789abcdef,fedcba9876543260\n')
diff --git a/research/delf/delf/python/training/README.md b/research/delf/delf/python/training/README.md
new file mode 100644
index 00000000..942010e4
--- /dev/null
+++ b/research/delf/delf/python/training/README.md
@@ -0,0 +1,23 @@
+# DELF training instructions
+
+## Data preparation
+
+See the
+[build_image_dataset.py](https://github.com/andrefaraujo/models/blob/master/research/delf/delf/python/training/build_image_dataset.py)
+script to prepare the data, following the instructions therein to download the
+dataset (via Kaggle) and then running the script.
+
+## Running training
+
+Assuming the data was downloaded to `/tmp/gld_tfrecord/`, running the following
+command should start training a model:
+
+```sh
+python tensorflow_models/research/delf/delf/python/training/train.py \
+  --train_file_pattern=/tmp/gld_tfrecord/train* \
+  --validation_file_pattern=/tmp/gld_tfrecord/train* \
+  --debug
+```
+
+Note that one may want to split the train TFRecords into a train/val (for
+training, we usually simply split it 80/20 randomly).
diff --git a/research/delf/delf/python/training/__init__.py b/research/delf/delf/python/training/__init__.py
new file mode 100644
index 00000000..c87f3d89
--- /dev/null
+++ b/research/delf/delf/python/training/__init__.py
@@ -0,0 +1,22 @@
+# Copyright 2020 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Module for DELF training."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# pylint: disable=unused-import
+from delf.python.training import build_image_dataset
+# pylint: enable=unused-import
diff --git a/research/delf/delf/python/training/build_image_dataset.py b/research/delf/delf/python/training/build_image_dataset.py
new file mode 100644
index 00000000..94fb30cc
--- /dev/null
+++ b/research/delf/delf/python/training/build_image_dataset.py
@@ -0,0 +1,239 @@
+#!/usr/bin/python
+# Copyright 2020 Google Inc. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Converts landmark image data to TFRecords file format with Example protos.
+
+The image data set is expected to reside in JPEG files ends up with '.jpg'.
+
+This script assumes you have downloaded using the provided script:
+https://www.kaggle.com/tobwey/landmark-recognition-challenge-image-downloader
+
+This script converts the training and testing data into
+a sharded data set consisting of TFRecord files
+  train_directory/train-00000-of-00128
+  train_directory/train-00001-of-00128
+  ...
+  train_directory/train-00127-of-00128
+and
+  test_directory/test-00000-of-00128
+  test_directory/test-00001-of-00128
+  ...
+  test_directory/test-00127-of-00128
+where we have selected 128 shards for both data sets. Each record
+within the TFRecord file is a serialized Example proto. The Example proto
+contains the following fields:
+  image/encoded: string containing JPEG encoded image in RGB colorspace
+  image/height: integer, image height in pixels
+  image/width: integer, image width in pixels
+  image/colorspace: string, specifying the colorspace, always 'RGB'
+  image/channels: integer, specifying the number of channels, always 3
+  image/format: string, specifying the format, always 'JPEG'
+  image/filename: string, the unique id of the image file
+            e.g. '97c0a12e07ae8dd5' or '650c989dd3493748'
+Furthermore, if the data set type is training, it would contain one more field:
+  image/class/label: integer, the landmark_id from the input training csv file.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+from absl import app
+from absl import flags
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('train_directory', '/tmp/', 'Training data directory.')
+flags.DEFINE_string('test_directory', '/tmp/', 'Testing data directory.')
+flags.DEFINE_string('output_directory', '/tmp/', 'Output data directory.')
+flags.DEFINE_string('train_csv_path', '/tmp/train.csv',
+                    'Training data csv file path.')
+flags.DEFINE_string('test_csv_path', '/tmp/test.csv',
+                    'Testing data csv file path.')
+flags.DEFINE_integer('num_shards', 128, 'Number of shards in output data.')
+
+
+def _get_image_files_and_labels(name, csv_path, image_dir):
+  """Process input and get the image file paths, image ids and the labels.
+
+  Args:
+    name: 'train' or 'test'.
+    csv_path: path to the Google-landmark Dataset csv Data Sources files.
+    image_dir: directory that stores downloaded images.
+
+  Returns:
+    image_paths: the paths to all images in the image_dir.
+    file_ids: the unique ids of images.
+    labels: the landmark id of all images. When name='test', the returned labels
+      will be an empty list.
+  Raises:
+    ValueError: if input name is not supported.
+  """
+
+  image_paths = tf.io.gfile.glob(image_dir + '/*.jpg')
+  file_ids = [os.path.basename(os.path.normpath(f))[:-4] for f in image_paths]
+  if name == 'train':
+    with tf.io.gfile.GFile(csv_path, 'rb') as csv_file:
+      df = pd.read_csv(csv_file)
+    df = df.set_index('id')
+    labels = [int(df.loc[fid]['landmark_id']) for fid in file_ids]
+  elif name == 'test':
+    labels = []
+  else:
+    raise ValueError('Unsupported dataset split name: %s' % name)
+  return image_paths, file_ids, labels
+
+
+def _process_image(filename):
+  """Process a single image file.
+
+  Args:
+    filename: string, path to an image file e.g., '/path/to/example.jpg'.
+
+  Returns:
+    image_buffer: string, JPEG encoding of RGB image.
+    height: integer, image height in pixels.
+    width: integer, image width in pixels.
+  Raises:
+    ValueError: if parsed image has wrong number of dimensions or channels.
+  """
+  # Read the image file.
+  with tf.io.gfile.GFile(filename, 'rb') as f:
+    image_data = f.read()
+
+  # Decode the RGB JPEG.
+  image = tf.io.decode_jpeg(image_data, channels=3)
+
+  # Check that image converted to RGB
+  if len(image.shape) != 3:
+    raise ValueError('The parsed image number of dimensions is not 3 but %d' %
+                     (image.shape))
+  height = image.shape[0]
+  width = image.shape[1]
+  if image.shape[2] != 3:
+    raise ValueError('The parsed image channels is not 3 but %d' %
+                     (image.shape[2]))
+
+  return image_data, height, width
+
+
+def _int64_feature(value):
+  """Returns an int64_list from a bool / enum / int / uint."""
+  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))
+
+
+def _bytes_feature(value):
+  """Returns a bytes_list from a string / byte."""
+  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
+
+
+def _convert_to_example(file_id, image_buffer, height, width, label=None):
+  """Build an Example proto for the given inputs.
+
+  Args:
+    file_id: string, unique id of an image file, e.g., '97c0a12e07ae8dd5'.
+    image_buffer: string, JPEG encoding of RGB image.
+    height: integer, image height in pixels.
+    width: integer, image width in pixels.
+    label: integer, the landmark id and prediction label.
+
+  Returns:
+    Example proto.
+  """
+  colorspace = 'RGB'
+  channels = 3
+  image_format = 'JPEG'
+  features = {
+      'image/height': _int64_feature(height),
+      'image/width': _int64_feature(width),
+      'image/colorspace': _bytes_feature(colorspace.encode('utf-8')),
+      'image/channels': _int64_feature(channels),
+      'image/format': _bytes_feature(image_format.encode('utf-8')),
+      'image/id': _bytes_feature(file_id.encode('utf-8')),
+      'image/encoded': _bytes_feature(image_buffer)
+  }
+  if label is not None:
+    features['image/class/label'] = _int64_feature(label)
+  example = tf.train.Example(features=tf.train.Features(feature=features))
+
+  return example
+
+
+def _write_tfrecord(output_prefix, image_paths, file_ids, labels):
+  """Read image files and write image and label data into TFRecord files.
+
+  Args:
+    output_prefix: string, the prefix of output files, e.g. 'train'.
+    image_paths: list of strings, the paths to images to be converted.
+    file_ids: list of strings, the image unique ids.
+    labels: list of integers, the landmark ids of images. It is an empty list
+      when output_prefix='test'.
+
+  Raises:
+    ValueError: if the length of input images, ids and labels don't match
+  """
+  if output_prefix == 'test':
+    labels = [None] * len(image_paths)
+  if not len(image_paths) == len(file_ids) == len(labels):
+    raise ValueError('length of image_paths, file_ids, labels shoud be the' +
+                     ' same. But they are %d, %d, %d, respectively' %
+                     (len(image_paths), len(file_ids), len(labels)))
+
+  spacing = np.linspace(0, len(image_paths), FLAGS.num_shards + 1, dtype=np.int)
+
+  for shard in range(FLAGS.num_shards):
+    output_file = os.path.join(
+        FLAGS.output_directory,
+        '%s-%.5d-of-%.5d' % (output_prefix, shard, FLAGS.num_shards))
+    writer = tf.io.TFRecordWriter(output_file)
+    print('Processing shard ', shard, ' and writing file ', output_file)
+    for i in range(spacing[shard], spacing[shard + 1]):
+      image_buffer, height, width = _process_image(image_paths[i])
+      example = _convert_to_example(file_ids[i], image_buffer, height, width,
+                                    labels[i])
+      writer.write(example.SerializeToString())
+    writer.close()
+
+
+def _build_tfrecord_dataset(name, csv_path, image_dir):
+  """Build a TFRecord dataset.
+
+  Args:
+    name: 'train' or 'test' to indicate which set of data to be processed.
+    csv_path: path to the Google-landmark Dataset csv Data Sources files.
+    image_dir: directory that stores downloaded images.
+
+  Returns:
+    Nothing. After the function call, sharded TFRecord files are materialized.
+  """
+
+  image_paths, file_ids, labels = _get_image_files_and_labels(
+      name, csv_path, image_dir)
+  _write_tfrecord(name, image_paths, file_ids, labels)
+
+
+def main(unused_argv):
+  _build_tfrecord_dataset('train', FLAGS.train_csv_path, FLAGS.train_directory)
+  _build_tfrecord_dataset('test', FLAGS.test_csv_path, FLAGS.test_directory)
+
+
+if __name__ == '__main__':
+  app.run(main)
diff --git a/research/delf/delf/python/training/datasets/__init__.py b/research/delf/delf/python/training/datasets/__init__.py
new file mode 100644
index 00000000..7e0a6727
--- /dev/null
+++ b/research/delf/delf/python/training/datasets/__init__.py
@@ -0,0 +1,22 @@
+# Copyright 2020 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Module exposing datasets for training."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# pylint: disable=unused-import
+from delf.python.training.datasets import googlelandmarks
+# pylint: enable=unused-import
diff --git a/research/delf/delf/python/training/datasets/googlelandmarks.py b/research/delf/delf/python/training/datasets/googlelandmarks.py
new file mode 100644
index 00000000..dd0942fc
--- /dev/null
+++ b/research/delf/delf/python/training/datasets/googlelandmarks.py
@@ -0,0 +1,169 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Google Landmarks Dataset(GLD).
+
+Placeholder for Google Landmarks dataset.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+import tensorflow as tf
+
+
+class _DataAugmentationParams(object):
+  """Default parameters for augmentation."""
+  # The following are used for training.
+  min_object_covered = 0.1
+  aspect_ratio_range_min = 3. / 4
+  aspect_ratio_range_max = 4. / 3
+  area_range_min = 0.08
+  area_range_max = 1.0
+  max_attempts = 100
+  update_labels = False
+  # 'central_fraction' is used for central crop in inference.
+  central_fraction = 0.875
+
+  random_reflection = False
+  input_rows = 321
+  input_cols = 321
+
+
+def NormalizeImages(images, pixel_value_scale=0.5, pixel_value_offset=0.5):
+  """Normalize pixel values in image.
+
+  Output is computed as
+  normalized_images = (images - pixel_value_offset) / pixel_value_scale.
+
+  Args:
+    images: `Tensor`, images to normalize.
+    pixel_value_scale: float, scale.
+    pixel_value_offset: float, offset.
+
+  Returns:
+    normalized_images: `Tensor`, normalized images.
+  """
+  images = tf.cast(images, tf.float32)
+  normalized_images = tf.math.divide(
+      tf.subtract(images, pixel_value_offset), pixel_value_scale)
+  return normalized_images
+
+
+def _ImageNetCrop(image):
+  """Imagenet-style crop with random bbox and aspect ratio.
+
+  Args:
+    image: a `Tensor`, image to crop.
+
+  Returns:
+    cropped_image: `Tensor`, cropped image.
+  """
+
+  params = _DataAugmentationParams()
+  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
+  (bbox_begin, bbox_size, _) = tf.image.sample_distorted_bounding_box(
+      tf.shape(image),
+      bounding_boxes=bbox,
+      min_object_covered=params.min_object_covered,
+      aspect_ratio_range=(params.aspect_ratio_range_min,
+                          params.aspect_ratio_range_max),
+      area_range=(params.area_range_min, params.area_range_max),
+      max_attempts=params.max_attempts,
+      use_image_if_no_bounding_boxes=True)
+  cropped_image = tf.slice(image, bbox_begin, bbox_size)
+  cropped_image.set_shape([None, None, 3])
+
+  cropped_image = tf.image.resize(
+      cropped_image, [params.input_rows, params.input_cols], method='area')
+  if params.random_reflection:
+    cropped_image = tf.image.random_flip_left_right(cropped_image)
+
+  return cropped_image
+
+
+def _ParseFunction(example, name_to_features, image_size, augmentation):
+  """Parse a single TFExample to get the image and label and process the image.
+
+  Args:
+    example: a `TFExample`.
+    name_to_features: a `dict`. The mapping from feature names to its type.
+    image_size: an `int`. The image size for the decoded image, on each side.
+    augmentation: a `boolean`. True if the image will be augmented.
+
+  Returns:
+    image: a `Tensor`. The processed image.
+    label: a `Tensor`. The ground-truth label.
+  """
+  parsed_example = tf.io.parse_single_example(example, name_to_features)
+  # Parse to get image.
+  image = parsed_example['image/encoded']
+  image = tf.io.decode_jpeg(image)
+  if augmentation:
+    image = _ImageNetCrop(image)
+  else:
+    image = tf.image.resize(image, [image_size, image_size])
+    image.set_shape([image_size, image_size, 3])
+  # Parse to get label.
+  label = parsed_example['image/class/label']
+  return image, label
+
+
+def CreateDataset(file_pattern,
+                  image_size=321,
+                  batch_size=32,
+                  augmentation=False,
+                  seed=0):
+  """Creates a dataset.
+
+  Args:
+    file_pattern: str, file pattern of the dataset files.
+    image_size: int, image size.
+    batch_size: int, batch size.
+    augmentation: bool, whether to apply augmentation.
+    seed: int, seed for shuffling the dataset.
+
+  Returns:
+     tf.data.TFRecordDataset.
+  """
+
+  filenames = tf.io.gfile.glob(file_pattern)
+
+  dataset = tf.data.TFRecordDataset(filenames)
+  dataset = dataset.repeat().shuffle(buffer_size=100, seed=seed)
+
+  # Create a description of the features.
+  feature_description = {
+      'image/height': tf.io.FixedLenFeature([], tf.int64, default_value=0),
+      'image/width': tf.io.FixedLenFeature([], tf.int64, default_value=0),
+      'image/channels': tf.io.FixedLenFeature([], tf.int64, default_value=0),
+      'image/format': tf.io.FixedLenFeature([], tf.string, default_value=''),
+      'image/filename': tf.io.FixedLenFeature([], tf.string, default_value=''),
+      'image/encoded': tf.io.FixedLenFeature([], tf.string, default_value=''),
+      'image/class/label': tf.io.FixedLenFeature([], tf.int64, default_value=0),
+  }
+
+  customized_parse_func = functools.partial(
+      _ParseFunction,
+      name_to_features=feature_description,
+      image_size=image_size,
+      augmentation=augmentation)
+  dataset = dataset.map(customized_parse_func)
+  dataset = dataset.batch(batch_size)
+
+  return dataset
diff --git a/research/delf/delf/python/training/model/__init__.py b/research/delf/delf/python/training/model/__init__.py
new file mode 100644
index 00000000..dcc888bd
--- /dev/null
+++ b/research/delf/delf/python/training/model/__init__.py
@@ -0,0 +1,24 @@
+# Copyright 2020 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""DELF model module, used for training and exporting."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# pylint: disable=unused-import
+from delf.python.training.model import delf_model
+from delf.python.training.model import export_model_utils
+from delf.python.training.model import resnet50
+# pylint: enable=unused-import
diff --git a/research/delf/delf/python/training/model/delf_model.py b/research/delf/delf/python/training/model/delf_model.py
new file mode 100644
index 00000000..27409de9
--- /dev/null
+++ b/research/delf/delf/python/training/model/delf_model.py
@@ -0,0 +1,141 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""DELF model implementation based on the following paper.
+
+  Large-Scale Image Retrieval with Attentive Deep Local Features
+  https://arxiv.org/abs/1612.06321
+"""
+
+import tensorflow as tf
+
+from delf.python.training.model import resnet50 as resnet
+
+layers = tf.keras.layers
+reg = tf.keras.regularizers
+
+_DECAY = 0.0001
+
+
+class AttentionModel(tf.keras.Model):
+  """Instantiates attention model.
+
+  Uses two [kernel_size x kernel_size] convolutions and softplus as activation
+  to compute an attention map with the same resolution as the featuremap.
+  Features l2-normalized and aggregated using attention probabilites as weights.
+  """
+
+  def __init__(self, kernel_size=1, decay=_DECAY, name='attention'):
+    """Initialization of attention model.
+
+    Args:
+      kernel_size: int, kernel size of convolutions.
+      decay: float, decay for l2 regularization of kernel weights.
+      name: str, name to identify model.
+    """
+    super(AttentionModel, self).__init__(name=name)
+
+    # First convolutional layer (called with relu activation).
+    self.conv1 = layers.Conv2D(
+        512,
+        kernel_size,
+        kernel_regularizer=reg.l2(decay),
+        padding='same',
+        name='attn_conv1')
+    self.bn_conv1 = layers.BatchNormalization(axis=3, name='bn_conv1')
+
+    # Second convolutional layer, with softplus activation.
+    self.conv2 = layers.Conv2D(
+        1,
+        kernel_size,
+        kernel_regularizer=reg.l2(decay),
+        padding='same',
+        name='attn_conv2')
+    self.activation_layer = layers.Activation('softplus')
+
+  def call(self, inputs, training=True):
+    x = self.conv1(inputs)
+    x = self.bn_conv1(x, training=training)
+    x = tf.nn.relu(x)
+
+    score = self.conv2(x)
+    prob = self.activation_layer(score)
+
+    # L2-normalize the featuremap before pooling.
+    inputs = tf.nn.l2_normalize(inputs, axis=-1)
+    feat = tf.reduce_mean(tf.multiply(inputs, prob), [1, 2], keepdims=False)
+
+    return feat, prob, score
+
+
+class Delf(tf.keras.Model):
+  """Instantiates Keras DELF model using ResNet50 as backbone.
+
+  This class implements the [DELF](https://arxiv.org/abs/1612.06321) model for
+  extracting local features from images. The backbone is a ResNet50 network
+  that extracts featuremaps from both conv_4 and conv_5 layers. Activations
+  from conv_4 are used to compute an attention map of the same resolution.
+  """
+
+  def __init__(self, block3_strides=True, name='DELF'):
+    """Initialization of DELF model.
+
+    Args:
+      block3_strides: bool, whether to add strides to the output of block3.
+      name: str, name to identify model.
+    """
+    super(Delf, self).__init__(name=name)
+
+    # Backbone using Keras ResNet50.
+    self.backbone = resnet.ResNet50(
+        'channels_last',
+        name='backbone',
+        include_top=False,
+        pooling='avg',
+        block3_strides=block3_strides,
+        average_pooling=False)
+
+    # Attention model.
+    self.attention = AttentionModel(name='attention')
+
+  # Define classifiers for training backbone and attention models.
+  def init_classifiers(self, num_classes):
+    self.num_classes = num_classes
+    self.desc_classification = layers.Dense(
+        num_classes, activation=None, kernel_regularizer=None, name='desc_fc')
+
+    self.attn_classification = layers.Dense(
+        num_classes, activation=None, kernel_regularizer=None, name='att_fc')
+
+  # Weights to optimize for descriptor fine tuning.
+  @property
+  def desc_trainable_weights(self):
+    return (self.backbone.trainable_weights +
+            self.desc_classification.trainable_weights)
+
+  # Weights to optimize for attention model training.
+  @property
+  def attn_trainable_weights(self):
+    return (self.attention.trainable_weights +
+            self.attn_classification.trainable_weights)
+
+  def call(self, input_image, training=True):
+    blocks = {'block3': None}
+    self.backbone(input_image, intermediates_dict=blocks, training=training)
+
+    features = blocks['block3']
+    _, probs, _ = self.attention(features, training=training)
+
+    return probs, features
diff --git a/research/delf/delf/python/training/model/delf_model_test.py b/research/delf/delf/python/training/model/delf_model_test.py
new file mode 100644
index 00000000..c4cbcef5
--- /dev/null
+++ b/research/delf/delf/python/training/model/delf_model_test.py
@@ -0,0 +1,115 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for the DELF model."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from absl.testing import parameterized
+import tensorflow as tf
+
+from delf.python.training.model import delf_model
+
+
+class DelfTest(tf.test.TestCase, parameterized.TestCase):
+
+  @parameterized.named_parameters(
+      ('block3_stridesTrue', True),
+      ('block3_stridesFalse', False),
+  )
+  def test_build_model(self, block3_strides):
+    image_size = 321
+    num_classes = 1000
+    batch_size = 2
+    input_shape = (batch_size, image_size, image_size, 3)
+
+    model = delf_model.Delf(block3_strides=block3_strides, name='DELF')
+    model.init_classifiers(num_classes)
+
+    images = tf.random.uniform(input_shape, minval=-1.0, maxval=1.0, seed=0)
+    blocks = {}
+
+    # Get global feature by pooling block4 features.
+    desc_prelogits = model.backbone(
+        images, intermediates_dict=blocks, training=False)
+    desc_logits = model.desc_classification(desc_prelogits)
+    self.assertAllEqual(desc_prelogits.shape, (batch_size, 2048))
+    self.assertAllEqual(desc_logits.shape, (batch_size, num_classes))
+
+    features = blocks['block3']
+    attn_prelogits, _, _ = model.attention(features)
+    attn_logits = model.attn_classification(attn_prelogits)
+    self.assertAllEqual(attn_prelogits.shape, (batch_size, 1024))
+    self.assertAllEqual(attn_logits.shape, (batch_size, num_classes))
+
+  @parameterized.named_parameters(
+      ('block3_stridesTrue', True),
+      ('block3_stridesFalse', False),
+  )
+  def test_train_step(self, block3_strides):
+
+    image_size = 321
+    num_classes = 1000
+    batch_size = 2
+    clip_val = 10.0
+    input_shape = (batch_size, image_size, image_size, 3)
+
+    model = delf_model.Delf(block3_strides=block3_strides, name='DELF')
+    model.init_classifiers(num_classes)
+
+    optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)
+
+    images = tf.random.uniform(input_shape, minval=0.0, maxval=1.0, seed=0)
+    labels = tf.random.uniform((batch_size,),
+                               minval=0,
+                               maxval=model.num_classes - 1,
+                               dtype=tf.int64)
+
+    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
+        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
+
+    def compute_loss(labels, predictions):
+      per_example_loss = loss_object(labels, predictions)
+      return tf.nn.compute_average_loss(
+          per_example_loss, global_batch_size=batch_size)
+
+    with tf.GradientTape() as desc_tape:
+      blocks = {}
+      desc_prelogits = model.backbone(
+          images, intermediates_dict=blocks, training=False)
+      desc_logits = model.desc_classification(desc_prelogits)
+      desc_logits = model.desc_classification(desc_prelogits)
+      desc_loss = compute_loss(labels, desc_logits)
+
+    gradients = desc_tape.gradient(desc_loss, model.desc_trainable_weights)
+    clipped, _ = tf.clip_by_global_norm(gradients, clip_norm=clip_val)
+    optimizer.apply_gradients(zip(clipped, model.desc_trainable_weights))
+
+    with tf.GradientTape() as attn_tape:
+      block3 = blocks['block3']
+      block3 = tf.stop_gradient(block3)
+      attn_prelogits, _, _ = model.attention(block3, training=True)
+      attn_logits = model.attn_classification(attn_prelogits)
+      attn_loss = compute_loss(labels, attn_logits)
+
+    gradients = attn_tape.gradient(attn_loss, model.attn_trainable_weights)
+    clipped, _ = tf.clip_by_global_norm(gradients, clip_norm=clip_val)
+    optimizer.apply_gradients(zip(clipped, model.attn_trainable_weights))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/delf/delf/python/training/model/export_model.py b/research/delf/delf/python/training/model/export_model.py
new file mode 100644
index 00000000..4af69a23
--- /dev/null
+++ b/research/delf/delf/python/training/model/export_model.py
@@ -0,0 +1,137 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Export DELF tensorflow inference model.
+
+This model includes feature extraction, receptive field calculation and
+key-point selection and outputs the selected feature descriptors.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+from absl import app
+from absl import flags
+import tensorflow as tf
+
+from delf.python.training.model import delf_model
+from delf.python.training.model import export_model_utils
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('ckpt_path', '/tmp/delf-logdir/delf-weights',
+                    'Path to saved checkpoint.')
+flags.DEFINE_string('export_path', None, 'Path where model will be exported.')
+flags.DEFINE_boolean('block3_strides', False,
+                     'Whether to apply strides after block3.')
+flags.DEFINE_float('iou', 1.0, 'IOU for non-max suppression.')
+
+
+def _build_tensor_info(tensor_dict):
+  """Replace the dict's value by the tensor info.
+
+  Args:
+    tensor_dict: A dictionary contains <string, tensor>.
+
+  Returns:
+    dict: New dictionary contains <string, tensor_info>.
+  """
+  return {
+      k: tf.compat.v1.saved_model.utils.build_tensor_info(t)
+      for k, t in tensor_dict.items()
+  }
+
+
+def main(argv):
+  if len(argv) > 1:
+    raise app.UsageError('Too many command-line arguments.')
+
+  export_path = FLAGS.export_path
+  if os.path.exists(export_path):
+    raise ValueError('Export_path already exists.')
+
+  with tf.Graph().as_default() as g, tf.compat.v1.Session(graph=g) as sess:
+
+    # Setup the DELF model for extraction.
+    model = delf_model.Delf(block3_strides=FLAGS.block3_strides, name='DELF')
+
+    # Initial forward pass to build model.
+    images = tf.zeros((1, 321, 321, 3), dtype=tf.float32)
+    model(images)
+
+    stride_factor = 2.0 if FLAGS.block3_strides else 1.0
+
+    # Setup the multiscale keypoint extraction.
+    input_image = tf.compat.v1.placeholder(
+        tf.uint8, shape=(None, None, 3), name='input_image')
+    input_abs_thres = tf.compat.v1.placeholder(
+        tf.float32, shape=(), name='input_abs_thres')
+    input_scales = tf.compat.v1.placeholder(
+        tf.float32, shape=[None], name='input_scales')
+    input_max_feature_num = tf.compat.v1.placeholder(
+        tf.int32, shape=(), name='input_max_feature_num')
+
+    extracted_features = export_model_utils.ExtractLocalFeatures(
+        input_image, input_scales, input_max_feature_num, input_abs_thres,
+        FLAGS.iou, lambda x: model(x, training=False), stride_factor)
+
+    # Load the weights.
+    checkpoint_path = FLAGS.ckpt_path
+    model.load_weights(checkpoint_path)
+    print('Checkpoint loaded from ', checkpoint_path)
+
+    named_input_tensors = {
+        'input_image': input_image,
+        'input_scales': input_scales,
+        'input_abs_thres': input_abs_thres,
+        'input_max_feature_num': input_max_feature_num,
+    }
+
+    # Outputs to the exported model.
+    named_output_tensors = {}
+    named_output_tensors['boxes'] = tf.identity(
+        extracted_features[0], name='boxes')
+    named_output_tensors['features'] = tf.identity(
+        extracted_features[1], name='features')
+    named_output_tensors['scales'] = tf.identity(
+        extracted_features[2], name='scales')
+    named_output_tensors['scores'] = tf.identity(
+        extracted_features[3], name='scores')
+
+    # Export the model.
+    signature_def = tf.compat.v1.saved_model.signature_def_utils.build_signature_def(
+        inputs=_build_tensor_info(named_input_tensors),
+        outputs=_build_tensor_info(named_output_tensors))
+
+    print('Exporting trained model to:', export_path)
+    builder = tf.compat.v1.saved_model.builder.SavedModelBuilder(export_path)
+
+    init_op = None
+    builder.add_meta_graph_and_variables(
+        sess, [tf.compat.v1.saved_model.tag_constants.SERVING],
+        signature_def_map={
+            tf.compat.v1.saved_model.signature_constants
+            .DEFAULT_SERVING_SIGNATURE_DEF_KEY:
+                signature_def
+        },
+        main_op=init_op)
+    builder.save()
+
+
+if __name__ == '__main__':
+  app.run(main)
diff --git a/research/delf/delf/python/training/model/export_model_utils.py b/research/delf/delf/python/training/model/export_model_utils.py
new file mode 100644
index 00000000..f4302aca
--- /dev/null
+++ b/research/delf/delf/python/training/model/export_model_utils.py
@@ -0,0 +1,171 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Helper functions for DELF model exporting."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from delf import feature_extractor
+from delf.python.training.datasets import googlelandmarks as gld
+from object_detection.core import box_list
+from object_detection.core import box_list_ops
+
+
+def ExtractLocalFeatures(image, image_scales, max_feature_num, abs_thres, iou,
+                         attention_model_fn, stride_factor):
+  """Extract local features for input image.
+
+  Args:
+    image: image tensor of type tf.uint8 with shape [h, w, channels].
+    image_scales: 1D float tensor which contains float scales used for image
+      pyramid construction.
+    max_feature_num: int tensor denotes the maximum selected feature points.
+    abs_thres: float tensor denotes the score threshold for feature selection.
+    iou: float scalar denotes the iou threshold for NMS.
+    attention_model_fn: model function. Follows the signature:
+      * Args:
+        * `images`: Image tensor which is re-scaled.
+      * Returns:
+        * `attention_prob`: attention map after the non-linearity.
+        * `feature_map`: feature map after ResNet convolution.
+    stride_factor: integer accounting for striding after block3.
+
+  Returns:
+    boxes: [N, 4] float tensor which denotes the selected receptive box. N is
+      the number of final feature points which pass through keypoint selection
+      and NMS steps.
+    features: [N, depth] float tensor.
+    feature_scales: [N] float tensor. It is the inverse of the input image
+      scales such that larger image scales correspond to larger image regions,
+      which is compatible with keypoints detected with other techniques, for
+      example Congas.
+    scores: [N, 1] float tensor denotes the attention score.
+
+  """
+  original_image_shape_float = tf.gather(
+      tf.dtypes.cast(tf.shape(image), tf.float32), [0, 1])
+
+  image_tensor = gld.NormalizeImages(
+      image, pixel_value_offset=128.0, pixel_value_scale=128.0)
+  image_tensor = tf.expand_dims(image_tensor, 0, name='image/expand_dims')
+
+  # Hard code the feature depth and receptive field parameters for now.
+  rf, stride, padding = [291.0, 16.0 * stride_factor, 145.0]
+  feature_depth = 1024
+
+  def _ProcessSingleScale(scale_index, boxes, features, scales, scores):
+    """Resizes the image and run feature extraction and keypoint selection.
+
+       This function will be passed into tf.while_loop() and be called
+       repeatedly. The input boxes are collected from the previous iteration
+       [0: scale_index -1]. We get the current scale by
+       image_scales[scale_index], and run resize image, feature extraction and
+       keypoint selection. Then we will get a new set of selected_boxes for
+       current scale. In the end, we concat the previous boxes with current
+       selected_boxes as the output.
+    Args:
+      scale_index: A valid index in the image_scales.
+      boxes: Box tensor with the shape of [N, 4].
+      features: Feature tensor with the shape of [N, depth].
+      scales: Scale tensor with the shape of [N].
+      scores: Attention score tensor with the shape of [N].
+
+    Returns:
+      scale_index: The next scale index for processing.
+      boxes: Concatenated box tensor with the shape of [K, 4]. K >= N.
+      features: Concatenated feature tensor with the shape of [K, depth].
+      scales: Concatenated scale tensor with the shape of [K].
+      scores: Concatenated score tensor with the shape of [K].
+    """
+    scale = tf.gather(image_scales, scale_index)
+    new_image_size = tf.dtypes.cast(
+        tf.round(original_image_shape_float * scale), tf.int32)
+    resized_image = tf.image.resize(image_tensor, new_image_size)
+
+    attention_prob, feature_map = attention_model_fn(resized_image)
+    attention_prob = tf.squeeze(attention_prob, axis=[0])
+    feature_map = tf.squeeze(feature_map, axis=[0])
+
+    rf_boxes = feature_extractor.CalculateReceptiveBoxes(
+        tf.shape(feature_map)[0],
+        tf.shape(feature_map)[1], rf, stride, padding)
+
+    # Re-project back to the original image space.
+    rf_boxes = tf.divide(rf_boxes, scale)
+    attention_prob = tf.reshape(attention_prob, [-1])
+    feature_map = tf.reshape(feature_map, [-1, feature_depth])
+
+    # Use attention score to select feature vectors.
+    indices = tf.reshape(tf.where(attention_prob >= abs_thres), [-1])
+    selected_boxes = tf.gather(rf_boxes, indices)
+    selected_features = tf.gather(feature_map, indices)
+    selected_scores = tf.gather(attention_prob, indices)
+    selected_scales = tf.ones_like(selected_scores, tf.float32) / scale
+
+    # Concat with the previous result from different scales.
+    boxes = tf.concat([boxes, selected_boxes], 0)
+    features = tf.concat([features, selected_features], 0)
+    scales = tf.concat([scales, selected_scales], 0)
+    scores = tf.concat([scores, selected_scores], 0)
+
+    return scale_index + 1, boxes, features, scales, scores
+
+  output_boxes = tf.zeros([0, 4], dtype=tf.float32)
+  output_features = tf.zeros([0, feature_depth], dtype=tf.float32)
+  output_scales = tf.zeros([0], dtype=tf.float32)
+  output_scores = tf.zeros([0], dtype=tf.float32)
+
+  # Process the first scale separately, the following scales will reuse the
+  # graph variables.
+  (_, output_boxes, output_features, output_scales,
+   output_scores) = _ProcessSingleScale(0, output_boxes, output_features,
+                                        output_scales, output_scores)
+
+  i = tf.constant(1, dtype=tf.int32)
+  num_scales = tf.shape(image_scales)[0]
+  keep_going = lambda j, b, f, scales, scores: tf.less(j, num_scales)
+
+  (_, output_boxes, output_features, output_scales,
+   output_scores) = tf.while_loop(
+       cond=keep_going,
+       body=_ProcessSingleScale,
+       loop_vars=[
+           i, output_boxes, output_features, output_scales, output_scores
+       ],
+       shape_invariants=[
+           i.get_shape(),
+           tf.TensorShape([None, 4]),
+           tf.TensorShape([None, feature_depth]),
+           tf.TensorShape([None]),
+           tf.TensorShape([None])
+       ],
+       back_prop=False)
+
+  feature_boxes = box_list.BoxList(output_boxes)
+  feature_boxes.add_field('features', output_features)
+  feature_boxes.add_field('scales', output_scales)
+  feature_boxes.add_field('scores', output_scores)
+
+  nms_max_boxes = tf.minimum(max_feature_num, feature_boxes.num_boxes())
+  final_boxes = box_list_ops.non_max_suppression(feature_boxes, iou,
+                                                 nms_max_boxes)
+
+  return final_boxes.get(), final_boxes.get_field(
+      'features'), final_boxes.get_field('scales'), tf.expand_dims(
+          final_boxes.get_field('scores'), 1)
diff --git a/research/delf/delf/python/training/model/resnet50.py b/research/delf/delf/python/training/model/resnet50.py
new file mode 100644
index 00000000..1c4d7c2f
--- /dev/null
+++ b/research/delf/delf/python/training/model/resnet50.py
@@ -0,0 +1,358 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""ResNet50 backbone used in DELF model.
+
+Copied over from tensorflow/python/eager/benchmarks/resnet50/resnet50.py,
+because that code does not support dependencies.
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+import tensorflow as tf
+
+layers = tf.keras.layers
+
+
+class _IdentityBlock(tf.keras.Model):
+  """_IdentityBlock is the block that has no conv layer at shortcut.
+
+  Args:
+    kernel_size: the kernel size of middle conv layer at main path
+    filters: list of integers, the filters of 3 conv layer at main path
+    stage: integer, current stage label, used for generating layer names
+    block: 'a','b'..., current block label, used for generating layer names
+    data_format: data_format for the input ('channels_first' or
+      'channels_last').
+  """
+
+  def __init__(self, kernel_size, filters, stage, block, data_format):
+    super(_IdentityBlock, self).__init__(name='')
+    filters1, filters2, filters3 = filters
+
+    conv_name_base = 'res' + str(stage) + block + '_branch'
+    bn_name_base = 'bn' + str(stage) + block + '_branch'
+    bn_axis = 1 if data_format == 'channels_first' else 3
+
+    self.conv2a = layers.Conv2D(
+        filters1, (1, 1), name=conv_name_base + '2a', data_format=data_format)
+    self.bn2a = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2a')
+
+    self.conv2b = layers.Conv2D(
+        filters2,
+        kernel_size,
+        padding='same',
+        data_format=data_format,
+        name=conv_name_base + '2b')
+    self.bn2b = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2b')
+
+    self.conv2c = layers.Conv2D(
+        filters3, (1, 1), name=conv_name_base + '2c', data_format=data_format)
+    self.bn2c = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2c')
+
+  def call(self, input_tensor, training=False):
+    x = self.conv2a(input_tensor)
+    x = self.bn2a(x, training=training)
+    x = tf.nn.relu(x)
+
+    x = self.conv2b(x)
+    x = self.bn2b(x, training=training)
+    x = tf.nn.relu(x)
+
+    x = self.conv2c(x)
+    x = self.bn2c(x, training=training)
+
+    x += input_tensor
+    return tf.nn.relu(x)
+
+
+class _ConvBlock(tf.keras.Model):
+  """_ConvBlock is the block that has a conv layer at shortcut.
+
+  Args:
+      kernel_size: the kernel size of middle conv layer at main path
+      filters: list of integers, the filters of 3 conv layer at main path
+      stage: integer, current stage label, used for generating layer names
+      block: 'a','b'..., current block label, used for generating layer names
+      data_format: data_format for the input ('channels_first' or
+        'channels_last').
+      strides: strides for the convolution. Note that from stage 3, the first
+        conv layer at main path is with strides=(2,2), and the shortcut should
+        have strides=(2,2) as well.
+  """
+
+  def __init__(self,
+               kernel_size,
+               filters,
+               stage,
+               block,
+               data_format,
+               strides=(2, 2)):
+    super(_ConvBlock, self).__init__(name='')
+    filters1, filters2, filters3 = filters
+
+    conv_name_base = 'res' + str(stage) + block + '_branch'
+    bn_name_base = 'bn' + str(stage) + block + '_branch'
+    bn_axis = 1 if data_format == 'channels_first' else 3
+
+    self.conv2a = layers.Conv2D(
+        filters1, (1, 1),
+        strides=strides,
+        name=conv_name_base + '2a',
+        data_format=data_format)
+    self.bn2a = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2a')
+
+    self.conv2b = layers.Conv2D(
+        filters2,
+        kernel_size,
+        padding='same',
+        name=conv_name_base + '2b',
+        data_format=data_format)
+    self.bn2b = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2b')
+
+    self.conv2c = layers.Conv2D(
+        filters3, (1, 1), name=conv_name_base + '2c', data_format=data_format)
+    self.bn2c = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '2c')
+
+    self.conv_shortcut = layers.Conv2D(
+        filters3, (1, 1),
+        strides=strides,
+        name=conv_name_base + '1',
+        data_format=data_format)
+    self.bn_shortcut = layers.BatchNormalization(
+        axis=bn_axis, name=bn_name_base + '1')
+
+  def call(self, input_tensor, training=False):
+    x = self.conv2a(input_tensor)
+    x = self.bn2a(x, training=training)
+    x = tf.nn.relu(x)
+
+    x = self.conv2b(x)
+    x = self.bn2b(x, training=training)
+    x = tf.nn.relu(x)
+
+    x = self.conv2c(x)
+    x = self.bn2c(x, training=training)
+
+    shortcut = self.conv_shortcut(input_tensor)
+    shortcut = self.bn_shortcut(shortcut, training=training)
+
+    x += shortcut
+    return tf.nn.relu(x)
+
+
+# pylint: disable=not-callable
+class ResNet50(tf.keras.Model):
+  """Instantiates the ResNet50 architecture.
+
+  Args:
+    data_format: format for the image. Either 'channels_first' or
+      'channels_last'.  'channels_first' is typically faster on GPUs while
+      'channels_last' is typically faster on CPUs. See
+      https://www.tensorflow.org/performance/performance_guide#data_formats
+    name: Prefix applied to names of variables created in the model.
+    include_top: whether to include the fully-connected layer at the top of the
+      network.
+    pooling: Optional pooling mode for feature extraction when `include_top` is
+      False. 'None' means that the output of the model will be the 4D tensor
+      output of the last convolutional layer. 'avg' means that global average
+      pooling will be applied to the output of the last convolutional layer, and
+      thus the output of the model will be a 2D tensor. 'max' means that global
+      max pooling will be applied.
+    block3_strides: whether to add a stride of 2 to block3 to make it compatible
+      with tf.slim ResNet implementation.
+    average_pooling: whether to do average pooling of block4 features before
+      global pooling.
+    classes: optional number of classes to classify images into, only to be
+      specified if `include_top` is True.
+
+  Raises:
+      ValueError: in case of invalid argument for data_format.
+  """
+
+  def __init__(self,
+               data_format,
+               name='',
+               include_top=True,
+               pooling=None,
+               block3_strides=False,
+               average_pooling=True,
+               classes=1000):
+    super(ResNet50, self).__init__(name=name)
+
+    valid_channel_values = ('channels_first', 'channels_last')
+    if data_format not in valid_channel_values:
+      raise ValueError('Unknown data_format: %s. Valid values: %s' %
+                       (data_format, valid_channel_values))
+    self.include_top = include_top
+    self.block3_strides = block3_strides
+    self.average_pooling = average_pooling
+    self.pooling = pooling
+
+    def conv_block(filters, stage, block, strides=(2, 2)):
+      return _ConvBlock(
+          3,
+          filters,
+          stage=stage,
+          block=block,
+          data_format=data_format,
+          strides=strides)
+
+    def id_block(filters, stage, block):
+      return _IdentityBlock(
+          3, filters, stage=stage, block=block, data_format=data_format)
+
+    self.conv1 = layers.Conv2D(
+        64, (7, 7),
+        strides=(2, 2),
+        data_format=data_format,
+        padding='same',
+        name='conv1')
+    bn_axis = 1 if data_format == 'channels_first' else 3
+    self.bn_conv1 = layers.BatchNormalization(axis=bn_axis, name='bn_conv1')
+    self.max_pool = layers.MaxPooling2D((3, 3),
+                                        strides=(2, 2),
+                                        data_format=data_format)
+
+    self.l2a = conv_block([64, 64, 256], stage=2, block='a', strides=(1, 1))
+    self.l2b = id_block([64, 64, 256], stage=2, block='b')
+    self.l2c = id_block([64, 64, 256], stage=2, block='c')
+
+    self.l3a = conv_block([128, 128, 512], stage=3, block='a')
+    self.l3b = id_block([128, 128, 512], stage=3, block='b')
+    self.l3c = id_block([128, 128, 512], stage=3, block='c')
+    self.l3d = id_block([128, 128, 512], stage=3, block='d')
+
+    self.l4a = conv_block([256, 256, 1024], stage=4, block='a')
+    self.l4b = id_block([256, 256, 1024], stage=4, block='b')
+    self.l4c = id_block([256, 256, 1024], stage=4, block='c')
+    self.l4d = id_block([256, 256, 1024], stage=4, block='d')
+    self.l4e = id_block([256, 256, 1024], stage=4, block='e')
+    self.l4f = id_block([256, 256, 1024], stage=4, block='f')
+
+    # Striding layer that can be used on top of block3 to produce feature maps
+    # with the same resolution as the TF-Slim implementation.
+    if self.block3_strides:
+      self.subsampling_layer = layers.MaxPooling2D((1, 1),
+                                                   strides=(2, 2),
+                                                   data_format=data_format)
+      self.l5a = conv_block([512, 512, 2048],
+                            stage=5,
+                            block='a',
+                            strides=(1, 1))
+    else:
+      self.l5a = conv_block([512, 512, 2048], stage=5, block='a')
+    self.l5b = id_block([512, 512, 2048], stage=5, block='b')
+    self.l5c = id_block([512, 512, 2048], stage=5, block='c')
+
+    self.avg_pool = layers.AveragePooling2D((7, 7),
+                                            strides=(7, 7),
+                                            data_format=data_format)
+
+    if self.include_top:
+      self.flatten = layers.Flatten()
+      self.fc1000 = layers.Dense(classes, name='fc1000')
+    else:
+      reduction_indices = [1, 2] if data_format == 'channels_last' else [2, 3]
+      reduction_indices = tf.constant(reduction_indices)
+      if pooling == 'avg':
+        self.global_pooling = functools.partial(
+            tf.reduce_mean, axis=reduction_indices, keepdims=False)
+      elif pooling == 'max':
+        self.global_pooling = functools.partial(
+            tf.reduce_max, axis=reduction_indices, keepdims=False)
+      else:
+        self.global_pooling = None
+
+  def call(self, inputs, training=True, intermediates_dict=None):
+    """Call the ResNet50 model.
+
+    Args:
+      inputs: Images to compute features for.
+      training: Whether model is in training phase.
+      intermediates_dict: `None` or dictionary. If not None, accumulate feature
+        maps from intermediate blocks into the dictionary. ""
+
+    Returns:
+      Tensor with featuremap.
+    """
+
+    x = self.conv1(inputs)
+    x = self.bn_conv1(x, training=training)
+    x = tf.nn.relu(x)
+    if intermediates_dict is not None:
+      intermediates_dict['block0'] = x
+
+    x = self.max_pool(x)
+    if intermediates_dict is not None:
+      intermediates_dict['block0mp'] = x
+
+    # Block 1 (equivalent to "conv2" in Resnet paper).
+    x = self.l2a(x, training=training)
+    x = self.l2b(x, training=training)
+    x = self.l2c(x, training=training)
+    if intermediates_dict is not None:
+      intermediates_dict['block1'] = x
+
+    # Block 2 (equivalent to "conv3" in Resnet paper).
+    x = self.l3a(x, training=training)
+    x = self.l3b(x, training=training)
+    x = self.l3c(x, training=training)
+    x = self.l3d(x, training=training)
+    if intermediates_dict is not None:
+      intermediates_dict['block2'] = x
+
+    # Block 3 (equivalent to "conv4" in Resnet paper).
+    x = self.l4a(x, training=training)
+    x = self.l4b(x, training=training)
+    x = self.l4c(x, training=training)
+    x = self.l4d(x, training=training)
+    x = self.l4e(x, training=training)
+    x = self.l4f(x, training=training)
+
+    if self.block3_strides:
+      x = self.subsampling_layer(x)
+      if intermediates_dict is not None:
+        intermediates_dict['block3'] = x
+    else:
+      if intermediates_dict is not None:
+        intermediates_dict['block3'] = x
+
+    x = self.l5a(x, training=training)
+    x = self.l5b(x, training=training)
+    x = self.l5c(x, training=training)
+
+    if self.average_pooling:
+      x = self.avg_pool(x)
+      if intermediates_dict is not None:
+        intermediates_dict['block4'] = x
+    else:
+      if intermediates_dict is not None:
+        intermediates_dict['block4'] = x
+
+    if self.include_top:
+      return self.fc1000(self.flatten(x))
+    elif self.global_pooling:
+      return self.global_pooling(x)
+    else:
+      return x
diff --git a/research/delf/delf/python/training/train.py b/research/delf/delf/python/training/train.py
new file mode 100644
index 00000000..dcf61b3f
--- /dev/null
+++ b/research/delf/delf/python/training/train.py
@@ -0,0 +1,438 @@
+# Lint as: python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Training script for DELF on Google Landmarks Dataset.
+
+Script to train DELF using classification loss on Google Landmarks Dataset
+using MirroredStrategy to so it can run on multiple GPUs.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+from absl import app
+from absl import flags
+from absl import logging
+import tensorflow as tf
+import tensorflow_probability as tfp
+
+# Placeholder for internal import. Do not remove this line.
+from delf.python.training.datasets import googlelandmarks as gld
+from delf.python.training.model import delf_model
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_boolean('debug', False, 'Debug mode.')
+flags.DEFINE_string('logdir', '/tmp/delf', 'WithTensorBoard logdir.')
+flags.DEFINE_string('train_file_pattern', '/tmp/data/train*',
+                    'File pattern of training dataset files.')
+flags.DEFINE_string('validation_file_pattern', '/tmp/data/validation*',
+                    'File pattern of validation dataset files.')
+flags.DEFINE_integer('seed', 0, 'Seed to training dataset.')
+flags.DEFINE_float('initial_lr', 0.001, 'Initial learning rate.')
+flags.DEFINE_integer('batch_size', 32, 'Global batch size.')
+flags.DEFINE_integer('max_iters', 500000, 'Maximum iterations.')
+flags.DEFINE_boolean('block3_strides', False, 'Whether to use block3_strides.')
+flags.DEFINE_boolean('use_augmentation', True,
+                     'Whether to use ImageNet style augmentation.')
+
+
+def _record_accuracy(metric, logits, labels):
+  """Record accuracy given predicted logits and ground-truth labels."""
+  softmax_probabilities = tf.keras.layers.Softmax()(logits)
+  metric.update_state(labels, softmax_probabilities)
+
+
+def _attention_summaries(scores, global_step):
+  """Record statistics of the attention score."""
+  tf.summary.scalar('attention/max', tf.reduce_max(scores), step=global_step)
+  tf.summary.scalar('attention/min', tf.reduce_min(scores), step=global_step)
+  tf.summary.scalar('attention/mean', tf.reduce_mean(scores), step=global_step)
+  tf.summary.scalar(
+      'attention/percent_25',
+      tfp.stats.percentile(scores, 25.0),
+      step=global_step)
+  tf.summary.scalar(
+      'attention/percent_50',
+      tfp.stats.percentile(scores, 50.0),
+      step=global_step)
+  tf.summary.scalar(
+      'attention/percent_75',
+      tfp.stats.percentile(scores, 75.0),
+      step=global_step)
+
+
+def create_model(num_classes):
+  """Define DELF model, and initialize classifiers."""
+  model = delf_model.Delf(block3_strides=FLAGS.block3_strides, name='DELF')
+  model.init_classifiers(num_classes)
+  return model
+
+
+def _learning_rate_schedule(global_step_value, max_iters, initial_lr):
+  """Calculates learning_rate with linear decay.
+
+  Args:
+    global_step_value: int, global step.
+    max_iters: int, maximum iterations.
+    initial_lr: float, initial learning rate.
+
+  Returns:
+    lr: float, learning rate.
+  """
+  lr = initial_lr * (1.0 - global_step_value / max_iters)
+  return lr
+
+
+def main(argv):
+  if len(argv) > 1:
+    raise app.UsageError('Too many command-line arguments.')
+
+  #-------------------------------------------------------------
+  # Log flags used.
+  logging.info('Running training script with\n')
+  logging.info('logdir= %s', FLAGS.logdir)
+  logging.info('initial_lr= %f', FLAGS.initial_lr)
+  logging.info('block3_strides= %s', str(FLAGS.block3_strides))
+
+  # ------------------------------------------------------------
+  # Create the strategy.
+  strategy = tf.distribute.MirroredStrategy()
+  logging.info('Number of devices: %d', strategy.num_replicas_in_sync)
+  if FLAGS.debug:
+    print('Number of devices:', strategy.num_replicas_in_sync)
+
+  max_iters = FLAGS.max_iters
+  global_batch_size = FLAGS.batch_size
+  image_size = 321
+  num_eval = 1000
+  report_interval = 100
+  eval_interval = 1000
+  save_interval = 20000
+
+  initial_lr = FLAGS.initial_lr
+
+  clip_val = tf.constant(10.0)
+
+  if FLAGS.debug:
+    global_batch_size = 4
+    max_iters = 4
+    num_eval = 1
+    save_interval = 1
+    report_interval = 1
+
+  # TODO(andrearaujo): Using placeholder, replace with actual value using
+  # GoogleLandmarksInfo() from datasets/googlelandmarks.py.
+  num_classes = 14951
+
+  # ------------------------------------------------------------
+  # Create the distributed train/validation sets.
+  train_dataset = gld.CreateDataset(
+      file_pattern=FLAGS.train_file_pattern,
+      batch_size=global_batch_size,
+      image_size=image_size,
+      augmentation=FLAGS.use_augmentation,
+      seed=FLAGS.seed)
+  validation_dataset = gld.CreateDataset(
+      file_pattern=FLAGS.validation_file_pattern,
+      batch_size=global_batch_size,
+      image_size=image_size,
+      augmentation=False,
+      seed=FLAGS.seed)
+
+  train_iterator = strategy.make_dataset_iterator(train_dataset)
+  validation_iterator = strategy.make_dataset_iterator(validation_dataset)
+
+  train_iterator.initialize()
+  validation_iterator.initialize()
+
+  # Create a checkpoint directory to store the checkpoints.
+  checkpoint_prefix = os.path.join(FLAGS.logdir, 'delf_tf2-ckpt')
+
+  # ------------------------------------------------------------
+  # Finally, we do everything in distributed scope.
+  with strategy.scope():
+    # Compute loss.
+    # Set reduction to `none` so we can do the reduction afterwards and divide
+    # by global batch size.
+    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
+        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)
+
+    def compute_loss(labels, predictions):
+      per_example_loss = loss_object(labels, predictions)
+      return tf.nn.compute_average_loss(
+          per_example_loss, global_batch_size=global_batch_size)
+
+    # Set up metrics.
+    desc_validation_loss = tf.keras.metrics.Mean(name='desc_validation_loss')
+    attn_validation_loss = tf.keras.metrics.Mean(name='attn_validation_loss')
+    desc_train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
+        name='desc_train_accuracy')
+    attn_train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
+        name='attn_train_accuracy')
+    desc_validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
+        name='desc_validation_accuracy')
+    attn_validation_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(
+        name='attn_validation_accuracy')
+
+    # ------------------------------------------------------------
+    # Setup DELF model and optimizer.
+    model = create_model(num_classes)
+    logging.info('Model, datasets loaded.\nnum_classes= %d', num_classes)
+
+    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_lr, momentum=0.9)
+
+    # Setup summary writer.
+    summary_writer = tf.summary.create_file_writer(
+        os.path.join(FLAGS.logdir, 'train_logs'), flush_millis=10000)
+
+    # Setup checkpoint directory.
+    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
+    manager = tf.train.CheckpointManager(
+        checkpoint, checkpoint_prefix, max_to_keep=3)
+
+    # ------------------------------------------------------------
+    # Train step to run on one GPU.
+    def train_step(inputs):
+      """Train one batch."""
+      images, labels = inputs
+      # Temporary workaround to avoid some corrupted labels.
+      labels = tf.clip_by_value(labels, 0, model.num_classes)
+
+      global_step = optimizer.iterations
+      tf.summary.scalar(
+          'image_range/max', tf.reduce_max(images), step=global_step)
+      tf.summary.scalar(
+          'image_range/min', tf.reduce_min(images), step=global_step)
+
+      def _backprop_loss(tape, loss, weights):
+        """Backpropogate losses using clipped gradients.
+
+        Args:
+          tape: gradient tape.
+          loss: scalar Tensor, loss value.
+          weights: keras model weights.
+        """
+        gradients = tape.gradient(loss, weights)
+        clipped, _ = tf.clip_by_global_norm(gradients, clip_norm=clip_val)
+        optimizer.apply_gradients(zip(clipped, weights))
+
+      # Record gradients and loss through backbone.
+      with tf.GradientTape() as desc_tape:
+
+        blocks = {}
+        prelogits = model.backbone(
+            images, intermediates_dict=blocks, training=True)
+
+        # Report sparsity.
+        activations_zero_fractions = {
+            'sparsity/%s' % k: tf.nn.zero_fraction(v)
+            for k, v in blocks.items()
+        }
+        for k, v in activations_zero_fractions.items():
+          tf.summary.scalar(k, v, step=global_step)
+
+        # Apply descriptor classifier.
+        logits = model.desc_classification(prelogits)
+
+        desc_loss = compute_loss(labels, logits)
+
+      # Backprop only through backbone weights.
+      _backprop_loss(desc_tape, desc_loss, model.desc_trainable_weights)
+
+      # Record descriptor train accuracy.
+      _record_accuracy(desc_train_accuracy, logits, labels)
+
+      # Record gradients and loss through attention block.
+      with tf.GradientTape() as attn_tape:
+        block3 = blocks['block3']  # pytype: disable=key-error
+
+        # Stopping gradients according to DELG paper:
+        # (https://arxiv.org/abs/2001.05027).
+        block3 = tf.stop_gradient(block3)
+
+        prelogits, scores, _ = model.attention(block3, training=True)
+        _attention_summaries(scores, global_step)
+
+        # Apply attention block classifier.
+        logits = model.attn_classification(prelogits)
+
+        attn_loss = compute_loss(labels, logits)
+
+      # Backprop only through attention weights.
+      _backprop_loss(attn_tape, attn_loss, model.attn_trainable_weights)
+
+      # Record attention train accuracy.
+      _record_accuracy(attn_train_accuracy, logits, labels)
+
+      return desc_loss, attn_loss
+
+    # ------------------------------------------------------------
+    def validation_step(inputs):
+      """Validate one batch."""
+      images, labels = inputs
+      labels = tf.clip_by_value(labels, 0, model.num_classes)
+
+      # Get descriptor predictions.
+      blocks = {}
+      prelogits = model.backbone(
+          images, intermediates_dict=blocks, training=False)
+      logits = model.desc_classification(prelogits, training=False)
+      softmax_probabilities = tf.keras.layers.Softmax()(logits)
+
+      validation_loss = loss_object(labels, logits)
+      desc_validation_loss.update_state(validation_loss)
+      desc_validation_accuracy.update_state(labels, softmax_probabilities)
+
+      # Get attention predictions.
+      block3 = blocks['block3']  # pytype: disable=key-error
+      prelogits, _, _ = model.attention(block3, training=False)
+
+      logits = model.attn_classification(prelogits, training=False)
+      softmax_probabilities = tf.keras.layers.Softmax()(logits)
+
+      validation_loss = loss_object(labels, logits)
+      attn_validation_loss.update_state(validation_loss)
+      attn_validation_accuracy.update_state(labels, softmax_probabilities)
+
+      return desc_validation_accuracy.result(), attn_validation_accuracy.result(
+      )
+
+    # `run` replicates the provided computation and runs it
+    # with the distributed input.
+    @tf.function
+    def distributed_train_step(dataset_inputs):
+      """Get the actual losses."""
+      # Each (desc, attn) is a list of 3 losses - crossentropy, reg, total.
+      desc_per_replica_loss, attn_per_replica_loss = (
+          strategy.run(train_step, args=(dataset_inputs,)))
+
+      # Reduce over the replicas.
+      desc_global_loss = strategy.reduce(
+          tf.distribute.ReduceOp.SUM, desc_per_replica_loss, axis=None)
+      attn_global_loss = strategy.reduce(
+          tf.distribute.ReduceOp.SUM, attn_per_replica_loss, axis=None)
+
+      return desc_global_loss, attn_global_loss
+
+    @tf.function
+    def distributed_validation_step(dataset_inputs):
+      return strategy.run(validation_step, args=(dataset_inputs,))
+
+    # ------------------------------------------------------------
+    # *** TRAIN LOOP ***
+    with summary_writer.as_default():
+      with tf.summary.record_if(
+          tf.math.equal(0, optimizer.iterations % report_interval)):
+
+        global_step_value = optimizer.iterations.numpy()
+        while global_step_value < max_iters:
+
+          # input_batch : images(b, h, w, c), labels(b,).
+          try:
+            input_batch = train_iterator.get_next()
+          except tf.errors.OutOfRangeError:
+            # Break if we run out of data in the dataset.
+            logging.info('Stopping training at global step %d, no more data',
+                         global_step_value)
+            break
+
+          # Set learning rate for optimizer to use.
+          global_step = optimizer.iterations
+          global_step_value = global_step.numpy()
+
+          learning_rate = _learning_rate_schedule(global_step_value, max_iters,
+                                                  initial_lr)
+          optimizer.learning_rate = learning_rate
+          tf.summary.scalar(
+              'learning_rate', optimizer.learning_rate, step=global_step)
+
+          # Run the training step over num_gpu gpus.
+          desc_dist_loss, attn_dist_loss = distributed_train_step(input_batch)
+
+          # Log losses and accuracies to tensorboard.
+          tf.summary.scalar(
+              'loss/desc/crossentropy', desc_dist_loss, step=global_step)
+          tf.summary.scalar(
+              'loss/attn/crossentropy', attn_dist_loss, step=global_step)
+          tf.summary.scalar(
+              'train_accuracy/desc',
+              desc_train_accuracy.result(),
+              step=global_step)
+          tf.summary.scalar(
+              'train_accuracy/attn',
+              attn_train_accuracy.result(),
+              step=global_step)
+
+          # Print to console if running locally.
+          if FLAGS.debug:
+            if global_step_value % report_interval == 0:
+              print(global_step.numpy())
+              print('desc:', desc_dist_loss.numpy())
+              print('attn:', attn_dist_loss.numpy())
+
+          # Validate once in {eval_interval*n, n \in N} steps.
+          if global_step_value % eval_interval == 0:
+            for i in range(num_eval):
+              try:
+                validation_batch = validation_iterator.get_next()
+                desc_validation_result, attn_validation_result = (
+                    distributed_validation_step(validation_batch))
+              except tf.errors.OutOfRangeError:
+                logging.info('Stopping eval at batch %d, no more data', i)
+                break
+
+            # Log validation results to tensorboard.
+            tf.summary.scalar(
+                'validation/desc', desc_validation_result, step=global_step)
+            tf.summary.scalar(
+                'validation/attn', attn_validation_result, step=global_step)
+
+            logging.info('\nValidation(%f)\n', global_step_value)
+            logging.info(': desc: %f\n', desc_validation_result.numpy())
+            logging.info(': attn: %f\n', attn_validation_result.numpy())
+            # Print to console.
+            if FLAGS.debug:
+              print('Validation: desc:', desc_validation_result.numpy())
+              print('          : attn:', attn_validation_result.numpy())
+
+          # Save checkpoint once (each save_interval*n, n \in N) steps.
+          if global_step_value % save_interval == 0:
+            save_path = manager.save()
+            logging.info('Saved({global_step_value}) at %s', save_path)
+
+            file_path = '%s/delf_weights' % FLAGS.logdir
+            model.save_weights(file_path, save_format='tf')
+            logging.info('Saved weights({global_step_value}) at %s', file_path)
+
+          # Reset metrics for next step.
+          desc_train_accuracy.reset_states()
+          attn_train_accuracy.reset_states()
+          desc_validation_loss.reset_states()
+          attn_validation_loss.reset_states()
+          desc_validation_accuracy.reset_states()
+          attn_validation_accuracy.reset_states()
+
+          if global_step.numpy() > max_iters:
+            break
+
+    logging.info('Finished training for %d steps.', max_iters)
+
+
+if __name__ == '__main__':
+  app.run(main)
diff --git a/research/delf/setup.py b/research/delf/setup.py
index 61b0ac50..090078ef 100644
--- a/research/delf/setup.py
+++ b/research/delf/setup.py
@@ -16,10 +16,22 @@
 
 from setuptools import setup, find_packages
 
+install_requires = [
+    'absl-py >= 0.7.1',
+    'protobuf >= 3.8.0',
+    'pandas >= 0.24.2',
+    'numpy >= 1.16.1',
+    'scipy >= 1.2.2',
+    'tensorflow >= 2.1',
+    'tf_slim >= 1.1',
+    'tensorflow_probability >= 0.9.0',
+]
+
 setup(
     name='delf',
-    version='0.1',
+    version='2.0',
     include_package_data=True,
     packages=find_packages(),
+    install_requires=install_requires,
     description='DELF (DEep Local Features)',
 )
