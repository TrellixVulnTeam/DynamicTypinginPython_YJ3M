commit d40fddbe7da1fb6cacc2fb1760fe07bb8dd62b21
Author: Mark Daoust <markdaoust@google.com>
Date:   Fri Jul 20 10:22:53 2018 -0700

    Add clarifications.

diff --git a/samples/core/tutorials/keras/overfit_and_underfit.ipynb b/samples/core/tutorials/keras/overfit_and_underfit.ipynb
index 87fe7aeb..f85a1157 100644
--- a/samples/core/tutorials/keras/overfit_and_underfit.ipynb
+++ b/samples/core/tutorials/keras/overfit_and_underfit.ipynb
@@ -142,7 +142,7 @@
         "\n",
         "In other words, our model would *overfit* to the training data. Learning how to deal with overfitting is important. Although it's often possible to achieve high accuracy on the *training set*, what we really want is to develop models that generalize well to a *testing data* (or data they haven't seen before).\n",
         "\n",
-        "The opposite of overfitting is *underfitting*. Underfitting occurs when there is still room for improvement on the training data if you continue to train for more epochs. This means the network has not yet learned all the relevant patterns in the training data. \n",
+        "The opposite of overfitting is *underfitting*. Underfitting occurs when there is still room for improvement on the test data. This can happen for a number of reasons: If the model is not powerful enough, is over-regularized, or has simply not been trained long enough. This means the network has not learned the relevant patterns in the training data. \n",
         "\n",
         "If you train for too long though, the model will start to overfit and learn patterns from the training data that don't generalize to the test data. We need to strike a balance. Understanding how to train for an appropriate number of epochs as we'll explore below is a useful skill.\n",
         "\n",
