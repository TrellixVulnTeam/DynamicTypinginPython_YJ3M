commit 1862b9c3ee60575a54d5c02a8f00e12cccf85d71
Author: Hongkun Yu <hongkuny@google.com>
Date:   Tue Sep 17 10:34:33 2019 -0700

    Move Bert to NLP. Tasks are moved to nlp/bert/
    Refactor basic utils to modeling/
    
    PiperOrigin-RevId: 269600561

diff --git a/official/README.md b/official/README.md
index 7a113999..43c57966 100644
--- a/official/README.md
+++ b/official/README.md
@@ -10,12 +10,16 @@ same speed and performance with each new TensorFlow build.
 
 ## Tensorflow releases
 
-The master branch of the models are **in development**, and they target the
+The master branch of the models are **in development** with TensorFlow 2.x, and
+they target the
 [nightly binaries](https://github.com/tensorflow/tensorflow#installation) built
 from the
 [master branch of TensorFlow](https://github.com/tensorflow/tensorflow/tree/master).
-We aim to keep them backwards compatible with the latest release when possible
-(currently TensorFlow 1.5), but we cannot always guarantee compatibility.
+or install with pip:
+
+```shell
+pip install tf-nightly-2.0-preview
+```
 
 **Stable versions** of the official models targeting releases of TensorFlow are
 available as tagged branches or
@@ -53,13 +57,24 @@ installable Official Models package. This is being tracked in
 **NOTE:** Please make sure to follow the steps in the
 [Requirements](#requirements) section.
 
-*   [bert](bert): A powerful pre-trained language representation model: BERT,
-    which stands for Bidirectional Encoder Representations from Transformers.
-*   [mnist](mnist): A basic model to classify digits from the MNIST dataset.
-*   [resnet](vision/image_classification): A deep residual network that can be
-    used to classify both CIFAR-10 and ImageNet's dataset of 1000 classes.
+### Natural Language Processing:
+
+*   [bert](nlp/bert): A powerful pre-trained language representation model:
+    BERT, which stands for Bidirectional Encoder Representations from
+    Transformers.
 *   [transformer](transformer): A transformer model to translate the WMT English
     to German dataset.
+*   [xlnet](nlp/xlnet): XLNet: Generalized Autoregressive Pretraining for
+    Language Understanding
+
+### Computer Vision
+
+*   [resnet](vision/image_classification): A deep residual network that can be
+    used to classify both CIFAR-10 and ImageNet's dataset of 1000 classes.
+
+### Others
+
+*   [mnist](mnist): A basic model to classify digits from the MNIST dataset.
 *   [ncf](recommendation): Neural Collaborative Filtering model for
     recommendation tasks.
 
diff --git a/official/bert/__init__.py b/official/bert/__init__.py
deleted file mode 100644
index e69de29b..00000000
diff --git a/official/bert/benchmark/bert_benchmark.py b/official/bert/benchmark/bert_benchmark.py
index d493143c..d6deb6e1 100644
--- a/official/bert/benchmark/bert_benchmark.py
+++ b/official/bert/benchmark/bert_benchmark.py
@@ -29,9 +29,9 @@ from absl.testing import flagsaver
 import tensorflow as tf
 # pylint: enable=g-bad-import-order
 
-from official.bert import modeling
-from official.bert import run_classifier
 from official.bert.benchmark import benchmark_utils
+from official.nlp import bert_modeling as modeling
+from official.nlp.bert import run_classifier
 from official.utils.misc import distribution_utils
 
 # pylint: disable=line-too-long
diff --git a/official/bert/benchmark/bert_squad_benchmark.py b/official/bert/benchmark/bert_squad_benchmark.py
index ce1e1e02..7e2cb55c 100644
--- a/official/bert/benchmark/bert_squad_benchmark.py
+++ b/official/bert/benchmark/bert_squad_benchmark.py
@@ -28,9 +28,9 @@ from absl.testing import flagsaver
 import tensorflow as tf
 # pylint: enable=g-bad-import-order
 
-from official.bert import run_squad
 from official.bert.benchmark import benchmark_utils
 from official.bert.benchmark import squad_evaluate_v1_1
+from official.nlp.bert import run_squad
 from official.utils.misc import distribution_utils
 
 # pylint: disable=line-too-long
diff --git a/official/bert/model_training_utils.py b/official/modeling/model_training_utils.py
similarity index 99%
rename from official/bert/model_training_utils.py
rename to official/modeling/model_training_utils.py
index ef848178..eecee6f8 100644
--- a/official/bert/model_training_utils.py
+++ b/official/modeling/model_training_utils.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Utilities to train BERT models."""
+"""A light weight utilities to train NLP models."""
 
 from __future__ import absolute_import
 from __future__ import division
@@ -22,7 +22,7 @@ import json
 import os
 
 from absl import logging
-import tensorflow as tf
+import tensorflow.compat.v2 as tf
 from official.utils.misc import distribution_utils
 from official.utils.misc import tpu_lib
 
diff --git a/official/modeling/tf_utils.py b/official/modeling/tf_utils.py
new file mode 100644
index 00000000..0ce5686b
--- /dev/null
+++ b/official/modeling/tf_utils.py
@@ -0,0 +1,185 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Common TF utilities."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import math
+import six
+import tensorflow.compat.v2 as tf
+
+
+def pack_inputs(inputs):
+  """Pack a list of `inputs` tensors to a tuple.
+
+  Args:
+    inputs: a list of tensors.
+
+  Returns:
+    a tuple of tensors. if any input is None, replace it with a special constant
+    tensor.
+  """
+  inputs = tf.nest.flatten(inputs)
+  outputs = []
+  for x in inputs:
+    if x is None:
+      outputs.append(tf.constant(0, shape=[], dtype=tf.int32))
+    else:
+      outputs.append(x)
+  return tuple(outputs)
+
+
+def unpack_inputs(inputs):
+  """unpack a tuple of `inputs` tensors to a tuple.
+
+  Args:
+    inputs: a list of tensors.
+
+  Returns:
+    a tuple of tensors. if any input is a special constant tensor, replace it
+    with None.
+  """
+  inputs = tf.nest.flatten(inputs)
+  outputs = []
+  for x in inputs:
+    if is_special_none_tensor(x):
+      outputs.append(None)
+    else:
+      outputs.append(x)
+  x = tuple(outputs)
+
+  # To trick the very pointless 'unbalanced-tuple-unpacking' pylint check
+  # from triggering.
+  if len(x) == 1:
+    return x[0]
+  return tuple(outputs)
+
+
+def is_special_none_tensor(tensor):
+  """Checks if a tensor is a special None Tensor."""
+  return tensor.shape.ndims == 0 and tensor.dtype == tf.int32
+
+
+def gelu(x):
+  """Gaussian Error Linear Unit.
+
+  This is a smoother version of the RELU.
+  Original paper: https://arxiv.org/abs/1606.08415
+  Args:
+    x: float Tensor to perform activation.
+
+  Returns:
+    `x` with the GELU activation applied.
+  """
+  cdf = 0.5 * (1.0 + tf.tanh(
+      (math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3)))))
+  return x * cdf
+
+
+def get_activation(identifier):
+  """Maps a string to a Python function, e.g., "relu" => `tf.nn.relu`.
+
+  Args:
+    identifier: String name of the activation function.
+
+  Returns:
+    A Python function corresponding to the activation function. If
+    `identifier` is None, empty, or "linear", this will return None.
+    If `identifier` is not a string, it will return `identifier`.
+
+  Raises:
+    ValueError: The `identifier` does not correspond to a known
+      activation.
+  """
+  if identifier is None:
+    return None
+  elif isinstance(identifier, six.string_types):
+    name_to_fn = {
+        "linear": None,
+        "relu": tf.nn.relu,
+        "gelu": gelu,
+        "tanh": tf.nn.tanh,
+    }
+    identifier = str(identifier).lower()
+    if identifier not in name_to_fn:
+      raise ValueError("Unsupported activation function: %s" % (identifier))
+    return name_to_fn[identifier]
+  elif callable(identifier):
+    return identifier
+  else:
+    raise ValueError("Could not interpret activation "
+                     "function identifier: %s" % (identifier))
+
+
+def get_shape_list(tensor, expected_rank=None, name=None):
+  """Returns a list of the shape of tensor, preferring static dimensions.
+
+  Args:
+    tensor: A tf.Tensor object to find the shape of.
+    expected_rank: (optional) int. The expected rank of `tensor`. If this is
+      specified and the `tensor` has a different rank, and exception will be
+      thrown.
+    name: Optional name of the tensor for the error message.
+
+  Returns:
+    A list of dimensions of the shape of tensor. All static dimensions will
+    be returned as python integers, and dynamic dimensions will be returned
+    as tf.Tensor scalars.
+  """
+  if expected_rank is not None:
+    assert_rank(tensor, expected_rank, name)
+
+  shape = tensor.shape.as_list()
+
+  non_static_indexes = []
+  for (index, dim) in enumerate(shape):
+    if dim is None:
+      non_static_indexes.append(index)
+
+  if not non_static_indexes:
+    return shape
+
+  dyn_shape = tf.shape(tensor)
+  for index in non_static_indexes:
+    shape[index] = dyn_shape[index]
+  return shape
+
+
+def assert_rank(tensor, expected_rank, name=None):
+  """Raises an exception if the tensor rank is not of the expected rank.
+
+  Args:
+    tensor: A tf.Tensor to check the rank of.
+    expected_rank: Python integer or list of integers, expected rank.
+    name: Optional name of the tensor for the error message.
+
+  Raises:
+    ValueError: If the expected shape doesn't match the actual shape.
+  """
+  expected_rank_dict = {}
+  if isinstance(expected_rank, six.integer_types):
+    expected_rank_dict[expected_rank] = True
+  else:
+    for x in expected_rank:
+      expected_rank_dict[x] = True
+
+  actual_rank = tensor.shape.ndims
+  if actual_rank not in expected_rank_dict:
+    raise ValueError(
+        "For the tensor `%s`, the actual tensor rank `%d` (shape = %s) is not "
+        "equal to the expected tensor rank `%s`" %
+        (name, actual_rank, str(tensor.shape), str(expected_rank)))
diff --git a/official/bert/README.md b/official/nlp/bert/README.md
similarity index 100%
rename from official/bert/README.md
rename to official/nlp/bert/README.md
diff --git a/official/nlp/bert/__init__.py b/official/nlp/bert/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/official/nlp/bert/__init__.py
@@ -0,0 +1 @@
+
diff --git a/official/bert/classifier_data_lib.py b/official/nlp/bert/classifier_data_lib.py
similarity index 99%
rename from official/bert/classifier_data_lib.py
rename to official/nlp/bert/classifier_data_lib.py
index 01607a3d..0706d4a8 100644
--- a/official/bert/classifier_data_lib.py
+++ b/official/nlp/bert/classifier_data_lib.py
@@ -25,7 +25,7 @@ import os
 from absl import logging
 import tensorflow as tf
 
-from official.bert import tokenization
+from official.nlp.bert import tokenization
 
 
 class InputExample(object):
diff --git a/official/bert/common_flags.py b/official/nlp/bert/common_flags.py
similarity index 100%
rename from official/bert/common_flags.py
rename to official/nlp/bert/common_flags.py
diff --git a/official/bert/create_finetuning_data.py b/official/nlp/bert/create_finetuning_data.py
similarity index 98%
rename from official/bert/create_finetuning_data.py
rename to official/nlp/bert/create_finetuning_data.py
index 7a8f3cb9..c318f165 100644
--- a/official/bert/create_finetuning_data.py
+++ b/official/nlp/bert/create_finetuning_data.py
@@ -24,8 +24,8 @@ from absl import app
 from absl import flags
 import tensorflow as tf
 
-from official.bert import classifier_data_lib
-from official.bert import squad_lib
+from official.nlp.bert import classifier_data_lib
+from official.nlp.bert import squad_lib
 
 FLAGS = flags.FLAGS
 
diff --git a/official/bert/input_pipeline.py b/official/nlp/bert/input_pipeline.py
similarity index 100%
rename from official/bert/input_pipeline.py
rename to official/nlp/bert/input_pipeline.py
diff --git a/official/bert/model_saving_utils.py b/official/nlp/bert/model_saving_utils.py
similarity index 100%
rename from official/bert/model_saving_utils.py
rename to official/nlp/bert/model_saving_utils.py
diff --git a/official/bert/run_classifier.py b/official/nlp/bert/run_classifier.py
similarity index 95%
rename from official/bert/run_classifier.py
rename to official/nlp/bert/run_classifier.py
index 0b57c3a6..108eae73 100644
--- a/official/bert/run_classifier.py
+++ b/official/nlp/bert/run_classifier.py
@@ -27,14 +27,14 @@ from absl import flags
 from absl import logging
 import tensorflow as tf
 
-# Import BERT model libraries.
-from official.bert import bert_models
-from official.bert import common_flags
-from official.bert import input_pipeline
-from official.bert import model_saving_utils
-from official.bert import model_training_utils
-from official.bert import modeling
-from official.bert import optimization
+# pylint: disable=g-import-not-at-top,redefined-outer-name,reimported
+from official.modeling import model_training_utils
+from official.nlp import bert_modeling as modeling
+from official.nlp import bert_models
+from official.nlp import optimization
+from official.nlp.bert import common_flags
+from official.nlp.bert import input_pipeline
+from official.nlp.bert import model_saving_utils
 from official.utils.misc import keras_utils
 from official.utils.misc import tpu_lib
 
diff --git a/official/bert/run_pretraining.py b/official/nlp/bert/run_pretraining.py
similarity index 95%
rename from official/bert/run_pretraining.py
rename to official/nlp/bert/run_pretraining.py
index 9b58ec76..962b17d2 100644
--- a/official/bert/run_pretraining.py
+++ b/official/nlp/bert/run_pretraining.py
@@ -25,14 +25,14 @@ from absl import flags
 from absl import logging
 import tensorflow as tf
 
-# Import BERT model libraries.
-from official.bert import bert_models
-from official.bert import common_flags
-from official.bert import input_pipeline
-from official.bert import model_saving_utils
-from official.bert import model_training_utils
-from official.bert import modeling
-from official.bert import optimization
+# pylint: disable=unused-import,g-import-not-at-top,redefined-outer-name,reimported
+from official.modeling import model_training_utils
+from official.nlp import bert_modeling as modeling
+from official.nlp import bert_models
+from official.nlp import optimization
+from official.nlp.bert import common_flags
+from official.nlp.bert import input_pipeline
+from official.nlp.bert import model_saving_utils
 from official.utils.misc import tpu_lib
 
 flags.DEFINE_string('input_files', None,
diff --git a/official/bert/run_squad.py b/official/nlp/bert/run_squad.py
similarity index 96%
rename from official/bert/run_squad.py
rename to official/nlp/bert/run_squad.py
index 104b1592..57208f7d 100644
--- a/official/bert/run_squad.py
+++ b/official/nlp/bert/run_squad.py
@@ -27,16 +27,16 @@ from absl import flags
 from absl import logging
 import tensorflow as tf
 
-# Import BERT model libraries.
-from official.bert import bert_models
-from official.bert import common_flags
-from official.bert import input_pipeline
-from official.bert import model_saving_utils
-from official.bert import model_training_utils
-from official.bert import modeling
-from official.bert import optimization
-from official.bert import squad_lib
-from official.bert import tokenization
+# pylint: disable=unused-import,g-import-not-at-top,redefined-outer-name,reimported
+from official.modeling import model_training_utils
+from official.nlp import bert_modeling as modeling
+from official.nlp import bert_models
+from official.nlp import optimization
+from official.nlp.bert import common_flags
+from official.nlp.bert import input_pipeline
+from official.nlp.bert import model_saving_utils
+from official.nlp.bert import squad_lib
+from official.nlp.bert import tokenization
 from official.utils.misc import keras_utils
 from official.utils.misc import tpu_lib
 
diff --git a/official/bert/squad_lib.py b/official/nlp/bert/squad_lib.py
similarity index 99%
rename from official/bert/squad_lib.py
rename to official/nlp/bert/squad_lib.py
index 5ad01ddb..53d03ba8 100644
--- a/official/bert/squad_lib.py
+++ b/official/nlp/bert/squad_lib.py
@@ -28,8 +28,7 @@ import six
 from absl import logging
 import tensorflow as tf
 
-from official.bert import tokenization
-# pylint: enable=g-bad-import-order
+from official.nlp.bert import tokenization
 
 
 class SquadExample(object):
diff --git a/official/bert/tools/tf1_to_keras_checkpoint_converter.py b/official/nlp/bert/tf1_to_keras_checkpoint_converter.py
similarity index 98%
rename from official/bert/tools/tf1_to_keras_checkpoint_converter.py
rename to official/nlp/bert/tf1_to_keras_checkpoint_converter.py
index e3d29009..964cb50c 100644
--- a/official/bert/tools/tf1_to_keras_checkpoint_converter.py
+++ b/official/nlp/bert/tf1_to_keras_checkpoint_converter.py
@@ -16,7 +16,7 @@ r"""Convert checkpoints created by Estimator (tf1) to be Keras compatible.
 
 Keras manages variable names internally, which results in subtly different names
 for variables between the Estimator and Keras version.
-The script should be ran with TF 1.x.
+The script should be used with TF 1.x.
 
 Usage:
 
@@ -29,7 +29,7 @@ from __future__ import division
 from __future__ import print_function
 
 from absl import app
-import tensorflow as tf
+import tensorflow as tf  # TF 1.x
 
 flags = tf.flags
 
diff --git a/official/bert/tools/tf2_checkpoint_converter.py b/official/nlp/bert/tf2_checkpoint_converter.py
similarity index 97%
rename from official/bert/tools/tf2_checkpoint_converter.py
rename to official/nlp/bert/tf2_checkpoint_converter.py
index 5e933325..50de899a 100644
--- a/official/bert/tools/tf2_checkpoint_converter.py
+++ b/official/nlp/bert/tf2_checkpoint_converter.py
@@ -26,8 +26,8 @@ from __future__ import print_function
 from absl import app
 from absl import flags
 
-import tensorflow as tf
-from official.bert import modeling
+import tensorflow as tf  # TF 1.x
+from official.nlp import bert_modeling as modeling
 
 FLAGS = flags.FLAGS
 
diff --git a/official/bert/tokenization.py b/official/nlp/bert/tokenization.py
similarity index 100%
rename from official/bert/tokenization.py
rename to official/nlp/bert/tokenization.py
diff --git a/official/bert/tokenization_test.py b/official/nlp/bert/tokenization_test.py
similarity index 99%
rename from official/bert/tokenization_test.py
rename to official/nlp/bert/tokenization_test.py
index 2e916644..c59bfc09 100644
--- a/official/bert/tokenization_test.py
+++ b/official/nlp/bert/tokenization_test.py
@@ -22,7 +22,7 @@ import tempfile
 import six
 import tensorflow as tf
 
-from official.bert import tokenization
+from official.nlp.bert import tokenization
 
 
 class TokenizationTest(tf.test.TestCase):
diff --git a/official/bert/modeling.py b/official/nlp/bert_modeling.py
similarity index 86%
rename from official/bert/modeling.py
rename to official/nlp/bert_modeling.py
index 4c5d64da..c0457929 100644
--- a/official/bert/modeling.py
+++ b/official/nlp/bert_modeling.py
@@ -24,6 +24,8 @@ import math
 import six
 import tensorflow as tf
 
+from official.modeling import tf_utils
+
 
 class BertConfig(object):
   """Configuration for `BertModel`."""
@@ -191,7 +193,7 @@ class BertModel(tf.keras.layers.Layer):
                input_mask=None,
                input_type_ids=None,
                **kwargs):
-    inputs = pack_inputs([input_word_ids, input_mask, input_type_ids])
+    inputs = tf_utils.pack_inputs([input_word_ids, input_mask, input_type_ids])
     return super(BertModel, self).__call__(inputs, **kwargs)
 
   def call(self, inputs, mode="bert"):
@@ -205,7 +207,7 @@ class BertModel(tf.keras.layers.Layer):
       is a float Tensor of shape [batch_size, seq_length, hidden_size] or
       a list of output tensors for encoder usage (mode=`encoder`).
     """
-    unpacked_inputs = unpack_inputs(inputs)
+    unpacked_inputs = tf_utils.unpack_inputs(inputs)
     input_word_ids = unpacked_inputs[0]
     input_mask = unpacked_inputs[1]
     input_type_ids = unpacked_inputs[2]
@@ -260,7 +262,7 @@ class EmbeddingLookup(tf.keras.layers.Layer):
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    input_shape = get_shape_list(inputs)
+    input_shape = tf_utils.get_shape_list(inputs)
     flat_input = tf.reshape(inputs, [-1])
     output = tf.gather(self.embeddings, flat_input)
     output = tf.reshape(output, input_shape + [self.embedding_size])
@@ -323,15 +325,15 @@ class EmbeddingPostprocessor(tf.keras.layers.Layer):
     super(EmbeddingPostprocessor, self).build(input_shapes)
 
   def __call__(self, word_embeddings, token_type_ids=None, **kwargs):
-    inputs = pack_inputs([word_embeddings, token_type_ids])
+    inputs = tf_utils.pack_inputs([word_embeddings, token_type_ids])
     return super(EmbeddingPostprocessor, self).__call__(inputs, **kwargs)
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    unpacked_inputs = unpack_inputs(inputs)
+    unpacked_inputs = tf_utils.unpack_inputs(inputs)
     word_embeddings = unpacked_inputs[0]
     token_type_ids = unpacked_inputs[1]
-    input_shape = get_shape_list(word_embeddings, expected_rank=3)
+    input_shape = tf_utils.get_shape_list(word_embeddings, expected_rank=3)
     batch_size = input_shape[0]
     seq_length = input_shape[1]
     width = input_shape[2]
@@ -429,12 +431,12 @@ class Attention(tf.keras.layers.Layer):
     return output_tensor
 
   def __call__(self, from_tensor, to_tensor, attention_mask=None, **kwargs):
-    inputs = pack_inputs([from_tensor, to_tensor, attention_mask])
+    inputs = tf_utils.pack_inputs([from_tensor, to_tensor, attention_mask])
     return super(Attention, self).__call__(inputs, **kwargs)
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    (from_tensor, to_tensor, attention_mask) = unpack_inputs(inputs)
+    (from_tensor, to_tensor, attention_mask) = tf_utils.unpack_inputs(inputs)
 
     # Scalar dimensions referenced here:
     #   B = batch size (number of sequences)
@@ -707,7 +709,8 @@ class TransformerBlock(tf.keras.layers.Layer):
     self.hidden_size = hidden_size
     self.num_attention_heads = num_attention_heads
     self.intermediate_size = intermediate_size
-    self.intermediate_activation = get_activation(intermediate_activation)
+    self.intermediate_activation = tf_utils.get_activation(
+        intermediate_activation)
     self.hidden_dropout_prob = hidden_dropout_prob
     self.attention_probs_dropout_prob = attention_probs_dropout_prob
     self.initializer_range = initializer_range
@@ -769,12 +772,12 @@ class TransformerBlock(tf.keras.layers.Layer):
     ]
 
   def __call__(self, input_tensor, attention_mask=None):
-    inputs = pack_inputs([input_tensor, attention_mask])
+    inputs = tf_utils.pack_inputs([input_tensor, attention_mask])
     return super(TransformerBlock, self).__call__(inputs)
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    (input_tensor, attention_mask) = unpack_inputs(inputs)
+    (input_tensor, attention_mask) = tf_utils.unpack_inputs(inputs)
     attention_output = self.attention_layer(
         from_tensor=input_tensor,
         to_tensor=input_tensor,
@@ -835,7 +838,8 @@ class Transformer(tf.keras.layers.Layer):
     self.hidden_size = hidden_size
     self.num_attention_heads = num_attention_heads
     self.intermediate_size = intermediate_size
-    self.intermediate_activation = get_activation(intermediate_activation)
+    self.intermediate_activation = tf_utils.get_activation(
+        intermediate_activation)
     self.hidden_dropout_prob = hidden_dropout_prob
     self.attention_probs_dropout_prob = attention_probs_dropout_prob
     self.initializer_range = initializer_range
@@ -861,7 +865,7 @@ class Transformer(tf.keras.layers.Layer):
     super(Transformer, self).build(unused_input_shapes)
 
   def __call__(self, input_tensor, attention_mask=None, **kwargs):
-    inputs = pack_inputs([input_tensor, attention_mask])
+    inputs = tf_utils.pack_inputs([input_tensor, attention_mask])
     return super(Transformer, self).__call__(inputs=inputs, **kwargs)
 
   def call(self, inputs, return_all_layers=False):
@@ -874,7 +878,7 @@ class Transformer(tf.keras.layers.Layer):
     Returns:
       Output tensor of the last layer or a list of output tensors.
     """
-    unpacked_inputs = unpack_inputs(inputs)
+    unpacked_inputs = tf_utils.unpack_inputs(inputs)
     input_tensor = unpacked_inputs[0]
     attention_mask = unpacked_inputs[1]
     output_tensor = input_tensor
@@ -890,108 +894,6 @@ class Transformer(tf.keras.layers.Layer):
     return all_layer_outputs[-1]
 
 
-def pack_inputs(inputs):
-  """Pack a list of `inputs` tensors to a tuple.
-
-  Args:
-    inputs: a list of tensors.
-
-  Returns:
-    a tuple of tensors. if any input is None, replace it with a special constant
-    tensor.
-  """
-  inputs = tf.nest.flatten(inputs)
-  outputs = []
-  for x in inputs:
-    if x is None:
-      outputs.append(tf.constant(0, shape=[], dtype=tf.int32))
-    else:
-      outputs.append(x)
-  return tuple(outputs)
-
-
-def unpack_inputs(inputs):
-  """unpack a tuple of `inputs` tensors to a tuple.
-
-  Args:
-    inputs: a list of tensors.
-
-  Returns:
-    a tuple of tensors. if any input is a special constant tensor, replace it
-    with None.
-  """
-  inputs = tf.nest.flatten(inputs)
-  outputs = []
-  for x in inputs:
-    if is_special_none_tensor(x):
-      outputs.append(None)
-    else:
-      outputs.append(x)
-  x = tuple(outputs)
-
-  # To trick the very pointless 'unbalanced-tuple-unpacking' pylint check
-  # from triggering.
-  if len(x) == 1:
-    return x[0]
-  return tuple(outputs)
-
-
-def is_special_none_tensor(tensor):
-  """Checks if a tensor is a special None Tensor."""
-  return tensor.shape.ndims == 0 and tensor.dtype == tf.int32
-
-
-def gelu(x):
-  """Gaussian Error Linear Unit.
-
-  This is a smoother version of the RELU.
-  Original paper: https://arxiv.org/abs/1606.08415
-  Args:
-    x: float Tensor to perform activation.
-
-  Returns:
-    `x` with the GELU activation applied.
-  """
-  cdf = 0.5 * (1.0 + tf.tanh(
-      (math.sqrt(2 / math.pi) * (x + 0.044715 * tf.pow(x, 3)))))
-  return x * cdf
-
-
-def get_activation(identifier):
-  """Maps a string to a Python function, e.g., "relu" => `tf.nn.relu`.
-
-  Args:
-    identifier: String name of the activation function.
-
-  Returns:
-    A Python function corresponding to the activation function. If
-    `identifier` is None, empty, or "linear", this will return None.
-    If `identifier` is not a string, it will return `identifier`.
-
-  Raises:
-    ValueError: The `identifier` does not correspond to a known
-      activation.
-  """
-  if identifier is None:
-    return None
-  elif isinstance(identifier, six.string_types):
-    name_to_fn = {
-        "linear": None,
-        "relu": tf.nn.relu,
-        "gelu": gelu,
-        "tanh": tf.nn.tanh,
-    }
-    identifier = str(identifier).lower()
-    if identifier not in name_to_fn:
-      raise ValueError("Unsupported activation function: %s" % (identifier))
-    return name_to_fn[identifier]
-  elif callable(identifier):
-    return identifier
-  else:
-    raise ValueError("Could not interpret activation "
-                     "function identifier: %s" % (identifier))
-
-
 def get_initializer(initializer_range=0.02):
   """Creates a `tf.initializers.truncated_normal` with the given range.
 
@@ -1004,66 +906,6 @@ def get_initializer(initializer_range=0.02):
   return tf.keras.initializers.TruncatedNormal(stddev=initializer_range)
 
 
-def get_shape_list(tensor, expected_rank=None, name=None):
-  """Returns a list of the shape of tensor, preferring static dimensions.
-
-  Args:
-    tensor: A tf.Tensor object to find the shape of.
-    expected_rank: (optional) int. The expected rank of `tensor`. If this is
-      specified and the `tensor` has a different rank, and exception will be
-      thrown.
-    name: Optional name of the tensor for the error message.
-
-  Returns:
-    A list of dimensions of the shape of tensor. All static dimensions will
-    be returned as python integers, and dynamic dimensions will be returned
-    as tf.Tensor scalars.
-  """
-  if expected_rank is not None:
-    assert_rank(tensor, expected_rank, name)
-
-  shape = tensor.shape.as_list()
-
-  non_static_indexes = []
-  for (index, dim) in enumerate(shape):
-    if dim is None:
-      non_static_indexes.append(index)
-
-  if not non_static_indexes:
-    return shape
-
-  dyn_shape = tf.shape(tensor)
-  for index in non_static_indexes:
-    shape[index] = dyn_shape[index]
-  return shape
-
-
-def assert_rank(tensor, expected_rank, name=None):
-  """Raises an exception if the tensor rank is not of the expected rank.
-
-  Args:
-    tensor: A tf.Tensor to check the rank of.
-    expected_rank: Python integer or list of integers, expected rank.
-    name: Optional name of the tensor for the error message.
-
-  Raises:
-    ValueError: If the expected shape doesn't match the actual shape.
-  """
-  expected_rank_dict = {}
-  if isinstance(expected_rank, six.integer_types):
-    expected_rank_dict[expected_rank] = True
-  else:
-    for x in expected_rank:
-      expected_rank_dict[x] = True
-
-  actual_rank = tensor.shape.ndims
-  if actual_rank not in expected_rank_dict:
-    raise ValueError(
-        "For the tensor `%s`, the actual tensor rank `%d` (shape = %s) is not "
-        "equal to the expected tensor rank `%s`" %
-        (name, actual_rank, str(tensor.shape), str(expected_rank)))
-
-
 def create_attention_mask_from_input_mask(from_tensor, to_mask):
   """Create 3D attention mask from a 2D tensor mask.
 
@@ -1074,11 +916,11 @@ def create_attention_mask_from_input_mask(from_tensor, to_mask):
   Returns:
     float Tensor of shape [batch_size, from_seq_length, to_seq_length].
   """
-  from_shape = get_shape_list(from_tensor, expected_rank=[2, 3])
+  from_shape = tf_utils.get_shape_list(from_tensor, expected_rank=[2, 3])
   batch_size = from_shape[0]
   from_seq_length = from_shape[1]
 
-  to_shape = get_shape_list(to_mask, expected_rank=2)
+  to_shape = tf_utils.get_shape_list(to_mask, expected_rank=2)
   to_seq_length = to_shape[1]
 
   to_mask = tf.cast(
diff --git a/official/bert/bert_models.py b/official/nlp/bert_models.py
similarity index 97%
rename from official/bert/bert_models.py
rename to official/nlp/bert_models.py
index 7c72fcc8..dba47dc0 100644
--- a/official/bert/bert_models.py
+++ b/official/nlp/bert_models.py
@@ -21,7 +21,8 @@ from __future__ import print_function
 import copy
 import tensorflow as tf
 
-from official.bert import modeling
+from official.modeling import tf_utils
+from official.nlp import bert_modeling as modeling
 
 
 def gather_indexes(sequence_tensor, positions):
@@ -40,7 +41,7 @@ def gather_indexes(sequence_tensor, positions):
       Masked out sequence tensor of shape (batch_size * max_predictions_per_seq,
       num_hidden).
   """
-  sequence_shape = modeling.get_shape_list(
+  sequence_shape = tf_utils.get_shape_list(
       sequence_tensor, name='sequence_output_tensor')
   batch_size = sequence_shape[0]
   seq_length = sequence_shape[1]
@@ -92,7 +93,7 @@ class BertPretrainLayer(tf.keras.layers.Layer):
         initializer=tf.keras.initializers.Zeros())
     self.lm_dense = tf.keras.layers.Dense(
         self.config.hidden_size,
-        activation=modeling.get_activation(self.config.hidden_act),
+        activation=tf_utils.get_activation(self.config.hidden_act),
         kernel_initializer=self.initializer,
         name='predictions/transform/dense')
     self.lm_layer_norm = tf.keras.layers.LayerNormalization(
@@ -115,13 +116,13 @@ class BertPretrainLayer(tf.keras.layers.Layer):
                pooled_output,
                sequence_output=None,
                masked_lm_positions=None):
-    inputs = modeling.pack_inputs(
+    inputs = tf_utils.pack_inputs(
         [pooled_output, sequence_output, masked_lm_positions])
     return super(BertPretrainLayer, self).__call__(inputs)
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    unpacked_inputs = modeling.unpack_inputs(inputs)
+    unpacked_inputs = tf_utils.unpack_inputs(inputs)
     pooled_output = unpacked_inputs[0]
     sequence_output = unpacked_inputs[1]
     masked_lm_positions = unpacked_inputs[2]
@@ -153,7 +154,7 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
                lm_label_ids=None,
                lm_label_weights=None,
                sentence_labels=None):
-    inputs = modeling.pack_inputs([
+    inputs = tf_utils.pack_inputs([
         lm_output, sentence_output, lm_label_ids, lm_label_weights,
         sentence_labels
     ])
@@ -186,7 +187,7 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
 
   def call(self, inputs):
     """Implements call() for the layer."""
-    unpacked_inputs = modeling.unpack_inputs(inputs)
+    unpacked_inputs = tf_utils.unpack_inputs(inputs)
     lm_output = unpacked_inputs[0]
     sentence_output = unpacked_inputs[1]
     lm_label_ids = unpacked_inputs[2]
diff --git a/official/bert/optimization.py b/official/nlp/optimization.py
similarity index 100%
rename from official/bert/optimization.py
rename to official/nlp/optimization.py
diff --git a/official/nlp/xlnet/optimization.py b/official/nlp/xlnet/optimization.py
index 53da5e17..0d903164 100644
--- a/official/nlp/xlnet/optimization.py
+++ b/official/nlp/xlnet/optimization.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 from absl import logging
 import tensorflow as tf
-from official.bert.optimization import AdamWeightDecay
+from official.nlp import optimization
 
 
 class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):
@@ -86,7 +86,7 @@ def create_optimizer(init_lr,
     logging.info(
         "Using AdamWeightDecay with adam_epsilon=%.9f weight_decay_rate=%.3f",
         adam_epsilon, weight_decay_rate)
-    optimizer = AdamWeightDecay(
+    optimizer = optimization.AdamWeightDecay(
         learning_rate=learning_rate_fn,
         weight_decay_rate=weight_decay_rate,
         beta_1=0.9,
diff --git a/official/transformer/v2/attention_layer.py b/official/transformer/v2/attention_layer.py
index 94a92e22..3a712c95 100644
--- a/official/transformer/v2/attention_layer.py
+++ b/official/transformer/v2/attention_layer.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
-from official.bert import modeling as common_layer
+from official.nlp import bert_modeling as common_layer
 
 
 class Attention(tf.keras.layers.Layer):
