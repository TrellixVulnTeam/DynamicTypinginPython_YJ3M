commit b41ff7f1bfc7cca6727ce96d40e6400f66575286
Author: Neal Wu <neal@nealwu.com>
Date:   Mon Mar 13 19:06:48 2017 -0700

    Remove name arguments from tf.summary.scalar

diff --git a/slim/deployment/model_deploy.py b/slim/deployment/model_deploy.py
index 1ad69a46..4888d08a 100644
--- a/slim/deployment/model_deploy.py
+++ b/slim/deployment/model_deploy.py
@@ -232,11 +232,9 @@ def _gather_clone_loss(clone, num_clones, regularization_losses):
       sum_loss = tf.add_n(all_losses)
   # Add the summaries out of the clone device block.
   if clone_loss is not None:
-    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss,
-                      name='clone_loss')
+    tf.summary.scalar(clone.scope + '/clone_loss', clone_loss)
   if regularization_loss is not None:
-    tf.summary.scalar('regularization_loss', regularization_loss,
-                      name='regularization_loss')
+    tf.summary.scalar('regularization_loss', regularization_loss)
   return sum_loss
 
 
@@ -404,8 +402,7 @@ def deploy(config,
 
     if total_loss is not None:
       # Add total_loss to summary.
-      summaries.add(tf.summary.scalar('total_loss', total_loss,
-                                      name='total_loss'))
+      summaries.add(tf.summary.scalar('total_loss', total_loss))
 
     if summaries:
       # Merge all summaries together.
diff --git a/slim/train_image_classifier.py b/slim/train_image_classifier.py
index b2509e50..060cd6b9 100644
--- a/slim/train_image_classifier.py
+++ b/slim/train_image_classifier.py
@@ -517,8 +517,7 @@ def main(_):
     with tf.device(deploy_config.optimizer_device()):
       learning_rate = _configure_learning_rate(dataset.num_samples, global_step)
       optimizer = _configure_optimizer(learning_rate)
-      summaries.add(tf.summary.scalar('learning_rate', learning_rate,
-                                      name='learning_rate'))
+      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
 
     if FLAGS.sync_replicas:
       # If sync_replicas is enabled, the averaging will be done in the chief
@@ -543,8 +542,7 @@ def main(_):
         optimizer,
         var_list=variables_to_train)
     # Add total_loss to summary.
-    summaries.add(tf.summary.scalar('total_loss', total_loss,
-                                    name='total_loss'))
+    summaries.add(tf.summary.scalar('total_loss', total_loss))
 
     # Create gradient updates.
     grad_updates = optimizer.apply_gradients(clones_gradients,
diff --git a/street/python/vgsl_model.py b/street/python/vgsl_model.py
index d6215493..13d8bbe4 100644
--- a/street/python/vgsl_model.py
+++ b/street/python/vgsl_model.py
@@ -369,7 +369,7 @@ class VGSLImageModel(object):
     if self.mode == 'train':
       # Setup loss for training.
       self.loss = self._AddLossFunction(logits, height_in, out_dims, out_func)
-      tf.summary.scalar('loss', self.loss, name='loss')
+      tf.summary.scalar('loss', self.loss)
     elif out_dims == 0:
       # Be sure the labels match the output, even in eval mode.
       self.labels = tf.slice(self.labels, [0, 0], [-1, 1])
@@ -484,7 +484,7 @@ class VGSLImageModel(object):
       opt = tf.train.AdamOptimizer(learning_rate=learn_rate_dec)
     else:
       raise ValueError('Invalid optimizer type: ' + optimizer_type)
-    tf.summary.scalar('learn_rate', learn_rate_dec, name='lr_summ')
+    tf.summary.scalar('learn_rate', learn_rate_dec)
 
     self.train_op = opt.minimize(
         self.loss, global_step=self.global_step, name='train')
