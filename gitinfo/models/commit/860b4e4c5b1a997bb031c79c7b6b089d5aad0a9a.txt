commit 860b4e4c5b1a997bb031c79c7b6b089d5aad0a9a
Author: Jacob Buckman <buckman@google.com>
Date:   Tue Jun 26 23:01:11 2018 -0700

    Added new model, STEvE.

diff --git a/CODEOWNERS b/CODEOWNERS
index 660cd38d..2a28504c 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -41,6 +41,7 @@
 /research/seq2species/ @apbusia @depristo
 /research/skip_thoughts/ @cshallue
 /research/slim/ @sguada @nathansilberman
+/research/steve/ @buckman
 /research/street/ @theraysmith
 /research/swivel/ @waterson
 /research/syntaxnet/ @calberti @andorardo @bogatyy @markomernick
diff --git a/research/steve/README.md b/research/steve/README.md
new file mode 100644
index 00000000..eff98492
--- /dev/null
+++ b/research/steve/README.md
@@ -0,0 +1,90 @@
+# Stochastic Ensemble Value Expansion
+
+*A hybrid model-based/model-free reinforcement learning algorithm for sample-efficient continuous control.*
+
+This is the code repository accompanying the paper Sample-Efficient Reinforcement Learning with
+Stochastic Ensemble Value Expansion, by Buckman et al. (2018).
+
+#### Abstract:
+Merging model-free and model-based approaches in reinforcement learning has the potential to achieve
+the high performance of model-free algorithms with low sample complexity. This is difficult because
+an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently
+complex environments, the dynamics model will always be imperfect. As a result, a key challenge is to
+combine model-based approaches with model-free learning in such a way that errors in the model do not
+degrade performance. We propose *stochastic ensemble value expansion* (STEVE), a novel model-based
+technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon
+lengths for each individual example, STEVE ensures that the model is only utilized when doing so does not
+introduce significant errors. Our approach outperforms model-free baselines on challenging continuous
+control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous
+model-based approaches, performance does not degrade as the environment gets more complex.
+
+## Installation
+This code is compatible with Ubuntu 16.04 and Python 2.7. There are several prerequisites:
+*    Numpy, Scipy, and Portalocker: `pip install numpy scipy portalocker`
+*    TensorFlow 1.6 or above. Instructions can be found on the official TensorFlow page:
+     [https://www.tensorflow.org/install/install_linux](https://www.tensorflow.org/install/install_linux).
+     We suggest installing the GPU version of TensorFlow to speed up training.
+*    OpenAI Gym version 0.9.4. Instructions can be found in the OpenAI Gym repository:
+     [https://github.com/openai/gym#installation](https://github.com/openai/gym#installation).
+     Note that you need to replace "pip install gym[all]" with "pip install gym[all]==0.9.4", which
+     will ensure that you get the correct version of Gym. (The current version of Gym has deprecated
+     the -v1 MuJoCo environments, which are the environments studied in this paper.)
+*    MuJoCo version 1.31, which can be downloaded here: [https://www.roboti.us/download/mjpro131_linux.zip](https://www.roboti.us/download/mjpro131_linux.zip).
+     Simply run: ```
+     cd ~; mkdir -p .mujoco; cd .mujoco/; wget https://www.roboti.us/download/mjpro131_linux.zip; unzip mjpro131_linux.zip```
+     You also need to get a license, and put the license key in ~/.mujoco/ as well.
+*    Optionally, Roboschool version 1.1. This is needed only to replicate the Roboschool experiments.
+     Instructions can be found in the OpenAI Roboschool repository:
+     [https://github.com/openai/roboschool#installation](https://github.com/openai/roboschool#installation).
+*    Optionally, MoviePy to render trained agents. Instructions on the MoviePy homepage:
+     [https://zulko.github.io/moviepy/install.html](https://zulko.github.io/moviepy/install.html).
+
+## Running Experiments
+To run an experiment, run master.py and pass in a config file and GPU ID. For example: ```
+python master.py config/experiments/speedruns/humanoid/speedy_steve0.json 0```
+The `config/experiments/`
+directory contains configuration files for all of the experiments run in the paper.
+
+The GPU ID specifies the GPU that should be used to learn the policy. For model-based approaches, the
+next GPU (i.e. GPU_ID+1) is used to learn the worldmodel in parallel.
+
+To resume an experiment that was interrupted, use the same config file and pass the `--resume` flag: ```
+python master.py config/experiments/speedruns/humanoid/speedy_steve0.json 0 --resume```
+
+## Output
+For each experiment, two folders are created in the output directory: `<ENVIRONMENT>/<EXPERIMENT>/log`
+and `<ENVIRONMENT>/<EXPERIMENT>/checkpoints`. The log directory contains the following:
+
+*  `hps.json` contains the accumulated hyperparameters of the config file used to generate these results
+*  `valuerl.log` and `worldmodel.log` contain the log output of the learners. `worldmodel.log` will not
+   exist if you are not learning a worldmodel.
+*  `<EXPERIMENT>.greedy.csv` records all of the scores of our evaluators. The four columns contain time (hours),
+   epochs, frames, and score.
+
+The checkpoints directory contains the most recent versions of the policy and worldmodel, as well as checkpoints
+of the policy, worldmodel, and their respective replay buffers at various points throughout training.
+
+## Code Organization
+`master.py` launches four types of processes: a ValueRlLearner to learn the policy, a WorldmodelLearner
+to learn the dynamics model, several Interactors to gather data from the environment to train on, and
+a few Evaluators to run the greedy policy in the environment and record the score.
+
+`learner.py` contains a general framework for models which learn from a replay buffer. This is where
+most of the code for the overall training loop is located. `valuerl_learner.py` and `worldmodel_learner.py`
+contain a small amount of model-specific training loop code.
+
+`valuerl.py` implements the core model for all value-function-based policy learning techniques studied
+in the paper, including DDPG, MVE, STEVE, etc. Similarly, `worldmodel.py` contains the core model for
+our dynamics model and reward function.
+
+`replay.py` contains the code for the replay buffer. `nn.py`, `envwrap.py`, `config.py`, and `util.py`
+each contain various helper functions.
+
+`toy_demo.py` is a self-contained demo, written in numpy, that was used to generate the results for the
+toy examples in the first segment of the paper.
+
+`visualizer.py` is a utility script for loading trained policies and inspecting them. In addition to a
+config file and a GPU, it takes the filename of the model to load as a mandatory third argument.
+
+## Contact
+Please contact GitHub user buckman-google (jacobbuckman@gmail.com) with any questions.
\ No newline at end of file
diff --git a/research/steve/agent.py b/research/steve/agent.py
new file mode 100644
index 00000000..25069e29
--- /dev/null
+++ b/research/steve/agent.py
@@ -0,0 +1,143 @@
+from __future__ import print_function
+from builtins import zip
+from builtins import range
+from builtins import object
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import numpy as np
+import tensorflow as tf
+import time, os, traceback, multiprocessing, portalocker
+
+import envwrap
+import valuerl
+import util
+from config import config
+
+
+def run_env(pipe):
+  env = envwrap.get_env(config["env"]["name"])
+  reset = True
+  while True:
+    if reset is True: pipe.send(env.reset())
+    action = pipe.recv()
+    obs, reward, done, reset = env.step(action)
+    pipe.send((obs, reward, done, reset))
+
+class AgentManager(object):
+  """
+  Interact with the environment according to the learned policy,
+  """
+  def __init__(self, proc_num, evaluation, policy_lock, batch_size, config):
+    self.evaluation = evaluation
+    self.policy_lock = policy_lock
+    self.batch_size = batch_size
+    self.config = config
+
+    self.log_path =  util.create_directory("%s/%s/%s/%s" % (config["output_root"], config["env"]["name"], config["name"], config["log_path"])) + "/%s" % config["name"]
+    self.load_path = util.create_directory("%s/%s/%s/%s" % (config["output_root"], config["env"]["name"], config["name"], config["save_model_path"]))
+
+    ## placeholders for intermediate states (basis for rollout)
+    self.obs_loader = tf.placeholder(tf.float32, [self.batch_size, np.prod(self.config["env"]["obs_dims"])])
+
+    ## build model
+    self.valuerl =  valuerl.ValueRL(self.config["name"], self.config["env"], self.config["policy_config"])
+    self.policy_actions = self.valuerl.build_evalution_graph(self.obs_loader, mode="exploit" if self.evaluation else "explore")
+
+    # interactors
+    self.agent_pipes, self.agent_child_pipes = list(zip(*[multiprocessing.Pipe() for _ in range(self.batch_size)]))
+    self.agents = [multiprocessing.Process(target=run_env, args=(self.agent_child_pipes[i],)) for i in range(self.batch_size)]
+    for agent in self.agents: agent.start()
+    self.obs = [pipe.recv() for pipe in self.agent_pipes]
+    self.total_rewards = [0. for _ in self.agent_pipes]
+    self.loaded_policy = False
+
+    self.sess = tf.Session()
+    self.sess.run(tf.global_variables_initializer())
+
+    self.rollout_i = 0
+    self.proc_num = proc_num
+    self.epoch = -1
+    self.frame_total = 0
+    self.hours = 0.
+
+    self.first = True
+
+  def get_action(self, obs):
+    if self.loaded_policy:
+      all_actions = self.sess.run(self.policy_actions, feed_dict={self.obs_loader: obs})
+      all_actions = np.clip(all_actions, -1., 1.)
+      return all_actions[:self.batch_size]
+    else:
+      return [self.get_random_action() for _ in range(obs.shape[0])]
+
+  def get_random_action(self, *args, **kwargs):
+    return np.random.random(self.config["env"]["action_dim"]) * 2 - 1
+
+  def step(self):
+    actions = self.get_action(np.stack(self.obs))
+    self.first = False
+    [pipe.send(action) for pipe, action in zip(self.agent_pipes, actions)]
+    next_obs, rewards, dones, resets = list(zip(*[pipe.recv() for pipe in self.agent_pipes]))
+
+    frames = list(zip(self.obs, next_obs, actions, rewards, dones))
+
+    self.obs = [o if resets[i] is False else self.agent_pipes[i].recv() for i, o in enumerate(next_obs)]
+
+    for i, (t,r,reset) in enumerate(zip(self.total_rewards, rewards, resets)):
+      if reset:
+        self.total_rewards[i] = 0.
+        if self.evaluation and self.loaded_policy:
+          with portalocker.Lock(self.log_path+'.greedy.csv', mode="a") as f: f.write("%2f,%d,%d,%2f\n" % (self.hours, self.epoch, self.frame_total, t+r))
+
+      else:
+        self.total_rewards[i] = t + r
+
+    if self.evaluation and np.any(resets): self.reload()
+
+    self.rollout_i += 1
+    return frames
+
+  def reload(self):
+    if not os.path.exists("%s/%s.params.index" % (self.load_path ,self.valuerl.saveid)): return False
+    with self.policy_lock:
+      self.valuerl.load(self.sess, self.load_path)
+      self.epoch, self.frame_total, self.hours = self.sess.run([self.valuerl.epoch_n, self.valuerl.frame_n, self.valuerl.hours])
+    self.loaded_policy = True
+    self.first = True
+    return True
+
+def main(proc_num, evaluation, policy_replay_frame_queue, model_replay_frame_queue, policy_lock, config):
+  try:
+    np.random.seed((proc_num * int(time.time())) % (2 ** 32 - 1))
+    agentmanager = AgentManager(proc_num, evaluation, policy_lock, config["evaluator_config"]["batch_size"] if evaluation else config["agent_config"]["batch_size"], config)
+    frame_i = 0
+    while True:
+      new_frames = agentmanager.step()
+      if not evaluation:
+        policy_replay_frame_queue.put(new_frames)
+        if model_replay_frame_queue is not None: model_replay_frame_queue.put(new_frames)
+        if frame_i % config["agent_config"]["reload_every_n"] == 0: agentmanager.reload()
+        frame_i += len(new_frames)
+
+  except Exception as e:
+    print('Caught exception in agent process %d' % proc_num)
+    traceback.print_exc()
+    print()
+    try:
+      for i in agentmanager.agents: i.join()
+    except:
+      pass
+    raise e
diff --git a/research/steve/config.py b/research/steve/config.py
new file mode 100644
index 00000000..4a6da98c
--- /dev/null
+++ b/research/steve/config.py
@@ -0,0 +1,38 @@
+from __future__ import print_function
+from builtins import str
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import argparse, json, util, traceback
+
+parser = argparse.ArgumentParser()
+parser.add_argument("config")
+parser.add_argument("root_gpu", type=int)
+parser.add_argument("--resume", action="store_true")
+args = parser.parse_args()
+
+config_loc = args.config
+config = util.ConfigDict(config_loc)
+
+config["name"] = config_loc.split("/")[-1][:-5]
+config["resume"] = args.resume
+
+cstr = str(config)
+
+def log_config():
+  HPS_PATH = util.create_directory("output/" + config["env"]["name"] + "/" + config["name"] + "/" + config["log_path"]) + "/hps.json"
+  print("ROOT GPU: " + str(args.root_gpu) + "\n" + str(cstr))
+  with open(HPS_PATH, "w") as f:
+    f.write("ROOT GPU: " + str(args.root_gpu) + "\n" + str(cstr))
\ No newline at end of file
diff --git a/research/steve/config/algos/ddpg.json b/research/steve/config/algos/ddpg.json
new file mode 100644
index 00000000..e76c1069
--- /dev/null
+++ b/research/steve/config/algos/ddpg.json
@@ -0,0 +1,3 @@
+{
+  "inherits": ["config/core/basic.json"]
+}
\ No newline at end of file
diff --git a/research/steve/config/algos/mve_mean.json b/research/steve/config/algos/mve_mean.json
new file mode 100644
index 00000000..729bccc6
--- /dev/null
+++ b/research/steve/config/algos/mve_mean.json
@@ -0,0 +1,14 @@
+{
+  "inherits": [
+    "config/core/basic.json",
+    "config/core/model.json"
+  ],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 3,
+        "mean_k_return": true
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/algos/mve_tdk.json b/research/steve/config/algos/mve_tdk.json
new file mode 100644
index 00000000..222fd40c
--- /dev/null
+++ b/research/steve/config/algos/mve_tdk.json
@@ -0,0 +1,14 @@
+{
+  "inherits": [
+    "config/core/basic.json",
+    "config/core/model.json"
+  ],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 3,
+        "tdk_trick": true
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/algos/mve_tdlambda.json b/research/steve/config/algos/mve_tdlambda.json
new file mode 100644
index 00000000..3414dda5
--- /dev/null
+++ b/research/steve/config/algos/mve_tdlambda.json
@@ -0,0 +1,14 @@
+{
+  "inherits": [
+    "config/core/basic.json",
+    "config/core/model.json"
+  ],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 3,
+        "lambda_return": 0.25
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/algos/steve.json b/research/steve/config/algos/steve.json
new file mode 100644
index 00000000..ca2bc039
--- /dev/null
+++ b/research/steve/config/algos/steve.json
@@ -0,0 +1,15 @@
+{
+  "inherits": [
+    "config/core/basic.json",
+    "config/core/model.json",
+    "config/core/bayesian.json"
+              ],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 3,
+        "steve_reweight": true
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/algos/steve_cov.json b/research/steve/config/algos/steve_cov.json
new file mode 100644
index 00000000..4dbf46e1
--- /dev/null
+++ b/research/steve/config/algos/steve_cov.json
@@ -0,0 +1,16 @@
+{
+  "inherits": [
+    "config/core/basic.json",
+    "config/core/model.json",
+    "config/core/bayesian.json"
+              ],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 3,
+        "steve_reweight": true,
+        "covariances": true
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/core/basic.json b/research/steve/config/core/basic.json
new file mode 100644
index 00000000..411e7b65
--- /dev/null
+++ b/research/steve/config/core/basic.json
@@ -0,0 +1,32 @@
+{
+  "updates": {
+    "output_root": "output",
+    "save_model_path": "checkpoints",
+    "log_path": "log",
+
+    "agent_config": {
+      "count": 1,
+      "batch_size": 8,
+      "reload_every_n": 1,
+      "full_random_n": 10000
+    },
+
+    "evaluator_config": {
+      "count": 2,
+      "batch_size": 1
+    },
+
+    "policy_config": {
+      "algo": "ddpg",
+      "hidden_dim": 128,
+      "explore_chance": 0.05,
+      "batch_size": 512,
+      "replay_size": 1000000,
+      "frames_before_learning": 10000,
+      "log_every_n": 500,
+      "epoch_every_n": 500,
+      "backup_every_n": 2500000,
+      "frames_per_update": 0.25
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/core/bayesian.json b/research/steve/config/core/bayesian.json
new file mode 100644
index 00000000..ea7d9554
--- /dev/null
+++ b/research/steve/config/core/bayesian.json
@@ -0,0 +1,26 @@
+{
+  "updates": {
+    "policy_config": {
+      "bayesian": {
+        "ensemble_size": 4,
+        "train_sample_count": 4,
+        "eval_sample_count": 4
+      }
+    },
+
+    "*model_config": {
+      "bayesian": {
+        "transition": {
+          "ensemble_size": 4,
+          "train_sample_count": 4,
+          "eval_sample_count": 4
+        },
+        "reward": {
+          "ensemble_size": 4,
+          "train_sample_count": 4,
+          "eval_sample_count": 4
+        }
+      }
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/core/model.json b/research/steve/config/core/model.json
new file mode 100644
index 00000000..485146ab
--- /dev/null
+++ b/research/steve/config/core/model.json
@@ -0,0 +1,16 @@
+{
+  "updates": {
+    "model_config": {
+      "transition_hidden_dim": 512,
+      "aux_hidden_dim": 128,
+      "batch_size": 512,
+      "replay_size": 1000000,
+      "frames_before_learning": 10000,
+      "log_every_n": 500,
+      "epoch_every_n": 500,
+      "backup_every_n": 2500000,
+      "pretrain_n": 10000,
+      "frames_per_update": 0.25
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/flagrun.json b/research/steve/config/envs/flagrun.json
new file mode 100644
index 00000000..09ecc7cd
--- /dev/null
+++ b/research/steve/config/envs/flagrun.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "RoboschoolHumanoidFlagrun-v1",
+      "obs_dims": [44],
+      "action_dim": 17,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/halfcheetah.json b/research/steve/config/envs/halfcheetah.json
new file mode 100644
index 00000000..e0c9b389
--- /dev/null
+++ b/research/steve/config/envs/halfcheetah.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "HalfCheetah-v1",
+      "obs_dims": [17],
+      "action_dim": 6,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/hardcore.json b/research/steve/config/envs/hardcore.json
new file mode 100644
index 00000000..af372b28
--- /dev/null
+++ b/research/steve/config/envs/hardcore.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "BipedalWalkerHardcore-v2",
+      "obs_dims": [24],
+      "action_dim": 4,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/hopper.json b/research/steve/config/envs/hopper.json
new file mode 100644
index 00000000..012def18
--- /dev/null
+++ b/research/steve/config/envs/hopper.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "Hopper-v1",
+      "obs_dims": [11],
+      "action_dim": 3,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/humanoid.json b/research/steve/config/envs/humanoid.json
new file mode 100644
index 00000000..39aeeb29
--- /dev/null
+++ b/research/steve/config/envs/humanoid.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "Humanoid-v1",
+      "obs_dims": [376],
+      "action_dim": 17,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/rshum.json b/research/steve/config/envs/rshum.json
new file mode 100644
index 00000000..0ad54b2b
--- /dev/null
+++ b/research/steve/config/envs/rshum.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "RoboschoolHumanoid-v1",
+      "obs_dims": [44],
+      "action_dim": 17,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/swimmer.json b/research/steve/config/envs/swimmer.json
new file mode 100644
index 00000000..0fcf2f32
--- /dev/null
+++ b/research/steve/config/envs/swimmer.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "Swimmer-v1",
+      "obs_dims": [8],
+      "action_dim": 2,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/envs/walker2d.json b/research/steve/config/envs/walker2d.json
new file mode 100644
index 00000000..03ed94f7
--- /dev/null
+++ b/research/steve/config/envs/walker2d.json
@@ -0,0 +1,12 @@
+{
+  "updates": {
+    "env": {
+      "name": "Walker2d-v1",
+      "obs_dims": [17],
+      "action_dim": 6,
+      "reward_scale":1.0,
+      "discount":0.99,
+      "max_frames": 1000
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/experimental_setups/speedrun.json b/research/steve/config/experimental_setups/speedrun.json
new file mode 100644
index 00000000..b34a9b70
--- /dev/null
+++ b/research/steve/config/experimental_setups/speedrun.json
@@ -0,0 +1,11 @@
+{
+  "updates": {
+    "policy_config": {
+      "frames_per_update": false
+    },
+    "*model_config":{
+      "frames_per_update": false,
+      "pretrain_n": false
+    }
+  }
+}
\ No newline at end of file
diff --git a/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk0.json b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk0.json
new file mode 100644
index 00000000..da54f631
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/core/bayesian", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk1.json b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk1.json
new file mode 100644
index 00000000..da54f631
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/core/bayesian", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk2.json b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk2.json
new file mode 100644
index 00000000..da54f631
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/ensemble_mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/core/bayesian", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_25tdlambda0.json b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda0.json
new file mode 100644
index 00000000..b9e3dcd4
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda0.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.25
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_25tdlambda1.json b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda1.json
new file mode 100644
index 00000000..b9e3dcd4
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda1.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.25
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_25tdlambda2.json b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda2.json
new file mode 100644
index 00000000..b9e3dcd4
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_25tdlambda2.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.25
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_75tdlambda0.json b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda0.json
new file mode 100644
index 00000000..7366ba77
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda0.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.75
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_75tdlambda1.json b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda1.json
new file mode 100644
index 00000000..7366ba77
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda1.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.75
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_75tdlambda2.json b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda2.json
new file mode 100644
index 00000000..7366ba77
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_75tdlambda2.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/mve_tdlambda.json", "config/envs/humanoid.json"],
+  "updates":{
+    "policy_config": {
+      "value_expansion": {
+        "lambda_return": 0.75
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_meank0.json b/research/steve/config/experiments/ablations/baselines/mve_meank0.json
new file mode 100644
index 00000000..ce7d9b1e
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_meank0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_mean.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_meank1.json b/research/steve/config/experiments/ablations/baselines/mve_meank1.json
new file mode 100644
index 00000000..ce7d9b1e
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_meank1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_mean.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/mve_meank2.json b/research/steve/config/experiments/ablations/baselines/mve_meank2.json
new file mode 100644
index 00000000..ce7d9b1e
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/mve_meank2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_mean.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/steve_cov0.json b/research/steve/config/experiments/ablations/baselines/steve_cov0.json
new file mode 100644
index 00000000..df2e8a0d
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/steve_cov0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve_cov.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/steve_cov1.json b/research/steve/config/experiments/ablations/baselines/steve_cov1.json
new file mode 100644
index 00000000..df2e8a0d
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/steve_cov1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve_cov.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/baselines/steve_cov2.json b/research/steve/config/experiments/ablations/baselines/steve_cov2.json
new file mode 100644
index 00000000..df2e8a0d
--- /dev/null
+++ b/research/steve/config/experiments/ablations/baselines/steve_cov2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve_cov.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_1h0.json b/research/steve/config/experiments/ablations/horizons/steve_1h0.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_1h0.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_1h1.json b/research/steve/config/experiments/ablations/horizons/steve_1h1.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_1h1.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_1h2.json b/research/steve/config/experiments/ablations/horizons/steve_1h2.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_1h2.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_2h0.json b/research/steve/config/experiments/ablations/horizons/steve_2h0.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_2h0.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_2h1.json b/research/steve/config/experiments/ablations/horizons/steve_2h1.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_2h1.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_2h2.json b/research/steve/config/experiments/ablations/horizons/steve_2h2.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_2h2.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_5h0.json b/research/steve/config/experiments/ablations/horizons/steve_5h0.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_5h0.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_5h1.json b/research/steve/config/experiments/ablations/horizons/steve_5h1.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_5h1.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/ablations/horizons/steve_5h2.json b/research/steve/config/experiments/ablations/horizons/steve_5h2.json
new file mode 100644
index 00000000..48b6730b
--- /dev/null
+++ b/research/steve/config/experiments/ablations/horizons/steve_5h2.json
@@ -0,0 +1,10 @@
+{
+  "inherits": ["config/algos/steve.json", "config/envs/humanoid.json"],
+  "updates": {
+    "policy_config": {
+      "value_expansion": {
+        "rollout_len": 1
+      }
+    }
+  }
+}
diff --git a/research/steve/config/experiments/goodruns/flagrun/ddpg0.json b/research/steve/config/experiments/goodruns/flagrun/ddpg0.json
new file mode 100644
index 00000000..a68ee412
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/ddpg1.json b/research/steve/config/experiments/goodruns/flagrun/ddpg1.json
new file mode 100644
index 00000000..a68ee412
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/ddpg2.json b/research/steve/config/experiments/goodruns/flagrun/ddpg2.json
new file mode 100644
index 00000000..a68ee412
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/ddpg3.json b/research/steve/config/experiments/goodruns/flagrun/ddpg3.json
new file mode 100644
index 00000000..a68ee412
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/mve_tdk0.json b/research/steve/config/experiments/goodruns/flagrun/mve_tdk0.json
new file mode 100644
index 00000000..8da85dd3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/mve_tdk1.json b/research/steve/config/experiments/goodruns/flagrun/mve_tdk1.json
new file mode 100644
index 00000000..8da85dd3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/mve_tdk2.json b/research/steve/config/experiments/goodruns/flagrun/mve_tdk2.json
new file mode 100644
index 00000000..8da85dd3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/mve_tdk3.json b/research/steve/config/experiments/goodruns/flagrun/mve_tdk3.json
new file mode 100644
index 00000000..8da85dd3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/steve0.json b/research/steve/config/experiments/goodruns/flagrun/steve0.json
new file mode 100644
index 00000000..21d32930
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/steve1.json b/research/steve/config/experiments/goodruns/flagrun/steve1.json
new file mode 100644
index 00000000..21d32930
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/steve2.json b/research/steve/config/experiments/goodruns/flagrun/steve2.json
new file mode 100644
index 00000000..21d32930
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/flagrun/steve3.json b/research/steve/config/experiments/goodruns/flagrun/steve3.json
new file mode 100644
index 00000000..21d32930
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/flagrun/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/ddpg0.json b/research/steve/config/experiments/goodruns/halfcheetah/ddpg0.json
new file mode 100644
index 00000000..fc9d9eef
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/ddpg1.json b/research/steve/config/experiments/goodruns/halfcheetah/ddpg1.json
new file mode 100644
index 00000000..fc9d9eef
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/ddpg2.json b/research/steve/config/experiments/goodruns/halfcheetah/ddpg2.json
new file mode 100644
index 00000000..fc9d9eef
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/ddpg3.json b/research/steve/config/experiments/goodruns/halfcheetah/ddpg3.json
new file mode 100644
index 00000000..fc9d9eef
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk0.json b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk0.json
new file mode 100644
index 00000000..dcae7eb4
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk1.json b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk1.json
new file mode 100644
index 00000000..dcae7eb4
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk2.json b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk2.json
new file mode 100644
index 00000000..dcae7eb4
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk3.json b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk3.json
new file mode 100644
index 00000000..dcae7eb4
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/steve0.json b/research/steve/config/experiments/goodruns/halfcheetah/steve0.json
new file mode 100644
index 00000000..f2fd36d3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/steve1.json b/research/steve/config/experiments/goodruns/halfcheetah/steve1.json
new file mode 100644
index 00000000..f2fd36d3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/steve2.json b/research/steve/config/experiments/goodruns/halfcheetah/steve2.json
new file mode 100644
index 00000000..f2fd36d3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/halfcheetah/steve3.json b/research/steve/config/experiments/goodruns/halfcheetah/steve3.json
new file mode 100644
index 00000000..f2fd36d3
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/halfcheetah/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/halfcheetah.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/ddpg0.json b/research/steve/config/experiments/goodruns/hardcore/ddpg0.json
new file mode 100644
index 00000000..3dce87b1
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/ddpg1.json b/research/steve/config/experiments/goodruns/hardcore/ddpg1.json
new file mode 100644
index 00000000..3dce87b1
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/ddpg2.json b/research/steve/config/experiments/goodruns/hardcore/ddpg2.json
new file mode 100644
index 00000000..3dce87b1
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/ddpg3.json b/research/steve/config/experiments/goodruns/hardcore/ddpg3.json
new file mode 100644
index 00000000..3dce87b1
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/mve_tdk0.json b/research/steve/config/experiments/goodruns/hardcore/mve_tdk0.json
new file mode 100644
index 00000000..095d8763
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/mve_tdk1.json b/research/steve/config/experiments/goodruns/hardcore/mve_tdk1.json
new file mode 100644
index 00000000..095d8763
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/mve_tdk2.json b/research/steve/config/experiments/goodruns/hardcore/mve_tdk2.json
new file mode 100644
index 00000000..095d8763
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/mve_tdk3.json b/research/steve/config/experiments/goodruns/hardcore/mve_tdk3.json
new file mode 100644
index 00000000..095d8763
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/steve0.json b/research/steve/config/experiments/goodruns/hardcore/steve0.json
new file mode 100644
index 00000000..f0942085
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/steve1.json b/research/steve/config/experiments/goodruns/hardcore/steve1.json
new file mode 100644
index 00000000..f0942085
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/steve2.json b/research/steve/config/experiments/goodruns/hardcore/steve2.json
new file mode 100644
index 00000000..f0942085
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hardcore/steve3.json b/research/steve/config/experiments/goodruns/hardcore/steve3.json
new file mode 100644
index 00000000..f0942085
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hardcore/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hardcore.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/ddpg0.json b/research/steve/config/experiments/goodruns/hopper/ddpg0.json
new file mode 100644
index 00000000..4916ab11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/ddpg1.json b/research/steve/config/experiments/goodruns/hopper/ddpg1.json
new file mode 100644
index 00000000..4916ab11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/ddpg2.json b/research/steve/config/experiments/goodruns/hopper/ddpg2.json
new file mode 100644
index 00000000..4916ab11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/ddpg3.json b/research/steve/config/experiments/goodruns/hopper/ddpg3.json
new file mode 100644
index 00000000..4916ab11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/mve_tdk0.json b/research/steve/config/experiments/goodruns/hopper/mve_tdk0.json
new file mode 100644
index 00000000..40663e8b
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/mve_tdk1.json b/research/steve/config/experiments/goodruns/hopper/mve_tdk1.json
new file mode 100644
index 00000000..40663e8b
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/mve_tdk2.json b/research/steve/config/experiments/goodruns/hopper/mve_tdk2.json
new file mode 100644
index 00000000..40663e8b
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/mve_tdk3.json b/research/steve/config/experiments/goodruns/hopper/mve_tdk3.json
new file mode 100644
index 00000000..40663e8b
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/steve0.json b/research/steve/config/experiments/goodruns/hopper/steve0.json
new file mode 100644
index 00000000..708ce891
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/steve1.json b/research/steve/config/experiments/goodruns/hopper/steve1.json
new file mode 100644
index 00000000..708ce891
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/steve2.json b/research/steve/config/experiments/goodruns/hopper/steve2.json
new file mode 100644
index 00000000..708ce891
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/hopper/steve3.json b/research/steve/config/experiments/goodruns/hopper/steve3.json
new file mode 100644
index 00000000..708ce891
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/hopper/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/hopper.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/ddpg0.json b/research/steve/config/experiments/goodruns/humanoid/ddpg0.json
new file mode 100644
index 00000000..3bd27e7d
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/ddpg1.json b/research/steve/config/experiments/goodruns/humanoid/ddpg1.json
new file mode 100644
index 00000000..3bd27e7d
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/ddpg2.json b/research/steve/config/experiments/goodruns/humanoid/ddpg2.json
new file mode 100644
index 00000000..3bd27e7d
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/ddpg3.json b/research/steve/config/experiments/goodruns/humanoid/ddpg3.json
new file mode 100644
index 00000000..3bd27e7d
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/mve_tdk0.json b/research/steve/config/experiments/goodruns/humanoid/mve_tdk0.json
new file mode 100644
index 00000000..542ed8d8
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/mve_tdk1.json b/research/steve/config/experiments/goodruns/humanoid/mve_tdk1.json
new file mode 100644
index 00000000..542ed8d8
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/mve_tdk2.json b/research/steve/config/experiments/goodruns/humanoid/mve_tdk2.json
new file mode 100644
index 00000000..542ed8d8
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/mve_tdk3.json b/research/steve/config/experiments/goodruns/humanoid/mve_tdk3.json
new file mode 100644
index 00000000..542ed8d8
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/steve0.json b/research/steve/config/experiments/goodruns/humanoid/steve0.json
new file mode 100644
index 00000000..835b3f62
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/steve1.json b/research/steve/config/experiments/goodruns/humanoid/steve1.json
new file mode 100644
index 00000000..835b3f62
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/steve2.json b/research/steve/config/experiments/goodruns/humanoid/steve2.json
new file mode 100644
index 00000000..835b3f62
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/humanoid/steve3.json b/research/steve/config/experiments/goodruns/humanoid/steve3.json
new file mode 100644
index 00000000..835b3f62
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/humanoid/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/ddpg0.json b/research/steve/config/experiments/goodruns/rshum/ddpg0.json
new file mode 100644
index 00000000..9fd98d11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/ddpg1.json b/research/steve/config/experiments/goodruns/rshum/ddpg1.json
new file mode 100644
index 00000000..9fd98d11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/ddpg2.json b/research/steve/config/experiments/goodruns/rshum/ddpg2.json
new file mode 100644
index 00000000..9fd98d11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/ddpg3.json b/research/steve/config/experiments/goodruns/rshum/ddpg3.json
new file mode 100644
index 00000000..9fd98d11
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/mve_tdk0.json b/research/steve/config/experiments/goodruns/rshum/mve_tdk0.json
new file mode 100644
index 00000000..ade2434e
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/mve_tdk1.json b/research/steve/config/experiments/goodruns/rshum/mve_tdk1.json
new file mode 100644
index 00000000..ade2434e
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/mve_tdk2.json b/research/steve/config/experiments/goodruns/rshum/mve_tdk2.json
new file mode 100644
index 00000000..ade2434e
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/mve_tdk3.json b/research/steve/config/experiments/goodruns/rshum/mve_tdk3.json
new file mode 100644
index 00000000..ade2434e
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/steve0.json b/research/steve/config/experiments/goodruns/rshum/steve0.json
new file mode 100644
index 00000000..510854fb
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/steve1.json b/research/steve/config/experiments/goodruns/rshum/steve1.json
new file mode 100644
index 00000000..510854fb
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/steve2.json b/research/steve/config/experiments/goodruns/rshum/steve2.json
new file mode 100644
index 00000000..510854fb
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/rshum/steve3.json b/research/steve/config/experiments/goodruns/rshum/steve3.json
new file mode 100644
index 00000000..510854fb
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/rshum/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/rshum.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/ddpg0.json b/research/steve/config/experiments/goodruns/swimmer/ddpg0.json
new file mode 100644
index 00000000..a94fc7c5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/ddpg1.json b/research/steve/config/experiments/goodruns/swimmer/ddpg1.json
new file mode 100644
index 00000000..a94fc7c5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/ddpg2.json b/research/steve/config/experiments/goodruns/swimmer/ddpg2.json
new file mode 100644
index 00000000..a94fc7c5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/ddpg3.json b/research/steve/config/experiments/goodruns/swimmer/ddpg3.json
new file mode 100644
index 00000000..a94fc7c5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/mve_tdk0.json b/research/steve/config/experiments/goodruns/swimmer/mve_tdk0.json
new file mode 100644
index 00000000..14210117
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/mve_tdk1.json b/research/steve/config/experiments/goodruns/swimmer/mve_tdk1.json
new file mode 100644
index 00000000..14210117
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/mve_tdk2.json b/research/steve/config/experiments/goodruns/swimmer/mve_tdk2.json
new file mode 100644
index 00000000..14210117
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/mve_tdk3.json b/research/steve/config/experiments/goodruns/swimmer/mve_tdk3.json
new file mode 100644
index 00000000..14210117
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/steve0.json b/research/steve/config/experiments/goodruns/swimmer/steve0.json
new file mode 100644
index 00000000..d3358328
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/steve1.json b/research/steve/config/experiments/goodruns/swimmer/steve1.json
new file mode 100644
index 00000000..d3358328
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/steve2.json b/research/steve/config/experiments/goodruns/swimmer/steve2.json
new file mode 100644
index 00000000..d3358328
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/swimmer/steve3.json b/research/steve/config/experiments/goodruns/swimmer/steve3.json
new file mode 100644
index 00000000..d3358328
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/swimmer/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/swimmer.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/ddpg0.json b/research/steve/config/experiments/goodruns/walker2d/ddpg0.json
new file mode 100644
index 00000000..81fe2ff5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/ddpg1.json b/research/steve/config/experiments/goodruns/walker2d/ddpg1.json
new file mode 100644
index 00000000..81fe2ff5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/ddpg2.json b/research/steve/config/experiments/goodruns/walker2d/ddpg2.json
new file mode 100644
index 00000000..81fe2ff5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/ddpg2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/ddpg3.json b/research/steve/config/experiments/goodruns/walker2d/ddpg3.json
new file mode 100644
index 00000000..81fe2ff5
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/ddpg3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/mve_tdk0.json b/research/steve/config/experiments/goodruns/walker2d/mve_tdk0.json
new file mode 100644
index 00000000..d8420eff
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/mve_tdk1.json b/research/steve/config/experiments/goodruns/walker2d/mve_tdk1.json
new file mode 100644
index 00000000..d8420eff
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/mve_tdk2.json b/research/steve/config/experiments/goodruns/walker2d/mve_tdk2.json
new file mode 100644
index 00000000..d8420eff
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/mve_tdk2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/mve_tdk3.json b/research/steve/config/experiments/goodruns/walker2d/mve_tdk3.json
new file mode 100644
index 00000000..d8420eff
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/mve_tdk3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/steve0.json b/research/steve/config/experiments/goodruns/walker2d/steve0.json
new file mode 100644
index 00000000..a98c410c
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/steve1.json b/research/steve/config/experiments/goodruns/walker2d/steve1.json
new file mode 100644
index 00000000..a98c410c
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/steve2.json b/research/steve/config/experiments/goodruns/walker2d/steve2.json
new file mode 100644
index 00000000..a98c410c
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/steve2.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/goodruns/walker2d/steve3.json b/research/steve/config/experiments/goodruns/walker2d/steve3.json
new file mode 100644
index 00000000..a98c410c
--- /dev/null
+++ b/research/steve/config/experiments/goodruns/walker2d/steve3.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/walker2d.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg0.json b/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg0.json
new file mode 100644
index 00000000..b7280d71
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg1.json b/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg1.json
new file mode 100644
index 00000000..b7280d71
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk0.json b/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk0.json
new file mode 100644
index 00000000..73252566
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk1.json b/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk1.json
new file mode 100644
index 00000000..73252566
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_steve0.json b/research/steve/config/experiments/speedruns/flagrun/speedy_steve0.json
new file mode 100644
index 00000000..ba5708f1
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/flagrun/speedy_steve1.json b/research/steve/config/experiments/speedruns/flagrun/speedy_steve1.json
new file mode 100644
index 00000000..ba5708f1
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/flagrun/speedy_steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/flagrun.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg0.json b/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg0.json
new file mode 100644
index 00000000..eb07f31d
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg1.json b/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg1.json
new file mode 100644
index 00000000..eb07f31d
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_ddpg1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/ddpg.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk0.json b/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk0.json
new file mode 100644
index 00000000..51a3bdcb
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk1.json b/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk1.json
new file mode 100644
index 00000000..51a3bdcb
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_mve_tdk1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/mve_tdk.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_steve0.json b/research/steve/config/experiments/speedruns/humanoid/speedy_steve0.json
new file mode 100644
index 00000000..0d2bfaa4
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_steve0.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/config/experiments/speedruns/humanoid/speedy_steve1.json b/research/steve/config/experiments/speedruns/humanoid/speedy_steve1.json
new file mode 100644
index 00000000..0d2bfaa4
--- /dev/null
+++ b/research/steve/config/experiments/speedruns/humanoid/speedy_steve1.json
@@ -0,0 +1 @@
+{"inherits": ["config/algos/steve.json", "config/envs/humanoid.json", "config/experimental_setups/speedrun.json"]}
diff --git a/research/steve/envwrap.py b/research/steve/envwrap.py
new file mode 100644
index 00000000..bd88c303
--- /dev/null
+++ b/research/steve/envwrap.py
@@ -0,0 +1,106 @@
+from builtins import object
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+try:
+  import roboschool
+except:
+  pass
+import gym
+import numpy as np
+
+from config import config
+
+MAX_FRAMES = config["env"]["max_frames"]
+
+gym.logger.level=40
+
+def get_env(env_name, *args, **kwargs):
+  MAPPING = {
+    "CartPole-v0": CartPoleWrapper,
+  }
+  if env_name in MAPPING: return MAPPING[env_name](env_name, *args, **kwargs)
+  else: return NoTimeLimitMujocoWrapper(env_name, *args, **kwargs)
+
+class GymWrapper(object):
+  """
+  Generic wrapper for OpenAI gym environments.
+  """
+  def __init__(self, env_name):
+    self.internal_env = gym.make(env_name)
+    self.observation_space = self.internal_env.observation_space
+    self.action_space = self.internal_env.action_space
+    self.custom_init()
+
+  def custom_init(self):
+    pass
+
+  def reset(self):
+    self.clock = 0
+    return self.preprocess_obs(self.internal_env.reset())
+
+  # returns normalized actions
+  def sample(self):
+    return self.action_space.sample()
+
+  # this is used for converting continuous approximations back to the original domain
+  def normalize_actions(self, actions):
+    return actions
+
+  # puts actions into a form where they can be predicted. by default, called after sample()
+  def unnormalize_actions(self, actions):
+    return actions
+
+  def preprocess_obs(self, obs):
+    # return np.append(obs, [self.clock/float(MAX_FRAMES)])
+    return obs
+
+  def step(self, normalized_action):
+    out = self.internal_env.step(normalized_action)
+    self.clock += 1
+    obs, reward, done = self.preprocess_obs(out[0]), out[1], float(out[2])
+    reset = done == 1. or self.clock == MAX_FRAMES
+    return obs, reward, done, reset
+
+  def render_rollout(self, states):
+    ## states is numpy array of size [timesteps, state]
+    self.internal_env.reset()
+    for state in states:
+      self.internal_env.env.state = state
+      self.internal_env.render()
+
+class CartPoleWrapper(GymWrapper):
+  """
+  Wrap CartPole.
+  """
+  def sample(self):
+    return np.array([np.random.uniform(0., 1.)])
+
+  def normalize_actions(self, action):
+    return 1 if action[0] >= 0 else 0
+
+  def unnormalize_actions(self, action):
+    return 2. * action - 1.
+
+class NoTimeLimitMujocoWrapper(GymWrapper):
+  """
+  Wrap Mujoco-style environments, removing the termination condition after time.
+  This is needed to keep it Markovian.
+  """
+  def __init__(self, env_name):
+    self.internal_env = gym.make(env_name).env
+    self.observation_space = self.internal_env.observation_space
+    self.action_space = self.internal_env.action_space
+    self.custom_init()
diff --git a/research/steve/learner.py b/research/steve/learner.py
new file mode 100644
index 00000000..0434cfd7
--- /dev/null
+++ b/research/steve/learner.py
@@ -0,0 +1,271 @@
+from __future__ import division
+from __future__ import print_function
+from builtins import zip
+from builtins import range
+from builtins import object
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import traceback, threading, time, warnings
+import tensorflow as tf
+import numpy as np
+
+import util
+from replay import ReplayBuffer
+
+class Learner(object):
+    """
+    Generic object which runs the main training loop of anything that trains using
+    a replay buffer. Handles updating, logging, saving/loading, batching, etc.
+    """
+    def __init__(self, interactor_queue, lock, config, env_config, learner_config, **bonus_kwargs):
+        self.learner_name = self.learner_name()
+        self.interactor_queue = interactor_queue
+        self.learner_lock = lock
+        self.config = config
+        self.env_config = env_config
+        self.learner_config = learner_config
+        self.bonus_kwargs = bonus_kwargs
+        self.kill_threads = False
+        self.permit_desync = False
+        self.need_frames_notification = threading.Condition()
+        self._reset_inspections()
+        self.total_frames = 0
+
+        self.save_path = util.create_directory("%s/%s/%s/%s" % (self.config["output_root"], self.config["env"]["name"], self.config["name"], self.config["save_model_path"]))
+        self.log_path = util.create_directory("%s/%s/%s/%s" % (self.config["output_root"], self.config["env"]["name"], self.config["name"],  self.config["log_path"])) + "/%s.log" % self.learner_name
+
+        # replay buffer to store data
+        self.replay_buffer_lock = threading.RLock()
+        self.replay_buffer = ReplayBuffer(self.learner_config["replay_size"],
+                                          np.prod(self.env_config["obs_dims"]),
+                                          self.env_config["action_dim"])
+
+        # data loaders pull data from the replay buffer and put it into the tfqueue for model usage
+        self.data_loaders = self.make_loader_placeholders()
+        queue_capacity = np.ceil(1./self.learner_config["frames_per_update"]) if self.learner_config["frames_per_update"] else 100
+        self.tf_queue = tf.FIFOQueue(capacity=queue_capacity, dtypes=[dl.dtype for dl in self.data_loaders])
+        self.enqueue_op = self.tf_queue.enqueue(self.data_loaders)
+        self.current_batch = self.tf_queue.dequeue()
+
+        # build the TF graph for the actual model to train
+        self.core, self.train_losses, self.train_ops, self.inspect_losses = self.make_core_model()
+        self.sess = tf.Session()
+        self.sess.run(tf.global_variables_initializer())
+
+    ## Mandatory functions to override
+    def learner_name(self): raise Exception('unimplemented: learner_name')
+    def make_loader_placeholders(self): raise Exception('unimplemented: make_loader_placeholders')
+    def make_core_model(self): raise Exception('unimplemented: make_core_model')
+
+    ## Optional functions to override
+    def initialize(self): warnings.warn('unimplemented: initialize')
+    def resume_from_checkpoint(self, epoch): warnings.warn('unimplemented: resume_from_checkpoint')
+    def checkpoint(self): warnings.warn('unimplemented: checkpoint')
+    def backup(self): warnings.warn('unimplemented: backup')
+
+    ## Internal functions
+    def _start(self):
+        # fetch data from the interactors to pre-fill the replay buffer
+        self.prefetch_thread = threading.Thread(target=self._poll_interactors, args=(True, self.learner_config["frames_before_learning"],))
+        self.prefetch_thread.start()
+        self.prefetch_thread.join()
+
+        # start the interactor and data loader
+        self.data_load_thread = threading.Thread(target=self._run_enqueue_data)
+        self.data_load_thread.start()
+
+        # initialize the learner, pretraining if needed
+        if self.config["resume"]: self._resume_from_checkpoint()
+        else:                     self._initialize()
+
+        # re-sync everything, and start up interactions with the environment
+        self.interactor_poll_thread = threading.Thread(target=self._poll_interactors)
+        self.interactor_poll_thread.start()
+
+        # start the clock
+        self._last_checkpoint_time = time.time()
+
+    def _learn(self, permit_desync=False, log=True, checkpoint=True, backup=True):
+        # this is to keep the frames/update synced properly
+        if self.learner_config["frames_per_update"] is not False and not permit_desync:
+            if not self._have_enough_frames():
+                with self.need_frames_notification:
+                    self.need_frames_notification.notify()
+                return
+
+        # log
+        if log and (self.update_i + 1) % self.learner_config["log_every_n"] == 0:
+            self._log()
+
+        # checkpoint
+        if checkpoint and (self.update_i + 1) % self.learner_config["epoch_every_n"] == 0:
+            self._checkpoint()
+
+        # backup
+        if backup and (self.update_i + 1) % self.learner_config["backup_every_n"] == 0:
+            self._backup()
+
+        # train
+        self._training_step()
+
+    def _have_enough_frames(self):
+        gathered_frames = self.total_frames - self.learner_config["frames_before_learning"]
+        return gathered_frames > self.learner_config["frames_per_update"] * self.update_i
+
+    def _initialize(self):
+        self.epoch = 0
+        self.update_i = 0
+        self.hours = 0
+        self._last_checkpoint_time = time.time()
+
+        self.initialize()
+
+        if self.learner_config["pretrain_n"]: self._pretrain()
+        self._checkpoint()
+
+    def _pretrain(self):
+        for _ in range(self.learner_config["pretrain_n"]):
+            self._learn(permit_desync=True, checkpoint=False, backup=False)
+        self.epoch = 0
+        self.update_i = 0
+
+    def _resume_from_checkpoint(self):
+        epoch = util.get_largest_epoch_in_dir(self.save_path, self.core.saveid)
+        if not self.config['keep_all_replay_buffers']: util.wipe_all_but_largest_epoch_in_dir(self.save_path, self.core.saveid)
+        if epoch is False:
+            raise Exception("Tried to reload but no model found")
+        with self.learner_lock:
+            self.core.load(self.sess, self.save_path, epoch)
+            self.epoch, self.update_i, self.total_frames, self.hours = self.sess.run([self.core.epoch_n, self.core.update_n, self.core.frame_n, self.core.hours])
+        with self.replay_buffer_lock:
+            self.replay_buffer.load(self.save_path, '%09d_%s' % (epoch, self.learner_name))
+        self.resume_from_checkpoint(epoch)
+
+    def _log(self):
+        logstring = "(%3.2f sec) h%-8.2f e%-8d s%-8d f%-8d\t" % (time.time() - self._log_time, self.hours, self.epoch, self.update_i + 1, self.total_frames) + ', '.join(["%8f" % x for x in (self.running_total / self.denom).tolist()])
+        print("%s\t%s" % (self.learner_name, logstring))
+        with open(self.log_path, "a") as f: f.write(logstring + "\n")
+        self._reset_inspections()
+
+    def _reset_inspections(self):
+        self.running_total = 0.
+        self.denom = 0.
+        self._log_time = time.time()
+
+    def _checkpoint(self):
+        self.checkpoint()
+        self.epoch += 1
+        self.hours += (time.time() - self._last_checkpoint_time) / 3600.
+        self._last_checkpoint_time = time.time()
+        self.core.update_epoch(self.sess, self.epoch, self.update_i, self.total_frames, self.hours)
+        with self.learner_lock: self.core.save(self.sess, self.save_path)
+
+    def _backup(self):
+        self.backup()
+        if not self.learner_config['keep_all_replay_buffers']: util.wipe_all_but_largest_epoch_in_dir(self.save_path, self.core.saveid)
+        with self.learner_lock:
+            self.core.save(self.sess, self.save_path, self.epoch)
+        with self.replay_buffer_lock:
+            self.replay_buffer.save(self.save_path, '%09d_%s' % (self.epoch, self.learner_name))
+
+    def _training_step(self):
+        train_ops = tuple([op for op, loss in zip(self.train_ops,
+                                                  self.train_losses)
+                           if loss is not None])
+        outs = self.sess.run(train_ops + self.inspect_losses)
+        self.running_total += np.array(outs[len(train_ops):])
+        self.denom += 1.
+        self.update_i += 1
+
+    def _poll_interactors(self, continuous_poll=False, frames_before_terminate=None):
+        # poll the interactors for new frames.
+        # the synced_condition semaphore prevents this from consuming too much CPU
+        while not self.kill_threads:
+            if self.learner_config["frames_per_update"] is not False and not continuous_poll:
+                with self.need_frames_notification: self.need_frames_notification.wait()
+            while not self.interactor_queue.empty():
+                new_frames = self.interactor_queue.get()
+                self._add_frames(new_frames)
+                if frames_before_terminate and self.total_frames >= frames_before_terminate: return
+
+    def _add_frames(self, frames):
+        with self.replay_buffer_lock:
+            for frame in frames:
+                self.replay_buffer.add_replay(*frame)
+            self.total_frames = self.replay_buffer.count
+        return self.total_frames
+
+    def _run_enqueue_data(self):
+        while not self.kill_threads:
+            data = self.replay_buffer.random_batch(self.learner_config["batch_size"])
+            self.sess.run(self.enqueue_op, feed_dict=dict(list(zip(self.data_loaders, data))))
+
+    def _kill_threads(self):
+        self.kill_threads = True
+
+
+class CoreModel(object):
+    """The base class for the "core" of learners."""
+    def __init__(self, name, env_config, learner_config):
+        self.name = self.saveid + "/" + name
+        self.env_config = env_config
+        self.learner_config = learner_config
+
+        with tf.variable_scope(self.name):
+            self.epoch_n = tf.get_variable('epoch_n', [], initializer=tf.constant_initializer(0), dtype=tf.int64, trainable=False)
+            self.update_n = tf.get_variable('update_n', [], initializer=tf.constant_initializer(0), dtype=tf.int64, trainable=False)
+            self.frame_n = tf.get_variable('frame_n', [], initializer=tf.constant_initializer(0), dtype=tf.int64, trainable=False)
+            self.hours = tf.get_variable('hours', [], initializer=tf.constant_initializer(0.), dtype=tf.float64, trainable=False)
+            self.epoch_n_placeholder = tf.placeholder(tf.int64, [])
+            self.update_n_placeholder = tf.placeholder(tf.int64, [])
+            self.frame_n_placeholder = tf.placeholder(tf.int64, [])
+            self.hours_placeholder = tf.placeholder(tf.float64, [])
+        self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]
+
+        self.create_params(env_config, learner_config)
+        self.model_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)
+        self.saver = tf.train.Saver(self.model_params)
+
+    @property
+    def saveid(self):
+        raise Exception("specify a save ID")
+
+    def create_params(self, env_config, learner_config):
+        raise Exception("unimplemented")
+
+    def update_epoch(self, sess, epoch, updates, frames, hours):
+        sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})
+
+    def save(self, sess, path, epoch=None):
+        if epoch is None:  self.saver.save(sess, path + "/%s.params" % self.saveid)
+        else:              self.saver.save(sess, path + "/%09d_%s.params" % (epoch, self.saveid))
+
+    def load(self, sess, path, epoch=None):
+        if epoch is None:  self.saver.restore(sess, path + "/%s.params" % self.saveid)
+        else:              self.saver.restore(sess, path + "/%09d_%s.params" % (epoch, self.saveid))
+
+def run_learner(learner_subclass, queue, lock, config, env_config, learner_config, **bonus_kwargs):
+    learner = learner_subclass(queue, lock, config, env_config, learner_config, **bonus_kwargs)
+    try:
+        learner._start()
+        while True: learner._learn()
+
+    except Exception as e:
+        print('Caught exception in learner process')
+        traceback.print_exc()
+        learner._kill_threads()
+        print()
+        raise e
diff --git a/research/steve/master.py b/research/steve/master.py
new file mode 100644
index 00000000..4d084747
--- /dev/null
+++ b/research/steve/master.py
@@ -0,0 +1,85 @@
+from builtins import str
+from builtins import range
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import multiprocessing
+import os, sys, time
+
+from config import config, log_config
+import util
+
+AGENT_COUNT = config["agent_config"]["count"]
+EVALUATOR_COUNT = config["evaluator_config"]["count"]
+MODEL_AUGMENTED = config["model_config"] is not False
+if config["resume"]:
+  ROOT_PATH = "output/" + config["env"]["name"] + "/" + config["name"]
+else:
+  ROOT_PATH = util.create_and_wipe_directory("output/" + config["env"]["name"] + "/" + config["name"])
+log_config()
+import learner, agent, valuerl_learner
+if MODEL_AUGMENTED: import worldmodel_learner
+
+if __name__ == '__main__':
+  all_procs = set([])
+  interaction_procs = set([])
+
+  # lock
+  policy_lock = multiprocessing.Lock()
+  model_lock = multiprocessing.Lock() if MODEL_AUGMENTED else None
+
+  # queue
+  policy_replay_frame_queue = multiprocessing.Queue(1)
+  model_replay_frame_queue = multiprocessing.Queue(1) if MODEL_AUGMENTED else None
+
+  # interactors
+  for interact_proc_i in range(AGENT_COUNT):
+    interact_proc = multiprocessing.Process(target=agent.main, args=(interact_proc_i, False, policy_replay_frame_queue, model_replay_frame_queue, policy_lock, config))
+    all_procs.add(interact_proc)
+    interaction_procs.add(interact_proc)
+
+  # evaluators
+  for interact_proc_i in range(EVALUATOR_COUNT):
+    interact_proc = multiprocessing.Process(target=agent.main, args=(interact_proc_i, True, policy_replay_frame_queue, model_replay_frame_queue, policy_lock, config))
+    all_procs.add(interact_proc)
+    interaction_procs.add(interact_proc)
+
+  # policy training
+  train_policy_proc = multiprocessing.Process(target=learner.run_learner, args=(valuerl_learner.ValueRLLearner, policy_replay_frame_queue, policy_lock, config, config["env"], config["policy_config"]), kwargs={"model_lock": model_lock})
+  all_procs.add(train_policy_proc)
+
+  # model training
+  if MODEL_AUGMENTED:
+    train_model_proc = multiprocessing.Process(target=learner.run_learner, args=(worldmodel_learner.WorldmodelLearner, model_replay_frame_queue, model_lock, config, config["env"], config["model_config"]))
+    all_procs.add(train_model_proc)
+
+  # start all policies
+  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
+  for i, proc in enumerate(interaction_procs):
+    os.environ['CUDA_VISIBLE_DEVICES'] = ''
+    proc.start()
+
+  os.environ['CUDA_VISIBLE_DEVICES'] = str(int(sys.argv[2]))
+  train_policy_proc.start()
+
+  if MODEL_AUGMENTED:
+    os.environ['CUDA_VISIBLE_DEVICES'] = str(1+int(sys.argv[2]))
+    train_model_proc.start()
+
+  while True:
+    try:
+      pass
+    except:
+      for proc in all_procs: proc.join()
diff --git a/research/steve/nn.py b/research/steve/nn.py
new file mode 100644
index 00000000..c87c6eb8
--- /dev/null
+++ b/research/steve/nn.py
@@ -0,0 +1,189 @@
+from builtins import range
+from builtins import object
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+import numpy as np
+from itertools import product
+
+class FeedForwardNet(object):
+    """Custom feed-forward network layer."""
+    def __init__(self, name, in_size, out_shape, layers=1, hidden_dim=32, final_nonlinearity=None, get_uncertainty=False):
+        self.name = name
+        self.in_size = in_size
+        self.out_shape = out_shape
+        self.out_size = np.prod(out_shape)
+        self.layers = layers
+        self.hidden_dim = hidden_dim
+        self.final_nonlinearity = (lambda x:x) if final_nonlinearity is None else final_nonlinearity
+        self.get_uncertainty = get_uncertainty
+
+        self.weights = [None] * layers
+        self.biases = [None] * layers
+
+        self.params_list = []
+
+        with tf.variable_scope(name):
+            for layer_i in range(self.layers):
+                in_size = self.hidden_dim
+                out_size = self.hidden_dim
+                if layer_i == 0: in_size = self.in_size
+                if layer_i == self.layers - 1: out_size = self.out_size
+                self.weights[layer_i] = tf.get_variable("weights%d" % layer_i, [in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())
+                self.biases[layer_i] = tf.get_variable("bias%d" % layer_i, [1, out_size], initializer=tf.constant_initializer(0.0))
+                self.params_list += [self.weights[layer_i], self.biases[layer_i]]
+
+    def __call__(self, x, stop_params_gradient=False, is_eval=True, ensemble_idxs=None, pre_expanded=None, reduce_mode="none"):
+        original_shape = tf.shape(x)
+        h = tf.reshape(x, [-1, self.in_size])
+        for layer_i in range(self.layers):
+            nonlinearity = tf.nn.relu if layer_i + 1 < self.layers else self.final_nonlinearity
+            if stop_params_gradient: h = nonlinearity(tf.matmul(h, tf.stop_gradient(self.weights[layer_i])) + tf.stop_gradient(self.biases[layer_i]))
+            else:             h = nonlinearity(tf.matmul(h, self.weights[layer_i]) + self.biases[layer_i])
+        if len(self.out_shape) > 0: h = tf.reshape(h, tf.concat([original_shape[:-1], tf.constant(self.out_shape)], -1))
+        else:                       h = tf.reshape(h, original_shape[:-1])
+        if pre_expanded is None: pre_expanded = ensemble_idxs is not None
+        if reduce_mode == "none" and not pre_expanded and self.get_uncertainty:
+            if len(self.out_shape) > 0: h = tf.expand_dims(h, -2)
+            else:                       h = tf.expand_dims(h, -1)
+        return h
+
+    def l2_loss(self):
+        return tf.add_n([tf.reduce_sum(.5 * tf.square(mu)) for mu in self.params_list])
+
+class BayesianDropoutFeedForwardNet(FeedForwardNet):
+    """Custom feed-forward network layer, with dropout as a Bayesian approximation."""
+    def __init__(self, name, in_size, out_shape, layers=1, hidden_dim=32, final_nonlinearity=None, get_uncertainty=False, keep_prob=.5, eval_sample_count=2, consistent_random_seed=False):
+        super(BayesianDropoutFeedForwardNet, self).__init__(name, in_size, out_shape, layers=layers, hidden_dim=hidden_dim,
+                                                            final_nonlinearity=final_nonlinearity, get_uncertainty=get_uncertainty)
+        self.keep_prob = keep_prob
+        self.eval_sample_count = eval_sample_count
+        if eval_sample_count < 2: raise Exception("eval_sample_count must be at least 2 to estimate uncertainty")
+        self.dropout_seed = tf.random_uniform([layers], maxval=1e18, dtype=tf.int64) if consistent_random_seed else [None] * layers
+
+    def __call__(self, x, stop_params_gradient=False, is_eval=True, pre_expanded=False, ensemble_idxs=None, reduce_mode="none"):
+        if is_eval:
+            x = tf.tile(tf.expand_dims(x,0), tf.concat([tf.constant([self.eval_sample_count]), tf.ones_like(tf.shape(x))], 0))
+        original_shape = tf.shape(x)
+        h = tf.reshape(x, [-1, self.in_size])
+        for layer_i in range(self.layers):
+            nonlinearity = tf.nn.relu if layer_i + 1 < self.layers else self.final_nonlinearity
+            if layer_i > 0: h = tf.nn.dropout(h, keep_prob=self.keep_prob, seed=self.dropout_seed[layer_i])
+            if stop_params_gradient: h = nonlinearity(tf.matmul(h, tf.stop_gradient(self.weights[layer_i])) + tf.stop_gradient(self.biases[layer_i]))
+            else:                    h = nonlinearity(tf.matmul(h, self.weights[layer_i]) + self.biases[layer_i])
+        if len(self.out_shape) > 0: h = tf.reshape(h, tf.concat([original_shape[:-1], tf.constant(self.out_shape)], -1))
+        else:                       h = tf.reshape(h, original_shape[:-1])
+        if is_eval:
+            h, uncertainty = tf.nn.moments(h, 0)
+            if self.get_uncertainty: return h, uncertainty
+            else:                    return h
+        else:
+            return h
+
+
+class EnsembleFeedForwardNet(FeedForwardNet):
+    """Custom feed-forward network layer with an ensemble."""
+    def __init__(self, name, in_size, out_shape, layers=1, hidden_dim=32, final_nonlinearity=None, get_uncertainty=False, ensemble_size=2, train_sample_count=2, eval_sample_count=2):
+        if train_sample_count > ensemble_size: raise Exception("train_sample_count cannot be larger than ensemble size")
+        if eval_sample_count > ensemble_size: raise Exception("eval_sample_count cannot be larger than ensemble size")
+        self.name = name
+        self.in_size = in_size
+        self.out_shape = out_shape
+        self.out_size = np.prod(out_shape)
+        self.layers = layers
+        self.hidden_dim = hidden_dim
+        self.final_nonlinearity = (lambda x:x) if final_nonlinearity is None else final_nonlinearity
+        self.get_uncertainty = get_uncertainty
+        self.ensemble_size = ensemble_size
+        self.train_sample_count = train_sample_count
+        self.eval_sample_count = eval_sample_count
+
+        self.weights = [None] * layers
+        self.biases = [None] * layers
+
+        self.params_list = []
+
+        with tf.variable_scope(name):
+            for layer_i in range(self.layers):
+                in_size = self.hidden_dim
+                out_size = self.hidden_dim
+                if layer_i == 0: in_size = self.in_size
+                if layer_i == self.layers - 1: out_size = self.out_size
+                self.weights[layer_i] = tf.get_variable("weights%d" % layer_i, [ensemble_size, in_size, out_size], initializer=tf.contrib.layers.xavier_initializer())
+                self.biases[layer_i] = tf.get_variable("bias%d" % layer_i, [ensemble_size, out_size], initializer=tf.constant_initializer(0.0))
+                self.params_list += [self.weights[layer_i], self.biases[layer_i]]
+
+    def __call__(self, x, stop_params_gradient=False, is_eval=True, ensemble_idxs=None, pre_expanded=None, reduce_mode="none"):
+        if pre_expanded is None: pre_expanded = ensemble_idxs is not None
+        if ensemble_idxs is None:
+            ensemble_idxs = tf.random_shuffle(tf.range(self.ensemble_size))
+            ensemble_sample_n = self.eval_sample_count if is_eval else self.train_sample_count
+            ensemble_idxs = ensemble_idxs[:ensemble_sample_n]
+        else:
+            ensemble_sample_n = tf.shape(ensemble_idxs)[0]
+
+        weights = [tf.gather(w, ensemble_idxs, axis=0) for w in self.weights]
+        biases = [tf.expand_dims(tf.gather(b, ensemble_idxs, axis=0),0) for b in self.biases]
+
+        original_shape = tf.shape(x)
+        if pre_expanded: h = tf.reshape(x, [-1, ensemble_sample_n, self.in_size])
+        else:            h = tf.tile(tf.reshape(x, [-1, 1, self.in_size]), [1, ensemble_sample_n, 1])
+        for layer_i in range(self.layers):
+            nonlinearity = tf.nn.relu if layer_i + 1 < self.layers else self.final_nonlinearity
+            if stop_params_gradient: h = nonlinearity(tf.einsum('bri,rij->brj', h, tf.stop_gradient(weights[layer_i])) + tf.stop_gradient(biases[layer_i]))
+            else:                    h = nonlinearity(tf.einsum('bri,rij->brj', h, weights[layer_i]) + biases[layer_i])
+
+        if pre_expanded:
+            if len(self.out_shape) > 0: h = tf.reshape(h, tf.concat([original_shape[:-1], tf.constant(self.out_shape)], -1))
+            else:                       h = tf.reshape(h, original_shape[:-1])
+        else:
+            if len(self.out_shape) > 0: h = tf.reshape(h, tf.concat([original_shape[:-1], tf.constant([ensemble_sample_n]), tf.constant(self.out_shape)], -1))
+            else:                       h = tf.reshape(h, tf.concat([original_shape[:-1], tf.constant([ensemble_sample_n])], -1))
+
+        if reduce_mode == "none":
+            pass
+        elif reduce_mode == "random":
+            if len(self.out_shape) > 0: h = tf.reduce_sum(h * tf.reshape(tf.one_hot(tf.random_uniform([tf.shape(h)[0]], 0, ensemble_sample_n, dtype=tf.int64), ensemble_sample_n), tf.concat([tf.shape(h)[:1], tf.ones_like(tf.shape(h)[1:-2]), tf.constant([ensemble_sample_n]), tf.constant([1])], 0)), -2)
+            else:                       h = tf.reduce_sum(h * tf.reshape(tf.one_hot(tf.random_uniform([tf.shape(h)[0]], 0, ensemble_sample_n, dtype=tf.int64), ensemble_sample_n), tf.concat([tf.shape(h)[:1], tf.ones_like(tf.shape(h)[1:-1]), tf.constant([ensemble_sample_n])], 0)), -1)
+        elif reduce_mode == "mean":
+            if len(self.out_shape) > 0: h = tf.reduce_mean(h, -2)
+            else:                       h = tf.reduce_mean(h, -1)
+        else: raise Exception("use a valid reduce mode: none, random, or mean")
+
+        return h
+
+
+class ReparamNormal(object):
+    """Wrapper to make a feedforward network that outputs both mu and logsigma,
+    for use in the reparameterization trick."""
+    def __init__(self, base_net, name, in_size, out_shape, layers=2, hidden_dim=32, final_nonlinearity=None, ls_start_bias=0.0, final_net=FeedForwardNet, logsigma_min=-5., logsigma_max=2., **kwargs):
+        assert layers > 1
+        self.main_encoder = base_net(name+"_base", in_size, [hidden_dim], layers, hidden_dim, final_nonlinearity=tf.nn.relu, **kwargs)
+        self.mu = final_net(name+"_mu", hidden_dim, out_shape, layers=1, final_nonlinearity=final_nonlinearity, **kwargs)
+        self.logsigma = final_net(name+"_logsigma", hidden_dim, out_shape, layers=1, final_nonlinearity=None, **kwargs)
+        self.ls_start_bias = ls_start_bias
+        self.params_list = self.main_encoder.params_list + self.mu.params_list + self.logsigma.params_list
+        self.logsigma_min = logsigma_min
+        self.logsigma_max = logsigma_max
+
+    def __call__(self, x):
+        encoded = self.main_encoder(x)
+        mu = self.mu(encoded)
+        logsigma = tf.clip_by_value(self.logsigma(encoded) + self.ls_start_bias, self.logsigma_min, self.logsigma_max)
+        return mu, logsigma
+
+    def l2_loss(self):
+        return self.main_encoder.l2_loss() + self.mu.l2_loss() + self.logsigma.l2_loss()
diff --git a/research/steve/replay.py b/research/steve/replay.py
new file mode 100644
index 00000000..8efabd90
--- /dev/null
+++ b/research/steve/replay.py
@@ -0,0 +1,109 @@
+from __future__ import print_function
+from future import standard_library
+standard_library.install_aliases()
+from builtins import zip
+from builtins import str
+from builtins import object
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import numpy as np
+import pickle
+import multiprocessing
+
+class ReplayBuffer(object):
+    """
+    Stores frames sampled from the environment, with the ability to sample a batch
+    for training.
+    """
+
+    def __init__(self, max_size, obs_dim, action_dim, roundrobin=True):
+        self.max_size = max_size
+        self.obs_dim = obs_dim
+        self.action_dim = action_dim
+        self.roundrobin = roundrobin
+
+        self.obs_buffer = np.zeros([max_size, obs_dim])
+        self.next_obs_buffer = np.zeros([max_size, obs_dim])
+        self.action_buffer = np.zeros([max_size, action_dim])
+        self.reward_buffer = np.zeros([max_size])
+        self.done_buffer = np.zeros([max_size])
+
+        self.count = 0
+
+    def random_batch(self, batch_size):
+        indices = np.random.randint(0, min(self.count, self.max_size), batch_size)
+
+        return (
+            self.obs_buffer[indices],
+            self.next_obs_buffer[indices],
+            self.action_buffer[indices],
+            self.reward_buffer[indices],
+            self.done_buffer[indices],
+            self.count
+        )
+
+    def add_replay(self, obs, next_obs, action, reward, done):
+        if self.count >= self.max_size:
+            if self.roundrobin: index = self.count % self.max_size
+            else:               index = np.random.randint(0, self.max_size)
+        else:
+            index = self.count
+
+        self.obs_buffer[index] = obs
+        self.next_obs_buffer[index] = next_obs
+        self.action_buffer[index] = action
+        self.reward_buffer[index] = reward
+        self.done_buffer[index] = done
+
+        self.count += 1
+
+    def save(self, path, name):
+        def _save(datas, fnames):
+            print("saving replay buffer...")
+            for data, fname in zip(datas, fnames):
+                with open("%s.npz"%fname, "w") as f:
+                    pickle.dump(data, f)
+            with open("%s/%s.count" % (path,name), "w") as f:
+                f.write(str(self.count))
+            print("...done saving.")
+
+        datas = [
+            self.obs_buffer,
+            self.next_obs_buffer,
+            self.action_buffer,
+            self.reward_buffer,
+            self.done_buffer
+        ]
+
+        fnames = [
+            "%s/%s.obs_buffer" % (path, name),
+            "%s/%s.next_obs_buffer" % (path, name),
+            "%s/%s.action_buffer" % (path, name),
+            "%s/%s.reward_buffer" % (path, name),
+            "%s/%s.done_buffer" % (path, name)
+         ]
+
+        proc = multiprocessing.Process(target=_save, args=(datas, fnames))
+        proc.start()
+
+    def load(self, path, name):
+        print("Loading %s replay buffer (may take a while...)" % name)
+        with open("%s/%s.obs_buffer.npz" % (path,name)) as f: self.obs_buffer = pickle.load(f)
+        with open("%s/%s.next_obs_buffer.npz" % (path,name)) as f: self.next_obs_buffer = pickle.load(f)
+        with open("%s/%s.action_buffer.npz" % (path,name)) as f: self.action_buffer = pickle.load(f)
+        with open("%s/%s.reward_buffer.npz" % (path,name)) as f: self.reward_buffer = pickle.load(f)
+        with open("%s/%s.done_buffer.npz" % (path,name)) as f: self.done_buffer = pickle.load(f)
+        with open("%s/%s.count" % (path,name), "r") as f: self.count = int(f.read())
diff --git a/research/steve/toy_demo.py b/research/steve/toy_demo.py
new file mode 100644
index 00000000..859a86f7
--- /dev/null
+++ b/research/steve/toy_demo.py
@@ -0,0 +1,430 @@
+from __future__ import division
+from __future__ import print_function
+from builtins import range
+from past.utils import old_div
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import numpy as np
+import scipy
+import matplotlib.pyplot as plt
+import seaborn as sns
+
+### Hyperparameters
+
+NONTERMINAL_STATE_COUNT = 100
+NOISE_AMOUNT = 0.1
+TRAIN_STEPS = 10000
+Q_ENSEMBLE_SIZE = 8
+MODEL_ENSEMBLE_SIZE = 8
+HORIZON = 5
+TRIAL_N = 10
+
+### Helper functions
+
+initial_state = 0
+terminal_state = NONTERMINAL_STATE_COUNT + 1
+nonterminal_state_count = NONTERMINAL_STATE_COUNT
+state_count = NONTERMINAL_STATE_COUNT + 1
+final_reward = NONTERMINAL_STATE_COUNT
+colors = sns.color_palette('husl', 4)
+plt.rcParams["figure.figsize"] = (6,5)
+
+def step(state):
+  if state == terminal_state: next_state = terminal_state
+  else:                       next_state = state + 1
+
+  if state == terminal_state:     reward = 0
+  elif state+1 == terminal_state: reward = final_reward
+  else:                           reward = -1
+
+  return next_state, reward
+
+def noisy_step(state):
+  if state == terminal_state:               next_state = terminal_state
+  elif np.random.random([]) < NOISE_AMOUNT: next_state = np.random.randint(0, state_count)
+  else:                                     next_state = state + 1
+
+  if state == terminal_state:     reward = 0
+  elif state+1 == terminal_state: reward = final_reward
+  else:                           reward = -1
+
+  return next_state, reward
+
+def get_error(Q):
+  losses = np.square(np.arange(state_count) - Q[:-1])
+  return np.mean(losses)
+
+def downsample(array, factor):
+  pad_size = np.ceil(old_div(float(array.size),factor))*factor - array.size
+  array_padded = np.append(array, np.zeros([pad_size.astype(np.int64)])*np.NaN)
+  return scipy.nanmean(array_padded.reshape(-1,factor), axis=1)
+
+
+######################
+### Main experiments
+######################
+
+# Basic Q
+if True:
+  print("Running basic Q-learning.")
+  trial_results = []
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[state_count+1]).astype(np.float64)
+    Q[state_count] = 0
+    losses = []
+    for step_i in range(TRAIN_STEPS):
+      state = np.random.randint(0,state_count)
+      next_state, reward = step(state)
+      Q[state] = reward + Q[next_state]
+      losses.append(get_error(Q))
+    trial_results.append(losses)
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="Basic Q-learning", color=colors[0])
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[0])
+  with open('Toy-v1/baseline.csv', 'w') as f:
+    data = []
+    for frame_i in range(result.shape[0]):
+      for loss in result[frame_i]:
+        data.append("%f,%f,%f,%f" % (frame_i, frame_i, frame_i, loss))
+    f.write("\n".join(data))
+
+# Ensemble Q
+if True:
+  print("Running ensemble Q-learning.")
+  trial_results = []
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[Q_ENSEMBLE_SIZE, state_count+1]).astype(np.float64)
+    Q[:, state_count] = 0
+    losses = []
+    for step_i in range(TRAIN_STEPS):
+      for q_ensemble_i in range(Q_ENSEMBLE_SIZE):
+        state = np.random.randint(0,state_count)
+        next_state, reward = step(state)
+        Q[q_ensemble_i, state] = reward + np.mean(Q[:, next_state])
+      losses.append(get_error(np.mean(Q, axis=0)))
+    trial_results.append(losses)
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="Ensemble Q-learning", color=colors[1])
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[1])
+
+# Ensemble MVE-Oracle
+if True:
+  print("Running ensemble oracle MVE.")
+  trial_results = []
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[Q_ENSEMBLE_SIZE, state_count+1]).astype(np.float64)
+    Q[:, state_count] = 0
+    losses = []
+    for step_i in range(TRAIN_STEPS):
+      for q_ensemble_i in range(Q_ENSEMBLE_SIZE):
+        state = np.random.randint(0,state_count)
+        next_state, reward = step(state)
+
+        # MVE rollout
+        target = reward
+        for _ in range(HORIZON):
+          next_state, reward = step(next_state)
+          target += reward
+        target += np.mean(Q[:,next_state])
+
+        Q[q_ensemble_i, state] = target
+      losses.append(get_error(np.mean(Q, axis=0)))
+    trial_results.append(losses)
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="MVE-oracle", color=colors[2])
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[2])
+  with open('Toy-v1/mve_oracle.csv', 'w') as f:
+    data = []
+    for frame_i in range(result.shape[0]):
+      for loss in result[frame_i]:
+        data.append("%f,%f,%f,%f" % (frame_i, frame_i, frame_i, loss))
+    f.write("\n".join(data))
+
+# Ensemble MVE-Noisy
+if True:
+  print("Running ensemble noisy MVE.")
+  trial_results = []
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[Q_ENSEMBLE_SIZE, state_count+1]).astype(np.float64)
+    Q[:, state_count] = 0
+    losses = []
+    for step_i in range(TRAIN_STEPS):
+      for q_ensemble_i in range(Q_ENSEMBLE_SIZE):
+        state = np.random.randint(0,state_count)
+        next_state, reward = step(state)
+
+        # MVE rollout
+        targets = []
+        first_next_state, first_reward = next_state, reward
+        for model_ensemble_i in range(MODEL_ENSEMBLE_SIZE):
+          next_state, reward = first_next_state, first_reward
+          target = reward
+          for _ in range(HORIZON):
+            next_state, reward = noisy_step(next_state)
+            target += reward
+          target += np.mean(Q[:,next_state])
+          targets.append(target)
+
+        Q[q_ensemble_i, state] = np.mean(targets)
+      losses.append(get_error(np.mean(Q, axis=0)))
+    trial_results.append(losses)
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="MVE-noisy", color=colors[2], linestyle='dotted')
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[2])
+  with open('Toy-v1/mve_noisy.csv', 'w') as f:
+    data = []
+    for frame_i in range(result.shape[0]):
+      for loss in result[frame_i]:
+        data.append("%f,%f,%f,%f" % (frame_i, frame_i, frame_i, loss))
+    f.write("\n".join(data))
+
+# STEVE-Oracle
+if True:
+  print("Running ensemble oracle STEVE.")
+  trial_results = []
+
+  oracle_q_estimate_errors = []
+  oracle_mve_estimate_errors = []
+  oracle_steve_estimate_errors = []
+  oracle_opt_estimate_errors = []
+
+
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[Q_ENSEMBLE_SIZE, state_count+1]).astype(np.float64)
+    Q[:, state_count] = 0
+    losses = []
+
+    q_estimate_errors = []
+    mve_estimate_errors = []
+    steve_estimate_errors = []
+    opt_estimate_errors = []
+    steve_beat_freq= []
+
+    for step_i in range(TRAIN_STEPS):
+      _q_estimate_errors = []
+      _mve_estimate_errors = []
+      _steve_estimate_errors = []
+      _opt_estimate_errors = []
+      _steve_beat_freq = []
+
+      for q_ensemble_i in range(Q_ENSEMBLE_SIZE):
+        state = np.random.randint(0,state_count)
+        next_state, reward = step(state)
+
+        # STEVE rollout
+        Q_est_mat = np.zeros([HORIZON + 1, Q_ENSEMBLE_SIZE])
+        reward_est_mat = np.zeros([HORIZON + 1, 1])
+        first_next_state, first_reward = next_state, reward
+        next_state, reward = first_next_state, first_reward
+        Q_est_mat[0, :] = Q[:, next_state]
+        reward_est_mat[0, 0] = reward
+        for timestep_i in range(1,HORIZON+1):
+          next_state, reward = step(next_state)
+          Q_est_mat[timestep_i, :] = Q[:, next_state]
+          reward_est_mat[timestep_i, 0] = reward
+        all_targets = Q_est_mat + np.cumsum(reward_est_mat, axis=0)
+
+        # STEVE weight calculation
+        estimates = np.mean(all_targets, axis=1)
+        confidences = old_div(1., (np.var(all_targets, axis=1) + 1e-8))
+        coefficients = old_div(confidences, np.sum(confidences))
+        target = np.sum(estimates * coefficients)
+
+        Q[q_ensemble_i, state] = target
+
+        true_target = state + 1. if state != terminal_state else 0.
+        _q_estimate_errors.append(np.square(estimates[0] - true_target))
+        _mve_estimate_errors.append(np.square(estimates[-1] - true_target))
+        _steve_estimate_errors.append(np.square(np.sum(estimates * coefficients) - true_target))
+        _opt_estimate_errors.append(np.min(np.square(estimates - true_target)))
+
+      losses.append(get_error(np.mean(Q, axis=0)))
+      q_estimate_errors.append(np.mean(_q_estimate_errors))
+      mve_estimate_errors.append(np.mean(_mve_estimate_errors))
+      steve_estimate_errors.append(np.mean(_steve_estimate_errors))
+      opt_estimate_errors.append(np.mean(_opt_estimate_errors))
+    trial_results.append(losses)
+    oracle_q_estimate_errors.append(q_estimate_errors)
+    oracle_mve_estimate_errors.append(mve_estimate_errors)
+    oracle_steve_estimate_errors.append(steve_estimate_errors)
+    oracle_opt_estimate_errors.append(opt_estimate_errors)
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="STEVE-oracle", color=colors[3])
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[3])
+  with open('Toy-v1/steve_oracle.csv', 'w') as f:
+    data = []
+    for frame_i in range(result.shape[0]):
+      for loss in result[frame_i]:
+        data.append("%f,%f,%f,%f" % (frame_i, frame_i, frame_i, loss))
+    f.write("\n".join(data))
+
+# STEVE-Noisy
+if True:
+  print("Running ensemble noisy STEVE.")
+  trial_results = []
+
+  noisy_q_estimate_errors = []
+  noisy_mve_estimate_errors = []
+  noisy_steve_estimate_errors = []
+  noisy_opt_estimate_errors = []
+  noisy_steve_beat_freq = []
+
+  for run_i in range(TRIAL_N):
+    print("Trial %d" % run_i)
+    Q = np.random.randint(0,state_count,[Q_ENSEMBLE_SIZE, state_count+1]).astype(np.float64)
+    Q[:, state_count] = 0
+    losses = []
+
+    q_estimate_errors = []
+    mve_estimate_errors = []
+    steve_estimate_errors = []
+    opt_estimate_errors = []
+    steve_beat_freq= []
+
+    for step_i in range(TRAIN_STEPS):
+      _q_estimate_errors = []
+      _mve_estimate_errors = []
+      _steve_estimate_errors = []
+      _opt_estimate_errors = []
+      _steve_beat_freq = []
+      for q_ensemble_i in range(Q_ENSEMBLE_SIZE):
+        state = np.random.randint(0,state_count)
+        next_state, reward = step(state)
+
+        # STEVE rollout
+        Q_est_mat = np.zeros([HORIZON + 1, MODEL_ENSEMBLE_SIZE, Q_ENSEMBLE_SIZE])
+        reward_est_mat = np.zeros([HORIZON + 1, MODEL_ENSEMBLE_SIZE, 1])
+        first_next_state, first_reward = next_state, reward
+        for model_ensemble_i in range(MODEL_ENSEMBLE_SIZE):
+          next_state, reward = first_next_state, first_reward
+          Q_est_mat[0, model_ensemble_i, :] = Q[:, next_state]
+          reward_est_mat[0, model_ensemble_i, 0] = reward
+          for timestep_i in range(1,HORIZON+1):
+            next_state, reward = noisy_step(next_state)
+            Q_est_mat[timestep_i, model_ensemble_i, :] = Q[:, next_state]
+            reward_est_mat[timestep_i, model_ensemble_i, 0] = reward
+        all_targets = Q_est_mat + np.cumsum(reward_est_mat, axis=0)
+
+        # STEVE weight calculation
+        all_targets = np.reshape(all_targets, [HORIZON+1, MODEL_ENSEMBLE_SIZE * Q_ENSEMBLE_SIZE])
+        estimates = np.mean(all_targets, axis=1)
+        confidences = old_div(1., (np.var(all_targets, axis=1) + 1e-8))
+        coefficients = old_div(confidences, np.sum(confidences))
+        target = np.sum(estimates * coefficients)
+        # target = estimates[0]
+
+        Q[q_ensemble_i, state] = target
+
+        true_target = state + 1. if state != terminal_state else 0.
+        _q_estimate_errors.append(np.square(estimates[0] - true_target))
+        _mve_estimate_errors.append(np.square(estimates[-1] - true_target))
+        _steve_estimate_errors.append(np.square(np.sum(estimates * coefficients) - true_target))
+        _opt_estimate_errors.append(np.min(np.square(estimates - true_target)))
+        _steve_beat_freq.append(float(np.square(estimates[0] - true_target) > np.square(target - true_target)))
+
+      losses.append(get_error(np.mean(Q, axis=0)))
+      q_estimate_errors.append(np.mean(_q_estimate_errors))
+      mve_estimate_errors.append(np.mean(_mve_estimate_errors))
+      steve_estimate_errors.append(np.mean(_steve_estimate_errors))
+      opt_estimate_errors.append(np.mean(_opt_estimate_errors))
+      steve_beat_freq.append(np.mean(_steve_beat_freq))
+    trial_results.append(losses)
+    noisy_q_estimate_errors.append(q_estimate_errors)
+    noisy_mve_estimate_errors.append(mve_estimate_errors)
+    noisy_steve_estimate_errors.append(steve_estimate_errors)
+    noisy_opt_estimate_errors.append(opt_estimate_errors)
+    noisy_steve_beat_freq.append(steve_beat_freq)
+
+  print("...complete.\n")
+
+  result = np.stack(trial_results, axis=1)
+  means = np.mean(result, axis=1)
+  stdevs = np.std(result, axis=1)
+  plt.plot(means, label="STEVE-noisy", color=colors[3], linestyle='dotted')
+  plt.fill_between(np.arange(TRAIN_STEPS), means - stdevs, means + stdevs, alpha=.2, color=colors[3])
+  with open('Toy-v1/steve_noisy.csv', 'w') as f:
+    data = []
+    for frame_i in range(result.shape[0]):
+      for loss in result[frame_i]:
+        data.append("%f,%f,%f,%f" % (frame_i, frame_i, frame_i, loss))
+    f.write("\n".join(data))
+
+# ### Display results
+# plt.title("Comparison of convergence rates")
+# plt.legend()
+# plt.savefig("comparison.pdf")
+# plt.show()
+#
+# ### Display secondary results - error comparison
+# DOWNSAMPLE = 50
+# colors = sns.color_palette('husl', 8)
+# for i, (error_curve, label) in enumerate([
+#                                           (oracle_q_estimate_errors, "Oracle Q error"),
+#                                           (oracle_mve_estimate_errors, "Oracle MVE error"),
+#                                           (oracle_steve_estimate_errors, "Oracle STEVE error"),
+#                                           # (oracle_opt_estimate_errors, "Oracle minimum single-estimate error"),
+#                                          ]):
+#   result = np.stack(error_curve, axis=1)
+#   means = downsample(np.mean(result, axis=1), DOWNSAMPLE)
+#   stdevs = downsample(np.std(result, axis=1), DOWNSAMPLE)
+#   plt.plot(means, label=label, color=colors[i])
+#   plt.fill_between(np.arange(means.shape[0]), means - stdevs, means + stdevs, alpha=.2, color=colors[i])
+#
+# plt.title("Comparison of errors for oracle dynamics")
+# plt.legend()
+# plt.show()
+#
+# for i, (error_curve, label) in enumerate([
+#                                           (noisy_q_estimate_errors, "Noisy Q error"),
+#                                           (noisy_mve_estimate_errors, "Noisy MVE error"),
+#                                           (noisy_steve_estimate_errors, "Noisy STEVE error"),
+#                                           # (noisy_opt_estimate_errors, "Noisy minimum single-estimate error"),
+#                                           # (trial_steve_beat_freq, "STEVE beat freq"),
+#                                         ]):
+#   result = np.stack(error_curve, axis=1)
+#   means = downsample(np.mean(result, axis=1), DOWNSAMPLE)
+#   stdevs = downsample(np.std(result, axis=1), DOWNSAMPLE)
+#   plt.plot(means, label=label, color=colors[i])
+#   plt.fill_between(np.arange(means.shape[0]), means - stdevs, means + stdevs, alpha=.2, color=colors[i])
+#
+# plt.title("Comparison of errors for noisy dynamics")
+# plt.legend()
+# plt.show()
\ No newline at end of file
diff --git a/research/steve/util.py b/research/steve/util.py
new file mode 100644
index 00000000..bf0abec0
--- /dev/null
+++ b/research/steve/util.py
@@ -0,0 +1,164 @@
+from __future__ import division
+from future import standard_library
+standard_library.install_aliases()
+from builtins import str
+from builtins import range
+from past.utils import old_div
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import numpy as np
+import tensorflow as tf
+import os, random, gc, math, re
+import multiprocessing, types, shutil, pickle, json
+from collections import defaultdict, MutableMapping
+
+def tanh_sample_info(mu, logsigma, stop_action_gradient=False, n_samples=1):
+    if n_samples > 1:
+      mu = tf.expand_dims(mu, 2)
+      logsigma = tf.expand_dims(logsigma, 2)
+      sample_shape = tf.concat([tf.shape(mu), n_samples], 0)
+    else:
+      sample_shape = tf.shape(mu)
+
+    flat_act = mu + tf.random_normal(sample_shape) * tf.exp(logsigma)
+    if stop_action_gradient: flat_act = tf.stop_gradient(flat_act)
+    normalized_dist_t = (flat_act - mu) * tf.exp(-logsigma)  # ... x D
+    quadratic = - 0.5 * tf.reduce_sum(normalized_dist_t ** 2, axis=-1) # ... x (None)
+    log_z = tf.reduce_sum(logsigma, axis=-1)  # ... x (None)
+    D_t = tf.cast(tf.shape(mu)[-1], tf.float32)
+    log_z += 0.5 * D_t * np.log(2 * np.pi)
+    flat_ll = quadratic - log_z
+
+    scaled_act = tf.tanh(flat_act)
+    corr = tf.reduce_sum(tf.log(1. - tf.square(scaled_act) + 1e-6), axis=-1)
+    scaled_ll = flat_ll - corr
+    return flat_act, flat_ll, scaled_act, scaled_ll
+
+def tf_cheating_contcartpole(state, action):
+    gravity = 9.8
+    masscart = 1.0
+    masspole = 0.1
+    total_mass = (masspole + masscart)
+    length = 0.5 # actually half the pole's length
+    polemass_length = (masspole * length)
+    force_mag = 10.0
+    tau = 0.02  # seconds between state updates
+
+    # Angle at which to fail the episode
+    theta_threshold_radians = 12 * 2 * math.pi / 360
+    x_threshold = 2.4
+
+    x, x_dot, theta, theta_dot = tf.split(state, 4, axis=-1)
+    done =  tf.logical_or(x < -x_threshold,
+                          tf.logical_or(x > x_threshold,
+                          tf.logical_or(theta < -theta_threshold_radians,
+                                        theta > theta_threshold_radians)))
+
+    force = force_mag * action
+    costheta = tf.cos(theta)
+    sintheta = tf.sin(theta)
+    temp = old_div((force + polemass_length * theta_dot * theta_dot * sintheta), total_mass)
+    thetaacc = old_div((gravity * sintheta - costheta* temp), (length * (old_div(4.0,3.0) - masspole * costheta * costheta / total_mass)))
+    xacc  = temp - polemass_length * thetaacc * costheta / total_mass
+    x  = x + tau * x_dot
+    x_dot = x_dot + tau * xacc
+    theta = theta + tau * theta_dot
+    theta_dot = theta_dot + tau * thetaacc
+    state = tf.concat([x,x_dot,theta,theta_dot], -1)
+    done = tf.squeeze(tf.cast(done, tf.float32), -1)
+    reward = 1.0 - done
+    done *= 0.
+    return state, reward, done
+
+def create_directory(dir):
+    dir_chunks = dir.split("/")
+    for i in range(len(dir_chunks)):
+        partial_dir = "/".join(dir_chunks[:i+1])
+        try:
+            os.makedirs(partial_dir)
+        except OSError:
+            pass
+    return dir
+
+def create_and_wipe_directory(dir):
+    shutil.rmtree(create_directory(dir))
+    create_directory(dir)
+
+def wipe_file(fname):
+    with open(fname, "w") as f:
+        f.write("")
+    return fname
+
+def get_largest_epoch_in_dir(dir, saveid):
+    reg_matches = [re.findall('\d+_%s'%saveid,filename) for filename in os.listdir(dir)]
+    epoch_labels = [int(regmatch[0].split("_")[0]) for regmatch in reg_matches if regmatch]
+    if len(epoch_labels) == 0: return False
+    return max(epoch_labels)
+
+def wipe_all_but_largest_epoch_in_dir(dir, saveid):
+    largest = get_largest_epoch_in_dir(dir, saveid)
+    reg_matches = [(filename, re.findall('\d+_%s'%saveid,filename)) for filename in os.listdir(dir)]
+    for filename, regmatch in reg_matches:
+        if regmatch and int(regmatch[0].split("_")[0]) != largest:
+            os.remove(os.path.join(dir,filename))
+
+class ConfigDict(dict):
+    def __init__(self, loc=None, ghost=False):
+        self._dict = defaultdict(lambda :False)
+        self.ghost = ghost
+        if loc:
+            with open(loc) as f: raw = json.load(f)
+            if "inherits" in raw and raw["inherits"]:
+                for dep_loc in raw["inherits"]:
+                    self.update(ConfigDict(dep_loc))
+            if "updates" in raw and raw["updates"]:
+                self.update(raw["updates"], include_all=True)
+
+    def __getitem__(self, key):
+        return self._dict[key]
+
+    def __setitem__(self, key, value):
+        self._dict[key] = value
+
+    def __str__(self):
+        return str(dict(self._dict))
+
+    def __repr__(self):
+        return str(dict(self._dict))
+
+    def __iter__(self):
+        return self._dict.__iter__()
+
+    def __bool__(self):
+        return bool(self._dict)
+
+    def __nonzero__(self):
+        return bool(self._dict)
+
+    def update(self, dictlike, include_all=False):
+        for key in dictlike:
+            value = dictlike[key]
+            if isinstance(value, dict):
+                if key[0] == "*": # this means only override, do not set
+                    key = key[1:]
+                    ghost = True
+                else:
+                    ghost = False
+                if not include_all and isinstance(value, ConfigDict) and key not in self._dict and value.ghost: continue
+                if key not in self._dict: self._dict[key] = ConfigDict(ghost=ghost)
+                self._dict[key].update(value)
+            else:
+                self._dict[key] = value
diff --git a/research/steve/valuerl.py b/research/steve/valuerl.py
new file mode 100644
index 00000000..4819dd08
--- /dev/null
+++ b/research/steve/valuerl.py
@@ -0,0 +1,307 @@
+from __future__ import division
+from builtins import zip
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+import numpy as np
+import nn
+import util
+from learner import CoreModel
+
+
+class ValueRL(CoreModel):
+  """
+  Learn a state-action value function and its corresponding policy.
+  """
+
+  @property
+  def saveid(self):
+    return "valuerl"
+
+  def create_params(self, env_config, learner_config):
+    self.obs_dim = np.prod(env_config["obs_dims"])
+    self.action_dim = env_config["action_dim"]
+    self.reward_scale = env_config["reward_scale"]
+    self.discount = env_config["discount"]
+
+    self.hidden_dim = learner_config["hidden_dim"]
+    self.bayesian_config = learner_config["bayesian"]
+    self.value_expansion = learner_config["value_expansion"]
+    self.explore_chance = learner_config["ddpg_explore_chance"]
+
+    with tf.variable_scope(self.name):
+      self.policy = nn.FeedForwardNet('policy', self.obs_dim, [self.action_dim], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=False)
+
+      if self.bayesian_config:
+        self.Q = nn.EnsembleFeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config["ensemble_size"], train_sample_count=self.bayesian_config["train_sample_count"], eval_sample_count=self.bayesian_config["eval_sample_count"])
+        self.old_Q = nn.EnsembleFeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config["ensemble_size"], train_sample_count=self.bayesian_config["train_sample_count"], eval_sample_count=self.bayesian_config["eval_sample_count"])
+      else:
+        self.Q = nn.FeedForwardNet('Q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)
+        self.old_Q = nn.FeedForwardNet('old_q', self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.hidden_dim, get_uncertainty=True)
+
+    self.policy_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if "policy" in v.name]
+    self.Q_params = [v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name) if "Q" in v.name]
+    self.agent_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)
+
+    self.copy_to_old_ops = [tf.assign(p_old, p) for p_old, p in zip(self.old_Q.params_list, self.Q.params_list)]
+    self.assign_epoch_op = [tf.assign(self.epoch_n, self.epoch_n_placeholder), tf.assign(self.update_n, self.update_n_placeholder), tf.assign(self.frame_n, self.frame_n_placeholder), tf.assign(self.hours, self.hours_placeholder)]
+
+  def update_epoch(self, sess, epoch, updates, frames, hours):
+    sess.run(self.assign_epoch_op, feed_dict={self.epoch_n_placeholder: int(epoch), self.update_n_placeholder: int(updates), self.frame_n_placeholder: int(frames), self.hours_placeholder: float(hours)})
+
+  def copy_to_old(self, sess):
+    sess.run(self.copy_to_old_ops)
+
+  def build_evalution_graph(self, obs, get_full_info=False, mode="regular", n_samples=1):
+    assert mode in {"regular", "explore", "exploit"}
+    policy_actions_pretanh = self.policy(obs)
+
+    if mode == "regular" or mode == "exploit":
+      policy_actions = tf.tanh(policy_actions_pretanh)
+    elif mode == "explore":
+      _, _, exploring_policy_actions, _ = util.tanh_sample_info(policy_actions_pretanh, tf.zeros_like(policy_actions_pretanh), n_samples=n_samples)
+      policy_actions = tf.where(tf.random_uniform(tf.shape(exploring_policy_actions)) < self.explore_chance, x=exploring_policy_actions, y=tf.tanh(policy_actions_pretanh))
+    else: raise Exception('this should never happen')
+
+    if get_full_info:     return policy_actions_pretanh, policy_actions
+    else:                 return policy_actions
+
+  def build_training_graph(self, obs, next_obs, empirical_actions, rewards, dones, data_size, worldmodel=None):
+    average_model_use = tf.constant(0.)
+    empirical_Q_info = tf.concat([obs, empirical_actions], 1)
+
+    if worldmodel is None:
+      policy_action_pretanh, policy_actions = self.build_evalution_graph(obs, get_full_info=True)
+      policy_Q_info = tf.concat([obs, policy_actions], 1)
+      state_value_estimate = self.Q(policy_Q_info, reduce_mode="mean")
+
+      next_policy_actions = self.build_evalution_graph(next_obs)
+      policy_next_Q_info = tf.concat([next_obs, next_policy_actions], 1)
+      next_Q_estimate = self.old_Q(policy_next_Q_info, reduce_mode="mean")
+
+      Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode="random")
+      Q_target = rewards * self.reward_scale + self.discount * next_Q_estimate * (1. - dones)
+
+      policy_losses = -state_value_estimate
+      Q_losses = .5 * tf.square( Q_guess - tf.stop_gradient(Q_target) )
+
+    else:
+      targets, confidence, Q_guesses, reach_probs = self.build_Q_expansion_graph(next_obs, rewards, dones, worldmodel, rollout_len=self.value_expansion["rollout_len"], model_ensembling=worldmodel.bayesian_config is not False)
+
+      # targets is a 3D matrix: [batch_i, start_timestep, end_timestep]. here, we reduce out the last dimension, turning
+      # it into a [batch_i, start_timestep] matrix. in other words, we are taking a bunch of candidate targets and reducing
+      # them into a single target. the four options here correspond to the four ways to do that reduction.
+      if self.value_expansion["mean_k_return"]:
+        target_counts = self.value_expansion["rollout_len"]+1 - tf.reshape(tf.range(self.value_expansion["rollout_len"]+1), [1, self.value_expansion["rollout_len"]+1])
+        k_returns = tf.reduce_sum(targets, 2) / tf.cast(target_counts, tf.float32)
+      elif self.value_expansion["lambda_return"]:
+        cont_coeffs = self.value_expansion["lambda_return"] ** tf.cast(tf.reshape(tf.range(self.value_expansion["rollout_len"]+1), [1,1,self.value_expansion["rollout_len"]+1]), tf.float32)
+        stop_coeffs = tf.concat([(1 - self.value_expansion["lambda_return"]) * tf.ones_like(targets)[:,:,:-1], tf.ones_like(targets)[:,:,-1:]], 2)
+        k_returns = tf.reduce_sum(targets * stop_coeffs * cont_coeffs, 2)
+      elif self.value_expansion["steve_reweight"]:
+        k_returns = tf.reduce_sum(targets * confidence, 2)
+        average_model_use = 1. - tf.reduce_mean(confidence[:,0,0])
+      else:
+        # MVE objective: just take the last one
+        k_returns = targets[:,:,-1]
+
+      # now we have [batch_i, start_timestep]. if we are using the TDK trick, then we want to use all of the targets,
+      # so we construct a corresponding [batch_i, start_timestep] matrix of guesses. otherwise, we just take the targets
+      # for the first timestep.
+      Q_guess = self.Q(empirical_Q_info, is_eval=False, reduce_mode="random")
+      if self.value_expansion["tdk_trick"]:
+        Q_guess = tf.concat([tf.expand_dims(Q_guess, 1), Q_guesses], 1)
+        reach_probs = tf.concat([tf.expand_dims(tf.ones_like(reach_probs[:,0]), 1), reach_probs[:,:-1]], 1)
+        Q_target = k_returns
+      else:
+        # non-TDK trick means we just take the first one
+        Q_target = k_returns[:,0]
+
+      policy_action_pretanh, policy_actions = self.build_evalution_graph(obs, get_full_info=True)
+      policy_Q_info = tf.concat([obs, policy_actions], 1)
+      state_value_estimate = self.Q(policy_Q_info, stop_params_gradient=True, reduce_mode="mean")
+
+      policy_losses = -state_value_estimate
+      Q_losses = .5 * tf.square( Q_guess - tf.stop_gradient(Q_target) )
+      if self.value_expansion["tdk_trick"]: Q_losses *= reach_probs # we downscale the various TDK-trick losses by
+                                                                    # the likelihood of actually reaching the state
+                                                                    # from which the guess was made
+    policy_loss = tf.reduce_mean(policy_losses)
+    Q_loss = tf.reduce_mean(Q_losses)
+    policy_reg_loss = tf.reduce_mean(tf.square(policy_action_pretanh)) * .001 # a small regularization to make sure the
+                                                                              # tanh does not saturate
+
+    # anything in inspect gets logged
+    inspect = (policy_loss, Q_loss, policy_reg_loss, average_model_use)
+
+    return (policy_loss + policy_reg_loss, Q_loss), inspect
+
+
+  def build_Q_expansion_graph(self, obs, first_rewards, first_done, worldmodel, rollout_len=1, model_ensembling=False):
+    ### this sets up the machinery for having multiple parallel rollouts, each of which has a single consistent transition
+    ensemble_idxs, transition_sample_n, reward_sample_n = worldmodel.get_ensemble_idx_info()
+    q_sample_n = self.bayesian_config["eval_sample_count"] if self.bayesian_config is not False else 1
+    first_rewards = tf.tile(tf.expand_dims(tf.expand_dims(first_rewards,1),1), [1,transition_sample_n,reward_sample_n])
+    first_rewards.set_shape([None, transition_sample_n, reward_sample_n])
+    if model_ensembling:
+      obs = tf.tile(tf.expand_dims(obs,1), [1,transition_sample_n,1])
+      obs.set_shape([None, transition_sample_n, self.obs_dim])
+      first_done = tf.tile(tf.expand_dims(first_done, 1), [1, transition_sample_n])
+      first_done.set_shape([None, transition_sample_n])
+
+    ### below, we use a while loop to actually do the iterative model rollout
+    extra_info = worldmodel.init_extra_info(obs)
+
+    action_ta = tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)
+    obs_ta =       tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)
+    done_ta =     tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)
+    extra_info_ta =tf.TensorArray(size=rollout_len, dynamic_size=False, dtype=tf.float32)
+
+    def rollout_loop_body(r_i, xxx_todo_changeme):
+      (obs, done, extra_info, action_ta, obs_ta, dones_ta, extra_info_ta) = xxx_todo_changeme
+      action_pretanh, action = self.build_evalution_graph(tf.stop_gradient(obs), get_full_info=True)
+
+      if model_ensembling:
+        next_obs, next_dones, next_extra_info = worldmodel.transition(obs, action, extra_info, ensemble_idxs=ensemble_idxs)
+      else:
+        next_obs, next_dones, next_extra_info = worldmodel.transition(obs, action, extra_info)
+        next_obs = tf.reduce_mean(next_obs, -2)
+        next_dones = tf.reduce_mean(next_dones, -1)
+
+      action_ta = action_ta.write(r_i, action)
+      obs_ta = obs_ta.write(r_i, obs)
+      dones_ta = dones_ta.write(r_i, done)
+      extra_info_ta = extra_info_ta.write(r_i, extra_info)
+      return r_i+1, (next_obs, next_dones, next_extra_info, action_ta, obs_ta, dones_ta, extra_info_ta)
+
+    _, (final_obs, final_done, final_extra_info, action_ta, obs_ta, done_ta, extra_info_ta) = tf.while_loop(
+        lambda r_i, _: r_i < rollout_len,
+        rollout_loop_body,
+        [0, (obs, first_done, extra_info, action_ta, obs_ta, done_ta, extra_info_ta)]
+    )
+
+    final_action_pretanh, final_action = self.build_evalution_graph(tf.stop_gradient(final_obs), get_full_info=True)
+
+    ### compile the TensorArrays into useful tensors
+    obss = obs_ta.stack()
+    obss = tf.reshape(obss, tf.stack([rollout_len, -1, transition_sample_n, self.obs_dim]))
+    obss = tf.transpose(obss, [1, 0, 2, 3])
+    final_obs = tf.reshape(final_obs, tf.stack([-1, 1, transition_sample_n, self.obs_dim]))
+    all_obss = tf.concat([obss, final_obs],1)
+    next_obss = all_obss[:,1:]
+
+    dones = done_ta.stack()
+    dones = tf.reshape(dones, tf.stack([rollout_len, -1, transition_sample_n]))
+    dones = tf.transpose(dones, [1, 0, 2])
+    final_done = tf.reshape(final_done, tf.stack([-1, 1, transition_sample_n]))
+    all_dones = tf.concat([dones, final_done],1)
+
+    actions = action_ta.stack()
+    actions = tf.reshape(actions, tf.stack([rollout_len, -1, transition_sample_n, self.action_dim]))
+    actions = tf.transpose(actions , [1, 0, 2, 3])
+    final_action = tf.reshape(final_action, tf.stack([-1, 1, transition_sample_n, self.action_dim]))
+    all_actions = tf.concat([actions, final_action],1)
+
+    continue_probs = tf.cumprod(1. - all_dones, axis=1)
+    rewards = worldmodel.get_rewards(obss, actions, next_obss)
+    rawrew = rewards = tf.concat([tf.expand_dims(first_rewards, 1), rewards],1)
+
+    ### TDK trick means we have to guess at every timestep
+    if self.value_expansion["tdk_trick"]:
+      guess_info = tf.concat([obss,actions], -1)
+      Q_guesses = self.Q(guess_info, reduce_mode="random")
+      Q_guesses = tf.reduce_mean(Q_guesses, -1) # make it so there's only one guess per rollout length, which is the mean of the guesses under all the various model rollouts
+      reached_this_point_to_guess_prob = tf.reduce_mean(continue_probs, -1)
+    else:
+      Q_guesses = None
+      reached_this_point_to_guess_prob = None
+
+    ### use the Q function at every timestep to get value estimates
+    target_info = tf.concat([all_obss, all_actions], -1)
+    Q_targets = self.old_Q(target_info, reduce_mode="none")
+
+    rollout_frames = rollout_len + 1 # if we take N steps, we have N+1 frames
+
+    ### create "decay-exponent matrix" of size [1,ROLLOUT_FRAMES,ROLLOUT_FRAMES,1]. the first ROLLOUT_FRAMES corresponds to the index of the source, the second to the target.
+    ts_count_mat = (tf.cast(tf.reshape(tf.range(rollout_frames), [1, rollout_frames]) - tf.reshape(tf.range(rollout_frames), [rollout_frames, 1]), tf.float32))
+    reward_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** ts_count_mat
+    value_coeff_matrix = tf.matrix_band_part(tf.ones([rollout_frames, rollout_frames]), 0, -1) * self.discount ** (1. + ts_count_mat)
+    reward_coeff_matrix = tf.reshape(reward_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])
+    value_coeff_matrix = tf.reshape(value_coeff_matrix, [1, rollout_frames, rollout_frames, 1, 1])
+
+    ### similarly, create a "done" matrix
+    shifted_continue_probs = tf.concat([tf.expand_dims(tf.ones_like(continue_probs[:,0]),1), continue_probs[:,:-1]], 1)
+    reward_continue_matrix = tf.expand_dims(shifted_continue_probs, 1) / tf.expand_dims(shifted_continue_probs+1e-8, 2)
+    value_continue_matrix = tf.expand_dims(continue_probs, 1) / tf.expand_dims(shifted_continue_probs+1e-8, 2)
+    reward_continue_matrix = tf.expand_dims(reward_continue_matrix, -1)
+    value_continue_matrix = tf.expand_dims(value_continue_matrix, -1)
+
+    ### apply the discounting factors to the rewards and values
+    rewards = tf.expand_dims(rewards, 1) * reward_coeff_matrix * reward_continue_matrix
+    rewards = tf.cumsum(rewards, axis=2)
+    values = tf.expand_dims(Q_targets, 1) * value_coeff_matrix * value_continue_matrix
+
+    ### compute the targets using the Bellman equation
+    sampled_targets = tf.expand_dims(rewards,-2) * self.reward_scale + tf.expand_dims(values,-1)
+
+    ### flatten out the various sources of variance (transition, reward, and Q-function ensembles) to get a set of estimates for each candidate target
+    sampled_targets = tf.reshape(sampled_targets, tf.stack([-1, rollout_frames, rollout_frames, transition_sample_n * reward_sample_n * q_sample_n]))
+
+    ### compute the mean and variance for each candidate target
+    target_means, target_variances = tf.nn.moments(sampled_targets, 3)
+
+    ### compute the confidence, either using the full covariance matrix, or approximating all the estimators as independent
+    if self.value_expansion["covariances"]:
+      targetdiffs = sampled_targets - tf.expand_dims(target_means,3)
+      target_covariances = tf.einsum("abij,abjk->abik", targetdiffs, tf.transpose(targetdiffs, [0,1,3,2]))
+      target_confidence = tf.squeeze(tf.matrix_solve(target_covariances + tf.expand_dims(tf.expand_dims(tf.matrix_band_part(tf.ones(tf.shape(target_covariances)[-2:]),0,0) * 1e-3,0),0), tf.ones(tf.concat([tf.shape(target_covariances)[:-1], tf.constant([1])],0))),-1)
+    else:
+      target_confidence = 1./(target_variances + 1e-8)
+
+    ### normalize so weights sum to 1
+    target_confidence *= tf.matrix_band_part(tf.ones([1, rollout_frames, rollout_frames]), 0, -1)
+    target_confidence = target_confidence / tf.reduce_sum(target_confidence, axis=2, keepdims=True)
+
+    ### below here is a bunch of debugging Print statements that I use as a sanity check:
+    # target_confidence = tf.Print(target_confidence, [], message="raw rewards")
+    # target_confidence = tf.Print(target_confidence, [rawrew[0,:,0,0]], summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [], message="\n", summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [(1. - all_dones)[0,:,0]], message="contin", summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [continue_probs[0,:,0]], message="cum_contin", summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [shifted_continue_probs[0,:,0]], message="shifted contin", summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [], message="reward_coeff")
+    # for i in range(rollout_len+1): target_means = tf.Print(target_means, [reward_coeff_matrix[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [], message="reward_continue")
+    # for i in range(rollout_len+1): target_means = tf.Print(target_means, [reward_continue_matrix[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [], message="value_coeff")
+    # for i in range(rollout_len+1): target_means = tf.Print(target_means, [value_coeff_matrix[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [], message="value_continue")
+    # for i in range(rollout_len+1): target_means = tf.Print(target_means, [value_continue_matrix[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="rewards")
+    # for i in range(rollout_len+1): target_confidence = tf.Print(target_confidence, [rewards[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="target Qs")
+    # target_confidence = tf.Print(target_confidence, [Q_targets[0,:,0,0]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="values")
+    # for i in range(rollout_len+1): target_confidence = tf.Print(target_confidence, [values[0,i,:,0,0]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="target_means")
+    # for i in range(rollout_len+1): target_confidence = tf.Print(target_confidence, [target_means[0,i,:]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="target_variance")
+    # for i in range(rollout_len+1): target_confidence = tf.Print(target_confidence, [target_variances[0,i,:]], summarize=rollout_len+1)
+    # target_confidence = tf.Print(target_confidence, [], message="target_confidence")
+    # for i in range(rollout_len+1): target_confidence = tf.Print(target_confidence, [target_confidence[0,i,:]], summarize=rollout_len+1)
+    # target_means = tf.Print(target_means, [target_confidence, action_lls, tf.shape(Q_targets)], message="\n\n", summarize=10)
+
+    return target_means, target_confidence, Q_guesses, reached_this_point_to_guess_prob
\ No newline at end of file
diff --git a/research/steve/valuerl_learner.py b/research/steve/valuerl_learner.py
new file mode 100644
index 00000000..a3c6308f
--- /dev/null
+++ b/research/steve/valuerl_learner.py
@@ -0,0 +1,81 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+import numpy as np
+import os
+
+from learner import Learner
+from valuerl import ValueRL
+from worldmodel import DeterministicWorldModel
+
+class ValueRLLearner(Learner):
+  """
+  ValueRL-specific training loop details.
+  """
+
+  def learner_name(self): return "valuerl"
+
+  def make_loader_placeholders(self):
+    self.obs_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], np.prod(self.env_config["obs_dims"])])
+    self.next_obs_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], np.prod(self.env_config["obs_dims"])])
+    self.action_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], self.env_config["action_dim"]])
+    self.reward_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"]])
+    self.done_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"]])
+    self.datasize_loader = tf.placeholder(tf.float64, [])
+    return [self.obs_loader, self.next_obs_loader, self.action_loader, self.reward_loader, self.done_loader, self.datasize_loader]
+
+  def make_core_model(self):
+    if self.config["model_config"] is not False:
+        self.worldmodel = DeterministicWorldModel(self.config["name"], self.env_config, self.config["model_config"])
+    else:
+        self.worldmodel = None
+
+    valuerl = ValueRL(self.config["name"], self.env_config, self.learner_config)
+    (policy_loss, Q_loss), inspect_losses = valuerl.build_training_graph(*self.current_batch, worldmodel=self.worldmodel)
+
+    policy_optimizer = tf.train.AdamOptimizer(3e-4)
+    policy_gvs = policy_optimizer.compute_gradients(policy_loss, var_list=valuerl.policy_params)
+    capped_policy_gvs = policy_gvs
+    policy_train_op = policy_optimizer.apply_gradients(capped_policy_gvs)
+
+    Q_optimizer = tf.train.AdamOptimizer(3e-4)
+    Q_gvs = Q_optimizer.compute_gradients(Q_loss, var_list=valuerl.Q_params)
+    capped_Q_gvs = Q_gvs
+    Q_train_op = Q_optimizer.apply_gradients(capped_Q_gvs)
+
+    return valuerl, (policy_loss, Q_loss), (policy_train_op, Q_train_op), inspect_losses
+
+  ## Optional functions to override
+  def initialize(self):
+      if self.config["model_config"] is not False:
+          while not self.load_worldmodel(): pass
+
+  def resume_from_checkpoint(self, epoch):
+      if self.config["model_config"] is not False:
+          with self.bonus_kwargs["model_lock"]: self.worldmodel.load(self.sess, self.save_path, epoch)
+
+  def checkpoint(self):
+      self.core.copy_to_old(self.sess)
+      if self.config["model_config"] is not False:
+          self.load_worldmodel()
+
+  def backup(self): pass
+
+  # Other functions
+  def load_worldmodel(self):
+      if not os.path.exists("%s/%s.params.index" % (self.save_path, self.worldmodel.saveid)): return False
+      with self.bonus_kwargs["model_lock"]: self.worldmodel.load(self.sess, self.save_path)
+      return True
diff --git a/research/steve/visualizer.py b/research/steve/visualizer.py
new file mode 100644
index 00000000..825f1a23
--- /dev/null
+++ b/research/steve/visualizer.py
@@ -0,0 +1,107 @@
+from __future__ import print_function
+from builtins import range
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import numpy as np
+import tensorflow as tf
+# import moviepy.editor as mpy
+import time, os, traceback, multiprocessing, portalocker, sys
+
+import envwrap
+import util
+import valuerl, worldmodel
+from config import config
+
+MODEL_NAME = config["name"]
+LOG_PATH = util.create_directory("output/" + config["env"] + "/" + MODEL_NAME + "/" + config["log_path"]) + "/" + MODEL_NAME
+LOAD_PATH =    util.create_directory("output/" + config["env"] + "/" + MODEL_NAME + "/" + config["save_model_path"])
+OBS_DIM =   np.prod(config["obs_dims"])
+HIDDEN_DIM = config["hidden_dim"]
+ACTION_DIM = config["action_dim"]
+MAX_FRAMES = config["max_frames"]
+REWARD_SCALE = config["reward_scale"]
+DISCOUNT = config["discount"]
+ALGO = config["policy_config"]["algo"]
+AGENT_BATCH_SIZE = config["agent_config"]["batch_size"]
+EVALUATOR_BATCH_SIZE = config["evaluator_config"]["batch_size"]
+RELOAD_EVERY_N = config["agent_config"]["reload_every_n"]
+FRAMES_BEFORE_LEARNING = config["policy_config"]["frames_before_learning"]
+FRAMES_PER_UPDATE = config["policy_config"]["frames_per_update"]
+LEARNER_EPOCH_N = config["policy_config"]["epoch_n"]
+SYNC_UPDATES = config["policy_config"]["frames_per_update"] >= 0
+POLICY_BAYESIAN_CONFIG = config["policy_config"]["bayesian"]
+AUX_CONFIG = config["aux_config"]
+DDPG_EXPLORE_CHANCE = config["policy_config"]["explore_chance"] if ALGO == "ddpg" else 0.
+MODEL_AUGMENTED = config["model_config"] is not False
+if MODEL_AUGMENTED: MODEL_BAYESIAN_CONFIG = config["model_config"]["bayesian"]
+
+FILENAME = sys.argv[3]
+
+if __name__ == '__main__':
+    oprl = valuerl.ValueRL(MODEL_NAME, ALGO, OBS_DIM, ACTION_DIM, HIDDEN_DIM, REWARD_SCALE, DISCOUNT, POLICY_BAYESIAN_CONFIG, AUX_CONFIG, DDPG_EXPLORE_CHANCE)
+
+    obs_loader = tf.placeholder(tf.float32, [1, OBS_DIM])
+    policy_actions, _ = oprl.build_evalution_graph(obs_loader, mode="exploit")
+
+    if MODEL_AUGMENTED:
+        next_obs_loader = tf.placeholder(tf.float32, [1, OBS_DIM])
+        reward_loader = tf.placeholder(tf.float32, [1])
+        done_loader = tf.placeholder(tf.float32, [1])
+        worldmodel = worldmodel.DeterministicWorldModel(MODEL_NAME, OBS_DIM, ACTION_DIM, HIDDEN_DIM, REWARD_SCALE, DISCOUNT, MODEL_BAYESIAN_CONFIG)
+        _, _, _, _, _, confidence, _ = oprl.build_Q_expansion_graph(next_obs_loader, reward_loader, done_loader, worldmodel, rollout_len=3, model_ensembling=True)
+
+    sess = tf.Session()
+    sess.run(tf.global_variables_initializer())
+
+    oprl.load(sess, FILENAME)
+    if MODEL_AUGMENTED: worldmodel.load(sess, FILENAME)
+
+    env = envwrap.get_env(config["env"])
+
+    hist = np.zeros([4, 10])
+    for _ in range(10):
+        ts = 0
+        rgb_frames = []
+        obs, reward, done, reset = env.reset(), 0, False, False
+        while not reset:
+            # env.internal_env.render()
+            # rgb_frames.append(env.internal_env.render(mode='rgb_array'))
+            # action = env.action_space.sample()
+            all_actions = sess.run(policy_actions, feed_dict={obs_loader: np.array([obs])})
+            all_actions = np.clip(all_actions, -1., 1.)
+            action = all_actions[0]
+            obs, _reward, done, reset = env.step(action)
+
+            if MODEL_AUGMENTED:
+                _confidences = sess.run(confidence, feed_dict={next_obs_loader: np.expand_dims(obs,0),
+                                                               reward_loader: np.expand_dims(_reward,0),
+                                                               done_loader: np.expand_dims(done,0)})
+                # print "%.02f %.02f %.02f %.02f" % tuple(_confidences[0,0])
+                for h in range(4):
+                    bucket = int((_confidences[0,0,h]-1e-5)*10)
+                    hist[h,bucket] += 1
+
+            reward += _reward
+            ts += 1
+            # print ts, _reward, reward
+        print(ts, reward)
+    hist /= np.sum(hist, axis=1, keepdims=True)
+    for row in reversed(hist.T): print(' '.join(["%.02f"] * 4) % tuple(row))
+
+    #clip = mpy.ImageSequenceClip(rgb_frames, fps=100)
+    #clip.write_videofile(FILENAME + "/movie.mp4")
+
+
diff --git a/research/steve/worldmodel.py b/research/steve/worldmodel.py
new file mode 100644
index 00000000..613bc6cb
--- /dev/null
+++ b/research/steve/worldmodel.py
@@ -0,0 +1,104 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+import numpy as np
+import nn
+
+from learner import CoreModel
+
+class DeterministicWorldModel(CoreModel):
+  """
+  A simple feed-forward neural network world model, with an option for an ensemble.
+  """
+
+  @property
+  def saveid(self):
+    return "worldmodel"
+
+  def create_params(self, env_config, learner_config):
+    self.obs_dim = np.prod(env_config["obs_dims"])
+    self.action_dim = env_config["action_dim"]
+    self.reward_scale = env_config["reward_scale"]
+    self.discount = env_config["discount"]
+
+    self.aux_hidden_dim = self.learner_config["aux_hidden_dim"]
+    self.transition_hidden_dim = self.learner_config["transition_hidden_dim"]
+    self.bayesian_config = self.learner_config["bayesian"]
+
+    with tf.variable_scope(self.name):
+      if self.bayesian_config:
+        self.transition_predictor = nn.EnsembleFeedForwardNet('transition_predictor', self.obs_dim + self.action_dim, [self.obs_dim], layers=8, hidden_dim=self.transition_hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config["transition"]["ensemble_size"], train_sample_count=self.bayesian_config["transition"]["train_sample_count"], eval_sample_count=self.bayesian_config["transition"]["eval_sample_count"])
+        self.done_predictor =       nn.EnsembleFeedForwardNet('done_predictor', self.obs_dim + self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.aux_hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config["transition"]["ensemble_size"], train_sample_count=self.bayesian_config["transition"]["train_sample_count"], eval_sample_count=self.bayesian_config["transition"]["eval_sample_count"])
+        self.reward_predictor =     nn.EnsembleFeedForwardNet('reward_predictor', self.obs_dim + self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.aux_hidden_dim, get_uncertainty=True, ensemble_size=self.bayesian_config["reward"]["ensemble_size"], train_sample_count=self.bayesian_config["reward"]["train_sample_count"], eval_sample_count=self.bayesian_config["reward"]["eval_sample_count"])
+      else:
+        self.transition_predictor = nn.FeedForwardNet('transition_predictor', self.obs_dim + self.action_dim, [self.obs_dim], layers=8, hidden_dim=self.transition_hidden_dim, get_uncertainty=True)
+        self.done_predictor =       nn.FeedForwardNet('done_predictor',   self.obs_dim + self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.aux_hidden_dim, get_uncertainty=True)
+        self.reward_predictor =     nn.FeedForwardNet('reward_predictor', self.obs_dim + self.obs_dim + self.action_dim, [], layers=4, hidden_dim=self.aux_hidden_dim, get_uncertainty=True)
+
+  def get_ensemble_idx_info(self):
+    if self.bayesian_config is not False:
+      ensemble_idxs = tf.random_shuffle(tf.range(self.transition_predictor.ensemble_size))
+      transition_ensemble_sample_n = self.transition_predictor.eval_sample_count
+      reward_ensemble_sample_n = self.reward_predictor.eval_sample_count
+      ensemble_idxs = ensemble_idxs[:transition_ensemble_sample_n]
+      return ensemble_idxs, transition_ensemble_sample_n, reward_ensemble_sample_n
+    else:
+      return None, 1, 1
+
+  def build_training_graph(self, obs, next_obs, actions, rewards, dones, data_size):
+    info = tf.concat([obs, actions], -1)
+    predicted_next_obs = self.transition_predictor(info, is_eval=False, reduce_mode="random") + obs
+    next_info = tf.concat([next_obs, info], -1)
+    predicted_dones = self.done_predictor(next_info, is_eval=False, reduce_mode="random")
+    predicted_rewards = self.reward_predictor(next_info, is_eval=False, reduce_mode="random")
+
+    done_losses = tf.nn.sigmoid_cross_entropy_with_logits(labels=dones, logits=predicted_dones)
+    reward_losses = .5 * tf.square(rewards - predicted_rewards)
+    next_obs_losses = .5 * tf.reduce_sum(tf.square(next_obs - predicted_next_obs), -1)
+
+    done_loss = tf.reduce_mean(done_losses)
+    reward_loss = tf.reduce_mean(reward_losses)
+    next_obs_loss = tf.reduce_mean(next_obs_losses)
+    reg_loss = .0001 * (self.done_predictor.l2_loss() +
+                        self.reward_predictor.l2_loss() +
+                        self.transition_predictor.l2_loss())
+
+    total_loss = done_loss + reward_loss + next_obs_loss + reg_loss
+
+    inspect = (total_loss, done_loss, reward_loss, next_obs_loss, reg_loss)
+
+    return total_loss, inspect
+
+  def init_extra_info(self, obs):
+    return tf.zeros_like(obs)
+
+  def transition(self, obs, action, extra_info, ensemble_idxs=None, pre_expanded=None):
+    info = tf.concat([obs, action], -1)
+    next_obs_delta = self.transition_predictor(info, reduce_mode="none", ensemble_idxs=ensemble_idxs, pre_expanded=pre_expanded)
+    if ensemble_idxs is None:
+      next_obs = tf.expand_dims(obs,-2) + next_obs_delta
+      next_info = tf.concat([next_obs, tf.expand_dims(info,-2)], -1)
+    else:
+      next_obs = obs + next_obs_delta
+      next_info = tf.concat([next_obs, info], -1)
+    done = tf.nn.sigmoid(self.done_predictor(next_info, reduce_mode="none", ensemble_idxs=ensemble_idxs, pre_expanded=True))
+    extra_info = tf.zeros_like(obs)
+    return next_obs, done, extra_info
+
+  def get_rewards(self, obs, action, next_obs):
+    next_info = tf.concat([next_obs, obs, action], -1)
+    reward = self.reward_predictor(next_info, reduce_mode="none")
+    return reward
\ No newline at end of file
diff --git a/research/steve/worldmodel_learner.py b/research/steve/worldmodel_learner.py
new file mode 100644
index 00000000..c36a50f6
--- /dev/null
+++ b/research/steve/worldmodel_learner.py
@@ -0,0 +1,55 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+import tensorflow as tf
+import numpy as np
+from learner import Learner
+from worldmodel import DeterministicWorldModel
+
+class WorldmodelLearner(Learner):
+    """
+    Worldmodel-specific training loop details.
+    """
+    def learner_name(self): return "worldmodel"
+
+    def make_loader_placeholders(self):
+        self.obs_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], np.prod(self.env_config["obs_dims"])])
+        self.next_obs_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], np.prod(self.env_config["obs_dims"])])
+        self.action_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"], self.env_config["action_dim"]])
+        self.reward_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"]])
+        self.done_loader = tf.placeholder(tf.float32, [self.learner_config["batch_size"]])
+        self.datasize_loader = tf.placeholder(tf.float64, [])
+        return [self.obs_loader, self.next_obs_loader, self.action_loader, self.reward_loader, self.done_loader, self.datasize_loader]
+
+    def make_core_model(self):
+        worldmodel = DeterministicWorldModel(self.config["name"], self.env_config, self.learner_config)
+        worldmodel_loss, inspect_losses = worldmodel.build_training_graph(*self.current_batch)
+
+        model_optimizer = tf.train.AdamOptimizer(3e-4)
+        model_gvs = model_optimizer.compute_gradients(worldmodel_loss, var_list=worldmodel.model_params)
+        capped_model_gvs = model_gvs
+        worldmodel_train_op = model_optimizer.apply_gradients(capped_model_gvs)
+
+        return worldmodel, (worldmodel_loss,), (worldmodel_train_op,), inspect_losses
+
+    ## Optional functions to override
+    def initialize(self): pass
+    def resume_from_checkpoint(self, epoch): pass
+    def checkpoint(self): pass
+    def backup(self): pass
+
+
+
+
