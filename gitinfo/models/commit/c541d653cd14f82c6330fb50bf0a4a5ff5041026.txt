commit c541d653cd14f82c6330fb50bf0a4a5ff5041026
Author: Alan Yee <alyee@ucsd.edu>
Date:   Fri Aug 18 14:37:58 2017 -0700

    Update AdditiveGaussianNoiseAutoencoderRunner.py
    
    -Fixed print styling
    -Fixed code according to PEP 8

diff --git a/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py b/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py
index e9117663..52a2a2d1 100644
--- a/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py
+++ b/autoencoder/AdditiveGaussianNoiseAutoencoderRunner.py
@@ -1,5 +1,8 @@
-import numpy as np
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
 
+import numpy as np
 import sklearn.preprocessing as prep
 import tensorflow as tf
 from tensorflow.examples.tutorials.mnist import input_data
@@ -8,16 +11,19 @@ from autoencoder_models.DenoisingAutoencoder import AdditiveGaussianNoiseAutoenc
 
 mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
 
+
 def standard_scale(X_train, X_test):
     preprocessor = prep.StandardScaler().fit(X_train)
     X_train = preprocessor.transform(X_train)
     X_test = preprocessor.transform(X_test)
     return X_train, X_test
 
+
 def get_random_block_from_data(data, batch_size):
     start_index = np.random.randint(0, len(data) - batch_size)
     return data[start_index:(start_index + batch_size)]
 
+
 X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
 
 n_samples = int(mnist.train.num_examples)
@@ -25,11 +31,12 @@ training_epochs = 20
 batch_size = 128
 display_step = 1
 
-autoencoder = AdditiveGaussianNoiseAutoencoder(n_input = 784,
-                                               n_hidden = 200,
-                                               transfer_function = tf.nn.softplus,
-                                               optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),
-                                               scale = 0.01)
+autoencoder = AdditiveGaussianNoiseAutoencoder(
+    n_input = 784,
+    n_hidden = 200,
+    transfer_function = tf.nn.softplus,
+    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),
+    scale = 0.01)
 
 for epoch in range(training_epochs):
     avg_cost = 0.
@@ -45,6 +52,7 @@ for epoch in range(training_epochs):
 
     # Display logs per epoch step
     if epoch % display_step == 0:
-        print("Epoch:", '%04d' % (epoch + 1), "cost=", "{:.9f}".format(avg_cost))
+        print("Epoch: ", '%d,' % (epoch + 1),
+              "Cost: ", "{:.9f}".format(avg_cost))
 
 print("Total cost: " + str(autoencoder.calc_total_cost(X_test)))
