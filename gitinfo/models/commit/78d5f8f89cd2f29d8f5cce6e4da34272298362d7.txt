commit 78d5f8f89cd2f29d8f5cce6e4da34272298362d7
Author: Zhichao Lu <lzc@google.com>
Date:   Tue Feb 27 09:38:32 2018 -0800

    Merged commit includes the following changes:
    187187978  by Zhichao Lu:
    
        Only updating hyperparameters if they have non-null values.
    
    --
    187097690  by Zhichao Lu:
    
        Rewrite some conditions a bit more clearly.
    
    --
    187085190  by Zhichao Lu:
    
        More informative error message.
    
    --
    186935376  by Zhichao Lu:
    
        Added option to evaluator.evaluate to use custom evaluator objects.
    
    --
    186808249  by Zhichao Lu:
    
        Fix documentation re: number of stages.
    
    --
    186775014  by Zhichao Lu:
    
        Change anchor generator interface to return a list of BoxLists containing anchors for different feature map layers.
    
    --
    186729028  by Zhichao Lu:
    
        Minor fixes to object detection.
    
    --
    186723716  by Zhichao Lu:
    
        Fix tf_example_decoder.py initailization issue.
    
    --
    186668505  by Zhichao Lu:
    
        Remove unused import.
    
    --
    186475361  by Zhichao Lu:
    
        Update the box predictor interface to return list of predictions - one from each feature map - instead of stacking them into one large tensor.
    
    --
    186410844  by Zhichao Lu:
    
        Fix PythonPath Dependencies.
    
    --
    186365384  by Zhichao Lu:
    
        Made some of the functions in exporter public so they can be reused.
    
    --
    186341438  by Zhichao Lu:
    
        Re-introducing check that label-map-path must be a valid (non-empty) string prior to overwriting pipeline config.
    
    --
    186036984  by Zhichao Lu:
    
        Adding default hyperparameters and allowing for overriding them via flags.
    
    --
    186026006  by Zhichao Lu:
    
        Strip `eval_` prefix from name argument give to TPUEstimator.evaluate since it adds the same prefix internally.
    
    --
    186016042  by Zhichao Lu:
    
        Add an option to evaluate models on training data.
    
    --
    185944986  by Zhichao Lu:
    
        let _update_label_map_path go through even if the path is empty
    
    --
    185860781  by Zhichao Lu:
    
        Add random normal initializer option to hyperparams builder.
    
        Scale the regression losses outside of the box encoder by adjusting huber loss delta and regression loss weight.
    
    --
    185846325  by Zhichao Lu:
    
        Add an option to normalize localization loss by the code size(number of box coordinates) in SSD Meta architecture.
    
    --
    185761217  by Zhichao Lu:
    
        Change multiscale_grid_anchor_generator to return anchors in normalized coordinates by default and add option to configure it.
    
        In SSD meta architecture, TargetAssigner operates in normalized coordinate space (i.e, groundtruth boxes are in normalized coordinates) hence we need the option to generate anchors in normalized coordinates.
    
    --
    185747733  by Zhichao Lu:
    
        Change the smooth L1 localization implementationt to use tf.losses.huber_loss and expose the delta parameter in the proto.
    
    --
    185715309  by Zhichao Lu:
    
        Obviates the need for prepadding on mobilenet v1 and v2 for fully convolutional models.
    
    --
    185685695  by Zhichao Lu:
    
        Fix manual stepping schedule to return first rate when there are no boundaries
    
    --
    185621650  by Zhichao Lu:
    
        Added target assigner proto for configuring negative class weights.
    
    --
    
    PiperOrigin-RevId: 187187978

diff --git a/research/object_detection/BUILD b/research/object_detection/BUILD
deleted file mode 100644
index e688edfe..00000000
--- a/research/object_detection/BUILD
+++ /dev/null
@@ -1,286 +0,0 @@
-# Tensorflow Object Detection API: main runnables.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-load("//learning/brain/contrib/learn/tpu:tpu.bzl", "cloud_tpu_py_binaries")
-
-licenses(["notice"])
-
-# Apache 2.0
-
-exports_files(["LICENSE"])
-
-py_library(
-    name = "inputs",
-    srcs = [
-        "inputs.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:dataset_builder",
-        "//tensorflow/models/research/object_detection/builders:image_resizer_builder",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/builders:preprocessor_builder",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_test(
-    name = "inputs_test",
-    srcs = [
-        "inputs_test.py",
-    ],
-    data = [
-        "//tensorflow/models/research/object_detection/data:pet_label_map.pbtxt",
-        "//tensorflow/models/research/object_detection/samples/configs:faster_rcnn_resnet50_pets.config",
-        "//tensorflow/models/research/object_detection/samples/configs:ssd_inception_v2_pets.config",
-        "//tensorflow/models/research/object_detection/test_data:pets_examples.record",
-    ],
-    deps = [
-        ":inputs",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-    ],
-)
-
-py_binary(
-    name = "model",
-    srcs = [
-        "model.py",
-    ],
-    deps = [
-        ":inputs",
-        ":model_hparams",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection:eval_util",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/builders:optimizer_builder",
-        "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//tensorflow/models/research/object_detection/utils:variables_helper",
-        "//tensorflow/models/research/object_detection/utils:visualization_utils",
-    ],
-)
-
-py_library(
-    name = "model_hparams",
-    srcs = [
-        "model_hparams.py",
-    ],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "model_test",
-    timeout = "long",
-    srcs = [
-        "model_test.py",
-    ],
-    data = [
-        "//tensorflow/models/research/object_detection/data:pet_label_map.pbtxt",
-        "//tensorflow/models/research/object_detection/samples/configs:faster_rcnn_resnet50_pets.config",
-        "//tensorflow/models/research/object_detection/samples/configs:ssd_inception_v2_pets.config",
-        "//tensorflow/models/research/object_detection/test_data:pets_examples.record",
-    ],
-    deps = [
-        ":inputs",
-        ":model",
-        ":model_hparams",
-        ":model_test_util",
-        "//mock",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-MODEL_TPU_DEPS = [
-    ":inputs",
-    ":model",
-    ":model_hparams",
-    "//tensorflow",
-    "//tensorflow/models/research/object_detection:eval_util",
-    "//tensorflow/models/research/object_detection/builders:model_builder",
-    "//tensorflow/models/research/object_detection/builders:optimizer_builder",
-    "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
-    "//tensorflow/models/research/object_detection/utils:config_util",
-    "//tensorflow/models/research/object_detection/utils:label_map_util",
-    "//tensorflow/models/research/object_detection/utils:ops",
-    "//tensorflow/models/research/object_detection/utils:variables_helper",
-    "//tensorflow/models/research/object_detection/utils:visualization_utils",
-]
-
-cloud_tpu_py_binaries(
-    name = "model_tpu",
-    srcs = [
-        "model_tpu.py",
-    ],
-    main = "model_tpu.py",
-    deps = MODEL_TPU_DEPS,
-)
-
-py_library(
-    name = "model_tpu_lib",
-    srcs = [
-        "model_tpu.py",
-    ],
-    deps = MODEL_TPU_DEPS,
-)
-
-py_library(
-    name = "model_test_util",
-    srcs = [
-        "model_test_util.py",
-    ],
-    deps = [
-        ":model",
-        ":model_hparams",
-        "//tensorflow",
-    ],
-)
-
-py_binary(
-    name = "train",
-    srcs = [
-        "train.py",
-    ],
-    deps = [
-        ":trainer",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:dataset_builder",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-    ],
-)
-
-py_library(
-    name = "trainer",
-    srcs = ["trainer.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:optimizer_builder",
-        "//tensorflow/models/research/object_detection/builders:preprocessor_builder",
-        "//tensorflow/models/research/object_detection/core:batcher",
-        "//tensorflow/models/research/object_detection/core:preprocessor",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:variables_helper",
-        "//third_party/tensorflow_models/slim:model_deploy",
-    ],
-)
-
-py_test(
-    name = "trainer_test",
-    srcs = ["trainer_test.py"],
-    deps = [
-        ":trainer",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/core:model",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
-    ],
-)
-
-py_library(
-    name = "eval_util",
-    srcs = [
-        "eval_util.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-        "//tensorflow/models/research/object_detection/core:keypoint_ops",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:visualization_utils",
-    ],
-)
-
-py_library(
-    name = "evaluator",
-    srcs = ["evaluator.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection:eval_util",
-        "//tensorflow/models/research/object_detection/core:prefetcher",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
-        "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:object_detection_evaluation",
-    ],
-)
-
-py_binary(
-    name = "eval",
-    srcs = [
-        "eval.py",
-    ],
-    deps = [
-        ":evaluator",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:dataset_builder",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_library(
-    name = "exporter",
-    srcs = [
-        "exporter.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/python/tools:freeze_graph_lib",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
-    ],
-)
-
-py_test(
-    name = "exporter_test",
-    srcs = [
-        "exporter_test.py",
-    ],
-    deps = [
-        ":exporter",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:model_builder",
-        "//tensorflow/models/research/object_detection/core:model",
-        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
-    ],
-)
-
-py_binary(
-    name = "export_inference_graph",
-    srcs = [
-        "export_inference_graph.py",
-    ],
-    deps = [
-        ":exporter",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
-    ],
-)
diff --git a/research/object_detection/anchor_generators/BUILD b/research/object_detection/anchor_generators/BUILD
deleted file mode 100644
index 63662fb6..00000000
--- a/research/object_detection/anchor_generators/BUILD
+++ /dev/null
@@ -1,83 +0,0 @@
-# Tensorflow Object Detection API: Anchor Generator implementations.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-py_library(
-    name = "grid_anchor_generator",
-    srcs = [
-        "grid_anchor_generator.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:anchor_generator",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_test(
-    name = "grid_anchor_generator_test",
-    srcs = [
-        "grid_anchor_generator_test.py",
-    ],
-    deps = [
-        ":grid_anchor_generator",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "multiple_grid_anchor_generator",
-    srcs = [
-        "multiple_grid_anchor_generator.py",
-    ],
-    deps = [
-        ":grid_anchor_generator",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:anchor_generator",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-    ],
-)
-
-py_test(
-    name = "multiple_grid_anchor_generator_test",
-    srcs = [
-        "multiple_grid_anchor_generator_test.py",
-    ],
-    deps = [
-        ":multiple_grid_anchor_generator",
-        "//numpy",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "multiscale_grid_anchor_generator",
-    srcs = [
-        "multiscale_grid_anchor_generator.py",
-    ],
-    deps = [
-        ":grid_anchor_generator",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:anchor_generator",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-    ],
-)
-
-py_test(
-    name = "multiscale_grid_anchor_generator_test",
-    srcs = [
-        "multiscale_grid_anchor_generator_test.py",
-    ],
-    deps = [
-        ":multiscale_grid_anchor_generator",
-        "//numpy",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index b3a51791..ba43f013 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -93,11 +93,9 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
         allowed.
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
-        this BoxList also holds a `feature_map_index` field which is set to 0
-        for each anchor; this field exists for interchangeability reasons with
-        the MultipleGridAnchorGenerator (see the docstring for the corresponding
-        `_generate` function in multiple_grid_anchor_generator.py)
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
+        the input feature map shapes.
+
     Raises:
       ValueError: if feature_map_shape_list, box_specs_list do not have the same
         length.
@@ -128,7 +126,7 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
       num_anchors = anchors.num_boxes()
     anchor_indices = tf.zeros([num_anchors])
     anchors.add_field('feature_map_index', anchor_indices)
-    return anchors
+    return [anchors]
 
 
 def tile_anchors(grid_height,
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator_test.py b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
index 6c53e89d..8de74aa7 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
@@ -31,8 +31,8 @@ class GridAnchorGeneratorTest(test_case.TestCase):
       anchor_offset = [7, -3]
       anchor_generator = grid_anchor_generator.GridAnchorGenerator(
           scales, aspect_ratios, anchor_offset=anchor_offset)
-      anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-      anchor_corners = anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
+      anchor_corners = anchors_list[0].get()
       return (anchor_corners,)
     exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],
                           [-505, -131, 519, 125], [-57, -67, 71, 61],
@@ -57,8 +57,8 @@ class GridAnchorGeneratorTest(test_case.TestCase):
           anchor_stride=anchor_stride,
           anchor_offset=anchor_offset)
 
-      anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-      anchor_corners = anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
+      anchor_corners = anchors_list[0].get()
       return (anchor_corners,)
     exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
                           [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],
@@ -83,9 +83,9 @@ class GridAnchorGeneratorTest(test_case.TestCase):
           anchor_stride=anchor_stride,
           anchor_offset=anchor_offset)
 
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
           feature_map_shape_list=[(feature_map_height, feature_map_width)])
-      anchor_corners = anchors.get()
+      anchor_corners = anchors_list[0].get()
       return (anchor_corners,)
 
     exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index ead0ad64..bd785c17 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -165,10 +165,9 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         grid.
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
-        this BoxList also holds a `feature_map_index` field which, for each
-        anchor, stores the index of the corresponding feature map which was used
-        to generate it.
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
+        the input feature map shapes.
+
     Raises:
       ValueError: if feature_map_shape_list, box_specs_list do not have the same
         length.
@@ -211,7 +210,6 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         raise ValueError('%s must be a list of pairs.' % arg_name)
 
     anchor_grid_list = []
-    anchor_indices_list = []
     min_im_shape = tf.minimum(im_height, im_width)
     scale_height = min_im_shape / im_height
     scale_width = min_im_shape / im_width
@@ -219,10 +217,11 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         scale_height * self._base_anchor_size[0],
         scale_width * self._base_anchor_size[1]
     ]
-    for feature_map_index, (
-        grid_size, scales, aspect_ratios, stride, offset) in enumerate(
-            zip(feature_map_shape_list, self._scales, self._aspect_ratios,
-                anchor_strides, anchor_offsets)):
+    for feature_map_index, (grid_size, scales, aspect_ratios, stride,
+                            offset) in enumerate(
+                                zip(feature_map_shape_list, self._scales,
+                                    self._aspect_ratios, anchor_strides,
+                                    anchor_offsets)):
       tiled_anchors = grid_anchor_generator.tile_anchors(
           grid_height=grid_size[0],
           grid_width=grid_size[1],
@@ -231,30 +230,17 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
           base_anchor_size=base_anchor_size,
           anchor_stride=stride,
           anchor_offset=offset)
-      anchor_grid_list.append(tiled_anchors)
+      if self._clip_window is not None:
+        tiled_anchors = box_list_ops.clip_to_window(
+            tiled_anchors, self._clip_window, filter_nonoverlapping=False)
       num_anchors_in_layer = tiled_anchors.num_boxes_static()
       if num_anchors_in_layer is None:
         num_anchors_in_layer = tiled_anchors.num_boxes()
-      anchor_indices_list.append(
-          feature_map_index * tf.ones([num_anchors_in_layer]))
-    concatenated_anchors = box_list_ops.concatenate(anchor_grid_list)
-    anchor_indices = tf.concat(anchor_indices_list, 0)
-    num_anchors = concatenated_anchors.num_boxes_static()
-    if num_anchors is None:
-      num_anchors = concatenated_anchors.num_boxes()
-    if self._clip_window is not None:
-      concatenated_anchors = box_list_ops.clip_to_window(
-          concatenated_anchors, self._clip_window, filter_nonoverlapping=False)
-      # TODO: make reshape an option for the clip_to_window op
-      concatenated_anchors.set(
-          tf.reshape(concatenated_anchors.get(), [num_anchors, 4]))
-
-    stddevs_tensor = 0.01 * tf.ones(
-        [num_anchors, 4], dtype=tf.float32, name='stddevs')
-    concatenated_anchors.add_field('stddev', stddevs_tensor)
-    concatenated_anchors.add_field('feature_map_index', anchor_indices)
-
-    return concatenated_anchors
+      anchor_indices = feature_map_index * tf.ones([num_anchors_in_layer])
+      tiled_anchors.add_field('feature_map_index', anchor_indices)
+      anchor_grid_list.append(tiled_anchors)
+
+    return anchor_grid_list
 
 
 def create_ssd_anchors(num_layers=6,
@@ -285,7 +271,7 @@ def create_ssd_anchors(num_layers=6,
       grid sizes passed in at generation time)
     min_scale: scale of anchors corresponding to finest resolution (float)
     max_scale: scale of anchors corresponding to coarsest resolution (float)
-    scales: As list of anchor scales to use. When not None and not emtpy,
+    scales: As list of anchor scales to use. When not None and not empty,
       min_scale and max_scale are not used.
     aspect_ratios: list or tuple of (float) aspect ratios to place on each
       grid point.
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
index 2afbf433..070d81d3 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
@@ -37,8 +37,8 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
           base_anchor_size=tf.constant([256, 256], dtype=tf.float32),
           anchor_strides=[(16, 16)],
           anchor_offsets=[(7, -3)])
-      anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
+      return anchors_list[0].get()
     exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],
                           [-505, -131, 519, 125], [-57, -67, 71, 61],
                           [-121, -131, 135, 125], [-249, -259, 263, 253],
@@ -57,8 +57,8 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
           base_anchor_size=tf.constant([10, 10], dtype=tf.float32),
           anchor_strides=[(19, 19)],
           anchor_offsets=[(0, 0)])
-      anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
+      return anchors_list[0].get()
     exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
                           [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],
                           [-5., 14., 5, 24], [-10., 9., 10, 29],
@@ -76,9 +76,9 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
       anchor_generator = ag.MultipleGridAnchorGenerator(
           box_specs_list, base_anchor_size=tf.constant([1, 1],
                                                        dtype=tf.float32))
-      anchors = anchor_generator.generate(feature_map_shape_list=[(tf.constant(
-          1, dtype=tf.int32), tf.constant(2, dtype=tf.int32))])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(
+          tf.constant(1, dtype=tf.int32), tf.constant(2, dtype=tf.int32))])
+      return anchors_list[0].get()
 
     exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]
     anchor_corners_out = self.execute(graph_fn, [])
@@ -91,9 +91,9 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
       anchor_generator = ag.MultipleGridAnchorGenerator(
           box_specs_list, base_anchor_size=tf.constant([1, 1],
                                                        dtype=tf.float32))
-      anchors = anchor_generator.generate(feature_map_shape_list=[(height,
-                                                                   width)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(height,
+                                                                        width)])
+      return anchors_list[0].get()
 
     exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]
 
@@ -109,12 +109,12 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
       anchor_generator = ag.MultipleGridAnchorGenerator(
           box_specs_list, base_anchor_size=tf.constant([1, 1],
                                                        dtype=tf.float32))
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
           feature_map_shape_list=[(tf.constant(1, dtype=tf.int32), tf.constant(
               2, dtype=tf.int32))],
           im_height=320,
           im_width=640)
-      return anchors.get()
+      return anchors_list[0].get()
 
     exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
     anchor_corners_out = self.execute(graph_fn, [])
@@ -131,9 +131,9 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
           base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
           anchor_strides=[(.25, .25), (.5, .5)],
           anchor_offsets=[(.125, .125), (.25, .25)])
-      anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4),
-                                                                  (2, 2)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (
+          2, 2)])
+      return [anchors.get() for anchors in anchors_list]
     # height and width of box with .5 aspect ratio
     h = np.sqrt(2)
     w = 1.0/np.sqrt(2)
@@ -150,7 +150,7 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
                             [.125-1.0, .125-1.0, .125+1.0, .125+1.0],
                             [.125-.5*h, .125-.5*w, .125+.5*h, .125+.5*w],]
 
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
     self.assertEquals(anchor_corners_out.shape, (56, 4))
     big_grid_corners = anchor_corners_out[0:3, :]
     small_grid_corners = anchor_corners_out[48:, :]
@@ -168,9 +168,9 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
           box_specs_list,
           base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
           clip_window=clip_window)
-      anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4),
-                                                                  (2, 2)])
-      return anchors.get()
+      anchors_list = anchor_generator.generate(feature_map_shape_list=[(4, 4), (
+          2, 2)])
+      return [anchors.get() for anchors in anchors_list]
     # height and width of box with .5 aspect ratio
     h = np.sqrt(2)
     w = 1.0/np.sqrt(2)
@@ -183,7 +183,7 @@ class MultipleGridAnchorGeneratorTest(test_case.TestCase):
                               [.25, .25, 1, 1],
                               [.75-.5*h, .75-.5*w, 1, 1]]
 
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
     small_grid_corners = anchor_corners_out[48:, :]
     self.assertAllClose(small_grid_corners, exp_small_grid_corners)
 
@@ -264,10 +264,10 @@ class CreateSSDAnchorsTest(test_case.TestCase):
 
       feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
                                 (5, 5), (3, 3), (1, 1)]
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
           feature_map_shape_list=feature_map_shape_list)
-      return anchors.get()
-    anchor_corners_out = self.execute(graph_fn1, [])
+      return [anchors.get() for anchors in anchors_list]
+    anchor_corners_out = np.concatenate(self.execute(graph_fn1, []), axis=0)
     self.assertEquals(anchor_corners_out.shape, (7308, 4))
 
     def graph_fn2():
@@ -278,10 +278,10 @@ class CreateSSDAnchorsTest(test_case.TestCase):
 
       feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
                                 (5, 5), (3, 3), (1, 1)]
-      anchors = anchor_generator.generate(
+      anchors_list = anchor_generator.generate(
           feature_map_shape_list=feature_map_shape_list)
-      return anchors.get()
-    anchor_corners_out = self.execute(graph_fn2, [])
+      return [anchors.get() for anchors in anchors_list]
+    anchor_corners_out = np.concatenate(self.execute(graph_fn2, []), axis=0)
     self.assertEquals(anchor_corners_out.shape, (11640, 4))
 
 
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
index ea8c266c..a8d227c7 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -21,14 +21,15 @@ T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
 """
 
 from object_detection.anchor_generators import grid_anchor_generator
+from object_detection.core import anchor_generator
 from object_detection.core import box_list_ops
 
 
-class MultiscaleGridAnchorGenerator(object):
+class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
   """Generate a grid of anchors for multiple CNN layers of different scale."""
 
   def __init__(self, min_level, max_level, anchor_scale, aspect_ratios,
-               scales_per_octave):
+               scales_per_octave, normalize_coordinates=True):
     """Constructs a MultiscaleGridAnchorGenerator.
 
     To construct anchors, at multiple scale resolutions, one must provide a
@@ -48,10 +49,13 @@ class MultiscaleGridAnchorGenerator(object):
       aspect_ratios: list or tuple of (float) aspect ratios to place on each
         grid point.
       scales_per_octave: integer number of intermediate scales per scale octave.
+      normalize_coordinates: whether to produce anchors in normalized
+        coordinates. (defaults to True).
     """
     self._anchor_grid_info = []
     self._aspect_ratios = aspect_ratios
     self._scales_per_octave = scales_per_octave
+    self._normalize_coordinates = normalize_coordinates
 
     for level in range(min_level, max_level + 1):
       anchor_stride = [2**level, 2**level]
@@ -80,7 +84,7 @@ class MultiscaleGridAnchorGenerator(object):
     return len(self._anchor_grid_info) * [
         len(self._aspect_ratios) * self._scales_per_octave]
 
-  def generate(self, feature_map_shape_list, im_height, im_width):
+  def _generate(self, feature_map_shape_list, im_height, im_width):
     """Generates a collection of bounding boxes to be used as anchors.
 
     Currently we require the input image shape to be statically defined.  That
@@ -95,7 +99,8 @@ class MultiscaleGridAnchorGenerator(object):
       im_width: the width of the image to generate the grid for.
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
+        the input feature map shapes.
     Raises:
       ValueError: if im_height and im_width are not integers.
     """
@@ -105,7 +110,7 @@ class MultiscaleGridAnchorGenerator(object):
     anchor_grid_list = []
     for feat_shape, grid_info in zip(feature_map_shape_list,
                                      self._anchor_grid_info):
-      # TODO check the feature_map_shape_list is consistent with
+      # TODO(rathodv) check the feature_map_shape_list is consistent with
       # self._anchor_grid_info
       level = grid_info['level']
       stride = 2**level
@@ -123,9 +128,11 @@ class MultiscaleGridAnchorGenerator(object):
           base_anchor_size=base_anchor_size,
           anchor_stride=anchor_stride,
           anchor_offset=anchor_offset)
-      anchor_grid_list.append(
-          ag.generate(feature_map_shape_list=[(feat_h, feat_w)]))
+      (anchor_grid,) = ag.generate(feature_map_shape_list=[(feat_h, feat_w)])
 
-    concatenated_anchors = box_list_ops.concatenate(anchor_grid_list)
+      if self._normalize_coordinates:
+        anchor_grid = box_list_ops.to_normalized_coordinates(
+            anchor_grid, im_height, im_width, check_range=False)
+      anchor_grid_list.append(anchor_grid)
 
-    return concatenated_anchors
+    return anchor_grid_list
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
index dd9b8970..c96bdae7 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -37,10 +37,35 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
                           [-16, -48, 112, 80],
                           [-16, -16, 112, 112]]
     anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-    anchors = anchor_generator.generate(feature_map_shape_list,
-                                        im_height, im_width)
-    anchor_corners = anchors.get()
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+        normalize_coordinates=False)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_in_normalized_coordinates(self):
+    min_level = 5
+    max_level = 5
+    anchor_scale = 4.0
+    aspect_ratios = [1.0]
+    scales_per_octave = 1
+    im_height = 64
+    im_width = 128
+    feature_map_shape_list = [(2, 2)]
+    exp_anchor_corners = [[-48./64, -48./128, 80./64, 80./128],
+                          [-48./64, -16./128, 80./64, 112./128],
+                          [-16./64, -48./128, 112./64, 80./128],
+                          [-16./64, -16./128, 112./64, 112./128]]
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+        normalize_coordinates=True)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
 
     with self.test_session():
       anchor_corners_out = anchor_corners.eval()
@@ -53,7 +78,8 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
     aspect_ratios = [1.0, 2.0]
     scales_per_octave = 3
     anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+        normalize_coordinates=False)
     self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])
 
   def test_construct_single_anchor_fails_with_tensor_image_size(self):
@@ -66,9 +92,11 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
     im_width = tf.constant(64)
     feature_map_shape_list = [(2, 2)]
     anchor_generator = mg.MultiscaleGridAnchorGenerator(
-        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+        normalize_coordinates=False)
     with self.assertRaises(ValueError):
-      anchor_generator.generate(feature_map_shape_list, im_height, im_width)
+      anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
 
   def test_construct_single_anchor_with_odd_input_dimension(self):
 
@@ -82,10 +110,11 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       im_width = 65
       feature_map_shape_list = [(3, 3)]
       anchor_generator = mg.MultiscaleGridAnchorGenerator(
-          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
-                                          im_width)
-      anchor_corners = anchors.get()
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(
+          feature_map_shape_list, im_height=im_height, im_width=im_width)
+      anchor_corners = anchors_list[0].get()
       return (anchor_corners,)
     anchor_corners_out = self.execute(graph_fn, [])
     exp_anchor_corners = [[-64, -64, 64, 64],
@@ -111,13 +140,15 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       im_width = 64
       feature_map_shape_list = [(2, 2), (1, 1)]
       anchor_generator = mg.MultiscaleGridAnchorGenerator(
-          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
-                                          im_width)
-      anchor_corners = anchors.get()
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
 
-    anchor_corners_out = self.execute(graph_fn, [])
+    anchor_corners_out = np.concatenate(self.execute(graph_fn, []), axis=0)
     exp_anchor_corners = [[-48, -48, 80, 80],
                           [-48, -16, 80, 112],
                           [-16, -48, 112, 80],
@@ -135,19 +166,22 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       scales_per_octave = 2
       im_height = 64
       im_width = 64
-      feature_map_shape_list = [(1, 1), (1, 1)]
+      feature_map_shape_list = [(1, 1)]
 
       anchor_generator = mg.MultiscaleGridAnchorGenerator(
-          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
-                                          im_width)
-      anchor_corners = anchors.get()
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
     # There are 4 set of anchors in this configuration. The order is:
     # [[2**0.0 intermediate scale + 1.0 aspect],
     #  [2**0.5 intermediate scale + 1.0 aspect]]
     exp_anchor_corners = [[-96., -96., 160., 160.],
                           [-149.0193, -149.0193, 213.0193, 213.0193]]
+
     anchor_corners_out = self.execute(graph_fn, [])
     self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
@@ -160,18 +194,21 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       scales_per_octave = 2
       im_height = 64
       im_width = 64
-      feature_map_shape_list = [(1, 1), (1, 1), (1, 1), (1, 1)]
+      feature_map_shape_list = [(1, 1)]
       anchor_generator = mg.MultiscaleGridAnchorGenerator(
-          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
-                                          im_width)
-      anchor_corners = anchors.get()
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
       return anchor_corners
     # There are 4 set of anchors in this configuration. The order is:
     # [[2**0.0 intermediate scale + 1.0 aspect],
     #  [2**0.5 intermediate scale + 1.0 aspect],
     #  [2**0.0 intermediate scale + 2.0 aspect],
     #  [2**0.5 intermediate scale + 2.0 aspect]]
+
     exp_anchor_corners = [[-96., -96., 160., 160.],
                           [-149.0193, -149.0193, 213.0193, 213.0193],
                           [-58.50967, -149.0193, 122.50967, 213.0193],
@@ -193,18 +230,22 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       feature_map_shape_list = [(feature_map1_height, feature_map1_width),
                                 (feature_map2_height, feature_map2_width)]
       anchor_generator = mg.MultiscaleGridAnchorGenerator(
-          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
-      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
-                                          im_width)
-      anchor_corners = anchors.get()
-      return (anchor_corners,)
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+          normalize_coordinates=False)
+      anchors_list = anchor_generator.generate(feature_map_shape_list,
+                                               im_height=im_height,
+                                               im_width=im_width)
+      anchor_corners = [anchors.get() for anchors in anchors_list]
+      return anchor_corners
 
-    anchor_corners_out = self.execute_cpu(graph_fn, [
-        np.array(2, dtype=np.int32),
-        np.array(2, dtype=np.int32),
-        np.array(1, dtype=np.int32),
-        np.array(1, dtype=np.int32)
-    ])
+    anchor_corners_out = np.concatenate(
+        self.execute_cpu(graph_fn, [
+            np.array(2, dtype=np.int32),
+            np.array(2, dtype=np.int32),
+            np.array(1, dtype=np.int32),
+            np.array(1, dtype=np.int32)
+        ]),
+        axis=0)
     exp_anchor_corners = [[-48, -48, 80, 80],
                           [-48, -16, 80, 112],
                           [-16, -48, 112, 80],
diff --git a/research/object_detection/box_coders/BUILD b/research/object_detection/box_coders/BUILD
deleted file mode 100644
index f74484b7..00000000
--- a/research/object_detection/box_coders/BUILD
+++ /dev/null
@@ -1,102 +0,0 @@
-# Tensorflow Object Detection API: Box Coder implementations.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-py_library(
-    name = "faster_rcnn_box_coder",
-    srcs = [
-        "faster_rcnn_box_coder.py",
-    ],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_box_coder_test",
-    srcs = [
-        "faster_rcnn_box_coder_test.py",
-    ],
-    deps = [
-        ":faster_rcnn_box_coder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_library(
-    name = "keypoint_box_coder",
-    srcs = [
-        "keypoint_box_coder.py",
-    ],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "keypoint_box_coder_test",
-    srcs = [
-        "keypoint_box_coder_test.py",
-    ],
-    deps = [
-        ":keypoint_box_coder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_library(
-    name = "mean_stddev_box_coder",
-    srcs = [
-        "mean_stddev_box_coder.py",
-    ],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_test(
-    name = "mean_stddev_box_coder_test",
-    srcs = [
-        "mean_stddev_box_coder_test.py",
-    ],
-    deps = [
-        ":mean_stddev_box_coder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_library(
-    name = "square_box_coder",
-    srcs = [
-        "square_box_coder.py",
-    ],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_test(
-    name = "square_box_coder_test",
-    srcs = [
-        "square_box_coder_test.py",
-    ],
-    deps = [
-        ":square_box_coder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
diff --git a/research/object_detection/builders/BUILD b/research/object_detection/builders/BUILD
deleted file mode 100644
index 0bb3458e..00000000
--- a/research/object_detection/builders/BUILD
+++ /dev/null
@@ -1,337 +0,0 @@
-# Tensorflow Object Detection API: component builders.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-py_library(
-    name = "model_builder",
-    srcs = ["model_builder.py"],
-    deps = [
-        ":anchor_generator_builder",
-        ":box_coder_builder",
-        ":box_predictor_builder",
-        ":hyperparams_builder",
-        ":image_resizer_builder",
-        ":losses_builder",
-        ":matcher_builder",
-        ":post_processing_builder",
-        ":region_similarity_calculator_builder",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow/models/research/object_detection/meta_architectures:rfcn_meta_arch",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/models:embedded_ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_nas_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_resnet_v1_fpn_feature_extractor",
-        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
-    ],
-)
-
-py_test(
-    name = "model_builder_test",
-    srcs = ["model_builder_test.py"],
-    deps = [
-        ":model_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/models:embedded_ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_nas_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow/models/research/object_detection/models:ssd_resnet_v1_fpn_feature_extractor",
-        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
-    ],
-)
-
-py_library(
-    name = "matcher_builder",
-    srcs = ["matcher_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
-        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
-        "//tensorflow/models/research/object_detection/protos:matcher_py_pb2",
-    ],
-)
-
-py_test(
-    name = "matcher_builder_test",
-    srcs = ["matcher_builder_test.py"],
-    deps = [
-        ":matcher_builder",
-        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
-        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
-        "//tensorflow/models/research/object_detection/protos:matcher_py_pb2",
-    ],
-)
-
-py_library(
-    name = "box_coder_builder",
-    srcs = ["box_coder_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:square_box_coder",
-        "//tensorflow/models/research/object_detection/protos:box_coder_py_pb2",
-    ],
-)
-
-py_test(
-    name = "box_coder_builder_test",
-    srcs = ["box_coder_builder_test.py"],
-    deps = [
-        ":box_coder_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:square_box_coder",
-        "//tensorflow/models/research/object_detection/protos:box_coder_py_pb2",
-    ],
-)
-
-py_library(
-    name = "anchor_generator_builder",
-    srcs = ["anchor_generator_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/anchor_generators:multiple_grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/anchor_generators:multiscale_grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/protos:anchor_generator_py_pb2",
-    ],
-)
-
-py_test(
-    name = "anchor_generator_builder_test",
-    srcs = ["anchor_generator_builder_test.py"],
-    deps = [
-        ":anchor_generator_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/anchor_generators:multiple_grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/anchor_generators:multiscale_grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/protos:anchor_generator_py_pb2",
-    ],
-)
-
-py_library(
-    name = "dataset_builder",
-    srcs = ["dataset_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-    ],
-)
-
-py_test(
-    name = "dataset_builder_test",
-    srcs = [
-        "dataset_builder_test.py",
-    ],
-    deps = [
-        ":dataset_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-    ],
-)
-
-py_library(
-    name = "input_reader_builder",
-    srcs = ["input_reader_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-    ],
-)
-
-py_test(
-    name = "input_reader_builder_test",
-    srcs = [
-        "input_reader_builder_test.py",
-    ],
-    deps = [
-        ":input_reader_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-    ],
-)
-
-py_library(
-    name = "losses_builder",
-    srcs = ["losses_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/protos:losses_py_pb2",
-    ],
-)
-
-py_test(
-    name = "losses_builder_test",
-    srcs = ["losses_builder_test.py"],
-    deps = [
-        ":losses_builder",
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/protos:losses_py_pb2",
-    ],
-)
-
-py_library(
-    name = "optimizer_builder",
-    srcs = ["optimizer_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:learning_schedules",
-    ],
-)
-
-py_test(
-    name = "optimizer_builder_test",
-    srcs = ["optimizer_builder_test.py"],
-    deps = [
-        ":optimizer_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:optimizer_py_pb2",
-    ],
-)
-
-py_library(
-    name = "post_processing_builder",
-    srcs = ["post_processing_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:post_processing",
-        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
-    ],
-)
-
-py_test(
-    name = "post_processing_builder_test",
-    srcs = ["post_processing_builder_test.py"],
-    deps = [
-        ":post_processing_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
-    ],
-)
-
-py_library(
-    name = "hyperparams_builder",
-    srcs = ["hyperparams_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
-    ],
-)
-
-py_test(
-    name = "hyperparams_builder_test",
-    srcs = ["hyperparams_builder_test.py"],
-    deps = [
-        ":hyperparams_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
-    ],
-)
-
-py_library(
-    name = "box_predictor_builder",
-    srcs = ["box_predictor_builder.py"],
-    deps = [
-        ":hyperparams_builder",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
-    ],
-)
-
-py_test(
-    name = "box_predictor_builder_test",
-    srcs = ["box_predictor_builder_test.py"],
-    deps = [
-        ":box_predictor_builder",
-        ":hyperparams_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
-    ],
-)
-
-py_library(
-    name = "region_similarity_calculator_builder",
-    srcs = ["region_similarity_calculator_builder.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/core:region_similarity_calculator",
-        "//tensorflow/models/research/object_detection/protos:region_similarity_calculator_py_pb2",
-    ],
-)
-
-py_test(
-    name = "region_similarity_calculator_builder_test",
-    srcs = ["region_similarity_calculator_builder_test.py"],
-    deps = [
-        ":region_similarity_calculator_builder",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "preprocessor_builder",
-    srcs = ["preprocessor_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:preprocessor",
-        "//tensorflow/models/research/object_detection/protos:preprocessor_py_pb2",
-    ],
-)
-
-py_test(
-    name = "preprocessor_builder_test",
-    srcs = [
-        "preprocessor_builder_test.py",
-    ],
-    deps = [
-        ":preprocessor_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:preprocessor",
-        "//tensorflow/models/research/object_detection/protos:preprocessor_py_pb2",
-    ],
-)
-
-py_library(
-    name = "image_resizer_builder",
-    srcs = ["image_resizer_builder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:preprocessor",
-        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
-    ],
-)
-
-py_test(
-    name = "image_resizer_builder_test",
-    srcs = ["image_resizer_builder_test.py"],
-    deps = [
-        ":image_resizer_builder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
-    ],
-)
diff --git a/research/object_detection/builders/anchor_generator_builder.py b/research/object_detection/builders/anchor_generator_builder.py
index 5bd3c84f..54cec3a1 100644
--- a/research/object_detection/builders/anchor_generator_builder.py
+++ b/research/object_detection/builders/anchor_generator_builder.py
@@ -87,7 +87,8 @@ def build(anchor_generator_config):
         cfg.max_level,
         cfg.anchor_scale,
         [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],
-        cfg.scales_per_octave
+        cfg.scales_per_octave,
+        cfg.normalize_coordinates
     )
   else:
     raise ValueError('Empty anchor generator.')
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index 90ed0a57..2a23c2d9 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -276,6 +276,24 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
       self.assertAllClose(anchor_grid_info['info'][2],
                           [4.0 * 2**level, 4.0 * 2**level])
       self.assertAllClose(anchor_grid_info['info'][3], [2**level, 2**level])
+      self.assertTrue(anchor_generator_object._normalize_coordinates)
+
+  def test_build_multiscale_anchor_generator_with_anchors_in_pixel_coordinates(
+      self):
+    anchor_generator_text_proto = """
+      multiscale_anchor_generator {
+        aspect_ratios: [1.0]
+        normalize_coordinates: false
+      }
+    """
+    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()
+    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
+    anchor_generator_object = anchor_generator_builder.build(
+        anchor_generator_proto)
+    self.assertTrue(isinstance(anchor_generator_object,
+                               multiscale_grid_anchor_generator.
+                               MultiscaleGridAnchorGenerator))
+    self.assertFalse(anchor_generator_object._normalize_coordinates)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/hyperparams_builder.py b/research/object_detection/builders/hyperparams_builder.py
index 094ff023..2f2516c7 100644
--- a/research/object_detection/builders/hyperparams_builder.py
+++ b/research/object_detection/builders/hyperparams_builder.py
@@ -134,6 +134,10 @@ def _build_initializer(initializer):
     return tf.truncated_normal_initializer(
         mean=initializer.truncated_normal_initializer.mean,
         stddev=initializer.truncated_normal_initializer.stddev)
+  if initializer_oneof == 'random_normal_initializer':
+    return tf.random_normal_initializer(
+        mean=initializer.random_normal_initializer.mean,
+        stddev=initializer.random_normal_initializer.stddev)
   if initializer_oneof == 'variance_scaling_initializer':
     enum_descriptor = (hyperparams_pb2.VarianceScalingInitializer.
                        DESCRIPTOR.enum_types_by_name['Mode'])
diff --git a/research/object_detection/builders/hyperparams_builder_test.py b/research/object_detection/builders/hyperparams_builder_test.py
index a9808076..7dc15a80 100644
--- a/research/object_detection/builders/hyperparams_builder_test.py
+++ b/research/object_detection/builders/hyperparams_builder_test.py
@@ -28,7 +28,7 @@ slim = tf.contrib.slim
 
 class HyperparamsBuilderTest(tf.test.TestCase):
 
-  # TODO: Make this a public api in slim arg_scope.py.
+  # TODO(rathodv): Make this a public api in slim arg_scope.py.
   def _get_scope_key(self, op):
     return getattr(op, '_key_op', str(op))
 
@@ -444,6 +444,27 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=0.49, tol=1e-1)
 
+  def test_variance_in_range_with_random_normal_initializer(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        random_normal_initializer {
+          mean: 0.0
+          stddev: 0.8
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    scope = hyperparams_builder.build(conv_hyperparams_proto, is_training=True)
+    conv_scope_arguments = scope.values()[0]
+    initializer = conv_scope_arguments['weights_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=0.64, tol=1e-1)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index 2dbceaec..c8cf63a4 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -72,8 +72,8 @@ def build(image_resizer_config):
     raise ValueError('image_resizer_config not of type '
                      'image_resizer_pb2.ImageResizer.')
 
-  if image_resizer_config.WhichOneof(
-      'image_resizer_oneof') == 'keep_aspect_ratio_resizer':
+  image_resizer_oneof = image_resizer_config.WhichOneof('image_resizer_oneof')
+  if image_resizer_oneof == 'keep_aspect_ratio_resizer':
     keep_aspect_ratio_config = image_resizer_config.keep_aspect_ratio_resizer
     if not (keep_aspect_ratio_config.min_dimension <=
             keep_aspect_ratio_config.max_dimension):
@@ -87,8 +87,7 @@ def build(image_resizer_config):
         pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension)
     if not keep_aspect_ratio_config.convert_to_grayscale:
       return image_resizer_fn
-  elif image_resizer_config.WhichOneof(
-      'image_resizer_oneof') == 'fixed_shape_resizer':
+  elif image_resizer_oneof == 'fixed_shape_resizer':
     fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer
     method = _tf_resize_method(fixed_shape_resizer_config.resize_method)
     image_resizer_fn = functools.partial(
@@ -99,7 +98,8 @@ def build(image_resizer_config):
     if not fixed_shape_resizer_config.convert_to_grayscale:
       return image_resizer_fn
   else:
-    raise ValueError('Invalid image resizer option.')
+    raise ValueError(
+        'Invalid image resizer option: \'%s\'.' % image_resizer_oneof)
 
   def grayscale_image_resizer(image):
     [resized_image, resized_image_shape] = image_resizer_fn(image)
diff --git a/research/object_detection/builders/losses_builder.py b/research/object_detection/builders/losses_builder.py
index 6f27da3d..7d8eb8cf 100644
--- a/research/object_detection/builders/losses_builder.py
+++ b/research/object_detection/builders/losses_builder.py
@@ -150,7 +150,8 @@ def _build_localization_loss(loss_config):
     return losses.WeightedL2LocalizationLoss()
 
   if loss_type == 'weighted_smooth_l1':
-    return losses.WeightedSmoothL1LocalizationLoss()
+    return losses.WeightedSmoothL1LocalizationLoss(
+        loss_config.weighted_smooth_l1.delta)
 
   if loss_type == 'weighted_iou':
     return losses.WeightedIOULocalizationLoss()
diff --git a/research/object_detection/builders/losses_builder_test.py b/research/object_detection/builders/losses_builder_test.py
index 5bef7192..bacd4b36 100644
--- a/research/object_detection/builders/losses_builder_test.py
+++ b/research/object_detection/builders/losses_builder_test.py
@@ -42,7 +42,7 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     self.assertTrue(isinstance(localization_loss,
                                losses.WeightedL2LocalizationLoss))
 
-  def test_build_weighted_smooth_l1_localization_loss(self):
+  def test_build_weighted_smooth_l1_localization_loss_default_delta(self):
     losses_text_proto = """
       localization_loss {
         weighted_smooth_l1 {
@@ -58,6 +58,26 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     _, localization_loss, _, _, _ = losses_builder.build(losses_proto)
     self.assertTrue(isinstance(localization_loss,
                                losses.WeightedSmoothL1LocalizationLoss))
+    self.assertAlmostEqual(localization_loss._delta, 1.0)
+
+  def test_build_weighted_smooth_l1_localization_loss_non_default_delta(self):
+    losses_text_proto = """
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 0.1
+        }
+      }
+      classification_loss {
+        weighted_softmax {
+        }
+      }
+    """
+    losses_proto = losses_pb2.Loss()
+    text_format.Merge(losses_text_proto, losses_proto)
+    _, localization_loss, _, _, _ = losses_builder.build(losses_proto)
+    self.assertTrue(isinstance(localization_loss,
+                               losses.WeightedSmoothL1LocalizationLoss))
+    self.assertAlmostEqual(localization_loss._delta, 0.1)
 
   def test_build_weighted_iou_localization_loss(self):
     losses_text_proto = """
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 219107ec..fc8d7c13 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -153,6 +153,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
   region_similarity_calculator = sim_calc.build(
       ssd_config.similarity_calculator)
   encode_background_as_zeros = ssd_config.encode_background_as_zeros
+  negative_class_weight = ssd_config.negative_class_weight
   ssd_box_predictor = box_predictor_builder.build(hyperparams_builder.build,
                                                   ssd_config.box_predictor,
                                                   is_training, num_classes)
@@ -165,6 +166,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
    localization_weight,
    hard_example_miner) = losses_builder.build(ssd_config.loss)
   normalize_loss_by_num_matches = ssd_config.normalize_loss_by_num_matches
+  normalize_loc_loss_by_codesize = ssd_config.normalize_loc_loss_by_codesize
 
   return ssd_meta_arch.SSDMetaArch(
       is_training,
@@ -175,6 +177,7 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
       matcher,
       region_similarity_calculator,
       encode_background_as_zeros,
+      negative_class_weight,
       image_resizer_fn,
       non_max_suppression_fn,
       score_conversion_fn,
@@ -184,7 +187,8 @@ def _build_ssd_model(ssd_config, is_training, add_summaries):
       localization_weight,
       normalize_loss_by_num_matches,
       hard_example_miner,
-      add_summaries=add_summaries)
+      add_summaries=add_summaries,
+      normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)
 
 
 def _build_faster_rcnn_feature_extractor(
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index 54df572f..d4960c3c 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -259,13 +259,15 @@ class ModelBuilderTest(tf.test.TestCase):
                 }
               }
               initializer {
-                truncated_normal_initializer {
+                random_normal_initializer {
                 }
               }
             }
             num_layers_before_predictor: 1
           }
         }
+        normalize_loss_by_num_matches: true
+        normalize_loc_loss_by_codesize: true
         loss {
           classification_loss {
             weighted_sigmoid_focal {
@@ -275,6 +277,7 @@ class ModelBuilderTest(tf.test.TestCase):
           }
           localization_loss {
             weighted_smooth_l1 {
+              delta: 0.1
             }
           }
           classification_weight: 1.0
@@ -344,6 +347,7 @@ class ModelBuilderTest(tf.test.TestCase):
             }
           }
         }
+        normalize_loc_loss_by_codesize: true
         loss {
           classification_loss {
             weighted_softmax {
@@ -362,6 +366,7 @@ class ModelBuilderTest(tf.test.TestCase):
     self.assertIsInstance(model._feature_extractor,
                           SSDMobileNetV1FeatureExtractor)
     self.assertTrue(model._feature_extractor._batch_norm_trainable)
+    self.assertTrue(model._normalize_loc_loss_by_codesize)
 
   def test_create_embedded_ssd_mobilenet_v1_model_from_config(self):
     model_text_proto = """
diff --git a/research/object_detection/builders/optimizer_builder_test.py b/research/object_detection/builders/optimizer_builder_test.py
index faebb05c..ee380305 100644
--- a/research/object_detection/builders/optimizer_builder_test.py
+++ b/research/object_detection/builders/optimizer_builder_test.py
@@ -187,7 +187,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(
         isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
-    # TODO: Find a way to not depend on the private members.
+    # TODO(rathodv): Find a way to not depend on the private members.
     self.assertAlmostEqual(optimizer._ema._decay, 0.2)
 
   def testBuildEmptyOptimizer(self):
diff --git a/research/object_detection/core/BUILD b/research/object_detection/core/BUILD
deleted file mode 100644
index c1fbc308..00000000
--- a/research/object_detection/core/BUILD
+++ /dev/null
@@ -1,384 +0,0 @@
-# Tensorflow Object Detection API: Core.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-# Apache 2.0
-
-py_library(
-    name = "batcher",
-    srcs = ["batcher.py"],
-    deps = [
-        ":prefetcher",
-        ":preprocessor",
-        ":standard_fields",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "batcher_test",
-    srcs = ["batcher_test.py"],
-    deps = [
-        ":batcher",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "box_list",
-    srcs = [
-        "box_list.py",
-    ],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "box_list_test",
-    srcs = ["box_list_test.py"],
-    deps = [
-        ":box_list",
-    ],
-)
-
-py_library(
-    name = "box_list_ops",
-    srcs = [
-        "box_list_ops.py",
-    ],
-    deps = [
-        ":box_list",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_test(
-    name = "box_list_ops_test",
-    srcs = ["box_list_ops_test.py"],
-    deps = [
-        ":box_list",
-        ":box_list_ops",
-    ],
-)
-
-py_library(
-    name = "box_coder",
-    srcs = [
-        "box_coder.py",
-    ],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "box_coder_test",
-    srcs = [
-        "box_coder_test.py",
-    ],
-    deps = [
-        ":box_coder",
-        ":box_list",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "keypoint_ops",
-    srcs = [
-        "keypoint_ops.py",
-    ],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "keypoint_ops_test",
-    srcs = ["keypoint_ops_test.py"],
-    deps = [
-        ":keypoint_ops",
-    ],
-)
-
-py_library(
-    name = "losses",
-    srcs = ["losses.py"],
-    deps = [
-        ":box_list",
-        ":box_list_ops",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_library(
-    name = "matcher",
-    srcs = [
-        "matcher.py",
-    ],
-    deps = [
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_library(
-    name = "model",
-    srcs = ["model.py"],
-    deps = [
-        ":standard_fields",
-    ],
-)
-
-py_test(
-    name = "matcher_test",
-    srcs = [
-        "matcher_test.py",
-    ],
-    deps = [
-        ":matcher",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "prefetcher",
-    srcs = ["prefetcher.py"],
-    deps = ["//tensorflow"],
-)
-
-py_library(
-    name = "preprocessor",
-    srcs = [
-        "preprocessor.py",
-    ],
-    deps = [
-        ":box_list",
-        ":box_list_ops",
-        ":keypoint_ops",
-        ":preprocessor_cache",
-        ":standard_fields",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_library(
-    name = "preprocessor_cache",
-    srcs = [
-        "preprocessor_cache.py",
-    ],
-)
-
-py_test(
-    name = "preprocessor_test",
-    srcs = [
-        "preprocessor_test.py",
-    ],
-    deps = [
-        ":preprocessor",
-        ":preprocessor_cache",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "losses_test",
-    srcs = ["losses_test.py"],
-    deps = [
-        ":box_list",
-        ":losses",
-        ":matcher",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "prefetcher_test",
-    srcs = ["prefetcher_test.py"],
-    deps = [
-        ":prefetcher",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "standard_fields",
-    srcs = [
-        "standard_fields.py",
-    ],
-)
-
-py_library(
-    name = "post_processing",
-    srcs = ["post_processing.py"],
-    deps = [
-        ":box_list",
-        ":box_list_ops",
-        ":standard_fields",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_test(
-    name = "post_processing_test",
-    srcs = ["post_processing_test.py"],
-    deps = [
-        ":box_list",
-        ":box_list_ops",
-        ":post_processing",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "target_assigner",
-    srcs = [
-        "target_assigner.py",
-    ],
-    deps = [
-        ":box_list",
-        ":matcher",
-        ":region_similarity_calculator",
-        ":standard_fields",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
-        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_test(
-    name = "target_assigner_test",
-    size = "large",
-    timeout = "long",
-    srcs = ["target_assigner_test.py"],
-    deps = [
-        ":box_list",
-        ":region_similarity_calculator",
-        ":target_assigner",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
-        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "data_decoder",
-    srcs = ["data_decoder.py"],
-)
-
-py_library(
-    name = "data_parser",
-    srcs = ["data_parser.py"],
-)
-
-py_library(
-    name = "box_predictor",
-    srcs = ["box_predictor.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//tensorflow/models/research/object_detection/utils:static_shape",
-    ],
-)
-
-py_test(
-    name = "box_predictor_test",
-    srcs = ["box_predictor_test.py"],
-    deps = [
-        ":box_predictor",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/builders:hyperparams_builder",
-        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "region_similarity_calculator",
-    srcs = [
-        "region_similarity_calculator.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-    ],
-)
-
-py_test(
-    name = "region_similarity_calculator_test",
-    srcs = [
-        "region_similarity_calculator_test.py",
-    ],
-    deps = [
-        ":region_similarity_calculator",
-        "//tensorflow/models/research/object_detection/core:box_list",
-    ],
-)
-
-py_library(
-    name = "anchor_generator",
-    srcs = [
-        "anchor_generator.py",
-    ],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "minibatch_sampler",
-    srcs = [
-        "minibatch_sampler.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_test(
-    name = "minibatch_sampler_test",
-    srcs = [
-        "minibatch_sampler_test.py",
-    ],
-    deps = [
-        ":minibatch_sampler",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "balanced_positive_negative_sampler",
-    srcs = [
-        "balanced_positive_negative_sampler.py",
-    ],
-    deps = [
-        ":minibatch_sampler",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "balanced_positive_negative_sampler_test",
-    srcs = [
-        "balanced_positive_negative_sampler_test.py",
-    ],
-    deps = [
-        ":balanced_positive_negative_sampler",
-        "//tensorflow",
-    ],
-)
diff --git a/research/object_detection/core/anchor_generator.py b/research/object_detection/core/anchor_generator.py
index 05660067..f2797ef7 100644
--- a/research/object_detection/core/anchor_generator.py
+++ b/research/object_detection/core/anchor_generator.py
@@ -77,7 +77,7 @@ class AnchorGenerator(object):
   def generate(self, feature_map_shape_list, **params):
     """Generates a collection of bounding boxes to be used as anchors.
 
-    TODO: remove **params from argument list and make stride and
+    TODO(rathodv): remove **params from argument list and make stride and
       offsets (for multiple_grid_anchor_generator) constructor arguments.
 
     Args:
@@ -88,7 +88,9 @@ class AnchorGenerator(object):
       **params: parameters for anchor generation op
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxLists each holding anchor boxes corresponding to
+        the input feature map shapes.
+
     Raises:
       ValueError: if the number of feature map shapes does not match the length
         of NumAnchorsPerLocation.
@@ -98,13 +100,14 @@ class AnchorGenerator(object):
       raise ValueError('Number of feature maps is expected to equal the length '
                        'of `num_anchors_per_location`.')
     with tf.name_scope(self.name_scope()):
-      anchors = self._generate(feature_map_shape_list, **params)
+      anchors_list = self._generate(feature_map_shape_list, **params)
       if self.check_num_anchors:
         with tf.control_dependencies([
             self._assert_correct_number_of_anchors(
-                anchors, feature_map_shape_list)]):
-          anchors.set(tf.identity(anchors.get()))
-      return anchors
+                anchors_list, feature_map_shape_list)]):
+          for item in anchors_list:
+            item.set(tf.identity(item.get()))
+      return anchors_list
 
   @abstractmethod
   def _generate(self, feature_map_shape_list, **params):
@@ -117,15 +120,17 @@ class AnchorGenerator(object):
       **params: parameters for anchor generation op
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes_list: a list of BoxList, each holding a collection of N anchor
+        boxes.
     """
     pass
 
-  def _assert_correct_number_of_anchors(self, anchors, feature_map_shape_list):
+  def _assert_correct_number_of_anchors(self, anchors_list,
+                                        feature_map_shape_list):
     """Assert that correct number of anchors was generated.
 
     Args:
-      anchors: box_list.BoxList object holding anchors generated
+      anchors_list: A list of box_list.BoxList object holding anchors generated.
       feature_map_shape_list: list of (height, width) pairs in the format
         [(height_0, width_0), (height_1, width_1), ...] that the generated
         anchors must align with.
@@ -134,10 +139,12 @@ class AnchorGenerator(object):
         match the number of expected anchors.
     """
     expected_num_anchors = 0
-    for num_anchors_per_location, feature_map_shape in zip(
-        self.num_anchors_per_location(), feature_map_shape_list):
+    actual_num_anchors = 0
+    for num_anchors_per_location, feature_map_shape, anchors in zip(
+        self.num_anchors_per_location(), feature_map_shape_list, anchors_list):
       expected_num_anchors += (num_anchors_per_location
                                * feature_map_shape[0]
                                * feature_map_shape[1])
-    return tf.assert_equal(expected_num_anchors, anchors.num_boxes())
+      actual_num_anchors += anchors.num_boxes()
+    return tf.assert_equal(expected_num_anchors, actual_num_anchors)
 
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index f886d9fd..09d85ae0 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -585,7 +585,7 @@ def sort_by_field(boxlist, field, order=SortOrder.descend, scope=None):
         ['Incorrect field size: actual vs expected.', num_entries, num_boxes])
 
     with tf.control_dependencies([length_assert]):
-      # TODO: Remove with tf.device when top_k operation runs
+      # TODO(derekjchow): Remove with tf.device when top_k operation runs
       # correctly on GPU.
       with tf.device('/cpu:0'):
         _, sorted_indices = tf.nn.top_k(field_to_sort, num_boxes, sorted=True)
@@ -657,7 +657,7 @@ def filter_greater_than(boxlist, thresh, scope=None):
   This op keeps the collection of boxes whose corresponding scores are
   greater than the input threshold.
 
-  TODO: Change function name to filter_scores_greater_than
+  TODO(jonathanhuang): Change function name to filter_scores_greater_than
 
   Args:
     boxlist: BoxList holding N boxes.  Must contain a 'scores' field
@@ -937,7 +937,7 @@ def box_voting(selected_boxes, pool_boxes, iou_thresh=0.5):
   iou_ = iou(selected_boxes, pool_boxes)
   match_indicator = tf.to_float(tf.greater(iou_, iou_thresh))
   num_matches = tf.reduce_sum(match_indicator, 1)
-  # TODO: Handle the case where some boxes in selected_boxes do not
+  # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not
   # match to any boxes in pool_boxes. For such boxes without any matches, we
   # should return the original boxes without voting.
   match_assert = tf.Assert(
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 296b73ce..6a139704 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -64,11 +64,9 @@ class BoxPredictor(object):
               scope=None, **params):
     """Computes encoded object locations and corresponding confidences.
 
-    Takes a high level image feature map as input and produce two predictions,
-    (1) a tensor encoding box locations, and
-    (2) a tensor encoding class scores for each corresponding box.
-    In this interface, we only assume that two tensors are returned as output
-    and do not assume anything about their shapes.
+    Takes a list of high level image feature maps as input and produces a list
+    of box encodings and a list of class scores where each element in the output
+    lists correspond to the feature maps in the input list.
 
     Args:
       image_features: A list of float tensors of shape [batch_size, height_i,
@@ -81,12 +79,14 @@ class BoxPredictor(object):
 
     Returns:
       A dictionary containing at least the following tensors.
-        box_encodings: A float tensor of shape
-          [batch_size, num_anchors, q, code_size] representing the location of
-          the objects, where q is 1 or the number of classes.
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, num_anchors, num_classes + 1] representing the class
-          predictions for the proposals.
+        box_encodings: A list of float tensors of shape
+          [batch_size, num_anchors_i, q, code_size] representing the location of
+          the objects, where q is 1 or the number of classes. Each entry in the
+          list corresponds to a feature map in the input `image_features` list.
+        class_predictions_with_background: A list of float tensors of shape
+          [batch_size, num_anchors_i, num_classes + 1] representing the class
+          predictions for the proposals. Each entry in the list corresponds to a
+          feature map in the input `image_features` list.
 
     Raises:
       ValueError: If length of `image_features` is not equal to length of
@@ -104,7 +104,7 @@ class BoxPredictor(object):
     return self._predict(image_features, num_predictions_per_location,
                          **params)
 
-  # TODO: num_predictions_per_location could be moved to constructor.
+  # TODO(rathodv): num_predictions_per_location could be moved to constructor.
   # This is currently only used by ConvolutionalBoxPredictor.
   @abstractmethod
   def _predict(self, image_features, num_predictions_per_location, **params):
@@ -120,12 +120,14 @@ class BoxPredictor(object):
 
     Returns:
       A dictionary containing at least the following tensors.
-        box_encodings: A float tensor of shape
-          [batch_size, num_anchors, q, code_size] representing the location of
-          the objects, where q is 1 or the number of classes.
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, num_anchors, num_classes + 1] representing the class
-          predictions for the proposals.
+        box_encodings: A list of float tensors of shape
+          [batch_size, num_anchors_i, q, code_size] representing the location of
+          the objects, where q is 1 or the number of classes. Each entry in the
+          list corresponds to a feature map in the input `image_features` list.
+        class_predictions_with_background: A list of float tensors of shape
+          [batch_size, num_anchors_i, num_classes + 1] representing the class
+          predictions for the proposals. Each entry in the list corresponds to a
+          feature map in the input `image_features` list.
     """
     pass
 
@@ -133,7 +135,7 @@ class BoxPredictor(object):
 class RfcnBoxPredictor(BoxPredictor):
   """RFCN Box Predictor.
 
-  Applies a position sensitve ROI pooling on position sensitive feature maps to
+  Applies a position sensitive ROI pooling on position sensitive feature maps to
   predict classes and refined locations. See https://arxiv.org/abs/1605.06409
   for details.
 
@@ -191,12 +193,14 @@ class RfcnBoxPredictor(BoxPredictor):
         box_code_size].
 
     Returns:
-      box_encodings: A float tensor of shape
-        [batch_size, num_anchors, num_classes, code_size] representing the
-        location of the objects.
-      class_predictions_with_background: A float tensor of shape
-        [batch_size, num_anchors, num_classes + 1] representing the class
-        predictions for the proposals.
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
 
     Raises:
       ValueError: if num_predictions_per_location is not 1 or if
@@ -266,11 +270,12 @@ class RfcnBoxPredictor(BoxPredictor):
           class_predictions_with_background,
           [batch_size * num_boxes, 1, total_classes])
 
-    return {BOX_ENCODINGS: box_encodings,
+    return {BOX_ENCODINGS: [box_encodings],
             CLASS_PREDICTIONS_WITH_BACKGROUND:
-            class_predictions_with_background}
+            [class_predictions_with_background]}
 
 
+# TODO(rathodv): Change the implementation to return lists of predictions.
 class MaskRCNNBoxPredictor(BoxPredictor):
   """Mask R-CNN Box Predictor.
 
@@ -644,18 +649,18 @@ class ConvolutionalBoxPredictor(BoxPredictor):
         feature map.
 
     Returns:
-      A dictionary containing the following tensors.
-        box_encodings: A float tensor of shape [batch_size, num_anchors, 1,
-          code_size] representing the location of the objects, where
-          num_anchors = feat_height * feat_width * num_predictions_per_location
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, num_anchors, num_classes + 1] representing the class
-          predictions for the proposals.
-
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
     """
     box_encodings_list = []
     class_predictions_list = []
-    # TODO: Come up with a better way to generate scope names
+    # TODO(rathodv): Come up with a better way to generate scope names
     # in box predictor once we have time to retrain all models in the zoo.
     # The following lines create scope names to be backwards compatible with the
     # existing checkpoints.
@@ -741,12 +746,13 @@ class ConvolutionalBoxPredictor(BoxPredictor):
                       num_predictions_per_location,
                       num_class_slots]))
         class_predictions_list.append(class_predictions_with_background)
-    return {BOX_ENCODINGS: tf.concat(box_encodings_list, axis=1),
-            CLASS_PREDICTIONS_WITH_BACKGROUND:
-            tf.concat(class_predictions_list, axis=1)}
+    return {
+        BOX_ENCODINGS: box_encodings_list,
+        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
+    }
 
 
-# TODO: Merge the implementation with ConvolutionalBoxPredictor above
+# TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above
 # since they are very similar.
 class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
   """Convolutional Box Predictor with weight sharing.
@@ -806,13 +812,14 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
         shared.
 
     Returns:
-      A dictionary containing the following tensors.
-        box_encodings: A float tensor of shape [batch_size, num_anchors, 1,
-          code_size] representing the location of the objects, where
-          num_anchors = feat_height * feat_width * num_predictions_per_location
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, num_anchors, num_classes + 1] representing the class
-          predictions for the proposals.
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
 
     Raises:
       ValueError: If the image feature maps do not have the same number of
@@ -890,6 +897,7 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                         num_predictions_per_location,
                         num_class_slots]))
           class_predictions_list.append(class_predictions_with_background)
-    return {BOX_ENCODINGS: tf.concat(box_encodings_list, axis=1),
-            CLASS_PREDICTIONS_WITH_BACKGROUND:
-            tf.concat(class_predictions_list, axis=1)}
+    return {
+        BOX_ENCODINGS: box_encodings_list,
+        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
+    }
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index 39c52993..0b36234f 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -165,9 +165,11 @@ class RfcnBoxPredictorTest(tf.test.TestCase):
         [image_features], num_predictions_per_location=[1],
         scope='BoxPredictor',
         proposal_boxes=proposal_boxes)
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    class_predictions_with_background = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
+        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+    class_predictions_with_background = tf.concat(
+        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        axis=1)
 
     init_op = tf.global_variables_initializer()
     with self.test_session() as sess:
@@ -215,9 +217,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
       box_predictions = conv_box_predictor.predict(
           [image_features], num_predictions_per_location=[5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      objectness_predictions = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      objectness_predictions = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
       return (box_encodings, objectness_predictions)
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, objectness_predictions) = self.execute(graph_fn,
@@ -242,9 +246,10 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
       box_predictions = conv_box_predictor.predict(
           [image_features], num_predictions_per_location=[1],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      objectness_predictions = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      objectness_predictions = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
       return (box_encodings, objectness_predictions)
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, objectness_predictions) = self.execute(graph_fn,
@@ -273,9 +278,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
           [image_features],
           num_predictions_per_location=[5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      class_predictions_with_background = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
       return (box_encodings, class_predictions_with_background)
     (box_encodings,
      class_predictions_with_background) = self.execute(graph_fn,
@@ -302,9 +309,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
     box_predictions = conv_box_predictor.predict(
         [image_features], num_predictions_per_location=[5],
         scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    objectness_predictions = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
+        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+    objectness_predictions = tf.concat(
+        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        axis=1)
     init_op = tf.global_variables_initializer()
 
     resolution = 32
@@ -348,9 +357,11 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
     box_predictions = conv_box_predictor.predict(
         [image_features], num_predictions_per_location=[5],
         scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    objectness_predictions = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
+        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+    objectness_predictions = tf.concat(
+        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        axis=1)
     init_op = tf.global_variables_initializer()
 
     resolution = 32
@@ -412,9 +423,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       box_predictions = conv_box_predictor.predict(
           [image_features], num_predictions_per_location=[5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      objectness_predictions = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      objectness_predictions = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
       return (box_encodings, objectness_predictions)
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
     (box_encodings, objectness_predictions) = self.execute(
@@ -438,9 +450,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
           [image_features],
           num_predictions_per_location=[5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      class_predictions_with_background = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
       return (box_encodings, class_predictions_with_background)
 
     image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
@@ -466,9 +479,11 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
           [image_features1, image_features2],
           num_predictions_per_location=[5, 5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      class_predictions_with_background = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
       return (box_encodings, class_predictions_with_background)
 
     image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)
@@ -493,9 +508,11 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
           [image_features1, image_features2],
           num_predictions_per_location=[5, 5],
           scope='BoxPredictor')
-      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-      class_predictions_with_background = box_predictions[
-          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
       return (box_encodings, class_predictions_with_background)
 
     with self.test_session(graph=tf.Graph()):
@@ -543,9 +560,10 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     box_predictions = conv_box_predictor.predict(
         [image_features], num_predictions_per_location=[5],
         scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    objectness_predictions = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(box_predictions[box_predictor.BOX_ENCODINGS],
+                              axis=1)
+    objectness_predictions = tf.concat(box_predictions[
+        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND], axis=1)
     init_op = tf.global_variables_initializer()
 
     resolution = 32
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index 2fd2f547..8bc044c0 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -116,14 +116,23 @@ class WeightedL2LocalizationLoss(Loss):
 
 
 class WeightedSmoothL1LocalizationLoss(Loss):
-  """Smooth L1 localization loss function.
+  """Smooth L1 localization loss function aka Huber Loss..
 
-  The smooth L1_loss is defined elementwise as .5 x^2 if |x|<1 and |x|-.5
-  otherwise, where x is the difference between predictions and target.
+  The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and
+  0.5 x^2 + delta * (|x|-delta) otherwise, where x is the difference between
+  predictions and target.
 
   See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)
   """
 
+  def __init__(self, delta=1.0):
+    """Constructor.
+
+    Args:
+      delta: delta for smooth L1 loss.
+    """
+    self._delta = delta
+
   def _compute_loss(self, prediction_tensor, target_tensor, weights):
     """Compute loss function.
 
@@ -138,13 +147,14 @@ class WeightedSmoothL1LocalizationLoss(Loss):
       loss: a float tensor of shape [batch_size, num_anchors] tensor
         representing the value of the loss function.
     """
-    diff = prediction_tensor - target_tensor
-    abs_diff = tf.abs(diff)
-    abs_diff_lt_1 = tf.less(abs_diff, 1)
-    anchorwise_smooth_l1norm = tf.reduce_sum(
-        tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5),
-        2) * weights
-    return anchorwise_smooth_l1norm
+    return tf.reduce_sum(tf.losses.huber_loss(
+        target_tensor,
+        prediction_tensor,
+        delta=self._delta,
+        weights=tf.expand_dims(weights, axis=2),
+        loss_collection=None,
+        reduction=tf.losses.Reduction.NONE
+    ), axis=2)
 
 
 class WeightedIOULocalizationLoss(Loss):
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index 46620c2c..9fcee8d5 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -545,7 +545,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
 
   def testReturnsCorrectAnchorWiseLossWithHighLogitScaleSetting(self):
     """At very high logit_scale, all predictions will be ~0.33."""
-    # TODO: Also test logit_scale with anchorwise=False.
+    # TODO(yonib): Also test logit_scale with anchorwise=False.
     logit_scale = 10e16
     prediction_tensor = tf.constant([[[-100, 100, -100],
                                       [100, -100, -100],
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index a0bcb422..e5421589 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -204,7 +204,7 @@ def _random_integer(minval, maxval, seed):
       [], minval=minval, maxval=maxval, dtype=tf.int32, seed=seed)
 
 
-# TODO: This method is needed because the current
+# TODO(mttang): This method is needed because the current
 # tf.image.rgb_to_grayscale method does not support quantization. Replace with
 # tf.image.rgb_to_grayscale after quantization support is added.
 def _rgb_to_grayscale(images, name=None):
@@ -2140,7 +2140,7 @@ def resize_to_range(image,
     return result
 
 
-# TODO: Make sure the static shapes are preserved.
+# TODO(alirezafathi): Make sure the static shapes are preserved.
 def resize_to_min_dimension(image, masks=None, min_dimension=600):
   """Resizes image and masks given the min size maintaining the aspect ratio.
 
@@ -2226,7 +2226,7 @@ def scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):
   return tuple(result)
 
 
-# TODO: Investigate if instead the function should return None if
+# TODO(alirezafathi): Investigate if instead the function should return None if
 # masks is None.
 # pylint: disable=g-doc-return-or-yield
 def resize_image(image,
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 70832792..14e66def 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -324,7 +324,7 @@ class TargetAssigner(object):
     return self._box_coder
 
 
-# TODO: This method pulls in all the implementation dependencies into
+# TODO(rathodv): This method pulls in all the implementation dependencies into
 # core. Therefore its best to have this factory method outside of core.
 def create_target_assigner(reference, stage=None,
                            negative_class_weight=1.0,
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index b626c3e8..ccebef97 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -836,7 +836,7 @@ class CreateTargetAssignerTest(tf.test.TestCase):
   def test_create_target_assigner(self):
     """Tests that named constructor gives working target assigners.
 
-    TODO: Make this test more general.
+    TODO(rathodv): Make this test more general.
     """
     corners = [[0.0, 0.0, 1.0, 1.0]]
     groundtruth = box_list.BoxList(tf.constant(corners))
diff --git a/research/object_detection/data/BUILD b/research/object_detection/data/BUILD
deleted file mode 100644
index 6bf397ac..00000000
--- a/research/object_detection/data/BUILD
+++ /dev/null
@@ -1,9 +0,0 @@
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-exports_files([
-    "pet_label_map.pbtxt",
-])
diff --git a/research/object_detection/data_decoders/BUILD b/research/object_detection/data_decoders/BUILD
deleted file mode 100644
index a336e582..00000000
--- a/research/object_detection/data_decoders/BUILD
+++ /dev/null
@@ -1,31 +0,0 @@
-# Tensorflow Object Detection API: data decoders.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-# Apache 2.0
-
-py_library(
-    name = "tf_example_decoder",
-    srcs = ["tf_example_decoder.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:data_decoder",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_test(
-    name = "tf_example_decoder_test",
-    srcs = ["tf_example_decoder_test.py"],
-    deps = [
-        ":tf_example_decoder",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-    ],
-)
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 849b7a01..a365034d 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -101,13 +101,18 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         'image/object/weight':
             tf.VarLenFeature(tf.float32),
     }
+    if dct_method:
+      image = slim_example_decoder.Image(
+          image_key='image/encoded',
+          format_key='image/format',
+          channels=3,
+          dct_method=dct_method)
+    else:
+      image = slim_example_decoder.Image(
+          image_key='image/encoded', format_key='image/format', channels=3)
     self.items_to_handlers = {
         fields.InputDataFields.image:
-            slim_example_decoder.Image(
-                image_key='image/encoded',
-                format_key='image/format',
-                channels=3,
-                dct_method=dct_method),
+            image,
         fields.InputDataFields.source_id: (
             slim_example_decoder.Tensor('image/source_id')),
         fields.InputDataFields.key: (
diff --git a/research/object_detection/dataset_tools/BUILD b/research/object_detection/dataset_tools/BUILD
deleted file mode 100644
index 4d7a6ed1..00000000
--- a/research/object_detection/dataset_tools/BUILD
+++ /dev/null
@@ -1,132 +0,0 @@
-# Tensorflow Object Detection API: dataset tools.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_binary(
-    name = "create_coco_tf_record",
-    srcs = [
-        "create_coco_tf_record.py",
-    ],
-    deps = [
-        "//PIL:pil",
-        "//pycocotools",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_test(
-    name = "create_coco_tf_record_test",
-    srcs = [
-        "create_coco_tf_record_test.py",
-    ],
-    deps = [
-        ":create_coco_tf_record",
-        "//tensorflow",
-    ],
-)
-
-py_binary(
-    name = "create_kitti_tf_record",
-    srcs = [
-        "create_kitti_tf_record.py",
-    ],
-    deps = [
-        "//PIL:pil",
-        "//lxml",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-        "//tensorflow/models/research/object_detection/utils:np_box_ops",
-    ],
-)
-
-py_test(
-    name = "create_kitti_tf_record_test",
-    srcs = [
-        "create_kitti_tf_record_test.py",
-    ],
-    deps = [
-        ":create_kitti_tf_record",
-        "//tensorflow",
-    ],
-)
-
-py_binary(
-    name = "create_pascal_tf_record",
-    srcs = [
-        "create_pascal_tf_record.py",
-    ],
-    deps = [
-        "//PIL:pil",
-        "//lxml",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_test(
-    name = "create_pascal_tf_record_test",
-    srcs = [
-        "create_pascal_tf_record_test.py",
-    ],
-    deps = [
-        ":create_pascal_tf_record",
-        "//tensorflow",
-    ],
-)
-
-py_binary(
-    name = "create_pet_tf_record",
-    srcs = [
-        "create_pet_tf_record.py",
-    ],
-    deps = [
-        "//PIL:pil",
-        "//lxml",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_library(
-    name = "oid_tfrecord_creation",
-    srcs = ["oid_tfrecord_creation.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-    ],
-)
-
-py_test(
-    name = "oid_tfrecord_creation_test",
-    srcs = ["oid_tfrecord_creation_test.py"],
-    deps = [
-        ":oid_tfrecord_creation",
-        "//contextlib2",
-        "//pandas",
-        "//tensorflow",
-    ],
-)
-
-py_binary(
-    name = "create_oid_tf_record",
-    srcs = ["create_oid_tf_record.py"],
-    deps = [
-        ":oid_tfrecord_creation",
-        "//contextlib2",
-        "//pandas",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record.py b/research/object_detection/dataset_tools/create_kitti_tf_record.py
index 25af7cc0..2bf2ff34 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record.py
@@ -120,7 +120,7 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
 
     # Filter all bounding boxes of this frame that are of a legal class, and
     # don't overlap with a dontcare region.
-    # TODO filter out targets that are truncated or heavily occluded.
+    # TODO(talremez) filter out targets that are truncated or heavily occluded.
     annotation_for_image = filter_annotations(img_anno, classes_to_use)
 
     example = prepare_example(image_path, annotation_for_image, label_map_dict)
diff --git a/research/object_detection/dataset_tools/create_pet_tf_record.py b/research/object_detection/dataset_tools/create_pet_tf_record.py
index ed339e3a..fe63505b 100644
--- a/research/object_detection/dataset_tools/create_pet_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pet_tf_record.py
@@ -256,7 +256,7 @@ def create_tf_record(output_filename,
   writer.close()
 
 
-# TODO: Add test for pet/PASCAL main files.
+# TODO(derekjchow): Add test for pet/PASCAL main files.
 def main(_):
   data_dir = FLAGS.data_dir
   label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index fa8d7b60..3b26d7c9 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -50,7 +50,7 @@ def write_metrics(metrics, global_step, summary_dir):
   logging.info('Metrics written to tf summary.')
 
 
-# TODO: Add tests.
+# TODO(rathodv): Add tests.
 def visualize_detection_results(result_dict,
                                 tag,
                                 global_step,
@@ -289,7 +289,7 @@ def _run_checkpoint_once(tensor_dict,
         for evaluator in evaluators:
           # TODO(b/65130867): Use image_id tensor once we fix the input data
           # decoders to return correct image_id.
-          # TODO: result_dict contains batches of images, while
+          # TODO(akuznetsa): result_dict contains batches of images, while
           # add_single_ground_truth_image_info expects a single image. Fix
           evaluator.add_single_ground_truth_image_info(
               image_id=batch, groundtruth_dict=result_dict)
@@ -314,7 +314,7 @@ def _run_checkpoint_once(tensor_dict,
   return (global_step, all_evaluator_metrics)
 
 
-# TODO: Add tests.
+# TODO(rathodv): Add tests.
 def repeated_checkpoint_run(tensor_dict,
                             summary_dir,
                             evaluators,
@@ -507,7 +507,7 @@ def result_dict_for_single_example(image,
 
   if detection_fields.detection_masks in detections:
     detection_masks = detections[detection_fields.detection_masks][0]
-    # TODO: This should be done in model's postprocess
+    # TODO(rathodv): This should be done in model's postprocess
     # function ideally.
     num_detections = tf.to_int32(detections[detection_fields.num_detections][0])
     detection_boxes = tf.slice(
diff --git a/research/object_detection/evaluator.py b/research/object_detection/evaluator.py
index a97bf02a..b763a32d 100644
--- a/research/object_detection/evaluator.py
+++ b/research/object_detection/evaluator.py
@@ -128,7 +128,7 @@ def get_evaluators(eval_config, categories):
 
 
 def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
-             checkpoint_dir, eval_dir, graph_hook_fn=None):
+             checkpoint_dir, eval_dir, graph_hook_fn=None, evaluator_list=None):
   """Evaluation function for detection models.
 
   Args:
@@ -143,6 +143,8 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
       completely built. This is helpful to perform additional changes to the
       training graph such as optimizing batchnorm. The function should modify
       the default graph.
+    evaluator_list: Optional list of instances of DetectionEvaluator. If not
+      given, this list of metrics is created according to the eval_config.
 
   Returns:
     metrics: A dictionary containing metric names and values from the latest
@@ -222,10 +224,13 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
     latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)
     saver.restore(sess, latest_checkpoint)
 
+  if not evaluator_list:
+    evaluator_list = get_evaluators(eval_config, categories)
+
   metrics = eval_util.repeated_checkpoint_run(
       tensor_dict=tensor_dict,
       summary_dir=eval_dir,
-      evaluators=get_evaluators(eval_config, categories),
+      evaluators=evaluator_list,
       batch_processor=_process_batch,
       checkpoint_dirs=[checkpoint_dir],
       variables_to_restore=None,
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index baf29944..45d58c9c 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -33,7 +33,7 @@ from object_detection.data_decoders import tf_example_decoder
 slim = tf.contrib.slim
 
 
-# TODO: Replace with freeze_graph.freeze_graph_with_def_protos when
+# TODO(derekjchow): Replace with freeze_graph.freeze_graph_with_def_protos when
 # newer version of Tensorflow becomes more common.
 def freeze_graph_with_def_protos(
     input_graph_def,
@@ -242,7 +242,7 @@ def _add_output_tensor_nodes(postprocessed_tensors,
   return outputs
 
 
-def _write_frozen_graph(frozen_graph_path, frozen_graph_def):
+def write_frozen_graph(frozen_graph_path, frozen_graph_def):
   """Writes frozen graph to disk.
 
   Args:
@@ -254,10 +254,10 @@ def _write_frozen_graph(frozen_graph_path, frozen_graph_def):
   logging.info('%d ops in the final graph.', len(frozen_graph_def.node))
 
 
-def _write_saved_model(saved_model_path,
-                       frozen_graph_def,
-                       inputs,
-                       outputs):
+def write_saved_model(saved_model_path,
+                      frozen_graph_def,
+                      inputs,
+                      outputs):
   """Writes SavedModel to disk.
 
   If checkpoint_path is not None bakes the weights into the graph thereby
@@ -301,10 +301,11 @@ def _write_saved_model(saved_model_path,
       builder.save()
 
 
-def _write_graph_and_checkpoint(inference_graph_def,
-                                model_path,
-                                input_saver_def,
-                                trained_checkpoint_prefix):
+def write_graph_and_checkpoint(inference_graph_def,
+                               model_path,
+                               input_saver_def,
+                               trained_checkpoint_prefix):
+  """Writes the graph and the checkpoint into disk."""
   for node in inference_graph_def.node:
     node.device = ''
   with tf.Graph().as_default():
@@ -316,6 +317,44 @@ def _write_graph_and_checkpoint(inference_graph_def,
       saver.save(sess, model_path)
 
 
+def _get_outputs_from_inputs(input_tensors, detection_model,
+                             output_collection_name):
+  inputs = tf.to_float(input_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+  output_tensors = detection_model.predict(
+      preprocessed_inputs, true_image_shapes)
+  postprocessed_tensors = detection_model.postprocess(
+      output_tensors, true_image_shapes)
+  return _add_output_tensor_nodes(postprocessed_tensors,
+                                  output_collection_name)
+
+
+def _build_detection_graph(input_type, detection_model, input_shape,
+                           output_collection_name, graph_hook_fn):
+  """Build the detection graph."""
+  if input_type not in input_placeholder_fn_map:
+    raise ValueError('Unknown input type: {}'.format(input_type))
+  placeholder_args = {}
+  if input_shape is not None:
+    if input_type != 'image_tensor':
+      raise ValueError('Can only specify input shape for `image_tensor` '
+                       'inputs.')
+    placeholder_args['input_shape'] = input_shape
+  placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type](
+      **placeholder_args)
+  outputs = _get_outputs_from_inputs(
+      input_tensors=input_tensors,
+      detection_model=detection_model,
+      output_collection_name=output_collection_name)
+
+  # Add global step to the graph.
+  slim.get_or_create_global_step()
+
+  if graph_hook_fn: graph_hook_fn()
+
+  return outputs, placeholder_tensor
+
+
 def _export_inference_graph(input_type,
                             detection_model,
                             use_moving_averages,
@@ -332,28 +371,12 @@ def _export_inference_graph(input_type,
   saved_model_path = os.path.join(output_directory, 'saved_model')
   model_path = os.path.join(output_directory, 'model.ckpt')
 
-  if input_type not in input_placeholder_fn_map:
-    raise ValueError('Unknown input type: {}'.format(input_type))
-  placeholder_args = {}
-  if input_shape is not None:
-    if input_type != 'image_tensor':
-      raise ValueError('Can only specify input shape for `image_tensor` '
-                       'inputs.')
-    placeholder_args['input_shape'] = input_shape
-  placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type](
-      **placeholder_args)
-  inputs = tf.to_float(input_tensors)
-  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
-  output_tensors = detection_model.predict(
-      preprocessed_inputs, true_image_shapes)
-  postprocessed_tensors = detection_model.postprocess(
-      output_tensors, true_image_shapes)
-  outputs = _add_output_tensor_nodes(postprocessed_tensors,
-                                     output_collection_name)
-  # Add global step to the graph.
-  slim.get_or_create_global_step()
-
-  if graph_hook_fn: graph_hook_fn()
+  outputs, placeholder_tensor = _build_detection_graph(
+      input_type=input_type,
+      detection_model=detection_model,
+      input_shape=input_shape,
+      output_collection_name=output_collection_name,
+      graph_hook_fn=graph_hook_fn)
 
   saver_kwargs = {}
   if use_moving_averages:
@@ -373,7 +396,7 @@ def _export_inference_graph(input_type,
   saver = tf.train.Saver(**saver_kwargs)
   input_saver_def = saver.as_saver_def()
 
-  _write_graph_and_checkpoint(
+  write_graph_and_checkpoint(
       inference_graph_def=tf.get_default_graph().as_graph_def(),
       model_path=model_path,
       input_saver_def=input_saver_def,
@@ -393,9 +416,9 @@ def _export_inference_graph(input_type,
       filename_tensor_name='save/Const:0',
       clear_devices=True,
       initializer_nodes='')
-  _write_frozen_graph(frozen_graph_path, frozen_graph_def)
-  _write_saved_model(saved_model_path, frozen_graph_def,
-                     placeholder_tensor, outputs)
+  write_frozen_graph(frozen_graph_path, frozen_graph_def)
+  write_saved_model(saved_model_path, frozen_graph_def,
+                    placeholder_tensor, outputs)
 
 
 def export_inference_graph(input_type,
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index a1be0f64..50599007 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -497,6 +497,66 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
       self.assertAllClose(num_detections_np, [2, 1])
 
+  def test_write_frozen_graph(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,
+                                          use_moving_averages=True)
+    output_directory = os.path.join(tmp_dir, 'output')
+    inference_graph_path = os.path.join(output_directory,
+                                        'frozen_inference_graph.pb')
+    tf.gfile.MakeDirs(output_directory)
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      pipeline_config.eval_config.use_moving_averages = False
+      detection_model = model_builder.build(pipeline_config.model,
+                                            is_training=False)
+      outputs, _ = exporter._build_detection_graph(
+          input_type='tf_example',
+          detection_model=detection_model,
+          input_shape=None,
+          output_collection_name='inference_op',
+          graph_hook_fn=None)
+      output_node_names = ','.join(outputs.keys())
+      saver = tf.train.Saver()
+      input_saver_def = saver.as_saver_def()
+      frozen_graph_def = exporter.freeze_graph_with_def_protos(
+          input_graph_def=tf.get_default_graph().as_graph_def(),
+          input_saver_def=input_saver_def,
+          input_checkpoint=trained_checkpoint_prefix,
+          output_node_names=output_node_names,
+          restore_op_name='save/restore_all',
+          filename_tensor_name='save/Const:0',
+          clear_devices=True,
+          initializer_nodes='')
+      exporter.write_frozen_graph(inference_graph_path, frozen_graph_def)
+
+    inference_graph = self._load_inference_graph(inference_graph_path)
+    tf_example_np = np.expand_dims(self._create_tf_example(
+        np.ones((4, 4, 3)).astype(np.uint8)), axis=0)
+    with self.test_session(graph=inference_graph) as sess:
+      tf_example = inference_graph.get_tensor_by_name('tf_example:0')
+      boxes = inference_graph.get_tensor_by_name('detection_boxes:0')
+      scores = inference_graph.get_tensor_by_name('detection_scores:0')
+      classes = inference_graph.get_tensor_by_name('detection_classes:0')
+      masks = inference_graph.get_tensor_by_name('detection_masks:0')
+      num_detections = inference_graph.get_tensor_by_name('num_detections:0')
+      (boxes_np, scores_np, classes_np, masks_np, num_detections_np) = sess.run(
+          [boxes, scores, classes, masks, num_detections],
+          feed_dict={tf_example: tf_example_np})
+      self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],
+                                      [0.5, 0.5, 0.8, 0.8]],
+                                     [[0.5, 0.5, 1.0, 1.0],
+                                      [0.0, 0.0, 0.0, 0.0]]])
+      self.assertAllClose(scores_np, [[0.7, 0.6],
+                                      [0.9, 0.0]])
+      self.assertAllClose(classes_np, [[1, 2],
+                                       [2, 1]])
+      self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
+      self.assertAllClose(num_detections_np, [2, 1])
+
   def test_export_graph_saves_pipeline_file(self):
     tmp_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
@@ -578,6 +638,82 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
         self.assertAllClose(num_detections_np, [2, 1])
 
+  def test_write_saved_model(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,
+                                          use_moving_averages=False)
+    output_directory = os.path.join(tmp_dir, 'output')
+    saved_model_path = os.path.join(output_directory, 'saved_model')
+    tf.gfile.MakeDirs(output_directory)
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      pipeline_config.eval_config.use_moving_averages = False
+      detection_model = model_builder.build(pipeline_config.model,
+                                            is_training=False)
+      outputs, placeholder_tensor = exporter._build_detection_graph(
+          input_type='tf_example',
+          detection_model=detection_model,
+          input_shape=None,
+          output_collection_name='inference_op',
+          graph_hook_fn=None)
+      output_node_names = ','.join(outputs.keys())
+      saver = tf.train.Saver()
+      input_saver_def = saver.as_saver_def()
+      frozen_graph_def = exporter.freeze_graph_with_def_protos(
+          input_graph_def=tf.get_default_graph().as_graph_def(),
+          input_saver_def=input_saver_def,
+          input_checkpoint=trained_checkpoint_prefix,
+          output_node_names=output_node_names,
+          restore_op_name='save/restore_all',
+          filename_tensor_name='save/Const:0',
+          clear_devices=True,
+          initializer_nodes='')
+      exporter.write_saved_model(
+          saved_model_path=saved_model_path,
+          frozen_graph_def=frozen_graph_def,
+          inputs=placeholder_tensor,
+          outputs=outputs)
+
+    tf_example_np = np.hstack([self._create_tf_example(
+        np.ones((4, 4, 3)).astype(np.uint8))] * 2)
+    with tf.Graph().as_default() as od_graph:
+      with self.test_session(graph=od_graph) as sess:
+        meta_graph = tf.saved_model.loader.load(
+            sess, [tf.saved_model.tag_constants.SERVING], saved_model_path)
+
+        signature = meta_graph.signature_def['serving_default']
+        input_tensor_name = signature.inputs['inputs'].name
+        tf_example = od_graph.get_tensor_by_name(input_tensor_name)
+
+        boxes = od_graph.get_tensor_by_name(
+            signature.outputs['detection_boxes'].name)
+        scores = od_graph.get_tensor_by_name(
+            signature.outputs['detection_scores'].name)
+        classes = od_graph.get_tensor_by_name(
+            signature.outputs['detection_classes'].name)
+        masks = od_graph.get_tensor_by_name(
+            signature.outputs['detection_masks'].name)
+        num_detections = od_graph.get_tensor_by_name(
+            signature.outputs['num_detections'].name)
+
+        (boxes_np, scores_np, classes_np, masks_np,
+         num_detections_np) = sess.run(
+             [boxes, scores, classes, masks, num_detections],
+             feed_dict={tf_example: tf_example_np})
+        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.8, 0.8]],
+                                       [[0.5, 0.5, 1.0, 1.0],
+                                        [0.0, 0.0, 0.0, 0.0]]])
+        self.assertAllClose(scores_np, [[0.7, 0.6],
+                                        [0.9, 0.0]])
+        self.assertAllClose(classes_np, [[1, 2],
+                                         [2, 1]])
+        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
+        self.assertAllClose(num_detections_np, [2, 1])
+
   def test_export_checkpoint_and_run_inference(self):
     tmp_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
@@ -626,6 +762,64 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
         self.assertAllClose(num_detections_np, [2, 1])
 
+  def test_write_graph_and_checkpoint(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,
+                                          use_moving_averages=False)
+    output_directory = os.path.join(tmp_dir, 'output')
+    model_path = os.path.join(output_directory, 'model.ckpt')
+    meta_graph_path = model_path + '.meta'
+    tf.gfile.MakeDirs(output_directory)
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel(add_detection_masks=True)
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      pipeline_config.eval_config.use_moving_averages = False
+      detection_model = model_builder.build(pipeline_config.model,
+                                            is_training=False)
+      exporter._build_detection_graph(
+          input_type='tf_example',
+          detection_model=detection_model,
+          input_shape=None,
+          output_collection_name='inference_op',
+          graph_hook_fn=None)
+      saver = tf.train.Saver()
+      input_saver_def = saver.as_saver_def()
+      exporter.write_graph_and_checkpoint(
+          inference_graph_def=tf.get_default_graph().as_graph_def(),
+          model_path=model_path,
+          input_saver_def=input_saver_def,
+          trained_checkpoint_prefix=trained_checkpoint_prefix)
+
+    tf_example_np = np.hstack([self._create_tf_example(
+        np.ones((4, 4, 3)).astype(np.uint8))] * 2)
+    with tf.Graph().as_default() as od_graph:
+      with self.test_session(graph=od_graph) as sess:
+        new_saver = tf.train.import_meta_graph(meta_graph_path)
+        new_saver.restore(sess, model_path)
+
+        tf_example = od_graph.get_tensor_by_name('tf_example:0')
+        boxes = od_graph.get_tensor_by_name('detection_boxes:0')
+        scores = od_graph.get_tensor_by_name('detection_scores:0')
+        classes = od_graph.get_tensor_by_name('detection_classes:0')
+        masks = od_graph.get_tensor_by_name('detection_masks:0')
+        num_detections = od_graph.get_tensor_by_name('num_detections:0')
+        (boxes_np, scores_np, classes_np, masks_np,
+         num_detections_np) = sess.run(
+             [boxes, scores, classes, masks, num_detections],
+             feed_dict={tf_example: tf_example_np})
+        self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.8, 0.8]],
+                                       [[0.5, 0.5, 1.0, 1.0],
+                                        [0.0, 0.0, 0.0, 0.0]]])
+        self.assertAllClose(scores_np, [[0.7, 0.6],
+                                        [0.9, 0.0]])
+        self.assertAllClose(classes_np, [[1, 2],
+                                         [2, 1]])
+        self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
+        self.assertAllClose(num_detections_np, [2, 1])
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index cad71b3d..a1afe498 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -4,15 +4,14 @@
 
 Tensorflow Object Detection API depends on the following libraries:
 
-* Protobuf 2.6
-* Python-tk
-* Pillow 1.0
-* lxml
-* tf Slim (which is included in the "tensorflow/models/research/" checkout)
-* Jupyter notebook
-* Matplotlib
-* Tensorflow
-* cocoapi
+*   Protobuf 2.6
+*   Pillow 1.0
+*   lxml
+*   tf Slim (which is included in the "tensorflow/models/research/" checkout)
+*   Jupyter notebook
+*   Matplotlib
+*   Tensorflow
+*   cocoapi
 
 For detailed steps to install Tensorflow, follow the [Tensorflow installation
 instructions](https://www.tensorflow.org/install/). A typical user can install
@@ -28,7 +27,7 @@ pip install tensorflow-gpu
 The remaining libraries can be installed on Ubuntu 16.04 using via apt-get:
 
 ``` bash
-sudo apt-get install protobuf-compiler python-pil python-lxml python-tk
+sudo apt-get install protobuf-compiler python-pil python-lxml
 sudo pip install jupyter
 sudo pip install matplotlib
 ```
diff --git a/research/object_detection/g3doc/using_your_own_dataset.md b/research/object_detection/g3doc/using_your_own_dataset.md
index d989d32e..397e394c 100644
--- a/research/object_detection/g3doc/using_your_own_dataset.md
+++ b/research/object_detection/g3doc/using_your_own_dataset.md
@@ -103,7 +103,7 @@ FLAGS = flags.FLAGS
 
 
 def create_tf_example(example):
-  # TODO: Populate the following variables from your example.
+  # TODO(user): Populate the following variables from your example.
   height = None # Image height
   width = None # Image width
   filename = None # Filename of the image. Empty if image is not from file
@@ -139,7 +139,7 @@ def create_tf_example(example):
 def main(_):
   writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
 
-  # TODO: Write code to read in your dataset to examples variable
+  # TODO(user): Write code to read in your dataset to examples variable
 
   for example in examples:
     tf_example = create_tf_example(example)
diff --git a/research/object_detection/inference/BUILD b/research/object_detection/inference/BUILD
deleted file mode 100644
index c651818a..00000000
--- a/research/object_detection/inference/BUILD
+++ /dev/null
@@ -1,40 +0,0 @@
-# Tensorflow Object Detection API: main runnables.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_library(
-    name = "detection_inference",
-    srcs = ["detection_inference.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "detection_inference_test",
-    srcs = ["detection_inference_test.py"],
-    deps = [
-        ":detection_inference",
-        "//PIL:pil",
-        "//numpy",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:dataset_util",
-    ],
-)
-
-py_binary(
-    name = "infer_detections",
-    srcs = ["infer_detections.py"],
-    deps = [
-        ":detection_inference",
-        "//tensorflow",
-    ],
-)
diff --git a/research/object_detection/inference/detection_inference_test.py b/research/object_detection/inference/detection_inference_test.py
index 94898970..eabb6b47 100644
--- a/research/object_detection/inference/detection_inference_test.py
+++ b/research/object_detection/inference/detection_inference_test.py
@@ -17,7 +17,6 @@ r"""Tests for detection_inference.py."""
 import os
 import StringIO
 
-
 import numpy as np
 from PIL import Image
 import tensorflow as tf
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index 3c3c36ed..8490ea42 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -34,7 +34,7 @@ FLAGS = tf.flags.FLAGS
 
 def _get_configs_for_model(model_name):
   """Returns configurations for model."""
-  # TODO: Make sure these tests work fine outside google3.
+  # TODO(ronnyvotel): Make sure these tests work fine outside google3.
   fname = os.path.join(
       FLAGS.test_srcdir,
       ('google3/third_party/tensorflow_models/'
diff --git a/research/object_detection/matchers/BUILD b/research/object_detection/matchers/BUILD
deleted file mode 100644
index 8f2ed5f4..00000000
--- a/research/object_detection/matchers/BUILD
+++ /dev/null
@@ -1,53 +0,0 @@
-# Tensorflow Object Detection API: Matcher implementations.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-py_library(
-    name = "argmax_matcher",
-    srcs = [
-        "argmax_matcher.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:matcher",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_test(
-    name = "argmax_matcher_test",
-    srcs = ["argmax_matcher_test.py"],
-    deps = [
-        ":argmax_matcher",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "bipartite_matcher",
-    srcs = [
-        "bipartite_matcher.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/contrib/image:image_py",
-        "//tensorflow/models/research/object_detection/core:matcher",
-    ],
-)
-
-py_test(
-    name = "bipartite_matcher_test",
-    srcs = [
-        "bipartite_matcher_test.py",
-    ],
-    deps = [
-        ":bipartite_matcher",
-        "//tensorflow",
-    ],
-)
diff --git a/research/object_detection/matchers/bipartite_matcher.py b/research/object_detection/matchers/bipartite_matcher.py
index f995e35a..a1bb0b84 100644
--- a/research/object_detection/matchers/bipartite_matcher.py
+++ b/research/object_detection/matchers/bipartite_matcher.py
@@ -38,7 +38,7 @@ class GreedyBipartiteMatcher(matcher.Matcher):
   def _match(self, similarity_matrix, num_valid_rows=-1):
     """Bipartite matches a collection rows and columns. A greedy bi-partite.
 
-    TODO: Add num_valid_columns options to match only that many columns
+    TODO(rathodv): Add num_valid_columns options to match only that many columns
     with all the rows.
 
     Args:
diff --git a/research/object_detection/meta_architectures/BUILD b/research/object_detection/meta_architectures/BUILD
deleted file mode 100644
index 0306e1be..00000000
--- a/research/object_detection/meta_architectures/BUILD
+++ /dev/null
@@ -1,110 +0,0 @@
-# Tensorflow Object Detection API: Meta-architectures.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_library(
-    name = "ssd_meta_arch",
-    srcs = ["ssd_meta_arch.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/core:model",
-        "//tensorflow/models/research/object_detection/core:target_assigner",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-        "//tensorflow/models/research/object_detection/utils:visualization_utils",
-    ],
-)
-
-py_test(
-    name = "ssd_meta_arch_test",
-    srcs = ["ssd_meta_arch_test.py"],
-    deps = [
-        ":ssd_meta_arch",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:anchor_generator",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/core:post_processing",
-        "//tensorflow/models/research/object_detection/core:region_similarity_calculator",
-        "//tensorflow/models/research/object_detection/utils:test_utils",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_meta_arch",
-    srcs = [
-        "faster_rcnn_meta_arch.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/core:balanced_positive_negative_sampler",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/core:model",
-        "//tensorflow/models/research/object_detection/core:post_processing",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/core:target_assigner",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_meta_arch_test_lib",
-    srcs = [
-        "faster_rcnn_meta_arch_test_lib.py",
-    ],
-    deps = [
-        ":faster_rcnn_meta_arch",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow/models/research/object_detection/builders:box_predictor_builder",
-        "//tensorflow/models/research/object_detection/builders:hyperparams_builder",
-        "//tensorflow/models/research/object_detection/builders:post_processing_builder",
-        "//tensorflow/models/research/object_detection/core:losses",
-        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_meta_arch_test",
-    srcs = ["faster_rcnn_meta_arch_test.py"],
-    deps = [
-        ":faster_rcnn_meta_arch_test_lib",
-    ],
-)
-
-py_library(
-    name = "rfcn_meta_arch",
-    srcs = ["rfcn_meta_arch.py"],
-    deps = [
-        ":faster_rcnn_meta_arch",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_test(
-    name = "rfcn_meta_arch_test",
-    srcs = ["rfcn_meta_arch_test.py"],
-    deps = [
-        ":faster_rcnn_meta_arch_test_lib",
-        ":rfcn_meta_arch",
-        "//tensorflow",
-    ],
-)
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 488a55fa..864f82a0 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -365,7 +365,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       ValueError: If first_stage_anchor_generator is not of type
         grid_anchor_generator.GridAnchorGenerator.
     """
-    # TODO: add_summaries is currently unused. Respect that directive
+    # TODO(rathodv): add_summaries is currently unused. Respect that directive
     # in the future.
     super(FasterRCNNMetaArch, self).__init__(num_classes=num_classes)
 
@@ -597,7 +597,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           `num_anchors` can differ depending on whether the model is created in
           training or inference mode.
 
-        (and if number_of_stages=1):
+        (and if number_of_stages > 1):
         7) refined_box_encodings: a 3-D tensor with shape
           [total_num_proposals, num_classes, 4] representing predicted
           (final) refined box encodings, where
@@ -910,8 +910,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         preprocessed_inputs, scope=self.first_stage_feature_extractor_scope)
 
     feature_map_shape = tf.shape(rpn_features_to_crop)
-    anchors = self._first_stage_anchor_generator.generate(
-        [(feature_map_shape[1], feature_map_shape[2])])
+    anchors = box_list_ops.concatenate(
+        self._first_stage_anchor_generator.generate([(feature_map_shape[1],
+                                                      feature_map_shape[2])]))
     with slim.arg_scope(self._first_stage_box_predictor_arg_scope):
       kernel_size = self._first_stage_box_predictor_kernel_size
       rpn_box_predictor_features = slim.conv2d(
@@ -957,9 +958,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
         num_anchors_per_location,
         scope=self.first_stage_box_predictor_scope)
 
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    objectness_predictions_with_background = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    box_encodings = tf.concat(
+        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+    objectness_predictions_with_background = tf.concat(
+        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        axis=1)
     return (tf.squeeze(box_encodings, axis=2),
             objectness_predictions_with_background)
 
@@ -1796,7 +1799,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
         # Create a new target assigner that matches the proposals to groundtruth
         # and returns the mask targets.
-        # TODO: Move `unmatched_cls_target` from constructor to assign
+        # TODO(rathodv): Move `unmatched_cls_target` from constructor to assign
         # function. This will enable reuse of a single target assigner for both
         # class targets and mask targets.
         mask_target_assigner = target_assigner.create_target_assigner(
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 00c522f6..2b1b9bc3 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -745,7 +745,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
-  # TODO: Split test into two - with and without masks.
+  # TODO(rathodv): Split test into two - with and without masks.
   def test_loss_first_stage_only_mode(self):
     model = self._build_model(
         is_training=True, number_of_stages=1, second_stage_batch_size=6)
@@ -797,7 +797,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       self.assertTrue('second_stage_localization_loss' not in loss_dict_out)
       self.assertTrue('second_stage_classification_loss' not in loss_dict_out)
 
-  # TODO: Split test into two - with and without masks.
+  # TODO(rathodv): Split test into two - with and without masks.
   def test_loss_full(self):
     model = self._build_model(
         is_training=True, number_of_stages=2, second_stage_batch_size=6)
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 219dae3b..858b0b1c 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -164,7 +164,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       ValueError: If first_stage_anchor_generator is not of type
         grid_anchor_generator.GridAnchorGenerator.
     """
-    # TODO: add_summaries is currently unused. Respect that directive
+    # TODO(rathodv): add_summaries is currently unused. Respect that directive
     # in the future.
     super(RFCNMetaArch, self).__init__(
         is_training,
@@ -275,9 +275,11 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         scope=self.second_stage_box_predictor_scope,
         proposal_boxes=proposal_boxes_normalized)
     refined_box_encodings = tf.squeeze(
-        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+        tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)
     class_predictions_with_background = tf.squeeze(
-        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+        tf.concat(
+            box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+            axis=1),
         axis=1)
 
     absolute_proposal_boxes = ops.normalized_to_image_coordinates(
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 37d43783..5d0c3657 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -23,6 +23,7 @@ import re
 import tensorflow as tf
 
 from object_detection.core import box_list
+from object_detection.core import box_list_ops
 from object_detection.core import model
 from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner
@@ -122,6 +123,7 @@ class SSDMetaArch(model.DetectionModel):
                matcher,
                region_similarity_calculator,
                encode_background_as_zeros,
+               negative_class_weight,
                image_resizer_fn,
                non_max_suppression_fn,
                score_conversion_fn,
@@ -131,7 +133,8 @@ class SSDMetaArch(model.DetectionModel):
                localization_loss_weight,
                normalize_loss_by_num_matches,
                hard_example_miner,
-               add_summaries=True):
+               add_summaries=True,
+               normalize_loc_loss_by_codesize=False):
     """SSDMetaArch Constructor.
 
     TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
@@ -151,6 +154,7 @@ class SSDMetaArch(model.DetectionModel):
       encode_background_as_zeros: boolean determining whether background
         targets are to be encoded as an all zeros vector or a one-hot
         vector (where background is the 0th class).
+      negative_class_weight: Weight for confidence loss of negative anchors.
       image_resizer_fn: a callable for image resizing.  This callable always
         takes a rank-3 image tensor (corresponding to a single image) and
         returns a rank-3 image tensor, possibly with new spatial dimensions and
@@ -175,6 +179,8 @@ class SSDMetaArch(model.DetectionModel):
       hard_example_miner: a losses.HardExampleMiner object (can be None)
       add_summaries: boolean (default: True) controlling whether summary ops
         should be added to tensorflow graph.
+      normalize_loc_loss_by_codesize: whether to normalize localization loss
+        by code size of the box encoder.
     """
     super(SSDMetaArch, self).__init__(num_classes=box_predictor.num_classes)
     self._is_training = is_training
@@ -191,7 +197,7 @@ class SSDMetaArch(model.DetectionModel):
     self._matcher = matcher
     self._region_similarity_calculator = region_similarity_calculator
 
-    # TODO: handle agnostic mode and positive/negative class
+    # TODO(jonathanhuang): handle agnostic mode
     # weights
     unmatched_cls_target = None
     unmatched_cls_target = tf.constant([1] + self.num_classes * [0],
@@ -204,7 +210,7 @@ class SSDMetaArch(model.DetectionModel):
         self._region_similarity_calculator,
         self._matcher,
         self._box_coder,
-        negative_class_weight=1.0,
+        negative_class_weight=negative_class_weight,
         unmatched_cls_target=unmatched_cls_target)
 
     self._classification_loss = classification_loss
@@ -212,6 +218,7 @@ class SSDMetaArch(model.DetectionModel):
     self._classification_loss_weight = classification_loss_weight
     self._localization_loss_weight = localization_loss_weight
     self._normalize_loss_by_num_matches = normalize_loss_by_num_matches
+    self._normalize_loc_loss_by_codesize = normalize_loc_loss_by_codesize
     self._hard_example_miner = hard_example_miner
 
     self._image_resizer_fn = image_resizer_fn
@@ -254,7 +261,7 @@ class SSDMetaArch(model.DetectionModel):
     if inputs.dtype is not tf.float32:
       raise ValueError('`preprocess` expects a tf.float32 tensor')
     with tf.name_scope('Preprocessor'):
-      # TODO: revisit whether to always use batch size as
+      # TODO(jonathanhuang): revisit whether to always use batch size as
       # the number of parallel iterations vs allow for dynamic batching.
       outputs = shape_utils.static_or_dynamic_map_fn(
           self._image_resizer_fn,
@@ -344,15 +351,17 @@ class SSDMetaArch(model.DetectionModel):
     feature_map_spatial_dims = self._get_feature_map_spatial_dims(feature_maps)
     image_shape = shape_utils.combined_static_and_dynamic_shape(
         preprocessed_inputs)
-    self._anchors = self._anchor_generator.generate(
-        feature_map_spatial_dims,
-        im_height=image_shape[1],
-        im_width=image_shape[2])
+    self._anchors = box_list_ops.concatenate(
+        self._anchor_generator.generate(
+            feature_map_spatial_dims,
+            im_height=image_shape[1],
+            im_width=image_shape[2]))
     prediction_dict = self._box_predictor.predict(
         feature_maps, self._anchor_generator.num_anchors_per_location())
-    box_encodings = tf.squeeze(prediction_dict['box_encodings'], axis=2)
-    class_predictions_with_background = prediction_dict[
-        'class_predictions_with_background']
+    box_encodings = tf.squeeze(
+        tf.concat(prediction_dict['box_encodings'], axis=1), axis=2)
+    class_predictions_with_background = tf.concat(
+        prediction_dict['class_predictions_with_background'], axis=1)
     predictions_dict = {
         'preprocessed_inputs': preprocessed_inputs,
         'box_encodings': box_encodings,
@@ -530,8 +539,11 @@ class SSDMetaArch(model.DetectionModel):
                                 1.0)
 
       with tf.name_scope('localization_loss'):
-        localization_loss = ((self._localization_loss_weight / normalizer) *
-                             localization_loss)
+        localization_loss_normalizer = normalizer
+        if self._normalize_loc_loss_by_codesize:
+          localization_loss_normalizer *= self._box_coder.code_size
+        localization_loss = ((self._localization_loss_weight / (
+            localization_loss_normalizer)) * localization_loss)
       with tf.name_scope('classification_loss'):
         classification_loss = ((self._classification_loss_weight / normalizer) *
                                classification_loss)
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 4349ac5a..b3387121 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -61,12 +61,12 @@ class MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):
     return [1]
 
   def _generate(self, feature_map_shape_list, im_height, im_width):
-    return box_list.BoxList(
+    return [box_list.BoxList(
         tf.constant([[0, 0, .5, .5],
                      [0, .5, .5, 1],
                      [.5, 0, 1, .5],
                      [1., 1., 1.5, 1.5]  # Anchor that is outside clip_window.
-                    ], tf.float32))
+                    ], tf.float32))]
 
   def num_anchors(self):
     return 4
@@ -74,7 +74,8 @@ class MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):
 
 class SsdMetaArchTest(test_case.TestCase):
 
-  def _create_model(self, apply_hard_mining=True):
+  def _create_model(self, apply_hard_mining=True,
+                    normalize_loc_loss_by_codesize=False):
     is_training = False
     num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
@@ -98,6 +99,7 @@ class SsdMetaArchTest(test_case.TestCase):
         max_total_size=5)
     classification_loss_weight = 1.0
     localization_loss_weight = 1.0
+    negative_class_weight = 1.0
     normalize_loss_by_num_matches = False
 
     hard_example_miner = None
@@ -111,10 +113,11 @@ class SsdMetaArchTest(test_case.TestCase):
     model = ssd_meta_arch.SSDMetaArch(
         is_training, mock_anchor_generator, mock_box_predictor, mock_box_coder,
         fake_feature_extractor, mock_matcher, region_similarity_calculator,
-        encode_background_as_zeros, image_resizer_fn, non_max_suppression_fn,
-        tf.identity, classification_loss, localization_loss,
-        classification_loss_weight, localization_loss_weight,
-        normalize_loss_by_num_matches, hard_example_miner, add_summaries=False)
+        encode_background_as_zeros, negative_class_weight, image_resizer_fn,
+        non_max_suppression_fn, tf.identity, classification_loss,
+        localization_loss, classification_loss_weight, localization_loss_weight,
+        normalize_loss_by_num_matches, hard_example_miner, add_summaries=False,
+        normalize_loc_loss_by_codesize=normalize_loc_loss_by_codesize)
     return model, num_classes, mock_anchor_generator.num_anchors(), code_size
 
   def test_preprocess_preserves_shapes_with_dynamic_input_image(self):
@@ -287,6 +290,37 @@ class SsdMetaArchTest(test_case.TestCase):
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
+  def test_loss_results_are_correct_with_normalize_by_codesize_true(self):
+
+    with tf.Graph().as_default():
+      _, _, _, _ = self._create_model()
+    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
+                 groundtruth_classes1, groundtruth_classes2):
+      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
+      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
+      model, _, _, _ = self._create_model(apply_hard_mining=False,
+                                          normalize_loc_loss_by_codesize=True)
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+      prediction_dict = model.predict(preprocessed_tensor,
+                                      true_image_shapes=None)
+      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
+      return (loss_dict['localization_loss'],)
+
+    batch_size = 2
+    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
+    groundtruth_boxes1 = np.array([[0, 0, 1, 1]], dtype=np.float32)
+    groundtruth_boxes2 = np.array([[0, 0, 1, 1]], dtype=np.float32)
+    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
+    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
+    expected_localization_loss = 0.5 / 4
+    localization_loss = self.execute(graph_fn, [preprocessed_input,
+                                                groundtruth_boxes1,
+                                                groundtruth_boxes2,
+                                                groundtruth_classes1,
+                                                groundtruth_classes2])
+    self.assertAllClose(localization_loss, expected_localization_loss)
+
   def test_loss_results_are_correct_with_hard_example_mining(self):
 
     with tf.Graph().as_default():
diff --git a/research/object_detection/metrics/BUILD b/research/object_detection/metrics/BUILD
deleted file mode 100644
index 1bb2b05a..00000000
--- a/research/object_detection/metrics/BUILD
+++ /dev/null
@@ -1,106 +0,0 @@
-# Tensorflow Object Detection API: main runnables.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_library(
-    name = "coco_tools",
-    srcs = [
-        "coco_tools.py",
-    ],
-    deps = [
-        "//file/localfile",
-        "//file/placer",
-        "//pycocotools",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:json_utils",
-    ],
-)
-
-py_test(
-    name = "coco_tools_test",
-    srcs = [
-        "coco_tools_test.py",
-    ],
-    deps = [
-        ":coco_tools",
-        "//testing/pybase",
-        "//numpy",
-    ],
-)
-
-py_library(
-    name = "coco_evaluation",
-    srcs = [
-        "coco_evaluation.py",
-    ],
-    deps = [
-        ":coco_tools",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:object_detection_evaluation",
-    ],
-)
-
-py_test(
-    name = "coco_evaluation_test",
-    srcs = [
-        "coco_evaluation_test.py",
-    ],
-    deps = [
-        ":coco_evaluation",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_binary(
-    name = "offline_eval_map_corloc",
-    srcs = [
-        "offline_eval_map_corloc.py",
-    ],
-    deps = [
-        ":tf_example_parser",
-        "//tensorflow/models/research/object_detection:evaluator",
-        "//tensorflow/models/research/object_detection/builders:input_reader_builder",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-        "//tensorflow/models/research/object_detection/utils:config_util",
-        "//tensorflow/models/research/object_detection/utils:label_map_util",
-    ],
-)
-
-py_test(
-    name = "offline_eval_map_corloc_test",
-    srcs = [
-        "offline_eval_map_corloc_test.py",
-    ],
-    deps = [
-        ":offline_eval_map_corloc",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "tf_example_parser",
-    srcs = ["tf_example_parser.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:data_parser",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "tf_example_parser_test",
-    srcs = ["tf_example_parser_test.py"],
-    deps = [
-        ":tf_example_parser",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 5380cb7c..81ecc03a 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -17,7 +17,7 @@
 Note that nothing in this file is tensorflow related and thus cannot
 be called directly as a slim metric, for example.
 
-TODO: wrap as a slim metric in metrics.py
+TODO(jonathanhuang): wrap as a slim metric in metrics.py
 
 
 Usage example: given a set of images with ids in the list image_ids
@@ -339,7 +339,7 @@ def ExportSingleImageGroundtruthToCoco(image_id,
 
   In the exported result, "area" fields are always set to the area of the
   groundtruth bounding box and "iscrowd" fields are always set to 0.
-  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+  TODO(jonathanhuang): pass in "iscrowd" array for evaluating on COCO dataset.
 
   Args:
     image_id: a unique image identifier either of type integer or string.
@@ -416,7 +416,7 @@ def ExportGroundtruthToCOCO(image_ids,
 
   In the exported result, "area" fields are always set to the area of the
   groundtruth bounding box and "iscrowd" fields are always set to 0.
-  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+  TODO(jonathanhuang): pass in "iscrowd" array for evaluating on COCO dataset.
 
   Args:
     image_ids: a list of unique image identifier either of type integer or
diff --git a/research/object_detection/model.py b/research/object_detection/model.py
index 4ec73913..ca3b8d66 100644
--- a/research/object_detection/model.py
+++ b/research/object_detection/model.py
@@ -319,9 +319,11 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       else:
         category_index = label_map_util.create_category_index_from_labelmap(
             eval_input_config.label_map_path)
-      detection_and_groundtruth = vis_utils.draw_side_by_side_evaluation_image(
-          eval_dict, category_index, max_boxes_to_draw=20, min_score_thresh=0.2)
       if not use_tpu:
+        detection_and_groundtruth = (
+            vis_utils.draw_side_by_side_evaluation_image(
+                eval_dict, category_index, max_boxes_to_draw=20,
+                min_score_thresh=0.2))
         tf.summary.image('Detections_Left_Groundtruth_Right',
                          detection_and_groundtruth)
 
@@ -424,11 +426,11 @@ def populate_experiment(run_config,
   eval_config = configs['eval_config']
   eval_input_config = configs['eval_input_config']
 
-  if train_steps is None:
-    train_steps = train_config.num_steps if train_config.num_steps else None
+  if train_steps is None and train_config.num_steps:
+    train_steps = train_config.num_steps
 
-  if eval_steps is None:
-    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+  if eval_steps is None and eval_config.num_examples:
+    eval_steps = eval_config.num_examples
 
   detection_model_fn = functools.partial(
       model_builder.build, model_config=model_config)
diff --git a/research/object_detection/model_tpu.py b/research/object_detection/model_tpu.py
index cb2407f5..22944853 100644
--- a/research/object_detection/model_tpu.py
+++ b/research/object_detection/model_tpu.py
@@ -77,6 +77,10 @@ tf.flags.DEFINE_integer('min_eval_interval_secs', 180,
 tf.flags.DEFINE_integer(
     'eval_timeout_secs', None,
     'Maximum seconds between checkpoints before evaluation terminates.')
+tf.flags.DEFINE_string('hparams_overrides', None, 'Comma-separated list of '
+                       'hyperparameters to override defaults.')
+tf.flags.DEFINE_boolean('eval_training_data', False,
+                        'If training data should be evaluated for this job.')
 
 FLAGS = tf.flags.FLAGS
 
@@ -122,7 +126,10 @@ def create_estimator(run_config,
   Returns:
     Estimator: A estimator object used for training and evaluation
     train_input_fn: Input function for the training loop
-    eval_input_fn: Input function for the evaluation run
+    eval_validation_input_fn: Input function to run for evaluation on
+      validation data.
+    eval_training_input_fn: Input function to run for evaluation on
+      training data.
     train_steps: Number of training steps either from arg `train_steps` or
       `TrainConfig` proto
     eval_steps: Number of evaluation steps either from arg `eval_steps` or
@@ -141,15 +148,17 @@ def create_estimator(run_config,
   train_input_config = configs['train_input_config']
   eval_config = configs['eval_config']
   eval_input_config = configs['eval_input_config']
+  if FLAGS.eval_training_data:
+    eval_input_config = configs['train_input_config']
 
   if params is None:
     params = {}
 
-  if train_steps is None:
-    train_steps = train_config.num_steps if train_config.num_steps else None
+  if train_steps is None and train_config.num_steps:
+    train_steps = train_config.num_steps
 
-  if eval_steps is None:
-    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+  if eval_steps is None and eval_config.num_examples:
+    eval_steps = eval_config.num_examples
 
   detection_model_fn = functools.partial(
       model_builder.build, model_config=model_config)
@@ -159,10 +168,14 @@ def create_estimator(run_config,
       train_config=train_config,
       train_input_config=train_input_config,
       model_config=model_config)
-  eval_input_fn = inputs.create_eval_input_fn(
+  eval_validation_input_fn = inputs.create_eval_input_fn(
       eval_config=eval_config,
       eval_input_config=eval_input_config,
       model_config=model_config)
+  eval_training_input_fn = inputs.create_eval_input_fn(
+      eval_config=eval_config,
+      eval_input_config=train_input_config,
+      model_config=model_config)
 
   estimator = tpu_estimator.TPUEstimator(
       model_fn=model_fn_creator(detection_model_fn, configs, hparams,
@@ -173,7 +186,8 @@ def create_estimator(run_config,
       use_tpu=use_tpu,
       config=run_config,
       params=params)
-  return estimator, train_input_fn, eval_input_fn, train_steps, eval_steps
+  return (estimator, train_input_fn, eval_validation_input_fn,
+          eval_training_input_fn, train_steps, eval_steps)
 
 
 def main(unused_argv):
@@ -204,24 +218,27 @@ def main(unused_argv):
           iterations_per_loop=FLAGS.iterations_per_loop,
           num_shards=FLAGS.num_shards))
   params = {}
-  estimator, train_input_fn, eval_input_fn, train_steps, eval_steps = (
-      create_estimator(
-          config,
-          model_hparams.create_hparams(),
-          FLAGS.pipeline_config_path,
-          train_steps=FLAGS.num_train_steps,
-          eval_steps=FLAGS.num_eval_steps,
-          train_batch_size=FLAGS.train_batch_size,
-          use_tpu=FLAGS.use_tpu,
-          num_shards=FLAGS.num_shards,
-          params=params))
+  (estimator, train_input_fn, eval_validation_input_fn, eval_training_input_fn,
+   train_steps, eval_steps) = (
+       create_estimator(
+           config,
+           model_hparams.create_hparams(
+               hparams_overrides=FLAGS.hparams_overrides),
+           FLAGS.pipeline_config_path,
+           train_steps=FLAGS.num_train_steps,
+           eval_steps=FLAGS.num_eval_steps,
+           train_batch_size=FLAGS.train_batch_size,
+           use_tpu=FLAGS.use_tpu,
+           num_shards=FLAGS.num_shards,
+           params=params))
 
   if FLAGS.mode in ['train', 'train_and_eval']:
     estimator.train(input_fn=train_input_fn, max_steps=train_steps)
 
   if FLAGS.mode == 'train_and_eval':
     # Eval one time.
-    eval_results = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
+    eval_results = estimator.evaluate(
+        input_fn=eval_validation_input_fn, steps=eval_steps)
     tf.logging.info('Eval results: %s' % eval_results)
 
   # Continuously evaluating.
@@ -239,11 +256,18 @@ def main(unused_argv):
         timeout_fn=terminate_eval):
 
       tf.logging.info('Starting to evaluate.')
+      if FLAGS.eval_training_data:
+        name = 'training_data'
+        input_fn = eval_training_input_fn
+      else:
+        name = 'validation_data'
+        input_fn = eval_validation_input_fn
       try:
         eval_results = estimator.evaluate(
-            input_fn=eval_input_fn,
+            input_fn=input_fn,
             steps=eval_steps,
-            checkpoint_path=ckpt)
+            checkpoint_path=ckpt,
+            name=name)
         tf.logging.info('Eval results: %s' % eval_results)
 
         # Terminate eval job when final checkpoint is reached
diff --git a/research/object_detection/models/BUILD b/research/object_detection/models/BUILD
deleted file mode 100644
index 6ed8f06c..00000000
--- a/research/object_detection/models/BUILD
+++ /dev/null
@@ -1,266 +0,0 @@
-# Tensorflow Object Detection API: Models.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_library(
-    name = "feature_map_generators",
-    srcs = [
-        "feature_map_generators.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:ops",
-    ],
-)
-
-py_test(
-    name = "feature_map_generators_test",
-    srcs = [
-        "feature_map_generators_test.py",
-    ],
-    deps = [
-        ":feature_map_generators",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "ssd_feature_extractor_test",
-    srcs = [
-        "ssd_feature_extractor_test.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:test_case",
-    ],
-)
-
-py_library(
-    name = "ssd_inception_v2_feature_extractor",
-    srcs = [
-        "ssd_inception_v2_feature_extractor.py",
-    ],
-    deps = [
-        ":feature_map_generators",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//third_party/tensorflow_models/slim:inception_v2",
-    ],
-)
-
-py_library(
-    name = "ssd_inception_v3_feature_extractor",
-    srcs = [
-        "ssd_inception_v3_feature_extractor.py",
-    ],
-    deps = [
-        ":feature_map_generators",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//third_party/tensorflow_models/slim:inception_v3",
-    ],
-)
-
-py_library(
-    name = "ssd_mobilenet_v1_feature_extractor",
-    srcs = ["ssd_mobilenet_v1_feature_extractor.py"],
-    deps = [
-        ":feature_map_generators",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//third_party/tensorflow_models/slim:mobilenet_v1",
-    ],
-)
-
-py_library(
-    name = "embedded_ssd_mobilenet_v1_feature_extractor",
-    srcs = ["embedded_ssd_mobilenet_v1_feature_extractor.py"],
-    deps = [
-        ":feature_map_generators",
-        ":ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//third_party/tensorflow_models/slim:mobilenet_v1",
-    ],
-)
-
-py_library(
-    name = "ssd_resnet_v1_fpn_feature_extractor",
-    srcs = ["ssd_resnet_v1_fpn_feature_extractor.py"],
-    deps = [
-        ":feature_map_generators",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow/models/research/object_detection/utils:ops",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-        "//third_party/tensorflow_models/slim:resnet_v1",
-    ],
-)
-
-py_library(
-    name = "ssd_resnet_v1_fpn_feature_extractor_testbase",
-    srcs = ["ssd_resnet_v1_fpn_feature_extractor_testbase.py"],
-    deps = [
-        "//tensorflow/models/research/object_detection/models:ssd_feature_extractor_test",
-    ],
-)
-
-py_test(
-    name = "ssd_resnet_v1_fpn_feature_extractor_test",
-    timeout = "long",
-    srcs = ["ssd_resnet_v1_fpn_feature_extractor_test.py"],
-    deps = [
-        ":ssd_resnet_v1_fpn_feature_extractor",
-        ":ssd_resnet_v1_fpn_feature_extractor_testbase",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "ssd_inception_v2_feature_extractor_test",
-    srcs = [
-        "ssd_inception_v2_feature_extractor_test.py",
-    ],
-    deps = [
-        ":ssd_feature_extractor_test",
-        ":ssd_inception_v2_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "ssd_inception_v3_feature_extractor_test",
-    srcs = [
-        "ssd_inception_v3_feature_extractor_test.py",
-    ],
-    deps = [
-        ":ssd_feature_extractor_test",
-        ":ssd_inception_v3_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "ssd_mobilenet_v1_feature_extractor_test",
-    srcs = ["ssd_mobilenet_v1_feature_extractor_test.py"],
-    deps = [
-        ":ssd_feature_extractor_test",
-        ":ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "embedded_ssd_mobilenet_v1_feature_extractor_test",
-    srcs = ["embedded_ssd_mobilenet_v1_feature_extractor_test.py"],
-    deps = [
-        ":embedded_ssd_mobilenet_v1_feature_extractor",
-        ":ssd_feature_extractor_test",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_nas_feature_extractor_test",
-    srcs = [
-        "faster_rcnn_nas_feature_extractor_test.py",
-    ],
-    deps = [
-        ":faster_rcnn_nas_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_nas_feature_extractor",
-    srcs = [
-        "faster_rcnn_nas_feature_extractor.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//third_party/tensorflow_models/slim:nasnet",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_inception_resnet_v2_feature_extractor",
-    srcs = [
-        "faster_rcnn_inception_resnet_v2_feature_extractor.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//third_party/tensorflow_models/slim:inception_resnet_v2",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_inception_resnet_v2_feature_extractor_test",
-    srcs = [
-        "faster_rcnn_inception_resnet_v2_feature_extractor_test.py",
-    ],
-    deps = [
-        ":faster_rcnn_inception_resnet_v2_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_inception_v2_feature_extractor",
-    srcs = [
-        "faster_rcnn_inception_v2_feature_extractor.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//third_party/tensorflow_models/slim:inception_v2",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_inception_v2_feature_extractor_test",
-    srcs = [
-        "faster_rcnn_inception_v2_feature_extractor_test.py",
-    ],
-    deps = [
-        ":faster_rcnn_inception_v2_feature_extractor",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "faster_rcnn_resnet_v1_feature_extractor",
-    srcs = [
-        "faster_rcnn_resnet_v1_feature_extractor.py",
-    ],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//third_party/tensorflow_models/slim:resnet_utils",
-        "//third_party/tensorflow_models/slim:resnet_v1",
-    ],
-)
-
-py_test(
-    name = "faster_rcnn_resnet_v1_feature_extractor_test",
-    srcs = [
-        "faster_rcnn_resnet_v1_feature_extractor_test.py",
-    ],
-    deps = [
-        ":faster_rcnn_resnet_v1_feature_extractor",
-        "//tensorflow",
-    ],
-)
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
index 5abedebd..86c7406b 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
@@ -108,7 +108,7 @@ def _build_nasnet_base(hidden_previous,
   return net
 
 
-# TODO: Only fixed_shape_resizer is currently supported for NASNet
+# TODO(shlens): Only fixed_shape_resizer is currently supported for NASNet
 # featurization. The reason for this is that nasnet.py only supports
 # inputs with fully known shapes. We need to update nasnet.py to handle
 # shapes not known at compile time.
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
index 2779026a..f2558712 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
@@ -111,7 +111,7 @@ class FasterRCNNResnetV1FeatureExtractor(
 
     with tf.control_dependencies([shape_assert]):
       # Disables batchnorm for fine-tuning with smaller batch sizes.
-      # TODO: Figure out if it is needed when image
+      # TODO(chensun): Figure out if it is needed when image
       # batch size is bigger.
       with slim.arg_scope(
           resnet_utils.resnet_arg_scope(
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index cbbf6cf1..2852449a 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -40,7 +40,7 @@ EMBEDDED_SSD_MOBILENET_V1_LAYOUT = {
 }
 
 
-# TODO: add tests with different anchor strides.
+# TODO(rathodv): add tests with different anchor strides.
 class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
 
   def test_get_expected_feature_map_shapes_with_inception_v2(self):
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index 7f400156..9bd5e970 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -27,13 +27,17 @@ from object_detection.utils import test_case
 class SsdFeatureExtractorTestBase(test_case.TestCase):
 
   @abstractmethod
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
     """Constructs a new feature extractor.
 
     Args:
       depth_multiplier: float depth multiplier for feature extractor
       pad_to_multiple: the nearest multiple to zero pad the input height and
         width dimensions to.
+      use_explicit_padding: use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
     Returns:
       an ssd_meta_arch.SSDFeatureExtractor object.
     """
@@ -41,10 +45,11 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
 
   def check_extract_features_returns_correct_shape(
       self, batch_size, image_height, image_width, depth_multiplier,
-      pad_to_multiple, expected_feature_map_shapes):
+      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):
     def graph_fn(image_tensor):
       feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                         pad_to_multiple)
+                                                         pad_to_multiple,
+                                                         use_explicit_padding)
       feature_maps = feature_extractor.extract_features(image_tensor)
       return feature_maps
 
@@ -57,10 +62,11 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
 
   def check_extract_features_returns_correct_shapes_with_dynamic_inputs(
       self, batch_size, image_height, image_width, depth_multiplier,
-      pad_to_multiple, expected_feature_map_shapes):
+      pad_to_multiple, expected_feature_map_shapes, use_explicit_padding=False):
     def graph_fn(image_height, image_width):
       feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                         pad_to_multiple)
+                                                         pad_to_multiple,
+                                                         use_explicit_padding)
       image_tensor = tf.random_uniform([batch_size, image_height, image_width,
                                         3], dtype=tf.float32)
       feature_maps = feature_extractor.extract_features(image_tensor)
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 29bf092c..7139f732 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -53,8 +53,9 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
-      use_explicit_padding: Whether to use explicit padding when extracting
-        features. Default is False.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
       use_depthwise: Whether to use depthwise convolutions. Default is False.
     """
     super(SSDMobileNetV1FeatureExtractor, self).__init__(
@@ -100,7 +101,7 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     }
 
     with slim.arg_scope(self._conv_hyperparams):
-      # TODO: Enable fused batch norm once quantization supports it.
+      # TODO(skligys): Enable fused batch norm once quantization supports it.
       with slim.arg_scope([slim.batch_norm], fused=False):
         with tf.variable_scope('MobilenetV1',
                                reuse=self._reuse_weights) as scope:
@@ -109,6 +110,7 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
               final_endpoint='Conv2d_13_pointwise',
               min_depth=self._min_depth,
               depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
               scope=scope)
           feature_maps = feature_map_generators.multi_resolution_feature_maps(
               feature_map_layout=feature_map_layout,
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index 84c73128..671ed918 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -27,7 +27,8 @@ class SsdMobilenetV1FeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
-                                is_training=True, batch_norm_trainable=True):
+                                is_training=True, batch_norm_trainable=True,
+                                use_explicit_padding=False):
     """Constructs a new feature extractor.
 
     Args:
@@ -37,6 +38,9 @@ class SsdMobilenetV1FeatureExtractorTest(
       is_training: whether the network is in training mode.
       batch_norm_trainable: Whether to update batch norm parameters during
         training or not.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
     Returns:
       an ssd_meta_arch.SSDFeatureExtractor object.
     """
@@ -45,7 +49,8 @@ class SsdMobilenetV1FeatureExtractorTest(
       conv_hyperparams = sc
     return ssd_mobilenet_v1_feature_extractor.SSDMobileNetV1FeatureExtractor(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable)
+        conv_hyperparams, batch_norm_trainable=batch_norm_trainable,
+        use_explicit_padding=use_explicit_padding)
 
   def test_extract_features_returns_correct_shapes_128(self):
     image_height = 128
@@ -57,7 +62,10 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
 
   def test_extract_features_returns_correct_shapes_299(self):
     image_height = 299
@@ -69,7 +77,10 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
 
   def test_extract_features_with_dynamic_image_shape(self):
     image_height = 128
@@ -81,7 +92,10 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
 
   def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
     image_height = 299
@@ -93,7 +107,10 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
     image_height = 299
@@ -105,7 +122,10 @@ class SsdMobilenetV1FeatureExtractorTest(
                                   (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
         2, image_height, image_width, depth_multiplier, pad_to_multiple,
-        expected_feature_map_shape)
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
 
   def test_extract_features_raises_error_with_invalid_image_size(self):
     image_height = 32
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 34961eb9..1411e5e3 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """SSD Feature Pyramid Network (FPN) feature extractors based on Resnet v1.
 
 See https://arxiv.org/abs/1708.02002 for details.
@@ -87,7 +101,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     return resized_inputs - [[channel_means]]
 
   def _filter_features(self, image_features):
-    # TODO: Change resnet endpoint to strip scope prefixes instead
+    # TODO(rathodv): Change resnet endpoint to strip scope prefixes instead
     # of munging the scope here.
     filtered_image_features = dict({})
     for key, feature in image_features.items():
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
index 83f8f80e..929d52a0 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """Tests for ssd resnet v1 FPN feature extractors."""
 import tensorflow as tf
 
@@ -10,14 +24,16 @@ class SSDResnet50V1FeatureExtractorTest(
     SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet50v1Fpn feature extractor test."""
 
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
     min_depth = 32
     conv_hyperparams = {}
     batch_norm_trainable = True
     is_training = True
     return ssd_resnet_v1_fpn_feature_extractor.SSDResnet50V1FpnFeatureExtractor(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable)
+        conv_hyperparams, batch_norm_trainable,
+        use_explicit_padding=use_explicit_padding)
 
   def _resnet_scope_name(self):
     return 'resnet_v1_50'
@@ -28,7 +44,8 @@ class SSDResnet101V1FeatureExtractorTest(
     SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet101v1Fpn feature extractor test."""
 
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
     min_depth = 32
     conv_hyperparams = {}
     batch_norm_trainable = True
@@ -36,7 +53,8 @@ class SSDResnet101V1FeatureExtractorTest(
     return (
         ssd_resnet_v1_fpn_feature_extractor.SSDResnet101V1FpnFeatureExtractor(
             is_training, depth_multiplier, min_depth, pad_to_multiple,
-            conv_hyperparams, batch_norm_trainable))
+            conv_hyperparams, batch_norm_trainable,
+            use_explicit_padding=use_explicit_padding))
 
   def _resnet_scope_name(self):
     return 'resnet_v1_101'
@@ -47,7 +65,8 @@ class SSDResnet152V1FeatureExtractorTest(
     SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet152v1Fpn feature extractor test."""
 
-  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
     min_depth = 32
     conv_hyperparams = {}
     batch_norm_trainable = True
@@ -55,7 +74,8 @@ class SSDResnet152V1FeatureExtractorTest(
     return (
         ssd_resnet_v1_fpn_feature_extractor.SSDResnet152V1FpnFeatureExtractor(
             is_training, depth_multiplier, min_depth, pad_to_multiple,
-            conv_hyperparams, batch_norm_trainable))
+            conv_hyperparams, batch_norm_trainable,
+            use_explicit_padding=use_explicit_padding))
 
   def _resnet_scope_name(self):
     return 'resnet_v1_152'
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index d5478823..186f2b17 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """Tests for ssd resnet v1 FPN feature extractors."""
 import abc
 import numpy as np
diff --git a/research/object_detection/object_detection_tutorial.ipynb b/research/object_detection/object_detection_tutorial.ipynb
index b5acce97..f64cf114 100644
--- a/research/object_detection/object_detection_tutorial.ipynb
+++ b/research/object_detection/object_detection_tutorial.ipynb
@@ -58,7 +58,7 @@
    "outputs": [],
    "source": [
     "# This is needed to display the images.\n",
-    "%matplotlib inline"
+    "%matplotlib inline",
    ]
   },
   {
diff --git a/research/object_detection/protos/BUILD b/research/object_detection/protos/BUILD
deleted file mode 100644
index 439d43f0..00000000
--- a/research/object_detection/protos/BUILD
+++ /dev/null
@@ -1,381 +0,0 @@
-# Tensorflow Object Detection API: Configuration protos.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-proto_library(
-    name = "argmax_matcher_proto",
-    srcs = ["argmax_matcher.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "argmax_matcher_py_pb2",
-    api_version = 2,
-    deps = [":argmax_matcher_proto"],
-)
-
-proto_library(
-    name = "bipartite_matcher_proto",
-    srcs = ["bipartite_matcher.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "bipartite_matcher_py_pb2",
-    api_version = 2,
-    deps = [":bipartite_matcher_proto"],
-)
-
-proto_library(
-    name = "matcher_proto",
-    srcs = ["matcher.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":argmax_matcher_proto",
-        ":bipartite_matcher_proto",
-    ],
-)
-
-py_proto_library(
-    name = "matcher_py_pb2",
-    api_version = 2,
-    deps = [":matcher_proto"],
-)
-
-proto_library(
-    name = "faster_rcnn_box_coder_proto",
-    srcs = ["faster_rcnn_box_coder.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "faster_rcnn_box_coder_py_pb2",
-    api_version = 2,
-    deps = [":faster_rcnn_box_coder_proto"],
-)
-
-proto_library(
-    name = "keypoint_box_coder_proto",
-    srcs = ["keypoint_box_coder.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "keypoint_box_coder_py_pb2",
-    api_version = 2,
-    deps = [":keypoint_box_coder_proto"],
-)
-
-proto_library(
-    name = "mean_stddev_box_coder_proto",
-    srcs = ["mean_stddev_box_coder.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "mean_stddev_box_coder_py_pb2",
-    api_version = 2,
-    deps = [":mean_stddev_box_coder_proto"],
-)
-
-proto_library(
-    name = "square_box_coder_proto",
-    srcs = ["square_box_coder.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "square_box_coder_py_pb2",
-    api_version = 2,
-    deps = [":square_box_coder_proto"],
-)
-
-proto_library(
-    name = "box_coder_proto",
-    srcs = ["box_coder.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":faster_rcnn_box_coder_proto",
-        ":keypoint_box_coder_proto",
-        ":mean_stddev_box_coder_proto",
-        ":square_box_coder_proto",
-    ],
-)
-
-py_proto_library(
-    name = "box_coder_py_pb2",
-    api_version = 2,
-    deps = [":box_coder_proto"],
-)
-
-proto_library(
-    name = "grid_anchor_generator_proto",
-    srcs = ["grid_anchor_generator.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "grid_anchor_generator_py_pb2",
-    api_version = 2,
-    deps = [":grid_anchor_generator_proto"],
-)
-
-proto_library(
-    name = "ssd_anchor_generator_proto",
-    srcs = ["ssd_anchor_generator.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "ssd_anchor_generator_py_pb2",
-    api_version = 2,
-    deps = [":ssd_anchor_generator_proto"],
-)
-
-proto_library(
-    name = "multiscale_anchor_generator_proto",
-    srcs = ["multiscale_anchor_generator.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "multiscale_anchor_generator_py_pb2",
-    api_version = 2,
-    deps = [":multiscale_anchor_generator_proto"],
-)
-
-proto_library(
-    name = "anchor_generator_proto",
-    srcs = ["anchor_generator.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":grid_anchor_generator_proto",
-        ":multiscale_anchor_generator_proto",
-        ":ssd_anchor_generator_proto",
-    ],
-)
-
-py_proto_library(
-    name = "anchor_generator_py_pb2",
-    api_version = 2,
-    deps = [":anchor_generator_proto"],
-)
-
-proto_library(
-    name = "input_reader_proto",
-    srcs = ["input_reader.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "input_reader_py_pb2",
-    api_version = 2,
-    deps = [":input_reader_proto"],
-)
-
-proto_library(
-    name = "losses_proto",
-    srcs = ["losses.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "losses_py_pb2",
-    api_version = 2,
-    deps = [":losses_proto"],
-)
-
-proto_library(
-    name = "optimizer_proto",
-    srcs = ["optimizer.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "optimizer_py_pb2",
-    api_version = 2,
-    deps = [":optimizer_proto"],
-)
-
-proto_library(
-    name = "post_processing_proto",
-    srcs = ["post_processing.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "post_processing_py_pb2",
-    api_version = 2,
-    deps = [":post_processing_proto"],
-)
-
-proto_library(
-    name = "hyperparams_proto",
-    srcs = ["hyperparams.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "hyperparams_py_pb2",
-    api_version = 2,
-    deps = [":hyperparams_proto"],
-)
-
-proto_library(
-    name = "box_predictor_proto",
-    srcs = ["box_predictor.proto"],
-    cc_api_version = 2,
-    deps = [":hyperparams_proto"],
-)
-
-py_proto_library(
-    name = "box_predictor_py_pb2",
-    api_version = 2,
-    deps = [":box_predictor_proto"],
-)
-
-proto_library(
-    name = "region_similarity_calculator_proto",
-    srcs = ["region_similarity_calculator.proto"],
-    cc_api_version = 2,
-    deps = [],
-)
-
-py_proto_library(
-    name = "region_similarity_calculator_py_pb2",
-    api_version = 2,
-    deps = [":region_similarity_calculator_proto"],
-)
-
-proto_library(
-    name = "preprocessor_proto",
-    srcs = ["preprocessor.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "preprocessor_py_pb2",
-    api_version = 2,
-    deps = [":preprocessor_proto"],
-)
-
-proto_library(
-    name = "train_proto",
-    srcs = ["train.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":optimizer_proto",
-        ":preprocessor_proto",
-    ],
-)
-
-py_proto_library(
-    name = "train_py_pb2",
-    api_version = 2,
-    deps = [":train_proto"],
-)
-
-proto_library(
-    name = "eval_proto",
-    srcs = ["eval.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "eval_py_pb2",
-    api_version = 2,
-    deps = [":eval_proto"],
-)
-
-proto_library(
-    name = "image_resizer_proto",
-    srcs = ["image_resizer.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "image_resizer_py_pb2",
-    api_version = 2,
-    deps = [":image_resizer_proto"],
-)
-
-proto_library(
-    name = "faster_rcnn_proto",
-    srcs = ["faster_rcnn.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":box_predictor_proto",
-        "//tensorflow/models/research/object_detection/protos:anchor_generator_proto",
-        "//tensorflow/models/research/object_detection/protos:hyperparams_proto",
-        "//tensorflow/models/research/object_detection/protos:image_resizer_proto",
-        "//tensorflow/models/research/object_detection/protos:losses_proto",
-        "//tensorflow/models/research/object_detection/protos:post_processing_proto",
-    ],
-)
-
-proto_library(
-    name = "ssd_proto",
-    srcs = ["ssd.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":anchor_generator_proto",
-        ":box_coder_proto",
-        ":box_predictor_proto",
-        ":hyperparams_proto",
-        ":image_resizer_proto",
-        ":losses_proto",
-        ":matcher_proto",
-        ":post_processing_proto",
-        ":region_similarity_calculator_proto",
-    ],
-)
-
-proto_library(
-    name = "model_proto",
-    srcs = ["model.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":faster_rcnn_proto",
-        ":ssd_proto",
-    ],
-)
-
-py_proto_library(
-    name = "model_py_pb2",
-    api_version = 2,
-    deps = [":model_proto"],
-)
-
-proto_library(
-    name = "pipeline_proto",
-    srcs = ["pipeline.proto"],
-    cc_api_version = 2,
-    deps = [
-        ":eval_proto",
-        ":input_reader_proto",
-        ":model_proto",
-        ":train_proto",
-    ],
-)
-
-py_proto_library(
-    name = "pipeline_py_pb2",
-    api_version = 2,
-    deps = [":pipeline_proto"],
-)
-
-proto_library(
-    name = "string_int_label_map_proto",
-    srcs = ["string_int_label_map.proto"],
-    cc_api_version = 2,
-)
-
-py_proto_library(
-    name = "string_int_label_map_py_pb2",
-    api_version = 2,
-    deps = [":string_int_label_map_proto"],
-)
diff --git a/research/object_detection/protos/hyperparams.proto b/research/object_detection/protos/hyperparams.proto
index b8b9972e..8b91302f 100644
--- a/research/object_detection/protos/hyperparams.proto
+++ b/research/object_detection/protos/hyperparams.proto
@@ -65,6 +65,7 @@ message Initializer {
   oneof initializer_oneof {
     TruncatedNormalInitializer truncated_normal_initializer = 1;
     VarianceScalingInitializer variance_scaling_initializer = 2;
+    RandomNormalInitializer random_normal_initializer = 3;
   }
 }
 
@@ -89,6 +90,13 @@ message VarianceScalingInitializer {
   optional Mode mode = 3 [default = FAN_IN];
 }
 
+// Configuration proto for random normal initializer. See
+// https://www.tensorflow.org/api_docs/python/tf/random_normal_initializer
+message RandomNormalInitializer {
+  optional float mean = 1 [default = 0.0];
+  optional float stddev = 2 [default = 1.0];
+}
+
 // Configuration proto for batch norm to apply after convolution op. See
 // https://www.tensorflow.org/api_docs/python/tf/contrib/layers/batch_norm
 message BatchNorm {
diff --git a/research/object_detection/protos/losses.proto b/research/object_detection/protos/losses.proto
index 3a2ae661..b8eaf434 100644
--- a/research/object_detection/protos/losses.proto
+++ b/research/object_detection/protos/losses.proto
@@ -38,11 +38,17 @@ message WeightedL2LocalizationLoss {
   optional bool anchorwise_output = 1 [default=false];
 }
 
-// SmoothL1 (Huber) location loss: .5 * x ^ 2 if |x| < 1 else |x| - .5
+// SmoothL1 (Huber) location loss.
+// The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and
+// 0.5 x^2 + delta * (|x|-delta) otherwise, where x is the difference between
+// predictions and target.
 message WeightedSmoothL1LocalizationLoss {
   // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 1 [default=false];
+
+  // Delta value for huber loss.
+  optional float delta = 2 [default=1.0];
 }
 
 // Intersection over union location loss: 1 - IOU
diff --git a/research/object_detection/protos/multiscale_anchor_generator.proto b/research/object_detection/protos/multiscale_anchor_generator.proto
index 6f6789a1..799db98a 100644
--- a/research/object_detection/protos/multiscale_anchor_generator.proto
+++ b/research/object_detection/protos/multiscale_anchor_generator.proto
@@ -20,4 +20,7 @@ message MultiscaleAnchorGenerator {
 
   // Number of intermediate scale each scale octave
   optional int32 scales_per_octave = 5 [default = 2];
+
+  // Whether to produce anchors in normalized coordinates.
+  optional bool normalize_coordinates = 6 [default = true];
 }
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 30863d8e..b879de19 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -36,6 +36,10 @@ message Ssd {
   // zeros vector or a one-hot vector (where background is the 0th class).
   optional bool encode_background_as_zeros = 12 [default=false];
 
+  // classification weight to be associated to negative
+  // anchors (default: 1.0). The weight must be in [0., 1.].
+  optional float negative_class_weight = 13 [default = 1.0];
+
   // Box predictor to attach to the features.
   optional BoxPredictor box_predictor = 7;
 
@@ -49,6 +53,10 @@ message Ssd {
   // the anchors.
   optional bool normalize_loss_by_num_matches = 10 [default=true];
 
+  // Whether to normalize the localization loss by the code size of the box
+  // encodings. This is applied along with other normalization factors.
+  optional bool normalize_loc_loss_by_codesize = 14 [default=false];
+
   // Loss configuration for training.
   optional Loss loss = 11;
 }
diff --git a/research/object_detection/samples/configs/BUILD b/research/object_detection/samples/configs/BUILD
deleted file mode 100644
index 2580dd89..00000000
--- a/research/object_detection/samples/configs/BUILD
+++ /dev/null
@@ -1,11 +0,0 @@
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-exports_files([
-    "faster_rcnn_resnet50_pets.config",
-    "ssd_inception_v2_pets.config",
-    "ssd_mobilenet_v1_focal_loss_pets.config",
-])
diff --git a/research/object_detection/test_data/BUILD b/research/object_detection/test_data/BUILD
deleted file mode 100644
index a198ca5e..00000000
--- a/research/object_detection/test_data/BUILD
+++ /dev/null
@@ -1,9 +0,0 @@
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-exports_files([
-    "pets_examples.record",
-])
diff --git a/research/object_detection/test_images/BUILD b/research/object_detection/test_images/BUILD
deleted file mode 100644
index 3ef7bd68..00000000
--- a/research/object_detection/test_images/BUILD
+++ /dev/null
@@ -1,10 +0,0 @@
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-exports_files([
-    "image1.jpg",
-    "image2.jpg",
-])
diff --git a/research/object_detection/trainer.py b/research/object_detection/trainer.py
index 8663e9aa..b5059b94 100644
--- a/research/object_detection/trainer.py
+++ b/research/object_detection/trainer.py
@@ -235,7 +235,7 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
           train_config.prefetch_queue_capacity, data_augmentation_options)
 
     # Gather initial summaries.
-    # TODO: See if summaries can be added/extracted from global tf
+    # TODO(rathodv): See if summaries can be added/extracted from global tf
     # collections so that they don't have to be passed around.
     summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
     global_summaries = set([])
diff --git a/research/object_detection/utils/BUILD b/research/object_detection/utils/BUILD
deleted file mode 100644
index 192a5764..00000000
--- a/research/object_detection/utils/BUILD
+++ /dev/null
@@ -1,419 +0,0 @@
-# Tensorflow Object Detection API: Utility functions.
-
-package(
-    default_visibility = ["//visibility:public"],
-)
-
-licenses(["notice"])
-
-# Apache 2.0
-
-py_library(
-    name = "test_case",
-    srcs = ["test_case.py"],
-    deps = ["//tensorflow"],
-)
-
-py_library(
-    name = "category_util",
-    srcs = ["category_util.py"],
-    deps = ["//tensorflow"],
-)
-
-py_library(
-    name = "config_util",
-    srcs = ["config_util.py"],
-    deps = [
-        "//pyglib/logging",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
-    ],
-)
-
-py_library(
-    name = "dataset_util",
-    srcs = ["dataset_util.py"],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "json_utils",
-    srcs = ["json_utils.py"],
-    deps = [],
-)
-
-py_test(
-    name = "json_utils_test",
-    srcs = ["json_utils_test.py"],
-    deps = [
-        ":json_utils",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "label_map_util",
-    srcs = ["label_map_util.py"],
-    deps = [
-        "//google/protobuf",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:string_int_label_map_py_pb2",
-    ],
-)
-
-py_library(
-    name = "learning_schedules",
-    srcs = ["learning_schedules.py"],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "metrics",
-    srcs = ["metrics.py"],
-    deps = ["//numpy"],
-)
-
-py_library(
-    name = "np_box_list",
-    srcs = ["np_box_list.py"],
-    deps = ["//numpy"],
-)
-
-py_library(
-    name = "np_box_mask_list",
-    srcs = ["np_box_mask_list.py"],
-    deps = [
-        ":np_box_list",
-        "//numpy",
-    ],
-)
-
-py_library(
-    name = "np_box_list_ops",
-    srcs = ["np_box_list_ops.py"],
-    deps = [
-        ":np_box_list",
-        ":np_box_ops",
-        "//numpy",
-    ],
-)
-
-py_library(
-    name = "np_box_mask_list_ops",
-    srcs = ["np_box_mask_list_ops.py"],
-    deps = [
-        ":np_box_list_ops",
-        ":np_box_mask_list",
-        ":np_mask_ops",
-        "//numpy",
-    ],
-)
-
-py_library(
-    name = "np_box_ops",
-    srcs = ["np_box_ops.py"],
-    deps = ["//tensorflow"],
-)
-
-py_library(
-    name = "np_mask_ops",
-    srcs = ["np_mask_ops.py"],
-    deps = ["//numpy"],
-)
-
-py_library(
-    name = "object_detection_evaluation",
-    srcs = ["object_detection_evaluation.py"],
-    deps = [
-        ":label_map_util",
-        ":metrics",
-        ":per_image_evaluation",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_library(
-    name = "ops",
-    srcs = ["ops.py"],
-    deps = [
-        ":shape_utils",
-        ":static_shape",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:box_list_ops",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_library(
-    name = "per_image_evaluation",
-    srcs = ["per_image_evaluation.py"],
-    deps = [
-        ":np_box_list",
-        ":np_box_list_ops",
-        ":np_box_mask_list",
-        ":np_box_mask_list_ops",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "shape_utils",
-    srcs = ["shape_utils.py"],
-    deps = [
-        ":static_shape",
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "static_shape",
-    srcs = ["static_shape.py"],
-    deps = [],
-)
-
-py_library(
-    name = "test_utils",
-    srcs = ["test_utils.py"],
-    deps = [
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:anchor_generator",
-        "//tensorflow/models/research/object_detection/core:box_coder",
-        "//tensorflow/models/research/object_detection/core:box_list",
-        "//tensorflow/models/research/object_detection/core:box_predictor",
-        "//tensorflow/models/research/object_detection/core:matcher",
-        "//tensorflow/models/research/object_detection/utils:shape_utils",
-    ],
-)
-
-py_library(
-    name = "variables_helper",
-    srcs = ["variables_helper.py"],
-    deps = [
-        "//tensorflow",
-    ],
-)
-
-py_library(
-    name = "visualization_utils",
-    srcs = ["visualization_utils.py"],
-    deps = [
-        "//PIL:pil",
-        "//Tkinter",  # buildcleaner: keep
-        "//matplotlib",
-        "//six",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "category_util_test",
-    srcs = ["category_util_test.py"],
-    deps = [
-        ":category_util",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "config_util_test",
-    srcs = ["config_util_test.py"],
-    deps = [
-        ":config_util",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
-        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
-    ],
-)
-
-py_test(
-    name = "dataset_util_test",
-    srcs = ["dataset_util_test.py"],
-    deps = [
-        ":dataset_util",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
-    ],
-)
-
-py_test(
-    name = "label_map_util_test",
-    srcs = ["label_map_util_test.py"],
-    deps = [
-        ":label_map_util",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "learning_schedules_test",
-    srcs = ["learning_schedules_test.py"],
-    deps = [
-        ":learning_schedules",
-        ":test_case",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "metrics_test",
-    srcs = ["metrics_test.py"],
-    deps = [
-        ":metrics",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_box_list_test",
-    srcs = ["np_box_list_test.py"],
-    deps = [
-        ":np_box_list",
-        "//numpy",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_box_mask_list_test",
-    srcs = ["np_box_mask_list_test.py"],
-    deps = [
-        ":np_box_mask_list",
-        "//numpy",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_box_list_ops_test",
-    srcs = ["np_box_list_ops_test.py"],
-    deps = [
-        ":np_box_list",
-        ":np_box_list_ops",
-        "//numpy",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_box_mask_list_ops_test",
-    srcs = ["np_box_mask_list_ops_test.py"],
-    deps = [
-        ":np_box_mask_list",
-        ":np_box_mask_list_ops",
-        "//numpy",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_box_ops_test",
-    srcs = ["np_box_ops_test.py"],
-    deps = [
-        ":np_box_ops",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "np_mask_ops_test",
-    srcs = ["np_mask_ops_test.py"],
-    deps = [
-        ":np_mask_ops",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "object_detection_evaluation_test",
-    srcs = ["object_detection_evaluation_test.py"],
-    deps = [
-        ":object_detection_evaluation",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "ops_test",
-    srcs = ["ops_test.py"],
-    deps = [
-        ":ops",
-        ":test_case",
-        "//tensorflow",
-        "//tensorflow/models/research/object_detection/core:standard_fields",
-    ],
-)
-
-py_test(
-    name = "per_image_evaluation_test",
-    srcs = ["per_image_evaluation_test.py"],
-    deps = [
-        ":per_image_evaluation",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "shape_utils_test",
-    srcs = ["shape_utils_test.py"],
-    deps = [
-        ":shape_utils",
-        "//numpy",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "static_shape_test",
-    srcs = ["static_shape_test.py"],
-    deps = [
-        ":static_shape",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "test_utils_test",
-    srcs = ["test_utils_test.py"],
-    deps = [
-        ":test_utils",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "variables_helper_test",
-    srcs = ["variables_helper_test.py"],
-    deps = [
-        ":variables_helper",
-        "//tensorflow",
-    ],
-)
-
-py_test(
-    name = "visualization_utils_test",
-    srcs = ["visualization_utils_test.py"],
-    data = [
-        "//tensorflow/models/research/object_detection/test_images:image1.jpg",
-    ],
-    deps = [
-        ":visualization_utils",
-        "//pyglib/flags",
-        "//PIL:pil",
-    ],
-)
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 6c8bf246..64a9d926 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -241,6 +241,10 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
   if hparams:
     kwargs.update(hparams.values())
   for key, value in kwargs.items():
+    # pylint: disable=g-explicit-bool-comparison
+    if value == "" or value is None:
+      continue
+    # pylint: enable=g-explicit-bool-comparison
     if key == "learning_rate":
       _update_initial_learning_rate(configs, value)
       tf.logging.info("Overwriting learning rate: %f", value)
@@ -270,9 +274,8 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
       _update_input_path(configs["eval_input_config"], value)
       tf.logging.info("Overwriting eval input path: %s", value)
     if key == "label_map_path":
-      if value:
-        _update_label_map_path(configs, value)
-        tf.logging.info("Overwriting label map path: %s", value)
+      _update_label_map_path(configs, value)
+      tf.logging.info("Overwriting label map path: %s", value)
     if key == "mask_type":
       _update_mask_type(configs, value)
       tf.logging.info("Overwritten mask type: %s", value)
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index bd65edea..08c76063 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -397,6 +397,27 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(new_label_map_path,
                      configs["eval_input_config"].label_map_path)
 
+  def testDontOverwriteEmptyLabelMapPath(self):
+    """Tests that label map path will not by overwritten with empty string."""
+    original_label_map_path = "path/to/original/label_map"
+    new_label_map_path = ""
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    train_input_reader = pipeline_config.train_input_reader
+    train_input_reader.label_map_path = original_label_map_path
+    eval_input_reader = pipeline_config.eval_input_reader
+    eval_input_reader.label_map_path = original_label_map_path
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, label_map_path=new_label_map_path)
+    self.assertEqual(original_label_map_path,
+                     configs["train_input_config"].label_map_path)
+    self.assertEqual(original_label_map_path,
+                     configs["eval_input_config"].label_map_path)
+
   def testNewMaskType(self):
     """Tests that mask type can be overwritten in input readers."""
     original_mask_type = input_reader_pb2.NUMERICAL_MASKS
diff --git a/research/object_detection/utils/json_utils.py b/research/object_detection/utils/json_utils.py
index 7507e078..c8d09eb0 100644
--- a/research/object_detection/utils/json_utils.py
+++ b/research/object_detection/utils/json_utils.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """Utilities for dealing with writing json strings.
 
 json_utils wraps json.dump and json.dumps so that they can be used to safely
diff --git a/research/object_detection/utils/json_utils_test.py b/research/object_detection/utils/json_utils_test.py
index 9499a76c..5e379043 100644
--- a/research/object_detection/utils/json_utils_test.py
+++ b/research/object_detection/utils/json_utils_test.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """Tests for google3.image.understanding.object_detection.utils.json_utils."""
 import os
 
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index afe7877a..7c6937b8 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -142,6 +142,7 @@ def manual_stepping(global_step, boundaries, rates):
   if len(rates) != len(boundaries) + 1:
     raise ValueError('Number of provided learning rates must exceed '
                      'number of boundary points by exactly 1.')
+  if not boundaries: return tf.constant(rates[0])
   step_boundaries = tf.constant(boundaries, tf.int32)
   num_boundaries = len(boundaries)
   learning_rates = tf.constant(rates, tf.float32)
diff --git a/research/object_detection/utils/learning_schedules_test.py b/research/object_detection/utils/learning_schedules_test.py
index bde783ea..878c62dc 100644
--- a/research/object_detection/utils/learning_schedules_test.py
+++ b/research/object_detection/utils/learning_schedules_test.py
@@ -75,5 +75,21 @@ class LearningSchedulesTest(test_case.TestCase):
     exp_rates = [1.0, 1.0, 2.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0]
     self.assertAllClose(output_rates, exp_rates)
 
+  def testManualSteppingWithZeroBoundaries(self):
+    def graph_fn(global_step):
+      boundaries = []
+      rates = [0.01]
+      learning_rate = learning_schedules.manual_stepping(
+          global_step, boundaries, rates)
+      return (learning_rate,)
+
+    output_rates = [
+        self.execute(graph_fn, [np.array(i).astype(np.int64)])
+        for i in range(4)
+    ]
+    exp_rates = [0.01] * 4
+    self.assertAllClose(output_rates, exp_rates)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/np_box_list_ops.py b/research/object_detection/utils/np_box_list_ops.py
index 15048fc4..9ed8ebfb 100644
--- a/research/object_detection/utils/np_box_list_ops.py
+++ b/research/object_detection/utils/np_box_list_ops.py
@@ -19,7 +19,6 @@ Example box operations that are supported:
   * Areas: compute bounding box areas
   * IOU: pairwise intersection-over-union scores
 """
-
 import numpy as np
 
 from object_detection.utils import np_box_list
diff --git a/research/object_detection/utils/np_box_mask_list_ops.py b/research/object_detection/utils/np_box_mask_list_ops.py
index 1fa9495c..ebaa8631 100644
--- a/research/object_detection/utils/np_box_mask_list_ops.py
+++ b/research/object_detection/utils/np_box_mask_list_ops.py
@@ -19,7 +19,6 @@ Example box operations that are supported:
   * Areas: compute bounding box areas
   * IOU: pairwise intersection-over-union scores
 """
-
 import numpy as np
 
 from object_detection.utils import np_box_list_ops
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 309892c3..59e38ea7 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -224,7 +224,7 @@ def padded_one_hot_encoding(indices, depth, left_pad):
     ValueError: if `indices` does not have rank 1 or if `left_pad` or `depth are
       either negative or non-integers.
 
-  TODO: add runtime checks for depth and indices.
+  TODO(rathodv): add runtime checks for depth and indices.
   """
   if depth < 0 or not isinstance(depth, six.integer_types):
     raise ValueError('`depth` must be a non-negative integer.')
@@ -474,7 +474,7 @@ def normalize_to_target(inputs,
   Note that the rank of `inputs` must be known and the dimension to which
   normalization is to be applied should be statically defined.
 
-  TODO: Add option to scale by L2 norm of the entire input.
+  TODO(jonathanhuang): Add option to scale by L2 norm of the entire input.
 
   Args:
     inputs: A `Tensor` of arbitrary size.
@@ -704,7 +704,7 @@ def reframe_box_masks_to_image_masks(box_masks, boxes, image_height,
   Returns:
     A tf.float32 tensor of size [num_masks, image_height, image_width].
   """
-  # TODO: Make this a public function.
+  # TODO(rathodv): Make this a public function.
   def transform_boxes_relative_to_boxes(boxes, reference_boxes):
     boxes = tf.reshape(boxes, [-1, 2, 2])
     min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index 06bbae92..06f389a8 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -152,7 +152,7 @@ def static_or_dynamic_map_fn(fn, elems, dtype=None,
   Tensors or lists of Tensors).  Likewise, the output of `fn` can only be a
   Tensor or list of Tensors.
 
-  TODO: make this function fully interchangeable with tf.map_fn.
+  TODO(jonathanhuang): make this function fully interchangeable with tf.map_fn.
 
   Args:
     fn: The callable to be performed. It accepts one argument, which will have
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
index b829a5e4..a4b4341f 100644
--- a/research/object_detection/utils/test_case.py
+++ b/research/object_detection/utils/test_case.py
@@ -1,3 +1,17 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
 """A convenience wrapper around tf.test.TestCase to enable TPU tests."""
 
 import tensorflow as tf
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 14aa3f32..98d00817 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -23,7 +23,7 @@ import tensorflow as tf
 slim = tf.contrib.slim
 
 
-# TODO: Consider replacing with tf.contrib.filter_variables in
+# TODO(derekjchow): Consider replacing with tf.contrib.filter_variables in
 # tensorflow/contrib/framework/python/ops/variables.py
 def filter_variables(variables, filter_regex_list, invert=False):
   """Filters out the variables matching the filter_regex.
@@ -104,7 +104,7 @@ def get_variables_available_in_checkpoint(variables,
   Inspects given checkpoint and returns the subset of variables that are
   available in it.
 
-  TODO: force input and output to be a dictionary.
+  TODO(rathodv): force input and output to be a dictionary.
 
   Args:
     variables: a list or dictionary of variables to find in checkpoint.
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index cab32720..e05e3afe 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -13,12 +13,7 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for image.understanding.object_detection.core.visualization_utils.
-
-Testing with visualization in the following colab:
-https://drive.google.com/a/google.com/file/d/0B5HnKS_hMsNARERpU3MtU3I5RFE/view?usp=sharing
-
-"""
+"""Tests for image.understanding.object_detection.core.visualization_utils."""
 import logging
 import os
 
