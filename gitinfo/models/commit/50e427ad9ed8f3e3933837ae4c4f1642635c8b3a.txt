commit 50e427ad9ed8f3e3933837ae4c4f1642635c8b3a
Author: Derek Chow <derekjchow@gmail.com>
Date:   Wed Jul 5 09:43:23 2017 -0700

    Update directories in running_pets.md
    
    Correct documentation so all commands should be run from the root
    of the git directory.

diff --git a/object_detection/g3doc/running_pets.md b/object_detection/g3doc/running_pets.md
index 6975b196..73c36613 100644
--- a/object_detection/g3doc/running_pets.md
+++ b/object_detection/g3doc/running_pets.md
@@ -51,29 +51,35 @@ dataset for Oxford-IIIT Pets lives
 [here](http://www.robots.ox.ac.uk/~vgg/data/pets/). You will need to download
 both the image dataset [`images.tar.gz`](http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz)
 and the groundtruth data [`annotations.tar.gz`](http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz)
-to the `tensorflow/models` directory. This may take some time. After downloading
-the tarballs, your `object_detection` directory should appear as follows:
+to the `tensorflow/models` directory and unzip them. This may take some time.
+
+``` bash
+# From tensorflow/models/
+wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
+wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
+tar -xvf images.tar.gz
+tar -xvf annotations.tar.gz
+```
+
+After downloading the tarballs, your `tensorflow/models` directory should appear
+as follows:
 
 ```lang-none
+- images.tar.gz
+- annotations.tar.gz
++ images/
++ annotations/
 + object_detection/
-  + data/
-  - images.tar.gz
-  - annotations.tar.gz
-  - create_pet_tf_record.py
-  ... other files and directories
+... other files and directories
 ```
 
 The Tensorflow Object Detection API expects data to be in the TFRecord format,
 so we'll now run the `create_pet_tf_record` script to convert from the raw
 Oxford-IIIT Pet dataset into TFRecords. Run the following commands from the
-`object_detection` directory:
+`tensorflow/models` directory:
 
 ``` bash
 # From tensorflow/models/
-wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz
-wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz
-tar -xvf annotations.tar.gz
-tar -xvf images.tar.gz
 python object_detection/create_pet_tf_record.py \
     --label_map_path=object_detection/data/pet_label_map.pbtxt \
     --data_dir=`pwd` \
@@ -84,7 +90,7 @@ Note: It is normal to see some warnings when running this script. You may ignore
 them.
 
 Two TFRecord files named `pet_train.record` and `pet_val.record` should be generated
-in the `object_detection` directory.
+in the `tensorflow/models` directory.
 
 Now that the data has been generated, we'll need to upload it to Google Cloud
 Storage so the data can be accessed by ML Engine. Run the following command to
@@ -279,7 +285,7 @@ three files:
 * `model.ckpt-${CHECKPOINT_NUMBER}.meta`
 
 After you've identified a candidate checkpoint to export, run the following
-command from `tensorflow/models/object_detection`:
+command from `tensorflow/models`:
 
 ``` bash
 # From tensorflow/models
