commit e12e38fc50990b1d138166bfd4b05743872f7fee
Author: Mark Daoust <markdaoust@google.com>
Date:   Tue Jul 17 17:34:57 2018 -0700

    Update basic_text_classification.ipynb

diff --git a/samples/core/tutorials/keras/basic_text_classification.ipynb b/samples/core/tutorials/keras/basic_text_classification.ipynb
index 243d5386..429b46a0 100644
--- a/samples/core/tutorials/keras/basic_text_classification.ipynb
+++ b/samples/core/tutorials/keras/basic_text_classification.ipynb
@@ -551,7 +551,7 @@
         "\n",
         "A model needs a loss function and an optimizer for training. Since this is a binary classification problem and the model outputs of a probability (a single-unit layer with a sigmoid activation), we'll use the `binary_crossentropy` loss function. \n",
         "\n",
-        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing without probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
+        "This isn't the only choice for a loss function, you could, for instance, choose `mean_squared_error`. But, generally, `binary_crossentropy` is better for dealing with probabilities—it measures the \"distance\" between probability distributions, or in our case, between the ground-truth distribution and the predictions.\n",
         "\n",
         "Later, when we are exploring regression problems (say, to predict the price of a house), we will see how to use another loss function called mean squared error.\n",
         "\n",
@@ -813,4 +813,4 @@
       ]
     }
   ]
-}
\ No newline at end of file
+}
