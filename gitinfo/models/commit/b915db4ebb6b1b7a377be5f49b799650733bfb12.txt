commit b915db4ebb6b1b7a377be5f49b799650733bfb12
Author: Menglong Zhu <menglong@google.com>
Date:   Mon Nov 26 14:33:26 2018 -0800

    Fix dependency issues.
    
    PiperOrigin-RevId: 222888231

diff --git a/research/lstm_object_detection/README b/research/lstm_object_detection/README
index dd1e9911..dc49a282 100644
--- a/research/lstm_object_detection/README
+++ b/research/lstm_object_detection/README
@@ -10,4 +10,7 @@ http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Mobile_Video_Object_CV
   year={2018}
 }
 
-If you have any questions regarding this codebase, please contact menglong@google.com
\ No newline at end of file
+If you have any questions regarding this codebase, please contact us:
+masonliuw@gmail.com
+yinxiao@google.com
+menglong@google.com
\ No newline at end of file
diff --git a/research/lstm_object_detection/lstm/lstm_cells.py b/research/lstm_object_detection/lstm/lstm_cells.py
index 87d1c29d..b66ab271 100644
--- a/research/lstm_object_detection/lstm/lstm_cells.py
+++ b/research/lstm_object_detection/lstm/lstm_cells.py
@@ -15,11 +15,11 @@
 
 """BottleneckConvLSTMCell implementation."""
 
-import google3
-import tensorflow.google as tf
-import google3.learning.brain.contrib.slim as slim
+import tensorflow as tf
 from tensorflow.contrib.framework.python.ops import variables
 
+slim = tf.contrib.slim
+
 _batch_norm = tf.contrib.layers.batch_norm
 
 
@@ -195,4 +195,5 @@ class BottleneckConvLSTMCell(tf.contrib.rnn.RNNCell):
       ]
     for s, r in zip(state_size, ret_flat):
       r.set_shape([None] + s)
-    return tf.nest.pack_sequence_as(structure=[1, 1], flat_sequence=ret_flat)
+    return tf.contrib.framework.nest.pack_sequence_as(
+        structure=[1, 1], flat_sequence=ret_flat)
diff --git a/research/lstm_object_detection/seq_dataset_builder.py b/research/lstm_object_detection/seq_dataset_builder.py
index 1f479adb..3e641b80 100644
--- a/research/lstm_object_detection/seq_dataset_builder.py
+++ b/research/lstm_object_detection/seq_dataset_builder.py
@@ -24,7 +24,6 @@ that wraps the build function.
 """
 import tensorflow as tf
 import tensorflow.google as google_tf
-from google3.learning.brain.contrib.slim.data import parallel_reader
 from tensorflow.contrib.training.python.training import sequence_queueing_state_saver as sqss
 from lstm_object_detection import tf_sequence_example_decoder
 from lstm_object_detection.protos import input_reader_google_pb2
@@ -34,6 +33,7 @@ from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import ops as util_ops
 
+parallel_reader = tf.contrib.slim.parallel_reader
 # TODO(yinxiao): Make the following variable into configurable proto.
 # Padding size for the labeled objects in each frame. Here we assume each
 # frame has a total number of objects less than _PADDING_SIZE.
@@ -142,7 +142,7 @@ def build(input_reader_config,
       min_after_dequeue=input_reader_config.min_after_dequeue)
 
   # TODO(yinxiao): Add loading instance mask option.
-  decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder()
+  decoder = tf_sequence_example_decoder.TFSequenceExampleDecoder()
 
   keys_to_decode = [
       fields.InputDataFields.image, fields.InputDataFields.groundtruth_boxes,
diff --git a/research/lstm_object_detection/tf_sequence_example_decoder.py b/research/lstm_object_detection/tf_sequence_example_decoder.py
index 7463b419..18e4f472 100644
--- a/research/lstm_object_detection/tf_sequence_example_decoder.py
+++ b/research/lstm_object_detection/tf_sequence_example_decoder.py
@@ -21,14 +21,74 @@ TODO(yinxiao): When TensorFlow object detection API officially supports
 tensorflow.SequenceExample, merge this decoder.
 """
 import tensorflow as tf
-from google3.learning.brain.contrib.slim.data import tfexample_decoder
 from object_detection.core import data_decoder
 from object_detection.core import standard_fields as fields
 
-slim_example_decoder = tf.contrib.slim.tfexample_decoder
+tfexample_decoder = tf.contrib.slim.tfexample_decoder
 
 
-class TfSequenceExampleDecoder(data_decoder.DataDecoder):
+class BoundingBoxSequence(tfexample_decoder.ItemHandler):
+  """An ItemHandler that concatenates SparseTensors to Bounding Boxes.
+  """
+
+  def __init__(self, keys=None, prefix=None, return_dense=True,
+               default_value=-1.0):
+    """Initialize the bounding box handler.
+
+    Args:
+      keys: A list of four key names representing the ymin, xmin, ymax, xmax
+        in the Example or SequenceExample.
+      prefix: An optional prefix for each of the bounding box keys in the
+        Example or SequenceExample. If provided, `prefix` is prepended to each
+        key in `keys`.
+      return_dense: if True, returns a dense tensor; if False, returns as
+        sparse tensor.
+      default_value: The value used when the `tensor_key` is not found in a
+        particular `TFExample`.
+
+    Raises:
+      ValueError: if keys is not `None` and also not a list of exactly 4 keys
+    """
+    if keys is None:
+      keys = ['ymin', 'xmin', 'ymax', 'xmax']
+    elif len(keys) != 4:
+      raise ValueError('BoundingBoxSequence expects 4 keys but got {}'.format(
+          len(keys)))
+    self._prefix = prefix
+    self._keys = keys
+    self._full_keys = [prefix + k for k in keys]
+    self._return_dense = return_dense
+    self._default_value = default_value
+    super(BoundingBoxSequence, self).__init__(self._full_keys)
+
+  def tensors_to_item(self, keys_to_tensors):
+    """Maps the given dictionary of tensors to a concatenated list of bboxes.
+
+    Args:
+      keys_to_tensors: a mapping of TF-Example keys to parsed tensors.
+
+    Returns:
+      [time, num_boxes, 4] tensor of bounding box coordinates, in order
+          [y_min, x_min, y_max, x_max]. Whether the tensor is a SparseTensor
+          or a dense Tensor is determined by the return_dense parameter. Empty
+          positions in the sparse tensor are filled with -1.0 values.
+    """
+    sides = []
+    for key in self._full_keys:
+      value = keys_to_tensors[key]
+      expanded_dims = tf.concat(
+          [tf.to_int64(tf.shape(value)),
+           tf.constant([1], dtype=tf.int64)], 0)
+      side = tf.sparse_reshape(value, expanded_dims)
+      sides.append(side)
+    bounding_boxes = tf.sparse_concat(2, sides)
+    if self._return_dense:
+      bounding_boxes = tf.sparse_tensor_to_dense(
+          bounding_boxes, default_value=self._default_value)
+    return bounding_boxes
+
+
+class TFSequenceExampleDecoder(data_decoder.DataDecoder):
   """Tensorflow Sequence Example proto decoder."""
 
   def __init__(self):
@@ -62,30 +122,30 @@ class TfSequenceExampleDecoder(data_decoder.DataDecoder):
     }
     self.items_to_handlers = {
         fields.InputDataFields.image:
-            slim_example_decoder.Image(
+            tfexample_decoder.Image(
                 image_key='image/encoded',
                 format_key='image/format',
                 channels=3,
                 repeated=True),
         fields.InputDataFields.source_id: (
-            slim_example_decoder.Tensor('image/source_id')),
+            tfexample_decoder.Tensor('image/source_id')),
         fields.InputDataFields.key: (
-            slim_example_decoder.Tensor('image/key/sha256')),
+            tfexample_decoder.Tensor('image/key/sha256')),
         fields.InputDataFields.filename: (
-            slim_example_decoder.Tensor('image/filename')),
+            tfexample_decoder.Tensor('image/filename')),
         # Object boxes and classes.
         fields.InputDataFields.groundtruth_boxes:
-            tfexample_decoder.BoundingBoxSequence(prefix='bbox/'),
+            BoundingBoxSequence(prefix='bbox/'),
         fields.InputDataFields.groundtruth_classes: (
-            slim_example_decoder.Tensor('bbox/label/index')),
+            tfexample_decoder.Tensor('bbox/label/index')),
         fields.InputDataFields.groundtruth_area:
-            slim_example_decoder.Tensor('area'),
+            tfexample_decoder.Tensor('area'),
         fields.InputDataFields.groundtruth_is_crowd: (
-            slim_example_decoder.Tensor('is_crowd')),
+            tfexample_decoder.Tensor('is_crowd')),
         fields.InputDataFields.groundtruth_difficult: (
-            slim_example_decoder.Tensor('difficult')),
+            tfexample_decoder.Tensor('difficult')),
         fields.InputDataFields.groundtruth_group_of: (
-            slim_example_decoder.Tensor('group_of'))
+            tfexample_decoder.Tensor('group_of'))
     }
 
   def decode(self, tf_seq_example_string_tensor, items=None):
diff --git a/research/lstm_object_detection/tf_sequence_example_decoder_test.py b/research/lstm_object_detection/tf_sequence_example_decoder_test.py
index 8cdcabd1..4cfa3a6f 100644
--- a/research/lstm_object_detection/tf_sequence_example_decoder_test.py
+++ b/research/lstm_object_detection/tf_sequence_example_decoder_test.py
@@ -13,17 +13,19 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for video_object_detection.tf_sequence_example_decoder."""
+"""Tests for lstm_object_detection.tf_sequence_example_decoder."""
 
 import numpy as np
 import tensorflow as tf
 from tensorflow.core.example import example_pb2
 from tensorflow.core.example import feature_pb2
+from tensorflow.python.framework import dtypes
+from tensorflow.python.ops import parsing_ops
 from lstm_object_detection import tf_sequence_example_decoder
 from object_detection.core import standard_fields as fields
 
 
-class TfSequenceExampleDecoderTest(tf.test.TestCase):
+class TFSequenceExampleDecoderTest(tf.test.TestCase):
   """Tests for sequence example decoder."""
 
   def _EncodeImage(self, image_tensor, encoding_type='jpeg'):
@@ -86,7 +88,7 @@ class TfSequenceExampleDecoderTest(tf.test.TestCase):
                     ]),
             })).SerializeToString()
 
-    example_decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder()
+    example_decoder = tf_sequence_example_decoder.TFSequenceExampleDecoder()
     tensor_dict = example_decoder.decode(tf.convert_to_tensor(sequence_example))
 
     # Test tensor dict image dimension.
