commit 3a97b68c27020016a7e038af52ce803098c70dab
Author: Toby Boyd <tobyboyd@google.com>
Date:   Wed May 22 13:00:57 2019 -0700

    fix lint issues. (#6855)

diff --git a/official/transformer/utils/tokenizer.py b/official/transformer/utils/tokenizer.py
index bf4a95da..6b21fcb5 100644
--- a/official/transformer/utils/tokenizer.py
+++ b/official/transformer/utils/tokenizer.py
@@ -117,7 +117,7 @@ class Subtokenizer(object):
           token_counts, alphabet, target_vocab_size, threshold, min_count,
           reserved_tokens)
       tf.compat.v1.logging.info("Generated vocabulary with %d subtokens." %
-          len(subtoken_list))
+                                len(subtoken_list))
       _save_vocab_file(vocab_file, subtoken_list)
     return Subtokenizer(vocab_file)
 
diff --git a/official/transformer/v2/embedding_layer.py b/official/transformer/v2/embedding_layer.py
index faf82ce6..fbc40721 100644
--- a/official/transformer/v2/embedding_layer.py
+++ b/official/transformer/v2/embedding_layer.py
@@ -36,6 +36,7 @@ class EmbeddingSharedWeights(tf.keras.layers.Layer):
     self.hidden_size = hidden_size
 
   def build(self, input_shape):
+    """Build embedding layer."""
     with tf.name_scope("embedding_and_softmax"):
       # Create and initialize weights. The random normal initializer was chosen
       # arbitrarily, and works well.
diff --git a/official/transformer/v2/metrics.py b/official/transformer/v2/metrics.py
index 09b60bfe..16d0f35d 100644
--- a/official/transformer/v2/metrics.py
+++ b/official/transformer/v2/metrics.py
@@ -22,7 +22,6 @@ Metrics:
  - ROUGE score. Source:
      https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/utils/rouge.py
 """
-
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
@@ -150,7 +149,7 @@ class MetricLayer(tf.keras.layers.Layer):
         (tf.keras.metrics.Mean("accuracy"), padded_accuracy),
         (tf.keras.metrics.Mean("accuracy_top5"), padded_accuracy_top5),
         (tf.keras.metrics.Mean("accuracy_per_sequence"),
-            padded_sequence_accuracy),
+         padded_sequence_accuracy),
         (tf.keras.metrics.Mean("neg_log_perplexity"), neg_log_perplexity),
     ]
     super(MetricLayer, self).build(input_shape)
diff --git a/official/transformer/v2/optimizer.py b/official/transformer/v2/optimizer.py
index 7f112c0d..4f3ae17f 100644
--- a/official/transformer/v2/optimizer.py
+++ b/official/transformer/v2/optimizer.py
@@ -12,8 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Optimizer from addons and learning rate scheduler.
-"""
+"""Optimizer from addons and learning rate scheduler."""
 
 from __future__ import absolute_import
 from __future__ import division
