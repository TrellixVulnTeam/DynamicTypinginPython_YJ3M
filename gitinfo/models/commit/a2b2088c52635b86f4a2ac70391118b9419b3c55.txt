commit a2b2088c52635b86f4a2ac70391118b9419b3c55
Author: Martin Kersner <m.kersner@gmail.com>
Date:   Wed Nov 15 12:18:24 2017 +0900

    Replace deprecated get_or_create_global_step
    
    tf.contrib.framework.get_or_create_global_step -> tf.train.get_or_create_global_step

diff --git a/research/adversarial_text/graphs.py b/research/adversarial_text/graphs.py
index f6d049f1..d14324b8 100644
--- a/research/adversarial_text/graphs.py
+++ b/research/adversarial_text/graphs.py
@@ -116,7 +116,7 @@ class VatxtModel(object):
   """
 
   def __init__(self, cl_logits_input_dim=None):
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
     self.vocab_freqs = _get_vocab_freqs()
 
     # Cache VatxtInput objects
diff --git a/research/learning_to_remember_rare_events/model.py b/research/learning_to_remember_rare_events/model.py
index ed34603f..ae8c8dd7 100644
--- a/research/learning_to_remember_rare_events/model.py
+++ b/research/learning_to_remember_rare_events/model.py
@@ -137,7 +137,7 @@ class Model(object):
     self.memory = self.get_memory()
     self.classifier = self.get_classifier()
 
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
 
   def get_embedder(self):
     return LeNet(int(self.input_dim ** 0.5), 1, self.rep_dim)
diff --git a/research/pcl_rl/trainer.py b/research/pcl_rl/trainer.py
index 84e868a4..06b124e1 100755
--- a/research/pcl_rl/trainer.py
+++ b/research/pcl_rl/trainer.py
@@ -364,7 +364,7 @@ class Trainer(object):
 
     if FLAGS.supervisor:
       with tf.device(tf.ReplicaDeviceSetter(FLAGS.ps_tasks, merge_devices=True)):
-        self.global_step = tf.contrib.framework.get_or_create_global_step()
+        self.global_step = tf.train.get_or_create_global_step()
         tf.set_random_seed(FLAGS.tf_seed)
         self.controller = self.get_controller()
         self.model = self.controller.model
@@ -382,7 +382,7 @@ class Trainer(object):
         sess = sv.PrepareSession(FLAGS.master)
     else:
       tf.set_random_seed(FLAGS.tf_seed)
-      self.global_step = tf.contrib.framework.get_or_create_global_step()
+      self.global_step = tf.train.get_or_create_global_step()
       self.controller = self.get_controller()
       self.model = self.controller.model
       self.controller.setup()
diff --git a/research/resnet/resnet_model.py b/research/resnet/resnet_model.py
index 2be68a13..5734e397 100644
--- a/research/resnet/resnet_model.py
+++ b/research/resnet/resnet_model.py
@@ -56,7 +56,7 @@ class ResNet(object):
 
   def build_graph(self):
     """Build a whole graph for the model."""
-    self.global_step = tf.contrib.framework.get_or_create_global_step()
+    self.global_step = tf.train.get_or_create_global_step()
     self._build_model()
     if self.mode == 'train':
       self._build_train_op()
diff --git a/research/slim/nets/nasnet/nasnet_utils.py b/research/slim/nets/nasnet/nasnet_utils.py
index 60693ac4..90dd1442 100644
--- a/research/slim/nets/nasnet/nasnet_utils.py
+++ b/research/slim/nets/nasnet/nasnet_utils.py
@@ -411,8 +411,9 @@ class NasNetABaseCell(object):
         tf.summary.scalar('layer_ratio', layer_ratio)
       drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)
       # Decrease the keep probability over time
-      current_step = tf.cast(tf.contrib.framework.get_or_create_global_step(),
+      current_step = tf.cast(tf.train.get_or_create_global_step(),
                              tf.float32)
+      print("HERE")
       drop_path_burn_in_steps = self._total_training_steps
       current_ratio = (
           current_step / drop_path_burn_in_steps)
diff --git a/tutorials/image/cifar10/cifar10_train.py b/tutorials/image/cifar10/cifar10_train.py
index fd1f4bac..75ea52d0 100644
--- a/tutorials/image/cifar10/cifar10_train.py
+++ b/tutorials/image/cifar10/cifar10_train.py
@@ -61,7 +61,7 @@ parser.add_argument('--log_frequency', type=int, default=10,
 def train():
   """Train CIFAR-10 for a number of steps."""
   with tf.Graph().as_default():
-    global_step = tf.contrib.framework.get_or_create_global_step()
+    global_step = tf.train.get_or_create_global_step()
 
     # Get images and labels for CIFAR-10.
     # Force input pipeline to CPU:0 to avoid operations sometimes ending up on
diff --git a/tutorials/rnn/ptb/ptb_word_lm.py b/tutorials/rnn/ptb/ptb_word_lm.py
index 9a21a1b1..9247b272 100644
--- a/tutorials/rnn/ptb/ptb_word_lm.py
+++ b/tutorials/rnn/ptb/ptb_word_lm.py
@@ -162,7 +162,7 @@ class PTBModel(object):
     optimizer = tf.train.GradientDescentOptimizer(self._lr)
     self._train_op = optimizer.apply_gradients(
         zip(grads, tvars),
-        global_step=tf.contrib.framework.get_or_create_global_step())
+        global_step=tf.train.get_or_create_global_step())
 
     self._new_lr = tf.placeholder(
         tf.float32, shape=[], name="new_learning_rate")
