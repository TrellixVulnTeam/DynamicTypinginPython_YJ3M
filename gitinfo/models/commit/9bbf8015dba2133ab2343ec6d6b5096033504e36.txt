commit 9bbf8015dba2133ab2343ec6d6b5096033504e36
Author: pkulzc <lzc@google.com>
Date:   Thu May 30 22:27:44 2019 -0700

    Merged commit includes the following changes: (#6932)
    
    250447559  by Zhichao Lu:
    
        Update expected files format for Instance Segmentation challenge:
        - add fields ImageWidth, ImageHeight and store the values per prediction
        - as mask, store only encoded image and assume its size is ImageWidth x ImageHeight
    
    --
    250402780  by rathodv:
    
        Fix failing Mask R-CNN TPU convergence test.
    
        Cast second stage prediction tensors from bfloat16 to float32 to prevent errors in third target assignment (Mask Prediction) - Concat with different types bfloat16 and bfloat32 isn't allowed.
    
    --
    250300240  by Zhichao Lu:
    
        Addion Open Images Challenge 2019 object detection and instance segmentation
        support into Estimator framework.
    
    --
    249944839  by rathodv:
    
        Modify exporter.py to add multiclass score nodes in exported inference graphs.
    
    --
    249935201  by rathodv:
    
        Modify postprocess methods to preserve multiclass scores after non max suppression.
    
    --
    249878079  by Zhichao Lu:
    
        This CL slightly refactors some Object Detection helper functions for data creation, evaluation, and groundtruth providing.
    
        This will allow the eager+function custom loops to share code with the existing estimator training loops.
    
        Concretely we make the following changes:
        1. In input creation we separate dataset-creation into top-level helpers, and allow it to optionally accept a pre-constructed model directly instead of always creating a model from the config just for feature preprocessing.
    
        2. In coco evaluation we split the update_op creation into its own function, which the custom loops will call directly.
    
        3. In model_lib we move groundtruth providing/ datastructure munging into a helper function
    
        4. For now we put an escape hatch in `_summarize_target_assignment` when executing in tf v2.0 behavior because the summary apis used only work w/ tf 1.x
    
    --
    249673507  by rathodv:
    
        Use explicit casts instead of tf.to_float and tf.to_int32 to avoid warnings.
    
    --
    249656006  by Zhichao Lu:
    
        Add named "raw_keypoint_locations" node that corresponds with the "raw_box_locations" node.
    
    --
    249651674  by rathodv:
    
        Keep proposal boxes in float format. MatMulCropAndResize can handle the type even when feature themselves are bfloat16s.
    
    --
    249568633  by rathodv:
    
        Support q > 1 in class agnostic NMS.
        Break post_processing_test.py into 3 separate files to avoid linter errors.
    
    --
    249535530  by rathodv:
    
        Update some deprecated arguments to tf ops.
    
    --
    249368223  by rathodv:
    
        Modify MatMulCropAndResize to use MultiLevelRoIAlign method and move the tests to spatial_transform_ops.py module.
    
        This cl establishes that CropAndResize and RoIAlign are equivalent and only differ in the sampling point grid within the boxes. CropAndResize uses a uniform size x size point grid such that the corner points exactly overlap box corners, while RoiAlign divides boxes into size x size cells and uses their centers as sampling points. In this cl, we switch MatMulCropAndResize to use the MultiLevelRoIAlign implementation with `align_corner` option as MultiLevelRoIAlign implementation is more memory efficient on TPU when compared to the original MatMulCropAndResize.
    
    --
    249337338  by chowdhery:
    
        Add class-agnostic non-max-suppression in post_processing
    
    --
    249139196  by Zhichao Lu:
    
        Fix positional argument bug in export_tflite_ssd_graph
    
    --
    249120219  by Zhichao Lu:
    
        Add evaluator for computing precision limited to a given recall range.
    
    --
    249030593  by Zhichao Lu:
    
        Evaluation util to run segmentation and detection challenge evaluation.
    
    --
    248554358  by Zhichao Lu:
    
        This change contains the auxiliary changes required for TF 2.0 style training with eager+functions+dist strat loops, but not the loops themselves.
    
        It includes:
        - Updates to shape usage to support both tensorshape v1 and tensorshape v2
        - A fix to FreezableBatchNorm to not override the `training` arg in call when `None` was passed to the constructor (Not an issue in the estimator loops but it was in the custom loops)
        - Puts some constants in init_scope so they work in eager + functions
        - Makes learning rate schedules return a callable in eager mode (required so they update when the global_step changes)
        - Makes DetectionModel a tf.module so it tracks variables (e.g. ones nested in layers)
        - Removes some references to `op.name` for some losses and replaces it w/ explicit names
        - A small part of the change to allow the coco evaluation metrics to work in eager mode
    
    --
    248271226  by rathodv:
    
        Add MultiLevel RoIAlign op.
    
    --
    248229103  by rathodv:
    
        Add functions to 1. pad features maps 2. ravel 5-D indices
    
    --
    248206769  by rathodv:
    
        Add utilities needed to introduce RoI Align op.
    
    --
    248177733  by pengchong:
    
        Internal changes
    
    --
    247742582  by Zhichao Lu:
    
        Open Images Challenge 2019 instance segmentation metric: part 2
    
    --
    247525401  by Zhichao Lu:
    
        Update comments on max_class_per_detection.
    
    --
    247520753  by rathodv:
    
        Add multilevel crop and resize operation that builds on top of matmul_crop_and_resize.
    
    --
    247391600  by Zhichao Lu:
    
        Open Images Challenge 2019 instance segmentation metric
    
    --
    247325813  by chowdhery:
    
        Quantized MobileNet v2 SSD FPNLite config with depth multiplier 0.75
    
    --
    
    PiperOrigin-RevId: 250447559

diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
index 01153b1b..352b4a41 100644
--- a/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
@@ -111,11 +111,11 @@ class FlexibleGridAnchorGenerator(anchor_generator.AnchorGenerator):
       anchor_grid = grid_anchor_generator.tile_anchors(
           feat_shape[0],
           feat_shape[1],
-          tf.to_float(tf.convert_to_tensor(base_sizes)),
-          tf.to_float(tf.convert_to_tensor(aspect_ratios)),
+          tf.cast(tf.convert_to_tensor(base_sizes), dtype=tf.float32),
+          tf.cast(tf.convert_to_tensor(aspect_ratios), dtype=tf.float32),
           tf.constant([1.0, 1.0]),
-          tf.to_float(tf.convert_to_tensor(anchor_stride)),
-          tf.to_float(tf.convert_to_tensor(anchor_offset)))
+          tf.cast(tf.convert_to_tensor(anchor_stride), dtype=tf.float32),
+          tf.cast(tf.convert_to_tensor(anchor_offset), dtype=tf.float32))
       num_anchors = anchor_grid.num_boxes_static()
       if num_anchors is None:
         num_anchors = anchor_grid.num_boxes()
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index 180b1534..42892563 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -105,12 +105,16 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
     if not all([isinstance(list_item, tuple) and len(list_item) == 2
                 for list_item in feature_map_shape_list]):
       raise ValueError('feature_map_shape_list must be a list of pairs.')
-    self._base_anchor_size = tf.to_float(tf.convert_to_tensor(
-        self._base_anchor_size))
-    self._anchor_stride = tf.to_float(tf.convert_to_tensor(
-        self._anchor_stride))
-    self._anchor_offset = tf.to_float(tf.convert_to_tensor(
-        self._anchor_offset))
+
+    # Create constants in init_scope so they can be created in tf.functions
+    # and accessed from outside of the function.
+    with tf.init_scope():
+      self._base_anchor_size = tf.cast(tf.convert_to_tensor(
+          self._base_anchor_size), dtype=tf.float32)
+      self._anchor_stride = tf.cast(tf.convert_to_tensor(
+          self._anchor_stride), dtype=tf.float32)
+      self._anchor_offset = tf.cast(tf.convert_to_tensor(
+          self._anchor_offset), dtype=tf.float32)
 
     grid_height, grid_width = feature_map_shape_list[0]
     scales_grid, aspect_ratios_grid = ops.meshgrid(self._scales,
@@ -179,9 +183,9 @@ def tile_anchors(grid_height,
   widths = scales * ratio_sqrts * base_anchor_size[1]
 
   # Get a grid of box centers
-  y_centers = tf.to_float(tf.range(grid_height))
+  y_centers = tf.cast(tf.range(grid_height), dtype=tf.float32)
   y_centers = y_centers * anchor_stride[0] + anchor_offset[0]
-  x_centers = tf.to_float(tf.range(grid_width))
+  x_centers = tf.cast(tf.range(grid_width), dtype=tf.float32)
   x_centers = x_centers * anchor_stride[1] + anchor_offset[1]
   x_centers, y_centers = ops.meshgrid(x_centers, y_centers)
 
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index 015f6ca1..86007c99 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -180,22 +180,23 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
                 for list_item in feature_map_shape_list]):
       raise ValueError('feature_map_shape_list must be a list of pairs.')
 
-    im_height = tf.to_float(im_height)
-    im_width = tf.to_float(im_width)
+    im_height = tf.cast(im_height, dtype=tf.float32)
+    im_width = tf.cast(im_width, dtype=tf.float32)
 
     if not self._anchor_strides:
-      anchor_strides = [(1.0 / tf.to_float(pair[0]), 1.0 / tf.to_float(pair[1]))
+      anchor_strides = [(1.0 / tf.cast(pair[0], dtype=tf.float32),
+                         1.0 / tf.cast(pair[1], dtype=tf.float32))
                         for pair in feature_map_shape_list]
     else:
-      anchor_strides = [(tf.to_float(stride[0]) / im_height,
-                         tf.to_float(stride[1]) / im_width)
+      anchor_strides = [(tf.cast(stride[0], dtype=tf.float32) / im_height,
+                         tf.cast(stride[1], dtype=tf.float32) / im_width)
                         for stride in self._anchor_strides]
     if not self._anchor_offsets:
       anchor_offsets = [(0.5 * stride[0], 0.5 * stride[1])
                         for stride in anchor_strides]
     else:
-      anchor_offsets = [(tf.to_float(offset[0]) / im_height,
-                         tf.to_float(offset[1]) / im_width)
+      anchor_offsets = [(tf.cast(offset[0], dtype=tf.float32) / im_height,
+                         tf.cast(offset[1], dtype=tf.float32) / im_width)
                         for offset in self._anchor_offsets]
 
     for arg, arg_name in zip([anchor_strides, anchor_offsets],
diff --git a/research/object_detection/box_coders/keypoint_box_coder.py b/research/object_detection/box_coders/keypoint_box_coder.py
index 67df3b82..fabcc5a8 100644
--- a/research/object_detection/box_coders/keypoint_box_coder.py
+++ b/research/object_detection/box_coders/keypoint_box_coder.py
@@ -66,9 +66,11 @@ class KeypointBoxCoder(box_coder.BoxCoder):
     self._scale_factors = scale_factors
     self._keypoint_scale_factors = None
     if scale_factors is not None:
-      self._keypoint_scale_factors = tf.expand_dims(tf.tile(
-          [tf.to_float(scale_factors[0]), tf.to_float(scale_factors[1])],
-          [num_keypoints]), 1)
+      self._keypoint_scale_factors = tf.expand_dims(
+          tf.tile([
+              tf.cast(scale_factors[0], dtype=tf.float32),
+              tf.cast(scale_factors[1], dtype=tf.float32)
+          ], [num_keypoints]), 1)
 
   @property
   def code_size(self):
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index f2d434ae..dcf7bf13 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -27,8 +27,9 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     text_format.Merge(text_proto, image_resizer_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-    images = tf.to_float(
-        tf.random_uniform(input_shape, minval=0, maxval=255, dtype=tf.int32))
+    images = tf.cast(
+        tf.random_uniform(input_shape, minval=0, maxval=255, dtype=tf.int32),
+        dtype=tf.float32)
     resized_images, _ = image_resizer_fn(images)
     with self.test_session() as sess:
       return sess.run(resized_images).shape
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index 8049001f..0e1cbba7 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -21,11 +21,13 @@ import tensorflow as tf
 from object_detection.utils import learning_schedules
 
 
-def build(optimizer_config):
+def build(optimizer_config, global_step=None):
   """Create optimizer based on config.
 
   Args:
     optimizer_config: A Optimizer proto message.
+    global_step: A variable representing the current step.
+      If None, defaults to tf.train.get_or_create_global_step()
 
   Returns:
     An optimizer and a list of variables for summary.
@@ -39,7 +41,8 @@ def build(optimizer_config):
   summary_vars = []
   if optimizer_type == 'rms_prop_optimizer':
     config = optimizer_config.rms_prop_optimizer
-    learning_rate = _create_learning_rate(config.learning_rate)
+    learning_rate = _create_learning_rate(config.learning_rate,
+                                          global_step=global_step)
     summary_vars.append(learning_rate)
     optimizer = tf.train.RMSPropOptimizer(
         learning_rate,
@@ -49,7 +52,8 @@ def build(optimizer_config):
 
   if optimizer_type == 'momentum_optimizer':
     config = optimizer_config.momentum_optimizer
-    learning_rate = _create_learning_rate(config.learning_rate)
+    learning_rate = _create_learning_rate(config.learning_rate,
+                                          global_step=global_step)
     summary_vars.append(learning_rate)
     optimizer = tf.train.MomentumOptimizer(
         learning_rate,
@@ -57,7 +61,8 @@ def build(optimizer_config):
 
   if optimizer_type == 'adam_optimizer':
     config = optimizer_config.adam_optimizer
-    learning_rate = _create_learning_rate(config.learning_rate)
+    learning_rate = _create_learning_rate(config.learning_rate,
+                                          global_step=global_step)
     summary_vars.append(learning_rate)
     optimizer = tf.train.AdamOptimizer(learning_rate)
 
@@ -72,11 +77,13 @@ def build(optimizer_config):
   return optimizer, summary_vars
 
 
-def _create_learning_rate(learning_rate_config):
+def _create_learning_rate(learning_rate_config, global_step=None):
   """Create optimizer learning rate based on config.
 
   Args:
     learning_rate_config: A LearningRate proto message.
+    global_step: A variable representing the current step.
+      If None, defaults to tf.train.get_or_create_global_step()
 
   Returns:
     A learning rate.
@@ -84,6 +91,8 @@ def _create_learning_rate(learning_rate_config):
   Raises:
     ValueError: when using an unsupported input data type.
   """
+  if global_step is None:
+    global_step = tf.train.get_or_create_global_step()
   learning_rate = None
   learning_rate_type = learning_rate_config.WhichOneof('learning_rate')
   if learning_rate_type == 'constant_learning_rate':
@@ -94,7 +103,7 @@ def _create_learning_rate(learning_rate_config):
   if learning_rate_type == 'exponential_decay_learning_rate':
     config = learning_rate_config.exponential_decay_learning_rate
     learning_rate = learning_schedules.exponential_decay_with_burnin(
-        tf.train.get_or_create_global_step(),
+        global_step,
         config.initial_learning_rate,
         config.decay_steps,
         config.decay_factor,
@@ -111,13 +120,13 @@ def _create_learning_rate(learning_rate_config):
     learning_rate_sequence = [config.initial_learning_rate]
     learning_rate_sequence += [x.learning_rate for x in config.schedule]
     learning_rate = learning_schedules.manual_stepping(
-        tf.train.get_or_create_global_step(), learning_rate_step_boundaries,
+        global_step, learning_rate_step_boundaries,
         learning_rate_sequence, config.warmup)
 
   if learning_rate_type == 'cosine_decay_learning_rate':
     config = learning_rate_config.cosine_decay_learning_rate
     learning_rate = learning_schedules.cosine_decay_with_warmup(
-        tf.train.get_or_create_global_step(),
+        global_step,
         config.learning_rate_base,
         config.total_steps,
         config.warmup_learning_rate,
diff --git a/research/object_detection/builders/post_processing_builder.py b/research/object_detection/builders/post_processing_builder.py
index a77165b4..f75d8bd2 100644
--- a/research/object_detection/builders/post_processing_builder.py
+++ b/research/object_detection/builders/post_processing_builder.py
@@ -85,14 +85,15 @@ def _build_non_max_suppressor(nms_config):
   if nms_config.max_detections_per_class > nms_config.max_total_detections:
     raise ValueError('max_detections_per_class should be no greater than '
                      'max_total_detections.')
-
   non_max_suppressor_fn = functools.partial(
       post_processing.batch_multiclass_non_max_suppression,
       score_thresh=nms_config.score_threshold,
       iou_thresh=nms_config.iou_threshold,
       max_size_per_class=nms_config.max_detections_per_class,
       max_total_size=nms_config.max_total_detections,
-      use_static_shapes=nms_config.use_static_shapes)
+      use_static_shapes=nms_config.use_static_shapes,
+      use_class_agnostic_nms=nms_config.use_class_agnostic_nms,
+      max_classes_per_detection=nms_config.max_classes_per_detection)
   return non_max_suppressor_fn
 
 
diff --git a/research/object_detection/builders/post_processing_builder_test.py b/research/object_detection/builders/post_processing_builder_test.py
index e49303ec..2867032f 100644
--- a/research/object_detection/builders/post_processing_builder_test.py
+++ b/research/object_detection/builders/post_processing_builder_test.py
@@ -41,6 +41,31 @@ class PostProcessingBuilderTest(tf.test.TestCase):
     self.assertAlmostEqual(non_max_suppressor.keywords['score_thresh'], 0.7)
     self.assertAlmostEqual(non_max_suppressor.keywords['iou_thresh'], 0.6)
 
+  def test_build_non_max_suppressor_with_correct_parameters_classagnostic_nms(
+      self):
+    post_processing_text_proto = """
+      batch_non_max_suppression {
+        score_threshold: 0.7
+        iou_threshold: 0.6
+        max_detections_per_class: 10
+        max_total_detections: 300
+        use_class_agnostic_nms: True
+        max_classes_per_detection: 1
+      }
+    """
+    post_processing_config = post_processing_pb2.PostProcessing()
+    text_format.Merge(post_processing_text_proto, post_processing_config)
+    non_max_suppressor, _ = post_processing_builder.build(
+        post_processing_config)
+    self.assertEqual(non_max_suppressor.keywords['max_size_per_class'], 10)
+    self.assertEqual(non_max_suppressor.keywords['max_total_size'], 300)
+    self.assertEqual(non_max_suppressor.keywords['max_classes_per_detection'],
+                     1)
+    self.assertEqual(non_max_suppressor.keywords['use_class_agnostic_nms'],
+                     True)
+    self.assertAlmostEqual(non_max_suppressor.keywords['score_thresh'], 0.7)
+    self.assertAlmostEqual(non_max_suppressor.keywords['iou_thresh'], 0.6)
+
   def test_build_identity_score_converter(self):
     post_processing_text_proto = """
       score_converter: IDENTITY
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index 633205e3..b7f72c44 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -39,7 +39,7 @@ def _get_step_config_from_proto(preprocessor_step_config, step_name):
     if field.name == step_name:
       return value
 
-  raise ValueError('Could not get field %s from proto!', step_name)
+  raise ValueError('Could not get field %s from proto!' % step_name)
 
 
 def _get_dict_from_proto(config):
@@ -194,7 +194,7 @@ def build(preprocessor_step_config):
       if len(pad_color) != 3:
         tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
 
-      pad_color = tf.to_float([x for x in config.pad_color])
+      pad_color = tf.cast([x for x in config.pad_color], dtype=tf.float32)
     return (preprocessor.random_pad_image,
             {
                 'min_image_size': min_image_size,
@@ -213,7 +213,7 @@ def build(preprocessor_step_config):
       if len(pad_color) != 3:
         tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
 
-      pad_color = tf.to_float([x for x in config.pad_color])
+      pad_color = tf.cast([x for x in config.pad_color], dtype=tf.float32)
 
     return (preprocessor.random_absolute_pad_image,
             {
@@ -234,7 +234,7 @@ def build(preprocessor_step_config):
       if len(pad_color) != 3:
         tf.logging.warn('pad_color should have 3 elements (RGB) if set!')
 
-      pad_color = tf.to_float([x for x in config.pad_color])
+      pad_color = tf.cast([x for x in config.pad_color], dtype=tf.float32)
 
     kwargs = {
         'min_object_covered': config.min_object_covered,
diff --git a/research/object_detection/core/balanced_positive_negative_sampler.py b/research/object_detection/core/balanced_positive_negative_sampler.py
index a38f82f6..89c1fc7f 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler.py
@@ -247,7 +247,7 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
 
         # Sample positive and negative samples separately
         if batch_size is None:
-          max_num_pos = tf.reduce_sum(tf.to_int32(positive_idx))
+          max_num_pos = tf.reduce_sum(tf.cast(positive_idx, dtype=tf.int32))
         else:
           max_num_pos = int(self._positive_fraction * batch_size)
         sampled_pos_idx = self.subsample_indicator(positive_idx, max_num_pos)
@@ -255,8 +255,10 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
         if batch_size is None:
           negative_positive_ratio = (
               1 - self._positive_fraction) / self._positive_fraction
-          max_num_neg = tf.to_int32(
-              negative_positive_ratio * tf.to_float(num_sampled_pos))
+          max_num_neg = tf.cast(
+              negative_positive_ratio *
+              tf.cast(num_sampled_pos, dtype=tf.float32),
+              dtype=tf.int32)
         else:
           max_num_neg = batch_size - num_sampled_pos
         sampled_neg_idx = self.subsample_indicator(negative_idx, max_num_neg)
diff --git a/research/object_detection/core/post_processing_test.py b/research/object_detection/core/batch_multiclass_nms_test.py
similarity index 59%
rename from research/object_detection/core/post_processing_test.py
rename to research/object_detection/core/batch_multiclass_nms_test.py
index f23886bb..a87fec0a 100644
--- a/research/object_detection/core/post_processing_test.py
+++ b/research/object_detection/core/batch_multiclass_nms_test.py
@@ -1,4 +1,4 @@
-# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
@@ -12,513 +12,14 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
-"""Tests for tensorflow_models.object_detection.core.post_processing."""
+"""Tests for google3.third_party.tensorflow_models.object_detection.core.batch_multiclass_nms."""
 import numpy as np
 import tensorflow as tf
 from object_detection.core import post_processing
-from object_detection.core import standard_fields as fields
 from object_detection.utils import test_case
 
 
-class MulticlassNonMaxSuppressionTest(test_case.TestCase):
-
-  def test_multiclass_nms_select_with_shared_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002],
-                       [0, 100, 1, 101]]
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-
-  def test_multiclass_nms_select_with_shared_boxes_pad_to_max_output_size(self):
-    boxes = np.array([[[0, 0, 1, 1]],
-                      [[0, 0.1, 1, 1.1]],
-                      [[0, -0.1, 1, 0.9]],
-                      [[0, 10, 1, 11]],
-                      [[0, 10.1, 1, 11.1]],
-                      [[0, 100, 1, 101]],
-                      [[0, 1000, 1, 1002]],
-                      [[0, 1000, 1, 1002.1]]], np.float32)
-    scores = np.array([[.9, 0.01], [.75, 0.05],
-                       [.6, 0.01], [.95, 0],
-                       [.5, 0.01], [.3, 0.01],
-                       [.01, .85], [.01, .5]], np.float32)
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_size_per_class = 4
-    max_output_size = 5
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002],
-                       [0, 100, 1, 101]]
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-
-    def graph_fn(boxes, scores):
-      nms, num_valid_nms_boxes = post_processing.multiclass_non_max_suppression(
-          boxes,
-          scores,
-          score_thresh,
-          iou_thresh,
-          max_size_per_class,
-          max_total_size=max_output_size,
-          pad_to_max_output_size=True)
-      return [nms.get(), nms.get_field(fields.BoxListFields.scores),
-              nms.get_field(fields.BoxListFields.classes), num_valid_nms_boxes]
-
-    [nms_corners_output, nms_scores_output, nms_classes_output,
-     num_valid_nms_boxes] = self.execute(graph_fn, [boxes, scores])
-
-    self.assertEqual(num_valid_nms_boxes, 4)
-    self.assertAllClose(nms_corners_output[0:num_valid_nms_boxes],
-                        exp_nms_corners)
-    self.assertAllClose(nms_scores_output[0:num_valid_nms_boxes],
-                        exp_nms_scores)
-    self.assertAllClose(nms_classes_output[0:num_valid_nms_boxes],
-                        exp_nms_classes)
-
-  def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    num_keypoints = 6
-    keypoints = tf.tile(
-        tf.reshape(tf.range(8), [8, 1, 1]),
-        [1, num_keypoints, 2])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002],
-                       [0, 100, 1, 101]]
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-    exp_nms_keypoints_tensor = tf.tile(
-        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
-        [1, num_keypoints, 2])
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={fields.BoxListFields.keypoints: keypoints})
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_keypoints,
-       exp_nms_keypoints) = sess.run([
-           nms.get(),
-           nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes),
-           nms.get_field(fields.BoxListFields.keypoints),
-           exp_nms_keypoints_tensor
-       ])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_keypoints, exp_nms_keypoints)
-
-  def test_multiclass_nms_with_shared_boxes_given_keypoint_heatmaps(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-
-    num_boxes = tf.shape(boxes)[0]
-    heatmap_height = 5
-    heatmap_width = 5
-    num_keypoints = 17
-    keypoint_heatmaps = tf.ones(
-        [num_boxes, heatmap_height, heatmap_width, num_keypoints],
-        dtype=tf.float32)
-
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002],
-                       [0, 100, 1, 101]]
-
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-    exp_nms_keypoint_heatmaps = np.ones(
-        (4, heatmap_height, heatmap_width, num_keypoints), dtype=np.float32)
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={
-            fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps
-        })
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_keypoint_heatmaps) = sess.run(
-           [nms.get(),
-            nms.get_field(fields.BoxListFields.scores),
-            nms.get_field(fields.BoxListFields.classes),
-            nms.get_field(fields.BoxListFields.keypoint_heatmaps)])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_keypoint_heatmaps, exp_nms_keypoint_heatmaps)
-
-  def test_multiclass_nms_with_additional_fields(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-
-    coarse_boxes_key = 'coarse_boxes'
-    coarse_boxes = tf.constant([[0.1, 0.1, 1.1, 1.1],
-                                [0.1, 0.2, 1.1, 1.2],
-                                [0.1, -0.2, 1.1, 1.0],
-                                [0.1, 10.1, 1.1, 11.1],
-                                [0.1, 10.2, 1.1, 11.2],
-                                [0.1, 100.1, 1.1, 101.1],
-                                [0.1, 1000.1, 1.1, 1002.1],
-                                [0.1, 1000.1, 1.1, 1002.2]], tf.float32)
-
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-
-    exp_nms_corners = np.array([[0, 10, 1, 11],
-                                [0, 0, 1, 1],
-                                [0, 1000, 1, 1002],
-                                [0, 100, 1, 101]], dtype=np.float32)
-
-    exp_nms_coarse_corners = np.array([[0.1, 10.1, 1.1, 11.1],
-                                       [0.1, 0.1, 1.1, 1.1],
-                                       [0.1, 1000.1, 1.1, 1002.1],
-                                       [0.1, 100.1, 1.1, 101.1]],
-                                      dtype=np.float32)
-
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        additional_fields={coarse_boxes_key: coarse_boxes})
-
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_coarse_corners) = sess.run(
-           [nms.get(),
-            nms.get_field(fields.BoxListFields.scores),
-            nms.get_field(fields.BoxListFields.classes),
-            nms.get_field(coarse_boxes_key)])
-
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_coarse_corners, exp_nms_coarse_corners)
-
-  def test_multiclass_nms_select_with_shared_boxes_given_masks(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    num_classes = 2
-    mask_height = 3
-    mask_width = 3
-    masks = tf.tile(
-        tf.reshape(tf.range(8), [8, 1, 1, 1]),
-        [1, num_classes, mask_height, mask_width])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002],
-                       [0, 100, 1, 101]]
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-    exp_nms_masks_tensor = tf.tile(
-        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
-        [1, mask_height, mask_width])
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size, masks=masks)
-    with self.test_session() as sess:
-      (nms_corners_output,
-       nms_scores_output,
-       nms_classes_output,
-       nms_masks,
-       exp_nms_masks) = sess.run([nms.get(),
-                                  nms.get_field(fields.BoxListFields.scores),
-                                  nms.get_field(fields.BoxListFields.classes),
-                                  nms.get_field(fields.BoxListFields.masks),
-                                  exp_nms_masks_tensor])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-      self.assertAllEqual(nms_masks, exp_nms_masks)
-
-  def test_multiclass_nms_select_with_clip_window(self):
-    boxes = tf.constant([[[0, 0, 10, 10]],
-                         [[1, 1, 11, 11]]], tf.float32)
-    scores = tf.constant([[.9], [.75]])
-    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
-    score_thresh = 0.0
-    iou_thresh = 0.5
-    max_output_size = 100
-
-    exp_nms_corners = [[5, 4, 8, 7]]
-    exp_nms_scores = [.9]
-    exp_nms_classes = [0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        clip_window=clip_window)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-
-  def test_multiclass_nms_select_with_clip_window_change_coordinate_frame(self):
-    boxes = tf.constant([[[0, 0, 10, 10]],
-                         [[1, 1, 11, 11]]], tf.float32)
-    scores = tf.constant([[.9], [.75]])
-    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
-    score_thresh = 0.0
-    iou_thresh = 0.5
-    max_output_size = 100
-
-    exp_nms_corners = [[0, 0, 1, 1]]
-    exp_nms_scores = [.9]
-    exp_nms_classes = [0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes,
-        scores,
-        score_thresh,
-        iou_thresh,
-        max_output_size,
-        clip_window=clip_window,
-        change_coordinate_frame=True)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-
-  def test_multiclass_nms_select_with_per_class_cap(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_size_per_class = 2
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 1000, 1, 1002]]
-    exp_nms_scores = [.95, .9, .85]
-    exp_nms_classes = [0, 0, 1]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_size_per_class)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-
-  def test_multiclass_nms_select_with_total_cap(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_size_per_class = 4
-    max_total_size = 2
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1]]
-    exp_nms_scores = [.95, .9]
-    exp_nms_classes = [0, 0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_size_per_class,
-        max_total_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
-
-  def test_multiclass_nms_threshold_then_select_with_shared_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1]],
-                         [[0, 0.1, 1, 1.1]],
-                         [[0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002]],
-                         [[0, 1000, 1, 1002.1]]], tf.float32)
-    scores = tf.constant([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 3
-
-    exp_nms = [[0, 10, 1, 11],
-               [0, 0, 1, 1],
-               [0, 100, 1, 101]]
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_output = sess.run(nms.get())
-      self.assertAllClose(nms_output, exp_nms)
-
-  def test_multiclass_nms_select_with_separate_boxes(self):
-    boxes = tf.constant([[[0, 0, 1, 1], [0, 0, 4, 5]],
-                         [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
-                         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
-                         [[0, 10, 1, 11], [0, 10, 1, 11]],
-                         [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
-                         [[0, 100, 1, 101], [0, 100, 1, 101]],
-                         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
-                         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]],
-                        tf.float32)
-    scores = tf.constant([[.9, 0.01], [.75, 0.05],
-                          [.6, 0.01], [.95, 0],
-                          [.5, 0.01], [.3, 0.01],
-                          [.01, .85], [.01, .5]])
-    score_thresh = 0.1
-    iou_thresh = .5
-    max_output_size = 4
-
-    exp_nms_corners = [[0, 10, 1, 11],
-                       [0, 0, 1, 1],
-                       [0, 999, 2, 1004],
-                       [0, 100, 1, 101]]
-    exp_nms_scores = [.95, .9, .85, .3]
-    exp_nms_classes = [0, 0, 1, 0]
-
-    nms, _ = post_processing.multiclass_non_max_suppression(
-        boxes, scores, score_thresh, iou_thresh, max_output_size)
-    with self.test_session() as sess:
-      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
-          [nms.get(), nms.get_field(fields.BoxListFields.scores),
-           nms.get_field(fields.BoxListFields.classes)])
-      self.assertAllClose(nms_corners_output, exp_nms_corners)
-      self.assertAllClose(nms_scores_output, exp_nms_scores)
-      self.assertAllClose(nms_classes_output, exp_nms_classes)
+class BatchMulticlassNonMaxSuppressionTest(test_case.TestCase):
 
   def test_batch_multiclass_nms_with_batch_size_1(self):
     boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],
diff --git a/research/object_detection/core/box_coder.py b/research/object_detection/core/box_coder.py
index f20ac956..d97bb856 100644
--- a/research/object_detection/core/box_coder.py
+++ b/research/object_detection/core/box_coder.py
@@ -32,6 +32,8 @@ from abc import abstractproperty
 
 import tensorflow as tf
 
+from object_detection.utils import shape_utils
+
 
 # Box coder types.
 FASTER_RCNN = 'faster_rcnn'
@@ -137,11 +139,12 @@ def batch_decode(encoded_boxes, box_coder, anchors):
     inconsistent.
   """
   encoded_boxes.get_shape().assert_has_rank(3)
-  if encoded_boxes.get_shape()[1].value != anchors.num_boxes_static():
+  if (shape_utils.get_dim_as_int(encoded_boxes.get_shape()[1])
+      != anchors.num_boxes_static()):
     raise ValueError('The number of anchors inferred from encoded_boxes'
                      ' and anchors are inconsistent: shape[1] of encoded_boxes'
                      ' %s should be equal to the number of anchors: %s.' %
-                     (encoded_boxes.get_shape()[1].value,
+                     (shape_utils.get_dim_as_int(encoded_boxes.get_shape()[1]),
                       anchors.num_boxes_static()))
 
   decoded_boxes = tf.stack([
diff --git a/research/object_detection/core/box_list.py b/research/object_detection/core/box_list.py
index c0196f05..6d3ddb0f 100644
--- a/research/object_detection/core/box_list.py
+++ b/research/object_detection/core/box_list.py
@@ -36,6 +36,8 @@ Some other notes:
 
 import tensorflow as tf
 
+from object_detection.utils import shape_utils
+
 
 class BoxList(object):
   """Box collection."""
@@ -73,7 +75,7 @@ class BoxList(object):
       Number of boxes held in collection (integer) or None if this is not
         inferrable at graph construction time.
     """
-    return self.data['boxes'].get_shape()[0].value
+    return shape_utils.get_dim_as_int(self.data['boxes'].get_shape()[0])
 
   def get_all_fields(self):
     """Returns all fields."""
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index 7c6d75c8..a1f2ef03 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -339,7 +339,7 @@ def prune_non_overlapping_boxes(
     ioa_ = ioa(boxlist2, boxlist1)  # [M, N] tensor
     ioa_ = tf.reduce_max(ioa_, reduction_indices=[0])  # [N] tensor
     keep_bool = tf.greater_equal(ioa_, tf.constant(min_overlap))
-    keep_inds = tf.squeeze(tf.where(keep_bool), squeeze_dims=[1])
+    keep_inds = tf.squeeze(tf.where(keep_bool), axis=[1])
     new_boxlist1 = gather(boxlist1, keep_inds)
     return new_boxlist1, keep_inds
 
@@ -457,7 +457,7 @@ def boolean_mask(boxlist, indicator, fields=None, scope=None,
     if use_static_shapes:
       if not (indicator_sum and isinstance(indicator_sum, int)):
         raise ValueError('`indicator_sum` must be a of type int')
-      selected_positions = tf.to_float(indicator)
+      selected_positions = tf.cast(indicator, dtype=tf.float32)
       indexed_positions = tf.cast(
           tf.multiply(
               tf.cumsum(selected_positions), selected_positions),
@@ -466,7 +466,7 @@ def boolean_mask(boxlist, indicator, fields=None, scope=None,
           indexed_positions - 1, indicator_sum, dtype=tf.float32)
       sampled_indices = tf.cast(
           tf.tensordot(
-              tf.to_float(tf.range(tf.shape(indicator)[0])),
+              tf.cast(tf.range(tf.shape(indicator)[0]), dtype=tf.float32),
               one_hot_selector,
               axes=[0, 0]),
           dtype=tf.int32)
@@ -962,7 +962,7 @@ def box_voting(selected_boxes, pool_boxes, iou_thresh=0.5):
     raise ValueError('pool_boxes must have a \'scores\' field')
 
   iou_ = iou(selected_boxes, pool_boxes)
-  match_indicator = tf.to_float(tf.greater(iou_, iou_thresh))
+  match_indicator = tf.cast(tf.greater(iou_, iou_thresh), dtype=tf.float32)
   num_matches = tf.reduce_sum(match_indicator, 1)
   # TODO(kbanoop): Handle the case where some boxes in selected_boxes do not
   # match to any boxes in pool_boxes. For such boxes without any matches, we
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index 727c198b..efe29913 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -581,8 +581,8 @@ class BoxListOpsTest(test_case.TestCase):
                            [0, 0, 3, 2]], tf.float32)
     boxes = box_list.BoxList(corners)
     image_and_boxes = box_list_ops.visualize_boxes_in_image(image, boxes)
-    image_and_boxes_bw = tf.to_float(
-        tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0))
+    image_and_boxes_bw = tf.cast(
+        tf.greater(tf.reduce_sum(image_and_boxes, 2), 0.0), dtype=tf.float32)
     exp_result = [[1, 1, 1, 0],
                   [1, 1, 1, 0],
                   [1, 1, 1, 0],
diff --git a/research/object_detection/core/class_agnostic_nms_test.py b/research/object_detection/core/class_agnostic_nms_test.py
new file mode 100644
index 00000000..add3865f
--- /dev/null
+++ b/research/object_detection/core/class_agnostic_nms_test.py
@@ -0,0 +1,144 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for google3.third_party.tensorflow_models.object_detection.core.class_agnostic_nms."""
+import tensorflow as tf
+from object_detection.core import post_processing
+from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
+
+
+class ClassAgnosticNonMaxSuppressionTest(test_case.TestCase):
+
+  def test_class_agnostic_nms_select_with_shared_boxes(self):
+    boxes = tf.constant(
+        [[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+         [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+         [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_classes_per_detection = 1
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11], [0, 0, 1, 1], [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    nms, _ = post_processing.class_agnostic_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
+        max_output_size)
+
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run([
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes)
+      ])
+
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_class_agnostic_nms_select_with_per_class_boxes(self):
+    boxes = tf.constant(
+        [[[4, 5, 9, 10], [0, 0, 1, 1]],
+         [[0, 0.1, 1, 1.1], [4, 5, 9, 10]],
+         [[0, -0.1, 1, 0.9], [4, 5, 9, 10]],
+         [[0, 10, 1, 11], [4, 5, 9, 10]],
+         [[0, 10.1, 1, 11.1], [4, 5, 9, 10]],
+         [[0, 100, 1, 101], [4, 5, 9, 10]],
+         [[4, 5, 9, 10], [0, 1000, 1, 1002]],
+         [[4, 5, 9, 10], [0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.01, 0.9],
+                          [.75, 0.05],
+                          [.6, 0.01],
+                          [.95, 0],
+                          [.5, 0.01],
+                          [.3, 0.01],
+                          [.01, .85],
+                          [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_classes_per_detection = 1
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 1, 1, 0]
+
+    nms, _ = post_processing.class_agnostic_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_classes_per_detection,
+        max_output_size)
+
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run([
+          nms.get(),
+          nms.get_field(fields.BoxListFields.scores),
+          nms.get_field(fields.BoxListFields.classes)
+      ])
+
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_batch_classagnostic_nms_with_batch_size_1(self):
+    boxes = tf.constant(
+        [[[[0, 0, 1, 1]], [[0, 0.1, 1, 1.1]], [[0, -0.1, 1, 0.9]],
+          [[0, 10, 1, 11]], [[0, 10.1, 1, 11.1]], [[0, 100, 1, 101]],
+          [[0, 1000, 1, 1002]], [[0, 1000, 1, 1002.1]]]], tf.float32)
+    scores = tf.constant([[[.9, 0.01], [.75, 0.05], [.6, 0.01], [.95, 0],
+                           [.5, 0.01], [.3, 0.01], [.01, .85], [.01, .5]]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+    max_classes_per_detection = 1
+    use_class_agnostic_nms = True
+
+    exp_nms_corners = [[[0, 10, 1, 11], [0, 0, 1, 1], [0, 1000, 1, 1002],
+                        [0, 100, 1, 101]]]
+    exp_nms_scores = [[.95, .9, .85, .3]]
+    exp_nms_classes = [[0, 0, 1, 0]]
+
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+     nmsed_additional_fields,
+     num_detections) = post_processing.batch_multiclass_non_max_suppression(
+         boxes,
+         scores,
+         score_thresh,
+         iou_thresh,
+         max_size_per_class=max_output_size,
+         max_total_size=max_output_size,
+         use_class_agnostic_nms=use_class_agnostic_nms,
+         max_classes_per_detection=max_classes_per_detection)
+
+    self.assertIsNone(nmsed_masks)
+    self.assertIsNone(nmsed_additional_fields)
+
+    with self.test_session() as sess:
+      (nmsed_boxes, nmsed_scores, nmsed_classes, num_detections) = sess.run(
+          [nmsed_boxes, nmsed_scores, nmsed_classes, num_detections])
+      self.assertAllClose(nmsed_boxes, exp_nms_corners)
+      self.assertAllClose(nmsed_scores, exp_nms_scores)
+      self.assertAllClose(nmsed_classes, exp_nms_classes)
+      self.assertEqual(num_detections, [4])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/core/freezable_batch_norm.py b/research/object_detection/core/freezable_batch_norm.py
index 68a56aa6..be82fcd2 100644
--- a/research/object_detection/core/freezable_batch_norm.py
+++ b/research/object_detection/core/freezable_batch_norm.py
@@ -36,12 +36,9 @@ class FreezableBatchNorm(tf.keras.layers.BatchNormalization):
   close to 0 and the activation standard deviation close to 1.
 
   Arguments:
-    training: Boolean or None. If True, the batch normalization layer will
-      normalize the input batch using the batch mean and standard deviation,
-      and update the total moving mean and standard deviations. If False, the
-      layer will normalize using the moving average and std. dev, without
-      updating the learned avg and std. dev.
-      If None, the layer will follow the keras BatchNormalization layer
+    training: If False, the layer will normalize using the moving average and
+      std. dev, without updating the learned avg and std. dev.
+      If None or True, the layer will follow the keras BatchNormalization layer
       strategy of checking the Keras learning phase at `call` time to decide
       what to do.
     **kwargs: The keyword arguments to forward to the keras BatchNormalization
@@ -65,6 +62,7 @@ class FreezableBatchNorm(tf.keras.layers.BatchNormalization):
     self._training = training
 
   def call(self, inputs, training=None):
-    if training is None:
+    # Override the call arg only if the batchnorm is frozen. (Ignore None)
+    if self._training is False:  # pylint: disable=g-bool-id-comparison
       training = self._training
     return super(FreezableBatchNorm, self).call(inputs, training=training)
diff --git a/research/object_detection/core/freezable_batch_norm_test.py b/research/object_detection/core/freezable_batch_norm_test.py
index 504b9e71..1c192652 100644
--- a/research/object_detection/core/freezable_batch_norm_test.py
+++ b/research/object_detection/core/freezable_batch_norm_test.py
@@ -43,7 +43,24 @@ class FreezableBatchNormTest(tf.test.TestCase):
     model.fit(train_data, train_data, epochs=4, verbose=0)
     return model.weights
 
-  def test_batchnorm_freezing_training_true(self):
+  def _test_batchnorm_layer(
+      self, norm, should_be_training, test_data,
+      testing_mean, testing_var, training_arg, training_mean, training_var):
+    out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32),
+                      training=training_arg)
+    out = tf.keras.backend.eval(out_tensor)
+    out -= tf.keras.backend.eval(norm.beta)
+    out /= tf.keras.backend.eval(norm.gamma)
+
+    if not should_be_training:
+      out *= training_var
+      out += (training_mean - testing_mean)
+      out /= testing_var
+
+    np.testing.assert_allclose(out.mean(), 0.0, atol=1.5e-1)
+    np.testing.assert_allclose(out.std(), 1.0, atol=1.5e-1)
+
+  def test_batchnorm_freezing_training_none(self):
     with self.test_session():
       training_mean = 5.0
       training_var = 10.0
@@ -69,14 +86,38 @@ class FreezableBatchNormTest(tf.test.TestCase):
           scale=testing_var,
           size=(1000, 10))
 
-      out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32))
-      out = tf.keras.backend.eval(out_tensor)
-
-      out -= tf.keras.backend.eval(norm.beta)
-      out /= tf.keras.backend.eval(norm.gamma)
-
-      np.testing.assert_allclose(out.mean(), 0.0, atol=1.5e-1)
-      np.testing.assert_allclose(out.std(), 1.0, atol=1.5e-1)
+      # Test with training=True passed to the call method:
+      training_arg = True
+      should_be_training = True
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      # Test with training=False passed to the call method:
+      training_arg = False
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      # Test the layer in various Keras learning phase scopes:
+      training_arg = None
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      tf.keras.backend.set_learning_phase(True)
+      should_be_training = True
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      tf.keras.backend.set_learning_phase(False)
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
 
   def test_batchnorm_freezing_training_false(self):
     with self.test_session():
@@ -104,18 +145,40 @@ class FreezableBatchNormTest(tf.test.TestCase):
           scale=testing_var,
           size=(1000, 10))
 
-      out_tensor = norm(tf.convert_to_tensor(test_data, dtype=tf.float32))
-      out = tf.keras.backend.eval(out_tensor)
-
-      out -= tf.keras.backend.eval(norm.beta)
-      out /= tf.keras.backend.eval(norm.gamma)
-
-      out *= training_var
-      out += (training_mean - testing_mean)
-      out /= testing_var
+      # Make sure that the layer is never training
+      # Test with training=True passed to the call method:
+      training_arg = True
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      # Test with training=False passed to the call method:
+      training_arg = False
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      # Test the layer in various Keras learning phase scopes:
+      training_arg = None
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      tf.keras.backend.set_learning_phase(True)
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
+
+      tf.keras.backend.set_learning_phase(False)
+      should_be_training = False
+      self._test_batchnorm_layer(norm, should_be_training, test_data,
+                                 testing_mean, testing_var, training_arg,
+                                 training_mean, training_var)
 
-      np.testing.assert_allclose(out.mean(), 0.0, atol=1.5e-1)
-      np.testing.assert_allclose(out.std(), 1.0, atol=1.5e-1)
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index b7d81fcb..e98af6ee 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -26,9 +26,7 @@ Classification losses:
  * WeightedSoftmaxClassificationAgainstLogitsLoss
  * BootstrappedSigmoidClassificationLoss
 """
-from abc import ABCMeta
-from abc import abstractmethod
-
+import abc
 import tensorflow as tf
 
 from object_detection.core import box_list
@@ -40,7 +38,7 @@ slim = tf.contrib.slim
 
 class Loss(object):
   """Abstract base class for loss functions."""
-  __metaclass__ = ABCMeta
+  __metaclass__ = abc.ABCMeta
 
   def __call__(self,
                prediction_tensor,
@@ -96,7 +94,7 @@ class Loss(object):
     loss_multiplier_shape = tf.stack([-1] + [1] * (len(tensor.shape) - 1))
     return tf.cast(tf.reshape(losses_mask, loss_multiplier_shape), tf.float32)
 
-  @abstractmethod
+  @abc.abstractmethod
   def _compute_loss(self, prediction_tensor, target_tensor, **params):
     """Method to be overridden by implementations.
 
@@ -616,8 +614,10 @@ class HardExampleMiner(object):
   def summarize(self):
     """Summarize the number of positives and negatives after mining."""
     if self._num_positives_list and self._num_negatives_list:
-      avg_num_positives = tf.reduce_mean(tf.to_float(self._num_positives_list))
-      avg_num_negatives = tf.reduce_mean(tf.to_float(self._num_negatives_list))
+      avg_num_positives = tf.reduce_mean(
+          tf.cast(self._num_positives_list, dtype=tf.float32))
+      avg_num_negatives = tf.reduce_mean(
+          tf.cast(self._num_negatives_list, dtype=tf.float32))
       tf.summary.scalar('HardExampleMiner/NumPositives', avg_num_positives)
       tf.summary.scalar('HardExampleMiner/NumNegatives', avg_num_negatives)
 
@@ -661,12 +661,13 @@ class HardExampleMiner(object):
     """
     positives_indicator = tf.gather(match.matched_column_indicator(), indices)
     negatives_indicator = tf.gather(match.unmatched_column_indicator(), indices)
-    num_positives = tf.reduce_sum(tf.to_int32(positives_indicator))
-    max_negatives = tf.maximum(min_negatives_per_image,
-                               tf.to_int32(max_negatives_per_positive *
-                                           tf.to_float(num_positives)))
+    num_positives = tf.reduce_sum(tf.cast(positives_indicator, dtype=tf.int32))
+    max_negatives = tf.maximum(
+        min_negatives_per_image,
+        tf.cast(max_negatives_per_positive *
+                tf.cast(num_positives, dtype=tf.float32), dtype=tf.int32))
     topk_negatives_indicator = tf.less_equal(
-        tf.cumsum(tf.to_int32(negatives_indicator)), max_negatives)
+        tf.cumsum(tf.cast(negatives_indicator, dtype=tf.int32)), max_negatives)
     subsampled_selection_indices = tf.where(
         tf.logical_or(positives_indicator, topk_negatives_indicator))
     num_negatives = tf.size(subsampled_selection_indices) - num_positives
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index a454533d..602d8a3b 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -31,9 +31,7 @@ consider this box a positive example (match) nor a negative example (no match).
 The Match class is used to store the match results and it provides simple apis
 to query the results.
 """
-from abc import ABCMeta
-from abc import abstractmethod
-
+import abc
 import tensorflow as tf
 
 from object_detection.utils import ops
@@ -170,7 +168,7 @@ class Match(object):
       row_indices: int32 tensor of shape [K] with row indices.
     """
     return self._reshape_and_cast(
-        self._gather_op(tf.to_float(self._match_results),
+        self._gather_op(tf.cast(self._match_results, dtype=tf.float32),
                         self.matched_column_indices()))
 
   def num_matched_rows(self):
@@ -215,7 +213,7 @@ class Match(object):
 class Matcher(object):
   """Abstract base class for matcher.
   """
-  __metaclass__ = ABCMeta
+  __metaclass__ = abc.ABCMeta
 
   def __init__(self, use_matmul_gather=False):
     """Constructs a Matcher.
@@ -249,7 +247,7 @@ class Matcher(object):
       return Match(self._match(similarity_matrix, valid_rows),
                    self._use_matmul_gather)
 
-  @abstractmethod
+  @abc.abstractmethod
   def _match(self, similarity_matrix, valid_rows):
     """Method to be overridden by implementations.
 
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index 6dda1e7a..d70afd23 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -55,12 +55,24 @@ a handful of auxiliary annotations associated with each bounding box, namely,
 instance masks and keypoints.
 """
 import abc
+import tensorflow as tf
 
 from object_detection.core import standard_fields as fields
 
 
-class DetectionModel(object):
-  """Abstract base class for detection models."""
+# If using a new enough version of TensorFlow, detection models should be a
+# tf module or keras model for tracking.
+try:
+  _BaseClass = tf.Module
+except AttributeError:
+  _BaseClass = object
+
+
+class DetectionModel(_BaseClass):
+  """Abstract base class for detection models.
+
+  Extends tf.Module to guarantee variable tracking.
+  """
   __metaclass__ = abc.ABCMeta
 
   def __init__(self, num_classes):
diff --git a/research/object_detection/core/multiclass_nms_test.py b/research/object_detection/core/multiclass_nms_test.py
new file mode 100644
index 00000000..932de693
--- /dev/null
+++ b/research/object_detection/core/multiclass_nms_test.py
@@ -0,0 +1,525 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for tensorflow_models.object_detection.core.post_processing."""
+import numpy as np
+import tensorflow as tf
+from object_detection.core import post_processing
+from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
+
+
+class MulticlassNonMaxSuppressionTest(test_case.TestCase):
+
+  def test_multiclass_nms_select_with_shared_boxes(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_output_size)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_multiclass_nms_select_with_shared_boxes_pad_to_max_output_size(self):
+    boxes = np.array([[[0, 0, 1, 1]],
+                      [[0, 0.1, 1, 1.1]],
+                      [[0, -0.1, 1, 0.9]],
+                      [[0, 10, 1, 11]],
+                      [[0, 10.1, 1, 11.1]],
+                      [[0, 100, 1, 101]],
+                      [[0, 1000, 1, 1002]],
+                      [[0, 1000, 1, 1002.1]]], np.float32)
+    scores = np.array([[.9, 0.01], [.75, 0.05],
+                       [.6, 0.01], [.95, 0],
+                       [.5, 0.01], [.3, 0.01],
+                       [.01, .85], [.01, .5]], np.float32)
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_size_per_class = 4
+    max_output_size = 5
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    def graph_fn(boxes, scores):
+      nms, num_valid_nms_boxes = post_processing.multiclass_non_max_suppression(
+          boxes,
+          scores,
+          score_thresh,
+          iou_thresh,
+          max_size_per_class,
+          max_total_size=max_output_size,
+          pad_to_max_output_size=True)
+      return [nms.get(), nms.get_field(fields.BoxListFields.scores),
+              nms.get_field(fields.BoxListFields.classes), num_valid_nms_boxes]
+
+    [nms_corners_output, nms_scores_output, nms_classes_output,
+     num_valid_nms_boxes] = self.execute(graph_fn, [boxes, scores])
+
+    self.assertEqual(num_valid_nms_boxes, 4)
+    self.assertAllClose(nms_corners_output[0:num_valid_nms_boxes],
+                        exp_nms_corners)
+    self.assertAllClose(nms_scores_output[0:num_valid_nms_boxes],
+                        exp_nms_scores)
+    self.assertAllClose(nms_classes_output[0:num_valid_nms_boxes],
+                        exp_nms_classes)
+
+  def test_multiclass_nms_select_with_shared_boxes_given_keypoints(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    num_keypoints = 6
+    keypoints = tf.tile(
+        tf.reshape(tf.range(8), [8, 1, 1]),
+        [1, num_keypoints, 2])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+    exp_nms_keypoints_tensor = tf.tile(
+        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
+        [1, num_keypoints, 2])
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes,
+        scores,
+        score_thresh,
+        iou_thresh,
+        max_output_size,
+        additional_fields={fields.BoxListFields.keypoints: keypoints})
+
+    with self.test_session() as sess:
+      (nms_corners_output,
+       nms_scores_output,
+       nms_classes_output,
+       nms_keypoints,
+       exp_nms_keypoints) = sess.run([
+           nms.get(),
+           nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes),
+           nms.get_field(fields.BoxListFields.keypoints),
+           exp_nms_keypoints_tensor
+       ])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+      self.assertAllEqual(nms_keypoints, exp_nms_keypoints)
+
+  def test_multiclass_nms_with_shared_boxes_given_keypoint_heatmaps(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+
+    num_boxes = tf.shape(boxes)[0]
+    heatmap_height = 5
+    heatmap_width = 5
+    num_keypoints = 17
+    keypoint_heatmaps = tf.ones(
+        [num_boxes, heatmap_height, heatmap_width, num_keypoints],
+        dtype=tf.float32)
+
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+    exp_nms_keypoint_heatmaps = np.ones(
+        (4, heatmap_height, heatmap_width, num_keypoints), dtype=np.float32)
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes,
+        scores,
+        score_thresh,
+        iou_thresh,
+        max_output_size,
+        additional_fields={
+            fields.BoxListFields.keypoint_heatmaps: keypoint_heatmaps
+        })
+
+    with self.test_session() as sess:
+      (nms_corners_output,
+       nms_scores_output,
+       nms_classes_output,
+       nms_keypoint_heatmaps) = sess.run(
+           [nms.get(),
+            nms.get_field(fields.BoxListFields.scores),
+            nms.get_field(fields.BoxListFields.classes),
+            nms.get_field(fields.BoxListFields.keypoint_heatmaps)])
+
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+      self.assertAllEqual(nms_keypoint_heatmaps, exp_nms_keypoint_heatmaps)
+
+  def test_multiclass_nms_with_additional_fields(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+
+    coarse_boxes_key = 'coarse_boxes'
+    coarse_boxes = tf.constant([[0.1, 0.1, 1.1, 1.1],
+                                [0.1, 0.2, 1.1, 1.2],
+                                [0.1, -0.2, 1.1, 1.0],
+                                [0.1, 10.1, 1.1, 11.1],
+                                [0.1, 10.2, 1.1, 11.2],
+                                [0.1, 100.1, 1.1, 101.1],
+                                [0.1, 1000.1, 1.1, 1002.1],
+                                [0.1, 1000.1, 1.1, 1002.2]], tf.float32)
+
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = np.array([[0, 10, 1, 11],
+                                [0, 0, 1, 1],
+                                [0, 1000, 1, 1002],
+                                [0, 100, 1, 101]], dtype=np.float32)
+
+    exp_nms_coarse_corners = np.array([[0.1, 10.1, 1.1, 11.1],
+                                       [0.1, 0.1, 1.1, 1.1],
+                                       [0.1, 1000.1, 1.1, 1002.1],
+                                       [0.1, 100.1, 1.1, 101.1]],
+                                      dtype=np.float32)
+
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes,
+        scores,
+        score_thresh,
+        iou_thresh,
+        max_output_size,
+        additional_fields={coarse_boxes_key: coarse_boxes})
+
+    with self.test_session() as sess:
+      (nms_corners_output,
+       nms_scores_output,
+       nms_classes_output,
+       nms_coarse_corners) = sess.run(
+           [nms.get(),
+            nms.get_field(fields.BoxListFields.scores),
+            nms.get_field(fields.BoxListFields.classes),
+            nms.get_field(coarse_boxes_key)])
+
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+      self.assertAllEqual(nms_coarse_corners, exp_nms_coarse_corners)
+
+  def test_multiclass_nms_select_with_shared_boxes_given_masks(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    num_classes = 2
+    mask_height = 3
+    mask_width = 3
+    masks = tf.tile(
+        tf.reshape(tf.range(8), [8, 1, 1, 1]),
+        [1, num_classes, mask_height, mask_width])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+    exp_nms_masks_tensor = tf.tile(
+        tf.reshape(tf.constant([3, 0, 6, 5], dtype=tf.float32), [4, 1, 1]),
+        [1, mask_height, mask_width])
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_output_size, masks=masks)
+    with self.test_session() as sess:
+      (nms_corners_output,
+       nms_scores_output,
+       nms_classes_output,
+       nms_masks,
+       exp_nms_masks) = sess.run([nms.get(),
+                                  nms.get_field(fields.BoxListFields.scores),
+                                  nms.get_field(fields.BoxListFields.classes),
+                                  nms.get_field(fields.BoxListFields.masks),
+                                  exp_nms_masks_tensor])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+      self.assertAllEqual(nms_masks, exp_nms_masks)
+
+  def test_multiclass_nms_select_with_clip_window(self):
+    boxes = tf.constant([[[0, 0, 10, 10]],
+                         [[1, 1, 11, 11]]], tf.float32)
+    scores = tf.constant([[.9], [.75]])
+    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
+    score_thresh = 0.0
+    iou_thresh = 0.5
+    max_output_size = 100
+
+    exp_nms_corners = [[5, 4, 8, 7]]
+    exp_nms_scores = [.9]
+    exp_nms_classes = [0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes,
+        scores,
+        score_thresh,
+        iou_thresh,
+        max_output_size,
+        clip_window=clip_window)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_multiclass_nms_select_with_clip_window_change_coordinate_frame(self):
+    boxes = tf.constant([[[0, 0, 10, 10]],
+                         [[1, 1, 11, 11]]], tf.float32)
+    scores = tf.constant([[.9], [.75]])
+    clip_window = tf.constant([5, 4, 8, 7], tf.float32)
+    score_thresh = 0.0
+    iou_thresh = 0.5
+    max_output_size = 100
+
+    exp_nms_corners = [[0, 0, 1, 1]]
+    exp_nms_scores = [.9]
+    exp_nms_classes = [0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes,
+        scores,
+        score_thresh,
+        iou_thresh,
+        max_output_size,
+        clip_window=clip_window,
+        change_coordinate_frame=True)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_multiclass_nms_select_with_per_class_cap(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_size_per_class = 2
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 1000, 1, 1002]]
+    exp_nms_scores = [.95, .9, .85]
+    exp_nms_classes = [0, 0, 1]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_size_per_class)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_multiclass_nms_select_with_total_cap(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_size_per_class = 4
+    max_total_size = 2
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1]]
+    exp_nms_scores = [.95, .9]
+    exp_nms_classes = [0, 0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_size_per_class,
+        max_total_size)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+  def test_multiclass_nms_threshold_then_select_with_shared_boxes(self):
+    boxes = tf.constant([[[0, 0, 1, 1]],
+                         [[0, 0.1, 1, 1.1]],
+                         [[0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002]],
+                         [[0, 1000, 1, 1002.1]]], tf.float32)
+    scores = tf.constant([[.9], [.75], [.6], [.95], [.5], [.3], [.01], [.01]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 3
+
+    exp_nms = [[0, 10, 1, 11],
+               [0, 0, 1, 1],
+               [0, 100, 1, 101]]
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_output_size)
+    with self.test_session() as sess:
+      nms_output = sess.run(nms.get())
+      self.assertAllClose(nms_output, exp_nms)
+
+  def test_multiclass_nms_select_with_separate_boxes(self):
+    boxes = tf.constant([[[0, 0, 1, 1], [0, 0, 4, 5]],
+                         [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
+                         [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
+                         [[0, 10, 1, 11], [0, 10, 1, 11]],
+                         [[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
+                         [[0, 100, 1, 101], [0, 100, 1, 101]],
+                         [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
+                         [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]],
+                        tf.float32)
+    scores = tf.constant([[.9, 0.01], [.75, 0.05],
+                          [.6, 0.01], [.95, 0],
+                          [.5, 0.01], [.3, 0.01],
+                          [.01, .85], [.01, .5]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = [[0, 10, 1, 11],
+                       [0, 0, 1, 1],
+                       [0, 999, 2, 1004],
+                       [0, 100, 1, 101]]
+    exp_nms_scores = [.95, .9, .85, .3]
+    exp_nms_classes = [0, 0, 1, 0]
+
+    nms, _ = post_processing.multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh, max_output_size)
+    with self.test_session() as sess:
+      nms_corners_output, nms_scores_output, nms_classes_output = sess.run(
+          [nms.get(), nms.get_field(fields.BoxListFields.scores),
+           nms.get_field(fields.BoxListFields.classes)])
+      self.assertAllClose(nms_corners_output, exp_nms_corners)
+      self.assertAllClose(nms_scores_output, exp_nms_scores)
+      self.assertAllClose(nms_classes_output, exp_nms_classes)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 5d887a53..cf725fb2 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -24,6 +24,94 @@ from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
 
 
+def _validate_boxes_scores_iou_thresh(boxes, scores, iou_thresh,
+                                      change_coordinate_frame, clip_window):
+  """Validates boxes, scores and iou_thresh.
+
+  This function validates the boxes, scores, iou_thresh
+     and if change_coordinate_frame is True, clip_window must be specified.
+
+  Args:
+    boxes: A [k, q, 4] float32 tensor containing k detections. `q` can be either
+      number of classes or 1 depending on whether a separate box is predicted
+      per class.
+    scores: A [k, num_classes] float32 tensor containing the scores for each of
+      the k detections. The scores have to be non-negative when
+      pad_to_max_output_size is True.
+    iou_thresh: scalar threshold for IOU (new boxes that have high IOU overlap
+      with previously selected boxes are removed).
+    change_coordinate_frame: Whether to normalize coordinates after clipping
+      relative to clip_window (this can only be set to True if a clip_window is
+      provided)
+    clip_window: A float32 tensor of the form [y_min, x_min, y_max, x_max]
+      representing the window to clip and normalize boxes to before performing
+      non-max suppression.
+
+  Raises:
+    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not
+    have a valid scores field.
+  """
+  if not 0 <= iou_thresh <= 1.0:
+    raise ValueError('iou_thresh must be between 0 and 1')
+  if scores.shape.ndims != 2:
+    raise ValueError('scores field must be of rank 2')
+  if scores.shape[1].value is None:
+    raise ValueError('scores must have statically defined second ' 'dimension')
+  if boxes.shape.ndims != 3:
+    raise ValueError('boxes must be of rank 3.')
+  if not (shape_utils.get_dim_as_int(
+      boxes.shape[1]) == shape_utils.get_dim_as_int(scores.shape[1]) or
+          shape_utils.get_dim_as_int(boxes.shape[1]) == 1):
+    raise ValueError('second dimension of boxes must be either 1 or equal '
+                     'to the second dimension of scores')
+  if boxes.shape[2].value != 4:
+    raise ValueError('last dimension of boxes must be of size 4.')
+  if change_coordinate_frame and clip_window is None:
+    raise ValueError('if change_coordinate_frame is True, then a clip_window'
+                     'must be specified.')
+
+
+def _clip_window_prune_boxes(sorted_boxes, clip_window, pad_to_max_output_size,
+                             change_coordinate_frame):
+  """Prune boxes with zero area.
+
+  Args:
+    sorted_boxes: A BoxList containing k detections.
+    clip_window: A float32 tensor of the form [y_min, x_min, y_max, x_max]
+      representing the window to clip and normalize boxes to before performing
+      non-max suppression.
+    pad_to_max_output_size: flag indicating whether to pad to max output size or
+      not.
+    change_coordinate_frame: Whether to normalize coordinates after clipping
+      relative to clip_window (this can only be set to True if a clip_window is
+      provided).
+
+  Returns:
+    sorted_boxes: A BoxList containing k detections after pruning.
+    num_valid_nms_boxes_cumulative: Number of valid NMS boxes
+  """
+  sorted_boxes = box_list_ops.clip_to_window(
+      sorted_boxes,
+      clip_window,
+      filter_nonoverlapping=not pad_to_max_output_size)
+  # Set the scores of boxes with zero area to -1 to keep the default
+  # behaviour of pruning out zero area boxes.
+  sorted_boxes_size = tf.shape(sorted_boxes.get())[0]
+  non_zero_box_area = tf.cast(box_list_ops.area(sorted_boxes), tf.bool)
+  sorted_boxes_scores = tf.where(
+      non_zero_box_area, sorted_boxes.get_field(fields.BoxListFields.scores),
+      -1 * tf.ones(sorted_boxes_size))
+  sorted_boxes.add_field(fields.BoxListFields.scores, sorted_boxes_scores)
+  num_valid_nms_boxes_cumulative = tf.reduce_sum(
+      tf.cast(tf.greater_equal(sorted_boxes_scores, 0), tf.int32))
+  sorted_boxes = box_list_ops.sort_by_field(sorted_boxes,
+                                            fields.BoxListFields.scores)
+  if change_coordinate_frame:
+    sorted_boxes = box_list_ops.change_coordinate_frame(sorted_boxes,
+                                                        clip_window)
+  return sorted_boxes, num_valid_nms_boxes_cumulative
+
+
 def multiclass_non_max_suppression(boxes,
                                    scores,
                                    score_thresh,
@@ -97,28 +185,12 @@ def multiclass_non_max_suppression(boxes,
     ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have
       a valid scores field.
   """
-  if not 0 <= iou_thresh <= 1.0:
-    raise ValueError('iou_thresh must be between 0 and 1')
-  if scores.shape.ndims != 2:
-    raise ValueError('scores field must be of rank 2')
-  if scores.shape[1].value is None:
-    raise ValueError('scores must have statically defined second '
-                     'dimension')
-  if boxes.shape.ndims != 3:
-    raise ValueError('boxes must be of rank 3.')
-  if not (boxes.shape[1].value == scores.shape[1].value or
-          boxes.shape[1].value == 1):
-    raise ValueError('second dimension of boxes must be either 1 or equal '
-                     'to the second dimension of scores')
-  if boxes.shape[2].value != 4:
-    raise ValueError('last dimension of boxes must be of size 4.')
-  if change_coordinate_frame and clip_window is None:
-    raise ValueError('if change_coordinate_frame is True, then a clip_window'
-                     'must be specified.')
+  _validate_boxes_scores_iou_thresh(boxes, scores, iou_thresh,
+                                    change_coordinate_frame, clip_window)
 
   with tf.name_scope(scope, 'MultiClassNonMaxSuppression'):
     num_scores = tf.shape(scores)[0]
-    num_classes = scores.get_shape()[1]
+    num_classes = shape_utils.get_dim_as_int(scores.get_shape()[1])
 
     selected_boxes_list = []
     num_valid_nms_boxes_cumulative = tf.constant(0)
@@ -128,7 +200,7 @@ def multiclass_non_max_suppression(boxes,
     if boundaries is not None:
       per_class_boundaries_list = tf.unstack(boundaries, axis=1)
     boxes_ids = (range(num_classes) if len(per_class_boxes_list) > 1
-                 else [0] * num_classes.value)
+                 else [0] * num_classes)
     for class_idx, boxes_idx in zip(range(num_classes), boxes_ids):
       per_class_boxes = per_class_boxes_list[boxes_idx]
       boxlist_and_class_scores = box_list.BoxList(per_class_boxes)
@@ -193,32 +265,13 @@ def multiclass_non_max_suppression(boxes,
     if clip_window is not None:
       # When pad_to_max_output_size is False, it prunes the boxes with zero
       # area.
-      sorted_boxes = box_list_ops.clip_to_window(
-          sorted_boxes,
-          clip_window,
-          filter_nonoverlapping=not pad_to_max_output_size)
-      # Set the scores of boxes with zero area to -1 to keep the default
-      # behaviour of pruning out zero area boxes.
-      sorted_boxes_size = tf.shape(sorted_boxes.get())[0]
-      non_zero_box_area = tf.cast(box_list_ops.area(sorted_boxes), tf.bool)
-      sorted_boxes_scores = tf.where(
-          non_zero_box_area,
-          sorted_boxes.get_field(fields.BoxListFields.scores),
-          -1*tf.ones(sorted_boxes_size))
-      sorted_boxes.add_field(fields.BoxListFields.scores, sorted_boxes_scores)
-      num_valid_nms_boxes_cumulative = tf.reduce_sum(
-          tf.cast(tf.greater_equal(sorted_boxes_scores, 0), tf.int32))
-      sorted_boxes = box_list_ops.sort_by_field(sorted_boxes,
-                                                fields.BoxListFields.scores)
-      if change_coordinate_frame:
-        sorted_boxes = box_list_ops.change_coordinate_frame(
-            sorted_boxes, clip_window)
+      sorted_boxes, num_valid_nms_boxes_cumulative = _clip_window_prune_boxes(
+          sorted_boxes, clip_window, pad_to_max_output_size,
+          change_coordinate_frame)
 
     if max_total_size:
-      max_total_size = tf.minimum(max_total_size,
-                                  sorted_boxes.num_boxes())
-      sorted_boxes = box_list_ops.gather(sorted_boxes,
-                                         tf.range(max_total_size))
+      max_total_size = tf.minimum(max_total_size, sorted_boxes.num_boxes())
+      sorted_boxes = box_list_ops.gather(sorted_boxes, tf.range(max_total_size))
       num_valid_nms_boxes_cumulative = tf.where(
           max_total_size > num_valid_nms_boxes_cumulative,
           num_valid_nms_boxes_cumulative, max_total_size)
@@ -230,6 +283,175 @@ def multiclass_non_max_suppression(boxes,
     return sorted_boxes, num_valid_nms_boxes_cumulative
 
 
+def class_agnostic_non_max_suppression(boxes,
+                                       scores,
+                                       score_thresh,
+                                       iou_thresh,
+                                       max_classes_per_detection=1,
+                                       max_total_size=0,
+                                       clip_window=None,
+                                       change_coordinate_frame=False,
+                                       masks=None,
+                                       boundaries=None,
+                                       pad_to_max_output_size=False,
+                                       additional_fields=None,
+                                       scope=None):
+  """Class-agnostic version of non maximum suppression.
+
+  This op greedily selects a subset of detection bounding boxes, pruning
+  away boxes that have high IOU (intersection over union) overlap (> thresh)
+  with already selected boxes.  It operates on all the boxes using
+  max scores across all classes for which scores are provided (via the scores
+  field of the input box_list), pruning boxes with score less than a provided
+  threshold prior to applying NMS.
+
+  Please note that this operation is performed in a class-agnostic way,
+  therefore any background classes should be removed prior to calling this
+  function.
+
+  Selected boxes are guaranteed to be sorted in decreasing order by score (but
+  the sort is not guaranteed to be stable).
+
+  Args:
+    boxes: A [k, q, 4] float32 tensor containing k detections. `q` can be either
+      number of classes or 1 depending on whether a separate box is predicted
+      per class.
+    scores: A [k, num_classes] float32 tensor containing the scores for each of
+      the k detections. The scores have to be non-negative when
+      pad_to_max_output_size is True.
+    score_thresh: scalar threshold for score (low scoring boxes are removed).
+    iou_thresh: scalar threshold for IOU (new boxes that have high IOU overlap
+      with previously selected boxes are removed).
+    max_classes_per_detection: maximum number of retained classes per detection
+      box in class-agnostic NMS.
+    max_total_size: maximum number of boxes retained over all classes. By
+      default returns all boxes retained after capping boxes per class.
+    clip_window: A float32 tensor of the form [y_min, x_min, y_max, x_max]
+      representing the window to clip and normalize boxes to before performing
+      non-max suppression.
+    change_coordinate_frame: Whether to normalize coordinates after clipping
+      relative to clip_window (this can only be set to True if a clip_window is
+      provided)
+    masks: (optional) a [k, q, mask_height, mask_width] float32 tensor
+      containing box masks. `q` can be either number of classes or 1 depending
+      on whether a separate mask is predicted per class.
+    boundaries: (optional) a [k, q, boundary_height, boundary_width] float32
+      tensor containing box boundaries. `q` can be either number of classes or 1
+      depending on whether a separate boundary is predicted per class.
+    pad_to_max_output_size: If true, the output nmsed boxes are padded to be of
+      length `max_size_per_class`. Defaults to false.
+    additional_fields: (optional) If not None, a dictionary that maps keys to
+      tensors whose first dimensions are all of size `k`. After non-maximum
+      suppression, all tensors corresponding to the selected boxes will be added
+      to resulting BoxList.
+    scope: name scope.
+
+  Returns:
+    A tuple of sorted_boxes and num_valid_nms_boxes. The sorted_boxes is a
+      BoxList holds M boxes with a rank-1 scores field representing
+      corresponding scores for each box with scores sorted in decreasing order
+      and a rank-1 classes field representing a class label for each box. The
+      num_valid_nms_boxes is a 0-D integer tensor representing the number of
+      valid elements in `BoxList`, with the valid elements appearing first.
+
+  Raises:
+    ValueError: if iou_thresh is not in [0, 1] or if input boxlist does not have
+      a valid scores field.
+  """
+  _validate_boxes_scores_iou_thresh(boxes, scores, iou_thresh,
+                                    change_coordinate_frame, clip_window)
+
+  if max_classes_per_detection > 1:
+    raise ValueError('Max classes per detection box >1 not supported.')
+  q = boxes.shape[1].value
+  if q > 1:
+    class_ids = tf.expand_dims(
+        tf.argmax(scores, axis=1, output_type=tf.int32), axis=1)
+    boxes = tf.batch_gather(boxes, class_ids)
+    if masks is not None:
+      masks = tf.batch_gather(masks, class_ids)
+    if boundaries is not None:
+      boundaries = tf.batch_gather(boundaries, class_ids)
+  boxes = tf.squeeze(boxes, axis=[1])
+  if masks is not None:
+    masks = tf.squeeze(masks, axis=[1])
+  if boundaries is not None:
+    boundaries = tf.squeeze(boundaries, axis=[1])
+
+  with tf.name_scope(scope, 'ClassAgnosticNonMaxSuppression'):
+    boxlist_and_class_scores = box_list.BoxList(boxes)
+    max_scores = tf.reduce_max(scores, axis=-1)
+    classes_with_max_scores = tf.argmax(scores, axis=-1)
+    boxlist_and_class_scores.add_field(fields.BoxListFields.scores, max_scores)
+    if masks is not None:
+      boxlist_and_class_scores.add_field(fields.BoxListFields.masks, masks)
+    if boundaries is not None:
+      boxlist_and_class_scores.add_field(fields.BoxListFields.boundaries,
+                                         boundaries)
+
+    if additional_fields is not None:
+      for key, tensor in additional_fields.items():
+        boxlist_and_class_scores.add_field(key, tensor)
+
+    if pad_to_max_output_size:
+      max_selection_size = max_total_size
+      selected_indices, num_valid_nms_boxes = (
+          tf.image.non_max_suppression_padded(
+              boxlist_and_class_scores.get(),
+              boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
+              max_selection_size,
+              iou_threshold=iou_thresh,
+              score_threshold=score_thresh,
+              pad_to_max_output_size=True))
+    else:
+      max_selection_size = tf.minimum(max_total_size,
+                                      boxlist_and_class_scores.num_boxes())
+      selected_indices = tf.image.non_max_suppression(
+          boxlist_and_class_scores.get(),
+          boxlist_and_class_scores.get_field(fields.BoxListFields.scores),
+          max_selection_size,
+          iou_threshold=iou_thresh,
+          score_threshold=score_thresh)
+      num_valid_nms_boxes = tf.shape(selected_indices)[0]
+      selected_indices = tf.concat([
+          selected_indices,
+          tf.zeros(max_selection_size - num_valid_nms_boxes, tf.int32)
+      ], 0)
+
+    nms_result = box_list_ops.gather(boxlist_and_class_scores, selected_indices)
+
+    valid_nms_boxes_indx = tf.less(
+        tf.range(max_selection_size), num_valid_nms_boxes)
+    nms_scores = nms_result.get_field(fields.BoxListFields.scores)
+    nms_result.add_field(
+        fields.BoxListFields.scores,
+        tf.where(valid_nms_boxes_indx, nms_scores,
+                 -1 * tf.ones(max_selection_size)))
+    selected_classes = tf.gather(classes_with_max_scores, selected_indices)
+    nms_result.add_field(fields.BoxListFields.classes, selected_classes)
+    selected_boxes = nms_result
+    sorted_boxes = box_list_ops.sort_by_field(selected_boxes,
+                                              fields.BoxListFields.scores)
+    if clip_window is not None:
+      # When pad_to_max_output_size is False, it prunes the boxes with zero
+      # area.
+      sorted_boxes, num_valid_nms_boxes = _clip_window_prune_boxes(
+          sorted_boxes, clip_window, pad_to_max_output_size,
+          change_coordinate_frame)
+
+    if max_total_size:
+      max_total_size = tf.minimum(max_total_size, sorted_boxes.num_boxes())
+      sorted_boxes = box_list_ops.gather(sorted_boxes, tf.range(max_total_size))
+      num_valid_nms_boxes = tf.where(max_total_size > num_valid_nms_boxes,
+                                     num_valid_nms_boxes, max_total_size)
+    # Select only the valid boxes if pad_to_max_output_size is False.
+    if not pad_to_max_output_size:
+      sorted_boxes = box_list_ops.gather(sorted_boxes,
+                                         tf.range(num_valid_nms_boxes))
+
+    return sorted_boxes, num_valid_nms_boxes
+
+
 def batch_multiclass_non_max_suppression(boxes,
                                          scores,
                                          score_thresh,
@@ -243,7 +465,9 @@ def batch_multiclass_non_max_suppression(boxes,
                                          additional_fields=None,
                                          scope=None,
                                          use_static_shapes=False,
-                                         parallel_iterations=32):
+                                         parallel_iterations=32,
+                                         use_class_agnostic_nms=False,
+                                         max_classes_per_detection=1):
   """Multi-class version of non maximum suppression that operates on a batch.
 
   This op is similar to `multiclass_non_max_suppression` but operates on a batch
@@ -253,8 +477,8 @@ def batch_multiclass_non_max_suppression(boxes,
   Args:
     boxes: A [batch_size, num_anchors, q, 4] float32 tensor containing
       detections. If `q` is 1 then same boxes are used for all classes
-        otherwise, if `q` is equal to number of classes, class-specific boxes
-        are used.
+      otherwise, if `q` is equal to number of classes, class-specific boxes are
+      used.
     scores: A [batch_size, num_anchors, num_classes] float32 tensor containing
       the scores for each of the `num_anchors` detections. The scores have to be
       non-negative when use_static_shapes is set True.
@@ -274,8 +498,8 @@ def batch_multiclass_non_max_suppression(boxes,
       relative to clip_window (this can only be set to True if a clip_window is
       provided)
     num_valid_boxes: (optional) a Tensor of type `int32`. A 1-D tensor of shape
-      [batch_size] representing the number of valid boxes to be considered
-      for each image in the batch.  This parameter allows for ignoring zero
+      [batch_size] representing the number of valid boxes to be considered for
+      each image in the batch.  This parameter allows for ignoring zero
       paddings.
     masks: (optional) a [batch_size, num_anchors, q, mask_height, mask_width]
       float32 tensor containing box masks. `q` can be either number of classes
@@ -288,6 +512,10 @@ def batch_multiclass_non_max_suppression(boxes,
       Defaults to false.
     parallel_iterations: (optional) number of batch items to process in
       parallel.
+    use_class_agnostic_nms: If true, this uses class-agnostic non max
+      suppression
+    max_classes_per_detection: Maximum number of retained classes per detection
+      box in class-agnostic NMS.
 
   Returns:
     'nmsed_boxes': A [batch_size, max_detections, 4] float32 tensor
@@ -313,8 +541,8 @@ def batch_multiclass_non_max_suppression(boxes,
     ValueError: if `q` in boxes.shape is not 1 or not equal to number of
       classes as inferred from scores.shape.
   """
-  q = boxes.shape[2].value
-  num_classes = scores.shape[2].value
+  q = shape_utils.get_dim_as_int(boxes.shape[2])
+  num_classes = shape_utils.get_dim_as_int(scores.shape[2])
   if q != 1 and q != num_classes:
     raise ValueError('third dimension of boxes must be either 1 or equal '
                      'to the third dimension of scores')
@@ -335,8 +563,8 @@ def batch_multiclass_non_max_suppression(boxes,
   del additional_fields
   with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
     boxes_shape = boxes.shape
-    batch_size = boxes_shape[0].value
-    num_anchors = boxes_shape[1].value
+    batch_size = shape_utils.get_dim_as_int(boxes_shape[0])
+    num_anchors = shape_utils.get_dim_as_int(boxes_shape[1])
 
     if batch_size is None:
       batch_size = tf.shape(boxes)[0]
@@ -434,31 +662,47 @@ def batch_multiclass_non_max_suppression(boxes,
         per_image_masks = tf.reshape(
             tf.slice(per_image_masks, 4 * [0],
                      tf.stack([per_image_num_valid_boxes, -1, -1, -1])),
-            [-1, q, per_image_masks.shape[2].value,
-             per_image_masks.shape[3].value])
+            [-1, q, shape_utils.get_dim_as_int(per_image_masks.shape[2]),
+             shape_utils.get_dim_as_int(per_image_masks.shape[3])])
         if per_image_additional_fields is not None:
           for key, tensor in per_image_additional_fields.items():
             additional_field_shape = tensor.get_shape()
             additional_field_dim = len(additional_field_shape)
             per_image_additional_fields[key] = tf.reshape(
-                tf.slice(per_image_additional_fields[key],
-                         additional_field_dim * [0],
-                         tf.stack([per_image_num_valid_boxes] +
-                                  (additional_field_dim - 1) * [-1])),
-                [-1] + [dim.value for dim in additional_field_shape[1:]])
-
-      nmsed_boxlist, num_valid_nms_boxes = multiclass_non_max_suppression(
-          per_image_boxes,
-          per_image_scores,
-          score_thresh,
-          iou_thresh,
-          max_size_per_class,
-          max_total_size,
-          clip_window=per_image_clip_window,
-          change_coordinate_frame=change_coordinate_frame,
-          masks=per_image_masks,
-          pad_to_max_output_size=use_static_shapes,
-          additional_fields=per_image_additional_fields)
+                tf.slice(
+                    per_image_additional_fields[key],
+                    additional_field_dim * [0],
+                    tf.stack([per_image_num_valid_boxes] +
+                             (additional_field_dim - 1) * [-1])), [-1] + [
+                                 shape_utils.get_dim_as_int(dim)
+                                 for dim in additional_field_shape[1:]
+                             ])
+      if use_class_agnostic_nms:
+        nmsed_boxlist, num_valid_nms_boxes = class_agnostic_non_max_suppression(
+            per_image_boxes,
+            per_image_scores,
+            score_thresh,
+            iou_thresh,
+            max_classes_per_detection,
+            max_total_size,
+            clip_window=per_image_clip_window,
+            change_coordinate_frame=change_coordinate_frame,
+            masks=per_image_masks,
+            pad_to_max_output_size=use_static_shapes,
+            additional_fields=per_image_additional_fields)
+      else:
+        nmsed_boxlist, num_valid_nms_boxes = multiclass_non_max_suppression(
+            per_image_boxes,
+            per_image_scores,
+            score_thresh,
+            iou_thresh,
+            max_size_per_class,
+            max_total_size,
+            clip_window=per_image_clip_window,
+            change_coordinate_frame=change_coordinate_frame,
+            masks=per_image_masks,
+            pad_to_max_output_size=use_static_shapes,
+            additional_fields=per_image_additional_fields)
 
       if not use_static_shapes:
         nmsed_boxlist = box_list_ops.pad_or_clip_box_list(
@@ -499,7 +743,7 @@ def batch_multiclass_non_max_suppression(boxes,
     if num_additional_fields > 0:
       # Sort the keys to ensure arranging elements in same order as
       # in _single_image_nms_fn.
-      batch_nmsed_keys = ordered_additional_fields.keys()
+      batch_nmsed_keys = list(ordered_additional_fields.keys())
       for i in range(len(batch_nmsed_keys)):
         batch_nmsed_additional_fields[
             batch_nmsed_keys[i]] = batch_nmsed_values[i]
diff --git a/research/object_detection/core/prefetcher.py b/research/object_detection/core/prefetcher.py
index e690c599..9bb7d658 100644
--- a/research/object_detection/core/prefetcher.py
+++ b/research/object_detection/core/prefetcher.py
@@ -55,7 +55,7 @@ def prefetch(tensor_dict, capacity):
   enqueue_op = prefetch_queue.enqueue(tensor_dict)
   tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(
       prefetch_queue, [enqueue_op]))
-  tf.summary.scalar('queue/%s/fraction_of_%d_full' % (prefetch_queue.name,
-                                                      capacity),
-                    tf.to_float(prefetch_queue.size()) * (1. / capacity))
+  tf.summary.scalar(
+      'queue/%s/fraction_of_%d_full' % (prefetch_queue.name, capacity),
+      tf.cast(prefetch_queue.size(), dtype=tf.float32) * (1. / capacity))
   return prefetch_queue
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index fc088abc..c2197f41 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -261,7 +261,7 @@ def normalize_image(image, original_minval, original_maxval, target_minval,
     original_maxval = float(original_maxval)
     target_minval = float(target_minval)
     target_maxval = float(target_maxval)
-    image = tf.to_float(image)
+    image = tf.cast(image, dtype=tf.float32)
     image = tf.subtract(image, original_minval)
     image = tf.multiply(image, (target_maxval - target_minval) /
                         (original_maxval - original_minval))
@@ -810,10 +810,12 @@ def random_image_scale(image,
         generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE,
         preprocess_vars_cache)
 
-    image_newysize = tf.to_int32(
-        tf.multiply(tf.to_float(image_height), size_coef))
-    image_newxsize = tf.to_int32(
-        tf.multiply(tf.to_float(image_width), size_coef))
+    image_newysize = tf.cast(
+        tf.multiply(tf.cast(image_height, dtype=tf.float32), size_coef),
+        dtype=tf.int32)
+    image_newxsize = tf.cast(
+        tf.multiply(tf.cast(image_width, dtype=tf.float32), size_coef),
+        dtype=tf.int32)
     image = tf.image.resize_images(
         image, [image_newysize, image_newxsize], align_corners=True)
     result.append(image)
@@ -1237,7 +1239,7 @@ def _strict_random_crop_image(image,
     new_image.set_shape([None, None, image.get_shape()[2]])
 
     # [1, 4]
-    im_box_rank2 = tf.squeeze(im_box, squeeze_dims=[0])
+    im_box_rank2 = tf.squeeze(im_box, axis=[0])
     # [4]
     im_box_rank1 = tf.squeeze(im_box)
 
@@ -1555,13 +1557,15 @@ def random_pad_image(image,
   new_image += image_color_padded
 
   # setting boxes
-  new_window = tf.to_float(
+  new_window = tf.cast(
       tf.stack([
           -offset_height, -offset_width, target_height - offset_height,
           target_width - offset_width
-      ]))
-  new_window /= tf.to_float(
-      tf.stack([image_height, image_width, image_height, image_width]))
+      ]),
+      dtype=tf.float32)
+  new_window /= tf.cast(
+      tf.stack([image_height, image_width, image_height, image_width]),
+      dtype=tf.float32)
   boxlist = box_list.BoxList(boxes)
   new_boxlist = box_list_ops.change_coordinate_frame(boxlist, new_window)
   new_boxes = new_boxlist.get()
@@ -1616,8 +1620,8 @@ def random_absolute_pad_image(image,
            form.
   """
   min_image_size = tf.shape(image)[:2]
-  max_image_size = min_image_size + tf.to_int32(
-      [max_height_padding, max_width_padding])
+  max_image_size = min_image_size + tf.cast(
+      [max_height_padding, max_width_padding], dtype=tf.int32)
   return random_pad_image(image, boxes, min_image_size=min_image_size,
                           max_image_size=max_image_size, pad_color=pad_color,
                           seed=seed,
@@ -1723,12 +1727,14 @@ def random_crop_pad_image(image,
 
   cropped_image, cropped_boxes, cropped_labels = result[:3]
 
-  min_image_size = tf.to_int32(
-      tf.to_float(tf.stack([image_height, image_width])) *
-      min_padded_size_ratio)
-  max_image_size = tf.to_int32(
-      tf.to_float(tf.stack([image_height, image_width])) *
-      max_padded_size_ratio)
+  min_image_size = tf.cast(
+      tf.cast(tf.stack([image_height, image_width]), dtype=tf.float32) *
+      min_padded_size_ratio,
+      dtype=tf.int32)
+  max_image_size = tf.cast(
+      tf.cast(tf.stack([image_height, image_width]), dtype=tf.float32) *
+      max_padded_size_ratio,
+      dtype=tf.int32)
 
   padded_image, padded_boxes = random_pad_image(
       cropped_image,
@@ -1840,16 +1846,23 @@ def random_crop_to_aspect_ratio(image,
     image_shape = tf.shape(image)
     orig_height = image_shape[0]
     orig_width = image_shape[1]
-    orig_aspect_ratio = tf.to_float(orig_width) / tf.to_float(orig_height)
+    orig_aspect_ratio = tf.cast(
+        orig_width, dtype=tf.float32) / tf.cast(
+            orig_height, dtype=tf.float32)
     new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)
+
     def target_height_fn():
-      return tf.to_int32(tf.round(tf.to_float(orig_width) / new_aspect_ratio))
+      return tf.cast(
+          tf.round(tf.cast(orig_width, dtype=tf.float32) / new_aspect_ratio),
+          dtype=tf.int32)
 
     target_height = tf.cond(orig_aspect_ratio >= new_aspect_ratio,
                             lambda: orig_height, target_height_fn)
 
     def target_width_fn():
-      return tf.to_int32(tf.round(tf.to_float(orig_height) * new_aspect_ratio))
+      return tf.cast(
+          tf.round(tf.cast(orig_height, dtype=tf.float32) * new_aspect_ratio),
+          dtype=tf.int32)
 
     target_width = tf.cond(orig_aspect_ratio <= new_aspect_ratio,
                            lambda: orig_width, target_width_fn)
@@ -1870,10 +1883,14 @@ def random_crop_to_aspect_ratio(image,
         image, offset_height, offset_width, target_height, target_width)
 
     im_box = tf.stack([
-        tf.to_float(offset_height) / tf.to_float(orig_height),
-        tf.to_float(offset_width) / tf.to_float(orig_width),
-        tf.to_float(offset_height + target_height) / tf.to_float(orig_height),
-        tf.to_float(offset_width + target_width) / tf.to_float(orig_width)
+        tf.cast(offset_height, dtype=tf.float32) /
+        tf.cast(orig_height, dtype=tf.float32),
+        tf.cast(offset_width, dtype=tf.float32) /
+        tf.cast(orig_width, dtype=tf.float32),
+        tf.cast(offset_height + target_height, dtype=tf.float32) /
+        tf.cast(orig_height, dtype=tf.float32),
+        tf.cast(offset_width + target_width, dtype=tf.float32) /
+        tf.cast(orig_width, dtype=tf.float32)
     ])
 
     boxlist = box_list.BoxList(boxes)
@@ -1996,8 +2013,8 @@ def random_pad_to_aspect_ratio(image,
 
   with tf.name_scope('RandomPadToAspectRatio', values=[image]):
     image_shape = tf.shape(image)
-    image_height = tf.to_float(image_shape[0])
-    image_width = tf.to_float(image_shape[1])
+    image_height = tf.cast(image_shape[0], dtype=tf.float32)
+    image_width = tf.cast(image_shape[1], dtype=tf.float32)
     image_aspect_ratio = image_width / image_height
     new_aspect_ratio = tf.constant(aspect_ratio, dtype=tf.float32)
     target_height = tf.cond(
@@ -2034,7 +2051,8 @@ def random_pad_to_aspect_ratio(image,
     target_width = tf.round(scale * target_width)
 
     new_image = tf.image.pad_to_bounding_box(
-        image, 0, 0, tf.to_int32(target_height), tf.to_int32(target_width))
+        image, 0, 0, tf.cast(target_height, dtype=tf.int32),
+        tf.cast(target_width, dtype=tf.int32))
 
     im_box = tf.stack([
         0.0,
@@ -2050,9 +2068,9 @@ def random_pad_to_aspect_ratio(image,
 
     if masks is not None:
       new_masks = tf.expand_dims(masks, -1)
-      new_masks = tf.image.pad_to_bounding_box(new_masks, 0, 0,
-                                               tf.to_int32(target_height),
-                                               tf.to_int32(target_width))
+      new_masks = tf.image.pad_to_bounding_box(
+          new_masks, 0, 0, tf.cast(target_height, dtype=tf.int32),
+          tf.cast(target_width, dtype=tf.int32))
       new_masks = tf.squeeze(new_masks, [-1])
       result.append(new_masks)
 
@@ -2106,10 +2124,12 @@ def random_black_patches(image,
     image_shape = tf.shape(image)
     image_height = image_shape[0]
     image_width = image_shape[1]
-    box_size = tf.to_int32(
+    box_size = tf.cast(
         tf.multiply(
-            tf.minimum(tf.to_float(image_height), tf.to_float(image_width)),
-            size_to_image_ratio))
+            tf.minimum(
+                tf.cast(image_height, dtype=tf.float32),
+                tf.cast(image_width, dtype=tf.float32)), size_to_image_ratio),
+        dtype=tf.int32)
 
     generator_func = functools.partial(tf.random_uniform, [], minval=0.0,
                                        maxval=(1.0 - size_to_image_ratio),
@@ -2123,8 +2143,12 @@ def random_black_patches(image,
         preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,
         preprocess_vars_cache, key=str(idx) + 'x')
 
-    y_min = tf.to_int32(normalized_y_min * tf.to_float(image_height))
-    x_min = tf.to_int32(normalized_x_min * tf.to_float(image_width))
+    y_min = tf.cast(
+        normalized_y_min * tf.cast(image_height, dtype=tf.float32),
+        dtype=tf.int32)
+    x_min = tf.cast(
+        normalized_x_min * tf.cast(image_width, dtype=tf.float32),
+        dtype=tf.int32)
     black_box = tf.ones([box_size, box_size, 3], dtype=tf.float32)
     mask = 1.0 - tf.image.pad_to_bounding_box(black_box, y_min, x_min,
                                               image_height, image_width)
@@ -2156,7 +2180,7 @@ def image_to_float(image):
     image: image in tf.float32 format.
   """
   with tf.name_scope('ImageToFloat', values=[image]):
-    image = tf.to_float(image)
+    image = tf.cast(image, dtype=tf.float32)
     return image
 
 
@@ -2342,10 +2366,12 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600,
     (image_height, image_width, num_channels) = _get_image_info(image)
     min_image_dimension = tf.minimum(image_height, image_width)
     min_target_dimension = tf.maximum(min_image_dimension, min_dimension)
-    target_ratio = tf.to_float(min_target_dimension) / tf.to_float(
-        min_image_dimension)
-    target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)
-    target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)
+    target_ratio = tf.cast(min_target_dimension, dtype=tf.float32) / tf.cast(
+        min_image_dimension, dtype=tf.float32)
+    target_height = tf.cast(
+        tf.cast(image_height, dtype=tf.float32) * target_ratio, dtype=tf.int32)
+    target_width = tf.cast(
+        tf.cast(image_width, dtype=tf.float32) * target_ratio, dtype=tf.int32)
     image = tf.image.resize_images(
         tf.expand_dims(image, axis=0), size=[target_height, target_width],
         method=method,
@@ -2398,10 +2424,12 @@ def resize_to_max_dimension(image, masks=None, max_dimension=600,
     (image_height, image_width, num_channels) = _get_image_info(image)
     max_image_dimension = tf.maximum(image_height, image_width)
     max_target_dimension = tf.minimum(max_image_dimension, max_dimension)
-    target_ratio = tf.to_float(max_target_dimension) / tf.to_float(
-        max_image_dimension)
-    target_height = tf.to_int32(tf.to_float(image_height) * target_ratio)
-    target_width = tf.to_int32(tf.to_float(image_width) * target_ratio)
+    target_ratio = tf.cast(max_target_dimension, dtype=tf.float32) / tf.cast(
+        max_image_dimension, dtype=tf.float32)
+    target_height = tf.cast(
+        tf.cast(image_height, dtype=tf.float32) * target_ratio, dtype=tf.int32)
+    target_width = tf.cast(
+        tf.cast(image_width, dtype=tf.float32) * target_ratio, dtype=tf.int32)
     image = tf.image.resize_images(
         tf.expand_dims(image, axis=0), size=[target_height, target_width],
         method=method,
@@ -2639,11 +2667,11 @@ def random_self_concat_image(
 
     if axis == 0:
       # Concat vertically, so need to reduce the y coordinates.
-      old_scaling = tf.to_float([0.5, 1.0, 0.5, 1.0])
-      new_translation = tf.to_float([0.5, 0.0, 0.5, 0.0])
+      old_scaling = tf.constant([0.5, 1.0, 0.5, 1.0])
+      new_translation = tf.constant([0.5, 0.0, 0.5, 0.0])
     elif axis == 1:
-      old_scaling = tf.to_float([1.0, 0.5, 1.0, 0.5])
-      new_translation = tf.to_float([0.0, 0.5, 0.0, 0.5])
+      old_scaling = tf.constant([1.0, 0.5, 1.0, 0.5])
+      new_translation = tf.constant([0.0, 0.5, 0.0, 0.5])
 
     old_boxes = old_scaling * boxes
     new_boxes = old_boxes + new_translation
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 030cec59..3937e4e5 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -795,8 +795,8 @@ class PreprocessorTest(tf.test.TestCase):
     images = self.createTestImages()
     tensor_dict = {fields.InputDataFields.image: images}
     tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
-    images_min = tf.to_float(images) * 0.9 / 255.0
-    images_max = tf.to_float(images) * 1.1 / 255.0
+    images_min = tf.cast(images, dtype=tf.float32) * 0.9 / 255.0
+    images_max = tf.cast(images, dtype=tf.float32) * 1.1 / 255.0
     images = tensor_dict[fields.InputDataFields.image]
     values_greater = tf.greater_equal(images, images_min)
     values_less = tf.less_equal(images, images_max)
@@ -858,20 +858,26 @@ class PreprocessorTest(tf.test.TestCase):
         value=images_gray, num_or_size_splits=3, axis=3)
     images_r, images_g, images_b = tf.split(
         value=images_original, num_or_size_splits=3, axis=3)
-    images_r_diff1 = tf.squared_difference(tf.to_float(images_r),
-                                           tf.to_float(images_gray_r))
-    images_r_diff2 = tf.squared_difference(tf.to_float(images_gray_r),
-                                           tf.to_float(images_gray_g))
+    images_r_diff1 = tf.squared_difference(
+        tf.cast(images_r, dtype=tf.float32),
+        tf.cast(images_gray_r, dtype=tf.float32))
+    images_r_diff2 = tf.squared_difference(
+        tf.cast(images_gray_r, dtype=tf.float32),
+        tf.cast(images_gray_g, dtype=tf.float32))
     images_r_diff = tf.multiply(images_r_diff1, images_r_diff2)
-    images_g_diff1 = tf.squared_difference(tf.to_float(images_g),
-                                           tf.to_float(images_gray_g))
-    images_g_diff2 = tf.squared_difference(tf.to_float(images_gray_g),
-                                           tf.to_float(images_gray_b))
+    images_g_diff1 = tf.squared_difference(
+        tf.cast(images_g, dtype=tf.float32),
+        tf.cast(images_gray_g, dtype=tf.float32))
+    images_g_diff2 = tf.squared_difference(
+        tf.cast(images_gray_g, dtype=tf.float32),
+        tf.cast(images_gray_b, dtype=tf.float32))
     images_g_diff = tf.multiply(images_g_diff1, images_g_diff2)
-    images_b_diff1 = tf.squared_difference(tf.to_float(images_b),
-                                           tf.to_float(images_gray_b))
-    images_b_diff2 = tf.squared_difference(tf.to_float(images_gray_b),
-                                           tf.to_float(images_gray_r))
+    images_b_diff1 = tf.squared_difference(
+        tf.cast(images_b, dtype=tf.float32),
+        tf.cast(images_gray_b, dtype=tf.float32))
+    images_b_diff2 = tf.squared_difference(
+        tf.cast(images_gray_b, dtype=tf.float32),
+        tf.cast(images_gray_r, dtype=tf.float32))
     images_b_diff = tf.multiply(images_b_diff1, images_b_diff2)
     image_zero1 = tf.constant(0, dtype=tf.float32, shape=[1, 4, 4, 1])
     with self.test_session() as sess:
@@ -2135,7 +2141,7 @@ class PreprocessorTest(tf.test.TestCase):
     boxes = self.createTestBoxes()
     labels = self.createTestLabels()
     tensor_dict = {
-        fields.InputDataFields.image: tf.to_float(images),
+        fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
         fields.InputDataFields.groundtruth_boxes: boxes,
         fields.InputDataFields.groundtruth_classes: labels,
     }
@@ -2856,7 +2862,7 @@ class PreprocessorTest(tf.test.TestCase):
     scores = self.createTestMultiClassScores()
 
     tensor_dict = {
-        fields.InputDataFields.image: tf.to_float(images),
+        fields.InputDataFields.image: tf.cast(images, dtype=tf.float32),
         fields.InputDataFields.groundtruth_boxes: boxes,
         fields.InputDataFields.groundtruth_classes: labels,
         fields.InputDataFields.groundtruth_weights: weights,
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index c4f9fef0..de11848c 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -109,6 +109,8 @@ class DetectionResultFields(object):
     key: unique key corresponding to image.
     detection_boxes: coordinates of the detection boxes in the image.
     detection_scores: detection scores for the detection boxes in the image.
+    detection_multiclass_scores: class score distribution (including background)
+      for detection boxes in the image including background class.
     detection_classes: detection-level class labels.
     detection_masks: contains a segmentation mask for each detection box.
     detection_boundaries: contains an object boundary for each detection box.
@@ -123,6 +125,7 @@ class DetectionResultFields(object):
   key = 'key'
   detection_boxes = 'detection_boxes'
   detection_scores = 'detection_scores'
+  detection_multiclass_scores = 'detection_multiclass_scores'
   detection_classes = 'detection_classes'
   detection_masks = 'detection_masks'
   detection_boundaries = 'detection_boundaries'
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 24254174..64ba0065 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -660,16 +660,16 @@ def batch_assign_confidences(target_assigner,
     explicit_example_mask = tf.logical_or(positive_mask, negative_mask)
     positive_anchors = tf.reduce_any(positive_mask, axis=-1)
 
-    regression_weights = tf.to_float(positive_anchors)
+    regression_weights = tf.cast(positive_anchors, dtype=tf.float32)
     regression_targets = (
         reg_targets * tf.expand_dims(regression_weights, axis=-1))
     regression_weights_expanded = tf.expand_dims(regression_weights, axis=-1)
 
     cls_targets_without_background = (
-        cls_targets_without_background * (1 - tf.to_float(negative_mask)))
-    cls_weights_without_background = (
-        (1 - implicit_class_weight) * tf.to_float(explicit_example_mask)
-        + implicit_class_weight)
+        cls_targets_without_background *
+        (1 - tf.cast(negative_mask, dtype=tf.float32)))
+    cls_weights_without_background = ((1 - implicit_class_weight) * tf.cast(
+        explicit_example_mask, dtype=tf.float32) + implicit_class_weight)
 
     if include_background_class:
       cls_weights_background = (
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 83c56b9b..80048589 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -59,8 +59,15 @@ class _ClassTensorHandler(slim_example_decoder.Tensor):
         label_map_proto_file, use_display_name=False)
     # We use a default_value of -1, but we expect all labels to be contained
     # in the label map.
-    name_to_id_table = tf.contrib.lookup.HashTable(
-        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
+    try:
+      # Dynamically try to load the tf v2 lookup, falling back to contrib
+      lookup = tf.compat.v2.lookup
+      hash_table_class = tf.compat.v2.lookup.StaticHashTable
+    except AttributeError:
+      lookup = tf.contrib.lookup
+      hash_table_class = tf.contrib.lookup.HashTable
+    name_to_id_table = hash_table_class(
+        initializer=lookup.KeyValueTensorInitializer(
             keys=tf.constant(list(name_to_id.keys())),
             values=tf.constant(list(name_to_id.values()), dtype=tf.int64)),
         default_value=-1)
@@ -68,8 +75,8 @@ class _ClassTensorHandler(slim_example_decoder.Tensor):
         label_map_proto_file, use_display_name=True)
     # We use a default_value of -1, but we expect all labels to be contained
     # in the label map.
-    display_name_to_id_table = tf.contrib.lookup.HashTable(
-        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
+    display_name_to_id_table = hash_table_class(
+        initializer=lookup.KeyValueTensorInitializer(
             keys=tf.constant(list(display_name_to_id.keys())),
             values=tf.constant(
                 list(display_name_to_id.values()), dtype=tf.int64)),
@@ -444,7 +451,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     masks = keys_to_tensors['image/object/mask']
     if isinstance(masks, tf.SparseTensor):
       masks = tf.sparse_tensor_to_dense(masks)
-    masks = tf.reshape(tf.to_float(tf.greater(masks, 0.0)), to_shape)
+    masks = tf.reshape(
+        tf.cast(tf.greater(masks, 0.0), dtype=tf.float32), to_shape)
     return tf.cast(masks, tf.float32)
 
   def _decode_png_instance_masks(self, keys_to_tensors):
@@ -465,7 +473,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       image = tf.squeeze(
           tf.image.decode_image(image_buffer, channels=1), axis=2)
       image.set_shape([None, None])
-      image = tf.to_float(tf.greater(image, 0))
+      image = tf.cast(tf.greater(image, 0), dtype=tf.float32)
       return image
 
     png_masks = keys_to_tensors['image/object/mask']
@@ -476,4 +484,4 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     return tf.cond(
         tf.greater(tf.size(png_masks), 0),
         lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),
-        lambda: tf.zeros(tf.to_int32(tf.stack([0, height, width]))))
+        lambda: tf.zeros(tf.cast(tf.stack([0, height, width]), dtype=tf.int32)))
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index a88fbc7b..a682de74 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -44,10 +44,15 @@ EVAL_METRICS_CLASS_DICT = {
         coco_evaluation.CocoMaskEvaluator,
     'oid_challenge_detection_metrics':
         object_detection_evaluation.OpenImagesDetectionChallengeEvaluator,
+    'oid_challenge_segmentation_metrics':
+        object_detection_evaluation
+        .OpenImagesInstanceSegmentationChallengeEvaluator,
     'pascal_voc_detection_metrics':
         object_detection_evaluation.PascalDetectionEvaluator,
     'weighted_pascal_voc_detection_metrics':
         object_detection_evaluation.WeightedPascalDetectionEvaluator,
+    'precision_at_recall_detection_metrics':
+        object_detection_evaluation.PrecisionAtRecallDetectionEvaluator,
     'pascal_voc_instance_segmentation_metrics':
         object_detection_evaluation.PascalInstanceSegmentationEvaluator,
     'weighted_pascal_voc_instance_segmentation_metrics':
@@ -776,7 +781,8 @@ def result_dict_for_batched_example(images,
   detection_fields = fields.DetectionResultFields
   detection_boxes = detections[detection_fields.detection_boxes]
   detection_scores = detections[detection_fields.detection_scores]
-  num_detections = tf.to_int32(detections[detection_fields.num_detections])
+  num_detections = tf.cast(detections[detection_fields.num_detections],
+                           dtype=tf.int32)
 
   if class_agnostic:
     detection_classes = tf.ones_like(detection_scores, dtype=tf.int64)
@@ -939,4 +945,9 @@ def evaluator_options_from_eval_config(eval_config):
           'include_metrics_per_category': (
               eval_config.include_metrics_per_category)
       }
+    elif eval_metric_fn_key == 'precision_at_recall_detection_metrics':
+      evaluator_options[eval_metric_fn_key] = {
+          'recall_lower_bound': (eval_config.recall_lower_bound),
+          'recall_upper_bound': (eval_config.recall_upper_bound)
+      }
   return evaluator_options
diff --git a/research/object_detection/eval_util_test.py b/research/object_detection/eval_util_test.py
index 98f5ab75..ee46e48f 100644
--- a/research/object_detection/eval_util_test.py
+++ b/research/object_detection/eval_util_test.py
@@ -31,9 +31,9 @@ from object_detection.utils import test_case
 class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
 
   def _get_categories_list(self):
-    return [{'id': 0, 'name': 'person'},
-            {'id': 1, 'name': 'dog'},
-            {'id': 2, 'name': 'cat'}]
+    return [{'id': 1, 'name': 'person'},
+            {'id': 2, 'name': 'dog'},
+            {'id': 3, 'name': 'cat'}]
 
   def _make_evaluation_dict(self,
                             resized_groundtruth_masks=False,
@@ -192,43 +192,66 @@ class EvalUtilTest(test_case.TestCase, parameterized.TestCase):
 
   def test_get_eval_metric_ops_for_evaluators(self):
     eval_config = eval_pb2.EvalConfig()
-    eval_config.metrics_set.extend(
-        ['coco_detection_metrics', 'coco_mask_metrics'])
+    eval_config.metrics_set.extend([
+        'coco_detection_metrics', 'coco_mask_metrics',
+        'precision_at_recall_detection_metrics'
+    ])
     eval_config.include_metrics_per_category = True
+    eval_config.recall_lower_bound = 0.2
+    eval_config.recall_upper_bound = 0.6
 
     evaluator_options = eval_util.evaluator_options_from_eval_config(
         eval_config)
-    self.assertTrue(evaluator_options['coco_detection_metrics'][
-        'include_metrics_per_category'])
-    self.assertTrue(evaluator_options['coco_mask_metrics'][
-        'include_metrics_per_category'])
+    self.assertTrue(evaluator_options['coco_detection_metrics']
+                    ['include_metrics_per_category'])
+    self.assertTrue(
+        evaluator_options['coco_mask_metrics']['include_metrics_per_category'])
+    self.assertAlmostEqual(
+        evaluator_options['precision_at_recall_detection_metrics']
+        ['recall_lower_bound'], eval_config.recall_lower_bound)
+    self.assertAlmostEqual(
+        evaluator_options['precision_at_recall_detection_metrics']
+        ['recall_upper_bound'], eval_config.recall_upper_bound)
 
   def test_get_evaluator_with_evaluator_options(self):
     eval_config = eval_pb2.EvalConfig()
-    eval_config.metrics_set.extend(['coco_detection_metrics'])
+    eval_config.metrics_set.extend(
+        ['coco_detection_metrics', 'precision_at_recall_detection_metrics'])
     eval_config.include_metrics_per_category = True
+    eval_config.recall_lower_bound = 0.2
+    eval_config.recall_upper_bound = 0.6
     categories = self._get_categories_list()
 
     evaluator_options = eval_util.evaluator_options_from_eval_config(
         eval_config)
-    evaluator = eval_util.get_evaluators(
-        eval_config, categories, evaluator_options)
+    evaluator = eval_util.get_evaluators(eval_config, categories,
+                                         evaluator_options)
 
     self.assertTrue(evaluator[0]._include_metrics_per_category)
+    self.assertAlmostEqual(evaluator[1]._recall_lower_bound,
+                           eval_config.recall_lower_bound)
+    self.assertAlmostEqual(evaluator[1]._recall_upper_bound,
+                           eval_config.recall_upper_bound)
 
   def test_get_evaluator_with_no_evaluator_options(self):
     eval_config = eval_pb2.EvalConfig()
-    eval_config.metrics_set.extend(['coco_detection_metrics'])
+    eval_config.metrics_set.extend(
+        ['coco_detection_metrics', 'precision_at_recall_detection_metrics'])
     eval_config.include_metrics_per_category = True
+    eval_config.recall_lower_bound = 0.2
+    eval_config.recall_upper_bound = 0.6
     categories = self._get_categories_list()
 
     evaluator = eval_util.get_evaluators(
         eval_config, categories, evaluator_options=None)
 
     # Even though we are setting eval_config.include_metrics_per_category = True
-    # this option is never passed into the DetectionEvaluator constructor (via
-    # `evaluator_options`).
+    # and bounds on recall, these options are never passed into the
+    # DetectionEvaluator constructor (via `evaluator_options`).
     self.assertFalse(evaluator[0]._include_metrics_per_category)
+    self.assertAlmostEqual(evaluator[1]._recall_lower_bound, 0.0)
+    self.assertAlmostEqual(evaluator[1]._recall_upper_bound, 1.0)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/export_tflite_ssd_graph.py b/research/object_detection/export_tflite_ssd_graph.py
index b7ed428d..2cce3865 100644
--- a/research/object_detection/export_tflite_ssd_graph.py
+++ b/research/object_detection/export_tflite_ssd_graph.py
@@ -106,7 +106,7 @@ flags.DEFINE_string('trained_checkpoint_prefix', None, 'Checkpoint prefix.')
 flags.DEFINE_integer('max_detections', 10,
                      'Maximum number of detections (boxes) to show.')
 flags.DEFINE_integer('max_classes_per_detection', 1,
-                     'Number of classes to display per detection box.')
+                     'Maximum number of classes to output per detection box.')
 flags.DEFINE_integer(
     'detections_per_class', 100,
     'Number of anchors used per class in Regular Non-Max-Suppression.')
@@ -136,7 +136,7 @@ def main(argv):
   export_tflite_ssd_graph_lib.export_tflite_graph(
       pipeline_config, FLAGS.trained_checkpoint_prefix, FLAGS.output_directory,
       FLAGS.add_postprocessing_op, FLAGS.max_detections,
-      FLAGS.max_classes_per_detection, FLAGS.use_regular_nms)
+      FLAGS.max_classes_per_detection, use_regular_nms=FLAGS.use_regular_nms)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 79e6bc09..54e105b8 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -176,6 +176,9 @@ def add_output_tensor_nodes(postprocessed_tensors,
       containing detected boxes.
     * detection_scores: float32 tensor of shape [batch_size, num_boxes]
       containing scores for the detected boxes.
+    * detection_multiclass_scores: (Optional) float32 tensor of shape
+      [batch_size, num_boxes, num_classes_with_background] for containing class
+      score distribution for detected boxes including background if any.
     * detection_classes: float32 tensor of shape [batch_size, num_boxes]
       containing class predictions for the detected boxes.
     * detection_keypoints: (Optional) float32 tensor of shape
@@ -189,6 +192,8 @@ def add_output_tensor_nodes(postprocessed_tensors,
     postprocessed_tensors: a dictionary containing the following fields
       'detection_boxes': [batch, max_detections, 4]
       'detection_scores': [batch, max_detections]
+      'detection_multiclass_scores': [batch, max_detections,
+        num_classes_with_background]
       'detection_classes': [batch, max_detections]
       'detection_masks': [batch, max_detections, mask_height, mask_width]
         (optional).
@@ -204,6 +209,8 @@ def add_output_tensor_nodes(postprocessed_tensors,
   label_id_offset = 1
   boxes = postprocessed_tensors.get(detection_fields.detection_boxes)
   scores = postprocessed_tensors.get(detection_fields.detection_scores)
+  multiclass_scores = postprocessed_tensors.get(
+      detection_fields.detection_multiclass_scores)
   raw_boxes = postprocessed_tensors.get(detection_fields.raw_detection_boxes)
   raw_scores = postprocessed_tensors.get(detection_fields.raw_detection_scores)
   classes = postprocessed_tensors.get(
@@ -216,6 +223,9 @@ def add_output_tensor_nodes(postprocessed_tensors,
       boxes, name=detection_fields.detection_boxes)
   outputs[detection_fields.detection_scores] = tf.identity(
       scores, name=detection_fields.detection_scores)
+  if multiclass_scores is not None:
+    outputs[detection_fields.detection_multiclass_scores] = tf.identity(
+        multiclass_scores, name=detection_fields.detection_multiclass_scores)
   outputs[detection_fields.detection_classes] = tf.identity(
       classes, name=detection_fields.detection_classes)
   outputs[detection_fields.num_detections] = tf.identity(
@@ -306,7 +316,7 @@ def write_graph_and_checkpoint(inference_graph_def,
 
 def _get_outputs_from_inputs(input_tensors, detection_model,
                              output_collection_name):
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
   output_tensors = detection_model.predict(
       preprocessed_inputs, true_image_shapes)
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index c647e44d..877726f8 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -59,6 +59,9 @@ class FakeModel(model.DetectionModel):
                                            [0.0, 0.0, 0.0, 0.0]]], tf.float32),
           'detection_scores': tf.constant([[0.7, 0.6],
                                            [0.9, 0.0]], tf.float32),
+          'detection_multiclass_scores': tf.constant([[[0.3, 0.7], [0.4, 0.6]],
+                                                      [[0.1, 0.9], [0.0, 0.0]]],
+                                                     tf.float32),
           'detection_classes': tf.constant([[0, 1],
                                             [1, 0]], tf.float32),
           'num_detections': tf.constant([2, 1], tf.float32),
@@ -371,6 +374,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       inference_graph.get_tensor_by_name('image_tensor:0')
       inference_graph.get_tensor_by_name('detection_boxes:0')
       inference_graph.get_tensor_by_name('detection_scores:0')
+      inference_graph.get_tensor_by_name('detection_multiclass_scores:0')
       inference_graph.get_tensor_by_name('detection_classes:0')
       inference_graph.get_tensor_by_name('detection_keypoints:0')
       inference_graph.get_tensor_by_name('detection_masks:0')
@@ -398,6 +402,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       inference_graph.get_tensor_by_name('image_tensor:0')
       inference_graph.get_tensor_by_name('detection_boxes:0')
       inference_graph.get_tensor_by_name('detection_scores:0')
+      inference_graph.get_tensor_by_name('detection_multiclass_scores:0')
       inference_graph.get_tensor_by_name('detection_classes:0')
       inference_graph.get_tensor_by_name('num_detections:0')
       with self.assertRaises(KeyError):
@@ -491,15 +496,20 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           'encoded_image_string_tensor:0')
       boxes = inference_graph.get_tensor_by_name('detection_boxes:0')
       scores = inference_graph.get_tensor_by_name('detection_scores:0')
+      multiclass_scores = inference_graph.get_tensor_by_name(
+          'detection_multiclass_scores:0')
       classes = inference_graph.get_tensor_by_name('detection_classes:0')
       keypoints = inference_graph.get_tensor_by_name('detection_keypoints:0')
       masks = inference_graph.get_tensor_by_name('detection_masks:0')
       num_detections = inference_graph.get_tensor_by_name('num_detections:0')
       for image_str in [jpg_image_str, png_image_str]:
         image_str_batch_np = np.hstack([image_str]* 2)
-        (boxes_np, scores_np, classes_np, keypoints_np, masks_np,
-         num_detections_np) = sess.run(
-             [boxes, scores, classes, keypoints, masks, num_detections],
+        (boxes_np, scores_np, multiclass_scores_np, classes_np, keypoints_np,
+         masks_np, num_detections_np) = sess.run(
+             [
+                 boxes, scores, multiclass_scores, classes, keypoints, masks,
+                 num_detections
+             ],
              feed_dict={image_str_tensor: image_str_batch_np})
         self.assertAllClose(boxes_np, [[[0.0, 0.0, 0.5, 0.5],
                                         [0.5, 0.5, 0.8, 0.8]],
@@ -507,6 +517,8 @@ class ExportInferenceGraphTest(tf.test.TestCase):
                                         [0.0, 0.0, 0.0, 0.0]]])
         self.assertAllClose(scores_np, [[0.7, 0.6],
                                         [0.9, 0.0]])
+        self.assertAllClose(multiclass_scores_np, [[[0.3, 0.7], [0.4, 0.6]],
+                                                   [[0.1, 0.9], [0.0, 0.0]]])
         self.assertAllClose(classes_np, [[1, 2],
                                          [2, 1]])
         self.assertAllClose(keypoints_np, np.arange(48).reshape([2, 2, 6, 2]))
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 981913c6..e8244870 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -127,7 +127,7 @@ def transform_input_data(tensor_dict,
   # Apply model preprocessing ops and resize instance masks.
   image = tensor_dict[fields.InputDataFields.image]
   preprocessed_resized_image, true_image_shape = model_preprocess_fn(
-      tf.expand_dims(tf.to_float(image), axis=0))
+      tf.expand_dims(tf.cast(image, dtype=tf.float32), axis=0))
   if use_bfloat16:
     preprocessed_resized_image = tf.cast(
         preprocessed_resized_image, tf.bfloat16)
@@ -219,14 +219,15 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
 
   num_additional_channels = 0
   if fields.InputDataFields.image_additional_channels in tensor_dict:
-    num_additional_channels = tensor_dict[
-        fields.InputDataFields.image_additional_channels].shape[2].value
+    num_additional_channels = shape_utils.get_dim_as_int(tensor_dict[
+        fields.InputDataFields.image_additional_channels].shape[2])
 
   # We assume that if num_additional_channels > 0, then it has already been
   # concatenated to the base image (but not the ground truth).
   num_channels = 3
   if fields.InputDataFields.image in tensor_dict:
-    num_channels = tensor_dict[fields.InputDataFields.image].shape[2].value
+    num_channels = shape_utils.get_dim_as_int(
+        tensor_dict[fields.InputDataFields.image].shape[2])
 
   if num_additional_channels:
     if num_additional_channels >= num_channels:
@@ -234,7 +235,8 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
           'Image must be already concatenated with additional channels.')
 
     if (fields.InputDataFields.original_image in tensor_dict and
-        tensor_dict[fields.InputDataFields.original_image].shape[2].value ==
+        shape_utils.get_dim_as_int(
+            tensor_dict[fields.InputDataFields.original_image].shape[2]) ==
         num_channels):
       raise ValueError(
           'Image must be already concatenated with additional channels.')
@@ -273,19 +275,21 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
 
   if fields.InputDataFields.original_image in tensor_dict:
     padding_shapes[fields.InputDataFields.original_image] = [
-        height, width, tensor_dict[fields.InputDataFields.
-                                   original_image].shape[2].value
+        height, width,
+        shape_utils.get_dim_as_int(tensor_dict[fields.InputDataFields.
+                                               original_image].shape[2])
     ]
   if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
     tensor_shape = (
         tensor_dict[fields.InputDataFields.groundtruth_keypoints].shape)
-    padding_shape = [max_num_boxes, tensor_shape[1].value,
-                     tensor_shape[2].value]
+    padding_shape = [max_num_boxes,
+                     shape_utils.get_dim_as_int(tensor_shape[1]),
+                     shape_utils.get_dim_as_int(tensor_shape[2])]
     padding_shapes[fields.InputDataFields.groundtruth_keypoints] = padding_shape
   if fields.InputDataFields.groundtruth_keypoint_visibilities in tensor_dict:
     tensor_shape = tensor_dict[fields.InputDataFields.
                                groundtruth_keypoint_visibilities].shape
-    padding_shape = [max_num_boxes, tensor_shape[1].value]
+    padding_shape = [max_num_boxes, shape_utils.get_dim_as_int(tensor_shape[1])]
     padding_shapes[fields.InputDataFields.
                    groundtruth_keypoint_visibilities] = padding_shape
 
@@ -318,7 +322,7 @@ def augment_input_data(tensor_dict, data_augmentation_options):
     input tensor dictionary.
   """
   tensor_dict[fields.InputDataFields.image] = tf.expand_dims(
-      tf.to_float(tensor_dict[fields.InputDataFields.image]), 0)
+      tf.cast(tensor_dict[fields.InputDataFields.image], dtype=tf.float32), 0)
 
   include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks
                             in tensor_dict)
@@ -438,97 +442,112 @@ def create_train_input_fn(train_config, train_input_config,
   """
 
   def _train_input_fn(params=None):
-    """Returns `features` and `labels` tensor dictionaries for training.
+    return train_input(train_config, train_input_config, model_config,
+                       params=params)
 
-    Args:
-      params: Parameter dictionary passed from the estimator.
+  return _train_input_fn
 
-    Returns:
-      A tf.data.Dataset that holds (features, labels) tuple.
-
-      features: Dictionary of feature tensors.
-        features[fields.InputDataFields.image] is a [batch_size, H, W, C]
-          float32 tensor with preprocessed images.
-        features[HASH_KEY] is a [batch_size] int32 tensor representing unique
-          identifiers for the images.
-        features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]
-          int32 tensor representing the true image shapes, as preprocessed
-          images could be padded.
-        features[fields.InputDataFields.original_image] (optional) is a
-          [batch_size, H, W, C] float32 tensor with original images.
-      labels: Dictionary of groundtruth tensors.
-        labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]
-          int32 tensor indicating the number of groundtruth boxes.
-        labels[fields.InputDataFields.groundtruth_boxes] is a
-          [batch_size, num_boxes, 4] float32 tensor containing the corners of
-          the groundtruth boxes.
-        labels[fields.InputDataFields.groundtruth_classes] is a
-          [batch_size, num_boxes, num_classes] float32 one-hot tensor of
-          classes.
-        labels[fields.InputDataFields.groundtruth_weights] is a
-          [batch_size, num_boxes] float32 tensor containing groundtruth weights
-          for the boxes.
-        -- Optional --
-        labels[fields.InputDataFields.groundtruth_instance_masks] is a
-          [batch_size, num_boxes, H, W] float32 tensor containing only binary
-          values, which represent instance masks for objects.
-        labels[fields.InputDataFields.groundtruth_keypoints] is a
-          [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing
-          keypoints for each box.
-
-    Raises:
-      TypeError: if the `train_config`, `train_input_config` or `model_config`
-        are not of the correct type.
-    """
-    if not isinstance(train_config, train_pb2.TrainConfig):
-      raise TypeError('For training mode, the `train_config` must be a '
-                      'train_pb2.TrainConfig.')
-    if not isinstance(train_input_config, input_reader_pb2.InputReader):
-      raise TypeError('The `train_input_config` must be a '
-                      'input_reader_pb2.InputReader.')
-    if not isinstance(model_config, model_pb2.DetectionModel):
-      raise TypeError('The `model_config` must be a '
-                      'model_pb2.DetectionModel.')
-
-    def transform_and_pad_input_data_fn(tensor_dict):
-      """Combines transform and pad operation."""
-      data_augmentation_options = [
-          preprocessor_builder.build(step)
-          for step in train_config.data_augmentation_options
-      ]
-      data_augmentation_fn = functools.partial(
-          augment_input_data,
-          data_augmentation_options=data_augmentation_options)
-
-      model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
-          model_config, is_training=True).preprocess
-      image_resizer_config = config_util.get_image_resizer_config(model_config)
-      image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-      transform_data_fn = functools.partial(
-          transform_input_data, model_preprocess_fn=model_preprocess_fn,
-          image_resizer_fn=image_resizer_fn,
-          num_classes=config_util.get_number_of_classes(model_config),
-          data_augmentation_fn=data_augmentation_fn,
-          merge_multiple_boxes=train_config.merge_multiple_label_boxes,
-          retain_original_image=train_config.retain_original_images,
-          use_multiclass_scores=train_config.use_multiclass_scores,
-          use_bfloat16=train_config.use_bfloat16)
-
-      tensor_dict = pad_input_data_to_static_shapes(
-          tensor_dict=transform_data_fn(tensor_dict),
-          max_num_boxes=train_input_config.max_number_of_boxes,
-          num_classes=config_util.get_number_of_classes(model_config),
-          spatial_image_shape=config_util.get_spatial_image_size(
-              image_resizer_config))
-      return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
-
-    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
-        train_input_config,
-        transform_input_data_fn=transform_and_pad_input_data_fn,
-        batch_size=params['batch_size'] if params else train_config.batch_size)
-    return dataset
 
-  return _train_input_fn
+def train_input(train_config, train_input_config,
+                model_config, model=None, params=None):
+  """Returns `features` and `labels` tensor dictionaries for training.
+
+  Args:
+    train_config: A train_pb2.TrainConfig.
+    train_input_config: An input_reader_pb2.InputReader.
+    model_config: A model_pb2.DetectionModel.
+    model: A pre-constructed Detection Model.
+      If None, one will be created from the config.
+    params: Parameter dictionary passed from the estimator.
+
+  Returns:
+    A tf.data.Dataset that holds (features, labels) tuple.
+
+    features: Dictionary of feature tensors.
+      features[fields.InputDataFields.image] is a [batch_size, H, W, C]
+        float32 tensor with preprocessed images.
+      features[HASH_KEY] is a [batch_size] int32 tensor representing unique
+        identifiers for the images.
+      features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]
+        int32 tensor representing the true image shapes, as preprocessed
+        images could be padded.
+      features[fields.InputDataFields.original_image] (optional) is a
+        [batch_size, H, W, C] float32 tensor with original images.
+    labels: Dictionary of groundtruth tensors.
+      labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]
+        int32 tensor indicating the number of groundtruth boxes.
+      labels[fields.InputDataFields.groundtruth_boxes] is a
+        [batch_size, num_boxes, 4] float32 tensor containing the corners of
+        the groundtruth boxes.
+      labels[fields.InputDataFields.groundtruth_classes] is a
+        [batch_size, num_boxes, num_classes] float32 one-hot tensor of
+        classes.
+      labels[fields.InputDataFields.groundtruth_weights] is a
+        [batch_size, num_boxes] float32 tensor containing groundtruth weights
+        for the boxes.
+      -- Optional --
+      labels[fields.InputDataFields.groundtruth_instance_masks] is a
+        [batch_size, num_boxes, H, W] float32 tensor containing only binary
+        values, which represent instance masks for objects.
+      labels[fields.InputDataFields.groundtruth_keypoints] is a
+        [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing
+        keypoints for each box.
+
+  Raises:
+    TypeError: if the `train_config`, `train_input_config` or `model_config`
+      are not of the correct type.
+  """
+  if not isinstance(train_config, train_pb2.TrainConfig):
+    raise TypeError('For training mode, the `train_config` must be a '
+                    'train_pb2.TrainConfig.')
+  if not isinstance(train_input_config, input_reader_pb2.InputReader):
+    raise TypeError('The `train_input_config` must be a '
+                    'input_reader_pb2.InputReader.')
+  if not isinstance(model_config, model_pb2.DetectionModel):
+    raise TypeError('The `model_config` must be a '
+                    'model_pb2.DetectionModel.')
+
+  if model is None:
+    model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
+        model_config, is_training=True).preprocess
+  else:
+    model_preprocess_fn = model.preprocess
+
+  def transform_and_pad_input_data_fn(tensor_dict):
+    """Combines transform and pad operation."""
+    data_augmentation_options = [
+        preprocessor_builder.build(step)
+        for step in train_config.data_augmentation_options
+    ]
+    data_augmentation_fn = functools.partial(
+        augment_input_data,
+        data_augmentation_options=data_augmentation_options)
+
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+    transform_data_fn = functools.partial(
+        transform_input_data, model_preprocess_fn=model_preprocess_fn,
+        image_resizer_fn=image_resizer_fn,
+        num_classes=config_util.get_number_of_classes(model_config),
+        data_augmentation_fn=data_augmentation_fn,
+        merge_multiple_boxes=train_config.merge_multiple_label_boxes,
+        retain_original_image=train_config.retain_original_images,
+        use_multiclass_scores=train_config.use_multiclass_scores,
+        use_bfloat16=train_config.use_bfloat16)
+
+    tensor_dict = pad_input_data_to_static_shapes(
+        tensor_dict=transform_data_fn(tensor_dict),
+        max_num_boxes=train_input_config.max_number_of_boxes,
+        num_classes=config_util.get_number_of_classes(model_config),
+        spatial_image_shape=config_util.get_spatial_image_size(
+            image_resizer_config))
+    return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
+
+  dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
+      train_input_config,
+      transform_input_data_fn=transform_and_pad_input_data_fn,
+      batch_size=params['batch_size'] if params else train_config.batch_size)
+  return dataset
 
 
 def create_eval_input_fn(eval_config, eval_input_config, model_config):
@@ -544,84 +563,99 @@ def create_eval_input_fn(eval_config, eval_input_config, model_config):
   """
 
   def _eval_input_fn(params=None):
-    """Returns `features` and `labels` tensor dictionaries for evaluation.
+    return eval_input(eval_config, eval_input_config, model_config,
+                      params=params)
 
-    Args:
-      params: Parameter dictionary passed from the estimator.
+  return _eval_input_fn
 
-    Returns:
-      A tf.data.Dataset that holds (features, labels) tuple.
-
-      features: Dictionary of feature tensors.
-        features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor
-          with preprocessed images.
-        features[HASH_KEY] is a [1] int32 tensor representing unique
-          identifiers for the images.
-        features[fields.InputDataFields.true_image_shape] is a [1, 3]
-          int32 tensor representing the true image shapes, as preprocessed
-          images could be padded.
-        features[fields.InputDataFields.original_image] is a [1, H', W', C]
-          float32 tensor with the original image.
-      labels: Dictionary of groundtruth tensors.
-        labels[fields.InputDataFields.groundtruth_boxes] is a [1, num_boxes, 4]
-          float32 tensor containing the corners of the groundtruth boxes.
-        labels[fields.InputDataFields.groundtruth_classes] is a
-          [num_boxes, num_classes] float32 one-hot tensor of classes.
-        labels[fields.InputDataFields.groundtruth_area] is a [1, num_boxes]
-          float32 tensor containing object areas.
-        labels[fields.InputDataFields.groundtruth_is_crowd] is a [1, num_boxes]
-          bool tensor indicating if the boxes enclose a crowd.
-        labels[fields.InputDataFields.groundtruth_difficult] is a [1, num_boxes]
-          int32 tensor indicating if the boxes represent difficult instances.
-        -- Optional --
-        labels[fields.InputDataFields.groundtruth_instance_masks] is a
-          [1, num_boxes, H, W] float32 tensor containing only binary values,
-          which represent instance masks for objects.
-
-    Raises:
-      TypeError: if the `eval_config`, `eval_input_config` or `model_config`
-        are not of the correct type.
-    """
-    params = params or {}
-    if not isinstance(eval_config, eval_pb2.EvalConfig):
-      raise TypeError('For eval mode, the `eval_config` must be a '
-                      'train_pb2.EvalConfig.')
-    if not isinstance(eval_input_config, input_reader_pb2.InputReader):
-      raise TypeError('The `eval_input_config` must be a '
-                      'input_reader_pb2.InputReader.')
-    if not isinstance(model_config, model_pb2.DetectionModel):
-      raise TypeError('The `model_config` must be a '
-                      'model_pb2.DetectionModel.')
-
-    def transform_and_pad_input_data_fn(tensor_dict):
-      """Combines transform and pad operation."""
-      num_classes = config_util.get_number_of_classes(model_config)
-      model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
-          model_config, is_training=False).preprocess
-
-      image_resizer_config = config_util.get_image_resizer_config(model_config)
-      image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-
-      transform_data_fn = functools.partial(
-          transform_input_data, model_preprocess_fn=model_preprocess_fn,
-          image_resizer_fn=image_resizer_fn,
-          num_classes=num_classes,
-          data_augmentation_fn=None,
-          retain_original_image=eval_config.retain_original_images)
-      tensor_dict = pad_input_data_to_static_shapes(
-          tensor_dict=transform_data_fn(tensor_dict),
-          max_num_boxes=eval_input_config.max_number_of_boxes,
-          num_classes=config_util.get_number_of_classes(model_config),
-          spatial_image_shape=config_util.get_spatial_image_size(
-              image_resizer_config))
-      return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
-    dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
-        eval_input_config,
-        batch_size=params['batch_size'] if params else eval_config.batch_size,
-        transform_input_data_fn=transform_and_pad_input_data_fn)
-    return dataset
 
-  return _eval_input_fn
+def eval_input(eval_config, eval_input_config, model_config,
+               model=None, params=None):
+  """Returns `features` and `labels` tensor dictionaries for evaluation.
+
+  Args:
+    eval_config: An eval_pb2.EvalConfig.
+    eval_input_config: An input_reader_pb2.InputReader.
+    model_config: A model_pb2.DetectionModel.
+    model: A pre-constructed Detection Model.
+      If None, one will be created from the config.
+    params: Parameter dictionary passed from the estimator.
+
+  Returns:
+    A tf.data.Dataset that holds (features, labels) tuple.
+
+    features: Dictionary of feature tensors.
+      features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor
+        with preprocessed images.
+      features[HASH_KEY] is a [1] int32 tensor representing unique
+        identifiers for the images.
+      features[fields.InputDataFields.true_image_shape] is a [1, 3]
+        int32 tensor representing the true image shapes, as preprocessed
+        images could be padded.
+      features[fields.InputDataFields.original_image] is a [1, H', W', C]
+        float32 tensor with the original image.
+    labels: Dictionary of groundtruth tensors.
+      labels[fields.InputDataFields.groundtruth_boxes] is a [1, num_boxes, 4]
+        float32 tensor containing the corners of the groundtruth boxes.
+      labels[fields.InputDataFields.groundtruth_classes] is a
+        [num_boxes, num_classes] float32 one-hot tensor of classes.
+      labels[fields.InputDataFields.groundtruth_area] is a [1, num_boxes]
+        float32 tensor containing object areas.
+      labels[fields.InputDataFields.groundtruth_is_crowd] is a [1, num_boxes]
+        bool tensor indicating if the boxes enclose a crowd.
+      labels[fields.InputDataFields.groundtruth_difficult] is a [1, num_boxes]
+        int32 tensor indicating if the boxes represent difficult instances.
+      -- Optional --
+      labels[fields.InputDataFields.groundtruth_instance_masks] is a
+        [1, num_boxes, H, W] float32 tensor containing only binary values,
+        which represent instance masks for objects.
+
+  Raises:
+    TypeError: if the `eval_config`, `eval_input_config` or `model_config`
+      are not of the correct type.
+  """
+  params = params or {}
+  if not isinstance(eval_config, eval_pb2.EvalConfig):
+    raise TypeError('For eval mode, the `eval_config` must be a '
+                    'train_pb2.EvalConfig.')
+  if not isinstance(eval_input_config, input_reader_pb2.InputReader):
+    raise TypeError('The `eval_input_config` must be a '
+                    'input_reader_pb2.InputReader.')
+  if not isinstance(model_config, model_pb2.DetectionModel):
+    raise TypeError('The `model_config` must be a '
+                    'model_pb2.DetectionModel.')
+
+  if model is None:
+    model_preprocess_fn = INPUT_BUILDER_UTIL_MAP['model_build'](
+        model_config, is_training=False).preprocess
+  else:
+    model_preprocess_fn = model.preprocess
+
+  def transform_and_pad_input_data_fn(tensor_dict):
+    """Combines transform and pad operation."""
+    num_classes = config_util.get_number_of_classes(model_config)
+
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+
+    transform_data_fn = functools.partial(
+        transform_input_data, model_preprocess_fn=model_preprocess_fn,
+        image_resizer_fn=image_resizer_fn,
+        num_classes=num_classes,
+        data_augmentation_fn=None,
+        retain_original_image=eval_config.retain_original_images)
+    tensor_dict = pad_input_data_to_static_shapes(
+        tensor_dict=transform_data_fn(tensor_dict),
+        max_num_boxes=eval_input_config.max_number_of_boxes,
+        num_classes=config_util.get_number_of_classes(model_config),
+        spatial_image_shape=config_util.get_spatial_image_size(
+            image_resizer_config))
+    return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
+  dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
+      eval_input_config,
+      batch_size=params['batch_size'] if params else eval_config.batch_size,
+      transform_input_data_fn=transform_and_pad_input_data_fn)
+  return dataset
 
 
 def create_predict_input_fn(model_config, predict_input_config):
@@ -664,7 +698,7 @@ def create_predict_input_fn(model_config, predict_input_config):
         load_instance_masks=False,
         num_additional_channels=predict_input_config.num_additional_channels)
     input_dict = transform_fn(decoder.decode(example))
-    images = tf.to_float(input_dict[fields.InputDataFields.image])
+    images = tf.cast(input_dict[fields.InputDataFields.image], dtype=tf.float32)
     images = tf.expand_dims(images, axis=0)
     true_image_shape = tf.expand_dims(
         input_dict[fields.InputDataFields.true_image_shape], axis=0)
diff --git a/research/object_detection/legacy/evaluator.py b/research/object_detection/legacy/evaluator.py
index 10c73a32..ac7565da 100644
--- a/research/object_detection/legacy/evaluator.py
+++ b/research/object_detection/legacy/evaluator.py
@@ -53,6 +53,9 @@ EVAL_METRICS_CLASS_DICT = {
     # DEPRECATED: please use oid_challenge_detection_metrics instead
     'oid_challenge_object_detection_metrics':
         object_detection_evaluation.OpenImagesDetectionChallengeEvaluator,
+    'oid_challenge_segmentation_metrics':
+        object_detection_evaluation
+        .OpenImagesInstanceSegmentationChallengeEvaluator,
 }
 
 EVAL_DEFAULT_METRIC = 'pascal_voc_detection_metrics'
@@ -80,7 +83,7 @@ def _extract_predictions_and_losses(model,
   input_dict = prefetch_queue.dequeue()
   original_image = tf.expand_dims(input_dict[fields.InputDataFields.image], 0)
   preprocessed_image, true_image_shapes = model.preprocess(
-      tf.to_float(original_image))
+      tf.cast(original_image, dtype=tf.float32))
   prediction_dict = model.predict(preprocessed_image, true_image_shapes)
   detections = model.postprocess(prediction_dict, true_image_shapes)
 
diff --git a/research/object_detection/legacy/trainer.py b/research/object_detection/legacy/trainer.py
index a4b46859..8b690812 100644
--- a/research/object_detection/legacy/trainer.py
+++ b/research/object_detection/legacy/trainer.py
@@ -62,7 +62,7 @@ def create_input_queue(batch_size_per_clone, create_tensor_dict_fn,
       tensor_dict[fields.InputDataFields.image], 0)
 
   images = tensor_dict[fields.InputDataFields.image]
-  float_images = tf.to_float(images)
+  float_images = tf.cast(images, dtype=tf.float32)
   tensor_dict[fields.InputDataFields.image] = float_images
 
   include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks
diff --git a/research/object_detection/matchers/argmax_matcher.py b/research/object_detection/matchers/argmax_matcher.py
index 07e22403..2ddbfc34 100644
--- a/research/object_detection/matchers/argmax_matcher.py
+++ b/research/object_detection/matchers/argmax_matcher.py
@@ -184,7 +184,7 @@ class ArgMaxMatcher(matcher.Matcher):
         return matches
 
     if similarity_matrix.shape.is_fully_defined():
-      if similarity_matrix.shape[0].value == 0:
+      if shape_utils.get_dim_as_int(similarity_matrix.shape[0]) == 0:
         return _match_when_rows_are_empty()
       else:
         return _match_when_rows_are_non_empty()
diff --git a/research/object_detection/matchers/bipartite_matcher.py b/research/object_detection/matchers/bipartite_matcher.py
index 56cff85a..ec27baf4 100644
--- a/research/object_detection/matchers/bipartite_matcher.py
+++ b/research/object_detection/matchers/bipartite_matcher.py
@@ -62,7 +62,7 @@ class GreedyBipartiteMatcher(matcher.Matcher):
     # Convert similarity matrix to distance matrix as tf.image.bipartite tries
     # to find minimum distance matches.
     distance_matrix = -1 * similarity_matrix
-    num_valid_rows = tf.reduce_sum(tf.to_float(valid_rows))
+    num_valid_rows = tf.reduce_sum(tf.cast(valid_rows, dtype=tf.float32))
     _, match_results = image_ops.bipartite_match(
         distance_matrix, num_valid_rows=num_valid_rows)
     match_results = tf.reshape(match_results, [-1])
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 57958446..2da13265 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -722,9 +722,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     clip_heights = image_shapes[:, 0]
     clip_widths = image_shapes[:, 1]
-    clip_window = tf.to_float(tf.stack([tf.zeros_like(clip_heights),
-                                        tf.zeros_like(clip_heights),
-                                        clip_heights, clip_widths], axis=1))
+    clip_window = tf.cast(
+        tf.stack([
+            tf.zeros_like(clip_heights),
+            tf.zeros_like(clip_heights), clip_heights, clip_widths
+        ],
+                 axis=1),
+        dtype=tf.float32)
     return clip_window
 
   def _proposal_postprocess(self, rpn_box_encodings,
@@ -732,7 +736,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                             image_shape, true_image_shapes):
     """Wraps over FasterRCNNMetaArch._postprocess_rpn()."""
     image_shape_2d = self._image_batch_shape_2d(image_shape)
-    proposal_boxes_normalized, _, num_proposals, _, _ = \
+    proposal_boxes_normalized, _, _, num_proposals, _, _ = \
         self._postprocess_rpn(
             rpn_box_encodings, rpn_objectness_predictions_with_background,
             anchors, image_shape_2d, true_image_shapes)
@@ -817,17 +821,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
     prediction_dict = self._predict_first_stage(preprocessed_inputs)
 
     if self._number_of_stages >= 2:
-      # If mixed-precision training on TPU is enabled, rpn_box_encodings and
-      # rpn_objectness_predictions_with_background are bfloat16 tensors.
-      # Considered prediction results, they need to be casted to float32
-      # tensors for correct postprocess_rpn computation in predict_second_stage.
       prediction_dict.update(
           self._predict_second_stage(
-              tf.to_float(prediction_dict['rpn_box_encodings']),
-              tf.to_float(
-                  prediction_dict['rpn_objectness_predictions_with_background']
-              ), prediction_dict['rpn_features_to_crop'],
-              prediction_dict['anchors'], prediction_dict['image_shape'],
+              prediction_dict['rpn_box_encodings'],
+              prediction_dict['rpn_objectness_predictions_with_background'],
+              prediction_dict['rpn_features_to_crop'],
+              prediction_dict['anchors'],
+              prediction_dict['image_shape'],
               true_image_shapes))
 
     if self._number_of_stages == 3:
@@ -848,21 +848,21 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
-        1) rpn_box_predictor_features: A 4-D float32 tensor with shape
+        1) rpn_box_predictor_features: A 4-D float32/bfloat16 tensor with shape
           [batch_size, height, width, depth] to be used for predicting proposal
           boxes and corresponding objectness scores.
-        2) rpn_features_to_crop: A 4-D float32 tensor with shape
+        2) rpn_features_to_crop: A 4-D float32/bfloat16 tensor with shape
           [batch_size, height, width, depth] representing image features to crop
           using the proposal boxes predicted by the RPN.
         3) image_shape: a 1-D tensor of shape [4] representing the input
           image shape.
-        4) rpn_box_encodings:  3-D float tensor of shape
+        4) rpn_box_encodings:  3-D float32 tensor of shape
           [batch_size, num_anchors, self._box_coder.code_size] containing
           predicted boxes.
-        5) rpn_objectness_predictions_with_background: 3-D float tensor of shape
-          [batch_size, num_anchors, 2] containing class
-          predictions (logits) for each of the anchors.  Note that this
-          tensor *includes* background class predictions (at class index 0).
+        5) rpn_objectness_predictions_with_background: 3-D float32 tensor of
+          shape [batch_size, num_anchors, 2] containing class predictions
+          (logits) for each of the anchors.  Note that this tensor *includes*
+          background class predictions (at class index 0).
         6) anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors
           for the first stage RPN (in absolute coordinates).  Note that
           `num_anchors` can differ depending on whether the model is created in
@@ -875,7 +875,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     # The Faster R-CNN paper recommends pruning anchors that venture outside
     # the image window at training time and clipping at inference time.
-    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
+    clip_window = tf.cast(tf.stack([0, 0, image_shape[1], image_shape[2]]),
+                          dtype=tf.float32)
     if self._is_training:
       if self.clip_anchors_to_image:
         anchors_boxlist = box_list_ops.clip_to_window(
@@ -899,9 +900,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         'image_shape':
             image_shape,
         'rpn_box_encodings':
-            rpn_box_encodings,
+            tf.cast(rpn_box_encodings, dtype=tf.float32),
         'rpn_objectness_predictions_with_background':
-            rpn_objectness_predictions_with_background,
+            tf.cast(rpn_objectness_predictions_with_background,
+                    dtype=tf.float32),
         'anchors':
             anchors_boxlist.data['boxes'],
     }
@@ -954,13 +956,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
-        1) refined_box_encodings: a 3-D tensor with shape
+        1) refined_box_encodings: a 3-D float32 tensor with shape
           [total_num_proposals, num_classes, self._box_coder.code_size]
           representing predicted (final) refined box encodings, where
           total_num_proposals=batch_size*self._max_num_proposals. If using a
           shared box across classes the shape will instead be
           [total_num_proposals, 1, self._box_coder.code_size].
-        2) class_predictions_with_background: a 3-D tensor with shape
+        2) class_predictions_with_background: a 3-D float32 tensor with shape
           [total_num_proposals, num_classes + 1] containing class
           predictions (logits) for each of the anchors, where
           total_num_proposals=batch_size*self._max_num_proposals.
@@ -980,7 +982,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           boxes proposed by the RPN, thus enabling one to extract features and
           get box classification and prediction for externally selected areas
           of the image.
-        6) box_classifier_features: a 4-D float32 or bfloat16 tensor
+        6) box_classifier_features: a 4-D float32/bfloat16 tensor
           representing the features for each proposal.
     """
     proposal_boxes_normalized, num_proposals = self._proposal_postprocess(
@@ -1008,13 +1010,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
-        1) refined_box_encodings: a 3-D tensor with shape
+        1) refined_box_encodings: a 3-D float32 tensor with shape
           [total_num_proposals, num_classes, self._box_coder.code_size]
           representing predicted (final) refined box encodings, where
           total_num_proposals=batch_size*self._max_num_proposals. If using a
           shared box across classes the shape will instead be
           [total_num_proposals, 1, self._box_coder.code_size].
-        2) class_predictions_with_background: a 3-D tensor with shape
+        2) class_predictions_with_background: a 3-D float32 tensor with shape
           [total_num_proposals, num_classes + 1] containing class
           predictions (logits) for each of the anchors, where
           total_num_proposals=batch_size*self._max_num_proposals.
@@ -1029,17 +1031,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
           boxes proposed by the RPN, thus enabling one to extract features and
           get box classification and prediction for externally selected areas
           of the image.
-        5) box_classifier_features: a 4-D float32 or bfloat16 tensor
+        5) box_classifier_features: a 4-D float32/bfloat16 tensor
           representing the features for each proposal.
     """
-    # If mixed-precision training on TPU is enabled, the dtype of
-    # rpn_features_to_crop is bfloat16, otherwise it is float32. tf.cast is
-    # used to match the dtype of proposal_boxes_normalized to that of
-    # rpn_features_to_crop for further computation.
     flattened_proposal_feature_maps = (
         self._compute_second_stage_input_feature_maps(
-            rpn_features_to_crop,
-            tf.cast(proposal_boxes_normalized, rpn_features_to_crop.dtype)))
+            rpn_features_to_crop, proposal_boxes_normalized))
 
     box_classifier_features = self._extract_box_classifier_features(
         flattened_proposal_feature_maps)
@@ -1066,9 +1063,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         proposal_boxes_normalized, image_shape, self._parallel_iterations)
 
     prediction_dict = {
-        'refined_box_encodings': refined_box_encodings,
+        'refined_box_encodings': tf.cast(refined_box_encodings,
+                                         dtype=tf.float32),
         'class_predictions_with_background':
-        class_predictions_with_background,
+        tf.cast(class_predictions_with_background, dtype=tf.float32),
         'proposal_boxes': absolute_proposal_boxes,
         'box_classifier_features': box_classifier_features,
         'proposal_boxes_normalized': proposal_boxes_normalized,
@@ -1215,7 +1213,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     _, num_classes, height, width = instance_masks.get_shape().as_list()
     k = tf.shape(instance_masks)[0]
     instance_masks = tf.reshape(instance_masks, [-1, height, width])
-    classes = tf.to_int32(tf.reshape(classes, [-1]))
+    classes = tf.cast(tf.reshape(classes, [-1]), dtype=tf.int32)
     gather_idx = tf.range(k) * num_classes + classes
     return tf.gather(instance_masks, gather_idx)
 
@@ -1415,6 +1413,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       detections: a dictionary containing the following fields
         detection_boxes: [batch, max_detection, 4]
         detection_scores: [batch, max_detections]
+        detection_multiclass_scores: [batch, max_detections, 2]
         detection_classes: [batch, max_detections]
           (this entry is only created if rpn_mode=False)
         num_detections: [batch]
@@ -1427,7 +1426,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     with tf.name_scope('FirstStagePostprocessor'):
       if self._number_of_stages == 1:
-        (proposal_boxes, proposal_scores, num_proposals, raw_proposal_boxes,
+        (proposal_boxes, proposal_scores, proposal_multiclass_scores,
+         num_proposals, raw_proposal_boxes,
          raw_proposal_scores) = self._postprocess_rpn(
              prediction_dict['rpn_box_encodings'],
              prediction_dict['rpn_objectness_predictions_with_background'],
@@ -1437,8 +1437,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
                 proposal_boxes,
             fields.DetectionResultFields.detection_scores:
                 proposal_scores,
+            fields.DetectionResultFields.detection_multiclass_scores:
+                proposal_multiclass_scores,
             fields.DetectionResultFields.num_detections:
-                tf.to_float(num_proposals),
+                tf.cast(num_proposals, dtype=tf.float32),
             fields.DetectionResultFields.raw_detection_boxes:
                 raw_proposal_boxes,
             fields.DetectionResultFields.raw_detection_scores:
@@ -1545,6 +1547,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
       proposal_scores:  A float tensor with shape
         [batch_size, max_num_proposals] representing the (potentially zero
         padded) proposal objectness scores for all images in the batch.
+      proposal_multiclass_scores:  A float tensor with shape
+        [batch_size, max_num_proposals, 2] representing the (potentially zero
+        padded) proposal multiclass scores for all images in the batch.
       num_proposals: A Tensor of type `int32`. A 1-D tensor of shape [batch]
         representing the number of proposals predicted for each image in
         the batch.
@@ -1566,10 +1571,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
         rpn_objectness_predictions_with_background_batch)
     rpn_objectness_softmax_without_background = rpn_objectness_softmax[:, :, 1]
     clip_window = self._compute_clip_window(image_shapes)
-    (proposal_boxes, proposal_scores, _, _, _,
+    additional_fields = {'multiclass_scores': rpn_objectness_softmax}
+    (proposal_boxes, proposal_scores, _, _, nmsed_additional_fields,
      num_proposals) = self._first_stage_nms_fn(
          tf.expand_dims(raw_proposal_boxes, axis=2),
          tf.expand_dims(rpn_objectness_softmax_without_background, axis=2),
+         additional_fields=additional_fields,
          clip_window=clip_window)
     if self._is_training:
       proposal_boxes = tf.stop_gradient(proposal_boxes)
@@ -1596,7 +1603,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         normalize_boxes,
         elems=[raw_proposal_boxes, image_shapes],
         dtype=tf.float32)
-    return (normalized_proposal_boxes, proposal_scores, num_proposals,
+    proposal_multiclass_scores = nmsed_additional_fields['multiclass_scores']
+    return (normalized_proposal_boxes, proposal_scores,
+            proposal_multiclass_scores, num_proposals,
             raw_normalized_proposal_boxes, rpn_objectness_softmax)
 
   def _sample_box_classifier_batch(
@@ -1713,11 +1722,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
         for i, boxes in enumerate(
             self.groundtruth_lists(fields.BoxListFields.boxes))
     ]
-    groundtruth_classes_with_background_list = [
-        tf.to_float(
-            tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'))
-        for one_hot_encoding in self.groundtruth_lists(
-            fields.BoxListFields.classes)]
+    groundtruth_classes_with_background_list = []
+    for one_hot_encoding in self.groundtruth_lists(
+        fields.BoxListFields.classes):
+      groundtruth_classes_with_background_list.append(
+          tf.cast(
+              tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'),
+              dtype=tf.float32))
 
     groundtruth_masks_list = self._groundtruth_lists.get(
         fields.BoxListFields.masks)
@@ -1860,6 +1871,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
       A dictionary containing:
         `detection_boxes`: [batch, max_detection, 4] in normalized co-ordinates.
         `detection_scores`: [batch, max_detections]
+         detection_multiclass_scores: [batch, max_detections,
+          num_classes_with_background] tensor with class score distribution for
+          post-processed detection boxes including background class if any.
         `detection_classes`: [batch, max_detections]
         `num_detections`: [batch]
         `detection_masks`:
@@ -1894,20 +1908,24 @@ class FasterRCNNMetaArch(model.DetectionModel):
     clip_window = self._compute_clip_window(image_shapes)
     mask_predictions_batch = None
     if mask_predictions is not None:
-      mask_height = mask_predictions.shape[2].value
-      mask_width = mask_predictions.shape[3].value
+      mask_height = shape_utils.get_dim_as_int(mask_predictions.shape[2])
+      mask_width = shape_utils.get_dim_as_int(mask_predictions.shape[3])
       mask_predictions = tf.sigmoid(mask_predictions)
       mask_predictions_batch = tf.reshape(
           mask_predictions, [-1, self.max_num_proposals,
                              self.num_classes, mask_height, mask_width])
 
-    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks, _,
-     num_detections) = self._second_stage_nms_fn(
+    additional_fields = {
+        'multiclass_scores': class_predictions_with_background_batch_normalized
+    }
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+     nmsed_additional_fields, num_detections) = self._second_stage_nms_fn(
          refined_decoded_boxes_batch,
          class_predictions_batch,
          clip_window=clip_window,
          change_coordinate_frame=True,
          num_valid_boxes=num_proposals,
+         additional_fields=additional_fields,
          masks=mask_predictions_batch)
     if refined_decoded_boxes_batch.shape[2] > 1:
       class_ids = tf.expand_dims(
@@ -1948,8 +1966,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
             nmsed_scores,
         fields.DetectionResultFields.detection_classes:
             nmsed_classes,
+        fields.DetectionResultFields.detection_multiclass_scores:
+            nmsed_additional_fields['multiclass_scores'],
         fields.DetectionResultFields.num_detections:
-            tf.to_float(num_detections),
+            tf.cast(num_detections, dtype=tf.float32),
         fields.DetectionResultFields.raw_detection_boxes:
             raw_normalized_detection_boxes,
         fields.DetectionResultFields.raw_detection_scores:
@@ -2096,18 +2116,18 @@ class FasterRCNNMetaArch(model.DetectionModel):
         return self._first_stage_sampler.subsample(
             tf.cast(cls_weights, tf.bool),
             self._first_stage_minibatch_size, tf.cast(cls_targets, tf.bool))
-      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(
+      batch_sampled_indices = tf.cast(shape_utils.static_or_dynamic_map_fn(
           _minibatch_subsample_fn,
           [batch_cls_targets, batch_cls_weights],
           dtype=tf.bool,
           parallel_iterations=self._parallel_iterations,
-          back_prop=True))
+          back_prop=True), dtype=tf.float32)
 
       # Normalize by number of examples in sampled minibatch
       normalizer = tf.maximum(
           tf.reduce_sum(batch_sampled_indices, axis=1), 1.0)
       batch_one_hot_targets = tf.one_hot(
-          tf.to_int32(batch_cls_targets), depth=2)
+          tf.cast(batch_cls_targets, dtype=tf.int32), depth=2)
       sampled_reg_indices = tf.multiply(batch_sampled_indices,
                                         batch_reg_weights)
 
@@ -2133,8 +2153,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                                       name='localization_loss')
       objectness_loss = tf.multiply(self._first_stage_obj_loss_weight,
                                     objectness_loss, name='objectness_loss')
-      loss_dict = {localization_loss.op.name: localization_loss,
-                   objectness_loss.op.name: objectness_loss}
+      loss_dict = {'Loss/RPNLoss/localization_loss': localization_loss,
+                   'Loss/RPNLoss/objectness_loss': objectness_loss}
     return loss_dict
 
   def _loss_box_classifier(self,
@@ -2216,8 +2236,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
           for proposal_boxes_single_image in tf.unstack(proposal_boxes)]
       batch_size = len(proposal_boxlists)
 
-      num_proposals_or_one = tf.to_float(tf.expand_dims(
-          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1))
+      num_proposals_or_one = tf.cast(tf.expand_dims(
+          tf.maximum(num_proposals, tf.ones_like(num_proposals)), 1),
+                                     dtype=tf.float32)
       normalizer = tf.tile(num_proposals_or_one,
                            [1, self.max_num_proposals]) * batch_size
 
@@ -2276,9 +2297,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
           ndims=2) / normalizer
 
       second_stage_loc_loss = tf.reduce_sum(
-          second_stage_loc_losses * tf.to_float(paddings_indicator))
+          second_stage_loc_losses * tf.cast(paddings_indicator,
+                                            dtype=tf.float32))
       second_stage_cls_loss = tf.reduce_sum(
-          second_stage_cls_losses * tf.to_float(paddings_indicator))
+          second_stage_cls_losses * tf.cast(paddings_indicator,
+                                            dtype=tf.float32))
 
       if self._hard_example_miner:
         (second_stage_loc_loss, second_stage_cls_loss
@@ -2293,8 +2316,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
                                         second_stage_cls_loss,
                                         name='classification_loss')
 
-      loss_dict = {localization_loss.op.name: localization_loss,
-                   classification_loss.op.name: classification_loss}
+      loss_dict = {'Loss/BoxClassifierLoss/localization_loss':
+                       localization_loss,
+                   'Loss/BoxClassifierLoss/classification_loss':
+                       classification_loss}
       second_stage_mask_loss = None
       if prediction_masks is not None:
         if groundtruth_masks_list is None:
@@ -2332,8 +2357,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
               prediction_masks_with_background,
               tf.greater(one_hot_flat_cls_targets_with_background, 0))
 
-        mask_height = prediction_masks.shape[2].value
-        mask_width = prediction_masks.shape[3].value
+        mask_height = shape_utils.get_dim_as_int(prediction_masks.shape[2])
+        mask_width = shape_utils.get_dim_as_int(prediction_masks.shape[3])
         reshaped_prediction_masks = tf.reshape(
             prediction_masks_masked_by_class_targets,
             [batch_size, -1, mask_height * mask_width])
@@ -2364,7 +2389,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
             [batch_size, -1, mask_height * mask_width])
 
         mask_losses_weights = (
-            batch_mask_target_weights * tf.to_float(paddings_indicator))
+            batch_mask_target_weights * tf.cast(paddings_indicator,
+                                                dtype=tf.float32))
         mask_losses = self._second_stage_mask_loss(
             reshaped_prediction_masks,
             batch_cropped_gt_mask,
@@ -2419,7 +2445,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         for detection_boxes_single_image in tf.unstack(proposal_boxes)
     ]
     paddings_indicator = self._padded_batched_proposals_indicator(
-        tf.to_int32(num_detections), detection_boxes.shape[1])
+        tf.cast(num_detections, dtype=tf.int32), detection_boxes.shape[1])
     (batch_cls_targets_with_background, _, _, _,
      _) = target_assigner.batch_assign_targets(
          target_assigner=self._detector_target_assigner,
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index c3daa864..ec4ec2a2 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -271,7 +271,8 @@ class FasterRCNNMetaArchTest(
           set(tensor_dict_out.keys()),
           set(expected_shapes.keys()).union(
               set([
-                  'detection_boxes', 'detection_scores', 'detection_classes',
+                  'detection_boxes', 'detection_scores',
+                  'detection_multiclass_scores', 'detection_classes',
                   'detection_masks', 'num_detections', 'mask_predictions',
                   'raw_detection_boxes', 'raw_detection_scores'
               ])))
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 266ce610..c0d99b1b 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -967,7 +967,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         [[0, 0, .5, .5], [.5, .5, 1, 1]], [[0, .5, .5, 1], [.5, 0, 1, .5]]]
     expected_proposal_scores = [[1, 1],
                                 [1, 1]]
-    expected_num_proposals = [2, 2]
+    expected_proposal_multiclass_scores = [[[0., 1.], [0., 1.]],
+                                           [[0., 1.], [0., 1.]]]
     expected_raw_proposal_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]],
                                    [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
@@ -975,31 +976,45 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_raw_scores = [[[0., 1.], [0., 1.], [0., 1.], [0., 1.]],
                            [[0., 1.], [0., 1.], [0., 1.], [0., 1.]]]
     expected_output_keys = set([
-        'detection_boxes', 'detection_scores', 'num_detections',
-        'raw_detection_boxes', 'raw_detection_scores'
+        'detection_boxes', 'detection_scores', 'detection_multiclass_scores',
+        'num_detections', 'raw_detection_boxes', 'raw_detection_scores'
     ])
     self.assertEqual(set(proposals.keys()), expected_output_keys)
 
     with self.test_session() as sess:
       proposals_out = sess.run(proposals)
       for image_idx in range(batch_size):
+        num_detections = int(proposals_out['num_detections'][image_idx])
+        boxes = proposals_out['detection_boxes'][
+            image_idx][:num_detections, :].tolist()
+        scores = proposals_out['detection_scores'][
+            image_idx][:num_detections].tolist()
+        multiclass_scores = proposals_out['detection_multiclass_scores'][
+            image_idx][:num_detections, :].tolist()
+        expected_boxes = expected_proposal_boxes[image_idx]
+        expected_scores = expected_proposal_scores[image_idx]
+        expected_multiclass_scores = expected_proposal_multiclass_scores[
+            image_idx]
         self.assertTrue(
-            test_utils.first_rows_close_as_set(
-                proposals_out['detection_boxes'][image_idx].tolist(),
-                expected_proposal_boxes[image_idx]))
-      self.assertAllClose(proposals_out['detection_scores'],
-                          expected_proposal_scores)
-      self.assertAllEqual(proposals_out['num_detections'],
-                          expected_num_proposals)
+            test_utils.first_rows_close_as_set(boxes, expected_boxes))
+        self.assertTrue(
+            test_utils.first_rows_close_as_set(scores, expected_scores))
+        self.assertTrue(
+            test_utils.first_rows_close_as_set(multiclass_scores,
+                                               expected_multiclass_scores))
+
     self.assertAllClose(proposals_out['raw_detection_boxes'],
                         expected_raw_proposal_boxes)
     self.assertAllClose(proposals_out['raw_detection_scores'],
                         expected_raw_scores)
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
+  @parameterized.named_parameters({
+      'testcase_name': 'keras',
+      'use_keras': True
+  }, {
+      'testcase_name': 'slim',
+      'use_keras': False
+  })
   def test_postprocess_first_stage_only_train_mode(self, use_keras=False):
     self._test_postprocess_first_stage_only_train_mode(use_keras=use_keras)
 
@@ -1066,7 +1081,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       return (detections['num_detections'], detections['detection_boxes'],
               detections['detection_scores'], detections['detection_classes'],
               detections['raw_detection_boxes'],
-              detections['raw_detection_scores'])
+              detections['raw_detection_scores'],
+              detections['detection_multiclass_scores'])
 
     proposal_boxes = np.array(
         [[[1, 1, 2, 3],
@@ -1097,6 +1113,17 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     expected_num_detections = [5, 4]
     expected_detection_classes = [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]]
     expected_detection_scores = [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]]
+    expected_multiclass_scores = [[[1, 1, 1],
+                                   [1, 1, 1],
+                                   [1, 1, 1],
+                                   [1, 1, 1],
+                                   [1, 1, 1]],
+                                  [[1, 1, 1],
+                                   [1, 1, 1],
+                                   [1, 1, 1],
+                                   [1, 1, 1],
+                                   [0, 0, 0]]]
+
     h = float(image_shape[1])
     w = float(image_shape[2])
     expected_raw_detection_boxes = np.array(
@@ -1114,6 +1141,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                           expected_detection_scores[indx][0:num_proposals])
       self.assertAllClose(results[3][indx][0:num_proposals],
                           expected_detection_classes[indx][0:num_proposals])
+      self.assertAllClose(results[6][indx][0:num_proposals],
+                          expected_multiclass_scores[indx][0:num_proposals])
 
     self.assertAllClose(results[4], expected_raw_detection_boxes)
     self.assertAllClose(results[5],
@@ -1895,8 +1924,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           number_of_stages=2, second_stage_batch_size=6)
 
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(tf.random_uniform(
+          inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
@@ -1921,8 +1950,8 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           is_training=False, use_keras=use_keras,
           number_of_stages=2, second_stage_batch_size=6)
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(tf.random_uniform(
+          inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
@@ -1942,8 +1971,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                  second_stage_batch_size=6, num_classes=42)
 
       inputs_shape2 = (2, 20, 20, 3)
-      inputs2 = tf.to_float(tf.random_uniform(
-          inputs_shape2, minval=0, maxval=255, dtype=tf.int32))
+      inputs2 = tf.cast(tf.random_uniform(
+          inputs_shape2, minval=0, maxval=255, dtype=tf.int32),
+                        dtype=tf.float32)
       preprocessed_inputs2, true_image_shapes = model2.preprocess(inputs2)
       prediction_dict2 = model2.predict(preprocessed_inputs2, true_image_shapes)
       model2.postprocess(prediction_dict2, true_image_shapes)
@@ -1974,8 +2004,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           num_classes=42)
 
       inputs_shape = (2, 20, 20, 3)
-      inputs = tf.to_float(
-          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(
+          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32),
+          dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 4d0a1d69..69318b0c 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -297,9 +297,10 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
     """
     image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0),
                              [image_shape[0], 1])
-    proposal_boxes_normalized, _, num_proposals, _, _ = self._postprocess_rpn(
-        rpn_box_encodings, rpn_objectness_predictions_with_background,
-        anchors, image_shape_2d, true_image_shapes)
+    (proposal_boxes_normalized, _, _, num_proposals, _,
+     _) = self._postprocess_rpn(rpn_box_encodings,
+                                rpn_objectness_predictions_with_background,
+                                anchors, image_shape_2d, true_image_shapes)
 
     box_classifier_features = (
         self._extract_box_classifier_features(rpn_features))
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 4ecdcc52..90722534 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -509,9 +509,9 @@ class SSDMetaArch(model.DetectionModel):
     resized_inputs_shape = shape_utils.combined_static_and_dynamic_shape(
         preprocessed_images)
     true_heights, true_widths, _ = tf.unstack(
-        tf.to_float(true_image_shapes), axis=1)
-    padded_height = tf.to_float(resized_inputs_shape[1])
-    padded_width = tf.to_float(resized_inputs_shape[2])
+        tf.cast(true_image_shapes, dtype=tf.float32), axis=1)
+    padded_height = tf.cast(resized_inputs_shape[1], dtype=tf.float32)
+    padded_width = tf.cast(resized_inputs_shape[2], dtype=tf.float32)
     return tf.stack(
         [
             tf.zeros_like(true_heights),
@@ -654,6 +654,9 @@ class SSDMetaArch(model.DetectionModel):
           detection boxes.
         detection_scores: [batch, max_detections] tensor with scalar scores for
           post-processed detection boxes.
+        detection_multiclass_scores: [batch, max_detections,
+          num_classes_with_background] tensor with class score distribution for
+          post-processed detection boxes including background class if any.
         detection_classes: [batch, max_detections] tensor with classes for
           post-processed detection classes.
         detection_keypoints: [batch, max_detections, num_keypoints, 2] (if
@@ -703,10 +706,13 @@ class SSDMetaArch(model.DetectionModel):
           feature_map_list.append(tf.reshape(feature_map, [batch_size, -1]))
         box_features = tf.concat(feature_map_list, 1)
         box_features = tf.identity(box_features, 'raw_box_features')
-
+      additional_fields = {
+          'multiclass_scores': detection_scores_with_background
+      }
       if detection_keypoints is not None:
-        additional_fields = {
-            fields.BoxListFields.keypoints: detection_keypoints}
+        detection_keypoints = tf.identity(
+            detection_keypoints, 'raw_keypoint_locations')
+        additional_fields[fields.BoxListFields.keypoints] = detection_keypoints
       (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
        nmsed_additional_fields, num_detections) = self._non_max_suppression_fn(
            detection_boxes,
@@ -722,8 +728,10 @@ class SSDMetaArch(model.DetectionModel):
               nmsed_scores,
           fields.DetectionResultFields.detection_classes:
               nmsed_classes,
+          fields.DetectionResultFields.detection_multiclass_scores:
+              nmsed_additional_fields['multiclass_scores'],
           fields.DetectionResultFields.num_detections:
-              tf.to_float(num_detections),
+              tf.cast(num_detections, dtype=tf.float32),
           fields.DetectionResultFields.raw_detection_boxes:
               tf.squeeze(detection_boxes, axis=2),
           fields.DetectionResultFields.raw_detection_scores:
@@ -786,13 +794,13 @@ class SSDMetaArch(model.DetectionModel):
       if self._random_example_sampler:
         batch_cls_per_anchor_weights = tf.reduce_mean(
             batch_cls_weights, axis=-1)
-        batch_sampled_indicator = tf.to_float(
+        batch_sampled_indicator = tf.cast(
             shape_utils.static_or_dynamic_map_fn(
                 self._minibatch_subsample_fn,
                 [batch_cls_targets, batch_cls_per_anchor_weights],
                 dtype=tf.bool,
                 parallel_iterations=self._parallel_iterations,
-                back_prop=True))
+                back_prop=True), dtype=tf.float32)
         batch_reg_weights = tf.multiply(batch_sampled_indicator,
                                         batch_reg_weights)
         batch_cls_weights = tf.multiply(
@@ -868,7 +876,8 @@ class SSDMetaArch(model.DetectionModel):
       # Optionally normalize by number of positive matches
       normalizer = tf.constant(1.0, dtype=tf.float32)
       if self._normalize_loss_by_num_matches:
-        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(batch_reg_weights)),
+        normalizer = tf.maximum(tf.cast(tf.reduce_sum(batch_reg_weights),
+                                        dtype=tf.float32),
                                 1.0)
 
       localization_loss_normalizer = normalizer
@@ -883,8 +892,8 @@ class SSDMetaArch(model.DetectionModel):
                                         name='classification_loss')
 
       loss_dict = {
-          str(localization_loss.op.name): localization_loss,
-          str(classification_loss.op.name): classification_loss
+          'Loss/localization_loss': localization_loss,
+          'Loss/classification_loss': classification_loss
       }
 
 
@@ -1025,17 +1034,35 @@ class SSDMetaArch(model.DetectionModel):
         with rows of the Match objects corresponding to groundtruth boxes
         and columns corresponding to anchors.
     """
-    avg_num_gt_boxes = tf.reduce_mean(tf.to_float(tf.stack(
-        [tf.shape(x)[0] for x in groundtruth_boxes_list])))
-    avg_num_matched_gt_boxes = tf.reduce_mean(tf.to_float(tf.stack(
-        [match.num_matched_rows() for match in match_list])))
-    avg_pos_anchors = tf.reduce_mean(tf.to_float(tf.stack(
-        [match.num_matched_columns() for match in match_list])))
-    avg_neg_anchors = tf.reduce_mean(tf.to_float(tf.stack(
-        [match.num_unmatched_columns() for match in match_list])))
-    avg_ignored_anchors = tf.reduce_mean(tf.to_float(tf.stack(
-        [match.num_ignored_columns() for match in match_list])))
+    avg_num_gt_boxes = tf.reduce_mean(
+        tf.cast(
+            tf.stack([tf.shape(x)[0] for x in groundtruth_boxes_list]),
+            dtype=tf.float32))
+    avg_num_matched_gt_boxes = tf.reduce_mean(
+        tf.cast(
+            tf.stack([match.num_matched_rows() for match in match_list]),
+            dtype=tf.float32))
+    avg_pos_anchors = tf.reduce_mean(
+        tf.cast(
+            tf.stack([match.num_matched_columns() for match in match_list]),
+            dtype=tf.float32))
+    avg_neg_anchors = tf.reduce_mean(
+        tf.cast(
+            tf.stack([match.num_unmatched_columns() for match in match_list]),
+            dtype=tf.float32))
+    avg_ignored_anchors = tf.reduce_mean(
+        tf.cast(
+            tf.stack([match.num_ignored_columns() for match in match_list]),
+            dtype=tf.float32))
     # TODO(rathodv): Add a test for these summaries.
+    try:
+      # TODO(kaftan): Integrate these summaries into the v2 style loops
+      with tf.compat.v2.init_scope():
+        if tf.compat.v2.executing_eagerly():
+          return
+    except AttributeError:
+      pass
+
     tf.summary.scalar('AvgNumGroundtruthBoxesPerImage',
                       avg_num_gt_boxes,
                       family='TargetAssignment')
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index f5b91c7b..98b4b410 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -176,6 +176,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         ]
     ]  # padding
     expected_scores = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
+    expected_multiclass_scores = [[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]],
+                                  [[0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]]
+
     expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
 
@@ -198,6 +201,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         detections = model.postprocess(prediction_dict, true_image_shapes)
         self.assertIn('detection_boxes', detections)
         self.assertIn('detection_scores', detections)
+        self.assertIn('detection_multiclass_scores', detections)
         self.assertIn('detection_classes', detections)
         self.assertIn('num_detections', detections)
         self.assertIn('raw_detection_boxes', detections)
@@ -217,6 +221,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                 expected_boxes[image_idx]))
       self.assertAllClose(detections_out['detection_scores'], expected_scores)
       self.assertAllClose(detections_out['detection_classes'], expected_classes)
+      self.assertAllClose(detections_out['detection_multiclass_scores'],
+                          expected_multiclass_scores)
       self.assertAllClose(detections_out['num_detections'],
                           expected_num_detections)
       self.assertAllEqual(detections_out['raw_detection_boxes'],
@@ -235,7 +241,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                       true_image_shapes)
       detections = model.postprocess(prediction_dict, true_image_shapes)
       return (detections['detection_boxes'], detections['detection_scores'],
-              detections['detection_classes'], detections['num_detections'])
+              detections['detection_classes'], detections['num_detections'],
+              detections['detection_multiclass_scores'])
 
     batch_size = 2
     image_size = 2
@@ -257,11 +264,14 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         ]
     ]  # padding
     expected_scores = [[0, 0, 0, 0], [0, 0, 0, 0]]
+    expected_multiclass_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
+                                  [[0, 0], [0, 0], [0, 0], [0, 0]]]
     expected_classes = [[0, 0, 0, 0], [0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
 
     (detection_boxes, detection_scores, detection_classes,
-     num_detections) = self.execute(graph_fn, [input_image])
+     num_detections, detection_multiclass_scores) = self.execute(graph_fn,
+                                                                 [input_image])
     for image_idx in range(batch_size):
       self.assertTrue(test_utils.first_rows_close_as_set(
           detection_boxes[image_idx][
@@ -270,6 +280,11 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       self.assertAllClose(
           detection_scores[image_idx][0:expected_num_detections[image_idx]],
           expected_scores[image_idx][0:expected_num_detections[image_idx]])
+      self.assertAllClose(
+          detection_multiclass_scores[image_idx]
+          [0:expected_num_detections[image_idx]],
+          expected_multiclass_scores[image_idx]
+          [0:expected_num_detections[image_idx]])
       self.assertAllClose(
           detection_classes[image_idx][0:expected_num_detections[image_idx]],
           expected_classes[image_idx][0:expected_num_detections[image_idx]])
@@ -600,8 +615,8 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     with test_graph_detection.as_default():
       model, _, _, _ = self._create_model(use_keras=use_keras)
       inputs_shape = [2, 2, 2, 3]
-      inputs = tf.to_float(tf.random_uniform(
-          inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(tf.random_uniform(
+          inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
@@ -620,8 +635,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     with test_graph_detection.as_default():
       model, _, _, _ = self._create_model(use_keras=use_keras)
       inputs_shape = [2, 2, 2, 3]
-      inputs = tf.to_float(
-          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      inputs = tf.cast(
+          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32),
+          dtype=tf.float32)
       preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       model.postprocess(prediction_dict, true_image_shapes)
diff --git a/research/object_detection/metrics/calibration_metrics.py b/research/object_detection/metrics/calibration_metrics.py
index 6c90d033..a94f4600 100644
--- a/research/object_detection/metrics/calibration_metrics.py
+++ b/research/object_detection/metrics/calibration_metrics.py
@@ -98,13 +98,16 @@ def expected_calibration_error(y_true, y_pred, nbins=20):
 
   with tf.control_dependencies([bin_ids]):
     update_bin_counts_op = tf.assign_add(
-        bin_counts, tf.to_float(tf.bincount(bin_ids, minlength=nbins)))
+        bin_counts, tf.cast(tf.bincount(bin_ids, minlength=nbins),
+                            dtype=tf.float32))
     update_bin_true_sum_op = tf.assign_add(
         bin_true_sum,
-        tf.to_float(tf.bincount(bin_ids, weights=y_true, minlength=nbins)))
+        tf.cast(tf.bincount(bin_ids, weights=y_true, minlength=nbins),
+                dtype=tf.float32))
     update_bin_preds_sum_op = tf.assign_add(
         bin_preds_sum,
-        tf.to_float(tf.bincount(bin_ids, weights=y_pred, minlength=nbins)))
+        tf.cast(tf.bincount(bin_ids, weights=y_pred, minlength=nbins),
+                dtype=tf.float32))
 
   ece_update_op = _ece_from_bins(
       update_bin_counts_op,
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index bb98d83c..5b6413d4 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -216,29 +216,23 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
                    for key, value in iter(box_metrics.items())}
     return box_metrics
 
-  def get_estimator_eval_metric_ops(self, eval_dict):
-    """Returns a dictionary of eval metric ops.
+  def add_eval_dict(self, eval_dict):
+    """Observes an evaluation result dict for a single example.
 
-    Note that once value_op is called, the detections and groundtruth added via
-    update_op are cleared.
+    When executing eagerly, once all observations have been observed by this
+    method you can use `.evaluate()` to get the final metrics.
 
-    This function can take in groundtruth and detections for a batch of images,
-    or for a single image. For the latter case, the batch dimension for input
-    tensors need not be present.
+    When using `tf.estimator.Estimator` for evaluation this function is used by
+    `get_estimator_eval_metric_ops()` to construct the metric update op.
 
     Args:
-      eval_dict: A dictionary that holds tensors for evaluating object detection
-        performance. For single-image evaluation, this dictionary may be
-        produced from eval_util.result_dict_for_single_example(). If multi-image
-        evaluation, `eval_dict` should contain the fields
-        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
-        properly unpad the tensors from the batch.
+      eval_dict: A dictionary that holds tensors for evaluating an object
+        detection model, returned from
+        eval_util.result_dict_for_single_example().
 
     Returns:
-      a dictionary of metric names to tuple of value_op and update_op that can
-      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
-      update ops must be run together and similarly all value ops must be run
-      together to guarantee correct behaviour.
+      None when executing eagerly, or an update_op that can be used to update
+      the eval metrics in `tf.estimator.EstimatorSpec`.
     """
     def update_op(
         image_id_batched,
@@ -328,16 +322,42 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       if is_annotated is None:
         is_annotated = tf.ones_like(image_id, dtype=tf.bool)
 
-    update_op = tf.py_func(update_op, [image_id,
-                                       groundtruth_boxes,
-                                       groundtruth_classes,
-                                       groundtruth_is_crowd,
-                                       num_gt_boxes_per_image,
-                                       detection_boxes,
-                                       detection_scores,
-                                       detection_classes,
-                                       num_det_boxes_per_image,
-                                       is_annotated], [])
+    return tf.py_func(update_op, [image_id,
+                                  groundtruth_boxes,
+                                  groundtruth_classes,
+                                  groundtruth_is_crowd,
+                                  num_gt_boxes_per_image,
+                                  detection_boxes,
+                                  detection_scores,
+                                  detection_classes,
+                                  num_det_boxes_per_image,
+                                  is_annotated], [])
+
+  def get_estimator_eval_metric_ops(self, eval_dict):
+    """Returns a dictionary of eval metric ops.
+
+    Note that once value_op is called, the detections and groundtruth added via
+    update_op are cleared.
+
+    This function can take in groundtruth and detections for a batch of images,
+    or for a single image. For the latter case, the batch dimension for input
+    tensors need not be present.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating object detection
+        performance. For single-image evaluation, this dictionary may be
+        produced from eval_util.result_dict_for_single_example(). If multi-image
+        evaluation, `eval_dict` should contain the fields
+        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
+        properly unpad the tensors from the batch.
+
+    Returns:
+      a dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
+      update ops must be run together and similarly all value ops must be run
+      together to guarantee correct behaviour.
+    """
+    update_op = self.add_eval_dict(eval_dict)
     metric_names = ['DetectionBoxes_Precision/mAP',
                     'DetectionBoxes_Precision/mAP@.50IOU',
                     'DetectionBoxes_Precision/mAP@.75IOU',
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation.py b/research/object_detection/metrics/oid_challenge_evaluation.py
similarity index 55%
rename from research/object_detection/metrics/oid_od_challenge_evaluation.py
rename to research/object_detection/metrics/oid_challenge_evaluation.py
index 6e6e7ac3..93ddb1a6 100644
--- a/research/object_detection/metrics/oid_od_challenge_evaluation.py
+++ b/research/object_detection/metrics/oid_challenge_evaluation.py
@@ -14,6 +14,8 @@
 # ==============================================================================
 r"""Runs evaluation using OpenImages groundtruth and predictions.
 
+Uses Open Images Challenge 2018, 2019 metrics
+
 Example usage:
 python models/research/object_detection/metrics/oid_od_challenge_evaluation.py \
     --input_annotations_boxes=/path/to/input/annotations-human-bbox.csv \
@@ -21,27 +23,50 @@ python models/research/object_detection/metrics/oid_od_challenge_evaluation.py \
     --input_class_labelmap=/path/to/input/class_labelmap.pbtxt \
     --input_predictions=/path/to/input/predictions.csv \
     --output_metrics=/path/to/output/metric.csv \
+    --input_annotations_segm=[/path/to/input/annotations-human-mask.csv] \
+
+If optional flag has_masks is True, Mask column is also expected in CSV.
 
-CSVs with bounding box annotations and image label (including the image URLs)
+CSVs with bounding box annotations, instance segmentations and image label
 can be downloaded from the Open Images Challenge website:
 https://storage.googleapis.com/openimages/web/challenge.html
 The format of the input csv and the metrics itself are described on the
-challenge website.
+challenge website as well.
+
+
 """
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import argparse
+from absl import app
+from absl import flags
 import pandas as pd
 from google.protobuf import text_format
 
 from object_detection.metrics import io_utils
-from object_detection.metrics import oid_od_challenge_evaluation_utils as utils
+from object_detection.metrics import oid_challenge_evaluation_utils as utils
 from object_detection.protos import string_int_label_map_pb2
 from object_detection.utils import object_detection_evaluation
 
+flags.DEFINE_string('input_annotations_boxes', None,
+                    'File with groundtruth boxes annotations.')
+flags.DEFINE_string('input_annotations_labels', None,
+                    'File with groundtruth labels annotations.')
+flags.DEFINE_string(
+    'input_predictions', None,
+    """File with detection predictions; NOTE: no postprocessing is applied in the evaluation script."""
+)
+flags.DEFINE_string('input_class_labelmap', None,
+                    'Open Images Challenge labelmap.')
+flags.DEFINE_string('output_metrics', None, 'Output file with csv metrics.')
+flags.DEFINE_string(
+    'input_annotations_segm', None,
+    'File with groundtruth instance segmentation annotations [OPTIONAL].')
+
+FLAGS = flags.FLAGS
+
 
 def _load_labelmap(labelmap_path):
   """Loads labelmap from the labelmap path.
@@ -66,26 +91,43 @@ def _load_labelmap(labelmap_path):
   return labelmap_dict, categories
 
 
-def main(parsed_args):
-  all_box_annotations = pd.read_csv(parsed_args.input_annotations_boxes)
-  all_label_annotations = pd.read_csv(parsed_args.input_annotations_labels)
+def main(unused_argv):
+  flags.mark_flag_as_required('input_annotations_boxes')
+  flags.mark_flag_as_required('input_annotations_labels')
+  flags.mark_flag_as_required('input_predictions')
+  flags.mark_flag_as_required('input_class_labelmap')
+  flags.mark_flag_as_required('output_metrics')
+
+  all_location_annotations = pd.read_csv(FLAGS.input_annotations_boxes)
+  all_label_annotations = pd.read_csv(FLAGS.input_annotations_labels)
   all_label_annotations.rename(
       columns={'Confidence': 'ConfidenceImageLabel'}, inplace=True)
-  all_annotations = pd.concat([all_box_annotations, all_label_annotations])
 
-  class_label_map, categories = _load_labelmap(parsed_args.input_class_labelmap)
+  is_instance_segmentation_eval = False
+  if FLAGS.input_annotations_segm:
+    is_instance_segmentation_eval = True
+    all_segm_annotations = pd.read_csv(FLAGS.input_annotations_segm)
+    # Note: this part is unstable as it requires the float point numbers in both
+    # csvs are exactly the same;
+    # Will be replaced by more stable solution: merge on LabelName and ImageID
+    # and filter down by IoU.
+    all_location_annotations = utils.merge_boxes_and_masks(
+        all_location_annotations, all_segm_annotations)
+  all_annotations = pd.concat([all_location_annotations, all_label_annotations])
+
+  class_label_map, categories = _load_labelmap(FLAGS.input_class_labelmap)
   challenge_evaluator = (
-      object_detection_evaluation.OpenImagesDetectionChallengeEvaluator(
-          categories))
+      object_detection_evaluation.OpenImagesChallengeEvaluator(
+          categories, evaluate_masks=is_instance_segmentation_eval))
 
   for _, groundtruth in enumerate(all_annotations.groupby('ImageID')):
     image_id, image_groundtruth = groundtruth
-    groundtruth_dictionary = utils.build_groundtruth_boxes_dictionary(
+    groundtruth_dictionary = utils.build_groundtruth_dictionary(
         image_groundtruth, class_label_map)
     challenge_evaluator.add_single_ground_truth_image_info(
         image_id, groundtruth_dictionary)
 
-  all_predictions = pd.read_csv(parsed_args.input_predictions)
+  all_predictions = pd.read_csv(FLAGS.input_predictions)
   for _, prediction_data in enumerate(all_predictions.groupby('ImageID')):
     image_id, image_predictions = prediction_data
     prediction_dictionary = utils.build_predictions_dictionary(
@@ -95,34 +137,9 @@ def main(parsed_args):
 
   metrics = challenge_evaluator.evaluate()
 
-  with open(parsed_args.output_metrics, 'w') as fid:
+  with open(FLAGS.output_metrics, 'w') as fid:
     io_utils.write_csv(fid, metrics)
 
 
 if __name__ == '__main__':
-
-  parser = argparse.ArgumentParser(
-      description='Evaluate Open Images Object Detection Challenge predictions.'
-  )
-  parser.add_argument(
-      '--input_annotations_boxes',
-      required=True,
-      help='File with groundtruth boxes annotations.')
-  parser.add_argument(
-      '--input_annotations_labels',
-      required=True,
-      help='File with groundtruth labels annotations')
-  parser.add_argument(
-      '--input_predictions',
-      required=True,
-      help="""File with detection predictions; NOTE: no postprocessing is
-      applied in the evaluation script.""")
-  parser.add_argument(
-      '--input_class_labelmap',
-      required=True,
-      help='Open Images Challenge labelmap.')
-  parser.add_argument(
-      '--output_metrics', required=True, help='Output file with csv metrics')
-
-  args = parser.parse_args()
-  main(args)
+  app.run(main)
diff --git a/research/object_detection/metrics/oid_challenge_evaluation_utils.py b/research/object_detection/metrics/oid_challenge_evaluation_utils.py
new file mode 100644
index 00000000..bb9a856d
--- /dev/null
+++ b/research/object_detection/metrics/oid_challenge_evaluation_utils.py
@@ -0,0 +1,185 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Converts data from CSV to the OpenImagesDetectionChallengeEvaluator format."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+
+from pycocotools import mask
+from object_detection.core import standard_fields
+
+
+def _to_normalized_box(mask_np):
+  """Decodes binary segmentation masks into np.arrays and boxes.
+
+  Args:
+    mask_np: np.ndarray of size NxWxH.
+
+  Returns:
+    a np.ndarray of the size Nx4, each row containing normalized coordinates
+    [YMin, XMin, YMax, XMax] of a box computed of axis parallel enclosing box of
+    a mask.
+  """
+  coord1, coord2 = np.nonzero(mask_np)
+  if coord1.size > 0:
+    ymin = float(min(coord1)) / mask_np.shape[0]
+    ymax = float(max(coord1) + 1) / mask_np.shape[0]
+    xmin = float(min(coord2)) / mask_np.shape[1]
+    xmax = float((max(coord2) + 1)) / mask_np.shape[1]
+
+    return np.array([ymin, xmin, ymax, xmax])
+  else:
+    return np.array([0.0, 0.0, 0.0, 0.0])
+
+
+def _decode_raw_data_into_masks_and_boxes(segments, image_widths,
+                                          image_heights):
+  """Decods binary segmentation masks into np.arrays and boxes.
+
+  Args:
+    segments: pandas Series object containing either None entries or strings
+    with COCO-encoded binary masks. All masks are expected to be the same size.
+    image_widths: pandas Series of mask widths.
+    image_heights: pandas Series of mask heights.
+
+  Returns:
+    a np.ndarray of the size NxWxH, where W and H is determined from the encoded
+    masks; for the None values, zero arrays of size WxH are created. if input
+    contains only None values, W=1, H=1.
+  """
+  segment_masks = []
+  segment_boxes = []
+  ind = segments.first_valid_index()
+  if ind is not None:
+    size = [int(image_heights.iloc[ind]), int(image_widths[ind])]
+  else:
+    # It does not matter which size we pick since no masks will ever be
+    # evaluated.
+    size = [1, 1]
+  for segment, im_width, im_height in zip(segments, image_widths,
+                                          image_heights):
+    if pd.isnull(segment):
+      segment_masks.append(np.zeros([1, size[0], size[1]], dtype=np.uint8))
+      segment_boxes.append(np.expand_dims(np.array([0.0, 0.0, 0.0, 0.0]), 0))
+    else:
+      encoding_dict = {'size': [im_height, im_width], 'counts': segment}
+      mask_tensor = mask.decode(encoding_dict)
+
+      segment_masks.append(np.expand_dims(mask_tensor, 0))
+      segment_boxes.append(np.expand_dims(_to_normalized_box(mask_tensor), 0))
+
+  return np.concatenate(
+      segment_masks, axis=0), np.concatenate(
+          segment_boxes, axis=0)
+
+
+def merge_boxes_and_masks(box_data, mask_data):
+  return pd.merge(
+      box_data,
+      mask_data,
+      how='outer',
+      on=['LabelName', 'ImageID', 'XMin', 'XMax', 'YMin', 'YMax', 'IsGroupOf'])
+
+
+def build_groundtruth_dictionary(data, class_label_map):
+  """Builds a groundtruth dictionary from groundtruth data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the groundtruth data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+
+  Returns:
+    A dictionary with keys suitable for passing to
+    OpenImagesDetectionChallengeEvaluator.add_single_ground_truth_image_info:
+        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array
+          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of
+          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        standard_fields.InputDataFields.groundtruth_classes: integer numpy array
+          of shape [num_boxes] containing 1-indexed groundtruth classes for the
+          boxes.
+        standard_fields.InputDataFields.verified_labels: integer 1D numpy array
+          containing all classes for which labels are verified.
+        standard_fields.InputDataFields.groundtruth_group_of: Optional length
+          M numpy boolean array denoting whether a groundtruth box contains a
+          group of instances.
+  """
+  data_location = data[data.XMin.notnull()]
+  data_labels = data[data.ConfidenceImageLabel.notnull()]
+
+  dictionary = {
+      standard_fields.InputDataFields.groundtruth_boxes:
+          data_location[['YMin', 'XMin', 'YMax', 'XMax']].as_matrix(),
+      standard_fields.InputDataFields.groundtruth_classes:
+          data_location['LabelName'].map(lambda x: class_label_map[x]
+                                        ).as_matrix(),
+      standard_fields.InputDataFields.groundtruth_group_of:
+          data_location['IsGroupOf'].as_matrix().astype(int),
+      standard_fields.InputDataFields.groundtruth_image_classes:
+          data_labels['LabelName'].map(lambda x: class_label_map[x]
+                                      ).as_matrix(),
+  }
+
+  if 'Mask' in data_location:
+    segments, _ = _decode_raw_data_into_masks_and_boxes(
+        data_location['Mask'], data_location['ImageWidth'],
+        data_location['ImageHeight'])
+    dictionary[
+        standard_fields.InputDataFields.groundtruth_instance_masks] = segments
+
+  return dictionary
+
+
+def build_predictions_dictionary(data, class_label_map):
+  """Builds a predictions dictionary from predictions data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the predictions data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+
+  Returns:
+    Dictionary with keys suitable for passing to
+    OpenImagesDetectionChallengeEvaluator.add_single_detected_image_info:
+        standard_fields.DetectionResultFields.detection_boxes: float32 numpy
+          array of shape [num_boxes, 4] containing `num_boxes` detection boxes
+          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        standard_fields.DetectionResultFields.detection_scores: float32 numpy
+          array of shape [num_boxes] containing detection scores for the boxes.
+        standard_fields.DetectionResultFields.detection_classes: integer numpy
+          array of shape [num_boxes] containing 1-indexed detection classes for
+          the boxes.
+
+  """
+  dictionary = {
+      standard_fields.DetectionResultFields.detection_classes:
+          data['LabelName'].map(lambda x: class_label_map[x]).as_matrix(),
+      standard_fields.DetectionResultFields.detection_scores:
+          data['Score'].as_matrix()
+  }
+
+  if 'Mask' in data:
+    segments, boxes = _decode_raw_data_into_masks_and_boxes(
+        data['Mask'], data['ImageWidth'], data['ImageHeight'])
+    dictionary[standard_fields.DetectionResultFields.detection_masks] = segments
+    dictionary[standard_fields.DetectionResultFields.detection_boxes] = boxes
+  else:
+    dictionary[standard_fields.DetectionResultFields.detection_boxes] = data[[
+        'YMin', 'XMin', 'YMax', 'XMax'
+    ]].as_matrix()
+
+  return dictionary
diff --git a/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
new file mode 100644
index 00000000..02892cab
--- /dev/null
+++ b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
@@ -0,0 +1,269 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for oid_od_challenge_evaluation_util."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+from pycocotools import mask
+import tensorflow as tf
+
+from object_detection.core import standard_fields
+from object_detection.metrics import oid_challenge_evaluation_utils as utils
+
+
+class OidUtilTest(tf.test.TestCase):
+
+  def testMaskToNormalizedBox(self):
+    mask_np = np.array([[0, 0, 0, 0], [0, 1, 0, 0], [0, 1, 0, 0], [0, 0, 0, 0]])
+    box = utils._to_normalized_box(mask_np)
+    self.assertAllEqual(np.array([0.25, 0.25, 0.75, 0.5]), box)
+    mask_np = np.array([[0, 0, 0, 0], [0, 1, 0, 1], [0, 1, 0, 1], [0, 1, 1, 1]])
+    box = utils._to_normalized_box(mask_np)
+    self.assertAllEqual(np.array([0.25, 0.25, 1.0, 1.0]), box)
+    mask_np = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])
+    box = utils._to_normalized_box(mask_np)
+    self.assertAllEqual(np.array([0.0, 0.0, 0.0, 0.0]), box)
+
+  def testDecodeToTensors(self):
+    mask1 = np.array([[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0]], dtype=np.uint8)
+    mask2 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], dtype=np.uint8)
+
+    encoding1 = mask.encode(np.asfortranarray(mask1))
+    encoding2 = mask.encode(np.asfortranarray(mask2))
+
+    vals = pd.Series([encoding1['counts'], encoding2['counts']])
+    image_widths = pd.Series([mask1.shape[1], mask2.shape[1]])
+    image_heights = pd.Series([mask1.shape[0], mask2.shape[0]])
+
+    segm, bbox = utils._decode_raw_data_into_masks_and_boxes(
+        vals, image_widths, image_heights)
+    expected_segm = np.concatenate(
+        [np.expand_dims(mask1, 0),
+         np.expand_dims(mask2, 0)], axis=0)
+    expected_bbox = np.array([[0.0, 0.5, 2.0 / 3.0, 1.0], [0, 0, 0, 0]])
+    self.assertAllEqual(expected_segm, segm)
+    self.assertAllEqual(expected_bbox, bbox)
+
+
+class OidChallengeEvaluationUtilTest(tf.test.TestCase):
+
+  def testBuildGroundtruthDictionaryBoxes(self):
+    np_data = pd.DataFrame(
+        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 1, None],
+         ['fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0, None],
+         ['fe58ec1b06db2bb7', '/m/04bcr3', None, None, None, None, None, 1],
+         ['fe58ec1b06db2bb7', '/m/083vt', None, None, None, None, None, 0],
+         ['fe58ec1b06db2bb7', '/m/02gy9n', None, None, None, None, None, 1]],
+        columns=[
+            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'IsGroupOf',
+            'ConfidenceImageLabel'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    groundtruth_dictionary = utils.build_groundtruth_dictionary(
+        np_data, class_label_map)
+
+    self.assertIn(standard_fields.InputDataFields.groundtruth_boxes,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_classes,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_group_of,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_image_classes,
+                  groundtruth_dictionary)
+
+    self.assertAllEqual(
+        np.array([1, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_classes])
+    self.assertAllEqual(
+        np.array([1, 0]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_group_of])
+
+    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2]])
+
+    self.assertNDArrayNear(
+        expected_boxes_data, groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_boxes], 1e-5)
+    self.assertAllEqual(
+        np.array([1, 2, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_image_classes])
+
+  def testBuildPredictionDictionaryBoxes(self):
+    np_data = pd.DataFrame(
+        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 0.1],
+         ['fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0.2],
+         ['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.1, 0.2, 0.3, 0.3]],
+        columns=[
+            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'Score'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    prediction_dictionary = utils.build_predictions_dictionary(
+        np_data, class_label_map)
+
+    self.assertIn(standard_fields.DetectionResultFields.detection_boxes,
+                  prediction_dictionary)
+    self.assertIn(standard_fields.DetectionResultFields.detection_classes,
+                  prediction_dictionary)
+    self.assertIn(standard_fields.DetectionResultFields.detection_scores,
+                  prediction_dictionary)
+
+    self.assertAllEqual(
+        np.array([1, 3, 1]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_classes])
+    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2],
+                                    [0.2, 0.0, 0.3, 0.1]])
+    self.assertNDArrayNear(
+        expected_boxes_data, prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_boxes], 1e-5)
+    self.assertNDArrayNear(
+        np.array([0.1, 0.2, 0.3]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_scores], 1e-5)
+
+  def testBuildGroundtruthDictionaryMasks(self):
+    mask1 = np.array([[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],
+                     dtype=np.uint8)
+    mask2 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                     dtype=np.uint8)
+
+    encoding1 = mask.encode(np.asfortranarray(mask1))
+    encoding2 = mask.encode(np.asfortranarray(mask2))
+
+    np_data = pd.DataFrame(
+        [[
+            'fe58ec1b06db2bb7', mask1.shape[1], mask1.shape[0], '/m/04bcr3',
+            0.0, 0.3, 0.5, 0.6, 0, None, encoding1['counts']
+        ],
+         [
+             'fe58ec1b06db2bb7', None, None, '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 1,
+             None, None
+         ],
+         [
+             'fe58ec1b06db2bb7', mask2.shape[1], mask2.shape[0], '/m/02gy9n',
+             0.5, 0.6, 0.8, 0.9, 0, None, encoding2['counts']
+         ],
+         [
+             'fe58ec1b06db2bb7', None, None, '/m/04bcr3', None, None, None,
+             None, None, 1, None
+         ],
+         [
+             'fe58ec1b06db2bb7', None, None, '/m/083vt', None, None, None, None,
+             None, 0, None
+         ],
+         [
+             'fe58ec1b06db2bb7', None, None, '/m/02gy9n', None, None, None,
+             None, None, 1, None
+         ]],
+        columns=[
+            'ImageID', 'ImageWidth', 'ImageHeight', 'LabelName', 'XMin', 'XMax',
+            'YMin', 'YMax', 'IsGroupOf', 'ConfidenceImageLabel', 'Mask'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    groundtruth_dictionary = utils.build_groundtruth_dictionary(
+        np_data, class_label_map)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_boxes,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_classes,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_group_of,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_image_classes,
+                  groundtruth_dictionary)
+    self.assertIn(standard_fields.InputDataFields.groundtruth_instance_masks,
+                  groundtruth_dictionary)
+    self.assertAllEqual(
+        np.array([1, 3, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_classes])
+    self.assertAllEqual(
+        np.array([0, 1, 0]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_group_of])
+
+    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2],
+                                    [0.8, 0.5, 0.9, 0.6]])
+
+    self.assertNDArrayNear(
+        expected_boxes_data, groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_boxes], 1e-5)
+    self.assertAllEqual(
+        np.array([1, 2, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_image_classes])
+
+    expected_segm = np.concatenate([
+        np.expand_dims(mask1, 0),
+        np.zeros((1, 4, 4), dtype=np.uint8),
+        np.expand_dims(mask2, 0)
+    ],
+                                   axis=0)
+    self.assertAllEqual(
+        expected_segm, groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_instance_masks])
+
+  def testBuildPredictionDictionaryMasks(self):
+    mask1 = np.array([[0, 0, 1, 1], [0, 0, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]],
+                     dtype=np.uint8)
+    mask2 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                     dtype=np.uint8)
+
+    encoding1 = mask.encode(np.asfortranarray(mask1))
+    encoding2 = mask.encode(np.asfortranarray(mask2))
+
+    np_data = pd.DataFrame(
+        [[
+            'fe58ec1b06db2bb7', mask1.shape[1], mask1.shape[0], '/m/04bcr3',
+            encoding1['counts'], 0.8
+        ],
+         [
+             'fe58ec1b06db2bb7', mask2.shape[1], mask2.shape[0], '/m/02gy9n',
+             encoding2['counts'], 0.6
+         ]],
+        columns=[
+            'ImageID', 'ImageWidth', 'ImageHeight', 'LabelName', 'Mask', 'Score'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/02gy9n': 3}
+    prediction_dictionary = utils.build_predictions_dictionary(
+        np_data, class_label_map)
+
+    self.assertIn(standard_fields.DetectionResultFields.detection_boxes,
+                  prediction_dictionary)
+    self.assertIn(standard_fields.DetectionResultFields.detection_classes,
+                  prediction_dictionary)
+    self.assertIn(standard_fields.DetectionResultFields.detection_scores,
+                  prediction_dictionary)
+    self.assertIn(standard_fields.DetectionResultFields.detection_masks,
+                  prediction_dictionary)
+
+    self.assertAllEqual(
+        np.array([1, 3]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_classes])
+
+    expected_boxes_data = np.array([[0.0, 0.5, 0.5, 1.0], [0, 0, 0, 0]])
+    self.assertNDArrayNear(
+        expected_boxes_data, prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_boxes], 1e-5)
+    self.assertNDArrayNear(
+        np.array([0.8, 0.6]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_scores], 1e-5)
+    expected_segm = np.concatenate(
+        [np.expand_dims(mask1, 0),
+         np.expand_dims(mask2, 0)], axis=0)
+    self.assertAllEqual(
+        expected_segm, prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_masks])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py b/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py
deleted file mode 100644
index bfd3ee3f..00000000
--- a/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py
+++ /dev/null
@@ -1,90 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-r"""Converts data from CSV to the OpenImagesDetectionChallengeEvaluator format.
-"""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-from object_detection.core import standard_fields
-
-
-def build_groundtruth_boxes_dictionary(data, class_label_map):
-  """Builds a groundtruth dictionary from groundtruth data in CSV file.
-
-  Args:
-    data: Pandas DataFrame with the groundtruth data for a single image.
-    class_label_map: Class labelmap from string label name to an integer.
-
-  Returns:
-    A dictionary with keys suitable for passing to
-    OpenImagesDetectionChallengeEvaluator.add_single_ground_truth_image_info:
-        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array
-          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of
-          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
-        standard_fields.InputDataFields.groundtruth_classes: integer numpy array
-          of shape [num_boxes] containing 1-indexed groundtruth classes for the
-          boxes.
-        standard_fields.InputDataFields.verified_labels: integer 1D numpy array
-          containing all classes for which labels are verified.
-        standard_fields.InputDataFields.groundtruth_group_of: Optional length
-          M numpy boolean array denoting whether a groundtruth box contains a
-          group of instances.
-  """
-  data_boxes = data[data.ConfidenceImageLabel.isnull()]
-  data_labels = data[data.XMin.isnull()]
-
-  return {
-      standard_fields.InputDataFields.groundtruth_boxes:
-          data_boxes[['YMin', 'XMin', 'YMax', 'XMax']].as_matrix(),
-      standard_fields.InputDataFields.groundtruth_classes:
-          data_boxes['LabelName'].map(lambda x: class_label_map[x]).as_matrix(),
-      standard_fields.InputDataFields.groundtruth_group_of:
-          data_boxes['IsGroupOf'].as_matrix().astype(int),
-      standard_fields.InputDataFields.groundtruth_image_classes:
-          data_labels['LabelName'].map(lambda x: class_label_map[x])
-          .as_matrix(),
-  }
-
-
-def build_predictions_dictionary(data, class_label_map):
-  """Builds a predictions dictionary from predictions data in CSV file.
-
-  Args:
-    data: Pandas DataFrame with the predictions data for a single image.
-    class_label_map: Class labelmap from string label name to an integer.
-
-  Returns:
-    Dictionary with keys suitable for passing to
-    OpenImagesDetectionChallengeEvaluator.add_single_detected_image_info:
-        standard_fields.DetectionResultFields.detection_boxes: float32 numpy
-          array of shape [num_boxes, 4] containing `num_boxes` detection boxes
-          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
-        standard_fields.DetectionResultFields.detection_scores: float32 numpy
-          array of shape [num_boxes] containing detection scores for the boxes.
-        standard_fields.DetectionResultFields.detection_classes: integer numpy
-          array of shape [num_boxes] containing 1-indexed detection classes for
-          the boxes.
-
-  """
-  return {
-      standard_fields.DetectionResultFields.detection_boxes:
-          data[['YMin', 'XMin', 'YMax', 'XMax']].as_matrix(),
-      standard_fields.DetectionResultFields.detection_classes:
-          data['LabelName'].map(lambda x: class_label_map[x]).as_matrix(),
-      standard_fields.DetectionResultFields.detection_scores:
-          data['Score'].as_matrix()
-  }
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py
deleted file mode 100644
index a7a1a585..00000000
--- a/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py
+++ /dev/null
@@ -1,103 +0,0 @@
-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Tests for oid_od_challenge_evaluation_util."""
-
-from __future__ import absolute_import
-from __future__ import division
-from __future__ import print_function
-
-import numpy as np
-import pandas as pd
-import tensorflow as tf
-from object_detection.core import standard_fields
-from object_detection.metrics import oid_od_challenge_evaluation_utils as utils
-
-
-class OidOdChallengeEvaluationUtilTest(tf.test.TestCase):
-
-  def testBuildGroundtruthDictionary(self):
-    np_data = pd.DataFrame(
-        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 1, None], [
-            'fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0, None
-        ], ['fe58ec1b06db2bb7', '/m/04bcr3', None, None, None, None, None, 1], [
-            'fe58ec1b06db2bb7', '/m/083vt', None, None, None, None, None, 0
-        ], ['fe58ec1b06db2bb7', '/m/02gy9n', None, None, None, None, None, 1]],
-        columns=[
-            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'IsGroupOf',
-            'ConfidenceImageLabel'
-        ])
-    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
-    groundtruth_dictionary = utils.build_groundtruth_boxes_dictionary(
-        np_data, class_label_map)
-
-    self.assertTrue(standard_fields.InputDataFields.groundtruth_boxes in
-                    groundtruth_dictionary)
-    self.assertTrue(standard_fields.InputDataFields.groundtruth_classes in
-                    groundtruth_dictionary)
-    self.assertTrue(standard_fields.InputDataFields.groundtruth_group_of in
-                    groundtruth_dictionary)
-    self.assertTrue(standard_fields.InputDataFields.groundtruth_image_classes in
-                    groundtruth_dictionary)
-
-    self.assertAllEqual(
-        np.array([1, 3]), groundtruth_dictionary[
-            standard_fields.InputDataFields.groundtruth_classes])
-    self.assertAllEqual(
-        np.array([1, 0]), groundtruth_dictionary[
-            standard_fields.InputDataFields.groundtruth_group_of])
-
-    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2]])
-
-    self.assertNDArrayNear(
-        expected_boxes_data, groundtruth_dictionary[
-            standard_fields.InputDataFields.groundtruth_boxes], 1e-5)
-    self.assertAllEqual(
-        np.array([1, 2, 3]), groundtruth_dictionary[
-            standard_fields.InputDataFields.groundtruth_image_classes])
-
-  def testBuildPredictionDictionary(self):
-    np_data = pd.DataFrame(
-        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 0.1], [
-            'fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0.2
-        ], ['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.1, 0.2, 0.3, 0.3]],
-        columns=[
-            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'Score'
-        ])
-    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
-    prediction_dictionary = utils.build_predictions_dictionary(
-        np_data, class_label_map)
-
-    self.assertTrue(standard_fields.DetectionResultFields.detection_boxes in
-                    prediction_dictionary)
-    self.assertTrue(standard_fields.DetectionResultFields.detection_classes in
-                    prediction_dictionary)
-    self.assertTrue(standard_fields.DetectionResultFields.detection_scores in
-                    prediction_dictionary)
-
-    self.assertAllEqual(
-        np.array([1, 3, 1]), prediction_dictionary[
-            standard_fields.DetectionResultFields.detection_classes])
-    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2],
-                                    [0.2, 0.0, 0.3, 0.1]])
-    self.assertNDArrayNear(
-        expected_boxes_data, prediction_dictionary[
-            standard_fields.DetectionResultFields.detection_boxes], 1e-5)
-    self.assertNDArrayNear(
-        np.array([0.1, 0.2, 0.3]), prediction_dictionary[
-            standard_fields.DetectionResultFields.detection_scores], 1e-5)
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
index 20b93e66..7a56c6bc 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
@@ -17,7 +17,7 @@ r"""Runs evaluation using OpenImages groundtruth and predictions.
 Example usage:
 python \
 models/research/object_detection/metrics/oid_vrd_challenge_evaluation.py \
-    --input_annotations_boxes=/path/to/input/annotations-human-bbox.csv \
+    --input_annotations_vrd=/path/to/input/annotations-human-bbox.csv \
     --input_annotations_labels=/path/to/input/annotations-label.csv \
     --input_class_labelmap=/path/to/input/class_labelmap.pbtxt \
     --input_relationship_labelmap=/path/to/input/relationship_labelmap.pbtxt \
@@ -126,7 +126,7 @@ if __name__ == '__main__':
       description=
       'Evaluate Open Images Visual Relationship Detection predictions.')
   parser.add_argument(
-      '--input_annotations_boxes',
+      '--input_annotations_vrd',
       required=True,
       help='File with groundtruth vrd annotations.')
   parser.add_argument(
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index b79320c1..4a429980 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -187,6 +187,46 @@ def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
   return unbatched_tensor_dict
 
 
+def _provide_groundtruth(model, labels):
+  """Provides the labels to a model as groundtruth.
+
+  This helper function extracts the corresponding boxes, classes,
+  keypoints, weights, masks, etc. from the labels, and provides it
+  as groundtruth to the models.
+
+  Args:
+    model: The detection model to provide groundtruth to.
+    labels: The labels for the training or evaluation inputs.
+  """
+  gt_boxes_list = labels[fields.InputDataFields.groundtruth_boxes]
+  gt_classes_list = labels[fields.InputDataFields.groundtruth_classes]
+  gt_masks_list = None
+  if fields.InputDataFields.groundtruth_instance_masks in labels:
+    gt_masks_list = labels[
+        fields.InputDataFields.groundtruth_instance_masks]
+  gt_keypoints_list = None
+  if fields.InputDataFields.groundtruth_keypoints in labels:
+    gt_keypoints_list = labels[fields.InputDataFields.groundtruth_keypoints]
+  gt_weights_list = None
+  if fields.InputDataFields.groundtruth_weights in labels:
+    gt_weights_list = labels[fields.InputDataFields.groundtruth_weights]
+  gt_confidences_list = None
+  if fields.InputDataFields.groundtruth_confidences in labels:
+    gt_confidences_list = labels[
+        fields.InputDataFields.groundtruth_confidences]
+  gt_is_crowd_list = None
+  if fields.InputDataFields.groundtruth_is_crowd in labels:
+    gt_is_crowd_list = labels[fields.InputDataFields.groundtruth_is_crowd]
+  model.provide_groundtruth(
+      groundtruth_boxes_list=gt_boxes_list,
+      groundtruth_classes_list=gt_classes_list,
+      groundtruth_confidences_list=gt_confidences_list,
+      groundtruth_masks_list=gt_masks_list,
+      groundtruth_keypoints_list=gt_keypoints_list,
+      groundtruth_weights_list=gt_weights_list,
+      groundtruth_is_crowd_list=gt_is_crowd_list)
+
+
 def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
                     postprocess_on_cpu=False):
   """Creates a model function for `Estimator`.
@@ -247,33 +287,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
           labels, unpad_groundtruth_tensors=unpad_groundtruth_tensors)
 
     if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
-      gt_boxes_list = labels[fields.InputDataFields.groundtruth_boxes]
-      gt_classes_list = labels[fields.InputDataFields.groundtruth_classes]
-      gt_masks_list = None
-      if fields.InputDataFields.groundtruth_instance_masks in labels:
-        gt_masks_list = labels[
-            fields.InputDataFields.groundtruth_instance_masks]
-      gt_keypoints_list = None
-      if fields.InputDataFields.groundtruth_keypoints in labels:
-        gt_keypoints_list = labels[fields.InputDataFields.groundtruth_keypoints]
-      gt_weights_list = None
-      if fields.InputDataFields.groundtruth_weights in labels:
-        gt_weights_list = labels[fields.InputDataFields.groundtruth_weights]
-      gt_confidences_list = None
-      if fields.InputDataFields.groundtruth_confidences in labels:
-        gt_confidences_list = labels[
-            fields.InputDataFields.groundtruth_confidences]
-      gt_is_crowd_list = None
-      if fields.InputDataFields.groundtruth_is_crowd in labels:
-        gt_is_crowd_list = labels[fields.InputDataFields.groundtruth_is_crowd]
-      detection_model.provide_groundtruth(
-          groundtruth_boxes_list=gt_boxes_list,
-          groundtruth_classes_list=gt_classes_list,
-          groundtruth_confidences_list=gt_confidences_list,
-          groundtruth_masks_list=gt_masks_list,
-          groundtruth_keypoints_list=gt_keypoints_list,
-          groundtruth_weights_list=gt_weights_list,
-          groundtruth_is_crowd_list=gt_is_crowd_list)
+      _provide_groundtruth(detection_model, labels)
 
     preprocessed_images = features[fields.InputDataFields.image]
     if use_tpu and train_config.use_bfloat16:
diff --git a/research/object_detection/models/keras_models/mobilenet_v2.py b/research/object_detection/models/keras_models/mobilenet_v2.py
index cc093ece..c5acd73c 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2.py
@@ -225,7 +225,10 @@ class _LayersOverride(object):
 
     placeholder_with_default = tf.placeholder_with_default(
         input=input_tensor, shape=[None] + shape)
-    return tf.keras.layers.Input(tensor=placeholder_with_default)
+    if tf.executing_eagerly():
+      return tf.keras.layers.Input(shape=shape)
+    else:
+      return tf.keras.layers.Input(tensor=placeholder_with_default)
 
   # pylint: disable=unused-argument
   def ReLU(self, *args, **kwargs):
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index ca6f3686..827dfa95 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -17,6 +17,7 @@
 import functools
 import tensorflow as tf
 from object_detection.core import box_predictor
+from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
 
 slim = tf.contrib.slim
@@ -350,7 +351,8 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
                        'feature maps, found: {}'.format(
                            num_predictions_per_location_list))
     feature_channels = [
-        image_feature.shape[3].value for image_feature in image_features
+        shape_utils.get_dim_as_int(image_feature.shape[3])
+        for image_feature in image_features
     ]
     has_different_feature_channels = len(set(feature_channels)) > 1
     if has_different_feature_channels:
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor.py b/research/object_detection/predictors/convolutional_keras_box_predictor.py
index 90fcb0dd..283b89ad 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor.py
@@ -19,6 +19,7 @@ import collections
 import tensorflow as tf
 
 from object_detection.core import box_predictor
+from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
 
 keras = tf.keras.layers
@@ -371,7 +372,8 @@ class WeightSharedConvolutionalBoxPredictor(box_predictor.KerasBoxPredictor):
   def build(self, input_shapes):
     """Creates the variables of the layer."""
     feature_channels = [
-        input_shape[3].value for input_shape in input_shapes
+        shape_utils.get_dim_as_int(input_shape[3])
+        for input_shape in input_shapes
     ]
     has_different_feature_channels = len(set(feature_channels)) > 1
     if has_different_feature_channels:
diff --git a/research/object_detection/predictors/heads/keras_mask_head.py b/research/object_detection/predictors/heads/keras_mask_head.py
index 5e6b6c2e..86cc48e3 100644
--- a/research/object_detection/predictors/heads/keras_mask_head.py
+++ b/research/object_detection/predictors/heads/keras_mask_head.py
@@ -24,6 +24,7 @@ import tensorflow as tf
 
 from object_detection.predictors.heads import head
 from object_detection.utils import ops
+from object_detection.utils import shape_utils
 
 
 class ConvolutionalMaskHead(head.KerasHead):
@@ -254,8 +255,10 @@ class MaskRCNNMaskHead(head.KerasHead):
     if self._convolve_then_upsample:
       # Replace Transposed Convolution with a Nearest Neighbor upsampling step
       # followed by 3x3 convolution.
-      height_scale = self._mask_height / input_shapes[1].value
-      width_scale = self._mask_width / input_shapes[2].value
+      height_scale = self._mask_height / shape_utils.get_dim_as_int(
+          input_shapes[1])
+      width_scale = self._mask_width / shape_utils.get_dim_as_int(
+          input_shapes[2])
       # pylint: disable=g-long-lambda
       self._mask_predictor_layers.append(tf.keras.layers.Lambda(
           lambda features: ops.nearest_neighbor_upsampling(
diff --git a/research/object_detection/predictors/rfcn_box_predictor.py b/research/object_detection/predictors/rfcn_box_predictor.py
index d16de044..a63ce203 100644
--- a/research/object_detection/predictors/rfcn_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_box_predictor.py
@@ -128,7 +128,7 @@ class RfcnBoxPredictor(box_predictor.BoxPredictor):
           crop_size=self._crop_size,
           num_spatial_bins=self._num_spatial_bins,
           global_pool=True)
-      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[2, 3])
+      box_encodings = tf.squeeze(box_encodings, axis=[2, 3])
       box_encodings = tf.reshape(box_encodings,
                                  [batch_size * num_boxes, 1, self.num_classes,
                                   self._box_code_size])
@@ -149,7 +149,7 @@ class RfcnBoxPredictor(box_predictor.BoxPredictor):
               num_spatial_bins=self._num_spatial_bins,
               global_pool=True))
       class_predictions_with_background = tf.squeeze(
-          class_predictions_with_background, squeeze_dims=[2, 3])
+          class_predictions_with_background, axis=[2, 3])
       class_predictions_with_background = tf.reshape(
           class_predictions_with_background,
           [batch_size * num_boxes, 1, total_classes])
diff --git a/research/object_detection/predictors/rfcn_keras_box_predictor.py b/research/object_detection/predictors/rfcn_keras_box_predictor.py
index 09582fa5..c7cc676e 100644
--- a/research/object_detection/predictors/rfcn_keras_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_keras_box_predictor.py
@@ -176,7 +176,7 @@ class RfcnKerasBoxPredictor(box_predictor.KerasBoxPredictor):
         crop_size=self._crop_size,
         num_spatial_bins=self._num_spatial_bins,
         global_pool=True)
-    box_encodings = tf.squeeze(box_encodings, squeeze_dims=[2, 3])
+    box_encodings = tf.squeeze(box_encodings, axis=[2, 3])
     box_encodings = tf.reshape(box_encodings,
                                [batch_size * num_boxes, 1, self.num_classes,
                                 self._box_code_size])
@@ -193,7 +193,7 @@ class RfcnKerasBoxPredictor(box_predictor.KerasBoxPredictor):
             num_spatial_bins=self._num_spatial_bins,
             global_pool=True))
     class_predictions_with_background = tf.squeeze(
-        class_predictions_with_background, squeeze_dims=[2, 3])
+        class_predictions_with_background, axis=[2, 3])
     class_predictions_with_background = tf.reshape(
         class_predictions_with_background,
         [batch_size * num_boxes, 1, self._total_classes])
diff --git a/research/object_detection/protos/eval.proto b/research/object_detection/protos/eval.proto
index ed5a3a9c..ec05f7ff 100644
--- a/research/object_detection/protos/eval.proto
+++ b/research/object_detection/protos/eval.proto
@@ -76,4 +76,8 @@ message EvalConfig {
 
   // If True, additionally include per-category metrics.
   optional bool include_metrics_per_category = 24 [default=false];
+
+  // Recall range within which precision should be computed.
+  optional float recall_lower_bound = 26 [default = 0.0];
+  optional float recall_upper_bound = 27 [default = 1.0];
 }
diff --git a/research/object_detection/protos/post_processing.proto b/research/object_detection/protos/post_processing.proto
index e9610a5e..d9bb0d58 100644
--- a/research/object_detection/protos/post_processing.proto
+++ b/research/object_detection/protos/post_processing.proto
@@ -22,6 +22,20 @@ message BatchNonMaxSuppression {
 
   // Whether to use the implementation of NMS that guarantees static shapes.
   optional bool use_static_shapes = 6 [default = false];
+
+  // Whether to use class agnostic NMS.
+  // Class-agnostic NMS function implements a class-agnostic version
+  // of Non Maximal Suppression where if max_classes_per_detection=k,
+  // 1) we keep the top-k scores for each detection and
+  // 2) during NMS, each detection only uses the highest class score for sorting.
+  // 3) Compared to regular NMS, the worst runtime of this version is O(N^2)
+  // instead of O(KN^2) where N is the number of detections and K the number of
+  // classes.
+  optional bool use_class_agnostic_nms = 7 [default = false];
+
+  // Number of classes retained per detection in class agnostic NMS.
+
+  optional int32 max_classes_per_detection = 8 [default = 1];
 }
 
 // Configuration proto for post-processing predicted boxes and
diff --git a/research/object_detection/tpu_exporters/faster_rcnn.py b/research/object_detection/tpu_exporters/faster_rcnn.py
index caddd26e..9122c6a3 100644
--- a/research/object_detection/tpu_exporters/faster_rcnn.py
+++ b/research/object_detection/tpu_exporters/faster_rcnn.py
@@ -87,7 +87,7 @@ def get_prediction_tensor_shapes(pipeline_config):
 
   _, input_tensors = exporter.input_placeholder_fn_map['image_tensor']()
 
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
 
   prediction_dict = detection_model.predict(preprocessed_inputs,
@@ -125,7 +125,7 @@ def build_graph(pipeline_config,
       exporter.input_placeholder_fn_map[input_type]()
 
   # CPU pre-processing
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
 
   # Dimshuffle: [b, h, w, c] -> [b, c, h, w]
diff --git a/research/object_detection/tpu_exporters/ssd.py b/research/object_detection/tpu_exporters/ssd.py
index 2dcfd130..6d434794 100644
--- a/research/object_detection/tpu_exporters/ssd.py
+++ b/research/object_detection/tpu_exporters/ssd.py
@@ -57,7 +57,7 @@ def get_prediction_tensor_shapes(pipeline_config):
   detection_model = model_builder.build(
       pipeline_config.model, is_training=False)
   _, input_tensors = exporter.input_placeholder_fn_map['image_tensor']()
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
   prediction_dict = detection_model.predict(preprocessed_inputs,
                                             true_image_shapes)
@@ -138,7 +138,7 @@ def build_graph(pipeline_config,
   placeholder_tensor, input_tensors = \
       exporter.input_placeholder_fn_map[input_type]()
 
-  inputs = tf.to_float(input_tensors)
+  inputs = tf.cast(input_tensors, dtype=tf.float32)
   preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
 
   # Dimshuffle: (b, h, w, c) -> (b, c, h, w)
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 8b62f709..aee981d8 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -47,20 +47,34 @@ def exponential_decay_with_burnin(global_step,
     staircase: whether use staircase decay.
 
   Returns:
-    a (scalar) float tensor representing learning rate
+    If executing eagerly:
+      returns a no-arg callable that outputs the (scalar)
+      float tensor learning rate given the current value of global_step.
+    If in a graph:
+      immediately returns a (scalar) float tensor representing learning rate.
   """
   if burnin_learning_rate == 0:
     burnin_learning_rate = learning_rate_base
-  post_burnin_learning_rate = tf.train.exponential_decay(
-      learning_rate_base,
-      global_step - burnin_steps,
-      learning_rate_decay_steps,
-      learning_rate_decay_factor,
-      staircase=staircase)
-  return tf.maximum(tf.where(
-      tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),
-      tf.constant(burnin_learning_rate),
-      post_burnin_learning_rate), min_learning_rate, name='learning_rate')
+
+  def eager_decay_rate():
+    """Callable to compute the learning rate."""
+    post_burnin_learning_rate = tf.train.exponential_decay(
+        learning_rate_base,
+        global_step - burnin_steps,
+        learning_rate_decay_steps,
+        learning_rate_decay_factor,
+        staircase=staircase)
+    if callable(post_burnin_learning_rate):
+      post_burnin_learning_rate = post_burnin_learning_rate()
+    return tf.maximum(tf.where(
+        tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),
+        tf.constant(burnin_learning_rate),
+        post_burnin_learning_rate), min_learning_rate, name='learning_rate')
+
+  if tf.executing_eagerly():
+    return eager_decay_rate
+  else:
+    return eager_decay_rate()
 
 
 def cosine_decay_with_warmup(global_step,
@@ -88,7 +102,11 @@ def cosine_decay_with_warmup(global_step,
       before decaying.
 
   Returns:
-    a (scalar) float tensor representing learning rate.
+    If executing eagerly:
+      returns a no-arg callable that outputs the (scalar)
+      float tensor learning rate given the current value of global_step.
+    If in a graph:
+      immediately returns a (scalar) float tensor representing learning rate.
 
   Raises:
     ValueError: if warmup_learning_rate is larger than learning_rate_base,
@@ -97,24 +115,32 @@ def cosine_decay_with_warmup(global_step,
   if total_steps < warmup_steps:
     raise ValueError('total_steps must be larger or equal to '
                      'warmup_steps.')
-  learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(
-      np.pi *
-      (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps
-      ) / float(total_steps - warmup_steps - hold_base_rate_steps)))
-  if hold_base_rate_steps > 0:
-    learning_rate = tf.where(global_step > warmup_steps + hold_base_rate_steps,
-                             learning_rate, learning_rate_base)
-  if warmup_steps > 0:
-    if learning_rate_base < warmup_learning_rate:
-      raise ValueError('learning_rate_base must be larger or equal to '
-                       'warmup_learning_rate.')
-    slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
-    warmup_rate = slope * tf.cast(global_step,
-                                  tf.float32) + warmup_learning_rate
-    learning_rate = tf.where(global_step < warmup_steps, warmup_rate,
-                             learning_rate)
-  return tf.where(global_step > total_steps, 0.0, learning_rate,
-                  name='learning_rate')
+  def eager_decay_rate():
+    """Callable to compute the learning rate."""
+    learning_rate = 0.5 * learning_rate_base * (1 + tf.cos(
+        np.pi *
+        (tf.cast(global_step, tf.float32) - warmup_steps - hold_base_rate_steps
+        ) / float(total_steps - warmup_steps - hold_base_rate_steps)))
+    if hold_base_rate_steps > 0:
+      learning_rate = tf.where(
+          global_step > warmup_steps + hold_base_rate_steps,
+          learning_rate, learning_rate_base)
+    if warmup_steps > 0:
+      if learning_rate_base < warmup_learning_rate:
+        raise ValueError('learning_rate_base must be larger or equal to '
+                         'warmup_learning_rate.')
+      slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
+      warmup_rate = slope * tf.cast(global_step,
+                                    tf.float32) + warmup_learning_rate
+      learning_rate = tf.where(global_step < warmup_steps, warmup_rate,
+                               learning_rate)
+    return tf.where(global_step > total_steps, 0.0, learning_rate,
+                    name='learning_rate')
+
+  if tf.executing_eagerly():
+    return eager_decay_rate
+  else:
+    return eager_decay_rate()
 
 
 def manual_stepping(global_step, boundaries, rates, warmup=False):
@@ -138,7 +164,11 @@ def manual_stepping(global_step, boundaries, rates, warmup=False):
       [0, boundaries[0]].
 
   Returns:
-    a (scalar) float tensor representing learning rate
+    If executing eagerly:
+      returns a no-arg callable that outputs the (scalar)
+      float tensor learning rate given the current value of global_step.
+    If in a graph:
+      immediately returns a (scalar) float tensor representing learning rate.
   Raises:
     ValueError: if one of the following checks fails:
       1. boundaries is a strictly increasing list of positive integers
@@ -168,8 +198,16 @@ def manual_stepping(global_step, boundaries, rates, warmup=False):
   else:
     boundaries = [0] + boundaries
   num_boundaries = len(boundaries)
-  rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
-                                      list(range(num_boundaries)),
-                                      [0] * num_boundaries))
-  return tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),
-                       name='learning_rate')
+
+  def eager_decay_rate():
+    """Callable to compute the learning rate."""
+    rate_index = tf.reduce_max(tf.where(
+        tf.greater_equal(global_step, boundaries),
+        list(range(num_boundaries)),
+        [0] * num_boundaries))
+    return tf.reduce_sum(rates * tf.one_hot(rate_index, depth=num_boundaries),
+                         name='learning_rate')
+  if tf.executing_eagerly():
+    return eager_decay_rate
+  else:
+    return eager_decay_rate()
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 474a8667..84facb50 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -70,6 +70,26 @@ class DetectionEvaluator(object):
     """
     self._categories = categories
 
+  def observe_result_dict_for_single_example(self, eval_dict):
+    """Observes an evaluation result dict for a single example.
+
+    When executing eagerly, once all observations have been observed by this
+    method you can use `.evaluate()` to get the final metrics.
+
+    When using `tf.estimator.Estimator` for evaluation this function is used by
+    `get_estimator_eval_metric_ops()` to construct the metric update op.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating an object
+        detection model, returned from
+        eval_util.result_dict_for_single_example().
+
+    Returns:
+      None when executing eagerly, or an update_op that can be used to update
+      the eval metrics in `tf.estimator.EstimatorSpec`.
+    """
+    raise NotImplementedError('Not implemented for this evaluator!')
+
   @abstractmethod
   def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
     """Adds groundtruth for a single image to be used for evaluation.
@@ -126,6 +146,8 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
   def __init__(self,
                categories,
                matching_iou_threshold=0.5,
+               recall_lower_bound=0.0,
+               recall_upper_bound=1.0,
                evaluate_corlocs=False,
                evaluate_precision_recall=False,
                metric_prefix=None,
@@ -140,6 +162,8 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
         'name': (required) string representing category name e.g., 'cat', 'dog'.
       matching_iou_threshold: IOU threshold to use for matching groundtruth
         boxes to detection boxes.
+      recall_lower_bound: lower bound of recall operating area.
+      recall_upper_bound: upper bound of recall operating area.
       evaluate_corlocs: (optional) boolean which determines if corloc scores
         are to be returned or not.
       evaluate_precision_recall: (optional) boolean which determines if
@@ -166,6 +190,8 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
     if min(cat['id'] for cat in categories) < 1:
       raise ValueError('Classes should be 1-indexed.')
     self._matching_iou_threshold = matching_iou_threshold
+    self._recall_lower_bound = recall_lower_bound
+    self._recall_upper_bound = recall_upper_bound
     self._use_weighted_mean_ap = use_weighted_mean_ap
     self._label_id_offset = 1
     self._evaluate_masks = evaluate_masks
@@ -173,6 +199,8 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
     self._evaluation = ObjectDetectionEvaluation(
         num_groundtruth_classes=self._num_classes,
         matching_iou_threshold=self._matching_iou_threshold,
+        recall_lower_bound=self._recall_lower_bound,
+        recall_upper_bound=self._recall_upper_bound,
         use_weighted_mean_ap=self._use_weighted_mean_ap,
         label_id_offset=self._label_id_offset,
         group_of_weight=self._group_of_weight)
@@ -195,11 +223,18 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
 
   def _build_metric_names(self):
     """Builds a list with metric names."""
-
-    self._metric_names = [
-        self._metric_prefix + 'Precision/mAP@{}IOU'.format(
-            self._matching_iou_threshold)
-    ]
+    if self._recall_lower_bound > 0.0 or self._recall_upper_bound < 1.0:
+      self._metric_names = [
+          self._metric_prefix +
+          'Precision/mAP@{}IOU@[{:.1f},{:.1f}]Recall'.format(
+              self._matching_iou_threshold, self._recall_lower_bound,
+              self._recall_upper_bound)
+      ]
+    else:
+      self._metric_names = [
+          self._metric_prefix +
+          'Precision/mAP@{}IOU'.format(self._matching_iou_threshold)
+      ]
     if self._evaluate_corlocs:
       self._metric_names.append(
           self._metric_prefix +
@@ -493,6 +528,24 @@ class WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):
         use_weighted_mean_ap=True)
 
 
+class PrecisionAtRecallDetectionEvaluator(ObjectDetectionEvaluator):
+  """A class to evaluate detections using precision@recall metrics."""
+
+  def __init__(self,
+               categories,
+               matching_iou_threshold=0.5,
+               recall_lower_bound=0.0,
+               recall_upper_bound=1.0):
+    super(PrecisionAtRecallDetectionEvaluator, self).__init__(
+        categories,
+        matching_iou_threshold=matching_iou_threshold,
+        recall_lower_bound=recall_lower_bound,
+        recall_upper_bound=recall_upper_bound,
+        evaluate_corlocs=False,
+        metric_prefix='PrecisionAtRecallBoxes',
+        use_weighted_mean_ap=False)
+
+
 class PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):
   """A class to evaluate instance masks using PASCAL metrics."""
 
@@ -540,6 +593,7 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
   def __init__(self,
                categories,
                matching_iou_threshold=0.5,
+               evaluate_masks=False,
                evaluate_corlocs=False,
                metric_prefix='OpenImagesV2',
                group_of_weight=0.0):
@@ -551,6 +605,7 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
         'name': (required) string representing category name e.g., 'cat', 'dog'.
       matching_iou_threshold: IOU threshold to use for matching groundtruth
         boxes to detection boxes.
+      evaluate_masks: if True, evaluator evaluates masks.
       evaluate_corlocs: if True, additionally evaluates and returns CorLoc.
       metric_prefix: Prefix name of the metric.
       group_of_weight: Weight of the group-of bounding box. If set to 0 (default
@@ -561,12 +616,14 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
         detection falls within a group-of box, weight group_of_weight is added
         to false negatives.
     """
+
     super(OpenImagesDetectionEvaluator, self).__init__(
         categories,
         matching_iou_threshold,
         evaluate_corlocs,
         metric_prefix=metric_prefix,
-        group_of_weight=group_of_weight)
+        group_of_weight=group_of_weight,
+        evaluate_masks=evaluate_masks)
     self._expected_keys = set([
         standard_fields.InputDataFields.key,
         standard_fields.InputDataFields.groundtruth_boxes,
@@ -576,6 +633,11 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
         standard_fields.DetectionResultFields.detection_scores,
         standard_fields.DetectionResultFields.detection_classes,
     ])
+    if evaluate_masks:
+      self._expected_keys.add(
+          standard_fields.InputDataFields.groundtruth_instance_masks)
+      self._expected_keys.add(
+          standard_fields.DetectionResultFields.detection_masks)
 
   def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
     """Adds groundtruth for a single image to be used for evaluation.
@@ -617,17 +679,26 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
         logging.warn(
             'image %s does not have groundtruth group_of flag specified',
             image_id)
+    if self._evaluate_masks:
+      groundtruth_masks = groundtruth_dict[
+          standard_fields.InputDataFields.groundtruth_instance_masks]
+    else:
+      groundtruth_masks = None
+
     self._evaluation.add_single_ground_truth_image_info(
         image_id,
         groundtruth_dict[standard_fields.InputDataFields.groundtruth_boxes],
         groundtruth_classes,
         groundtruth_is_difficult_list=None,
-        groundtruth_is_group_of_list=groundtruth_group_of)
+        groundtruth_is_group_of_list=groundtruth_group_of,
+        groundtruth_masks=groundtruth_masks)
     self._image_ids.update([image_id])
 
 
-class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
-  """A class implements Open Images Challenge Detection metrics.
+class OpenImagesChallengeEvaluator(OpenImagesDetectionEvaluator):
+  """A class implements Open Images Challenge metrics.
+
+    Both Detection and Instance Segmentation evaluation metrics are implemented.
 
     Open Images Challenge Detection metric has two major changes in comparison
     with Open Images V2 detection metric:
@@ -637,10 +708,25 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
     evaluation: in case in image has neither positive nor negative image level
     label of class c, all detections of this class on this image will be
     ignored.
+
+    Open Images Challenge Instance Segmentation metric allows to measure per
+    formance of models in case of incomplete annotations: some instances are
+    annotations only on box level and some - on image-level. In addition,
+    image-level labels are taken into account as in detection metric.
+
+    Open Images Challenge Detection metric default parameters:
+    evaluate_masks = False
+    group_of_weight = 1.0
+
+
+    Open Images Challenge Instance Segmentation metric default parameters:
+    evaluate_masks = True
+    (group_of_weight will not matter)
   """
 
   def __init__(self,
                categories,
+               evaluate_masks=False,
                matching_iou_threshold=0.5,
                evaluate_corlocs=False,
                group_of_weight=1.0):
@@ -650,35 +736,34 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
       categories: A list of dicts, each of which has the following keys -
         'id': (required) an integer id uniquely identifying this category.
         'name': (required) string representing category name e.g., 'cat', 'dog'.
+      evaluate_masks: set to true for instance segmentation metric and to false
+        for detection metric.
       matching_iou_threshold: IOU threshold to use for matching groundtruth
         boxes to detection boxes.
       evaluate_corlocs: if True, additionally evaluates and returns CorLoc.
       group_of_weight: weight of a group-of box. If set to 0, detections of the
         correct class within a group-of box are ignored. If weight is > 0
-        (default for Open Images Detection Challenge 2018), then if at least one
+        (default for Open Images Detection Challenge), then if at least one
         detection falls within a group-of box with matching_iou_threshold,
         weight group_of_weight is added to true positives. Consequently, if no
         detection falls within a group-of box, weight group_of_weight is added
         to false negatives.
     """
-    super(OpenImagesDetectionChallengeEvaluator, self).__init__(
+    if not evaluate_masks:
+      metrics_prefix = 'OpenImagesDetectionChallenge'
+    else:
+      metrics_prefix = 'OpenImagesInstanceSegmentationChallenge'
+    super(OpenImagesChallengeEvaluator, self).__init__(
         categories,
         matching_iou_threshold,
-        evaluate_corlocs,
-        metric_prefix='OpenImagesChallenge2018',
-        group_of_weight=group_of_weight)
+        evaluate_masks=evaluate_masks,
+        evaluate_corlocs=evaluate_corlocs,
+        group_of_weight=group_of_weight,
+        metric_prefix=metrics_prefix)
 
     self._evaluatable_labels = {}
-    self._expected_keys = set([
-        standard_fields.InputDataFields.key,
-        standard_fields.InputDataFields.groundtruth_boxes,
-        standard_fields.InputDataFields.groundtruth_classes,
-        standard_fields.InputDataFields.groundtruth_group_of,
-        standard_fields.InputDataFields.groundtruth_image_classes,
-        standard_fields.DetectionResultFields.detection_boxes,
-        standard_fields.DetectionResultFields.detection_scores,
-        standard_fields.DetectionResultFields.detection_classes,
-    ])
+    self._expected_keys.add(
+        standard_fields.InputDataFields.groundtruth_image_classes)
 
   def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
     """Adds groundtruth for a single image to be used for evaluation.
@@ -701,7 +786,7 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
     Raises:
       ValueError: On adding groundtruth for an image more than once.
     """
-    super(OpenImagesDetectionChallengeEvaluator,
+    super(OpenImagesChallengeEvaluator,
           self).add_single_ground_truth_image_info(image_id, groundtruth_dict)
     groundtruth_classes = (
         groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -
@@ -747,16 +832,22 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
     detected_scores = detections_dict[
         standard_fields.DetectionResultFields.detection_scores][allowed_classes]
 
+    if self._evaluate_masks:
+      detection_masks = detections_dict[standard_fields.DetectionResultFields
+                                        .detection_masks][allowed_classes]
+    else:
+      detection_masks = None
     self._evaluation.add_single_detected_image_info(
         image_key=image_id,
         detected_boxes=detected_boxes,
         detected_scores=detected_scores,
-        detected_class_labels=detection_classes)
+        detected_class_labels=detection_classes,
+        detected_masks=detection_masks)
 
   def clear(self):
     """Clears stored data."""
 
-    super(OpenImagesDetectionChallengeEvaluator, self).clear()
+    super(OpenImagesChallengeEvaluator, self).clear()
     self._evaluatable_labels.clear()
 
 
@@ -767,6 +858,73 @@ ObjectDetectionEvalMetrics = collections.namedtuple(
     ])
 
 
+class OpenImagesDetectionChallengeEvaluator(OpenImagesChallengeEvaluator):
+  """A class implements Open Images Detection Challenge metric."""
+
+  def __init__(self,
+               categories,
+               matching_iou_threshold=0.5,
+               evaluate_corlocs=False,
+               group_of_weight=1.0):
+    """Constructor.
+
+    Args:
+      categories: A list of dicts, each of which has the following keys -
+        'id': (required) an integer id uniquely identifying this category.
+        'name': (required) string representing category name e.g., 'cat', 'dog'.
+      matching_iou_threshold: IOU threshold to use for matching groundtruth
+        boxes to detection boxes.
+      evaluate_corlocs: if True, additionally evaluates and returns CorLoc.
+      group_of_weight: weight of a group-of box. If set to 0, detections of the
+        correct class within a group-of box are ignored. If weight is > 0
+        (default for Open Images Detection Challenge), then if at least one
+        detection falls within a group-of box with matching_iou_threshold,
+        weight group_of_weight is added to true positives. Consequently, if no
+        detection falls within a group-of box, weight group_of_weight is added
+        to false negatives.
+    """
+    super(OpenImagesDetectionChallengeEvaluator, self).__init__(
+        categories=categories,
+        evaluate_masks=False,
+        matching_iou_threshold=matching_iou_threshold,
+        evaluate_corlocs=False,
+        group_of_weight=1.0)
+
+
+class OpenImagesInstanceSegmentationChallengeEvaluator(
+    OpenImagesChallengeEvaluator):
+  """A class implements Open Images Instance Segmentation Challenge metric."""
+
+  def __init__(self,
+               categories,
+               matching_iou_threshold=0.5,
+               evaluate_corlocs=False,
+               group_of_weight=1.0):
+    """Constructor.
+
+    Args:
+      categories: A list of dicts, each of which has the following keys -
+        'id': (required) an integer id uniquely identifying this category.
+        'name': (required) string representing category name e.g., 'cat', 'dog'.
+      matching_iou_threshold: IOU threshold to use for matching groundtruth
+        boxes to detection boxes.
+      evaluate_corlocs: if True, additionally evaluates and returns CorLoc.
+      group_of_weight: weight of a group-of box. If set to 0, detections of the
+        correct class within a group-of box are ignored. If weight is > 0
+        (default for Open Images Detection Challenge), then if at least one
+        detection falls within a group-of box with matching_iou_threshold,
+        weight group_of_weight is added to true positives. Consequently, if no
+        detection falls within a group-of box, weight group_of_weight is added
+        to false negatives.
+    """
+    super(OpenImagesInstanceSegmentationChallengeEvaluator, self).__init__(
+        categories=categories,
+        evaluate_masks=True,
+        matching_iou_threshold=matching_iou_threshold,
+        evaluate_corlocs=False,
+        group_of_weight=1.0)
+
+
 class ObjectDetectionEvaluation(object):
   """Internal implementation of Pascal object detection metrics."""
 
@@ -775,6 +933,8 @@ class ObjectDetectionEvaluation(object):
                matching_iou_threshold=0.5,
                nms_iou_threshold=1.0,
                nms_max_output_boxes=10000,
+               recall_lower_bound=0.0,
+               recall_upper_bound=1.0,
                use_weighted_mean_ap=False,
                label_id_offset=0,
                group_of_weight=0.0,
@@ -788,6 +948,8 @@ class ObjectDetectionEvaluation(object):
       nms_iou_threshold: IOU threshold used for non-maximum suppression.
       nms_max_output_boxes: Maximum number of boxes returned by non-maximum
         suppression.
+      recall_lower_bound: lower bound of recall operating area
+      recall_upper_bound: upper bound of recall operating area
       use_weighted_mean_ap: (optional) boolean which determines if the mean
         average precision is computed directly from the scores and tp_fp_labels
         of all classes.
@@ -813,6 +975,8 @@ class ObjectDetectionEvaluation(object):
         nms_iou_threshold=nms_iou_threshold,
         nms_max_output_boxes=nms_max_output_boxes,
         group_of_weight=group_of_weight)
+    self.recall_lower_bound = recall_lower_bound
+    self.recall_upper_bound = recall_upper_bound
     self.group_of_weight = group_of_weight
     self.num_class = num_groundtruth_classes
     self.use_weighted_mean_ap = use_weighted_mean_ap
@@ -1036,10 +1200,17 @@ class ObjectDetectionEvaluation(object):
         all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)
       precision, recall = metrics.compute_precision_recall(
           scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])
-
-      self.precisions_per_class[class_index] = precision
-      self.recalls_per_class[class_index] = recall
-      average_precision = metrics.compute_average_precision(precision, recall)
+      recall_within_bound_indices = [
+          index for index, value in enumerate(recall) if
+          value >= self.recall_lower_bound and value <= self.recall_upper_bound
+      ]
+      recall_within_bound = recall[recall_within_bound_indices]
+      precision_within_bound = precision[recall_within_bound_indices]
+
+      self.precisions_per_class[class_index] = precision_within_bound
+      self.recalls_per_class[class_index] = recall_within_bound
+      average_precision = metrics.compute_average_precision(
+          precision_within_bound, recall_within_bound)
       self.average_precision_per_class[class_index] = average_precision
       logging.info('average_precision: %f', average_precision)
 
@@ -1051,7 +1222,14 @@ class ObjectDetectionEvaluation(object):
       num_gt_instances = np.sum(self.num_gt_instances_per_class)
       precision, recall = metrics.compute_precision_recall(
           all_scores, all_tp_fp_labels, num_gt_instances)
-      mean_ap = metrics.compute_average_precision(precision, recall)
+      recall_within_bound_indices = [
+          index for index, value in enumerate(recall) if
+          value >= self.recall_lower_bound and value <= self.recall_upper_bound
+      ]
+      recall_within_bound = recall[recall_within_bound_indices]
+      precision_within_bound = precision[recall_within_bound_indices]
+      mean_ap = metrics.compute_average_precision(precision_within_bound,
+                                                  recall_within_bound)
     else:
       mean_ap = np.nanmean(self.average_precision_per_class)
     mean_corloc = np.nanmean(self.corloc_per_class)
diff --git a/research/object_detection/utils/object_detection_evaluation_test.py b/research/object_detection/utils/object_detection_evaluation_test.py
index a625c99d..51459710 100644
--- a/research/object_detection/utils/object_detection_evaluation_test.py
+++ b/research/object_detection/utils/object_detection_evaluation_test.py
@@ -101,9 +101,9 @@ class OpenImagesV2EvaluationTest(tf.test.TestCase):
     self.assertFalse(oiv2_evaluator._image_ids)
 
 
-class OpenImagesDetectionChallengeEvaluatorTest(tf.test.TestCase):
+class OpenImagesChallengeEvaluatorTest(tf.test.TestCase):
 
-  def test_returns_correct_metric_values(self):
+  def test_returns_correct_detection_metric_values(self):
     categories = [{
         'id': 1,
         'name': 'cat'
@@ -115,8 +115,8 @@ class OpenImagesDetectionChallengeEvaluatorTest(tf.test.TestCase):
         'name': 'elephant'
     }]
     oivchallenge_evaluator = (
-        object_detection_evaluation.OpenImagesDetectionChallengeEvaluator(
-            categories, group_of_weight=0.5))
+        object_detection_evaluation.OpenImagesChallengeEvaluator(
+            categories, evaluate_masks=False, group_of_weight=0.5))
 
     image_key = 'img1'
     groundtruth_boxes = np.array(
@@ -203,19 +203,124 @@ class OpenImagesDetectionChallengeEvaluatorTest(tf.test.TestCase):
                 detected_class_labels
         })
     metrics = oivchallenge_evaluator.evaluate()
+    expected_metric_name = 'OpenImagesDetectionChallenge'
 
     self.assertAlmostEqual(
-        metrics['OpenImagesChallenge2018_PerformanceByCategory/AP@0.5IOU/dog'],
+        metrics[
+            expected_metric_name + '_PerformanceByCategory/AP@0.5IOU/dog'],
         0.3333333333)
     self.assertAlmostEqual(
         metrics[
-            'OpenImagesChallenge2018_PerformanceByCategory/AP@0.5IOU/elephant'],
+            expected_metric_name + '_PerformanceByCategory/AP@0.5IOU/elephant'],
         0.333333333333)
     self.assertAlmostEqual(
-        metrics['OpenImagesChallenge2018_PerformanceByCategory/AP@0.5IOU/cat'],
+        metrics[
+            expected_metric_name + '_PerformanceByCategory/AP@0.5IOU/cat'],
         0.142857142857)
     self.assertAlmostEqual(
-        metrics['OpenImagesChallenge2018_Precision/mAP@0.5IOU'], 0.269841269)
+        metrics[expected_metric_name + '_Precision/mAP@0.5IOU'],
+        0.269841269)
+
+    oivchallenge_evaluator.clear()
+    self.assertFalse(oivchallenge_evaluator._image_ids)
+
+  def test_returns_correct_instance_segm_metric_values(self):
+    categories = [{'id': 1, 'name': 'cat'}, {'id': 2, 'name': 'dog'}]
+    oivchallenge_evaluator = (
+        object_detection_evaluation.OpenImagesChallengeEvaluator(
+            categories, evaluate_masks=True))
+
+    image_key = 'img1'
+    groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
+                                 dtype=float)
+    groundtruth_class_labels = np.array([1, 2, 1], dtype=int)
+    groundtruth_is_group_of_list = np.array([False, False, True], dtype=bool)
+    groundtruth_verified_labels = np.array([1, 2, 3], dtype=int)
+    groundtruth_mask_0 = np.array([[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    zero_mask = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                         dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0, zero_mask, zero_mask],
+                                 axis=0)
+    oivchallenge_evaluator.add_single_ground_truth_image_info(
+        image_key, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels,
+            standard_fields.InputDataFields.groundtruth_group_of:
+                groundtruth_is_group_of_list,
+            standard_fields.InputDataFields.groundtruth_image_classes:
+                groundtruth_verified_labels,
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+                groundtruth_masks
+        })
+    image_key = 'img3'
+    groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)
+    groundtruth_class_labels = np.array([2], dtype=int)
+    groundtruth_mask_0 = np.array([[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0], axis=0)
+    oivchallenge_evaluator.add_single_ground_truth_image_info(
+        image_key, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels,
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+                groundtruth_masks
+        })
+    image_key = 'img1'
+    detected_boxes = np.array([[0, 0, 2, 2], [2, 2, 3, 3]], dtype=float)
+    detection_mask_0 = np.array([[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks = np.stack([detection_mask_0, zero_mask], axis=0)
+    detected_class_labels = np.array([2, 1], dtype=int)
+    detected_scores = np.array([0.7, 0.8], dtype=float)
+    oivchallenge_evaluator.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_boxes,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_labels,
+            standard_fields.DetectionResultFields.detection_masks:
+                detected_masks
+        })
+    image_key = 'img3'
+    detected_boxes = np.array([[0, 0, 1, 1]], dtype=float)
+    detected_class_labels = np.array([2], dtype=int)
+    detected_scores = np.array([0.5], dtype=float)
+    detected_mask_0 = np.array([[1, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                               dtype=np.uint8)
+    detected_masks = np.stack([detected_mask_0], axis=0)
+    oivchallenge_evaluator.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_boxes,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_labels,
+            standard_fields.DetectionResultFields.detection_masks:
+                detected_masks
+        })
+    metrics = oivchallenge_evaluator.evaluate()
+    expected_metric_name = 'OpenImagesInstanceSegmentationChallenge'
+
+    self.assertAlmostEqual(
+        metrics[
+            expected_metric_name + '_PerformanceByCategory/AP@0.5IOU/dog'],
+        0.5)
+    self.assertAlmostEqual(
+        metrics[
+            expected_metric_name + '_PerformanceByCategory/AP@0.5IOU/cat'],
+        0)
+    self.assertAlmostEqual(
+        metrics[
+            expected_metric_name + '_Precision/mAP@0.5IOU'],
+        0.25)
 
     oivchallenge_evaluator.clear()
     self.assertFalse(oivchallenge_evaluator._image_ids)
@@ -572,6 +677,157 @@ class WeightedPascalEvaluationTest(tf.test.TestCase):
            groundtruth_class_labels1})
 
 
+class PrecisionAtRecallEvaluationTest(tf.test.TestCase):
+
+  def setUp(self):
+    self.categories = [{
+        'id': 1,
+        'name': 'cat'
+    }, {
+        'id': 2,
+        'name': 'dog'
+    }, {
+        'id': 3,
+        'name': 'elephant'
+    }]
+
+  def create_and_add_common_ground_truth(self):
+    #  Add groundtruth
+    self.wp_eval = (
+        object_detection_evaluation.PrecisionAtRecallDetectionEvaluator(
+            self.categories, recall_lower_bound=0.0, recall_upper_bound=0.5))
+
+    image_key1 = 'img1'
+    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
+                                  dtype=float)
+    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)
+    self.wp_eval.add_single_ground_truth_image_info(
+        image_key1, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes1,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels1
+        })
+    # add 'img2' separately
+    image_key3 = 'img3'
+    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)
+    groundtruth_class_labels3 = np.array([2], dtype=int)
+    self.wp_eval.add_single_ground_truth_image_info(
+        image_key3, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes3,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels3
+        })
+
+  def add_common_detected(self):
+    image_key = 'img2'
+    detected_boxes = np.array(
+        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],
+        dtype=float)
+    detected_class_labels = np.array([1, 1, 3], dtype=int)
+    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)
+    self.wp_eval.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_boxes,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_labels
+        })
+
+  def test_returns_correct_metric_values(self):
+    self.create_and_add_common_ground_truth()
+    image_key2 = 'img2'
+    groundtruth_boxes2 = np.array(
+        [[10, 10, 11, 11], [500, 500, 510, 510], [10, 10, 12, 12]], dtype=float)
+    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)
+    self.wp_eval.add_single_ground_truth_image_info(
+        image_key2, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes2,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels2
+        })
+    self.add_common_detected()
+
+    metrics = self.wp_eval.evaluate()
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/cat'], 0.5 / 4)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'Precision/mAP@0.5IOU@[0.0,0.5]Recall'], 1. / (3 + 1 + 2) / 4)
+    self.wp_eval.clear()
+    self.assertFalse(self.wp_eval._image_ids)
+
+  def test_returns_correct_metric_values_with_difficult_list(self):
+    self.create_and_add_common_ground_truth()
+    image_key2 = 'img2'
+    groundtruth_boxes2 = np.array(
+        [[10, 10, 11, 11], [500, 500, 510, 510], [10, 10, 12, 12]], dtype=float)
+    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)
+    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)
+    self.wp_eval.add_single_ground_truth_image_info(
+        image_key2, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes2,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels2,
+            standard_fields.InputDataFields.groundtruth_difficult:
+                groundtruth_is_difficult_list2
+        })
+    self.add_common_detected()
+
+    metrics = self.wp_eval.evaluate()
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'PerformanceByCategory/AP@0.5IOU/cat'], 0.5 / 3)
+    self.assertAlmostEqual(
+        metrics[self.wp_eval._metric_prefix +
+                'Precision/mAP@0.5IOU@[0.0,0.5]Recall'], 1. / (3 + 1 + 2) / 3)
+    self.wp_eval.clear()
+    self.assertFalse(self.wp_eval._image_ids)
+
+  def test_value_error_on_duplicate_images(self):
+    #  Add groundtruth
+    self.wp_eval = (
+        object_detection_evaluation.PrecisionAtRecallDetectionEvaluator(
+            self.categories, recall_lower_bound=0.0, recall_upper_bound=0.5))
+    image_key1 = 'img1'
+    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
+                                  dtype=float)
+    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)
+    self.wp_eval.add_single_ground_truth_image_info(
+        image_key1, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes1,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels1
+        })
+    with self.assertRaises(ValueError):
+      self.wp_eval.add_single_ground_truth_image_info(
+          image_key1, {
+              standard_fields.InputDataFields.groundtruth_boxes:
+                  groundtruth_boxes1,
+              standard_fields.InputDataFields.groundtruth_classes:
+                  groundtruth_class_labels1
+          })
+
+
 class ObjectDetectionEvaluationTest(tf.test.TestCase):
 
   def setUp(self):
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index cb51411c..7d3e81c6 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -22,9 +22,15 @@ import tensorflow as tf
 
 from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
+from object_detection.utils import spatial_transform_ops as spatial_ops
 from object_detection.utils import static_shape
 
 
+matmul_crop_and_resize = spatial_ops.matmul_crop_and_resize
+multilevel_roi_align = spatial_ops.multilevel_roi_align
+native_crop_and_resize = spatial_ops.native_crop_and_resize
+
+
 def expanded_shape(orig_shape, start_dim, num_dims):
   """Inserts multiple ones into a shape vector.
 
@@ -176,16 +182,22 @@ def pad_to_multiple(tensor, multiple):
 
   if tensor_height is None:
     tensor_height = tf.shape(tensor)[1]
-    padded_tensor_height = tf.to_int32(
-        tf.ceil(tf.to_float(tensor_height) / tf.to_float(multiple))) * multiple
+    padded_tensor_height = tf.cast(
+        tf.ceil(
+            tf.cast(tensor_height, dtype=tf.float32) /
+            tf.cast(multiple, dtype=tf.float32)),
+        dtype=tf.int32) * multiple
   else:
     padded_tensor_height = int(
         math.ceil(float(tensor_height) / multiple) * multiple)
 
   if tensor_width is None:
     tensor_width = tf.shape(tensor)[2]
-    padded_tensor_width = tf.to_int32(
-        tf.ceil(tf.to_float(tensor_width) / tf.to_float(multiple))) * multiple
+    padded_tensor_width = tf.cast(
+        tf.ceil(
+            tf.cast(tensor_width, dtype=tf.float32) /
+            tf.cast(multiple, dtype=tf.float32)),
+        dtype=tf.int32) * multiple
   else:
     padded_tensor_width = int(
         math.ceil(float(tensor_width) / multiple) * multiple)
@@ -309,11 +321,11 @@ def indices_to_dense_vector(indices,
     dense 1D Tensor of shape [size] with indices set to indices_values and the
         rest set to default_value.
   """
-  size = tf.to_int32(size)
+  size = tf.cast(size, dtype=tf.int32)
   zeros = tf.ones([size], dtype=dtype) * default_value
   values = tf.ones_like(indices, dtype=dtype) * indices_value
 
-  return tf.dynamic_stitch([tf.range(size), tf.to_int32(indices)],
+  return tf.dynamic_stitch([tf.range(size), tf.cast(indices, dtype=tf.int32)],
                            [zeros, values])
 
 
@@ -469,8 +481,8 @@ def filter_groundtruth_with_nan_box_coordinates(tensor_dict):
     boxes.
   """
   groundtruth_boxes = tensor_dict[fields.InputDataFields.groundtruth_boxes]
-  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.to_int32(
-      tf.is_nan(groundtruth_boxes)), reduction_indices=[1]), 0)
+  nan_indicator_vector = tf.greater(tf.reduce_sum(tf.cast(
+      tf.is_nan(groundtruth_boxes), dtype=tf.int32), reduction_indices=[1]), 0)
   valid_indicator_vector = tf.logical_not(nan_indicator_vector)
   valid_indices = tf.where(valid_indicator_vector)
 
@@ -576,7 +588,6 @@ def normalize_to_target(inputs,
         trainable=trainable)
     if summarize:
       mean = tf.reduce_mean(target_norm)
-      mean = tf.Print(mean, ['NormalizeToTarget:', mean])
       tf.summary.scalar(tf.get_variable_scope().name, mean)
     lengths = epsilon + tf.sqrt(tf.reduce_sum(tf.square(inputs), dim, True))
     mult_shape = input_rank*[1]
@@ -754,7 +765,7 @@ def position_sensitive_crop_regions(image,
     position_sensitive_features = tf.add_n(image_crops) / len(image_crops)
     # Then average over spatial positions within the bins.
     position_sensitive_features = tf.reduce_mean(
-        position_sensitive_features, [1, 2], keep_dims=True)
+        position_sensitive_features, [1, 2], keepdims=True)
   else:
     # Reorder height/width to depth channel.
     block_size = bin_crop_size[0]
@@ -770,7 +781,7 @@ def position_sensitive_crop_regions(image,
         tf.batch_to_space_nd(position_sensitive_features,
                              block_shape=[1] + num_spatial_bins,
                              crops=tf.zeros((3, 2), dtype=tf.int32)),
-        squeeze_dims=[0])
+        axis=[0])
 
     # Reorder back the depth channel.
     if block_size >= 2:
@@ -908,7 +919,7 @@ def merge_boxes_with_multiple_labels(boxes,
         dtype=(tf.int64, tf.float32))
 
     merged_boxes = tf.reshape(merged_boxes, [-1, 4])
-    class_encodings = tf.to_int32(class_encodings)
+    class_encodings = tf.cast(class_encodings, dtype=tf.int32)
     class_encodings = tf.reshape(class_encodings, [-1, num_classes])
     confidence_encodings = tf.reshape(confidence_encodings, [-1, num_classes])
     merged_box_indices = tf.reshape(merged_box_indices, [-1])
@@ -983,132 +994,39 @@ def matmul_gather_on_zeroth_axis(params, indices, scope=None):
                       tf.stack(indices_shape + params_shape[1:]))
 
 
-def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
-  """Matrix multiplication based implementation of the crop and resize op.
-
-  Extracts crops from the input image tensor and bilinearly resizes them
-  (possibly with aspect ratio change) to a common output size specified by
-  crop_size. This is more general than the crop_to_bounding_box op which
-  extracts a fixed size slice from the input image and does not allow
-  resizing or aspect ratio change.
-
-  Returns a tensor with crops from the input image at positions defined at
-  the bounding box locations in boxes. The cropped boxes are all resized
-  (with bilinear interpolation) to a fixed size = `[crop_height, crop_width]`.
-  The result is a 5-D tensor `[batch, num_boxes, crop_height, crop_width,
-  depth]`.
-
-  Running time complexity:
-    O((# channels) * (# boxes) * (crop_size)^2 * M), where M is the number
-  of pixels of the longer edge of the image.
-
-  Note that this operation is meant to replicate the behavior of the standard
-  tf.image.crop_and_resize operation but there are a few differences.
-  Specifically:
-    1) The extrapolation value (the values that are interpolated from outside
-      the bounds of the image window) is always zero
-    2) Only XLA supported operations are used (e.g., matrix multiplication).
-    3) There is no `box_indices` argument --- to run this op on multiple images,
-      one must currently call this op independently on each image.
-    4) The `crop_size` parameter is assumed to be statically defined.
-      Moreover, the number of boxes must be strictly nonzero.
+def fpn_feature_levels(num_levels, unit_scale_index, image_ratio, boxes):
+  """Returns fpn feature level for each box based on its area.
+
+  See section 4.2 of https://arxiv.org/pdf/1612.03144.pdf for details.
 
   Args:
-    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,
-      `int16`, `int32`, `int64`, `half`, 'bfloat16', `float32`, `float64`.
-      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
-      Both `image_height` and `image_width` need to be positive.
-    boxes: A `Tensor` of type `float32` or 'bfloat16'.
-      A 3-D tensor of shape `[batch, num_boxes, 4]`. The boxes are specified in
-      normalized coordinates and are of the form `[y1, x1, y2, x2]`. A
-      normalized coordinate value of `y` is mapped to the image coordinate at
-      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image
-      height is mapped to `[0, image_height - 1] in image height coordinates.
-      We do allow y1 > y2, in which case the sampled crop is an up-down flipped
-      version of the original image. The width dimension is treated similarly.
-      Normalized coordinates outside the `[0, 1]` range are allowed, in which
-      case we use `extrapolation_value` to extrapolate the input image values.
-    crop_size: A list of two integers `[crop_height, crop_width]`. All
-      cropped image patches are resized to this size. The aspect ratio of the
-      image content is not preserved. Both `crop_height` and `crop_width` need
-      to be positive.
-    scope: A name for the operation (optional).
+    num_levels: An integer indicating the number of feature levels to crop boxes
+      from.
+    unit_scale_index: An 0-based integer indicating the index of feature map
+      which most closely matches the resolution of the pretrained model.
+    image_ratio: A float indicating the ratio of input image area to pretraining
+      image area.
+    boxes: A float tensor of shape [batch, num_boxes, 4] containing boxes of the
+      form [ymin, xmin, ymax, xmax] in normalized coordinates.
 
   Returns:
-    A 5-D tensor of shape `[batch, num_boxes, crop_height, crop_width, depth]`
+    An int32 tensor of shape [batch_size, num_boxes] containing feature indices.
   """
-  img_shape = tf.shape(image)
-  img_height = img_shape[1]
-  img_width = img_shape[2]
-
-  def _lin_space_weights(num, img_size):
-    if num > 1:
-      start_weights = tf.linspace(tf.to_float(img_size) - 1.0, 0.0, num)
-      stop_weights = tf.to_float(img_size) - 1.0 - start_weights
-    else:
-      start_weights = tf.ones([num], dtype=tf.float32) * \
-          .5 * (tf.to_float(img_size) - 1.0)
-      stop_weights = tf.ones([num], dtype=tf.float32) * \
-          .5 * (tf.to_float(img_size) - 1.0)
-    return (start_weights, stop_weights)
-
-  with tf.name_scope(scope, 'MatMulCropAndResize'):
-    y1_weights, y2_weights = _lin_space_weights(crop_size[0], img_height)
-    x1_weights, x2_weights = _lin_space_weights(crop_size[1], img_width)
-    y1_weights = tf.cast(y1_weights, boxes.dtype)
-    y2_weights = tf.cast(y2_weights, boxes.dtype)
-    x1_weights = tf.cast(x1_weights, boxes.dtype)
-    x2_weights = tf.cast(x2_weights, boxes.dtype)
-    [y1, x1, y2, x2] = tf.unstack(boxes, axis=2)
-
-    # Pixel centers of input image and grid points along height and width
-    image_idx_h = tf.cast(
-        tf.reshape(tf.range(img_height), (1, 1, 1, img_height)),
-        dtype=boxes.dtype)
-    image_idx_w = tf.cast(
-        tf.reshape(tf.range(img_width), (1, 1, 1, img_width)),
-        dtype=boxes.dtype)
-    grid_pos_h = tf.expand_dims(
-        tf.einsum('ab,c->abc', y1, y1_weights) +
-        tf.einsum('ab,c->abc', y2, y2_weights),
-        axis=3)
-    grid_pos_w = tf.expand_dims(
-        tf.einsum('ab,c->abc', x1, x1_weights) +
-        tf.einsum('ab,c->abc', x2, x2_weights),
-        axis=3)
-
-    # Create kernel matrices of pairwise kernel evaluations between pixel
-    # centers of image and grid points.
-    kernel_h = tf.nn.relu(1 - tf.abs(image_idx_h - grid_pos_h))
-    kernel_w = tf.nn.relu(1 - tf.abs(image_idx_w - grid_pos_w))
-
-    # Compute matrix multiplication between
-    # the spatial dimensions of the image
-    # and height-wise kernel using einsum.
-    intermediate_image = tf.einsum('abci,aiop->abcop', kernel_h, image)
-    # Compute matrix multiplication between the spatial dimensions of the
-    # intermediate_image and width-wise kernel using einsum.
-    return tf.einsum('abno,abcop->abcnp', kernel_w, intermediate_image)
-
-
-def native_crop_and_resize(image, boxes, crop_size, scope=None):
-  """Same as `matmul_crop_and_resize` but uses tf.image.crop_and_resize."""
-  def get_box_inds(proposals):
-    proposals_shape = proposals.get_shape().as_list()
-    if any(dim is None for dim in proposals_shape):
-      proposals_shape = tf.shape(proposals)
-    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)
-    multiplier = tf.expand_dims(
-        tf.range(start=0, limit=proposals_shape[0]), 1)
-    return tf.reshape(ones_mat * multiplier, [-1])
-
-  with tf.name_scope(scope, 'CropAndResize'):
-    cropped_regions = tf.image.crop_and_resize(
-        image, tf.reshape(boxes, [-1] + boxes.shape.as_list()[2:]),
-        get_box_inds(boxes), crop_size)
-    final_shape = tf.concat([tf.shape(boxes)[:2],
-                             tf.shape(cropped_regions)[1:]], axis=0)
-    return tf.reshape(cropped_regions, final_shape)
+  assert num_levels > 0, (
+      '`num_levels` must be > 0. Found {}'.format(num_levels))
+  assert unit_scale_index < num_levels and unit_scale_index >= 0, (
+      '`unit_scale_index` must be in [0, {}). Found {}.'.format(
+          num_levels, unit_scale_index))
+  box_height_width = boxes[:, :, 2:4] - boxes[:, :, 0:2]
+  areas_sqrt = tf.sqrt(tf.reduce_prod(box_height_width, axis=2))
+  log_2 = tf.cast(tf.log(2.0), dtype=boxes.dtype)
+  levels = tf.cast(
+      tf.floordiv(tf.log(areas_sqrt * image_ratio), log_2)
+      +
+      unit_scale_index,
+      dtype=tf.int32)
+  levels = tf.maximum(0, tf.minimum(num_levels - 1, levels))
+  return levels
 
 
 def bfloat16_to_float32_nested(tensor_nested):
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index c4f82e70..5bf74539 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -851,7 +851,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     # work as the usual crop and resize for just one channel.
     crop = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), boxes,
                                     box_ind, crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
 
     ps_crop_and_pool = ops.position_sensitive_crop_regions(
         tiled_image,
@@ -937,8 +937,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
           image, boxes, crop_size, num_spatial_bins, global_pool=False)
       with self.test_session() as sess:
         output = sess.run(ps_crop)
-
-      self.assertAllEqual(output, expected_output[crop_size_mult - 1])
+      self.assertAllClose(output, expected_output[crop_size_mult - 1])
 
   def test_position_sensitive_with_global_pool_false_and_do_global_pool(self):
     num_spatial_bins = [3, 2]
@@ -981,7 +980,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
       ps_crop = ops.position_sensitive_crop_regions(
           image, boxes, crop_size, num_spatial_bins, global_pool=False)
       ps_crop_and_pool = tf.reduce_mean(
-          ps_crop, reduction_indices=(1, 2), keep_dims=True)
+          ps_crop, reduction_indices=(1, 2), keepdims=True)
 
       with self.test_session() as sess:
         output = sess.run(ps_crop_and_pool)
@@ -1349,154 +1348,31 @@ class MatmulGatherOnZerothAxis(test_case.TestCase):
       self.assertAllClose(gather_output, expected_output)
 
 
-class OpsTestMatMulCropAndResize(test_case.TestCase):
-
-  def testMatMulCropAndResize2x2To1x1(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])
-
-    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
-    expected_output = [[[[[2.5]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize2x2To1x1Flipped(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])
-
-    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
-    boxes = np.array([[[1, 1, 0, 0]]], dtype=np.float32)
-    expected_output = [[[[[2.5]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize2x2To3x3(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])
-
-    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
-    expected_output = [[[[[1.0], [1.5], [2.0]],
-                         [[2.0], [2.5], [3.0]],
-                         [[3.0], [3.5], [4.0]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize2x2To3x3Flipped(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])
-
-    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
-    boxes = np.array([[[1, 1, 0, 0]]], dtype=np.float32)
-    expected_output = [[[[[4.0], [3.5], [3.0]],
-                         [[3.0], [2.5], [2.0]],
-                         [[2.0], [1.5], [1.0]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize3x3To2x2(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
-
-    image = np.array([[[[1], [2], [3]],
-                       [[4], [5], [6]],
-                       [[7], [8], [9]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1],
-                       [0, 0, .5, .5]]], dtype=np.float32)
-    expected_output = [[[[[1], [3]], [[7], [9]]],
-                        [[[1], [2]], [[4], [5]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize3x3To2x2_2Channels(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
-
-    image = np.array([[[[1, 0], [2, 1], [3, 2]],
-                       [[4, 3], [5, 4], [6, 5]],
-                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1],
-                       [0, 0, .5, .5]]], dtype=np.float32)
-    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
-                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testBatchMatMulCropAndResize3x3To2x2_2Channels(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
-
-    image = np.array([[[[1, 0], [2, 1], [3, 2]],
-                       [[4, 3], [5, 4], [6, 5]],
-                       [[7, 6], [8, 7], [9, 8]]],
-                      [[[1, 0], [2, 1], [3, 2]],
-                       [[4, 3], [5, 4], [6, 5]],
-                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1],
-                       [0, 0, .5, .5]],
-                      [[1, 1, 0, 0],
-                       [.5, .5, 0, 0]]], dtype=np.float32)
-    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
-                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]],
-                       [[[[9, 8], [7, 6]], [[3, 2], [1, 0]]],
-                        [[[5, 4], [4, 3]], [[2, 1], [1, 0]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testMatMulCropAndResize3x3To2x2Flipped(self):
-
-    def graph_fn(image, boxes):
-      return ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
-
-    image = np.array([[[[1], [2], [3]],
-                       [[4], [5], [6]],
-                       [[7], [8], [9]]]], dtype=np.float32)
-    boxes = np.array([[[1, 1, 0, 0],
-                       [.5, .5, 0, 0]]], dtype=np.float32)
-    expected_output = [[[[[9], [7]], [[3], [1]]],
-                        [[[5], [4]], [[2], [1]]]]]
-    crop_output = self.execute(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
-
-  def testInvalidInputShape(self):
-    image = tf.constant([[[1], [2]], [[3], [4]]], dtype=tf.float32)
-    boxes = tf.constant([[-1, -1, 1, 1]], dtype=tf.float32)
-    crop_size = [4, 4]
-    with self.assertRaises(ValueError):
-      _ = ops.matmul_crop_and_resize(image, boxes, crop_size)
-
-
-class OpsTestCropAndResize(test_case.TestCase):
-
-  def testBatchCropAndResize3x3To2x2_2Channels(self):
-
-    def graph_fn(image, boxes):
-      return ops.native_crop_and_resize(image, boxes, crop_size=[2, 2])
-
-    image = np.array([[[[1, 0], [2, 1], [3, 2]],
-                       [[4, 3], [5, 4], [6, 5]],
-                       [[7, 6], [8, 7], [9, 8]]],
-                      [[[1, 0], [2, 1], [3, 2]],
-                       [[4, 3], [5, 4], [6, 5]],
-                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
-    boxes = np.array([[[0, 0, 1, 1],
-                       [0, 0, .5, .5]],
-                      [[1, 1, 0, 0],
-                       [.5, .5, 0, 0]]], dtype=np.float32)
-    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
-                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]],
-                       [[[[9, 8], [7, 6]], [[3, 2], [1, 0]]],
-                        [[[5, 4], [4, 3]], [[2, 1], [1, 0]]]]]
-    crop_output = self.execute_cpu(graph_fn, [image, boxes])
-    self.assertAllClose(crop_output, expected_output)
+class FpnFeatureLevelsTest(test_case.TestCase):
+
+  def test_correct_fpn_levels(self):
+    image_size = 640
+    pretraininig_image_size = 224
+    image_ratio = image_size * 1.0 / pretraininig_image_size
+    boxes = np.array(
+        [
+            [
+                [0, 0, 111, 111],  # Level 0.
+                [0, 0, 113, 113],  # Level 1.
+                [0, 0, 223, 223],  # Level 1.
+                [0, 0, 225, 225],  # Level 2.
+                [0, 0, 449, 449]   # Level 3.
+            ],
+        ],
+        dtype=np.float32) / image_size
+
+    def graph_fn(boxes):
+      return ops.fpn_feature_levels(
+          num_levels=5, unit_scale_index=2, image_ratio=image_ratio,
+          boxes=boxes)
+
+    levels = self.execute(graph_fn, [boxes])
+    self.assertAllEqual([[0, 1, 1, 2, 3]], levels)
 
 
 class TestBfloat16ToFloat32(test_case.TestCase):
diff --git a/research/object_detection/utils/per_image_evaluation.py b/research/object_detection/utils/per_image_evaluation.py
index 0bc1533f..9623b3aa 100644
--- a/research/object_detection/utils/per_image_evaluation.py
+++ b/research/object_detection/utils/per_image_evaluation.py
@@ -42,7 +42,7 @@ class PerImageEvaluation(object):
     Args:
       num_groundtruth_classes: Number of ground truth object classes
       matching_iou_threshold: A ratio of area intersection to union, which is
-          the threshold to consider whether a detection is true positive or not
+        the threshold to consider whether a detection is true positive or not
       nms_iou_threshold: IOU threshold used in Non Maximum Suppression.
       nms_max_output_boxes: Number of maximum output boxes in NMS.
       group_of_weight: Weight of the group-of boxes.
@@ -53,11 +53,16 @@ class PerImageEvaluation(object):
     self.num_groundtruth_classes = num_groundtruth_classes
     self.group_of_weight = group_of_weight
 
-  def compute_object_detection_metrics(
-      self, detected_boxes, detected_scores, detected_class_labels,
-      groundtruth_boxes, groundtruth_class_labels,
-      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
-      detected_masks=None, groundtruth_masks=None):
+  def compute_object_detection_metrics(self,
+                                       detected_boxes,
+                                       detected_scores,
+                                       detected_class_labels,
+                                       groundtruth_boxes,
+                                       groundtruth_class_labels,
+                                       groundtruth_is_difficult_list,
+                                       groundtruth_is_group_of_list,
+                                       detected_masks=None,
+                                       groundtruth_masks=None):
     """Evaluates detections as being tp, fp or weighted from a single image.
 
     The evaluation is done in two stages:
@@ -68,25 +73,24 @@ class PerImageEvaluation(object):
 
     Args:
       detected_boxes: A float numpy array of shape [N, 4], representing N
-          regions of detected object regions.
-          Each row is of the format [y_min, x_min, y_max, x_max]
-      detected_scores: A float numpy array of shape [N, 1], representing
-          the confidence scores of the detected N object instances.
+        regions of detected object regions. Each row is of the format [y_min,
+        x_min, y_max, x_max]
+      detected_scores: A float numpy array of shape [N, 1], representing the
+        confidence scores of the detected N object instances.
       detected_class_labels: A integer numpy array of shape [N, 1], repreneting
-          the class labels of the detected N object instances.
+        the class labels of the detected N object instances.
       groundtruth_boxes: A float numpy array of shape [M, 4], representing M
-          regions of object instances in ground truth
+        regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
-          representing M class labels of object instances in ground truth
+        representing M class labels of object instances in ground truth
       groundtruth_is_difficult_list: A boolean numpy array of length M denoting
-          whether a ground truth box is a difficult instance or not
+        whether a ground truth box is a difficult instance or not
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
-          whether a ground truth box has group-of tag
-      detected_masks: (optional) A uint8 numpy array of shape
-        [N, height, width]. If not None, the metrics will be computed based
-        on masks.
-      groundtruth_masks: (optional) A uint8 numpy array of shape
-        [M, height, width].
+        whether a ground truth box has group-of tag
+      detected_masks: (optional) A uint8 numpy array of shape [N, height,
+        width]. If not None, the metrics will be computed based on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape [M, height,
+        width]. Can have empty masks, i.e. where all values are 0.
 
     Returns:
       scores: A list of C float numpy arrays. Each numpy array is of
@@ -124,29 +128,32 @@ class PerImageEvaluation(object):
 
     return scores, tp_fp_labels, is_class_correctly_detected_in_image
 
-  def _compute_cor_loc(self, detected_boxes, detected_scores,
-                       detected_class_labels, groundtruth_boxes,
-                       groundtruth_class_labels, detected_masks=None,
+  def _compute_cor_loc(self,
+                       detected_boxes,
+                       detected_scores,
+                       detected_class_labels,
+                       groundtruth_boxes,
+                       groundtruth_class_labels,
+                       detected_masks=None,
                        groundtruth_masks=None):
     """Compute CorLoc score for object detection result.
 
     Args:
       detected_boxes: A float numpy array of shape [N, 4], representing N
-          regions of detected object regions.
-          Each row is of the format [y_min, x_min, y_max, x_max]
-      detected_scores: A float numpy array of shape [N, 1], representing
-          the confidence scores of the detected N object instances.
+        regions of detected object regions. Each row is of the format [y_min,
+        x_min, y_max, x_max]
+      detected_scores: A float numpy array of shape [N, 1], representing the
+        confidence scores of the detected N object instances.
       detected_class_labels: A integer numpy array of shape [N, 1], repreneting
-          the class labels of the detected N object instances.
+        the class labels of the detected N object instances.
       groundtruth_boxes: A float numpy array of shape [M, 4], representing M
-          regions of object instances in ground truth
+        regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
-          representing M class labels of object instances in ground truth
-      detected_masks: (optional) A uint8 numpy array of shape
-        [N, height, width]. If not None, the scores will be computed based
-        on masks.
-      groundtruth_masks: (optional) A uint8 numpy array of shape
-        [M, height, width].
+        representing M class labels of object instances in ground truth
+      detected_masks: (optional) A uint8 numpy array of shape [N, height,
+        width]. If not None, the scores will be computed based on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape [M, height,
+        width].
 
     Returns:
       is_class_correctly_detected_in_image: a numpy integer array of
@@ -162,8 +169,7 @@ class PerImageEvaluation(object):
                                        groundtruth_masks is not None):
       raise ValueError(
           'If `detected_masks` is provided, then `groundtruth_masks` should '
-          'also be provided.'
-      )
+          'also be provided.')
 
     is_class_correctly_detected_in_image = np.zeros(
         self.num_groundtruth_classes, dtype=int)
@@ -184,23 +190,25 @@ class PerImageEvaluation(object):
 
     return is_class_correctly_detected_in_image
 
-  def _compute_is_class_correctly_detected_in_image(
-      self, detected_boxes, detected_scores, groundtruth_boxes,
-      detected_masks=None, groundtruth_masks=None):
+  def _compute_is_class_correctly_detected_in_image(self,
+                                                    detected_boxes,
+                                                    detected_scores,
+                                                    groundtruth_boxes,
+                                                    detected_masks=None,
+                                                    groundtruth_masks=None):
     """Compute CorLoc score for a single class.
 
     Args:
       detected_boxes: A numpy array of shape [N, 4] representing detected box
-          coordinates
+        coordinates
       detected_scores: A 1-d numpy array of length N representing classification
-          score
+        score
       groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
-          box coordinates
-      detected_masks: (optional) A np.uint8 numpy array of shape
-        [N, height, width]. If not None, the scores will be computed based
-        on masks.
-      groundtruth_masks: (optional) A np.uint8 numpy array of shape
-        [M, height, width].
+        box coordinates
+      detected_masks: (optional) A np.uint8 numpy array of shape [N, height,
+        width]. If not None, the scores will be computed based on masks.
+      groundtruth_masks: (optional) A np.uint8 numpy array of shape [M, height,
+        width].
 
     Returns:
       is_class_correctly_detected_in_image: An integer 1 or 0 denoting whether a
@@ -228,34 +236,38 @@ class PerImageEvaluation(object):
           return 1
     return 0
 
-  def _compute_tp_fp(self, detected_boxes, detected_scores,
-                     detected_class_labels, groundtruth_boxes,
-                     groundtruth_class_labels, groundtruth_is_difficult_list,
+  def _compute_tp_fp(self,
+                     detected_boxes,
+                     detected_scores,
+                     detected_class_labels,
+                     groundtruth_boxes,
+                     groundtruth_class_labels,
+                     groundtruth_is_difficult_list,
                      groundtruth_is_group_of_list,
-                     detected_masks=None, groundtruth_masks=None):
+                     detected_masks=None,
+                     groundtruth_masks=None):
     """Labels true/false positives of detections of an image across all classes.
 
     Args:
       detected_boxes: A float numpy array of shape [N, 4], representing N
-          regions of detected object regions.
-          Each row is of the format [y_min, x_min, y_max, x_max]
-      detected_scores: A float numpy array of shape [N, 1], representing
-          the confidence scores of the detected N object instances.
+        regions of detected object regions. Each row is of the format [y_min,
+        x_min, y_max, x_max]
+      detected_scores: A float numpy array of shape [N, 1], representing the
+        confidence scores of the detected N object instances.
       detected_class_labels: A integer numpy array of shape [N, 1], repreneting
-          the class labels of the detected N object instances.
+        the class labels of the detected N object instances.
       groundtruth_boxes: A float numpy array of shape [M, 4], representing M
-          regions of object instances in ground truth
+        regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
-          representing M class labels of object instances in ground truth
+        representing M class labels of object instances in ground truth
       groundtruth_is_difficult_list: A boolean numpy array of length M denoting
-          whether a ground truth box is a difficult instance or not
+        whether a ground truth box is a difficult instance or not
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
-          whether a ground truth box has group-of tag
-      detected_masks: (optional) A np.uint8 numpy array of shape
-        [N, height, width]. If not None, the scores will be computed based
-        on masks.
-      groundtruth_masks: (optional) A np.uint8 numpy array of shape
-        [M, height, width].
+        whether a ground truth box has group-of tag
+      detected_masks: (optional) A np.uint8 numpy array of shape [N, height,
+        width]. If not None, the scores will be computed based on masks.
+      groundtruth_masks: (optional) A np.uint8 numpy array of shape [M, height,
+        width].
 
     Returns:
       result_scores: A list of float numpy arrays. Each numpy array is of
@@ -293,34 +305,33 @@ class PerImageEvaluation(object):
           detected_boxes=detected_boxes_at_ith_class,
           detected_scores=detected_scores_at_ith_class,
           groundtruth_boxes=gt_boxes_at_ith_class,
-          groundtruth_is_difficult_list=
-          groundtruth_is_difficult_list_at_ith_class,
-          groundtruth_is_group_of_list=
-          groundtruth_is_group_of_list_at_ith_class,
+          groundtruth_is_difficult_list=groundtruth_is_difficult_list_at_ith_class,
+          groundtruth_is_group_of_list=groundtruth_is_group_of_list_at_ith_class,
           detected_masks=detected_masks_at_ith_class,
           groundtruth_masks=gt_masks_at_ith_class)
       result_scores.append(scores)
       result_tp_fp_labels.append(tp_fp_labels)
     return result_scores, result_tp_fp_labels
 
-  def _get_overlaps_and_scores_mask_mode(
-      self, detected_boxes, detected_scores, detected_masks, groundtruth_boxes,
-      groundtruth_masks, groundtruth_is_group_of_list):
+  def _get_overlaps_and_scores_mask_mode(self, detected_boxes, detected_scores,
+                                         detected_masks, groundtruth_boxes,
+                                         groundtruth_masks,
+                                         groundtruth_is_group_of_list):
     """Computes overlaps and scores between detected and groudntruth masks.
 
     Args:
       detected_boxes: A numpy array of shape [N, 4] representing detected box
-          coordinates
+        coordinates
       detected_scores: A 1-d numpy array of length N representing classification
-          score
+        score
       detected_masks: A uint8 numpy array of shape [N, height, width]. If not
-          None, the scores will be computed based on masks.
+        None, the scores will be computed based on masks.
       groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
-          box coordinates
+        box coordinates
       groundtruth_masks: A uint8 numpy array of shape [M, height, width].
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
-          whether a ground truth box has group-of tag. If a groundtruth box
-          is group-of box, every detection matching this box is ignored.
+        whether a ground truth box has group-of tag. If a groundtruth box is
+        group-of box, every detection matching this box is ignored.
 
     Returns:
       iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
@@ -348,24 +359,21 @@ class PerImageEvaluation(object):
     num_boxes = detected_boxlist.num_boxes()
     return iou, ioa, scores, num_boxes
 
-  def _get_overlaps_and_scores_box_mode(
-      self,
-      detected_boxes,
-      detected_scores,
-      groundtruth_boxes,
-      groundtruth_is_group_of_list):
+  def _get_overlaps_and_scores_box_mode(self, detected_boxes, detected_scores,
+                                        groundtruth_boxes,
+                                        groundtruth_is_group_of_list):
     """Computes overlaps and scores between detected and groudntruth boxes.
 
     Args:
       detected_boxes: A numpy array of shape [N, 4] representing detected box
-          coordinates
+        coordinates
       detected_scores: A 1-d numpy array of length N representing classification
-          score
+        score
       groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
-          box coordinates
+        box coordinates
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
-          whether a ground truth box has group-of tag. If a groundtruth box
-          is group-of box, every detection matching this box is ignored.
+        whether a ground truth box has group-of tag. If a groundtruth box is
+        group-of box, every detection matching this box is ignored.
 
     Returns:
       iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
@@ -390,31 +398,34 @@ class PerImageEvaluation(object):
     num_boxes = detected_boxlist.num_boxes()
     return iou, ioa, scores, num_boxes
 
-  def _compute_tp_fp_for_single_class(
-      self, detected_boxes, detected_scores, groundtruth_boxes,
-      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
-      detected_masks=None, groundtruth_masks=None):
+  def _compute_tp_fp_for_single_class(self,
+                                      detected_boxes,
+                                      detected_scores,
+                                      groundtruth_boxes,
+                                      groundtruth_is_difficult_list,
+                                      groundtruth_is_group_of_list,
+                                      detected_masks=None,
+                                      groundtruth_masks=None):
     """Labels boxes detected with the same class from the same image as tp/fp.
 
     Args:
       detected_boxes: A numpy array of shape [N, 4] representing detected box
-          coordinates
+        coordinates
       detected_scores: A 1-d numpy array of length N representing classification
-          score
+        score
       groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
-          box coordinates
+        box coordinates
       groundtruth_is_difficult_list: A boolean numpy array of length M denoting
-          whether a ground truth box is a difficult instance or not. If a
-          groundtruth box is difficult, every detection matching this box
-          is ignored.
+        whether a ground truth box is a difficult instance or not. If a
+        groundtruth box is difficult, every detection matching this box is
+        ignored.
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
-          whether a ground truth box has group-of tag. If a groundtruth box
-          is group-of box, every detection matching this box is ignored.
-      detected_masks: (optional) A uint8 numpy array of shape
-        [N, height, width]. If not None, the scores will be computed based
-        on masks.
-      groundtruth_masks: (optional) A uint8 numpy array of shape
-        [M, height, width].
+        whether a ground truth box has group-of tag. If a groundtruth box is
+        group-of box, every detection matching this box is ignored.
+      detected_masks: (optional) A uint8 numpy array of shape [N, height,
+        width]. If not None, the scores will be computed based on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape [M, height,
+        width].
 
     Returns:
       Two arrays of the same size, containing all boxes that were evaluated as
@@ -432,16 +443,39 @@ class PerImageEvaluation(object):
     if detected_masks is not None and groundtruth_masks is not None:
       mask_mode = True
 
+    iou = np.ndarray([0, 0])
+    ioa = np.ndarray([0, 0])
+    iou_mask = np.ndarray([0, 0])
+    ioa_mask = np.ndarray([0, 0])
     if mask_mode:
-      (iou, ioa, scores,
+      # For Instance Segmentation Evaluation on Open Images V5, not all boxed
+      # instances have corresponding segmentation annotations. Those boxes that
+      # dont have segmentation annotations are represented as empty masks in
+      # groundtruth_masks nd array.
+      mask_presence_indicator = (np.sum(groundtruth_masks, axis=(1, 2)) > 0)
+
+      (iou_mask, ioa_mask, scores,
        num_detected_boxes) = self._get_overlaps_and_scores_mask_mode(
            detected_boxes=detected_boxes,
            detected_scores=detected_scores,
            detected_masks=detected_masks,
-           groundtruth_boxes=groundtruth_boxes,
-           groundtruth_masks=groundtruth_masks,
-           groundtruth_is_group_of_list=groundtruth_is_group_of_list)
+           groundtruth_boxes=groundtruth_boxes[mask_presence_indicator, :],
+           groundtruth_masks=groundtruth_masks[mask_presence_indicator, :],
+           groundtruth_is_group_of_list=groundtruth_is_group_of_list[
+               mask_presence_indicator])
+      if sum(mask_presence_indicator) < len(mask_presence_indicator):
+        # Not all masks are present - some masks are empty
+        (iou, ioa, _,
+         num_detected_boxes) = self._get_overlaps_and_scores_box_mode(
+             detected_boxes=detected_boxes,
+             detected_scores=detected_scores,
+             groundtruth_boxes=groundtruth_boxes[~mask_presence_indicator, :],
+             groundtruth_is_group_of_list=groundtruth_is_group_of_list[
+                 ~mask_presence_indicator])
+      num_detected_boxes = detected_boxes.shape[0]
     else:
+      mask_presence_indicator = np.zeros(
+          groundtruth_is_group_of_list.shape, dtype=bool)
       (iou, ioa, scores,
        num_detected_boxes) = self._get_overlaps_and_scores_box_mode(
            detected_boxes=detected_boxes,
@@ -453,55 +487,135 @@ class PerImageEvaluation(object):
       return scores, np.zeros(num_detected_boxes, dtype=bool)
 
     tp_fp_labels = np.zeros(num_detected_boxes, dtype=bool)
-    is_matched_to_difficult_box = np.zeros(num_detected_boxes, dtype=bool)
-    is_matched_to_group_of_box = np.zeros(num_detected_boxes, dtype=bool)
-
-    # The evaluation is done in two stages:
-    # 1. All detections are matched to non group-of boxes; true positives are
-    #    determined and detections matched to difficult boxes are ignored.
-    # 2. Detections that are determined as false positives are matched against
-    #    group-of boxes and scored with weight w per ground truth box is
-    # matched.
-
-    # Tp-fp evaluation for non-group of boxes (if any).
-    if iou.shape[1] > 0:
-      groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[
-          ~groundtruth_is_group_of_list]
+    is_matched_to_box = np.zeros(num_detected_boxes, dtype=bool)
+    is_matched_to_difficult = np.zeros(num_detected_boxes, dtype=bool)
+    is_matched_to_group_of = np.zeros(num_detected_boxes, dtype=bool)
+
+    def compute_match_iou(iou, groundtruth_nongroup_of_is_difficult_list,
+                          is_box):
+      """Computes TP/FP for non group-of box matching.
+
+      The function updates the following local variables:
+        tp_fp_labels - if a box is matched to group-of
+        is_matched_to_difficult - the detections that were processed at this are
+          matched to difficult box.
+        is_matched_to_box - the detections that were processed at this stage are
+          marked as is_box.
+
+      Args:
+        iou: intersection-over-union matrix [num_gt_boxes]x[num_det_boxes].
+        groundtruth_nongroup_of_is_difficult_list: boolean that specifies if gt
+          box is difficult.
+        is_box: boolean that specifies if currently boxes or masks are
+          processed.
+      """
       max_overlap_gt_ids = np.argmax(iou, axis=1)
-      is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)
+      is_gt_detected = np.zeros(iou.shape[1], dtype=bool)
       for i in range(num_detected_boxes):
         gt_id = max_overlap_gt_ids[i]
-        if iou[i, gt_id] >= self.matching_iou_threshold:
+        is_evaluatable = (not tp_fp_labels[i] and
+                          not is_matched_to_difficult[i] and
+                          iou[i, gt_id] >= self.matching_iou_threshold and
+                          not is_matched_to_group_of[i])
+        if is_evaluatable:
           if not groundtruth_nongroup_of_is_difficult_list[gt_id]:
-            if not is_gt_box_detected[gt_id]:
+            if not is_gt_detected[gt_id]:
               tp_fp_labels[i] = True
-              is_gt_box_detected[gt_id] = True
+              is_gt_detected[gt_id] = True
+              is_matched_to_box[i] = is_box
           else:
-            is_matched_to_difficult_box[i] = True
-
-    scores_group_of = np.zeros(ioa.shape[1], dtype=float)
-    tp_fp_labels_group_of = self.group_of_weight * np.ones(
-        ioa.shape[1], dtype=float)
-    # Tp-fp evaluation for group of boxes.
-    if ioa.shape[1] > 0:
+            is_matched_to_difficult[i] = True
+
+    def compute_match_ioa(ioa, is_box):
+      """Computes TP/FP for group-of box matching.
+
+      The function updates the following local variables:
+        is_matched_to_group_of - if a box is matched to group-of
+        is_matched_to_box - the detections that were processed at this stage are
+          marked as is_box.
+
+      Args:
+        ioa: intersection-over-area matrix [num_gt_boxes]x[num_det_boxes].
+        is_box: boolean that specifies if currently boxes or masks are
+          processed.
+
+      Returns:
+        scores_group_of: of detections matched to group-of boxes
+        [num_groupof_matched].
+        tp_fp_labels_group_of: boolean array of size [num_groupof_matched], all
+          values are True.
+      """
+      scores_group_of = np.zeros(ioa.shape[1], dtype=float)
+      tp_fp_labels_group_of = self.group_of_weight * np.ones(
+          ioa.shape[1], dtype=float)
       max_overlap_group_of_gt_ids = np.argmax(ioa, axis=1)
       for i in range(num_detected_boxes):
         gt_id = max_overlap_group_of_gt_ids[i]
-        if (not tp_fp_labels[i] and not is_matched_to_difficult_box[i] and
-            ioa[i, gt_id] >= self.matching_iou_threshold):
-          is_matched_to_group_of_box[i] = True
+        is_evaluatable = (not tp_fp_labels[i] and
+                          not is_matched_to_difficult[i] and
+                          ioa[i, gt_id] >= self.matching_iou_threshold and
+                          not is_matched_to_group_of[i])
+        if is_evaluatable:
+          is_matched_to_group_of[i] = True
+          is_matched_to_box[i] = is_box
           scores_group_of[gt_id] = max(scores_group_of[gt_id], scores[i])
       selector = np.where((scores_group_of > 0) & (tp_fp_labels_group_of > 0))
       scores_group_of = scores_group_of[selector]
       tp_fp_labels_group_of = tp_fp_labels_group_of[selector]
 
-    return np.concatenate(
-        (scores[~is_matched_to_difficult_box
-                & ~is_matched_to_group_of_box],
-         scores_group_of)), np.concatenate(
-             (tp_fp_labels[~is_matched_to_difficult_box
-                           & ~is_matched_to_group_of_box].astype(float),
-              tp_fp_labels_group_of))
+      return scores_group_of, tp_fp_labels_group_of
+
+    # The evaluation is done in two stages:
+    # 1. Evaluate all objects that actually have instance level masks.
+    # 2. Evaluate all objects that are not already evaluated as boxes.
+    if iou_mask.shape[1] > 0:
+      groundtruth_is_difficult_mask_list = groundtruth_is_difficult_list[
+          mask_presence_indicator]
+      groundtruth_is_group_of_mask_list = groundtruth_is_group_of_list[
+          mask_presence_indicator]
+      compute_match_iou(
+          iou_mask,
+          groundtruth_is_difficult_mask_list[
+              ~groundtruth_is_group_of_mask_list],
+          is_box=False)
+
+    scores_mask_group_of = np.ndarray([0], dtype=float)
+    tp_fp_labels_mask_group_of = np.ndarray([0], dtype=float)
+    if ioa_mask.shape[1] > 0:
+      scores_mask_group_of, tp_fp_labels_mask_group_of = compute_match_ioa(
+          ioa_mask, is_box=False)
+
+    # Tp-fp evaluation for non-group of boxes (if any).
+    if iou.shape[1] > 0:
+      groundtruth_is_difficult_box_list = groundtruth_is_difficult_list[
+          ~mask_presence_indicator]
+      groundtruth_is_group_of_box_list = groundtruth_is_group_of_list[
+          ~mask_presence_indicator]
+      compute_match_iou(
+          iou,
+          groundtruth_is_difficult_box_list[~groundtruth_is_group_of_box_list],
+          is_box=True)
+
+    scores_box_group_of = np.ndarray([0], dtype=float)
+    tp_fp_labels_box_group_of = np.ndarray([0], dtype=float)
+    if ioa.shape[1] > 0:
+      scores_box_group_of, tp_fp_labels_box_group_of = compute_match_ioa(
+          ioa, is_box=True)
+
+    if mask_mode:
+      # Note: here crowds are treated as ignore regions.
+      valid_entries = (~is_matched_to_difficult & ~is_matched_to_group_of
+                       & ~is_matched_to_box)
+      return np.concatenate(
+          (scores[valid_entries], scores_mask_group_of)), np.concatenate(
+              (tp_fp_labels[valid_entries].astype(float),
+               tp_fp_labels_mask_group_of))
+    else:
+      valid_entries = (~is_matched_to_difficult & ~is_matched_to_group_of)
+      return np.concatenate(
+          (scores[valid_entries], scores_box_group_of)), np.concatenate(
+              (tp_fp_labels[valid_entries].astype(float),
+               tp_fp_labels_box_group_of))
 
   def _get_ith_class_arrays(self, detected_boxes, detected_scores,
                             detected_masks, detected_class_labels,
@@ -549,8 +663,11 @@ class PerImageEvaluation(object):
             detected_boxes_at_ith_class, detected_scores_at_ith_class,
             detected_masks_at_ith_class)
 
-  def _remove_invalid_boxes(self, detected_boxes, detected_scores,
-                            detected_class_labels, detected_masks=None):
+  def _remove_invalid_boxes(self,
+                            detected_boxes,
+                            detected_scores,
+                            detected_class_labels,
+                            detected_masks=None):
     """Removes entries with invalid boxes.
 
     A box is invalid if either its xmax is smaller than its xmin, or its ymax
diff --git a/research/object_detection/utils/per_image_evaluation_test.py b/research/object_detection/utils/per_image_evaluation_test.py
index a870843f..2135ed68 100644
--- a/research/object_detection/utils/per_image_evaluation_test.py
+++ b/research/object_detection/utils/per_image_evaluation_test.py
@@ -472,6 +472,158 @@ class SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
 
+class SingleClassTpFpEmptyMaskAndBoxesTest(tf.test.TestCase):
+
+  def setUp(self):
+    num_groundtruth_classes = 1
+    matching_iou_threshold_iou = 0.5
+    nms_iou_threshold = 1.0
+    nms_max_output_boxes = 10000
+    self.eval = per_image_evaluation.PerImageEvaluation(
+        num_groundtruth_classes, matching_iou_threshold_iou, nms_iou_threshold,
+        nms_max_output_boxes)
+
+  def test_mask_tp_and_ignore(self):
+    # GT: one box with mask, one without
+    # Det: One mask matches gt1, one matches box gt2 and is ignored
+    groundtruth_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 2]], dtype=float)
+    groundtruth_mask_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_mask_1 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0, groundtruth_mask_1],
+                                 axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False, False],
+                                                        dtype=bool)
+
+    detected_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 2]], dtype=float)
+    detected_scores = np.array([0.6, 0.8], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks = np.stack([detected_masks_0, detected_masks_1], axis=0)
+
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        detected_boxes, detected_scores, groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list, detected_masks,
+        groundtruth_masks)
+    expected_scores = np.array([0.6], dtype=float)
+    expected_tp_fp_labels = np.array([True], dtype=bool)
+
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
+  def test_mask_one_tp_one_fp(self):
+    # GT: one box with mask, one without
+    # Det: one mask matches gt1, one is fp (box does not match)
+    groundtruth_boxes = np.array([[0, 0, 2, 3], [2, 2, 4, 4]], dtype=float)
+    groundtruth_mask_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_mask_1 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0, groundtruth_mask_1],
+                                 axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False, False],
+                                                        dtype=bool)
+
+    detected_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 2]], dtype=float)
+    detected_scores = np.array([0.6, 0.8], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks = np.stack([detected_masks_0, detected_masks_1], axis=0)
+
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        detected_boxes,
+        detected_scores,
+        groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=detected_masks,
+        groundtruth_masks=groundtruth_masks)
+    expected_scores = np.array([0.8, 0.6], dtype=float)
+    expected_tp_fp_labels = np.array([False, True], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
+  def test_two_mask_one_gt_one_ignore(self):
+    # GT: one box with mask, one without.
+    # Det: two mask matches same gt, one is tp, one is passed down to box match
+    # and ignored.
+    groundtruth_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 3]], dtype=float)
+    groundtruth_mask_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_mask_1 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0, groundtruth_mask_1],
+                                 axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False, False],
+                                                        dtype=bool)
+
+    detected_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 3]], dtype=float)
+    detected_scores = np.array([0.6, 0.8], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks_1 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks = np.stack([detected_masks_0, detected_masks_1], axis=0)
+
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        detected_boxes,
+        detected_scores,
+        groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=detected_masks,
+        groundtruth_masks=groundtruth_masks)
+    expected_scores = np.array([0.8], dtype=float)
+    expected_tp_fp_labels = np.array([True], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
+  def test_two_mask_one_gt_one_fp(self):
+    # GT: one box with mask, one without.
+    # Det: two mask matches same gt, one is tp, one is passed down to box match
+    # and is fp.
+    groundtruth_boxes = np.array([[0, 0, 2, 3], [2, 3, 4, 6]], dtype=float)
+    groundtruth_mask_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_mask_1 = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],
+                                  dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_mask_0, groundtruth_mask_1],
+                                 axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False, False],
+                                                        dtype=bool)
+
+    detected_boxes = np.array([[0, 0, 2, 3], [0, 0, 2, 3]], dtype=float)
+    detected_scores = np.array([0.6, 0.8], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks_1 = np.array([[0, 1, 1, 0], [0, 0, 1, 0], [0, 0, 0, 0]],
+                                dtype=np.uint8)
+    detected_masks = np.stack([detected_masks_0, detected_masks_1], axis=0)
+
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        detected_boxes,
+        detected_scores,
+        groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=detected_masks,
+        groundtruth_masks=groundtruth_masks)
+    expected_scores = np.array([0.8, 0.6], dtype=float)
+    expected_tp_fp_labels = np.array([True, False], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
+
 class MultiClassesTpFpTest(tf.test.TestCase):
 
   def test_tp_fp(self):
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index b8b30fd9..71b3640a 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -20,6 +20,9 @@ import tensorflow as tf
 from object_detection.utils import static_shape
 
 
+get_dim_as_int = static_shape.get_dim_as_int
+
+
 def _is_tensor(t):
   """Returns a boolean indicating whether the input is a tensor.
 
@@ -365,3 +368,95 @@ def assert_box_normalized(boxes, maximum_normalized_coordinate=1.1):
           tf.less_equal(box_maximum, maximum_normalized_coordinate),
           tf.greater_equal(box_minimum, 0)),
       [boxes])
+
+
+def flatten_dimensions(inputs, first, last):
+  """Flattens `K-d` tensor along [first, last) dimensions.
+
+  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape
+  [D0, D1, ..., D(first) * D(first+1) * ... * D(last-1), D(last), ..., D(K-1)].
+
+  Example:
+  `inputs` is a tensor with initial shape [10, 5, 20, 20, 3].
+  new_tensor = flatten_dimensions(inputs, last=4, first=2)
+  new_tensor.shape -> [10, 100, 20, 3].
+
+  Args:
+    inputs: a tensor with shape [D0, D1, ..., D(K-1)].
+    first: first value for the range of dimensions to flatten.
+    last: last value for the range of dimensions to flatten. Note that the last
+      dimension itself is excluded.
+
+  Returns:
+    a tensor with shape
+    [D0, D1, ..., D(first) * D(first + 1) * ... * D(last - 1), D(last), ...,
+     D(K-1)].
+
+  Raises:
+    ValueError: if first and last arguments are incorrect.
+  """
+  if first >= inputs.shape.ndims or last > inputs.shape.ndims:
+    raise ValueError('`first` and `last` must be less than inputs.shape.ndims. '
+                     'found {} and {} respectively while ndims is {}'.format(
+                         first, last, inputs.shape.ndims))
+  shape = combined_static_and_dynamic_shape(inputs)
+  flattened_dim_prod = tf.reduce_prod(shape[first:last],
+                                      keepdims=True)
+  new_shape = tf.concat([shape[:first], flattened_dim_prod,
+                         shape[last:]], axis=0)
+  return tf.reshape(inputs, new_shape)
+
+
+def flatten_first_n_dimensions(inputs, n):
+  """Flattens `K-d` tensor along first n dimension to be a `(K-n+1)-d` tensor.
+
+  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape
+  [D0 * D1 * ... * D(n-1), D(n), ... D(K-1)].
+
+  Example:
+  `inputs` is a tensor with initial shape [10, 5, 20, 20, 3].
+  new_tensor = flatten_first_n_dimensions(inputs, 2)
+  new_tensor.shape -> [50, 20, 20, 3].
+
+  Args:
+    inputs: a tensor with shape [D0, D1, ..., D(K-1)].
+    n: The number of dimensions to flatten.
+
+  Returns:
+    a tensor with shape [D0 * D1 * ... * D(n-1), D(n), ... D(K-1)].
+  """
+  return flatten_dimensions(inputs, first=0, last=n)
+
+
+def expand_first_dimension(inputs, dims):
+  """Expands `K-d` tensor along first dimension to be a `(K+n-1)-d` tensor.
+
+  Converts `inputs` with shape [D0, D1, ..., D(K-1)] into a tensor of shape
+  [dims[0], dims[1], ..., dims[-1], D1, ..., D(k-1)].
+
+  Example:
+  `inputs` is a tensor with shape [50, 20, 20, 3].
+  new_tensor = expand_first_dimension(inputs, [10, 5]).
+  new_tensor.shape -> [10, 5, 20, 20, 3].
+
+  Args:
+    inputs: a tensor with shape [D0, D1, ..., D(K-1)].
+    dims: List with new dimensions to expand first axis into. The length of
+      `dims` is typically 2 or larger.
+
+  Returns:
+    a tensor with shape [dims[0], dims[1], ..., dims[-1], D1, ..., D(k-1)].
+  """
+  inputs_shape = combined_static_and_dynamic_shape(inputs)
+  expanded_shape = tf.stack(dims + inputs_shape[1:])
+
+  # Verify that it is possible to expand the first axis of inputs.
+  assert_op = tf.assert_equal(
+      inputs_shape[0], tf.reduce_prod(tf.stack(dims)),
+      message=('First dimension of `inputs` cannot be expanded into provided '
+               '`dims`'))
+
+  with tf.control_dependencies([assert_op]):
+    inputs_reshaped = tf.reshape(inputs, expanded_shape)
+
+  return inputs_reshaped
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index b2b33456..ab9ea7ed 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -333,5 +333,79 @@ class AssertShapeEqualTest(tf.test.TestCase):
                               tensor_b: np.zeros([5])})
 
 
+class FlattenExpandDimensionTest(tf.test.TestCase):
+
+  def test_flatten_given_dims(self):
+    inputs = tf.random_uniform([5, 2, 10, 10, 3])
+    actual_flattened = shape_utils.flatten_dimensions(inputs, first=1, last=3)
+    expected_flattened = tf.reshape(inputs, [5, 20, 10, 3])
+    with self.test_session() as sess:
+      (actual_flattened_np,
+       expected_flattened_np) = sess.run([actual_flattened, expected_flattened])
+    self.assertAllClose(expected_flattened_np, actual_flattened_np)
+
+  def test_raises_value_error_incorrect_dimensions(self):
+    inputs = tf.random_uniform([5, 2, 10, 10, 3])
+    with self.assertRaises(ValueError):
+      shape_utils.flatten_dimensions(inputs, first=0, last=6)
+
+  def test_flatten_first_two_dimensions(self):
+    inputs = tf.constant(
+        [
+            [[1, 2], [3, 4]],
+            [[5, 6], [7, 8]],
+            [[9, 10], [11, 12]]
+        ], dtype=tf.int32)
+    flattened_tensor = shape_utils.flatten_first_n_dimensions(
+        inputs, 2)
+    with self.test_session() as sess:
+      flattened_tensor_out = sess.run(flattened_tensor)
+
+    expected_output = [[1, 2],
+                       [3, 4],
+                       [5, 6],
+                       [7, 8],
+                       [9, 10],
+                       [11, 12]]
+    self.assertAllEqual(expected_output, flattened_tensor_out)
+
+  def test_expand_first_dimension(self):
+    inputs = tf.constant(
+        [
+            [1, 2],
+            [3, 4],
+            [5, 6],
+            [7, 8],
+            [9, 10],
+            [11, 12]
+        ], dtype=tf.int32)
+    dims = [3, 2]
+    expanded_tensor = shape_utils.expand_first_dimension(
+        inputs, dims)
+    with self.test_session() as sess:
+      expanded_tensor_out = sess.run(expanded_tensor)
+
+    expected_output = [
+        [[1, 2], [3, 4]],
+        [[5, 6], [7, 8]],
+        [[9, 10], [11, 12]]]
+    self.assertAllEqual(expected_output, expanded_tensor_out)
+
+  def test_expand_first_dimension_with_incompatible_dims(self):
+    inputs_default = tf.constant(
+        [
+            [[1, 2]],
+            [[3, 4]],
+            [[5, 6]],
+        ], dtype=tf.int32)
+    inputs = tf.placeholder_with_default(inputs_default, [None, 1, 2])
+    dims = [3, 2]
+    expanded_tensor = shape_utils.expand_first_dimension(
+        inputs, dims)
+    with self.test_session() as sess:
+      with self.assertRaises(tf.errors.InvalidArgumentError):
+        sess.run(expanded_tensor)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/spatial_transform_ops.py b/research/object_detection/utils/spatial_transform_ops.py
new file mode 100644
index 00000000..222a1b29
--- /dev/null
+++ b/research/object_detection/utils/spatial_transform_ops.py
@@ -0,0 +1,472 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Spatial transformation ops like RoIAlign, CropAndResize."""
+import tensorflow as tf
+
+
+def _coordinate_vector_1d(start, end, size, align_endpoints):
+  """Generates uniformly spaced coordinate vector.
+
+  Args:
+    start: A float tensor of shape [batch, num_boxes] indicating start values.
+    end: A float tensor of shape [batch, num_boxes] indicating end values.
+    size: Number of points in coordinate vector.
+    align_endpoints: Whether to align first and last points exactly to
+      endpoints.
+
+  Returns:
+    A 3D float tensor of shape [batch, num_boxes, size] containing grid
+    coordinates.
+  """
+  start = tf.expand_dims(start, -1)
+  end = tf.expand_dims(end, -1)
+  length = tf.cast(end - start, dtype=tf.float32)
+  if align_endpoints:
+    relative_grid_spacing = tf.linspace(0.0, 1.0, size)
+    offset = 0 if size > 1 else length / 2
+  else:
+    relative_grid_spacing = tf.linspace(0.0, 1.0, size + 1)[:-1]
+    offset = length / (2 * size)
+  relative_grid_spacing = tf.reshape(relative_grid_spacing, [1, 1, size])
+  absolute_grid = start + offset + relative_grid_spacing * length
+  return absolute_grid
+
+
+def box_grid_coordinate_vectors(boxes, size_y, size_x, align_corners=False):
+  """Generates coordinate vectors for a `size x size` grid in boxes.
+
+  Each box is subdivided uniformly into a grid consisting of size x size
+  rectangular cells. This function returns coordinate vectors describing
+  the center of each cell.
+
+  If `align_corners` is true, grid points are uniformly spread such that the
+  corner points on the grid exactly overlap corners of the boxes.
+
+  Note that output coordinates are expressed in the same coordinate frame as
+  input boxes.
+
+  Args:
+    boxes: A float tensor of shape [batch, num_boxes, 4] containing boxes of the
+      form [ymin, xmin, ymax, xmax].
+    size_y: Size of the grid in y axis.
+    size_x: Size of the grid in x axis.
+    align_corners: Whether to align the corner grid points exactly with box
+      corners.
+
+  Returns:
+    box_grid_y: A float tensor of shape [batch, num_boxes, size_y] containing y
+      coordinates for grid points.
+    box_grid_x: A float tensor of shape [batch, num_boxes, size_x] containing x
+      coordinates for grid points.
+  """
+  ymin, xmin, ymax, xmax = tf.unstack(boxes, axis=-1)
+  box_grid_y = _coordinate_vector_1d(ymin, ymax, size_y, align_corners)
+  box_grid_x = _coordinate_vector_1d(xmin, xmax, size_x, align_corners)
+  return box_grid_y, box_grid_x
+
+
+def feature_grid_coordinate_vectors(box_grid_y, box_grid_x):
+  """Returns feature grid point coordinate vectors for bilinear interpolation.
+
+  Box grid is specified in absolute coordinate system with origin at left top
+  (0, 0). The returned coordinate vectors contain 0-based feature point indices.
+
+  This function snaps each point in the box grid to nearest 4 points on the
+  feature map.
+
+  In this function we also follow the convention of treating feature pixels as
+  point objects with no spatial extent.
+
+  Args:
+    box_grid_y: A float tensor of shape [batch, num_boxes, size] containing y
+      coordinate vector of the box grid.
+    box_grid_x: A float tensor of shape [batch, num_boxes, size] containing x
+      coordinate vector of the box grid.
+
+  Returns:
+    feature_grid_y0: An int32 tensor of shape [batch, num_boxes, size]
+      containing y coordinate vector for the top neighbors.
+    feature_grid_x0: A int32 tensor of shape [batch, num_boxes, size]
+      containing x coordinate vector for the left neighbors.
+    feature_grid_y1: A int32 tensor of shape [batch, num_boxes, size]
+      containing y coordinate vector for the bottom neighbors.
+    feature_grid_x1: A int32 tensor of shape [batch, num_boxes, size]
+      containing x coordinate vector for the right neighbors.
+  """
+  feature_grid_y0 = tf.floor(box_grid_y)
+  feature_grid_x0 = tf.floor(box_grid_x)
+  feature_grid_y1 = tf.floor(box_grid_y + 1)
+  feature_grid_x1 = tf.floor(box_grid_x + 1)
+  feature_grid_y0 = tf.cast(feature_grid_y0, dtype=tf.int32)
+  feature_grid_y1 = tf.cast(feature_grid_y1, dtype=tf.int32)
+  feature_grid_x0 = tf.cast(feature_grid_x0, dtype=tf.int32)
+  feature_grid_x1 = tf.cast(feature_grid_x1, dtype=tf.int32)
+  return (feature_grid_y0, feature_grid_x0, feature_grid_y1, feature_grid_x1)
+
+
+def _valid_indicator(feature_grid_y, feature_grid_x, true_feature_shapes):
+  """Computes a indicator vector for valid indices.
+
+  Computes an indicator vector which is true for points on feature map and
+  false for points off feature map.
+
+  Args:
+    feature_grid_y: An int32 tensor of shape [batch, num_boxes, size_y]
+      containing y coordinate vector.
+    feature_grid_x: An int32 tensor of shape [batch, num_boxes, size_x]
+      containing x coordinate vector.
+    true_feature_shapes: A int32 tensor of shape [batch, num_boxes, 2]
+      containing valid height and width of feature maps. Feature maps are
+      assumed to be aligned to the left top corner.
+
+  Returns:
+    indices: A 1D bool tensor indicating valid feature indices.
+  """
+  height = tf.cast(true_feature_shapes[:, :, 0:1], dtype=feature_grid_y.dtype)
+  width = tf.cast(true_feature_shapes[:, :, 1:2], dtype=feature_grid_x.dtype)
+  valid_indicator = tf.logical_and(
+      tf.expand_dims(
+          tf.logical_and(feature_grid_y >= 0, tf.less(feature_grid_y, height)),
+          3),
+      tf.expand_dims(
+          tf.logical_and(feature_grid_x >= 0, tf.less(feature_grid_x, width)),
+          2))
+  return tf.reshape(valid_indicator, [-1])
+
+
+def ravel_indices(feature_grid_y, feature_grid_x, num_levels, height, width,
+                  box_levels):
+  """Returns grid indices in a flattened feature map of shape [-1, channels].
+
+  The returned 1-D array can be used to gather feature grid points from a
+  feature map that has been flattened from [batch, num_levels, max_height,
+  max_width, channels] to [batch * num_levels * max_height * max_width,
+  channels].
+
+  Args:
+    feature_grid_y: An int32 tensor of shape [batch, num_boxes, size_y]
+      containing y coordinate vector.
+    feature_grid_x: An int32 tensor of shape [batch, num_boxes, size_x]
+      containing x coordinate vector.
+    num_levels: Number of feature levels.
+    height: An integer indicating the padded height of feature maps.
+    width: An integer indicating the padded width of feature maps.
+    box_levels: An int32 tensor of shape [batch, num_boxes] indicating
+      feature level assigned to each box.
+
+  Returns:
+    indices: A 1D int32 tensor containing feature point indices in a flattened
+      feature grid.
+  """
+  assert feature_grid_y.shape[0] == feature_grid_x.shape[0]
+  assert feature_grid_y.shape[1] == feature_grid_x.shape[1]
+  num_boxes = feature_grid_y.shape[1].value
+  batch_size = feature_grid_y.shape[0].value
+  size_y = feature_grid_y.shape[2]
+  size_x = feature_grid_x.shape[2]
+  height_dim_offset = width
+  level_dim_offset = height * height_dim_offset
+  batch_dim_offset = num_levels * level_dim_offset
+
+  batch_dim_indices = (
+      tf.reshape(
+          tf.range(batch_size) * batch_dim_offset, [batch_size, 1, 1, 1]) *
+      tf.ones([1, num_boxes, size_y, size_x], dtype=tf.int32))
+  box_level_indices = (
+      tf.reshape(box_levels * level_dim_offset, [batch_size, num_boxes, 1, 1]) *
+      tf.ones([1, 1, size_y, size_x], dtype=tf.int32))
+  height_indices = (
+      tf.reshape(feature_grid_y * height_dim_offset,
+                 [batch_size, num_boxes, size_y, 1]) *
+      tf.ones([1, 1, 1, size_x], dtype=tf.int32))
+  width_indices = (
+      tf.reshape(feature_grid_x, [batch_size, num_boxes, 1, size_x])
+      * tf.ones([1, 1, size_y, 1], dtype=tf.int32))
+  indices = (
+      batch_dim_indices + box_level_indices + height_indices + width_indices)
+  flattened_indices = tf.reshape(indices, [-1])
+  return flattened_indices
+
+
+def pad_to_max_size(features):
+  """Pads features to max height and max width and stacks them up.
+
+  Args:
+    features: A list of num_levels 4D float tensors of shape [batch, height_i,
+      width_i, channels] containing feature maps.
+
+  Returns:
+    stacked_features: A 5D float tensor of shape [batch, num_levels, max_height,
+      max_width, channels] containing stacked features.
+    true_feature_shapes: A 2D int32 tensor of shape [num_levels, 2] containing
+      height and width of the feature maps before padding.
+  """
+  heights = [feature.shape[1].value for feature in features]
+  widths = [feature.shape[2].value for feature in features]
+  max_height = max(heights)
+  max_width = max(widths)
+
+  features_all = [
+      tf.image.pad_to_bounding_box(feature, 0, 0, max_height,
+                                   max_width) for feature in features
+  ]
+  features_all = tf.stack(features_all, axis=1)
+  true_feature_shapes = tf.stack([feature.shape[1:3] for feature in features])
+  return features_all, true_feature_shapes
+
+
+def _gather_valid_indices(tensor, indices, padding_value=0.0):
+  """Gather values for valid indices.
+
+  TODO(rathodv): We can't use ops.gather_with_padding_values due to cyclic
+  dependency. Start using it after migrating all users of spatial ops to import
+  this module directly rather than util/ops.py
+
+  Args:
+    tensor: A tensor to gather valid values from.
+    indices: A 1-D int32 tensor containing indices along axis 0 of `tensor`.
+      Invalid indices must be marked with -1.
+    padding_value: Value to return for invalid indices.
+
+  Returns:
+    A tensor sliced based on indices. For indices that are equal to -1, returns
+    rows of padding value.
+  """
+  padded_tensor = tf.concat(
+      [
+          padding_value *
+          tf.ones([1, tensor.shape[-1].value], dtype=tensor.dtype), tensor
+      ],
+      axis=0,
+  )
+  # tf.concat gradient op uses tf.where(condition) (which is not
+  # supported on TPU) when the inputs to it are tf.IndexedSlices instead of
+  # tf.Tensor. Since gradient op for tf.gather returns tf.IndexedSlices,
+  # we add a dummy op inbetween tf.concat and tf.gather to ensure tf.concat
+  # gradient function gets tf.Tensor inputs and not tf.IndexedSlices.
+  padded_tensor *= 1.0
+  return tf.gather(padded_tensor, indices + 1)
+
+
+def multilevel_roi_align(features, boxes, box_levels, output_size,
+                         num_samples_per_cell_y=1, num_samples_per_cell_x=1,
+                         align_corners=False, extrapolation_value=0.0,
+                         scope=None):
+  """Applies RoI Align op and returns feature for boxes.
+
+  Given multiple features maps indexed by different levels, and a set of boxes
+  where each box is mapped to a certain level, this function selectively crops
+  and resizes boxes from the corresponding feature maps.
+
+  We follow the RoI Align technique in https://arxiv.org/pdf/1703.06870.pdf
+  figure 3. Specifically, each box is subdivided uniformly into a grid
+  consisting of output_size[0] x output_size[1] rectangular cells. Within each
+  cell we select `num_points` points uniformly and compute feature values using
+  bilinear interpolation. Finally, we average pool the interpolated values in
+  each cell to obtain a [output_size[0], output_size[1], channels] feature.
+
+  If `align_corners` is true, sampling points are uniformly spread such that
+  corner points exactly overlap corners of the boxes.
+
+  In this function we also follow the convention of treating feature pixels as
+  point objects with no spatial extent.
+
+  Args:
+    features: A list of 4D float tensors of shape [batch_size, max_height,
+      max_width, channels] containing features.
+    boxes: A 3D float tensor of shape [batch_size, num_boxes, 4] containing
+      boxes of the form [ymin, xmin, ymax, xmax] in normalized coordinates.
+    box_levels: A 3D int32 tensor of shape [batch_size, num_boxes, 1]
+      representing the feature level index for each box.
+    output_size: An list of two integers [size_y, size_x] indicating the output
+      feature size for each box.
+    num_samples_per_cell_y: Number of grid points to sample along y axis in each
+      cell.
+    num_samples_per_cell_x: Number of grid points to sample along x axis in each
+      cell.
+    align_corners: Whether to align the corner grid points exactly with box
+      corners.
+    extrapolation_value: a float value to use for extrapolation.
+    scope: Scope name to use for this op.
+
+  Returns:
+    A 5D float tensor of shape [batch_size, num_boxes, output_size[0],
+    output_size[1], channels] representing the cropped features.
+  """
+  with tf.name_scope(scope, 'MultiLevelRoIAlign'):
+    features, true_feature_shapes = pad_to_max_size(features)
+    (batch_size, num_levels, max_feature_height, max_feature_width,
+     num_filters) = features.get_shape().as_list()
+    _, num_boxes, _ = boxes.get_shape().as_list()
+
+    # Convert boxes to absolute co-ordinates.
+    true_feature_shapes = tf.cast(true_feature_shapes, dtype=boxes.dtype)
+    true_feature_shapes = tf.gather(true_feature_shapes, box_levels)
+    boxes *= tf.concat([true_feature_shapes - 1] * 2, axis=-1)
+
+    size_y = output_size[0] * num_samples_per_cell_y
+    size_x = output_size[1] * num_samples_per_cell_x
+    box_grid_y, box_grid_x = box_grid_coordinate_vectors(
+        boxes, size_y=size_y, size_x=size_x, align_corners=align_corners)
+    (feature_grid_y0, feature_grid_x0, feature_grid_y1,
+     feature_grid_x1) = feature_grid_coordinate_vectors(box_grid_y, box_grid_x)
+    feature_grid_y = tf.reshape(
+        tf.stack([feature_grid_y0, feature_grid_y1], axis=3),
+        [batch_size, num_boxes, -1])
+    feature_grid_x = tf.reshape(
+        tf.stack([feature_grid_x0, feature_grid_x1], axis=3),
+        [batch_size, num_boxes, -1])
+    feature_coordinates = ravel_indices(feature_grid_y, feature_grid_x,
+                                        num_levels, max_feature_height,
+                                        max_feature_width, box_levels)
+    valid_indices = _valid_indicator(feature_grid_y, feature_grid_x,
+                                     true_feature_shapes)
+    feature_coordinates = tf.where(valid_indices, feature_coordinates,
+                                   -1 * tf.ones_like(feature_coordinates))
+    flattened_features = tf.reshape(features, [-1, num_filters])
+    flattened_feature_values = _gather_valid_indices(flattened_features,
+                                                     feature_coordinates,
+                                                     extrapolation_value)
+    features_per_box = tf.reshape(
+        flattened_feature_values,
+        [batch_size, num_boxes, size_y * 2, size_x * 2, num_filters])
+
+    # Cast tensors into dtype of features.
+    box_grid_y = tf.cast(box_grid_y, dtype=features_per_box.dtype)
+    box_grid_x = tf.cast(box_grid_x, dtype=features_per_box.dtype)
+    feature_grid_y0 = tf.cast(feature_grid_y0, dtype=features_per_box.dtype)
+    feature_grid_x0 = tf.cast(feature_grid_x0, dtype=features_per_box.dtype)
+
+    # RoI Align operation is a bilinear interpolation of four
+    # neighboring feature points f0, f1, f2, and f3 onto point y, x given by
+    # f(y, x) = [hy, ly] * [[f00, f01], * [hx, lx]^T
+    #                       [f10, f11]]
+    #
+    # Unrolling the matrix multiplies gives us:
+    # f(y, x) = (hy * hx) f00 + (hy * lx) f01 + (ly * hx) f10 + (lx * ly) f11
+    # f(y, x) = w00 * f00 + w01 * f01 + w10 * f10 + w11 * f11
+    #
+    # This can be computed by applying pointwise multiplication and sum_pool in
+    # a 2x2 window.
+    ly = box_grid_y - feature_grid_y0
+    lx = box_grid_x - feature_grid_x0
+    hy = 1.0 - ly
+    hx = 1.0 - lx
+
+    kernel_y = tf.reshape(
+        tf.stack([hy, ly], axis=3), [batch_size, num_boxes, size_y * 2, 1])
+
+    kernel_x = tf.reshape(
+        tf.stack([hx, lx], axis=3), [batch_size, num_boxes, 1, size_x * 2])
+
+    # Multiplier 4 is to make tf.nn.avg_pool behave like sum_pool.
+    interpolation_kernel = kernel_y * kernel_x * 4
+
+    # Interpolate the gathered features with computed interpolation kernels.
+    features_per_box *= tf.expand_dims(interpolation_kernel, axis=4),
+    features_per_box = tf.reshape(
+        features_per_box,
+        [batch_size * num_boxes, size_y * 2, size_x * 2, num_filters])
+
+    # This combines the two pooling operations - sum_pool to perform bilinear
+    # interpolation and avg_pool to pool the values in each bin.
+    features_per_box = tf.nn.avg_pool(
+        features_per_box,
+        [1, num_samples_per_cell_y * 2, num_samples_per_cell_x * 2, 1],
+        [1, num_samples_per_cell_y * 2, num_samples_per_cell_x * 2, 1], 'VALID')
+    features_per_box = tf.reshape(
+        features_per_box,
+        [batch_size, num_boxes, output_size[0], output_size[1], num_filters])
+
+    return features_per_box
+
+
+def native_crop_and_resize(image, boxes, crop_size, scope=None):
+  """Same as `matmul_crop_and_resize` but uses tf.image.crop_and_resize."""
+  def get_box_inds(proposals):
+    proposals_shape = proposals.get_shape().as_list()
+    if any(dim is None for dim in proposals_shape):
+      proposals_shape = tf.shape(proposals)
+    ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)
+    multiplier = tf.expand_dims(
+        tf.range(start=0, limit=proposals_shape[0]), 1)
+    return tf.reshape(ones_mat * multiplier, [-1])
+
+  with tf.name_scope(scope, 'CropAndResize'):
+    cropped_regions = tf.image.crop_and_resize(
+        image, tf.reshape(boxes, [-1] + boxes.shape.as_list()[2:]),
+        get_box_inds(boxes), crop_size)
+    final_shape = tf.concat([tf.shape(boxes)[:2],
+                             tf.shape(cropped_regions)[1:]], axis=0)
+    return tf.reshape(cropped_regions, final_shape)
+
+
+def matmul_crop_and_resize(image, boxes, crop_size, extrapolation_value=0.0,
+                           scope=None):
+  """Matrix multiplication based implementation of the crop and resize op.
+
+  Extracts crops from the input image tensor and bilinearly resizes them
+  (possibly with aspect ratio change) to a common output size specified by
+  crop_size. This is more general than the crop_to_bounding_box op which
+  extracts a fixed size slice from the input image and does not allow
+  resizing or aspect ratio change.
+
+  Returns a tensor with crops from the input image at positions defined at
+  the bounding box locations in boxes. The cropped boxes are all resized
+  (with bilinear interpolation) to a fixed size = `[crop_height, crop_width]`.
+  The result is a 5-D tensor `[batch, num_boxes, crop_height, crop_width,
+  depth]`.
+
+  Note that this operation is meant to replicate the behavior of the standard
+  tf.image.crop_and_resize operation but there are a few differences.
+  Specifically:
+    1) There is no `box_indices` argument --- to run this op on multiple images,
+      one must currently call this op independently on each image.
+    2) The `crop_size` parameter is assumed to be statically defined.
+      Moreover, the number of boxes must be strictly nonzero.
+
+  Args:
+    image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,
+      `int16`, `int32`, `int64`, `half`, 'bfloat16', `float32`, `float64`.
+      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
+      Both `image_height` and `image_width` need to be positive.
+    boxes: A `Tensor` of type `float32` or 'bfloat16'.
+      A 3-D tensor of shape `[batch, num_boxes, 4]`. The boxes are specified in
+      normalized coordinates and are of the form `[y1, x1, y2, x2]`. A
+      normalized coordinate value of `y` is mapped to the image coordinate at
+      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image
+      height is mapped to `[0, image_height - 1] in image height coordinates.
+      We do allow y1 > y2, in which case the sampled crop is an up-down flipped
+      version of the original image. The width dimension is treated similarly.
+      Normalized coordinates outside the `[0, 1]` range are allowed, in which
+      case we use `extrapolation_value` to extrapolate the input image values.
+    crop_size: A list of two integers `[crop_height, crop_width]`. All
+      cropped image patches are resized to this size. The aspect ratio of the
+      image content is not preserved. Both `crop_height` and `crop_width` need
+      to be positive.
+    extrapolation_value: a float value to use for extrapolation.
+    scope: A name for the operation (optional).
+
+  Returns:
+    A 5-D tensor of shape `[batch, num_boxes, crop_height, crop_width, depth]`
+  """
+  with tf.name_scope(scope, 'MatMulCropAndResize'):
+    box_levels = tf.zeros(boxes.shape.as_list()[:2], dtype=tf.int32)
+    return multilevel_roi_align([image],
+                                boxes,
+                                box_levels,
+                                crop_size,
+                                align_corners=True,
+                                extrapolation_value=extrapolation_value)
diff --git a/research/object_detection/utils/spatial_transform_ops_test.py b/research/object_detection/utils/spatial_transform_ops_test.py
new file mode 100644
index 00000000..02ead8ac
--- /dev/null
+++ b/research/object_detection/utils/spatial_transform_ops_test.py
@@ -0,0 +1,553 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for object_detection.utils.spatial_transform_ops."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import spatial_transform_ops as spatial_ops
+from object_detection.utils import test_case
+
+
+class BoxGridCoordinateTest(test_case.TestCase):
+
+  def test_4x4_grid(self):
+    boxes = np.array([[[0., 0., 6., 6.]]], dtype=np.float32)
+    def graph_fn(boxes):
+      return spatial_ops.box_grid_coordinate_vectors(boxes, size_y=4, size_x=4)
+
+    grid_y, grid_x = self.execute(graph_fn, [boxes])
+    expected_grid_y = np.array([[[0.75, 2.25, 3.75, 5.25]]])
+    expected_grid_x = np.array([[[0.75, 2.25, 3.75, 5.25]]])
+    self.assertAllClose(expected_grid_y, grid_y)
+    self.assertAllClose(expected_grid_x, grid_x)
+
+  def test_2x2_grid(self):
+    def graph_fn(boxes):
+      return spatial_ops.box_grid_coordinate_vectors(boxes, size_x=2, size_y=2)
+    boxes = np.array([[[0., 0., 6., 3.],
+                       [0., 0., 3., 6.]]], dtype=np.float32)
+
+    grid_y, grid_x = self.execute(graph_fn, [boxes])
+    expected_grid_y = np.array([[[1.5, 4.5],
+                                 [0.75, 2.25]]])
+    expected_grid_x = np.array([[[0.75, 2.25],
+                                 [1.5, 4.5]]])
+    self.assertAllClose(expected_grid_y, grid_y)
+    self.assertAllClose(expected_grid_x, grid_x)
+
+  def test_2x4_grid(self):
+    boxes = np.array([[[0., 0., 6., 6.]]], dtype=np.float32)
+    def graph_fn(boxes):
+      return spatial_ops.box_grid_coordinate_vectors(boxes, size_y=2, size_x=4)
+
+    grid_y, grid_x = self.execute(graph_fn, [boxes])
+    expected_grid_y = np.array([[[1.5, 4.5]]])
+    expected_grid_x = np.array([[[0.75, 2.25, 3.75, 5.25]]])
+    self.assertAllClose(expected_grid_y, grid_y)
+    self.assertAllClose(expected_grid_x, grid_x)
+
+  def test_2x4_grid_with_aligned_corner(self):
+    boxes = np.array([[[0., 0., 6., 6.]]], dtype=np.float32)
+    def graph_fn(boxes):
+      return spatial_ops.box_grid_coordinate_vectors(boxes, size_y=2, size_x=4,
+                                                     align_corners=True)
+
+    grid_y, grid_x = self.execute(graph_fn, [boxes])
+    expected_grid_y = np.array([[[0, 6]]])
+    expected_grid_x = np.array([[[0, 2, 4, 6]]])
+    self.assertAllClose(expected_grid_y, grid_y)
+    self.assertAllClose(expected_grid_x, grid_x)
+
+  def test_offgrid_boxes(self):
+    boxes = np.array([[[1.2, 2.3, 7.2, 8.3]]], dtype=np.float32)
+    def graph_fn(boxes):
+      return spatial_ops.box_grid_coordinate_vectors(boxes, size_y=4, size_x=4)
+
+    grid_y, grid_x = self.execute(graph_fn, [boxes])
+    expected_grid_y = np.array([[[0.75, 2.25, 3.75, 5.25]]]) + 1.2
+    expected_grid_x = np.array([[[0.75, 2.25, 3.75, 5.25]]]) + 2.3
+    self.assertAllClose(expected_grid_y, grid_y)
+    self.assertAllClose(expected_grid_x, grid_x)
+
+
+class FeatureGridCoordinateTest(test_case.TestCase):
+
+  def test_snap_box_points_to_nearest_4_pixels(self):
+    box_grid_y = np.array([[[1.5, 4.6]]], dtype=np.float32)
+    box_grid_x = np.array([[[2.4, 5.3]]], dtype=np.float32)
+
+    def graph_fn(box_grid_y, box_grid_x):
+      return spatial_ops.feature_grid_coordinate_vectors(box_grid_y, box_grid_x)
+
+    (feature_grid_y0,
+     feature_grid_x0, feature_grid_y1, feature_grid_x1) = self.execute(
+         graph_fn, [box_grid_y, box_grid_x])
+    expected_grid_y0 = np.array([[[1, 4]]])
+    expected_grid_y1 = np.array([[[2, 5]]])
+    expected_grid_x0 = np.array([[[2, 5]]])
+    expected_grid_x1 = np.array([[[3, 6]]])
+    self.assertAllEqual(expected_grid_y0, feature_grid_y0)
+    self.assertAllEqual(expected_grid_y1, feature_grid_y1)
+    self.assertAllEqual(expected_grid_x0, feature_grid_x0)
+    self.assertAllEqual(expected_grid_x1, feature_grid_x1)
+
+  def test_snap_box_points_outside_pixel_grid_to_nearest_neighbor(self):
+    box_grid_y = np.array([[[0.33, 1., 1.66]]], dtype=np.float32)
+    box_grid_x = np.array([[[-0.5, 1., 1.66]]], dtype=np.float32)
+
+    def graph_fn(box_grid_y, box_grid_x):
+      return spatial_ops.feature_grid_coordinate_vectors(box_grid_y, box_grid_x)
+
+    (feature_grid_y0,
+     feature_grid_x0, feature_grid_y1, feature_grid_x1) = self.execute(
+         graph_fn, [box_grid_y, box_grid_x])
+    expected_grid_y0 = np.array([[[0, 1, 1]]])
+    expected_grid_y1 = np.array([[[1, 2, 2]]])
+    expected_grid_x0 = np.array([[[-1, 1, 1]]])
+    expected_grid_x1 = np.array([[[0, 2, 2]]])
+    self.assertAllEqual(expected_grid_y0, feature_grid_y0)
+    self.assertAllEqual(expected_grid_y1, feature_grid_y1)
+    self.assertAllEqual(expected_grid_x0, feature_grid_x0)
+    self.assertAllEqual(expected_grid_x1, feature_grid_x1)
+
+
+class RavelIndicesTest(test_case.TestCase):
+
+  def test_feature_point_indices(self):
+    feature_grid_y = np.array([[[1, 2, 4, 5],
+                                [2, 3, 4, 5]]], dtype=np.int32)
+    feature_grid_x = np.array([[[1, 3, 4],
+                                [2, 3, 4]]], dtype=np.int32)
+    num_feature_levels = 2
+    feature_height = 6
+    feature_width = 5
+    box_levels = np.array([[0, 1]], dtype=np.int32)
+
+    def graph_fn(feature_grid_y, feature_grid_x, box_levels):
+      return spatial_ops.ravel_indices(feature_grid_y, feature_grid_x,
+                                       num_feature_levels, feature_height,
+                                       feature_width, box_levels)
+
+    indices = self.execute(graph_fn,
+                           [feature_grid_y, feature_grid_x, box_levels])
+    expected_indices = np.array([[[[6, 8, 9],
+                                   [11, 13, 14],
+                                   [21, 23, 24],
+                                   [26, 28, 29]],
+                                  [[42, 43, 44],
+                                   [47, 48, 49],
+                                   [52, 53, 54],
+                                   [57, 58, 59]]]])
+    self.assertAllEqual(expected_indices.flatten(), indices)
+
+
+class MultiLevelRoIAlignTest(test_case.TestCase):
+
+  def test_perfectly_aligned_cell_center_and_feature_pixels(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[2, 2])
+
+    image = np.arange(25).reshape(1, 5, 5, 1).astype(np.float32)
+    boxes = np.array([[[0, 0, 1.0, 1.0]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+
+    expected_output = [[[[[6], [8]],
+                         [[16], [18]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(crop_output, expected_output)
+
+  def test_interpolation_with_4_points_per_bin(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[1, 1],
+                                              num_samples_per_cell_y=2,
+                                              num_samples_per_cell_x=2)
+
+    image = np.array([[[[1], [2], [3], [4]],
+                       [[5], [6], [7], [8]],
+                       [[9], [10], [11], [12]],
+                       [[13], [14], [15], [16]]]],
+                     dtype=np.float32)
+    boxes = np.array([[[1./3, 1./3, 2./3, 2./3]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+
+    expected_output = [[[[[(7.25 + 7.75 + 9.25 + 9.75) / 4]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(expected_output, crop_output)
+
+  def test_1x1_crop_on_2x2_features(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[1, 1])
+
+    image = np.array([[[[1], [2]],
+                       [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+    expected_output = [[[[[2.5]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(crop_output, expected_output)
+
+  def test_3x3_crops_on_2x2_features(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[3, 3])
+
+    image = np.array([[[[1], [2]],
+                       [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+    expected_output = [[[[[9./6], [11./6], [13./6]],
+                         [[13./6], [15./6], [17./6]],
+                         [[17./6], [19./6], [21./6]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(crop_output, expected_output)
+
+  def test_2x2_crops_on_3x3_features(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[2, 2])
+
+    image = np.array([[[[1], [2], [3]],
+                       [[4], [5], [6]],
+                       [[7], [8], [9]]]],
+                     dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1],
+                       [0, 0, .5, .5]]],
+                     dtype=np.float32)
+    box_levels = np.array([[0, 0]], dtype=np.int32)
+    expected_output = [[[[[3], [4]],
+                         [[6], [7]]],
+                        [[[2.], [2.5]],
+                         [[3.5], [4.]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(crop_output, expected_output)
+
+  def test_2x2_crop_on_4x4_features(self):
+
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[2, 2])
+
+    image = np.array([[[[0], [1], [2], [3]],
+                       [[4], [5], [6], [7]],
+                       [[8], [9], [10], [11]],
+                       [[12], [13], [14], [15]]]],
+                     dtype=np.float32)
+    boxes = np.array([[[0, 0, 2./3, 2./3],
+                       [0, 0, 2./3, 1.0]]],
+                     dtype=np.float32)
+    box_levels = np.array([[0, 0]], dtype=np.int32)
+
+    expected_output = np.array([[[[[2.5], [3.5]],
+                                  [[6.5], [7.5]]],
+                                 [[[2.75], [4.25]],
+                                  [[6.75], [8.25]]]]])
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(expected_output, crop_output)
+
+  def test_extrapolate_3x3_crop_on_2x2_features(self):
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[3, 3])
+    image = np.array([[[[1], [2]],
+                       [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[-1, -1, 2, 2]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+
+    expected_output = np.array([[[[[0.25], [0.75], [0.5]],
+                                  [[1.0], [2.5], [1.5]],
+                                  [[0.75], [1.75], [1]]]]])
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(expected_output, crop_output)
+
+  def test_extrapolate_with_non_zero_value(self):
+    def graph_fn(image, boxes, levels):
+      return spatial_ops.multilevel_roi_align([image],
+                                              boxes,
+                                              levels,
+                                              output_size=[3, 3],
+                                              extrapolation_value=2.0)
+    image = np.array([[[[4], [4]],
+                       [[4], [4]]]], dtype=np.float32)
+    boxes = np.array([[[-1, -1, 2, 2]]], dtype=np.float32)
+    box_levels = np.array([[0]], dtype=np.int32)
+
+    expected_output = np.array([[[[[2.5], [3.0], [2.5]],
+                                  [[3.0], [4.0], [3.0]],
+                                  [[2.5], [3.0], [2.5]]]]])
+    crop_output = self.execute(graph_fn, [image, boxes, box_levels])
+    self.assertAllClose(expected_output, crop_output)
+
+  def test_multilevel_roi_align(self):
+    image_size = 640
+    fpn_min_level = 2
+    fpn_max_level = 5
+    batch_size = 1
+    output_size = [2, 2]
+    num_filters = 1
+    features = []
+    for level in range(fpn_min_level, fpn_max_level + 1):
+      feat_size = int(image_size / 2**level)
+      features.append(
+          float(level) *
+          np.ones([batch_size, feat_size, feat_size, num_filters],
+                  dtype=np.float32))
+    boxes = np.array(
+        [
+            [
+                [0, 0, 111, 111],  # Level 2.
+                [0, 0, 113, 113],  # Level 3.
+                [0, 0, 223, 223],  # Level 3.
+                [0, 0, 225, 225],  # Level 4.
+                [0, 0, 449, 449]   # Level 5.
+            ],
+        ],
+        dtype=np.float32) / image_size
+    levels = np.array([[0, 1, 1, 2, 3]], dtype=np.int32)
+
+    def graph_fn(feature1, feature2, feature3, feature4, boxes, levels):
+      roi_features = spatial_ops.multilevel_roi_align(
+          [feature1, feature2, feature3, feature4],
+          boxes,
+          levels,
+          output_size)
+      return roi_features
+
+    roi_features = self.execute(graph_fn, features + [boxes, levels])
+    self.assertAllClose(roi_features[0][0], 2 * np.ones((2, 2, 1)))
+    self.assertAllClose(roi_features[0][1], 3 * np.ones((2, 2, 1)))
+    self.assertAllClose(roi_features[0][2], 3 * np.ones((2, 2, 1)))
+    self.assertAllClose(roi_features[0][3], 4 * np.ones((2, 2, 1)))
+    self.assertAllClose(roi_features[0][4], 5 * np.ones((2, 2, 1)))
+
+  def test_large_input(self):
+    if test_case.FLAGS.tpu_test:
+      input_size = 1408
+      min_level = 2
+      max_level = 6
+      batch_size = 2
+      num_boxes = 512
+      num_filters = 256
+      output_size = [7, 7]
+      with self.test_session() as sess:
+        features = []
+        for level in range(min_level, max_level + 1):
+          feat_size = int(input_size / 2**level)
+          features.append(tf.constant(
+              np.reshape(
+                  np.arange(
+                      batch_size * feat_size * feat_size * num_filters,
+                      dtype=np.float32),
+                  [batch_size, feat_size, feat_size, num_filters]),
+              dtype=tf.bfloat16))
+        boxes = np.array([
+            [[0, 0, 256, 256]]*num_boxes,
+        ], dtype=np.float32) / input_size
+        boxes = np.tile(boxes, [batch_size, 1, 1])
+        tf_boxes = tf.constant(boxes)
+        tf_levels = tf.random_uniform([batch_size, num_boxes], maxval=5,
+                                      dtype=tf.int32)
+        def crop_and_resize_fn():
+          return spatial_ops.multilevel_roi_align(
+              features, tf_boxes, tf_levels, output_size)
+
+        tpu_crop_and_resize_fn = tf.contrib.tpu.rewrite(crop_and_resize_fn)
+        sess.run(tf.contrib.tpu.initialize_system())
+        sess.run(tf.global_variables_initializer())
+        roi_features = sess.run(tpu_crop_and_resize_fn)
+        self.assertEqual(roi_features[0].shape,
+                         (batch_size, num_boxes, output_size[0], output_size[1],
+                          num_filters))
+        sess.run(tf.contrib.tpu.shutdown_system())
+
+
+class MatMulCropAndResizeTest(test_case.TestCase):
+
+  def testMatMulCropAndResize2x2To1x1(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])
+
+    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
+    expected_output = [[[[[2.5]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize2x2To1x1Flipped(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[1, 1])
+
+    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[1, 1, 0, 0]]], dtype=np.float32)
+    expected_output = [[[[[2.5]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize2x2To3x3(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])
+
+    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1]]], dtype=np.float32)
+    expected_output = [[[[[1.0], [1.5], [2.0]],
+                         [[2.0], [2.5], [3.0]],
+                         [[3.0], [3.5], [4.0]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize2x2To3x3Flipped(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[3, 3])
+
+    image = np.array([[[[1], [2]], [[3], [4]]]], dtype=np.float32)
+    boxes = np.array([[[1, 1, 0, 0]]], dtype=np.float32)
+    expected_output = [[[[[4.0], [3.5], [3.0]],
+                         [[3.0], [2.5], [2.0]],
+                         [[2.0], [1.5], [1.0]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize3x3To2x2(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
+
+    image = np.array([[[[1], [2], [3]],
+                       [[4], [5], [6]],
+                       [[7], [8], [9]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1],
+                       [0, 0, .5, .5]]], dtype=np.float32)
+    expected_output = [[[[[1], [3]], [[7], [9]]],
+                        [[[1], [2]], [[4], [5]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize3x3To2x2_2Channels(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
+
+    image = np.array([[[[1, 0], [2, 1], [3, 2]],
+                       [[4, 3], [5, 4], [6, 5]],
+                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1],
+                       [0, 0, .5, .5]]], dtype=np.float32)
+    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
+                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testBatchMatMulCropAndResize3x3To2x2_2Channels(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
+
+    image = np.array([[[[1, 0], [2, 1], [3, 2]],
+                       [[4, 3], [5, 4], [6, 5]],
+                       [[7, 6], [8, 7], [9, 8]]],
+                      [[[1, 0], [2, 1], [3, 2]],
+                       [[4, 3], [5, 4], [6, 5]],
+                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1],
+                       [0, 0, .5, .5]],
+                      [[1, 1, 0, 0],
+                       [.5, .5, 0, 0]]], dtype=np.float32)
+    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
+                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]],
+                       [[[[9, 8], [7, 6]], [[3, 2], [1, 0]]],
+                        [[[5, 4], [4, 3]], [[2, 1], [1, 0]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testMatMulCropAndResize3x3To2x2Flipped(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.matmul_crop_and_resize(image, boxes, crop_size=[2, 2])
+
+    image = np.array([[[[1], [2], [3]],
+                       [[4], [5], [6]],
+                       [[7], [8], [9]]]], dtype=np.float32)
+    boxes = np.array([[[1, 1, 0, 0],
+                       [.5, .5, 0, 0]]], dtype=np.float32)
+    expected_output = [[[[[9], [7]], [[3], [1]]],
+                        [[[5], [4]], [[2], [1]]]]]
+    crop_output = self.execute(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+  def testInvalidInputShape(self):
+    image = tf.constant([[[1], [2]], [[3], [4]]], dtype=tf.float32)
+    boxes = tf.constant([[-1, -1, 1, 1]], dtype=tf.float32)
+    crop_size = [4, 4]
+    with self.assertRaises(ValueError):
+      spatial_ops.matmul_crop_and_resize(image, boxes, crop_size)
+
+
+class NativeCropAndResizeTest(test_case.TestCase):
+
+  def testBatchCropAndResize3x3To2x2_2Channels(self):
+
+    def graph_fn(image, boxes):
+      return spatial_ops.native_crop_and_resize(image, boxes, crop_size=[2, 2])
+
+    image = np.array([[[[1, 0], [2, 1], [3, 2]],
+                       [[4, 3], [5, 4], [6, 5]],
+                       [[7, 6], [8, 7], [9, 8]]],
+                      [[[1, 0], [2, 1], [3, 2]],
+                       [[4, 3], [5, 4], [6, 5]],
+                       [[7, 6], [8, 7], [9, 8]]]], dtype=np.float32)
+    boxes = np.array([[[0, 0, 1, 1],
+                       [0, 0, .5, .5]],
+                      [[1, 1, 0, 0],
+                       [.5, .5, 0, 0]]], dtype=np.float32)
+    expected_output = [[[[[1, 0], [3, 2]], [[7, 6], [9, 8]]],
+                        [[[1, 0], [2, 1]], [[4, 3], [5, 4]]]],
+                       [[[[9, 8], [7, 6]], [[3, 2], [1, 0]]],
+                        [[[5, 4], [4, 3]], [[2, 1], [1, 0]]]]]
+    crop_output = self.execute_cpu(graph_fn, [image, boxes])
+    self.assertAllClose(crop_output, expected_output)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/static_shape.py b/research/object_detection/utils/static_shape.py
index 8e4e522f..307c4d3d 100644
--- a/research/object_detection/utils/static_shape.py
+++ b/research/object_detection/utils/static_shape.py
@@ -19,6 +19,21 @@ The rank 4 tensor_shape must be of the form [batch_size, height, width, depth].
 """
 
 
+def get_dim_as_int(dim):
+  """Utility to get v1 or v2 TensorShape dim as an int.
+
+  Args:
+    dim: The TensorShape dimension to get as an int
+
+  Returns:
+    None or an int.
+  """
+  try:
+    return dim.value
+  except AttributeError:
+    return dim
+
+
 def get_batch_size(tensor_shape):
   """Returns batch size from the tensor shape.
 
@@ -29,7 +44,7 @@ def get_batch_size(tensor_shape):
     An integer representing the batch size of the tensor.
   """
   tensor_shape.assert_has_rank(rank=4)
-  return tensor_shape[0].value
+  return get_dim_as_int(tensor_shape[0])
 
 
 def get_height(tensor_shape):
@@ -42,7 +57,7 @@ def get_height(tensor_shape):
     An integer representing the height of the tensor.
   """
   tensor_shape.assert_has_rank(rank=4)
-  return tensor_shape[1].value
+  return get_dim_as_int(tensor_shape[1])
 
 
 def get_width(tensor_shape):
@@ -55,7 +70,7 @@ def get_width(tensor_shape):
     An integer representing the width of the tensor.
   """
   tensor_shape.assert_has_rank(rank=4)
-  return tensor_shape[2].value
+  return get_dim_as_int(tensor_shape[2])
 
 
 def get_depth(tensor_shape):
@@ -68,4 +83,4 @@ def get_depth(tensor_shape):
     An integer representing the depth of the tensor.
   """
   tensor_shape.assert_has_rank(rank=4)
-  return tensor_shape[3].value
+  return get_dim_as_int(tensor_shape[3])
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index d8c2b2ce..7c488488 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -1005,9 +1005,13 @@ class EvalMetricOpsVisualization(object):
           lambda: tf.summary.image(summary_name, image),
           lambda: tf.constant(''))
 
-    update_op = tf.py_func(self.add_images, [[images[0]]], [])
-    image_tensors = tf.py_func(
-        get_images, [], [tf.uint8] * self._max_examples_to_draw)
+    if tf.executing_eagerly():
+      update_op = self.add_images([[images[0]]])
+      image_tensors = get_images()
+    else:
+      update_op = tf.py_func(self.add_images, [[images[0]]], [])
+      image_tensors = tf.py_func(
+          get_images, [], [tf.uint8] * self._max_examples_to_draw)
     eval_metric_ops = {}
     for i, image in enumerate(image_tensors):
       summary_name = self._summary_name_prefix + '/' + str(i)
