commit 02a30e2880a3e5bb95894f8641488cb5deb81976
Author: Mark Daoust <markdaoust@google.com>
Date:   Thu Jul 12 14:44:19 2018 -0700

    Move Fairness note to second paragraph.

diff --git a/samples/core/tutorials/estimators/wide.ipynb b/samples/core/tutorials/estimators/wide.ipynb
index e5072603..a397c205 100644
--- a/samples/core/tutorials/estimators/wide.ipynb
+++ b/samples/core/tutorials/estimators/wide.ipynb
@@ -75,14 +75,16 @@
       "cell_type": "markdown",
       "source": [
         "In this tutorial, we will use the `tf.estimator` API in TensorFlow to solve a\n",
-        "binary classification problem: Given census data about a person such as age,\n",
-        "education, marital status, and occupation (the features), we will try to predict\n",
-        "whether or not the person earns more than 50,000 dollars a year (the target\n",
-        "label). We will train a **logistic regression** model, and given an individual's\n",
-        "information our model will output a number between 0 and 1, which can be\n",
-        "interpreted as the probability that the individual has an annual income of over\n",
+        "standard benchmark binary classification problem: Given census data about a \n",
+        "person such as age, education, marital status, and occupation (the features),\n",
+        "we will try to predict whether or not the person earns more than 50,000 dollars\n",
+        "a year (the target label). We will train a **logistic regression** model, and given \n",
+        "an individual's information our model will output a number between 0 and 1, which\n",
+        "can be interpreted as the probability that the individual has an annual income of over\n",
         "50,000 dollars.\n",
         "\n",
+        "Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is each  feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/).\n",
+        "\n",
         "## Setup\n",
         "\n",
         "To try the code for this tutorial:\n",
@@ -316,16 +318,6 @@
       "execution_count": 0,
       "outputs": []
     },
-    {
-      "metadata": {
-        "id": "mLUJpWKoeCAE",
-        "colab_type": "text"
-      },
-      "cell_type": "markdown",
-      "source": [
-        "Key Point: As a modeler and developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model like this could reinforce societal biases and disparities. Is a feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/)."
-      ]
-    },
     {
       "metadata": {
         "id": "QZZtXes4cYvf",
