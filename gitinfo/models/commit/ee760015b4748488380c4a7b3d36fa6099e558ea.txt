commit ee760015b4748488380c4a7b3d36fa6099e558ea
Author: Maxim Neumann <maximneumann@google.com>
Date:   Tue May 26 01:20:50 2020 -0700

    Add learning_rate to the summary.
    
    PiperOrigin-RevId: 313148142

diff --git a/official/nlp/bert/model_training_utils.py b/official/nlp/bert/model_training_utils.py
index 6a9f6cf0..5ed498bd 100644
--- a/official/nlp/bert/model_training_utils.py
+++ b/official/nlp/bert/model_training_utils.py
@@ -488,6 +488,11 @@ def run_customized_training_loop(
         summary_writer = tf.summary.create_noop_writer()
 
       with summary_writer.as_default():
+        if callable(optimizer.learning_rate):
+          tf.summary.scalar(
+              'learning_rate',
+              optimizer.learning_rate(current_step),
+              step=current_step)
         tf.summary.scalar(
             train_loss_metric.name, train_loss, step=current_step)
         for metric in train_metrics + model.metrics:
