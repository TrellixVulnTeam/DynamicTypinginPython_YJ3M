commit 6f0e3a0bf5b4be72985d02d33b15a23c685b963b
Author: Hongkun Yu <hongkuny@google.com>
Date:   Fri Feb 21 20:28:35 2020 -0800

    Internal change
    
    PiperOrigin-RevId: 296561671

diff --git a/official/nlp/optimization.py b/official/nlp/optimization.py
index 5e968943..b59b208e 100644
--- a/official/nlp/optimization.py
+++ b/official/nlp/optimization.py
@@ -137,16 +137,10 @@ class AdamWeightDecay(tf.keras.optimizers.Adam):
           use_locking=self._use_locking)
     return tf.no_op()
 
-  def apply_gradients(self,
-                      grads_and_vars,
-                      name=None,
-                      all_reduce_sum_gradients=True):
+  def apply_gradients(self, grads_and_vars, name=None):
     grads, tvars = list(zip(*grads_and_vars))
     (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)
-    return super(AdamWeightDecay, self).apply_gradients(
-        zip(grads, tvars),
-        name=name,
-        all_reduce_sum_gradients=all_reduce_sum_gradients)
+    return super(AdamWeightDecay, self).apply_gradients(zip(grads, tvars))
 
   def _get_lr(self, var_device, var_dtype, apply_state):
     """Retrieves the learning rate with the given state."""
