commit 7f9263535a0612dbd47ecdd377a667678fe4fb9f
Author: Hongkun Yu <hongkuny@google.com>
Date:   Thu Feb 13 14:14:57 2020 -0800

    [Refactor] TF models: move all contents of transformer to nlp/transformer
    
    PiperOrigin-RevId: 294997928

diff --git a/official/benchmark/transformer_benchmark.py b/official/benchmark/transformer_benchmark.py
index ac5044d8..011ae68d 100644
--- a/official/benchmark/transformer_benchmark.py
+++ b/official/benchmark/transformer_benchmark.py
@@ -23,8 +23,8 @@ import time
 from absl import flags
 import tensorflow as tf
 
-from official.transformer.v2 import misc
-from official.transformer.v2 import transformer_main as transformer_main
+from official.nlp.transformer import misc
+from official.nlp.transformer import transformer_main as transformer_main
 from official.utils.flags import core as flags_core
 from official.utils.testing import benchmark_wrappers
 from official.utils.testing.perfzero_benchmark import PerfZeroBenchmark
diff --git a/official/transformer/README.md b/official/nlp/transformer/README.md
similarity index 89%
rename from official/transformer/README.md
rename to official/nlp/transformer/README.md
index 93120d4e..1215ed57 100644
--- a/official/transformer/README.md
+++ b/official/nlp/transformer/README.md
@@ -30,7 +30,7 @@ model.
 # https://github.com/tensorflow/models/tree/master/official#requirements
 export PYTHONPATH="$PYTHONPATH:/path/to/models"
 
-cd /path/to/models/official/transformer/v2
+cd /path/to/models/official/nlp/transformer
 
 # Export variables
 PARAM_SET=big
@@ -94,7 +94,7 @@ tensorboard --logdir=$MODEL_DIR
 
 2. ### Model training and evaluation
 
-   [transformer_main.py](v2/transformer_main.py) creates a Transformer keras model,
+   [transformer_main.py](transformer_main.py) creates a Transformer keras model,
    and trains it uses keras model.fit().
 
    Users need to adjust `batch_size` and `num_gpus` to get good performance
@@ -199,16 +199,16 @@ tensorboard --logdir=$MODEL_DIR
 A brief look at each component in the code:
 
 ### Model Definition
-* [transformer.py](v2/transformer.py): Defines a tf.keras.Model: `Transformer`.
-* [embedding_layer.py](v2/embedding_layer.py): Contains the layer that calculates the embeddings. The embedding weights are also used to calculate the pre-softmax probabilities from the decoder output.
-* [attention_layer.py](v2/attention_layer.py): Defines the multi-headed and self attention layers that are used in the encoder/decoder stacks.
-* [ffn_layer.py](v2/ffn_layer.py): Defines the feedforward network that is used in the encoder/decoder stacks. The network is composed of 2 fully connected layers.
+* [transformer.py](transformer.py): Defines a tf.keras.Model: `Transformer`.
+* [embedding_layer.py](embedding_layer.py): Contains the layer that calculates the embeddings. The embedding weights are also used to calculate the pre-softmax probabilities from the decoder output.
+* [attention_layer.py](attention_layer.py): Defines the multi-headed and self attention layers that are used in the encoder/decoder stacks.
+* [ffn_layer.py](ffn_layer.py): Defines the feedforward network that is used in the encoder/decoder stacks. The network is composed of 2 fully connected layers.
 
 Other files:
-* [beam_search.py](v2/beam_search.py) contains the beam search implementation, which is used during model inference to find high scoring translations.
+* [beam_search.py](beam_search.py) contains the beam search implementation, which is used during model inference to find high scoring translations.
 
 ### Model Trainer
-[transformer_main.py](v2/transformer_main.py) creates an `TransformerTask` to train and evaluate the model using tf.keras.
+[transformer_main.py](transformer_main.py) creates an `TransformerTask` to train and evaluate the model using tf.keras.
 
 ### Test dataset
 The [newstest2014 files](https://storage.googleapis.com/tf-perf-public/official_transformer/test_data/newstest2014.tgz)
diff --git a/official/transformer/v2/attention_layer.py b/official/nlp/transformer/attention_layer.py
similarity index 100%
rename from official/transformer/v2/attention_layer.py
rename to official/nlp/transformer/attention_layer.py
diff --git a/official/transformer/v2/beam_search.py b/official/nlp/transformer/beam_search.py
similarity index 98%
rename from official/transformer/v2/beam_search.py
rename to official/nlp/transformer/beam_search.py
index 671e3844..fa1ae52f 100644
--- a/official/transformer/v2/beam_search.py
+++ b/official/nlp/transformer/beam_search.py
@@ -12,13 +12,12 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Beam search in TF v2.
-"""
+"""Beam search in TF v2."""
 
 import tensorflow as tf
 
 from official.nlp.transformer import beam_search_v1 as v1
-from official.transformer.v2 import misc
+from official.nlp.transformer import misc
 
 _StateKeys = v1._StateKeys  # pylint: disable=protected-access
 
diff --git a/official/transformer/compute_bleu.py b/official/nlp/transformer/compute_bleu.py
similarity index 97%
rename from official/transformer/compute_bleu.py
rename to official/nlp/transformer/compute_bleu.py
index 0d9bf4fe..552b450e 100644
--- a/official/transformer/compute_bleu.py
+++ b/official/nlp/transformer/compute_bleu.py
@@ -33,8 +33,8 @@ from absl import flags
 import tensorflow as tf
 # pylint: enable=g-bad-import-order
 
-from official.transformer.utils import metrics
-from official.transformer.utils import tokenizer
+from official.nlp.transformer.utils import metrics
+from official.nlp.transformer.utils import tokenizer
 from official.utils.flags import core as flags_core
 
 
diff --git a/official/transformer/compute_bleu_test.py b/official/nlp/transformer/compute_bleu_test.py
similarity index 95%
rename from official/transformer/compute_bleu_test.py
rename to official/nlp/transformer/compute_bleu_test.py
index 81bc19b5..6c578e36 100644
--- a/official/transformer/compute_bleu_test.py
+++ b/official/nlp/transformer/compute_bleu_test.py
@@ -16,9 +16,9 @@
 
 import tempfile
 
-import tensorflow as tf  # pylint: disable=g-bad-import-order
+import tensorflow as tf
 
-from official.transformer import compute_bleu
+from official.nlp.transformer import compute_bleu
 
 
 class ComputeBleuTest(tf.test.TestCase):
diff --git a/official/transformer/data_download.py b/official/nlp/transformer/data_download.py
similarity index 99%
rename from official/transformer/data_download.py
rename to official/nlp/transformer/data_download.py
index f01e8cce..299dcee0 100644
--- a/official/transformer/data_download.py
+++ b/official/nlp/transformer/data_download.py
@@ -31,7 +31,7 @@ from absl import logging
 import tensorflow.compat.v1 as tf
 # pylint: enable=g-bad-import-order
 
-from official.transformer.utils import tokenizer
+from official.nlp.transformer.utils import tokenizer
 from official.utils.flags import core as flags_core
 
 # Data sources for training/evaluating the transformer translation model.
@@ -88,7 +88,7 @@ VOCAB_FILE = "vocab.ende.%d" % _TARGET_VOCAB_SIZE
 _PREFIX = "wmt32k"
 _TRAIN_TAG = "train"
 _EVAL_TAG = "dev"  # Following WMT and Tensor2Tensor conventions, in which the
-                   # evaluation datasets are tagged as "dev" for development.
+# evaluation datasets are tagged as "dev" for development.
 
 # Number of files to split train and evaluation data
 _TRAIN_SHARDS = 100
diff --git a/official/transformer/v2/data_pipeline.py b/official/nlp/transformer/data_pipeline.py
similarity index 99%
rename from official/transformer/v2/data_pipeline.py
rename to official/nlp/transformer/data_pipeline.py
index edf61cb9..a9a16217 100644
--- a/official/transformer/v2/data_pipeline.py
+++ b/official/nlp/transformer/data_pipeline.py
@@ -57,7 +57,7 @@ import os
 from absl import logging
 import tensorflow as tf
 
-from official.transformer.v2 import misc
+from official.nlp.transformer import misc
 from official.utils.misc import model_helpers
 
 # Buffer size for reading records from a TFRecord file. Each training file is
diff --git a/official/transformer/v2/embedding_layer.py b/official/nlp/transformer/embedding_layer.py
similarity index 100%
rename from official/transformer/v2/embedding_layer.py
rename to official/nlp/transformer/embedding_layer.py
diff --git a/official/transformer/v2/ffn_layer.py b/official/nlp/transformer/ffn_layer.py
similarity index 100%
rename from official/transformer/v2/ffn_layer.py
rename to official/nlp/transformer/ffn_layer.py
diff --git a/official/transformer/v2/metrics.py b/official/nlp/transformer/metrics.py
similarity index 100%
rename from official/transformer/v2/metrics.py
rename to official/nlp/transformer/metrics.py
diff --git a/official/transformer/v2/misc.py b/official/nlp/transformer/misc.py
similarity index 100%
rename from official/transformer/v2/misc.py
rename to official/nlp/transformer/misc.py
diff --git a/official/transformer/v2/optimizer.py b/official/nlp/transformer/optimizer.py
similarity index 100%
rename from official/transformer/v2/optimizer.py
rename to official/nlp/transformer/optimizer.py
diff --git a/official/transformer/v2/transformer.py b/official/nlp/transformer/transformer.py
similarity index 98%
rename from official/transformer/v2/transformer.py
rename to official/nlp/transformer/transformer.py
index 620b0e1d..34d0d092 100644
--- a/official/transformer/v2/transformer.py
+++ b/official/nlp/transformer/transformer.py
@@ -22,14 +22,13 @@ from __future__ import division
 from __future__ import print_function
 
 import tensorflow as tf
-
+from official.nlp.transformer import attention_layer
+from official.nlp.transformer import beam_search
+from official.nlp.transformer import embedding_layer
+from official.nlp.transformer import ffn_layer
+from official.nlp.transformer import metrics
 from official.nlp.transformer import model_utils
-from official.transformer.utils.tokenizer import EOS_ID
-from official.transformer.v2 import attention_layer
-from official.transformer.v2 import beam_search
-from official.transformer.v2 import embedding_layer
-from official.transformer.v2 import ffn_layer
-from official.transformer.v2 import metrics
+from official.nlp.transformer.utils.tokenizer import EOS_ID
 
 
 # Disable the not-callable lint error, since it claims many objects are not
diff --git a/official/transformer/v2/transformer_layers_test.py b/official/nlp/transformer/transformer_layers_test.py
similarity index 94%
rename from official/transformer/v2/transformer_layers_test.py
rename to official/nlp/transformer/transformer_layers_test.py
index ace05173..fd37721c 100644
--- a/official/transformer/v2/transformer_layers_test.py
+++ b/official/nlp/transformer/transformer_layers_test.py
@@ -20,10 +20,10 @@ from __future__ import print_function
 
 import tensorflow as tf
 
-from official.transformer.v2 import attention_layer
-from official.transformer.v2 import embedding_layer
-from official.transformer.v2 import ffn_layer
-from official.transformer.v2 import metrics
+from official.nlp.transformer import attention_layer
+from official.nlp.transformer import embedding_layer
+from official.nlp.transformer import ffn_layer
+from official.nlp.transformer import metrics
 
 
 class TransformerLayersTest(tf.test.TestCase):
diff --git a/official/transformer/v2/transformer_main.py b/official/nlp/transformer/transformer_main.py
similarity index 98%
rename from official/transformer/v2/transformer_main.py
rename to official/nlp/transformer/transformer_main.py
index f7f4ee2d..c65e6018 100644
--- a/official/transformer/v2/transformer_main.py
+++ b/official/nlp/transformer/transformer_main.py
@@ -31,14 +31,14 @@ from absl import logging
 import tensorflow as tf
 
 # pylint: disable=g-bad-import-order
-from official.transformer import compute_bleu
-from official.transformer.utils import tokenizer
-from official.transformer.v2 import data_pipeline
-from official.transformer.v2 import metrics
-from official.transformer.v2 import misc
-from official.transformer.v2 import optimizer
-from official.transformer.v2 import transformer
-from official.transformer.v2 import translate
+from official.nlp.transformer import compute_bleu
+from official.nlp.transformer.utils import tokenizer
+from official.nlp.transformer import data_pipeline
+from official.nlp.transformer import metrics
+from official.nlp.transformer import misc
+from official.nlp.transformer import optimizer
+from official.nlp.transformer import transformer
+from official.nlp.transformer import translate
 from official.utils.flags import core as flags_core
 from official.utils.logs import logger
 from official.utils.misc import keras_utils
diff --git a/official/transformer/v2/transformer_main_test.py b/official/nlp/transformer/transformer_main_test.py
similarity index 98%
rename from official/transformer/v2/transformer_main_test.py
rename to official/nlp/transformer/transformer_main_test.py
index ef5167c3..b9b30b1f 100644
--- a/official/transformer/v2/transformer_main_test.py
+++ b/official/nlp/transformer/transformer_main_test.py
@@ -26,12 +26,10 @@ import unittest
 from absl import flags
 from absl.testing import flagsaver
 import tensorflow as tf
-
-from official.transformer.v2 import misc
-from official.transformer.v2 import transformer_main
-from official.utils.misc import keras_utils
-
 from tensorflow.python.eager import context  # pylint: disable=ungrouped-imports
+from official.nlp.transformer import misc
+from official.nlp.transformer import transformer_main
+from official.utils.misc import keras_utils
 
 FLAGS = flags.FLAGS
 FIXED_TIMESTAMP = 'my_time_stamp'
diff --git a/official/transformer/v2/transformer_test.py b/official/nlp/transformer/transformer_test.py
similarity index 98%
rename from official/transformer/v2/transformer_test.py
rename to official/nlp/transformer/transformer_test.py
index c2365174..85e82c65 100644
--- a/official/transformer/v2/transformer_test.py
+++ b/official/nlp/transformer/transformer_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import tensorflow as tf
 
 from official.nlp.transformer import model_params
-from official.transformer.v2 import transformer
+from official.nlp.transformer import transformer
 
 
 class TransformerV2Test(tf.test.TestCase):
diff --git a/official/transformer/v2/translate.py b/official/nlp/transformer/translate.py
similarity index 99%
rename from official/transformer/v2/translate.py
rename to official/nlp/transformer/translate.py
index a2e85a65..dd7b30a8 100644
--- a/official/transformer/v2/translate.py
+++ b/official/nlp/transformer/translate.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import numpy as np
 import tensorflow as tf
 
-from official.transformer.utils import tokenizer
+from official.nlp.transformer.utils import tokenizer
 
 _EXTRA_DECODE_LENGTH = 100
 _BEAM_SIZE = 4
diff --git a/official/transformer/__init__.py b/official/nlp/transformer/utils/__init__.py
similarity index 100%
rename from official/transformer/__init__.py
rename to official/nlp/transformer/utils/__init__.py
diff --git a/official/transformer/utils/metrics.py b/official/nlp/transformer/utils/metrics.py
similarity index 99%
rename from official/transformer/utils/metrics.py
rename to official/nlp/transformer/utils/metrics.py
index 3e41f985..7900cf80 100644
--- a/official/transformer/utils/metrics.py
+++ b/official/nlp/transformer/utils/metrics.py
@@ -33,7 +33,7 @@ import math
 import numpy as np
 import six
 from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def _pad_tensors_to_same_length(x, y):
diff --git a/official/transformer/utils/tokenizer.py b/official/nlp/transformer/utils/tokenizer.py
similarity index 100%
rename from official/transformer/utils/tokenizer.py
rename to official/nlp/transformer/utils/tokenizer.py
diff --git a/official/transformer/utils/tokenizer_test.py b/official/nlp/transformer/utils/tokenizer_test.py
similarity index 99%
rename from official/transformer/utils/tokenizer_test.py
rename to official/nlp/transformer/utils/tokenizer_test.py
index f757389f..bbe28e2d 100644
--- a/official/transformer/utils/tokenizer_test.py
+++ b/official/nlp/transformer/utils/tokenizer_test.py
@@ -19,7 +19,7 @@ import tempfile
 
 import tensorflow as tf  # pylint: disable=g-bad-import-order
 
-from official.transformer.utils import tokenizer
+from official.nlp.transformer.utils import tokenizer
 
 
 class SubtokenizerTest(tf.test.TestCase):
diff --git a/official/r1/transformer/transformer.py b/official/r1/transformer/transformer.py
index c4d348dd..1aafc6bd 100644
--- a/official/r1/transformer/transformer.py
+++ b/official/r1/transformer/transformer.py
@@ -22,14 +22,14 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf  # pylint: disable=g-bad-import-order
+import tensorflow as tf
 
 from official.nlp.transformer import beam_search_v1 as beam_search
 from official.nlp.transformer import model_utils
+from official.nlp.transformer.utils.tokenizer import EOS_ID
 from official.r1.transformer import attention_layer
 from official.r1.transformer import embedding_layer
 from official.r1.transformer import ffn_layer
-from official.transformer.utils.tokenizer import EOS_ID
 
 _NEG_INF = -1e9
 
diff --git a/official/r1/transformer/transformer_main.py b/official/r1/transformer/transformer_main.py
index 2640ea26..243e2e8e 100644
--- a/official/r1/transformer/transformer_main.py
+++ b/official/r1/transformer/transformer_main.py
@@ -39,9 +39,9 @@ from official.r1.transformer import translate
 from official.r1.transformer import transformer
 from official.r1.transformer import dataset
 from official.r1.transformer import schedule
-from official.transformer import compute_bleu
-from official.transformer.utils import metrics
-from official.transformer.utils import tokenizer
+from official.nlp.transformer import compute_bleu
+from official.nlp.transformer.utils import metrics
+from official.nlp.transformer.utils import tokenizer
 from official.utils.flags import core as flags_core
 from official.utils.logs import hooks_helper
 from official.utils.logs import logger
diff --git a/official/r1/transformer/translate.py b/official/r1/transformer/translate.py
index 9e753ed9..2f7f944a 100644
--- a/official/r1/transformer/translate.py
+++ b/official/r1/transformer/translate.py
@@ -26,7 +26,7 @@ from absl import flags
 import tensorflow as tf
 # pylint: enable=g-bad-import-order
 
-from official.transformer.utils import tokenizer
+from official.nlp.transformer.utils import tokenizer
 from official.utils.flags import core as flags_core
 
 _DECODE_BATCH_SIZE = 32
diff --git a/official/transformer/utils/__init__.py b/official/transformer/utils/__init__.py
deleted file mode 100644
index e69de29b..00000000
diff --git a/official/transformer/v2/__init__.py b/official/transformer/v2/__init__.py
deleted file mode 100644
index e69de29b..00000000
diff --git a/official/utils/misc/distribution_utils.py b/official/utils/misc/distribution_utils.py
index 9fb0600d..fd5c7423 100644
--- a/official/utils/misc/distribution_utils.py
+++ b/official/utils/misc/distribution_utils.py
@@ -22,7 +22,7 @@ import json
 import os
 import random
 import string
-import tensorflow as tf
+import tensorflow.compat.v2 as tf
 
 from official.utils.misc import tpu_lib
 
diff --git a/official/utils/misc/distribution_utils_test.py b/official/utils/misc/distribution_utils_test.py
index c2810096..856c3b3a 100644
--- a/official/utils/misc/distribution_utils_test.py
+++ b/official/utils/misc/distribution_utils_test.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf  # pylint: disable=g-bad-import-order
+import tensorflow.compat.v2 as tf
 
 from official.utils.misc import distribution_utils
 
