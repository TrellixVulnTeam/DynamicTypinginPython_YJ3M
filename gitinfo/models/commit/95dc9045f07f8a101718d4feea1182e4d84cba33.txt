commit 95dc9045f07f8a101718d4feea1182e4d84cba33
Author: Hongkun Yu <hongkuny@google.com>
Date:   Tue Nov 12 10:37:02 2019 -0800

    Fix MLM accuracy bug. We should not just take reduce mean. The sum of weights should be the denominator.
    
    PiperOrigin-RevId: 280002181

diff --git a/official/nlp/bert_models.py b/official/nlp/bert_models.py
index 96922e39..8785328a 100644
--- a/official/nlp/bert_models.py
+++ b/official/nlp/bert_models.py
@@ -172,7 +172,9 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
     """Adds metrics."""
     masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(
         lm_labels, lm_output)
-    masked_lm_accuracy = tf.reduce_mean(masked_lm_accuracy * lm_label_weights)
+    numerator = tf.reduce_sum(masked_lm_accuracy * lm_label_weights)
+    denominator = tf.reduce_sum(lm_label_weights) + 1e-5
+    masked_lm_accuracy = numerator / denominator
     self.add_metric(
         masked_lm_accuracy, name='masked_lm_accuracy', aggregation='mean')
 
