commit 55a34ae564f5ab49adaf98bd4552dcd4bf8f85f3
Author: daviddao <contact.daviddao@gmail.com>
Date:   Sat Mar 19 01:32:42 2016 -0400

    fixing issues #23 and #25

diff --git a/autoencoder/VariationalAutoencoderRunner.py b/autoencoder/VariationalAutoencoderRunner.py
index 2fd07f70..ce8dc3a7 100644
--- a/autoencoder/VariationalAutoencoderRunner.py
+++ b/autoencoder/VariationalAutoencoderRunner.py
@@ -9,8 +9,8 @@ from autoencoder.autoencoder_models.VariationalAutoencoder import VariationalAut
 mnist = input_data.read_data_sets('MNIST_data', one_hot = True)
 
 
-def standard_scale(X_train, X_test):
-    preprocessor = prep.StandardScaler().fit(X_train)
+def minmax_scale(X_train, X_test):
+    preprocessor = prep.MinMaxScaler(feature_range=(0, 1)).fit(X_train)
     X_train = preprocessor.transform(X_train)
     X_test = preprocessor.transform(X_test)
     return X_train, X_test
@@ -21,7 +21,7 @@ def get_random_block_from_data(data, batch_size):
     return data[start_index:(start_index + batch_size)]
 
 
-X_train, X_test = standard_scale(mnist.train.images, mnist.test.images)
+X_train, X_test = minmax_scale(mnist.train.images, mnist.test.images)
 
 n_samples = int(mnist.train.num_examples)
 training_epochs = 20
@@ -30,8 +30,7 @@ display_step = 1
 
 autoencoder = VariationalAutoencoder(n_input = 784,
                                      n_hidden = 200,
-                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001),
-                                     gaussian_sample_size = 128)
+                                     optimizer = tf.train.AdamOptimizer(learning_rate = 0.001))
 
 for epoch in range(training_epochs):
     avg_cost = 0.
diff --git a/autoencoder/autoencoder_models/VariationalAutoencoder.py b/autoencoder/autoencoder_models/VariationalAutoencoder.py
index 4b199a17..0dbc5051 100644
--- a/autoencoder/autoencoder_models/VariationalAutoencoder.py
+++ b/autoencoder/autoencoder_models/VariationalAutoencoder.py
@@ -4,11 +4,9 @@ import autoencoder.Utils
 
 class VariationalAutoencoder(object):
 
-    def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer(),
-                 gaussian_sample_size = 128):
+    def __init__(self, n_input, n_hidden, optimizer = tf.train.AdamOptimizer()):
         self.n_input = n_input
         self.n_hidden = n_hidden
-        self.gaussian_sample_size = gaussian_sample_size
 
         network_weights = self._initialize_weights()
         self.weights = network_weights
@@ -18,14 +16,12 @@ class VariationalAutoencoder(object):
         self.z_mean = tf.add(tf.matmul(self.x, self.weights['w1']), self.weights['b1'])
         self.z_log_sigma_sq = tf.add(tf.matmul(self.x, self.weights['log_sigma_w1']), self.weights['log_sigma_b1'])
 
-
         # sample from gaussian distribution
-        eps = tf.random_normal((self.gaussian_sample_size, n_hidden), 0, 1, dtype = tf.float32)
+        eps = tf.random_normal(tf.pack([tf.shape(self.x)[0], self.n_hidden]), 0, 1, dtype = tf.float32)
         self.z = tf.add(self.z_mean, tf.mul(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))
 
         self.reconstruction = tf.add(tf.matmul(self.z, self.weights['w2']), self.weights['b2'])
 
-
         # cost
         reconstr_loss = 0.5 * tf.reduce_sum(tf.pow(tf.sub(self.reconstruction, self.x), 2.0))
         latent_loss = -0.5 * tf.reduce_sum(1 + self.z_log_sigma_sq
@@ -38,7 +34,6 @@ class VariationalAutoencoder(object):
         self.sess = tf.Session()
         self.sess.run(init)
 
-
     def _initialize_weights(self):
         all_weights = dict()
         all_weights['w1'] = tf.Variable(autoencoder.Utils.xavier_init(self.n_input, self.n_hidden))
