commit a156e20367c2a8195ba11da2e1d8589e93afdf40
Author: saberkun <saberkun@users.noreply.github.com>
Date:   Tue Jun 25 11:19:55 2019 -0700

    Merged commit includes the following changes: (#7100)
    
    254874613  by hongkuny<hongkuny@google.com>:
    
        Update glue tasks enum to match directory name
    
    --
    254866171  by taylorrobie<taylorrobie@google.com>:
    
        Internal change
    
    PiperOrigin-RevId: 254874613

diff --git a/official/bert/create_finetuning_data.py b/official/bert/create_finetuning_data.py
index 48b0ce7f..7a8f3cb9 100644
--- a/official/bert/create_finetuning_data.py
+++ b/official/bert/create_finetuning_data.py
@@ -40,8 +40,8 @@ flags.DEFINE_string(
     "The input data dir. Should contain the .tsv files (or other data files) "
     "for the task.")
 
-flags.DEFINE_enum("classification_task_name", "mnli",
-                  ["cola", "mnli", "mrpc", "xnli"],
+flags.DEFINE_enum("classification_task_name", "MNLI",
+                  ["COLA", "MNLI", "MRPC", "XNLI"],
                   "The name of the task to train BERT classifier.")
 
 # BERT Squad task specific flags.
diff --git a/official/bert/optimization.py b/official/bert/optimization.py
index 66e85892..bd5e1d9d 100644
--- a/official/bert/optimization.py
+++ b/official/bert/optimization.py
@@ -136,14 +136,24 @@ class AdamWeightDecay(tf.keras.optimizers.Adam):
 
   def _resource_apply_dense(self, grad, var):
     var_dtype = var.dtype.base_dtype
-    lr_t = self._decayed_lr_t[var_dtype]
+
+    try:
+      lr_t = self.apply_cache[var.device, var.dtype.base_dtype].lr_t
+    except AttributeError:
+      lr_t = self._decayed_lr_t[var_dtype]
+
     with tf.control_dependencies([self._decay_weights_op(var, lr_t)]):
       return super(AdamWeightDecay, self)._resource_apply_dense(
           grad, var)
 
   def _resource_apply_sparse(self, grad, var, indices):
     var_dtype = var.dtype.base_dtype
-    lr_t = self._decayed_lr_t[var_dtype]
+
+    try:
+      lr_t = self.apply_cache[var.device, var.dtype.base_dtype].lr_t
+    except AttributeError:
+      lr_t = self._decayed_lr_t[var_dtype]
+
     with tf.control_dependencies([self._decay_weights_op(var, lr_t)]):
       return super(AdamWeightDecay, self)._resource_apply_sparse(
           grad, var, indices)
