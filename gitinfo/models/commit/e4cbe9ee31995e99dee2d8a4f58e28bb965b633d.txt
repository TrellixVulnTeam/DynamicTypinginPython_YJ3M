commit e4cbe9ee31995e99dee2d8a4f58e28bb965b633d
Author: Neal Wu <neal@nealwu.com>
Date:   Wed Mar 22 10:58:01 2017 -0700

    Fix the argument order for tf.nn.sampled_softmax_loss in textsum

diff --git a/textsum/seq2seq_attention_model.py b/textsum/seq2seq_attention_model.py
index 6e4355f4..97a9ffd3 100644
--- a/textsum/seq2seq_attention_model.py
+++ b/textsum/seq2seq_attention_model.py
@@ -227,8 +227,9 @@ class Seq2SeqAttentionModel(object):
         def sampled_loss_func(inputs, labels):
           with tf.device('/cpu:0'):  # Try gpu.
             labels = tf.reshape(labels, [-1, 1])
-            return tf.nn.sampled_softmax_loss(w_t, v, inputs, labels,
-                                              hps.num_softmax_samples, vsize)
+            return tf.nn.sampled_softmax_loss(
+                weights=w_t, biases=v, labels=labels, inputs=inputs,
+                num_sampled=hps.num_softmax_samples, num_classes=vsize)
 
         if hps.num_softmax_samples != 0 and hps.mode == 'train':
           self._loss = seq2seq_lib.sampled_sequence_loss(
