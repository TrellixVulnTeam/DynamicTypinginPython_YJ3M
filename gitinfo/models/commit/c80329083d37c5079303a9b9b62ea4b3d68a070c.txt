commit c80329083d37c5079303a9b9b62ea4b3d68a070c
Author: cheerss <zjdxwwx@163.com>
Date:   Thu Nov 1 16:22:01 2018 +0800

    update the calculation of num_batches_per_epoch
    
    the number of batches per epoch also depends on the number of gpus. it should be `num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size / FLAGS.num_gpus)`

diff --git a/tutorials/image/cifar10/cifar10_multi_gpu_train.py b/tutorials/image/cifar10/cifar10_multi_gpu_train.py
index d139f131..db8fdc7f 100644
--- a/tutorials/image/cifar10/cifar10_multi_gpu_train.py
+++ b/tutorials/image/cifar10/cifar10_multi_gpu_train.py
@@ -147,7 +147,7 @@ def train():
 
     # Calculate the learning rate schedule.
     num_batches_per_epoch = (cifar10.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN /
-                             FLAGS.batch_size)
+                             FLAGS.batch_size / FLAGS.num_gpus)
     decay_steps = int(num_batches_per_epoch * cifar10.NUM_EPOCHS_PER_DECAY)
 
     # Decay the learning rate exponentially based on the number of steps.
