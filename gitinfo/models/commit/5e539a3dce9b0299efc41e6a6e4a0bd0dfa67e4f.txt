commit 5e539a3dce9b0299efc41e6a6e4a0bd0dfa67e4f
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Apr 21 10:35:31 2020 -0700

    Test that training=True activates dropout in a reusable SavedModel for BERT.
    
    PiperOrigin-RevId: 307633374

diff --git a/official/nlp/bert/export_tfhub_test.py b/official/nlp/bert/export_tfhub_test.py
index 2287239b..6b6fd40f 100644
--- a/official/nlp/bert/export_tfhub_test.py
+++ b/official/nlp/bert/export_tfhub_test.py
@@ -84,6 +84,16 @@ class ExportTfhubTest(tf.test.TestCase):
       self.assertAllClose(source_output.numpy(), hub_output.numpy())
       self.assertAllClose(source_output.numpy(), encoder_output.numpy())
 
+    # Test that training=True makes a difference (activates dropout).
+    def _dropout_mean_stddev(training, num_runs=20):
+      input_ids = np.array([[14, 12, 42, 95, 99]], np.int32)
+      inputs = [input_ids, np.ones_like(input_ids), np.zeros_like(input_ids)]
+      outputs = np.concatenate(
+          [hub_layer(inputs, training=training)[0] for _ in range(num_runs)])
+      return np.mean(np.std(outputs, axis=0))
+    self.assertLess(_dropout_mean_stddev(training=False), 1e-6)
+    self.assertGreater(_dropout_mean_stddev(training=True), 1e-3)
+
     # Test propagation of seq_length in shape inference.
     input_word_ids = tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32)
     input_mask = tf.keras.layers.Input(shape=(seq_length,), dtype=tf.int32)
