commit 18de53807e64e7716c8ef1db502a2b649ad95a08
Author: Yash Katariya <yash.katariya10@gmail.com>
Date:   Fri Jun 22 20:54:08 2018 +0000

    Replaced 'relu' with tf.nn.relu and similarly for other activations

diff --git a/samples/core/get_started/overfit_and_underfit.ipynb b/samples/core/get_started/overfit_and_underfit.ipynb
index 0ab06cc9..d590c17c 100644
--- a/samples/core/get_started/overfit_and_underfit.ipynb
+++ b/samples/core/get_started/overfit_and_underfit.ipynb
@@ -292,9 +292,9 @@
       "cell_type": "code",
       "source": [
         "baseline_model = keras.Sequential([\n",
-        "    keras.layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
-        "    keras.layers.Dense(16, activation='relu'),\n",
-        "    keras.layers.Dense(1, activation='sigmoid')\n",
+        "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(10000,)),\n",
+        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
+        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
         "])\n",
         "\n",
         "baseline_model.compile(optimizer='adam',\n",
@@ -363,9 +363,9 @@
       "cell_type": "code",
       "source": [
         "smaller_model = keras.Sequential([\n",
-        "    keras.layers.Dense(4, activation='relu', input_shape=(10000,)),\n",
-        "    keras.layers.Dense(4, activation='relu'),\n",
-        "    keras.layers.Dense(1, activation='sigmoid')\n",
+        "    keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(10000,)),\n",
+        "    keras.layers.Dense(4, activation=tf.nn.relu),\n",
+        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
         "])\n",
         "\n",
         "smaller_model.compile(optimizer='adam',\n",
@@ -436,9 +436,9 @@
       "cell_type": "code",
       "source": [
         "bigger_model = keras.models.Sequential([\n",
-        "    keras.layers.Dense(512, activation='relu', input_shape=(10000,)),\n",
-        "    keras.layers.Dense(512, activation='relu'),\n",
-        "    keras.layers.Dense(1, activation='sigmoid')\n",
+        "    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(10000,)),\n",
+        "    keras.layers.Dense(512, activation=tf.nn.relu),\n",
+        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
         "])\n",
         "\n",
         "bigger_model.compile(optimizer='adam',\n",
@@ -604,10 +604,10 @@
       "source": [
         "l2_model = keras.models.Sequential([\n",
         "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
-        "                       activation='relu', input_shape=(10000,)),\n",
+        "                       activation=tf.nn.relu, input_shape=(10000,)),\n",
         "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
-        "                       activation='relu'),\n",
-        "    keras.layers.Dense(1, activation='sigmoid')\n",
+        "                       activation=tf.nn.relu),\n",
+        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
         "])\n",
         "\n",
         "l2_model.compile(optimizer='adam',\n",
@@ -695,11 +695,11 @@
       "cell_type": "code",
       "source": [
         "dpt_model = keras.models.Sequential([\n",
-        "    keras.layers.Dense(16, activation='relu', input_shape=(10000,)),\n",
+        "    keras.layers.Dense(16, activation=tf.nn.relu, input_shape=(10000,)),\n",
         "    keras.layers.Dropout(0.5),\n",
-        "    keras.layers.Dense(16, activation='relu'),\n",
+        "    keras.layers.Dense(16, activation=tf.nn.relu),\n",
         "    keras.layers.Dropout(0.5),\n",
-        "    keras.layers.Dense(1, activation='sigmoid')\n",
+        "    keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
         "])\n",
         "\n",
         "dpt_model.compile(optimizer='adam',\n",
