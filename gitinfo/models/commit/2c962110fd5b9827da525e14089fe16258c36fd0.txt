commit 2c962110fd5b9827da525e14089fe16258c36fd0
Author: huihui-personal <40228379+huihui-personal@users.noreply.github.com>
Date:   Sun Feb 24 10:51:39 2019 +0800

    Open source deeplab changes. Check change log in README.md for detailed info. (#6231)
    
    * \nRefactor deeplab to use MonitoredTrainingSession\n
    
    PiperOrigin-RevId: 234237190
    
    * Update export_model.py
    
    * Update nas_cell.py
    
    * Update nas_network.py
    
    * Update train.py
    
    * Update deeplab_demo.ipynb
    
    * Update nas_cell.py

diff --git a/research/deeplab/README.md b/research/deeplab/README.md
index d2e737d2..8a728692 100644
--- a/research/deeplab/README.md
+++ b/research/deeplab/README.md
@@ -124,40 +124,62 @@ tensorflow/models GitHub [issue
 tracker](https://github.com/tensorflow/models/issues), prefixing the issue name
 with "deeplab".
 
+## License
+
+All the codes in deeplab folder is covered by the [LICENSE](https://github.com/tensorflow/models/blob/master/LICENSE)
+under tensorflow/models. Please refer to the LICENSE for details.
+
 ## Change Logs
 
+### February 6, 2019
+
+* Update decoder module to exploit multiple low-level features with different
+output_strides.
+
+### December 3, 2018
+
+* Released the MobileNet-v2 checkpoint on ADE20K.
+
+
+### November 19, 2018
+
+* Supported NAS architecture for feature extraction. **Contributor**: Chenxi Liu.
+
+* Supported hard pixel mining during training.
+
+
 ### October 1, 2018
 
-Released MobileNet-v2 depth-multiplier = 0.5 COCO-pretrained checkpoints on
+* Released MobileNet-v2 depth-multiplier = 0.5 COCO-pretrained checkpoints on
 PASCAL VOC 2012, and Xception-65 COCO pretrained checkpoint (i.e., no PASCAL
 pretrained).
 
 
 ### September 5, 2018
 
-Released Cityscapes pretrained checkpoints with found best dense prediction cell.
+* Released Cityscapes pretrained checkpoints with found best dense prediction cell.
 
 
 ### May 26, 2018
 
-Updated ADE20K pretrained checkpoint.
+* Updated ADE20K pretrained checkpoint.
 
 
 ### May 18, 2018
-1.  Added builders for ResNet-v1 and Xception model variants.
-1.  Added ADE20K support, including colormap and pretrained Xception_65 checkpoint.
-1.  Fixed a bug on using non-default depth_multiplier for MobileNet-v2.
+* Added builders for ResNet-v1 and Xception model variants.
+* Added ADE20K support, including colormap and pretrained Xception_65 checkpoint.
+* Fixed a bug on using non-default depth_multiplier for MobileNet-v2.
 
 
 ### March 22, 2018
 
-Released checkpoints using MobileNet-V2 as network backbone and pretrained on
+* Released checkpoints using MobileNet-V2 as network backbone and pretrained on
 PASCAL VOC 2012 and Cityscapes.
 
 
 ### March 5, 2018
 
-First release of DeepLab in TensorFlow including deeper Xception network
+* First release of DeepLab in TensorFlow including deeper Xception network
 backbone. Included chekcpoints that have been pretrained on PASCAL VOC 2012
 and Cityscapes.
 
diff --git a/research/deeplab/common.py b/research/deeplab/common.py
index 83b73f07..c5e01872 100644
--- a/research/deeplab/common.py
+++ b/research/deeplab/common.py
@@ -19,7 +19,6 @@ Common flags from train/eval/vis/export_model.py are collected in this script.
 import collections
 import copy
 import json
-
 import tensorflow as tf
 
 flags = tf.app.flags
@@ -53,12 +52,16 @@ flags.DEFINE_multi_float('image_pyramid', None,
 flags.DEFINE_boolean('add_image_level_feature', True,
                      'Add image level feature.')
 
-flags.DEFINE_multi_integer(
+flags.DEFINE_list(
     'image_pooling_crop_size', None,
     'Image pooling crop size [height, width] used in the ASPP module. When '
     'value is None, the model performs image pooling with "crop_size". This'
     'flag is useful when one likes to use different image pooling sizes.')
 
+flags.DEFINE_list(
+    'image_pooling_stride', '1,1',
+    'Image pooling stride [height, width] used in the ASPP image pooling. ')
+
 flags.DEFINE_boolean('aspp_with_batch_norm', True,
                      'Use batch norm parameters for ASPP or not.')
 
@@ -74,11 +77,18 @@ flags.DEFINE_float('depth_multiplier', 1.0,
                    'Multiplier for the depth (number of channels) for all '
                    'convolution ops used in MobileNet.')
 
+flags.DEFINE_integer('divisible_by', None,
+                     'An integer that ensures the layer # channels are '
+                     'divisible by this value. Used in MobileNet.')
+
 # For `xception_65`, use decoder_output_stride = 4. For `mobilenet_v2`, use
 # decoder_output_stride = None.
-flags.DEFINE_integer('decoder_output_stride', None,
-                     'The ratio of input to output spatial resolution when '
-                     'employing decoder to refine segmentation results.')
+flags.DEFINE_list('decoder_output_stride', None,
+                  'Comma-separated list of strings with the number specifying '
+                  'output stride of low-level features at each network level.'
+                  'Current semantic segmentation implementation assumes at '
+                  'most one output stride (i.e., either None or a list with '
+                  'only one element.')
 
 flags.DEFINE_boolean('decoder_use_separable_conv', True,
                      'Employ separable convolution for decoder or not.')
@@ -86,11 +96,28 @@ flags.DEFINE_boolean('decoder_use_separable_conv', True,
 flags.DEFINE_enum('merge_method', 'max', ['max', 'avg'],
                   'Scheme to merge multi scale features.')
 
+flags.DEFINE_boolean(
+    'prediction_with_upsampled_logits', True,
+    'When performing prediction, there are two options: (1) bilinear '
+    'upsampling the logits followed by argmax, or (2) armax followed by '
+    'nearest upsampling the predicted labels. The second option may introduce '
+    'some "blocking effect", but it is more computationally efficient. '
+    'Currently, prediction_with_upsampled_logits=False is only supported for '
+    'single-scale inference.')
+
 flags.DEFINE_string(
     'dense_prediction_cell_json',
     '',
     'A JSON file that specifies the dense prediction cell.')
 
+flags.DEFINE_integer(
+    'nas_stem_output_num_conv_filters', 20,
+    'Number of filters of the stem output tensor in NAS models.')
+
+flags.DEFINE_bool('use_bounded_activation', False,
+                  'Whether or not to use bounded activations. Bounded '
+                  'activations better lend themselves to quantized inference.')
+
 FLAGS = flags.FLAGS
 
 # Constants
@@ -117,9 +144,11 @@ class ModelOptions(
         'crop_size',
         'atrous_rates',
         'output_stride',
+        'preprocessed_images_dtype',
         'merge_method',
         'add_image_level_feature',
         'image_pooling_crop_size',
+        'image_pooling_stride',
         'aspp_with_batch_norm',
         'aspp_with_separable_conv',
         'multi_grid',
@@ -128,7 +157,11 @@ class ModelOptions(
         'logits_kernel_size',
         'model_variant',
         'depth_multiplier',
+        'divisible_by',
+        'prediction_with_upsampled_logits',
         'dense_prediction_cell_config',
+        'nas_stem_output_num_conv_filters',
+        'use_bounded_activation'
     ])):
   """Immutable class to hold model options."""
 
@@ -138,7 +171,8 @@ class ModelOptions(
               outputs_to_num_classes,
               crop_size=None,
               atrous_rates=None,
-              output_stride=8):
+              output_stride=8,
+              preprocessed_images_dtype=tf.float32):
     """Constructor to set default values.
 
     Args:
@@ -148,6 +182,7 @@ class ModelOptions(
       crop_size: A tuple [crop_height, crop_width].
       atrous_rates: A list of atrous convolution rates for ASPP.
       output_stride: The ratio of input to output spatial resolution.
+      preprocessed_images_dtype: The type after the preprocessing function.
 
     Returns:
       A new ModelOptions instance.
@@ -156,18 +191,35 @@ class ModelOptions(
     if FLAGS.dense_prediction_cell_json:
       with tf.gfile.Open(FLAGS.dense_prediction_cell_json, 'r') as f:
         dense_prediction_cell_config = json.load(f)
-
+    decoder_output_stride = None
+    if FLAGS.decoder_output_stride:
+      decoder_output_stride = [
+          int(x) for x in FLAGS.decoder_output_stride]
+      if sorted(decoder_output_stride, reverse=True) != decoder_output_stride:
+        raise ValueError('Decoder output stride need to be sorted in the '
+                         'descending order.')
+    image_pooling_crop_size = None
+    if FLAGS.image_pooling_crop_size:
+      image_pooling_crop_size = [int(x) for x in FLAGS.image_pooling_crop_size]
+    image_pooling_stride = [1, 1]
+    if FLAGS.image_pooling_stride:
+      image_pooling_stride = [int(x) for x in FLAGS.image_pooling_stride]
     return super(ModelOptions, cls).__new__(
         cls, outputs_to_num_classes, crop_size, atrous_rates, output_stride,
-        FLAGS.merge_method, FLAGS.add_image_level_feature,
-        FLAGS.image_pooling_crop_size, FLAGS.aspp_with_batch_norm,
-        FLAGS.aspp_with_separable_conv, FLAGS.multi_grid,
-        FLAGS.decoder_output_stride, FLAGS.decoder_use_separable_conv,
-        FLAGS.logits_kernel_size, FLAGS.model_variant, FLAGS.depth_multiplier,
-        dense_prediction_cell_config)
+        preprocessed_images_dtype, FLAGS.merge_method,
+        FLAGS.add_image_level_feature,
+        image_pooling_crop_size,
+        image_pooling_stride,
+        FLAGS.aspp_with_batch_norm,
+        FLAGS.aspp_with_separable_conv, FLAGS.multi_grid, decoder_output_stride,
+        FLAGS.decoder_use_separable_conv, FLAGS.logits_kernel_size,
+        FLAGS.model_variant, FLAGS.depth_multiplier, FLAGS.divisible_by,
+        FLAGS.prediction_with_upsampled_logits, dense_prediction_cell_config,
+        FLAGS.nas_stem_output_num_conv_filters, FLAGS.use_bounded_activation)
 
   def __deepcopy__(self, memo):
     return ModelOptions(copy.deepcopy(self.outputs_to_num_classes),
                         self.crop_size,
                         self.atrous_rates,
-                        self.output_stride)
+                        self.output_stride,
+                        self.preprocessed_images_dtype)
diff --git a/research/deeplab/core/dense_prediction_cell.py b/research/deeplab/core/dense_prediction_cell.py
index c1498586..b0258954 100644
--- a/research/deeplab/core/dense_prediction_cell.py
+++ b/research/deeplab/core/dense_prediction_cell.py
@@ -64,12 +64,12 @@ def dense_prediction_cell_hparams():
           to 8, we need to double the convolution rates correspondingly.
   """
   return {
-    'reduction_size': 256,
-    'dropout_on_concat_features': True,
-    'dropout_on_projection_features': False,
-    'dropout_keep_prob': 0.9,
-    'concat_channels': 256,
-    'conv_rate_multiplier': 1,
+      'reduction_size': 256,
+      'dropout_on_concat_features': True,
+      'dropout_on_projection_features': False,
+      'dropout_keep_prob': 0.9,
+      'concat_channels': 256,
+      'conv_rate_multiplier': 1,
   }
 
 
@@ -127,7 +127,7 @@ class DensePredictionCell(object):
     return ([resize_height, resize_width], [pooled_height, pooled_width])
 
   def _parse_operation(self, config, crop_size, output_stride,
-      image_pooling_crop_size=None):
+                       image_pooling_crop_size=None):
     """Parses one operation.
 
     When 'operation' is 'pyramid_pooling', we compute the required
@@ -150,23 +150,23 @@ class DensePredictionCell(object):
     if config[_OP] == _PYRAMID_POOLING:
       (config[_TARGET_SIZE],
        config[_KERNEL]) = self._get_pyramid_pooling_arguments(
-          crop_size=crop_size,
-          output_stride=output_stride,
-          image_grid=config[_GRID_SIZE],
-          image_pooling_crop_size=image_pooling_crop_size)
+           crop_size=crop_size,
+           output_stride=output_stride,
+           image_grid=config[_GRID_SIZE],
+           image_pooling_crop_size=image_pooling_crop_size)
 
     return config
 
   def build_cell(self,
-      features,
-      output_stride=16,
-      crop_size=None,
-      image_pooling_crop_size=None,
-      weight_decay=0.00004,
-      reuse=None,
-      is_training=False,
-      fine_tune_batch_norm=False,
-      scope=None):
+                 features,
+                 output_stride=16,
+                 crop_size=None,
+                 image_pooling_crop_size=None,
+                 weight_decay=0.00004,
+                 reuse=None,
+                 is_training=False,
+                 fine_tune_batch_norm=False,
+                 scope=None):
     """Builds the dense prediction cell based on the config.
 
     Args:
@@ -194,10 +194,10 @@ class DensePredictionCell(object):
         the operation is not recognized.
     """
     batch_norm_params = {
-      'is_training': is_training and fine_tune_batch_norm,
-      'decay': 0.9997,
-      'epsilon': 1e-5,
-      'scale': True,
+        'is_training': is_training and fine_tune_batch_norm,
+        'decay': 0.9997,
+        'epsilon': 1e-5,
+        'scale': True,
     }
     hparams = self.hparams
     with slim.arg_scope(
@@ -226,7 +226,7 @@ class DensePredictionCell(object):
               operation_input = branch_logits[current_config[_INPUT]]
             if current_config[_OP] == _CONV:
               if current_config[_KERNEL] == [1, 1] or current_config[
-                _KERNEL] == 1:
+                  _KERNEL] == 1:
                 branch_logits.append(
                     slim.conv2d(operation_input, depth, 1, scope=scope))
               else:
@@ -285,4 +285,4 @@ class DensePredictionCell(object):
                 keep_prob=self.hparams['dropout_keep_prob'],
                 is_training=is_training,
                 scope=_CONCAT_PROJECTION_SCOPE + '_dropout')
-          return concat_logits
\ No newline at end of file
+          return concat_logits
diff --git a/research/deeplab/core/dense_prediction_cell_test.py b/research/deeplab/core/dense_prediction_cell_test.py
index 9ae84ae8..90490e0f 100644
--- a/research/deeplab/core/dense_prediction_cell_test.py
+++ b/research/deeplab/core/dense_prediction_cell_test.py
@@ -29,49 +29,49 @@ class DensePredictionCellTest(tf.test.TestCase):
   def setUp(self):
     self.segmentation_layer = dense_prediction_cell.DensePredictionCell(
         config=[
-          {
-            dense_prediction_cell._INPUT: -1,
-            dense_prediction_cell._OP: dense_prediction_cell._CONV,
-            dense_prediction_cell._KERNEL: 1,
-          },
-          {
-            dense_prediction_cell._INPUT: 0,
-            dense_prediction_cell._OP: dense_prediction_cell._CONV,
-            dense_prediction_cell._KERNEL: 3,
-            dense_prediction_cell._RATE: [1, 3],
-          },
-          {
-            dense_prediction_cell._INPUT: 1,
-            dense_prediction_cell._OP: (
-              dense_prediction_cell._PYRAMID_POOLING),
-            dense_prediction_cell._GRID_SIZE: [1, 2],
-          },
+            {
+                dense_prediction_cell._INPUT: -1,
+                dense_prediction_cell._OP: dense_prediction_cell._CONV,
+                dense_prediction_cell._KERNEL: 1,
+            },
+            {
+                dense_prediction_cell._INPUT: 0,
+                dense_prediction_cell._OP: dense_prediction_cell._CONV,
+                dense_prediction_cell._KERNEL: 3,
+                dense_prediction_cell._RATE: [1, 3],
+            },
+            {
+                dense_prediction_cell._INPUT: 1,
+                dense_prediction_cell._OP: (
+                    dense_prediction_cell._PYRAMID_POOLING),
+                dense_prediction_cell._GRID_SIZE: [1, 2],
+            },
         ],
         hparams={'conv_rate_multiplier': 2})
 
   def testPyramidPoolingArguments(self):
     features_size, pooled_kernel = (
-      self.segmentation_layer._get_pyramid_pooling_arguments(
-          crop_size=[513, 513],
-          output_stride=16,
-          image_grid=[4, 4]))
+        self.segmentation_layer._get_pyramid_pooling_arguments(
+            crop_size=[513, 513],
+            output_stride=16,
+            image_grid=[4, 4]))
     self.assertListEqual(features_size, [33, 33])
     self.assertListEqual(pooled_kernel, [9, 9])
 
   def testPyramidPoolingArgumentsWithImageGrid1x1(self):
     features_size, pooled_kernel = (
-      self.segmentation_layer._get_pyramid_pooling_arguments(
-          crop_size=[257, 257],
-          output_stride=16,
-          image_grid=[1, 1]))
+        self.segmentation_layer._get_pyramid_pooling_arguments(
+            crop_size=[257, 257],
+            output_stride=16,
+            image_grid=[1, 1]))
     self.assertListEqual(features_size, [17, 17])
     self.assertListEqual(pooled_kernel, [17, 17])
 
   def testParseOperationStringWithConv1x1(self):
     operation = self.segmentation_layer._parse_operation(
         config={
-          dense_prediction_cell._OP: dense_prediction_cell._CONV,
-          dense_prediction_cell._KERNEL: [1, 1],
+            dense_prediction_cell._OP: dense_prediction_cell._CONV,
+            dense_prediction_cell._KERNEL: [1, 1],
         },
         crop_size=[513, 513], output_stride=16)
     self.assertEqual(operation[dense_prediction_cell._OP],
@@ -81,9 +81,9 @@ class DensePredictionCellTest(tf.test.TestCase):
   def testParseOperationStringWithConv3x3(self):
     operation = self.segmentation_layer._parse_operation(
         config={
-          dense_prediction_cell._OP: dense_prediction_cell._CONV,
-          dense_prediction_cell._KERNEL: [3, 3],
-          dense_prediction_cell._RATE: [9, 6],
+            dense_prediction_cell._OP: dense_prediction_cell._CONV,
+            dense_prediction_cell._KERNEL: [3, 3],
+            dense_prediction_cell._RATE: [9, 6],
         },
         crop_size=[513, 513], output_stride=16)
     self.assertEqual(operation[dense_prediction_cell._OP],
@@ -94,8 +94,8 @@ class DensePredictionCellTest(tf.test.TestCase):
   def testParseOperationStringWithPyramidPooling2x2(self):
     operation = self.segmentation_layer._parse_operation(
         config={
-          dense_prediction_cell._OP: dense_prediction_cell._PYRAMID_POOLING,
-          dense_prediction_cell._GRID_SIZE: [2, 2],
+            dense_prediction_cell._OP: dense_prediction_cell._PYRAMID_POOLING,
+            dense_prediction_cell._GRID_SIZE: [2, 2],
         },
         crop_size=[513, 513],
         output_stride=16)
@@ -132,4 +132,4 @@ class DensePredictionCellTest(tf.test.TestCase):
 
 
 if __name__ == '__main__':
-  tf.test.main()
\ No newline at end of file
+  tf.test.main()
diff --git a/research/deeplab/core/feature_extractor.py b/research/deeplab/core/feature_extractor.py
index da89dfe9..d7304497 100644
--- a/research/deeplab/core/feature_extractor.py
+++ b/research/deeplab/core/feature_extractor.py
@@ -17,6 +17,7 @@
 import functools
 import tensorflow as tf
 
+from deeplab.core import nas_network
 from deeplab.core import resnet_v1_beta
 from deeplab.core import xception
 from tensorflow.contrib.slim.nets import resnet_utils
@@ -32,6 +33,7 @@ _MOBILENET_V2_FINAL_ENDPOINT = 'layer_18'
 def _mobilenet_v2(net,
                   depth_multiplier,
                   output_stride,
+                  divisible_by=None,
                   reuse=None,
                   scope=None,
                   final_endpoint=None):
@@ -48,6 +50,8 @@ def _mobilenet_v2(net,
       if necessary to prevent the network from reducing the spatial resolution
       of the activation maps. Allowed values are 8 (accurate fully convolutional
       mode), 16 (fast fully convolutional mode), 32 (classification mode).
+    divisible_by: None (use default setting) or an integer that ensures all
+      layers # channels will be divisible by this number. Used in MobileNet.
     reuse: Reuse model variables.
     scope: Optional variable scope.
     final_endpoint: The endpoint to construct the network up to.
@@ -55,6 +59,8 @@ def _mobilenet_v2(net,
   Returns:
     Features extracted by MobileNetv2.
   """
+  if divisible_by is None:
+    divisible_by = 8 if depth_multiplier == 1.0 else 1
   with tf.variable_scope(
       scope, 'MobilenetV2', [net], reuse=reuse) as scope:
     return mobilenet_v2.mobilenet_base(
@@ -62,7 +68,7 @@ def _mobilenet_v2(net,
         conv_defs=mobilenet_v2.V2_DEF,
         depth_multiplier=depth_multiplier,
         min_depth=8 if depth_multiplier == 1.0 else 1,
-        divisible_by=8 if depth_multiplier == 1.0 else 1,
+        divisible_by=divisible_by,
         final_endpoint=final_endpoint or _MOBILENET_V2_FINAL_ENDPOINT,
         output_stride=output_stride,
         scope=scope)
@@ -78,6 +84,8 @@ networks_map = {
     'xception_41': xception.xception_41,
     'xception_65': xception.xception_65,
     'xception_71': xception.xception_71,
+    'nas_pnasnet': nas_network.pnasnet,
+    'nas_hnasnet': nas_network.hnasnet,
 }
 
 # A map from network name to network arg scope.
@@ -90,6 +98,8 @@ arg_scopes_map = {
     'xception_41': xception.xception_arg_scope,
     'xception_65': xception.xception_arg_scope,
     'xception_71': xception.xception_arg_scope,
+    'nas_pnasnet': nas_network.nas_arg_scope,
+    'nas_hnasnet': nas_network.nas_arg_scope,
 }
 
 # Names for end point features.
@@ -98,37 +108,57 @@ DECODER_END_POINTS = 'decoder_end_points'
 # A dictionary from network name to a map of end point features.
 networks_to_feature_maps = {
     'mobilenet_v2': {
-        DECODER_END_POINTS: ['layer_4/depthwise_output'],
+        DECODER_END_POINTS: {
+            4: ['layer_4/depthwise_output'],
+        },
     },
     'resnet_v1_50': {
-        DECODER_END_POINTS: ['block1/unit_2/bottleneck_v1/conv3'],
+        DECODER_END_POINTS: {
+            4: ['block1/unit_2/bottleneck_v1/conv3'],
+        },
     },
     'resnet_v1_50_beta': {
-        DECODER_END_POINTS: ['block1/unit_2/bottleneck_v1/conv3'],
+        DECODER_END_POINTS: {
+            4: ['block1/unit_2/bottleneck_v1/conv3'],
+        },
     },
     'resnet_v1_101': {
-        DECODER_END_POINTS: ['block1/unit_2/bottleneck_v1/conv3'],
+        DECODER_END_POINTS: {
+            4: ['block1/unit_2/bottleneck_v1/conv3'],
+        },
     },
     'resnet_v1_101_beta': {
-        DECODER_END_POINTS: ['block1/unit_2/bottleneck_v1/conv3'],
+        DECODER_END_POINTS: {
+            4: ['block1/unit_2/bottleneck_v1/conv3'],
+        },
     },
     'xception_41': {
-        DECODER_END_POINTS: [
-            'entry_flow/block2/unit_1/xception_module/'
-            'separable_conv2_pointwise',
-        ],
+        DECODER_END_POINTS: {
+            4: ['entry_flow/block2/unit_1/xception_module/'
+                'separable_conv2_pointwise'],
+        },
     },
     'xception_65': {
-        DECODER_END_POINTS: [
-            'entry_flow/block2/unit_1/xception_module/'
-            'separable_conv2_pointwise',
-        ],
+        DECODER_END_POINTS: {
+            4: ['entry_flow/block2/unit_1/xception_module/'
+                'separable_conv2_pointwise'],
+        },
     },
     'xception_71': {
-        DECODER_END_POINTS: [
-            'entry_flow/block3/unit_1/xception_module/'
-            'separable_conv2_pointwise',
-        ],
+        DECODER_END_POINTS: {
+            4: ['entry_flow/block3/unit_1/xception_module/'
+                'separable_conv2_pointwise'],
+        },
+    },
+    'nas_pnasnet': {
+        DECODER_END_POINTS: {
+            4: ['Stem'],
+        },
+    },
+    'nas_hnasnet': {
+        DECODER_END_POINTS: {
+            4: ['Cell_2'],
+        },
     },
 }
 
@@ -143,21 +173,28 @@ name_scope = {
     'xception_41': 'xception_41',
     'xception_65': 'xception_65',
     'xception_71': 'xception_71',
+    'nas_pnasnet': 'pnasnet',
+    'nas_hnasnet': 'hnasnet',
 }
 
 # Mean pixel value.
 _MEAN_RGB = [123.15, 115.90, 103.06]
 
 
-def _preprocess_subtract_imagenet_mean(inputs):
+def _preprocess_subtract_imagenet_mean(inputs, dtype=tf.float32):
   """Subtract Imagenet mean RGB value."""
   mean_rgb = tf.reshape(_MEAN_RGB, [1, 1, 1, 3])
-  return inputs - mean_rgb
+  num_channels = tf.shape(inputs)[-1]
+  # We set mean pixel as 0 for the non-RGB channels.
+  mean_rgb_extended = tf.concat(
+      [mean_rgb, tf.zeros([1, 1, 1, num_channels - 3])], axis=3)
+  return tf.cast(inputs - mean_rgb_extended, dtype=dtype)
 
 
-def _preprocess_zero_mean_unit_range(inputs):
+def _preprocess_zero_mean_unit_range(inputs, dtype=tf.float32):
   """Map image values from [0, 255] to [-1, 1]."""
-  return (2.0 / 255.0) * tf.to_float(inputs) - 1.0
+  preprocessed_inputs = (2.0 / 255.0) * tf.to_float(inputs) - 1.0
+  return tf.cast(preprocessed_inputs, dtype=dtype)
 
 
 _PREPROCESS_FN = {
@@ -169,6 +206,8 @@ _PREPROCESS_FN = {
     'xception_41': _preprocess_zero_mean_unit_range,
     'xception_65': _preprocess_zero_mean_unit_range,
     'xception_71': _preprocess_zero_mean_unit_range,
+    'nas_pnasnet': _preprocess_zero_mean_unit_range,
+    'nas_hnasnet': _preprocess_zero_mean_unit_range,
 }
 
 
@@ -201,6 +240,7 @@ def extract_features(images,
                      output_stride=8,
                      multi_grid=None,
                      depth_multiplier=1.0,
+                     divisible_by=None,
                      final_endpoint=None,
                      model_variant=None,
                      weight_decay=0.0001,
@@ -209,8 +249,12 @@ def extract_features(images,
                      fine_tune_batch_norm=False,
                      regularize_depthwise=False,
                      preprocess_images=True,
+                     preprocessed_images_dtype=tf.float32,
                      num_classes=None,
-                     global_pool=False):
+                     global_pool=False,
+                     nas_stem_output_num_conv_filters=20,
+                     nas_training_hyper_parameters=None,
+                     use_bounded_activation=False):
   """Extracts features by the particular model_variant.
 
   Args:
@@ -219,6 +263,8 @@ def extract_features(images,
     multi_grid: Employ a hierarchy of different atrous rates within network.
     depth_multiplier: Float multiplier for the depth (number of channels)
       for all convolution ops used in MobileNet.
+    divisible_by: None (use default setting) or an integer that ensures all
+      layers # channels will be divisible by this number. Used in MobileNet.
     final_endpoint: The MobileNet endpoint to construct the network up to.
     model_variant: Model variant for feature extraction.
     weight_decay: The weight decay for model variables.
@@ -231,10 +277,22 @@ def extract_features(images,
       True. Set to False if preprocessing will be done by other functions. We
       supprot two types of preprocessing: (1) Mean pixel substraction and (2)
       Pixel values normalization to be [-1, 1].
+    preprocessed_images_dtype: The type after the preprocessing function.
     num_classes: Number of classes for image classification task. Defaults
       to None for dense prediction tasks.
     global_pool: Global pooling for image classification task. Defaults to
       False, since dense prediction tasks do not use this.
+    nas_stem_output_num_conv_filters: Number of filters of the NAS stem output
+      tensor.
+    nas_training_hyper_parameters: A dictionary storing hyper-parameters for
+      training nas models. It is either None or its keys are:
+      - `drop_path_keep_prob`: Probability to keep each path in the cell when
+        training.
+      - `total_training_steps`: Total training steps to help drop path
+        probability calculation.
+    use_bounded_activation: Whether or not to use bounded activations. Bounded
+      activations better lend themselves to quantized inference. Currently,
+      bounded activation is only used in xception model.
 
   Returns:
     features: A tensor of size [batch, feature_height, feature_width,
@@ -253,7 +311,7 @@ def extract_features(images,
         batch_norm_epsilon=1e-5,
         batch_norm_scale=True)
     features, end_points = get_network(
-        model_variant, preprocess_images, arg_scope)(
+        model_variant, preprocess_images, preprocessed_images_dtype, arg_scope)(
             inputs=images,
             num_classes=num_classes,
             is_training=(is_training and fine_tune_batch_norm),
@@ -268,9 +326,10 @@ def extract_features(images,
         batch_norm_decay=0.9997,
         batch_norm_epsilon=1e-3,
         batch_norm_scale=True,
-        regularize_depthwise=regularize_depthwise)
+        regularize_depthwise=regularize_depthwise,
+        use_bounded_activation=use_bounded_activation)
     features, end_points = get_network(
-        model_variant, preprocess_images, arg_scope)(
+        model_variant, preprocess_images, preprocessed_images_dtype, arg_scope)(
             inputs=images,
             num_classes=num_classes,
             is_training=(is_training and fine_tune_batch_norm),
@@ -285,25 +344,44 @@ def extract_features(images,
         is_training=(is_training and fine_tune_batch_norm),
         weight_decay=weight_decay)
     features, end_points = get_network(
-        model_variant, preprocess_images, arg_scope)(
+        model_variant, preprocess_images, preprocessed_images_dtype, arg_scope)(
             inputs=images,
             depth_multiplier=depth_multiplier,
+            divisible_by=divisible_by,
             output_stride=output_stride,
             reuse=reuse,
             scope=name_scope[model_variant],
             final_endpoint=final_endpoint)
+  elif model_variant.startswith('nas'):
+    arg_scope = arg_scopes_map[model_variant](
+        weight_decay=weight_decay,
+        batch_norm_decay=0.9997,
+        batch_norm_epsilon=1e-3)
+    features, end_points = get_network(
+        model_variant, preprocess_images, preprocessed_images_dtype, arg_scope)(
+            inputs=images,
+            num_classes=num_classes,
+            is_training=(is_training and fine_tune_batch_norm),
+            global_pool=global_pool,
+            output_stride=output_stride,
+            nas_stem_output_num_conv_filters=nas_stem_output_num_conv_filters,
+            nas_training_hyper_parameters=nas_training_hyper_parameters,
+            reuse=reuse,
+            scope=name_scope[model_variant])
   else:
     raise ValueError('Unknown model variant %s.' % model_variant)
 
   return features, end_points
 
 
-def get_network(network_name, preprocess_images, arg_scope=None):
+def get_network(network_name, preprocess_images,
+                preprocessed_images_dtype=tf.float32, arg_scope=None):
   """Gets the network.
 
   Args:
     network_name: Network name.
     preprocess_images: Preprocesses the images or not.
+    preprocessed_images_dtype: The type after the preprocessing function.
     arg_scope: Optional, arg_scope to build the network. If not provided the
       default arg_scope of the network would be used.
 
@@ -316,8 +394,8 @@ def get_network(network_name, preprocess_images, arg_scope=None):
   if network_name not in networks_map:
     raise ValueError('Unsupported network %s.' % network_name)
   arg_scope = arg_scope or arg_scopes_map[network_name]()
-  def _identity_function(inputs):
-    return inputs
+  def _identity_function(inputs, dtype=preprocessed_images_dtype):
+    return tf.cast(inputs, dtype=dtype)
   if preprocess_images:
     preprocess_function = _PREPROCESS_FN[network_name]
   else:
@@ -326,5 +404,6 @@ def get_network(network_name, preprocess_images, arg_scope=None):
   @functools.wraps(func)
   def network_fn(inputs, *args, **kwargs):
     with slim.arg_scope(arg_scope):
-      return func(preprocess_function(inputs), *args, **kwargs)
+      return func(preprocess_function(inputs, preprocessed_images_dtype),
+                  *args, **kwargs)
   return network_fn
diff --git a/research/deeplab/core/nas_cell.py b/research/deeplab/core/nas_cell.py
new file mode 100644
index 00000000..085eeb45
--- /dev/null
+++ b/research/deeplab/core/nas_cell.py
@@ -0,0 +1,177 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Cell structure used by NAS."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from deeplab.core.utils import resize_bilinear
+from deeplab.core.utils import scale_dimension
+
+arg_scope = tf.contrib.framework.arg_scope
+slim = tf.contrib.slim
+
+
+class NASBaseCell(object):
+  """NASNet Cell class that is used as a 'layer' in image architectures.
+  See https://arxiv.org/abs/1707.07012 and https://arxiv.org/abs/1712.00559.
+
+  Args:
+    num_conv_filters: The number of filters for each convolution operation.
+    operations: List of operations that are performed in the NASNet Cell in
+      order.
+    used_hiddenstates: Binary array that signals if the hiddenstate was used
+      within the cell. This is used to determine what outputs of the cell
+      should be concatenated together.
+    hiddenstate_indices: Determines what hiddenstates should be combined
+      together with the specified operations to create the NASNet cell.
+  """
+
+  def __init__(self, num_conv_filters, operations, used_hiddenstates,
+               hiddenstate_indices, drop_path_keep_prob, total_num_cells,
+               total_training_steps):
+    if len(hiddenstate_indices) != len(operations):
+      raise ValueError(
+          'Number of hiddenstate_indices and operations should be the same.')
+    if len(operations) % 2:
+      raise ValueError('Number of operations should be even.')
+    self._num_conv_filters = num_conv_filters
+    self._operations = operations
+    self._used_hiddenstates = used_hiddenstates
+    self._hiddenstate_indices = hiddenstate_indices
+    self._drop_path_keep_prob = drop_path_keep_prob
+    self._total_num_cells = total_num_cells
+    self._total_training_steps = total_training_steps
+
+  def __call__(self, net, scope, filter_scaling, stride, prev_layer, cell_num):
+    """Runs the conv cell."""
+    self._cell_num = cell_num
+    self._filter_scaling = filter_scaling
+    self._filter_size = int(self._num_conv_filters * filter_scaling)
+
+    with tf.variable_scope(scope):
+      net = self._cell_base(net, prev_layer)
+      for i in range(len(self._operations) // 2):
+        with tf.variable_scope('comb_iter_{}'.format(i)):
+          h1 = net[self._hiddenstate_indices[i * 2]]
+          h2 = net[self._hiddenstate_indices[i * 2 + 1]]
+          with tf.variable_scope('left'):
+            h1 = self._apply_conv_operation(
+                h1, self._operations[i * 2], stride,
+                self._hiddenstate_indices[i * 2] < 2)
+          with tf.variable_scope('right'):
+            h2 = self._apply_conv_operation(
+                h2, self._operations[i * 2 + 1], stride,
+                self._hiddenstate_indices[i * 2 + 1] < 2)
+          with tf.variable_scope('combine'):
+            h = h1 + h2
+          net.append(h)
+
+      with tf.variable_scope('cell_output'):
+        net = self._combine_unused_states(net)
+
+      return net
+
+  def _cell_base(self, net, prev_layer):
+    """Runs the beginning of the conv cell before the chosen ops are run."""
+    filter_size = self._filter_size
+
+    if prev_layer is None:
+      prev_layer = net
+    else:
+      if net.shape[2] != prev_layer.shape[2]:
+        prev_layer = resize_bilinear(
+            prev_layer, tf.shape(net)[1:3], prev_layer.dtype)
+      if filter_size != prev_layer.shape[3]:
+        prev_layer = tf.nn.relu(prev_layer)
+        prev_layer = slim.conv2d(prev_layer, filter_size, 1, scope='prev_1x1')
+        prev_layer = slim.batch_norm(prev_layer, scope='prev_bn')
+
+    net = tf.nn.relu(net)
+    net = slim.conv2d(net, filter_size, 1, scope='1x1')
+    net = slim.batch_norm(net, scope='beginning_bn')
+    net = tf.split(axis=3, num_or_size_splits=1, value=net)
+    net.append(prev_layer)
+    return net
+
+  def _apply_conv_operation(self, net, operation, stride,
+                            is_from_original_input):
+    """Applies the predicted conv operation to net."""
+    if stride > 1 and not is_from_original_input:
+      stride = 1
+    input_filters = net.shape[3]
+    filter_size = self._filter_size
+    if 'separable' in operation:
+      num_layers = int(operation.split('_')[-1])
+      kernel_size = int(operation.split('x')[0][-1])
+      for layer_num in range(num_layers):
+        net = tf.nn.relu(net)
+        net = slim.separable_conv2d(
+            net,
+            filter_size,
+            kernel_size,
+            depth_multiplier=1,
+            scope='separable_{0}x{0}_{1}'.format(kernel_size, layer_num + 1),
+            stride=stride)
+        net = slim.batch_norm(
+            net, scope='bn_sep_{0}x{0}_{1}'.format(kernel_size, layer_num + 1))
+        stride = 1
+    elif 'atrous' in operation:
+      kernel_size = int(operation.split('x')[0][-1])
+      net = tf.nn.relu(net)
+      if stride == 2:
+        scaled_height = scale_dimension(tf.shape(net)[1], 0.5)
+        scaled_width = scale_dimension(tf.shape(net)[2], 0.5)
+        net = resize_bilinear(net, [scaled_height, scaled_width], net.dtype)
+        net = slim.conv2d(net, filter_size, kernel_size, rate=1,
+                          scope='atrous_{0}x{0}'.format(kernel_size))
+      else:
+        net = slim.conv2d(net, filter_size, kernel_size, rate=2,
+                          scope='atrous_{0}x{0}'.format(kernel_size))
+      net = slim.batch_norm(net, scope='bn_atr_{0}x{0}'.format(kernel_size))
+    elif operation in ['none']:
+      if stride > 1 or (input_filters != filter_size):
+        net = tf.nn.relu(net)
+        net = slim.conv2d(net, filter_size, 1, stride=stride, scope='1x1')
+        net = slim.batch_norm(net, scope='bn_1')
+    elif 'pool' in operation:
+      pooling_type = operation.split('_')[0]
+      pooling_shape = int(operation.split('_')[-1].split('x')[0])
+      if pooling_type == 'avg':
+        net = slim.avg_pool2d(net, pooling_shape, stride=stride, padding='SAME')
+      elif pooling_type == 'max':
+        net = slim.max_pool2d(net, pooling_shape, stride=stride, padding='SAME')
+      else:
+        raise ValueError('Unimplemented pooling type: ', pooling_type)
+      if input_filters != filter_size:
+        net = slim.conv2d(net, filter_size, 1, stride=1, scope='1x1')
+        net = slim.batch_norm(net, scope='bn_1')
+    else:
+      raise ValueError('Unimplemented operation', operation)
+
+    if operation != 'none':
+      net = self._apply_drop_path(net)
+    return net
+
+  def _combine_unused_states(self, net):
+    """Concatenates the unused hidden states of the cell."""
+    used_hiddenstates = self._used_hiddenstates
+    states_to_combine = ([
+        h for h, is_used in zip(net, used_hiddenstates) if not is_used])
+    net = tf.concat(values=states_to_combine, axis=3)
+    return net
diff --git a/research/deeplab/core/nas_genotypes.py b/research/deeplab/core/nas_genotypes.py
new file mode 100644
index 00000000..18228297
--- /dev/null
+++ b/research/deeplab/core/nas_genotypes.py
@@ -0,0 +1,41 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Genotypes used by NAS."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from deeplab.core import nas_cell
+
+
+class PNASCell(nas_cell.NASBaseCell):
+  """Configuration and construction of the PNASNet-5 Cell."""
+
+  def __init__(self, num_conv_filters, drop_path_keep_prob, total_num_cells,
+               total_training_steps):
+    # Name of operations: op_kernel-size_num-layers.
+    operations = [
+        'separable_5x5_2', 'max_pool_3x3', 'separable_7x7_2', 'max_pool_3x3',
+        'separable_5x5_2', 'separable_3x3_2', 'separable_3x3_2', 'max_pool_3x3',
+        'separable_3x3_2', 'none'
+    ]
+    used_hiddenstates = [1, 1, 0, 0, 0, 0, 0]
+    hiddenstate_indices = [1, 1, 0, 0, 0, 0, 4, 0, 1, 0]
+
+    super(PNASCell, self).__init__(
+        num_conv_filters, operations, used_hiddenstates, hiddenstate_indices,
+        drop_path_keep_prob, total_num_cells, total_training_steps)
diff --git a/research/deeplab/core/nas_network.py b/research/deeplab/core/nas_network.py
new file mode 100644
index 00000000..f0436fec
--- /dev/null
+++ b/research/deeplab/core/nas_network.py
@@ -0,0 +1,264 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Network structure used by NAS."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from deeplab.core import nas_genotypes
+from deeplab.core.nas_cell import NASBaseCell
+from deeplab.core.utils import resize_bilinear
+from deeplab.core.utils import scale_dimension
+
+arg_scope = tf.contrib.framework.arg_scope
+slim = tf.contrib.slim
+
+
+def config(num_conv_filters=20,
+           total_training_steps=500000,
+           drop_path_keep_prob=1.0):
+  return tf.contrib.training.HParams(
+      # Multiplier when spatial size is reduced by 2.
+      filter_scaling_rate=2.0,
+      # Number of filters of the stem output tensor.
+      num_conv_filters=num_conv_filters,
+      # Probability to keep each path in the cell when training.
+      drop_path_keep_prob=drop_path_keep_prob,
+      # Total training steps to help drop path probability calculation.
+      total_training_steps=total_training_steps,
+  )
+
+
+def nas_arg_scope(weight_decay=4e-5, batch_norm_decay=0.9997,
+                  batch_norm_epsilon=0.001):
+  """Default arg scope for the NAS models."""
+  batch_norm_params = {
+      # Decay for the moving averages.
+      'decay': batch_norm_decay,
+      # epsilon to prevent 0s in variance.
+      'epsilon': batch_norm_epsilon,
+      'scale': True,
+      'fused': True,
+  }
+  weights_regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  weights_initializer = tf.contrib.layers.variance_scaling_initializer(
+      factor=1/3.0, mode='FAN_IN', uniform=True)
+  with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
+                 weights_regularizer=weights_regularizer,
+                 weights_initializer=weights_initializer):
+    with arg_scope([slim.fully_connected],
+                   activation_fn=None, scope='FC'):
+      with arg_scope([slim.conv2d, slim.separable_conv2d],
+                     activation_fn=None, biases_initializer=None):
+        with arg_scope([slim.batch_norm], **batch_norm_params) as sc:
+          return sc
+
+
+def _nas_stem(inputs):
+  """Stem used for NAS models."""
+  net = slim.conv2d(inputs, 64, [3, 3], stride=2,
+                    scope='conv0', padding='SAME')
+  net = slim.batch_norm(net, scope='conv0_bn')
+  net = tf.nn.relu(net)
+  net = slim.conv2d(net, 64, [3, 3], stride=1,
+                    scope='conv1', padding='SAME')
+  net = slim.batch_norm(net, scope='conv1_bn')
+  cell_outputs = [net]
+  net = tf.nn.relu(net)
+  net = slim.conv2d(net, 128, [3, 3], stride=2,
+                    scope='conv2', padding='SAME')
+  net = slim.batch_norm(net, scope='conv2_bn')
+  cell_outputs.append(net)
+  return net, cell_outputs
+
+
+def _build_nas_base(images,
+                    cell,
+                    backbone,
+                    num_classes,
+                    hparams,
+                    global_pool=False,
+                    reuse=None,
+                    scope=None,
+                    final_endpoint=None):
+  """Constructs a NAS model.
+
+  Args:
+    images: A tensor of size [batch, height, width, channels].
+    cell: Cell structure used in the network.
+    backbone: Backbone structure used in the network. A list of integers in
+      which value 0 means "output_stride=4", value 1 means "output_stride=8",
+      value 2 means "output_stride=16", and value 3 means "output_stride=32".
+    num_classes: Number of classes to predict.
+    hparams: Hyperparameters needed to construct the network.
+    global_pool: If True, we perform global average pooling before computing the
+      logits. Set to True for image classification, False for dense prediction.
+    reuse: Whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
+    scope: Optional variable_scope.
+    final_endpoint: The endpoint to construct the network up to.
+
+  Returns:
+    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+  """
+  with tf.variable_scope(scope, 'nas', [images], reuse=reuse):
+    end_points = {}
+    def add_and_check_endpoint(endpoint_name, net):
+      end_points[endpoint_name] = net
+      return final_endpoint and (endpoint_name == final_endpoint)
+
+    net, cell_outputs = _nas_stem(images)
+    if add_and_check_endpoint('Stem', net):
+      return net, end_points
+
+    # Run the cells
+    filter_scaling = 1.0
+    for cell_num in range(len(backbone)):
+      stride = 1
+      if cell_num == 0:
+        if backbone[0] == 1:
+          stride = 2
+          filter_scaling *= hparams.filter_scaling_rate
+      else:
+        if backbone[cell_num] == backbone[cell_num - 1] + 1:
+          stride = 2
+          filter_scaling *= hparams.filter_scaling_rate
+        elif backbone[cell_num] == backbone[cell_num - 1] - 1:
+          scaled_height = scale_dimension(tf.shape(net)[1], 2)
+          scaled_width = scale_dimension(tf.shape(net)[2], 2)
+          net = resize_bilinear(net, [scaled_height, scaled_width], net.dtype)
+          filter_scaling /= hparams.filter_scaling_rate
+      net = cell(
+          net,
+          scope='cell_{}'.format(cell_num),
+          filter_scaling=filter_scaling,
+          stride=stride,
+          prev_layer=cell_outputs[-2],
+          cell_num=cell_num)
+      if add_and_check_endpoint('Cell_{}'.format(cell_num), net):
+        return net, end_points
+      cell_outputs.append(net)
+    net = tf.nn.relu(net)
+
+    if global_pool:
+      # Global average pooling.
+      net = tf.reduce_mean(net, [1, 2], name='global_pool', keepdims=True)
+    if num_classes is not None:
+      net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
+                        normalizer_fn=None, scope='logits')
+      end_points['predictions'] = slim.softmax(net, scope='predictions')
+    return net, end_points
+
+
+def pnasnet(images,
+            num_classes,
+            is_training=True,
+            global_pool=False,
+            output_stride=16,
+            nas_stem_output_num_conv_filters=20,
+            nas_training_hyper_parameters=None,
+            reuse=None,
+            scope='pnasnet',
+            final_endpoint=None):
+  """Builds PNASNet model."""
+  hparams = config(num_conv_filters=nas_stem_output_num_conv_filters)
+  if nas_training_hyper_parameters:
+    hparams.set_hparam('drop_path_keep_prob',
+                       nas_training_hyper_parameters['drop_path_keep_prob'])
+    hparams.set_hparam('total_training_steps',
+                       nas_training_hyper_parameters['total_training_steps'])
+  if not is_training:
+    tf.logging.info('During inference, setting drop_path_keep_prob = 1.0.')
+    hparams.set_hparam('drop_path_keep_prob', 1.0)
+  tf.logging.info(hparams)
+  if output_stride == 8:
+    backbone = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
+  elif output_stride == 16:
+    backbone = [1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2]
+  elif output_stride == 32:
+    backbone = [1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]
+  else:
+    raise ValueError('Unsupported output_stride ', output_stride)
+  cell = nas_genotypes.PNASCell(hparams.num_conv_filters,
+                                hparams.drop_path_keep_prob,
+                                len(backbone),
+                                hparams.total_training_steps)
+  with arg_scope([slim.dropout, slim.batch_norm], is_training=is_training):
+    return _build_nas_base(
+        images,
+        cell=cell,
+        backbone=backbone,
+        num_classes=num_classes,
+        hparams=hparams,
+        global_pool=global_pool,
+        reuse=reuse,
+        scope=scope,
+        final_endpoint=final_endpoint)
+
+
+# pylint: disable=unused-argument
+def hnasnet(images,
+            num_classes,
+            is_training=True,
+            global_pool=False,
+            output_stride=16,
+            nas_stem_output_num_conv_filters=20,
+            nas_training_hyper_parameters=None,
+            reuse=None,
+            scope='hnasnet',
+            final_endpoint=None):
+  """Builds hierarchical model."""
+  hparams = config(num_conv_filters=nas_stem_output_num_conv_filters)
+  if nas_training_hyper_parameters:
+    hparams.set_hparam('drop_path_keep_prob',
+                       nas_training_hyper_parameters['drop_path_keep_prob'])
+    hparams.set_hparam('total_training_steps',
+                       nas_training_hyper_parameters['total_training_steps'])
+  if not is_training:
+    tf.logging.info('During inference, setting drop_path_keep_prob = 1.0.')
+    hparams.set_hparam('drop_path_keep_prob', 1.0)
+  tf.logging.info(hparams)
+  operations = [
+      'atrous_5x5', 'separable_3x3_2', 'separable_3x3_2', 'atrous_3x3',
+      'separable_3x3_2', 'separable_3x3_2', 'separable_5x5_2',
+      'separable_5x5_2', 'separable_5x5_2', 'atrous_5x5'
+  ]
+  used_hiddenstates = [1, 1, 0, 0, 0, 0, 0]
+  hiddenstate_indices = [1, 0, 1, 0, 3, 1, 4, 2, 3, 5]
+  backbone = [0, 0, 0, 1, 2, 1, 2, 2, 3, 3, 2, 1]
+  cell = NASBaseCell(hparams.num_conv_filters,
+                     operations,
+                     used_hiddenstates,
+                     hiddenstate_indices,
+                     hparams.drop_path_keep_prob,
+                     len(backbone),
+                     hparams.total_training_steps)
+  with arg_scope([slim.dropout, slim.batch_norm], is_training=is_training):
+    return _build_nas_base(
+        images,
+        cell=cell,
+        backbone=backbone,
+        num_classes=num_classes,
+        hparams=hparams,
+        global_pool=global_pool,
+        reuse=reuse,
+        scope=scope,
+        final_endpoint=final_endpoint)
diff --git a/research/deeplab/core/nas_network_test.py b/research/deeplab/core/nas_network_test.py
new file mode 100644
index 00000000..b1922373
--- /dev/null
+++ b/research/deeplab/core/nas_network_test.py
@@ -0,0 +1,98 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for resnet_v1_beta module."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import google3
+import numpy as np
+import tensorflow as tf
+
+from deeplab.core import nas_genotypes
+from deeplab.core import nas_network
+
+arg_scope = tf.contrib.framework.arg_scope
+slim = tf.contrib.slim
+
+
+def create_test_input(batch, height, width, channels):
+  """Creates test input tensor."""
+  if None in [batch, height, width, channels]:
+    return tf.placeholder(tf.float32, (batch, height, width, channels))
+  else:
+    return tf.to_float(
+        np.tile(
+            np.reshape(
+                np.reshape(np.arange(height), [height, 1]) +
+                np.reshape(np.arange(width), [1, width]),
+                [1, height, width, 1]),
+            [batch, 1, 1, channels]))
+
+
+class NASNetworkTest(tf.test.TestCase):
+  """Tests with complete small NAS networks."""
+
+  def _pnasnet_small(self,
+                     images,
+                     num_classes,
+                     is_training=True,
+                     output_stride=16,
+                     final_endpoint=None):
+    """Build PNASNet model backbone."""
+    hparams = tf.contrib.training.HParams(
+        filter_scaling_rate=2.0,
+        num_conv_filters=10,
+        drop_path_keep_prob=1.0,
+        total_training_steps=200000,
+    )
+    if not is_training:
+      hparams.set_hparam('drop_path_keep_prob', 1.0)
+
+    backbone = [1, 2, 2]
+    cell = nas_genotypes.PNASCell(hparams.num_conv_filters,
+                                  hparams.drop_path_keep_prob,
+                                  len(backbone),
+                                  hparams.total_training_steps)
+    with arg_scope([slim.dropout, slim.batch_norm], is_training=is_training):
+      return nas_network._build_nas_base(
+          images,
+          cell=cell,
+          backbone=backbone,
+          num_classes=num_classes,
+          hparams=hparams,
+          reuse=tf.AUTO_REUSE,
+          scope='pnasnet_small',
+          final_endpoint=final_endpoint)
+
+  def testFullyConvolutionalEndpointShapes(self):
+    num_classes = 10
+    inputs = create_test_input(2, 321, 321, 3)
+    with slim.arg_scope(nas_network.nas_arg_scope()):
+      _, end_points = self._pnasnet_small(inputs,
+                                          num_classes)
+      endpoint_to_shape = {
+          'Stem': [2, 81, 81, 128],
+          'Cell_0': [2, 41, 41, 100],
+          'Cell_1': [2, 21, 21, 200],
+          'Cell_2': [2, 21, 21, 200]}
+      for endpoint, shape in endpoint_to_shape.iteritems():
+        self.assertListEqual(end_points[endpoint].get_shape().as_list(), shape)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/core/utils.py b/research/deeplab/core/utils.py
index 60a87430..d9c7de22 100644
--- a/research/deeplab/core/utils.py
+++ b/research/deeplab/core/utils.py
@@ -19,6 +19,22 @@ import tensorflow as tf
 slim = tf.contrib.slim
 
 
+def resize_bilinear(images, size, output_dtype=tf.float32):
+  """Returns resized images as output_type.
+
+  Args:
+    images: A tensor of size [batch, height_in, width_in, channels].
+    size: A 1-D int32 Tensor of 2 elements: new_height, new_width. The new size
+      for the images.
+    output_dtype: The destination type.
+  Returns:
+    A tensor of size [batch, height_out, width_out, channels] as a dtype of
+      output_dtype.
+  """
+  images = tf.image.resize_bilinear(images, size, align_corners=True)
+  return tf.cast(images, dtype=output_dtype)
+
+
 def scale_dimension(dim, scale):
   """Scales the input dimension.
 
@@ -36,13 +52,13 @@ def scale_dimension(dim, scale):
 
 
 def split_separable_conv2d(inputs,
-    filters,
-    kernel_size=3,
-    rate=1,
-    weight_decay=0.00004,
-    depthwise_weights_initializer_stddev=0.33,
-    pointwise_weights_initializer_stddev=0.06,
-    scope=None):
+                           filters,
+                           kernel_size=3,
+                           rate=1,
+                           weight_decay=0.00004,
+                           depthwise_weights_initializer_stddev=0.33,
+                           pointwise_weights_initializer_stddev=0.06,
+                           scope=None):
   """Splits a separable conv2d into depthwise and pointwise conv2d.
 
   This operation differs from `tf.layers.separable_conv2d` as this operation
@@ -81,4 +97,4 @@ def split_separable_conv2d(inputs,
       weights_initializer=tf.truncated_normal_initializer(
           stddev=pointwise_weights_initializer_stddev),
       weights_regularizer=slim.l2_regularizer(weight_decay),
-      scope=scope + '_pointwise')
\ No newline at end of file
+      scope=scope + '_pointwise')
diff --git a/research/deeplab/core/utils_test.py b/research/deeplab/core/utils_test.py
index 83843282..4d63df4c 100644
--- a/research/deeplab/core/utils_test.py
+++ b/research/deeplab/core/utils_test.py
@@ -1,4 +1,3 @@
-
 # Copyright 2018 The TensorFlow Authors All Rights Reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
@@ -29,4 +28,4 @@ class UtilsTest(tf.test.TestCase):
 
 
 if __name__ == '__main__':
-  tf.test.main()
\ No newline at end of file
+  tf.test.main()
diff --git a/research/deeplab/core/xception.py b/research/deeplab/core/xception.py
index 6fd306cb..575acce1 100644
--- a/research/deeplab/core/xception.py
+++ b/research/deeplab/core/xception.py
@@ -52,6 +52,8 @@ slim = tf.contrib.slim
 
 
 _DEFAULT_MULTI_GRID = [1, 1, 1]
+# The cap for tf.clip_by_value.
+_CLIP_CAP = 6
 
 
 class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):
@@ -200,7 +202,9 @@ def xception_module(inputs,
                     activation_fn_in_separable_conv=False,
                     regularize_depthwise=False,
                     outputs_collections=None,
-                    scope=None):
+                    scope=None,
+                    use_bounded_activation=False,
+                    use_explicit_padding=True):
   """An Xception module.
 
   The output of one Xception module is equal to the sum of `residual` and
@@ -230,6 +234,11 @@ def xception_module(inputs,
       depthwise convolution weights.
     outputs_collections: Collection to add the Xception unit output.
     scope: Optional variable_scope.
+    use_bounded_activation: Whether or not to use bounded activations. Bounded
+      activations better lend themselves to quantized inference.
+    use_explicit_padding: If True, use explicit padding to make the model fully
+      compatible with the open source version, otherwise use the native
+      Tensorflow 'SAME' padding.
 
   Returns:
     The Xception module's output.
@@ -250,11 +259,19 @@ def xception_module(inputs,
 
     def _separable_conv(features, depth, kernel_size, depth_multiplier,
                         regularize_depthwise, rate, stride, scope):
+      """Separable conv block."""
       if activation_fn_in_separable_conv:
-        activation_fn = tf.nn.relu
+        activation_fn = tf.nn.relu6 if use_bounded_activation else tf.nn.relu
       else:
-        activation_fn = None
-        features = tf.nn.relu(features)
+        if use_bounded_activation:
+          # When use_bounded_activation is True, we clip the feature values and
+          # apply relu6 for activation.
+          activation_fn = lambda x: tf.clip_by_value(x, -_CLIP_CAP, _CLIP_CAP)
+          features = tf.nn.relu6(features)
+        else:
+          # Original network design.
+          activation_fn = None
+          features = tf.nn.relu(features)
       return separable_conv2d_same(features,
                                    depth,
                                    kernel_size,
@@ -262,6 +279,7 @@ def xception_module(inputs,
                                    stride=stride,
                                    rate=rate,
                                    activation_fn=activation_fn,
+                                   use_explicit_padding=use_explicit_padding,
                                    regularize_depthwise=regularize_depthwise,
                                    scope=scope)
     for i in range(3):
@@ -280,9 +298,19 @@ def xception_module(inputs,
                              stride=stride,
                              activation_fn=None,
                              scope='shortcut')
+      if use_bounded_activation:
+        residual = tf.clip_by_value(residual, -_CLIP_CAP, _CLIP_CAP)
+        shortcut = tf.clip_by_value(shortcut, -_CLIP_CAP, _CLIP_CAP)
       outputs = residual + shortcut
+      if use_bounded_activation:
+        outputs = tf.nn.relu6(outputs)
     elif skip_connection_type == 'sum':
+      if use_bounded_activation:
+        residual = tf.clip_by_value(residual, -_CLIP_CAP, _CLIP_CAP)
+        inputs = tf.clip_by_value(inputs, -_CLIP_CAP, _CLIP_CAP)
       outputs = residual + inputs
+      if use_bounded_activation:
+        outputs = tf.nn.relu6(outputs)
     elif skip_connection_type == 'none':
       outputs = residual
     else:
@@ -713,9 +741,9 @@ def xception_arg_scope(weight_decay=0.00004,
                        batch_norm_epsilon=0.001,
                        batch_norm_scale=True,
                        weights_initializer_stddev=0.09,
-                       activation_fn=tf.nn.relu,
                        regularize_depthwise=False,
-                       use_batch_norm=True):
+                       use_batch_norm=True,
+                       use_bounded_activation=False):
   """Defines the default Xception arg scope.
 
   Args:
@@ -728,10 +756,11 @@ def xception_arg_scope(weight_decay=0.00004,
       activations in the batch normalization layer.
     weights_initializer_stddev: The standard deviation of the trunctated normal
       weight initializer.
-    activation_fn: The activation function in Xception.
     regularize_depthwise: Whether or not apply L2-norm regularization on the
       depthwise convolution weights.
     use_batch_norm: Whether or not to use batch normalization.
+    use_bounded_activation: Whether or not to use bounded activations. Bounded
+      activations better lend themselves to quantized inference.
 
   Returns:
     An `arg_scope` to use for the Xception models.
@@ -745,6 +774,7 @@ def xception_arg_scope(weight_decay=0.00004,
     depthwise_regularizer = slim.l2_regularizer(weight_decay)
   else:
     depthwise_regularizer = None
+  activation_fn = tf.nn.relu6 if use_bounded_activation else tf.nn.relu
   with slim.arg_scope(
       [slim.conv2d, slim.separable_conv2d],
       weights_initializer=tf.truncated_normal_initializer(
@@ -757,5 +787,9 @@ def xception_arg_scope(weight_decay=0.00004,
           weights_regularizer=slim.l2_regularizer(weight_decay)):
         with slim.arg_scope(
             [slim.separable_conv2d],
-            weights_regularizer=depthwise_regularizer) as arg_sc:
-          return arg_sc
+            weights_regularizer=depthwise_regularizer):
+          with slim.arg_scope(
+              [xception_module],
+              use_bounded_activation=use_bounded_activation,
+              use_explicit_padding=not use_bounded_activation) as arg_sc:
+            return arg_sc
diff --git a/research/deeplab/core/xception_test.py b/research/deeplab/core/xception_test.py
index 4ef25ba5..df925e56 100644
--- a/research/deeplab/core/xception_test.py
+++ b/research/deeplab/core/xception_test.py
@@ -462,6 +462,24 @@ class XceptionNetworkTest(tf.test.TestCase):
           reuse=True)
     self.assertItemsEqual(end_points0.keys(), end_points1.keys())
 
+  def testUseBoundedAcitvation(self):
+    global_pool = False
+    num_classes = 10
+    output_stride = 8
+    for use_bounded_activation in (True, False):
+      tf.reset_default_graph()
+      inputs = create_test_input(2, 321, 321, 3)
+      with slim.arg_scope(xception.xception_arg_scope(
+          use_bounded_activation=use_bounded_activation)):
+        _, _ = self._xception_small(
+            inputs,
+            num_classes,
+            global_pool=global_pool,
+            output_stride=output_stride,
+            scope='xception')
+        for node in tf.get_default_graph().as_graph_def().node:
+          if node.op.startswith('Relu'):
+            self.assertEqual(node.op == 'Relu6', use_bounded_activation)
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/deeplab/datasets/data_generator.py b/research/deeplab/datasets/data_generator.py
new file mode 100644
index 00000000..7abda7d1
--- /dev/null
+++ b/research/deeplab/datasets/data_generator.py
@@ -0,0 +1,347 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Wrapper for providing semantic segmentaion data.
+
+The SegmentationDataset class provides both images and annotations (semantic
+segmentation and/or instance segmentation) for TensorFlow. Currently, we
+support the following datasets:
+
+1. PASCAL VOC 2012 (http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).
+
+PASCAL VOC 2012 semantic segmentation dataset annotates 20 foreground objects
+(e.g., bike, person, and so on) and leaves all the other semantic classes as
+one background class. The dataset contains 1464, 1449, and 1456 annotated
+images for the training, validation and test respectively.
+
+2. Cityscapes dataset (https://www.cityscapes-dataset.com)
+
+The Cityscapes dataset contains 19 semantic labels (such as road, person, car,
+and so on) for urban street scenes.
+
+3. ADE20K dataset (http://groups.csail.mit.edu/vision/datasets/ADE20K)
+
+The ADE20K dataset contains 150 semantic labels both urban street scenes and
+indoor scenes.
+
+References:
+  M. Everingham, S. M. A. Eslami, L. V. Gool, C. K. I. Williams, J. Winn,
+  and A. Zisserman, The pascal visual object classes challenge a retrospective.
+  IJCV, 2014.
+
+  M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
+  U. Franke, S. Roth, and B. Schiele, "The cityscapes dataset for semantic urban
+  scene understanding," In Proc. of CVPR, 2016.
+
+  B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A. Torralba, "Scene Parsing
+  through ADE20K dataset", In Proc. of CVPR, 2017.
+"""
+
+import collections
+import os
+import tensorflow as tf
+from deeplab import common
+from deeplab import input_preprocess
+
+# Named tuple to describe the dataset properties.
+DatasetDescriptor = collections.namedtuple(
+    'DatasetDescriptor',
+    [
+        'splits_to_sizes',  # Splits of the dataset into training, val and test.
+        'num_classes',  # Number of semantic classes, including the
+                        # background class (if exists). For example, there
+                        # are 20 foreground classes + 1 background class in
+                        # the PASCAL VOC 2012 dataset. Thus, we set
+                        # num_classes=21.
+        'ignore_label',  # Ignore label value.
+    ])
+
+_CITYSCAPES_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 2975,
+        'val': 500,
+    },
+    num_classes=19,
+    ignore_label=255,
+)
+
+_PASCAL_VOC_SEG_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 1464,
+        'train_aug': 10582,
+        'trainval': 2913,
+        'val': 1449,
+    },
+    num_classes=21,
+    ignore_label=255,
+)
+
+_ADE20K_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 20210,  # num of samples in images/training
+        'val': 2000,  # num of samples in images/validation
+    },
+    num_classes=151,
+    ignore_label=0,
+)
+
+_DATASETS_INFORMATION = {
+    'cityscapes': _CITYSCAPES_INFORMATION,
+    'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
+    'ade20k': _ADE20K_INFORMATION,
+}
+
+# Default file pattern of TFRecord of TensorFlow Example.
+_FILE_PATTERN = '%s-*'
+
+
+def get_cityscapes_dataset_name():
+  return 'cityscapes'
+
+
+class Dataset(object):
+  """Represents input dataset for deeplab model."""
+
+  def __init__(self,
+               dataset_name,
+               split_name,
+               dataset_dir,
+               batch_size,
+               crop_size,
+               min_resize_value=None,
+               max_resize_value=None,
+               resize_factor=None,
+               min_scale_factor=1.,
+               max_scale_factor=1.,
+               scale_factor_step_size=0,
+               model_variant=None,
+               num_readers=1,
+               is_training=False,
+               should_shuffle=False,
+               should_repeat=False):
+    """Initializes the dataset.
+
+    Args:
+      dataset_name: Dataset name.
+      split_name: A train/val Split name.
+      dataset_dir: The directory of the dataset sources.
+      batch_size: Batch size.
+      crop_size: The size used to crop the image and label.
+      min_resize_value: Desired size of the smaller image side.
+      max_resize_value: Maximum allowed size of the larger image side.
+      resize_factor: Resized dimensions are multiple of factor plus one.
+      min_scale_factor: Minimum scale factor value.
+      max_scale_factor: Maximum scale factor value.
+      scale_factor_step_size: The step size from min scale factor to max scale
+        factor. The input is randomly scaled based on the value of
+        (min_scale_factor, max_scale_factor, scale_factor_step_size).
+      model_variant: Model variant (string) for choosing how to mean-subtract
+        the images. See feature_extractor.network_map for supported model
+        variants.
+      num_readers: Number of readers for data provider.
+      is_training: Boolean, if dataset is for training or not.
+      should_shuffle: Boolean, if should shuffle the input data.
+      should_repeat: Boolean, if should repeat the input data.
+
+    Raises:
+      ValueError: Dataset name and split name are not supported.
+    """
+    if dataset_name not in _DATASETS_INFORMATION:
+      raise ValueError('The specified dataset is not supported yet.')
+    self.dataset_name = dataset_name
+
+    splits_to_sizes = _DATASETS_INFORMATION[dataset_name].splits_to_sizes
+
+    if split_name not in splits_to_sizes:
+      raise ValueError('data split name %s not recognized' % split_name)
+
+    if model_variant is None:
+      tf.logging.warning('Please specify a model_variant. See '
+                         'feature_extractor.network_map for supported model '
+                         'variants.')
+
+    self.split_name = split_name
+    self.dataset_dir = dataset_dir
+    self.batch_size = batch_size
+    self.crop_size = crop_size
+    self.min_resize_value = min_resize_value
+    self.max_resize_value = max_resize_value
+    self.resize_factor = resize_factor
+    self.min_scale_factor = min_scale_factor
+    self.max_scale_factor = max_scale_factor
+    self.scale_factor_step_size = scale_factor_step_size
+    self.model_variant = model_variant
+    self.num_readers = num_readers
+    self.is_training = is_training
+    self.should_shuffle = should_shuffle
+    self.should_repeat = should_repeat
+
+    self.num_of_classes = _DATASETS_INFORMATION[self.dataset_name].num_classes
+    self.ignore_label = _DATASETS_INFORMATION[self.dataset_name].ignore_label
+
+  def _parse_function(self, example_proto):
+    """Function to parse the example proto.
+
+    Args:
+      example_proto: Proto in the format of tf.Example.
+
+    Returns:
+      A dictionary with parsed image, label, height, width and image name.
+
+    Raises:
+      ValueError: Label is of wrong shape.
+    """
+
+    # Currently only supports jpeg and png.
+    # Need to use this logic because the shape is not known for
+    # tf.image.decode_image and we rely on this info to
+    # extend label if necessary.
+    def _decode_image(content, channels):
+      return tf.cond(
+          tf.image.is_jpeg(content),
+          lambda: tf.image.decode_jpeg(content, channels),
+          lambda: tf.image.decode_png(content, channels))
+
+    features = {
+        'image/encoded':
+            tf.FixedLenFeature((), tf.string, default_value=''),
+        'image/filename':
+            tf.FixedLenFeature((), tf.string, default_value=''),
+        'image/format':
+            tf.FixedLenFeature((), tf.string, default_value='jpeg'),
+        'image/height':
+            tf.FixedLenFeature((), tf.int64, default_value=0),
+        'image/width':
+            tf.FixedLenFeature((), tf.int64, default_value=0),
+        'image/segmentation/class/encoded':
+            tf.FixedLenFeature((), tf.string, default_value=''),
+        'image/segmentation/class/format':
+            tf.FixedLenFeature((), tf.string, default_value='png'),
+    }
+
+    parsed_features = tf.parse_single_example(example_proto, features)
+
+    image = _decode_image(parsed_features['image/encoded'], channels=3)
+
+    label = None
+    if self.split_name != common.TEST_SET:
+      label = _decode_image(
+          parsed_features['image/segmentation/class/encoded'], channels=1)
+
+    image_name = parsed_features['image/filename']
+    if image_name is None:
+      image_name = tf.constant('')
+
+    sample = {
+        common.IMAGE: image,
+        common.IMAGE_NAME: image_name,
+        common.HEIGHT: parsed_features['image/height'],
+        common.WIDTH: parsed_features['image/width'],
+    }
+
+    if label is not None:
+      if label.get_shape().ndims == 2:
+        label = tf.expand_dims(label, 2)
+      elif label.get_shape().ndims == 3 and label.shape.dims[2] == 1:
+        pass
+      else:
+        raise ValueError('Input label shape must be [height, width], or '
+                         '[height, width, 1].')
+
+      label.set_shape([None, None, 1])
+
+      sample[common.LABELS_CLASS] = label
+
+    return sample
+
+  def _preprocess_image(self, sample):
+    """Preprocesses the image and label.
+
+    Args:
+      sample: A sample containing image and label.
+
+    Returns:
+      sample: Sample with preprocessed image and label.
+
+    Raises:
+      ValueError: Ground truth label not provided during training.
+    """
+    image = sample[common.IMAGE]
+    label = sample[common.LABELS_CLASS]
+
+    original_image, image, label = input_preprocess.preprocess_image_and_label(
+        image=image,
+        label=label,
+        crop_height=self.crop_size[0],
+        crop_width=self.crop_size[1],
+        min_resize_value=self.min_resize_value,
+        max_resize_value=self.max_resize_value,
+        resize_factor=self.resize_factor,
+        min_scale_factor=self.min_scale_factor,
+        max_scale_factor=self.max_scale_factor,
+        scale_factor_step_size=self.scale_factor_step_size,
+        ignore_label=self.ignore_label,
+        is_training=self.is_training,
+        model_variant=self.model_variant)
+
+    sample[common.IMAGE] = image
+
+    if not self.is_training:
+      # Original image is only used during visualization.
+      sample[common.ORIGINAL_IMAGE] = original_image
+
+    if label is not None:
+      sample[common.LABEL] = label
+
+    # Remove common.LABEL_CLASS key in the sample since it is only used to
+    # derive label and not used in training and evaluation.
+    sample.pop(common.LABELS_CLASS, None)
+
+    return sample
+
+  def get_one_shot_iterator(self):
+    """Gets an iterator that iterates across the dataset once.
+
+    Returns:
+      An iterator of type tf.data.Iterator.
+    """
+
+    files = self._get_all_files()
+
+    dataset = (
+        tf.data.TFRecordDataset(files, num_parallel_reads=self.num_readers)
+        .map(self._parse_function, num_parallel_calls=self.num_readers)
+        .map(self._preprocess_image, num_parallel_calls=self.num_readers))
+
+    if self.should_shuffle:
+      dataset = dataset.shuffle(buffer_size=100)
+
+    if self.should_repeat:
+      dataset = dataset.repeat()  # Repeat forever for training.
+    else:
+      dataset = dataset.repeat(1)
+
+    dataset = dataset.batch(self.batch_size).prefetch(self.batch_size)
+    return dataset.make_one_shot_iterator()
+
+  def _get_all_files(self):
+    """Gets all the files to read data from.
+
+    Returns:
+      A list of input files.
+    """
+    file_pattern = _FILE_PATTERN
+    file_pattern = os.path.join(self.dataset_dir,
+                                file_pattern % self.split_name)
+    return tf.gfile.Glob(file_pattern)
diff --git a/research/deeplab/datasets/data_generator_test.py b/research/deeplab/datasets/data_generator_test.py
new file mode 100644
index 00000000..fbe58d1d
--- /dev/null
+++ b/research/deeplab/datasets/data_generator_test.py
@@ -0,0 +1,222 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for deeplab.datasets.data_generator."""
+
+from __future__ import print_function
+
+import collections
+import google3
+import tensorflow as tf
+
+from deeplab import common
+from deeplab.datasets import data_generator
+
+ImageAttributes = collections.namedtuple(
+    'ImageAttributes', ['image', 'label', 'height', 'width', 'image_name'])
+
+
+class DatasetTest(tf.test.TestCase):
+
+  # Note: training dataset cannot be tested since there is shuffle operation.
+  # When disabling the shuffle, training dataset is operated same as validation
+  # dataset. Therefore it is not tested again.
+  def testPascalVocSegTestData(self):
+    dataset = data_generator.Dataset(
+        dataset_name='pascal_voc_seg',
+        split_name='val',
+        dataset_dir=
+        'research/deeplab/testing/pascal_voc_seg',
+        batch_size=1,
+        crop_size=[3, 3],  # Use small size for testing.
+        min_resize_value=3,
+        max_resize_value=3,
+        resize_factor=None,
+        min_scale_factor=0.01,
+        max_scale_factor=2.0,
+        scale_factor_step_size=0.25,
+        is_training=False,
+        model_variant='mobilenet_v2')
+
+    self.assertAllEqual(dataset.num_of_classes, 21)
+    self.assertAllEqual(dataset.ignore_label, 255)
+
+    num_of_images = 3
+    with self.test_session() as sess:
+      iterator = dataset.get_one_shot_iterator()
+
+      for i in range(num_of_images):
+        batch = iterator.get_next()
+        batch, = sess.run([batch])
+        image_attributes = _get_attributes_of_image(i)
+
+        self.assertAllEqual(batch[common.IMAGE][0], image_attributes.image)
+        self.assertAllEqual(batch[common.LABEL][0], image_attributes.label)
+        self.assertEqual(batch[common.HEIGHT][0], image_attributes.height)
+        self.assertEqual(batch[common.WIDTH][0], image_attributes.width)
+        self.assertEqual(batch[common.IMAGE_NAME][0],
+                         image_attributes.image_name)
+
+      # All data have been read.
+      with self.assertRaisesRegexp(tf.errors.OutOfRangeError, ''):
+        sess.run([iterator.get_next()])
+
+
+def _get_attributes_of_image(index):
+  """Gets the attributes of the image.
+
+  Args:
+    index: Index of image in all images.
+
+  Returns:
+    Attributes of the image in the format of ImageAttributes.
+
+  Raises:
+    ValueError: If index is of wrong value.
+  """
+  if index == 0:
+    return ImageAttributes(
+        image=IMAGE_1,
+        label=LABEL_1,
+        height=366,
+        width=500,
+        image_name='2007_000033')
+  elif index == 1:
+    return ImageAttributes(
+        image=IMAGE_2,
+        label=LABEL_2,
+        height=335,
+        width=500,
+        image_name='2007_000042')
+  elif index == 2:
+    return ImageAttributes(
+        image=IMAGE_3,
+        label=LABEL_3,
+        height=333,
+        width=500,
+        image_name='2007_000061')
+  else:
+    raise ValueError('Index can only be 0, 1 or 2.')
+
+
+IMAGE_1 = (
+    (
+        (57., 41., 18.),
+        (151.5, 138., 111.5),
+        (107., 158., 143.),
+    ),
+    (
+        (104.5, 141., 191.),
+        (101.75, 72.5, 120.75),
+        (86.5, 139.5, 120.),
+    ),
+    (
+        (96., 85., 145.),
+        (123.5, 107.5, 97.),
+        (61., 148., 116.),
+    ),
+)
+
+LABEL_1 = (
+    (
+        (70,),
+        (227,),
+        (251,),
+    ),
+    (
+        (101,),
+        (0,),
+        (10,),
+    ),
+    (
+        (145,),
+        (245,),
+        (146,),
+    ),
+)
+
+IMAGE_2 = (
+    (
+        (94., 64., 98.),
+        (145.5, 136.5, 134.5),
+        (108., 162., 172.),
+    ),
+    (
+        (168., 157., 213.),
+        (161.5, 154.5, 148.),
+        (25., 46., 93.),
+    ),
+    (
+        (255., 204., 237.),
+        (124., 102., 126.5),
+        (155., 181., 82.),
+    ),
+)
+
+LABEL_2 = (
+    (
+        (44,),
+        (146,),
+        (121,),
+    ),
+    (
+        (108,),
+        (118,),
+        (6,),
+    ),
+    (
+        (246,),
+        (121,),
+        (108,),
+    ),
+)
+
+IMAGE_3 = (
+    (
+        (235., 173., 150.),
+        (145.5, 83.5, 102.),
+        (82., 149., 158.),
+    ),
+    (
+        (130., 95., 14.),
+        (132.5, 141.5, 93.),
+        (119., 85., 86.),
+    ),
+    (
+        (127.5, 127.5, 127.5),
+        (127.5, 127.5, 127.5),
+        (127.5, 127.5, 127.5),
+    ),
+)
+
+LABEL_3 = (
+    (
+        (91,),
+        (120,),
+        (132,),
+    ),
+    (
+        (135,),
+        (139,),
+        (72,),
+    ),
+    (
+        (255,),
+        (255,),
+        (255,),
+    ),
+)
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/deeplab/deprecated/__init__.py b/research/deeplab/deprecated/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/deeplab/datasets/segmentation_dataset.py b/research/deeplab/deprecated/segmentation_dataset.py
similarity index 100%
rename from research/deeplab/datasets/segmentation_dataset.py
rename to research/deeplab/deprecated/segmentation_dataset.py
diff --git a/research/deeplab/eval.py b/research/deeplab/eval.py
index 600bb5f7..6fdbea4e 100644
--- a/research/deeplab/eval.py
+++ b/research/deeplab/eval.py
@@ -17,18 +17,12 @@
 See model.py for more details and usage.
 """
 
-import math
-import six
 import tensorflow as tf
 from deeplab import common
 from deeplab import model
-from deeplab.datasets import segmentation_dataset
-from deeplab.utils import input_generator
-
-slim = tf.contrib.slim
+from deeplab.datasets import data_generator
 
 flags = tf.app.flags
-
 FLAGS = flags.FLAGS
 
 flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')
@@ -84,31 +78,40 @@ flags.DEFINE_integer('max_number_of_evaluations', 0,
 
 def main(unused_argv):
   tf.logging.set_verbosity(tf.logging.INFO)
-  # Get dataset-dependent information.
-  dataset = segmentation_dataset.get_dataset(
-      FLAGS.dataset, FLAGS.eval_split, dataset_dir=FLAGS.dataset_dir)
+
+  dataset = data_generator.Dataset(
+      dataset_name=FLAGS.dataset,
+      split_name=FLAGS.eval_split,
+      dataset_dir=FLAGS.dataset_dir,
+      batch_size=FLAGS.eval_batch_size,
+      crop_size=FLAGS.eval_crop_size,
+      min_resize_value=FLAGS.min_resize_value,
+      max_resize_value=FLAGS.max_resize_value,
+      resize_factor=FLAGS.resize_factor,
+      model_variant=FLAGS.model_variant,
+      num_readers=2,
+      is_training=False,
+      should_shuffle=False,
+      should_repeat=False)
 
   tf.gfile.MakeDirs(FLAGS.eval_logdir)
   tf.logging.info('Evaluating on %s set', FLAGS.eval_split)
 
   with tf.Graph().as_default():
-    samples = input_generator.get(
-        dataset,
-        FLAGS.eval_crop_size,
-        FLAGS.eval_batch_size,
-        min_resize_value=FLAGS.min_resize_value,
-        max_resize_value=FLAGS.max_resize_value,
-        resize_factor=FLAGS.resize_factor,
-        dataset_split=FLAGS.eval_split,
-        is_training=False,
-        model_variant=FLAGS.model_variant)
+    samples = dataset.get_one_shot_iterator().get_next()
 
     model_options = common.ModelOptions(
-        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},
+        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_of_classes},
         crop_size=FLAGS.eval_crop_size,
         atrous_rates=FLAGS.atrous_rates,
         output_stride=FLAGS.output_stride)
 
+    # Set shape in order for tf.contrib.tfprof.model_analyzer to work properly.
+    samples[common.IMAGE].set_shape(
+        [FLAGS.eval_batch_size,
+         FLAGS.eval_crop_size[0],
+         FLAGS.eval_crop_size[1],
+         3])
     if tuple(FLAGS.eval_scales) == (1.0,):
       tf.logging.info('Performing single-scale test.')
       predictions = model.predict_labels(samples[common.IMAGE], model_options,
@@ -138,34 +141,32 @@ def main(unused_argv):
       predictions_tag += '_flipped'
 
     # Define the evaluation metric.
-    metric_map = {}
-    metric_map[predictions_tag] = tf.metrics.mean_iou(
-        predictions, labels, dataset.num_classes, weights=weights)
-
-    metrics_to_values, metrics_to_updates = (
-        tf.contrib.metrics.aggregate_metric_map(metric_map))
+    miou, update_op = tf.metrics.mean_iou(
+        predictions, labels, dataset.num_of_classes, weights=weights)
+    tf.summary.scalar(predictions_tag, miou)
 
-    for metric_name, metric_value in six.iteritems(metrics_to_values):
-      slim.summaries.add_scalar_summary(
-          metric_value, metric_name, print_summary=True)
-
-    num_batches = int(
-        math.ceil(dataset.num_samples / float(FLAGS.eval_batch_size)))
-
-    tf.logging.info('Eval num images %d', dataset.num_samples)
-    tf.logging.info('Eval batch size %d and num batch %d',
-                    FLAGS.eval_batch_size, num_batches)
+    summary_op = tf.summary.merge_all()
+    summary_hook = tf.contrib.training.SummaryAtEndHook(
+        log_dir=FLAGS.eval_logdir, summary_op=summary_op)
+    hooks = [summary_hook]
 
     num_eval_iters = None
     if FLAGS.max_number_of_evaluations > 0:
       num_eval_iters = FLAGS.max_number_of_evaluations
-    slim.evaluation.evaluation_loop(
+
+    tf.contrib.tfprof.model_analyzer.print_model_analysis(
+        tf.get_default_graph(),
+        tfprof_options=tf.contrib.tfprof.model_analyzer.
+        TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
+    tf.contrib.tfprof.model_analyzer.print_model_analysis(
+        tf.get_default_graph(),
+        tfprof_options=tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS)
+    tf.contrib.training.evaluate_repeatedly(
         master=FLAGS.master,
         checkpoint_dir=FLAGS.checkpoint_dir,
-        logdir=FLAGS.eval_logdir,
-        num_evals=num_batches,
-        eval_op=list(metrics_to_updates.values()),
+        eval_ops=[update_op],
         max_number_of_evaluations=num_eval_iters,
+        hooks=hooks,
         eval_interval_secs=FLAGS.eval_interval_secs)
 
 
diff --git a/research/deeplab/export_model.py b/research/deeplab/export_model.py
index c2ad6a61..ec2e0f6a 100644
--- a/research/deeplab/export_model.py
+++ b/research/deeplab/export_model.py
@@ -53,11 +53,15 @@ flags.DEFINE_multi_float('inference_scales', [1.0],
 flags.DEFINE_bool('add_flipped_images', False,
                   'Add flipped images during inference or not.')
 
+flags.DEFINE_bool('save_inference_graph', False,
+                  'Save inference graph in text proto.')
+
 # Input name of the exported model.
 _INPUT_NAME = 'ImageTensor'
 
 # Output name of the exported model.
 _OUTPUT_NAME = 'SemanticPredictions'
+_RAW_OUTPUT_NAME = 'RawSemanticPredictions'
 
 
 def _create_input_tensors():
@@ -128,8 +132,10 @@ def main(unused_argv):
 
     predictions = tf.cast(predictions[common.OUTPUT_TYPE], tf.float32)
     # Crop the valid regions from the predictions.
+    raw_predictions = tf.identity(
+        predictions[common.OUTPUT_TYPE], _RAW_OUTPUT_NAME)
     semantic_predictions = tf.slice(
-        predictions,
+        raw_predictions,
         [0, 0, 0],
         [1, resized_image_size[0], resized_image_size[1]])
     # Resize back the prediction to the original image size.
@@ -147,9 +153,11 @@ def main(unused_argv):
 
     saver = tf.train.Saver(tf.model_variables())
 
-    tf.gfile.MakeDirs(os.path.dirname(FLAGS.export_path))
+    dirname = os.path.dirname(FLAGS.export_path)
+    tf.gfile.MakeDirs(dirname)
+    graph_def = tf.get_default_graph().as_graph_def(add_shapes=True)
     freeze_graph.freeze_graph_with_def_protos(
-        tf.get_default_graph().as_graph_def(add_shapes=True),
+        graph_def,
         saver.as_saver_def(),
         FLAGS.checkpoint_path,
         _OUTPUT_NAME,
@@ -159,6 +167,9 @@ def main(unused_argv):
         clear_devices=True,
         initializer_nodes=None)
 
+    if FLAGS.save_inference_graph:
+      tf.train.write_graph(graph_def, dirname, 'inference_graph.pbtxt')
+
 
 if __name__ == '__main__':
   flags.mark_flag_as_required('checkpoint_path')
diff --git a/research/deeplab/g3doc/cityscapes.md b/research/deeplab/g3doc/cityscapes.md
index 8e897031..6224633b 100644
--- a/research/deeplab/g3doc/cityscapes.md
+++ b/research/deeplab/g3doc/cityscapes.md
@@ -79,12 +79,14 @@ ${PATH_TO_DATASET} is the directory in which the Cityscapes dataset resides.
 3.  The users could skip the flag, `decoder_output_stride`, if you do not want
     to use the decoder structure.
 
-4.  Change and add the following flags in order to use the provided dense prediction cell.
+4.  Change and add the following flags in order to use the provided dense
+    prediction cell. Note we need to set decoder_output_stride if you want to
+    use the provided checkpoints which include the decoder module.
 
 ```bash
 --model_variant="xception_71"
 --dense_prediction_cell_json="deeplab/core/dense_prediction_cell_branch5_top1_cityscapes.json"
-
+--decoder_output_stride=4
 ```
 
 A local evaluation job using `xception_65` can be run with the following
diff --git a/research/deeplab/g3doc/faq.md b/research/deeplab/g3doc/faq.md
index c8162497..b2c93105 100644
--- a/research/deeplab/g3doc/faq.md
+++ b/research/deeplab/g3doc/faq.md
@@ -66,6 +66,11 @@ A: Our model uses whole-image inference, meaning that we need to set `eval_crop_
 image dimension in the dataset. For example, we have `eval_crop_size` = 513x513 for PASCAL dataset whose largest image dimension is 512. Similarly, we set `eval_crop_size` = 1025x2049 for Cityscapes images whose
 image dimension is all equal to 1024x2048.
 ___
+Q9: Why multi-gpu training is slow?
+
+A: Please try to use more threads to pre-process the inputs. For, example change [num_readers = 4](https://github.com/tensorflow/models/blob/master/research/deeplab/utils/input_generator.py#L71) and [num_threads = 4](https://github.com/tensorflow/models/blob/master/research/deeplab/utils/input_generator.py#L72).
+___
+
 
 ## References
 
diff --git a/research/deeplab/g3doc/installation.md b/research/deeplab/g3doc/installation.md
index 481015e9..db809592 100644
--- a/research/deeplab/g3doc/installation.md
+++ b/research/deeplab/g3doc/installation.md
@@ -32,13 +32,13 @@ sudo pip install matplotlib
 
 ## Add Libraries to PYTHONPATH
 
-When running locally, the tensorflow/models/research/ and slim directories
-should be appended to PYTHONPATH. This can be done by running the following from
+When running locally, the tensorflow/models/research/ directory should be
+appended to PYTHONPATH. This can be done by running the following from
 tensorflow/models/research/:
 
 ```bash
 # From tensorflow/models/research/
-export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim
+export PYTHONPATH=$PYTHONPATH:`pwd`
 ```
 
 Note: This command needs to run from every new terminal you start. If you wish
diff --git a/research/deeplab/g3doc/model_zoo.md b/research/deeplab/g3doc/model_zoo.md
index 12e026dc..54c45cc5 100644
--- a/research/deeplab/g3doc/model_zoo.md
+++ b/research/deeplab/g3doc/model_zoo.md
@@ -88,13 +88,18 @@ We provide some checkpoints that have been pretrained on ADE20K training set.
 Note that the model has only been pretrained on ImageNet, following the
 dataset rule.
 
-Checkpoint name                       | Network backbone | Pretrained dataset                      | ASPP                                             | Decoder
-------------------------------------- | :--------------: | :-------------------------------------: | :----------------------------------------------: | :-----:
-xception65_ade20k_train                 | Xception_65      | ImageNet <br> ADE20K training set       | [6, 12, 18] for OS=16 <br> [12, 24, 36] for OS=8 | OS = 4
+Checkpoint name                       | Network backbone | Pretrained dataset                      | ASPP                                             | Decoder | Input size
+------------------------------------- | :--------------: | :-------------------------------------: | :----------------------------------------------: | :-----: | :-----:
+mobilenetv2_ade20k_train              | MobileNet-v2     | ImageNet <br> ADE20K training set       | N/A                                              | OS = 4  | 257x257
+xception65_ade20k_train               | Xception_65      | ImageNet <br> ADE20K training set       | [6, 12, 18] for OS=16 <br> [12, 24, 36] for OS=8 | OS = 4  | 513x513
+
+The input dimensions of ADE20K have a huge amount of variation. We resize inputs so that the longest size is 257 for MobileNet-v2 (faster inference) and 513 for Xception_65 (better performation). Note that we also include the decoder module in the MobileNet-v2 checkpoint.
 
 Checkpoint name                       | Eval OS   | Eval scales                 | Left-right Flip |  mIOU                 | Pixel-wise Accuracy | File Size
 ------------------------------------- | :-------: | :-------------------------: | :-------------: | :-------------------: | :-------------------: | :-------:
-[xception65_ade20k_train](http://download.tensorflow.org/models/deeplabv3_xception_ade20k_train_2018_05_29.tar.gz)              | 8 | [0.5:0.25:1.75] | Yes     | 45.65% (val) | 82.52% (val) | 439MB
+[mobilenetv2_ade20k_train](http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz)           | 16 | [1.0] | No     | 32.04% (val) | 75.41% (val) | 24.8MB
+[xception65_ade20k_train](http://download.tensorflow.org/models/deeplabv3_xception_ade20k_train_2018_05_29.tar.gz)        | 8 | [0.5:0.25:1.75] | Yes     | 45.65% (val) | 82.52% (val) | 439MB
+
 
 ## Checkpoints pretrained on ImageNet
 
diff --git a/research/deeplab/input_preprocess.py b/research/deeplab/input_preprocess.py
index 135e24cf..84dc1a80 100644
--- a/research/deeplab/input_preprocess.py
+++ b/research/deeplab/input_preprocess.py
@@ -82,7 +82,7 @@ def preprocess_image_and_label(image,
     label = tf.cast(label, tf.int32)
 
   # Resize image and label to the desired range.
-  if min_resize_value is not None or max_resize_value is not None:
+  if min_resize_value or max_resize_value:
     [processed_image, label] = (
         preprocess_utils.resize_to_range(
             image=processed_image,
diff --git a/research/deeplab/model.py b/research/deeplab/model.py
index 8a687395..64d9e19e 100644
--- a/research/deeplab/model.py
+++ b/research/deeplab/model.py
@@ -56,7 +56,6 @@ from deeplab.core import dense_prediction_cell
 from deeplab.core import feature_extractor
 from deeplab.core import utils
 
-
 slim = tf.contrib.slim
 
 LOGITS_SCOPE_NAME = 'logits'
@@ -67,9 +66,11 @@ CONCAT_PROJECTION_SCOPE = 'concat_projection'
 DECODER_SCOPE = 'decoder'
 META_ARCHITECTURE_SCOPE = 'meta_architecture'
 
+_resize_bilinear = utils.resize_bilinear
 scale_dimension = utils.scale_dimension
 split_separable_conv2d = utils.split_separable_conv2d
 
+
 def get_extra_layer_scopes(last_layers_contain_logits_only=False):
   """Gets the scopes for extra layers.
 
@@ -135,20 +136,20 @@ def predict_labels_multi_scale(images,
 
     for output in sorted(outputs_to_scales_to_logits):
       scales_to_logits = outputs_to_scales_to_logits[output]
-      logits = tf.image.resize_bilinear(
+      logits = _resize_bilinear(
           scales_to_logits[MERGED_LOGITS_SCOPE],
           tf.shape(images)[1:3],
-          align_corners=True)
+          scales_to_logits[MERGED_LOGITS_SCOPE].dtype)
       outputs_to_predictions[output].append(
           tf.expand_dims(tf.nn.softmax(logits), 4))
 
       if add_flipped_images:
         scales_to_logits_reversed = (
             outputs_to_scales_to_logits_reversed[output])
-        logits_reversed = tf.image.resize_bilinear(
+        logits_reversed = _resize_bilinear(
             tf.reverse_v2(scales_to_logits_reversed[MERGED_LOGITS_SCOPE], [2]),
             tf.shape(images)[1:3],
-            align_corners=True)
+            scales_to_logits_reversed[MERGED_LOGITS_SCOPE].dtype)
         outputs_to_predictions[output].append(
             tf.expand_dims(tf.nn.softmax(logits_reversed), 4))
 
@@ -184,37 +185,35 @@ def predict_labels(images, model_options, image_pyramid=None):
   predictions = {}
   for output in sorted(outputs_to_scales_to_logits):
     scales_to_logits = outputs_to_scales_to_logits[output]
-    logits = tf.image.resize_bilinear(
-        scales_to_logits[MERGED_LOGITS_SCOPE],
-        tf.shape(images)[1:3],
-        align_corners=True)
-    predictions[output] = tf.argmax(logits, 3)
+    logits = scales_to_logits[MERGED_LOGITS_SCOPE]
+    # There are two ways to obtain the final prediction results: (1) bilinear
+    # upsampling the logits followed by argmax, or (2) argmax followed by
+    # nearest neighbor upsampling. The second option may introduce the "blocking
+    # effect" but is computationally efficient.
+    if model_options.prediction_with_upsampled_logits:
+      logits = _resize_bilinear(logits,
+                                tf.shape(images)[1:3],
+                                scales_to_logits[MERGED_LOGITS_SCOPE].dtype)
+      predictions[output] = tf.argmax(logits, 3)
+    else:
+      argmax_results = tf.argmax(logits, 3)
+      argmax_results = tf.image.resize_nearest_neighbor(
+          tf.expand_dims(argmax_results, 3),
+          tf.shape(images)[1:3],
+          align_corners=True,
+          name='resize_prediction')
+      predictions[output] = tf.squeeze(argmax_results, 3)
 
   return predictions
 
 
-def _resize_bilinear(images, size, output_dtype=tf.float32):
-  """Returns resized images as output_type.
-
-  Args:
-    images: A tensor of size [batch, height_in, width_in, channels].
-    size: A 1-D int32 Tensor of 2 elements: new_height, new_width. The new size
-      for the images.
-    output_dtype: The destination type.
-  Returns:
-    A tensor of size [batch, height_out, width_out, channels] as a dtype of
-      output_dtype.
-  """
-  images = tf.image.resize_bilinear(images, size, align_corners=True)
-  return tf.cast(images, dtype=output_dtype)
-
-
 def multi_scale_logits(images,
                        model_options,
                        image_pyramid,
                        weight_decay=0.0001,
                        is_training=False,
-                       fine_tune_batch_norm=False):
+                       fine_tune_batch_norm=False,
+                       nas_training_hyper_parameters=None):
   """Gets the logits for multi-scale inputs.
 
   The returned logits are all downsampled (due to max-pooling layers)
@@ -227,6 +226,12 @@ def multi_scale_logits(images,
     weight_decay: The weight decay for model variables.
     is_training: Is training or not.
     fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    nas_training_hyper_parameters: A dictionary storing hyper-parameters for
+      training nas models. Its keys are:
+      - `drop_path_keep_prob`: Probability to keep each path in the cell when
+        training.
+      - `total_training_steps`: Total training steps to help drop path
+        probability calculation.
 
   Returns:
     outputs_to_scales_to_logits: A map of maps from output_type (e.g.,
@@ -250,10 +255,15 @@ def multi_scale_logits(images,
   crop_width = (
       model_options.crop_size[1]
       if model_options.crop_size else tf.shape(images)[2])
+  if model_options.image_pooling_crop_size:
+    image_pooling_crop_height = model_options.image_pooling_crop_size[0]
+    image_pooling_crop_width = model_options.image_pooling_crop_size[1]
 
   # Compute the height, width for the output logits.
-  logits_output_stride = (
-      model_options.decoder_output_stride or model_options.output_stride)
+  if model_options.decoder_output_stride:
+    logits_output_stride = min(model_options.decoder_output_stride)
+  else:
+    logits_output_stride = model_options.output_stride
 
   logits_height = scale_dimension(
       crop_height,
@@ -268,33 +278,45 @@ def multi_scale_logits(images,
       for k in model_options.outputs_to_num_classes
   }
 
+  num_channels = images.get_shape().as_list()[-1]
+
   for image_scale in image_pyramid:
     if image_scale != 1.0:
       scaled_height = scale_dimension(crop_height, image_scale)
       scaled_width = scale_dimension(crop_width, image_scale)
       scaled_crop_size = [scaled_height, scaled_width]
-      scaled_images = tf.image.resize_bilinear(
-          images, scaled_crop_size, align_corners=True)
+      scaled_images = _resize_bilinear(images, scaled_crop_size, images.dtype)
       if model_options.crop_size:
-        scaled_images.set_shape([None, scaled_height, scaled_width, 3])
+        scaled_images.set_shape(
+            [None, scaled_height, scaled_width, num_channels])
+      # Adjust image_pooling_crop_size accordingly.
+      scaled_image_pooling_crop_size = None
+      if model_options.image_pooling_crop_size:
+        scaled_image_pooling_crop_size = [
+            scale_dimension(image_pooling_crop_height, image_scale),
+            scale_dimension(image_pooling_crop_width, image_scale)]
     else:
       scaled_crop_size = model_options.crop_size
       scaled_images = images
+      scaled_image_pooling_crop_size = model_options.image_pooling_crop_size
 
-    updated_options = model_options._replace(crop_size=scaled_crop_size)
+    updated_options = model_options._replace(
+        crop_size=scaled_crop_size,
+        image_pooling_crop_size=scaled_image_pooling_crop_size)
     outputs_to_logits = _get_logits(
         scaled_images,
         updated_options,
         weight_decay=weight_decay,
         reuse=tf.AUTO_REUSE,
         is_training=is_training,
-        fine_tune_batch_norm=fine_tune_batch_norm)
+        fine_tune_batch_norm=fine_tune_batch_norm,
+        nas_training_hyper_parameters=nas_training_hyper_parameters)
 
     # Resize the logits to have the same dimension before merging.
     for output in sorted(outputs_to_logits):
-      outputs_to_logits[output] = tf.image.resize_bilinear(
+      outputs_to_logits[output] = _resize_bilinear(
           outputs_to_logits[output], [logits_height, logits_width],
-          align_corners=True)
+          outputs_to_logits[output].dtype)
 
     # Return when only one input scale.
     if len(image_pyramid) == 1:
@@ -330,7 +352,8 @@ def extract_features(images,
                      weight_decay=0.0001,
                      reuse=None,
                      is_training=False,
-                     fine_tune_batch_norm=False):
+                     fine_tune_batch_norm=False,
+                     nas_training_hyper_parameters=None):
   """Extracts features by the particular model_variant.
 
   Args:
@@ -340,6 +363,12 @@ def extract_features(images,
     reuse: Reuse the model variables or not.
     is_training: Is training or not.
     fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    nas_training_hyper_parameters: A dictionary storing hyper-parameters for
+      training nas models. Its keys are:
+      - `drop_path_keep_prob`: Probability to keep each path in the cell when
+        training.
+      - `total_training_steps`: Total training steps to help drop path
+        probability calculation.
 
   Returns:
     concat_logits: A tensor of size [batch, feature_height, feature_width,
@@ -354,10 +383,16 @@ def extract_features(images,
       multi_grid=model_options.multi_grid,
       model_variant=model_options.model_variant,
       depth_multiplier=model_options.depth_multiplier,
+      divisible_by=model_options.divisible_by,
       weight_decay=weight_decay,
       reuse=reuse,
       is_training=is_training,
-      fine_tune_batch_norm=fine_tune_batch_norm)
+      preprocessed_images_dtype=model_options.preprocessed_images_dtype,
+      fine_tune_batch_norm=fine_tune_batch_norm,
+      nas_stem_output_num_conv_filters=(
+          model_options.nas_stem_output_num_conv_filters),
+      nas_training_hyper_parameters=nas_training_hyper_parameters,
+      use_bounded_activation=model_options.use_bounded_activation)
 
   if not model_options.aspp_with_batch_norm:
     return features, end_points
@@ -367,7 +402,7 @@ def extract_features(images,
       dense_prediction_layer = dense_prediction_cell.DensePredictionCell(
           config=model_options.dense_prediction_cell_config,
           hparams={
-            'conv_rate_multiplier': 16 // model_options.output_stride,
+              'conv_rate_multiplier': 16 // model_options.output_stride,
           })
       concat_logits = dense_prediction_layer.build_cell(
           features,
@@ -380,21 +415,24 @@ def extract_features(images,
           fine_tune_batch_norm=fine_tune_batch_norm)
       return concat_logits, end_points
     else:
-      # The following codes employ the DeepLabv3 ASPP module. Note that We
+      # The following codes employ the DeepLabv3 ASPP module. Note that we
       # could express the ASPP module as one particular dense prediction
-      # cell architecture. We do not do so but leave the following codes in
-      # order for backward compatibility.
+      # cell architecture. We do not do so but leave the following codes
+      # for backward compatibility.
       batch_norm_params = {
-        'is_training': is_training and fine_tune_batch_norm,
-        'decay': 0.9997,
-        'epsilon': 1e-5,
-        'scale': True,
+          'is_training': is_training and fine_tune_batch_norm,
+          'decay': 0.9997,
+          'epsilon': 1e-5,
+          'scale': True,
       }
 
+      activation_fn = (
+          tf.nn.relu6 if model_options.use_bounded_activation else tf.nn.relu)
+
       with slim.arg_scope(
           [slim.conv2d, slim.separable_conv2d],
           weights_regularizer=slim.l2_regularizer(weight_decay),
-          activation_fn=tf.nn.relu,
+          activation_fn=activation_fn,
           normalizer_fn=slim.batch_norm,
           padding='SAME',
           stride=1,
@@ -416,7 +454,8 @@ def extract_features(images,
                   image_pooling_crop_size[1],
                   1. / model_options.output_stride)
               image_feature = slim.avg_pool2d(
-                  features, [pool_height, pool_width], [1, 1], padding='VALID')
+                  features, [pool_height, pool_width],
+                  model_options.image_pooling_stride, padding='VALID')
               resize_height = scale_dimension(
                   model_options.crop_size[0],
                   1. / model_options.output_stride)
@@ -483,7 +522,8 @@ def _get_logits(images,
                 weight_decay=0.0001,
                 reuse=None,
                 is_training=False,
-                fine_tune_batch_norm=False):
+                fine_tune_batch_norm=False,
+                nas_training_hyper_parameters=None):
   """Gets the logits by atrous/image spatial pyramid pooling.
 
   Args:
@@ -493,6 +533,12 @@ def _get_logits(images,
     reuse: Reuse the model variables or not.
     is_training: Is training or not.
     fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    nas_training_hyper_parameters: A dictionary storing hyper-parameters for
+      training nas models. Its keys are:
+      - `drop_path_keep_prob`: Probability to keep each path in the cell when
+        training.
+      - `total_training_steps`: Total training steps to help drop path
+        probability calculation.
 
   Returns:
     outputs_to_logits: A map from output_type to logits.
@@ -503,29 +549,22 @@ def _get_logits(images,
       weight_decay=weight_decay,
       reuse=reuse,
       is_training=is_training,
-      fine_tune_batch_norm=fine_tune_batch_norm)
+      fine_tune_batch_norm=fine_tune_batch_norm,
+      nas_training_hyper_parameters=nas_training_hyper_parameters)
 
   if model_options.decoder_output_stride is not None:
-    if model_options.crop_size is None:
-      height = tf.shape(images)[1]
-      width = tf.shape(images)[2]
-    else:
-      height, width = model_options.crop_size
-    decoder_height = scale_dimension(height,
-                                     1.0 / model_options.decoder_output_stride)
-    decoder_width = scale_dimension(width,
-                                    1.0 / model_options.decoder_output_stride)
     features = refine_by_decoder(
         features,
         end_points,
-        decoder_height=decoder_height,
-        decoder_width=decoder_width,
+        crop_size=model_options.crop_size,
+        decoder_output_stride=model_options.decoder_output_stride,
         decoder_use_separable_conv=model_options.decoder_use_separable_conv,
         model_variant=model_options.model_variant,
         weight_decay=weight_decay,
         reuse=reuse,
         is_training=is_training,
-        fine_tune_batch_norm=fine_tune_batch_norm)
+        fine_tune_batch_norm=fine_tune_batch_norm,
+        use_bounded_activation=model_options.use_bounded_activation)
 
   outputs_to_logits = {}
   for output in sorted(model_options.outputs_to_num_classes):
@@ -544,14 +583,15 @@ def _get_logits(images,
 
 def refine_by_decoder(features,
                       end_points,
-                      decoder_height,
-                      decoder_width,
+                      crop_size=None,
+                      decoder_output_stride=None,
                       decoder_use_separable_conv=False,
                       model_variant=None,
                       weight_decay=0.0001,
                       reuse=None,
                       is_training=False,
-                      fine_tune_batch_norm=False):
+                      fine_tune_batch_norm=False,
+                      use_bounded_activation=False):
   """Adds the decoder to obtain sharper segmentation results.
 
   Args:
@@ -559,19 +599,28 @@ def refine_by_decoder(features,
       features_channels].
     end_points: A dictionary from components of the network to the corresponding
       activation.
-    decoder_height: The height of decoder feature maps.
-    decoder_width: The width of decoder feature maps.
+    crop_size: A tuple [crop_height, crop_width] specifying whole patch crop
+      size.
+    decoder_output_stride: A list of integers specifying the output stride of
+      low-level features used in the decoder module.
     decoder_use_separable_conv: Employ separable convolution for decoder or not.
     model_variant: Model variant for feature extraction.
     weight_decay: The weight decay for model variables.
     reuse: Reuse the model variables or not.
     is_training: Is training or not.
     fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    use_bounded_activation: Whether or not to use bounded activations. Bounded
+      activations better lend themselves to quantized inference.
 
   Returns:
     Decoder output with size [batch, decoder_height, decoder_width,
       decoder_channels].
+
+  Raises:
+    ValueError: If crop_size is None.
   """
+  if crop_size is None:
+    raise ValueError('crop_size must be provided when using decoder.')
   batch_norm_params = {
       'is_training': is_training and fine_tune_batch_norm,
       'decay': 0.9997,
@@ -582,25 +631,28 @@ def refine_by_decoder(features,
   with slim.arg_scope(
       [slim.conv2d, slim.separable_conv2d],
       weights_regularizer=slim.l2_regularizer(weight_decay),
-      activation_fn=tf.nn.relu,
+      activation_fn=tf.nn.relu6 if use_bounded_activation else tf.nn.relu,
       normalizer_fn=slim.batch_norm,
       padding='SAME',
       stride=1,
       reuse=reuse):
     with slim.arg_scope([slim.batch_norm], **batch_norm_params):
       with tf.variable_scope(DECODER_SCOPE, DECODER_SCOPE, [features]):
-        feature_list = feature_extractor.networks_to_feature_maps[
-            model_variant][feature_extractor.DECODER_END_POINTS]
-        if feature_list is None:
-          tf.logging.info('Not found any decoder end points.')
-          return features
-        else:
-          decoder_features = features
+        decoder_features = features
+        decoder_stage = 0
+        scope_suffix = ''
+        for output_stride in decoder_output_stride:
+          feature_list = feature_extractor.networks_to_feature_maps[
+              model_variant][
+                  feature_extractor.DECODER_END_POINTS][output_stride]
+          # If only one decoder stage, we do not change the scope name in
+          # order for backward compactibility.
+          if decoder_stage:
+            scope_suffix = '_{}'.format(decoder_stage)
           for i, name in enumerate(feature_list):
             decoder_features_list = [decoder_features]
-
-            # MobileNet variants use different naming convention.
-            if 'mobilenet' in model_variant:
+            # MobileNet and NAS variants use different naming convention.
+            if 'mobilenet' in model_variant or model_variant.startswith('nas'):
               feature_name = name
             else:
               feature_name = '{}/{}'.format(
@@ -610,11 +662,14 @@ def refine_by_decoder(features,
                     end_points[feature_name],
                     48,
                     1,
-                    scope='feature_projection' + str(i)))
+                    scope='feature_projection' + str(i) + scope_suffix))
+            # Determine the output size.
+            decoder_height = scale_dimension(crop_size[0], 1.0 / output_stride)
+            decoder_width = scale_dimension(crop_size[1], 1.0 / output_stride)
             # Resize to decoder_height/decoder_width.
             for j, feature in enumerate(decoder_features_list):
-              decoder_features_list[j] = tf.image.resize_bilinear(
-                  feature, [decoder_height, decoder_width], align_corners=True)
+              decoder_features_list[j] = _resize_bilinear(
+                  feature, [decoder_height, decoder_width], feature.dtype)
               h = (None if isinstance(decoder_height, tf.Tensor)
                    else decoder_height)
               w = (None if isinstance(decoder_width, tf.Tensor)
@@ -627,13 +682,13 @@ def refine_by_decoder(features,
                   filters=decoder_depth,
                   rate=1,
                   weight_decay=weight_decay,
-                  scope='decoder_conv0')
+                  scope='decoder_conv0' + scope_suffix)
               decoder_features = split_separable_conv2d(
                   decoder_features,
                   filters=decoder_depth,
                   rate=1,
                   weight_decay=weight_decay,
-                  scope='decoder_conv1')
+                  scope='decoder_conv1' + scope_suffix)
             else:
               num_convs = 2
               decoder_features = slim.repeat(
@@ -642,8 +697,9 @@ def refine_by_decoder(features,
                   slim.conv2d,
                   decoder_depth,
                   3,
-                  scope='decoder_conv' + str(i))
-          return decoder_features
+                  scope='decoder_conv' + str(i) + scope_suffix)
+          decoder_stage += 1
+        return decoder_features
 
 
 def get_branch_logits(features,
diff --git a/research/deeplab/model_test.py b/research/deeplab/model_test.py
index 50471c27..9e495cbc 100644
--- a/research/deeplab/model_test.py
+++ b/research/deeplab/model_test.py
@@ -87,6 +87,7 @@ class DeeplabModelTest(tf.test.TestCase):
         add_image_level_feature=True,
         aspp_with_batch_norm=True,
         logits_kernel_size=1,
+        decoder_output_stride=[4],
         model_variant='mobilenet_v2')  # Employ MobileNetv2 for fast test.
 
     g = tf.Graph()
@@ -116,16 +117,16 @@ class DeeplabModelTest(tf.test.TestCase):
     outputs_to_num_classes = {'semantic': 2}
     expected_endpoints = ['merged_logits']
     dense_prediction_cell_config = [
-      {'kernel': 3, 'rate': [1, 6], 'op': 'conv', 'input': -1},
-      {'kernel': 3, 'rate': [18, 15], 'op': 'conv', 'input': 0},
+        {'kernel': 3, 'rate': [1, 6], 'op': 'conv', 'input': -1},
+        {'kernel': 3, 'rate': [18, 15], 'op': 'conv', 'input': 0},
     ]
     model_options = common.ModelOptions(
         outputs_to_num_classes,
         crop_size,
         output_stride=16)._replace(
-        aspp_with_batch_norm=True,
-        model_variant='mobilenet_v2',
-        dense_prediction_cell_config=dense_prediction_cell_config)
+            aspp_with_batch_norm=True,
+            model_variant='mobilenet_v2',
+            dense_prediction_cell_config=dense_prediction_cell_config)
     g = tf.Graph()
     with g.as_default():
       with self.test_session(graph=g):
@@ -137,8 +138,8 @@ class DeeplabModelTest(tf.test.TestCase):
             image_pyramid=[1.0])
         for output in outputs_to_num_classes:
           scales_to_model_results = outputs_to_scales_to_model_results[output]
-          self.assertListEqual(scales_to_model_results.keys(),
-                               expected_endpoints)
+          self.assertListEqual(
+              list(scales_to_model_results), expected_endpoints)
           self.assertEqual(len(scales_to_model_results), 1)
 
 
diff --git a/research/deeplab/testing/info.md b/research/deeplab/testing/info.md
new file mode 100644
index 00000000..b84d2adb
--- /dev/null
+++ b/research/deeplab/testing/info.md
@@ -0,0 +1,6 @@
+This directory contains testing data.
+
+# pascal_voc_seg
+This folder contains data specific to pascal_voc_seg dataset. val-00000-of-00001.tfrecord contains
+three randomly generated images with format defined in
+tensorflow/models/research/deeplab/datasets/build_voc2012_data.py.
diff --git a/research/deeplab/testing/pascal_voc_seg/val-00000-of-00001.tfrecord b/research/deeplab/testing/pascal_voc_seg/val-00000-of-00001.tfrecord
new file mode 100644
index 00000000..e81455b2
Binary files /dev/null and b/research/deeplab/testing/pascal_voc_seg/val-00000-of-00001.tfrecord differ
diff --git a/research/deeplab/train.py b/research/deeplab/train.py
index bf362922..d9234a15 100644
--- a/research/deeplab/train.py
+++ b/research/deeplab/train.py
@@ -19,19 +19,13 @@ See model.py for more details and usage.
 
 import six
 import tensorflow as tf
+from tensorflow.python.ops import math_ops
 from deeplab import common
 from deeplab import model
-from deeplab.datasets import segmentation_dataset
-from deeplab.utils import input_generator
+from deeplab.datasets import data_generator
 from deeplab.utils import train_utils
-from deployment import model_deploy
-
-slim = tf.contrib.slim
-
-prefetch_queue = slim.prefetch_queue
 
 flags = tf.app.flags
-
 FLAGS = flags.FLAGS
 
 # Settings for multi-GPUs/multi-replicas training.
@@ -45,9 +39,10 @@ flags.DEFINE_integer('num_replicas', 1, 'Number of worker replicas.')
 flags.DEFINE_integer('startup_delay_steps', 15,
                      'Number of training steps between replicas startup.')
 
-flags.DEFINE_integer('num_ps_tasks', 0,
-                     'The number of parameter servers. If the value is 0, then '
-                     'the parameters are handled locally by the worker.')
+flags.DEFINE_integer(
+    'num_ps_tasks', 0,
+    'The number of parameter servers. If the value is 0, then '
+    'the parameters are handled locally by the worker.')
 
 flags.DEFINE_string('master', '', 'BNS name of the tensorflow server')
 
@@ -67,9 +62,15 @@ flags.DEFINE_integer('save_interval_secs', 1200,
 flags.DEFINE_integer('save_summaries_secs', 600,
                      'How often, in seconds, we compute the summaries.')
 
-flags.DEFINE_boolean('save_summaries_images', False,
-                     'Save sample inputs, labels, and semantic predictions as '
-                     'images to summary.')
+flags.DEFINE_boolean(
+    'save_summaries_images', False,
+    'Save sample inputs, labels, and semantic predictions as '
+    'images to summary.')
+
+# Settings for profiling.
+
+flags.DEFINE_string('profile_logdir', None,
+                    'Where the profile files are stored.')
 
 # Settings for training strategy.
 
@@ -109,13 +110,20 @@ flags.DEFINE_float('weight_decay', 0.00004,
 flags.DEFINE_multi_integer('train_crop_size', [513, 513],
                            'Image crop size [height, width] during training.')
 
-flags.DEFINE_float('last_layer_gradient_multiplier', 1.0,
-                   'The gradient multiplier for last layers, which is used to '
-                   'boost the gradient of last layers if the value > 1.')
+flags.DEFINE_float(
+    'last_layer_gradient_multiplier', 1.0,
+    'The gradient multiplier for last layers, which is used to '
+    'boost the gradient of last layers if the value > 1.')
 
 flags.DEFINE_boolean('upsample_logits', True,
                      'Upsample logits during training.')
 
+# Hyper-parameters for NAS training strategy.
+
+flags.DEFINE_float(
+    'drop_path_keep_prob', 1.0,
+    'Probability to keep each path in the NAS cell when training.')
+
 # Settings for fine-tuning the network.
 
 flags.DEFINE_string('tf_initial_checkpoint', None,
@@ -157,6 +165,22 @@ flags.DEFINE_multi_integer('atrous_rates', None,
 flags.DEFINE_integer('output_stride', 16,
                      'The ratio of input to output spatial resolution.')
 
+# Hard example mining related flags.
+
+flags.DEFINE_integer(
+    'hard_example_mining_step', 0,
+    'The training step in which exact hard example mining kicks off. Note we '
+    'gradually reduce the mining percent to the specified '
+    'top_k_percent_pixels. For example, if hard_example_mining_step=100K and '
+    'top_k_percent_pixels=0.25, then mining percent will gradually reduce from '
+    '100% to 25% until 100K steps after which we only mine top 25% pixels.')
+
+
+flags.DEFINE_float(
+    'top_k_percent_pixels', 1.0,
+    'The top k percent pixels (in terms of the loss values) used to compute '
+    'loss during training. This is useful for hard pixel mining.')
+
 # Dataset settings.
 flags.DEFINE_string('dataset', 'pascal_voc_seg',
                     'Name of the segmentation dataset.')
@@ -167,50 +191,44 @@ flags.DEFINE_string('train_split', 'train',
 flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')
 
 
-def _build_deeplab(inputs_queue, outputs_to_num_classes, ignore_label):
+def _build_deeplab(iterator, outputs_to_num_classes, ignore_label):
   """Builds a clone of DeepLab.
 
   Args:
-    inputs_queue: A prefetch queue for images and labels.
-    outputs_to_num_classes: A map from output type to the number of classes.
-      For example, for the task of semantic segmentation with 21 semantic
-      classes, we would have outputs_to_num_classes['semantic'] = 21.
+    iterator: An iterator of type tf.data.Iterator for images and labels.
+    outputs_to_num_classes: A map from output type to the number of classes. For
+      example, for the task of semantic segmentation with 21 semantic classes,
+      we would have outputs_to_num_classes['semantic'] = 21.
     ignore_label: Ignore label.
-
-  Returns:
-    A map of maps from output_type (e.g., semantic prediction) to a
-      dictionary of multi-scale logits names to logits. For each output_type,
-      the dictionary has keys which correspond to the scales and values which
-      correspond to the logits. For example, if `scales` equals [1.0, 1.5],
-      then the keys would include 'merged_logits', 'logits_1.00' and
-      'logits_1.50'.
   """
-  samples = inputs_queue.dequeue()
+  samples = iterator.get_next()
 
   # Add name to input and label nodes so we can add to summary.
-  samples[common.IMAGE] = tf.identity(
-      samples[common.IMAGE], name=common.IMAGE)
-  samples[common.LABEL] = tf.identity(
-      samples[common.LABEL], name=common.LABEL)
+  samples[common.IMAGE] = tf.identity(samples[common.IMAGE], name=common.IMAGE)
+  samples[common.LABEL] = tf.identity(samples[common.LABEL], name=common.LABEL)
 
   model_options = common.ModelOptions(
       outputs_to_num_classes=outputs_to_num_classes,
       crop_size=FLAGS.train_crop_size,
       atrous_rates=FLAGS.atrous_rates,
       output_stride=FLAGS.output_stride)
+
   outputs_to_scales_to_logits = model.multi_scale_logits(
       samples[common.IMAGE],
       model_options=model_options,
       image_pyramid=FLAGS.image_pyramid,
       weight_decay=FLAGS.weight_decay,
       is_training=True,
-      fine_tune_batch_norm=FLAGS.fine_tune_batch_norm)
+      fine_tune_batch_norm=FLAGS.fine_tune_batch_norm,
+      nas_training_hyper_parameters={
+          'drop_path_keep_prob': FLAGS.drop_path_keep_prob,
+          'total_training_steps': FLAGS.training_number_of_steps,
+      })
 
   # Add name to graph node so we can add to summary.
   output_type_dict = outputs_to_scales_to_logits[common.OUTPUT_TYPE]
   output_type_dict[model.MERGED_LOGITS_SCOPE] = tf.identity(
-      output_type_dict[model.MERGED_LOGITS_SCOPE],
-      name=common.OUTPUT_TYPE)
+      output_type_dict[model.MERGED_LOGITS_SCOPE], name=common.OUTPUT_TYPE)
 
   for output, num_classes in six.iteritems(outputs_to_num_classes):
     train_utils.add_softmax_cross_entropy_loss_for_each_scale(
@@ -220,175 +238,262 @@ def _build_deeplab(inputs_queue, outputs_to_num_classes, ignore_label):
         ignore_label,
         loss_weight=1.0,
         upsample_logits=FLAGS.upsample_logits,
+        hard_example_mining_step=FLAGS.hard_example_mining_step,
+        top_k_percent_pixels=FLAGS.top_k_percent_pixels,
         scope=output)
 
-  return outputs_to_scales_to_logits
+    # Log the summary
+    _log_summaries(samples[common.IMAGE], samples[common.LABEL], num_classes,
+                   output_type_dict[model.MERGED_LOGITS_SCOPE])
 
 
-def main(unused_argv):
-  tf.logging.set_verbosity(tf.logging.INFO)
-  # Set up deployment (i.e., multi-GPUs and/or multi-replicas).
-  config = model_deploy.DeploymentConfig(
-      num_clones=FLAGS.num_clones,
-      clone_on_cpu=FLAGS.clone_on_cpu,
-      replica_id=FLAGS.task,
-      num_replicas=FLAGS.num_replicas,
-      num_ps_tasks=FLAGS.num_ps_tasks)
+def _tower_loss(iterator, num_of_classes, ignore_label, scope, reuse_variable):
+  """Calculates the total loss on a single tower running the deeplab model.
+
+  Args:
+    iterator: An iterator of type tf.data.Iterator for images and labels.
+    num_of_classes: Number of classes for the dataset.
+    ignore_label: Ignore label for the dataset.
+    scope: Unique prefix string identifying the deeplab tower.
+    reuse_variable: If the variable should be reused.
+
+  Returns:
+     The total loss for a batch of data.
+  """
+  with tf.variable_scope(
+      tf.get_variable_scope(), reuse=True if reuse_variable else None):
+    _build_deeplab(iterator, {common.OUTPUT_TYPE: num_of_classes}, ignore_label)
 
-  # Split the batch across GPUs.
-  assert FLAGS.train_batch_size % config.num_clones == 0, (
-      'Training batch size not divisble by number of clones (GPUs).')
+  losses = tf.losses.get_losses(scope=scope)
+  for loss in losses:
+    tf.summary.scalar('Losses/%s' % loss.op.name, loss)
 
-  clone_batch_size = FLAGS.train_batch_size // config.num_clones
+  regularization_loss = tf.losses.get_regularization_loss(scope=scope)
+  tf.summary.scalar('Losses/%s' % regularization_loss.op.name,
+                    regularization_loss)
 
-  # Get dataset-dependent information.
-  dataset = segmentation_dataset.get_dataset(
-      FLAGS.dataset, FLAGS.train_split, dataset_dir=FLAGS.dataset_dir)
+  total_loss = tf.add_n([tf.add_n(losses), regularization_loss])
+  return total_loss
+
+
+def _average_gradients(tower_grads):
+  """Calculates average of gradient for each shared variable across all towers.
+
+  Note that this function provides a synchronization point across all towers.
+
+  Args:
+    tower_grads: List of lists of (gradient, variable) tuples. The outer list is
+      over individual gradients. The inner list is over the gradient calculation
+      for each tower.
+
+  Returns:
+     List of pairs of (gradient, variable) where the gradient has been summed
+       across all towers.
+  """
+  average_grads = []
+  for grad_and_vars in zip(*tower_grads):
+    # Note that each grad_and_vars looks like the following:
+    #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))
+    grads, variables = zip(*grad_and_vars)
+    grad = tf.reduce_mean(tf.stack(grads, axis=0), axis=0)
+
+    # All vars are of the same value, using the first tower here.
+    average_grads.append((grad, variables[0]))
+
+  return average_grads
+
+
+def _log_summaries(input_image, label, num_of_classes, output):
+  """Logs the summaries for the model.
+
+  Args:
+    input_image: Input image of the model. Its shape is [batch_size, height,
+      width, channel].
+    label: Label of the image. Its shape is [batch_size, height, width].
+    num_of_classes: The number of classes of the dataset.
+    output: Output of the model. Its shape is [batch_size, height, width].
+  """
+  # Add summaries for model variables.
+  for model_var in tf.model_variables():
+    tf.summary.histogram(model_var.op.name, model_var)
+
+  # Add summaries for images, labels, semantic predictions.
+  if FLAGS.save_summaries_images:
+    tf.summary.image('samples/%s' % common.IMAGE, input_image)
+
+    # Scale up summary image pixel values for better visualization.
+    pixel_scaling = max(1, 255 // num_of_classes)
+    summary_label = tf.cast(label * pixel_scaling, tf.uint8)
+    tf.summary.image('samples/%s' % common.LABEL, summary_label)
+
+    predictions = tf.expand_dims(tf.argmax(output, 3), -1)
+    summary_predictions = tf.cast(predictions * pixel_scaling, tf.uint8)
+    tf.summary.image('samples/%s' % common.OUTPUT_TYPE, summary_predictions)
+
+
+def _train_deeplab_model(iterator, num_of_classes, ignore_label):
+  """Trains the deeplab model.
+
+  Args:
+    iterator: An iterator of type tf.data.Iterator for images and labels.
+    num_of_classes: Number of classes for the dataset.
+    ignore_label: Ignore label for the dataset.
+
+  Returns:
+    train_tensor: A tensor to update the model variables.
+    summary_op: An operation to log the summaries.
+  """
+  global_step = tf.train.get_or_create_global_step()
+  summaries = []
+
+  learning_rate = train_utils.get_model_learning_rate(
+      FLAGS.learning_policy, FLAGS.base_learning_rate,
+      FLAGS.learning_rate_decay_step, FLAGS.learning_rate_decay_factor,
+      FLAGS.training_number_of_steps, FLAGS.learning_power,
+      FLAGS.slow_start_step, FLAGS.slow_start_learning_rate)
+  summaries.append(tf.summary.scalar('learning_rate', learning_rate))
+
+  optimizer = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)
+
+  tower_grads = []
+  tower_summaries = None
+  for i in xrange(FLAGS.num_clones):
+    with tf.device('/gpu:%d' % i):
+      with tf.name_scope('clone_%d' % i) as scope:
+        loss = _tower_loss(
+            iterator=iterator,
+            num_of_classes=num_of_classes,
+            ignore_label=ignore_label,
+            scope=scope,
+            reuse_variable=(i != 0))
+        grads = optimizer.compute_gradients(loss)
+        tower_grads.append(grads)
+
+        # Retain the summaries from the first tower.
+        if not i:
+          tower_summaries = tf.summary.merge_all(scope=scope)
+
+  with tf.device('/cpu:0'):
+    grads_and_vars = _average_gradients(tower_grads)
+    if tower_summaries is not None:
+      summaries.append(tower_summaries)
+
+    # Modify the gradients for biases and last layer variables.
+    last_layers = model.get_extra_layer_scopes(
+        FLAGS.last_layers_contain_logits_only)
+    grad_mult = train_utils.get_model_gradient_multipliers(
+        last_layers, FLAGS.last_layer_gradient_multiplier)
+    if grad_mult:
+      grads_and_vars = tf.contrib.training.multiply_gradients(
+          grads_and_vars, grad_mult)
+
+    # Create gradient update op.
+    grad_updates = optimizer.apply_gradients(
+        grads_and_vars, global_step=global_step)
+
+    # Gather update_ops. These contain, for example,
+    # the updates for the batch_norm variables created by model_fn.
+    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
+    update_ops.append(grad_updates)
+    update_op = tf.group(*update_ops)
+
+    total_loss = tf.losses.get_total_loss(add_regularization_losses=True)
+
+    # Print total loss to the terminal.
+    # This implementation is mirrored from tf.slim.summaries.
+    should_log = math_ops.equal(math_ops.mod(global_step, FLAGS.log_steps), 0)
+    total_loss = tf.cond(
+        should_log,
+        lambda: tf.Print(total_loss, [total_loss], 'Total loss is :'),
+        lambda: total_loss)
+
+    summaries.append(tf.summary.scalar('total_loss', total_loss))
+
+    with tf.control_dependencies([update_op]):
+      train_tensor = tf.identity(total_loss, name='train_op')
+    summary_op = tf.summary.merge(summaries)
+
+  return train_tensor, summary_op
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
 
   tf.gfile.MakeDirs(FLAGS.train_logdir)
   tf.logging.info('Training on %s set', FLAGS.train_split)
 
-  with tf.Graph().as_default() as graph:
-    with tf.device(config.inputs_device()):
-      samples = input_generator.get(
-          dataset,
-          FLAGS.train_crop_size,
-          clone_batch_size,
+  graph = tf.Graph()
+  with graph.as_default():
+    with tf.device(tf.train.replica_device_setter(ps_tasks=FLAGS.num_ps_tasks)):
+      assert FLAGS.train_batch_size % FLAGS.num_clones == 0, (
+          'Training batch size not divisble by number of clones (GPUs).')
+      clone_batch_size = FLAGS.train_batch_size // FLAGS.num_clones
+
+      dataset = data_generator.Dataset(
+          dataset_name=FLAGS.dataset,
+          split_name=FLAGS.train_split,
+          dataset_dir=FLAGS.dataset_dir,
+          batch_size=clone_batch_size,
+          crop_size=FLAGS.train_crop_size,
           min_resize_value=FLAGS.min_resize_value,
           max_resize_value=FLAGS.max_resize_value,
           resize_factor=FLAGS.resize_factor,
           min_scale_factor=FLAGS.min_scale_factor,
           max_scale_factor=FLAGS.max_scale_factor,
           scale_factor_step_size=FLAGS.scale_factor_step_size,
-          dataset_split=FLAGS.train_split,
+          model_variant=FLAGS.model_variant,
+          num_readers=2,
           is_training=True,
-          model_variant=FLAGS.model_variant)
-      inputs_queue = prefetch_queue.prefetch_queue(
-          samples, capacity=128 * config.num_clones)
-
-    # Create the global step on the device storing the variables.
-    with tf.device(config.variables_device()):
-      global_step = tf.train.get_or_create_global_step()
-
-      # Define the model and create clones.
-      model_fn = _build_deeplab
-      model_args = (inputs_queue, {
-          common.OUTPUT_TYPE: dataset.num_classes
-      }, dataset.ignore_label)
-      clones = model_deploy.create_clones(config, model_fn, args=model_args)
-
-      # Gather update_ops from the first clone. These contain, for example,
-      # the updates for the batch_norm variables created by model_fn.
-      first_clone_scope = config.clone_scope(0)
-      update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
-
-    # Gather initial summaries.
-    summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
-
-    # Add summaries for model variables.
-    for model_var in slim.get_model_variables():
-      summaries.add(tf.summary.histogram(model_var.op.name, model_var))
-
-    # Add summaries for images, labels, semantic predictions
-    if FLAGS.save_summaries_images:
-      summary_image = graph.get_tensor_by_name(
-          ('%s/%s:0' % (first_clone_scope, common.IMAGE)).strip('/'))
-      summaries.add(
-          tf.summary.image('samples/%s' % common.IMAGE, summary_image))
-
-      first_clone_label = graph.get_tensor_by_name(
-          ('%s/%s:0' % (first_clone_scope, common.LABEL)).strip('/'))
-      # Scale up summary image pixel values for better visualization.
-      pixel_scaling = max(1, 255 // dataset.num_classes)
-      summary_label = tf.cast(first_clone_label * pixel_scaling, tf.uint8)
-      summaries.add(
-          tf.summary.image('samples/%s' % common.LABEL, summary_label))
-
-      first_clone_output = graph.get_tensor_by_name(
-          ('%s/%s:0' % (first_clone_scope, common.OUTPUT_TYPE)).strip('/'))
-      predictions = tf.expand_dims(tf.argmax(first_clone_output, 3), -1)
-
-      summary_predictions = tf.cast(predictions * pixel_scaling, tf.uint8)
-      summaries.add(
-          tf.summary.image(
-              'samples/%s' % common.OUTPUT_TYPE, summary_predictions))
-
-    # Add summaries for losses.
-    for loss in tf.get_collection(tf.GraphKeys.LOSSES, first_clone_scope):
-      summaries.add(tf.summary.scalar('losses/%s' % loss.op.name, loss))
-
-    # Build the optimizer based on the device specification.
-    with tf.device(config.optimizer_device()):
-      learning_rate = train_utils.get_model_learning_rate(
-          FLAGS.learning_policy, FLAGS.base_learning_rate,
-          FLAGS.learning_rate_decay_step, FLAGS.learning_rate_decay_factor,
-          FLAGS.training_number_of_steps, FLAGS.learning_power,
-          FLAGS.slow_start_step, FLAGS.slow_start_learning_rate)
-      optimizer = tf.train.MomentumOptimizer(learning_rate, FLAGS.momentum)
-      summaries.add(tf.summary.scalar('learning_rate', learning_rate))
-
-    startup_delay_steps = FLAGS.task * FLAGS.startup_delay_steps
-    for variable in slim.get_model_variables():
-      summaries.add(tf.summary.histogram(variable.op.name, variable))
-
-    with tf.device(config.variables_device()):
-      total_loss, grads_and_vars = model_deploy.optimize_clones(
-          clones, optimizer)
-      total_loss = tf.check_numerics(total_loss, 'Loss is inf or nan.')
-      summaries.add(tf.summary.scalar('total_loss', total_loss))
-
-      # Modify the gradients for biases and last layer variables.
+          should_shuffle=True,
+          should_repeat=True)
+
+      train_tensor, summary_op = _train_deeplab_model(
+          dataset.get_one_shot_iterator(), dataset.num_of_classes,
+          dataset.ignore_label)
+
+      # Soft placement allows placing on CPU ops without GPU implementation.
+      session_config = tf.ConfigProto(
+          allow_soft_placement=True, log_device_placement=False)
+
       last_layers = model.get_extra_layer_scopes(
           FLAGS.last_layers_contain_logits_only)
-      grad_mult = train_utils.get_model_gradient_multipliers(
-          last_layers, FLAGS.last_layer_gradient_multiplier)
-      if grad_mult:
-        grads_and_vars = slim.learning.multiply_gradients(
-            grads_and_vars, grad_mult)
-
-      # Create gradient update op.
-      grad_updates = optimizer.apply_gradients(
-          grads_and_vars, global_step=global_step)
-      update_ops.append(grad_updates)
-      update_op = tf.group(*update_ops)
-      with tf.control_dependencies([update_op]):
-        train_tensor = tf.identity(total_loss, name='train_op')
-
-    # Add the summaries from the first clone. These contain the summaries
-    # created by model_fn and either optimize_clones() or _gather_clone_loss().
-    summaries |= set(
-        tf.get_collection(tf.GraphKeys.SUMMARIES, first_clone_scope))
-
-    # Merge all summaries together.
-    summary_op = tf.summary.merge(list(summaries))
-
-    # Soft placement allows placing on CPU ops without GPU implementation.
-    session_config = tf.ConfigProto(
-        allow_soft_placement=True, log_device_placement=False)
-
-    # Start the training.
-    slim.learning.train(
-        train_tensor,
-        logdir=FLAGS.train_logdir,
-        log_every_n_steps=FLAGS.log_steps,
-        master=FLAGS.master,
-        number_of_steps=FLAGS.training_number_of_steps,
-        is_chief=(FLAGS.task == 0),
-        session_config=session_config,
-        startup_delay_steps=startup_delay_steps,
-        init_fn=train_utils.get_model_init_fn(
+      init_fn = None
+      if FLAGS.tf_initial_checkpoint:
+        init_fn = train_utils.get_model_init_fn(
             FLAGS.train_logdir,
             FLAGS.tf_initial_checkpoint,
             FLAGS.initialize_last_layer,
             last_layers,
-            ignore_missing_vars=True),
-        summary_op=summary_op,
-        save_summaries_secs=FLAGS.save_summaries_secs,
-        save_interval_secs=FLAGS.save_interval_secs)
+            ignore_missing_vars=True)
+
+      scaffold = tf.train.Scaffold(
+          init_fn=init_fn,
+          summary_op=summary_op,
+      )
+
+      stop_hook = tf.train.StopAtStepHook(FLAGS.training_number_of_steps)
+
+      profile_dir = FLAGS.profile_logdir
+      if profile_dir is not None:
+        tf.gfile.MakeDirs(profile_dir)
+
+      with tf.contrib.tfprof.ProfileContext(
+          enabled=profile_dir is not None, profile_dir=profile_dir):
+        with tf.train.MonitoredTrainingSession(
+            master=FLAGS.master,
+            is_chief=(FLAGS.task == 0),
+            config=session_config,
+            scaffold=scaffold,
+            checkpoint_dir=FLAGS.train_logdir,
+            log_step_count_steps=FLAGS.log_steps,
+            save_summaries_steps=FLAGS.save_summaries_secs,
+            save_checkpoint_secs=FLAGS.save_interval_secs,
+            hooks=[stop_hook]) as sess:
+          while not sess.should_stop():
+            sess.run([train_tensor])
 
 
 if __name__ == '__main__':
   flags.mark_flag_as_required('train_logdir')
-  flags.mark_flag_as_required('tf_initial_checkpoint')
   flags.mark_flag_as_required('dataset_dir')
   tf.app.run()
diff --git a/research/deeplab/utils/get_dataset_colormap.py b/research/deeplab/utils/get_dataset_colormap.py
index eff9b195..2cea273d 100644
--- a/research/deeplab/utils/get_dataset_colormap.py
+++ b/research/deeplab/utils/get_dataset_colormap.py
@@ -37,7 +37,7 @@ _PASCAL = 'pascal'
 # Max number of entries in the colormap for each dataset.
 _DATASET_MAX_ENTRIES = {
     _ADE20K: 151,
-    _CITYSCAPES: 19,
+    _CITYSCAPES: 256,
     _MAPILLARY_VISTAS: 66,
     _PASCAL: 256,
 }
@@ -210,27 +210,27 @@ def create_cityscapes_label_colormap():
   Returns:
     A colormap for visualizing segmentation results.
   """
-  return np.asarray([
-      [128, 64, 128],
-      [244, 35, 232],
-      [70, 70, 70],
-      [102, 102, 156],
-      [190, 153, 153],
-      [153, 153, 153],
-      [250, 170, 30],
-      [220, 220, 0],
-      [107, 142, 35],
-      [152, 251, 152],
-      [70, 130, 180],
-      [220, 20, 60],
-      [255, 0, 0],
-      [0, 0, 142],
-      [0, 0, 70],
-      [0, 60, 100],
-      [0, 80, 100],
-      [0, 0, 230],
-      [119, 11, 32],
-  ])
+  colormap = np.zeros((256, 3), dtype=np.uint8)
+  colormap[0] = [128, 64, 128]
+  colormap[1] = [244, 35, 232]
+  colormap[2] = [70, 70, 70]
+  colormap[3] = [102, 102, 156]
+  colormap[4] = [190, 153, 153]
+  colormap[5] = [153, 153, 153]
+  colormap[6] = [250, 170, 30]
+  colormap[7] = [220, 220, 0]
+  colormap[8] = [107, 142, 35]
+  colormap[9] = [152, 251, 152]
+  colormap[10] = [70, 130, 180]
+  colormap[11] = [220, 20, 60]
+  colormap[12] = [255, 0, 0]
+  colormap[13] = [0, 0, 142]
+  colormap[14] = [0, 0, 70]
+  colormap[15] = [0, 60, 100]
+  colormap[16] = [0, 80, 100]
+  colormap[17] = [0, 0, 230]
+  colormap[18] = [119, 11, 32]
+  return colormap
 
 
 def create_mapillary_vistas_label_colormap():
@@ -396,10 +396,16 @@ def label_to_color_image(label, dataset=_PASCAL):
       map maximum entry.
   """
   if label.ndim != 2:
-    raise ValueError('Expect 2-D input label')
+    raise ValueError('Expect 2-D input label. Got {}'.format(label.shape))
 
   if np.max(label) >= _DATASET_MAX_ENTRIES[dataset]:
-    raise ValueError('label value too large.')
+    raise ValueError(
+        'label value too large: {} >= {}.'.format(
+            np.max(label), _DATASET_MAX_ENTRIES[dataset]))
 
   colormap = create_label_colormap(dataset)
   return colormap[label]
+
+
+def get_dataset_colormap_max_entries(dataset):
+  return _DATASET_MAX_ENTRIES[dataset]
diff --git a/research/deeplab/utils/input_generator.py b/research/deeplab/utils/input_generator.py
deleted file mode 100644
index 4d4d09a5..00000000
--- a/research/deeplab/utils/input_generator.py
+++ /dev/null
@@ -1,168 +0,0 @@
-# Copyright 2018 The TensorFlow Authors All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
-"""Wrapper for providing semantic segmentation data."""
-
-import tensorflow as tf
-from deeplab import common
-from deeplab import input_preprocess
-
-slim = tf.contrib.slim
-
-dataset_data_provider = slim.dataset_data_provider
-
-
-def _get_data(data_provider, dataset_split):
-  """Gets data from data provider.
-
-  Args:
-    data_provider: An object of slim.data_provider.
-    dataset_split: Dataset split.
-
-  Returns:
-    image: Image Tensor.
-    label: Label Tensor storing segmentation annotations.
-    image_name: Image name.
-    height: Image height.
-    width: Image width.
-
-  Raises:
-    ValueError: Failed to find label.
-  """
-  if common.LABELS_CLASS not in data_provider.list_items():
-    raise ValueError('Failed to find labels.')
-
-  image, height, width = data_provider.get(
-      [common.IMAGE, common.HEIGHT, common.WIDTH])
-
-  # Some datasets do not contain image_name.
-  if common.IMAGE_NAME in data_provider.list_items():
-    image_name, = data_provider.get([common.IMAGE_NAME])
-  else:
-    image_name = tf.constant('')
-
-  label = None
-  if dataset_split != common.TEST_SET:
-    label, = data_provider.get([common.LABELS_CLASS])
-
-  return image, label, image_name, height, width
-
-
-def get(dataset,
-        crop_size,
-        batch_size,
-        min_resize_value=None,
-        max_resize_value=None,
-        resize_factor=None,
-        min_scale_factor=1.,
-        max_scale_factor=1.,
-        scale_factor_step_size=0,
-        num_readers=1,
-        num_threads=1,
-        dataset_split=None,
-        is_training=True,
-        model_variant=None):
-  """Gets the dataset split for semantic segmentation.
-
-  This functions gets the dataset split for semantic segmentation. In
-  particular, it is a wrapper of (1) dataset_data_provider which returns the raw
-  dataset split, (2) input_preprcess which preprocess the raw data, and (3) the
-  Tensorflow operation of batching the preprocessed data. Then, the output could
-  be directly used by training, evaluation or visualization.
-
-  Args:
-    dataset: An instance of slim Dataset.
-    crop_size: Image crop size [height, width].
-    batch_size: Batch size.
-    min_resize_value: Desired size of the smaller image side.
-    max_resize_value: Maximum allowed size of the larger image side.
-    resize_factor: Resized dimensions are multiple of factor plus one.
-    min_scale_factor: Minimum scale factor value.
-    max_scale_factor: Maximum scale factor value.
-    scale_factor_step_size: The step size from min scale factor to max scale
-      factor. The input is randomly scaled based on the value of
-      (min_scale_factor, max_scale_factor, scale_factor_step_size).
-    num_readers: Number of readers for data provider.
-    num_threads: Number of threads for batching data.
-    dataset_split: Dataset split.
-    is_training: Is training or not.
-    model_variant: Model variant (string) for choosing how to mean-subtract the
-      images. See feature_extractor.network_map for supported model variants.
-
-  Returns:
-    A dictionary of batched Tensors for semantic segmentation.
-
-  Raises:
-    ValueError: dataset_split is None, failed to find labels, or label shape
-      is not valid.
-  """
-  if dataset_split is None:
-    raise ValueError('Unknown dataset split.')
-  if model_variant is None:
-    tf.logging.warning('Please specify a model_variant. See '
-                       'feature_extractor.network_map for supported model '
-                       'variants.')
-
-  data_provider = dataset_data_provider.DatasetDataProvider(
-      dataset,
-      num_readers=num_readers,
-      num_epochs=None if is_training else 1,
-      shuffle=is_training)
-  image, label, image_name, height, width = _get_data(data_provider,
-                                                      dataset_split)
-  if label is not None:
-    if label.shape.ndims == 2:
-      label = tf.expand_dims(label, 2)
-    elif label.shape.ndims == 3 and label.shape.dims[2] == 1:
-      pass
-    else:
-      raise ValueError('Input label shape must be [height, width], or '
-                       '[height, width, 1].')
-
-    label.set_shape([None, None, 1])
-  original_image, image, label = input_preprocess.preprocess_image_and_label(
-      image,
-      label,
-      crop_height=crop_size[0],
-      crop_width=crop_size[1],
-      min_resize_value=min_resize_value,
-      max_resize_value=max_resize_value,
-      resize_factor=resize_factor,
-      min_scale_factor=min_scale_factor,
-      max_scale_factor=max_scale_factor,
-      scale_factor_step_size=scale_factor_step_size,
-      ignore_label=dataset.ignore_label,
-      is_training=is_training,
-      model_variant=model_variant)
-  sample = {
-      common.IMAGE: image,
-      common.IMAGE_NAME: image_name,
-      common.HEIGHT: height,
-      common.WIDTH: width
-  }
-  if label is not None:
-    sample[common.LABEL] = label
-
-  if not is_training:
-    # Original image is only used during visualization.
-    sample[common.ORIGINAL_IMAGE] = original_image,
-    num_threads = 1
-
-  return tf.train.batch(
-      sample,
-      batch_size=batch_size,
-      num_threads=num_threads,
-      capacity=32 * batch_size,
-      allow_smaller_final_batch=not is_training,
-      dynamic_pad=True)
diff --git a/research/deeplab/utils/save_annotation.py b/research/deeplab/utils/save_annotation.py
index 9f3c7e78..ea520fcc 100644
--- a/research/deeplab/utils/save_annotation.py
+++ b/research/deeplab/utils/save_annotation.py
@@ -29,16 +29,20 @@ def save_annotation(label,
                     save_dir,
                     filename,
                     add_colormap=True,
+                    normalize_to_unit_values=False,
+                    scale_values=False,
                     colormap_type=get_dataset_colormap.get_pascal_name()):
   """Saves the given label to image on disk.
 
   Args:
     label: The numpy array to be saved. The data will be converted
       to uint8 and saved as png image.
-    save_dir: The directory to which the results will be saved.
-    filename: The image filename.
-    add_colormap: Add color map to the label or not.
-    colormap_type: Colormap type for visualization.
+    save_dir: String, the directory to which the results will be saved.
+    filename: String, the image filename.
+    add_colormap: Boolean, add color map to the label or not.
+    normalize_to_unit_values: Boolean, normalize the input values to [0, 1].
+    scale_values: Boolean, scale the input values to [0, 255] for visualization.
+    colormap_type: String, colormap type for visualization.
   """
   # Add colormap for visualizing the prediction.
   if add_colormap:
@@ -46,6 +50,15 @@ def save_annotation(label,
         label, colormap_type)
   else:
     colored_label = label
+    if normalize_to_unit_values:
+      min_value = np.amin(colored_label)
+      max_value = np.amax(colored_label)
+      range_value = max_value - min_value
+      if range_value != 0:
+        colored_label = (colored_label - min_value) / range_value
+
+    if scale_values:
+      colored_label = 255. * colored_label
 
   pil_image = img.fromarray(colored_label.astype(dtype=np.uint8))
   with tf.gfile.Open('%s/%s.png' % (save_dir, filename), mode='w') as f:
diff --git a/research/deeplab/utils/train_utils.py b/research/deeplab/utils/train_utils.py
index 4eeffb1e..477fdebc 100644
--- a/research/deeplab/utils/train_utils.py
+++ b/research/deeplab/utils/train_utils.py
@@ -19,7 +19,11 @@ import six
 import tensorflow as tf
 from deeplab.core import preprocess_utils
 
-slim = tf.contrib.slim
+
+def _div_maybe_zero(total_loss, num_present):
+  """Normalizes the total loss with the number of present pixels."""
+  return tf.to_float(num_present > 0) * tf.div(total_loss,
+                                               tf.maximum(1e-5, num_present))
 
 
 def add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,
@@ -28,6 +32,8 @@ def add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,
                                                   ignore_label,
                                                   loss_weight=1.0,
                                                   upsample_logits=True,
+                                                  hard_example_mining_step=0,
+                                                  top_k_percent_pixels=1.0,
                                                   scope=None):
   """Adds softmax cross entropy loss for logits of each scale.
 
@@ -39,6 +45,15 @@ def add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,
     ignore_label: Integer, label to ignore.
     loss_weight: Float, loss weight.
     upsample_logits: Boolean, upsample logits or not.
+    hard_example_mining_step: An integer, the training step in which the hard
+      exampling mining kicks off. Note that we gradually reduce the mining
+      percent to the top_k_percent_pixels. For example, if
+      hard_example_mining_step = 100K and top_k_percent_pixels = 0.25, then
+      mining percent will gradually reduce from 100% to 25% until 100K steps
+      after which we only mine top 25% pixels.
+    top_k_percent_pixels: A float, the value lies in [0.0, 1.0]. When its value
+      < 1.0, only compute the loss for the top k percent pixels (e.g., the top
+      20% pixels). This is useful for hard pixel mining.
     scope: String, the scope for the loss.
 
   Raises:
@@ -69,13 +84,48 @@ def add_softmax_cross_entropy_loss_for_each_scale(scales_to_logits,
     scaled_labels = tf.reshape(scaled_labels, shape=[-1])
     not_ignore_mask = tf.to_float(tf.not_equal(scaled_labels,
                                                ignore_label)) * loss_weight
-    one_hot_labels = slim.one_hot_encoding(
+    one_hot_labels = tf.one_hot(
         scaled_labels, num_classes, on_value=1.0, off_value=0.0)
-    tf.losses.softmax_cross_entropy(
-        one_hot_labels,
-        tf.reshape(logits, shape=[-1, num_classes]),
-        weights=not_ignore_mask,
-        scope=loss_scope)
+
+    if top_k_percent_pixels == 1.0:
+      # Compute the loss for all pixels.
+      tf.losses.softmax_cross_entropy(
+          one_hot_labels,
+          tf.reshape(logits, shape=[-1, num_classes]),
+          weights=not_ignore_mask,
+          scope=loss_scope)
+    else:
+      logits = tf.reshape(logits, shape=[-1, num_classes])
+      weights = not_ignore_mask
+      with tf.name_scope(loss_scope, 'softmax_hard_example_mining',
+                         [logits, one_hot_labels, weights]):
+        one_hot_labels = tf.stop_gradient(
+            one_hot_labels, name='labels_stop_gradient')
+        pixel_losses = tf.nn.softmax_cross_entropy_with_logits_v2(
+            labels=one_hot_labels,
+            logits=logits,
+            name='pixel_losses')
+        weighted_pixel_losses = tf.multiply(pixel_losses, weights)
+        num_pixels = tf.to_float(tf.shape(logits)[0])
+        # Compute the top_k_percent pixels based on current training step.
+        if hard_example_mining_step == 0:
+          # Directly focus on the top_k pixels.
+          top_k_pixels = tf.to_int32(top_k_percent_pixels * num_pixels)
+        else:
+          # Gradually reduce the mining percent to top_k_percent_pixels.
+          global_step = tf.to_float(tf.train.get_or_create_global_step())
+          ratio = tf.minimum(1.0, global_step / hard_example_mining_step)
+          top_k_pixels = tf.to_int32(
+              (ratio * top_k_percent_pixels + (1.0 - ratio)) * num_pixels)
+        top_k_losses, _ = tf.nn.top_k(weighted_pixel_losses,
+                                      k=top_k_pixels,
+                                      sorted=True,
+                                      name='top_k_percent_pixels')
+        total_loss = tf.reduce_sum(top_k_losses)
+        num_present = tf.reduce_sum(
+            tf.to_float(tf.not_equal(top_k_losses, 0.0)))
+        loss = _div_maybe_zero(total_loss, num_present)
+        tf.losses.add_loss(loss)
 
 
 def get_model_init_fn(train_logdir,
@@ -110,13 +160,22 @@ def get_model_init_fn(train_logdir,
   if not initialize_last_layer:
     exclude_list.extend(last_layers)
 
-  variables_to_restore = slim.get_variables_to_restore(exclude=exclude_list)
+  variables_to_restore = tf.contrib.framework.get_variables_to_restore(
+      exclude=exclude_list)
 
   if variables_to_restore:
-    return slim.assign_from_checkpoint_fn(
+    init_op, init_feed_dict = tf.contrib.framework.assign_from_checkpoint(
         tf_initial_checkpoint,
         variables_to_restore,
         ignore_missing_vars=ignore_missing_vars)
+    global_step = tf.train.get_or_create_global_step()
+
+    def restore_fn(unused_scaffold, sess):
+      sess.run(init_op, init_feed_dict)
+      sess.run([global_step])
+
+    return restore_fn
+
   return None
 
 
@@ -138,7 +197,7 @@ def get_model_gradient_multipliers(last_layers, last_layer_gradient_multiplier):
   """
   gradient_multipliers = {}
 
-  for var in slim.get_model_variables():
+  for var in tf.model_variables():
     # Double the learning rate for biases.
     if 'biases' in var.op.name:
       gradient_multipliers[var.op.name] = 2.
@@ -155,10 +214,15 @@ def get_model_gradient_multipliers(last_layers, last_layer_gradient_multiplier):
   return gradient_multipliers
 
 
-def get_model_learning_rate(
-    learning_policy, base_learning_rate, learning_rate_decay_step,
-    learning_rate_decay_factor, training_number_of_steps, learning_power,
-    slow_start_step, slow_start_learning_rate):
+def get_model_learning_rate(learning_policy,
+                            base_learning_rate,
+                            learning_rate_decay_step,
+                            learning_rate_decay_factor,
+                            training_number_of_steps,
+                            learning_power,
+                            slow_start_step,
+                            slow_start_learning_rate,
+                            slow_start_burnin_type='none'):
   """Gets model's learning rate.
 
   Computes the model's learning rate for different learning policy.
@@ -181,31 +245,51 @@ def get_model_learning_rate(
     slow_start_step: Training model with small learning rate for the first
       few steps.
     slow_start_learning_rate: The learning rate employed during slow start.
+    slow_start_burnin_type: The burnin type for the slow start stage. Can be
+      `none` which means no burnin or `linear` which means the learning rate
+      increases linearly from slow_start_learning_rate and reaches
+      base_learning_rate after slow_start_steps.
 
   Returns:
     Learning rate for the specified learning policy.
 
   Raises:
-    ValueError: If learning policy is not recognized.
+    ValueError: If learning policy or slow start burnin type is not recognized.
   """
   global_step = tf.train.get_or_create_global_step()
+  adjusted_global_step = global_step
+
+  if slow_start_burnin_type != 'none':
+    adjusted_global_step -= slow_start_step
+
   if learning_policy == 'step':
     learning_rate = tf.train.exponential_decay(
         base_learning_rate,
-        global_step,
+        adjusted_global_step,
         learning_rate_decay_step,
         learning_rate_decay_factor,
         staircase=True)
   elif learning_policy == 'poly':
     learning_rate = tf.train.polynomial_decay(
         base_learning_rate,
-        global_step,
+        adjusted_global_step,
         training_number_of_steps,
         end_learning_rate=0,
         power=learning_power)
   else:
     raise ValueError('Unknown learning policy.')
 
+  adjusted_slow_start_learning_rate = slow_start_learning_rate
+  if slow_start_burnin_type == 'linear':
+    # Do linear burnin. Increase linearly from slow_start_learning_rate and
+    # reach base_learning_rate after (global_step >= slow_start_steps).
+    adjusted_slow_start_learning_rate = (
+        slow_start_learning_rate +
+        (base_learning_rate - slow_start_learning_rate) *
+        tf.to_float(global_step) / slow_start_step)
+  elif slow_start_burnin_type != 'none':
+    raise ValueError('Unknown burnin type.')
+
   # Employ small learning rate at the first few steps for warm start.
-  return tf.where(global_step < slow_start_step, slow_start_learning_rate,
-                  learning_rate)
+  return tf.where(global_step < slow_start_step,
+                  adjusted_slow_start_learning_rate, learning_rate)
diff --git a/research/deeplab/vis.py b/research/deeplab/vis.py
index 9f75d104..e5033bce 100644
--- a/research/deeplab/vis.py
+++ b/research/deeplab/vis.py
@@ -17,19 +17,15 @@
 
 See model.py for more details and usage.
 """
-import math
 import os.path
 import time
 import numpy as np
 import tensorflow as tf
 from deeplab import common
 from deeplab import model
-from deeplab.datasets import segmentation_dataset
-from deeplab.utils import input_generator
+from deeplab.datasets import data_generator
 from deeplab.utils import save_annotation
 
-slim = tf.contrib.slim
-
 flags = tf.app.flags
 
 FLAGS = flags.FLAGS
@@ -186,11 +182,24 @@ def _process_batch(sess, original_images, semantic_predictions, image_names,
 
 def main(unused_argv):
   tf.logging.set_verbosity(tf.logging.INFO)
+
   # Get dataset-dependent information.
-  dataset = segmentation_dataset.get_dataset(
-      FLAGS.dataset, FLAGS.vis_split, dataset_dir=FLAGS.dataset_dir)
+  dataset = data_generator.Dataset(
+      dataset_name=FLAGS.dataset,
+      split_name=FLAGS.vis_split,
+      dataset_dir=FLAGS.dataset_dir,
+      batch_size=FLAGS.vis_batch_size,
+      crop_size=FLAGS.vis_crop_size,
+      min_resize_value=FLAGS.min_resize_value,
+      max_resize_value=FLAGS.max_resize_value,
+      resize_factor=FLAGS.resize_factor,
+      model_variant=FLAGS.model_variant,
+      is_training=False,
+      should_shuffle=False,
+      should_repeat=False)
+
   train_id_to_eval_id = None
-  if dataset.name == segmentation_dataset.get_cityscapes_dataset_name():
+  if dataset.dataset_name == data_generator.get_cityscapes_dataset_name():
     tf.logging.info('Cityscapes requires converting train_id to eval_id.')
     train_id_to_eval_id = _CITYSCAPES_TRAIN_ID_TO_EVAL_ID
 
@@ -204,20 +213,11 @@ def main(unused_argv):
 
   tf.logging.info('Visualizing on %s set', FLAGS.vis_split)
 
-  g = tf.Graph()
-  with g.as_default():
-    samples = input_generator.get(dataset,
-                                  FLAGS.vis_crop_size,
-                                  FLAGS.vis_batch_size,
-                                  min_resize_value=FLAGS.min_resize_value,
-                                  max_resize_value=FLAGS.max_resize_value,
-                                  resize_factor=FLAGS.resize_factor,
-                                  dataset_split=FLAGS.vis_split,
-                                  is_training=False,
-                                  model_variant=FLAGS.model_variant)
+  with tf.Graph().as_default():
+    samples = dataset.get_one_shot_iterator().get_next()
 
     model_options = common.ModelOptions(
-        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_classes},
+        outputs_to_num_classes={common.OUTPUT_TYPE: dataset.num_of_classes},
         crop_size=FLAGS.vis_crop_size,
         atrous_rates=FLAGS.atrous_rates,
         output_stride=FLAGS.output_stride)
@@ -244,7 +244,7 @@ def main(unused_argv):
 
       # Reverse the resizing and padding operations performed in preprocessing.
       # First, we slice the valid regions (i.e., remove padded region) and then
-      # we reisze the predictions back.
+      # we resize the predictions back.
       original_image = tf.squeeze(samples[common.ORIGINAL_IMAGE])
       original_image_shape = tf.shape(original_image)
       predictions = tf.slice(
@@ -259,40 +259,35 @@ def main(unused_argv):
                                  method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
                                  align_corners=True), 3)
 
-    tf.train.get_or_create_global_step()
-    saver = tf.train.Saver(slim.get_variables_to_restore())
-    sv = tf.train.Supervisor(graph=g,
-                             logdir=FLAGS.vis_logdir,
-                             init_op=tf.global_variables_initializer(),
-                             summary_op=None,
-                             summary_writer=None,
-                             global_step=None,
-                             saver=saver)
-    num_batches = int(math.ceil(
-        dataset.num_samples / float(FLAGS.vis_batch_size)))
-    last_checkpoint = None
-
-    # Loop to visualize the results when new checkpoint is created.
-    num_iters = 0
-    while (FLAGS.max_number_of_iterations <= 0 or
-           num_iters < FLAGS.max_number_of_iterations):
-      num_iters += 1
-      last_checkpoint = slim.evaluation.wait_for_new_checkpoint(
-          FLAGS.checkpoint_dir, last_checkpoint)
-      start = time.time()
+    num_iteration = 0
+    max_num_iteration = FLAGS.max_number_of_iterations
+
+    checkpoints_iterator = tf.contrib.training.checkpoints_iterator(
+        FLAGS.checkpoint_dir, min_interval_secs=FLAGS.eval_interval_secs)
+    for checkpoint_path in checkpoints_iterator:
+      if max_num_iteration > 0 and num_iteration > max_num_iteration:
+        break
+      num_iteration += 1
+
       tf.logging.info(
           'Starting visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',
                                                        time.gmtime()))
-      tf.logging.info('Visualizing with model %s', last_checkpoint)
-
-      with sv.managed_session(FLAGS.master,
-                              start_standard_services=False) as sess:
-        sv.start_queue_runners(sess)
-        sv.saver.restore(sess, last_checkpoint)
-
+      tf.logging.info('Visualizing with model %s', checkpoint_path)
+
+      tf.train.get_or_create_global_step()
+
+      scaffold = tf.train.Scaffold(init_op=tf.global_variables_initializer())
+      session_creator = tf.train.ChiefSessionCreator(
+          scaffold=scaffold,
+          master=FLAGS.master,
+          checkpoint_filename_with_path=checkpoint_path)
+      with tf.train.MonitoredSession(
+          session_creator=session_creator, hooks=None) as sess:
+        batch = 0
         image_id_offset = 0
-        for batch in range(num_batches):
-          tf.logging.info('Visualizing batch %d / %d', batch + 1, num_batches)
+
+        while not sess.should_stop():
+          tf.logging.info('Visualizing batch %d', batch + 1)
           _process_batch(sess=sess,
                          original_images=samples[common.ORIGINAL_IMAGE],
                          semantic_predictions=predictions,
@@ -304,14 +299,11 @@ def main(unused_argv):
                          raw_save_dir=raw_save_dir,
                          train_id_to_eval_id=train_id_to_eval_id)
           image_id_offset += FLAGS.vis_batch_size
+          batch += 1
 
       tf.logging.info(
           'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',
                                                        time.gmtime()))
-      time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()
-      if time_to_next_eval > 0:
-        time.sleep(time_to_next_eval)
-
 
 if __name__ == '__main__':
   flags.mark_flag_as_required('checkpoint_dir')
