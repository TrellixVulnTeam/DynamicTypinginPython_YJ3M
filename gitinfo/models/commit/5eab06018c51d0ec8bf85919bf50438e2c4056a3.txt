commit 5eab06018c51d0ec8bf85919bf50438e2c4056a3
Author: andrewghoward <howarda@google.com>
Date:   Tue Jun 13 21:57:48 2017 -0700

    MobileNet V1 commit (#1551)
    
    * MobileNet V1 commit
    
    * updates to README

diff --git a/slim/BUILD b/slim/BUILD
index 77a1ae50..348ca759 100644
--- a/slim/BUILD
+++ b/slim/BUILD
@@ -132,6 +132,7 @@ py_library(
         ":cifarnet",
         ":inception",
         ":lenet",
+        ":mobilenet_v1",
         ":overfeat",
         ":resnet_v1",
         ":resnet_v2",
@@ -269,6 +270,23 @@ py_library(
     srcs = ["nets/lenet.py"],
 )
 
+py_library(
+    name = "mobilenet_v1",
+    srcs = ["nets/mobilenet_v1.py"],
+    srcs_version = "PY2AND3",
+)
+
+py_test(
+    name = "mobilenet_v1_test",
+    size = "large",
+    srcs = ["nets/mobilenet_v1_test.py"],
+    shard_count = 3,
+    srcs_version = "PY2AND3",
+    deps = [
+        ":mobilenet_v1",
+    ],
+)
+
 py_library(
     name = "overfeat",
     srcs = ["nets/overfeat.py"],
diff --git a/slim/README.md b/slim/README.md
index 85275e8d..628931f7 100644
--- a/slim/README.md
+++ b/slim/README.md
@@ -194,21 +194,24 @@ Model | TF-Slim File | Checkpoint | Top-1 Accuracy| Top-5 Accuracy |
 [Inception V2](http://arxiv.org/abs/1502.03167)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/inception_v2.py)|[inception_v2_2016_08_28.tar.gz](http://download.tensorflow.org/models/inception_v2_2016_08_28.tar.gz)|73.9|91.8|
 [Inception V3](http://arxiv.org/abs/1512.00567)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/inception_v3.py)|[inception_v3_2016_08_28.tar.gz](http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)|78.0|93.9|
 [Inception V4](http://arxiv.org/abs/1602.07261)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/inception_v4.py)|[inception_v4_2016_09_09.tar.gz](http://download.tensorflow.org/models/inception_v4_2016_09_09.tar.gz)|80.2|95.2|
-[Inception-ResNet-v2](http://arxiv.org/abs/1602.07261)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py)|[inception_resnet_v2.tar.gz](http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz)|80.4|95.3|
-[ResNet V1 50](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_50.tar.gz](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz)|75.2|92.2|
-[ResNet V1 101](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_101.tar.gz](http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz)|76.4|92.9|
-[ResNet V1 152](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_152.tar.gz](http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz)|76.8|93.2|
-[ResNet V2 50](https://arxiv.org/abs/1603.05027)^|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py)|[resnet_v2_50.tar.gz](http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz)|75.6|92.8|
-[ResNet V2 101](https://arxiv.org/abs/1603.05027)^|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py)|[resnet_v2_101.tar.gz](http://download.tensorflow.org/models/resnet_v2_101_2017_04_14.tar.gz)|77.0|93.7|
-[ResNet V2 152](https://arxiv.org/abs/1603.05027)^|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py)|[resnet_v2_152.tar.gz](http://download.tensorflow.org/models/resnet_v2_152_2017_04_14.tar.gz)|77.8|94.1|
-[VGG 16](http://arxiv.org/abs/1409.1556.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py)|[vgg_16.tar.gz](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz)|71.5|89.8|
-[VGG 19](http://arxiv.org/abs/1409.1556.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py)|[vgg_19.tar.gz](http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz)|71.1|89.8|
-
+[Inception-ResNet-v2](http://arxiv.org/abs/1602.07261)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py)|[inception_resnet_v2_2016_08_30.tar.gz](http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz)|80.4|95.3|
+[ResNet 50](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_50_2016_08_28.tar.gz](http://download.tensorflow.org/models/resnet_v1_50_2016_08_28.tar.gz)|75.2|92.2|
+[ResNet 101](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_101_2016_08_28.tar.gz](http://download.tensorflow.org/models/resnet_v1_101_2016_08_28.tar.gz)|76.4|92.9|
+[ResNet 152](https://arxiv.org/abs/1512.03385)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v1.py)|[resnet_v1_152_2016_08_28.tar.gz](http://download.tensorflow.org/models/resnet_v1_152_2016_08_28.tar.gz)|76.8|93.2|
+[ResNet V2 200](https://arxiv.org/abs/1603.05027)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/resnet_v2.py)|[TBA]()|79.9\*|95.2\*|
+[VGG 16](http://arxiv.org/abs/1409.1556.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py)|[vgg_16_2016_08_28.tar.gz](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz)|71.5|89.8|
+[VGG 19](http://arxiv.org/abs/1409.1556.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/vgg.py)|[vgg_19_2016_08_28.tar.gz](http://download.tensorflow.org/models/vgg_19_2016_08_28.tar.gz)|71.1|89.8|
+[MobileNet_v1_1.0_224](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[mobilenet_v1_1.0_224_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)|70.7|89.5|
+[MobileNet_v1_0.50_160](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[mobilenet_v1_0.50_160_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.50_160_2017_06_14.tar.gz)|59.9|82.5|
+[MobileNet_v1_0.25_128](https://arxiv.org/pdf/1704.04861.pdf)|[Code](https://github.com/tensorflow/models/blob/master/slim/nets/mobilenet_v1.py)|[mobilenet_v1_0.25_128_2017_06_14.tar.gz](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_2017_06_14.tar.gz)|41.3|66.2|
 ^ ResNet V2 models use Inception pre-processing and input image size of 299 (use
 `--preprocessing_name inception --eval_image_size 299` when using
 `eval_image_classifier.py`). Performance numbers for ResNet V2 models are
-reported on ImageNet valdiation set. 
+reported on ImageNet valdiation set.
+
+All 16 MobileNet Models reported in the [MobileNet Paper](https://arxiv.org/abs/1704.04861) can be found [here](https://github.com/tensorflow/models/tree/master/slim/nets/mobilenet_v1.md).
 
+(\*): Results quoted from the [paper](https://arxiv.org/abs/1603.05027).
 Here is an example of how to download the Inception V3 checkpoint:
 
 ```shell
@@ -375,4 +378,3 @@ image_preprocessing_fn = preprocessing_factory.get_preprocessing(
 
 See
 [Hardware Specifications](https://github.com/tensorflow/models/tree/master/inception#what-hardware-specification-are-these-hyper-parameters-targeted-for).
-
diff --git a/slim/nets/mobilenet_v1.md b/slim/nets/mobilenet_v1.md
new file mode 100644
index 00000000..3ce23117
--- /dev/null
+++ b/slim/nets/mobilenet_v1.md
@@ -0,0 +1,47 @@
+# MobileNet_v1
+
+[MobileNets](https://arxiv.org/abs/1704.04861) are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices with [TensorFlow Mobile](https://www.tensorflow.org/mobile/).
+
+MobileNets trade off between latency, size and accuracy while comparing favorably with popular models from the literature.
+
+![alt text](https://github.com/tensorflow/models/tree/master/slim/nets/mobilenet_v1.png, "MobileNet Graph")
+
+# Pre-trained Models
+
+Choose the right MobileNet model to fit your latency and size budget. The size of the network in memory and on disk is proportional to the number of parameters. The latency and power usage of the network scales with the number of Multiply-Accumulates (MACs) which measures the number of fused Multiplication and Addition operations. These MobileNet models have been trained on the
+[ILSVRC-2012-CLS](http://www.image-net.org/challenges/LSVRC/2012/)
+image classification dataset. Accuracies were computed by evaluating using a single image crop.
+
+Model Checkpoint | Million MACs | Million Parameters | Top-1 Accuracy| Top-5 Accuracy |
+:----:|:------------:|:----------:|:-------:|:-------:|
+[MobileNet_v1_1.0_224](http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz)|569|4.24|70.7|89.5|
+[MobileNet_v1_1.0_192](http://download.tensorflow.org/models/mobilenet_v1_1.0_192_2017_06_14.tar.gz)|418|4.24|69.3|88.9|
+[MobileNet_v1_1.0_160](http://download.tensorflow.org/models/mobilenet_v1_1.0_160_2017_06_14.tar.gz)|291|4.24|67.2|87.5|
+[MobileNet_v1_1.0_128](http://download.tensorflow.org/models/mobilenet_v1_1.0_128_2017_06_14.tar.gz)|186|4.24|64.1|85.3|
+[MobileNet_v1_0.75_224](http://download.tensorflow.org/models/mobilenet_v1_0.75_224_2017_06_14.tar.gz)|317|2.59|68.4|88.2|
+[MobileNet_v1_0.75_192](http://download.tensorflow.org/models/mobilenet_v1_0.75_192_2017_06_14.tar.gz)|233|2.59|67.4|87.3|
+[MobileNet_v1_0.75_160](http://download.tensorflow.org/models/mobilenet_v1_0.75_160_2017_06_14.tar.gz)|162|2.59|65.2|86.1|
+[MobileNet_v1_0.75_128](http://download.tensorflow.org/models/mobilenet_v1_0.75_128_2017_06_14.tar.gz)|104|2.59|61.8|83.6|
+[MobileNet_v1_0.50_224](http://download.tensorflow.org/models/mobilenet_v1_0.50_224_2017_06_14.tar.gz)|150|1.34|64.0|85.4|
+[MobileNet_v1_0.50_192](http://download.tensorflow.org/models/mobilenet_v1_0.50_192_2017_06_14.tar.gz)|110|1.34|62.1|84.0|
+[MobileNet_v1_0.50_160](http://download.tensorflow.org/models/mobilenet_v1_0.50_160_2017_06_14.tar.gz)|77|1.34|59.9|82.5|
+[MobileNet_v1_0.50_128](http://download.tensorflow.org/models/mobilenet_v1_0.50_128_2017_06_14.tar.gz)|49|1.34|56.2|79.6|
+[MobileNet_v1_0.25_224](http://download.tensorflow.org/models/mobilenet_v1_0.25_224_2017_06_14.tar.gz)|41|0.47|50.6|75.0|
+[MobileNet_v1_0.25_192](http://download.tensorflow.org/models/mobilenet_v1_0.25_192_2017_06_14.tar.gz)|34|0.47|49.0|73.6|
+[MobileNet_v1_0.25_160](http://download.tensorflow.org/models/mobilenet_v1_0.25_160_2017_06_14.tar.gz)|21|0.47|46.0|70.7|
+[MobileNet_v1_0.25_128](http://download.tensorflow.org/models/mobilenet_v1_0.25_128_2017_06_14.tar.gz)|14|0.47|41.3|66.2|
+
+
+Here is an example of how to download the MobileNet_v1_1.0_224 checkpoint:
+
+```shell
+$ CHECKPOINT_DIR=/tmp/checkpoints
+$ mkdir ${CHECKPOINT_DIR}
+$ wget http://download.tensorflow.org/models/mobilenet_v1_1.0_224_2017_06_14.tar.gz
+$ tar -xvf mobilenet_v1_1.0_224_2017_06_14.tar.gz
+$ mv mobilenet_v1_1.0_224.ckpt.* ${CHECKPOINT_DIR}
+$ rm mobilenet_v1_1.0_224_2017_06_14.tar.gz
+```
+More information on integrating MobileNets into your project can be found at the [TF-Slim Image Classification Library](https://github.com/tensorflow/models/blob/master/slim/README.md).
+
+To get started running models on-device go to [TensorFlow Mobile](https://www.tensorflow.org/mobile/).
diff --git a/slim/nets/mobilenet_v1.png b/slim/nets/mobilenet_v1.png
new file mode 100644
index 00000000..a4583451
Binary files /dev/null and b/slim/nets/mobilenet_v1.png differ
diff --git a/slim/nets/mobilenet_v1.py b/slim/nets/mobilenet_v1.py
new file mode 100644
index 00000000..9b25145f
--- /dev/null
+++ b/slim/nets/mobilenet_v1.py
@@ -0,0 +1,397 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# =============================================================================
+"""MobileNet v1.
+
+MobileNet is a general architecture and can be used for multiple use cases.
+Depending on the use case, it can use different input layer size and different
+head (for example: embeddings, localization and classification).
+
+As described in https://arxiv.org/abs/1704.04861.
+
+  MobileNets: Efficient Convolutional Neural Networks for
+    Mobile Vision Applications
+  Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
+    Tobias Weyand, Marco Andreetto, Hartwig Adam
+
+100% Mobilenet V1 (base) with input size 224x224:
+
+Layer                                                     params           macs
+--------------------------------------------------------------------------------
+MobilenetV1/Conv2d_0/Conv2D:                                 864      10,838,016
+MobilenetV1/Conv2d_1_depthwise/depthwise:                    288       3,612,672
+MobilenetV1/Conv2d_1_pointwise/Conv2D:                     2,048      25,690,112
+MobilenetV1/Conv2d_2_depthwise/depthwise:                    576       1,806,336
+MobilenetV1/Conv2d_2_pointwise/Conv2D:                     8,192      25,690,112
+MobilenetV1/Conv2d_3_depthwise/depthwise:                  1,152       3,612,672
+MobilenetV1/Conv2d_3_pointwise/Conv2D:                    16,384      51,380,224
+MobilenetV1/Conv2d_4_depthwise/depthwise:                  1,152         903,168
+MobilenetV1/Conv2d_4_pointwise/Conv2D:                    32,768      25,690,112
+MobilenetV1/Conv2d_5_depthwise/depthwise:                  2,304       1,806,336
+MobilenetV1/Conv2d_5_pointwise/Conv2D:                    65,536      51,380,224
+MobilenetV1/Conv2d_6_depthwise/depthwise:                  2,304         451,584
+MobilenetV1/Conv2d_6_pointwise/Conv2D:                   131,072      25,690,112
+MobilenetV1/Conv2d_7_depthwise/depthwise:                  4,608         903,168
+MobilenetV1/Conv2d_7_pointwise/Conv2D:                   262,144      51,380,224
+MobilenetV1/Conv2d_8_depthwise/depthwise:                  4,608         903,168
+MobilenetV1/Conv2d_8_pointwise/Conv2D:                   262,144      51,380,224
+MobilenetV1/Conv2d_9_depthwise/depthwise:                  4,608         903,168
+MobilenetV1/Conv2d_9_pointwise/Conv2D:                   262,144      51,380,224
+MobilenetV1/Conv2d_10_depthwise/depthwise:                 4,608         903,168
+MobilenetV1/Conv2d_10_pointwise/Conv2D:                  262,144      51,380,224
+MobilenetV1/Conv2d_11_depthwise/depthwise:                 4,608         903,168
+MobilenetV1/Conv2d_11_pointwise/Conv2D:                  262,144      51,380,224
+MobilenetV1/Conv2d_12_depthwise/depthwise:                 4,608         225,792
+MobilenetV1/Conv2d_12_pointwise/Conv2D:                  524,288      25,690,112
+MobilenetV1/Conv2d_13_depthwise/depthwise:                 9,216         451,584
+MobilenetV1/Conv2d_13_pointwise/Conv2D:                1,048,576      51,380,224
+--------------------------------------------------------------------------------
+Total:                                                 3,185,088     567,716,352
+
+
+75% Mobilenet V1 (base) with input size 128x128:
+
+Layer                                                     params           macs
+--------------------------------------------------------------------------------
+MobilenetV1/Conv2d_0/Conv2D:                                 648       2,654,208
+MobilenetV1/Conv2d_1_depthwise/depthwise:                    216         884,736
+MobilenetV1/Conv2d_1_pointwise/Conv2D:                     1,152       4,718,592
+MobilenetV1/Conv2d_2_depthwise/depthwise:                    432         442,368
+MobilenetV1/Conv2d_2_pointwise/Conv2D:                     4,608       4,718,592
+MobilenetV1/Conv2d_3_depthwise/depthwise:                    864         884,736
+MobilenetV1/Conv2d_3_pointwise/Conv2D:                     9,216       9,437,184
+MobilenetV1/Conv2d_4_depthwise/depthwise:                    864         221,184
+MobilenetV1/Conv2d_4_pointwise/Conv2D:                    18,432       4,718,592
+MobilenetV1/Conv2d_5_depthwise/depthwise:                  1,728         442,368
+MobilenetV1/Conv2d_5_pointwise/Conv2D:                    36,864       9,437,184
+MobilenetV1/Conv2d_6_depthwise/depthwise:                  1,728         110,592
+MobilenetV1/Conv2d_6_pointwise/Conv2D:                    73,728       4,718,592
+MobilenetV1/Conv2d_7_depthwise/depthwise:                  3,456         221,184
+MobilenetV1/Conv2d_7_pointwise/Conv2D:                   147,456       9,437,184
+MobilenetV1/Conv2d_8_depthwise/depthwise:                  3,456         221,184
+MobilenetV1/Conv2d_8_pointwise/Conv2D:                   147,456       9,437,184
+MobilenetV1/Conv2d_9_depthwise/depthwise:                  3,456         221,184
+MobilenetV1/Conv2d_9_pointwise/Conv2D:                   147,456       9,437,184
+MobilenetV1/Conv2d_10_depthwise/depthwise:                 3,456         221,184
+MobilenetV1/Conv2d_10_pointwise/Conv2D:                  147,456       9,437,184
+MobilenetV1/Conv2d_11_depthwise/depthwise:                 3,456         221,184
+MobilenetV1/Conv2d_11_pointwise/Conv2D:                  147,456       9,437,184
+MobilenetV1/Conv2d_12_depthwise/depthwise:                 3,456          55,296
+MobilenetV1/Conv2d_12_pointwise/Conv2D:                  294,912       4,718,592
+MobilenetV1/Conv2d_13_depthwise/depthwise:                 6,912         110,592
+MobilenetV1/Conv2d_13_pointwise/Conv2D:                  589,824       9,437,184
+--------------------------------------------------------------------------------
+Total:                                                 1,800,144     106,002,432
+
+"""
+
+# Tensorflow mandates these.
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+
+import tensorflow as tf
+
+slim = tf.contrib.slim
+
+# Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
+# Conv defines 3x3 convolution layers
+# DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.
+# stride is the stride of the convolution
+# depth is the number of channels or filters in a layer
+Conv = namedtuple('Conv', ['kernel', 'stride', 'depth'])
+DepthSepConv = namedtuple('DepthSepConv', ['kernel', 'stride', 'depth'])
+
+# _CONV_DEFS specifies the MobileNet body
+_CONV_DEFS = [
+    Conv(kernel=[3, 3], stride=2, depth=32),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=64),
+    DepthSepConv(kernel=[3, 3], stride=2, depth=128),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=128),
+    DepthSepConv(kernel=[3, 3], stride=2, depth=256),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=256),
+    DepthSepConv(kernel=[3, 3], stride=2, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=512),
+    DepthSepConv(kernel=[3, 3], stride=2, depth=1024),
+    DepthSepConv(kernel=[3, 3], stride=1, depth=1024)
+]
+
+
+def mobilenet_v1_base(inputs,
+                      final_endpoint='Conv2d_13_pointwise',
+                      min_depth=8,
+                      depth_multiplier=1.0,
+                      conv_defs=None,
+                      output_stride=None,
+                      scope=None):
+  """Mobilenet v1.
+
+  Constructs a Mobilenet v1 network from inputs to the given final endpoint.
+
+  Args:
+    inputs: a tensor of shape [batch_size, height, width, channels].
+    final_endpoint: specifies the endpoint to construct the network up to. It
+      can be one of ['Conv2d_0', 'Conv2d_1_pointwise', 'Conv2d_2_pointwise',
+      'Conv2d_3_pointwise', 'Conv2d_4_pointwise', 'Conv2d_5'_pointwise,
+      'Conv2d_6_pointwise', 'Conv2d_7_pointwise', 'Conv2d_8_pointwise',
+      'Conv2d_9_pointwise', 'Conv2d_10_pointwise', 'Conv2d_11_pointwise',
+      'Conv2d_12_pointwise', 'Conv2d_13_pointwise'].
+    min_depth: Minimum depth value (number of channels) for all convolution ops.
+      Enforced when depth_multiplier < 1, and not an active constraint when
+      depth_multiplier >= 1.
+    depth_multiplier: Float multiplier for the depth (number of channels)
+      for all convolution ops. The value must be greater than zero. Typical
+      usage will be to set this value in (0, 1) to reduce the number of
+      parameters or computation cost of the model.
+    conv_defs: A list of ConvDef namedtuples specifying the net architecture.
+    output_stride: An integer that specifies the requested ratio of input to
+      output spatial resolution. If not None, then we invoke atrous convolution
+      if necessary to prevent the network from reducing the spatial resolution
+      of the activation maps. Allowed values are 8 (accurate fully convolutional
+      mode), 16 (fast fully convolutional mode), 32 (classification mode).
+    scope: Optional variable_scope.
+
+  Returns:
+    tensor_out: output tensor corresponding to the final_endpoint.
+    end_points: a set of activations for external use, for example summaries or
+                losses.
+
+  Raises:
+    ValueError: if final_endpoint is not set to one of the predefined values,
+                or depth_multiplier <= 0, or the target output_stride is not
+                allowed.
+  """
+  depth = lambda d: max(int(d * depth_multiplier), min_depth)
+  end_points = {}
+
+  # Used to find thinned depths for each layer.
+  if depth_multiplier <= 0:
+    raise ValueError('depth_multiplier is not greater than zero.')
+
+  if conv_defs is None:
+    conv_defs = _CONV_DEFS
+
+  if output_stride is not None and output_stride not in [8, 16, 32]:
+    raise ValueError('Only allowed output_stride values are 8, 16, 32.')
+
+  with tf.variable_scope(scope, 'MobilenetV1', [inputs]):
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding='SAME'):
+      # The current_stride variable keeps track of the output stride of the
+      # activations, i.e., the running product of convolution strides up to the
+      # current network layer. This allows us to invoke atrous convolution
+      # whenever applying the next convolution would result in the activations
+      # having output stride larger than the target output_stride.
+      current_stride = 1
+
+      # The atrous convolution rate parameter.
+      rate = 1
+
+      net = inputs
+      for i, conv_def in enumerate(conv_defs):
+        end_point_base = 'Conv2d_%d' % i
+
+        if output_stride is not None and current_stride == output_stride:
+          # If we have reached the target output_stride, then we need to employ
+          # atrous convolution with stride=1 and multiply the atrous rate by the
+          # current unit's stride for use in subsequent layers.
+          layer_stride = 1
+          layer_rate = rate
+          rate *= conv_def.stride
+        else:
+          layer_stride = conv_def.stride
+          layer_rate = 1
+          current_stride *= conv_def.stride
+
+        if isinstance(conv_def, Conv):
+          end_point = end_point_base
+          net = slim.conv2d(net, depth(conv_def.depth), conv_def.kernel,
+                            stride=conv_def.stride,
+                            normalizer_fn=slim.batch_norm,
+                            scope=end_point)
+          end_points[end_point] = net
+          if end_point == final_endpoint:
+            return net, end_points
+
+        elif isinstance(conv_def, DepthSepConv):
+          end_point = end_point_base + '_depthwise'
+
+          # By passing filters=None
+          # separable_conv2d produces only a depthwise convolution layer
+          net = slim.separable_conv2d(net, None, conv_def.kernel,
+                                      depth_multiplier=1,
+                                      stride=layer_stride,
+                                      rate=layer_rate,
+                                      normalizer_fn=slim.batch_norm,
+                                      scope=end_point)
+
+          end_points[end_point] = net
+          if end_point == final_endpoint:
+            return net, end_points
+
+          end_point = end_point_base + '_pointwise'
+
+          net = slim.conv2d(net, depth(conv_def.depth), [1, 1],
+                            stride=1,
+                            normalizer_fn=slim.batch_norm,
+                            scope=end_point)
+
+          end_points[end_point] = net
+          if end_point == final_endpoint:
+            return net, end_points
+        else:
+          raise ValueError('Unknown convolution type %s for layer %d'
+                           % (conv_def.ltype, i))
+  raise ValueError('Unknown final endpoint %s' % final_endpoint)
+
+
+def mobilenet_v1(inputs,
+                 num_classes=1000,
+                 dropout_keep_prob=0.999,
+                 is_training=True,
+                 min_depth=8,
+                 depth_multiplier=1.0,
+                 conv_defs=None,
+                 prediction_fn=tf.contrib.layers.softmax,
+                 spatial_squeeze=True,
+                 reuse=None,
+                 scope='MobilenetV1'):
+  """Mobilenet v1 model for classification.
+
+  Args:
+    inputs: a tensor of shape [batch_size, height, width, channels].
+    num_classes: number of predicted classes.
+    dropout_keep_prob: the percentage of activation values that are retained.
+    is_training: whether is training or not.
+    min_depth: Minimum depth value (number of channels) for all convolution ops.
+      Enforced when depth_multiplier < 1, and not an active constraint when
+      depth_multiplier >= 1.
+    depth_multiplier: Float multiplier for the depth (number of channels)
+      for all convolution ops. The value must be greater than zero. Typical
+      usage will be to set this value in (0, 1) to reduce the number of
+      parameters or computation cost of the model.
+    conv_defs: A list of ConvDef namedtuples specifying the net architecture.
+    prediction_fn: a function to get predictions out of logits.
+    spatial_squeeze: if True, logits is of shape is [B, C], if false logits is
+        of shape [B, 1, 1, C], where B is batch_size and C is number of classes.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
+    scope: Optional variable_scope.
+
+  Returns:
+    logits: the pre-softmax activations, a tensor of size
+      [batch_size, num_classes]
+    end_points: a dictionary from components of the network to the corresponding
+      activation.
+
+  Raises:
+    ValueError: Input rank is invalid.
+  """
+  input_shape = inputs.get_shape().as_list()
+  if len(input_shape) != 4:
+    raise ValueError('Invalid input tensor rank, expected 4, was: %d' %
+                     len(input_shape))
+
+  with tf.variable_scope(scope, 'MobilenetV1', [inputs, num_classes],
+                         reuse=reuse) as scope:
+    with slim.arg_scope([slim.batch_norm, slim.dropout],
+                        is_training=is_training):
+      net, end_points = mobilenet_v1_base(inputs, scope=scope,
+                                          min_depth=min_depth,
+                                          depth_multiplier=depth_multiplier,
+                                          conv_defs=conv_defs)
+      with tf.variable_scope('Logits'):
+        kernel_size = _reduced_kernel_size_for_small_input(net, [7, 7])
+        net = slim.avg_pool2d(net, kernel_size, padding='VALID',
+                              scope='AvgPool_1a')
+        end_points['AvgPool_1a'] = net
+        # 1 x 1 x 1024
+        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')
+        logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
+                             normalizer_fn=None, scope='Conv2d_1c_1x1')
+        if spatial_squeeze:
+          logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')
+      end_points['Logits'] = logits
+      if prediction_fn:
+        end_points['Predictions'] = prediction_fn(logits, scope='Predictions')
+  return logits, end_points
+
+mobilenet_v1.default_image_size = 224
+
+
+def _reduced_kernel_size_for_small_input(input_tensor, kernel_size):
+  """Define kernel size which is automatically reduced for small input.
+
+  If the shape of the input images is unknown at graph construction time this
+  function assumes that the input images are large enough.
+
+  Args:
+    input_tensor: input tensor of size [batch_size, height, width, channels].
+    kernel_size: desired kernel size of length 2: [kernel_height, kernel_width]
+
+  Returns:
+    a tensor with the kernel size.
+  """
+  shape = input_tensor.get_shape().as_list()
+  if shape[1] is None or shape[2] is None:
+    kernel_size_out = kernel_size
+  else:
+    kernel_size_out = [min(shape[1], kernel_size[0]),
+                       min(shape[2], kernel_size[1])]
+  return kernel_size_out
+
+
+def mobilenet_v1_arg_scope(is_training=True,
+                           weight_decay=0.00004,
+                           stddev=0.09,
+                           regularize_depthwise=False):
+  """Defines the default MobilenetV1 arg scope.
+
+  Args:
+    is_training: Whether or not we're training the model.
+    weight_decay: The weight decay to use for regularizing the model.
+    stddev: The standard deviation of the trunctated normal weight initializer.
+    regularize_depthwise: Whether or not apply regularization on depthwise.
+
+  Returns:
+    An `arg_scope` to use for the mobilenet v1 model.
+  """
+  batch_norm_params = {
+      'is_training': is_training,
+      'center': True,
+      'scale': True,
+      'decay': 0.9997,
+      'epsilon': 0.001,
+  }
+
+  # Set weight_decay for weights in Conv and DepthSepConv layers.
+  weights_init = tf.truncated_normal_initializer(stddev=stddev)
+  regularizer = tf.contrib.layers.l2_regularizer(weight_decay)
+  if regularize_depthwise:
+    depthwise_regularizer = regularizer
+  else:
+    depthwise_regularizer = None
+  with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                      weights_initializer=weights_init,
+                      activation_fn=tf.nn.relu6, normalizer_fn=slim.batch_norm):
+    with slim.arg_scope([slim.batch_norm], **batch_norm_params):
+      with slim.arg_scope([slim.conv2d], weights_regularizer=regularizer):
+        with slim.arg_scope([slim.separable_conv2d],
+                            weights_regularizer=depthwise_regularizer) as sc:
+          return sc
diff --git a/slim/nets/mobilenet_v1_test.py b/slim/nets/mobilenet_v1_test.py
new file mode 100644
index 00000000..44e66446
--- /dev/null
+++ b/slim/nets/mobilenet_v1_test.py
@@ -0,0 +1,450 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+# http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# =============================================================================
+"""Tests for MobileNet v1."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from nets import mobilenet_v1
+
+slim = tf.contrib.slim
+
+
+class MobilenetV1Test(tf.test.TestCase):
+
+  def testBuildClassificationNetwork(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+    self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    self.assertTrue('Predictions' in end_points)
+    self.assertListEqual(end_points['Predictions'].get_shape().as_list(),
+                         [batch_size, num_classes])
+
+  def testBuildBaseNetwork(self):
+    batch_size = 5
+    height, width = 224, 224
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)
+    self.assertTrue(net.op.name.startswith('MobilenetV1/Conv2d_13'))
+    self.assertListEqual(net.get_shape().as_list(),
+                         [batch_size, 7, 7, 1024])
+    expected_endpoints = ['Conv2d_0',
+                          'Conv2d_1_depthwise', 'Conv2d_1_pointwise',
+                          'Conv2d_2_depthwise', 'Conv2d_2_pointwise',
+                          'Conv2d_3_depthwise', 'Conv2d_3_pointwise',
+                          'Conv2d_4_depthwise', 'Conv2d_4_pointwise',
+                          'Conv2d_5_depthwise', 'Conv2d_5_pointwise',
+                          'Conv2d_6_depthwise', 'Conv2d_6_pointwise',
+                          'Conv2d_7_depthwise', 'Conv2d_7_pointwise',
+                          'Conv2d_8_depthwise', 'Conv2d_8_pointwise',
+                          'Conv2d_9_depthwise', 'Conv2d_9_pointwise',
+                          'Conv2d_10_depthwise', 'Conv2d_10_pointwise',
+                          'Conv2d_11_depthwise', 'Conv2d_11_pointwise',
+                          'Conv2d_12_depthwise', 'Conv2d_12_pointwise',
+                          'Conv2d_13_depthwise', 'Conv2d_13_pointwise']
+    self.assertItemsEqual(end_points.keys(), expected_endpoints)
+
+  def testBuildOnlyUptoFinalEndpoint(self):
+    batch_size = 5
+    height, width = 224, 224
+    endpoints = ['Conv2d_0',
+                 'Conv2d_1_depthwise', 'Conv2d_1_pointwise',
+                 'Conv2d_2_depthwise', 'Conv2d_2_pointwise',
+                 'Conv2d_3_depthwise', 'Conv2d_3_pointwise',
+                 'Conv2d_4_depthwise', 'Conv2d_4_pointwise',
+                 'Conv2d_5_depthwise', 'Conv2d_5_pointwise',
+                 'Conv2d_6_depthwise', 'Conv2d_6_pointwise',
+                 'Conv2d_7_depthwise', 'Conv2d_7_pointwise',
+                 'Conv2d_8_depthwise', 'Conv2d_8_pointwise',
+                 'Conv2d_9_depthwise', 'Conv2d_9_pointwise',
+                 'Conv2d_10_depthwise', 'Conv2d_10_pointwise',
+                 'Conv2d_11_depthwise', 'Conv2d_11_pointwise',
+                 'Conv2d_12_depthwise', 'Conv2d_12_pointwise',
+                 'Conv2d_13_depthwise', 'Conv2d_13_pointwise']
+    for index, endpoint in enumerate(endpoints):
+      with tf.Graph().as_default():
+        inputs = tf.random_uniform((batch_size, height, width, 3))
+        out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(
+            inputs, final_endpoint=endpoint)
+        self.assertTrue(out_tensor.op.name.startswith(
+            'MobilenetV1/' + endpoint))
+        self.assertItemsEqual(endpoints[:index+1], end_points)
+
+  def testBuildCustomNetworkUsingConvDefs(self):
+    batch_size = 5
+    height, width = 224, 224
+    conv_defs = [
+        mobilenet_v1.Conv(kernel=[3, 3], stride=2, depth=32),
+        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=64),
+        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=2, depth=128),
+        mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)
+    ]
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    net, end_points = mobilenet_v1.mobilenet_v1_base(
+        inputs, final_endpoint='Conv2d_3_pointwise', conv_defs=conv_defs)
+    self.assertTrue(net.op.name.startswith('MobilenetV1/Conv2d_3'))
+    self.assertListEqual(net.get_shape().as_list(),
+                         [batch_size, 56, 56, 512])
+    expected_endpoints = ['Conv2d_0',
+                          'Conv2d_1_depthwise', 'Conv2d_1_pointwise',
+                          'Conv2d_2_depthwise', 'Conv2d_2_pointwise',
+                          'Conv2d_3_depthwise', 'Conv2d_3_pointwise']
+    self.assertItemsEqual(end_points.keys(), expected_endpoints)
+
+  def testBuildAndCheckAllEndPointsUptoConv2d_13(self):
+    batch_size = 5
+    height, width = 224, 224
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                        normalizer_fn=slim.batch_norm):
+      _, end_points = mobilenet_v1.mobilenet_v1_base(
+          inputs, final_endpoint='Conv2d_13_pointwise')
+    endpoints_shapes = {'Conv2d_0': [batch_size, 112, 112, 32],
+                        'Conv2d_1_depthwise': [batch_size, 112, 112, 32],
+                        'Conv2d_1_pointwise': [batch_size, 112, 112, 64],
+                        'Conv2d_2_depthwise': [batch_size, 56, 56, 64],
+                        'Conv2d_2_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_depthwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_4_depthwise': [batch_size, 28, 28, 128],
+                        'Conv2d_4_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_depthwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_6_depthwise': [batch_size, 14, 14, 256],
+                        'Conv2d_6_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_7_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_7_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_8_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_8_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_9_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_9_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_10_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_10_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_11_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_11_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_12_depthwise': [batch_size, 7, 7, 512],
+                        'Conv2d_12_pointwise': [batch_size, 7, 7, 1024],
+                        'Conv2d_13_depthwise': [batch_size, 7, 7, 1024],
+                        'Conv2d_13_pointwise': [batch_size, 7, 7, 1024]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testOutputStride16BuildAndCheckAllEndPointsUptoConv2d_13(self):
+    batch_size = 5
+    height, width = 224, 224
+    output_stride = 16
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                        normalizer_fn=slim.batch_norm):
+      _, end_points = mobilenet_v1.mobilenet_v1_base(
+          inputs, output_stride=output_stride,
+          final_endpoint='Conv2d_13_pointwise')
+    endpoints_shapes = {'Conv2d_0': [batch_size, 112, 112, 32],
+                        'Conv2d_1_depthwise': [batch_size, 112, 112, 32],
+                        'Conv2d_1_pointwise': [batch_size, 112, 112, 64],
+                        'Conv2d_2_depthwise': [batch_size, 56, 56, 64],
+                        'Conv2d_2_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_depthwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_4_depthwise': [batch_size, 28, 28, 128],
+                        'Conv2d_4_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_depthwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_6_depthwise': [batch_size, 14, 14, 256],
+                        'Conv2d_6_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_7_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_7_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_8_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_8_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_9_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_9_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_10_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_10_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_11_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_11_pointwise': [batch_size, 14, 14, 512],
+                        'Conv2d_12_depthwise': [batch_size, 14, 14, 512],
+                        'Conv2d_12_pointwise': [batch_size, 14, 14, 1024],
+                        'Conv2d_13_depthwise': [batch_size, 14, 14, 1024],
+                        'Conv2d_13_pointwise': [batch_size, 14, 14, 1024]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testOutputStride8BuildAndCheckAllEndPointsUptoConv2d_13(self):
+    batch_size = 5
+    height, width = 224, 224
+    output_stride = 8
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                        normalizer_fn=slim.batch_norm):
+      _, end_points = mobilenet_v1.mobilenet_v1_base(
+          inputs, output_stride=output_stride,
+          final_endpoint='Conv2d_13_pointwise')
+    endpoints_shapes = {'Conv2d_0': [batch_size, 112, 112, 32],
+                        'Conv2d_1_depthwise': [batch_size, 112, 112, 32],
+                        'Conv2d_1_pointwise': [batch_size, 112, 112, 64],
+                        'Conv2d_2_depthwise': [batch_size, 56, 56, 64],
+                        'Conv2d_2_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_depthwise': [batch_size, 56, 56, 128],
+                        'Conv2d_3_pointwise': [batch_size, 56, 56, 128],
+                        'Conv2d_4_depthwise': [batch_size, 28, 28, 128],
+                        'Conv2d_4_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_depthwise': [batch_size, 28, 28, 256],
+                        'Conv2d_5_pointwise': [batch_size, 28, 28, 256],
+                        'Conv2d_6_depthwise': [batch_size, 28, 28, 256],
+                        'Conv2d_6_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_7_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_7_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_8_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_8_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_9_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_9_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_10_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_10_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_11_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_11_pointwise': [batch_size, 28, 28, 512],
+                        'Conv2d_12_depthwise': [batch_size, 28, 28, 512],
+                        'Conv2d_12_pointwise': [batch_size, 28, 28, 1024],
+                        'Conv2d_13_depthwise': [batch_size, 28, 28, 1024],
+                        'Conv2d_13_pointwise': [batch_size, 28, 28, 1024]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testBuildAndCheckAllEndPointsApproximateFaceNet(self):
+    batch_size = 5
+    height, width = 128, 128
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                        normalizer_fn=slim.batch_norm):
+      _, end_points = mobilenet_v1.mobilenet_v1_base(
+          inputs, final_endpoint='Conv2d_13_pointwise', depth_multiplier=0.75)
+    # For the Conv2d_0 layer FaceNet has depth=16
+    endpoints_shapes = {'Conv2d_0': [batch_size, 64, 64, 24],
+                        'Conv2d_1_depthwise': [batch_size, 64, 64, 24],
+                        'Conv2d_1_pointwise': [batch_size, 64, 64, 48],
+                        'Conv2d_2_depthwise': [batch_size, 32, 32, 48],
+                        'Conv2d_2_pointwise': [batch_size, 32, 32, 96],
+                        'Conv2d_3_depthwise': [batch_size, 32, 32, 96],
+                        'Conv2d_3_pointwise': [batch_size, 32, 32, 96],
+                        'Conv2d_4_depthwise': [batch_size, 16, 16, 96],
+                        'Conv2d_4_pointwise': [batch_size, 16, 16, 192],
+                        'Conv2d_5_depthwise': [batch_size, 16, 16, 192],
+                        'Conv2d_5_pointwise': [batch_size, 16, 16, 192],
+                        'Conv2d_6_depthwise': [batch_size, 8, 8, 192],
+                        'Conv2d_6_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_7_depthwise': [batch_size, 8, 8, 384],
+                        'Conv2d_7_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_8_depthwise': [batch_size, 8, 8, 384],
+                        'Conv2d_8_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_9_depthwise': [batch_size, 8, 8, 384],
+                        'Conv2d_9_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_10_depthwise': [batch_size, 8, 8, 384],
+                        'Conv2d_10_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_11_depthwise': [batch_size, 8, 8, 384],
+                        'Conv2d_11_pointwise': [batch_size, 8, 8, 384],
+                        'Conv2d_12_depthwise': [batch_size, 4, 4, 384],
+                        'Conv2d_12_pointwise': [batch_size, 4, 4, 768],
+                        'Conv2d_13_depthwise': [batch_size, 4, 4, 768],
+                        'Conv2d_13_pointwise': [batch_size, 4, 4, 768]}
+    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+      self.assertTrue(endpoint_name in end_points)
+      self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
+                           expected_shape)
+
+  def testModelHasExpectedNumberOfParameters(self):
+    batch_size = 5
+    height, width = 224, 224
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
+                        normalizer_fn=slim.batch_norm):
+      mobilenet_v1.mobilenet_v1_base(inputs)
+      total_params, _ = slim.model_analyzer.analyze_vars(
+          slim.get_model_variables())
+      self.assertAlmostEqual(3217920L, total_params)
+
+  def testBuildEndPointsWithDepthMultiplierLessThanOne(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+
+    endpoint_keys = [key for key in end_points.keys() if key.startswith('Conv')]
+
+    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(
+        inputs, num_classes, scope='depth_multiplied_net',
+        depth_multiplier=0.5)
+
+    for key in endpoint_keys:
+      original_depth = end_points[key].get_shape().as_list()[3]
+      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]
+      self.assertEqual(0.5 * original_depth, new_depth)
+
+  def testBuildEndPointsWithDepthMultiplierGreaterThanOne(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+
+    endpoint_keys = [key for key in end_points.keys()
+                     if key.startswith('Mixed') or key.startswith('Conv')]
+
+    _, end_points_with_multiplier = mobilenet_v1.mobilenet_v1(
+        inputs, num_classes, scope='depth_multiplied_net',
+        depth_multiplier=2.0)
+
+    for key in endpoint_keys:
+      original_depth = end_points[key].get_shape().as_list()[3]
+      new_depth = end_points_with_multiplier[key].get_shape().as_list()[3]
+      self.assertEqual(2.0 * original_depth, new_depth)
+
+  def testRaiseValueErrorWithInvalidDepthMultiplier(self):
+    batch_size = 5
+    height, width = 224, 224
+    num_classes = 1000
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    with self.assertRaises(ValueError):
+      _ = mobilenet_v1.mobilenet_v1(
+          inputs, num_classes, depth_multiplier=-0.1)
+    with self.assertRaises(ValueError):
+      _ = mobilenet_v1.mobilenet_v1(
+          inputs, num_classes, depth_multiplier=0.0)
+
+  def testHalfSizeImages(self):
+    batch_size = 5
+    height, width = 112, 112
+    num_classes = 1000
+
+    inputs = tf.random_uniform((batch_size, height, width, 3))
+    logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+    self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [batch_size, num_classes])
+    pre_pool = end_points['Conv2d_13_pointwise']
+    self.assertListEqual(pre_pool.get_shape().as_list(),
+                         [batch_size, 4, 4, 1024])
+
+  def testUnknownImageShape(self):
+    tf.reset_default_graph()
+    batch_size = 2
+    height, width = 224, 224
+    num_classes = 1000
+    input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
+    with self.test_session() as sess:
+      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+      self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+      self.assertListEqual(logits.get_shape().as_list(),
+                           [batch_size, num_classes])
+      pre_pool = end_points['Conv2d_13_pointwise']
+      feed_dict = {inputs: input_np}
+      tf.global_variables_initializer().run()
+      pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
+      self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
+
+  def testUnknowBatchSize(self):
+    batch_size = 1
+    height, width = 224, 224
+    num_classes = 1000
+
+    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+    logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)
+    self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
+    self.assertListEqual(logits.get_shape().as_list(),
+                         [None, num_classes])
+    images = tf.random_uniform((batch_size, height, width, 3))
+
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(logits, {inputs: images.eval()})
+      self.assertEquals(output.shape, (batch_size, num_classes))
+
+  def testEvaluation(self):
+    batch_size = 2
+    height, width = 224, 224
+    num_classes = 1000
+
+    eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,
+                                          is_training=False)
+    predictions = tf.argmax(logits, 1)
+
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(predictions)
+      self.assertEquals(output.shape, (batch_size,))
+
+  def testTrainEvalWithReuse(self):
+    train_batch_size = 5
+    eval_batch_size = 2
+    height, width = 150, 150
+    num_classes = 1000
+
+    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+    mobilenet_v1.mobilenet_v1(train_inputs, num_classes)
+    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+    logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,
+                                          reuse=True)
+    predictions = tf.argmax(logits, 1)
+
+    with self.test_session() as sess:
+      sess.run(tf.global_variables_initializer())
+      output = sess.run(predictions)
+      self.assertEquals(output.shape, (eval_batch_size,))
+
+  def testLogitsNotSqueezed(self):
+    num_classes = 25
+    images = tf.random_uniform([1, 224, 224, 3])
+    logits, _ = mobilenet_v1.mobilenet_v1(images,
+                                          num_classes=num_classes,
+                                          spatial_squeeze=False)
+
+    with self.test_session() as sess:
+      tf.global_variables_initializer().run()
+      logits_out = sess.run(logits)
+      self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/slim/nets/nets_factory.py b/slim/nets/nets_factory.py
index bd8d7127..7c041616 100644
--- a/slim/nets/nets_factory.py
+++ b/slim/nets/nets_factory.py
@@ -25,6 +25,7 @@ from nets import alexnet
 from nets import cifarnet
 from nets import inception
 from nets import lenet
+from nets import mobilenet_v1
 from nets import overfeat
 from nets import resnet_v1
 from nets import resnet_v2
@@ -52,6 +53,7 @@ networks_map = {'alexnet_v2': alexnet.alexnet_v2,
                 'resnet_v2_101': resnet_v2.resnet_v2_101,
                 'resnet_v2_152': resnet_v2.resnet_v2_152,
                 'resnet_v2_200': resnet_v2.resnet_v2_200,
+                'mobilenet_v1': mobilenet_v1.mobilenet_v1,
                }
 
 arg_scopes_map = {'alexnet_v2': alexnet.alexnet_v2_arg_scope,
@@ -75,6 +77,7 @@ arg_scopes_map = {'alexnet_v2': alexnet.alexnet_v2_arg_scope,
                   'resnet_v2_101': resnet_v2.resnet_arg_scope,
                   'resnet_v2_152': resnet_v2.resnet_arg_scope,
                   'resnet_v2_200': resnet_v2.resnet_arg_scope,
+                  'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,
                  }
 
 
@@ -97,10 +100,10 @@ def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):
   """
   if name not in networks_map:
     raise ValueError('Name of network unknown %s' % name)
+  arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
   func = networks_map[name]
   @functools.wraps(func)
   def network_fn(images):
-    arg_scope = arg_scopes_map[name](weight_decay=weight_decay)
     with slim.arg_scope(arg_scope):
       return func(images, num_classes, is_training=is_training)
   if hasattr(func, 'default_image_size'):
diff --git a/slim/preprocessing/preprocessing_factory.py b/slim/preprocessing/preprocessing_factory.py
index 35f8645e..3ab79a01 100644
--- a/slim/preprocessing/preprocessing_factory.py
+++ b/slim/preprocessing/preprocessing_factory.py
@@ -53,12 +53,10 @@ def get_preprocessing(name, is_training=False):
       'inception_v4': inception_preprocessing,
       'inception_resnet_v2': inception_preprocessing,
       'lenet': lenet_preprocessing,
+      'mobilenet_v1': inception_preprocessing,
       'resnet_v1_50': vgg_preprocessing,
       'resnet_v1_101': vgg_preprocessing,
       'resnet_v1_152': vgg_preprocessing,
-      'resnet_v2_50': vgg_preprocessing,
-      'resnet_v2_101': vgg_preprocessing,
-      'resnet_v2_152': vgg_preprocessing,
       'vgg': vgg_preprocessing,
       'vgg_a': vgg_preprocessing,
       'vgg_16': vgg_preprocessing,
