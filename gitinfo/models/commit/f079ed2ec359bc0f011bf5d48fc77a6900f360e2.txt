commit f079ed2ec359bc0f011bf5d48fc77a6900f360e2
Author: Hongkun Yu <hongkuny@google.com>
Date:   Thu Dec 5 11:26:44 2019 -0800

    Internal change
    
    PiperOrigin-RevId: 284016229

diff --git a/official/vision/image_classification/common.py b/official/vision/image_classification/common.py
index 1a127e9b..4997066f 100644
--- a/official/vision/image_classification/common.py
+++ b/official/vision/image_classification/common.py
@@ -24,7 +24,6 @@ import numpy as np
 import tensorflow as tf
 
 from tensorflow.python.keras.optimizer_v2 import gradient_descent as gradient_descent_v2
-import tensorflow_model_optimization as tfmot
 from official.utils.flags import core as flags_core
 from official.utils.misc import keras_utils
 
@@ -181,12 +180,7 @@ def get_optimizer(learning_rate=0.1):
 
 
 # TODO(hongkuny,haoyuzhang): make cifar model use_tensor_lr to clean up code.
-def get_callbacks(
-    steps_per_epoch,
-    learning_rate_schedule_fn=None,
-    pruning_method='',
-    enable_checkpoint_and_export=False,
-    model_dir=''):
+def get_callbacks(steps_per_epoch, learning_rate_schedule_fn=None):
   """Returns common callbacks."""
   time_callback = keras_utils.TimeHistory(FLAGS.batch_size, FLAGS.log_steps)
   callbacks = [time_callback]
@@ -211,17 +205,6 @@ def get_callbacks(
         steps_per_epoch)
     callbacks.append(profiler_callback)
 
-  if model_dir:
-    if pruning_method == 'polynomial_decay':
-      callbacks.append(tfmot.sparsity.keras.PruningSummaries(
-          log_dir=model_dir, profile_batch=0))
-      callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())
-
-    if enable_checkpoint_and_export:
-      ckpt_full_path = os.path.join(model_dir, 'model.ckpt-{epoch:04d}')
-      callbacks.append(
-          tf.keras.callbacks.ModelCheckpoint(ckpt_full_path,
-                                             save_weights_only=True))
   return callbacks
 
 
@@ -375,31 +358,6 @@ def get_synth_data(height, width, num_channels, num_classes, dtype):
   return inputs, labels
 
 
-def define_pruning_flags():
-  """Define flags for pruning methods."""
-  flags.DEFINE_string('pruning_method', '',
-                      'Pruning method.'
-                      'Empty string (no pruning) or polynomial_decay.')
-  flags.DEFINE_float('pruning_initial_sparsity', 0.0,
-                     'Initial sparsity for pruning.')
-  flags.DEFINE_float('pruning_final_sparsity', 0.5,
-                     'Final sparsity for pruning.')
-  flags.DEFINE_integer('pruning_begin_step', 0,
-                       'Begin step for pruning.')
-  flags.DEFINE_integer('pruning_end_step', 100000,
-                       'End step for pruning.')
-  flags.DEFINE_integer('pruning_frequency', 100,
-                       'Frequency for pruning.')
-
-  flags.DEFINE_string('model', 'resnet50_v1.5',
-                      'Name of model preset. (mobilenet, resnet50_v1.5)')
-  flags.DEFINE_string('optimizer', 'resnet50_default',
-                      'Name of optimizer preset. '
-                      '(mobilenet_default, resnet50_default)')
-  flags.DEFINE_string('pretrained_filepath', '',
-                      'Pretrained file path.')
-
-
 def get_synth_input_fn(height, width, num_channels, num_classes,
                        dtype=tf.float32, drop_remainder=True):
   """Returns an input function that returns a dataset with random data.
diff --git a/official/vision/image_classification/imagenet_preprocessing.py b/official/vision/image_classification/imagenet_preprocessing.py
index 7e9f6e36..7a26da2b 100644
--- a/official/vision/image_classification/imagenet_preprocessing.py
+++ b/official/vision/image_classification/imagenet_preprocessing.py
@@ -246,24 +246,6 @@ def parse_record(raw_record, is_training, dtype):
   return image, label
 
 
-def get_parse_record_fn(use_keras_image_data_format=False):
-  """Get function to use for parsing the records.
-
-  Args:
-    use_keras_image_data_format: A boolean denoting whether data format is keras
-      backend image data format.
-  Returns:
-    Function to use for parsing the records.
-  """
-  def parse_record_fn(raw_record, is_training, dtype):
-    image, label = parse_record(raw_record, is_training, dtype)
-    if use_keras_image_data_format:
-      if tf.keras.backend.image_data_format() == 'channels_first':
-        image = tf.transpose(image, perm=[2, 0, 1])
-    return image, label
-  return parse_record_fn
-
-
 def input_fn(is_training,
              data_dir,
              batch_size,
diff --git a/official/vision/image_classification/resnet_imagenet_main.py b/official/vision/image_classification/resnet_imagenet_main.py
index 1b564c68..081bde32 100644
--- a/official/vision/image_classification/resnet_imagenet_main.py
+++ b/official/vision/image_classification/resnet_imagenet_main.py
@@ -25,8 +25,6 @@ from absl import flags
 from absl import logging
 import tensorflow as tf
 
-import tensorflow_model_optimization as tfmot
-
 from official.benchmark.models import trivial_model
 from official.utils.flags import core as flags_core
 from official.utils.logs import logger
@@ -46,7 +44,6 @@ def run(flags_obj):
 
   Raises:
     ValueError: If fp16 is passed as it is not currently supported.
-    NotImplementedError: If some features are not currently supported.
 
   Returns:
     Dictionary of training and eval stats.
@@ -123,20 +120,12 @@ def run(flags_obj):
   # in the dataset, as XLA-GPU doesn't support dynamic shapes.
   drop_remainder = flags_obj.enable_xla
 
-  # Current resnet_model.resnet50 input format is always channel-last.
-  # We use keras_application mobilenet model which input format is depends on
-  # the keras beckend image data format.
-  # This use_keras_image_data_format flags indicates whether image preprocessor
-  # output format should be same as the keras backend image data format or just
-  # channel-last format.
-  use_keras_image_data_format = (flags_obj.model == 'mobilenet')
   train_input_dataset = input_fn(
       is_training=True,
       data_dir=flags_obj.data_dir,
       batch_size=flags_obj.batch_size,
       num_epochs=flags_obj.train_epochs,
-      parse_record_fn=imagenet_preprocessing.get_parse_record_fn(
-          use_keras_image_data_format=use_keras_image_data_format),
+      parse_record_fn=imagenet_preprocessing.parse_record,
       datasets_num_private_threads=flags_obj.datasets_num_private_threads,
       dtype=dtype,
       drop_remainder=drop_remainder,
@@ -151,8 +140,7 @@ def run(flags_obj):
         data_dir=flags_obj.data_dir,
         batch_size=flags_obj.batch_size,
         num_epochs=flags_obj.train_epochs,
-        parse_record_fn=imagenet_preprocessing.get_parse_record_fn(
-            use_keras_image_data_format=use_keras_image_data_format),
+        parse_record_fn=imagenet_preprocessing.parse_record,
         dtype=dtype,
         drop_remainder=drop_remainder)
 
@@ -165,27 +153,9 @@ def run(flags_obj):
         boundaries=list(p[1] for p in common.LR_SCHEDULE[1:]),
         multipliers=list(p[0] for p in common.LR_SCHEDULE),
         compute_lr_on_cpu=True)
-  steps_per_epoch = (
-      imagenet_preprocessing.NUM_IMAGES['train'] // flags_obj.batch_size)
 
-  learning_rate_schedule_fn = None
   with strategy_scope:
-    if flags_obj.optimizer == 'resnet50_default':
-      optimizer = common.get_optimizer(lr_schedule)
-      learning_rate_schedule_fn = common.learning_rate_schedule
-    elif flags_obj.optimizer == 'mobilenet_default':
-      lr_decay_factor = 0.94
-      num_epochs_per_decay = 2.5
-      initial_learning_rate_per_sample = 0.000007
-      initial_learning_rate = \
-          initial_learning_rate_per_sample * flags_obj.batch_size
-      optimizer = tf.keras.optimizers.SGD(
-          learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(
-              initial_learning_rate,
-              decay_steps=steps_per_epoch * num_epochs_per_decay,
-              decay_rate=lr_decay_factor,
-              staircase=True),
-          momentum=0.9)
+    optimizer = common.get_optimizer(lr_schedule)
     if flags_obj.fp16_implementation == 'graph_rewrite':
       # Note: when flags_obj.fp16_implementation == "graph_rewrite", dtype as
       # determined by flags_core.get_tf_dtype(flags_obj) would be 'float32'
@@ -199,30 +169,9 @@ def run(flags_obj):
     if flags_obj.use_trivial_model:
       model = trivial_model.trivial_model(
           imagenet_preprocessing.NUM_CLASSES)
-    elif flags_obj.model == 'resnet50_v1.5':
+    else:
       model = resnet_model.resnet50(
           num_classes=imagenet_preprocessing.NUM_CLASSES)
-    elif flags_obj.model == 'mobilenet':
-      model = tf.keras.applications.mobilenet.MobileNet(
-          weights=None,
-          classes=imagenet_preprocessing.NUM_CLASSES)
-    if flags_obj.pretrained_filepath:
-      model.load_weights(flags_obj.pretrained_filepath)
-
-    if flags_obj.pruning_method == 'polynomial_decay':
-      if dtype != tf.float32:
-        raise NotImplementedError(
-            'Pruning is currently only supported on dtype=tf.float32.')
-      pruning_params = {
-          'pruning_schedule':
-              tfmot.sparsity.keras.PolynomialDecay(
-                  initial_sparsity=flags_obj.pruning_initial_sparsity,
-                  final_sparsity=flags_obj.pruning_final_sparsity,
-                  begin_step=flags_obj.pruning_begin_step,
-                  end_step=flags_obj.pruning_end_step,
-                  frequency=flags_obj.pruning_frequency),
-      }
-      model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)
 
     # TODO(b/138957587): Remove when force_v2_in_keras_compile is on longer
     # a valid arg for this model. Also remove as a valid flag.
@@ -242,14 +191,16 @@ def run(flags_obj):
                    if flags_obj.report_accuracy_metrics else None),
           run_eagerly=flags_obj.run_eagerly)
 
+  steps_per_epoch = (
+      imagenet_preprocessing.NUM_IMAGES['train'] // flags_obj.batch_size)
   train_epochs = flags_obj.train_epochs
 
-  callbacks = common.get_callbacks(
-      steps_per_epoch=steps_per_epoch,
-      learning_rate_schedule_fn=learning_rate_schedule_fn,
-      pruning_method=flags_obj.pruning_method,
-      enable_checkpoint_and_export=flags_obj.enable_checkpoint_and_export,
-      model_dir=flags_obj.model_dir)
+  callbacks = common.get_callbacks(steps_per_epoch,
+                                   common.learning_rate_schedule)
+  if flags_obj.enable_checkpoint_and_export:
+    ckpt_full_path = os.path.join(flags_obj.model_dir, 'model.ckpt-{epoch:04d}')
+    callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_full_path,
+                                                        save_weights_only=True))
 
   # if mutliple epochs, ignore the train_steps flag.
   if train_epochs <= 1 and flags_obj.train_steps:
@@ -285,23 +236,20 @@ def run(flags_obj):
                       validation_data=validation_data,
                       validation_freq=flags_obj.epochs_between_evals,
                       verbose=2)
-
-  eval_output = None
-  if not flags_obj.skip_eval:
-    eval_output = model.evaluate(eval_input_dataset,
-                                 steps=num_eval_steps,
-                                 verbose=2)
-
-  if flags_obj.pruning_method == 'polynomial_decay':
-    model = tfmot.sparsity.keras.strip_pruning(model)
   if flags_obj.enable_checkpoint_and_export:
     if dtype == tf.bfloat16:
-      logging.warning('Keras model.save does not support bfloat16 dtype.')
+      logging.warning("Keras model.save does not support bfloat16 dtype.")
     else:
       # Keras model.save assumes a float32 input designature.
       export_path = os.path.join(flags_obj.model_dir, 'saved_model')
       model.save(export_path, include_optimizer=False)
 
+  eval_output = None
+  if not flags_obj.skip_eval:
+    eval_output = model.evaluate(eval_input_dataset,
+                                 steps=num_eval_steps,
+                                 verbose=2)
+
   if not strategy and flags_obj.explicit_gpu_placement:
     no_dist_strat_device.__exit__()
 
@@ -311,7 +259,6 @@ def run(flags_obj):
 
 def define_imagenet_keras_flags():
   common.define_keras_flags()
-  common.define_pruning_flags()
   flags_core.set_defaults()
   flags.adopt_module_key_flags(common)
 
diff --git a/official/vision/image_classification/resnet_imagenet_test.py b/official/vision/image_classification/resnet_imagenet_test.py
index ddd2232c..dc7c2df8 100644
--- a/official/vision/image_classification/resnet_imagenet_test.py
+++ b/official/vision/image_classification/resnet_imagenet_test.py
@@ -18,7 +18,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl.testing import parameterized
 import tensorflow as tf
 
 from tensorflow.python.eager import context
@@ -28,45 +27,14 @@ from official.vision.image_classification import imagenet_preprocessing
 from official.vision.image_classification import resnet_imagenet_main
 
 
-@parameterized.parameters(
-    "resnet",
-    "resnet_polynomial_decay",
-    "mobilenet",
-    "mobilenet_polynomial_decay")
 class KerasImagenetTest(tf.test.TestCase):
-  """Unit tests for Keras Models with ImageNet."""
-  _extra_flags_dict = {
-      "resnet": [
-          "-batch_size", "4",
-          "-train_steps", "1",
-          "-use_synthetic_data", "true"
-          "-model", "resnet50_v1.5",
-          "-optimizer", "resnet50_default",
-      ],
-      "resnet_polynomial_decay": [
-          "-batch_size", "4",
-          "-train_steps", "1",
-          "-use_synthetic_data", "true",
-          "-model", "resnet50_v1.5",
-          "-optimizer", "resnet50_default",
-          "-pruning_method", "polynomial_decay",
-      ],
-      "mobilenet": [
-          "-batch_size", "4",
-          "-train_steps", "1",
-          "-use_synthetic_data", "true"
-          "-model", "mobilenet",
-          "-optimizer", "mobilenet_default",
-      ],
-      "mobilenet_polynomial_decay": [
-          "-batch_size", "4",
-          "-train_steps", "1",
-          "-use_synthetic_data", "true",
-          "-model", "mobilenet",
-          "-optimizer", "mobilenet_default",
-          "-pruning_method", "polynomial_decay",
-      ],
-  }
+  """Unit tests for Keras ResNet with ImageNet."""
+
+  _extra_flags = [
+      "-batch_size", "4",
+      "-train_steps", "1",
+      "-use_synthetic_data", "true"
+  ]
   _tempdir = None
 
   @classmethod
@@ -82,7 +50,7 @@ class KerasImagenetTest(tf.test.TestCase):
     super(KerasImagenetTest, self).tearDown()
     tf.io.gfile.rmtree(self.get_temp_dir())
 
-  def test_end_to_end_no_dist_strat(self, flags_key):
+  def test_end_to_end_no_dist_strat(self):
     """Test Keras model with 1 GPU, no distribution strategy."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -91,7 +59,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-distribution_strategy", "off",
         "-data_format", "channels_last",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -99,14 +67,14 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_graph_no_dist_strat(self, flags_key):
+  def test_end_to_end_graph_no_dist_strat(self):
     """Test Keras model in legacy graph mode with 1 GPU, no dist strat."""
     extra_flags = [
         "-enable_eager", "false",
         "-distribution_strategy", "off",
         "-data_format", "channels_last",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -114,7 +82,7 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_1_gpu(self, flags_key):
+  def test_end_to_end_1_gpu(self):
     """Test Keras model with 1 GPU."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -130,7 +98,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-data_format", "channels_last",
         "-enable_checkpoint_and_export", "1",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -138,7 +106,7 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_1_gpu_fp16(self, flags_key):
+  def test_end_to_end_1_gpu_fp16(self):
     """Test Keras model with 1 GPU and fp16."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -154,10 +122,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-distribution_strategy", "mirrored",
         "-data_format", "channels_last",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
-
-    if "polynomial_decay" in extra_flags:
-      self.skipTest("Pruning with fp16 is not currently supported.")
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -165,7 +130,8 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_2_gpu(self, flags_key):
+
+  def test_end_to_end_2_gpu(self):
     """Test Keras model with 2 GPUs."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -179,7 +145,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-num_gpus", "2",
         "-distribution_strategy", "mirrored",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -187,7 +153,7 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_xla_2_gpu(self, flags_key):
+  def test_end_to_end_xla_2_gpu(self):
     """Test Keras model with XLA and 2 GPUs."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -202,7 +168,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-enable_xla", "true",
         "-distribution_strategy", "mirrored",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -210,7 +176,7 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_2_gpu_fp16(self, flags_key):
+  def test_end_to_end_2_gpu_fp16(self):
     """Test Keras model with 2 GPUs and fp16."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -225,10 +191,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-dtype", "fp16",
         "-distribution_strategy", "mirrored",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
-
-    if "polynomial_decay" in extra_flags:
-      self.skipTest("Pruning with fp16 is not currently supported.")
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
@@ -236,7 +199,7 @@ class KerasImagenetTest(tf.test.TestCase):
         extra_flags=extra_flags
     )
 
-  def test_end_to_end_xla_2_gpu_fp16(self, flags_key):
+  def test_end_to_end_xla_2_gpu_fp16(self):
     """Test Keras model with XLA, 2 GPUs and fp16."""
     config = keras_utils.get_config_proto_v1()
     tf.compat.v1.enable_eager_execution(config=config)
@@ -252,10 +215,7 @@ class KerasImagenetTest(tf.test.TestCase):
         "-enable_xla", "true",
         "-distribution_strategy", "mirrored",
     ]
-    extra_flags = extra_flags + self._extra_flags_dict[flags_key]
-
-    if "polynomial_decay" in extra_flags:
-      self.skipTest("Pruning with fp16 is not currently supported.")
+    extra_flags = extra_flags + self._extra_flags
 
     integration.run_synthetic(
         main=resnet_imagenet_main.run,
