commit 6f7a682da07be93b49b3e9853a6d7af2cd5ac83b
Author: Chen Chen <chendouble@google.com>
Date:   Mon May 18 11:16:19 2020 -0700

    Internal Change
    
    PiperOrigin-RevId: 312116965

diff --git a/official/nlp/modeling/layers/transformer_scaffold.py b/official/nlp/modeling/layers/transformer_scaffold.py
index bb2ee95e..51d8236c 100644
--- a/official/nlp/modeling/layers/transformer_scaffold.py
+++ b/official/nlp/modeling/layers/transformer_scaffold.py
@@ -133,17 +133,6 @@ class TransformerScaffold(tf.keras.layers.Layer):
         attention_cfg = self._attention_cfg
       self._attention_layer = self._attention_cls(**attention_cfg)
 
-    self._attention_output_dense = dense_einsum.DenseEinsum(
-        output_shape=hidden_size,
-        num_summed_dimensions=2,
-        kernel_initializer=self._kernel_initializer,
-        bias_initializer=self._bias_initializer,
-        kernel_regularizer=self._kernel_regularizer,
-        bias_regularizer=self._bias_regularizer,
-        activity_regularizer=self._activity_regularizer,
-        kernel_constraint=self._kernel_constraint,
-        bias_constraint=self._bias_constraint,
-        name="self_attention_output")
     self._attention_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
     # Use float32 in layernorm for numeric stability.
     # It is probably safe in mixed_float16, but we haven't validated this yet.
