commit 85bdf764e7ae6eaba594eb6cea3384eb16e39666
Author: saberkun <saberkun@users.noreply.github.com>
Date:   Wed May 22 12:57:31 2019 -0700

    Merged commit includes the following changes: (#6856)
    
    249500988  by hongkuny<hongkuny@google.com>:
    
        Lints
    
    --
    
    PiperOrigin-RevId: 249500988

diff --git a/official/bert/bert_models.py b/official/bert/bert_models.py
index a623348b..5a53a99e 100644
--- a/official/bert/bert_models.py
+++ b/official/bert/bert_models.py
@@ -85,6 +85,7 @@ class BertPretrainLayer(tf.keras.layers.Layer):
           stddev=self.config.initializer_range)
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.lm_dense = tf.keras.layers.Dense(
         self.config.hidden_size,
         activation=modeling.get_activation(self.config.hidden_act),
@@ -108,6 +109,7 @@ class BertPretrainLayer(tf.keras.layers.Layer):
     return super(BertPretrainLayer, self).__call__(inputs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     unpacked_inputs = modeling.unpack_inputs(inputs)
     pooled_output = unpacked_inputs[0]
     sequence_output = unpacked_inputs[1]
@@ -151,6 +153,7 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
   def _add_metrics(self, lm_output, lm_labels, lm_label_weights,
                    lm_per_example_loss, sentence_output, sentence_labels,
                    sentence_per_example_loss):
+    """Adds metrics."""
     masked_lm_accuracy = tf.keras.metrics.sparse_categorical_accuracy(
         lm_labels, lm_output)
     masked_lm_accuracy = tf.reduce_mean(masked_lm_accuracy * lm_label_weights)
@@ -173,6 +176,7 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
         next_sentence_mean_loss, name='next_sentence_loss', aggregation='mean')
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     unpacked_inputs = modeling.unpack_inputs(inputs)
     lm_output = unpacked_inputs[0]
     sentence_output = unpacked_inputs[1]
@@ -284,11 +288,13 @@ class BertSquadLogitsLayer(tf.keras.layers.Layer):
     self.float_type = float_type
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.final_dense = tf.keras.layers.Dense(
         units=2, kernel_initializer=self.initializer, name='final_dense')
     super(BertSquadLogitsLayer, self).build(unused_input_shapes)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     sequence_output = inputs
 
     input_shape = sequence_output.shape.as_list()
diff --git a/official/bert/classifier_data_lib.py b/official/bert/classifier_data_lib.py
index 1b651789..01607a3d 100644
--- a/official/bert/classifier_data_lib.py
+++ b/official/bert/classifier_data_lib.py
@@ -366,13 +366,13 @@ def convert_single_example(ex_index, example, label_list, max_seq_length,
   label_id = label_map[example.label]
   if ex_index < 5:
     logging.info("*** Example ***")
-    logging.info("guid: %s" % (example.guid))
-    logging.info("tokens: %s" %
-                    " ".join([tokenization.printable_text(x) for x in tokens]))
-    logging.info("input_ids: %s" % " ".join([str(x) for x in input_ids]))
-    logging.info("input_mask: %s" % " ".join([str(x) for x in input_mask]))
-    logging.info("segment_ids: %s" % " ".join([str(x) for x in segment_ids]))
-    logging.info("label: %s (id = %d)" % (example.label, label_id))
+    logging.info("guid: %s", (example.guid))
+    logging.info("tokens: %s",
+                 " ".join([tokenization.printable_text(x) for x in tokens]))
+    logging.info("input_ids: %s", " ".join([str(x) for x in input_ids]))
+    logging.info("input_mask: %s", " ".join([str(x) for x in input_mask]))
+    logging.info("segment_ids: %s", " ".join([str(x) for x in segment_ids]))
+    logging.info("label: %s (id = %d)", example.label, label_id)
 
   feature = InputFeatures(
       input_ids=input_ids,
@@ -392,7 +392,7 @@ def file_based_convert_examples_to_features(examples, label_list,
 
   for (ex_index, example) in enumerate(examples):
     if ex_index % 10000 == 0:
-      logging.info("Writing example %d of %d" % (ex_index, len(examples)))
+      logging.info("Writing example %d of %d", ex_index, len(examples))
 
     feature = convert_single_example(ex_index, example, label_list,
                                      max_seq_length, tokenizer)
diff --git a/official/bert/create_finetuning_data.py b/official/bert/create_finetuning_data.py
index 197058e0..4a41b627 100644
--- a/official/bert/create_finetuning_data.py
+++ b/official/bert/create_finetuning_data.py
@@ -64,12 +64,14 @@ flags.DEFINE_string("vocab_file", None,
 
 flags.DEFINE_string(
     "train_data_output_path", None,
-    "The path in which generated training input data will be written as tf records."
+    "The path in which generated training input data will be written as tf"
+    " records."
 )
 
 flags.DEFINE_string(
     "eval_data_output_path", None,
-    "The path in which generated training input data will be written as tf records."
+    "The path in which generated training input data will be written as tf"
+    " records."
 )
 
 flags.DEFINE_string("meta_data_file_path", None,
diff --git a/official/bert/modeling.py b/official/bert/modeling.py
index 7472be32..80172145 100644
--- a/official/bert/modeling.py
+++ b/official/bert/modeling.py
@@ -151,6 +151,7 @@ class BertModel(tf.keras.layers.Layer):
     self.float_type = float_type
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.embedding_lookup = EmbeddingLookup(
         vocab_size=self.config.vocab_size,
         embedding_size=self.config.hidden_size,
@@ -192,6 +193,7 @@ class BertModel(tf.keras.layers.Layer):
     return super(BertModel, self).__call__(inputs, **kwargs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     unpacked_inputs = unpack_inputs(inputs)
     input_word_ids = unpacked_inputs[0]
     input_mask = unpacked_inputs[1]
@@ -232,6 +234,7 @@ class EmbeddingLookup(tf.keras.layers.Layer):
     self.initializer_range = initializer_range
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.embeddings = self.add_weight(
         "embeddings",
         shape=[self.vocab_size, self.embedding_size],
@@ -240,6 +243,7 @@ class EmbeddingLookup(tf.keras.layers.Layer):
     super(EmbeddingLookup, self).build(unused_input_shapes)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     input_shape = get_shape_list(inputs)
     flat_input = tf.reshape(inputs, [-1])
     output = tf.gather(self.embeddings, flat_input)
@@ -271,6 +275,7 @@ class EmbeddingPostprocessor(tf.keras.layers.Layer):
                        "`token_type_vocab_size` must be specified.")
 
   def build(self, input_shapes):
+    """Implements build() for the layer."""
     (word_embeddings_shape, _) = input_shapes
     width = word_embeddings_shape.as_list()[-1]
     self.type_embeddings = None
@@ -299,6 +304,7 @@ class EmbeddingPostprocessor(tf.keras.layers.Layer):
     return super(EmbeddingPostprocessor, self).__call__(inputs, **kwargs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     unpacked_inputs = unpack_inputs(inputs)
     word_embeddings = unpacked_inputs[0]
     token_type_ids = unpacked_inputs[1]
@@ -378,6 +384,7 @@ class Attention(tf.keras.layers.Layer):
     self.backward_compatible = backward_compatible
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.query_dense = self._projection_dense_layer("query")
     self.key_dense = self._projection_dense_layer("key")
     self.value_dense = self._projection_dense_layer("value")
@@ -403,6 +410,7 @@ class Attention(tf.keras.layers.Layer):
     return super(Attention, self).__call__(inputs, **kwargs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     (from_tensor, to_tensor, attention_mask) = unpack_inputs(inputs)
 
     # Scalar dimensions referenced here:
@@ -453,6 +461,7 @@ class Attention(tf.keras.layers.Layer):
     return context_tensor
 
   def _projection_dense_layer(self, name):
+    """A helper to define a projection layer."""
     return Dense3D(
         num_attention_heads=self.num_attention_heads,
         size_per_head=self.size_per_head,
@@ -507,6 +516,7 @@ class Dense3D(tf.keras.layers.Layer):
     return [self.num_attention_heads, self.size_per_head]
 
   def build(self, input_shape):
+    """Implements build() for the layer."""
     dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())
     if not (dtype.is_floating or dtype.is_complex):
       raise TypeError("Unable to build `Dense` layer with non-floating point "
@@ -586,6 +596,7 @@ class Dense2DProjection(tf.keras.layers.Layer):
     self.activation = activation
 
   def build(self, input_shape):
+    """Implements build() for the layer."""
     dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())
     if not (dtype.is_floating or dtype.is_complex):
       raise TypeError("Unable to build `Dense` layer with non-floating point "
@@ -661,6 +672,7 @@ class TransformerBlock(tf.keras.layers.Layer):
     self.attention_head_size = int(self.hidden_size / self.num_attention_heads)
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.attention_layer = Attention(
         num_attention_heads=self.num_attention_heads,
         size_per_head=self.attention_head_size,
@@ -699,6 +711,7 @@ class TransformerBlock(tf.keras.layers.Layer):
     return super(TransformerBlock, self).__call__(inputs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     (input_tensor, attention_mask) = unpack_inputs(inputs)
     attention_output = self.attention_layer(
         from_tensor=input_tensor,
@@ -751,6 +764,7 @@ class Transformer(tf.keras.layers.Layer):
     self.backward_compatible = backward_compatible
 
   def build(self, unused_input_shapes):
+    """Implements build() for the layer."""
     self.layers = []
     for i in range(self.num_hidden_layers):
       self.layers.append(
@@ -775,6 +789,7 @@ class Transformer(tf.keras.layers.Layer):
     return super(Transformer, self).__call__(inputs=inputs)
 
   def call(self, inputs):
+    """Implements call() for the layer."""
     unpacked_inputs = unpack_inputs(inputs)
     input_tensor = unpacked_inputs[0]
     attention_mask = unpacked_inputs[1]
@@ -832,6 +847,7 @@ def unpack_inputs(inputs):
 
 
 def is_special_none_tensor(tensor):
+  """Checks if a tensor is a special None Tensor."""
   return tensor.shape.ndims == 0 and tensor.dtype == tf.int32
 
 
