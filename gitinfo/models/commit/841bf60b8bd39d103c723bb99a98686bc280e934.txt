commit 841bf60b8bd39d103c723bb99a98686bc280e934
Author: Hongkun Yu <hongkuny@google.com>
Date:   Mon Sep 16 11:10:14 2019 -0700

    Update readme to emphasize 'global batch size'.
    
    PiperOrigin-RevId: 269376599

diff --git a/official/transformer/v2/README.md b/official/transformer/v2/README.md
index ec3f1b0a..ebc6ccf4 100644
--- a/official/transformer/v2/README.md
+++ b/official/transformer/v2/README.md
@@ -96,6 +96,11 @@ tensorboard --logdir=$MODEL_DIR
    Users need to adjust `batch_size` and `num_gpus` to get good performance
    running multiple GPUs.
 
+   **Note that:**
+   when using multiple GPUs or TPUs, this is the global batch size for all
+   devices. For example, if the batch size is `4096*4` and there are 4 devices,
+   each device will take 4096 tokens as a batch budget.
+
    Command to run:
    ```
    python3 transformer_main.py --data_dir=$DATA_DIR --model_dir=$MODEL_DIR \
