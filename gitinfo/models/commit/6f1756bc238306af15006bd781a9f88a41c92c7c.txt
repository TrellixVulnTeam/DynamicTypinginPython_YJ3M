commit 6f1756bc238306af15006bd781a9f88a41c92c7c
Author: Zhichao Lu <lzc@google.com>
Date:   Mon Mar 19 16:12:58 2018 -0700

    Enabling both train and eval image summaries. Note that eval summaries are not created tf-learn environment. To get them to show up, added the summary image into the eval_metric_ops.
    
    PiperOrigin-RevId: 189658259

diff --git a/research/object_detection/g3doc/running_pets.md b/research/object_detection/g3doc/running_pets.md
index 04220c96..013c5579 100644
--- a/research/object_detection/g3doc/running_pets.md
+++ b/research/object_detection/g3doc/running_pets.md
@@ -37,9 +37,6 @@ environment variable below:
 export YOUR_GCS_BUCKET=${YOUR_GCS_BUCKET}
 ```
 
-It is also possible to run locally by following 
-[the running locally instructions](running_locally.md).
-
 ## Installing Tensorflow and the Tensorflow Object Detection API
 
 Please run through the [installation instructions](installation.md) to install
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 93830853..0d4b323a 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -184,6 +184,8 @@ def create_train_input_fn(train_config, train_input_config,
         features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]
           int32 tensor representing the true image shapes, as preprocessed
           images could be padded.
+        features[fields.InputDataFields.image] (optional) is a
+          [batch_size, H, W, C] float32 tensor with original images.
       labels: Dictionary of groundtruth tensors.
         labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]
           int32 tensor indicating the number of groundtruth boxes.
@@ -233,7 +235,8 @@ def create_train_input_fn(train_config, train_input_config,
         transform_input_data, model_preprocess_fn=model.preprocess,
         image_resizer_fn=image_resizer_fn,
         num_classes=config_util.get_number_of_classes(model_config),
-        data_augmentation_fn=data_augmentation_fn)
+        data_augmentation_fn=data_augmentation_fn,
+        retain_original_image=train_config.retain_original_images)
     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
         train_input_config,
         transform_input_data_fn=transform_data_fn,
@@ -252,6 +255,9 @@ def create_train_input_fn(train_config, train_input_config,
         fields.InputDataFields.true_image_shape: tensor_dict[
             fields.InputDataFields.true_image_shape]
     }
+    if fields.InputDataFields.original_image in tensor_dict:
+      features[fields.InputDataFields.original_image] = tensor_dict[
+          fields.InputDataFields.original_image]
 
     labels = {
         fields.InputDataFields.num_groundtruth_boxes: tensor_dict[
@@ -345,7 +351,7 @@ def create_eval_input_fn(eval_config, eval_input_config, model_config):
         image_resizer_fn=image_resizer_fn,
         num_classes=num_classes,
         data_augmentation_fn=None,
-        retain_original_image=True)
+        retain_original_image=eval_config.retain_original_images)
     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
         eval_input_config,
         transform_input_data_fn=transform_data_fn,
diff --git a/research/object_detection/leakr_badfiles.dic b/research/object_detection/leakr_badfiles.dic
new file mode 100644
index 00000000..41b2186c
--- /dev/null
+++ b/research/object_detection/leakr_badfiles.dic
@@ -0,0 +1,10 @@
+;; We use bazel, so BUILD files are required.
+$RE:BUILD$;0;Allow BUILD file
+
+;; Android Studio auto generated files
+$RE:\.class$!!.*org\/gradle\/cli\/.*\.class!!.*org\/gradle\/wrapper\/.*\.class;0;Allow gradle cli classes
+
+;; We are shipping source code so override the source code entries in
+;; the default dictionary.
+$RE:\.(?:cc|cpp)(?:.bak|)$;0;Allow C++ source code
+$RE:\.proto(?:.bak|)$;0;Allow Proto file
diff --git a/research/object_detection/leakr_badwords.dic b/research/object_detection/leakr_badwords.dic
new file mode 100644
index 00000000..2d0baafb
--- /dev/null
+++ b/research/object_detection/leakr_badwords.dic
@@ -0,0 +1,55 @@
+;; Bad words to avoid
+adbrain;2;
+bamm;2;
+cloudml;2;
+mactruck;2;
+seastar;2;
+$RE:(^|[\W])((?i)borg(?-i))($|[\W]);2;Use regex as borg is a common letter combo
+blaze;2;
+mognet;2;
+/cns/;2;
+
+;; XLA and Jellyfish-related terms.
+techila;2;
+jellyfish;2;
+jelly fish;2;
+dragonfish;2;
+dragon fish;2;
+tensorcore;2;
+tensor core;2;
+barnacore;2;
+barna core;2;
+barnacle;2;
+nautilus;2;
+polyp;2;
+XLA:JF;2;
+
+;; OSS licensing wants this to be true
+google3;2;
+
+;; We should attempt to remove all/most occurrences of brain:
+brain;1;
+
+;; make sure internal paths don't make it out
+net/proto2;2;
+
+;; We are allowing todos (while scrubbing out some people's user names)
+todo:;0;
+todo(;0;
+
+;; Override default dictionary and make the following errors instead
+;; of warnings.
+//go/;2;
+//goto/;2;
+//cr/;2;
+//cl/;2;
+//test/;2;
+//sites/;2;
+a/google.com;2;
+corp.google.com;2;
+.googleplex.com;2;
+.mtv;2;
+sandbox.;2;
+
+;; Python 3 compatibility
+basestring;2;
diff --git a/research/object_detection/model.py b/research/object_detection/model.py
index 77c85e8b..4c91638e 100644
--- a/research/object_detection/model.py
+++ b/research/object_detection/model.py
@@ -341,17 +341,16 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
       }
 
     eval_metric_ops = None
-    if mode == tf.estimator.ModeKeys.EVAL:
-      # Detection summaries during eval.
+    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
       class_agnostic = (fields.DetectionResultFields.detection_classes
                         not in detections)
       groundtruth = _get_groundtruth_data(detection_model, class_agnostic)
       use_original_images = fields.InputDataFields.original_image in features
-      eval_images = (
+      original_images = (
           features[fields.InputDataFields.original_image] if use_original_images
           else features[fields.InputDataFields.image])
       eval_dict = eval_util.result_dict_for_single_example(
-          eval_images[0:1],
+          original_images[0:1],
           features[inputs.HASH_KEY][0],
           detections,
           groundtruth,
@@ -368,16 +367,19 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
             vis_utils.draw_side_by_side_evaluation_image(
                 eval_dict, category_index, max_boxes_to_draw=20,
                 min_score_thresh=0.2))
-        tf.summary.image('Detections_Left_Groundtruth_Right',
-                         detection_and_groundtruth)
-
-      # Eval metrics on a single image.
-      eval_metrics = eval_config.metrics_set
-      if not eval_metrics:
-        eval_metrics = ['coco_detection_metrics']
-      eval_metric_ops = eval_util.get_eval_metric_ops_for_evaluators(
-          eval_metrics, category_index.values(), eval_dict,
-          include_metrics_per_category=False)
+        img_summary = tf.summary.image('Detections_Left_Groundtruth_Right',
+                                       detection_and_groundtruth)
+
+      if mode == tf.estimator.ModeKeys.EVAL:
+        # Eval metrics on a single example.
+        eval_metrics = eval_config.metrics_set
+        if not eval_metrics:
+          eval_metrics = ['coco_detection_metrics']
+        eval_metric_ops = eval_util.get_eval_metric_ops_for_evaluators(
+            eval_metrics, category_index.values(), eval_dict,
+            include_metrics_per_category=False)
+        eval_metric_ops['Detections_Left_Groundtruth_Right'] = (
+            img_summary, tf.no_op())
 
     if use_tpu:
       return tf.contrib.tpu.TPUEstimatorSpec(
diff --git a/research/object_detection/protos/eval.proto b/research/object_detection/protos/eval.proto
index 9410b6f9..f216c91a 100644
--- a/research/object_detection/protos/eval.proto
+++ b/research/object_detection/protos/eval.proto
@@ -68,4 +68,8 @@ message EvalConfig {
   // Whether to keep image identifier in filename when exported to
   // visualization_export_dir.
   optional bool keep_image_id_for_visualization_export = 19 [default=false];
+
+  // Whether to retain original images (i.e. not pre-processed) in the tensor
+  // dictionary, so that they can be displayed in Tensorboard.
+  optional bool retain_original_images = 23 [default=true];
 }
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index d2786787..e8bf871b 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -94,4 +94,9 @@ message TrainConfig {
   // Whether to remove padding along `num_boxes` dimension of the groundtruth
   // tensors.
   optional bool unpad_groundtruth_tensors = 21 [default=true];
+
+  // Whether to retain original images (i.e. not pre-processed) in the tensor
+  // dictionary, so that they can be displayed in Tensorboard. Note that this
+  // will lead to a larger memory footprint.
+  optional bool retain_original_images = 23 [default=false];
 }
