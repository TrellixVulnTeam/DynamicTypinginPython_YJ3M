commit d723e734cf7c93a531575c85c738c841843b221e
Author: Alexander Gorban <gorban@google.com>
Date:   Fri Oct 20 08:43:41 2017 -0700

    Fix all deprecation warnings.
    
    1. Update README.md
    2. argmax, use axis instead of deminsion
    3. use tf.profiler.profile instead of model_analyzer.print_model_analysis

diff --git a/research/attention_ocr/README.md b/research/attention_ocr/README.md
index 65a78789..2e9c0fce 100644
--- a/research/attention_ocr/README.md
+++ b/research/attention_ocr/README.md
@@ -34,7 +34,7 @@ pip install --upgrade tensorflow-gpu
 2. At least 158GB of free disk space to download the FSNS dataset:
 
 ```
-cd models/attention_ocr/python/datasets
+cd research/attention_ocr/python/datasets
 aria2c -c -j 20 -i ../../../street/python/fsns_urls.txt
 cd ..
 ```
@@ -50,7 +50,7 @@ cd ..
 To run all unit tests:
 
 ```
-cd models/attention_ocr/python
+cd research/attention_ocr/python
 python -m unittest discover -p  '*_test.py'
 ```
 
diff --git a/research/attention_ocr/python/model.py b/research/attention_ocr/python/model.py
index 4abd6387..7c49d864 100644
--- a/research/attention_ocr/python/model.py
+++ b/research/attention_ocr/python/model.py
@@ -299,7 +299,7 @@ class Model(object):
           with shape [batch_size x seq_length].
     """
     log_prob = utils.logits_to_log_prob(chars_logit)
-    ids = tf.to_int32(tf.argmax(log_prob, dimension=2), name='predicted_chars')
+    ids = tf.to_int32(tf.argmax(log_prob, axis=2), name='predicted_chars')
     mask = tf.cast(
         slim.one_hot_encoding(ids, self._params.num_char_classes), tf.bool)
     all_scores = tf.nn.softmax(chars_logit)
diff --git a/research/attention_ocr/python/model_test.py b/research/attention_ocr/python/model_test.py
index 987f9504..c3fd5218 100644
--- a/research/attention_ocr/python/model_test.py
+++ b/research/attention_ocr/python/model_test.py
@@ -19,7 +19,6 @@ import numpy as np
 import string
 import tensorflow as tf
 from tensorflow.contrib import slim
-from tensorflow.contrib.tfprof import model_analyzer
 
 import model
 import data_provider
@@ -127,9 +126,9 @@ class ModelTest(tf.test.TestCase):
     ocr_model = self.create_model()
     ocr_model.create_base(images=self.fake_images, labels_one_hot=None)
     with self.test_session() as sess:
-      tfprof_root = model_analyzer.print_model_analysis(
+      tfprof_root = tf.profiler.profile(
           sess.graph,
-          tfprof_options=model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
+          options=tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())
 
       model_size_bytes = 4 * tfprof_root.total_parameters
       self.assertLess(model_size_bytes, 1 * 2**30)
diff --git a/research/attention_ocr/python/sequence_layers.py b/research/attention_ocr/python/sequence_layers.py
index 6e1e8493..9261f210 100644
--- a/research/attention_ocr/python/sequence_layers.py
+++ b/research/attention_ocr/python/sequence_layers.py
@@ -216,7 +216,7 @@ class SequenceLayerBase(object):
     Returns:
       A tensor with shape [batch_size, num_char_classes]
     """
-    prediction = tf.argmax(logit, dimension=1)
+    prediction = tf.argmax(logit, axis=1)
     return slim.one_hot_encoding(prediction, self._params.num_char_classes)
 
   def get_input(self, prev, i):
