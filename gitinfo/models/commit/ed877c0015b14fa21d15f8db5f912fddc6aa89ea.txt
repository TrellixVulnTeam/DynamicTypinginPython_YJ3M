commit ed877c0015b14fa21d15f8db5f912fddc6aa89ea
Author: Zhichao Lu <lzc@google.com>
Date:   Fri Mar 23 16:19:25 2018 -0700

    Expose read block length via input reader proto so it can be tuned.
    
    PiperOrigin-RevId: 190292280

diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index 4db5f023..451bc1f4 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -51,6 +51,9 @@ message InputReader {
   // Number of reader instances to create.
   optional uint32 num_readers = 6 [default=32];
 
+  // Number of records to read from each reader at once.
+  optional uint32 read_block_length = 15 [default=32];
+
   // Number of decoded records to prefetch before batching.
   optional uint32 prefetch_size = 13 [default = 512];
 
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index 5e584e05..4100076d 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -127,7 +127,8 @@ def read_dataset(file_read_func, decode_func, input_files, config):
 
   records_dataset = filename_dataset.apply(
       tf.contrib.data.parallel_interleave(
-          file_read_func, cycle_length=config.num_readers, sloppy=True))
+          file_read_func, cycle_length=config.num_readers,
+          block_length=config.read_block_length, sloppy=True))
   if config.shuffle:
     records_dataset.shuffle(config.shuffle_buffer_size)
   tensor_dataset = records_dataset.map(
