commit e1a1328c92c0004a7f2ce62d4f60e9ae2ab7d4d4
Author: Mark Daoust <markdaoust@google.com>
Date:   Tue Dec 19 07:18:14 2017 -0800

    slice fix for 1.5

diff --git a/samples/outreach/blogs/Blog_Custom_Estimators.py b/samples/outreach/blogs/Blog_Custom_Estimators.py
index 9363a1f3..f867e9b6 100644
--- a/samples/outreach/blogs/Blog_Custom_Estimators.py
+++ b/samples/outreach/blogs/Blog_Custom_Estimators.py
@@ -66,7 +66,7 @@ feature_names = [
 def my_input_fn(file_path, repeat_count=1, shuffle_count=1):
     def decode_csv(line):
         parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])
-        label = parsed_line[-1:]  # Last element is the label
+        label = parsed_line[-1]  # Last element is the label
         del parsed_line[-1]  # Delete last element
         features = parsed_line  # Everything but last elements are the features
         d = dict(zip(feature_names, features)), label
@@ -136,9 +136,7 @@ def my_model_fn(
 
     # Evaluation and Training mode
 
-    # To calculate the loss, we need to convert our labels
-    # Our input labels have shape: [batch_size, 1]
-    labels = tf.squeeze(labels, 1)          # Convert to shape [batch_size]
+    # Calculate the loss
     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)
 
     # Calculate the accuracy between the true labels, and our predictions
