commit 0d5f3137200b382a5e1fcba338f9482858524b4b
Author: Edouard Fouch√© <edouard.fouche@hotmail.fr>
Date:   Fri Apr 28 19:36:40 2017 +0200

    Update lm_1b_eval.py
    
    Solve Python 3 compatibility:
    - replacing xrange to range
    - add .decode() after f.read() to get strings instead of bytes when reading text file

diff --git a/lm_1b/lm_1b_eval.py b/lm_1b/lm_1b_eval.py
index 65c48aa4..4d1a7c20 100644
--- a/lm_1b/lm_1b_eval.py
+++ b/lm_1b/lm_1b_eval.py
@@ -83,7 +83,7 @@ def _LoadModel(gd_file, ckpt_file):
   with tf.Graph().as_default():
     sys.stderr.write('Recovering graph.\n')
     with tf.gfile.FastGFile(gd_file, 'r') as f:
-      s = f.read()
+      s = f.read().decode()
       gd = tf.GraphDef()
       text_format.Merge(s, gd)
 
@@ -177,7 +177,7 @@ def _SampleModel(prefix_words, vocab):
 
   prefix = [vocab.word_to_id(w) for w in prefix_words.split()]
   prefix_char_ids = [vocab.word_to_char_ids(w) for w in prefix_words.split()]
-  for _ in xrange(FLAGS.num_samples):
+  for _ in range(FLAGS.num_samples):
     inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)
     char_ids_inputs = np.zeros(
         [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)
@@ -269,7 +269,7 @@ def _DumpSentenceEmbedding(sentence, vocab):
   inputs = np.zeros([BATCH_SIZE, NUM_TIMESTEPS], np.int32)
   char_ids_inputs = np.zeros(
       [BATCH_SIZE, NUM_TIMESTEPS, vocab.max_word_length], np.int32)
-  for i in xrange(len(word_ids)):
+  for i in range(len(word_ids)):
     inputs[0, 0] = word_ids[i]
     char_ids_inputs[0, 0, :] = char_ids[i]
 
