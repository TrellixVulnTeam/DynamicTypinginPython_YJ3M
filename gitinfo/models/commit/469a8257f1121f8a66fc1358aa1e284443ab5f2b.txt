commit 469a8257f1121f8a66fc1358aa1e284443ab5f2b
Author: Mark Daoust <markdaoust@google.com>
Date:   Mon May 14 11:38:24 2018 -0700

    Clarify softmax for probabilities

diff --git a/samples/core/get_started/eager.ipynb b/samples/core/get_started/eager.ipynb
index 2c9633e0..caba3931 100644
--- a/samples/core/get_started/eager.ipynb
+++ b/samples/core/get_started/eager.ipynb
@@ -553,7 +553,7 @@
       "source": [
         "For each example it returns a *[logit](https://developers.google.com/machine-learning/crash-course/glossary#logits)* score for each class. \n",
         "\n",
-        "You can calculate the probability that the model assigns to each class using the [`tf.nn.softmax`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function.\n",
+        "You can convert logits to probabilities for each class using the [`tf.nn.softmax`](https://www.tensorflow.org/api_docs/python/tf/nn/softmax) function.\n",
         "\n",
         "The model hasn't been trained yet, so these aren't very good predictions."
       ]
