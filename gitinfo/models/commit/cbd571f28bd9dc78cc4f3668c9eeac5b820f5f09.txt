commit cbd571f28bd9dc78cc4f3668c9eeac5b820f5f09
Author: Corey Lynch <coreylynch@google.com>
Date:   Wed Dec 6 18:33:35 2017 -0800

    Adding TCN.

diff --git a/CODEOWNERS b/CODEOWNERS
index 1d0b6002..d5422f82 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -32,6 +32,7 @@ research/slim/* @sguada @nathansilberman
 research/street/* @theraysmith
 research/swivel/* @waterson
 research/syntaxnet/* @calberti @andorardo @bogatyy @markomernick
+research/tcn/* @coreylynch @sermanet
 research/textsum/* @panyx0718 @peterjliu
 research/transformer/* @daviddao
 research/video_prediction/* @cbfinn
diff --git a/research/README.md b/research/README.md
index 5e631c3f..69bb810a 100644
--- a/research/README.md
+++ b/research/README.md
@@ -61,6 +61,7 @@ installation](https://www.tensorflow.org/install).
     using a Deep RNN.
 -   [swivel](swivel): the Swivel algorithm for generating word embeddings.
 -   [syntaxnet](syntaxnet): neural models of natural language syntax.
+-   [tcn](tcn): Self-supervised representation learning from multi-view video.
 -   [textsum](textsum): sequence-to-sequence with attention model for text
     summarization.
 -   [transformer](transformer): spatial transformer network, which allows the
diff --git a/research/tcn/BUILD b/research/tcn/BUILD
new file mode 100644
index 00000000..39297d4b
--- /dev/null
+++ b/research/tcn/BUILD
@@ -0,0 +1,213 @@
+package(default_visibility = [":internal"])
+
+licenses(["notice"])  # Apache 2.0
+
+exports_files(["LICENSE"])
+
+package_group(
+    name = "internal",
+    packages = [
+        "//tcn/...",
+    ],
+)
+
+py_binary(
+    name = "download_pretrained",
+    srcs = [
+        "download_pretrained.py",
+    ],
+)
+
+py_binary(
+    name = "generate_videos",
+    srcs = [
+        "generate_videos.py",
+    ],
+    main = "generate_videos.py",
+    deps = [
+        ":data_providers",
+        ":get_estimator",
+        ":util",
+    ],
+)
+
+py_test(
+    name = "svtcn_loss_test",
+    size = "medium",
+    srcs = [
+        "estimators/svtcn_loss.py",
+        "estimators/svtcn_loss_test.py",
+    ],
+    deps = [
+        ":util",
+    ],
+)
+
+py_library(
+    name = "data_providers",
+    srcs = [
+        "data_providers.py",
+    ],
+    deps = [
+        ":preprocessing",
+    ],
+)
+
+py_test(
+    name = "data_providers_test",
+    size = "large",
+    srcs = ["data_providers_test.py"],
+    deps = [
+        ":data_providers",
+    ],
+)
+
+py_library(
+    name = "preprocessing",
+    srcs = [
+        "preprocessing.py",
+    ],
+)
+
+py_binary(
+    name = "get_estimator",
+    srcs = [
+        "estimators/get_estimator.py",
+    ],
+    deps = [
+        ":mvtcn_estimator",
+        ":svtcn_estimator",
+    ],
+)
+
+py_binary(
+    name = "base_estimator",
+    srcs = [
+        "estimators/base_estimator.py",
+        "model.py",
+    ],
+    deps = [
+        ":data_providers",
+        ":util",
+    ],
+)
+
+py_library(
+    name = "util",
+    srcs = [
+        "utils/luatables.py",
+        "utils/progress.py",
+        "utils/util.py",
+    ],
+)
+
+py_binary(
+    name = "mvtcn_estimator",
+    srcs = [
+        "estimators/mvtcn_estimator.py",
+    ],
+    deps = [
+        ":base_estimator",
+    ],
+)
+
+py_binary(
+    name = "svtcn_estimator",
+    srcs = [
+        "estimators/svtcn_estimator.py",
+        "estimators/svtcn_loss.py",
+    ],
+    deps = [
+        ":base_estimator",
+    ],
+)
+
+py_binary(
+    name = "train",
+    srcs = [
+        "train.py",
+    ],
+    deps = [
+        ":data_providers",
+        ":get_estimator",
+        ":util",
+    ],
+)
+
+py_binary(
+    name = "labeled_eval",
+    srcs = [
+        "labeled_eval.py",
+    ],
+    deps = [
+        ":get_estimator",
+    ],
+)
+
+py_test(
+    name = "labeled_eval_test",
+    size = "small",
+    srcs = ["labeled_eval_test.py"],
+    deps = [
+        ":labeled_eval",
+    ],
+)
+
+py_binary(
+    name = "eval",
+    srcs = [
+        "eval.py",
+    ],
+    deps = [
+        ":get_estimator",
+    ],
+)
+
+py_binary(
+    name = "alignment",
+    srcs = [
+        "alignment.py",
+    ],
+    deps = [
+        ":get_estimator",
+    ],
+)
+
+py_binary(
+    name = "visualize_embeddings",
+    srcs = [
+        "visualize_embeddings.py",
+    ],
+    deps = [
+        ":data_providers",
+        ":get_estimator",
+        ":util",
+    ],
+)
+
+py_binary(
+    name = "webcam",
+    srcs = [
+        "dataset/webcam.py",
+    ],
+    main = "dataset/webcam.py",
+)
+
+py_binary(
+    name = "images_to_videos",
+    srcs = [
+        "dataset/images_to_videos.py",
+    ],
+    main = "dataset/images_to_videos.py",
+)
+
+py_binary(
+    name = "videos_to_tfrecords",
+    srcs = [
+        "dataset/videos_to_tfrecords.py",
+    ],
+    main = "dataset/videos_to_tfrecords.py",
+    deps = [
+        ":preprocessing",
+    ],
+)
diff --git a/research/tcn/CONTRIBUTING.md b/research/tcn/CONTRIBUTING.md
new file mode 100644
index 00000000..6053119c
--- /dev/null
+++ b/research/tcn/CONTRIBUTING.md
@@ -0,0 +1,23 @@
+# Contributing guidelines
+
+If you have created a model and would like to publish it here, please send us a
+pull request. For those just getting started with pull requests, GitHub has a
+[howto](https://help.github.com/articles/using-pull-requests/).
+
+The code for any model in this repository is licensed under the Apache License
+2.0.
+
+In order to accept our code, we have to make sure that we can publish your code:
+You have to sign a Contributor License Agreement (CLA).
+
+### Contributor License Agreements
+
+Please fill out either the individual or corporate Contributor License Agreement (CLA).
+
+  * If you are an individual writing original source code and you're sure you own the intellectual property, then you'll need to sign an [individual CLA](http://code.google.com/legal/individual-cla-v1.0.html).
+  * If you work for a company that wants to allow you to contribute your work, then you'll need to sign a [corporate CLA](http://code.google.com/legal/corporate-cla-v1.0.html).
+
+Follow either of the two links above to access the appropriate CLA and instructions for how to sign and return it. Once we receive it, we'll be able to accept your pull requests.
+
+***NOTE***: Only original source code from you and other people that have signed the CLA can be accepted into the repository.
+
diff --git a/research/tcn/LICENSE b/research/tcn/LICENSE
new file mode 100644
index 00000000..489485ec
--- /dev/null
+++ b/research/tcn/LICENSE
@@ -0,0 +1,203 @@
+Copyright 2016 The TensorFlow Authors.  All rights reserved.
+
+                                 Apache License
+                           Version 2.0, January 2004
+                        http://www.apache.org/licenses/
+
+   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+   1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright 2016, The Authors.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
\ No newline at end of file
diff --git a/research/tcn/README.md b/research/tcn/README.md
new file mode 100644
index 00000000..0bd8cd81
--- /dev/null
+++ b/research/tcn/README.md
@@ -0,0 +1,555 @@
+# Time Contrastive Networks
+
+This implements ["Time Contrastive Networks"](https://arxiv.org/abs/1704.06888),
+which is part of the larger [Self-Supervised Imitation
+Learning](https://sermanet.github.io/imitation/) project.
+
+![](https://sermanet.github.io/tcn/docs/figs/mvTCN.png)
+
+## Contacts
+
+Maintainers of TCN:
+
+*   Corey Lynch: [github](https://github.com/coreylynch),
+    [twitter](https://twitter.com/coreylynch)
+*   Pierre Sermanet: [github](https://github.com/sermanet),
+    [twitter](https://twitter.com/psermanet)
+
+## Contents
+
+*   [Getting Started](#getting-started)
+    *   [Install Dependencies](#install-dependencies)
+    *   [Download the Inception v3
+        Checkpoint](#download-pretrained-inceptionv3-checkpoint)
+    *   [Run all the tests](#run-all-the-tests)
+*   [Concepts](#concepts)
+    *   [Multi-view Webcam Video](#multi-view-webcam-video)
+    *   [Data Pipelines](#data-pipelines)
+    *   [Estimators](#estimators)
+    *   [Models](#models)
+    *   [Losses](#losses)
+    *   [Inference](#inference)
+    *   [Configuration](#configuration)
+    *   [Monitoring Training](#monitoring-training)
+        *   [KNN Classification Error](#knn-classification-error)
+        *   [KNN Classification Error](#multi-view-alignment)
+    *   [Visualization](#visualization)
+        *   [Nearest Neighbor Imitation
+            Videos](#nearest-neighbor-imitation-videos)
+        *   [PCA & T-SNE Visualization](#pca-t-sne-visualization)
+*   [Tutorial Part I: Collecting Multi-View Webcam
+    Videos](#tutorial-part-i-collecting-multi-view-webcam-videos)
+    *   [Collect Webcam Videos](#collect-webcam-videos)
+    *   [Create TFRecords](#create-tfrecords)
+*   [Tutorial Part II: Training, Evaluation, and
+    Visualization](#tutorial-part-ii-training-evaluation-and-visualization)
+    *   [Download Data](#download-data)
+    *   [Download the Inception v3
+        Checkpoint](#download-pretrained-inceptionv3-checkpoint)
+    *   [Define a Config](#define-a-config)
+    *   [Train](#train)
+    *   [Evaluate](#evaluate)
+    *   [Monitor training](#monior-training)
+    *   [Visualize](#visualize)
+        *   [Generate Imitation Videos](#generate-imitation-videos)
+        *   [Run PCA & T-SNE Visualization](#t-sne-pca-visualization)
+
+## Getting started
+
+### Install Dependencies
+
+*   [Tensorflow nightly build](https://pypi.python.org/pypi/tf-nightly-gpu) or
+    via `pip install tf-nightly-gpu`.
+*   [Bazel](http://bazel.io/docs/install.html)
+*   matplotlib
+*   sklearn
+*   opencv
+
+### Download Pretrained InceptionV3 Checkpoint
+
+Run the script that downloads the pretrained InceptionV3 checkpoint:
+
+```bash
+cd tensorflow-models/tcn
+python download_pretrained.py
+```
+
+### Run all the tests
+
+```bash
+bazel test :all
+```
+
+## Concepts
+
+### Multi-View Webcam Video
+
+We provide utilities to collect your own multi-view videos in dataset/webcam.py.
+See the [webcam tutorial](#tutorial-part-i-collecting-multi-view-webcam-videos)
+for an end to end example of how to collect multi-view webcam data and convert
+it to the TFRecord format expected by this library.
+
+## Data Pipelines
+
+We use the [tf.data.Dataset
+API](https://www.tensorflow.org/programmers_guide/datasets) to construct input
+pipelines that feed training, evaluation, and visualization. These pipelines are
+defined in `data_providers.py`.
+
+## Estimators
+
+We define training, evaluation, and inference behavior using the
+[tf.estimator.Estimator
+API](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator). See
+`estimators/mvtcn_estimator.py` for an example of how multi-view TCN training,
+evaluation, and inference is implemented.
+
+## Models
+
+Different embedder architectures are implemented in model.py. We used the
+`InceptionConvSSFCEmbedder` in the pouring experiments, but we're also
+evaluating `Resnet` embedders.
+
+## Losses
+
+We use the
+[tf.contrib.losses.metric_learning](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/losses/metric_learning)
+library's implementations of triplet loss with semi-hard negative mining and
+npairs loss. In our experiments, npairs loss has better empirical convergence
+and produces the best qualitative visualizations, and will likely be our choice
+for future experiments. See the
+[paper](http://www.nec-labs.com/uploads/images/Department-Images/MediaAnalytics/papers/nips16_npairmetriclearning.pdf)
+for details on the algorithm.
+
+## Inference
+
+We support 3 modes of inference for trained TCN models:
+
+*   Mode 1: Input is a tf.Estimator input_fn (see
+    [this](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict)
+    for details). Output is an iterator over embeddings and additional metadata.
+    See `labeled_eval.py` for a usage example.
+
+*   Mode 2: Input is a TFRecord or (or list of TFRecords). This returns an
+    iterator over tuples of (embeddings, raw_image_strings, sequence_name),
+    where embeddings is the [num views, sequence length, embedding size] numpy
+    array holding the full embedded sequence (for all views), raw_image_strings
+    is a [num views, sequence length] string array holding the jpeg-encoded raw
+    image strings, and sequence_name is the name of the sequence. See
+    `generate_videos.py` for a usage example.
+
+*   Mode 3: Input is a numpy array of size [num images, height, width, num
+    channels]. This returns a tuple of (embeddings, raw_image_strings), where
+    embeddings is a 2-D float32 numpy array holding [num_images, embedding_size]
+    image embeddings, and raw_image_strings is a 1-D string numpy array holding
+    [batch_size] jpeg-encoded image strings. This can be used as follows:
+
+    ```python
+    images = np.random.uniform(0, 1, size=(batch_size, 1080, 1920, 3))
+    embeddings, _ = estimator.inference(
+        images, checkpoint_path=checkpoint_path)
+    ```
+
+See `estimators/base_estimator.py` for details.
+
+## Configuration
+
+Data pipelines, training, eval, and visualization are all configured using
+key-value parameters passed as [YAML](https://en.wikipedia.org/wiki/YAML) files.
+Configurations can be nested, e.g.:
+
+```yaml
+learning:
+  optimizer: 'adam'
+  learning_rate: 0.001
+```
+
+### T objects
+
+YAML configs are converted to LuaTable-like `T` object (see
+`utils/luatables.py`), which behave like a python `dict`, but allow you to use
+dot notation to access (nested) keys. For example we could access the learning
+rate in the above config snippet via `config.learning.learning_rate`.
+
+### Multiple Configs
+
+Multiple configs can be passed to the various binaries as a comma separated list
+of config paths via the `--config_paths` flag. This allows us to specify a
+default config that applies to all experiments (e.g. how often to write
+checkpoints, default embedder hyperparams) and one config per experiment holding
+the just hyperparams specific to the experiment (path to data, etc.).
+
+See `configs/tcn_default.yml` for an example of our default config and
+`configs/pouring.yml` for an example of how we define the pouring experiments.
+
+Configs are applied left to right. For example, consider two config files:
+
+default.yml
+
+```yaml
+learning:
+  learning_rate: 0.001 # Default learning rate.
+  optimizer: 'adam'
+```
+
+myexperiment.yml
+
+```yaml
+learning:
+  learning_rate: 1.0 # Experiment learning rate (overwrites default).
+data:
+  training: '/path/to/myexperiment/training.tfrecord'
+```
+
+Running
+
+```bash
+bazel run train.py --config_paths='default.yml,myexperiment.yml'
+```
+
+results in a final merged config called final_training_config.yml
+
+```yaml
+learning:
+  optimizer: 'adam'
+  learning_rate: 1.0
+data:
+  training: '/path/to/myexperiment/training.tfrecord'
+```
+
+which is created automatically and stored in the experiment log directory
+alongside model checkpoints and tensorboard summaries. This gives us a record of
+the exact configs that went into each trial.
+
+## Monitoring training
+
+We usually look at two validation metrics during training: knn classification
+error and multi-view alignment.
+
+### KNN-Classification Error
+
+In cases where we have labeled validation data, we can compute the average
+cross-sequence KNN classification error (1.0 - recall@k=1) over all embedded
+labeled images in the validation set. See `labeled_eval.py`.
+
+### Multi-view Alignment
+
+In cases where there is no labeled validation data, we can look at the how well
+our model aligns multiple views of same embedded validation sequences. That is,
+for each embedded validation sequence, for all cross-view pairs, we compute the
+scaled absolute distance between ground truth time indices and knn time indices.
+See `alignment.py`.
+
+## Visualization
+
+We visualize the embedding space learned by our models in two ways: nearest
+neighbor imitation videos and PCA/T-SNE.
+
+### Nearest Neighbor Imitation Videos
+
+One of the easiest way to evaluate the understanding of your model is to see how
+well the model can semantically align two videos via nearest neighbors in
+embedding space.
+
+Consider the case where we have multiple validation demo videos of a human or
+robot performing the same task. For example, in the pouring experiments, we
+collected many different multiview validation videos of a person pouring the
+contents of one container into another, then setting the container down. If we'd
+like to see how well our embeddings generalize across viewpoint, object/agent
+appearance, and background, we can construct what we call "Nearest Neighbor
+Imitation" videos, by embedding some validation query sequence `i` from view 1,
+and finding the nearest neighbor for each query frame in some embedded target
+sequence `j` filmed from view 1.
+[Here's](https://sermanet.github.io/tcn/docs/figs/pouring_human.mov.gif) an
+example of the final product.
+
+See `generate_videos.py` for details.
+
+### PCA & T-SNE Visualization
+
+We can also embed a set of images taken randomly from validation videos and
+visualize the embedding space using PCA projection and T-SNE in the tensorboard
+projector. See `visualize_embeddings.py` for details.
+
+## Tutorial Part I: Collecting Multi-View Webcam Videos
+
+Here we give an end-to-end example of how to collect your own multiview webcam
+videos and convert them to the TFRecord format expected by training.
+
+Note: This was tested with up to 8 concurrent [Logitech c930e
+webcams](https://www.logitech.com/en-us/product/c930e-webcam) extended with
+[Plugable 5 Meter (16 Foot) USB 2.0 Active Repeater Extension
+Cables](https://www.amazon.com/gp/product/B006LFL4X0/ref=oh_aui_detailpage_o05_s00?ie=UTF8&psc=1).
+
+### Collect webcam videos
+
+Go to dataset/webcam.py
+
+1.  Plug your webcams in and run
+
+    ```bash
+    ls -ltrh /dev/video*
+    ```
+
+    You should see one device listed per connected webcam.
+
+2.  Define some environment variables describing the dataset you're collecting.
+
+    ```bash
+    dataset=tutorial  # Name of the dataset.
+    mode=train  # E.g. 'train', 'validation', 'test', 'demo'.
+    num_views=2 # Number of webcams.
+    viddir=/tmp/tcn/videos # Output directory for the videos.
+    tmp_imagedir=/tmp/tcn/tmp_images # Temp directory to hold images.
+    debug_vids=1 # Whether or not to generate side-by-side debug videos.
+    export DISPLAY=:0.0  # This allows real time matplotlib display.
+    ```
+
+3.  Run the webcam.py script.
+
+    ```bash
+    bazel build -c opt --copt=-mavx webcam && \
+    bazel-bin/webcam \
+    --dataset $dataset \
+    --mode $mode \
+    --num_views $num_views \
+    --tmp_imagedir $tmp_imagedir \
+    --viddir $viddir \
+    --debug_vids 1
+    ```
+
+4.  Hit Ctrl-C when done collecting, upon which the script will compile videos
+    for each view and optionally a debug video concatenating multiple
+    simultaneous views.
+
+5.  If `--seqname` flag isn't set, the script will name the first sequence '0',
+    the second sequence '1', and so on (meaning you can just keep rerunning step
+    3.). When you are finished, you should see an output viddir with the
+    following structure:
+
+    ```bash
+    videos/0_view0.mov
+    videos/0_view1.mov
+    ...
+    videos/0_viewM.mov
+    videos/1_viewM.mov
+    ...
+    videos/N_viewM.mov
+    for N sequences and M webcam views.
+    ```
+
+### Create TFRecords
+
+Use `dataset/videos_to_tfrecords.py` to convert the directory of videos into a
+directory of TFRecords files, one per multi-view sequence.
+
+```bash
+viddir=/tmp/tcn/videos
+dataset=tutorial
+mode=train
+videos=$viddir/$dataset
+
+bazel build -c opt videos_to_tfrecords && \
+bazel-bin/videos_to_tfrecords --logtostderr \
+--input_dir $videos/$mode \
+--output_dir ~/tcn_data/$dataset/$mode \
+--max_per_shard 400
+```
+
+Setting `--max_per_shard` > 0 allows you to shard training data. We've observed
+that sharding long training sequences provides better performance in terms of
+global steps/sec.
+
+This should be left at the default of 0 for validation / test data.
+
+You should now have a directory of TFRecords files with the following structure:
+
+```bash
+output_dir/0.tfrecord
+...
+output_dir/N.tfrecord
+
+1 TFRecord file for each of N multi-view sequences.
+```
+
+Now we're ready to move on to part II: training, evaluation, and visualization.
+
+## Tutorial Part II: Training, Evaluation, and Visualization
+
+Here we give an end-to-end example of how to train, evaluate, and visualize the
+embedding space learned by TCN models.
+
+### Download Data
+
+We will be using the 'Multiview Pouring' dataset, which can be downloaded using
+the download.sh script
+[here.](https://sites.google.com/site/brainrobotdata/home/multiview-pouring)
+
+The rest of the tutorial will assume that you have your data downloaded to a
+folder at `~/tcn_data`.
+
+```bash
+mkdir ~/tcn_data
+mv ~/Downloads/download.sh ~/tcn_data
+./download.sh
+```
+
+You should now have the following path containing all the data:
+
+```bash
+ls ~/tcn_data/multiview-pouring
+labels  README.txt  tfrecords  videos
+```
+
+### Download Pretrained Inception Checkpoint
+
+If you haven't already, run the script that downloads the pretrained InceptionV3
+checkpoint:
+
+```bash
+python download_pretrained.py
+```
+
+### Define A Config
+
+For our experiment, we create 2 configs:
+
+*   `configs/tcn_default.yml`: This contains all the default hyperparameters
+    that generally don't vary across experiments.
+*   `configs/pouring.yml`: This contains all the hyperparameters that are
+    specific to the pouring experiment.
+
+Important note about `configs/pouring.yml`:
+
+*   data.eval_cropping: We use 'pad200' for the pouring dataset, which was
+    filmed rather close up on iphone cameras. A better choice for data filmed on
+    webcam is likely 'crop_center'. See preprocessing.py for options.
+
+### Train
+
+Run the training binary:
+
+```yaml
+logdir=/tmp/tcn/pouring
+c=configs
+configs=$c/tcn_default.yml,$c/pouring.yml
+
+bazel build -c opt --copt=-mavx --config=cuda train && \
+bazel-bin/train \
+--config_paths $configs --logdir $logdir
+```
+
+### Evaluate
+
+Run the binary that computes running validation loss. Set `export
+CUDA_VISIBLE_DEVICES=` to run on CPU.
+
+```bash
+bazel build -c opt --copt=-mavx eval && \
+bazel-bin/eval \
+--config_paths $configs --logdir $logdir
+```
+
+Run the binary that computes running validation cross-view sequence alignment.
+Set `export CUDA_VISIBLE_DEVICES=` to run on CPU.
+
+```bash
+bazel build -c opt --copt=-mavx alignment && \
+bazel-bin/alignment \
+--config_paths $configs --checkpointdir $logdir --outdir $logdir
+```
+
+Run the binary that computes running labeled KNN validation error. Set `export
+CUDA_VISIBLE_DEVICES=` to run on CPU.
+
+```bash
+bazel build -c opt --copt=-mavx labeled_eval && \
+bazel-bin/labeled_eval \
+--config_paths $configs --checkpointdir $logdir --outdir $logdir
+```
+
+### Monitor training
+
+Run `tensorboard --logdir=$logdir`. After a bit of training, you should see
+curves that look like this:
+
+#### Training loss
+
+<img src="g3doc/loss.png" title="Training Loss" />
+
+#### Validation loss
+
+<img src="g3doc/val_loss.png" title="Validation Loss" />
+
+#### Validation Alignment
+
+<img src="g3doc/alignment.png" title="Validation Alignment" />
+
+#### Average Validation KNN Classification Error
+
+<img src="g3doc/avg_error.png" title="Validation Average KNN Error" />
+
+#### Individual Validation KNN Classification Errors
+
+<img src="g3doc/all_error.png" title="All Validation Average KNN Errors" />
+
+### Visualize
+
+To visualize the embedding space learned by a model, we can:
+
+#### Generate Imitation Videos
+
+```bash
+# Use the automatically generated final config file as config.
+configs=$logdir/final_training_config.yml
+# Visualize checkpoint 40001.
+checkpoint_iter=40001
+# Use validation records for visualization.
+records=~/tcn_data/multiview-pouring/tfrecords/val
+# Write videos to this location.
+outdir=$logdir/tcn_viz/imitation_vids
+```
+
+```bash
+bazel build -c opt --config=cuda --copt=-mavx generate_videos && \
+bazel-bin/generate_videos \
+--config_paths $configs \
+--checkpointdir $logdir \
+--checkpoint_iter $checkpoint_iter \
+--query_records_dir $records \
+--target_records_dir $records \
+--outdir $outdir
+```
+
+After the script completes, you should see a directory of videos with names
+like:
+
+`$outdir/qtrain_clearodwalla_to_clear1_realv1_imtrain_clearsoda_to_white13_realv0.mp4`
+
+that look like this: <img src="g3doc/im.gif" title="Imitation Video" />
+
+#### T-SNE / PCA Visualization
+
+Run the binary that generates embeddings and metadata.
+
+```bash
+outdir=$logdir/tcn_viz/embedding_viz
+bazel build -c opt --config=cuda --copt=-mavx visualize_embeddings && \
+bazel-bin/visualize_embeddings \
+--config_paths $configs \
+--checkpointdir $logdir \
+--checkpoint_iter $checkpoint_iter \
+--embedding_records $records \
+--outdir $outdir \
+--num_embed 1000 \
+--sprite_dim 64
+```
+
+Run tensorboard, pointed at the embedding viz output directory.
+
+```
+tensorboard --logdir=$outdir
+```
+
+You should see something like this in tensorboard.
+<img src="g3doc/pca.png" title="PCA" />
diff --git a/research/tcn/WORKSPACE b/research/tcn/WORKSPACE
new file mode 100644
index 00000000..87d59232
--- /dev/null
+++ b/research/tcn/WORKSPACE
@@ -0,0 +1,2 @@
+workspace(name = "tcn")
+
diff --git a/research/tcn/alignment.py b/research/tcn/alignment.py
new file mode 100644
index 00000000..461d261d
--- /dev/null
+++ b/research/tcn/alignment.py
@@ -0,0 +1,133 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Calculates test sequence alignment score."""
+from __future__ import absolute_import
+from __future__ import absolute_import
+from __future__ import division
+
+import os
+import numpy as np
+from estimators.get_estimator import get_estimator
+from utils import util
+import tensorflow as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string(
+    'checkpoint_iter', '', 'Evaluate this specific checkpoint.')
+tf.app.flags.DEFINE_string(
+    'checkpointdir', '/tmp/tcn', 'Path to model checkpoints.')
+tf.app.flags.DEFINE_string('outdir', '/tmp/tcn', 'Path to write summaries to.')
+FLAGS = tf.app.flags.FLAGS
+
+
+def compute_average_alignment(
+    seqname_to_embeddings, num_views, summary_writer, training_step):
+  """Computes the average cross-view alignment for all sequence view pairs.
+
+  Args:
+    seqname_to_embeddings: Dict, mapping sequence name to a
+      [num_views, embedding size] numpy matrix holding all embedded views.
+    num_views: Int, number of simultaneous views in the dataset.
+    summary_writer: A `SummaryWriter` object.
+    training_step: Int, the training step of the model used to embed images.
+
+  Alignment is the scaled absolute difference between the ground truth time
+  and the knn aligned time.
+  abs(|time_i - knn_time|) / sequence_length
+  """
+  all_alignments = []
+  for _, view_embeddings in seqname_to_embeddings.iteritems():
+    for idx_i in range(num_views):
+      for idx_j in range(idx_i+1, num_views):
+        embeddings_view_i = view_embeddings[idx_i]
+        embeddings_view_j = view_embeddings[idx_j]
+
+        seq_len = len(embeddings_view_i)
+
+        times_i = np.array(range(seq_len))
+        # Get the nearest time_index for each embedding in view_i.
+        times_j = np.array([util.KNNIdsWithDistances(
+            q, embeddings_view_j, k=1)[0][0] for q in embeddings_view_i])
+
+        # Compute sequence view pair alignment.
+        alignment = np.mean(
+            np.abs(np.array(times_i)-np.array(times_j))/float(seq_len))
+        all_alignments.append(alignment)
+        print 'alignment so far %f' % alignment
+  average_alignment = np.mean(all_alignments)
+  print 'Average alignment %f' % average_alignment
+  summ = tf.Summary(value=[tf.Summary.Value(
+      tag='validation/alignment', simple_value=average_alignment)])
+  summary_writer.add_summary(summ, int(training_step))
+
+
+def evaluate_once(
+    config, checkpointdir, validation_records, checkpoint_path, batch_size,
+    num_views):
+  """Evaluates and reports the validation alignment."""
+  # Choose an estimator based on training strategy.
+  estimator = get_estimator(config, checkpointdir)
+
+  # Embed all validation sequences.
+  seqname_to_embeddings = {}
+  for (view_embeddings, _, seqname) in estimator.inference(
+      validation_records, checkpoint_path, batch_size):
+    seqname_to_embeddings[seqname] = view_embeddings
+
+  # Compute and report alignment statistics.
+  ckpt_step = int(checkpoint_path.split('-')[-1])
+  summary_dir = os.path.join(FLAGS.outdir, 'alignment_summaries')
+  summary_writer = tf.summary.FileWriter(summary_dir)
+  compute_average_alignment(
+      seqname_to_embeddings, num_views, summary_writer, ckpt_step)
+
+
+def main(_):
+  # Parse config dict from yaml config files / command line flags.
+  config = util.ParseConfigsToLuaTable(FLAGS.config_paths, FLAGS.model_params)
+  num_views = config.data.num_views
+
+  validation_records = util.GetFilesRecursively(config.data.validation)
+  batch_size = config.data.batch_size
+
+  checkpointdir = FLAGS.checkpointdir
+
+  # If evaluating a specific checkpoint, do that.
+  if FLAGS.checkpoint_iter:
+    checkpoint_path = os.path.join(
+        '%s/model.ckpt-%s' % (checkpointdir, FLAGS.checkpoint_iter))
+    evaluate_once(
+        config, checkpointdir, validation_records, checkpoint_path, batch_size,
+        num_views)
+  else:
+    for checkpoint_path in tf.contrib.training.checkpoints_iterator(
+        checkpointdir):
+      evaluate_once(
+          config, checkpointdir, validation_records, checkpoint_path,
+          batch_size, num_views)
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/configs/pouring.yml b/research/tcn/configs/pouring.yml
new file mode 100644
index 00000000..4cfd9627
--- /dev/null
+++ b/research/tcn/configs/pouring.yml
@@ -0,0 +1,58 @@
+# Train with Multi-View TCN.
+training_strategy: 'mvtcn'
+
+# Use the 'inception_conv_ss_fc' embedder, which has the structure:
+# InceptionV3 -> 2 conv adaptation layers -> spatial softmax -> fully connected
+# -> embedding.
+embedder_strategy: 'inception_conv_ss_fc'
+
+# Use npairs loss.
+loss_strategy: 'npairs'
+
+learning:
+  learning_rate: 0.0001
+
+# Set some hyperparameters for our embedder.
+inception_conv_ss_fc:
+  # Don't finetune the pre-trained weights.
+  finetune_inception: false
+  dropout:
+    # Don't dropout convolutional activations.
+    keep_conv: 1.0
+    # Use a dropout of 0.8 on the fully connected activations.
+    keep_fc: 0.8
+    # Use a dropout of 0.8 on the inception activations.
+    keep_pretrained: 0.8
+
+# Size of the TCN embedding.
+embedding_size: 32
+
+data:
+  raw_height: 480
+  raw_width: 360
+  batch_size: 32
+  examples_per_sequence: 32
+  num_views: 2
+  preprocessing:
+    # Inference-time image cropping strategy.
+    eval_cropping: 'pad200'
+  augmentation:
+    # Do scale augmentation.
+    minscale: 0.8 # When downscaling, zoom in to 80% of the central bounding box.
+    maxscale: 3.0 # When upscaling, zoom out to 300% of the central bounding box.
+    proportion_scaled_up: 0.5 # Proportion of the time to scale up rather than down.
+    color: true # Do color augmentation.
+    fast_mode: true
+  # Paths to the data.
+  training: '~/tcn_data/multiview-pouring/tfrecords/train'
+  validation: '~/tcn_data/multiview-pouring/tfrecords/val'
+  test: 'path/to/test'
+  labeled:
+    image_attr_keys: ['image/view0', 'image/view1', 'task']
+    label_attr_keys: ['contact', 'distance', 'liquid_flowing', 'has_liquid', 'container_angle']
+    validation: '~/tcn_data/multiview-pouring/monolithic-labeled/val'
+    test: '~/tcn_data/multiview-pouring/monolithic-labeled/test'
+
+logging:
+  checkpoint:
+    save_checkpoints_steps: 1000
\ No newline at end of file
diff --git a/research/tcn/configs/tcn_default.yml b/research/tcn/configs/tcn_default.yml
new file mode 100644
index 00000000..992f36d7
--- /dev/null
+++ b/research/tcn/configs/tcn_default.yml
@@ -0,0 +1,115 @@
+#  These configs are the defaults we used for both the pouring and pose
+#  experiments.
+
+# Train on TPU?
+use_tpu: false # Default is to run without TPU locally.
+tpu:
+  num_shards: 1
+  iterations: 100
+
+# SGD / general learning hyperparameters.
+learning:
+  max_step: 1000000
+  learning_rate: 0.001
+  decay_steps: 10000
+  decay_factor: 1.00
+  l2_reg_weight: 0.000001
+  optimizer: 'adam'
+
+# Default metric learning loss hyperparameters.
+triplet_semihard:
+  embedding_l2: true # Suggestion from Hyun Oh Song's slides.
+  margin: .2 # Default value for Facenet.
+npairs:
+  embedding_l2: false # Suggestion from Hyun Oh Song's slides.
+clustering_loss:
+  embedding_l2: true # Suggestion from Hyun Oh Song's slides.
+  margin: 1.0 # Default in deep_metric_learning.
+lifted_struct:
+  embedding_l2: false # Suggestion from Hyun Oh Song's slides.
+  margin: 1.0
+contrastive:
+  embedding_l2: true # Suggestion from Hyun Oh Song's slides.
+  margin: 1.0
+
+# Which method to use to train the embedding.
+# Options are "mvtcn", "svtcn".
+training_strategy: 'mvtcn'
+
+# Which embedder architecture to use.
+# Options are 'inception_conv_ss_fc' (used in pouring / pose experiments),
+# 'resnet'.
+embedder_strategy: 'inception_conv_ss_fc'
+
+# Size of the TCN embedding.
+embedding_size: 32
+
+# Default hyperparameters for the different embedder architectures.
+inception_conv_ss_fc:
+  pretrained_checkpoint: 'pretrained_checkpoints/inception/inception_v3.ckpt'
+  pretrained_layer: 'Mixed_5d'
+  additional_conv_sizes: [512, 512]
+  fc_hidden_sizes: [2048]
+  finetune: false
+  dropout:
+    keep_pretrained: 1.0
+    keep_conv: 1.0
+    keep_fc: 1.0
+
+resnet:
+  pretrained_checkpoint: 'pretrained_checkpoints/resnet/resnet_v2_50.ckpt'
+  pretrained_layer: 4
+  finetune: false
+  adaptation_blocks: '512_3-512_3'
+  emb_connection: 'conv'
+  fc_hidden_sizes: 'None'
+  dropout:
+    keep_pretrained: 1.0
+
+# Loss hyperparameters.
+mvtcn:
+  # Size of the window in timesteps to get random anchor-positive pairs for
+  # training.
+  window: 580 # 29fps * 20 seconds.
+
+svtcn:
+  pos_radius: 6  # 0.2 seconds * 29fps ~ 6 timesteps.
+  neg_radius: 12 # 2.0 * pos_radius.
+
+# Data configs.
+data:
+  height: 299
+  width: 299
+  preprocessing:
+    # Strategy to use when cropping images at inference time.
+    # See preprocessing.py for options.
+    eval_cropping: 'crop_center'
+  # Training scale, color augmentation hyparameters.
+  augmentation:
+    # See preprocessing.py for a discussion of how to use these parameters.
+    minscale: 1.0
+    maxscale: 1.0
+    proportion_scaled_up: 0.5
+    color: true
+    fast_mode: true
+  num_parallel_calls: 12
+  sequence_prefetch_size: 12
+  batch_prefetch_size: 12
+  batch_size: 36
+  eval_batch_size: 36
+  embed_batch_size: 128
+
+val:
+  recall_at_k_list: [1]
+  num_eval_samples: 1000
+  eval_interval_secs: 300
+
+logging:
+  summary:
+    image_summaries: false
+    save_summaries_steps: 100
+    flush_secs: 600
+  checkpoint:
+    num_to_keep: 0 # Keep all checkpoints.
+    save_checkpoints_steps: 1000
+    secs: 1800
\ No newline at end of file
diff --git a/research/tcn/configs/test_estimator.yml b/research/tcn/configs/test_estimator.yml
new file mode 100644
index 00000000..4e451947
--- /dev/null
+++ b/research/tcn/configs/test_estimator.yml
@@ -0,0 +1,29 @@
+use_tpu: False
+training_strategy: 'mvtcn'
+loss_strategy: 'triplet_semihard'
+
+learning:
+  max_step: 2
+  optimizer: 'adam'
+
+embedding_size: 8
+
+data:
+  embed_batch_size: 12
+  batch_size: 12
+  examples_per_sequence: 12
+  num_views: 2
+  num_parallel_calls: 1
+  sequence_prefetch_size: 1
+  batch_prefetch_size: 1
+
+logging:
+  summary:
+    image_summaries: false
+    save_summaries_steps: 100
+    flush_secs: 600
+    save_summaries_secs: 60
+  checkpoint:
+    num_to_keep: 0 # Keep all checkpoints.
+    save_checkpoints_steps: 1000
+    secs: 1800
\ No newline at end of file
diff --git a/research/tcn/data_providers.py b/research/tcn/data_providers.py
new file mode 100644
index 00000000..86e55239
--- /dev/null
+++ b/research/tcn/data_providers.py
@@ -0,0 +1,505 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Defines data providers used in training and evaluating TCNs."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+import random
+import numpy as np
+import preprocessing
+import tensorflow as tf
+
+
+def record_dataset(filename):
+  """Generate a TFRecordDataset from a `filename`."""
+  return tf.data.TFRecordDataset(filename)
+
+
+def full_sequence_provider(file_list, num_views):
+  """Provides full preprocessed image sequences.
+
+  Args:
+    file_list: List of strings, paths to TFRecords to preprocess.
+    num_views: Int, the number of simultaneous viewpoints at each timestep in
+      the dataset.
+  Returns:
+    preprocessed: A 4-D float32 `Tensor` holding a sequence of preprocessed
+      images.
+    raw_image_strings: A 2-D string `Tensor` holding a sequence of raw
+      jpeg-encoded image strings.
+    task: String, the name of the sequence.
+    seq_len: Int, the number of timesteps in the sequence.
+  """
+  def _parse_sequence(x):
+    context, views, seq_len = parse_sequence_example(x, num_views)
+    task = context['task']
+    return views, task, seq_len
+
+  data_files = tf.contrib.slim.parallel_reader.get_data_files(file_list)
+  dataset = tf.data.Dataset.from_tensor_slices(data_files)
+  dataset = dataset.repeat(1)
+  # Get a dataset of sequences.
+  dataset = dataset.flat_map(record_dataset)
+
+  # Build a dataset of TFRecord files.
+  dataset = dataset.repeat(1)
+  # Prefetch a number of opened files.
+  dataset = dataset.prefetch(12)
+  # Use _parse_sequence to deserialize (but not decode) image strings.
+  dataset = dataset.map(_parse_sequence, num_parallel_calls=12)
+  # Prefetch batches of images.
+  dataset = dataset.prefetch(12)
+  dataset = dataset.make_one_shot_iterator()
+  views, task, seq_len = dataset.get_next()
+  return views, task, seq_len
+
+
+def parse_labeled_example(
+    example_proto, view_index, preprocess_fn, image_attr_keys, label_attr_keys):
+  """Parses a labeled test example from a specified view.
+
+  Args:
+    example_proto: A scalar string Tensor.
+    view_index: Int, index on which view to parse.
+    preprocess_fn: A function with the signature (raw_images, is_training) ->
+      preprocessed_images, where raw_images is a 4-D float32 image `Tensor`
+      of raw images, is_training is a Boolean describing if we're in training,
+      and preprocessed_images is a 4-D float32 image `Tensor` holding
+      preprocessed images.
+    image_attr_keys: List of Strings, names for image keys.
+    label_attr_keys: List of Strings, names for label attributes.
+  Returns:
+    data: A tuple of images, attributes and tasks `Tensors`.
+  """
+  features = {}
+  for attr_key in image_attr_keys:
+    features[attr_key] = tf.FixedLenFeature((), tf.string)
+  for attr_key in label_attr_keys:
+    features[attr_key] = tf.FixedLenFeature((), tf.int64)
+  parsed_features = tf.parse_single_example(example_proto, features)
+  image_only_keys = [i for i in image_attr_keys if 'image' in i]
+  view_image_key = image_only_keys[view_index]
+  image = preprocessing.decode_image(parsed_features[view_image_key])
+  preprocessed = preprocess_fn(image, is_training=False)
+  attributes = [parsed_features[k] for k in label_attr_keys]
+  task = parsed_features['task']
+  return tuple([preprocessed] + attributes + [task])
+
+
+def labeled_data_provider(
+    filenames, preprocess_fn, view_index, image_attr_keys, label_attr_keys,
+    batch_size=32, num_epochs=1):
+  """Gets a batched dataset iterator over annotated test images + labels.
+
+  Provides a single view, specifed in `view_index`.
+
+  Args:
+    filenames: List of Strings, paths to tfrecords on disk.
+    preprocess_fn: A function with the signature (raw_images, is_training) ->
+      preprocessed_images, where raw_images is a 4-D float32 image `Tensor`
+      of raw images, is_training is a Boolean describing if we're in training,
+      and preprocessed_images is a 4-D float32 image `Tensor` holding
+      preprocessed images.
+    view_index: Int, the index of the view to embed.
+    image_attr_keys: List of Strings, names for image keys.
+    label_attr_keys: List of Strings, names for label attributes.
+    batch_size: Int, size of the batch.
+    num_epochs: Int, number of epochs over the classification dataset.
+  Returns:
+    batch_images: 4-d float `Tensor` holding the batch images for the view.
+    labels: K-d int `Tensor` holding the K label attributes.
+    tasks: 1-D String `Tensor`, holding the task names for each batch element.
+  """
+  dataset = tf.data.TFRecordDataset(filenames)
+  # pylint: disable=g-long-lambda
+  dataset = dataset.map(
+      lambda p: parse_labeled_example(
+          p, view_index, preprocess_fn, image_attr_keys, label_attr_keys))
+  dataset = dataset.repeat(num_epochs)
+  dataset = dataset.batch(batch_size)
+  data_iterator = dataset.make_one_shot_iterator()
+  batch_data = data_iterator.get_next()
+  batch_images = batch_data[0]
+
+  batch_labels = tf.stack(batch_data[1:-1], 1)
+
+  batch_tasks = batch_data[-1]
+
+  batch_images = set_image_tensor_batch_dim(batch_images, batch_size)
+  batch_labels.set_shape([batch_size, len(label_attr_keys)])
+  batch_tasks.set_shape([batch_size])
+
+  return batch_images, batch_labels, batch_tasks
+
+
+def parse_sequence_example(serialized_example, num_views):
+  """Parses a serialized sequence example into views, sequence length data."""
+  context_features = {
+      'task': tf.FixedLenFeature(shape=[], dtype=tf.string),
+      'len': tf.FixedLenFeature(shape=[], dtype=tf.int64)
+  }
+  view_names = ['view%d' % i for i in range(num_views)]
+  fixed_features = [
+      tf.FixedLenSequenceFeature(
+          shape=[], dtype=tf.string) for _ in range(len(view_names))]
+  sequence_features = dict(zip(view_names, fixed_features))
+  context_parse, sequence_parse = tf.parse_single_sequence_example(
+      serialized=serialized_example,
+      context_features=context_features,
+      sequence_features=sequence_features)
+  views = tf.stack([sequence_parse[v] for v in view_names])
+  lens = [sequence_parse[v].get_shape().as_list()[0] for v in view_names]
+  assert len(set(lens)) == 1
+  seq_len = tf.shape(sequence_parse[v])[0]
+  return context_parse, views, seq_len
+
+
+def get_shuffled_input_records(file_list):
+  """Build a tf.data.Dataset of shuffled input TFRecords that repeats."""
+  dataset = tf.data.Dataset.from_tensor_slices(file_list)
+  dataset = dataset.shuffle(len(file_list))
+  dataset = dataset.repeat()
+  dataset = dataset.flat_map(record_dataset)
+  dataset = dataset.repeat()
+  return dataset
+
+
+def get_tcn_anchor_pos_indices(seq_len, num_views, num_pairs, window):
+  """Gets batch TCN anchor positive timestep and view indices.
+
+  This gets random (anchor, positive) timesteps from a sequence, and chooses
+  2 random differing viewpoints for each anchor positive pair.
+
+  Args:
+    seq_len: Int, the size of the batch sequence in timesteps.
+    num_views: Int, the number of simultaneous viewpoints at each timestep.
+    num_pairs: Int, the number of pairs to build.
+    window: Int, the window (in frames) from which to take anchor, positive
+      and negative indices.
+  Returns:
+    ap_time_indices: 1-D Int `Tensor` with size [num_pairs], holding the
+      timestep for each (anchor,pos) pair.
+    a_view_indices: 1-D Int `Tensor` with size [num_pairs], holding the
+      view index for each anchor.
+    p_view_indices: 1-D Int `Tensor` with size [num_pairs], holding the
+      view index for each positive.
+  """
+  # Get anchor, positive time indices.
+  def f1():
+    # Choose a random window-length range from the sequence.
+    range_min = tf.random_shuffle(tf.range(seq_len-window))[0]
+    range_max = range_min+window
+    return tf.range(range_min, range_max)
+  def f2():
+    # Consider the full sequence.
+    return tf.range(seq_len)
+  time_indices = tf.cond(tf.greater(seq_len, window), f1, f2)
+  shuffled_indices = tf.random_shuffle(time_indices)
+  num_pairs = tf.minimum(seq_len, num_pairs)
+  ap_time_indices = shuffled_indices[:num_pairs]
+
+  # Get opposing anchor, positive view indices.
+  view_indices = tf.tile(
+      tf.expand_dims(tf.range(num_views), 0), (num_pairs, 1))
+  shuffled_view_indices = tf.map_fn(tf.random_shuffle, view_indices)
+  a_view_indices = shuffled_view_indices[:, 0]
+  p_view_indices = shuffled_view_indices[:, 1]
+  return ap_time_indices, a_view_indices, p_view_indices
+
+
+def set_image_tensor_batch_dim(tensor, batch_dim):
+  """Sets the batch dimension on an image tensor."""
+  shape = tensor.get_shape()
+  tensor.set_shape([batch_dim, shape[1], shape[2], shape[3]])
+  return tensor
+
+
+def parse_sequence_to_pairs_batch(
+    serialized_example, preprocess_fn, is_training, num_views, batch_size,
+    window):
+  """Parses a serialized sequence example into a batch of preprocessed data.
+
+  Args:
+    serialized_example: A serialized SequenceExample.
+    preprocess_fn: A function with the signature (raw_images, is_training) ->
+      preprocessed_images.
+    is_training: Boolean, whether or not we're in training.
+    num_views: Int, the number of simultaneous viewpoints at each timestep in
+      the dataset.
+    batch_size: Int, size of the batch to get.
+    window: Int, only take pairs from a maximium window of this size.
+  Returns:
+    preprocessed: A 4-D float32 `Tensor` holding preprocessed images.
+    anchor_images: A 4-D float32 `Tensor` holding raw anchor images.
+    pos_images: A 4-D float32 `Tensor` holding raw positive images.
+  """
+  _, views, seq_len = parse_sequence_example(serialized_example, num_views)
+
+  # Get random (anchor, positive) timestep and viewpoint indices.
+  num_pairs = batch_size // 2
+  ap_time_indices, a_view_indices, p_view_indices = get_tcn_anchor_pos_indices(
+      seq_len, num_views, num_pairs, window)
+
+  # Gather the image strings.
+  combined_anchor_indices = tf.concat(
+      [tf.expand_dims(a_view_indices, 1),
+       tf.expand_dims(ap_time_indices, 1)], 1)
+  combined_pos_indices = tf.concat(
+      [tf.expand_dims(p_view_indices, 1),
+       tf.expand_dims(ap_time_indices, 1)], 1)
+  anchor_images = tf.gather_nd(views, combined_anchor_indices)
+  pos_images = tf.gather_nd(views, combined_pos_indices)
+
+  # Decode images.
+  anchor_images = tf.map_fn(
+      preprocessing.decode_image, anchor_images, dtype=tf.float32)
+  pos_images = tf.map_fn(
+      preprocessing.decode_image, pos_images, dtype=tf.float32)
+
+  # Concatenate [anchor, postitive] images into a batch and preprocess it.
+  concatenated = tf.concat([anchor_images, pos_images], 0)
+  preprocessed = preprocess_fn(concatenated, is_training)
+  anchor_prepro, positive_prepro = tf.split(preprocessed, num_or_size_splits=2,
+                                            axis=0)
+
+  # Set static batch dimensions for all image tensors
+  ims = [anchor_prepro, positive_prepro, anchor_images, pos_images]
+  ims = [set_image_tensor_batch_dim(i, num_pairs) for i in ims]
+  [anchor_prepro, positive_prepro, anchor_images, pos_images] = ims
+
+  # Assign each anchor and positive the same label.
+  anchor_labels = tf.range(1, num_pairs+1)
+  positive_labels = tf.range(1, num_pairs+1)
+
+  return (anchor_prepro, positive_prepro, anchor_images, pos_images,
+          anchor_labels, positive_labels, seq_len)
+
+
+def multiview_pairs_provider(file_list,
+                             preprocess_fn,
+                             num_views,
+                             window,
+                             is_training,
+                             batch_size,
+                             examples_per_seq=2,
+                             num_parallel_calls=12,
+                             sequence_prefetch_size=12,
+                             batch_prefetch_size=12):
+  """Provides multi-view TCN anchor-positive image pairs.
+
+  Returns batches of Multi-view TCN pairs, where each pair consists of an
+  anchor and a positive coming from different views from the same timestep.
+  Batches are filled one entire sequence at a time until
+  batch_size is exhausted. Pairs are chosen randomly without replacement
+  within a sequence.
+
+  Used by:
+    * triplet semihard loss.
+    * clustering loss.
+    * npairs loss.
+    * lifted struct loss.
+    * contrastive loss.
+
+  Args:
+    file_list: List of Strings, paths to tfrecords.
+    preprocess_fn: A function with the signature (raw_images, is_training) ->
+      preprocessed_images, where raw_images is a 4-D float32 image `Tensor`
+      of raw images, is_training is a Boolean describing if we're in training,
+      and preprocessed_images is a 4-D float32 image `Tensor` holding
+      preprocessed images.
+    num_views: Int, the number of simultaneous viewpoints at each timestep.
+    window: Int, size of the window (in frames) from which to draw batch ids.
+    is_training: Boolean, whether or not we're in training.
+    batch_size: Int, how many examples in the batch (num pairs * 2).
+    examples_per_seq: Int, how many examples to take per sequence.
+    num_parallel_calls: Int, the number of elements to process in parallel by
+      mapper.
+    sequence_prefetch_size: Int, size of the buffer used to prefetch sequences.
+    batch_prefetch_size: Int, size of the buffer used to prefetch batches.
+  Returns:
+    batch_images: A 4-D float32 `Tensor` holding preprocessed batch images.
+    anchor_labels: A 1-D int32 `Tensor` holding anchor image labels.
+    anchor_images: A 4-D float32 `Tensor` holding raw anchor images.
+    positive_labels: A 1-D int32 `Tensor` holding positive image labels.
+    pos_images: A 4-D float32 `Tensor` holding raw positive images.
+  """
+  def _parse_sequence(x):
+    return parse_sequence_to_pairs_batch(
+        x, preprocess_fn, is_training, num_views, examples_per_seq, window)
+
+  # Build a buffer of shuffled input TFRecords that repeats forever.
+  dataset = get_shuffled_input_records(file_list)
+
+  # Prefetch a number of opened TFRecords.
+  dataset = dataset.prefetch(sequence_prefetch_size)
+
+  # Use _parse_sequence to map sequences to batches (one sequence per batch).
+  dataset = dataset.map(
+      _parse_sequence, num_parallel_calls=num_parallel_calls)
+
+  # Filter out sequences that don't have at least examples_per_seq.
+  def seq_greater_than_min(seqlen, maximum):
+    return seqlen >= maximum
+  filter_fn = functools.partial(seq_greater_than_min, maximum=examples_per_seq)
+  dataset = dataset.filter(lambda a, b, c, d, e, f, seqlen: filter_fn(seqlen))
+
+  # Take a number of sequences for the batch.
+  assert batch_size % examples_per_seq == 0
+  sequences_per_batch = batch_size // examples_per_seq
+  dataset = dataset.batch(sequences_per_batch)
+
+  # Prefetch batches of images.
+  dataset = dataset.prefetch(batch_prefetch_size)
+
+  iterator = dataset.make_one_shot_iterator()
+  data = iterator.get_next()
+
+  # Pull out images, reshape to [batch_size, ...], concatenate anchor and pos.
+  ims = list(data[:4])
+  anchor_labels, positive_labels = data[4:6]
+
+  # Set labels shape.
+  anchor_labels.set_shape([sequences_per_batch, None])
+  positive_labels.set_shape([sequences_per_batch, None])
+
+  def _reshape_to_batchsize(im):
+    """[num_sequences, num_per_seq, ...] images to [batch_size, ...]."""
+    sequence_ims = tf.split(im, num_or_size_splits=sequences_per_batch, axis=0)
+    sequence_ims = [tf.squeeze(i) for i in sequence_ims]
+    return tf.concat(sequence_ims, axis=0)
+
+  # Reshape labels.
+  anchor_labels = _reshape_to_batchsize(anchor_labels)
+  positive_labels = _reshape_to_batchsize(positive_labels)
+
+  def _set_shape(im):
+    """Sets a static shape for an image tensor of [sequences_per_batch,...] ."""
+    shape = im.get_shape()
+    im.set_shape([sequences_per_batch, shape[1], shape[2], shape[3], shape[4]])
+    return im
+  ims = [_set_shape(im) for im in ims]
+  ims = [_reshape_to_batchsize(im) for im in ims]
+
+  anchor_prepro, positive_prepro, anchor_images, pos_images = ims
+  batch_images = tf.concat([anchor_prepro, positive_prepro], axis=0)
+
+  return batch_images, anchor_labels, positive_labels, anchor_images, pos_images
+
+
+def get_svtcn_indices(seq_len, batch_size, num_views):
+  """Gets a random window of contiguous time indices from a sequence.
+
+  Args:
+    seq_len: Int, number of timesteps in the image sequence.
+    batch_size: Int, size of the batch to construct.
+    num_views: Int, the number of simultaneous viewpoints at each
+      timestep in the dataset.
+
+  Returns:
+    time_indices: 1-D Int `Tensor` with size [batch_size], holding the
+      timestep for each batch image.
+    view_indices: 1-D Int `Tensor` with size [batch_size], holding the
+      view for each batch image. This is consistent across the batch.
+  """
+  # Get anchor, positive time indices.
+  def f1():
+    # Choose a random contiguous range from within the sequence.
+    range_min = tf.random_shuffle(tf.range(seq_len-batch_size))[0]
+    range_max = range_min+batch_size
+    return tf.range(range_min, range_max)
+  def f2():
+    # Consider the full sequence.
+    return tf.range(seq_len)
+  time_indices = tf.cond(tf.greater(seq_len, batch_size), f1, f2)
+  # Get opposing anchor, positive view indices.
+  random_view = tf.random_shuffle(tf.range(num_views))[0]
+  view_indices = tf.tile([random_view], (batch_size,))
+  return time_indices, view_indices
+
+
+def parse_sequence_to_svtcn_batch(
+    serialized_example, preprocess_fn, is_training, num_views, batch_size):
+  """Parses a serialized sequence example into a batch of SVTCN data."""
+  _, views, seq_len = parse_sequence_example(serialized_example, num_views)
+  # Get svtcn indices.
+  time_indices, view_indices = get_svtcn_indices(seq_len, batch_size, num_views)
+  combined_indices = tf.concat(
+      [tf.expand_dims(view_indices, 1),
+       tf.expand_dims(time_indices, 1)], 1)
+
+  # Gather the image strings.
+  images = tf.gather_nd(views, combined_indices)
+
+  # Decode images.
+  images = tf.map_fn(preprocessing.decode_image, images, dtype=tf.float32)
+
+  # Concatenate anchor and postitive images, preprocess the batch.
+  preprocessed = preprocess_fn(images, is_training)
+
+  return preprocessed, images, time_indices
+
+
+def singleview_tcn_provider(file_list,
+                            preprocess_fn,
+                            num_views,
+                            is_training,
+                            batch_size,
+                            num_parallel_calls=12,
+                            sequence_prefetch_size=12,
+                            batch_prefetch_size=12):
+  """Provides data to train singleview TCNs.
+
+  Args:
+    file_list: List of Strings, paths to tfrecords.
+    preprocess_fn: A function with the signature (raw_images, is_training) ->
+      preprocessed_images, where raw_images is a 4-D float32 image `Tensor`
+      of raw images, is_training is a Boolean describing if we're in training,
+      and preprocessed_images is a 4-D float32 image `Tensor` holding
+      preprocessed images.
+    num_views: Int, the number of simultaneous viewpoints at each timestep.
+    is_training: Boolean, whether or not we're in training.
+    batch_size: Int, how many examples in the batch.
+    num_parallel_calls: Int, the number of elements to process in parallel by
+      mapper.
+    sequence_prefetch_size: Int, size of the buffer used to prefetch sequences.
+    batch_prefetch_size: Int, size of the buffer used to prefetch batches.
+
+  Returns:
+    batch_images: A 4-D float32 `Tensor` of preprocessed images.
+    raw_images: A 4-D float32 `Tensor` of raw images.
+    timesteps: A 1-D int32 `Tensor` of timesteps associated with each image.
+  """
+  def _parse_sequence(x):
+    return parse_sequence_to_svtcn_batch(
+        x, preprocess_fn, is_training, num_views, batch_size)
+
+  # Build a buffer of shuffled input TFRecords that repeats forever.
+  dataset = get_shuffled_input_records(file_list)
+
+  # Prefetch a number of opened files.
+  dataset = dataset.prefetch(sequence_prefetch_size)
+
+  # Use _parse_sequence to map sequences to image batches.
+  dataset = dataset.map(
+      _parse_sequence, num_parallel_calls=num_parallel_calls)
+
+  # Prefetch batches of images.
+  dataset = dataset.prefetch(batch_prefetch_size)
+  dataset = dataset.make_one_shot_iterator()
+  batch_images, raw_images, timesteps = dataset.get_next()
+  return batch_images, raw_images, timesteps
diff --git a/research/tcn/data_providers_test.py b/research/tcn/data_providers_test.py
new file mode 100644
index 00000000..e5012310
--- /dev/null
+++ b/research/tcn/data_providers_test.py
@@ -0,0 +1,69 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for data_providers."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import data_providers
+import tensorflow as tf
+
+
+class DataTest(tf.test.TestCase):
+
+  def testMVTripletIndices(self):
+    """Ensures anchor/pos indices for a TCN batch are valid."""
+    tf.set_random_seed(0)
+    window = 580
+    batch_size = 36
+    num_pairs = batch_size // 2
+    num_views = 2
+    seq_len = 600
+    # Get anchor time and view indices for this sequence.
+    (_, a_view_indices,
+     p_view_indices) = data_providers.get_tcn_anchor_pos_indices(
+         seq_len, num_views, num_pairs, window)
+    with self.test_session() as sess:
+      (np_a_view_indices,
+       np_p_view_indices) = sess.run([a_view_indices, p_view_indices])
+
+      # Assert no overlap between anchor and pos view indices.
+      np.testing.assert_equal(
+          np.any(np.not_equal(np_a_view_indices, np_p_view_indices)), True)
+
+      # Assert set of view indices is a subset of expected set of view indices.
+      view_set = set(range(num_views))
+      self.assertTrue(set(np_a_view_indices).issubset(view_set))
+      self.assertTrue(set(np_p_view_indices).issubset(view_set))
+
+  def testSVTripletIndices(self):
+    """Ensures time indices for a SV triplet batch are valid."""
+    seq_len = 600
+    batch_size = 36
+    num_views = 2
+    time_indices, _ = data_providers.get_svtcn_indices(
+        seq_len, batch_size, num_views)
+    with self.test_session() as sess:
+      np_time_indices = sess.run(time_indices)
+      first = np_time_indices[0]
+      last = np_time_indices[-1]
+      # Make sure batch time indices are a contiguous range.
+      self.assertTrue(np.array_equal(np_time_indices, range(first, last+1)))
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/research/tcn/dataset/images_to_videos.py b/research/tcn/dataset/images_to_videos.py
new file mode 100644
index 00000000..ad1a7387
--- /dev/null
+++ b/research/tcn/dataset/images_to_videos.py
@@ -0,0 +1,86 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Converts temp directories of images to videos."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import os
+import shutil
+# pylint: disable=invalid-name
+
+parser = argparse.ArgumentParser()
+parser.add_argument(
+    '--view_dirs', type=str, default='',
+    help='Comma-separated list of temp view image directories.')
+parser.add_argument(
+    '--vid_paths', type=str, default='',
+    help='Comma-separated list of video output paths.')
+parser.add_argument(
+    '--debug_path', type=str, default='',
+    help='Output path to debug video.')
+
+parser.add_argument(
+    '--debug_lhs_view', type=str, default='',
+    help='Output path to debug video.')
+parser.add_argument(
+    '--debug_rhs_view', type=str, default='',
+    help='Output path to debug video.')
+
+
+def create_vids(view_dirs, vid_paths, debug_path=None,
+                debug_lhs_view=0, debug_rhs_view=1):
+  """Creates one video per view per sequence."""
+
+  # Create the view videos.
+  for (view_dir, vidpath) in zip(view_dirs, vid_paths):
+    encode_vid_cmd = r'mencoder mf://%s/*.png \
+    -mf fps=29:type=png \
+    -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell \
+    -oac copy -o %s' % (view_dir, vidpath)
+    os.system(encode_vid_cmd)
+
+  # Optionally create a debug side-by-side video.
+  if debug_path:
+    lhs = vid_paths[int(debug_lhs_view)]
+    rhs = vid_paths[int(debug_rhs_view)]
+    os.system(r"avconv \
+      -i %s \
+      -i %s \
+      -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' \
+      -map [vid] \
+      -c:v libx264 \
+      -crf 23 \
+      -preset veryfast \
+      %s" % (lhs, rhs, debug_path))
+
+
+def main():
+  FLAGS, _ = parser.parse_known_args()
+  assert FLAGS.view_dirs
+  assert FLAGS.vid_paths
+  view_dirs = FLAGS.view_dirs.split(',')
+  vid_paths = FLAGS.vid_paths.split(',')
+  create_vids(view_dirs, vid_paths, FLAGS.debug_path,
+              FLAGS.debug_lhs_view, FLAGS.debug_rhs_view)
+
+  # Cleanup temp image dirs.
+  for i in view_dirs:
+    shutil.rmtree(i)
+
+if __name__ == '__main__':
+  main()
diff --git a/research/tcn/dataset/videos_to_tfrecords.py b/research/tcn/dataset/videos_to_tfrecords.py
new file mode 100644
index 00000000..a17411f3
--- /dev/null
+++ b/research/tcn/dataset/videos_to_tfrecords.py
@@ -0,0 +1,458 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Converts videos to training, validation, test, and debug tfrecords on cns.
+
+Example usage:
+
+# From phone videos.
+x=learning/brain/research/tcn/videos_to_tfrecords && \
+blaze build -c opt $x && \
+set=tmp && videos=~/data/tcn/datasets/$set/ && \
+blaze-bin/$x --logtostderr --output_dir /cns/oi-d/home/$USER/tcn_data/$set \
+--input_dir $videos/train
+--debug $dataset/debug --rotate 90 --max_per_shard 400
+
+# From webcam videos.
+mode=train
+x=learning/brain/research/tcn/videos_to_tfrecords && \
+blaze build -c opt $x && \
+set=tmp && videos=/tmp/tcn/videos/$set/ && \
+blaze-bin/$x --logtostderr \
+--output_dir /cns/oi-d/home/$USER/tcn_data/$set/$mode \
+--input_dir $videos/$mode --max_per_shard 400
+
+"""
+import glob
+import math
+import multiprocessing
+from multiprocessing.pool import ThreadPool
+import os
+from random import shuffle
+import re
+from StringIO import StringIO
+import cv2
+from PIL import Image
+from PIL import ImageFile
+from preprocessing import cv2resizeminedge
+from preprocessing import cv2rotateimage
+from preprocessing import shapestring
+from utils.progress import Progress
+import tensorflow.google as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+
+tf.app.flags.DEFINE_string('view_pattern', '_view[_]*[0]+[.].*',
+                           'view regexp pattern for first view')
+tf.app.flags.DEFINE_string('input_dir', '', '''input data path''')
+tf.app.flags.DEFINE_integer('resize_min_edge', 0,
+                            '''resize the smallest edge to this size.''')
+tf.app.flags.DEFINE_integer('rotate', 0, '''rotate the image in degrees.''')
+tf.app.flags.DEFINE_string('rotate_if_matching', None,
+                           'rotate only if video path matches regexp.')
+tf.app.flags.DEFINE_string('output_dir', '', 'output directory for the dataset')
+tf.app.flags.DEFINE_integer(
+    'max_per_shard', -1, 'max # of frames per data chunk')
+tf.app.flags.DEFINE_integer('expected_views', 2, 'expected number of views')
+tf.app.flags.DEFINE_integer('log_frequency', 50, 'frequency of logging')
+tf.app.flags.DEFINE_integer(
+    'max_views_discrepancy', 100,
+    'Maximum length difference (in frames) allowed between views')
+tf.app.flags.DEFINE_boolean('overwrite', False, 'overwrite output files')
+FLAGS = tf.app.flags.FLAGS
+
+feature = tf.train.Feature
+bytes_feature = lambda v: feature(bytes_list=tf.train.BytesList(value=v))
+int64_feature = lambda v: feature(int64_list=tf.train.Int64List(value=v))
+float_feature = lambda v: feature(float_list=tf.train.FloatList(value=v))
+
+
+def FindPatternFiles(path, view_pattern, errors):
+  """Recursively find all files matching a certain pattern."""
+  if not path:
+    return None
+  tf.logging.info(
+      'Recursively searching for files matching pattern \'%s\' in %s' %
+      (view_pattern, path))
+  view_patt = re.compile('.*' + view_pattern)
+  sequences = []
+  for root, _, filenames in os.walk(path, followlinks=True):
+    path_root = root[:len(path)]
+    assert path_root == path
+
+    for filename in filenames:
+      if view_patt.match(filename):
+        fullpath = os.path.join(root, re.sub(view_pattern, '', filename))
+        shortpath = re.sub(path, '', fullpath).lstrip('/')
+
+        # Determine if this sequence should be sharded or not.
+        shard = False
+        if FLAGS.max_per_shard > 0:
+          shard = True
+
+        # Retrieve number of frames for this sequence.
+        num_views, length, view_paths, num_frames = GetViewInfo(
+            fullpath + view_pattern[0] + '*')
+        if num_views != FLAGS.expected_views:
+          tf.logging.info('Expected %d views but found: %s' %
+                          (FLAGS.expected_views, str(view_paths)))
+        assert num_views == FLAGS.expected_views
+        assert length > 0
+        # Drop sequences if view lengths differ too much.
+        if max(num_frames) - min(num_frames) > FLAGS.max_views_discrepancy:
+          error_msg = (
+              'Error: ignoring sequence with views with length difference > %d:'
+              '%s in %s') % (FLAGS.max_views_discrepancy, str(num_frames),
+                             fullpath)
+          errors.append(error_msg)
+          tf.logging.error(error_msg)
+        else:
+          # Append sequence info.
+          sequences.append({'full': fullpath, 'name': shortpath, 'len': length,
+                            'start': 0, 'end': length, 'num_views': num_views,
+                            'shard': shard})
+  return sorted(sequences, key=lambda k: k['name'])
+
+
+def ShardSequences(sequences, max_per_shard):
+  """Find all sequences, shard and randomize them."""
+  total_shards_len = 0
+  total_shards = 0
+  assert max_per_shard > 0
+  for sequence in sequences:
+    if sequence['shard']:
+      sequence['shard'] = False  # Reset shard flag.
+      length = sequence['len']
+      start = sequence['start']
+      end = sequence['end']
+      name = sequence['name']
+      assert end - start == length
+      if length > max_per_shard:
+        # Dividing sequence into smaller shards.
+        num_shards = int(math.floor(length / max_per_shard)) + 1
+        size = int(math.ceil(length / num_shards))
+        tf.logging.info(
+            'splitting sequence of length %d into %d shards of size %d' %
+            (length, num_shards, size))
+        last_end = 0
+        for i in range(num_shards):
+          shard_start = last_end
+          shard_end = min(length, shard_start + size)
+          if i == num_shards - 1:
+            shard_end = length
+          shard_len = shard_end - shard_start
+          total_shards_len += shard_len
+          shard_name = name + '_shard%02d' % i
+          last_end = shard_end
+
+          # Enqueuing shard.
+          if i == 0:  # Replace current sequence.
+            sequence['len'] = shard_len
+            sequence['start'] = shard_start
+            sequence['end'] = shard_end
+            sequence['name'] = shard_name
+          else:  # Enqueue new sequence.
+            sequences.append(
+                {'full': sequence['full'], 'name': shard_name,
+                 'len': shard_len, 'start': shard_start, 'end': shard_end,
+                 'num_views': sequence['num_views'], 'shard': False})
+
+        total_shards += num_shards
+        assert last_end == length
+
+  # Print resulting sharding.
+  if total_shards > 0:
+    tf.logging.info('%d shards of average length %d' %
+                    (total_shards, total_shards_len / total_shards))
+  return sorted(sequences, key=lambda k: k['name'])
+
+
+def RandomizeSets(sets):
+  """Randomize each set."""
+  for _, sequences in sorted(sets.iteritems()):
+    if sequences:
+      # Randomize order.
+      shuffle(sequences)
+
+
+def GetSpecificFrame(vid_path, frame_index):
+  """Gets a frame at a specified index in a video."""
+  cap = cv2.VideoCapture(vid_path)
+  cap.set(1, frame_index)
+  _, bgr = cap.read()
+  cap.release()
+  rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)
+  return rgb
+
+
+def JpegString(image, jpeg_quality=90):
+  """Returns given PIL.Image instance as jpeg string.
+
+  Args:
+    image: A PIL image.
+    jpeg_quality: The image quality, on a scale from 1 (worst) to 95 (best).
+
+  Returns:
+    a jpeg_string.
+  """
+  # This fix to PIL makes sure that we don't get an error when saving large
+  # jpeg files. This is a workaround for a bug in PIL. The value should be
+  # substantially larger than the size of the image being saved.
+  ImageFile.MAXBLOCK = 640 * 512 * 64
+
+  output_jpeg = StringIO()
+  image.save(output_jpeg, 'jpeg', quality=jpeg_quality, optimize=True)
+  return output_jpeg.getvalue()
+
+
+def ParallelPreprocessing(args):
+  """Parallel preprocessing: rotation, resize and jpeg encoding to string."""
+  (vid_path, timestep, num_timesteps, view) = args
+  try:
+    image = GetSpecificFrame(vid_path, timestep)
+
+    # Resizing.
+    resize_str = ''
+    if FLAGS.resize_min_edge > 0:
+      resize_str += ', resize ' + shapestring(image)
+      image = cv2resizeminedge(image, FLAGS.resize_min_edge)
+      resize_str += ' => ' + shapestring(image)
+
+    # Rotating.
+    rotate = None
+    if FLAGS.rotate:
+      rotate = FLAGS.rotate
+      if FLAGS.rotate_if_matching is not None:
+        rotate = None
+        patt = re.compile(FLAGS.rotate_if_matching)
+        if patt.match(vid_path) is not None:
+          rotate = FLAGS.rotate
+      if rotate is not None:
+        image = cv2rotateimage(image, FLAGS.rotate)
+
+    # Jpeg encoding.
+    image = Image.fromarray(image)
+    im_string = bytes_feature([JpegString(image)])
+
+    if timestep % FLAGS.log_frequency == 0:
+      tf.logging.info('Loaded frame %d / %d for %s (rotation %s%s) from %s' %
+                      (timestep, num_timesteps, view, str(rotate), resize_str,
+                       vid_path))
+    return im_string
+  except cv2.error as e:
+    tf.logging.error('Error while loading frame %d of %s: %s' %
+                     (timestep, vid_path, str(e)))
+    return None
+
+
+def GetNumFrames(vid_path):
+  """Gets the number of frames in a video."""
+  cap = cv2.VideoCapture(vid_path)
+  total_frames = cap.get(7)
+  cap.release()
+  return int(total_frames)
+
+
+def GetViewInfo(views_fullname):
+  """Return information about a group of views."""
+  view_paths = sorted(glob.glob(views_fullname))
+  num_frames = [GetNumFrames(i) for i in view_paths]
+  min_num_frames = min(num_frames)
+  num_views = len(view_paths)
+  return num_views, min_num_frames, view_paths, num_frames
+
+
+def AddSequence(sequence, writer, progress, errors):
+  """Converts a sequence to a SequenceExample.
+
+  Sequences have multiple viewpoint videos. Extract all frames from all
+  viewpoint videos in parallel, build a single SequenceExample containing
+  all viewpoint images for every timestep.
+
+  Args:
+    sequence: a dict with information on a sequence.
+    writer: A TFRecordWriter.
+    progress: A Progress object to report processing progress.
+    errors: a list of string to append to in case of errors.
+  """
+  fullname = sequence['full']
+  shortname = sequence['name']
+  start = sequence['start']
+  end = sequence['end']
+  num_timesteps = sequence['len']
+
+  # Build a list of all view paths for this fullname.
+  path = fullname + FLAGS.view_pattern[0] + '*'
+  tf.logging.info('Loading sequence from ' + path)
+  view_paths = sorted(glob.glob(path))
+  # Extract all images for all views
+  num_frames = [GetNumFrames(i) for i in view_paths]
+  tf.logging.info('Loading %s with [%d, %d[ (%d frames) from: %s %s' %
+                  (shortname, start, end, num_timesteps,
+                   str(num_frames), str(view_paths)))
+  num_views = len(view_paths)
+  total_timesteps = int(min(num_frames))
+  assert num_views == FLAGS.expected_views
+  assert num_views == sequence['num_views']
+
+  # Create a worker pool to parallelize loading/rotating
+  worker_pool = ThreadPool(multiprocessing.cpu_count())
+
+  # Collect all images for each view.
+  view_to_feature_list = {}
+  view_images = []
+  for view_idx, view in enumerate(
+      ['view'+str(i) for i in range(num_views)]):
+    # Flatten list to process in parallel
+    work = []
+    for i in range(start, end):
+      work.append((view_paths[view_idx], i, total_timesteps, view))
+    # Load and rotate images in parallel
+    view_images.append(worker_pool.map(ParallelPreprocessing, work))
+    # Report progress.
+    progress.Add(len(view_images[view_idx]))
+    tf.logging.info('%s' % str(progress))
+
+  # Remove error frames from all views
+  i = start
+  num_errors = 0
+  while i < len(view_images[0]):
+    remove_frame = False
+    # Check if one or more views have an error for this frame.
+    for view_idx in range(num_views):
+      if view_images[view_idx][i] is None:
+        remove_frame = True
+        error_msg = 'Removing frame %d for all views for %s ' % (i, fullname)
+        errors.append(error_msg)
+        tf.logging.error(error_msg)
+    # Remove faulty frames.
+    if remove_frame:
+      num_errors += 1
+      for view_idx in range(num_views):
+        del view_images[view_idx][i]
+    else:
+      i += 1
+
+  # Ignore sequences that have errors.
+  if num_errors > 0:
+    error_msg = 'Dropping sequence because of frame errors for %s' % fullname
+    errors.append(error_msg)
+    tf.logging.error(error_msg)
+  else:
+    # Build FeatureList objects for each view.
+    for view_idx, view in enumerate(
+        ['view'+str(i) for i in range(num_views)]):
+      # Construct FeatureList from repeated feature.
+      view_to_feature_list[view] = tf.train.FeatureList(
+          feature=view_images[view_idx])
+
+    context_features = tf.train.Features(feature={
+        'task': bytes_feature([shortname]),
+        'len': int64_feature([num_timesteps])
+    })
+    feature_lists = tf.train.FeatureLists(feature_list=view_to_feature_list)
+    ex = tf.train.SequenceExample(
+        context=context_features, feature_lists=feature_lists)
+    writer.write(ex.SerializeToString())
+    tf.logging.info('Done adding %s with %d timesteps'
+                    % (fullname, num_timesteps))
+
+
+def PrintSequencesInfo(sequences, prefix):
+  """Print information about sequences and return the total number of frames."""
+  tf.logging.info('')
+  tf.logging.info(prefix)
+  num_frames = 0
+  for sequence in sequences:
+    shard_str = ''
+    if sequence['shard']:
+      shard_str = ' (sharding)'
+    tf.logging.info('frames [%d, %d[\t(%d frames * %d views)%s\t%s' % (
+        sequence['start'], sequence['end'], sequence['len'],
+        sequence['num_views'], shard_str, sequence['name']))
+    num_frames += sequence['len'] * sequence['num_views']
+  tf.logging.info(('%d frames (all views), %d sequences, average sequence'
+                   ' length (all views): %d') %
+                  (num_frames, len(sequences), num_frames / len(sequences)))
+  tf.logging.info('')
+  return num_frames
+
+
+def CheckRecord(filename, sequence):
+  """Check that an existing tfrecord corresponds to the expected sequence."""
+  num_sequences = 0
+  total_frames = 0
+  for serialized_example in tf.python_io.tf_record_iterator(filename):
+    num_sequences += 1
+    example = tf.train.SequenceExample()
+    example.ParseFromString(serialized_example)
+    length = example.context.feature['len'].int64_list.value[0]
+    name = example.context.feature['task'].bytes_list.value[0]
+    total_frames += len(example.feature_lists.feature_list) * length
+    if sequence['name'] != name or sequence['len'] != length:
+      return False, total_frames
+  if num_sequences == 0:
+    return False, total_frames
+  return True, total_frames
+
+
+def AddSequences():
+  """Creates one training, validation."""
+  errors = []
+
+  # Generate datasets file lists.
+  sequences = FindPatternFiles(FLAGS.input_dir, FLAGS.view_pattern, errors)
+  num_frames = PrintSequencesInfo(sequences,
+                                  'Found the following datasets and files:')
+
+  # Sharding and randomizing sets.
+  if FLAGS.max_per_shard > 0:
+    sequences = ShardSequences(sequences, FLAGS.max_per_shard)
+    num_frames = PrintSequencesInfo(sequences, 'After sharding:')
+    tf.logging.info('')
+
+  # Process sets.
+  progress = Progress(num_frames)
+  output_list = []
+  for sequence in sequences:
+    record_name = os.path.join(
+        FLAGS.output_dir, '%s.tfrecord' % sequence['name'])
+    if tf.gfile.Exists(record_name) and not FLAGS.overwrite:
+      ok, num_frames = CheckRecord(record_name, sequence)
+      if ok:
+        progress.Add(num_frames)
+        tf.logging.info('Skipping existing output file: %s' % record_name)
+        continue
+      else:
+        tf.logging.info('File does not match sequence, reprocessing...')
+    output_dir = os.path.dirname(record_name)
+    if not tf.gfile.Exists(output_dir):
+      tf.logging.info('Creating output directory: %s' % output_dir)
+      tf.gfile.MakeDirs(output_dir)
+    output_list.append(record_name)
+    tf.logging.info('Writing to ' + record_name)
+    writer = tf.python_io.TFRecordWriter(record_name)
+    AddSequence(sequence, writer, progress, errors)
+    writer.close()
+  tf.logging.info('Wrote dataset files: ' + str(output_list))
+  tf.logging.info('All errors (%d): %s' % (len(errors), str(errors)))
+
+
+def main(_):
+  AddSequences()
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/dataset/webcam.py b/research/tcn/dataset/webcam.py
new file mode 100644
index 00000000..0ad98991
--- /dev/null
+++ b/research/tcn/dataset/webcam.py
@@ -0,0 +1,490 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Collect images from multiple simultaneous webcams.
+
+Usage:
+
+1. Define some environment variables that describe what you're collecting.
+dataset=your_dataset_name
+mode=train
+num_views=2
+viddir=/tmp/tcn/videos
+tmp_imagedir=/tmp/tcn/tmp_images
+debug_vids=1
+
+2. Run the script.
+export DISPLAY=:0.0 && \
+root=learning/brain/research/tcn && \
+bazel build -c opt --copt=-mavx tcn/webcam && \
+bazel-bin/tcn/webcam \
+--dataset $dataset \
+--mode $mode \
+--num_views $num_views \
+--tmp_imagedir $tmp_imagedir \
+--viddir $viddir \
+--debug_vids 1 \
+--logtostderr
+
+3. Hit Ctrl-C when done collecting, upon which the script will compile videos
+for each view and optionally a debug video concatenating multiple
+simultaneous views.
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import multiprocessing
+from multiprocessing import Process
+import os
+import subprocess
+import sys
+import time
+import cv2
+import matplotlib
+matplotlib.use('TkAgg')
+from matplotlib import animation  # pylint: disable=g-import-not-at-top
+import matplotlib.pyplot as plt
+import numpy as np
+import tensorflow as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+
+tf.flags.DEFINE_string('dataset', '', 'Name of the dataset we`re collecting.')
+tf.flags.DEFINE_string('mode', '',
+                       'What type of data we`re collecting. E.g.:'
+                       '`train`,`valid`,`test`, or `demo`')
+tf.flags.DEFINE_string('seqname', '',
+                       'Name of this sequence. If empty, the script will use'
+                       'the name seq_N+1 where seq_N is the latest'
+                       'integer-named sequence in the videos directory.')
+tf.flags.DEFINE_integer('num_views', 2,
+                        'Number of webcams.')
+tf.flags.DEFINE_string('tmp_imagedir', '/tmp/tcn/data',
+                       'Temporary outdir to write images.')
+tf.flags.DEFINE_string('viddir', '/tmp/tcn/videos',
+                       'Base directory to write debug videos.')
+tf.flags.DEFINE_boolean('debug_vids', True,
+                        'Whether to generate debug vids with multiple'
+                        'concatenated views.')
+tf.flags.DEFINE_string('debug_lhs_view', '0',
+                       'Which viewpoint to use for the lhs video.')
+tf.flags.DEFINE_string('debug_rhs_view', '1',
+                       'Which viewpoint to use for the rhs video.')
+tf.flags.DEFINE_integer('height', 1080, 'Raw input height.')
+tf.flags.DEFINE_integer('width', 1920, 'Raw input width.')
+tf.flags.DEFINE_string('webcam_ports', None,
+                       'Comma-separated list of each webcam usb port.')
+FLAGS = tf.app.flags.FLAGS
+
+
+class ImageQueue(object):
+  """An image queue holding each stream's most recent image.
+
+  Basically implements a process-safe collections.deque(maxlen=1).
+  """
+
+  def __init__(self):
+    self.lock = multiprocessing.Lock()
+    self._queue = multiprocessing.Queue(maxsize=1)
+
+  def append(self, data):
+    with self.lock:
+      if self._queue.full():
+        # Pop the first element.
+        _ = self._queue.get()
+      self._queue.put(data)
+
+  def get(self):
+    with self.lock:
+      return self._queue.get()
+
+  def empty(self):
+    return self._queue.empty()
+
+  def close(self):
+    return self._queue.close()
+
+
+class WebcamViewer(object):
+  """A class which displays a live stream from the webcams."""
+
+  def __init__(self, display_queues):
+    """Create a WebcamViewer instance."""
+    self.height = FLAGS.height
+    self.width = FLAGS.width
+    self.queues = display_queues
+
+  def _get_next_images(self):
+    """Gets the next image to display."""
+    # Wait for one image per view.
+    not_found = True
+    while not_found:
+      if True in [q.empty() for q in self.queues]:
+        # At least one image queue is empty; wait.
+        continue
+      else:
+        # Retrieve the images.
+        latest = [q.get() for q in self.queues]
+        combined = np.concatenate(latest, axis=1)
+      not_found = False
+    return combined
+
+  def run(self):
+    """Displays the Kcam live stream in a window.
+
+    This function blocks until the window is closed.
+    """
+    fig, rgb_axis = plt.subplots()
+
+    image_rows = self.height
+    image_cols = self.width * FLAGS.num_views
+    initial_image = np.zeros((image_rows, image_cols, 3))
+    rgb_image = rgb_axis.imshow(initial_image, interpolation='nearest')
+
+    def update_figure(frame_index):
+      """Animation function for matplotlib FuncAnimation. Updates the image.
+
+      Args:
+        frame_index: The frame number.
+      Returns:
+        An iterable of matplotlib drawables to clear.
+      """
+      _ = frame_index
+      images = self._get_next_images()
+      images = images[..., [2, 1, 0]]
+      rgb_image.set_array(images)
+      return rgb_image,
+
+    # We must keep a reference to this animation in order for it to work.
+    unused_animation = animation.FuncAnimation(
+        fig, update_figure, interval=50, blit=True)
+    mng = plt.get_current_fig_manager()
+    mng.resize(*mng.window.maxsize())
+    plt.show()
+
+
+def reconcile(queues, write_queue):
+  """Gets a list of concurrent images from each view queue.
+
+  This waits for latest images to be available in all view queues,
+  then continuously:
+  - Creates a list of current images for each view.
+  - Writes the list to a queue of image lists to write to disk.
+  Args:
+    queues: A list of `ImageQueues`, holding the latest image from each webcam.
+    write_queue: A multiprocessing.Queue holding lists of concurrent images.
+  """
+  # Loop forever.
+  while True:
+    # Wait till all queues have an image.
+    if True in [q.empty() for q in queues]:
+      continue
+    else:
+      # Retrieve all views' images.
+      latest = [q.get() for q in queues]
+      # Copy the list of all concurrent images to the write queue.
+      write_queue.put(latest)
+
+
+def persist(write_queue, view_dirs):
+  """Pulls lists of concurrent images off a write queue, writes them to disk.
+
+  Args:
+    write_queue: A multiprocessing.Queue holding lists of concurrent images;
+      one image per view.
+    view_dirs: A list of strings, holding the output image directories for each
+      view.
+  """
+  timestep = 0
+  while True:
+    # Wait till there is work in the queue.
+    if write_queue.empty():
+      continue
+    # Get a list of concurrent images to write to disk.
+    view_ims = write_queue.get()
+    for view_idx, image in enumerate(view_ims):
+      view_base = view_dirs[view_idx]
+      # Assign all concurrent view images the same sequence timestep.
+      fname = os.path.join(view_base, '%s.png' % str(timestep).zfill(10))
+      cv2.imwrite(fname, image)
+    # Move to the next timestep.
+    timestep += 1
+
+
+def get_image(camera):
+  """Captures a single image from the camera and returns it in PIL format."""
+  data = camera.read()
+  _, im = data
+  return im
+
+
+def capture_webcam(camera, display_queue, reconcile_queue):
+  """Captures images from simultaneous webcams, writes them to queues.
+
+  Args:
+    camera: A cv2.VideoCapture object representing an open webcam stream.
+    display_queue: An ImageQueue.
+    reconcile_queue: An ImageQueue.
+  """
+  # Take some ramp images to allow cams to adjust for brightness etc.
+  for i in range(60):
+    tf.logging.info('Taking ramp image %d.' % i)
+    get_image(camera)
+
+  cnt = 0
+  start = time.time()
+  while True:
+    # Get images for all cameras.
+    im = get_image(camera)
+    # Replace the current image in the display and reconcile queues.
+    display_queue.append(im)
+    reconcile_queue.append(im)
+    cnt += 1
+    current = time.time()
+    if cnt % 100 == 0:
+      tf.logging.info('Collected %s of video, %d frames at ~%.2f fps.' % (
+          timer(start, current), cnt, cnt/(current-start)))
+
+
+def timer(start, end):
+  """Returns a formatted time elapsed."""
+  hours, rem = divmod(end-start, 3600)
+  minutes, seconds = divmod(rem, 60)
+  return '{:0>2}:{:0>2}:{:05.2f}'.format(int(hours), int(minutes), seconds)
+
+
+def display_webcams(display_queues):
+  """Builds an WebcamViewer to animate incoming images, runs it."""
+  viewer = WebcamViewer(display_queues)
+  viewer.run()
+
+
+def create_vids(view_dirs, seqname):
+  """Creates one video per view per sequence."""
+  vidbase = os.path.join(FLAGS.viddir, FLAGS.dataset, FLAGS.mode)
+  if not os.path.exists(vidbase):
+    os.makedirs(vidbase)
+  vidpaths = []
+  for idx, view_dir in enumerate(view_dirs):
+    vidname = os.path.join(vidbase, '%s_view%d.mp4' % (seqname, idx))
+    encode_vid_cmd = r'mencoder mf://%s/*.png \
+    -mf fps=29:type=png \
+    -ovc lavc -lavcopts vcodec=mpeg4:mbd=2:trell \
+    -oac copy -o %s' % (view_dir, vidname)
+    os.system(encode_vid_cmd)
+    vidpaths.append(vidname)
+
+  debugpath = None
+  if FLAGS.debug_vids:
+    lhs = vidpaths[FLAGS.debug_lhs_view]
+    rhs = vidpaths[FLAGS.debug_rhs_view]
+    debug_base = os.path.join('%s_debug' % FLAGS.viddir, FLAGS.dataset,
+                              FLAGS.mode)
+    if not os.path.exists(debug_base):
+      os.makedirs(debug_base)
+    debugpath = '%s/%s.mp4' % (debug_base, seqname)
+    os.system(r"avconv \
+      -i %s \
+      -i %s \
+      -filter_complex '[0:v]pad=iw*2:ih[int];[int][1:v]overlay=W/2:0[vid]' \
+      -map [vid] \
+      -c:v libx264 \
+      -crf 23 \
+      -preset veryfast \
+      %s" % (lhs, rhs, debugpath))
+
+  return vidpaths, debugpath
+
+
+def setup_paths():
+  """Sets up the necessary paths to collect videos."""
+  assert FLAGS.dataset
+  assert FLAGS.mode
+  assert FLAGS.num_views
+
+  # Setup directory for final images used to create videos for this sequence.
+  tmp_imagedir = os.path.join(FLAGS.tmp_imagedir, FLAGS.dataset, FLAGS.mode)
+  if not os.path.exists(tmp_imagedir):
+    os.makedirs(tmp_imagedir)
+
+  # Create a base directory to hold all sequence videos if it doesn't exist.
+  vidbase = os.path.join(FLAGS.viddir, FLAGS.dataset, FLAGS.mode)
+  if not os.path.exists(vidbase):
+    os.makedirs(vidbase)
+
+  # Get one directory per concurrent view and a sequence name.
+  view_dirs, seqname = get_view_dirs(vidbase, tmp_imagedir)
+
+  # Get an output path to each view's video.
+  vid_paths = []
+  for idx, _ in enumerate(view_dirs):
+    vid_path = os.path.join(vidbase, '%s_view%d.mp4' % (seqname, idx))
+    vid_paths.append(vid_path)
+
+  # Optionally build paths to debug_videos.
+  debug_path = None
+  if FLAGS.debug_vids:
+    debug_base = os.path.join('%s_debug' % FLAGS.viddir, FLAGS.dataset,
+                              FLAGS.mode)
+    if not os.path.exists(debug_base):
+      os.makedirs(debug_base)
+    debug_path = '%s/%s.mp4' % (debug_base, seqname)
+
+  return view_dirs, vid_paths, debug_path
+
+
+def get_view_dirs(vidbase, tmp_imagedir):
+  """Creates and returns one view directory per webcam."""
+  # Create and append a sequence name.
+  if FLAGS.seqname:
+    seqname = FLAGS.seqname
+  else:
+    # If there's no video directory, this is the first sequence.
+    if not os.listdir(vidbase):
+      seqname = '0'
+    else:
+      # Otherwise, get the latest sequence name and increment it.
+      seq_names = [i.split('_')[0] for i in os.listdir(vidbase)]
+      latest_seq = sorted(map(int, seq_names), reverse=True)[0]
+      seqname = str(latest_seq+1)
+    tf.logging.info('No seqname specified, using: %s' % seqname)
+  view_dirs = [os.path.join(
+      tmp_imagedir, '%s_view%d' % (seqname, v)) for v in range(FLAGS.num_views)]
+  for d in view_dirs:
+    if not os.path.exists(d):
+      os.makedirs(d)
+  return view_dirs, seqname
+
+
+def get_cameras():
+  """Opens cameras using cv2, ensures they can take images."""
+  # Try to get free webcam ports.
+  if FLAGS.webcam_ports:
+    ports = map(int, FLAGS.webcam_ports.split(','))
+  else:
+    ports = range(FLAGS.num_views)
+  cameras = [cv2.VideoCapture(i) for i in ports]
+
+  if not all([i.isOpened() for i in cameras]):
+    try:
+      # Try to find and kill hanging cv2 process_ids.
+      output = subprocess.check_output(['lsof -t /dev/video*'], shell=True)
+      tf.logging.info('Found hanging cv2 process_ids: \n')
+      tf.logging.info(output)
+      tf.logging.info('Killing hanging processes...')
+      for process_id in output.split('\n')[:-1]:
+        subprocess.call(['kill %s' % process_id], shell=True)
+      time.sleep(3)
+      # Recapture webcams.
+      cameras = [cv2.VideoCapture(i) for i in ports]
+    except subprocess.CalledProcessError:
+      raise ValueError(
+          'Cannot connect to cameras. Try running: \n'
+          'ls -ltrh /dev/video* \n '
+          'to see which ports your webcams are connected to. Then hand those '
+          'ports as a comma-separated list to --webcam_ports, e.g. '
+          '--webcam_ports 0,1')
+
+  # Verify each camera is able to capture images.
+  ims = map(get_image, cameras)
+  assert False not in [i is not None for i in ims]
+  return cameras
+
+
+def launch_images_to_videos(view_dirs, vid_paths, debug_path):
+  """Launch job in separate process to convert images to videos."""
+
+  f = 'learning/brain/research/tcn/dataset/images_to_videos.py'
+  cmd = ['python %s ' % f]
+  cmd += ['--view_dirs %s ' % ','.join(i for i in view_dirs)]
+  cmd += ['--vid_paths %s ' % ','.join(i for i in vid_paths)]
+  cmd += ['--debug_path %s ' % debug_path]
+  cmd += ['--debug_lhs_view %s ' % FLAGS.debug_lhs_view]
+  cmd += ['--debug_rhs_view %s ' % FLAGS.debug_rhs_view]
+  cmd += [' & ']
+  cmd = ''.join(i for i in cmd)
+
+  # Call images_to_videos asynchronously.
+  fnull = open(os.devnull, 'w')
+  subprocess.Popen([cmd], stdout=fnull, stderr=subprocess.STDOUT, shell=True)
+
+  for p in vid_paths:
+    tf.logging.info('Writing final video to: %s' % p)
+  if debug_path:
+    tf.logging.info('Writing debug video to: %s' % debug_path)
+
+
+def main(_):
+  # Initialize the camera capture objects.
+  cameras = get_cameras()
+  # Get one output directory per view.
+  view_dirs, vid_paths, debug_path = setup_paths()
+  try:
+    # Wait for user input.
+    try:
+      tf.logging.info('About to write to:')
+      for v in view_dirs:
+        tf.logging.info(v)
+      raw_input('Press Enter to continue...')
+    except SyntaxError:
+      pass
+
+    # Create a queue per view for displaying and saving images.
+    display_queues = [ImageQueue() for _ in range(FLAGS.num_views)]
+    reconcile_queues = [ImageQueue() for _ in range(FLAGS.num_views)]
+
+    # Create a queue for collecting all tuples of multi-view images to write to
+    # disk.
+    write_queue = multiprocessing.Queue()
+
+    processes = []
+    # Create a process to display collected images in real time.
+    processes.append(Process(target=display_webcams, args=(display_queues,)))
+    # Create a process to collect the latest simultaneous images from each view.
+    processes.append(Process(
+        target=reconcile, args=(reconcile_queues, write_queue,)))
+    # Create a process to collect the latest simultaneous images from each view.
+    processes.append(Process(
+        target=persist, args=(write_queue, view_dirs,)))
+
+    for (cam, dq, rq) in zip(cameras, display_queues, reconcile_queues):
+      processes.append(Process(
+          target=capture_webcam, args=(cam, dq, rq,)))
+
+    for p in processes:
+      p.start()
+    for p in processes:
+      p.join()
+
+  except KeyboardInterrupt:
+    # Close the queues.
+    for q in display_queues + reconcile_queues:
+      q.close()
+    # Release the cameras.
+    for cam in cameras:
+      cam.release()
+
+    # Launch images_to_videos script asynchronously.
+    launch_images_to_videos(view_dirs, vid_paths, debug_path)
+
+    try:
+      sys.exit(0)
+    except SystemExit:
+      os._exit(0)  # pylint: disable=protected-access
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/download_pretrained.py b/research/tcn/download_pretrained.py
new file mode 100644
index 00000000..4d42ee73
--- /dev/null
+++ b/research/tcn/download_pretrained.py
@@ -0,0 +1,54 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Downloads pretrained InceptionV3 and ResnetV2-50 checkpoints."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import tarfile
+import urllib
+
+INCEPTION_URL = 'http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz'
+RESNET_URL = 'http://download.tensorflow.org/models/resnet_v2_50_2017_04_14.tar.gz'
+
+
+def DownloadWeights(model_dir, url):
+  os.makedirs(model_dir)
+  tar_path = os.path.join(model_dir, 'ckpt.tar.gz')
+  urllib.urlretrieve(url, tar_path)
+  tar = tarfile.open(os.path.join(model_dir, 'ckpt.tar.gz'))
+  tar.extractall(model_dir)
+
+
+if __name__ == '__main__':
+
+  # Create a directory for all pretrained checkpoints.
+  ckpt_dir = 'pretrained_checkpoints'
+  if not os.path.exists(ckpt_dir):
+    os.makedirs(ckpt_dir)
+
+  # Download inception.
+  print('Downloading inception pretrained weights...')
+  inception_dir = os.path.join(ckpt_dir, 'inception')
+  DownloadWeights(inception_dir, INCEPTION_URL)
+  print('Done downloading inception pretrained weights.')
+
+  print('Downloading resnet pretrained weights...')
+  resnet_dir = os.path.join(ckpt_dir, 'resnet')
+  DownloadWeights(resnet_dir, RESNET_URL)
+  print('Done downloading resnet pretrained weights.')
+
diff --git a/research/tcn/estimators/base_estimator.py b/research/tcn/estimators/base_estimator.py
new file mode 100644
index 00000000..742da66f
--- /dev/null
+++ b/research/tcn/estimators/base_estimator.py
@@ -0,0 +1,702 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Base estimator defining TCN training, test, and inference."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from abc import ABCMeta
+from abc import abstractmethod
+import os
+import numpy as np
+import numpy as np
+import data_providers
+import preprocessing
+from utils import util
+import tensorflow as tf
+import tensorflow.contrib.slim as slim
+from tensorflow.contrib.tpu.python.tpu import tpu_config
+from tensorflow.contrib.tpu.python.tpu import tpu_estimator
+from tensorflow.contrib.tpu.python.tpu import tpu_optimizer
+from tensorflow.python.training import session_run_hook
+
+tf.app.flags.DEFINE_integer(
+    'tf_random_seed', 0, 'Random seed.')
+FLAGS = tf.app.flags.FLAGS
+
+
+class InitFromPretrainedCheckpointHook(session_run_hook.SessionRunHook):
+  """Hook that can init graph from a pretrained checkpoint."""
+
+  def __init__(self, pretrained_checkpoint_dir):
+    """Initializes a `InitFromPretrainedCheckpointHook`.
+
+    Args:
+      pretrained_checkpoint_dir: The dir of pretrained checkpoint.
+
+    Raises:
+      ValueError: If pretrained_checkpoint_dir is invalid.
+    """
+    if pretrained_checkpoint_dir is None:
+      raise ValueError('pretrained_checkpoint_dir must be specified.')
+    self._pretrained_checkpoint_dir = pretrained_checkpoint_dir
+
+  def begin(self):
+    checkpoint_reader = tf.contrib.framework.load_checkpoint(
+        self._pretrained_checkpoint_dir)
+    variable_shape_map = checkpoint_reader.get_variable_to_shape_map()
+
+    exclude_scopes = 'logits/,final_layer/,aux_'
+    # Skip restoring global_step as to run fine tuning from step=0.
+    exclusions = ['global_step']
+    if exclude_scopes:
+      exclusions.extend([scope.strip() for scope in exclude_scopes.split(',')])
+
+    variable_to_restore = tf.contrib.framework.get_model_variables()
+
+    # Variable filtering by given exclude_scopes.
+    filtered_variables_to_restore = {}
+    for v in variable_to_restore:
+      excluded = False
+      for exclusion in exclusions:
+        if v.name.startswith(exclusion):
+          excluded = True
+          break
+      if not excluded:
+        var_name = v.name.split(':')[0]
+        filtered_variables_to_restore[var_name] = v
+
+    # Final filter by checking shape matching and skipping variables that
+    # are not in the checkpoint.
+    final_variables_to_restore = {}
+    for var_name, var_tensor in filtered_variables_to_restore.iteritems():
+      if var_name not in variable_shape_map:
+        # Try moving average version of variable.
+        var_name = os.path.join(var_name, 'ExponentialMovingAverage')
+        if var_name not in variable_shape_map:
+          tf.logging.info(
+              'Skip init [%s] because it is not in ckpt.', var_name)
+          # Skip variables not in the checkpoint.
+          continue
+
+      if not var_tensor.get_shape().is_compatible_with(
+          variable_shape_map[var_name]):
+        # Skip init variable from ckpt if shape dismatch.
+        tf.logging.info(
+            'Skip init [%s] from [%s] in ckpt because shape dismatch: %s vs %s',
+            var_tensor.name, var_name,
+            var_tensor.get_shape(), variable_shape_map[var_name])
+        continue
+
+      tf.logging.info('Init %s from %s in ckpt' % (var_tensor, var_name))
+      final_variables_to_restore[var_name] = var_tensor
+
+    self._init_fn = tf.contrib.framework.assign_from_checkpoint_fn(
+        self._pretrained_checkpoint_dir,
+        final_variables_to_restore)
+
+  def after_create_session(self, session, coord):
+    tf.logging.info('Restoring InceptionV3 weights.')
+    self._init_fn(session)
+    tf.logging.info('Done restoring InceptionV3 weights.')
+
+
+class BaseEstimator(object):
+  """Abstract TCN base estimator class."""
+  __metaclass__ = ABCMeta
+
+  def __init__(self, config, logdir):
+    """Constructor.
+
+    Args:
+      config: A Luatable-like T object holding training config.
+      logdir: String, a directory where checkpoints and summaries are written.
+    """
+    self._config = config
+    self._logdir = logdir
+
+  @abstractmethod
+  def construct_input_fn(self, records, is_training):
+    """Builds an estimator input_fn.
+
+    The input_fn is used to pass feature and target data to the train,
+    evaluate, and predict methods of the Estimator.
+
+    Method to be overridden by implementations.
+
+    Args:
+      records: A list of Strings, paths to TFRecords with image data.
+      is_training: Boolean, whether or not we're training.
+
+    Returns:
+      Function, that has signature of ()->(dict of features, target).
+        features is a dict mapping feature names to `Tensors`
+        containing the corresponding feature data (typically, just a single
+        key/value pair 'raw_data' -> image `Tensor` for TCN.
+        labels is a 1-D int32 `Tensor` holding labels.
+    """
+    pass
+
+  def preprocess_data(self, images, is_training):
+    """Preprocesses raw images for either training or inference.
+
+    Args:
+      images: A 4-D float32 `Tensor` holding images to preprocess.
+      is_training: Boolean, whether or not we're in training.
+
+    Returns:
+      data_preprocessed: data after the preprocessor.
+    """
+    config = self._config
+    height = config.data.height
+    width = config.data.width
+    min_scale = config.data.augmentation.minscale
+    max_scale = config.data.augmentation.maxscale
+    p_scale_up = config.data.augmentation.proportion_scaled_up
+    aug_color = config.data.augmentation.color
+    fast_mode = config.data.augmentation.fast_mode
+    crop_strategy = config.data.preprocessing.eval_cropping
+    preprocessed_images = preprocessing.preprocess_images(
+        images, is_training, height, width,
+        min_scale, max_scale, p_scale_up,
+        aug_color=aug_color, fast_mode=fast_mode,
+        crop_strategy=crop_strategy)
+    return preprocessed_images
+
+  @abstractmethod
+  def forward(self, images, is_training, reuse=False):
+    """Defines the forward pass that converts batch images to embeddings.
+
+    Method to be overridden by implementations.
+
+    Args:
+      images: A 4-D float32 `Tensor` holding images to be embedded.
+      is_training: Boolean, whether or not we're in training mode.
+      reuse: Boolean, whether or not to reuse embedder.
+    Returns:
+      embeddings: A 2-D float32 `Tensor` holding embedded images.
+    """
+    pass
+
+  @abstractmethod
+  def define_loss(self, embeddings, labels, is_training):
+    """Defines the loss function on the embedding vectors.
+
+    Method to be overridden by implementations.
+
+    Args:
+      embeddings: A 2-D float32 `Tensor` holding embedded images.
+      labels: A 1-D int32 `Tensor` holding problem labels.
+      is_training: Boolean, whether or not we're in training mode.
+
+    Returns:
+      loss: tf.float32 scalar.
+    """
+    pass
+
+  @abstractmethod
+  def define_eval_metric_ops(self):
+    """Defines the dictionary of eval metric tensors.
+
+    Method to be overridden by implementations.
+
+    Returns:
+      eval_metric_ops:  A dict of name/value pairs specifying the
+        metrics that will be calculated when the model runs in EVAL mode.
+    """
+    pass
+
+  def get_train_op(self, loss):
+    """Creates a training op.
+
+    Args:
+      loss: A float32 `Tensor` representing the total training loss.
+    Returns:
+      train_op: A slim.learning.create_train_op train_op.
+    Raises:
+      ValueError: If specified optimizer isn't supported.
+    """
+    # Get variables to train (defined in subclass).
+    assert self.variables_to_train
+
+    # Define a learning rate schedule.
+    decay_steps = self._config.learning.decay_steps
+    decay_factor = self._config.learning.decay_factor
+    learning_rate = float(self._config.learning.learning_rate)
+
+    # Define a learning rate schedule.
+    global_step = slim.get_or_create_global_step()
+    learning_rate = tf.train.exponential_decay(
+        learning_rate,
+        global_step,
+        decay_steps,
+        decay_factor,
+        staircase=True)
+
+    # Create an optimizer.
+    opt_type = self._config.learning.optimizer
+    if opt_type == 'adam':
+      opt = tf.train.AdamOptimizer(learning_rate)
+    elif opt_type == 'momentum':
+      opt = tf.train.MomentumOptimizer(learning_rate, 0.9)
+    elif opt_type == 'rmsprop':
+      opt = tf.train.RMSPropOptimizer(learning_rate, momentum=0.9,
+                                      epsilon=1.0, decay=0.9)
+    else:
+      raise ValueError('Unsupported optimizer %s' % opt_type)
+
+    if self._config.use_tpu:
+      opt = tpu_optimizer.CrossShardOptimizer(opt)
+
+    # Create a training op.
+    # train_op = opt.minimize(loss, var_list=self.variables_to_train)
+    # Create a training op.
+    train_op = slim.learning.create_train_op(
+        loss,
+        optimizer=opt,
+        variables_to_train=self.variables_to_train,
+        update_ops=tf.get_collection(tf.GraphKeys.UPDATE_OPS))
+
+    return train_op
+
+  def _get_model_fn(self):
+    """Defines behavior for training, evaluation, and inference (prediction).
+
+    Returns:
+      `model_fn` for `Estimator`.
+    """
+    # pylint: disable=unused-argument
+    def model_fn(features, labels, mode, params):
+      """Build the model based on features, labels, and mode.
+
+      Args:
+        features: Dict, strings to `Tensor` input data, returned by the
+          input_fn.
+        labels: The labels Tensor returned by the input_fn.
+        mode: A string indicating the mode. This will be either
+          tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.PREDICT,
+          or tf.estimator.ModeKeys.EVAL.
+        params: A dict holding training parameters, passed in during TPU
+          training.
+
+      Returns:
+        A tf.estimator.EstimatorSpec specifying train/test/inference behavior.
+      """
+      is_training = mode == tf.estimator.ModeKeys.TRAIN
+
+      # Get preprocessed images from the features dict.
+      batch_preprocessed = features['batch_preprocessed']
+
+      # Do a forward pass to embed data.
+      batch_encoded = self.forward(batch_preprocessed, is_training)
+
+      # Optionally set the pretrained initialization function.
+      initializer_fn = None
+      if mode == tf.estimator.ModeKeys.TRAIN:
+        initializer_fn = self.pretrained_init_fn
+
+      # If we're training or evaluating, define total loss.
+      total_loss = None
+      if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
+        loss = self.define_loss(batch_encoded, labels, is_training)
+        tf.losses.add_loss(loss)
+        total_loss = tf.losses.get_total_loss()
+
+      # If we're training, define a train op.
+      train_op = None
+      if mode == tf.estimator.ModeKeys.TRAIN:
+        train_op = self.get_train_op(total_loss)
+
+      # If we're doing inference, set the output to be the embedded images.
+      predictions_dict = None
+      if mode == tf.estimator.ModeKeys.PREDICT:
+        predictions_dict = {'embeddings': batch_encoded}
+        # Pass through additional metadata stored in features.
+        for k, v in features.iteritems():
+          predictions_dict[k] = v
+
+      # If we're evaluating, define some eval metrics.
+      eval_metric_ops = None
+      if mode == tf.estimator.ModeKeys.EVAL:
+        eval_metric_ops = self.define_eval_metric_ops()
+
+      # Define training scaffold to load pretrained weights.
+      num_checkpoint_to_keep = self._config.logging.checkpoint.num_to_keep
+      saver = tf.train.Saver(
+          max_to_keep=num_checkpoint_to_keep)
+
+      if is_training and self._config.use_tpu:
+        # TPU doesn't have a scaffold option at the moment, so initialize
+        # pretrained weights using a custom train_hook instead.
+        return tpu_estimator.TPUEstimatorSpec(
+            mode,
+            loss=total_loss,
+            eval_metrics=None,
+            train_op=train_op,
+            predictions=predictions_dict)
+      else:
+        # Build a scaffold to initialize pretrained weights.
+        scaffold = tf.train.Scaffold(
+            init_fn=initializer_fn,
+            saver=saver,
+            summary_op=None)
+        return tf.estimator.EstimatorSpec(
+            mode=mode,
+            predictions=predictions_dict,
+            loss=total_loss,
+            train_op=train_op,
+            eval_metric_ops=eval_metric_ops,
+            scaffold=scaffold)
+    return model_fn
+
+  def train(self):
+    """Runs training."""
+    # Get a list of training tfrecords.
+    config = self._config
+    training_dir = config.data.training
+    training_records = util.GetFilesRecursively(training_dir)
+
+    # Define batch size.
+    self._batch_size = config.data.batch_size
+
+    # Create a subclass-defined training input function.
+    train_input_fn = self.construct_input_fn(
+        training_records, is_training=True)
+
+    # Create the estimator.
+    estimator = self._build_estimator(is_training=True)
+
+    train_hooks = None
+    if config.use_tpu:
+      # TPU training initializes pretrained weights using a custom train hook.
+      train_hooks = []
+      if tf.train.latest_checkpoint(self._logdir) is None:
+        train_hooks.append(
+            InitFromPretrainedCheckpointHook(
+                config[config.embedder_strategy].pretrained_checkpoint))
+
+    # Run training.
+    estimator.train(input_fn=train_input_fn, hooks=train_hooks,
+                    steps=config.learning.max_step)
+
+  def _build_estimator(self, is_training):
+    """Returns an Estimator object.
+
+    Args:
+      is_training: Boolean, whether or not we're in training mode.
+
+    Returns:
+      A tf.estimator.Estimator.
+    """
+    config = self._config
+    save_checkpoints_steps = config.logging.checkpoint.save_checkpoints_steps
+    keep_checkpoint_max = self._config.logging.checkpoint.num_to_keep
+    if is_training and config.use_tpu:
+      iterations = config.tpu.iterations
+      num_shards = config.tpu.num_shards
+      run_config = tpu_config.RunConfig(
+          save_checkpoints_secs=None,
+          save_checkpoints_steps=save_checkpoints_steps,
+          keep_checkpoint_max=keep_checkpoint_max,
+          master=FLAGS.master,
+          evaluation_master=FLAGS.master,
+          model_dir=self._logdir,
+          tpu_config=tpu_config.TPUConfig(
+              iterations_per_loop=iterations,
+              num_shards=num_shards,
+              per_host_input_for_training=num_shards <= 8),
+          tf_random_seed=FLAGS.tf_random_seed)
+
+      batch_size = config.data.batch_size
+      return tpu_estimator.TPUEstimator(
+          model_fn=self._get_model_fn(),
+          config=run_config,
+          use_tpu=True,
+          train_batch_size=batch_size,
+          eval_batch_size=batch_size)
+    else:
+      run_config = tf.estimator.RunConfig().replace(
+          model_dir=self._logdir,
+          save_checkpoints_steps=save_checkpoints_steps,
+          keep_checkpoint_max=keep_checkpoint_max,
+          tf_random_seed=FLAGS.tf_random_seed)
+      return tf.estimator.Estimator(
+          model_fn=self._get_model_fn(),
+          config=run_config)
+
+  def evaluate(self):
+    """Runs `Estimator` validation.
+    """
+    config = self._config
+
+    # Get a list of validation tfrecords.
+    validation_dir = config.data.validation
+    validation_records = util.GetFilesRecursively(validation_dir)
+
+    # Define batch size.
+    self._batch_size = config.data.batch_size
+
+    # Create a subclass-defined training input function.
+    validation_input_fn = self.construct_input_fn(
+        validation_records, False)
+
+    # Create the estimator.
+    estimator = self._build_estimator(is_training=False)
+
+    # Run validation.
+    eval_batch_size = config.data.batch_size
+    num_eval_samples = config.val.num_eval_samples
+    num_eval_batches = int(num_eval_samples / eval_batch_size)
+    estimator.evaluate(input_fn=validation_input_fn, steps=num_eval_batches)
+
+  def inference(
+      self, inference_input, checkpoint_path, batch_size=None, **kwargs):
+    """Defines 3 of modes of inference.
+
+    Inputs:
+    * Mode 1: Input is an input_fn.
+    * Mode 2: Input is a TFRecord (or list of TFRecords).
+    * Mode 3: Input is a numpy array holding an image (or array of images).
+
+    Outputs:
+    * Mode 1: this returns an iterator over embeddings and additional
+      metadata. See
+      https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#predict
+      for details.
+    * Mode 2: Returns an iterator over tuples of
+      (embeddings, raw_image_strings, sequence_name), where embeddings is a
+      2-D float32 numpy array holding [sequence_size, embedding_size] image
+      embeddings, raw_image_strings is a 1-D string numpy array holding
+      [sequence_size] jpeg-encoded image strings, and sequence_name is a
+      string holding the name of the embedded sequence.
+    * Mode 3: Returns a tuple of (embeddings, raw_image_strings), where
+      embeddings is a 2-D float32 numpy array holding
+      [batch_size, embedding_size] image embeddings, raw_image_strings is a
+      1-D string numpy array holding [batch_size] jpeg-encoded image strings.
+
+    Args:
+      inference_input: This can be a tf.Estimator input_fn, a TFRecord path,
+        a list of TFRecord paths, a numpy image, or an array of numpy images.
+      checkpoint_path: String, path to the checkpoint to restore for inference.
+      batch_size: Int, the size of the batch to use for inference.
+      **kwargs: Additional keyword arguments, depending on the mode.
+        See _input_fn_inference, _tfrecord_inference, and _np_inference.
+    Returns:
+      inference_output: Inference output depending on mode, see above for
+        details.
+    Raises:
+      ValueError: If inference_input isn't a tf.Estimator input_fn,
+        a TFRecord path, a list of TFRecord paths, or a numpy array,
+    """
+    # Mode 1: input is a callable tf.Estimator input_fn.
+    if callable(inference_input):
+      return self._input_fn_inference(
+          input_fn=inference_input, checkpoint_path=checkpoint_path, **kwargs)
+    # Mode 2: Input is a TFRecord path (or list of TFRecord paths).
+    elif util.is_tfrecord_input(inference_input):
+      return self._tfrecord_inference(
+          records=inference_input, checkpoint_path=checkpoint_path,
+          batch_size=batch_size, **kwargs)
+    # Mode 3: Input is a numpy array of raw images.
+    elif util.is_np_array(inference_input):
+      return self._np_inference(
+          np_images=inference_input, checkpoint_path=checkpoint_path, **kwargs)
+    else:
+      raise ValueError(
+          'inference input must be a tf.Estimator input_fn, a TFRecord path,'
+          'a list of TFRecord paths, or a numpy array. Got: %s' % str(type(
+              inference_input)))
+
+  def _input_fn_inference(self, input_fn, checkpoint_path, predict_keys=None):
+    """Mode 1: tf.Estimator inference.
+
+    Args:
+      input_fn: Function, that has signature of ()->(dict of features, None).
+        This is a function called by the estimator to get input tensors (stored
+        in the features dict) to do inference over.
+      checkpoint_path: String, path to a specific checkpoint to restore.
+      predict_keys: List of strings, the keys of the `Tensors` in the features
+        dict (returned by the input_fn) to evaluate during inference.
+    Returns:
+      predictions: An Iterator, yielding evaluated values of `Tensors`
+        specified in `predict_keys`.
+    """
+    # Create the estimator.
+    estimator = self._build_estimator(is_training=False)
+
+    # Create an iterator of predicted embeddings.
+    predictions = estimator.predict(input_fn=input_fn,
+                                    checkpoint_path=checkpoint_path,
+                                    predict_keys=predict_keys)
+    return predictions
+
+  def _tfrecord_inference(self, records, checkpoint_path, batch_size,
+                          num_sequences=-1, reuse=False):
+    """Mode 2: TFRecord inference.
+
+    Args:
+      records: List of strings, paths to TFRecords.
+      checkpoint_path: String, path to a specific checkpoint to restore.
+      batch_size: Int, size of inference batch.
+      num_sequences: Int, number of sequences to embed. If -1,
+        embed everything.
+      reuse: Boolean, whether or not to reuse embedder weights.
+    Yields:
+      (embeddings, raw_image_strings, sequence_name):
+        embeddings is a 2-D float32 numpy array holding
+        [sequence_size, embedding_size] image embeddings.
+        raw_image_strings is a 1-D string numpy array holding
+        [sequence_size] jpeg-encoded image strings.
+        sequence_name is a string holding the name of the embedded sequence.
+    """
+    tf.reset_default_graph()
+    if not isinstance(records, list):
+      records = list(records)
+
+    # Map the list of tfrecords to a dataset of preprocessed images.
+    num_views = self._config.data.num_views
+    (views, task, seq_len) = data_providers.full_sequence_provider(
+        records, num_views)
+    tensor_dict = {
+        'raw_image_strings': views,
+        'task': task,
+        'seq_len': seq_len
+    }
+
+    # Create a preprocess function over raw image string placeholders.
+    image_str_placeholder = tf.placeholder(tf.string, shape=[None])
+    decoded = preprocessing.decode_images(image_str_placeholder)
+    decoded.set_shape([batch_size, None, None, 3])
+    preprocessed = self.preprocess_data(decoded, is_training=False)
+
+    # Create an inference graph over preprocessed images.
+    embeddings = self.forward(preprocessed, is_training=False, reuse=reuse)
+
+    # Create a saver to restore model variables.
+    tf.train.get_or_create_global_step()
+    saver = tf.train.Saver(tf.all_variables())
+
+    # Create a session and restore model variables.
+    with tf.train.MonitoredSession() as sess:
+      saver.restore(sess, checkpoint_path)
+      cnt = 0
+      # If num_sequences is specified, embed that many sequences, else embed
+      # everything.
+      try:
+        while cnt < num_sequences if num_sequences != -1 else True:
+          # Get a preprocessed image sequence.
+          np_data = sess.run(tensor_dict)
+          np_raw_images = np_data['raw_image_strings']
+          np_seq_len = np_data['seq_len']
+          np_task = np_data['task']
+
+          # Embed each view.
+          embedding_size = self._config.embedding_size
+          view_embeddings = [
+              np.zeros((0, embedding_size)) for _ in range(num_views)]
+          for view_index in range(num_views):
+            view_raw = np_raw_images[view_index]
+            # Embed the full sequence.
+            t = 0
+            while t < np_seq_len:
+              # Decode and preprocess the batch of image strings.
+              embeddings_np = sess.run(
+                  embeddings, feed_dict={
+                      image_str_placeholder: view_raw[t:t+batch_size]})
+              view_embeddings[view_index] = np.append(
+                  view_embeddings[view_index], embeddings_np, axis=0)
+              tf.logging.info('Embedded %d images for task %s' % (t, np_task))
+              t += batch_size
+
+          # Done embedding for all views.
+          view_raw_images = np_data['raw_image_strings']
+          yield (view_embeddings, view_raw_images, np_task)
+          cnt += 1
+      except tf.errors.OutOfRangeError:
+        tf.logging.info('Done embedding entire dataset.')
+
+  def _np_inference(self, np_images, checkpoint_path):
+    """Mode 3: Call this repeatedly to do inference over numpy images.
+
+    This mode is for when we we want to do real-time inference over
+    some stream of images (represented as numpy arrays).
+
+    Args:
+      np_images: A float32 numpy array holding images to embed.
+      checkpoint_path: String, path to a specific checkpoint to restore.
+    Returns:
+      (embeddings, raw_image_strings):
+        embeddings is a 2-D float32 numpy array holding
+        [inferred batch_size, embedding_size] image embeddings.
+        raw_image_strings is a 1-D string numpy array holding
+        [inferred batch_size] jpeg-encoded image strings.
+    """
+    if isinstance(np_images, list):
+      np_images = np.asarray(np_images)
+    # Add a batch dimension if only 3-dimensional.
+    if len(np_images.shape) == 3:
+      np_images = np.expand_dims(np_images, axis=0)
+
+    # If np_images are in the range [0,255], convert to [0,1].
+    assert np.min(np_images) >= 0.
+    if (np.min(np_images), np.max(np_images)) == (0, 255):
+      np_images = np_images.astype(np.float32) / 255.
+      assert (np.min(np_images), np.max(np_images)) == (0., 1.)
+
+    # If this is the first pass, set up inference graph.
+    if not hasattr(self, '_np_inf_tensor_dict'):
+      self._setup_np_inference(np_images, checkpoint_path)
+
+    # Convert np_images to embeddings.
+    np_tensor_dict = self._sess.run(self._np_inf_tensor_dict, feed_dict={
+        self._image_placeholder: np_images
+    })
+    return np_tensor_dict['embeddings'], np_tensor_dict['raw_image_strings']
+
+  def _setup_np_inference(self, np_images, checkpoint_path):
+    """Sets up and restores inference graph, creates and caches a Session."""
+    tf.logging.info('Restoring model weights.')
+
+    # Define inference over an image placeholder.
+    _, height, width, _ = np.shape(np_images)
+    image_placeholder = tf.placeholder(
+        tf.float32, shape=(None, height, width, 3))
+
+    # Preprocess batch.
+    preprocessed = self.preprocess_data(image_placeholder, is_training=False)
+
+    # Unscale and jpeg encode preprocessed images for display purposes.
+    im_strings = preprocessing.unscale_jpeg_encode(preprocessed)
+
+    # Do forward pass to get embeddings.
+    embeddings = self.forward(preprocessed, is_training=False)
+
+    # Create a saver to restore model variables.
+    tf.train.get_or_create_global_step()
+    saver = tf.train.Saver(tf.all_variables())
+
+    self._image_placeholder = image_placeholder
+    self._batch_encoded = embeddings
+
+    self._np_inf_tensor_dict = {
+        'embeddings': embeddings,
+        'raw_image_strings': im_strings,
+    }
+
+    # Create a session and restore model variables.
+    self._sess = tf.Session()
+    saver.restore(self._sess, checkpoint_path)
diff --git a/research/tcn/estimators/get_estimator.py b/research/tcn/estimators/get_estimator.py
new file mode 100644
index 00000000..30b850ed
--- /dev/null
+++ b/research/tcn/estimators/get_estimator.py
@@ -0,0 +1,60 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Get a configured estimator."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from estimators import mvtcn_estimator as mvtcn_estimators
+from estimators import svtcn_estimator
+
+
+def get_mvtcn_estimator(loss_strategy, config, logdir):
+  """Returns a configured MVTCN estimator."""
+  loss_to_trainer = {
+      'triplet_semihard': mvtcn_estimators.MVTCNTripletEstimator,
+      'npairs': mvtcn_estimators.MVTCNNpairsEstimator,
+  }
+  if loss_strategy not in loss_to_trainer:
+    raise ValueError('Unknown loss for MVTCN: %s' % loss_strategy)
+  estimator = loss_to_trainer[loss_strategy](config, logdir)
+  return estimator
+
+
+def get_estimator(config, logdir):
+  """Returns an unsupervised model trainer based on config.
+
+  Args:
+    config: A T object holding training configs.
+    logdir: String, path to directory where model checkpoints and summaries
+      are saved.
+  Returns:
+    estimator: A configured `TCNEstimator` object.
+  Raises:
+    ValueError: If unknown training strategy is specified.
+  """
+  # Get the training strategy.
+  training_strategy = config.training_strategy
+  if training_strategy == 'mvtcn':
+    loss_strategy = config.loss_strategy
+    estimator = get_mvtcn_estimator(
+        loss_strategy, config, logdir)
+  elif training_strategy == 'svtcn':
+    estimator = svtcn_estimator.SVTCNTripletEstimator(config, logdir)
+  else:
+    raise ValueError('Unknown training strategy: %s' % training_strategy)
+  return estimator
diff --git a/research/tcn/estimators/mvtcn_estimator.py b/research/tcn/estimators/mvtcn_estimator.py
new file mode 100644
index 00000000..4a036b43
--- /dev/null
+++ b/research/tcn/estimators/mvtcn_estimator.py
@@ -0,0 +1,165 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""MVTCN trainer implementations with various metric learning losses."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+import data_providers
+import model as model_module
+from estimators import base_estimator
+import tensorflow as tf
+
+
+class MVTCNEstimator(base_estimator.BaseEstimator):
+  """Multi-view TCN base class."""
+
+  def __init__(self, config, logdir):
+    super(MVTCNEstimator, self).__init__(config, logdir)
+
+  def _pairs_provider(self, records, is_training):
+    config = self._config
+    num_views = config.data.num_views
+    window = config.mvtcn.window
+    num_parallel_calls = config.data.num_parallel_calls
+    sequence_prefetch_size = config.data.sequence_prefetch_size
+    batch_prefetch_size = config.data.batch_prefetch_size
+    examples_per_seq = config.data.examples_per_sequence
+    return functools.partial(
+        data_providers.multiview_pairs_provider,
+        file_list=records,
+        preprocess_fn=self.preprocess_data,
+        num_views=num_views,
+        window=window,
+        is_training=is_training,
+        examples_per_seq=examples_per_seq,
+        num_parallel_calls=num_parallel_calls,
+        sequence_prefetch_size=sequence_prefetch_size,
+        batch_prefetch_size=batch_prefetch_size)
+
+  def forward(self, images_concat, is_training, reuse=False):
+    """See base class."""
+    embedder_strategy = self._config.embedder_strategy
+    loss_strategy = self._config.loss_strategy
+    l2_normalize_embedding = self._config[loss_strategy].embedding_l2
+    embedder = model_module.get_embedder(
+        embedder_strategy,
+        self._config,
+        images_concat,
+        is_training=is_training,
+        l2_normalize_embedding=l2_normalize_embedding, reuse=reuse)
+    embeddings_concat = embedder.construct_embedding()
+    variables_to_train = embedder.get_trainable_variables()
+    self.variables_to_train = variables_to_train
+    self.pretrained_init_fn = embedder.init_fn
+    return embeddings_concat
+
+  def _collect_image_summaries(self, anchor_images, positive_images,
+                               images_concat):
+    image_summaries = self._config.logging.summary.image_summaries
+    if image_summaries and not self._config.use_tpu:
+      batch_pairs_summary = tf.concat(
+          [anchor_images, positive_images], axis=2)
+      tf.summary.image('training/mvtcn_pairs', batch_pairs_summary)
+      tf.summary.image('training/images_preprocessed_concat', images_concat)
+
+
+class MVTCNTripletEstimator(MVTCNEstimator):
+  """Multi-View TCN with semihard triplet loss."""
+
+  def __init__(self, config, logdir):
+    super(MVTCNTripletEstimator, self).__init__(config, logdir)
+
+  def construct_input_fn(self, records, is_training):
+    """See base class."""
+    def input_fn(params):
+      """Provides input to MVTCN models."""
+      if is_training and self._config.use_tpu:
+        batch_size = params['batch_size']
+      else:
+        batch_size = self._batch_size
+      (images_concat,
+       anchor_labels,
+       positive_labels,
+       anchor_images,
+       positive_images) = self._pairs_provider(
+           records, is_training)(batch_size=batch_size)
+      if is_training:
+        self._collect_image_summaries(anchor_images, positive_images,
+                                      images_concat)
+      labels = tf.concat([anchor_labels, positive_labels], axis=0)
+      features = {'batch_preprocessed': images_concat}
+      return (features, labels)
+    return input_fn
+
+  def define_loss(self, embeddings, labels, is_training):
+    """See base class."""
+    margin = self._config.triplet_semihard.margin
+    loss = tf.contrib.losses.metric_learning.triplet_semihard_loss(
+        labels=labels, embeddings=embeddings, margin=margin)
+    self._loss = loss
+    if is_training and not self._config.use_tpu:
+      tf.summary.scalar('training/triplet_semihard', loss)
+    return loss
+
+  def define_eval_metric_ops(self):
+    """See base class."""
+    return {'validation/triplet_semihard': tf.metrics.mean(self._loss)}
+
+
+class MVTCNNpairsEstimator(MVTCNEstimator):
+  """Multi-View TCN with npairs loss."""
+
+  def __init__(self, config, logdir):
+    super(MVTCNNpairsEstimator, self).__init__(config, logdir)
+
+  def construct_input_fn(self, records, is_training):
+    """See base class."""
+    def input_fn(params):
+      """Provides input to MVTCN models."""
+      if is_training and self._config.use_tpu:
+        batch_size = params['batch_size']
+      else:
+        batch_size = self._batch_size
+      (images_concat,
+       npairs_labels,
+       _,
+       anchor_images,
+       positive_images) = self._pairs_provider(
+           records, is_training)(batch_size=batch_size)
+      if is_training:
+        self._collect_image_summaries(anchor_images, positive_images,
+                                      images_concat)
+      features = {'batch_preprocessed': images_concat}
+      return (features, npairs_labels)
+    return input_fn
+
+  def define_loss(self, embeddings, labels, is_training):
+    """See base class."""
+    embeddings_anchor, embeddings_positive = tf.split(embeddings, 2, axis=0)
+    loss = tf.contrib.losses.metric_learning.npairs_loss(
+        labels=labels, embeddings_anchor=embeddings_anchor,
+        embeddings_positive=embeddings_positive)
+    self._loss = loss
+    if is_training and not self._config.use_tpu:
+      tf.summary.scalar('training/npairs', loss)
+    return loss
+
+  def define_eval_metric_ops(self):
+    """See base class."""
+    return {'validation/npairs': tf.metrics.mean(self._loss)}
diff --git a/research/tcn/estimators/svtcn_estimator.py b/research/tcn/estimators/svtcn_estimator.py
new file mode 100644
index 00000000..069f7e8d
--- /dev/null
+++ b/research/tcn/estimators/svtcn_estimator.py
@@ -0,0 +1,100 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SVTCN estimator implementation."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import data_providers
+import model as model_module
+from estimators import base_estimator
+from estimators import svtcn_loss
+import tensorflow as tf
+
+
+class SVTCNEstimator(base_estimator.BaseEstimator):
+  """Single-view TCN Estimator base class."""
+
+  def __init__(self, config, logdir):
+    super(SVTCNEstimator, self).__init__(config, logdir)
+
+  def construct_input_fn(self, records, is_training):
+    """See base class."""
+    config = self._config
+    num_views = config.data.num_views
+    num_parallel_calls = config.data.num_parallel_calls
+    sequence_prefetch_size = config.data.sequence_prefetch_size
+    batch_prefetch_size = config.data.batch_prefetch_size
+
+    def input_fn():
+      """Provides input to SVTCN models."""
+      (images_preprocessed,
+       images_raw,
+       timesteps) = data_providers.singleview_tcn_provider(
+           file_list=records,
+           preprocess_fn=self.preprocess_data,
+           num_views=num_views,
+           is_training=is_training,
+           batch_size=self._batch_size,
+           num_parallel_calls=num_parallel_calls,
+           sequence_prefetch_size=sequence_prefetch_size,
+           batch_prefetch_size=batch_prefetch_size)
+
+      if config.logging.summary.image_summaries and is_training:
+        tf.summary.image('training/svtcn_images', images_raw)
+
+      features = {'batch_preprocessed': images_preprocessed}
+      return (features, timesteps)
+    return input_fn
+
+  def forward(self, images, is_training, reuse=False):
+    """See base class."""
+    embedder_strategy = self._config.embedder_strategy
+    embedder = model_module.get_embedder(
+        embedder_strategy,
+        self._config,
+        images,
+        is_training=is_training, reuse=reuse)
+    embeddings = embedder.construct_embedding()
+
+    if is_training:
+      self.variables_to_train = embedder.get_trainable_variables()
+      self.pretrained_init_fn = embedder.init_fn
+    return embeddings
+
+
+class SVTCNTripletEstimator(SVTCNEstimator):
+  """Single-View TCN with semihard triplet loss."""
+
+  def __init__(self, config, logdir):
+    super(SVTCNTripletEstimator, self).__init__(config, logdir)
+
+  def define_loss(self, embeddings, timesteps, is_training):
+    """See base class."""
+    pos_radius = self._config.svtcn.pos_radius
+    neg_radius = self._config.svtcn.neg_radius
+    margin = self._config.triplet_semihard.margin
+    loss = svtcn_loss.singleview_tcn_loss(
+        embeddings, timesteps, pos_radius, neg_radius, margin=margin)
+    self._loss = loss
+    if is_training:
+      tf.summary.scalar('training/svtcn_loss', loss)
+    return loss
+
+  def define_eval_metric_ops(self):
+    """See base class."""
+    return {'validation/svtcn_loss': tf.metrics.mean(self._loss)}
diff --git a/research/tcn/estimators/svtcn_loss.py b/research/tcn/estimators/svtcn_loss.py
new file mode 100644
index 00000000..26178034
--- /dev/null
+++ b/research/tcn/estimators/svtcn_loss.py
@@ -0,0 +1,217 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""This implements single view TCN triplet loss."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def pairwise_squared_distance(feature):
+  """Computes the squared pairwise distance matrix.
+
+  output[i, j] = || feature[i, :] - feature[j, :] ||_2^2
+
+  Args:
+    feature: 2-D Tensor of size [number of data, feature dimension]
+
+  Returns:
+    pairwise_squared_distances: 2-D Tensor of size
+      [number of data, number of data]
+  """
+  pairwise_squared_distances = tf.add(
+      tf.reduce_sum(
+          tf.square(feature), axis=1, keep_dims=True),
+      tf.reduce_sum(
+          tf.square(tf.transpose(feature)), axis=0,
+          keep_dims=True)) - 2.0 * tf.matmul(feature, tf.transpose(feature))
+
+  # Deal with numerical inaccuracies. Set small negatives to zero.
+  pairwise_squared_distances = tf.maximum(pairwise_squared_distances, 0.0)
+  return pairwise_squared_distances
+
+
+def masked_maximum(data, mask, dim=1):
+  """Computes the axis wise maximum over chosen elements.
+
+  Args:
+    data: N-D Tensor.
+    mask: N-D Tensor of zeros or ones.
+    dim: The dimension over which to compute the maximum.
+
+  Returns:
+    masked_maximums: N-D Tensor.
+      The maximized dimension is of size 1 after the operation.
+  """
+  axis_minimums = tf.reduce_min(data, dim, keep_dims=True)
+  masked_maximums = tf.reduce_max(
+      tf.multiply(
+          data - axis_minimums, mask), dim, keep_dims=True) + axis_minimums
+  return masked_maximums
+
+
+def masked_minimum(data, mask, dim=1):
+  """Computes the axis wise minimum over chosen elements.
+
+  Args:
+    data: 2-D Tensor of size [n, m].
+    mask: 2-D Boolean Tensor of size [n, m].
+    dim: The dimension over which to compute the minimum.
+
+  Returns:
+    masked_minimums: N-D Tensor.
+      The minimized dimension is of size 1 after the operation.
+  """
+  axis_maximums = tf.reduce_max(data, dim, keep_dims=True)
+  masked_minimums = tf.reduce_min(
+      tf.multiply(
+          data - axis_maximums, mask), dim, keep_dims=True) + axis_maximums
+  return masked_minimums
+
+
+def singleview_tcn_loss(
+    embeddings, timesteps, pos_radius, neg_radius, margin=1.0,
+    sequence_ids=None, multiseq=False):
+  """Computes the single view triplet loss with semi-hard negative mining.
+
+  The loss encourages the positive distances (between a pair of embeddings with
+  the same labels) to be smaller than the minimum negative distance among
+  which are at least greater than the positive distance plus the margin constant
+  (called semi-hard negative) in the mini-batch. If no such negative exists,
+  uses the largest negative distance instead.
+
+  Anchor, positive, negative selection is as follow:
+  Anchors: We consider every embedding timestep as an anchor.
+  Positives: pos_radius defines a radius (in timesteps) around each anchor from
+    which positives can be drawn. E.g. An anchor with t=10 and a pos_radius of
+    2 produces a set of 4 (anchor,pos) pairs [(a=10, p=8), ... (a=10, p=12)].
+  Negatives: neg_radius defines a boundary (in timesteps) around each anchor,
+    outside of which negatives can be drawn. E.g. An anchor with t=10 and a
+    neg_radius of 4 means negatives can be any t_neg where t_neg < 6 and
+    t_neg > 14.
+
+  Args:
+    embeddings: 2-D Tensor of embedding vectors.
+    timesteps: 1-D Tensor with shape [batch_size, 1] of sequence timesteps.
+    pos_radius: int32; the size of the window (in timesteps) around each anchor
+      timestep that a positive can be drawn from.
+    neg_radius: int32; the size of the window (in timesteps) around each anchor
+      timestep that defines a negative boundary. Negatives can only be chosen
+      where negative timestep t is < negative boundary min or > negative
+      boundary max.
+    margin: Float; the triplet loss margin hyperparameter.
+    sequence_ids: (Optional) 1-D Tensor with shape [batch_size, 1] of sequence
+      ids. Together (sequence_id, sequence_timestep) give us a unique index for
+      each image if we have multiple sequences in a batch.
+    multiseq: Boolean, whether or not the batch is composed of multiple
+      sequences (with possibly colliding timesteps).
+
+  Returns:
+    triplet_loss: tf.float32 scalar.
+  """
+  assert neg_radius > pos_radius
+
+  # If timesteps shape isn't [batchsize, 1], reshape to [batch_size, 1].
+  tshape = tf.shape(timesteps)
+  assert tshape.shape == 2 or tshape.shape == 1
+  if tshape.shape == 1:
+    timesteps = tf.reshape(timesteps, [tshape[0], 1])
+
+  # Build pairwise squared distance matrix.
+  pdist_matrix = pairwise_squared_distance(embeddings)
+
+  # Build pairwise binary adjacency matrix, where adjacency[i,j] is True
+  # if timestep j is inside the positive range for timestep i and both
+  # timesteps come from the same sequence.
+  pos_radius = tf.cast(pos_radius, tf.int32)
+
+  if multiseq:
+    # If sequence_ids shape isn't [batchsize, 1], reshape to [batch_size, 1].
+    tshape = tf.shape(sequence_ids)
+    assert tshape.shape == 2 or tshape.shape == 1
+    if tshape.shape == 1:
+      sequence_ids = tf.reshape(sequence_ids, [tshape[0], 1])
+
+    # Build pairwise binary adjacency matrix based on sequence_ids
+    sequence_adjacency = tf.equal(sequence_ids, tf.transpose(sequence_ids))
+
+    # Invert so we can select negatives only.
+    sequence_adjacency_not = tf.logical_not(sequence_adjacency)
+
+    in_pos_range = tf.logical_and(
+        tf.less_equal(
+            tf.abs(timesteps - tf.transpose(timesteps)), pos_radius),
+        sequence_adjacency)
+    # Build pairwise binary discordance matrix, where discordance[i,j] is True
+    # if timestep j is inside the negative range for timestep i or if the
+    # timesteps come from different sequences.
+    in_neg_range = tf.logical_or(
+        tf.greater(tf.abs(timesteps - tf.transpose(timesteps)), neg_radius),
+        sequence_adjacency_not
+    )
+  else:
+    in_pos_range = tf.less_equal(
+        tf.abs(timesteps - tf.transpose(timesteps)), pos_radius)
+    in_neg_range = tf.greater(tf.abs(timesteps - tf.transpose(timesteps)),
+                              neg_radius)
+
+  batch_size = tf.size(timesteps)
+
+  # compute the mask
+  pdist_matrix_tile = tf.tile(pdist_matrix, [batch_size, 1])
+  mask = tf.logical_and(
+      tf.tile(in_neg_range, [batch_size, 1]),
+      tf.greater(pdist_matrix_tile,
+                 tf.reshape(tf.transpose(pdist_matrix), [-1, 1])))
+  mask_final = tf.reshape(
+      tf.greater(
+          tf.reduce_sum(
+              tf.cast(
+                  mask, dtype=tf.float32), 1, keep_dims=True),
+          0.0), [batch_size, batch_size])
+  mask_final = tf.transpose(mask_final)
+
+  in_neg_range = tf.cast(in_neg_range, dtype=tf.float32)
+  mask = tf.cast(mask, dtype=tf.float32)
+
+  # negatives_outside: smallest D_an where D_an > D_ap
+  negatives_outside = tf.reshape(
+      masked_minimum(pdist_matrix_tile, mask), [batch_size, batch_size])
+  negatives_outside = tf.transpose(negatives_outside)
+
+  # negatives_inside: largest D_an
+  negatives_inside = tf.tile(
+      masked_maximum(pdist_matrix, in_neg_range), [1, batch_size])
+  semi_hard_negatives = tf.where(
+      mask_final, negatives_outside, negatives_inside)
+
+  loss_mat = tf.add(margin, pdist_matrix - semi_hard_negatives)
+
+  mask_positives = tf.cast(
+      in_pos_range, dtype=tf.float32) - tf.diag(tf.ones([batch_size]))
+
+  # In lifted-struct, the authors multiply 0.5 for upper triangular
+  #   in semihard, they take all positive pairs except the diagonal.
+  num_positives = tf.reduce_sum(mask_positives)
+
+  triplet_loss = tf.truediv(
+      tf.reduce_sum(tf.maximum(tf.multiply(loss_mat, mask_positives), 0.0)),
+      num_positives,
+      name='triplet_svtcn_loss')
+
+  return triplet_loss
diff --git a/research/tcn/estimators/svtcn_loss_test.py b/research/tcn/estimators/svtcn_loss_test.py
new file mode 100644
index 00000000..f5bdfd98
--- /dev/null
+++ b/research/tcn/estimators/svtcn_loss_test.py
@@ -0,0 +1,106 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for svtcn_loss.py."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+from sklearn.metrics.pairwise import euclidean_distances
+from estimators import svtcn_loss
+import tensorflow as tf
+
+
+class SVTCNLoss(tf.test.TestCase):
+
+  def testSVTCNLoss(self):
+    with self.test_session():
+      num_data = 64
+      num_sequences = 2
+      num_data_per_seq = num_data // num_sequences
+      feat_dim = 6
+      margin = 1.0
+      times = np.tile(np.arange(num_data_per_seq, dtype=np.int32),
+                      num_sequences)
+      times = np.reshape(times, [times.shape[0], 1])
+      sequence_ids = np.concatenate(
+          [np.ones(num_data_per_seq)*i for i in range(num_sequences)])
+      sequence_ids = np.reshape(sequence_ids, [sequence_ids.shape[0], 1])
+
+      pos_radius = 6
+      neg_radius = 12
+
+      embedding = np.random.rand(num_data, feat_dim).astype(np.float32)
+
+      # Compute the loss in NP
+
+      # Get a positive mask, i.e. indices for each time index
+      # that are inside the positive range.
+      in_pos_range = np.less_equal(
+          np.abs(times - times.transpose()), pos_radius)
+
+      # Get a negative mask, i.e. indices for each time index
+      # that are inside the negative range (> t + (neg_mult * pos_radius)
+      # and < t - (neg_mult * pos_radius).
+      in_neg_range = np.greater(np.abs(times - times.transpose()), neg_radius)
+
+      sequence_adjacency = sequence_ids == sequence_ids.T
+      sequence_adjacency_not = np.logical_not(sequence_adjacency)
+
+      pdist_matrix = euclidean_distances(embedding, squared=True)
+      loss_np = 0.0
+      num_positives = 0.0
+      for i in range(num_data):
+        for j in range(num_data):
+          if in_pos_range[i, j] and i != j and sequence_adjacency[i, j]:
+            num_positives += 1.0
+
+            pos_distance = pdist_matrix[i][j]
+            neg_distances = []
+
+            for k in range(num_data):
+              if in_neg_range[i, k] or sequence_adjacency_not[i, k]:
+                neg_distances.append(pdist_matrix[i][k])
+
+            neg_distances.sort()  # sort by distance
+            chosen_neg_distance = neg_distances[0]
+
+            for l in range(len(neg_distances)):
+              chosen_neg_distance = neg_distances[l]
+              if chosen_neg_distance > pos_distance:
+                break
+
+            loss_np += np.maximum(
+                0.0, margin - chosen_neg_distance + pos_distance)
+
+      loss_np /= num_positives
+
+      # Compute the loss in TF
+      loss_tf = svtcn_loss.singleview_tcn_loss(
+          embeddings=tf.convert_to_tensor(embedding),
+          timesteps=tf.convert_to_tensor(times),
+          pos_radius=pos_radius,
+          neg_radius=neg_radius,
+          margin=margin,
+          sequence_ids=tf.convert_to_tensor(sequence_ids),
+          multiseq=True
+      )
+      loss_tf = loss_tf.eval()
+      self.assertAllClose(loss_np, loss_tf)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/tcn/eval.py b/research/tcn/eval.py
new file mode 100644
index 00000000..de24e93e
--- /dev/null
+++ b/research/tcn/eval.py
@@ -0,0 +1,63 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Calculates running validation of TCN models (and baseline comparisons)."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import time
+from estimators.get_estimator import get_estimator
+from utils import util
+import tensorflow as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string('master', 'local',
+                           'BNS name of the TensorFlow master to use')
+tf.app.flags.DEFINE_string(
+    'logdir', '/tmp/tcn', 'Directory where to write event logs.')
+FLAGS = tf.app.flags.FLAGS
+
+
+def main(_):
+  """Runs main eval loop."""
+  # Parse config dict from yaml config files / command line flags.
+  logdir = FLAGS.logdir
+  config = util.ParseConfigsToLuaTable(FLAGS.config_paths, FLAGS.model_params)
+
+  # Choose an estimator based on training strategy.
+  estimator = get_estimator(config, logdir)
+
+  # Wait for the first checkpoint file to be written.
+  while not tf.train.latest_checkpoint(logdir):
+    tf.logging.info('Waiting for a checkpoint file...')
+    time.sleep(10)
+
+  # Run validation.
+  while True:
+    estimator.evaluate()
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/g3doc/alignment.png b/research/tcn/g3doc/alignment.png
new file mode 100644
index 00000000..7cfdfece
Binary files /dev/null and b/research/tcn/g3doc/alignment.png differ
diff --git a/research/tcn/g3doc/all_error.png b/research/tcn/g3doc/all_error.png
new file mode 100644
index 00000000..c7b2d5b4
Binary files /dev/null and b/research/tcn/g3doc/all_error.png differ
diff --git a/research/tcn/g3doc/avg_error.png b/research/tcn/g3doc/avg_error.png
new file mode 100644
index 00000000..0b421824
Binary files /dev/null and b/research/tcn/g3doc/avg_error.png differ
diff --git a/research/tcn/g3doc/im.gif b/research/tcn/g3doc/im.gif
new file mode 100644
index 00000000..fd1ac4e8
Binary files /dev/null and b/research/tcn/g3doc/im.gif differ
diff --git a/research/tcn/g3doc/loss.png b/research/tcn/g3doc/loss.png
new file mode 100644
index 00000000..44eaa6d6
Binary files /dev/null and b/research/tcn/g3doc/loss.png differ
diff --git a/research/tcn/g3doc/pca.png b/research/tcn/g3doc/pca.png
new file mode 100644
index 00000000..2a9ce8f3
Binary files /dev/null and b/research/tcn/g3doc/pca.png differ
diff --git a/research/tcn/g3doc/val_loss.png b/research/tcn/g3doc/val_loss.png
new file mode 100644
index 00000000..73ad725c
Binary files /dev/null and b/research/tcn/g3doc/val_loss.png differ
diff --git a/research/tcn/generate_videos.py b/research/tcn/generate_videos.py
new file mode 100644
index 00000000..2b2ecba8
--- /dev/null
+++ b/research/tcn/generate_videos.py
@@ -0,0 +1,426 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Generates imitation videos.
+
+Generate single pairwise imitation videos:
+blaze build -c opt --config=cuda --copt=-mavx \
+learning/brain/research/tcn/generate_videos && \
+blaze-bin/learning/brain/research/tcn/generate_videos \
+--logtostderr \
+--config_paths $config_paths \
+--checkpointdir $checkpointdir \
+--checkpoint_iter $checkpoint_iter \
+--query_records_dir $query_records_dir \
+--target_records_dir $target_records_dir \
+--outdir $outdir \
+--mode single \
+--num_query_sequences 1 \
+--num_target_sequences -1
+
+# Generate imitation videos with multiple sequences in the target set:
+query_records_path
+blaze build -c opt --config=cuda --copt=-mavx \
+learning/brain/research/tcn/generate_videos && \
+blaze-bin/learning/brain/research/tcn/generate_videos \
+--logtostderr \
+--config_paths $config_paths \
+--checkpointdir $checkpointdir \
+--checkpoint_iter $checkpoint_iter \
+--query_records_dir $query_records_dir \
+--target_records_dir $target_records_dir \
+--outdir $outdir \
+--num_multi_targets 1 \
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import cv2
+import tensorflow as tf
+import os
+import matplotlib
+matplotlib.use("pdf")
+import matplotlib.animation as animation
+import matplotlib.pyplot as plt
+import numpy as np
+from estimators.get_estimator import get_estimator
+from utils import util
+tf.logging.set_verbosity(tf.logging.INFO)
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string(
+    'checkpointdir', '/tmp/tcn', 'Path to model checkpoints.')
+tf.app.flags.DEFINE_string(
+    'checkpoint_iter', '', 'Checkpoint iter to use.')
+tf.app.flags.DEFINE_integer(
+    'num_multi_targets', -1,
+    'Number of imitation vids in the target set per imitation video.')
+tf.app.flags.DEFINE_string(
+    'outdir', '/tmp/tcn', 'Path to write embeddings to.')
+tf.app.flags.DEFINE_string(
+    'mode', 'single', 'single | multi. Single means generate imitation vids'
+                      'where query is being imitated by single sequence. Multi'
+                      'means generate imitation vids where query is being'
+                      'imitated by multiple.')
+tf.app.flags.DEFINE_string('query_records_dir', '',
+                           'Directory of image tfrecords.')
+tf.app.flags.DEFINE_string('target_records_dir', '',
+                           'Directory of image tfrecords.')
+tf.app.flags.DEFINE_integer('query_view', 1,
+                            'Viewpoint of the query video.')
+tf.app.flags.DEFINE_integer('target_view', 0,
+                            'Viewpoint of the imitation video.')
+tf.app.flags.DEFINE_integer('smoothing_window', 5,
+                            'Number of frames to smooth over.')
+tf.app.flags.DEFINE_integer('num_query_sequences', -1,
+                            'Number of query sequences to embed.')
+tf.app.flags.DEFINE_integer('num_target_sequences', -1,
+                            'Number of target sequences to embed.')
+FLAGS = tf.app.flags.FLAGS
+
+
+def SmoothEmbeddings(embs):
+  """Temporally smoothes a sequence of embeddings."""
+  new_embs = []
+  window = int(FLAGS.smoothing_window)
+  for i in range(len(embs)):
+    min_i = max(i-window, 0)
+    max_i = min(i+window, len(embs))
+    new_embs.append(np.mean(embs[min_i:max_i, :], axis=0))
+  return np.array(new_embs)
+
+
+def MakeImitationVideo(
+    outdir, vidname, query_im_strs, knn_im_strs, height=640, width=360):
+  """Creates a KNN imitation video.
+
+  For each frame in vid0, pair with the frame at index in knn_indices in
+  vids1. Write video to disk.
+
+  Args:
+    outdir: String, directory to write videos.
+    vidname: String, name of video.
+    query_im_strs: Numpy array holding query image strings.
+    knn_im_strs: Numpy array holding knn image strings.
+    height: Int, height of raw images.
+    width: Int, width of raw images.
+  """
+  if not tf.gfile.Exists(outdir):
+    tf.gfile.MakeDirs(outdir)
+  vid_path = os.path.join(outdir, vidname)
+  combined = zip(query_im_strs, knn_im_strs)
+
+  # Create and write the video.
+  fig = plt.figure()
+  ax = fig.add_subplot(111)
+  ax.set_aspect('equal')
+  ax.get_xaxis().set_visible(False)
+  ax.get_yaxis().set_visible(False)
+  im = ax.imshow(
+      np.zeros((height, width*2, 3)), cmap='gray', interpolation='nearest')
+  im.set_clim([0, 1])
+  plt.tight_layout(pad=0, w_pad=0, h_pad=0)
+  # pylint: disable=invalid-name
+  def update_img(pair):
+    """Decode pairs of image strings, update a video."""
+    im_i, im_j = pair
+    nparr_i = np.fromstring(str(im_i), np.uint8)
+    img_np_i = cv2.imdecode(nparr_i, 1)
+    img_np_i = img_np_i[..., [2, 1, 0]]
+    nparr_j = np.fromstring(str(im_j), np.uint8)
+    img_np_j = cv2.imdecode(nparr_j, 1)
+    img_np_j = img_np_j[..., [2, 1, 0]]
+
+    # Optionally reshape the images to be same size.
+    frame = np.concatenate([img_np_i, img_np_j], axis=1)
+    im.set_data(frame)
+    return im
+  ani = animation.FuncAnimation(fig, update_img, combined, interval=15)
+  writer = animation.writers['ffmpeg'](fps=15)
+  dpi = 100
+  tf.logging.info('Writing video to:\n %s \n' % vid_path)
+  ani.save('%s.mp4' % vid_path, writer=writer, dpi=dpi)
+
+
+def GenerateImitationVideo(
+    vid_name, query_ims, query_embs, target_ims, target_embs, height, width):
+  """Generates a single cross-sequence imitation video.
+
+  For each frame in some query sequence, find the nearest neighbor from
+  some target sequence in embedding space.
+
+  Args:
+    vid_name: String, the name of the video.
+    query_ims: Numpy array of shape [query sequence length, height, width, 3].
+    query_embs: Numpy array of shape [query sequence length, embedding size].
+    target_ims: Numpy array of shape [target sequence length, height, width,
+      3].
+    target_embs: Numpy array of shape [target sequence length, embedding
+      size].
+    height: Int, height of the raw image.
+    width: Int, width of the raw image.
+  """
+  # For each query frame, find the index of the nearest neighbor in the
+  # target video.
+  knn_indices = [util.KNNIds(q, target_embs, k=1)[0] for q in query_embs]
+
+  # Create and write out the video.
+  assert knn_indices
+  knn_ims = np.array([target_ims[k] for k in knn_indices])
+  MakeImitationVideo(FLAGS.outdir, vid_name, query_ims, knn_ims, height, width)
+
+
+def SingleImitationVideos(
+    query_records, target_records, config, height, width):
+  """Generates pairwise imitation videos.
+
+  This creates all pairs of target imitating query videos, where each frame
+  on the left is matched to a nearest neighbor coming a single
+  embedded target video.
+
+  Args:
+    query_records: List of Strings, paths to tfrecord datasets to use as
+      queries.
+    target_records: List of Strings, paths to tfrecord datasets to use as
+      targets.
+    config: A T object describing training config.
+    height: Int, height of the raw image.
+    width: Int, width of the raw image.
+  """
+  # Embed query and target data.
+  (query_sequences_to_data,
+   target_sequences_to_data) = EmbedQueryTargetData(
+       query_records, target_records, config)
+
+  qview = FLAGS.query_view
+  tview = FLAGS.target_view
+
+  # Loop over query videos.
+  for task_i, data_i in query_sequences_to_data.iteritems():
+    for task_j, data_j in target_sequences_to_data.iteritems():
+      i_ims = data_i['images']
+      i_embs = data_i['embeddings']
+      query_embs = SmoothEmbeddings(i_embs[qview])
+      query_ims = i_ims[qview]
+
+      j_ims = data_j['images']
+      j_embs = data_j['embeddings']
+      target_embs = SmoothEmbeddings(j_embs[tview])
+      target_ims = j_ims[tview]
+
+      tf.logging.info('Generating %s imitating %s video.' % (task_j, task_i))
+      vid_name = 'q%sv%s_im%sv%s' % (task_i, qview, task_j, tview)
+      vid_name = vid_name.replace('/', '_')
+      GenerateImitationVideo(vid_name, query_ims, query_embs,
+                             target_ims, target_embs, height, width)
+
+
+def MultiImitationVideos(
+    query_records, target_records, config, height, width):
+  """Creates multi-imitation videos.
+
+  This creates videos where every frame on the left is matched to a nearest
+  neighbor coming from a set of multiple embedded target videos.
+
+  Args:
+    query_records: List of Strings, paths to tfrecord datasets to use as
+      queries.
+    target_records: List of Strings, paths to tfrecord datasets to use as
+      targets.
+    config: A T object describing training config.
+    height: Int, height of the raw image.
+    width: Int, width of the raw image.
+  """
+  # Embed query and target data.
+  (query_sequences_to_data,
+   target_sequences_to_data) = EmbedQueryTargetData(
+       query_records, target_records, config)
+
+  qview = FLAGS.query_view
+  tview = FLAGS.target_view
+
+  # Loop over query videos.
+  for task_i, data_i in query_sequences_to_data.iteritems():
+    i_ims = data_i['images']
+    i_embs = data_i['embeddings']
+    query_embs = SmoothEmbeddings(i_embs[qview])
+    query_ims = i_ims[qview]
+
+    all_target_embs = []
+    all_target_ims = []
+
+    # If num_imitation_vids is -1, add all seq embeddings to the target set.
+    if FLAGS.num_multi_targets == -1:
+      num_multi_targets = len(target_sequences_to_data)
+    else:
+      # Else, add some specified number of seq embeddings to the target set.
+      num_multi_targets = FLAGS.num_multi_targets
+    for j in range(num_multi_targets):
+      task_j = target_sequences_to_data.keys()[j]
+      data_j = target_sequences_to_data[task_j]
+      print('Adding %s to target set' % task_j)
+      j_ims = data_j['images']
+      j_embs = data_j['embeddings']
+
+      target_embs = SmoothEmbeddings(j_embs[tview])
+      target_ims = j_ims[tview]
+      all_target_embs.extend(target_embs)
+      all_target_ims.extend(target_ims)
+
+    # Generate a "j imitating i" video.
+    tf.logging.info('Generating all imitating %s video.' % task_i)
+    vid_name = 'q%sv%s_multiv%s' % (task_i, qview, tview)
+    vid_name = vid_name.replace('/', '_')
+    GenerateImitationVideo(vid_name, query_ims, query_embs,
+                           all_target_ims, all_target_embs, height, width)
+
+
+def SameSequenceVideos(query_records, config, height, width):
+  """Generate same sequence, cross-view imitation videos."""
+  batch_size = config.data.embed_batch_size
+
+  # Choose an estimator based on training strategy.
+  estimator = get_estimator(config, FLAGS.checkpointdir)
+
+  # Choose a checkpoint path to restore.
+  checkpointdir = FLAGS.checkpointdir
+  checkpoint_path = os.path.join(checkpointdir,
+                                 'model.ckpt-%s' % FLAGS.checkpoint_iter)
+
+  # Embed num_sequences query sequences, store embeddings and image strings in
+  # query_sequences_to_data.
+  sequences_to_data = {}
+  for (view_embeddings, view_raw_image_strings, seqname) in estimator.inference(
+      query_records, checkpoint_path, batch_size,
+      num_sequences=FLAGS.num_query_sequences):
+    sequences_to_data[seqname] = {
+        'embeddings': view_embeddings,
+        'images': view_raw_image_strings,
+    }
+
+  # Loop over query videos.
+  qview = FLAGS.query_view
+  tview = FLAGS.target_view
+  for task_i, data_i in sequences_to_data.iteritems():
+    ims = data_i['images']
+    embs = data_i['embeddings']
+    query_embs = SmoothEmbeddings(embs[qview])
+    query_ims = ims[qview]
+
+    target_embs = SmoothEmbeddings(embs[tview])
+    target_ims = ims[tview]
+
+    tf.logging.info('Generating %s imitating %s video.' % (task_i, task_i))
+    vid_name = 'q%sv%s_im%sv%s' % (task_i, qview, task_i, tview)
+    vid_name = vid_name.replace('/', '_')
+    GenerateImitationVideo(vid_name, query_ims, query_embs,
+                           target_ims, target_embs, height, width)
+
+
+def EmbedQueryTargetData(query_records, target_records, config):
+  """Embeds the full set of query_records and target_records.
+
+  Args:
+    query_records: List of Strings, paths to tfrecord datasets to use as
+      queries.
+    target_records: List of Strings, paths to tfrecord datasets to use as
+      targets.
+    config: A T object describing training config.
+
+  Returns:
+    query_sequences_to_data: A dict holding 'embeddings' and 'images'
+    target_sequences_to_data: A dict holding 'embeddings' and 'images'
+  """
+  batch_size = config.data.embed_batch_size
+
+  # Choose an estimator based on training strategy.
+  estimator = get_estimator(config, FLAGS.checkpointdir)
+
+  # Choose a checkpoint path to restore.
+  checkpointdir = FLAGS.checkpointdir
+  checkpoint_path = os.path.join(checkpointdir,
+                                 'model.ckpt-%s' % FLAGS.checkpoint_iter)
+
+  # Embed num_sequences query sequences, store embeddings and image strings in
+  # query_sequences_to_data.
+  num_query_sequences = FLAGS.num_query_sequences
+  num_target_sequences = FLAGS.num_target_sequences
+  query_sequences_to_data = {}
+  for (view_embeddings, view_raw_image_strings, seqname) in estimator.inference(
+      query_records, checkpoint_path, batch_size,
+      num_sequences=num_query_sequences):
+    query_sequences_to_data[seqname] = {
+        'embeddings': view_embeddings,
+        'images': view_raw_image_strings,
+    }
+
+  if (query_records == target_records) and (
+      num_query_sequences == num_target_sequences):
+    target_sequences_to_data = query_sequences_to_data
+  else:
+    # Embed num_sequences target sequences, store embeddings and image strings
+    # in sequences_to_data.
+    target_sequences_to_data = {}
+    for (view_embeddings, view_raw_image_strings,
+         seqname) in estimator.inference(
+             target_records, checkpoint_path, batch_size,
+             num_sequences=num_target_sequences):
+      target_sequences_to_data[seqname] = {
+          'embeddings': view_embeddings,
+          'images': view_raw_image_strings,
+      }
+  return query_sequences_to_data, target_sequences_to_data
+
+
+def main(_):
+  # Parse config dict from yaml config files / command line flags.
+  config = util.ParseConfigsToLuaTable(FLAGS.config_paths, FLAGS.model_params)
+
+  # Get tables to embed.
+  query_records_dir = FLAGS.query_records_dir
+  query_records = util.GetFilesRecursively(query_records_dir)
+
+  target_records_dir = FLAGS.target_records_dir
+  target_records = util.GetFilesRecursively(target_records_dir)
+
+  height = config.data.raw_height
+  width = config.data.raw_width
+  mode = FLAGS.mode
+  if mode == 'multi':
+    # Generate videos where target set is composed of multiple videos.
+    MultiImitationVideos(query_records, target_records, config,
+                         height, width)
+  elif mode == 'single':
+    # Generate videos where target set is a single video.
+    SingleImitationVideos(query_records, target_records, config,
+                          height, width)
+  elif mode == 'same':
+    # Generate videos where target set is the same as query, but diff view.
+    SameSequenceVideos(query_records, config, height, width)
+  else:
+    raise ValueError('Unknown mode %s' % mode)
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/labeled_eval.py b/research/tcn/labeled_eval.py
new file mode 100644
index 00000000..98dd43c2
--- /dev/null
+++ b/research/tcn/labeled_eval.py
@@ -0,0 +1,308 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Generates test Recall@K statistics on labeled classification problems."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import defaultdict
+import os
+import numpy as np
+from sklearn.metrics.pairwise import pairwise_distances
+import data_providers
+from estimators.get_estimator import get_estimator
+from utils import util
+import tensorflow as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string(
+    'mode', 'validation',
+    'Which dataset to evaluate: `validation` | `test`.')
+tf.app.flags.DEFINE_string('master', 'local',
+                           'BNS name of the TensorFlow master to use')
+tf.app.flags.DEFINE_string(
+    'checkpoint_iter', '', 'Evaluate this specific checkpoint.')
+tf.app.flags.DEFINE_string(
+    'checkpointdir', '/tmp/tcn', 'Path to model checkpoints.')
+tf.app.flags.DEFINE_string('outdir', '/tmp/tcn', 'Path to write summaries to.')
+FLAGS = tf.app.flags.FLAGS
+
+
+def nearest_cross_sequence_neighbors(data, tasks, n_neighbors=1):
+  """Computes the n_neighbors nearest neighbors for every row in data.
+
+  Args:
+    data: A np.float32 array of shape [num_data, embedding size] holding
+      an embedded validation / test dataset.
+    tasks: A list of strings of size [num_data] holding the task or sequence
+      name that each row belongs to.
+    n_neighbors: The number of knn indices to return for each row.
+  Returns:
+    indices: an np.int32 array of size [num_data, n_neighbors] holding the
+      n_neighbors nearest indices for every row in data. These are
+      restricted to be from different named sequences (as defined in `tasks`).
+  """
+
+  # Compute the pairwise sequence adjacency matrix from `tasks`.
+  num_data = data.shape[0]
+  tasks = np.array(tasks)
+  tasks = np.reshape(tasks, (num_data, 1))
+  assert len(tasks.shape) == 2
+  not_adjacent = (tasks != tasks.T)
+
+  # Compute the symmetric pairwise distance matrix.
+  pdist = pairwise_distances(data, metric='sqeuclidean')
+
+  # For every row in the pairwise distance matrix, only consider
+  # cross-sequence columns.
+  indices = np.zeros((num_data, n_neighbors), dtype=np.int32)
+  for idx in range(num_data):
+    # Restrict to cross_sequence neighbors.
+    distances = [(
+        pdist[idx][i], i) for i in xrange(num_data) if not_adjacent[idx][i]]
+    _, nearest_indices = zip(*sorted(
+        distances, key=lambda x: x[0])[:n_neighbors])
+    indices[idx] = nearest_indices
+  return indices
+
+
+def compute_cross_sequence_recall_at_k(retrieved_labels, labels, k_list):
+  """Compute recall@k for a given list of k values.
+
+  Recall is one if an example of the same class is retrieved among the
+    top k nearest neighbors given a query example and zero otherwise.
+    Counting the recall for all examples and averaging the counts returns
+    recall@k score.
+
+  Args:
+    retrieved_labels: 2-D Numpy array of KNN labels for every embedding.
+    labels: 1-D Numpy array of shape [number of data].
+    k_list: List of k values to evaluate recall@k.
+
+  Returns:
+    recall_list: List of recall@k values.
+  """
+  kvalue_to_recall = dict(zip(k_list, np.zeros(len(k_list))))
+
+  # For each value of K.
+  for k in k_list:
+    matches = defaultdict(float)
+    counts = defaultdict(float)
+    # For each (row index, label value) in the query labels.
+    for i, label_value in enumerate(labels):
+      # Loop over the K nearest retrieved labels.
+      if label_value in retrieved_labels[i][:k]:
+        matches[label_value] += 1.
+      # Increment the denominator.
+      counts[label_value] += 1.
+    kvalue_to_recall[k] = np.mean(
+        [matches[l]/counts[l] for l in matches])
+  return [kvalue_to_recall[i] for i in k_list]
+
+
+def compute_cross_sequence_recalls_at_k(
+    embeddings, labels, label_attr_keys, tasks, k_list, summary_writer,
+    training_step):
+  """Computes and reports the recall@k for each classification problem.
+
+  This takes an embedding matrix and an array of multiclass labels
+  with size [num_data, number of classification problems], then
+  computes the average recall@k for each classification problem
+  as well as the average across problems.
+
+  Args:
+    embeddings: A np.float32 array of size [num_data, embedding_size]
+      representing the embedded validation or test dataset.
+    labels: A np.int32 array of size [num_data, num_classification_problems]
+      holding multiclass labels for each embedding for each problem.
+    label_attr_keys: List of strings, holds the names of the classification
+      problems.
+    tasks: A list of strings describing the video sequence each row
+      belongs to. This is used to restrict the recall@k computation
+      to cross-sequence examples.
+    k_list: A list of ints, the k values to evaluate recall@k.
+    summary_writer: A tf.summary.FileWriter.
+    training_step: Int, the current training step we're evaluating.
+  """
+  num_data = float(embeddings.shape[0])
+  assert labels.shape[0] == num_data
+
+  # Compute knn indices.
+  indices = nearest_cross_sequence_neighbors(
+      embeddings, tasks, n_neighbors=max(k_list))
+  retrieved_labels = labels[indices]
+
+  # Compute the recall@k for each classification problem.
+  recall_lists = []
+  for idx, label_attr in enumerate(label_attr_keys):
+    problem_labels = labels[:, idx]
+    # Take all indices, all k labels for the problem indexed by idx.
+    problem_retrieved = retrieved_labels[:, :, idx]
+    recall_list = compute_cross_sequence_recall_at_k(
+        retrieved_labels=problem_retrieved,
+        labels=problem_labels,
+        k_list=k_list)
+    recall_lists.append(recall_list)
+    for (k, recall) in zip(k_list, recall_list):
+      recall_error = 1-recall
+      summ = tf.Summary(value=[tf.Summary.Value(
+          tag='validation/classification/%s error@top%d' % (
+              label_attr, k),
+          simple_value=recall_error)])
+      print('%s recall@K=%d' % (label_attr, k), recall_error)
+      summary_writer.add_summary(summ, int(training_step))
+
+  # Report an average recall@k across problems.
+  recall_lists = np.array(recall_lists)
+  for i in range(recall_lists.shape[1]):
+    average_recall = np.mean(recall_lists[:, i])
+    recall_error = 1 - average_recall
+    summ = tf.Summary(value=[tf.Summary.Value(
+        tag='validation/classification/average error@top%d' % k_list[i],
+        simple_value=recall_error)])
+    print('Average recall@K=%d' % k_list[i], recall_error)
+    summary_writer.add_summary(summ, int(training_step))
+
+
+def evaluate_once(
+    estimator, input_fn_by_view, batch_size, checkpoint_path,
+    label_attr_keys, embedding_size, num_views, k_list):
+  """Compute the recall@k for a given checkpoint path.
+
+  Args:
+    estimator: an `Estimator` object to evaluate.
+    input_fn_by_view: An input_fn to an `Estimator's` predict method. Takes
+      a view index and returns a dict holding ops for getting raw images for
+      the view.
+    batch_size: Int, size of the labeled eval batch.
+    checkpoint_path: String, path to the specific checkpoint being evaluated.
+    label_attr_keys: A list of Strings, holding each attribute name.
+    embedding_size: Int, the size of the embedding.
+    num_views: Int, number of views in the dataset.
+    k_list: List of ints, list of K values to compute recall at K for.
+  """
+  feat_matrix = np.zeros((0, embedding_size))
+  label_vect = np.zeros((0, len(label_attr_keys)))
+  tasks = []
+  eval_tensor_keys = ['embeddings', 'tasks', 'classification_labels']
+
+  # Iterate all views in the dataset.
+  for view_index in range(num_views):
+    # Set up a graph for embedding entire dataset.
+    predictions = estimator.inference(
+        input_fn_by_view(view_index), checkpoint_path,
+        batch_size, predict_keys=eval_tensor_keys)
+
+    # Enumerate predictions.
+    for i, p in enumerate(predictions):
+      if i % 100 == 0:
+        tf.logging.info('Embedded %d images for view %d' % (i, view_index))
+
+      label = p['classification_labels']
+      task = p['tasks']
+      embedding = p['embeddings']
+
+      # Collect (embedding, label, task) data.
+      feat_matrix = np.append(feat_matrix, [embedding], axis=0)
+      label_vect = np.append(label_vect, [label], axis=0)
+      tasks.append(task)
+
+  # Compute recall statistics.
+  ckpt_step = int(checkpoint_path.split('-')[-1])
+  summary_dir = os.path.join(FLAGS.outdir, 'labeled_eval_summaries')
+  summary_writer = tf.summary.FileWriter(summary_dir)
+  compute_cross_sequence_recalls_at_k(
+      feat_matrix, label_vect, label_attr_keys, tasks, k_list,
+      summary_writer, ckpt_step)
+
+
+def get_labeled_tables(config):
+  """Gets either labeled test or validation tables, based on flags."""
+  # Get a list of filenames corresponding to labeled data.
+  mode = FLAGS.mode
+  if mode == 'validation':
+    labeled_tables = util.GetFilesRecursively(config.data.labeled.validation)
+  elif mode == 'test':
+    labeled_tables = util.GetFilesRecursively(config.data.labeled.test)
+  else:
+    raise ValueError('Unknown dataset: %s' % mode)
+  return labeled_tables
+
+
+def main(_):
+  """Runs main labeled eval loop."""
+  # Parse config dict from yaml config files / command line flags.
+  config = util.ParseConfigsToLuaTable(FLAGS.config_paths, FLAGS.model_params)
+
+  # Choose an estimator based on training strategy.
+  checkpointdir = FLAGS.checkpointdir
+  estimator = get_estimator(config, checkpointdir)
+
+  # Get data configs.
+  image_attr_keys = config.data.labeled.image_attr_keys
+  label_attr_keys = config.data.labeled.label_attr_keys
+  embedding_size = config.embedding_size
+  num_views = config.data.num_views
+  k_list = config.val.recall_at_k_list
+  batch_size = config.data.batch_size
+
+  # Get either labeled validation or test tables.
+  labeled_tables = get_labeled_tables(config)
+
+  def input_fn_by_view(view_index):
+    """Returns an input_fn for use with a tf.Estimator by view."""
+    def input_fn():
+      # Get raw labeled images.
+      (preprocessed_images, labels,
+       tasks) = data_providers.labeled_data_provider(
+           labeled_tables,
+           estimator.preprocess_data, view_index, image_attr_keys,
+           label_attr_keys, batch_size=batch_size)
+      return {
+          'batch_preprocessed': preprocessed_images,
+          'tasks': tasks,
+          'classification_labels': labels,
+      }, None
+    return input_fn
+
+  # If evaluating a specific checkpoint, do that.
+  if FLAGS.checkpoint_iter:
+    checkpoint_path = os.path.join(
+        '%s/model.ckpt-%s' % (checkpointdir, FLAGS.checkpoint_iter))
+    evaluate_once(
+        estimator, input_fn_by_view, batch_size, checkpoint_path,
+        label_attr_keys, embedding_size, num_views, k_list)
+  else:
+    for checkpoint_path in tf.contrib.training.checkpoints_iterator(
+        checkpointdir):
+      evaluate_once(
+          estimator, input_fn_by_view, batch_size, checkpoint_path,
+          label_attr_keys, embedding_size, num_views, k_list)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/labeled_eval_test.py b/research/tcn/labeled_eval_test.py
new file mode 100644
index 00000000..e586e218
--- /dev/null
+++ b/research/tcn/labeled_eval_test.py
@@ -0,0 +1,86 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for tcn.labeled_eval."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import labeled_eval
+import tensorflow as tf
+
+
+class LabeledEvalTest(tf.test.TestCase):
+
+  def testNearestCrossSequenceNeighbors(self):
+    # Generate embeddings.
+    num_data = 64
+    embedding_size = 4
+    num_tasks = 8
+    n_neighbors = 2
+    data = np.random.randn(num_data, embedding_size)
+    tasks = np.repeat(range(num_tasks), num_data // num_tasks)
+
+    # Get nearest cross-sequence indices.
+    indices = labeled_eval.nearest_cross_sequence_neighbors(
+        data, tasks, n_neighbors=n_neighbors)
+
+    # Assert that no nearest neighbor indices come from the same task.
+    repeated_tasks = np.tile(np.reshape(tasks, (num_data, 1)), n_neighbors)
+    self.assertTrue(np.all(np.not_equal(repeated_tasks, tasks[indices])))
+
+  def testPerfectCrossSequenceRecall(self):
+    # Make sure cross-sequence recall@k returns 1.0 for near-duplicate features.
+    embeddings = np.random.randn(10, 2)
+    embeddings[5:, :] = 0.00001 + embeddings[:5, :]
+    tasks = np.repeat([0, 1], 5)
+    labels = np.array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])
+    # find k=1, k=2 nearest neighbors.
+    k_list = [1, 2]
+
+    # Compute knn indices.
+    indices = labeled_eval.nearest_cross_sequence_neighbors(
+        embeddings, tasks, n_neighbors=max(k_list))
+    retrieved_labels = labels[indices]
+    recall_list = labeled_eval.compute_cross_sequence_recall_at_k(
+        retrieved_labels=retrieved_labels,
+        labels=labels,
+        k_list=k_list)
+    self.assertTrue(np.allclose(
+        np.array(recall_list), np.array([1.0, 1.0])))
+
+  def testRelativeRecall(self):
+    # Make sure cross-sequence recall@k is strictly non-decreasing over k.
+    num_data = 100
+    num_tasks = 10
+    embeddings = np.random.randn(100, 5)
+    tasks = np.repeat(range(num_tasks), num_data // num_tasks)
+    labels = np.random.randint(0, 5, 100)
+
+    k_list = [1, 2, 4, 8, 16, 32, 64]
+    indices = labeled_eval.nearest_cross_sequence_neighbors(
+        embeddings, tasks, n_neighbors=max(k_list))
+    retrieved_labels = labels[indices]
+    recall_list = labeled_eval.compute_cross_sequence_recall_at_k(
+        retrieved_labels=retrieved_labels,
+        labels=labels,
+        k_list=k_list)
+    recall_list_sorted = sorted(recall_list)
+    self.assertTrue(np.allclose(
+        np.array(recall_list), np.array(recall_list_sorted)))
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/research/tcn/model.py b/research/tcn/model.py
new file mode 100644
index 00000000..91db1b3e
--- /dev/null
+++ b/research/tcn/model.py
@@ -0,0 +1,410 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Model implementations."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from abc import ABCMeta
+from abc import abstractmethod
+import tensorflow as tf
+import tensorflow.contrib.slim as slim
+from tensorflow.contrib.slim.python.slim.nets import inception
+from tensorflow.contrib.slim.python.slim.nets import resnet_v2 as resnet_v2
+from tensorflow.contrib.slim.python.slim.nets import resnet_utils as resnet_utils
+
+
+def get_embedder(
+    embedder_strategy, config, images, is_training, reuse=False,
+    l2_normalize_embedding=True):
+  """Returns an embedder based on config.
+
+  Args:
+    embedder_strategy: String, name of embedder version to return.
+    config: LuaTable object, training config.
+    images: 4-D float `Tensor` containing batch images.
+    is_training: Boolean or placeholder for boolean,
+      indicator for whether or not we're training.
+    reuse: Boolean: Reuse embedder variable scope.
+    l2_normalize_embedding: Boolean, whether or not to l2 normalize the
+      embedding.
+  Returns:
+    embedder: An `Embedder` object.
+  Raises:
+    ValueError: if unknown embedder_strategy specified.
+  """
+  if embedder_strategy == 'inception_baseline':
+    pretrained_ckpt = config.inception_conv_ss_fc.pretrained_checkpoint
+    return InceptionBaselineEmbedder(
+        images,
+        pretrained_ckpt,
+        config.random_projection,
+        config.random_projection_dim)
+
+  strategy_to_embedder = {
+      'inception_conv_ss_fc': InceptionConvSSFCEmbedder,
+      'resnet': ResnetEmbedder,
+  }
+  if embedder_strategy not in strategy_to_embedder:
+    raise ValueError('unknown embedder_strategy', embedder_strategy)
+
+  embedding_size = config.embedding_size
+  l2_reg_weight = config.learning.l2_reg_weight
+  embedder = strategy_to_embedder[embedder_strategy](
+      config[embedder_strategy], images, embedding_size,
+      is_training, embedding_l2=l2_normalize_embedding,
+      l2_reg_weight=l2_reg_weight, reuse=reuse)
+  return embedder
+
+
+def build_inceptionv3_graph(images, endpoint, is_training, checkpoint,
+                            reuse=False):
+  """Builds an InceptionV3 model graph.
+
+  Args:
+    images: A 4-D float32 `Tensor` of batch images.
+    endpoint: String, name of the InceptionV3 endpoint.
+    is_training: Boolean, whether or not to build a training or inference graph.
+    checkpoint: String, path to the pretrained model checkpoint.
+    reuse: Boolean, whether or not we are reusing the embedder.
+  Returns:
+    inception_output: `Tensor` holding the InceptionV3 output.
+    inception_variables: List of inception variables.
+    init_fn: Function to initialize the weights (if not reusing, then None).
+  """
+  with slim.arg_scope(inception.inception_v3_arg_scope()):
+    _, endpoints = inception.inception_v3(
+        images, num_classes=1001, is_training=is_training)
+    inception_output = endpoints[endpoint]
+    inception_variables = slim.get_variables_to_restore()
+    inception_variables = [
+        i for i in inception_variables if 'global_step' not in i.name]
+    if is_training and not reuse:
+      init_saver = tf.train.Saver(inception_variables)
+      def init_fn(scaffold, sess):
+        del scaffold
+        init_saver.restore(sess, checkpoint)
+    else:
+      init_fn = None
+    return inception_output, inception_variables, init_fn
+
+
+class InceptionBaselineEmbedder(object):
+  """Produces pre-trained InceptionV3 embeddings."""
+
+  def __init__(self, images, pretrained_ckpt, reuse=False,
+               random_projection=False, random_projection_dim=32):
+    # Build InceptionV3 graph.
+    (inception_output,
+     self.inception_variables,
+     self.init_fn) = build_inceptionv3_graph(
+         images, 'Mixed_7c', False, pretrained_ckpt, reuse)
+
+    # Pool 8x8x2048 -> 1x1x2048.
+    embedding = slim.avg_pool2d(inception_output, [8, 8], stride=1)
+    embedding = tf.squeeze(embedding, [1, 2])
+
+    if random_projection:
+      embedding = tf.matmul(
+          embedding, tf.random_normal(
+              shape=[2048, random_projection_dim], seed=123))
+    self.embedding = embedding
+
+
+class PretrainedEmbedder(object):
+  """Base class for embedders that take pre-trained networks as input."""
+  __metaclass__ = ABCMeta
+
+  def __init__(self, config, images, embedding_size, is_training,
+               embedding_l2=True, l2_reg_weight=1e-6, reuse=False):
+    """Constructor.
+
+    Args:
+      config: A T object holding training config.
+      images: A 4-D float32 `Tensor` holding images to embed.
+      embedding_size: Int, the size of the embedding.
+      is_training: Boolean, whether or not this is a training or inference-time
+        graph.
+      embedding_l2: Boolean, whether or not to l2 normalize the embedding.
+      l2_reg_weight: Float, weight applied to l2 weight regularization.
+      reuse: Boolean, whether or not we're reusing this graph.
+    """
+    # Pull out all the embedder hyperparameters.
+    self._config = config
+    self._embedding_size = embedding_size
+    self._l2_reg_weight = l2_reg_weight
+    self._embedding_l2 = embedding_l2
+    self._is_training = is_training
+    self._reuse = reuse
+
+    # Pull out pretrained hparams.
+    pretrained_checkpoint = config.pretrained_checkpoint
+    pretrained_layer = config.pretrained_layer
+    pretrained_keep_prob = config.dropout.keep_pretrained
+
+    # Build pretrained graph.
+    (pretrained_output,
+     self._pretrained_variables,
+     self.init_fn) = self.build_pretrained_graph(
+         images, pretrained_layer, pretrained_checkpoint, is_training, reuse)
+
+    # Optionally drop out the activations.
+    pretrained_output = slim.dropout(
+        pretrained_output, keep_prob=pretrained_keep_prob,
+        is_training=is_training)
+    self._pretrained_output = pretrained_output
+
+  @abstractmethod
+  def build_pretrained_graph(self, images, layer, pretrained_checkpoint,
+                             is_training, reuse):
+    """Builds the graph for the pre-trained network.
+
+    Method to be overridden by implementations.
+
+    Args:
+      images: A 4-D tf.float32 `Tensor` holding images to embed.
+      layer: String, defining which pretrained layer to take as input
+        to adaptation layers.
+      pretrained_checkpoint: String, path to a checkpoint used to load
+        pretrained weights.
+      is_training: Boolean, whether or not we're in training mode.
+      reuse: Boolean, whether or not to reuse embedder weights.
+
+    Returns:
+      pretrained_output: A 2 or 3-d tf.float32 `Tensor` holding pretrained
+        activations.
+    """
+    pass
+
+  @abstractmethod
+  def construct_embedding(self):
+    """Builds an embedding function on top of images.
+
+    Method to be overridden by implementations.
+
+    Returns:
+      embeddings: A 2-d float32 `Tensor` of shape [batch_size, embedding_size]
+        holding the embedded images.
+    """
+    pass
+
+  def get_trainable_variables(self):
+    """Gets a list of variables to optimize."""
+    if self._config.finetune:
+      return tf.trainable_variables()
+    else:
+      adaptation_only_vars = tf.get_collection(
+          tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._adaptation_scope)
+      return adaptation_only_vars
+
+
+class ResnetEmbedder(PretrainedEmbedder):
+  """Resnet TCN.
+
+  ResnetV2 -> resnet adaptation layers -> optional l2 normalize -> embedding.
+  """
+
+  def __init__(self, config, images, embedding_size, is_training,
+               embedding_l2=True, l2_reg_weight=1e-6, reuse=False):
+    super(ResnetEmbedder, self).__init__(
+        config, images, embedding_size, is_training, embedding_l2,
+        l2_reg_weight, reuse)
+
+  def build_pretrained_graph(
+      self, images, resnet_layer, checkpoint, is_training, reuse=False):
+    """See baseclass."""
+    with slim.arg_scope(resnet_v2.resnet_arg_scope()):
+      _, endpoints = resnet_v2.resnet_v2_50(
+          images, is_training=is_training, reuse=reuse)
+      resnet_layer = 'resnet_v2_50/block%d' % resnet_layer
+      resnet_output = endpoints[resnet_layer]
+      resnet_variables = slim.get_variables_to_restore()
+      resnet_variables = [
+          i for i in resnet_variables if 'global_step' not in i.name]
+      if is_training and not reuse:
+        init_saver = tf.train.Saver(resnet_variables)
+        def init_fn(scaffold, sess):
+          del scaffold
+          init_saver.restore(sess, checkpoint)
+      else:
+        init_fn = None
+
+      return resnet_output, resnet_variables, init_fn
+
+  def construct_embedding(self):
+    """Builds an embedding function on top of images.
+
+    Method to be overridden by implementations.
+
+    Returns:
+      embeddings: A 2-d float32 `Tensor` of shape [batch_size, embedding_size]
+        holding the embedded images.
+    """
+    with tf.variable_scope('tcn_net', reuse=self._reuse) as vs:
+      self._adaptation_scope = vs.name
+      net = self._pretrained_output
+
+      # Define some adaptation blocks on top of the pre-trained resnet output.
+      adaptation_blocks = []
+      adaptation_block_params = [map(
+          int, i.split('_')) for i in self._config.adaptation_blocks.split('-')]
+      for i, (depth, num_units) in enumerate(adaptation_block_params):
+        block = resnet_v2.resnet_v2_block(
+            'adaptation_block_%d' % i, base_depth=depth, num_units=num_units,
+            stride=1)
+        adaptation_blocks.append(block)
+
+      # Stack them on top of the resent output.
+      net = resnet_utils.stack_blocks_dense(
+          net, adaptation_blocks, output_stride=None)
+
+      # Average pool the output.
+      net = tf.reduce_mean(net, [1, 2], name='adaptation_pool', keep_dims=True)
+
+      if self._config.emb_connection == 'fc':
+        # Use fully connected layer to project to embedding layer.
+        fc_hidden_sizes = self._config.fc_hidden_sizes
+        if fc_hidden_sizes == 'None':
+          fc_hidden_sizes = []
+        else:
+          fc_hidden_sizes = map(int, fc_hidden_sizes.split('_'))
+        fc_hidden_keep_prob = self._config.dropout.keep_fc
+        net = tf.squeeze(net)
+        for fc_hidden_size in fc_hidden_sizes:
+          net = slim.layers.fully_connected(net, fc_hidden_size)
+          if fc_hidden_keep_prob < 1.0:
+            net = slim.dropout(net, keep_prob=fc_hidden_keep_prob,
+                               is_training=self._is_training)
+
+        # Connect last FC layer to embedding.
+        embedding = slim.layers.fully_connected(net, self._embedding_size,
+                                                activation_fn=None)
+      else:
+        # Use 1x1 conv layer to project to embedding layer.
+        embedding = slim.conv2d(
+            net, self._embedding_size, [1, 1], activation_fn=None,
+            normalizer_fn=None, scope='embedding')
+        embedding = tf.squeeze(embedding)
+
+      # Optionally L2 normalize the embedding.
+      if self._embedding_l2:
+        embedding = tf.nn.l2_normalize(embedding, dim=1)
+
+      return embedding
+
+  def get_trainable_variables(self):
+    """Gets a list of variables to optimize."""
+    if self._config.finetune:
+      return tf.trainable_variables()
+    else:
+      adaptation_only_vars = tf.get_collection(
+          tf.GraphKeys.TRAINABLE_VARIABLES, scope=self._adaptation_scope)
+      return adaptation_only_vars
+
+
+class InceptionEmbedderBase(PretrainedEmbedder):
+  """Base class for embedders that take pre-trained InceptionV3 activations."""
+
+  def __init__(self, config, images, embedding_size, is_training,
+               embedding_l2=True, l2_reg_weight=1e-6, reuse=False):
+    super(InceptionEmbedderBase, self).__init__(
+        config, images, embedding_size, is_training, embedding_l2,
+        l2_reg_weight, reuse)
+
+  def build_pretrained_graph(
+      self, images, inception_layer, checkpoint, is_training, reuse=False):
+    """See baseclass."""
+    # Build InceptionV3 graph.
+    inception_output, inception_variables, init_fn = build_inceptionv3_graph(
+        images, inception_layer, is_training, checkpoint, reuse)
+    return inception_output, inception_variables, init_fn
+
+
+class InceptionConvSSFCEmbedder(InceptionEmbedderBase):
+  """TCN Embedder V1.
+
+  InceptionV3 (mixed_5d) -> conv layers -> spatial softmax ->
+    fully connected -> optional l2 normalize -> embedding.
+  """
+
+  def __init__(self, config, images, embedding_size, is_training,
+               embedding_l2=True, l2_reg_weight=1e-6, reuse=False):
+    super(InceptionConvSSFCEmbedder, self).__init__(
+        config, images, embedding_size, is_training, embedding_l2,
+        l2_reg_weight, reuse)
+
+    # Pull out all the hyperparameters specific to this embedder.
+    self._additional_conv_sizes = config.additional_conv_sizes
+    self._conv_hidden_keep_prob = config.dropout.keep_conv
+    self._fc_hidden_sizes = config.fc_hidden_sizes
+    self._fc_hidden_keep_prob = config.dropout.keep_fc
+
+  def construct_embedding(self):
+    """Builds a conv -> spatial softmax -> FC adaptation network."""
+    is_training = self._is_training
+    normalizer_params = {'is_training': is_training}
+    with tf.variable_scope('tcn_net', reuse=self._reuse) as vs:
+      self._adaptation_scope = vs.name
+      with slim.arg_scope(
+          [slim.layers.conv2d],
+          activation_fn=tf.nn.relu,
+          normalizer_fn=slim.batch_norm, normalizer_params=normalizer_params,
+          weights_regularizer=slim.regularizers.l2_regularizer(
+              self._l2_reg_weight),
+          biases_regularizer=slim.regularizers.l2_regularizer(
+              self._l2_reg_weight)):
+        with slim.arg_scope(
+            [slim.layers.fully_connected],
+            activation_fn=tf.nn.relu,
+            normalizer_fn=slim.batch_norm, normalizer_params=normalizer_params,
+            weights_regularizer=slim.regularizers.l2_regularizer(
+                self._l2_reg_weight),
+            biases_regularizer=slim.regularizers.l2_regularizer(
+                self._l2_reg_weight)):
+
+          # Input to embedder is pre-trained inception output.
+          net = self._pretrained_output
+
+          # Optionally add more conv layers.
+          for num_filters in self._additional_conv_sizes:
+            net = slim.layers.conv2d(
+                net, num_filters, kernel_size=[3, 3], stride=[1, 1])
+            net = slim.dropout(net, keep_prob=self._conv_hidden_keep_prob,
+                               is_training=is_training)
+
+          # Take the spatial soft arg-max of the last convolutional layer.
+          # This is a form of spatial attention over the activations.
+          # See more here: http://arxiv.org/abs/1509.06113.
+          net = tf.contrib.layers.spatial_softmax(net)
+          self.spatial_features = net
+
+          # Add fully connected layers.
+          net = slim.layers.flatten(net)
+          for fc_hidden_size in self._fc_hidden_sizes:
+            net = slim.layers.fully_connected(net, fc_hidden_size)
+            if self._fc_hidden_keep_prob < 1.0:
+              net = slim.dropout(net, keep_prob=self._fc_hidden_keep_prob,
+                                 is_training=is_training)
+
+          # Connect last FC layer to embedding.
+          net = slim.layers.fully_connected(net, self._embedding_size,
+                                            activation_fn=None)
+
+          # Optionally L2 normalize the embedding.
+          if self._embedding_l2:
+            net = tf.nn.l2_normalize(net, dim=1)
+
+          return net
diff --git a/research/tcn/preprocessing.py b/research/tcn/preprocessing.py
new file mode 100644
index 00000000..8c63b19f
--- /dev/null
+++ b/research/tcn/preprocessing.py
@@ -0,0 +1,686 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Image preprocessing helpers."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import cv2
+from scipy import ndimage
+import tensorflow as tf
+from tensorflow.python.ops import control_flow_ops
+
+
+def apply_with_random_selector(x, func, num_cases):
+  """Computes func(x, sel), with sel sampled from [0...num_cases-1].
+
+  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are
+  pipfied.
+  Source:
+  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py
+
+  Args:
+    x: input Tensor.
+    func: Python function to apply.
+    num_cases: Python int32, number of cases to sample sel from.
+  Returns:
+    The result of func(x, sel), where func receives the value of the
+    selector as a python integer, but sel is sampled dynamically.
+  """
+  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)
+  # Pass the real x only to one of the func calls.
+  return control_flow_ops.merge([
+      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)
+      for case in range(num_cases)])[0]
+
+
+def distorted_bounding_box_crop(image,
+                                bbox,
+                                min_object_covered=0.1,
+                                aspect_ratio_range=(0.75, 1.33),
+                                area_range=(0.05, 1.0),
+                                max_attempts=100,
+                                scope=None):
+  """Generates cropped_image using a one of the bboxes randomly distorted.
+
+  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are
+  pipfied.
+  Source:
+  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py
+
+  See `tf.image.sample_distorted_bounding_box` for more documentation.
+
+  Args:
+    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).
+    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]
+      where each coordinate is [0, 1) and the coordinates are arranged
+      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole
+      image.
+    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped
+      area of the image must contain at least this fraction of any bounding box
+      supplied.
+    aspect_ratio_range: An optional list of `floats`. The cropped area of the
+      image must have an aspect ratio = width / height within this range.
+    area_range: An optional list of `floats`. The cropped area of the image
+      must contain a fraction of the supplied image within in this range.
+    max_attempts: An optional `int`. Number of attempts at generating a cropped
+      region of the image of the specified constraints. After `max_attempts`
+      failures, return the entire image.
+    scope: Optional scope for name_scope.
+  Returns:
+    A tuple, a 3-D Tensor cropped_image and the distorted bbox
+  """
+  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):
+    # Each bounding box has shape [1, num_boxes, box coords] and
+    # the coordinates are ordered [ymin, xmin, ymax, xmax].
+
+    # A large fraction of image datasets contain a human-annotated bounding
+    # box delineating the region of the image containing the object of interest.
+    # We choose to create a new bounding box for the object which is a randomly
+    # distorted version of the human-annotated bounding box that obeys an
+    # allowed range of aspect ratios, sizes and overlap with the human-annotated
+    # bounding box. If no box is supplied, then we assume the bounding box is
+    # the entire image.
+    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
+        tf.shape(image),
+        bounding_boxes=bbox,
+        min_object_covered=min_object_covered,
+        aspect_ratio_range=aspect_ratio_range,
+        area_range=area_range,
+        max_attempts=max_attempts,
+        use_image_if_no_bounding_boxes=True)
+    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box
+
+    # Crop the image to the specified bounding box.
+    cropped_image = tf.slice(image, bbox_begin, bbox_size)
+    return cropped_image, distort_bbox
+
+
+def distort_color(image, color_ordering=0, fast_mode=True, scope=None):
+  """Distort the color of a Tensor image.
+
+  TODO(coreylynch): add as a dependency, when slim or tensorflow/models are
+  pipfied.
+  Source:
+  https://raw.githubusercontent.com/tensorflow/models/a9d0e6e8923a4/slim/preprocessing/inception_preprocessing.py
+
+  Each color distortion is non-commutative and thus ordering of the color ops
+  matters. Ideally we would randomly permute the ordering of the color ops.
+  Rather then adding that level of complication, we select a distinct ordering
+  of color ops for each preprocessing thread.
+  Args:
+    image: 3-D Tensor containing single image in [0, 1].
+    color_ordering: Python int, a type of distortion (valid values: 0-3).
+    fast_mode: Avoids slower ops (random_hue and random_contrast)
+    scope: Optional scope for name_scope.
+  Returns:
+    3-D Tensor color-distorted image on range [0, 1]
+  Raises:
+    ValueError: if color_ordering not in [0, 3]
+  """
+  with tf.name_scope(scope, 'distort_color', [image]):
+    if fast_mode:
+      if color_ordering == 0:
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+      else:
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+    else:
+      if color_ordering == 0:
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+        image = tf.image.random_hue(image, max_delta=0.2)
+        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+      elif color_ordering == 1:
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+        image = tf.image.random_hue(image, max_delta=0.2)
+      elif color_ordering == 2:
+        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+        image = tf.image.random_hue(image, max_delta=0.2)
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+      elif color_ordering == 3:
+        image = tf.image.random_hue(image, max_delta=0.2)
+        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
+        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+        image = tf.image.random_brightness(image, max_delta=32. / 255.)
+      else:
+        raise ValueError('color_ordering must be in [0, 3]')
+
+    # The random_* ops do not necessarily clamp.
+    return tf.clip_by_value(image, 0.0, 1.0)
+
+
+def crop_center(image):
+  """Returns a cropped square image."""
+  shape = tf.shape(image)
+  new_shape = tf.minimum(shape[0], shape[1])
+  offset_y = tf.maximum(shape[0] - shape[1], 0) // 2
+  offset_x = tf.maximum(shape[1] - shape[0], 0) // 2
+  image = tf.image.crop_to_bounding_box(
+      image, offset_y, offset_x, new_shape, new_shape)
+  return image
+
+
+def pad(image):
+  """Returns an image padded to be square."""
+  shape = tf.shape(image)
+  new_shape = tf.maximum(shape[0], shape[1])
+  height = shape[0]
+  width = shape[1]
+  offset_x = tf.maximum((height-width), 0) // 2
+  offset_y = tf.maximum((width-height), 0) // 2
+  image = tf.image.pad_to_bounding_box(
+      image, offset_y, offset_x, new_shape, new_shape)
+  return image
+
+
+def pad_200(image):
+  """Returns an image padded width-padded with 200 pixels."""
+  shape = tf.shape(image)
+  image = tf.image.pad_to_bounding_box(
+      image, 0, 200, shape[0], shape[1]+400)
+  shape = tf.shape(image)
+  new_shape = tf.minimum(shape[0], shape[1])
+  offset_y = tf.maximum(shape[0] - shape[1], 0) // 2
+  offset_x = tf.maximum(shape[1] - shape[0], 0) // 2
+  image = tf.image.crop_to_bounding_box(
+      image, offset_y, offset_x, new_shape, new_shape)
+  return image
+
+
+def pad_crop_central(image, central_fraction=0.875):
+  """Pads the image to the maximum length, crops the central fraction."""
+  # Pad the image to be square.
+  image = pad(image)
+  # Crop the central region of the image with an area containing 87.5% of
+  # the original image.
+  image = tf.image.central_crop(image, central_fraction=central_fraction)
+  return image
+
+
+def crop_image_by_strategy(image, cropping):
+  """Crops an image according to a strategy defined in config.
+
+  Args:
+    image: 3-d image tensor.
+    cropping: str, name of cropping strategy.
+  Returns:
+    image: cropped image.
+  Raises:
+    ValueError: When unknown cropping strategy is specified.
+  """
+  strategy_to_method = {
+      'crop_center': crop_center,
+      'pad': pad,
+      'pad200': pad_200,
+      'pad_crop_central': pad_crop_central
+  }
+  tf.logging.info('Cropping strategy: %s.' % cropping)
+  if cropping not in strategy_to_method:
+    raise ValueError('Unknown cropping strategy: %s' % cropping)
+  return strategy_to_method[cropping](image)
+
+
+def scale_augment_crop(image, central_bbox, area_range, min_object_covered):
+  """Training time scale augmentation.
+
+  Args:
+    image: 3-d float tensor.
+    central_bbox: Bounding box defining the central region of interest.
+    area_range: Range of allowed areas for the augmented bounding box.
+    min_object_covered: Constraint for the fraction of original image in
+      augmented bounding box.
+  Returns:
+    distort_image: The scaled, cropped image.
+  """
+  (distorted_image, _) = distorted_bounding_box_crop(
+      image, central_bbox, area_range=area_range,
+      aspect_ratio_range=(1.0, 1.0),
+      min_object_covered=min_object_covered)
+  # Restore the shape since the dynamic slice based upon the bbox_size loses
+  # the third dimension.
+  distorted_image.set_shape([None, None, 3])
+  return distorted_image
+
+
+def scale_to_inception_range(image):
+  """Scales an image in the range [0,1] to [-1,1] as expected by inception."""
+  # Assert that incoming images have been properly scaled to [0,1].
+  with tf.control_dependencies(
+      [tf.assert_less_equal(tf.reduce_max(image), 1.),
+       tf.assert_greater_equal(tf.reduce_min(image), 0.)]):
+    image = tf.subtract(image, 0.5)
+    image = tf.multiply(image, 2.0)
+    return image
+
+
+def resize_image(image, height, width):
+  """Resizes an image to a target height and width."""
+  image = tf.expand_dims(image, 0)
+  image = tf.image.resize_bilinear(image, [height, width], align_corners=False)
+  image = tf.squeeze(image, [0])
+  return image
+
+
+def crop_or_pad(image, curr_height, curr_width, new, height=True, crop=True):
+  """Crops or pads an image.
+
+  Args:
+    image: 3-D float32 `Tensor` image.
+    curr_height: Int, current height.
+    curr_width: Int, current width.
+    new: Int, new width or height.
+    height: Boolean, cropping or padding for height.
+    crop: Boolean, True if we're cropping, False if we're padding.
+  Returns:
+    image: 3-D float32 `Tensor` image.
+  """
+  # Crop the image to fit the new shape.
+  abs_diff = tf.abs(new-curr_height)//2 if height else tf.abs(new-curr_width)//2
+  offset_x = 0 if height else abs_diff
+  offset_y = abs_diff if height else 0
+
+  # We process height first, so always pad/crop to new height.
+  target_height = new
+  # We process height first, so pad/crop to new width only if not doing height.
+  target_width = curr_width if height else new
+
+  if crop:
+    image = tf.image.crop_to_bounding_box(
+        image, offset_y, offset_x, target_height, target_width)
+  else:
+    image = tf.image.pad_to_bounding_box(
+        image, offset_y, offset_x, target_height, target_width)
+  return image
+
+
+def get_central_bbox(min_side, new_size):
+  """Gets the central bounding box for an image.
+
+  If image is square, returns bounding box [0,0,1,1].
+  Otherwise, returns the bounding box containing the central
+  smallest side x smallest side square.
+
+  Args:
+    min_side: Int, size of smallest side in pixels.
+    new_size: Int, resize image to a square of new_size x new_size pixels.
+  Returns:
+    bbox: A 4-D Int `Tensor`, holding the coordinates of the central bounding
+      box.
+  """
+  max_shape = tf.cast(new_size, tf.float32)
+  min_shape = tf.cast(min_side, tf.float32)
+  top_xy = ((max_shape-min_shape)/2)/max_shape
+  bottom_xy = (min_shape+(max_shape-min_shape)/2)/max_shape
+  # Create a bbox for the center region of interest.
+  bbox = tf.stack([[[top_xy, top_xy, bottom_xy, bottom_xy]]])
+  bbox.set_shape([1, 1, 4])
+  return bbox
+
+
+def pad_to_max(image, max_scale):
+  """Pads an image to max_scale times the current center crop size.
+
+  E.g.: For an image with dimensions 1920x1080 and a max_scale of 1.5,
+  returns an image that is 1.5 * (1080x1080).
+
+  Args:
+    image: 3-D float32 `Tensor` image.
+    max_scale: Float, maximum scale of the image, as a multiplier on the
+      central bounding box.
+  Returns:
+    image: 3-D float32 `Tensor` image.
+  """
+  orig_shape = tf.shape(image)
+  orig_height = orig_shape[0]
+  orig_width = orig_shape[1]
+
+  # Find the smallest side and corresponding new size.
+  min_side = tf.cast(tf.minimum(orig_height, orig_width), tf.float32)
+  new_shape = tf.cast(tf.sqrt(max_scale*min_side*min_side), tf.int32)
+
+  # Crop or pad height.
+  # pylint: disable=g-long-lambda
+  image = tf.cond(
+      orig_height >= new_shape,
+      lambda: crop_or_pad(
+          image, orig_height, orig_width, new_shape, height=True, crop=True),
+      lambda: crop_or_pad(
+          image, orig_height, orig_width, new_shape, height=True, crop=False))
+
+  # Crop or pad width.
+  image = tf.cond(
+      orig_width >= new_shape,
+      lambda: crop_or_pad(
+          image, orig_height, orig_width, new_shape, height=False, crop=True),
+      lambda: crop_or_pad(
+          image, orig_height, orig_width, new_shape, height=False, crop=False))
+
+  # Get the bounding box of the original centered box in the new resized image.
+  original_bounding_box = get_central_bbox(min_side, new_shape)
+  return image, original_bounding_box
+
+
+def scale_up_augmentation(image, max_scale):
+  """Scales an image randomly >100% up to some max scale."""
+  # Pad to max size.
+  image, original_central_bbox = pad_to_max(image, max_scale)
+
+  # Determine area range of the augmented crop, as a percentage of the
+  # new max area.
+  # aug_max == 100% of new max area.
+  aug_max = 1.0
+  # aug_min == original_area/new_area == original_area/(max_scale*original_area)
+  # == 1/max_scale.
+  aug_min = 1.0/max_scale
+  area_range = (aug_min, aug_max)
+  # Since we're doing >100% scale, always have the full original crop in frame.
+  min_object_covered = 1.0
+  # Get a random scaled, cropped image.
+  image = scale_augment_crop(image, original_central_bbox, area_range,
+                             min_object_covered)
+  return image
+
+
+def scale_down_augmentation(image, min_scale):
+  """Scales an image randomly <100% down to some min scale."""
+  # Crop the center, and consider the whole image the bounding box ROI.
+  image = crop_center(image)
+  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
+  # Determine area range of the augmented crop, as a percentage of the
+  # original crop center area.
+  # aug_max == 100% of original area.
+  area_range = (min_scale, 1.0)
+  # Get a random scaled, cropped image.
+  image = scale_augment_crop(image, bbox, area_range, min_scale)
+  return image
+
+
+def augment_image_scale(image, min_scale, max_scale, p_scale_up):
+  """Training time scale augmentation.
+
+  Args:
+    image: 3-d float tensor representing image.
+    min_scale: minimum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    max_scale: maximum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    p_scale_up: Fraction of images scaled up.
+  Returns:
+    image: The scale-augmented image.
+  """
+  assert max_scale >= 1.0
+  assert min_scale <= 1.0
+  if min_scale == max_scale == 1.0:
+    tf.logging.info('Min and max scale are 1.0, don`t augment.')
+    # Do no augmentation, just crop the center.
+    return crop_center(image)
+  elif (max_scale == 1.0) and (min_scale < 1.0):
+    tf.logging.info('Max scale is 1.0, only scale down augment.')
+    # Always do <100% augmentation.
+    return scale_down_augmentation(image, min_scale)
+  elif (min_scale == 1.0) and (max_scale > 1.0):
+    tf.logging.info('Min scale is 1.0, only scale up augment.')
+    # Always do >100% augmentation.
+    return scale_up_augmentation(image, max_scale)
+  else:
+    tf.logging.info('Sample both augmentations.')
+    # Choose to scale image up or down.
+    rn = tf.random_uniform([], minval=0., maxval=1., dtype=tf.float32)
+    image = tf.cond(rn >= p_scale_up,
+                    lambda: scale_up_augmentation(image, max_scale),
+                    lambda: scale_down_augmentation(image, min_scale))
+  return image
+
+
+def decode_image(image_str):
+  """Decodes a jpeg-encoded image string into a image in range [0,1]."""
+  # Decode jpeg string into np.uint8 tensor.
+  image = tf.image.decode_jpeg(image_str, channels=3)
+  # Convert the image to range [0,1].
+  if image.dtype != tf.float32:
+    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
+  return image
+
+
+def decode_images(image_strs):
+  """Decodes a tensor of image strings."""
+  return tf.map_fn(decode_image, image_strs, dtype=tf.float32)
+
+
+def preprocess_training_images(images, height, width, min_scale, max_scale,
+                               p_scale_up, aug_color=True, fast_mode=True):
+  """Preprocesses a batch of images for training.
+
+  This applies training-time scale and color augmentation, crops/resizes,
+  and scales images to the [-1,1] range expected by pre-trained Inception nets.
+
+  Args:
+    images: A 4-D float32 `Tensor` holding raw images to be preprocessed.
+    height: Int, height in pixels to resize image to.
+    width: Int, width in pixels to resize image to.
+    min_scale: Float, minimum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    max_scale: Float, maximum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    p_scale_up: Float, fraction of images scaled up.
+    aug_color: Whether or not to do color augmentation.
+    fast_mode: Boolean, avoids slower ops (random_hue and random_contrast).
+  Returns:
+    preprocessed_images: A 4-D float32 `Tensor` holding preprocessed images.
+  """
+  def _prepro_train(im):
+    """Map this preprocessing function over each image in the batch."""
+    return preprocess_training_image(
+        im, height, width, min_scale, max_scale, p_scale_up,
+        aug_color=aug_color, fast_mode=fast_mode)
+  return tf.map_fn(_prepro_train, images)
+
+
+def preprocess_training_image(
+    image, height, width, min_scale, max_scale, p_scale_up,
+    aug_color=True, fast_mode=True):
+  """Preprocesses an image for training.
+
+  Args:
+    image: A 3-d float tensor representing the image.
+    height: Target image height.
+    width: Target image width.
+    min_scale: Minimum scale of bounding box (as a percentage of full
+      bounding box) used to crop image during scale augmentation.
+    max_scale: Minimum scale of bounding box (as a percentage of full
+      bounding box) used to crop image during scale augmentation.
+    p_scale_up: Fraction of images to scale >100%.
+    aug_color: Whether or not to do color augmentation.
+    fast_mode: Avoids slower ops (random_hue and random_contrast).
+  Returns:
+    scaled_image: An scaled image tensor in the range [-1,1].
+  """
+  # Get a random scaled, cropped image.
+  image = augment_image_scale(image, min_scale, max_scale, p_scale_up)
+
+  # Resize image to desired height, width.
+  image = tf.expand_dims(image, 0)
+  image = tf.image.resize_bilinear(image, [height, width], align_corners=False)
+  image = tf.squeeze(image, [0])
+
+  # Optionally augment the color.
+  # pylint: disable=g-long-lambda
+  if aug_color:
+    image = apply_with_random_selector(
+        image,
+        lambda x, ordering: distort_color(
+            x, ordering, fast_mode=fast_mode), num_cases=4)
+
+  # Scale to [-1,1] range as expected by inception.
+  scaled_image = scale_to_inception_range(image)
+  return scaled_image
+
+
+def preprocess_test_image(image, height, width, crop_strategy):
+  """Preprocesses an image for test/inference.
+
+  Args:
+    image: A 3-d float tensor representing the image.
+    height: Target image height.
+    width: Target image width.
+    crop_strategy: String, name of the strategy used to crop test-time images.
+      Can be: 'crop_center', 'pad', 'pad_200', 'pad_crop_central'.
+  Returns:
+    scaled_image: An scaled image tensor in the range [-1,1].
+  """
+  image = crop_image_by_strategy(image, crop_strategy)
+  # Resize.
+  image = resize_image(image, height, width)
+  # Scale the input range to [-1,1] as expected by inception.
+  image = scale_to_inception_range(image)
+  return image
+
+
+def preprocess_test_images(images, height, width, crop_strategy):
+  """Apply test-time preprocessing to a batch of images.
+
+  This crops images (given a named strategy for doing so), resizes them,
+  and scales them to the [-1,1] range expected by pre-trained Inception nets.
+
+  Args:
+    images: A 4-D float32 `Tensor` holding raw images to be preprocessed.
+    height: Int, height in pixels to resize image to.
+    width: Int, width in pixels to resize image to.
+    crop_strategy: String, name of the strategy used to crop test-time images.
+      Can be: 'crop_center', 'pad', 'pad_200', 'pad_crop_central'.
+  Returns:
+    preprocessed_images: A 4-D float32 `Tensor` holding preprocessed images.
+  """
+  def _prepro_test(im):
+    """Map this preprocessing function over each image in the batch."""
+    return preprocess_test_image(im, height, width, crop_strategy)
+  if len(images.shape) == 3:
+    return _prepro_test(images)
+  else:
+    return tf.map_fn(_prepro_test, images)
+
+
+def preprocess_images(
+    images, is_training, height, width,
+    min_scale=1.0, max_scale=1.0, p_scale_up=0.0,
+    aug_color=True, fast_mode=True,
+    crop_strategy='pad_crop_central'):
+  """Preprocess a batch of images.
+
+  Args:
+    images: A 4-D float32 `Tensor` holding raw images to be preprocessed.
+    is_training: Boolean, whether to preprocess them for training or test.
+    height: Int, height in pixels to resize image to.
+    width: Int, width in pixels to resize image to.
+    min_scale: Float, minimum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    max_scale: Float, maximum scale augmentation allowed, as a fraction of the
+      central min_side * min_side area of the original image.
+    p_scale_up: Float, fraction of images scaled up.
+    aug_color: Whether or not to do color augmentation.
+    fast_mode: Boolean, avoids slower ops (random_hue and random_contrast).
+    crop_strategy: String, name of the strategy used to crop test-time images.
+      Can be: 'crop_center', 'pad', 'pad_200', 'pad_crop_central'.
+  Returns:
+    preprocessed_images: A 4-D float32 `Tensor` holding preprocessed images.
+  """
+  if is_training:
+    return preprocess_training_images(
+        images, height, width, min_scale, max_scale,
+        p_scale_up, aug_color, fast_mode)
+  else:
+    return preprocess_test_images(
+        images, height, width, crop_strategy)
+
+
+def cv2rotateimage(image, angle):
+  """Efficient rotation if 90 degrees rotations, slow otherwise.
+
+  Not a tensorflow function, using cv2 and scipy on numpy arrays.
+
+  Args:
+    image: a numpy array with shape [height, width, channels].
+    angle: the rotation angle in degrees in the range [-180, 180].
+  Returns:
+    The rotated image.
+  """
+  # Limit angle to [-180, 180] degrees.
+  assert angle <= 180 and angle >= -180
+  if angle == 0:
+    return image
+  # Efficient rotations.
+  if angle == -90:
+    image = cv2.transpose(image)
+    image = cv2.flip(image, 0)
+  elif angle == 90:
+    image = cv2.transpose(image)
+    image = cv2.flip(image, 1)
+  elif angle == 180 or angle == -180:
+    image = cv2.flip(image, 0)
+    image = cv2.flip(image, 1)
+  else:  # Slow rotation.
+    image = ndimage.interpolation.rotate(image, 270)
+  return image
+
+
+def cv2resizeminedge(image, min_edge_size):
+  """Resize smallest edge of image to min_edge_size."""
+  assert min_edge_size >= 0
+  height, width = (image.shape[0], image.shape[1])
+  new_height, new_width = (0, 0)
+  if height > width:
+    new_width = min_edge_size
+    new_height = int(height * new_width / float(width))
+  else:
+    new_height = min_edge_size
+    new_width = int(width * new_height / float(height))
+  return cv2.resize(image, (new_width, new_height),
+                    interpolation=cv2.INTER_AREA)
+
+
+def shapestring(array):
+  """Returns a compact string describing shape of an array."""
+  shape = array.shape
+  s = str(shape[0])
+  for i in range(1, len(shape)):
+    s += 'x' + str(shape[i])
+  return s
+
+
+def unscale_jpeg_encode(ims):
+  """Unscales pixel values and jpeg encodes preprocessed image.
+
+  Args:
+    ims: A 4-D float32 `Tensor` holding preprocessed images.
+  Returns:
+    im_strings: A 1-D string `Tensor` holding images that have been unscaled
+      (reversing the inception [-1,1] scaling), and jpeg encoded.
+  """
+  ims /= 2.0
+  ims += 0.5
+  ims *= 255.0
+  ims = tf.clip_by_value(ims, 0, 255)
+  ims = tf.cast(ims, tf.uint8)
+  im_strings = tf.map_fn(
+      lambda x: tf.image.encode_jpeg(x, format='rgb', quality=100),
+      ims, dtype=tf.string)
+  return im_strings
diff --git a/research/tcn/train.py b/research/tcn/train.py
new file mode 100644
index 00000000..f35cb4c6
--- /dev/null
+++ b/research/tcn/train.py
@@ -0,0 +1,61 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Trains TCN models (and baseline comparisons)."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from estimators.get_estimator import get_estimator
+from utils import util
+import tensorflow as tf
+tf.logging.set_verbosity(tf.logging.INFO)
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string('master', 'local',
+                           'BNS name of the TensorFlow master to use')
+tf.app.flags.DEFINE_string(
+    'logdir', '/tmp/tcn', 'Directory where to write event logs.')
+tf.app.flags.DEFINE_integer(
+    'task', 0, 'Task id of the replica running the training.')
+tf.app.flags.DEFINE_integer(
+    'ps_tasks', 0, 'Number of tasks in the ps job. If 0 no ps job is used.')
+FLAGS = tf.app.flags.FLAGS
+
+
+def main(_):
+  """Runs main training loop."""
+  # Parse config dict from yaml config files / command line flags.
+  config = util.ParseConfigsToLuaTable(
+      FLAGS.config_paths, FLAGS.model_params, save=True, logdir=FLAGS.logdir)
+
+  # Choose an estimator based on training strategy.
+  estimator = get_estimator(config, FLAGS.logdir)
+
+  # Run training
+  estimator.train()
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/tcn/utils/luatables.py b/research/tcn/utils/luatables.py
new file mode 100644
index 00000000..565d0386
--- /dev/null
+++ b/research/tcn/utils/luatables.py
@@ -0,0 +1,80 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+# pylint: disable=line-too-long,g-explicit-length-test
+"""A convenience class replicating some lua table syntax with a python dict.
+
+In general, should behave like a dictionary except that we can use dot notation
+ to access keys. Users should be careful to only provide keys suitable for
+ instance variable names.
+
+Nota bene: do not use the key "keys" since it will collide with the method keys.
+
+Usage example:
+
+>>> t = T(a=5,b='kaw', c=T(v=[],x=33))
+>>> t.a
+5
+>>> t.z = None
+>>> print t
+T(a=5, z=None, c=T(x=33, v=[]), b='kaw')
+
+>>> t2 = T({'h':'f','x':4})
+>>> t2
+T(h='f', x=4)
+>>> t2['x']
+4
+"""
+
+
+class T(object):
+  """Class for emulating lua tables."""
+
+  def __init__(self, *args, **kwargs):
+    if len(args) > 1 or (len(args) == 1 and len(kwargs) > 0):
+      errmsg = '''constructor only allows a single dict as a positional
+      argument or keyword arguments'''
+      raise ValueError(errmsg)
+    if len(args) == 1 and isinstance(args[0], dict):
+      self.__dict__.update(args[0])
+    else:
+      self.__dict__.update(kwargs)
+
+  def __repr__(self):
+    fmt = ', '.join('%s=%s' for i in range(len(self.__dict__)))
+    kwargstr = fmt % tuple(
+        x for tup in self.__dict__.items() for x in [str(tup[0]), repr(tup[1])])
+    return 'T(' + kwargstr + ')'
+
+  def __getitem__(self, key):
+    return self.__dict__[key]
+
+  def __setitem__(self, key, val):
+    self.__dict__[key] = val
+
+  def __delitem__(self, key):
+    del self.__dict__[key]
+
+  def __iter__(self):
+    return iter(self.__dict__)
+
+  def __len__(self):
+    return len(self.__dict__)
+
+  def keys(self):  # Needed for dict(T( ... )) to work.
+    return self.__dict__.keys()
+
+  def iteritems(self):
+    return self.__dict__.iteritems()
diff --git a/research/tcn/utils/progress.py b/research/tcn/utils/progress.py
new file mode 100644
index 00000000..1043261b
--- /dev/null
+++ b/research/tcn/utils/progress.py
@@ -0,0 +1,50 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A utility class for reporting processing progress."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import datetime
+
+
+class Progress(object):
+  """A utility class for reporting processing progress."""
+
+  def __init__(self, target_size):
+    self.target_size = target_size
+    self.current_size = 0
+    self.start_time = datetime.datetime.now()
+
+  def Update(self, current_size):
+    """Replaces internal current_size with current_size."""
+    self.current_size = current_size
+
+  def Add(self, size):
+    """Increments internal current_size by size."""
+    self.current_size += size
+
+  def __str__(self):
+    processed = 1e-5 + self.current_size / float(self.target_size)
+    current_time = datetime.datetime.now()
+    elapsed = current_time - self.start_time
+    eta = datetime.timedelta(
+        seconds=elapsed.total_seconds() / processed - elapsed.total_seconds())
+    return "%d / %d (elapsed %s eta %s)" % (
+        self.current_size, self.target_size,
+        str(elapsed).split(".")[0],
+        str(eta).split(".")[0])
diff --git a/research/tcn/utils/util.py b/research/tcn/utils/util.py
new file mode 100644
index 00000000..9f50366e
--- /dev/null
+++ b/research/tcn/utils/util.py
@@ -0,0 +1,247 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""General utility functions."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import numpy as np
+import six
+from utils.luatables import T
+import tensorflow as tf
+import yaml
+from yaml.constructor import ConstructorError
+# pylint: disable=invalid-name
+
+
+def GetFilesRecursively(topdir):
+  """Gets all records recursively for some topdir.
+
+  Args:
+    topdir: String, path to top directory.
+  Returns:
+    allpaths: List of Strings, full paths to all leaf records.
+  Raises:
+    ValueError: If there are no files found for this directory.
+  """
+  assert topdir
+  topdir = os.path.expanduser(topdir)
+  allpaths = []
+  for path, _, leaffiles in tf.gfile.Walk(topdir):
+    if leaffiles:
+      allpaths.extend([os.path.join(path, i) for i in leaffiles])
+  if not allpaths:
+    raise ValueError('No files found for top directory %s' % topdir)
+  return allpaths
+
+
+def NoDuplicatesConstructor(loader, node, deep=False):
+  """Check for duplicate keys."""
+  mapping = {}
+  for key_node, value_node in node.value:
+    key = loader.construct_object(key_node, deep=deep)
+    value = loader.construct_object(value_node, deep=deep)
+    if key in mapping:
+      raise ConstructorError('while constructing a mapping', node.start_mark,
+                             'found duplicate key (%s)' % key,
+                             key_node.start_mark)
+    mapping[key] = value
+  return loader.construct_mapping(node, deep)
+
+
+def WriteConfigAsYaml(config, logdir, filename):
+  """Writes a config dict as yaml to logdir/experiment.yml."""
+  if not tf.gfile.Exists(logdir):
+    tf.gfile.MakeDirs(logdir)
+  config_filename = os.path.join(logdir, filename)
+  with tf.gfile.GFile(config_filename, 'w') as f:
+    f.write(yaml.dump(config))
+  tf.logging.info('wrote config to %s', config_filename)
+
+
+def LoadConfigDict(config_paths, model_params):
+  """Loads config dictionary from specified yaml files or command line yaml."""
+
+  # Ensure that no duplicate keys can be loaded (causing pain).
+  yaml.add_constructor(yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,
+                       NoDuplicatesConstructor)
+
+  # Handle either ',' or '#' separated config lists, since borg will only
+  # accept '#'.
+  sep = ',' if ',' in config_paths else '#'
+
+  # Load flags from config file.
+  final_config = {}
+  if config_paths:
+    for config_path in config_paths.split(sep):
+      config_path = config_path.strip()
+      if not config_path:
+        continue
+      config_path = os.path.abspath(config_path)
+      tf.logging.info('Loading config from %s', config_path)
+      with tf.gfile.GFile(config_path.strip()) as config_file:
+        config_flags = yaml.load(config_file)
+        final_config = DeepMergeDict(final_config, config_flags)
+  if model_params:
+    model_params = MaybeLoadYaml(model_params)
+    final_config = DeepMergeDict(final_config, model_params)
+  tf.logging.info('Final Config:\n%s', yaml.dump(final_config))
+  return final_config
+
+
+def MaybeLoadYaml(item):
+  """Parses item if it's a string. If it's a dictionary it's returned as-is."""
+  if isinstance(item, six.string_types):
+    return yaml.load(item)
+  elif isinstance(item, dict):
+    return item
+  else:
+    raise ValueError('Got {}, expected YAML string or dict', type(item))
+
+
+def DeepMergeDict(dict_x, dict_y, path=None):
+  """Recursively merges dict_y into dict_x."""
+  if path is None: path = []
+  for key in dict_y:
+    if key in dict_x:
+      if isinstance(dict_x[key], dict) and isinstance(dict_y[key], dict):
+        DeepMergeDict(dict_x[key], dict_y[key], path + [str(key)])
+      elif dict_x[key] == dict_y[key]:
+        pass  # same leaf value
+      else:
+        dict_x[key] = dict_y[key]
+    else:
+      dict_x[key] = dict_y[key]
+  return dict_x
+
+
+def ParseConfigsToLuaTable(config_paths, extra_model_params=None,
+                           save=False, save_name='final_training_config.yml',
+                           logdir=None):
+  """Maps config_paths and extra_model_params to a Luatable-like object."""
+  # Parse config dict from yaml config files / command line flags.
+  config = LoadConfigDict(config_paths, extra_model_params)
+  if save:
+    WriteConfigAsYaml(config, logdir, save_name)
+  # Convert config dictionary to T object with dot notation.
+  config = RecursivelyConvertToLuatable(config)
+  return config
+
+
+def SetNestedValue(d, keys, value):
+  """Sets a value in a nested dictionary.
+
+  Example:
+    d = {}, keys = ['data','augmentation','minscale'], value = 1.0.
+    returns {'data': {'augmentation' : {'minscale': 1.0 }}}
+
+  Args:
+    d: A dictionary to set a nested value in.
+    keys: list of dict keys nesting left to right.
+    value: the nested value to set.
+  Returns:
+    None
+  """
+  for key in keys[:-1]:
+    d = d.setdefault(key, {})
+  d[keys[-1]] = value
+
+
+def RecursivelyConvertToLuatable(yaml_dict):
+  """Converts a dictionary to a LuaTable-like T object."""
+  if isinstance(yaml_dict, dict):
+    yaml_dict = T(yaml_dict)
+  for key, item in yaml_dict.iteritems():
+    if isinstance(item, dict):
+      yaml_dict[key] = RecursivelyConvertToLuatable(item)
+  return yaml_dict
+
+
+def KNNIds(query_vec, target_seq, k=1):
+  """Gets the knn ids to the query vec from the target sequence."""
+  sorted_distances = KNNIdsWithDistances(query_vec, target_seq, k)
+  return [i[0] for i in sorted_distances]
+
+
+def KNNIdsWithDistances(query_vec, target_seq, k=1):
+  """Gets the knn ids to the query vec from the target sequence."""
+  if not isinstance(np.array(target_seq), np.ndarray):
+    target_seq = np.array(target_seq)
+  assert np.shape(query_vec) == np.shape(target_seq[0])
+  distances = [(i, np.linalg.norm(query_vec-target_vec)) for (
+      i, target_vec) in enumerate(target_seq)]
+  sorted_distances = sorted(distances, key=lambda x: x[1])
+  return sorted_distances[:k]
+
+
+def CopyLocalConfigsToCNS(outdir, configs, gfs_user):
+  """Copies experiment yaml config files to the job_logdir on /cns."""
+  assert configs
+  assert outdir
+  conf_files = configs.split(',')
+  for conf_file in conf_files:
+    copy_command = 'fileutil --gfs_user %s cp -f %s %s' % (
+        gfs_user, conf_file, outdir)
+    tf.logging.info(copy_command)
+    os.system(copy_command)
+
+
+def pairwise_distances(feature, squared=True):
+  """Computes the pairwise distance matrix in numpy.
+
+  Args:
+    feature: 2-D numpy array of size [number of data, feature dimension]
+    squared: Boolean. If true, output is the pairwise squared euclidean
+      distance matrix; else, output is the pairwise euclidean distance matrix.
+
+  Returns:
+    pdists: 2-D numpy array of size
+      [number of data, number of data].
+  """
+  triu = np.triu_indices(feature.shape[0], 1)
+  upper_tri_pdists = np.linalg.norm(feature[triu[1]] - feature[triu[0]], axis=1)
+  if squared:
+    upper_tri_pdists **= 2.
+  num_data = feature.shape[0]
+  pdists = np.zeros((num_data, num_data))
+  pdists[np.triu_indices(num_data, 1)] = upper_tri_pdists
+  # Make symmetrical.
+  pdists = pdists + pdists.T - np.diag(
+      pdists.diagonal())
+  return pdists
+
+
+def is_tfrecord_input(inp):
+  """Checks if input is a TFRecord or list of TFRecords."""
+  def _is_tfrecord(inp):
+    if not isinstance(inp, str):
+      return False
+    _, extension = os.path.splitext(inp)
+    return extension == '.tfrecord'
+  if isinstance(inp, str):
+    return _is_tfrecord(inp)
+  if isinstance(inp, list):
+    return all(map(_is_tfrecord, inp))
+  return False
+
+
+def is_np_array(inp):
+  if isinstance(inp, np.ndarray):
+    return True
+  if isinstance(inp, list):
+    return all([isinstance(i, np.ndarray) for i in inp])
+  return False
diff --git a/research/tcn/visualize_embeddings.py b/research/tcn/visualize_embeddings.py
new file mode 100644
index 00000000..298c1ab1
--- /dev/null
+++ b/research/tcn/visualize_embeddings.py
@@ -0,0 +1,198 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Visualizes embeddings in tensorboard.
+
+Usage:
+root=experimental/users/sermanet/imitation/mirror && \
+blaze build -c opt --copt=-mavx --config=cuda $root:visualize_embeddings && \
+blaze-bin/$root/visualize_embeddings \
+--checkpointdir $checkpointdir \
+--checkpoint_iter $checkpoint_iter \
+--embedding_records $embedding_records \
+--outdir $outdir \
+--num_embed 1000 \
+--sprite_dim 64 \
+--config_paths $configs \
+--logtostderr
+
+blaze build third_party/tensorboard && \
+blaze-bin/third_party/tensorboard/tensorboard --logdir=$outdir
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import random
+import cv2
+import numpy as np
+from scipy.misc import imresize
+from scipy.misc import imsave
+from estimators.get_estimator import get_estimator
+from utils import util
+import tensorflow as tf
+from tensorflow.contrib.tensorboard.plugins import projector
+tf.logging.set_verbosity(tf.logging.INFO)
+
+tf.flags.DEFINE_string(
+    'config_paths', '',
+    """
+    Path to a YAML configuration files defining FLAG values. Multiple files
+    can be separated by the `#` symbol. Files are merged recursively. Setting
+    a key in these files is equivalent to setting the FLAG value with
+    the same name.
+    """)
+tf.flags.DEFINE_string(
+    'model_params', '{}', 'YAML configuration string for the model parameters.')
+tf.app.flags.DEFINE_string(
+    'checkpoint_iter', '', 'Evaluate this specific checkpoint.')
+tf.app.flags.DEFINE_string(
+    'checkpointdir', '/tmp/tcn', 'Path to model checkpoints.')
+tf.app.flags.DEFINE_string(
+    'outdir', '/tmp/tcn', 'Path to write tensorboard info to.')
+tf.app.flags.DEFINE_integer(
+    'num_embed', 4000, 'Number of embeddings.')
+tf.app.flags.DEFINE_integer(
+    'num_sequences', -1, 'Number of sequences, -1 for all.')
+tf.app.flags.DEFINE_integer(
+    'sprite_dim', 64, 'Height, width of the square sprite image.')
+tf.app.flags.DEFINE_string(
+    'embedding_records', None, 'path to embedding records')
+FLAGS = tf.app.flags.FLAGS
+
+
+def images_to_sprite(data):
+  """Creates the sprite image along with any necessary padding.
+
+  Taken from: https://github.com/tensorflow/tensorflow/issues/6322
+
+  Args:
+    data: NxHxW[x3] tensor containing the images.
+
+  Returns:
+    data: Properly shaped HxWx3 image with any necessary padding.
+  """
+  if len(data.shape) == 3:
+    data = np.tile(data[..., np.newaxis], (1, 1, 1, 3))
+  data = data.astype(np.float32)
+  min_v = np.min(data.reshape((data.shape[0], -1)), axis=1)
+  data = (data.transpose(1, 2, 3, 0) - min_v).transpose(3, 0, 1, 2)
+  max_v = np.max(data.reshape((data.shape[0], -1)), axis=1)
+  data = (data.transpose(1, 2, 3, 0) / max_v).transpose(3, 0, 1, 2)
+  n = int(np.ceil(np.sqrt(data.shape[0])))
+  padding = ((0, n ** 2 - data.shape[0]), (0, 0),
+             (0, 0)) + ((0, 0),) * (data.ndim - 3)
+  data = np.pad(data, padding, mode='constant',
+                constant_values=0)
+  # Tile the individual thumbnails into an image.
+  data = data.reshape((n, n) + data.shape[1:]).transpose(
+      (0, 2, 1, 3) + tuple(range(4, data.ndim + 1)))
+  data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])
+  data = (data * 255).astype(np.uint8)
+  return data
+
+
+def main(_):
+  """Runs main labeled eval loop."""
+  # Parse config dict from yaml config files / command line flags.
+  config = util.ParseConfigsToLuaTable(FLAGS.config_paths, FLAGS.model_params)
+
+  # Choose an estimator based on training strategy.
+  checkpointdir = FLAGS.checkpointdir
+  checkpoint_path = os.path.join(
+      '%s/model.ckpt-%s' % (checkpointdir, FLAGS.checkpoint_iter))
+  estimator = get_estimator(config, checkpointdir)
+
+  # Get records to embed.
+  validation_dir = FLAGS.embedding_records
+  validation_records = util.GetFilesRecursively(validation_dir)
+
+  sequences_to_data = {}
+  for (view_embeddings, view_raw_image_strings, seqname) in estimator.inference(
+      validation_records, checkpoint_path, config.data.embed_batch_size,
+      num_sequences=FLAGS.num_sequences):
+    sequences_to_data[seqname] = {
+        'embeddings': view_embeddings,
+        'images': view_raw_image_strings,
+    }
+
+  all_embeddings = np.zeros((0, config.embedding_size))
+  all_ims = []
+  all_seqnames = []
+
+  num_embeddings = FLAGS.num_embed
+  # Concatenate all views from all sequences into a big flat list.
+  for seqname, data in sequences_to_data.iteritems():
+    embs = data['embeddings']
+    ims = data['images']
+    for v in range(config.data.num_views):
+      for (emb, im) in zip(embs[v], ims[v]):
+        all_embeddings = np.append(all_embeddings, [emb], axis=0)
+        all_ims.append(im)
+        all_seqnames.append(seqname)
+
+  # Choose N indices uniformly from all images.
+  random_indices = range(all_embeddings.shape[0])
+  random.shuffle(random_indices)
+  viz_indices = random_indices[:num_embeddings]
+
+  # Extract embs.
+  viz_embs = np.array(all_embeddings[viz_indices])
+
+  # Extract and decode ims.
+  viz_ims = list(np.array(all_ims)[viz_indices])
+  decoded_ims = []
+
+  sprite_dim = FLAGS.sprite_dim
+  for i, im in enumerate(viz_ims):
+    if i % 100 == 0:
+      print('Decoding image %d/%d.' % (i, num_embeddings))
+    nparr_i = np.fromstring(str(im), np.uint8)
+    img_np = cv2.imdecode(nparr_i, 1)
+    img_np = img_np[..., [2, 1, 0]]
+
+    img_np = imresize(img_np, [sprite_dim, sprite_dim, 3])
+    decoded_ims.append(img_np)
+  decoded_ims = np.array(decoded_ims)
+
+  # Extract sequence names.
+  outdir = FLAGS.outdir
+
+  # The embedding variable, which needs to be stored
+  # Note this must a Variable not a Tensor!
+  embedding_var = tf.Variable(viz_embs, name='viz_embs')
+
+  with tf.Session() as sess:
+    sess.run(embedding_var.initializer)
+    summary_writer = tf.summary.FileWriter(outdir)
+    config = projector.ProjectorConfig()
+    embedding = config.embeddings.add()
+    embedding.tensor_name = embedding_var.name
+
+    # Comment out if you don't want sprites
+    embedding.sprite.image_path = os.path.join(outdir, 'sprite.png')
+    embedding.sprite.single_image_dim.extend(
+        [decoded_ims.shape[1], decoded_ims.shape[1]])
+
+    projector.visualize_embeddings(summary_writer, config)
+    saver = tf.train.Saver([embedding_var])
+    saver.save(sess, os.path.join(outdir, 'model2.ckpt'), 1)
+
+  sprite = images_to_sprite(decoded_ims)
+  imsave(os.path.join(outdir, 'sprite.png'), sprite)
+
+if __name__ == '__main__':
+  tf.app.run(main)
