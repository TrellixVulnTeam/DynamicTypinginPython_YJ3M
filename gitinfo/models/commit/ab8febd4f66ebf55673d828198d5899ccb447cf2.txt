commit ab8febd4f66ebf55673d828198d5899ccb447cf2
Author: Hongkun Yu <saberkun@users.noreply.github.com>
Date:   Tue Jul 23 18:06:10 2019 -0700

    Merged commit includes the following changes: (#7289)
    
    259649972  by hongkuny<hongkuny@google.com>:
    
        Update docs.
    
    --
    259470074  by hongkuny<hongkuny@google.com>:
    
        Adds a dedup phase for trainable variables.
    
    --
    
    PiperOrigin-RevId: 259649972

diff --git a/official/bert/README.md b/official/bert/README.md
index c563415b..3886de0c 100644
--- a/official/bert/README.md
+++ b/official/bert/README.md
@@ -1,4 +1,4 @@
-# BERT in TensorFlow
+# BERT (Bidirectional Encoder Representations from Transformers)
 
 Note> Please do not create pull request. This model is still under development
 and testing.
diff --git a/official/bert/model_training_utils.py b/official/bert/model_training_utils.py
index 69b9358d..78c867ac 100644
--- a/official/bert/model_training_utils.py
+++ b/official/bert/model_training_utils.py
@@ -242,7 +242,8 @@ def run_customized_training_loop(
           model_outputs = model(inputs)
           loss = loss_fn(labels, model_outputs)
 
-        tvars = model.trainable_variables
+        # De-dupes variables due to keras tracking issues.
+        tvars = list(set(model.trainable_variables))
         grads = tape.gradient(loss, tvars)
         optimizer.apply_gradients(zip(grads, tvars))
         # For reporting, the metric takes the mean of losses.
