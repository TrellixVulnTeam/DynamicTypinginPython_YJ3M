commit d4eedbb9a7ceccfbcf08ec843abcc38d7f7dc4d7
Author: Mark Sandler <sandler@google.com>
Date:   Tue Jan 21 18:08:13 2020 -0800

    Merged commit includes the following changes: (#8077)
    
    Internal cleanup (py2->py3) plus the following changes:
    
    285513318  by Sergio Guadarrama:
    
        Adds a script for post-training quantization
    
    284222305  by Sergio Guadarrama:
    
        Modified squeeze-excite operation to accommodate tensors of undefined (Nonetype) H/W.
    
    282028343  by Sergio Guadarrama:
    
        Add MobilenetV3 and MobilenetEdgeTPU to the slim/nets_factory.
    
    PiperOrigin-RevId: 289455329
    
    Co-authored-by: Sergio Guadarrama <sguada@gmail.com>

diff --git a/research/slim/BUILD b/research/slim/BUILD
index ee778bc9..1ba7522a 100644
--- a/research/slim/BUILD
+++ b/research/slim/BUILD
@@ -1,6 +1,7 @@
 # Description:
 #   Contains files for loading, training and evaluating TF-Slim-based models.
 # load("//devtools/python/blaze:python3.bzl", "py2and3_test")
+load("//devtools/python/blaze:pytype.bzl", "pytype_strict_binary")
 
 package(
     default_visibility = ["//visibility:public"],
@@ -475,11 +476,10 @@ py_test(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "inception_v2_test",
     size = "large",
     srcs = ["nets/inception_v2_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
@@ -590,14 +590,14 @@ py_library(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "mobilenet_v2_test",
     srcs = ["nets/mobilenet/mobilenet_v2_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet",
         ":mobilenet_common",
+        "//third_party/py/six",
         # "//tensorflow",
         # "//tensorflow/contrib/slim",
     ],
@@ -755,11 +755,10 @@ py_library(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "overfeat_test",
     size = "medium",
     srcs = ["nets/overfeat_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":overfeat",
@@ -890,11 +889,10 @@ py_library(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "vgg_test",
     size = "medium",
     srcs = ["nets/vgg_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":vgg",
@@ -912,11 +910,10 @@ py_library(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "nets_factory_test",
     size = "large",
     srcs = ["nets/nets_factory_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
@@ -925,9 +922,24 @@ py_test(
     ],
 )
 
+pytype_strict_binary(
+    name = "post_training_quantization",
+    srcs = ["nets/post_training_quantization.py"],
+    python_version = "PY3",
+    deps = [
+        ":nets_factory",
+        ":preprocessing_factory",
+        "//third_party/py/absl:app",
+        "//third_party/py/absl/flags",
+        # "//tensorflow",
+        # "//tensorflow_datasets",
+    ],
+)
+
 py_library(
     name = "train_image_classifier_lib",
     srcs = ["train_image_classifier.py"],
+    srcs_version = "PY2AND3",
     deps = [
         ":dataset_factory",
         ":model_deploy",
diff --git a/research/slim/datasets/download_and_convert_visualwakewords_lib.py b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
index d5ebea7f..4c3d2004 100644
--- a/research/slim/datasets/download_and_convert_visualwakewords_lib.py
+++ b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
@@ -201,7 +201,7 @@ def create_tf_record_for_visualwakewords_dataset(annotations_file, image_dir,
     groundtruth_data = json.load(fid)
     images = groundtruth_data['images']
     annotations_index = groundtruth_data['annotations']
-    annotations_index = {int(k): v for k, v in annotations_index.items()}
+    annotations_index = {int(k): v for k, v in annotations_index.iteritems()}
     # convert 'unicode' key to 'int' key after we parse the json file
 
     for idx, image in enumerate(images):
diff --git a/research/slim/nets/alexnet.py b/research/slim/nets/alexnet.py
index a2ea6ee6..9733d795 100644
--- a/research/slim/nets/alexnet.py
+++ b/research/slim/nets/alexnet.py
@@ -40,13 +40,16 @@ import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 
 
 def alexnet_v2_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
-                      biases_initializer=tf.constant_initializer(0.1),
+                      biases_initializer=tf.compat.v1.constant_initializer(0.1),
                       weights_regularizer=slim.l2_regularizer(weight_decay)):
     with slim.arg_scope([slim.conv2d], padding='SAME'):
       with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
@@ -94,7 +97,7 @@ def alexnet_v2(inputs,
       or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
+  with tf.compat.v1.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -110,9 +113,10 @@ def alexnet_v2(inputs,
       net = slim.max_pool2d(net, [3, 3], 2, scope='pool5')
 
       # Use conv2d instead of fully_connected layers.
-      with slim.arg_scope([slim.conv2d],
-                          weights_initializer=trunc_normal(0.005),
-                          biases_initializer=tf.constant_initializer(0.1)):
+      with slim.arg_scope(
+          [slim.conv2d],
+          weights_initializer=trunc_normal(0.005),
+          biases_initializer=tf.compat.v1.constant_initializer(0.1)):
         net = slim.conv2d(net, 4096, [5, 5], padding='VALID',
                           scope='fc6')
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
@@ -122,16 +126,19 @@ def alexnet_v2(inputs,
         end_points = slim.utils.convert_collection_to_dict(
             end_points_collection)
         if global_pool:
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
           end_points['global_pool'] = net
         if num_classes:
           net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                              scope='dropout7')
-          net = slim.conv2d(net, num_classes, [1, 1],
-                            activation_fn=None,
-                            normalizer_fn=None,
-                            biases_initializer=tf.zeros_initializer(),
-                            scope='fc8')
+          net = slim.conv2d(
+              net,
+              num_classes, [1, 1],
+              activation_fn=None,
+              normalizer_fn=None,
+              biases_initializer=tf.compat.v1.zeros_initializer(),
+              scope='fc8')
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
           end_points[sc.name + '/fc8'] = net
diff --git a/research/slim/nets/alexnet_test.py b/research/slim/nets/alexnet_test.py
index 15f003a2..b6fcdf4e 100644
--- a/research/slim/nets/alexnet_test.py
+++ b/research/slim/nets/alexnet_test.py
@@ -32,7 +32,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(inputs, num_classes)
       self.assertEquals(logits.op.name, 'alexnet_v2/fc8/squeezed')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -43,7 +43,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 300, 400
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False)
       self.assertEquals(logits.op.name, 'alexnet_v2/fc8/BiasAdd')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -54,7 +54,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(inputs, num_classes, spatial_squeeze=False,
                                      global_pool=True)
       self.assertEquals(logits.op.name, 'alexnet_v2/fc8/BiasAdd')
@@ -66,7 +66,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = alexnet.alexnet_v2(inputs, num_classes)
       expected_names = ['alexnet_v2/conv1',
                         'alexnet_v2/pool1',
@@ -87,7 +87,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, end_points = alexnet.alexnet_v2(inputs, num_classes)
       expected_names = ['alexnet_v2/conv1',
                         'alexnet_v2/pool1',
@@ -110,7 +110,7 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       alexnet.alexnet_v2(inputs, num_classes)
       expected_names = ['alexnet_v2/conv1/weights',
                         'alexnet_v2/conv1/biases',
@@ -137,11 +137,11 @@ class AlexnetV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
-      predictions = tf.argmax(logits, 1)
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertListEqual(predictions.get_shape().as_list(), [batch_size])
 
   def testTrainEvalWithReuse(self):
@@ -151,29 +151,29 @@ class AlexnetV2Test(tf.test.TestCase):
     eval_height, eval_width = 300, 400
     num_classes = 1000
     with self.test_session():
-      train_inputs = tf.random_uniform(
+      train_inputs = tf.random.uniform(
           (train_batch_size, train_height, train_width, 3))
       logits, _ = alexnet.alexnet_v2(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.get_variable_scope().reuse_variables()
-      eval_inputs = tf.random_uniform(
+      tf.compat.v1.get_variable_scope().reuse_variables()
+      eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,
                                      spatial_squeeze=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [eval_batch_size, 4, 7, num_classes])
-      logits = tf.reduce_mean(logits, [1, 2])
-      predictions = tf.argmax(logits, 1)
+      logits = tf.reduce_mean(input_tensor=logits, axis=[1, 2])
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])
 
   def testForward(self):
     batch_size = 1
     height, width = 224, 224
     with self.test_session() as sess:
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(inputs)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/nets/cifarnet.py b/research/slim/nets/cifarnet.py
index 2e151c88..1dae82cd 100644
--- a/research/slim/nets/cifarnet.py
+++ b/research/slim/nets/cifarnet.py
@@ -23,7 +23,9 @@ from tensorflow.contrib import slim as contrib_slim
 
 slim = contrib_slim
 
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(stddev=stddev)
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    stddev=stddev)
 
 
 def cifarnet(images, num_classes=10, is_training=False,
@@ -61,7 +63,7 @@ def cifarnet(images, num_classes=10, is_training=False,
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'CifarNet', [images]):
+  with tf.compat.v1.variable_scope(scope, 'CifarNet', [images]):
     net = slim.conv2d(images, 64, [5, 5], scope='conv1')
     end_points['conv1'] = net
     net = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
@@ -82,12 +84,14 @@ def cifarnet(images, num_classes=10, is_training=False,
     end_points['fc4'] = net
     if not num_classes:
       return net, end_points
-    logits = slim.fully_connected(net, num_classes,
-                                  biases_initializer=tf.zeros_initializer(),
-                                  weights_initializer=trunc_normal(1/192.0),
-                                  weights_regularizer=None,
-                                  activation_fn=None,
-                                  scope='logits')
+    logits = slim.fully_connected(
+        net,
+        num_classes,
+        biases_initializer=tf.compat.v1.zeros_initializer(),
+        weights_initializer=trunc_normal(1 / 192.0),
+        weights_regularizer=None,
+        activation_fn=None,
+        scope='logits')
 
     end_points['Logits'] = logits
     end_points['Predictions'] = prediction_fn(logits, scope='Predictions')
@@ -107,11 +111,12 @@ def cifarnet_arg_scope(weight_decay=0.004):
   """
   with slim.arg_scope(
       [slim.conv2d],
-      weights_initializer=tf.truncated_normal_initializer(stddev=5e-2),
+      weights_initializer=tf.compat.v1.truncated_normal_initializer(
+          stddev=5e-2),
       activation_fn=tf.nn.relu):
     with slim.arg_scope(
         [slim.fully_connected],
-        biases_initializer=tf.constant_initializer(0.1),
+        biases_initializer=tf.compat.v1.constant_initializer(0.1),
         weights_initializer=trunc_normal(0.04),
         weights_regularizer=slim.l2_regularizer(weight_decay),
         activation_fn=tf.nn.relu) as sc:
diff --git a/research/slim/nets/cyclegan.py b/research/slim/nets/cyclegan.py
index dc87bd0b..7c642371 100644
--- a/research/slim/nets/cyclegan.py
+++ b/research/slim/nets/cyclegan.py
@@ -61,7 +61,8 @@ def cyclegan_arg_scope(instance_norm_center=True,
       [layers.conv2d],
       normalizer_fn=layers.instance_norm,
       normalizer_params=instance_norm_params,
-      weights_initializer=tf.random_normal_initializer(0, weights_init_stddev),
+      weights_initializer=tf.compat.v1.random_normal_initializer(
+          0, weights_init_stddev),
       weights_regularizer=weights_regularizer) as sc:
     return sc
 
@@ -90,8 +91,8 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
   Raises:
     ValueError: if `method` is not recognized.
   """
-  with tf.variable_scope('upconv'):
-    net_shape = tf.shape(net)
+  with tf.compat.v1.variable_scope('upconv'):
+    net_shape = tf.shape(input=net)
     height = net_shape[1]
     width = net_shape[2]
 
@@ -101,15 +102,16 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
     spatial_pad_1 = np.array([[0, 0], [1, 1], [1, 1], [0, 0]])
 
     if method == 'nn_upsample_conv':
-      net = tf.image.resize_nearest_neighbor(
-          net, [stride[0] * height, stride[1] * width])
-      net = tf.pad(net, spatial_pad_1, pad_mode)
+      net = tf.image.resize(
+          net, [stride[0] * height, stride[1] * width],
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
+      net = tf.pad(tensor=net, paddings=spatial_pad_1, mode=pad_mode)
       net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
     elif method == 'bilinear_upsample_conv':
-      net = tf.image.resize_bilinear(
+      net = tf.compat.v1.image.resize_bilinear(
           net, [stride[0] * height, stride[1] * width],
           align_corners=align_corners)
-      net = tf.pad(net, spatial_pad_1, pad_mode)
+      net = tf.pad(tensor=net, paddings=spatial_pad_1, mode=pad_mode)
       net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
     elif method == 'conv2d_transpose':
       # This corrects 1 pixel offset for images with even width and height.
@@ -126,7 +128,7 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
 
 
 def _dynamic_or_static_shape(tensor):
-  shape = tf.shape(tensor)
+  shape = tf.shape(input=tensor)
   static_shape = contrib_util.constant_value(shape)
   return static_shape if static_shape is not None else shape
 
@@ -204,40 +206,40 @@ def cyclegan_generator_resnet(images,
     ###########
     # Encoder #
     ###########
-    with tf.variable_scope('input'):
+    with tf.compat.v1.variable_scope('input'):
       # 7x7 input stage
-      net = tf.pad(images, spatial_pad_3, 'REFLECT')
+      net = tf.pad(tensor=images, paddings=spatial_pad_3, mode='REFLECT')
       net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')
       end_points['encoder_0'] = net
 
-    with tf.variable_scope('encoder'):
+    with tf.compat.v1.variable_scope('encoder'):
       with contrib_framework.arg_scope([layers.conv2d],
                                        kernel_size=kernel_size,
                                        stride=2,
                                        activation_fn=tf.nn.relu,
                                        padding='VALID'):
 
-        net = tf.pad(net, paddings, 'REFLECT')
+        net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
         net = layers.conv2d(net, num_filters * 2)
         end_points['encoder_1'] = net
-        net = tf.pad(net, paddings, 'REFLECT')
+        net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
         net = layers.conv2d(net, num_filters * 4)
         end_points['encoder_2'] = net
 
     ###################
     # Residual Blocks #
     ###################
-    with tf.variable_scope('residual_blocks'):
+    with tf.compat.v1.variable_scope('residual_blocks'):
       with contrib_framework.arg_scope([layers.conv2d],
                                        kernel_size=kernel_size,
                                        stride=1,
                                        activation_fn=tf.nn.relu,
                                        padding='VALID'):
         for block_id in xrange(num_resnet_blocks):
-          with tf.variable_scope('block_{}'.format(block_id)):
-            res_net = tf.pad(net, paddings, 'REFLECT')
+          with tf.compat.v1.variable_scope('block_{}'.format(block_id)):
+            res_net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
             res_net = layers.conv2d(res_net, num_filters * 4)
-            res_net = tf.pad(res_net, paddings, 'REFLECT')
+            res_net = tf.pad(tensor=res_net, paddings=paddings, mode='REFLECT')
             res_net = layers.conv2d(res_net, num_filters * 4,
                                     activation_fn=None)
             net += res_net
@@ -247,23 +249,23 @@ def cyclegan_generator_resnet(images,
     ###########
     # Decoder #
     ###########
-    with tf.variable_scope('decoder'):
+    with tf.compat.v1.variable_scope('decoder'):
 
       with contrib_framework.arg_scope([layers.conv2d],
                                        kernel_size=kernel_size,
                                        stride=1,
                                        activation_fn=tf.nn.relu):
 
-        with tf.variable_scope('decoder1'):
+        with tf.compat.v1.variable_scope('decoder1'):
           net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])
         end_points['decoder1'] = net
 
-        with tf.variable_scope('decoder2'):
+        with tf.compat.v1.variable_scope('decoder2'):
           net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])
         end_points['decoder2'] = net
 
-    with tf.variable_scope('output'):
-      net = tf.pad(net, spatial_pad_3, 'REFLECT')
+    with tf.compat.v1.variable_scope('output'):
+      net = tf.pad(tensor=net, paddings=spatial_pad_3, mode='REFLECT')
       logits = layers.conv2d(
           net,
           num_outputs, [7, 7],
diff --git a/research/slim/nets/cyclegan_test.py b/research/slim/nets/cyclegan_test.py
index 395773ea..96f0e248 100644
--- a/research/slim/nets/cyclegan_test.py
+++ b/research/slim/nets/cyclegan_test.py
@@ -31,7 +31,7 @@ class CycleganTest(tf.test.TestCase):
     img_batch = tf.zeros([2, 32, 32, 3])
     model_output, _ = cyclegan.cyclegan_generator_resnet(img_batch)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       sess.run(model_output)
 
   def _test_generator_graph_helper(self, shape):
@@ -50,13 +50,13 @@ class CycleganTest(tf.test.TestCase):
 
   def test_generator_unknown_batch_dim(self):
     """Check that generator can take unknown batch dimension inputs."""
-    img = tf.placeholder(tf.float32, shape=[None, 32, None, 3])
+    img = tf.compat.v1.placeholder(tf.float32, shape=[None, 32, None, 3])
     output_imgs, _ = cyclegan.cyclegan_generator_resnet(img)
 
     self.assertAllEqual([None, 32, None, 3], output_imgs.shape.as_list())
 
   def _input_and_output_same_shape_helper(self, kernel_size):
-    img_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])
+    img_batch = tf.compat.v1.placeholder(tf.float32, shape=[None, 32, 32, 3])
     output_img_batch, _ = cyclegan.cyclegan_generator_resnet(
         img_batch, kernel_size=kernel_size)
 
@@ -77,10 +77,9 @@ class CycleganTest(tf.test.TestCase):
 
   def _error_if_height_not_multiple_of_four_helper(self, height):
     self.assertRaisesRegexp(
-        ValueError,
-        'The input height must be a multiple of 4.',
+        ValueError, 'The input height must be a multiple of 4.',
         cyclegan.cyclegan_generator_resnet,
-        tf.placeholder(tf.float32, shape=[None, height, 32, 3]))
+        tf.compat.v1.placeholder(tf.float32, shape=[None, height, 32, 3]))
 
   def test_error_if_height_not_multiple_of_four_height29(self):
     self._error_if_height_not_multiple_of_four_helper(29)
@@ -93,10 +92,9 @@ class CycleganTest(tf.test.TestCase):
 
   def _error_if_width_not_multiple_of_four_helper(self, width):
     self.assertRaisesRegexp(
-        ValueError,
-        'The input width must be a multiple of 4.',
+        ValueError, 'The input width must be a multiple of 4.',
         cyclegan.cyclegan_generator_resnet,
-        tf.placeholder(tf.float32, shape=[None, 32, width, 3]))
+        tf.compat.v1.placeholder(tf.float32, shape=[None, 32, width, 3]))
 
   def test_error_if_width_not_multiple_of_four_width29(self):
     self._error_if_width_not_multiple_of_four_helper(29)
diff --git a/research/slim/nets/dcgan.py b/research/slim/nets/dcgan.py
index d1461f58..598b642a 100644
--- a/research/slim/nets/dcgan.py
+++ b/research/slim/nets/dcgan.py
@@ -82,7 +82,8 @@ def discriminator(inputs,
   inp_shape = inputs.get_shape().as_list()[1]
 
   end_points = {}
-  with tf.variable_scope(scope, values=[inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, values=[inputs], reuse=reuse) as scope:
     with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
       with slim.arg_scope([slim.conv2d],
                           stride=2,
@@ -156,7 +157,8 @@ def generator(inputs,
 
   end_points = {}
   num_layers = int(log(final_size, 2)) - 1
-  with tf.variable_scope(scope, values=[inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, values=[inputs], reuse=reuse) as scope:
     with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
       with slim.arg_scope([slim.conv2d_transpose],
                           normalizer_fn=normalizer_fn,
diff --git a/research/slim/nets/dcgan_test.py b/research/slim/nets/dcgan_test.py
index 343de628..53fd9fb7 100644
--- a/research/slim/nets/dcgan_test.py
+++ b/research/slim/nets/dcgan_test.py
@@ -27,21 +27,21 @@ from nets import dcgan
 class DCGANTest(tf.test.TestCase):
 
   def test_generator_run(self):
-    tf.set_random_seed(1234)
-    noise = tf.random_normal([100, 64])
+    tf.compat.v1.set_random_seed(1234)
+    noise = tf.random.normal([100, 64])
     image, _ = dcgan.generator(noise)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       image.eval()
 
   def test_generator_graph(self):
-    tf.set_random_seed(1234)
+    tf.compat.v1.set_random_seed(1234)
     # Check graph construction for a number of image size/depths and batch
     # sizes.
     for i, batch_size in zip(xrange(3, 7), xrange(3, 8)):
-      tf.reset_default_graph()
+      tf.compat.v1.reset_default_graph()
       final_size = 2 ** i
-      noise = tf.random_normal([batch_size, 64])
+      noise = tf.random.normal([batch_size, 64])
       image, end_points = dcgan.generator(
           noise,
           depth=32,
@@ -71,19 +71,19 @@ class DCGANTest(tf.test.TestCase):
       dcgan.generator(correct_input, final_size=4)
 
   def test_discriminator_run(self):
-    image = tf.random_uniform([5, 32, 32, 3], -1, 1)
+    image = tf.random.uniform([5, 32, 32, 3], -1, 1)
     output, _ = dcgan.discriminator(image)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output.eval()
 
   def test_discriminator_graph(self):
     # Check graph construction for a number of image size/depths and batch
     # sizes.
     for i, batch_size in zip(xrange(1, 6), xrange(3, 8)):
-      tf.reset_default_graph()
+      tf.compat.v1.reset_default_graph()
       img_w = 2 ** i
-      image = tf.random_uniform([batch_size, img_w, img_w, 3], -1, 1)
+      image = tf.random.uniform([batch_size, img_w, img_w, 3], -1, 1)
       output, end_points = dcgan.discriminator(
           image,
           depth=32)
@@ -103,7 +103,8 @@ class DCGANTest(tf.test.TestCase):
     with self.assertRaises(ValueError):
       dcgan.discriminator(wrong_dim_img)
 
-    spatially_undefined_shape = tf.placeholder(tf.float32, [5, 32, None, 3])
+    spatially_undefined_shape = tf.compat.v1.placeholder(
+        tf.float32, [5, 32, None, 3])
     with self.assertRaises(ValueError):
       dcgan.discriminator(spatially_undefined_shape)
 
diff --git a/research/slim/nets/i3d.py b/research/slim/nets/i3d.py
index 08a9fa43..28974ead 100644
--- a/research/slim/nets/i3d.py
+++ b/research/slim/nets/i3d.py
@@ -31,7 +31,10 @@ from nets import i3d_utils
 from nets import s3dg
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 conv3d_spatiotemporal = i3d_utils.conv3d_spatiotemporal
 
 
@@ -149,12 +152,12 @@ def i3d(inputs,
       activation.
   """
   # Final pooling and prediction
-  with tf.variable_scope(
+  with tf.compat.v1.variable_scope(
       scope, 'InceptionV1', [inputs, num_classes], reuse=reuse) as scope:
     with slim.arg_scope(
         [slim.batch_norm, slim.dropout], is_training=is_training):
       net, end_points = i3d_base(inputs, scope=scope)
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         kernel_size = i3d_utils.reduced_kernel_size_3d(net, [2, 7, 7])
         net = slim.avg_pool3d(
             net, kernel_size, stride=1, scope='AvgPool_0a_7x7')
@@ -166,7 +169,7 @@ def i3d(inputs,
             normalizer_fn=None,
             scope='Conv2d_0c_1x1')
         # Temporal average pooling.
-        logits = tf.reduce_mean(logits, axis=1)
+        logits = tf.reduce_mean(input_tensor=logits, axis=1)
         if spatial_squeeze:
           logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')
 
diff --git a/research/slim/nets/i3d_test.py b/research/slim/nets/i3d_test.py
index ec3bb56b..307233cb 100644
--- a/research/slim/nets/i3d_test.py
+++ b/research/slim/nets/i3d_test.py
@@ -31,7 +31,7 @@ class I3DTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     logits, end_points = i3d.i3d(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -45,7 +45,7 @@ class I3DTest(tf.test.TestCase):
     num_frames = 64
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_6c, end_points = i3d.i3d_base(inputs)
     self.assertTrue(mixed_6c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_6c.get_shape().as_list(),
@@ -68,7 +68,7 @@ class I3DTest(tf.test.TestCase):
                  'Mixed_5c']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+        inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
         out_tensor, end_points = i3d.i3d_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
@@ -80,7 +80,7 @@ class I3DTest(tf.test.TestCase):
     num_frames = 64
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     _, end_points = i3d.i3d_base(inputs,
                                  final_endpoint='Mixed_5c')
     endpoints_shapes = {'Conv2d_1a_7x7': [5, 32, 112, 112, 64],
@@ -111,7 +111,7 @@ class I3DTest(tf.test.TestCase):
     num_frames = 64
     height, width = 112, 112
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_5c, _ = i3d.i3d_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -122,7 +122,7 @@ class I3DTest(tf.test.TestCase):
     num_frames = 10
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_5c, _ = i3d.i3d_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -134,13 +134,13 @@ class I3DTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     logits, _ = i3d.i3d(eval_inputs, num_classes,
                         is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
diff --git a/research/slim/nets/i3d_utils.py b/research/slim/nets/i3d_utils.py
index c843bb50..05df3016 100644
--- a/research/slim/nets/i3d_utils.py
+++ b/research/slim/nets/i3d_utils.py
@@ -228,13 +228,13 @@ def inception_block_v1_3d(inputs,
   """
   use_gating = self_gating_fn is not None
 
-  with tf.variable_scope(scope):
-    with tf.variable_scope('Branch_0'):
+  with tf.compat.v1.variable_scope(scope):
+    with tf.compat.v1.variable_scope('Branch_0'):
       branch_0 = layers.conv3d(
           inputs, num_outputs_0_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       if use_gating:
         branch_0 = self_gating_fn(branch_0, scope='Conv2d_0a_1x1')
-    with tf.variable_scope('Branch_1'):
+    with tf.compat.v1.variable_scope('Branch_1'):
       branch_1 = layers.conv3d(
           inputs, num_outputs_1_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       branch_1 = conv3d_spatiotemporal(
@@ -242,7 +242,7 @@ def inception_block_v1_3d(inputs,
           scope='Conv2d_0b_3x3')
       if use_gating:
         branch_1 = self_gating_fn(branch_1, scope='Conv2d_0b_3x3')
-    with tf.variable_scope('Branch_2'):
+    with tf.compat.v1.variable_scope('Branch_2'):
       branch_2 = layers.conv3d(
           inputs, num_outputs_2_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       branch_2 = conv3d_spatiotemporal(
@@ -250,7 +250,7 @@ def inception_block_v1_3d(inputs,
           scope='Conv2d_0b_3x3')
       if use_gating:
         branch_2 = self_gating_fn(branch_2, scope='Conv2d_0b_3x3')
-    with tf.variable_scope('Branch_3'):
+    with tf.compat.v1.variable_scope('Branch_3'):
       branch_3 = layers.max_pool3d(inputs, [3, 3, 3], scope='MaxPool_0a_3x3')
       branch_3 = layers.conv3d(
           branch_3, num_outputs_3_0b, [1, 1, 1], scope='Conv2d_0b_1x1')
diff --git a/research/slim/nets/inception_resnet_v2.py b/research/slim/nets/inception_resnet_v2.py
index 33c19163..cff58953 100644
--- a/research/slim/nets/inception_resnet_v2.py
+++ b/research/slim/nets/inception_resnet_v2.py
@@ -33,13 +33,13 @@ slim = contrib_slim
 
 def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 35x35 resnet block."""
-  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):
-    with tf.variable_scope('Branch_0'):
+  with tf.compat.v1.variable_scope(scope, 'Block35', [net], reuse=reuse):
+    with tf.compat.v1.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')
-    with tf.variable_scope('Branch_1'):
+    with tf.compat.v1.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')
-    with tf.variable_scope('Branch_2'):
+    with tf.compat.v1.variable_scope('Branch_2'):
       tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')
       tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')
       tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')
@@ -59,10 +59,10 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
 def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 17x17 resnet block."""
-  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):
-    with tf.variable_scope('Branch_0'):
+  with tf.compat.v1.variable_scope(scope, 'Block17', [net], reuse=reuse):
+    with tf.compat.v1.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')
-    with tf.variable_scope('Branch_1'):
+    with tf.compat.v1.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],
                                   scope='Conv2d_0b_1x7')
@@ -85,10 +85,10 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
 def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 8x8 resnet block."""
-  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):
-    with tf.variable_scope('Branch_0'):
+  with tf.compat.v1.variable_scope(scope, 'Block8', [net], reuse=reuse):
+    with tf.compat.v1.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')
-    with tf.variable_scope('Branch_1'):
+    with tf.compat.v1.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],
                                   scope='Conv2d_0b_1x3')
@@ -155,7 +155,7 @@ def inception_resnet_v2_base(inputs,
     end_points[name] = net
     return name == final_endpoint
 
-  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionResnetV2', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='SAME'):
       # 149 x 149 x 32
@@ -188,20 +188,20 @@ def inception_resnet_v2_base(inputs,
       if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points
 
       # 35 x 35 x 320
-      with tf.variable_scope('Mixed_5b'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_5b'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,
                                       scope='Conv2d_0b_5x5')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')
           tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,
                                       scope='Conv2d_0b_3x3')
           tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,
                                       scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',
                                        scope='AvgPool_0a_3x3')
           tower_pool_1 = slim.conv2d(tower_pool, 64, 1,
@@ -218,12 +218,12 @@ def inception_resnet_v2_base(inputs,
       # 33 x 33 x 1088 if output_stride == 16
       use_atrous = output_stride == 8
 
-      with tf.variable_scope('Mixed_6a'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_6a'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,
                                    padding=padding,
                                    scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,
                                       scope='Conv2d_0b_3x3')
@@ -231,7 +231,7 @@ def inception_resnet_v2_base(inputs,
                                       stride=1 if use_atrous else 2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,
                                        padding=padding,
                                        scope='MaxPool_1a_3x3')
@@ -251,25 +251,25 @@ def inception_resnet_v2_base(inputs,
                          'PreAuxlogits end_point for now.')
 
       # 8 x 8 x 2080
-      with tf.variable_scope('Mixed_7a'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_7a'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,
                                      padding=padding,
                                      scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,
                                       scope='Conv2d_0b_3x3')
           tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           tower_pool = slim.max_pool2d(net, 3, stride=2,
                                        padding=padding,
                                        scope='MaxPool_1a_3x3')
@@ -320,8 +320,8 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs],
-                         reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'InceptionResnetV2', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
 
@@ -329,7 +329,7 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
                                                  activation_fn=activation_fn)
 
       if create_aux_logits and num_classes:
-        with tf.variable_scope('AuxLogits'):
+        with tf.compat.v1.variable_scope('AuxLogits'):
           aux = end_points['PreAuxLogits']
           aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',
                                 scope='Conv2d_1a_3x3')
@@ -341,7 +341,7 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
                                      scope='Logits')
           end_points['AuxLogits'] = aux
 
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
         # can be set to False to disable pooling here (as in resnet_*()).
         kernel_size = net.get_shape()[1:3]
@@ -349,7 +349,8 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
           net = slim.avg_pool2d(net, kernel_size, padding='VALID',
                                 scope='AvgPool_1a_8x8')
         else:
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
         end_points['global_pool'] = net
         if not num_classes:
           return net, end_points
@@ -371,7 +372,7 @@ def inception_resnet_v2_arg_scope(
     batch_norm_decay=0.9997,
     batch_norm_epsilon=0.001,
     activation_fn=tf.nn.relu,
-    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
+    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
     batch_norm_scale=False):
   """Returns the scope with the default parameters for inception_resnet_v2.
 
diff --git a/research/slim/nets/inception_resnet_v2_test.py b/research/slim/nets/inception_resnet_v2_test.py
index 7ee633b2..348c44d3 100644
--- a/research/slim/nets/inception_resnet_v2_test.py
+++ b/research/slim/nets/inception_resnet_v2_test.py
@@ -30,7 +30,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, endpoints = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue('AuxLogits' in endpoints)
       auxlogits = endpoints['AuxLogits']
@@ -47,7 +47,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, endpoints = inception.inception_resnet_v2(inputs, num_classes,
                                                         create_aux_logits=False)
       self.assertTrue('AuxLogits' not in endpoints)
@@ -60,7 +60,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, endpoints = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue('AuxLogits' not in endpoints)
       self.assertTrue('Logits' not in endpoints)
@@ -73,7 +73,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue('Logits' in end_points)
       logits = end_points['Logits']
@@ -91,7 +91,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_resnet_v2_base(inputs)
     self.assertTrue(net.op.name.startswith('InceptionResnetV2/Conv2d_7b_1x1'))
     self.assertListEqual(net.get_shape().as_list(),
@@ -111,7 +111,7 @@ class InceptionTest(tf.test.TestCase):
                  'PreAuxLogits', 'Mixed_7a', 'Conv2d_7b_1x1']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = inception.inception_resnet_v2_base(
             inputs, final_endpoint=endpoint)
         if endpoint != 'PreAuxLogits':
@@ -123,7 +123,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_resnet_v2_base(
         inputs, final_endpoint='PreAuxLogits')
     endpoints_shapes = {'Conv2d_1a_3x3': [5, 149, 149, 32],
@@ -149,7 +149,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_resnet_v2_base(
         inputs, final_endpoint='PreAuxLogits', align_feature_maps=True)
     endpoints_shapes = {'Conv2d_1a_3x3': [5, 150, 150, 32],
@@ -175,7 +175,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_resnet_v2_base(
         inputs, final_endpoint='PreAuxLogits', output_stride=8)
     endpoints_shapes = {'Conv2d_1a_3x3': [5, 149, 149, 32],
@@ -202,15 +202,17 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       # Force all Variables to reside on the device.
-      with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
+      with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
         inception.inception_resnet_v2(inputs, num_classes)
-      with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
+      with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
         inception.inception_resnet_v2(inputs, num_classes)
-      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+      for v in tf.compat.v1.get_collection(
+          tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
         self.assertDeviceEqual(v.device, '/cpu:0')
-      for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+      for v in tf.compat.v1.get_collection(
+          tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
         self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testHalfSizeImages(self):
@@ -218,7 +220,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, end_points = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
@@ -232,7 +234,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 330, 400
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, end_points = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
@@ -246,15 +248,15 @@ class InceptionTest(tf.test.TestCase):
     height, width = 330, 400
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(tf.float32, (batch_size, None, None, 3))
       logits, end_points = inception.inception_resnet_v2(
           inputs, num_classes, create_aux_logits=False)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_7b_1x1']
-      images = tf.random_uniform((batch_size, height, width, 3))
-      sess.run(tf.global_variables_initializer())
+      images = tf.random.uniform((batch_size, height, width, 3))
+      sess.run(tf.compat.v1.global_variables_initializer())
       logits_out, pre_pool_out = sess.run([logits, pre_pool],
                                           {inputs: images.eval()})
       self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
@@ -265,13 +267,13 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
       logits, _ = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
-      images = tf.random_uniform((batch_size, height, width, 3))
-      sess.run(tf.global_variables_initializer())
+      images = tf.random.uniform((batch_size, height, width, 3))
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -280,12 +282,12 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = inception.inception_resnet_v2(eval_inputs,
                                                 num_classes,
                                                 is_training=False)
-      predictions = tf.argmax(logits, 1)
-      sess.run(tf.global_variables_initializer())
+      predictions = tf.argmax(input=logits, axis=1)
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -295,39 +297,40 @@ class InceptionTest(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
     with self.test_session() as sess:
-      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+      train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
       inception.inception_resnet_v2(train_inputs, num_classes)
-      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
       logits, _ = inception.inception_resnet_v2(eval_inputs,
                                                 num_classes,
                                                 is_training=False,
                                                 reuse=True)
-      predictions = tf.argmax(logits, 1)
-      sess.run(tf.global_variables_initializer())
+      predictions = tf.argmax(input=logits, axis=1)
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with contrib_slim.arg_scope(inception.inception_resnet_v2_arg_scope()):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with contrib_slim.arg_scope(
         inception.inception_resnet_v2_arg_scope(batch_norm_scale=True)):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
     gamma_names = set(
-        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
+        v.op.name
+        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_utils.py b/research/slim/nets/inception_utils.py
index b9cedd38..493a684c 100644
--- a/research/slim/nets/inception_utils.py
+++ b/research/slim/nets/inception_utils.py
@@ -30,13 +30,14 @@ from tensorflow.contrib import slim as contrib_slim
 slim = contrib_slim
 
 
-def inception_arg_scope(weight_decay=0.00004,
-                        use_batch_norm=True,
-                        batch_norm_decay=0.9997,
-                        batch_norm_epsilon=0.001,
-                        activation_fn=tf.nn.relu,
-                        batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
-                        batch_norm_scale=False):
+def inception_arg_scope(
+    weight_decay=0.00004,
+    use_batch_norm=True,
+    batch_norm_decay=0.9997,
+    batch_norm_epsilon=0.001,
+    activation_fn=tf.nn.relu,
+    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
+    batch_norm_scale=False):
   """Defines the default arg scope for inception models.
 
   Args:
diff --git a/research/slim/nets/inception_v1.py b/research/slim/nets/inception_v1.py
index f17ebb1a..b84104af 100644
--- a/research/slim/nets/inception_v1.py
+++ b/research/slim/nets/inception_v1.py
@@ -24,7 +24,10 @@ from tensorflow.contrib import slim as contrib_slim
 from nets import inception_utils
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 
 
 def inception_v1_base(inputs,
@@ -59,7 +62,7 @@ def inception_v1_base(inputs,
     ValueError: if final_endpoint is not set to one of the predefined values.
   """
   end_points = {}
-  with tf.variable_scope(scope, 'InceptionV1', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionV1', [inputs]):
     with slim.arg_scope(
         [slim.conv2d, slim.fully_connected],
         weights_initializer=trunc_normal(0.01)):
@@ -94,16 +97,16 @@ def inception_v1_base(inputs,
             return net, end_points
 
         end_point = 'Mixed_3b'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 96, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 16, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -112,16 +115,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_3c'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -135,16 +138,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4b'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 96, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 16, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -153,16 +156,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4c'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 112, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 24, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -171,16 +174,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4d'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 24, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -189,16 +192,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4e'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 112, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 144, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -207,16 +210,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4f'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -230,16 +233,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_5b'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0a_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -248,16 +251,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_5c'
-        with tf.variable_scope(end_point):
-          with tf.variable_scope('Branch_0'):
+        with tf.compat.v1.variable_scope(end_point):
+          with tf.compat.v1.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.variable_scope('Branch_1'):
+          with tf.compat.v1.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_2'):
+          with tf.compat.v1.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.variable_scope('Branch_3'):
+          with tf.compat.v1.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -313,14 +316,16 @@ def inception_v1(inputs,
       activation.
   """
   # Final pooling and prediction
-  with tf.variable_scope(scope, 'InceptionV1', [inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'InceptionV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v1_base(inputs, scope=scope)
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
           end_points['global_pool'] = net
         else:
           # Pooling with a fixed kernel size.
diff --git a/research/slim/nets/inception_v1_test.py b/research/slim/nets/inception_v1_test.py
index 617265d7..ce0fca42 100644
--- a/research/slim/nets/inception_v1_test.py
+++ b/research/slim/nets/inception_v1_test.py
@@ -34,7 +34,7 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith(
         'InceptionV1/Logits/SpatialSqueeze'))
@@ -49,7 +49,7 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_v1(inputs, num_classes)
     self.assertTrue(net.op.name.startswith('InceptionV1/Logits/AvgPool'))
     self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
@@ -60,7 +60,7 @@ class InceptionV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     mixed_6c, end_points = inception.inception_v1_base(inputs)
     self.assertTrue(mixed_6c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_6c.get_shape().as_list(),
@@ -82,7 +82,7 @@ class InceptionV1Test(tf.test.TestCase):
                  'Mixed_5c']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = inception.inception_v1_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
@@ -93,7 +93,7 @@ class InceptionV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v1_base(inputs,
                                                 final_endpoint='Mixed_5c')
     endpoints_shapes = {
@@ -125,7 +125,7 @@ class InceptionV1Test(tf.test.TestCase):
   def testModelHasExpectedNumberOfParameters(self):
     batch_size = 5
     height, width = 224, 224
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope(inception.inception_v1_arg_scope()):
       inception.inception_v1_base(inputs)
     total_params, _ = slim.model_analyzer.analyze_vars(
@@ -136,7 +136,7 @@ class InceptionV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 112, 112
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     mixed_5c, _ = inception.inception_v1_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -147,7 +147,7 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 28, 28
     channels = 192
 
-    inputs = tf.random_uniform((batch_size, height, width, channels))
+    inputs = tf.random.uniform((batch_size, height, width, channels))
     _, end_points = inception.inception_v1_base(
         inputs, include_root_block=False)
     endpoints_shapes = {
@@ -172,31 +172,33 @@ class InceptionV1Test(tf.test.TestCase):
                            expected_shape)
 
   def testUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v1(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v1(inputs, num_classes,
                                                   global_pool=True)
       self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
@@ -204,7 +206,7 @@ class InceptionV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -213,15 +215,15 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [None, num_classes])
-    images = tf.random_uniform((batch_size, height, width, 3))
+    images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -230,13 +232,13 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, _ = inception.inception_v1(eval_inputs, num_classes,
                                        is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -246,50 +248,51 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+    train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
     inception.inception_v1(train_inputs, num_classes)
-    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
     logits, _ = inception.inception_v1(eval_inputs, num_classes, reuse=True)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testLogitsNotSqueezed(self):
     num_classes = 25
-    images = tf.random_uniform([1, 224, 224, 3])
+    images = tf.random.uniform([1, 224, 224, 3])
     logits, _ = inception.inception_v1(images,
                                        num_classes=num_classes,
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v1_arg_scope()):
       inception.inception_v1(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v1_arg_scope(batch_norm_scale=True)):
       inception.inception_v1(inputs, num_classes, is_training=False)
 
     gamma_names = set(
-        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
+        v.op.name
+        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v2.py b/research/slim/nets/inception_v2.py
index 2bae3fea..859c901a 100644
--- a/research/slim/nets/inception_v2.py
+++ b/research/slim/nets/inception_v2.py
@@ -24,7 +24,10 @@ from tensorflow.contrib import slim as contrib_slim
 from nets import inception_utils
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 
 
 def inception_v2_base(inputs,
@@ -92,7 +95,7 @@ def inception_v2_base(inputs,
     )
 
   concat_dim = 3 if data_format == 'NHWC' else 1
-  with tf.variable_scope(scope, 'InceptionV2', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionV2', [inputs]):
     with slim.arg_scope(
         [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
         stride=1,
@@ -167,17 +170,17 @@ def inception_v2_base(inputs,
       # 28 x 28 x 192
       # Inception module.
       end_point = 'Mixed_3b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -186,7 +189,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(32), [1, 1],
@@ -198,17 +201,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 256
       end_point = 'Mixed_3c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -217,7 +220,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(64), [1, 1],
@@ -229,15 +232,15 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 320
       end_point = 'Mixed_4a'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -246,7 +249,7 @@ def inception_v2_base(inputs,
               branch_1, depth(96), [3, 3], scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(
               branch_1, depth(96), [3, 3], stride=2, scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(
               net, [3, 3], stride=2, scope='MaxPool_1a_3x3')
         net = tf.concat(axis=concat_dim, values=[branch_0, branch_1, branch_2])
@@ -254,17 +257,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(224), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(
               branch_1, depth(96), [3, 3], scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -273,7 +276,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -285,17 +288,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -304,7 +307,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -316,17 +319,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4d'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -335,7 +338,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(96), [1, 1],
@@ -347,17 +350,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4e'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(96), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(160), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -366,7 +369,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(96), [1, 1],
@@ -378,15 +381,15 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_5a'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -395,7 +398,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2,
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(
@@ -404,17 +407,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 7 x 7 x 1024
       end_point = 'Mixed_5b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(352), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(160), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -423,7 +426,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -435,17 +438,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 7 x 7 x 1024
       end_point = 'Mixed_5c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(352), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -454,7 +457,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -525,16 +528,18 @@ def inception_v2(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
 
   # Final pooling and prediction
-  with tf.variable_scope(scope, 'InceptionV2', [inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'InceptionV2', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v2_base(
           inputs, scope=scope, min_depth=min_depth,
           depth_multiplier=depth_multiplier)
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
           end_points['global_pool'] = net
         else:
           # Pooling with a fixed kernel size.
diff --git a/research/slim/nets/inception_v2_test.py b/research/slim/nets/inception_v2_test.py
index 52b2f345..089a64de 100644
--- a/research/slim/nets/inception_v2_test.py
+++ b/research/slim/nets/inception_v2_test.py
@@ -34,7 +34,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v2(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith(
         'InceptionV2/Logits/SpatialSqueeze'))
@@ -49,7 +49,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_v2(inputs, num_classes)
     self.assertTrue(net.op.name.startswith('InceptionV2/Logits/AvgPool'))
     self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
@@ -60,7 +60,7 @@ class InceptionV2Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     mixed_5c, end_points = inception.inception_v2_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV2/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -70,7 +70,7 @@ class InceptionV2Test(tf.test.TestCase):
                           'Mixed_5b', 'Mixed_5c', 'Conv2d_1a_7x7',
                           'MaxPool_2a_3x3', 'Conv2d_2b_1x1', 'Conv2d_2c_3x3',
                           'MaxPool_3a_3x3']
-    self.assertItemsEqual(end_points.keys(), expected_endpoints)
+    self.assertItemsEqual(list(end_points.keys()), expected_endpoints)
 
   def testBuildOnlyUptoFinalEndpoint(self):
     batch_size = 5
@@ -81,18 +81,18 @@ class InceptionV2Test(tf.test.TestCase):
                  'Mixed_5a', 'Mixed_5b', 'Mixed_5c']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = inception.inception_v2_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
             'InceptionV2/' + endpoint))
-        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
+        self.assertItemsEqual(endpoints[:index + 1], list(end_points.keys()))
 
   def testBuildAndCheckAllEndPointsUptoMixed5c(self):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v2_base(inputs,
                                                 final_endpoint='Mixed_5c')
     endpoints_shapes = {'Mixed_3b': [batch_size, 28, 28, 256],
@@ -110,7 +110,8 @@ class InceptionV2Test(tf.test.TestCase):
                         'Conv2d_2b_1x1': [batch_size, 56, 56, 64],
                         'Conv2d_2c_3x3': [batch_size, 56, 56, 192],
                         'MaxPool_3a_3x3': [batch_size, 28, 28, 192]}
-    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    self.assertItemsEqual(
+        list(endpoints_shapes.keys()), list(end_points.keys()))
     for endpoint_name in endpoints_shapes:
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
@@ -120,7 +121,7 @@ class InceptionV2Test(tf.test.TestCase):
   def testModelHasExpectedNumberOfParameters(self):
     batch_size = 5
     height, width = 224, 224
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope(inception.inception_v2_arg_scope()):
       inception.inception_v2_base(inputs)
     total_params, _ = slim.model_analyzer.analyze_vars(
@@ -132,7 +133,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v2(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys()
@@ -152,7 +153,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v2(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys()
@@ -172,7 +173,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with self.assertRaises(ValueError):
       _ = inception.inception_v2(inputs, num_classes, depth_multiplier=-0.1)
     with self.assertRaises(ValueError):
@@ -182,7 +183,7 @@ class InceptionV2Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v2_base(inputs)
 
     endpoint_keys = [
@@ -205,7 +206,7 @@ class InceptionV2Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v2_base(inputs)
 
     endpoint_keys = [
@@ -213,7 +214,7 @@ class InceptionV2Test(tf.test.TestCase):
         if key.startswith('Mixed') or key.startswith('Conv')
     ]
 
-    inputs_in_nchw = tf.random_uniform((batch_size, 3, height, width))
+    inputs_in_nchw = tf.random.uniform((batch_size, 3, height, width))
     _, end_points_with_replacement = inception.inception_v2_base(
         inputs_in_nchw, use_separable_conv=False, data_format='NCHW')
 
@@ -221,7 +222,7 @@ class InceptionV2Test(tf.test.TestCase):
     # shape from the original shape with the 'NHWC' layout.
     for key in endpoint_keys:
       transposed_original_shape = tf.transpose(
-          end_points[key], [0, 3, 1, 2]).get_shape().as_list()
+          a=end_points[key], perm=[0, 3, 1, 2]).get_shape().as_list()
       self.assertTrue(key in end_points_with_replacement)
       new_shape = end_points_with_replacement[key].get_shape().as_list()
       self.assertListEqual(transposed_original_shape, new_shape)
@@ -230,7 +231,7 @@ class InceptionV2Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
 
     # 'NCWH' data format is not supported.
     with self.assertRaises(ValueError):
@@ -245,7 +246,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 112, 112
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v2(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -259,7 +260,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 28, 28
     channels = 192
 
-    inputs = tf.random_uniform((batch_size, height, width, channels))
+    inputs = tf.random.uniform((batch_size, height, width, channels))
     _, end_points = inception.inception_v2_base(
         inputs, include_root_block=False)
     endpoints_shapes = {
@@ -274,7 +275,8 @@ class InceptionV2Test(tf.test.TestCase):
         'Mixed_5b': [batch_size, 7, 7, 1024],
         'Mixed_5c': [batch_size, 7, 7, 1024]
     }
-    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
+    self.assertItemsEqual(
+        list(endpoints_shapes.keys()), list(end_points.keys()))
     for endpoint_name in endpoints_shapes:
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
@@ -282,31 +284,33 @@ class InceptionV2Test(tf.test.TestCase):
                            expected_shape)
 
   def testUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v2(inputs, num_classes,
                                                   global_pool=True)
       self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
@@ -314,7 +318,7 @@ class InceptionV2Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -323,15 +327,15 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v2(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [None, num_classes])
-    images = tf.random_uniform((batch_size, height, width, 3))
+    images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -340,13 +344,13 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, _ = inception.inception_v2(eval_inputs, num_classes,
                                        is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -356,50 +360,51 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
 
-    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+    train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
     inception.inception_v2(train_inputs, num_classes)
-    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
     logits, _ = inception.inception_v2(eval_inputs, num_classes, reuse=True)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testLogitsNotSqueezed(self):
     num_classes = 25
-    images = tf.random_uniform([1, 224, 224, 3])
+    images = tf.random.uniform([1, 224, 224, 3])
     logits, _ = inception.inception_v2(images,
                                        num_classes=num_classes,
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v2_arg_scope()):
       inception.inception_v2(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v2_arg_scope(batch_norm_scale=True)):
       inception.inception_v2(inputs, num_classes, is_training=False)
 
     gamma_names = set(
-        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
+        v.op.name
+        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v3.py b/research/slim/nets/inception_v3.py
index 92896b5a..7c9fd7d9 100644
--- a/research/slim/nets/inception_v3.py
+++ b/research/slim/nets/inception_v3.py
@@ -24,7 +24,10 @@ from tensorflow.contrib import slim as contrib_slim
 from nets import inception_utils
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 
 
 def inception_v3_base(inputs,
@@ -97,7 +100,7 @@ def inception_v3_base(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.variable_scope(scope, 'InceptionV3', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionV3', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='VALID'):
       # 299 x 299 x 3
@@ -142,20 +145,20 @@ def inception_v3_base(inputs,
                         stride=1, padding='SAME'):
       # mixed: 35 x 35 x 256.
       end_point = 'Mixed_5b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv2d_0b_5x5')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -165,21 +168,21 @@ def inception_v3_base(inputs,
 
       # mixed_1: 35 x 35 x 288.
       end_point = 'Mixed_5c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0b_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv_1_0c_5x5')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1],
                                  scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -189,20 +192,20 @@ def inception_v3_base(inputs,
 
       # mixed_2: 35 x 35 x 288.
       end_point = 'Mixed_5d'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv2d_0b_5x5')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -212,17 +215,17 @@ def inception_v3_base(inputs,
 
       # mixed_3: 17 x 17 x 768.
       end_point = 'Mixed_6a'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_1x1')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -231,16 +234,16 @@ def inception_v3_base(inputs,
 
       # mixed4: 17 x 17 x 768.
       end_point = 'Mixed_6b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(128), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(128), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -250,7 +253,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -260,16 +263,16 @@ def inception_v3_base(inputs,
 
       # mixed_5: 17 x 17 x 768.
       end_point = 'Mixed_6c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -279,7 +282,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -288,16 +291,16 @@ def inception_v3_base(inputs,
       if end_point == final_endpoint: return net, end_points
       # mixed_6: 17 x 17 x 768.
       end_point = 'Mixed_6d'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -307,7 +310,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -317,16 +320,16 @@ def inception_v3_base(inputs,
 
       # mixed_7: 17 x 17 x 768.
       end_point = 'Mixed_6e'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -336,7 +339,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -346,12 +349,12 @@ def inception_v3_base(inputs,
 
       # mixed_8: 8 x 8 x 1280.
       end_point = 'Mixed_7a'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],
                                  scope='Conv2d_0b_1x7')
@@ -359,7 +362,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0c_7x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -367,22 +370,22 @@ def inception_v3_base(inputs,
       if end_point == final_endpoint: return net, end_points
       # mixed_9: 8 x 8 x 2048.
       end_point = 'Mixed_7b'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0b_3x1')])
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
           branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
@@ -392,22 +395,22 @@ def inception_v3_base(inputs,
 
       # mixed_10: 8 x 8 x 2048.
       end_point = 'Mixed_7c'
-      with tf.variable_scope(end_point):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope(end_point):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0c_3x1')])
-        with tf.variable_scope('Branch_2'):
+        with tf.compat.v1.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
           branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
-        with tf.variable_scope('Branch_3'):
+        with tf.compat.v1.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
@@ -483,7 +486,8 @@ def inception_v3(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.variable_scope(scope, 'InceptionV3', [inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'InceptionV3', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v3_base(
@@ -495,7 +499,7 @@ def inception_v3(inputs,
         with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                             stride=1, padding='SAME'):
           aux_logits = end_points['Mixed_6e']
-          with tf.variable_scope('AuxLogits'):
+          with tf.compat.v1.variable_scope('AuxLogits'):
             aux_logits = slim.avg_pool2d(
                 aux_logits, [5, 5], stride=3, padding='VALID',
                 scope='AvgPool_1a_5x5')
@@ -518,10 +522,11 @@ def inception_v3(inputs,
             end_points['AuxLogits'] = aux_logits
 
       # Final pooling and prediction
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='GlobalPool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='GlobalPool')
           end_points['global_pool'] = net
         else:
           # Pooling with a fixed kernel size.
diff --git a/research/slim/nets/inception_v3_test.py b/research/slim/nets/inception_v3_test.py
index 86499b33..1bc2c13f 100644
--- a/research/slim/nets/inception_v3_test.py
+++ b/research/slim/nets/inception_v3_test.py
@@ -34,7 +34,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v3(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith(
         'InceptionV3/Logits/SpatialSqueeze'))
@@ -49,7 +49,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = None
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_v3(inputs, num_classes)
     self.assertTrue(net.op.name.startswith('InceptionV3/Logits/AvgPool'))
     self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 2048])
@@ -60,7 +60,7 @@ class InceptionV3Test(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     final_endpoint, end_points = inception.inception_v3_base(inputs)
     self.assertTrue(final_endpoint.op.name.startswith(
         'InceptionV3/Mixed_7c'))
@@ -84,7 +84,7 @@ class InceptionV3Test(tf.test.TestCase):
 
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = inception.inception_v3_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
@@ -95,7 +95,7 @@ class InceptionV3Test(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v3_base(
         inputs, final_endpoint='Mixed_7c')
     endpoints_shapes = {'Conv2d_1a_3x3': [batch_size, 149, 149, 32],
@@ -126,7 +126,7 @@ class InceptionV3Test(tf.test.TestCase):
   def testModelHasExpectedNumberOfParameters(self):
     batch_size = 5
     height, width = 299, 299
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope(inception.inception_v3_arg_scope()):
       inception.inception_v3_base(inputs)
     total_params, _ = slim.model_analyzer.analyze_vars(
@@ -138,7 +138,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v3(inputs, num_classes)
     self.assertTrue('Logits' in end_points)
     logits = end_points['Logits']
@@ -162,7 +162,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v3(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys()
@@ -182,7 +182,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v3(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys()
@@ -202,7 +202,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with self.assertRaises(ValueError):
       _ = inception.inception_v3(inputs, num_classes, depth_multiplier=-0.1)
     with self.assertRaises(ValueError):
@@ -213,7 +213,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v3(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV3/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -223,37 +223,39 @@ class InceptionV3Test(tf.test.TestCase):
                          [batch_size, 3, 3, 2048])
 
   def testUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 2
     height, width = 299, 299
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v3(inputs, num_classes)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 1
     height, width = 330, 400
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v3(inputs, num_classes,
                                                   global_pool=True)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7c']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 11, 2048])
 
@@ -262,15 +264,15 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v3(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV3/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [None, num_classes])
-    images = tf.random_uniform((batch_size, height, width, 3))
+    images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -279,13 +281,13 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, _ = inception.inception_v3(eval_inputs, num_classes,
                                        is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -295,51 +297,52 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
 
-    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+    train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
     inception.inception_v3(train_inputs, num_classes)
-    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
     logits, _ = inception.inception_v3(eval_inputs, num_classes,
                                        is_training=False, reuse=True)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testLogitsNotSqueezed(self):
     num_classes = 25
-    images = tf.random_uniform([1, 299, 299, 3])
+    images = tf.random.uniform([1, 299, 299, 3])
     logits, _ = inception.inception_v3(images,
                                        num_classes=num_classes,
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v3_arg_scope()):
       inception.inception_v3(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v3_arg_scope(batch_norm_scale=True)):
       inception.inception_v3(inputs, num_classes, is_training=False)
 
     gamma_names = set(
-        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
+        v.op.name
+        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v4.py b/research/slim/nets/inception_v4.py
index c5cb7f36..7c6678c5 100644
--- a/research/slim/nets/inception_v4.py
+++ b/research/slim/nets/inception_v4.py
@@ -37,17 +37,18 @@ def block_inception_a(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.variable_scope(scope, 'BlockInceptionA', [inputs], reuse=reuse):
-      with tf.variable_scope('Branch_0'):
+    with tf.compat.v1.variable_scope(
+        scope, 'BlockInceptionA', [inputs], reuse=reuse):
+      with tf.compat.v1.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.variable_scope('Branch_1'):
+      with tf.compat.v1.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')
-      with tf.variable_scope('Branch_2'):
+      with tf.compat.v1.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
         branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')
-      with tf.variable_scope('Branch_3'):
+      with tf.compat.v1.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -58,16 +59,17 @@ def block_reduction_a(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.variable_scope(scope, 'BlockReductionA', [inputs], reuse=reuse):
-      with tf.variable_scope('Branch_0'):
+    with tf.compat.v1.variable_scope(
+        scope, 'BlockReductionA', [inputs], reuse=reuse):
+      with tf.compat.v1.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID',
                                scope='Conv2d_1a_3x3')
-      with tf.variable_scope('Branch_1'):
+      with tf.compat.v1.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
         branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.variable_scope('Branch_2'):
+      with tf.compat.v1.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -78,20 +80,21 @@ def block_inception_b(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.variable_scope(scope, 'BlockInceptionB', [inputs], reuse=reuse):
-      with tf.variable_scope('Branch_0'):
+    with tf.compat.v1.variable_scope(
+        scope, 'BlockInceptionB', [inputs], reuse=reuse):
+      with tf.compat.v1.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.variable_scope('Branch_1'):
+      with tf.compat.v1.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')
         branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')
-      with tf.variable_scope('Branch_2'):
+      with tf.compat.v1.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')
         branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')
         branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')
         branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')
-      with tf.variable_scope('Branch_3'):
+      with tf.compat.v1.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -102,18 +105,19 @@ def block_reduction_b(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.variable_scope(scope, 'BlockReductionB', [inputs], reuse=reuse):
-      with tf.variable_scope('Branch_0'):
+    with tf.compat.v1.variable_scope(
+        scope, 'BlockReductionB', [inputs], reuse=reuse):
+      with tf.compat.v1.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.variable_scope('Branch_1'):
+      with tf.compat.v1.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')
         branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')
         branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.variable_scope('Branch_2'):
+      with tf.compat.v1.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -124,22 +128,23 @@ def block_inception_c(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.variable_scope(scope, 'BlockInceptionC', [inputs], reuse=reuse):
-      with tf.variable_scope('Branch_0'):
+    with tf.compat.v1.variable_scope(
+        scope, 'BlockInceptionC', [inputs], reuse=reuse):
+      with tf.compat.v1.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.variable_scope('Branch_1'):
+      with tf.compat.v1.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = tf.concat(axis=3, values=[
             slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'),
             slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])
-      with tf.variable_scope('Branch_2'):
+      with tf.compat.v1.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')
         branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')
         branch_2 = tf.concat(axis=3, values=[
             slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'),
             slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])
-      with tf.variable_scope('Branch_3'):
+      with tf.compat.v1.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -171,7 +176,7 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
     end_points[name] = net
     return name == final_endpoint
 
-  with tf.variable_scope(scope, 'InceptionV4', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionV4', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='SAME'):
       # 299 x 299 x 3
@@ -186,23 +191,23 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
       net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')
       if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points
       # 147 x 147 x 64
-      with tf.variable_scope('Mixed_3a'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_3a'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_0a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID',
                                  scope='Conv2d_0a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1])
         if add_and_check_final('Mixed_3a', net): return net, end_points
 
       # 73 x 73 x 160
-      with tf.variable_scope('Mixed_4a'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_4a'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding='VALID',
                                  scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')
@@ -212,11 +217,11 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
         if add_and_check_final('Mixed_4a', net): return net, end_points
 
       # 71 x 71 x 192
-      with tf.variable_scope('Mixed_5a'):
-        with tf.variable_scope('Branch_0'):
+      with tf.compat.v1.variable_scope('Mixed_5a'):
+        with tf.compat.v1.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding='VALID',
                                  scope='Conv2d_1a_3x3')
-        with tf.variable_scope('Branch_1'):
+        with tf.compat.v1.variable_scope('Branch_1'):
           branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1])
@@ -281,7 +286,8 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
     end_points: the set of end_points from the inception model.
   """
   end_points = {}
-  with tf.variable_scope(scope, 'InceptionV4', [inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'InceptionV4', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v4_base(inputs, scope=scope)
@@ -290,7 +296,7 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
                           stride=1, padding='SAME'):
         # Auxiliary Head logits
         if create_aux_logits and num_classes:
-          with tf.variable_scope('AuxLogits'):
+          with tf.compat.v1.variable_scope('AuxLogits'):
             # 17 x 17 x 1024
             aux_logits = end_points['Mixed_6h']
             aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,
@@ -310,15 +316,18 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
         # Final pooling and prediction
         # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
         # can be set to False to disable pooling here (as in resnet_*()).
-        with tf.variable_scope('Logits'):
+        with tf.compat.v1.variable_scope('Logits'):
           # 8 x 8 x 1536
           kernel_size = net.get_shape()[1:3]
           if kernel_size.is_fully_defined():
             net = slim.avg_pool2d(net, kernel_size, padding='VALID',
                                   scope='AvgPool_1a')
           else:
-            net = tf.reduce_mean(net, [1, 2], keep_dims=True,
-                                 name='global_pool')
+            net = tf.reduce_mean(
+                input_tensor=net,
+                axis=[1, 2],
+                keepdims=True,
+                name='global_pool')
           end_points['global_pool'] = net
           if not num_classes:
             return net, end_points
diff --git a/research/slim/nets/inception_v4_test.py b/research/slim/nets/inception_v4_test.py
index eb717333..bc8df478 100644
--- a/research/slim/nets/inception_v4_test.py
+++ b/research/slim/nets/inception_v4_test.py
@@ -29,7 +29,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v4(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
     predictions = end_points['Predictions']
@@ -48,7 +48,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_v4(inputs, num_classes)
     self.assertTrue(net.op.name.startswith('InceptionV4/Logits/AvgPool'))
     self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1536])
@@ -59,7 +59,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, endpoints = inception.inception_v4(inputs, num_classes,
                                                create_aux_logits=False)
     self.assertFalse('AuxLogits' in endpoints)
@@ -71,7 +71,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = inception.inception_v4(inputs, num_classes)
     endpoints_shapes = {'Conv2d_1a_3x3': [batch_size, 149, 149, 32],
                         'Conv2d_2a_3x3': [batch_size, 147, 147, 32],
@@ -116,7 +116,7 @@ class InceptionTest(tf.test.TestCase):
   def testBuildBaseNetwork(self):
     batch_size = 5
     height, width = 299, 299
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = inception.inception_v4_base(inputs)
     self.assertTrue(net.op.name.startswith(
         'InceptionV4/Mixed_7d'))
@@ -142,7 +142,7 @@ class InceptionTest(tf.test.TestCase):
         'Mixed_7b', 'Mixed_7c', 'Mixed_7d']
     for index, endpoint in enumerate(all_endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = inception.inception_v4_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
@@ -153,22 +153,24 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 5
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     # Force all Variables to reside on the device.
-    with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
+    with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
       inception.inception_v4(inputs, num_classes)
-    with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
+    with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
       inception.inception_v4(inputs, num_classes)
-    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+    for v in tf.compat.v1.get_collection(
+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
       self.assertDeviceEqual(v.device, '/cpu:0')
-    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+    for v in tf.compat.v1.get_collection(
+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
       self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testHalfSizeImages(self):
     batch_size = 5
     height, width = 150, 150
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v4(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -181,7 +183,7 @@ class InceptionTest(tf.test.TestCase):
     batch_size = 1
     height, width = 350, 400
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = inception.inception_v4(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -195,15 +197,15 @@ class InceptionTest(tf.test.TestCase):
     height, width = 350, 400
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(tf.float32, (batch_size, None, None, 3))
       logits, end_points = inception.inception_v4(
           inputs, num_classes, create_aux_logits=False)
       self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7d']
-      images = tf.random_uniform((batch_size, height, width, 3))
-      sess.run(tf.global_variables_initializer())
+      images = tf.random.uniform((batch_size, height, width, 3))
+      sess.run(tf.compat.v1.global_variables_initializer())
       logits_out, pre_pool_out = sess.run([logits, pre_pool],
                                           {inputs: images.eval()})
       self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
@@ -214,13 +216,13 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
       logits, _ = inception.inception_v4(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
-      images = tf.random_uniform((batch_size, height, width, 3))
-      sess.run(tf.global_variables_initializer())
+      images = tf.random.uniform((batch_size, height, width, 3))
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -229,12 +231,12 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = inception.inception_v4(eval_inputs,
                                          num_classes,
                                          is_training=False)
-      predictions = tf.argmax(logits, 1)
-      sess.run(tf.global_variables_initializer())
+      predictions = tf.argmax(input=logits, axis=1)
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -244,39 +246,40 @@ class InceptionTest(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
     with self.test_session() as sess:
-      train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+      train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
       inception.inception_v4(train_inputs, num_classes)
-      eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
       logits, _ = inception.inception_v4(eval_inputs,
                                          num_classes,
                                          is_training=False,
                                          reuse=True)
-      predictions = tf.argmax(logits, 1)
-      sess.run(tf.global_variables_initializer())
+      predictions = tf.argmax(input=logits, axis=1)
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with contrib_slim.arg_scope(inception.inception_v4_arg_scope()):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
     with contrib_slim.arg_scope(
         inception.inception_v4_arg_scope(batch_norm_scale=True)):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
     gamma_names = set(
-        v.op.name for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
+        v.op.name
+        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/lenet.py b/research/slim/nets/lenet.py
index 5d1d936a..9f269d27 100644
--- a/research/slim/nets/lenet.py
+++ b/research/slim/nets/lenet.py
@@ -59,7 +59,7 @@ def lenet(images, num_classes=10, is_training=False,
   """
   end_points = {}
 
-  with tf.variable_scope(scope, 'LeNet', [images]):
+  with tf.compat.v1.variable_scope(scope, 'LeNet', [images]):
     net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')
     net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
     net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')
@@ -93,6 +93,6 @@ def lenet_arg_scope(weight_decay=0.0):
   with slim.arg_scope(
       [slim.conv2d, slim.fully_connected],
       weights_regularizer=slim.l2_regularizer(weight_decay),
-      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
+      weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),
       activation_fn=tf.nn.relu) as sc:
     return sc
diff --git a/research/slim/nets/mobilenet/conv_blocks.py b/research/slim/nets/mobilenet/conv_blocks.py
index 61a7016a..85b07916 100644
--- a/research/slim/nets/mobilenet/conv_blocks.py
+++ b/research/slim/nets/mobilenet/conv_blocks.py
@@ -78,9 +78,10 @@ def _split_divisible(num, num_ways, divisible_by=8):
 
 @contextlib.contextmanager
 def _v1_compatible_scope_naming(scope):
+  """v1 compatible scope naming."""
   if scope is None:  # Create uniqified separable blocks.
-    with tf.variable_scope(None, default_name='separable') as s, \
-         tf.name_scope(s.original_name_scope):
+    with tf.compat.v1.variable_scope(None, default_name='separable') as s, \
+         tf.compat.v1.name_scope(s.original_name_scope):
       yield ''
   else:
     # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.
@@ -299,8 +300,8 @@ def expanded_conv(input_tensor,
   if depthwise_activation_fn is not None:
     dw_defaults['activation_fn'] = depthwise_activation_fn
   # pylint: disable=g-backslash-continuation
-  with tf.variable_scope(scope, default_name='expanded_conv') as s, \
-       tf.name_scope(s.original_name_scope), \
+  with tf.compat.v1.variable_scope(scope, default_name='expanded_conv') as s, \
+       tf.compat.v1.name_scope(s.original_name_scope), \
       slim.arg_scope((slim.conv2d,), **conv_defaults), \
        slim.arg_scope((slim.separable_conv2d,), **dw_defaults):
     prev_depth = input_tensor.get_shape().as_list()[3]
@@ -413,6 +414,11 @@ def squeeze_excite(input_tensor,
                    pool=None):
   """Squeeze excite block for Mobilenet V3.
 
+  If the squeeze_input_tensor - or the input_tensor if squeeze_input_tensor is
+  None - contains variable dimensions (Nonetype in tensor shape), perform
+  average pooling (as the first step in the squeeze operation) by calling
+  reduce_mean across the H/W of the input tensor.
+
   Args:
     input_tensor: input tensor to apply SE block to.
     divisible_by: ensures all inner dimensions are divisible by this number.
@@ -428,7 +434,7 @@ def squeeze_excite(input_tensor,
   Returns:
     Gated input_tensor. (e.g. X * SE(X))
   """
-  with tf.variable_scope('squeeze_excite'):
+  with tf.compat.v1.variable_scope('squeeze_excite'):
     if squeeze_input_tensor is None:
       squeeze_input_tensor = input_tensor
     input_size = input_tensor.shape.as_list()[1:3]
@@ -441,10 +447,13 @@ def squeeze_excite(input_tensor,
     squeeze_channels = _make_divisible(
         input_channels / squeeze_factor, divisor=divisible_by)
 
-    pooled = tf.nn.avg_pool(squeeze_input_tensor,
-                            (1, pool_height, pool_width, 1),
-                            strides=(1, stride, stride, 1),
-                            padding='VALID')
+    if pool is None:
+      pooled = tf.reduce_mean(squeeze_input_tensor, axis=[1, 2], keepdims=True)
+    else:
+      pooled = tf.nn.avg_pool(
+          squeeze_input_tensor, (1, pool_height, pool_width, 1),
+          strides=(1, stride, stride, 1),
+          padding='VALID')
     squeeze = slim.conv2d(
         pooled,
         kernel_size=(1, 1),
diff --git a/research/slim/nets/mobilenet/mobilenet.py b/research/slim/nets/mobilenet/mobilenet.py
index 67569043..908dad43 100644
--- a/research/slim/nets/mobilenet/mobilenet.py
+++ b/research/slim/nets/mobilenet/mobilenet.py
@@ -54,8 +54,10 @@ def _fixed_padding(inputs, kernel_size, rate=1):
   pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]
   pad_beg = [pad_total[0] // 2, pad_total[1] // 2]
   pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]
-  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],
-                                  [pad_beg[1], pad_end[1]], [0, 0]])
+  padded_inputs = tf.pad(
+      tensor=inputs,
+      paddings=[[0, 0], [pad_beg[0], pad_end[0]], [pad_beg[1], pad_end[1]],
+                [0, 0]])
   return padded_inputs
 
 
@@ -304,8 +306,8 @@ def mobilenet_base(  # pylint: disable=invalid-name
 
 @contextlib.contextmanager
 def _scope_all(scope, default_scope=None):
-  with tf.variable_scope(scope, default_name=default_scope) as s,\
-       tf.name_scope(s.original_name_scope):
+  with tf.compat.v1.variable_scope(scope, default_name=default_scope) as s,\
+       tf.compat.v1.name_scope(s.original_name_scope):
     yield s
 
 
@@ -361,7 +363,7 @@ def mobilenet(inputs,
   if len(input_shape) != 4:
     raise ValueError('Expected rank 4 input, was: %d' % len(input_shape))
 
-  with tf.variable_scope(scope, 'Mobilenet', reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(scope, 'Mobilenet', reuse=reuse) as scope:
     inputs = tf.identity(inputs, 'input')
     net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)
     if base_only:
@@ -369,7 +371,7 @@ def mobilenet(inputs,
 
     net = tf.identity(net, name='embedding')
 
-    with tf.variable_scope('Logits'):
+    with tf.compat.v1.variable_scope('Logits'):
       net = global_pool(net)
       end_points['global_pool'] = net
       if not num_classes:
@@ -382,7 +384,7 @@ def mobilenet(inputs,
           num_classes, [1, 1],
           activation_fn=None,
           normalizer_fn=None,
-          biases_initializer=tf.zeros_initializer(),
+          biases_initializer=tf.compat.v1.zeros_initializer(),
           scope='Conv2d_1c_1x1')
 
       logits = tf.squeeze(logits, [1, 2])
@@ -394,7 +396,7 @@ def mobilenet(inputs,
   return logits, end_points
 
 
-def global_pool(input_tensor, pool_op=tf.nn.avg_pool):
+def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d):
   """Applies avg pool to produce 1x1 output.
 
   NOTE: This function is funcitonally equivalenet to reduce_mean, but it has
@@ -408,9 +410,11 @@ def global_pool(input_tensor, pool_op=tf.nn.avg_pool):
   """
   shape = input_tensor.get_shape().as_list()
   if shape[1] is None or shape[2] is None:
-    kernel_size = tf.convert_to_tensor(
-        [1, tf.shape(input_tensor)[1],
-         tf.shape(input_tensor)[2], 1])
+    kernel_size = tf.convert_to_tensor(value=[
+        1,
+        tf.shape(input=input_tensor)[1],
+        tf.shape(input=input_tensor)[2], 1
+    ])
   else:
     kernel_size = [1, shape[1], shape[2], 1]
   output = pool_op(
@@ -458,7 +462,8 @@ def training_scope(is_training=True,
   if stddev < 0:
     weight_intitializer = slim.initializers.xavier_initializer()
   else:
-    weight_intitializer = tf.truncated_normal_initializer(stddev=stddev)
+    weight_intitializer = tf.compat.v1.truncated_normal_initializer(
+        stddev=stddev)
 
   # Set weight_decay for weights in Conv and FC layers.
   with slim.arg_scope(
diff --git a/research/slim/nets/mobilenet/mobilenet_v2_test.py b/research/slim/nets/mobilenet/mobilenet_v2_test.py
index 820c0ab5..11ab0eb9 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2_test.py
@@ -18,6 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 import copy
+from six.moves import range
 import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 from nets.mobilenet import conv_blocks as ops
@@ -36,19 +37,20 @@ def find_ops(optype):
   Returns:
      List of operations.
   """
-  gd = tf.get_default_graph()
+  gd = tf.compat.v1.get_default_graph()
   return [var for var in gd.get_operations() if var.type == optype]
 
 
 class MobilenetV2Test(tf.test.TestCase):
 
   def setUp(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
 
   def testCreation(self):
     spec = dict(mobilenet_v2.V2_DEF)
     _, ep = mobilenet.mobilenet(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        conv_defs=spec)
     num_convs = len(find_ops('Conv2D'))
 
     # This is mostly a sanity test. No deep reason for these particular
@@ -64,16 +66,17 @@ class MobilenetV2Test(tf.test.TestCase):
   def testCreationNoClasses(self):
     spec = copy.deepcopy(mobilenet_v2.V2_DEF)
     net, ep = mobilenet.mobilenet(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec,
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        conv_defs=spec,
         num_classes=None)
     self.assertIs(net, ep['global_pool'])
 
   def testImageSizes(self):
     for input_size, output_size in [(224, 7), (192, 6), (160, 5),
                                     (128, 4), (96, 3)]:
-      tf.reset_default_graph()
+      tf.compat.v1.reset_default_graph()
       _, ep = mobilenet_v2.mobilenet(
-          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))
+          tf.compat.v1.placeholder(tf.float32, (10, input_size, input_size, 3)))
 
       self.assertEqual(ep['layer_18/output'].get_shape().as_list()[1:3],
                        [output_size] * 2)
@@ -84,7 +87,8 @@ class MobilenetV2Test(tf.test.TestCase):
         (ops.expanded_conv,): dict(split_expansion=2),
     }
     _, _ = mobilenet.mobilenet(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)), conv_defs=spec)
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        conv_defs=spec)
     num_convs = len(find_ops('Conv2D'))
     # All but 3 op has 3 conv operatore, the remainign 3 have one
     # and there is one unaccounted.
@@ -92,16 +96,16 @@ class MobilenetV2Test(tf.test.TestCase):
 
   def testWithOutputStride8(self):
     out, _ = mobilenet.mobilenet_base(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=8,
         scope='MobilenetV2')
     self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])
 
   def testDivisibleBy(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     mobilenet_v2.mobilenet(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         divisible_by=16,
         min_depth=32)
@@ -111,25 +115,27 @@ class MobilenetV2Test(tf.test.TestCase):
                              1001], s)
 
   def testDivisibleByWithArgScope(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     # Verifies that depth_multiplier arg scope actually works
     # if no default min_depth is provided.
     with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):
       mobilenet_v2.mobilenet(
-          tf.placeholder(tf.float32, (10, 224, 224, 2)),
-          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)
+          tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 2)),
+          conv_defs=mobilenet_v2.V2_DEF,
+          depth_multiplier=0.1)
       s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops('Conv2D')]
       s = set(s)
       self.assertSameElements(s, [32, 192, 128, 1001])
 
   def testFineGrained(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     # Verifies that depth_multiplier arg scope actually works
     # if no default min_depth is provided.
 
     mobilenet_v2.mobilenet(
-        tf.placeholder(tf.float32, (10, 224, 224, 2)),
-        conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.01,
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 2)),
+        conv_defs=mobilenet_v2.V2_DEF,
+        depth_multiplier=0.01,
         finegrain_classification_mode=True)
     s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops('Conv2D')]
     s = set(s)
@@ -137,18 +143,19 @@ class MobilenetV2Test(tf.test.TestCase):
     self.assertSameElements(s, [8, 48, 1001, 1280])
 
   def testMobilenetBase(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     # Verifies that mobilenet_base returns pre-pooling layer.
     with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):
       net, _ = mobilenet_v2.mobilenet_base(
-          tf.placeholder(tf.float32, (10, 224, 224, 16)),
-          conv_defs=mobilenet_v2.V2_DEF, depth_multiplier=0.1)
+          tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+          conv_defs=mobilenet_v2.V2_DEF,
+          depth_multiplier=0.1)
       self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])
 
   def testWithOutputStride16(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=16)
     self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])
@@ -167,17 +174,18 @@ class MobilenetV2Test(tf.test.TestCase):
         multiplier_func=inverse_multiplier,
         num_outputs=16)
     _ = mobilenet_v2.mobilenet_base(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
-        conv_defs=new_def, depth_multiplier=0.1)
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        conv_defs=new_def,
+        depth_multiplier=0.1)
     s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops('Conv2D')]
     # Expect first layer to be 160 (16 / 0.1), and other layers
     # their max(original size * 0.1, 8)
     self.assertEqual([160, 8, 48, 8, 48], s[:5])
 
   def testWithOutputStride8AndExplicitPadding(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=8,
         use_explicit_padding=True,
@@ -185,9 +193,9 @@ class MobilenetV2Test(tf.test.TestCase):
     self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])
 
   def testWithOutputStride16AndExplicitPadding(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=16,
         use_explicit_padding=True)
diff --git a/research/slim/nets/mobilenet/mobilenet_v3.py b/research/slim/nets/mobilenet/mobilenet_v3.py
index 997ddaa4..36dbdaa9 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3.py
@@ -45,7 +45,7 @@ _se4 = lambda expansion_tensor, input_tensor: squeeze_excite(expansion_tensor)
 
 
 def hard_swish(x):
-  with tf.name_scope('hard_swish'):
+  with tf.compat.v1.name_scope('hard_swish'):
     return x * tf.nn.relu6(x + np.float32(3)) * np.float32(1. / 6.)
 
 
@@ -358,6 +358,12 @@ small = wrapped_partial(mobilenet, conv_defs=V3_SMALL)
 edge_tpu = wrapped_partial(mobilenet,
                            new_defaults={'scope': 'MobilenetEdgeTPU'},
                            conv_defs=V3_EDGETPU)
+edge_tpu_075 = wrapped_partial(
+    mobilenet,
+    new_defaults={'scope': 'MobilenetEdgeTPU'},
+    conv_defs=V3_EDGETPU,
+    depth_multiplier=0.75,
+    finegrain_classification_mode=True)
 
 # Minimalistic model that does not have Squeeze Excite blocks,
 # Hardswish, or 5x5 depthwise convolution.
diff --git a/research/slim/nets/mobilenet/mobilenet_v3_test.py b/research/slim/nets/mobilenet/mobilenet_v3_test.py
index a9656fc5..45f1b103 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3_test.py
@@ -28,23 +28,23 @@ class MobilenetV3Test(absltest.TestCase):
 
   def setUp(self):
     super(MobilenetV3Test, self).setUp()
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
 
   def testMobilenetV3Large(self):
     logits, endpoints = mobilenet_v3.mobilenet(
-        tf.placeholder(tf.float32, (1, 224, 224, 3)))
+        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)))
     self.assertEqual(endpoints['layer_19'].shape, [1, 1, 1, 1280])
     self.assertEqual(logits.shape, [1, 1001])
 
   def testMobilenetV3Small(self):
     _, endpoints = mobilenet_v3.mobilenet(
-        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)),
         conv_defs=mobilenet_v3.V3_SMALL)
     self.assertEqual(endpoints['layer_15'].shape, [1, 1, 1, 1024])
 
   def testMobilenetEdgeTpu(self):
     _, endpoints = mobilenet_v3.edge_tpu(
-        tf.placeholder(tf.float32, (1, 224, 224, 3)))
+        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)))
     self.assertIn('Inference mode is created by default',
                   mobilenet_v3.edge_tpu.__doc__)
     self.assertEqual(endpoints['layer_24'].shape, [1, 7, 7, 1280])
@@ -53,13 +53,13 @@ class MobilenetV3Test(absltest.TestCase):
 
   def testMobilenetEdgeTpuChangeScope(self):
     _, endpoints = mobilenet_v3.edge_tpu(
-        tf.placeholder(tf.float32, (1, 224, 224, 3)), scope='Scope')
+        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)), scope='Scope')
     self.assertStartsWith(
         endpoints['layer_24'].name, 'Scope')
 
   def testMobilenetV3BaseOnly(self):
     result, endpoints = mobilenet_v3.mobilenet(
-        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)),
         conv_defs=mobilenet_v3.V3_LARGE,
         base_only=True,
         final_endpoint='layer_17')
@@ -67,6 +67,16 @@ class MobilenetV3Test(absltest.TestCase):
     self.assertEqual(endpoints['layer_17'].shape, [1, 7, 7, 960])
     self.assertEqual(result, endpoints['layer_17'])
 
+  def testMobilenetV3BaseOnly_VariableInput(self):
+    result, endpoints = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (None, None, None, 3)),
+        conv_defs=mobilenet_v3.V3_LARGE,
+        base_only=True,
+        final_endpoint='layer_17')
+    # Get the latest layer before average pool.
+    self.assertEqual(endpoints['layer_17'].shape.as_list(),
+                     [None, None, None, 960])
+    self.assertEqual(result, endpoints['layer_17'])
 
 if __name__ == '__main__':
   absltest.main()
diff --git a/research/slim/nets/mobilenet_v1.md b/research/slim/nets/mobilenet_v1.md
index badc1be1..ba4cc239 100644
--- a/research/slim/nets/mobilenet_v1.md
+++ b/research/slim/nets/mobilenet_v1.md
@@ -1,5 +1,5 @@
 # MobilenetV2 and above
-For MobilenetV2+ see this file [mobilenet/README.md](mobilenet/README_md)
+For MobilenetV2+ see this file [mobilenet/README.md](mobilenet/README.md)
 
 # MobileNetV1
 
diff --git a/research/slim/nets/mobilenet_v1.py b/research/slim/nets/mobilenet_v1.py
index f8cffd43..107c3474 100644
--- a/research/slim/nets/mobilenet_v1.py
+++ b/research/slim/nets/mobilenet_v1.py
@@ -162,8 +162,10 @@ def _fixed_padding(inputs, kernel_size, rate=1):
   pad_total = [kernel_size_effective[0] - 1, kernel_size_effective[1] - 1]
   pad_beg = [pad_total[0] // 2, pad_total[1] // 2]
   pad_end = [pad_total[0] - pad_beg[0], pad_total[1] - pad_beg[1]]
-  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg[0], pad_end[0]],
-                                  [pad_beg[1], pad_end[1]], [0, 0]])
+  padded_inputs = tf.pad(
+      tensor=inputs,
+      paddings=[[0, 0], [pad_beg[0], pad_end[0]], [pad_beg[1], pad_end[1]],
+                [0, 0]])
   return padded_inputs
 
 
@@ -231,7 +233,7 @@ def mobilenet_v1_base(inputs,
   padding = 'SAME'
   if use_explicit_padding:
     padding = 'VALID'
-  with tf.variable_scope(scope, 'MobilenetV1', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'MobilenetV1', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=padding):
       # The current_stride variable keeps track of the output stride of the
       # activations, i.e., the running product of convolution strides up to the
@@ -357,17 +359,19 @@ def mobilenet_v1(inputs,
     raise ValueError('Invalid input tensor rank, expected 4, was: %d' %
                      len(input_shape))
 
-  with tf.variable_scope(scope, 'MobilenetV1', [inputs], reuse=reuse) as scope:
+  with tf.compat.v1.variable_scope(
+      scope, 'MobilenetV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = mobilenet_v1_base(inputs, scope=scope,
                                           min_depth=min_depth,
                                           depth_multiplier=depth_multiplier,
                                           conv_defs=conv_defs)
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
           end_points['global_pool'] = net
         else:
           # Pooling with a fixed kernel size.
@@ -431,7 +435,7 @@ def mobilenet_v1_arg_scope(
     regularize_depthwise=False,
     batch_norm_decay=0.9997,
     batch_norm_epsilon=0.001,
-    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
+    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
     normalizer_fn=slim.batch_norm):
   """Defines the default MobilenetV1 arg scope.
 
@@ -462,7 +466,7 @@ def mobilenet_v1_arg_scope(
     batch_norm_params['is_training'] = is_training
 
   # Set weight_decay for weights in Conv and DepthSepConv layers.
-  weights_init = tf.truncated_normal_initializer(stddev=stddev)
+  weights_init = tf.compat.v1.truncated_normal_initializer(stddev=stddev)
   regularizer = contrib_layers.l2_regularizer(weight_decay)
   if regularize_depthwise:
     depthwise_regularizer = regularizer
diff --git a/research/slim/nets/mobilenet_v1_eval.py b/research/slim/nets/mobilenet_v1_eval.py
index 97eb4110..c7bd590e 100644
--- a/research/slim/nets/mobilenet_v1_eval.py
+++ b/research/slim/nets/mobilenet_v1_eval.py
@@ -29,7 +29,7 @@ from preprocessing import preprocessing_factory
 
 slim = contrib_slim
 
-flags = tf.app.flags
+flags = tf.compat.v1.app.flags
 
 flags.DEFINE_string('master', '', 'Session master')
 flags.DEFINE_integer('batch_size', 250, 'Batch size')
@@ -74,7 +74,7 @@ def imagenet_input(is_training):
 
   image = image_preprocessing_fn(image, FLAGS.image_size, FLAGS.image_size)
 
-  images, labels = tf.train.batch(
+  images, labels = tf.compat.v1.train.batch(
       tensors=[image, label],
       batch_size=FLAGS.batch_size,
       num_threads=4,
@@ -94,8 +94,11 @@ def metrics(logits, labels):
   """
   labels = tf.squeeze(labels)
   names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
-      'Accuracy': tf.metrics.accuracy(tf.argmax(logits, 1), labels),
-      'Recall_5': tf.metrics.recall_at_k(labels, logits, 5),
+      'Accuracy':
+          tf.compat.v1.metrics.accuracy(
+              tf.argmax(input=logits, axis=1), labels),
+      'Recall_5':
+          tf.compat.v1.metrics.recall_at_k(labels, logits, 5),
   })
   for name, value in names_to_values.iteritems():
     slim.summaries.add_scalar_summary(
@@ -151,4 +154,4 @@ def main(unused_arg):
 
 
 if __name__ == '__main__':
-  tf.app.run(main)
+  tf.compat.v1.app.run(main)
diff --git a/research/slim/nets/mobilenet_v1_test.py b/research/slim/nets/mobilenet_v1_test.py
index fb43a440..3f8d9b2c 100644
--- a/research/slim/nets/mobilenet_v1_test.py
+++ b/research/slim/nets/mobilenet_v1_test.py
@@ -34,7 +34,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith(
         'MobilenetV1/Logits/SpatialSqueeze'))
@@ -49,7 +49,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
     self.assertTrue(net.op.name.startswith('MobilenetV1/Logits/AvgPool'))
     self.assertListEqual(net.get_shape().as_list(), [batch_size, 1, 1, 1024])
@@ -60,7 +60,7 @@ class MobilenetV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = mobilenet_v1.mobilenet_v1_base(inputs)
     self.assertTrue(net.op.name.startswith('MobilenetV1/Conv2d_13'))
     self.assertListEqual(net.get_shape().as_list(),
@@ -100,7 +100,7 @@ class MobilenetV1Test(tf.test.TestCase):
                  'Conv2d_13_depthwise', 'Conv2d_13_pointwise']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, height, width, 3))
+        inputs = tf.random.uniform((batch_size, height, width, 3))
         out_tensor, end_points = mobilenet_v1.mobilenet_v1_base(
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
@@ -117,7 +117,7 @@ class MobilenetV1Test(tf.test.TestCase):
         mobilenet_v1.DepthSepConv(kernel=[3, 3], stride=1, depth=512)
     ]
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     net, end_points = mobilenet_v1.mobilenet_v1_base(
         inputs, final_endpoint='Conv2d_3_pointwise', conv_defs=conv_defs)
     self.assertTrue(net.op.name.startswith('MobilenetV1/Conv2d_3'))
@@ -133,7 +133,7 @@ class MobilenetV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                         normalizer_fn=slim.batch_norm):
       _, end_points = mobilenet_v1.mobilenet_v1_base(
@@ -186,7 +186,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     output_stride = 16
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                         normalizer_fn=slim.batch_norm):
       _, end_points = mobilenet_v1.mobilenet_v1_base(
@@ -240,7 +240,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     output_stride = 8
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                         normalizer_fn=slim.batch_norm):
       _, end_points = mobilenet_v1.mobilenet_v1_base(
@@ -293,7 +293,7 @@ class MobilenetV1Test(tf.test.TestCase):
     batch_size = 5
     height, width = 128, 128
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                         normalizer_fn=slim.batch_norm):
       _, end_points = mobilenet_v1.mobilenet_v1_base(
@@ -345,7 +345,7 @@ class MobilenetV1Test(tf.test.TestCase):
   def testModelHasExpectedNumberOfParameters(self):
     batch_size = 5
     height, width = 224, 224
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d],
                         normalizer_fn=slim.batch_norm):
       mobilenet_v1.mobilenet_v1_base(inputs)
@@ -358,7 +358,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys() if key.startswith('Conv')]
@@ -377,7 +377,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     _, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
 
     endpoint_keys = [key for key in end_points.keys()
@@ -397,7 +397,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     with self.assertRaises(ValueError):
       _ = mobilenet_v1.mobilenet_v1(
           inputs, num_classes, depth_multiplier=-0.1)
@@ -410,7 +410,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 112, 112
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, height, width, 3))
+    inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -420,31 +420,33 @@ class MobilenetV1Test(tf.test.TestCase):
                          [batch_size, 4, 4, 1024])
 
   def testUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_13_pointwise']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.reset_default_graph()
+    tf.compat.v1.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, shape=(batch_size, None, None, 3))
+      inputs = tf.compat.v1.placeholder(
+          tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes,
                                                      global_pool=True)
       self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
@@ -452,7 +454,7 @@ class MobilenetV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_13_pointwise']
       feed_dict = {inputs: input_np}
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -461,15 +463,15 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
                          [None, num_classes])
-    images = tf.random_uniform((batch_size, height, width, 3))
+    images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -478,13 +480,13 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, height, width, 3))
     logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,
                                           is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -494,27 +496,27 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 150, 150
     num_classes = 1000
 
-    train_inputs = tf.random_uniform((train_batch_size, height, width, 3))
+    train_inputs = tf.random.uniform((train_batch_size, height, width, 3))
     mobilenet_v1.mobilenet_v1(train_inputs, num_classes)
-    eval_inputs = tf.random_uniform((eval_batch_size, height, width, 3))
+    eval_inputs = tf.random.uniform((eval_batch_size, height, width, 3))
     logits, _ = mobilenet_v1.mobilenet_v1(eval_inputs, num_classes,
                                           reuse=True)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testLogitsNotSqueezed(self):
     num_classes = 25
-    images = tf.random_uniform([1, 224, 224, 3])
+    images = tf.random.uniform([1, 224, 224, 3])
     logits, _ = mobilenet_v1.mobilenet_v1(images,
                                           num_classes=num_classes,
                                           spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.global_variables_initializer().run()
+      tf.compat.v1.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
diff --git a/research/slim/nets/mobilenet_v1_train.py b/research/slim/nets/mobilenet_v1_train.py
index a0ff7354..1035ea31 100644
--- a/research/slim/nets/mobilenet_v1_train.py
+++ b/research/slim/nets/mobilenet_v1_train.py
@@ -28,7 +28,7 @@ from preprocessing import preprocessing_factory
 
 slim = contrib_slim
 
-flags = tf.app.flags
+flags = tf.compat.v1.app.flags
 
 flags.DEFINE_string('master', '', 'Session master')
 flags.DEFINE_integer('task', 0, 'Task')
@@ -104,11 +104,10 @@ def imagenet_input(is_training):
 
   image = image_preprocessing_fn(image, FLAGS.image_size, FLAGS.image_size)
 
-  images, labels = tf.train.batch(
-      [image, label],
-      batch_size=FLAGS.batch_size,
-      num_threads=4,
-      capacity=5 * FLAGS.batch_size)
+  images, labels = tf.compat.v1.train.batch([image, label],
+                                            batch_size=FLAGS.batch_size,
+                                            num_threads=4,
+                                            capacity=5 * FLAGS.batch_size)
   labels = slim.one_hot_encoding(labels, FLAGS.num_classes)
   return images, labels
 
@@ -123,7 +122,7 @@ def build_model():
   """
   g = tf.Graph()
   with g.as_default(), tf.device(
-      tf.train.replica_device_setter(FLAGS.ps_tasks)):
+      tf.compat.v1.train.replica_device_setter(FLAGS.ps_tasks)):
     inputs, labels = imagenet_input(is_training=True)
     with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=True)):
       logits, _ = mobilenet_v1.mobilenet_v1(
@@ -132,7 +131,7 @@ def build_model():
           depth_multiplier=FLAGS.depth_multiplier,
           num_classes=FLAGS.num_classes)
 
-    tf.losses.softmax_cross_entropy(labels, logits)
+    tf.compat.v1.losses.softmax_cross_entropy(labels, logits)
 
     # Call rewriter to produce graph with fake quant ops and folded batch norms
     # quant_delay delays start of quantization till quant_delay steps, allowing
@@ -140,19 +139,19 @@ def build_model():
     if FLAGS.quantize:
       contrib_quantize.create_training_graph(quant_delay=get_quant_delay())
 
-    total_loss = tf.losses.get_total_loss(name='total_loss')
+    total_loss = tf.compat.v1.losses.get_total_loss(name='total_loss')
     # Configure the learning rate using an exponential decay.
     num_epochs_per_decay = 2.5
     imagenet_size = 1271167
     decay_steps = int(imagenet_size / FLAGS.batch_size * num_epochs_per_decay)
 
-    learning_rate = tf.train.exponential_decay(
+    learning_rate = tf.compat.v1.train.exponential_decay(
         get_learning_rate(),
-        tf.train.get_or_create_global_step(),
+        tf.compat.v1.train.get_or_create_global_step(),
         decay_steps,
         _LEARNING_RATE_DECAY_FACTOR,
         staircase=True)
-    opt = tf.train.GradientDescentOptimizer(learning_rate)
+    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
 
     train_tensor = slim.learning.create_train_op(
         total_loss,
@@ -167,7 +166,8 @@ def get_checkpoint_init_fn():
   """Returns the checkpoint init_fn if the checkpoint is provided."""
   if FLAGS.fine_tune_checkpoint:
     variables_to_restore = slim.get_variables_to_restore()
-    global_step_reset = tf.assign(tf.train.get_or_create_global_step(), 0)
+    global_step_reset = tf.compat.v1.assign(
+        tf.compat.v1.train.get_or_create_global_step(), 0)
     # When restoring from a floating point model, the min/max values for
     # quantized weights and activations are not present.
     # We instruct slim to ignore variables that are missing during restoration
@@ -203,7 +203,7 @@ def train_model():
         save_summaries_secs=FLAGS.save_summaries_secs,
         save_interval_secs=FLAGS.save_interval_secs,
         init_fn=get_checkpoint_init_fn(),
-        global_step=tf.train.get_global_step())
+        global_step=tf.compat.v1.train.get_global_step())
 
 
 def main(unused_arg):
@@ -211,4 +211,4 @@ def main(unused_arg):
 
 
 if __name__ == '__main__':
-  tf.app.run(main)
+  tf.compat.v1.app.run(main)
diff --git a/research/slim/nets/nasnet/nasnet.py b/research/slim/nets/nasnet/nasnet.py
index b7807d39..664fa302 100644
--- a/research/slim/nets/nasnet/nasnet.py
+++ b/research/slim/nets/nasnet/nasnet.py
@@ -231,9 +231,9 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
 def _build_aux_head(net, end_points, num_classes, hparams, scope):
   """Auxiliary head used for all models across all datasets."""
   activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
-  with tf.variable_scope(scope):
+  with tf.compat.v1.variable_scope(scope):
     aux_logits = tf.identity(net)
-    with tf.variable_scope('aux_logits'):
+    with tf.compat.v1.variable_scope('aux_logits'):
       aux_logits = slim.avg_pool2d(
           aux_logits, [5, 5], stride=3, padding='VALID')
       aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='proj')
@@ -302,11 +302,12 @@ def build_nasnet_cifar(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.logging.info('A GPU is available on the machine, consider using NCHW '
-                    'data format for increased speed on GPU.')
+    tf.compat.v1.logging.info(
+        'A GPU is available on the machine, consider using NCHW '
+        'data format for increased speed on GPU.')
 
   if hparams.data_format == 'NCHW':
-    images = tf.transpose(images, [0, 3, 1, 2])
+    images = tf.transpose(a=images, perm=[0, 3, 1, 2])
 
   # Calculate the total number of cells in the network
   # Add 2 for the reduction cells
@@ -354,11 +355,12 @@ def build_nasnet_mobile(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.logging.info('A GPU is available on the machine, consider using NCHW '
-                    'data format for increased speed on GPU.')
+    tf.compat.v1.logging.info(
+        'A GPU is available on the machine, consider using NCHW '
+        'data format for increased speed on GPU.')
 
   if hparams.data_format == 'NCHW':
-    images = tf.transpose(images, [0, 3, 1, 2])
+    images = tf.transpose(a=images, perm=[0, 3, 1, 2])
 
   # Calculate the total number of cells in the network
   # Add 2 for the reduction cells
@@ -409,11 +411,12 @@ def build_nasnet_large(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.logging.info('A GPU is available on the machine, consider using NCHW '
-                    'data format for increased speed on GPU.')
+    tf.compat.v1.logging.info(
+        'A GPU is available on the machine, consider using NCHW '
+        'data format for increased speed on GPU.')
 
   if hparams.data_format == 'NCHW':
-    images = tf.transpose(images, [0, 3, 1, 2])
+    images = tf.transpose(a=images, perm=[0, 3, 1, 2])
 
   # Calculate the total number of cells in the network
   # Add 2 for the reduction cells
@@ -534,7 +537,7 @@ def _build_nasnet_base(images,
     cell_outputs.append(net)
 
   # Final softmax layer
-  with tf.variable_scope('final_layer'):
+  with tf.compat.v1.variable_scope('final_layer'):
     net = activation_fn(net)
     net = nasnet_utils.global_avg_pool(net)
     if add_and_check_endpoint('global_pool', net) or not num_classes:
diff --git a/research/slim/nets/nasnet/nasnet_test.py b/research/slim/nets/nasnet/nasnet_test.py
index b9a63919..deb347d3 100644
--- a/research/slim/nets/nasnet/nasnet_test.py
+++ b/research/slim/nets/nasnet/nasnet_test.py
@@ -31,8 +31,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 32, 32
     num_classes = 10
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       logits, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -48,8 +48,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       logits, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -65,8 +65,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       logits, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -82,8 +82,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 32, 32
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       net, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -95,8 +95,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       net, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -108,8 +108,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       net, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -121,8 +121,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 32, 32
     num_classes = 10
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       _, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 32, 32, 96],
@@ -153,7 +153,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -164,9 +164,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     for use_aux_head in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
-      tf.train.create_global_step()
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
+      tf.compat.v1.train.create_global_step()
       config = nasnet.cifar_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
@@ -178,8 +178,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       _, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 28, 28, 88],
@@ -204,7 +204,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -215,9 +215,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
-      tf.train.create_global_step()
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
+      tf.compat.v1.train.create_global_step()
       config = nasnet.mobile_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
@@ -229,8 +229,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       _, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 42, 42, 336],
@@ -261,7 +261,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -272,9 +272,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
-      tf.train.create_global_step()
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
+      tf.compat.v1.train.create_global_step()
       config = nasnet.large_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
@@ -286,18 +286,20 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     # Force all Variables to reside on the device.
-    with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
+    with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         nasnet.build_nasnet_mobile(inputs, num_classes)
-    with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
+    with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         nasnet.build_nasnet_mobile(inputs, num_classes)
-    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+    for v in tf.compat.v1.get_collection(
+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
       self.assertDeviceEqual(v.device, '/cpu:0')
-    for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+    for v in tf.compat.v1.get_collection(
+        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
       self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testUnknownBatchSizeMobileModel(self):
@@ -305,13 +307,13 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         logits, _ = nasnet.build_nasnet_mobile(inputs, num_classes)
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
-      images = tf.random_uniform((batch_size, height, width, 3))
-      sess.run(tf.global_variables_initializer())
+      images = tf.random.uniform((batch_size, height, width, 3))
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -320,13 +322,13 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session() as sess:
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         logits, _ = nasnet.build_nasnet_mobile(eval_inputs,
                                                num_classes,
                                                is_training=False)
-      predictions = tf.argmax(logits, 1)
-      sess.run(tf.global_variables_initializer())
+      predictions = tf.argmax(input=logits, axis=1)
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -334,8 +336,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 32, 32
     num_classes = 10
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     config = nasnet.cifar_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
@@ -348,8 +350,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     config = nasnet.mobile_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
@@ -362,8 +364,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     config = nasnet.large_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
@@ -376,8 +378,8 @@ class NASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 32, 32
     num_classes = 10
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    global_step = tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    global_step = tf.compat.v1.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       logits, end_points = nasnet.build_nasnet_cifar(inputs,
                                                      num_classes,
@@ -396,14 +398,14 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     for use_bounded_activation in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       config = nasnet.cifar_config()
       config.set_hparam('use_bounded_activation', use_bounded_activation)
       with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
         _, _ = nasnet.build_nasnet_cifar(
             inputs, num_classes, config=config)
-      for node in tf.get_default_graph().as_graph_def().node:
+      for node in tf.compat.v1.get_default_graph().as_graph_def().node:
         if node.op.startswith('Relu'):
           self.assertEqual(node.op == 'Relu6', use_bounded_activation)
 
diff --git a/research/slim/nets/nasnet/nasnet_utils.py b/research/slim/nets/nasnet/nasnet_utils.py
index 163e14a8..1d68854f 100644
--- a/research/slim/nets/nasnet/nasnet_utils.py
+++ b/research/slim/nets/nasnet/nasnet_utils.py
@@ -82,9 +82,9 @@ def global_avg_pool(x, data_format=INVALID):
   assert data_format in ['NHWC', 'NCHW']
   assert x.shape.ndims == 4
   if data_format == 'NHWC':
-    return tf.reduce_mean(x, [1, 2])
+    return tf.reduce_mean(input_tensor=x, axis=[1, 2])
   else:
-    return tf.reduce_mean(x, [2, 3])
+    return tf.reduce_mean(input_tensor=x, axis=[2, 3])
 
 
 @contrib_framework.add_arg_scope
@@ -101,8 +101,12 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
     stride_spec = [1, 1, stride, stride]
 
   # Skip path 1
-  path1 = tf.nn.avg_pool(
-      net, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)
+  path1 = tf.compat.v2.nn.avg_pool2d(
+      input=net,
+      ksize=[1, 1, 1, 1],
+      strides=stride_spec,
+      padding='VALID',
+      data_format=data_format)
   path1 = slim.conv2d(path1, int(output_filters / 2), 1, scope='path1_conv')
 
   # Skip path 2
@@ -110,15 +114,19 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
   # include those 0's that were added.
   if data_format == 'NHWC':
     pad_arr = [[0, 0], [0, 1], [0, 1], [0, 0]]
-    path2 = tf.pad(net, pad_arr)[:, 1:, 1:, :]
+    path2 = tf.pad(tensor=net, paddings=pad_arr)[:, 1:, 1:, :]
     concat_axis = 3
   else:
     pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]
-    path2 = tf.pad(net, pad_arr)[:, :, 1:, 1:]
+    path2 = tf.pad(tensor=net, paddings=pad_arr)[:, :, 1:, 1:]
     concat_axis = 1
 
-  path2 = tf.nn.avg_pool(
-      path2, [1, 1, 1, 1], stride_spec, 'VALID', data_format=data_format)
+  path2 = tf.compat.v2.nn.avg_pool2d(
+      input=path2,
+      ksize=[1, 1, 1, 1],
+      strides=stride_spec,
+      padding='VALID',
+      data_format=data_format)
 
   # If odd number of filters, add an additional one to the second path.
   final_filter_size = int(output_filters / 2) + int(output_filters % 2)
@@ -134,10 +142,10 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
 def drop_path(net, keep_prob, is_training=True):
   """Drops out a whole example hiddenstate with the specified probability."""
   if is_training:
-    batch_size = tf.shape(net)[0]
+    batch_size = tf.shape(input=net)[0]
     noise_shape = [batch_size, 1, 1, 1]
     random_tensor = keep_prob
-    random_tensor += tf.random_uniform(noise_shape, dtype=tf.float32)
+    random_tensor += tf.random.uniform(noise_shape, dtype=tf.float32)
     binary_tensor = tf.cast(tf.floor(random_tensor), net.dtype)
     keep_prob_inv = tf.cast(1.0 / keep_prob, net.dtype)
     net = net * keep_prob_inv * binary_tensor
@@ -316,10 +324,10 @@ class NasNetABaseCell(object):
     self._filter_size = int(self._num_conv_filters * filter_scaling)
 
     i = 0
-    with tf.variable_scope(scope):
+    with tf.compat.v1.variable_scope(scope):
       net = self._cell_base(net, prev_layer)
       for iteration in range(5):
-        with tf.variable_scope('comb_iter_{}'.format(iteration)):
+        with tf.compat.v1.variable_scope('comb_iter_{}'.format(iteration)):
           left_hiddenstate_idx, right_hiddenstate_idx = (
               self._hiddenstate_indices[i],
               self._hiddenstate_indices[i + 1])
@@ -332,17 +340,17 @@ class NasNetABaseCell(object):
           operation_right = self._operations[i+1]
           i += 2
           # Apply conv operations
-          with tf.variable_scope('left'):
+          with tf.compat.v1.variable_scope('left'):
             h1 = self._apply_conv_operation(h1, operation_left,
                                             stride, original_input_left,
                                             current_step)
-          with tf.variable_scope('right'):
+          with tf.compat.v1.variable_scope('right'):
             h2 = self._apply_conv_operation(h2, operation_right,
                                             stride, original_input_right,
                                             current_step)
 
           # Combine hidden states using 'add'.
-          with tf.variable_scope('combine'):
+          with tf.compat.v1.variable_scope('combine'):
             h = h1 + h2
             if self._use_bounded_activation:
               h = tf.nn.relu6(h)
@@ -350,7 +358,7 @@ class NasNetABaseCell(object):
           # Add hiddenstate to the list of hiddenstates we can choose from
           net.append(h)
 
-      with tf.variable_scope('cell_output'):
+      with tf.compat.v1.variable_scope('cell_output'):
         net = self._combine_unused_states(net)
 
       return net
@@ -411,7 +419,7 @@ class NasNetABaseCell(object):
       should_reduce = should_reduce and not used_h
       if should_reduce:
         stride = 2 if final_height != curr_height else 1
-        with tf.variable_scope('reduction_{}'.format(idx)):
+        with tf.compat.v1.variable_scope('reduction_{}'.format(idx)):
           net[idx] = factorized_reduction(
               net[idx], final_num_filters, stride)
 
@@ -452,23 +460,24 @@ class NasNetABaseCell(object):
         layer_ratio = (self._cell_num + 1)/float(num_cells)
         if use_summaries:
           with tf.device('/cpu:0'):
-            tf.summary.scalar('layer_ratio', layer_ratio)
+            tf.compat.v1.summary.scalar('layer_ratio', layer_ratio)
         drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)
       if drop_connect_version in ['v1', 'v3']:
         # Decrease the keep probability over time
         if current_step is None:
-          current_step = tf.train.get_or_create_global_step()
+          current_step = tf.compat.v1.train.get_or_create_global_step()
         current_step = tf.cast(current_step, tf.float32)
         drop_path_burn_in_steps = self._total_training_steps
         current_ratio = current_step / drop_path_burn_in_steps
         current_ratio = tf.minimum(1.0, current_ratio)
         if use_summaries:
           with tf.device('/cpu:0'):
-            tf.summary.scalar('current_ratio', current_ratio)
+            tf.compat.v1.summary.scalar('current_ratio', current_ratio)
         drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob))
       if use_summaries:
         with tf.device('/cpu:0'):
-          tf.summary.scalar('drop_path_keep_prob', drop_path_keep_prob)
+          tf.compat.v1.summary.scalar('drop_path_keep_prob',
+                                      drop_path_keep_prob)
       net = drop_path(net, drop_path_keep_prob)
     return net
 
diff --git a/research/slim/nets/nasnet/nasnet_utils_test.py b/research/slim/nets/nasnet/nasnet_utils_test.py
index 60bf9029..d165418d 100644
--- a/research/slim/nets/nasnet/nasnet_utils_test.py
+++ b/research/slim/nets/nasnet/nasnet_utils_test.py
@@ -51,7 +51,7 @@ class NasnetUtilsTest(tf.test.TestCase):
 
   def testGlobalAvgPool(self):
     data_formats = ['NHWC', 'NCHW']
-    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))
+    inputs = tf.compat.v1.placeholder(tf.float32, (5, 10, 20, 10))
     for data_format in data_formats:
       output = nasnet_utils.global_avg_pool(
           inputs, data_format)
diff --git a/research/slim/nets/nasnet/pnasnet.py b/research/slim/nets/nasnet/pnasnet.py
index 42d8b8f6..5e612e33 100644
--- a/research/slim/nets/nasnet/pnasnet.py
+++ b/research/slim/nets/nasnet/pnasnet.py
@@ -147,7 +147,7 @@ def _build_pnasnet_base(images,
       # pylint: enable=protected-access
 
   # Final softmax layer
-  with tf.variable_scope('final_layer'):
+  with tf.compat.v1.variable_scope('final_layer'):
     net = activation_fn(net)
     net = nasnet_utils.global_avg_pool(net)
     if add_and_check_endpoint('global_pool', net) or not num_classes:
@@ -176,11 +176,12 @@ def build_pnasnet_large(images,
   # pylint: enable=protected-access
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.logging.info('A GPU is available on the machine, consider using NCHW '
-                    'data format for increased speed on GPU.')
+    tf.compat.v1.logging.info(
+        'A GPU is available on the machine, consider using NCHW '
+        'data format for increased speed on GPU.')
 
   if hparams.data_format == 'NCHW':
-    images = tf.transpose(images, [0, 3, 1, 2])
+    images = tf.transpose(a=images, perm=[0, 3, 1, 2])
 
   # Calculate the total number of cells in the network.
   # There is no distinction between reduction and normal cells in PNAS so the
@@ -224,11 +225,12 @@ def build_pnasnet_mobile(images,
   # pylint: enable=protected-access
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.logging.info('A GPU is available on the machine, consider using NCHW '
-                    'data format for increased speed on GPU.')
+    tf.compat.v1.logging.info(
+        'A GPU is available on the machine, consider using NCHW '
+        'data format for increased speed on GPU.')
 
   if hparams.data_format == 'NCHW':
-    images = tf.transpose(images, [0, 3, 1, 2])
+    images = tf.transpose(a=images, perm=[0, 3, 1, 2])
 
   # Calculate the total number of cells in the network.
   # There is no distinction between reduction and normal cells in PNAS so the
diff --git a/research/slim/nets/nasnet/pnasnet_test.py b/research/slim/nets/nasnet/pnasnet_test.py
index 34f7946d..8e1df4db 100644
--- a/research/slim/nets/nasnet/pnasnet_test.py
+++ b/research/slim/nets/nasnet/pnasnet_test.py
@@ -31,8 +31,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       logits, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -48,8 +48,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       logits, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -63,21 +63,21 @@ class PNASNetTest(tf.test.TestCase):
 
   def testBuildNonExistingLayerLargeModel(self):
     """Tests that the model is built correctly without unnecessary layers."""
-    inputs = tf.random_uniform((5, 331, 331, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((5, 331, 331, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       pnasnet.build_pnasnet_large(inputs, 1000)
-    vars_names = [x.op.name for x in tf.trainable_variables()]
+    vars_names = [x.op.name for x in tf.compat.v1.trainable_variables()]
     self.assertIn('cell_stem_0/1x1/weights', vars_names)
     self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names)
 
   def testBuildNonExistingLayerMobileModel(self):
     """Tests that the model is built correctly without unnecessary layers."""
-    inputs = tf.random_uniform((5, 224, 224, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((5, 224, 224, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       pnasnet.build_pnasnet_mobile(inputs, 1000)
-    vars_names = [x.op.name for x in tf.trainable_variables()]
+    vars_names = [x.op.name for x in tf.compat.v1.trainable_variables()]
     self.assertIn('cell_stem_0/1x1/weights', vars_names)
     self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names)
 
@@ -85,8 +85,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       net, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -98,8 +98,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = None
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       net, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -111,8 +111,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       _, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
 
@@ -138,7 +138,7 @@ class PNASNetTest(tf.test.TestCase):
     self.assertEqual(len(end_points), 17)
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertIn(endpoint_name, end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -148,8 +148,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       _, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
 
@@ -173,7 +173,7 @@ class PNASNetTest(tf.test.TestCase):
     self.assertEqual(len(end_points), 14)
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertIn(endpoint_name, end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -184,9 +184,9 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
-      tf.train.create_global_step()
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
+      tf.compat.v1.train.create_global_step()
       config = pnasnet.large_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
@@ -199,9 +199,9 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
-      tf.train.create_global_step()
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
+      tf.compat.v1.train.create_global_step()
       config = pnasnet.mobile_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
@@ -213,8 +213,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 331, 331
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     config = pnasnet.large_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
@@ -227,8 +227,8 @@ class PNASNetTest(tf.test.TestCase):
     batch_size = 5
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.random_uniform((batch_size, height, width, 3))
-    tf.train.create_global_step()
+    inputs = tf.random.uniform((batch_size, height, width, 3))
+    tf.compat.v1.train.create_global_step()
     config = pnasnet.mobile_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
@@ -242,14 +242,14 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_bounded_activation in (True, False):
-      tf.reset_default_graph()
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      tf.compat.v1.reset_default_graph()
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       config = pnasnet.mobile_imagenet_config()
       config.set_hparam('use_bounded_activation', use_bounded_activation)
       with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
         _, _ = pnasnet.build_pnasnet_mobile(
             inputs, num_classes, config=config)
-      for node in tf.get_default_graph().as_graph_def().node:
+      for node in tf.compat.v1.get_default_graph().as_graph_def().node:
         if node.op.startswith('Relu'):
           self.assertEqual(node.op == 'Relu6', use_bounded_activation)
 
diff --git a/research/slim/nets/nets_factory.py b/research/slim/nets/nets_factory.py
index c425c364..1c34f80e 100644
--- a/research/slim/nets/nets_factory.py
+++ b/research/slim/nets/nets_factory.py
@@ -32,84 +32,98 @@ from nets import resnet_v2
 from nets import s3dg
 from nets import vgg
 from nets.mobilenet import mobilenet_v2
+from nets.mobilenet import mobilenet_v3
 from nets.nasnet import nasnet
 from nets.nasnet import pnasnet
 
 
 slim = contrib_slim
 
-networks_map = {'alexnet_v2': alexnet.alexnet_v2,
-                'cifarnet': cifarnet.cifarnet,
-                'overfeat': overfeat.overfeat,
-                'vgg_a': vgg.vgg_a,
-                'vgg_16': vgg.vgg_16,
-                'vgg_19': vgg.vgg_19,
-                'inception_v1': inception.inception_v1,
-                'inception_v2': inception.inception_v2,
-                'inception_v3': inception.inception_v3,
-                'inception_v4': inception.inception_v4,
-                'inception_resnet_v2': inception.inception_resnet_v2,
-                'i3d': i3d.i3d,
-                's3dg': s3dg.s3dg,
-                'lenet': lenet.lenet,
-                'resnet_v1_50': resnet_v1.resnet_v1_50,
-                'resnet_v1_101': resnet_v1.resnet_v1_101,
-                'resnet_v1_152': resnet_v1.resnet_v1_152,
-                'resnet_v1_200': resnet_v1.resnet_v1_200,
-                'resnet_v2_50': resnet_v2.resnet_v2_50,
-                'resnet_v2_101': resnet_v2.resnet_v2_101,
-                'resnet_v2_152': resnet_v2.resnet_v2_152,
-                'resnet_v2_200': resnet_v2.resnet_v2_200,
-                'mobilenet_v1': mobilenet_v1.mobilenet_v1,
-                'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_075,
-                'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_050,
-                'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_025,
-                'mobilenet_v2': mobilenet_v2.mobilenet,
-                'mobilenet_v2_140': mobilenet_v2.mobilenet_v2_140,
-                'mobilenet_v2_035': mobilenet_v2.mobilenet_v2_035,
-                'nasnet_cifar': nasnet.build_nasnet_cifar,
-                'nasnet_mobile': nasnet.build_nasnet_mobile,
-                'nasnet_large': nasnet.build_nasnet_large,
-                'pnasnet_large': pnasnet.build_pnasnet_large,
-                'pnasnet_mobile': pnasnet.build_pnasnet_mobile,
-               }
+networks_map = {
+    'alexnet_v2': alexnet.alexnet_v2,
+    'cifarnet': cifarnet.cifarnet,
+    'overfeat': overfeat.overfeat,
+    'vgg_a': vgg.vgg_a,
+    'vgg_16': vgg.vgg_16,
+    'vgg_19': vgg.vgg_19,
+    'inception_v1': inception.inception_v1,
+    'inception_v2': inception.inception_v2,
+    'inception_v3': inception.inception_v3,
+    'inception_v4': inception.inception_v4,
+    'inception_resnet_v2': inception.inception_resnet_v2,
+    'i3d': i3d.i3d,
+    's3dg': s3dg.s3dg,
+    'lenet': lenet.lenet,
+    'resnet_v1_50': resnet_v1.resnet_v1_50,
+    'resnet_v1_101': resnet_v1.resnet_v1_101,
+    'resnet_v1_152': resnet_v1.resnet_v1_152,
+    'resnet_v1_200': resnet_v1.resnet_v1_200,
+    'resnet_v2_50': resnet_v2.resnet_v2_50,
+    'resnet_v2_101': resnet_v2.resnet_v2_101,
+    'resnet_v2_152': resnet_v2.resnet_v2_152,
+    'resnet_v2_200': resnet_v2.resnet_v2_200,
+    'mobilenet_v1': mobilenet_v1.mobilenet_v1,
+    'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_075,
+    'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_050,
+    'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_025,
+    'mobilenet_v2': mobilenet_v2.mobilenet,
+    'mobilenet_v2_140': mobilenet_v2.mobilenet_v2_140,
+    'mobilenet_v2_035': mobilenet_v2.mobilenet_v2_035,
+    'mobilenet_v3_small': mobilenet_v3.small,
+    'mobilenet_v3_large': mobilenet_v3.large,
+    'mobilenet_v3_small_minimalistic': mobilenet_v3.small_minimalistic,
+    'mobilenet_v3_large_minimalistic': mobilenet_v3.large_minimalistic,
+    'mobilenet_edgetpu': mobilenet_v3.edge_tpu,
+    'mobilenet_edgetpu_075': mobilenet_v3.edge_tpu_075,
+    'nasnet_cifar': nasnet.build_nasnet_cifar,
+    'nasnet_mobile': nasnet.build_nasnet_mobile,
+    'nasnet_large': nasnet.build_nasnet_large,
+    'pnasnet_large': pnasnet.build_pnasnet_large,
+    'pnasnet_mobile': pnasnet.build_pnasnet_mobile,
+}
 
-arg_scopes_map = {'alexnet_v2': alexnet.alexnet_v2_arg_scope,
-                  'cifarnet': cifarnet.cifarnet_arg_scope,
-                  'overfeat': overfeat.overfeat_arg_scope,
-                  'vgg_a': vgg.vgg_arg_scope,
-                  'vgg_16': vgg.vgg_arg_scope,
-                  'vgg_19': vgg.vgg_arg_scope,
-                  'inception_v1': inception.inception_v3_arg_scope,
-                  'inception_v2': inception.inception_v3_arg_scope,
-                  'inception_v3': inception.inception_v3_arg_scope,
-                  'inception_v4': inception.inception_v4_arg_scope,
-                  'inception_resnet_v2':
-                  inception.inception_resnet_v2_arg_scope,
-                  'i3d': i3d.i3d_arg_scope,
-                  's3dg': s3dg.s3dg_arg_scope,
-                  'lenet': lenet.lenet_arg_scope,
-                  'resnet_v1_50': resnet_v1.resnet_arg_scope,
-                  'resnet_v1_101': resnet_v1.resnet_arg_scope,
-                  'resnet_v1_152': resnet_v1.resnet_arg_scope,
-                  'resnet_v1_200': resnet_v1.resnet_arg_scope,
-                  'resnet_v2_50': resnet_v2.resnet_arg_scope,
-                  'resnet_v2_101': resnet_v2.resnet_arg_scope,
-                  'resnet_v2_152': resnet_v2.resnet_arg_scope,
-                  'resnet_v2_200': resnet_v2.resnet_arg_scope,
-                  'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,
-                  'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_arg_scope,
-                  'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_arg_scope,
-                  'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_arg_scope,
-                  'mobilenet_v2': mobilenet_v2.training_scope,
-                  'mobilenet_v2_035': mobilenet_v2.training_scope,
-                  'mobilenet_v2_140': mobilenet_v2.training_scope,
-                  'nasnet_cifar': nasnet.nasnet_cifar_arg_scope,
-                  'nasnet_mobile': nasnet.nasnet_mobile_arg_scope,
-                  'nasnet_large': nasnet.nasnet_large_arg_scope,
-                  'pnasnet_large': pnasnet.pnasnet_large_arg_scope,
-                  'pnasnet_mobile': pnasnet.pnasnet_mobile_arg_scope,
-                 }
+arg_scopes_map = {
+    'alexnet_v2': alexnet.alexnet_v2_arg_scope,
+    'cifarnet': cifarnet.cifarnet_arg_scope,
+    'overfeat': overfeat.overfeat_arg_scope,
+    'vgg_a': vgg.vgg_arg_scope,
+    'vgg_16': vgg.vgg_arg_scope,
+    'vgg_19': vgg.vgg_arg_scope,
+    'inception_v1': inception.inception_v3_arg_scope,
+    'inception_v2': inception.inception_v3_arg_scope,
+    'inception_v3': inception.inception_v3_arg_scope,
+    'inception_v4': inception.inception_v4_arg_scope,
+    'inception_resnet_v2': inception.inception_resnet_v2_arg_scope,
+    'i3d': i3d.i3d_arg_scope,
+    's3dg': s3dg.s3dg_arg_scope,
+    'lenet': lenet.lenet_arg_scope,
+    'resnet_v1_50': resnet_v1.resnet_arg_scope,
+    'resnet_v1_101': resnet_v1.resnet_arg_scope,
+    'resnet_v1_152': resnet_v1.resnet_arg_scope,
+    'resnet_v1_200': resnet_v1.resnet_arg_scope,
+    'resnet_v2_50': resnet_v2.resnet_arg_scope,
+    'resnet_v2_101': resnet_v2.resnet_arg_scope,
+    'resnet_v2_152': resnet_v2.resnet_arg_scope,
+    'resnet_v2_200': resnet_v2.resnet_arg_scope,
+    'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,
+    'mobilenet_v1_075': mobilenet_v1.mobilenet_v1_arg_scope,
+    'mobilenet_v1_050': mobilenet_v1.mobilenet_v1_arg_scope,
+    'mobilenet_v1_025': mobilenet_v1.mobilenet_v1_arg_scope,
+    'mobilenet_v2': mobilenet_v2.training_scope,
+    'mobilenet_v2_035': mobilenet_v2.training_scope,
+    'mobilenet_v2_140': mobilenet_v2.training_scope,
+    'mobilenet_v3_small': mobilenet_v3.training_scope,
+    'mobilenet_v3_large': mobilenet_v3.training_scope,
+    'mobilenet_v3_small_minimalistic': mobilenet_v3.training_scope,
+    'mobilenet_v3_large_minimalistic': mobilenet_v3.training_scope,
+    'mobilenet_edgetpu': mobilenet_v3.training_scope,
+    'mobilenet_edgetpu_075': mobilenet_v3.training_scope,
+    'nasnet_cifar': nasnet.nasnet_cifar_arg_scope,
+    'nasnet_mobile': nasnet.nasnet_mobile_arg_scope,
+    'nasnet_large': nasnet.nasnet_large_arg_scope,
+    'pnasnet_large': pnasnet.pnasnet_large_arg_scope,
+    'pnasnet_mobile': pnasnet.pnasnet_mobile_arg_scope,
+}
 
 
 def get_network_fn(name, num_classes, weight_decay=0.0, is_training=False):
diff --git a/research/slim/nets/nets_factory_test.py b/research/slim/nets/nets_factory_test.py
index e111fc22..3e16fc12 100644
--- a/research/slim/nets/nets_factory_test.py
+++ b/research/slim/nets/nets_factory_test.py
@@ -36,8 +36,7 @@ class NetworksTest(tf.test.TestCase):
         # Most networks use 224 as their default_image_size
         image_size = getattr(net_fn, 'default_image_size', 224)
         if net not in ['i3d', 's3dg']:
-          inputs = tf.random_uniform(
-              (batch_size, image_size, image_size, 3))
+          inputs = tf.random.uniform((batch_size, image_size, image_size, 3))
           logits, end_points = net_fn(inputs)
           self.assertTrue(isinstance(logits, tf.Tensor))
           self.assertTrue(isinstance(end_points, dict))
@@ -53,8 +52,7 @@ class NetworksTest(tf.test.TestCase):
         # Most networks use 224 as their default_image_size
         image_size = getattr(net_fn, 'default_image_size', 224)
         if net not in ['i3d', 's3dg']:
-          inputs = tf.random_uniform(
-              (batch_size, image_size, image_size, 3))
+          inputs = tf.random.uniform((batch_size, image_size, image_size, 3))
           logits, end_points = net_fn(inputs)
           self.assertTrue(isinstance(logits, tf.Tensor))
           self.assertTrue(isinstance(end_points, dict))
@@ -69,8 +67,7 @@ class NetworksTest(tf.test.TestCase):
         net_fn = nets_factory.get_network_fn(net, num_classes=num_classes)
         # Most networks use 224 as their default_image_size
         image_size = getattr(net_fn, 'default_image_size', 224) // 2
-        inputs = tf.random_uniform(
-            (batch_size, 10, image_size, image_size, 3))
+        inputs = tf.random.uniform((batch_size, 10, image_size, image_size, 3))
         logits, end_points = net_fn(inputs)
         self.assertTrue(isinstance(logits, tf.Tensor))
         self.assertTrue(isinstance(end_points, dict))
diff --git a/research/slim/nets/overfeat.py b/research/slim/nets/overfeat.py
index cdf14ded..8cd70960 100644
--- a/research/slim/nets/overfeat.py
+++ b/research/slim/nets/overfeat.py
@@ -35,14 +35,17 @@ import tensorflow as tf
 from tensorflow.contrib import slim as contrib_slim
 
 slim = contrib_slim
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 
 
 def overfeat_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.zeros_initializer()):
+                      biases_initializer=tf.compat.v1.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME'):
       with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
         return arg_sc
@@ -88,7 +91,7 @@ def overfeat(inputs,
       None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'overfeat', [inputs]) as sc:
+  with tf.compat.v1.variable_scope(scope, 'overfeat', [inputs]) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -104,9 +107,10 @@ def overfeat(inputs,
       net = slim.max_pool2d(net, [2, 2], scope='pool5')
 
       # Use conv2d instead of fully_connected layers.
-      with slim.arg_scope([slim.conv2d],
-                          weights_initializer=trunc_normal(0.005),
-                          biases_initializer=tf.constant_initializer(0.1)):
+      with slim.arg_scope(
+          [slim.conv2d],
+          weights_initializer=trunc_normal(0.005),
+          biases_initializer=tf.compat.v1.constant_initializer(0.1)):
         net = slim.conv2d(net, 3072, [6, 6], padding='VALID', scope='fc6')
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
@@ -115,16 +119,19 @@ def overfeat(inputs,
         end_points = slim.utils.convert_collection_to_dict(
             end_points_collection)
         if global_pool:
-          net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
           end_points['global_pool'] = net
         if num_classes:
           net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                              scope='dropout7')
-          net = slim.conv2d(net, num_classes, [1, 1],
-                            activation_fn=None,
-                            normalizer_fn=None,
-                            biases_initializer=tf.zeros_initializer(),
-                            scope='fc8')
+          net = slim.conv2d(
+              net,
+              num_classes, [1, 1],
+              activation_fn=None,
+              normalizer_fn=None,
+              biases_initializer=tf.compat.v1.zeros_initializer(),
+              scope='fc8')
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
           end_points[sc.name + '/fc8'] = net
diff --git a/research/slim/nets/overfeat_test.py b/research/slim/nets/overfeat_test.py
index fb3cb770..894df8e2 100644
--- a/research/slim/nets/overfeat_test.py
+++ b/research/slim/nets/overfeat_test.py
@@ -32,7 +32,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 231, 231
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(inputs, num_classes)
       self.assertEquals(logits.op.name, 'overfeat/fc8/squeezed')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -43,7 +43,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 281, 281
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False)
       self.assertEquals(logits.op.name, 'overfeat/fc8/BiasAdd')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -54,7 +54,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 281, 281
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(inputs, num_classes, spatial_squeeze=False,
                                     global_pool=True)
       self.assertEquals(logits.op.name, 'overfeat/fc8/BiasAdd')
@@ -66,7 +66,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 231, 231
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = overfeat.overfeat(inputs, num_classes)
       expected_names = ['overfeat/conv1',
                         'overfeat/pool1',
@@ -87,7 +87,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 231, 231
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, end_points = overfeat.overfeat(inputs, num_classes)
       expected_names = ['overfeat/conv1',
                         'overfeat/pool1',
@@ -108,7 +108,7 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 231, 231
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       overfeat.overfeat(inputs, num_classes)
       expected_names = ['overfeat/conv1/weights',
                         'overfeat/conv1/biases',
@@ -135,11 +135,11 @@ class OverFeatTest(tf.test.TestCase):
     height, width = 231, 231
     num_classes = 1000
     with self.test_session():
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(eval_inputs, is_training=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
-      predictions = tf.argmax(logits, 1)
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertListEqual(predictions.get_shape().as_list(), [batch_size])
 
   def testTrainEvalWithReuse(self):
@@ -149,29 +149,29 @@ class OverFeatTest(tf.test.TestCase):
     eval_height, eval_width = 281, 281
     num_classes = 1000
     with self.test_session():
-      train_inputs = tf.random_uniform(
+      train_inputs = tf.random.uniform(
           (train_batch_size, train_height, train_width, 3))
       logits, _ = overfeat.overfeat(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.get_variable_scope().reuse_variables()
-      eval_inputs = tf.random_uniform(
+      tf.compat.v1.get_variable_scope().reuse_variables()
+      eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = overfeat.overfeat(eval_inputs, is_training=False,
                                     spatial_squeeze=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [eval_batch_size, 2, 2, num_classes])
-      logits = tf.reduce_mean(logits, [1, 2])
-      predictions = tf.argmax(logits, 1)
+      logits = tf.reduce_mean(input_tensor=logits, axis=[1, 2])
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])
 
   def testForward(self):
     batch_size = 1
     height, width = 231, 231
     with self.test_session() as sess:
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(inputs)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/nets/pix2pix.py b/research/slim/nets/pix2pix.py
index 0b70f1c4..b393d653 100644
--- a/research/slim/nets/pix2pix.py
+++ b/research/slim/nets/pix2pix.py
@@ -58,7 +58,8 @@ def pix2pix_arg_scope():
       [layers.conv2d, layers.conv2d_transpose],
       normalizer_fn=layers.instance_norm,
       normalizer_params=instance_norm_params,
-      weights_initializer=tf.random_normal_initializer(0, 0.02)) as sc:
+      weights_initializer=tf.compat.v1.random_normal_initializer(0,
+                                                                 0.02)) as sc:
     return sc
 
 
@@ -80,13 +81,14 @@ def upsample(net, num_outputs, kernel_size, method='nn_upsample_conv'):
   Raises:
     ValueError: if `method` is not recognized.
   """
-  net_shape = tf.shape(net)
+  net_shape = tf.shape(input=net)
   height = net_shape[1]
   width = net_shape[2]
 
   if method == 'nn_upsample_conv':
-    net = tf.image.resize_nearest_neighbor(
-        net, [kernel_size[0] * height, kernel_size[1] * width])
+    net = tf.image.resize(
+        net, [kernel_size[0] * height, kernel_size[1] * width],
+        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
     net = layers.conv2d(net, num_outputs, [4, 4], activation_fn=None)
   elif method == 'conv2d_transpose':
     net = layers.conv2d_transpose(
@@ -166,7 +168,7 @@ def pix2pix_generator(net,
   ###########
   # Encoder #
   ###########
-  with tf.variable_scope('encoder'):
+  with tf.compat.v1.variable_scope('encoder'):
     with contrib_framework.arg_scope([layers.conv2d],
                                      kernel_size=[4, 4],
                                      stride=2,
@@ -194,7 +196,7 @@ def pix2pix_generator(net,
   reversed_blocks = list(blocks)
   reversed_blocks.reverse()
 
-  with tf.variable_scope('decoder'):
+  with tf.compat.v1.variable_scope('decoder'):
     # Dropout is used at both train and test time as per 'Image-to-Image',
     # Section 2.1 (last paragraph).
     with contrib_framework.arg_scope([layers.dropout], is_training=True):
@@ -210,7 +212,7 @@ def pix2pix_generator(net,
           net = layers.dropout(net, keep_prob=block.decoder_keep_prob)
         end_points['decoder%d' % block_id] = net
 
-  with tf.variable_scope('output'):
+  with tf.compat.v1.variable_scope('output'):
     # Explicitly set the normalizer_fn to None to override any default value
     # that may come from an arg_scope, such as pix2pix_arg_scope.
     logits = layers.conv2d(
@@ -249,11 +251,11 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
 
   def padded(net, scope):
     if padding:
-      with tf.variable_scope(scope):
+      with tf.compat.v1.variable_scope(scope):
         spatial_pad = tf.constant(
             [[0, 0], [padding, padding], [padding, padding], [0, 0]],
             dtype=tf.int32)
-        return tf.pad(net, spatial_pad, pad_mode)
+        return tf.pad(tensor=net, paddings=spatial_pad, mode=pad_mode)
     else:
       return net
 
diff --git a/research/slim/nets/pix2pix_test.py b/research/slim/nets/pix2pix_test.py
index 7e782cc0..d1bfa1b1 100644
--- a/research/slim/nets/pix2pix_test.py
+++ b/research/slim/nets/pix2pix_test.py
@@ -42,7 +42,7 @@ class GeneratorTest(tf.test.TestCase):
           upsample_method='nn_upsample_conv')
 
     with self.test_session() as session:
-      session.run(tf.global_variables_initializer())
+      session.run(tf.compat.v1.global_variables_initializer())
       np_outputs = session.run(logits)
       self.assertListEqual([batch_size, height, width, num_outputs],
                            list(np_outputs.shape))
@@ -59,7 +59,7 @@ class GeneratorTest(tf.test.TestCase):
           upsample_method='conv2d_transpose')
 
     with self.test_session() as session:
-      session.run(tf.global_variables_initializer())
+      session.run(tf.compat.v1.global_variables_initializer())
       np_outputs = session.run(logits)
       self.assertListEqual([batch_size, height, width, num_outputs],
                            list(np_outputs.shape))
diff --git a/research/slim/nets/post_training_quantization.py b/research/slim/nets/post_training_quantization.py
new file mode 100644
index 00000000..b2d6e3f6
--- /dev/null
+++ b/research/slim/nets/post_training_quantization.py
@@ -0,0 +1,181 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Export quantized tflite model from a trained checkpoint."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+from absl import app
+from absl import flags
+import tensorflow as tf
+import tensorflow_datasets as tfds
+from nets import nets_factory
+from preprocessing import preprocessing_factory
+
+flags.DEFINE_string("model_name", None,
+                    "The name of the architecture to quantize.")
+flags.DEFINE_string("checkpoint_path", None, "Path to the training checkpoint.")
+flags.DEFINE_string("dataset_name", "imagenet2012",
+                    "Name of the dataset to use for quantization calibration.")
+flags.DEFINE_string("dataset_dir", None, "Dataset location.")
+flags.DEFINE_string(
+    "dataset_split", "train",
+    "The dataset split (train, validation etc.) to use for calibration.")
+flags.DEFINE_string("output_tflite", None, "Path to output tflite file.")
+flags.DEFINE_boolean(
+    "use_model_specific_preprocessing", False,
+    "When true, uses the preprocessing corresponding to the model as specified "
+    "in preprocessing factory.")
+flags.DEFINE_boolean("enable_ema", True,
+                     "Load exponential moving average version of variables.")
+flags.DEFINE_integer(
+    "num_steps", 1000,
+    "Number of post-training quantization calibration steps to run.")
+flags.DEFINE_integer("image_size", 224, "Size of the input image.")
+flags.DEFINE_integer("num_classes", 1001,
+                     "Number of output classes for the model.")
+
+FLAGS = flags.FLAGS
+
+# Mean and standard deviation used for normalizing the image tensor.
+_MEAN_RGB = 127.5
+_STD_RGB = 127.5
+
+
+def _preprocess_for_quantization(image_data, image_size, crop_padding=32):
+  """Crops to center of image with padding then scales, normalizes image_size.
+
+  Args:
+    image_data: A 3D Tensor representing the RGB image data. Image can be of
+      arbitrary height and width.
+    image_size: image height/width dimension.
+    crop_padding: the padding size to use when centering the crop.
+
+  Returns:
+    A decoded and cropped image Tensor. Image is normalized to [-1,1].
+
+  """
+
+  shape = tf.shape(image_data)
+  image_height = shape[0]
+  image_width = shape[1]
+
+  padded_center_crop_size = tf.cast(
+      (image_size * 1.0 / (image_size + crop_padding)) *
+      tf.cast(tf.minimum(image_height, image_width), tf.float32), tf.int32)
+
+  offset_height = ((image_height - padded_center_crop_size) + 1) // 2
+  offset_width = ((image_width - padded_center_crop_size) + 1) // 2
+
+  image = tf.image.crop_to_bounding_box(
+      image_data,
+      offset_height=offset_height,
+      offset_width=offset_width,
+      target_height=padded_center_crop_size,
+      target_width=padded_center_crop_size)
+
+  image = tf.image.resize([image], [image_size, image_size],
+                          method=tf.image.ResizeMethod.BICUBIC)[0]
+  image = tf.cast(image, tf.float32)
+  image -= tf.constant(_MEAN_RGB)
+  image /= tf.constant(_STD_RGB)
+  return image
+
+
+def restore_model(sess, checkpoint_path, enable_ema=True):
+  """Restore variables from the checkpoint into the provided session.
+
+  Args:
+    sess: A tensorflow session where the checkpoint will be loaded.
+    checkpoint_path: Path to the trained checkpoint.
+    enable_ema: (optional) Whether to load the exponential moving average (ema)
+      version of the tensorflow variables. Defaults to True.
+  """
+  if enable_ema:
+    ema = tf.train.ExponentialMovingAverage(decay=0.0)
+    ema_vars = tf.trainable_variables() + tf.get_collection("moving_vars")
+    for v in tf.global_variables():
+      if "moving_mean" in v.name or "moving_variance" in v.name:
+        ema_vars.append(v)
+    ema_vars = list(set(ema_vars))
+    var_dict = ema.variables_to_restore(ema_vars)
+  else:
+    var_dict = None
+
+  sess.run(tf.global_variables_initializer())
+  saver = tf.train.Saver(var_dict, max_to_keep=1)
+  saver.restore(sess, checkpoint_path)
+
+
+def _representative_dataset_gen():
+  """Gets a python generator of numpy arrays for the given dataset."""
+  image_size = FLAGS.image_size
+  dataset = tfds.builder(FLAGS.dataset_name, data_dir=FLAGS.dataset_dir)
+  dataset.download_and_prepare()
+  data = dataset.as_dataset()[FLAGS.dataset_split]
+  iterator = tf.compat.v1.data.make_one_shot_iterator(data)
+  if FLAGS.use_model_specific_preprocessing:
+    preprocess_fn = functools.partial(
+        preprocessing_factory.get_preprocessing(name=FLAGS.model_name),
+        output_height=image_size,
+        output_width=image_size)
+  else:
+    preprocess_fn = functools.partial(
+        _preprocess_for_quantization, image_size=image_size)
+  features = iterator.get_next()
+  image = features["image"]
+  image = preprocess_fn(image)
+  image = tf.reshape(image, [1, image_size, image_size, 3])
+  for _ in range(FLAGS.num_steps):
+    yield [image.eval()]
+
+
+def main(_):
+  with tf.Graph().as_default(), tf.Session() as sess:
+    network_fn = nets_factory.get_network_fn(
+        FLAGS.model_name, num_classes=FLAGS.num_classes, is_training=False)
+    image_size = FLAGS.image_size
+    images = tf.placeholder(
+        tf.float32, shape=(1, image_size, image_size, 3), name="images")
+
+    logits, _ = network_fn(images)
+
+    output_tensor = tf.nn.softmax(logits)
+    restore_model(sess, FLAGS.checkpoint_path, enable_ema=FLAGS.enable_ema)
+
+    converter = tf.lite.TFLiteConverter.from_session(sess, [images],
+                                                     [output_tensor])
+
+    converter.representative_dataset = tf.lite.RepresentativeDataset(
+        _representative_dataset_gen)
+    converter.optimizations = [tf.lite.Optimize.DEFAULT]
+    converter.inference_input_type = tf.int8
+    converter.inference_output_type = tf.int8
+    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
+
+    tflite_buffer = converter.convert()
+    with tf.gfile.GFile(FLAGS.output_tflite, "wb") as output_tflite:
+      output_tflite.write(tflite_buffer)
+  print("tflite model written to %s" % FLAGS.output_tflite)
+
+
+if __name__ == "__main__":
+  flags.mark_flag_as_required("model_name")
+  flags.mark_flag_as_required("checkpoint_path")
+  flags.mark_flag_as_required("dataset_dir")
+  flags.mark_flag_as_required("output_tflite")
+  app.run(main)
diff --git a/research/slim/nets/resnet_utils.py b/research/slim/nets/resnet_utils.py
index 599d1d03..3230e280 100644
--- a/research/slim/nets/resnet_utils.py
+++ b/research/slim/nets/resnet_utils.py
@@ -117,8 +117,9 @@ def conv2d_same(inputs, num_outputs, kernel_size, stride, rate=1, scope=None):
     pad_total = kernel_size_effective - 1
     pad_beg = pad_total // 2
     pad_end = pad_total - pad_beg
-    inputs = tf.pad(inputs,
-                    [[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])
+    inputs = tf.pad(
+        tensor=inputs,
+        paddings=[[0, 0], [pad_beg, pad_end], [pad_beg, pad_end], [0, 0]])
     return slim.conv2d(inputs, num_outputs, kernel_size, stride=stride,
                        rate=rate, padding='VALID', scope=scope)
 
@@ -180,7 +181,7 @@ def stack_blocks_dense(net, blocks, output_stride=None,
   rate = 1
 
   for block in blocks:
-    with tf.variable_scope(block.scope, 'block', [net]) as sc:
+    with tf.compat.v1.variable_scope(block.scope, 'block', [net]) as sc:
       block_stride = 1
       for i, unit in enumerate(block.args):
         if store_non_strided_activations and i == len(block.args) - 1:
@@ -188,7 +189,7 @@ def stack_blocks_dense(net, blocks, output_stride=None,
           block_stride = unit.get('stride', 1)
           unit = dict(unit, stride=1)
 
-        with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
+        with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
           # If we have reached the target output_stride, then we need to employ
           # atrous convolution with stride=1 and multiply the atrous rate by the
           # current unit's stride for use in subsequent layers.
@@ -220,13 +221,14 @@ def stack_blocks_dense(net, blocks, output_stride=None,
   return net
 
 
-def resnet_arg_scope(weight_decay=0.0001,
-                     batch_norm_decay=0.997,
-                     batch_norm_epsilon=1e-5,
-                     batch_norm_scale=True,
-                     activation_fn=tf.nn.relu,
-                     use_batch_norm=True,
-                     batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):
+def resnet_arg_scope(
+    weight_decay=0.0001,
+    batch_norm_decay=0.997,
+    batch_norm_epsilon=1e-5,
+    batch_norm_scale=True,
+    activation_fn=tf.nn.relu,
+    use_batch_norm=True,
+    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS):
   """Defines the default ResNet arg scope.
 
   TODO(gpapan): The batch-normalization related default values above are
diff --git a/research/slim/nets/resnet_v1.py b/research/slim/nets/resnet_v1.py
index 148a7217..8451f466 100644
--- a/research/slim/nets/resnet_v1.py
+++ b/research/slim/nets/resnet_v1.py
@@ -109,7 +109,7 @@ def bottleneck(inputs,
   Returns:
     The ResNet unit's output.
   """
-  with tf.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:
+  with tf.compat.v1.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:
     depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
     if depth == depth_in:
       shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')
@@ -219,7 +219,8 @@ def resnet_v1(inputs,
   Raises:
     ValueError: If the target output_stride is not valid.
   """
-  with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
+  with tf.compat.v1.variable_scope(
+      scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
                          resnet_utils.stack_blocks_dense],
@@ -242,7 +243,8 @@ def resnet_v1(inputs,
 
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], name='pool5', keepdims=True)
           end_points['global_pool'] = net
         if num_classes:
           net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
diff --git a/research/slim/nets/resnet_v1_test.py b/research/slim/nets/resnet_v1_test.py
index 91df7f4c..fb70a5a3 100644
--- a/research/slim/nets/resnet_v1_test.py
+++ b/research/slim/nets/resnet_v1_test.py
@@ -27,6 +27,8 @@ from nets import resnet_v1
 
 slim = contrib_slim
 
+tf.compat.v1.disable_resource_variables()
+
 
 def create_test_input(batch_size, height, width, channels):
   """Create test input tensor.
@@ -43,28 +45,29 @@ def create_test_input(batch_size, height, width, channels):
     constant `Tensor` with the mesh grid values along the spatial dimensions.
   """
   if None in [batch_size, height, width, channels]:
-    return tf.placeholder(tf.float32, (batch_size, height, width, channels))
+    return tf.compat.v1.placeholder(tf.float32,
+                                    (batch_size, height, width, channels))
   else:
-    return tf.to_float(
+    return tf.cast(
         np.tile(
             np.reshape(
                 np.reshape(np.arange(height), [height, 1]) +
                 np.reshape(np.arange(width), [1, width]),
-                [1, height, width, 1]),
-            [batch_size, 1, 1, channels]))
+                [1, height, width, 1]), [batch_size, 1, 1, channels]),
+        dtype=tf.float32)
 
 
 class ResnetUtilsTest(tf.test.TestCase):
 
   def testSubsampleThreeByThree(self):
-    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])
+    x = tf.reshape(tf.cast(tf.range(9), dtype=tf.float32), [1, 3, 3, 1])
     x = resnet_utils.subsample(x, 2)
     expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])
     with self.test_session():
       self.assertAllClose(x.eval(), expected.eval())
 
   def testSubsampleFourByFour(self):
-    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])
+    x = tf.reshape(tf.cast(tf.range(16), dtype=tf.float32), [1, 4, 4, 1])
     x = resnet_utils.subsample(x, 2)
     expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])
     with self.test_session():
@@ -80,32 +83,29 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.get_variable('Conv/weights', initializer=w)
-    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.get_variable_scope().reuse_variables()
+    tf.compat.v1.get_variable('Conv/weights', initializer=w)
+    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.compat.v1.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
-    y1_expected = tf.to_float([[14, 28, 43, 26],
-                               [28, 48, 66, 37],
-                               [43, 66, 84, 46],
-                               [26, 37, 46, 22]])
+    y1_expected = tf.cast([[14, 28, 43, 26], [28, 48, 66, 37], [43, 66, 84, 46],
+                           [26, 37, 46, 22]],
+                          dtype=tf.float32)
     y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
 
     y2 = resnet_utils.subsample(y1, 2)
-    y2_expected = tf.to_float([[14, 43],
-                               [43, 84]])
+    y2_expected = tf.cast([[14, 43], [43, 84]], dtype=tf.float32)
     y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
 
     y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope='Conv')
     y3_expected = y2_expected
 
     y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope='Conv')
-    y4_expected = tf.to_float([[48, 37],
-                               [37, 22]])
+    y4_expected = tf.cast([[48, 37], [37, 22]], dtype=tf.float32)
     y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -121,22 +121,20 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.get_variable('Conv/weights', initializer=w)
-    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.get_variable_scope().reuse_variables()
+    tf.compat.v1.get_variable('Conv/weights', initializer=w)
+    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.compat.v1.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
-    y1_expected = tf.to_float([[14, 28, 43, 58, 34],
-                               [28, 48, 66, 84, 46],
-                               [43, 66, 84, 102, 55],
-                               [58, 84, 102, 120, 64],
-                               [34, 46, 55, 64, 30]])
+    y1_expected = tf.cast(
+        [[14, 28, 43, 58, 34], [28, 48, 66, 84, 46], [43, 66, 84, 102, 55],
+         [58, 84, 102, 120, 64], [34, 46, 55, 64, 30]],
+        dtype=tf.float32)
     y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
 
     y2 = resnet_utils.subsample(y1, 2)
-    y2_expected = tf.to_float([[14, 43, 34],
-                               [43, 84, 55],
-                               [34, 55, 30]])
+    y2_expected = tf.cast([[14, 43, 34], [43, 84, 55], [34, 55, 30]],
+                          dtype=tf.float32)
     y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
 
     y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope='Conv')
@@ -146,7 +144,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = y2_expected
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -154,7 +152,7 @@ class ResnetUtilsTest(tf.test.TestCase):
 
   def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):
     """A plain ResNet without extra layers before or after the ResNet blocks."""
-    with tf.variable_scope(scope, values=[inputs]):
+    with tf.compat.v1.variable_scope(scope, values=[inputs]):
       with slim.arg_scope([slim.conv2d], outputs_collections='end_points'):
         net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)
         end_points = slim.utils.convert_collection_to_dict('end_points')
@@ -191,9 +189,9 @@ class ResnetUtilsTest(tf.test.TestCase):
   def _stack_blocks_nondense(self, net, blocks):
     """A simplified ResNet Block stacker without output stride control."""
     for block in blocks:
-      with tf.variable_scope(block.scope, 'block', [net]):
+      with tf.compat.v1.variable_scope(block.scope, 'block', [net]):
         for i, unit in enumerate(block.args):
-          with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
+          with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
             net = block.unit_fn(net, rate=1, **unit)
     return net
 
@@ -221,7 +219,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.set_random_seed(0)
+              tf.compat.v1.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
               # Dense feature extraction followed by subsampling.
               output = resnet_utils.stack_blocks_dense(inputs,
@@ -234,10 +232,10 @@ class ResnetUtilsTest(tf.test.TestCase):
 
               output = resnet_utils.subsample(output, factor)
               # Make the two networks use the same weights.
-              tf.get_variable_scope().reuse_variables()
+              tf.compat.v1.get_variable_scope().reuse_variables()
               # Feature extraction at the nominal network rate.
               expected = self._stack_blocks_nondense(inputs, blocks)
-              sess.run(tf.global_variables_initializer())
+              sess.run(tf.compat.v1.global_variables_initializer())
               output, expected = sess.run([output, expected])
               self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)
 
@@ -264,7 +262,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.set_random_seed(0)
+              tf.compat.v1.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
 
               # Subsampling at the last unit of the block.
@@ -276,7 +274,7 @@ class ResnetUtilsTest(tf.test.TestCase):
                   'output')
 
               # Make the two networks use the same weights.
-              tf.get_variable_scope().reuse_variables()
+              tf.compat.v1.get_variable_scope().reuse_variables()
 
               # Subsample activations at the end of the blocks.
               expected = resnet_utils.stack_blocks_dense(
@@ -286,7 +284,7 @@ class ResnetUtilsTest(tf.test.TestCase):
               expected_end_points = slim.utils.convert_collection_to_dict(
                   'expected')
 
-              sess.run(tf.global_variables_initializer())
+              sess.run(tf.compat.v1.global_variables_initializer())
 
               # Make sure that the final output is the same.
               output, expected = sess.run([output, expected])
@@ -477,7 +475,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
       with slim.arg_scope(resnet_utils.resnet_arg_scope()):
         with tf.Graph().as_default():
           with self.test_session() as sess:
-            tf.set_random_seed(0)
+            tf.compat.v1.set_random_seed(0)
             inputs = create_test_input(2, 81, 81, 3)
             # Dense feature extraction followed by subsampling.
             output, _ = self._resnet_small(inputs, None, is_training=False,
@@ -489,11 +487,11 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
               factor = nominal_stride // output_stride
             output = resnet_utils.subsample(output, factor)
             # Make the two networks use the same weights.
-            tf.get_variable_scope().reuse_variables()
+            tf.compat.v1.get_variable_scope().reuse_variables()
             # Feature extraction at the nominal network rate.
             expected, _ = self._resnet_small(inputs, None, is_training=False,
                                              global_pool=False)
-            sess.run(tf.global_variables_initializer())
+            sess.run(tf.compat.v1.global_variables_initializer())
             self.assertAllClose(output.eval(), expected.eval(),
                                 atol=1e-4, rtol=1e-4)
 
@@ -513,7 +511,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [None, 1, 1, num_classes])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 1, 1, num_classes))
 
@@ -528,7 +526,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 3, 3, 32))
 
@@ -547,7 +545,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 9, 9, 32))
 
diff --git a/research/slim/nets/resnet_v2.py b/research/slim/nets/resnet_v2.py
index 07a9e5a6..b08af074 100644
--- a/research/slim/nets/resnet_v2.py
+++ b/research/slim/nets/resnet_v2.py
@@ -84,7 +84,7 @@ def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,
   Returns:
     The ResNet unit's output.
   """
-  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:
+  with tf.compat.v1.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:
     depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
     preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')
     if depth == depth_in:
@@ -181,7 +181,8 @@ def resnet_v2(inputs,
   Raises:
     ValueError: If the target output_stride is not valid.
   """
-  with tf.variable_scope(scope, 'resnet_v2', [inputs], reuse=reuse) as sc:
+  with tf.compat.v1.variable_scope(
+      scope, 'resnet_v2', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
                          resnet_utils.stack_blocks_dense],
@@ -211,7 +212,8 @@ def resnet_v2(inputs,
 
         if global_pool:
           # Global average pooling.
-          net = tf.reduce_mean(net, [1, 2], name='pool5', keep_dims=True)
+          net = tf.reduce_mean(
+              input_tensor=net, axis=[1, 2], name='pool5', keepdims=True)
           end_points['global_pool'] = net
         if num_classes:
           net = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,
diff --git a/research/slim/nets/resnet_v2_test.py b/research/slim/nets/resnet_v2_test.py
index 7e39b184..5ca52274 100644
--- a/research/slim/nets/resnet_v2_test.py
+++ b/research/slim/nets/resnet_v2_test.py
@@ -27,6 +27,8 @@ from nets import resnet_v2
 
 slim = contrib_slim
 
+tf.compat.v1.disable_resource_variables()
+
 
 def create_test_input(batch_size, height, width, channels):
   """Create test input tensor.
@@ -43,28 +45,29 @@ def create_test_input(batch_size, height, width, channels):
     constant `Tensor` with the mesh grid values along the spatial dimensions.
   """
   if None in [batch_size, height, width, channels]:
-    return tf.placeholder(tf.float32, (batch_size, height, width, channels))
+    return tf.compat.v1.placeholder(tf.float32,
+                                    (batch_size, height, width, channels))
   else:
-    return tf.to_float(
+    return tf.cast(
         np.tile(
             np.reshape(
                 np.reshape(np.arange(height), [height, 1]) +
                 np.reshape(np.arange(width), [1, width]),
-                [1, height, width, 1]),
-            [batch_size, 1, 1, channels]))
+                [1, height, width, 1]), [batch_size, 1, 1, channels]),
+        dtype=tf.float32)
 
 
 class ResnetUtilsTest(tf.test.TestCase):
 
   def testSubsampleThreeByThree(self):
-    x = tf.reshape(tf.to_float(tf.range(9)), [1, 3, 3, 1])
+    x = tf.reshape(tf.cast(tf.range(9), dtype=tf.float32), [1, 3, 3, 1])
     x = resnet_utils.subsample(x, 2)
     expected = tf.reshape(tf.constant([0, 2, 6, 8]), [1, 2, 2, 1])
     with self.test_session():
       self.assertAllClose(x.eval(), expected.eval())
 
   def testSubsampleFourByFour(self):
-    x = tf.reshape(tf.to_float(tf.range(16)), [1, 4, 4, 1])
+    x = tf.reshape(tf.cast(tf.range(16), dtype=tf.float32), [1, 4, 4, 1])
     x = resnet_utils.subsample(x, 2)
     expected = tf.reshape(tf.constant([0, 2, 8, 10]), [1, 2, 2, 1])
     with self.test_session():
@@ -80,32 +83,29 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.get_variable('Conv/weights', initializer=w)
-    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.get_variable_scope().reuse_variables()
+    tf.compat.v1.get_variable('Conv/weights', initializer=w)
+    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.compat.v1.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
-    y1_expected = tf.to_float([[14, 28, 43, 26],
-                               [28, 48, 66, 37],
-                               [43, 66, 84, 46],
-                               [26, 37, 46, 22]])
+    y1_expected = tf.cast([[14, 28, 43, 26], [28, 48, 66, 37], [43, 66, 84, 46],
+                           [26, 37, 46, 22]],
+                          dtype=tf.float32)
     y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
 
     y2 = resnet_utils.subsample(y1, 2)
-    y2_expected = tf.to_float([[14, 43],
-                               [43, 84]])
+    y2_expected = tf.cast([[14, 43], [43, 84]], dtype=tf.float32)
     y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
 
     y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope='Conv')
     y3_expected = y2_expected
 
     y4 = slim.conv2d(x, 1, [3, 3], stride=2, scope='Conv')
-    y4_expected = tf.to_float([[48, 37],
-                               [37, 22]])
+    y4_expected = tf.cast([[48, 37], [37, 22]], dtype=tf.float32)
     y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -121,22 +121,20 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.get_variable('Conv/weights', initializer=w)
-    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.get_variable_scope().reuse_variables()
+    tf.compat.v1.get_variable('Conv/weights', initializer=w)
+    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.compat.v1.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
-    y1_expected = tf.to_float([[14, 28, 43, 58, 34],
-                               [28, 48, 66, 84, 46],
-                               [43, 66, 84, 102, 55],
-                               [58, 84, 102, 120, 64],
-                               [34, 46, 55, 64, 30]])
+    y1_expected = tf.cast(
+        [[14, 28, 43, 58, 34], [28, 48, 66, 84, 46], [43, 66, 84, 102, 55],
+         [58, 84, 102, 120, 64], [34, 46, 55, 64, 30]],
+        dtype=tf.float32)
     y1_expected = tf.reshape(y1_expected, [1, n, n, 1])
 
     y2 = resnet_utils.subsample(y1, 2)
-    y2_expected = tf.to_float([[14, 43, 34],
-                               [43, 84, 55],
-                               [34, 55, 30]])
+    y2_expected = tf.cast([[14, 43, 34], [43, 84, 55], [34, 55, 30]],
+                          dtype=tf.float32)
     y2_expected = tf.reshape(y2_expected, [1, n2, n2, 1])
 
     y3 = resnet_utils.conv2d_same(x, 1, 3, stride=2, scope='Conv')
@@ -146,7 +144,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = y2_expected
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -154,7 +152,7 @@ class ResnetUtilsTest(tf.test.TestCase):
 
   def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):
     """A plain ResNet without extra layers before or after the ResNet blocks."""
-    with tf.variable_scope(scope, values=[inputs]):
+    with tf.compat.v1.variable_scope(scope, values=[inputs]):
       with slim.arg_scope([slim.conv2d], outputs_collections='end_points'):
         net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)
         end_points = slim.utils.convert_collection_to_dict('end_points')
@@ -191,9 +189,9 @@ class ResnetUtilsTest(tf.test.TestCase):
   def _stack_blocks_nondense(self, net, blocks):
     """A simplified ResNet Block stacker without output stride control."""
     for block in blocks:
-      with tf.variable_scope(block.scope, 'block', [net]):
+      with tf.compat.v1.variable_scope(block.scope, 'block', [net]):
         for i, unit in enumerate(block.args):
-          with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
+          with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
             net = block.unit_fn(net, rate=1, **unit)
     return net
 
@@ -221,7 +219,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.set_random_seed(0)
+              tf.compat.v1.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
               # Dense feature extraction followed by subsampling.
               output = resnet_utils.stack_blocks_dense(inputs,
@@ -234,10 +232,10 @@ class ResnetUtilsTest(tf.test.TestCase):
 
               output = resnet_utils.subsample(output, factor)
               # Make the two networks use the same weights.
-              tf.get_variable_scope().reuse_variables()
+              tf.compat.v1.get_variable_scope().reuse_variables()
               # Feature extraction at the nominal network rate.
               expected = self._stack_blocks_nondense(inputs, blocks)
-              sess.run(tf.global_variables_initializer())
+              sess.run(tf.compat.v1.global_variables_initializer())
               output, expected = sess.run([output, expected])
               self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)
 
@@ -394,7 +392,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
       with slim.arg_scope(resnet_utils.resnet_arg_scope()):
         with tf.Graph().as_default():
           with self.test_session() as sess:
-            tf.set_random_seed(0)
+            tf.compat.v1.set_random_seed(0)
             inputs = create_test_input(2, 81, 81, 3)
             # Dense feature extraction followed by subsampling.
             output, _ = self._resnet_small(inputs, None,
@@ -407,12 +405,12 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
               factor = nominal_stride // output_stride
             output = resnet_utils.subsample(output, factor)
             # Make the two networks use the same weights.
-            tf.get_variable_scope().reuse_variables()
+            tf.compat.v1.get_variable_scope().reuse_variables()
             # Feature extraction at the nominal network rate.
             expected, _ = self._resnet_small(inputs, None,
                                              is_training=False,
                                              global_pool=False)
-            sess.run(tf.global_variables_initializer())
+            sess.run(tf.compat.v1.global_variables_initializer())
             self.assertAllClose(output.eval(), expected.eval(),
                                 atol=1e-4, rtol=1e-4)
 
@@ -432,7 +430,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [None, 1, 1, num_classes])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 1, 1, num_classes))
 
@@ -448,7 +446,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 3, 3, 32))
 
@@ -467,7 +465,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 9, 9, 32))
 
diff --git a/research/slim/nets/s3dg.py b/research/slim/nets/s3dg.py
index 9fa8b423..3f443ad4 100644
--- a/research/slim/nets/s3dg.py
+++ b/research/slim/nets/s3dg.py
@@ -30,7 +30,9 @@ from tensorflow.contrib import layers as contrib_layers
 
 from nets import i3d_utils
 
-trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)
+# pylint: disable=g-long-lambda
+trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+    0.0, stddev)
 conv3d_spatiotemporal = i3d_utils.conv3d_spatiotemporal
 inception_block_v1_3d = i3d_utils.inception_block_v1_3d
 
@@ -209,7 +211,7 @@ def s3dg_base(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.variable_scope(scope, 'InceptionV1', [inputs]):
+  with tf.compat.v1.variable_scope(scope, 'InceptionV1', [inputs]):
     with arg_scope([layers.conv3d], weights_initializer=trunc_normal(0.01)):
       with arg_scope(
           [layers.conv3d, layers.max_pool3d, conv3d_spatiotemporal],
@@ -556,7 +558,7 @@ def s3dg(inputs,
   """
   assert data_format in ['NDHWC', 'NCDHW']
   # Final pooling and prediction
-  with tf.variable_scope(
+  with tf.compat.v1.variable_scope(
       scope, 'InceptionV1', [inputs, num_classes], reuse=reuse) as scope:
     with arg_scope(
         [layers.batch_norm, layers.dropout], is_training=is_training):
@@ -570,9 +572,9 @@ def s3dg(inputs,
           depth_multiplier=depth_multiplier,
           data_format=data_format,
           scope=scope)
-      with tf.variable_scope('Logits'):
+      with tf.compat.v1.variable_scope('Logits'):
         if data_format.startswith('NC'):
-          net = tf.transpose(net, [0, 2, 3, 4, 1])
+          net = tf.transpose(a=net, perm=[0, 2, 3, 4, 1])
         kernel_size = i3d_utils.reduced_kernel_size_3d(net, [2, 7, 7])
         net = layers.avg_pool3d(
             net,
@@ -589,7 +591,7 @@ def s3dg(inputs,
             data_format='NDHWC',
             scope='Conv2d_0c_1x1')
         # Temporal average pooling.
-        logits = tf.reduce_mean(logits, axis=1)
+        logits = tf.reduce_mean(input_tensor=logits, axis=1)
         if spatial_squeeze:
           logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')
 
diff --git a/research/slim/nets/s3dg_test.py b/research/slim/nets/s3dg_test.py
index 1758797e..c3dc57c1 100644
--- a/research/slim/nets/s3dg_test.py
+++ b/research/slim/nets/s3dg_test.py
@@ -31,7 +31,7 @@ class S3DGTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     logits, end_points = s3dg.s3dg(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -45,7 +45,7 @@ class S3DGTest(tf.test.TestCase):
     num_frames = 64
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_6c, end_points = s3dg.s3dg_base(inputs)
     self.assertTrue(mixed_6c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_6c.get_shape().as_list(),
@@ -68,7 +68,7 @@ class S3DGTest(tf.test.TestCase):
                  'Mixed_5c']
     for index, endpoint in enumerate(endpoints):
       with tf.Graph().as_default():
-        inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+        inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
         out_tensor, end_points = s3dg.s3dg_base(
             inputs, final_endpoint=endpoint, gating_startat=None)
         print(endpoint, out_tensor.op.name)
@@ -81,7 +81,7 @@ class S3DGTest(tf.test.TestCase):
     num_frames = 64
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     _, end_points = s3dg.s3dg_base(inputs,
                                    final_endpoint='Mixed_5c')
     endpoints_shapes = {'Conv2d_1a_7x7': [5, 32, 112, 112, 64],
@@ -112,7 +112,7 @@ class S3DGTest(tf.test.TestCase):
     num_frames = 64
     height, width = 112, 112
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_5c, _ = s3dg.s3dg_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -123,7 +123,7 @@ class S3DGTest(tf.test.TestCase):
     num_frames = 10
     height, width = 224, 224
 
-    inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     mixed_5c, _ = s3dg.s3dg_base(inputs)
     self.assertTrue(mixed_5c.op.name.startswith('InceptionV1/Mixed_5c'))
     self.assertListEqual(mixed_5c.get_shape().as_list(),
@@ -135,13 +135,13 @@ class S3DGTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    eval_inputs = tf.random_uniform((batch_size, num_frames, height, width, 3))
+    eval_inputs = tf.random.uniform((batch_size, num_frames, height, width, 3))
     logits, _ = s3dg.s3dg(eval_inputs, num_classes,
                           is_training=False)
-    predictions = tf.argmax(logits, 1)
+    predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
diff --git a/research/slim/nets/vgg.py b/research/slim/nets/vgg.py
index 960931db..2f30a744 100644
--- a/research/slim/nets/vgg.py
+++ b/research/slim/nets/vgg.py
@@ -59,7 +59,7 @@ def vgg_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.zeros_initializer()):
+                      biases_initializer=tf.compat.v1.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:
       return arg_sc
 
@@ -105,7 +105,7 @@ def vgg_a(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
+  with tf.compat.v1.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.max_pool2d],
@@ -129,7 +129,8 @@ def vgg_a(inputs,
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
       if global_pool:
-        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        net = tf.reduce_mean(
+            input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
         end_points['global_pool'] = net
       if num_classes:
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
@@ -186,7 +187,8 @@ def vgg_16(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_16', [inputs], reuse=reuse) as sc:
+  with tf.compat.v1.variable_scope(
+      scope, 'vgg_16', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -210,7 +212,8 @@ def vgg_16(inputs,
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
       if global_pool:
-        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        net = tf.reduce_mean(
+            input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
         end_points['global_pool'] = net
       if num_classes:
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
@@ -268,7 +271,8 @@ def vgg_19(inputs,
       None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.variable_scope(scope, 'vgg_19', [inputs], reuse=reuse) as sc:
+  with tf.compat.v1.variable_scope(
+      scope, 'vgg_19', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -292,7 +296,8 @@ def vgg_19(inputs,
       # Convert end_points_collection into a end_point dict.
       end_points = slim.utils.convert_collection_to_dict(end_points_collection)
       if global_pool:
-        net = tf.reduce_mean(net, [1, 2], keep_dims=True, name='global_pool')
+        net = tf.reduce_mean(
+            input_tensor=net, axis=[1, 2], keepdims=True, name='global_pool')
         end_points['global_pool'] = net
       if num_classes:
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
diff --git a/research/slim/nets/vgg_test.py b/research/slim/nets/vgg_test.py
index 3a2e0c0c..988c3dbe 100644
--- a/research/slim/nets/vgg_test.py
+++ b/research/slim/nets/vgg_test.py
@@ -32,7 +32,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(inputs, num_classes)
       self.assertEquals(logits.op.name, 'vgg_a/fc8/squeezed')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -43,7 +43,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False)
       self.assertEquals(logits.op.name, 'vgg_a/fc8/BiasAdd')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -54,7 +54,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(inputs, num_classes, spatial_squeeze=False,
                             global_pool=True)
       self.assertEquals(logits.op.name, 'vgg_a/fc8/BiasAdd')
@@ -66,7 +66,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = vgg.vgg_a(inputs, num_classes)
       expected_names = ['vgg_a/conv1/conv1_1',
                         'vgg_a/pool1',
@@ -92,7 +92,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, end_points = vgg.vgg_a(inputs, num_classes)
       expected_names = ['vgg_a/conv1/conv1_1',
                         'vgg_a/pool1',
@@ -118,7 +118,7 @@ class VGGATest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       vgg.vgg_a(inputs, num_classes)
       expected_names = ['vgg_a/conv1/conv1_1/weights',
                         'vgg_a/conv1/conv1_1/biases',
@@ -151,11 +151,11 @@ class VGGATest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(eval_inputs, is_training=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
-      predictions = tf.argmax(logits, 1)
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertListEqual(predictions.get_shape().as_list(), [batch_size])
 
   def testTrainEvalWithReuse(self):
@@ -165,29 +165,29 @@ class VGGATest(tf.test.TestCase):
     eval_height, eval_width = 256, 256
     num_classes = 1000
     with self.test_session():
-      train_inputs = tf.random_uniform(
+      train_inputs = tf.random.uniform(
           (train_batch_size, train_height, train_width, 3))
       logits, _ = vgg.vgg_a(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.get_variable_scope().reuse_variables()
-      eval_inputs = tf.random_uniform(
+      tf.compat.v1.get_variable_scope().reuse_variables()
+      eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_a(eval_inputs, is_training=False,
                             spatial_squeeze=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [eval_batch_size, 2, 2, num_classes])
-      logits = tf.reduce_mean(logits, [1, 2])
-      predictions = tf.argmax(logits, 1)
+      logits = tf.reduce_mean(input_tensor=logits, axis=[1, 2])
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])
 
   def testForward(self):
     batch_size = 1
     height, width = 224, 224
     with self.test_session() as sess:
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(inputs)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
@@ -199,7 +199,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(inputs, num_classes)
       self.assertEquals(logits.op.name, 'vgg_16/fc8/squeezed')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -210,7 +210,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False)
       self.assertEquals(logits.op.name, 'vgg_16/fc8/BiasAdd')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -221,7 +221,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(inputs, num_classes, spatial_squeeze=False,
                              global_pool=True)
       self.assertEquals(logits.op.name, 'vgg_16/fc8/BiasAdd')
@@ -233,7 +233,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = vgg.vgg_16(inputs, num_classes)
       expected_names = ['vgg_16/conv1/conv1_1',
                         'vgg_16/conv1/conv1_2',
@@ -264,7 +264,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, end_points = vgg.vgg_16(inputs, num_classes)
       expected_names = ['vgg_16/conv1/conv1_1',
                         'vgg_16/conv1/conv1_2',
@@ -295,7 +295,7 @@ class VGG16Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       vgg.vgg_16(inputs, num_classes)
       expected_names = ['vgg_16/conv1/conv1_1/weights',
                         'vgg_16/conv1/conv1_1/biases',
@@ -338,11 +338,11 @@ class VGG16Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(eval_inputs, is_training=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
-      predictions = tf.argmax(logits, 1)
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertListEqual(predictions.get_shape().as_list(), [batch_size])
 
   def testTrainEvalWithReuse(self):
@@ -352,29 +352,29 @@ class VGG16Test(tf.test.TestCase):
     eval_height, eval_width = 256, 256
     num_classes = 1000
     with self.test_session():
-      train_inputs = tf.random_uniform(
+      train_inputs = tf.random.uniform(
           (train_batch_size, train_height, train_width, 3))
       logits, _ = vgg.vgg_16(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.get_variable_scope().reuse_variables()
-      eval_inputs = tf.random_uniform(
+      tf.compat.v1.get_variable_scope().reuse_variables()
+      eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_16(eval_inputs, is_training=False,
                              spatial_squeeze=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [eval_batch_size, 2, 2, num_classes])
-      logits = tf.reduce_mean(logits, [1, 2])
-      predictions = tf.argmax(logits, 1)
+      logits = tf.reduce_mean(input_tensor=logits, axis=[1, 2])
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])
 
   def testForward(self):
     batch_size = 1
     height, width = 224, 224
     with self.test_session() as sess:
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(inputs)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
@@ -386,7 +386,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(inputs, num_classes)
       self.assertEquals(logits.op.name, 'vgg_19/fc8/squeezed')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -397,7 +397,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False)
       self.assertEquals(logits.op.name, 'vgg_19/fc8/BiasAdd')
       self.assertListEqual(logits.get_shape().as_list(),
@@ -408,7 +408,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 256, 256
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(inputs, num_classes, spatial_squeeze=False,
                              global_pool=True)
       self.assertEquals(logits.op.name, 'vgg_19/fc8/BiasAdd')
@@ -420,7 +420,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       _, end_points = vgg.vgg_19(inputs, num_classes)
       expected_names = [
           'vgg_19/conv1/conv1_1',
@@ -455,7 +455,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       net, end_points = vgg.vgg_19(inputs, num_classes)
       expected_names = [
           'vgg_19/conv1/conv1_1',
@@ -490,7 +490,7 @@ class VGG19Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       vgg.vgg_19(inputs, num_classes)
       expected_names = [
           'vgg_19/conv1/conv1_1/weights',
@@ -540,11 +540,11 @@ class VGG19Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session():
-      eval_inputs = tf.random_uniform((batch_size, height, width, 3))
+      eval_inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(eval_inputs, is_training=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
-      predictions = tf.argmax(logits, 1)
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertListEqual(predictions.get_shape().as_list(), [batch_size])
 
   def testTrainEvalWithReuse(self):
@@ -554,29 +554,29 @@ class VGG19Test(tf.test.TestCase):
     eval_height, eval_width = 256, 256
     num_classes = 1000
     with self.test_session():
-      train_inputs = tf.random_uniform(
+      train_inputs = tf.random.uniform(
           (train_batch_size, train_height, train_width, 3))
       logits, _ = vgg.vgg_19(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.get_variable_scope().reuse_variables()
-      eval_inputs = tf.random_uniform(
+      tf.compat.v1.get_variable_scope().reuse_variables()
+      eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_19(eval_inputs, is_training=False,
                              spatial_squeeze=False)
       self.assertListEqual(logits.get_shape().as_list(),
                            [eval_batch_size, 2, 2, num_classes])
-      logits = tf.reduce_mean(logits, [1, 2])
-      predictions = tf.argmax(logits, 1)
+      logits = tf.reduce_mean(input_tensor=logits, axis=[1, 2])
+      predictions = tf.argmax(input=logits, axis=1)
       self.assertEquals(predictions.get_shape().as_list(), [eval_batch_size])
 
   def testForward(self):
     batch_size = 1
     height, width = 224, 224
     with self.test_session() as sess:
-      inputs = tf.random_uniform((batch_size, height, width, 3))
+      inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(inputs)
-      sess.run(tf.global_variables_initializer())
+      sess.run(tf.compat.v1.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/preprocessing/preprocessing_factory.py b/research/slim/preprocessing/preprocessing_factory.py
index bbb63f83..d3f56d72 100644
--- a/research/slim/preprocessing/preprocessing_factory.py
+++ b/research/slim/preprocessing/preprocessing_factory.py
@@ -56,6 +56,12 @@ def get_preprocessing(name, is_training=False, use_grayscale=False):
       'mobilenet_v1': inception_preprocessing,
       'mobilenet_v2': inception_preprocessing,
       'mobilenet_v2_035': inception_preprocessing,
+      'mobilenet_v3_small': inception_preprocessing,
+      'mobilenet_v3_large': inception_preprocessing,
+      'mobilenet_v3_small_minimalistic': inception_preprocessing,
+      'mobilenet_v3_large_minimalistic': inception_preprocessing,
+      'mobilenet_edgetpu': inception_preprocessing,
+      'mobilenet_edgetpu_075': inception_preprocessing,
       'mobilenet_v2_140': inception_preprocessing,
       'nasnet_mobile': inception_preprocessing,
       'nasnet_large': inception_preprocessing,
diff --git a/research/slim/train_image_classifier.py b/research/slim/train_image_classifier.py
index 388948a1..4453cc23 100644
--- a/research/slim/train_image_classifier.py
+++ b/research/slim/train_image_classifier.py
@@ -35,6 +35,10 @@ tf.app.flags.DEFINE_string(
 tf.app.flags.DEFINE_string(
     'train_dir', '/tmp/tfmodel/',
     'Directory where checkpoints and event logs are written to.')
+tf.app.flags.DEFINE_float(
+    'warmup_epochs', 0,
+    'Linearly warmup learning rate from 0 to learning_rate over this '
+    'many epochs.')
 
 tf.app.flags.DEFINE_integer('num_clones', 1,
                             'Number of model clones to deploy. Note For '
@@ -252,33 +256,42 @@ def _configure_learning_rate(num_samples_per_epoch, global_step):
   # Note: when num_clones is > 1, this will actually have each clone to go
   # over each epoch FLAGS.num_epochs_per_decay times. This is different
   # behavior from sync replicas and is expected to produce different results.
-  decay_steps = int(num_samples_per_epoch * FLAGS.num_epochs_per_decay /
-                    FLAGS.batch_size)
-
+  steps_per_epoch = num_samples_per_epoch / FLAGS.batch_size
   if FLAGS.sync_replicas:
-    decay_steps /= FLAGS.replicas_to_aggregate
+    steps_per_epoch /= FLAGS.replicas_to_aggregate
+
+  decay_steps = int(steps_per_epoch * FLAGS.num_epochs_per_decay)
 
   if FLAGS.learning_rate_decay_type == 'exponential':
-    return tf.train.exponential_decay(FLAGS.learning_rate,
-                                      global_step,
-                                      decay_steps,
-                                      FLAGS.learning_rate_decay_factor,
-                                      staircase=True,
-                                      name='exponential_decay_learning_rate')
+    learning_rate = tf.train.exponential_decay(
+        FLAGS.learning_rate,
+        global_step,
+        decay_steps,
+        FLAGS.learning_rate_decay_factor,
+        staircase=True,
+        name='exponential_decay_learning_rate')
   elif FLAGS.learning_rate_decay_type == 'fixed':
-    return tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')
+    learning_rate = tf.constant(FLAGS.learning_rate, name='fixed_learning_rate')
   elif FLAGS.learning_rate_decay_type == 'polynomial':
-    return tf.train.polynomial_decay(FLAGS.learning_rate,
-                                     global_step,
-                                     decay_steps,
-                                     FLAGS.end_learning_rate,
-                                     power=1.0,
-                                     cycle=False,
-                                     name='polynomial_decay_learning_rate')
+    learning_rate = tf.train.polynomial_decay(
+        FLAGS.learning_rate,
+        global_step,
+        decay_steps,
+        FLAGS.end_learning_rate,
+        power=1.0,
+        cycle=False,
+        name='polynomial_decay_learning_rate')
   else:
     raise ValueError('learning_rate_decay_type [%s] was not recognized' %
                      FLAGS.learning_rate_decay_type)
 
+  if FLAGS.warmup_epochs:
+    warmup_lr = (
+        FLAGS.learning_rate * tf.cast(global_step, tf.float32) /
+        (steps_per_epoch * FLAGS.warmup_epochs))
+    learning_rate = tf.minimum(warmup_lr, learning_rate)
+  return learning_rate
+
 
 def _configure_optimizer(learning_rate):
   """Configures the optimizer used for training.
