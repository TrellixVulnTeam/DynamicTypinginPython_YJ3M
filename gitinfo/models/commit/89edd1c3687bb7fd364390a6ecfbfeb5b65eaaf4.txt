commit 89edd1c3687bb7fd364390a6ecfbfeb5b65eaaf4
Author: Qianli Scott Zhu <scottzhu@google.com>
Date:   Thu May 10 11:57:02 2018 -0700

    Fix model layer for resnet v1. (#4230)
    
    The final BN and ReLU layer is only need for v2 model since it was
    doing preactivation in each block.

diff --git a/official/resnet/resnet_model.py b/official/resnet/resnet_model.py
index 128a765a..1204bf27 100644
--- a/official/resnet/resnet_model.py
+++ b/official/resnet/resnet_model.py
@@ -424,6 +424,7 @@ class Model(object):
     self.block_strides = block_strides
     self.final_size = final_size
     self.dtype = dtype
+    self.pre_activation = resnet_version == 2
 
   def _custom_dtype_getter(self, getter, name, shape=None, dtype=DEFAULT_DTYPE,
                            *args, **kwargs):
@@ -518,8 +519,11 @@ class Model(object):
             strides=self.block_strides[i], training=training,
             name='block_layer{}'.format(i + 1), data_format=self.data_format)
 
-      inputs = batch_norm(inputs, training, self.data_format)
-      inputs = tf.nn.relu(inputs)
+      # Only apply the BN and ReLU for model that does pre_activation in each
+      # building/bottleneck block, eg resnet V2.
+      if self.pre_activation:
+        inputs = batch_norm(inputs, training, self.data_format)
+        inputs = tf.nn.relu(inputs)
 
       # The current top layer has shape
       # `batch_size x pool_size x pool_size x final_size`.
