commit 32e7d660a813c11da61a2ad35055d85df8f14b63
Author: pkulzc <lzc@google.com>
Date:   Mon Jul 2 05:11:49 2018 -0700

    Open Images Challenge 2018 tools, minor fixes and refactors. (#4661)
    
    * Merged commit includes the following changes:
    202804536  by Zhichao Lu:
    
        Return tf.data.Dataset from input_fn that goes into the estimator and use PER_HOST_V2 option for tpu input pipeline config.
    
        This change shaves off 100ms per step resulting in 25 minutes of total reduced training time for ssd mobilenet v1 (15k steps to convergence).
    
    --
    202769340  by Zhichao Lu:
    
        Adding as_matrix() transformation for image-level labels.
    
    --
    202768721  by Zhichao Lu:
    
        Challenge evaluation protocol modification: adding labelmaps creation.
    
    --
    202750966  by Zhichao Lu:
    
        Add the explicit names to two output nodes.
    
    --
    202732783  by Zhichao Lu:
    
        Enforcing that batch size is 1 for evaluation, and no original images are retained during evaluation when use_tpu=False (to avoid dynamic shapes).
    
    --
    202425430  by Zhichao Lu:
    
        Refactor input pipeline to improve performance.
    
    --
    202406389  by Zhichao Lu:
    
        Only check the validity of `warmup_learning_rate` if it will be used.
    
    --
    202330450  by Zhichao Lu:
    
        Adding the description of the flag input_image_label_annotations_csv to add
          image-level labels to tf.Example.
    
    --
    202029012  by Zhichao Lu:
    
        Enabling displaying relationship name in the final metrics output.
    
    --
    202024010  by Zhichao Lu:
    
        Update to the public README.
    
    --
    201999677  by Zhichao Lu:
    
        Fixing the way negative labels are handled in VRD evaluation.
    
    --
    201962313  by Zhichao Lu:
    
        Fix a bug in resize_to_range.
    
    --
    201808488  by Zhichao Lu:
    
        Update ssd_inception_v2_pets.config to use right filename of pets dataset tf records.
    
    --
    201779225  by Zhichao Lu:
    
        Update object detection API installation doc
    
    --
    201766518  by Zhichao Lu:
    
        Add shell script to create pycocotools package for CMLE.
    
    --
    201722377  by Zhichao Lu:
    
        Removes verified_labels field and uses groundtruth_image_classes field instead.
    
    --
    201616819  by Zhichao Lu:
    
        Disable eval_on_tpu since eval_metrics is not setup to execute on TPU.
        Do not use run_config.task_type to switch tpu mode for EVAL,
        since that won't work in unit test.
        Expand unit test to verify that the same instantiation of the Estimator can independently disable eval on TPU whereas training is enabled on TPU.
    
    --
    201524716  by Zhichao Lu:
    
        Disable export model to TPU, inference is not compatible with TPU.
        Add GOOGLE_INTERNAL support in object detection copy.bara.sky
    
    --
    201453347  by Zhichao Lu:
    
        Fixing bug when evaluating the quantized model.
    
    --
    200795826  by Zhichao Lu:
    
        Fixing parsing bug: image-level labels are parsed as tuples instead of numpy
        array.
    
    --
    200746134  by Zhichao Lu:
    
        Adding image_class_text and image_class_label fields into tf_example_decoder.py
    
    --
    200743003  by Zhichao Lu:
    
        Changes to model_main.py and model_tpu_main to enable training and continuous eval.
    
    --
    200736324  by Zhichao Lu:
    
        Replace deprecated squeeze_dims argument.
    
    --
    200730072  by Zhichao Lu:
    
        Make detections only during predict and eval mode while creating model function
    
    --
    200729699  by Zhichao Lu:
    
        Minor correction to internal documentation (definition of Huber loss)
    
    --
    200727142  by Zhichao Lu:
    
        Add command line parsing as a set of flags using argparse and add header to the
        resulting file.
    
    --
    200726169  by Zhichao Lu:
    
        A tutorial on running evaluation for the Open Images Challenge 2018.
    
    --
    200665093  by Zhichao Lu:
    
        Cleanup on variables_helper_test.py.
    
    --
    200652145  by Zhichao Lu:
    
        Add an option to write (non-frozen) graph when exporting inference graph.
    
    --
    200573810  by Zhichao Lu:
    
        Update ssd_mobilenet_v1_coco and ssd_inception_v2_coco download links to point to a newer version.
    
    --
    200498014  by Zhichao Lu:
    
        Add test for groundtruth mask resizing.
    
    --
    200453245  by Zhichao Lu:
    
        Cleaning up exporting_models.md along with exporting scripts
    
    --
    200311747  by Zhichao Lu:
    
        Resize groundtruth mask to match the size of the original image.
    
    --
    200287269  by Zhichao Lu:
    
        Having a option to use custom MatMul based crop_and_resize op as an alternate to the TF op in Faster-RCNN
    
    --
    200127859  by Zhichao Lu:
    
        Updating the instructions to run locally with new binary. Also updating pets configs since file path naming has changed.
    
    --
    200127044  by Zhichao Lu:
    
        A simpler evaluation util to compute Open Images Challenge
        2018 metric (object detection track).
    
    --
    200124019  by Zhichao Lu:
    
        Freshening up configuring_jobs.md
    
    --
    200086825  by Zhichao Lu:
    
        Make merge_multiple_label_boxes work for ssd model.
    
    --
    199843258  by Zhichao Lu:
    
        Allows inconsistent feature channels to be compatible with WeightSharedConvolutionalBoxPredictor.
    
    --
    199676082  by Zhichao Lu:
    
        Enable an override for `InputReader.shuffle` for object detection pipelines.
    
    --
    199599212  by Zhichao Lu:
    
        Markdown fixes.
    
    --
    199535432  by Zhichao Lu:
    
        Pass num_additional_channels to tf.example decoder in predict_input_fn.
    
    --
    199399439  by Zhichao Lu:
    
        Adding `num_additional_channels` field to specify how many additional channels to use in the model.
    
    --
    
    PiperOrigin-RevId: 202804536
    
    * Add original model builder and docs back.

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index 52bf3565..98ade416 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -72,6 +72,8 @@ Extras:
       Inference and evaluation on the Open Images dataset</a><br>
   * <a href='g3doc/instance_segmentation.md'>
       Run an instance segmentation model</a><br>
+  * <a href='g3doc/challenge_evaluation.md'>
+      Run the evaluation for the Open Images Challenge 2018.</a><br>
 
 ## Getting Help
 
@@ -90,6 +92,20 @@ reporting an issue.
 
 ## Release information
 
+### June 25, 2018
+
+Additional evaluation tools for the [Open Images Challenge 2018](https://storage.googleapis.com/openimages/web/challenge.html) are out.
+Check out our short tutorial on data preparation and running evaluation [here](g3doc/challenge_evaluation.md)!
+
+<b>Thanks to contributors</b>: Alina Kuznetsova
+
+### June 5, 2018
+
+We have released the implementation of evaluation metrics for both tracks of the [Open Images Challenge 2018](https://storage.googleapis.com/openimages/web/challenge.html) as a part of the Object Detection API - see the [evaluation protocols](g3doc/evaluation_protocols.md) for more details.
+Additionally, we have released a tool for hierarchical labels expansion for the Open Images Challenge: check out [oid_hierarchical_labels_expansion.py](dataset_tools/oid_hierarchical_labels_expansion.py).
+
+<b>Thanks to contributors</b>: Alina Kuznetsova, Vittorio Ferrari, Jasper Uijlings
+
 ### April 30, 2018
 
 We have released a Faster R-CNN detector with ResNet-101 feature extractor trained on [AVA](https://research.google.com/ava/) v2.1.
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 3628a85e..8af9d9cc 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -24,111 +24,66 @@ that wraps the build function.
 import functools
 import tensorflow as tf
 
-from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
-from object_detection.utils import dataset_util
 
 
-def _get_padding_shapes(dataset, max_num_boxes=None, num_classes=None,
-                        spatial_image_shape=None):
-  """Returns shapes to pad dataset tensors to before batching.
+def make_initializable_iterator(dataset):
+  """Creates an iterator, and initializes tables.
+
+  This is useful in cases where make_one_shot_iterator wouldn't work because
+  the graph contains a hash table that needs to be initialized.
 
   Args:
-    dataset: tf.data.Dataset object.
-    max_num_boxes: Max number of groundtruth boxes needed to computes shapes for
-      padding.
-    num_classes: Number of classes in the dataset needed to compute shapes for
-      padding.
-    spatial_image_shape: A list of two integers of the form [height, width]
-      containing expected spatial shape of the image.
+    dataset: A `tf.data.Dataset` object.
 
   Returns:
-    A dictionary keyed by fields.InputDataFields containing padding shapes for
-    tensors in the dataset.
-
-  Raises:
-    ValueError: If groundtruth classes is neither rank 1 nor rank 2.
+    A `tf.data.Iterator`.
   """
+  iterator = dataset.make_initializable_iterator()
+  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  return iterator
 
-  if not spatial_image_shape or spatial_image_shape == [-1, -1]:
-    height, width = None, None
-  else:
-    height, width = spatial_image_shape  # pylint: disable=unpacking-non-sequence
-
-  num_additional_channels = 0
-  if fields.InputDataFields.image_additional_channels in dataset.output_shapes:
-    num_additional_channels = dataset.output_shapes[
-        fields.InputDataFields.image_additional_channels].dims[2].value
-  padding_shapes = {
-      # Additional channels are merged before batching.
-      fields.InputDataFields.image: [
-          height, width, 3 + num_additional_channels
-      ],
-      fields.InputDataFields.image_additional_channels: [
-          height, width, num_additional_channels
-      ],
-      fields.InputDataFields.source_id: [],
-      fields.InputDataFields.filename: [],
-      fields.InputDataFields.key: [],
-      fields.InputDataFields.groundtruth_difficult: [max_num_boxes],
-      fields.InputDataFields.groundtruth_boxes: [max_num_boxes, 4],
-      fields.InputDataFields.groundtruth_instance_masks: [
-          max_num_boxes, height, width
-      ],
-      fields.InputDataFields.groundtruth_is_crowd: [max_num_boxes],
-      fields.InputDataFields.groundtruth_group_of: [max_num_boxes],
-      fields.InputDataFields.groundtruth_area: [max_num_boxes],
-      fields.InputDataFields.groundtruth_weights: [max_num_boxes],
-      fields.InputDataFields.num_groundtruth_boxes: [],
-      fields.InputDataFields.groundtruth_label_types: [max_num_boxes],
-      fields.InputDataFields.groundtruth_label_scores: [max_num_boxes],
-      fields.InputDataFields.true_image_shape: [3],
-      fields.InputDataFields.multiclass_scores: [
-          max_num_boxes, num_classes + 1 if num_classes is not None else None
-      ],
-  }
-  # Determine whether groundtruth_classes are integers or one-hot encodings, and
-  # apply batching appropriately.
-  classes_shape = dataset.output_shapes[
-      fields.InputDataFields.groundtruth_classes]
-  if len(classes_shape) == 1:  # Class integers.
-    padding_shapes[fields.InputDataFields.groundtruth_classes] = [max_num_boxes]
-  elif len(classes_shape) == 2:  # One-hot or k-hot encoding.
-    padding_shapes[fields.InputDataFields.groundtruth_classes] = [
-        max_num_boxes, num_classes]
-  else:
-    raise ValueError('Groundtruth classes must be a rank 1 tensor (classes) or '
-                     'rank 2 tensor (one-hot encodings)')
-
-  if fields.InputDataFields.original_image in dataset.output_shapes:
-    padding_shapes[fields.InputDataFields.original_image] = [
-        None, None, 3 + num_additional_channels
-    ]
-  if fields.InputDataFields.groundtruth_keypoints in dataset.output_shapes:
-    tensor_shape = dataset.output_shapes[fields.InputDataFields.
-                                         groundtruth_keypoints]
-    padding_shape = [max_num_boxes, tensor_shape[1].value,
-                     tensor_shape[2].value]
-    padding_shapes[fields.InputDataFields.groundtruth_keypoints] = padding_shape
-  if (fields.InputDataFields.groundtruth_keypoint_visibilities
-      in dataset.output_shapes):
-    tensor_shape = dataset.output_shapes[fields.InputDataFields.
-                                         groundtruth_keypoint_visibilities]
-    padding_shape = [max_num_boxes, tensor_shape[1].value]
-    padding_shapes[fields.InputDataFields.
-                   groundtruth_keypoint_visibilities] = padding_shape
-  return {tensor_key: padding_shapes[tensor_key]
-          for tensor_key, _ in dataset.output_shapes.items()}
-
-
-def build(input_reader_config,
-          transform_input_data_fn=None,
-          batch_size=None,
-          max_num_boxes=None,
-          num_classes=None,
-          spatial_image_shape=None,
-          num_additional_channels=0):
+
+def read_dataset(file_read_func, input_files, config):
+  """Reads a dataset, and handles repetition and shuffling.
+
+  Args:
+    file_read_func: Function to use in tf.contrib.data.parallel_interleave, to
+      read every individual file into a tf.data.Dataset.
+    input_files: A list of file paths to read.
+    config: A input_reader_builder.InputReader object.
+
+  Returns:
+    A tf.data.Dataset of (undecoded) tf-records based on config.
+  """
+  # Shard, shuffle, and read files.
+  filenames = tf.gfile.Glob(input_files)
+  num_readers = config.num_readers
+  if num_readers > len(filenames):
+    num_readers = len(filenames)
+    tf.logging.warning('num_readers has been reduced to %d to match input file '
+                       'shards.' % num_readers)
+  filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)
+  if config.shuffle:
+    filename_dataset = filename_dataset.shuffle(
+        config.filenames_shuffle_buffer_size)
+  elif num_readers > 1:
+    tf.logging.warning('`shuffle` is false, but the input data stream is '
+                       'still slightly shuffled since `num_readers` > 1.')
+  filename_dataset = filename_dataset.repeat(config.num_epochs or None)
+  records_dataset = filename_dataset.apply(
+      tf.contrib.data.parallel_interleave(
+          file_read_func,
+          cycle_length=num_readers,
+          block_length=config.read_block_length,
+          sloppy=config.shuffle))
+  if config.shuffle:
+    records_dataset = records_dataset.shuffle(config.shuffle_buffer_size)
+  return records_dataset
+
+
+def build(input_reader_config, batch_size=None, transform_input_data_fn=None):
   """Builds a tf.data.Dataset.
 
   Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all
@@ -136,17 +91,9 @@ def build(input_reader_config,
 
   Args:
     input_reader_config: A input_reader_pb2.InputReader object.
-    transform_input_data_fn: Function to apply to all records, or None if
-      no extra decoding is required.
-    batch_size: Batch size. If None, batching is not performed.
-    max_num_boxes: Max number of groundtruth boxes needed to compute shapes for
-      padding. If None, will use a dynamic shape.
-    num_classes: Number of classes in the dataset needed to compute shapes for
-      padding. If None, will use a dynamic shape.
-    spatial_image_shape: A list of two integers of the form [height, width]
-      containing expected spatial shape of the image after applying
-      transform_input_data_fn. If None, will use dynamic shapes.
-    num_additional_channels: Number of additional channels to use in the input.
+    batch_size: Batch size. If batch size is None, no batching is performed.
+    transform_input_data_fn: Function to apply transformation to all records,
+      or None if no extra decoding is required.
 
   Returns:
     A tf.data.Dataset based on the input_reader_config.
@@ -173,24 +120,31 @@ def build(input_reader_config,
         instance_mask_type=input_reader_config.mask_type,
         label_map_proto_file=label_map_proto_file,
         use_display_name=input_reader_config.use_display_name,
-        num_additional_channels=num_additional_channels)
+        num_additional_channels=input_reader_config.num_additional_channels)
 
     def process_fn(value):
-      processed = decoder.decode(value)
+      """Sets up tf graph that decodes, transforms and pads input data."""
+      processed_tensors = decoder.decode(value)
       if transform_input_data_fn is not None:
-        return transform_input_data_fn(processed)
-      return processed
+        processed_tensors = transform_input_data_fn(processed_tensors)
+      return processed_tensors
 
-    dataset = dataset_util.read_dataset(
+    dataset = read_dataset(
         functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000),
-        process_fn, config.input_path[:], input_reader_config)
-
+        config.input_path[:], input_reader_config)
+    # TODO(rathodv): make batch size a required argument once the old binaries
+    # are deleted.
+    if batch_size:
+      num_parallel_calls = batch_size * input_reader_config.num_parallel_batches
+    else:
+      num_parallel_calls = input_reader_config.num_parallel_map_calls
+    dataset = dataset.map(
+        process_fn,
+        num_parallel_calls=num_parallel_calls)
     if batch_size:
-      padding_shapes = _get_padding_shapes(dataset, max_num_boxes, num_classes,
-                                           spatial_image_shape)
       dataset = dataset.apply(
-          tf.contrib.data.padded_batch_and_drop_remainder(batch_size,
-                                                          padding_shapes))
+          tf.contrib.data.batch_and_drop_remainder(batch_size))
+    dataset = dataset.prefetch(input_reader_config.num_prefetch_batches)
     return dataset
 
   raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index 0f1360f5..cbcdb69c 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -25,7 +25,6 @@ from tensorflow.core.example import feature_pb2
 from object_detection.builders import dataset_builder
 from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
-from object_detection.utils import dataset_util
 
 
 class DatasetBuilderTest(tf.test.TestCase):
@@ -91,7 +90,7 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_util.make_initializable_iterator(
+    tensor_dict = dataset_builder.make_initializable_iterator(
         dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
     sv = tf.train.Supervisor(logdir=self.get_temp_dir())
@@ -124,7 +123,7 @@ class DatasetBuilderTest(tf.test.TestCase):
     """.format(tf_record_path)
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_util.make_initializable_iterator(
+    tensor_dict = dataset_builder.make_initializable_iterator(
         dataset_builder.build(input_reader_proto, batch_size=1)).get_next()
 
     sv = tf.train.Supervisor(logdir=self.get_temp_dir())
@@ -153,14 +152,11 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_util.make_initializable_iterator(
+    tensor_dict = dataset_builder.make_initializable_iterator(
         dataset_builder.build(
             input_reader_proto,
             transform_input_data_fn=one_hot_class_encoding_fn,
-            batch_size=2,
-            max_num_boxes=2,
-            num_classes=3,
-            spatial_image_shape=[4, 5])).get_next()
+            batch_size=2)).get_next()
 
     sv = tf.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
@@ -169,17 +165,15 @@ class DatasetBuilderTest(tf.test.TestCase):
 
     self.assertAllEqual([2, 4, 5, 3],
                         output_dict[fields.InputDataFields.image].shape)
-    self.assertAllEqual([2, 2, 3],
+    self.assertAllEqual([2, 1, 3],
                         output_dict[fields.InputDataFields.groundtruth_classes].
                         shape)
-    self.assertAllEqual([2, 2, 4],
+    self.assertAllEqual([2, 1, 4],
                         output_dict[fields.InputDataFields.groundtruth_boxes].
                         shape)
     self.assertAllEqual(
-        [[[0.0, 0.0, 1.0, 1.0],
-          [0.0, 0.0, 0.0, 0.0]],
-         [[0.0, 0.0, 1.0, 1.0],
-          [0.0, 0.0, 0.0, 0.0]]],
+        [[[0.0, 0.0, 1.0, 1.0]],
+         [[0.0, 0.0, 1.0, 1.0]]],
         output_dict[fields.InputDataFields.groundtruth_boxes])
 
   def test_build_tf_record_input_reader_with_batch_size_two_and_masks(self):
@@ -201,14 +195,11 @@ class DatasetBuilderTest(tf.test.TestCase):
           tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
       return tensor_dict
 
-    tensor_dict = dataset_util.make_initializable_iterator(
+    tensor_dict = dataset_builder.make_initializable_iterator(
         dataset_builder.build(
             input_reader_proto,
             transform_input_data_fn=one_hot_class_encoding_fn,
-            batch_size=2,
-            max_num_boxes=2,
-            num_classes=3,
-            spatial_image_shape=[4, 5])).get_next()
+            batch_size=2)).get_next()
 
     sv = tf.train.Supervisor(logdir=self.get_temp_dir())
     with sv.prepare_or_wait_for_session() as sess:
@@ -216,34 +207,9 @@ class DatasetBuilderTest(tf.test.TestCase):
       output_dict = sess.run(tensor_dict)
 
     self.assertAllEqual(
-        [2, 2, 4, 5],
+        [2, 1, 4, 5],
         output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
 
-  def test_build_tf_record_input_reader_with_additional_channels(self):
-    tf_record_path = self.create_tf_record(has_additional_channels=True)
-
-    input_reader_text_proto = """
-      shuffle: false
-      num_readers: 1
-      tf_record_input_reader {{
-        input_path: '{0}'
-      }}
-    """.format(tf_record_path)
-    input_reader_proto = input_reader_pb2.InputReader()
-    text_format.Merge(input_reader_text_proto, input_reader_proto)
-    tensor_dict = dataset_util.make_initializable_iterator(
-        dataset_builder.build(
-            input_reader_proto, batch_size=2,
-            num_additional_channels=2)).get_next()
-
-    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
-    with sv.prepare_or_wait_for_session() as sess:
-      sv.start_queue_runners(sess)
-      output_dict = sess.run(tensor_dict)
-
-    self.assertEquals((2, 4, 5, 5),
-                      output_dict[fields.InputDataFields.image].shape)
-
   def test_raises_error_with_no_input_paths(self):
     input_reader_text_proto = """
       shuffle: false
@@ -253,7 +219,114 @@ class DatasetBuilderTest(tf.test.TestCase):
     input_reader_proto = input_reader_pb2.InputReader()
     text_format.Merge(input_reader_text_proto, input_reader_proto)
     with self.assertRaises(ValueError):
-      dataset_builder.build(input_reader_proto)
+      dataset_builder.build(input_reader_proto, batch_size=1)
+
+
+class ReadDatasetTest(tf.test.TestCase):
+
+  def setUp(self):
+    self._path_template = os.path.join(self.get_temp_dir(), 'examples_%s.txt')
+
+    for i in range(5):
+      path = self._path_template % i
+      with tf.gfile.Open(path, 'wb') as f:
+        f.write('\n'.join([str(i + 1), str((i + 1) * 10)]))
+
+    self._shuffle_path_template = os.path.join(self.get_temp_dir(),
+                                               'shuffle_%s.txt')
+    for i in range(2):
+      path = self._shuffle_path_template % i
+      with tf.gfile.Open(path, 'wb') as f:
+        f.write('\n'.join([str(i)] * 5))
+
+  def _get_dataset_next(self, files, config, batch_size):
+    def decode_func(value):
+      return [tf.string_to_number(value, out_type=tf.int32)]
+
+    dataset = dataset_builder.read_dataset(
+        tf.data.TextLineDataset, files, config)
+    dataset = dataset.map(decode_func)
+    dataset = dataset.batch(batch_size)
+    return dataset.make_one_shot_iterator().get_next()
+
+  def test_make_initializable_iterator_with_hashTable(self):
+    keys = [1, 0, -1]
+    dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])
+    table = tf.contrib.lookup.HashTable(
+        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
+            keys=keys,
+            values=list(reversed(keys))),
+        default_value=100)
+    dataset = dataset.map(table.lookup)
+    data = dataset_builder.make_initializable_iterator(dataset).get_next()
+    init = tf.tables_initializer()
+
+    with self.test_session() as sess:
+      sess.run(init)
+      self.assertAllEqual(sess.run(data), [-1, 100, 1, 100])
+
+  def test_read_dataset(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '*'], config,
+                                  batch_size=20)
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data),
+                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
+                            30, 4, 40, 5, 50]])
+
+  def test_reduce_num_reader(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 10
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '*'], config,
+                                  batch_size=20)
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data),
+                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
+                            30, 4, 40, 5, 50]])
+
+  def test_enable_shuffle(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = True
+
+    tf.set_random_seed(1)  # Set graph level seed.
+    data = self._get_dataset_next(
+        [self._shuffle_path_template % '*'], config, batch_size=10)
+    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
+
+    with self.test_session() as sess:
+      self.assertTrue(
+          np.any(np.not_equal(sess.run(data), expected_non_shuffle_output)))
+
+  def test_disable_shuffle_(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next(
+        [self._shuffle_path_template % '*'], config, batch_size=10)
+    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
+
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data), [expected_non_shuffle_output])
+
+  def test_read_dataset_single_epoch(self):
+    config = input_reader_pb2.InputReader()
+    config.num_epochs = 1
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '0'], config,
+                                  batch_size=30)
+    with self.test_session() as sess:
+      # First batch will retrieve as much as it can, second batch will fail.
+      self.assertAllEqual(sess.run(data), [[1, 10]])
+      self.assertRaises(tf.errors.OutOfRangeError, sess.run, data)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 78d82423..aed1fd66 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -840,7 +840,9 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
     Args:
       image_features: A list of float tensors of shape [batch_size, height_i,
         width_i, channels] containing features for a batch of images. Note that
-        all tensors in the list must have the same number of channels.
+        when not all tensors in the list have the same number of channels, an
+        additional projection layer will be added on top the tensor to generate
+        feature map with number of channels consitent with the majority.
       num_predictions_per_location_list: A list of integers representing the
         number of box predictions to be made per spatial location for each
         feature map. Note that all values must be the same since the weights are
@@ -869,11 +871,17 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
     feature_channels = [
         image_feature.shape[3].value for image_feature in image_features
     ]
-    if len(set(feature_channels)) > 1:
-      raise ValueError('all feature maps must have the same number of '
-                       'channels, found: {}'.format(feature_channels))
+    has_different_feature_channels = len(set(feature_channels)) > 1
+    if has_different_feature_channels:
+      inserted_layer_counter = 0
+      target_channel = max(set(feature_channels), key=feature_channels.count)
+      tf.logging.info('Not all feature maps have the same number of '
+                      'channels, found: {}, addition project layers '
+                      'to bring all feature maps to uniform channels '
+                      'of {}'.format(feature_channels, target_channel))
     box_encodings_list = []
     class_predictions_list = []
+    num_class_slots = self.num_classes + 1
     for feature_index, (image_feature,
                         num_predictions_per_location) in enumerate(
                             zip(image_features,
@@ -881,11 +889,28 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
       # Add a slot for the background class.
       with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
                              reuse=tf.AUTO_REUSE):
-        num_class_slots = self.num_classes + 1
-        box_encodings_net = image_feature
-        class_predictions_net = image_feature
         with slim.arg_scope(self._conv_hyperparams_fn()) as sc:
           apply_batch_norm = _arg_scope_func_key(slim.batch_norm) in sc
+          # Insert an additional projection layer if necessary.
+          if (has_different_feature_channels and
+              image_feature.shape[3].value != target_channel):
+            image_feature = slim.conv2d(
+                image_feature,
+                target_channel, [1, 1],
+                stride=1,
+                padding='SAME',
+                activation_fn=None,
+                normalizer_fn=(tf.identity if apply_batch_norm else None),
+                scope='ProjectionLayer/conv2d_{}'.format(
+                    inserted_layer_counter))
+            if apply_batch_norm:
+              image_feature = slim.batch_norm(
+                  image_feature,
+                  scope='ProjectionLayer/conv2d_{}/BatchNorm'.format(
+                      inserted_layer_counter))
+            inserted_layer_counter += 1
+          box_encodings_net = image_feature
+          class_predictions_net = image_feature
           for i in range(self._num_layers_before_predictor):
             box_encodings_net = slim.conv2d(
                 box_encodings_net,
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index 49680596..2111f662 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -565,6 +565,38 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     self.assertAllEqual(class_predictions_with_background.shape,
                         [4, 640, num_classes_without_background+1])
 
+  def test_get_multi_class_predictions_from_feature_maps_of_different_depth(
+      self):
+
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2, image_features3):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=1,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2, image_features3],
+          num_predictions_per_location=[5, 5, 5],
+          scope='BoxPredictor')
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features3 = np.random.rand(4, 8, 8, 32).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features1, image_features2, image_features3])
+    self.assertAllEqual(box_encodings.shape, [4, 960, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 960, num_classes_without_background+1])
+
   def test_predictions_from_multiple_feature_maps_share_weights_not_batchnorm(
       self):
     num_classes_without_background = 6
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index 5471c955..2c274b6d 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -120,7 +120,7 @@ class WeightedSmoothL1LocalizationLoss(Loss):
   """Smooth L1 localization loss function aka Huber Loss..
 
   The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and
-  0.5 x^2 + delta * (|x|-delta) otherwise, where x is the difference between
+  delta * (|x|- 0.5*delta) otherwise, where x is the difference between
   predictions and target.
 
   See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 0fcdfcc6..85187ec7 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -2207,10 +2207,10 @@ def resize_to_range(image,
           new_size[:-1],
           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
           align_corners=align_corners)
-      new_masks = tf.squeeze(new_masks, 3)
       if pad_to_max_dimension:
         new_masks = tf.image.pad_to_bounding_box(
             new_masks, 0, 0, max_dimension, max_dimension)
+      new_masks = tf.squeeze(new_masks, 3)
       result.append(new_masks)
 
     result.append(new_size)
@@ -3136,7 +3136,7 @@ def preprocess(tensor_dict,
     images = tensor_dict[fields.InputDataFields.image]
     if len(images.get_shape()) != 4:
       raise ValueError('images in tensor_dict should be rank 4')
-    image = tf.squeeze(images, squeeze_dims=[0])
+    image = tf.squeeze(images, axis=0)
     tensor_dict[fields.InputDataFields.image] = image
 
   # Preprocess inputs based on preprocess_options
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 588a3f90..9e2c5056 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -2377,6 +2377,40 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllEqual(out_masks.get_shape().as_list(), expected_mask_shape)
       self.assertAllEqual(out_image.get_shape().as_list(), expected_image_shape)
 
+  def testResizeToRangeWithMasksAndPadToMaxDimension(self):
+    """Tests image resizing, checking output sizes."""
+    in_image_shape_list = [[60, 40, 3], [15, 30, 3]]
+    in_masks_shape_list = [[15, 60, 40], [10, 15, 30]]
+    min_dim = 50
+    max_dim = 100
+    expected_image_shape_list = [[100, 100, 3], [100, 100, 3]]
+    expected_masks_shape_list = [[15, 100, 100], [10, 100, 100]]
+
+    for (in_image_shape,
+         expected_image_shape, in_masks_shape, expected_mask_shape) in zip(
+             in_image_shape_list, expected_image_shape_list,
+             in_masks_shape_list, expected_masks_shape_list):
+      in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
+      in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
+      out_image, out_masks, _ = preprocessor.resize_to_range(
+          in_image,
+          in_masks,
+          min_dimension=min_dim,
+          max_dimension=max_dim,
+          pad_to_max_dimension=True)
+      out_image_shape = tf.shape(out_image)
+      out_masks_shape = tf.shape(out_masks)
+
+      with self.test_session() as sess:
+        out_image_shape, out_masks_shape = sess.run(
+            [out_image_shape, out_masks_shape],
+            feed_dict={
+                in_image: np.random.randn(*in_image_shape),
+                in_masks: np.random.randn(*in_masks_shape)
+            })
+        self.assertAllEqual(out_image_shape, expected_image_shape)
+        self.assertAllEqual(out_masks_shape, expected_mask_shape)
+
   def testResizeToRangeWithMasksAndDynamicSpatialShape(self):
     """Tests image resizing, checking output sizes."""
     in_image_shape_list = [[60, 40, 3], [15, 30, 3]]
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index 11282da6..99e04e66 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -62,8 +62,6 @@ class InputDataFields(object):
     num_groundtruth_boxes: number of groundtruth boxes.
     true_image_shapes: true shapes of images in the resized images, as resized
       images can be padded with zeros.
-    verified_labels: list of human-verified image-level labels (note, that a
-      label can be verified both as positive and negative).
     multiclass_scores: the label score per class for each box.
   """
   image = 'image'
@@ -91,7 +89,6 @@ class InputDataFields(object):
   groundtruth_weights = 'groundtruth_weights'
   num_groundtruth_boxes = 'num_groundtruth_boxes'
   true_image_shape = 'true_image_shape'
-  verified_labels = 'verified_labels'
   multiclass_scores = 'multiclass_scores'
 
 
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 8480a14b..787ef0fc 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Tensorflow Example proto decoder for object detection.
 
 A decoder to decode string tensors containing serialized tensorflow.Example
@@ -156,6 +155,11 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             tf.FixedLenFeature((), tf.int64, default_value=1),
         'image/width':
             tf.FixedLenFeature((), tf.int64, default_value=1),
+        # Image-level labels.
+        'image/class/text':
+            tf.VarLenFeature(tf.string),
+        'image/class/label':
+            tf.VarLenFeature(tf.int64),
         # Object boxes and classes.
         'image/object/bbox/xmin':
             tf.VarLenFeature(tf.float32),
@@ -281,10 +285,18 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       label_handler = BackupHandler(
           LookupTensor('image/object/class/text', table, default_value=''),
           slim_example_decoder.Tensor('image/object/class/label'))
+      image_label_handler = BackupHandler(
+          LookupTensor(
+              fields.TfExampleFields.image_class_text, table, default_value=''),
+          slim_example_decoder.Tensor(fields.TfExampleFields.image_class_label))
     else:
       label_handler = slim_example_decoder.Tensor('image/object/class/label')
+      image_label_handler = slim_example_decoder.Tensor(
+          fields.TfExampleFields.image_class_label)
     self.items_to_handlers[
         fields.InputDataFields.groundtruth_classes] = label_handler
+    self.items_to_handlers[
+        fields.InputDataFields.groundtruth_image_classes] = image_label_handler
 
   def decode(self, tf_example_string_tensor):
     """Decodes serialized tensorflow example and returns a tensor dictionary.
@@ -328,6 +340,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         the keypoints are ordered (y, x).
       fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
         shape [None, None, None] containing instance masks.
+      fields.InputDataFields.groundtruth_image_classes - 1D uint64 of shape
+        [None] containing classes for the boxes.
     """
     serialized_example = tf.reshape(tf_example_string_tensor, shape=[])
     decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index b567b8c2..5d38fa9e 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -762,6 +762,57 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertTrue(fields.InputDataFields.groundtruth_instance_masks
                     not in tensor_dict)
 
+  def testDecodeImageLabels(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded': self._BytesFeature(encoded_jpeg),
+                'image/format': self._BytesFeature('jpeg'),
+                'image/class/label': self._Int64Feature([1, 2]),
+            })).SerializeToString()
+    example_decoder = tf_example_decoder.TfExampleDecoder()
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+    self.assertTrue(
+        fields.InputDataFields.groundtruth_image_classes in tensor_dict)
+    self.assertAllEqual(
+        tensor_dict[fields.InputDataFields.groundtruth_image_classes],
+        np.array([1, 2]))
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded': self._BytesFeature(encoded_jpeg),
+                'image/format': self._BytesFeature('jpeg'),
+                'image/class/text': self._BytesFeature(['dog', 'cat']),
+            })).SerializeToString()
+    label_map_string = """
+      item {
+        id:3
+        name:'cat'
+      }
+      item {
+        id:1
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    with self.test_session() as sess:
+      sess.run(tf.tables_initializer())
+      tensor_dict = sess.run(tensor_dict)
+    self.assertTrue(
+        fields.InputDataFields.groundtruth_image_classes in tensor_dict)
+    self.assertAllEqual(
+        tensor_dict[fields.InputDataFields.groundtruth_image_classes],
+        np.array([1, 3]))
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/dataset_tools/create_pycocotools_package.sh b/research/object_detection/dataset_tools/create_pycocotools_package.sh
new file mode 100644
index 00000000..511df185
--- /dev/null
+++ b/research/object_detection/dataset_tools/create_pycocotools_package.sh
@@ -0,0 +1,53 @@
+#!/bin/bash
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+# Script to download pycocotools and make package for CMLE jobs.
+#
+# usage:
+#  bash object_detection/dataset_tools/create_pycocotools_package.sh \
+#    /tmp/pycocotools
+set -e
+
+if [ -z "$1" ]; then
+  echo "usage create_pycocotools_package.sh [output dir]"
+  exit
+fi
+
+# Create the output directory.
+OUTPUT_DIR="${1%/}"
+SCRATCH_DIR="${OUTPUT_DIR}/raw"
+mkdir -p "${OUTPUT_DIR}"
+mkdir -p "${SCRATCH_DIR}"
+
+cd ${SCRATCH_DIR}
+git clone https://github.com/cocodataset/cocoapi.git
+cd cocoapi/PythonAPI && mv ../common ./
+
+sed "s/\.\.\/common/common/g" setup.py > setup.py.updated
+cp -f setup.py.updated setup.py
+rm setup.py.updated
+
+sed "s/\.\.\/common/common/g" pycocotools/_mask.pyx > _mask.pyx.updated
+cp -f _mask.pyx.updated pycocotools/_mask.pyx
+rm _mask.pyx.updated
+
+sed "s/import matplotlib\.pyplot as plt/import matplotlib\nmatplotlib\.use\(\'Agg\'\)\nimport matplotlib\.pyplot as plt/g" pycocotools/coco.py > coco.py.updated
+cp -f coco.py.updated pycocotools/coco.py
+rm coco.py.updated
+
+cd "${OUTPUT_DIR}"
+tar -czf pycocotools-2.0.tar.gz -C "${SCRATCH_DIR}/cocoapi/" PythonAPI/
+rm -rf ${SCRATCH_DIR}
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
index 6c00ac42..013a0330 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -12,15 +12,19 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""A class and executable to expand hierarchically image-level labels and boxes.
+r"""An executable to expand hierarchically image-level labels and boxes.
 
 Example usage:
-    ./hierarchical_labels_expansion <path to JSON hierarchy> <input csv file>
-    <output csv file> [optional]labels_file
+python models/research/object_detection/dataset_tools/\
+oid_hierarchical_labels_expansion.py \
+--json_hierarchy_file=<path to JSON hierarchy> \
+--input_annotations=<input csv file> \
+--output_annotations=<output csv file> \
+--annotation_type=<1 (for boxes) or 2 (for image-level labels)>
 """
 
+import argparse
 import json
-import sys
 
 
 def _update_dict(initial_dict, update):
@@ -80,7 +84,7 @@ class OIDHierarchicalLabelsExpansion(object):
     """Constructor.
 
     Args:
-      hierarchy: labels hierarchy as JSON file.
+      hierarchy: labels hierarchy as JSON object.
     """
 
     self._hierarchy_keyed_parent, self._hierarchy_keyed_child, _ = (
@@ -100,14 +104,14 @@ class OIDHierarchicalLabelsExpansion(object):
     # Row header is expected to be exactly:
     # ImageID,Source,LabelName,Confidence,XMin,XMax,YMin,YMax,IsOccluded,
     # IsTruncated,IsGroupOf,IsDepiction,IsInside
-    cvs_row_splited = csv_row.split(',')
-    assert len(cvs_row_splited) == 13
+    cvs_row_splitted = csv_row.split(',')
+    assert len(cvs_row_splitted) == 13
     result = [csv_row]
-    assert cvs_row_splited[2] in self._hierarchy_keyed_child
-    parent_nodes = self._hierarchy_keyed_child[cvs_row_splited[2]]
+    assert cvs_row_splitted[2] in self._hierarchy_keyed_child
+    parent_nodes = self._hierarchy_keyed_child[cvs_row_splitted[2]]
     for parent_node in parent_nodes:
-      cvs_row_splited[2] = parent_node
-      result.append(','.join(cvs_row_splited))
+      cvs_row_splitted[2] = parent_node
+      result.append(','.join(cvs_row_splitted))
     return result
 
   def expand_labels_from_csv(self, csv_row):
@@ -141,32 +145,55 @@ class OIDHierarchicalLabelsExpansion(object):
     return result
 
 
-def main(argv):
+def main(parsed_args):
 
-  if len(argv) < 4:
-    print """Missing arguments. \n
-             Usage: ./hierarchical_labels_expansion <path to JSON hierarchy>
-             <input csv file> <output csv file> [optional]labels_file"""
-    return
-  with open(argv[1]) as f:
+  with open(parsed_args.json_hierarchy_file) as f:
     hierarchy = json.load(f)
   expansion_generator = OIDHierarchicalLabelsExpansion(hierarchy)
   labels_file = False
-  if len(argv) > 4 and argv[4] == 'labels_file':
+  if parsed_args.annotation_type == 2:
     labels_file = True
-  with open(argv[2], 'r') as source:
-    with open(argv[3], 'w') as target:
-      header_skipped = False
+  elif parsed_args.annotation_type != 1:
+    print '--annotation_type expected value is 1 or 2.'
+    return -1
+  with open(parsed_args.input_annotations, 'r') as source:
+    with open(parsed_args.output_annotations, 'w') as target:
+      header = None
       for line in source:
-        if not header_skipped:
-          header_skipped = True
+        if not header:
+          header = line
           continue
         if labels_file:
           expanded_lines = expansion_generator.expand_labels_from_csv(line)
         else:
           expanded_lines = expansion_generator.expand_boxes_from_csv(line)
+        expanded_lines = [header] + expanded_lines
         target.writelines(expanded_lines)
 
 
 if __name__ == '__main__':
-  main(sys.argv)
+
+  parser = argparse.ArgumentParser(
+      description='Hierarchically expand annotations (excluding root node).')
+  parser.add_argument(
+      '--json_hierarchy_file',
+      required=True,
+      help='Path to the file containing label hierarchy in JSON format.')
+  parser.add_argument(
+      '--input_annotations',
+      required=True,
+      help="""Path to Open Images annotations file (either bounding boxes or
+      image-level labels).""")
+  parser.add_argument(
+      '--output_annotations',
+      required=True,
+      help="""Path to the output file.""")
+  parser.add_argument(
+      '--annotation_type',
+      type=int,
+      required=True,
+      help="""Type of the input annotations: 1 - boxes, 2 - image-level
+      labels"""
+  )
+  args = parser.parse_args()
+  main(args)
diff --git a/research/object_detection/eval.py b/research/object_detection/eval.py
index f546442e..2a8c7153 100644
--- a/research/object_detection/eval.py
+++ b/research/object_detection/eval.py
@@ -52,7 +52,6 @@ from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.utils import config_util
-from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 
 
@@ -115,7 +114,7 @@ def main(unused_argv):
       is_training=False)
 
   def get_next(config):
-    return dataset_util.make_initializable_iterator(
+    return dataset_builder.make_initializable_iterator(
         dataset_builder.build(config)).get_next()
 
   create_input_dict_fn = functools.partial(get_next, input_config)
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 67b62a44..51c29455 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -556,8 +556,16 @@ def result_dict_for_single_example(image,
 
   if groundtruth:
     if input_data_fields.groundtruth_instance_masks in groundtruth:
+      masks = groundtruth[input_data_fields.groundtruth_instance_masks]
+      masks = tf.expand_dims(masks, 3)
+      masks = tf.image.resize_images(
+          masks,
+          image_shape[1:3],
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+          align_corners=True)
+      masks = tf.squeeze(masks, 3)
       groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(
-          groundtruth[input_data_fields.groundtruth_instance_masks], tf.uint8)
+          masks, tf.uint8)
     output_dict.update(groundtruth)
     if scale_to_absolute:
       groundtruth_boxes = groundtruth[input_data_fields.groundtruth_boxes]
@@ -641,5 +649,3 @@ def get_eval_metric_ops_for_evaluators(evaluation_metrics,
                        'Found {} in the evaluation metrics'.format(metric))
 
   return eval_metric_ops
-
-
diff --git a/research/object_detection/eval_util_test.py b/research/object_detection/eval_util_test.py
index e4b0ca3d..d82efcbf 100644
--- a/research/object_detection/eval_util_test.py
+++ b/research/object_detection/eval_util_test.py
@@ -32,7 +32,7 @@ class EvalUtilTest(tf.test.TestCase):
             {'id': 1, 'name': 'dog'},
             {'id': 2, 'name': 'cat'}]
 
-  def _make_evaluation_dict(self):
+  def _make_evaluation_dict(self, resized_groundtruth_masks=False):
     input_data_fields = fields.InputDataFields
     detection_fields = fields.DetectionResultFields
 
@@ -46,6 +46,8 @@ class EvalUtilTest(tf.test.TestCase):
     groundtruth_boxes = tf.constant([[0., 0., 1., 1.]])
     groundtruth_classes = tf.constant([1])
     groundtruth_instance_masks = tf.ones(shape=[1, 20, 20], dtype=tf.uint8)
+    if resized_groundtruth_masks:
+      groundtruth_instance_masks = tf.ones(shape=[1, 10, 10], dtype=tf.uint8)
     detections = {
         detection_fields.detection_boxes: detection_boxes,
         detection_fields.detection_scores: detection_scores,
@@ -99,6 +101,26 @@ class EvalUtilTest(tf.test.TestCase):
       self.assertAlmostEqual(1.0, metrics['DetectionBoxes_Precision/mAP'])
       self.assertAlmostEqual(1.0, metrics['DetectionMasks_Precision/mAP'])
 
+  def test_get_eval_metric_ops_for_coco_detections_and_resized_masks(self):
+    evaluation_metrics = ['coco_detection_metrics',
+                          'coco_mask_metrics']
+    categories = self._get_categories_list()
+    eval_dict = self._make_evaluation_dict(resized_groundtruth_masks=True)
+    metric_ops = eval_util.get_eval_metric_ops_for_evaluators(
+        evaluation_metrics, categories, eval_dict)
+    _, update_op_boxes = metric_ops['DetectionBoxes_Precision/mAP']
+    _, update_op_masks = metric_ops['DetectionMasks_Precision/mAP']
+
+    with self.test_session() as sess:
+      metrics = {}
+      for key, (value_op, _) in metric_ops.iteritems():
+        metrics[key] = value_op
+      sess.run(update_op_boxes)
+      sess.run(update_op_masks)
+      metrics = sess.run(metrics)
+      self.assertAlmostEqual(1.0, metrics['DetectionBoxes_Precision/mAP'])
+      self.assertAlmostEqual(1.0, metrics['DetectionMasks_Precision/mAP'])
+
   def test_get_eval_metric_ops_raises_error_with_unsupported_metric(self):
     evaluation_metrics = ['unsupported_metrics']
     categories = self._get_categories_list()
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 5d0699f1..1e9fcbda 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -16,7 +16,7 @@
 r"""Tool to export an object detection model for inference.
 
 Prepares an object detection tensorflow graph for inference using model
-configuration and an optional trained checkpoint. Outputs inference
+configuration and a trained checkpoint. Outputs inference
 graph, associated checkpoint files, a frozen inference graph and a
 SavedModel (https://tensorflow.github.io/serving/serving_basic.html).
 
@@ -59,7 +59,7 @@ python export_inference_graph \
 The expected output would be in the directory
 path/to/exported_model_directory (which is created if it does not exist)
 with contents:
- - graph.pbtxt
+ - inference_graph.pbtxt
  - model.ckpt.data-00000-of-00001
  - model.ckpt.info
  - model.ckpt.meta
@@ -120,6 +120,8 @@ flags.DEFINE_string('output_directory', None, 'Path to write outputs.')
 flags.DEFINE_string('config_override', '',
                     'pipeline_pb2.TrainEvalPipelineConfig '
                     'text proto to override pipeline_config_path.')
+flags.DEFINE_boolean('write_inference_graph', False,
+                     'If true, writes inference graph to disk.')
 tf.app.flags.mark_flag_as_required('pipeline_config_path')
 tf.app.flags.mark_flag_as_required('trained_checkpoint_prefix')
 tf.app.flags.mark_flag_as_required('output_directory')
@@ -140,7 +142,8 @@ def main(_):
     input_shape = None
   exporter.export_inference_graph(FLAGS.input_type, pipeline_config,
                                   FLAGS.trained_checkpoint_prefix,
-                                  FLAGS.output_directory, input_shape)
+                                  FLAGS.output_directory, input_shape,
+                                  FLAGS.write_inference_graph)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 05b09b1f..16ff79a3 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -18,7 +18,6 @@ import logging
 import os
 import tempfile
 import tensorflow as tf
-from google.protobuf import text_format
 from tensorflow.core.protobuf import saver_pb2
 from tensorflow.python import pywrap_tensorflow
 from tensorflow.python.client import session
@@ -29,6 +28,7 @@ from tensorflow.python.training import saver as saver_lib
 from object_detection.builders import model_builder
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
+from object_detection.utils import config_util
 
 slim = tf.contrib.slim
 
@@ -243,9 +243,7 @@ def _add_output_tensor_nodes(postprocessed_tensors,
         masks, name=detection_fields.detection_masks)
   for output_key in outputs:
     tf.add_to_collection(output_collection_name, outputs[output_key])
-  if masks is not None:
-    tf.add_to_collection(output_collection_name,
-                         outputs[detection_fields.detection_masks])
+
   return outputs
 
 
@@ -276,7 +274,7 @@ def write_saved_model(saved_model_path,
   Args:
     saved_model_path: Path to write SavedModel.
     frozen_graph_def: tf.GraphDef holding frozen graph.
-    inputs: The input image tensor to use for detection.
+    inputs: The input placeholder tensor.
     outputs: A tensor dictionary containing the outputs of a DetectionModel.
   """
   with tf.Graph().as_default():
@@ -370,7 +368,8 @@ def _export_inference_graph(input_type,
                             additional_output_tensor_names=None,
                             input_shape=None,
                             output_collection_name='inference_op',
-                            graph_hook_fn=None):
+                            graph_hook_fn=None,
+                            write_inference_graph=False):
   """Export helper."""
   tf.gfile.MakeDirs(output_directory)
   frozen_graph_path = os.path.join(output_directory,
@@ -408,6 +407,14 @@ def _export_inference_graph(input_type,
       model_path=model_path,
       input_saver_def=input_saver_def,
       trained_checkpoint_prefix=checkpoint_to_use)
+  if write_inference_graph:
+    inference_graph_def = tf.get_default_graph().as_graph_def()
+    inference_graph_path = os.path.join(output_directory,
+                                        'inference_graph.pbtxt')
+    for node in inference_graph_def.node:
+      node.device = ''
+    with gfile.GFile(inference_graph_path, 'wb') as f:
+      f.write(str(inference_graph_def))
 
   if additional_output_tensor_names is not None:
     output_node_names = ','.join(outputs.keys()+additional_output_tensor_names)
@@ -434,12 +441,13 @@ def export_inference_graph(input_type,
                            output_directory,
                            input_shape=None,
                            output_collection_name='inference_op',
-                           additional_output_tensor_names=None):
+                           additional_output_tensor_names=None,
+                           write_inference_graph=False):
   """Exports inference graph for the model specified in the pipeline config.
 
   Args:
-    input_type: Type of input for the graph. Can be one of [`image_tensor`,
-      `tf_example`].
+    input_type: Type of input for the graph. Can be one of ['image_tensor',
+      'encoded_image_string_tensor', 'tf_example'].
     pipeline_config: pipeline_pb2.TrainAndEvalPipelineConfig proto.
     trained_checkpoint_prefix: Path to the trained checkpoint file.
     output_directory: Path to write outputs.
@@ -449,17 +457,20 @@ def export_inference_graph(input_type,
       If None, does not add output tensors to a collection.
     additional_output_tensor_names: list of additional output
       tensors to include in the frozen graph.
+    write_inference_graph: If true, writes inference graph to disk.
   """
   detection_model = model_builder.build(pipeline_config.model,
                                         is_training=False)
-  _export_inference_graph(input_type, detection_model,
-                          pipeline_config.eval_config.use_moving_averages,
-                          trained_checkpoint_prefix,
-                          output_directory, additional_output_tensor_names,
-                          input_shape, output_collection_name,
-                          graph_hook_fn=None)
+  _export_inference_graph(
+      input_type,
+      detection_model,
+      pipeline_config.eval_config.use_moving_averages,
+      trained_checkpoint_prefix,
+      output_directory,
+      additional_output_tensor_names,
+      input_shape,
+      output_collection_name,
+      graph_hook_fn=None,
+      write_inference_graph=write_inference_graph)
   pipeline_config.eval_config.use_moving_averages = False
-  config_text = text_format.MessageToString(pipeline_config)
-  with tf.gfile.Open(
-      os.path.join(output_directory, 'pipeline.config'), 'wb') as f:
-    f.write(config_text)
+  config_util.save_pipeline_config(pipeline_config, output_directory)
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index cf6e85b2..fb766545 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -134,6 +134,26 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       self.assertTrue(os.path.exists(os.path.join(
           output_directory, 'saved_model', 'saved_model.pb')))
 
+  def test_write_inference_graph(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,
+                                          use_moving_averages=False)
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel()
+      output_directory = os.path.join(tmp_dir, 'output')
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      pipeline_config.eval_config.use_moving_averages = False
+      exporter.export_inference_graph(
+          input_type='image_tensor',
+          pipeline_config=pipeline_config,
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
+          output_directory=output_directory,
+          write_inference_graph=True)
+      self.assertTrue(os.path.exists(os.path.join(
+          output_directory, 'inference_graph.pbtxt')))
+
   def test_export_graph_with_fixed_size_image_tensor_input(self):
     input_shape = [1, 320, 320, 3]
 
diff --git a/research/object_detection/g3doc/challenge_evaluation.md b/research/object_detection/g3doc/challenge_evaluation.md
new file mode 100644
index 00000000..3758c772
--- /dev/null
+++ b/research/object_detection/g3doc/challenge_evaluation.md
@@ -0,0 +1,151 @@
+# Open Images Challenge Evaluation
+
+The Object Detection API is currently supporting several evaluation metrics used in the [Open Images Challenge 2018](https://storage.googleapis.com/openimages/web/challenge.html).
+In addition, several data processing tools are available. Detailed instructions on using the tools for each track are available below.
+
+**NOTE**: links to the external website in this tutorial may change after the Open Images Challenge 2018 is finished.
+
+## Object Detection Track
+
+The [Object Detection metric](https://storage.googleapis.com/openimages/web/object_detection_metric.html) protocol requires a pre-processing of the released data to ensure correct evaluation. The released data contains only leaf-most bounding box annotations and image-level labels.
+The evaluation metric implementation is available in the class `OpenImagesDetectionChallengeEvaluator`.
+
+1. Download class hierarchy of Open Images Challenge 2018 in JSON format from [here](https://storage.googleapis.com/openimages/challenge_2018/bbox_labels_500_hierarchy.json).
+2. Download ground-truth [boundling boxes](https://storage.googleapis.com/openimages/challenge_2018/train/challenge-2018-train-annotations-bbox.csv) and [image-level labels](https://storage.googleapis.com/openimages/challenge_2018/train/challenge-2018-train-annotations-human-imagelabels.csv).
+3. Filter the rows corresponding to the validation set images you want to use and store the results in the same CSV format.
+4. Run the following command to create hierarchical expansion of the bounding boxes annotations:
+
+```
+HIERARCHY_FILE=/path/to/bbox_labels_500_hierarchy.json
+BOUNDING_BOXES=/path/to/challenge-2018-train-annotations-bbox
+IMAGE_LABELS=/path/to/challenge-2018-train-annotations-human-imagelabels
+
+python object_detection/dataset_tools/oid_hierarchical_labels_expansion.py \
+    --json_hierarchy_file=${HIERARCHY_FILE} \
+    --input_annotations=${BOUNDING_BOXES}.csv \
+    --output_annotations=${BOUNDING_BOXES}_expanded.csv \
+    --annotation_type=1
+
+python object_detection/dataset_tools/oid_hierarchical_labels_expansion.py \
+    --json_hierarchy_file=${HIERARCHY_FILE} \
+    --input_annotations=${IMAGE_LABELS}.csv \
+    --output_annotations=${IMAGE_LABELS}_expanded.csv \
+    --annotation_type=2
+```
+
+After step 4 you will have produced the ground-truth files suitable for running 'OID Challenge Object Detection Metric 2018' evaluation.
+
+```
+INPUT_PREDICTIONS=/path/to/detection_predictions.csv
+OUTPUT_METRICS=/path/to/output/metrics/file
+
+python models/research/object_detection/metrics/oid_od_challenge_evaluation.py \
+    --input_annotations_boxes=${BOUNDING_BOXES}_expanded.csv \
+    --input_annotations_labels=${IMAGE_LABELS}_expanded.csv \
+    --input_class_labelmap=object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt \
+    --input_predictions=${INPUT_PREDICTIONS} \
+    --output_metrics=${OUTPUT_METRICS} \
+```
+
+### Running evaluation on CSV files directly
+
+5. If you are not using Tensorflow, you can run evaluation directly using your algorithm's output and generated ground-truth files. {value=5}
+
+
+### Running evaluation using TF Object Detection API
+
+5. Produce tf.Example files suitable for running inference: {value=5}
+
+```
+RAW_IMAGES_DIR=/path/to/raw_images_location
+OUTPUT_DIR=/path/to/output_tfrecords
+
+python object_detection/dataset_tools/create_oid_tf_record.py \
+    --input_box_annotations_csv ${BOUNDING_BOXES}_expanded.csv \
+    --input_image_label_annotations_csv ${IMAGE_LABELS}_expanded.csv \
+    --input_images_directory ${RAW_IMAGES_DIR} \
+    --input_label_map object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt \
+    --output_tf_record_path_prefix ${OUTPUT_DIR} \
+    --num_shards=100
+```
+
+6. Run inference of your model and fill corresponding fields in tf.Example: see [this tutorial](object_detection/g3doc/oid_inference_and_evaluation.md) on running the inference with Tensorflow Object Detection API models. {value=6}
+
+7. Finally, run the evaluation script to produce the final evaluation result.
+
+```
+INPUT_TFRECORDS_WITH_DETECTIONS=/path/to/tf_records_with_detections
+OUTPUT_CONFIG_DIR=/path/to/configs
+
+echo "
+label_map_path: 'object_detection/data/oid_object_detection_challenge_500_label_map.pbtxt'
+tf_record_input_reader: { input_path: '${INPUT_TFRECORDS_WITH_DETECTIONS}' }
+" > ${OUTPUT_CONFIG_DIR}/input_config.pbtxt
+
+echo "
+metrics_set: 'oid_challenge_object_detection_metrics'
+" > ${OUTPUT_CONFIG_DIR}/eval_config.pbtxt
+
+OUTPUT_METRICS_DIR=/path/to/metrics_csv
+
+python object_detection/metrics/offline_eval_map_corloc.py \
+    --eval_dir=${OUTPUT_METRICS_DIR} \
+    --eval_config_path=${OUTPUT_CONFIG_DIR}/eval_config.pbtxt \
+    --input_config_path=${OUTPUT_CONFIG_DIR}/input_config.pbtxt
+```
+
+The result of the evaluation will be stored in `${OUTPUT_METRICS_DIR}/metrics.csv`
+
+For the Object Detection Track, the participants will be ranked on:
+
+- "OpenImagesChallenge2018_Precision/mAP@0.5IOU"
+
+## Visual Relationships Detection Track
+
+The [Visual Relationships Detection metrics](https://storage.googleapis.com/openimages/web/vrd_detection_metric.html) can be directly evaluated using the ground-truth data and model predictions. The evaluation metric implementation is available in the class `VRDRelationDetectionEvaluator`,`VRDPhraseDetectionEvaluator`.
+
+1. Download the ground-truth [visual relationships annotations](https://storage.googleapis.com/openimages/challenge_2018/train/challenge-2018-train-vrd.csv) and [image-level labels](https://storage.googleapis.com/openimages/challenge_2018/train/challenge-2018-train-vrd-labels.csv).
+2. Filter the rows corresponding to the validation set images you want to use and store the results in the same CSV format.
+3. Run the follwing command to produce final metrics:
+
+```
+INPUT_ANNOTATIONS_BOXES=/path/to/challenge-2018-train-vrd.csv
+INPUT_ANNOTATIONS_LABELS=/path/to/challenge-2018-train-vrd-labels.csv
+INPUT_PREDICTIONS=/path/to/predictions.csv
+INPUT_CLASS_LABELMAP=/path/to/oid_object_detection_challenge_500_label_map.pbtxt
+INPUT_RELATIONSHIP_LABELMAP=/path/to/relationships_labelmap.pbtxt
+OUTPUT_METRICS=/path/to/output/metrics/file
+
+echo "item { name: '/m/02gy9n' id: 602 display_name: 'Transparent' }
+item { name: '/m/05z87' id: 603 display_name: 'Plastic' }
+item { name: '/m/0dnr7' id: 604 display_name: '(made of)Textile' }
+item { name: '/m/04lbp' id: 605 display_name: '(made of)Leather' }
+item { name: '/m/083vt' id: 606 display_name: 'Wooden'}
+">>${INPUT_CLASS_LABELMAP}
+
+echo "item { name: 'at' id: 1 display_name: 'at' }
+item { name: 'on' id: 2 display_name: 'on (top of)' }
+item { name: 'holds' id: 3 display_name: 'holds' }
+item { name: 'plays' id: 4 display_name: 'plays' }
+item { name: 'interacts_with' id: 5 display_name: 'interacts with' }
+item { name: 'wears' id: 6 display_name: 'wears' }
+item { name: 'is' id: 7 display_name: 'is' }
+item { name: 'inside_of' id: 8 display_name: 'inside of' }
+item { name: 'under' id: 9 display_name: 'under' }
+item { name: 'hits' id: 10 display_name: 'hits' }
+"> ${INPUT_RELATIONSHIP_LABELMAP}
+
+python object_detection/metrics/oid_vrd_challenge_evaluation.py \
+    --input_annotations_boxes=${INPUT_ANNOTATIONS_BOXES} \
+    --input_annotations_labels=${INPUT_ANNOTATIONS_LABELS} \
+    --input_predictions=${INPUT_PREDICTIONS} \
+    --input_class_labelmap=${INPUT_CLASS_LABELMAP} \
+    --input_relationship_labelmap=${INPUT_RELATIONSHIP_LABELMAP} \
+    --output_metrics=${OUTPUT_METRICS}
+```
+
+The participants of the challenge will be evaluated by weighted average of the following three metrics:
+
+- "VRDMetric_Relationships_mAP@0.5IOU"
+- "VRDMetric_Relationships_Recall@50@0.5IOU"
+- "VRDMetric_Phrases_mAP@0.5IOU"
diff --git a/research/object_detection/g3doc/configuring_jobs.md b/research/object_detection/g3doc/configuring_jobs.md
index 9b042625..c088169b 100644
--- a/research/object_detection/g3doc/configuring_jobs.md
+++ b/research/object_detection/g3doc/configuring_jobs.md
@@ -13,7 +13,7 @@ file is split into 5 parts:
 model parameters (ie. SGD parameters, input preprocessing and feature extractor
 initialization values).
 3. The `eval_config`, which determines what set of metrics will be reported for
-evaluation (currently we only support the PASCAL VOC metrics).
+evaluation.
 4. The `train_input_config`, which defines what dataset the model should be
 trained on.
 5. The `eval_input_config`, which defines what dataset the model will be
@@ -118,6 +118,7 @@ optimizer {
 }
 fine_tune_checkpoint: "/usr/home/username/tmp/model.ckpt-#####"
 from_detection_checkpoint: true
+load_all_detection_checkpoint_vars: true
 gradient_clipping_by_norm: 10.0
 data_augmentation_options {
   random_horizontal_flip {
@@ -130,8 +131,8 @@ data_augmentation_options {
 While optional, it is highly recommended that users utilize other object
 detection checkpoints. Training an object detector from scratch can take days.
 To speed up the training process, it is recommended that users re-use the
-feature extractor parameters from a pre-existing object classification or
-detection checkpoint. `train_config` provides two fields to specify
+feature extractor parameters from a pre-existing image classification or
+object detection checkpoint. `train_config` provides two fields to specify
 pre-existing checkpoints: `fine_tune_checkpoint` and
 `from_detection_checkpoint`. `fine_tune_checkpoint` should provide a path to
 the pre-existing checkpoint
@@ -157,6 +158,8 @@ number of workers, gpu type).
 
 ## Configuring the Evaluator
 
-Currently evaluation is fixed to generating metrics as defined by the PASCAL VOC
-challenge. The parameters for `eval_config` are set to reasonable defaults and
-typically do not need to be configured.
+The main components to set in `eval_config` are `num_examples` and
+`metrics_set`. The parameter `num_examples` indicates the number of batches (
+currently of batch size 1) used for an evaluation cycle, and often is the total
+size of the evaluation dataset. The parameter `metrics_set` indicates which
+metrics to run during evaluation (i.e. `"coco_detection_metrics"`).
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index 37cea8ad..cd188408 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -69,10 +69,10 @@ Some remarks on frozen inference graphs:
 
 | Model name  | Speed (ms) | COCO mAP[^1] | Outputs |
 | ------------ | :--------------: | :--------------: | :-------------: |
-| [ssd_mobilenet_v1_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz) | 30 | 21 | Boxes |
+| [ssd_mobilenet_v1_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz) | 30 | 21 | Boxes |
 | [ssd_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz) | 31 | 22 | Boxes |
 | [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) | 27 | 22 | Boxes |
-| [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz) | 42 | 24 | Boxes |
+| [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz) | 42 | 24 | Boxes |
 | [faster_rcnn_inception_v2_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz) | 58 | 28 | Boxes |
 | [faster_rcnn_resnet50_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz) | 89 | 30 | Boxes |
 | [faster_rcnn_resnet50_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28.tar.gz) | 64 |  | Boxes |
diff --git a/research/object_detection/g3doc/exporting_models.md b/research/object_detection/g3doc/exporting_models.md
index d4735b97..c6440830 100644
--- a/research/object_detection/g3doc/exporting_models.md
+++ b/research/object_detection/g3doc/exporting_models.md
@@ -12,16 +12,25 @@ command from tensorflow/models/research:
 
 ``` bash
 # From tensorflow/models/research/
+INPUT_TYPE=image_tensor
+PIPELINE_CONFIG_PATH={path to pipeline config file}
+TRAINED_CKPT_PREFIX={path to model.ckpt}
+EXPORT_DIR={path to folder that will be used for export}
 python object_detection/export_inference_graph.py \
-    --input_type image_tensor \
-    --pipeline_config_path ${PIPELINE_CONFIG_PATH} \
-    --trained_checkpoint_prefix ${TRAIN_PATH} \
-    --output_directory ${EXPORT_DIR}
+    --input_type=${INPUT_TYPE} \
+    --pipeline_config_path=${PIPELINE_CONFIG_PATH} \
+    --trained_checkpoint_prefix=${TRAINED_CKPT_PREFIX} \
+    --output_directory=${EXPORT_DIR}
 ```
 
-Afterwards, you should see the directory ${EXPORT_DIR} containing the following:
+NOTE: We are configuring our exported model to ingest 4-D image tensors. We can
+also configure the exported model to take encoded images or serialized
+`tf.Example`s.
+
+After export, you should see the directory ${EXPORT_DIR} containing the following:
 
-* output_inference_graph.pb, the frozen graph format of the exported model
 * saved_model/, a directory containing the saved model format of the exported model
+* frozen_inference_graph.pb, the frozen graph format of the exported model
 * model.ckpt.*, the model checkpoints used for exporting
 * checkpoint, a file specifying to restore included checkpoint files
+* pipeline.config, pipeline config file for the exported model
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index 27786c7d..4bb682d8 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -4,7 +4,7 @@
 
 Tensorflow Object Detection API depends on the following libraries:
 
-*   Protobuf 3+
+*   Protobuf 3.0.0
 *   Python-tk
 *   Pillow 1.0
 *   lxml
@@ -13,6 +13,7 @@ Tensorflow Object Detection API depends on the following libraries:
 *   Matplotlib
 *   Tensorflow
 *   Cython
+*   contextlib2
 *   cocoapi
 
 For detailed steps to install Tensorflow, follow the [Tensorflow installation
@@ -30,21 +31,28 @@ The remaining libraries can be installed on Ubuntu 16.04 using via apt-get:
 
 ``` bash
 sudo apt-get install protobuf-compiler python-pil python-lxml python-tk
-sudo pip install Cython
-sudo pip install jupyter
-sudo pip install matplotlib
+pip install --user Cython
+pip install --user contextlib2
+pip install --user jupyter
+pip install --user matplotlib
 ```
 
 Alternatively, users can install dependencies using pip:
 
 ``` bash
-sudo pip install Cython
-sudo pip install pillow
-sudo pip install lxml
-sudo pip install jupyter
-sudo pip install matplotlib
+pip install --user Cython
+pip install --user contextlib2
+pip install --user pillow
+pip install --user lxml
+pip install --user jupyter
+pip install --user matplotlib
 ```
 
+Note that sometimes "sudo apt-get install protobuf-compiler" will install
+Protobuf 3+ versions for you and some users have issues when using 3.5.
+If that is your case, you're suggested to download and install Protobuf 3.0.0
+(available [here](https://github.com/google/protobuf/releases/tag/v3.0.0)).
+
 ## COCO API installation
 
 Download the
diff --git a/research/object_detection/g3doc/oid_inference_and_evaluation.md b/research/object_detection/g3doc/oid_inference_and_evaluation.md
index 93aedf29..facb6232 100644
--- a/research/object_detection/g3doc/oid_inference_and_evaluation.md
+++ b/research/object_detection/g3doc/oid_inference_and_evaluation.md
@@ -13,8 +13,8 @@ inferred detections.
 
 Inferred detections will look like the following:
 
-![](img/oid_bus_72e19c28aac34ed8.jpg){height="300"}
-![](img/oid_monkey_3b4168c89cecbc5b.jpg){height="300"}
+![](img/oid_bus_72e19c28aac34ed8.jpg)
+![](img/oid_monkey_3b4168c89cecbc5b.jpg)
 
 On the validation set of Open Images, this tutorial requires 27GB of free disk
 space and the inference step takes approximately 9 hours on a single NVIDIA
@@ -100,6 +100,8 @@ python -m object_detection/dataset_tools/create_oid_tf_record \
   --num_shards=100
 ```
 
+To add image-level labels, use the `--input_image_label_annotations_csv` flag.
+
 This results in 100 TFRecord files (shards), written to
 `oid/${SPLIT}_tfrecords`, with filenames matching
 `${SPLIT}.tfrecord-000[0-9][0-9]-of-00100`. Each shard contains approximately
@@ -146,7 +148,7 @@ access to the images, `infer_detections` can optionally discard them with the
 `--discard_image_pixels` flag. Discarding the images drastically reduces the
 size of the output TFRecord.
 
-### Accelerating inference {#accelerating_inference}
+### Accelerating inference
 
 Running inference on the whole validation or test set can take a long time to
 complete due to the large number of images present in these sets (41,620 and
@@ -196,7 +198,7 @@ After all `infer_detections` processes finish, `tensorflow/models/research/oid`
 will contain one output TFRecord from each process, with name matching
 `validation_detections.tfrecord-0000[0-3]-of-00004`.
 
-## Computing evaluation measures {#compute_evaluation_measures}
+## Computing evaluation measures
 
 To compute evaluation measures on the inferred detections you first need to
 create the appropriate configuration files:
@@ -237,7 +239,7 @@ file contains an `object_detection.protos.EvalConfig` message that describes the
 evaluation metric. For more information about these protos see the corresponding
 source files.
 
-### Expected mAPs {#expected-maps}
+### Expected mAPs
 
 The result of running `offline_eval_map_corloc` is a CSV file located at
 `${SPLIT}_eval_metrics/metrics.csv`. With the above configuration, the file will
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index c9b3ae25..5afaf851 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -33,8 +33,8 @@ from object_detection.protos import input_reader_pb2
 from object_detection.protos import model_pb2
 from object_detection.protos import train_pb2
 from object_detection.utils import config_util
-from object_detection.utils import dataset_util
 from object_detection.utils import ops as util_ops
+from object_detection.utils import shape_utils
 
 HASH_KEY = 'hash'
 HASH_BINS = 1 << 31
@@ -91,6 +91,9 @@ def transform_input_data(tensor_dict,
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
     after applying all the transformations.
   """
+  if fields.InputDataFields.groundtruth_boxes in tensor_dict:
+    tensor_dict = util_ops.filter_groundtruth_with_nan_box_coordinates(
+        tensor_dict)
   if fields.InputDataFields.image_additional_channels in tensor_dict:
     channels = tensor_dict[fields.InputDataFields.image_additional_channels]
     tensor_dict[fields.InputDataFields.image] = tf.concat(
@@ -135,6 +138,103 @@ def transform_input_data(tensor_dict,
   return tensor_dict
 
 
+def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
+                                    spatial_image_shape=None):
+  """Pads input tensors to static shapes.
+
+  Args:
+    tensor_dict: Tensor dictionary of input data
+    max_num_boxes: Max number of groundtruth boxes needed to compute shapes for
+      padding.
+    num_classes: Number of classes in the dataset needed to compute shapes for
+      padding.
+    spatial_image_shape: A list of two integers of the form [height, width]
+      containing expected spatial shape of the image.
+
+  Returns:
+    A dictionary keyed by fields.InputDataFields containing padding shapes for
+    tensors in the dataset.
+
+  Raises:
+    ValueError: If groundtruth classes is neither rank 1 nor rank 2.
+  """
+
+  if not spatial_image_shape or spatial_image_shape == [-1, -1]:
+    height, width = None, None
+  else:
+    height, width = spatial_image_shape  # pylint: disable=unpacking-non-sequence
+
+  num_additional_channels = 0
+  if fields.InputDataFields.image_additional_channels in tensor_dict:
+    num_additional_channels = tensor_dict[
+        fields.InputDataFields.image_additional_channels].shape[2].value
+  padding_shapes = {
+      # Additional channels are merged before batching.
+      fields.InputDataFields.image: [
+          height, width, 3 + num_additional_channels
+      ],
+      fields.InputDataFields.image_additional_channels: [
+          height, width, num_additional_channels
+      ],
+      fields.InputDataFields.source_id: [],
+      fields.InputDataFields.filename: [],
+      fields.InputDataFields.key: [],
+      fields.InputDataFields.groundtruth_difficult: [max_num_boxes],
+      fields.InputDataFields.groundtruth_boxes: [max_num_boxes, 4],
+      fields.InputDataFields.groundtruth_classes: [max_num_boxes, num_classes],
+      fields.InputDataFields.groundtruth_instance_masks: [
+          max_num_boxes, height, width
+      ],
+      fields.InputDataFields.groundtruth_is_crowd: [max_num_boxes],
+      fields.InputDataFields.groundtruth_group_of: [max_num_boxes],
+      fields.InputDataFields.groundtruth_area: [max_num_boxes],
+      fields.InputDataFields.groundtruth_weights: [max_num_boxes],
+      fields.InputDataFields.num_groundtruth_boxes: [],
+      fields.InputDataFields.groundtruth_label_types: [max_num_boxes],
+      fields.InputDataFields.groundtruth_label_scores: [max_num_boxes],
+      fields.InputDataFields.true_image_shape: [3],
+      fields.InputDataFields.multiclass_scores: [
+          max_num_boxes, num_classes + 1 if num_classes is not None else None
+      ],
+      fields.InputDataFields.groundtruth_image_classes: [num_classes],
+  }
+
+  if fields.InputDataFields.original_image in tensor_dict:
+    padding_shapes[fields.InputDataFields.original_image] = [
+        None, None, 3 + num_additional_channels
+    ]
+  if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
+    tensor_shape = (
+        tensor_dict[fields.InputDataFields.groundtruth_keypoints].shape)
+    padding_shape = [max_num_boxes, tensor_shape[1].value,
+                     tensor_shape[2].value]
+    padding_shapes[fields.InputDataFields.groundtruth_keypoints] = padding_shape
+  if fields.InputDataFields.groundtruth_keypoint_visibilities in tensor_dict:
+    tensor_shape = tensor_dict[fields.InputDataFields.
+                               groundtruth_keypoint_visibilities].shape
+    padding_shape = [max_num_boxes, tensor_shape[1].value]
+    padding_shapes[fields.InputDataFields.
+                   groundtruth_keypoint_visibilities] = padding_shape
+
+  padded_tensor_dict = {}
+  for tensor_name in tensor_dict:
+    expected_shape = padding_shapes[tensor_name]
+    current_shape = shape_utils.combined_static_and_dynamic_shape(
+        tensor_dict[tensor_name])
+    trailing_paddings = [
+        expected_shape_dim - current_shape_dim if expected_shape_dim else 0
+        for expected_shape_dim, current_shape_dim in zip(
+            expected_shape, current_shape)
+    ]
+    paddings = tf.stack([tf.zeros(len(trailing_paddings), dtype=tf.int32),
+                         trailing_paddings],
+                        axis=1)
+    padded_tensor_dict[tensor_name] = tf.pad(
+        tensor_dict[tensor_name], paddings=paddings)
+    padded_tensor_dict[tensor_name].set_shape(expected_shape)
+  return padded_tensor_dict
+
+
 def augment_input_data(tensor_dict, data_augmentation_options):
   """Applies data augmentation ops to input tensors.
 
@@ -231,6 +331,8 @@ def create_train_input_fn(train_config, train_input_config,
       params: Parameter dictionary passed from the estimator.
 
     Returns:
+      A tf.data.Dataset that holds (features, labels) tuple.
+
       features: Dictionary of feature tensors.
         features[fields.InputDataFields.image] is a [batch_size, H, W, C]
           float32 tensor with preprocessed images.
@@ -275,33 +377,39 @@ def create_train_input_fn(train_config, train_input_config,
       raise TypeError('The `model_config` must be a '
                       'model_pb2.DetectionModel.')
 
-    data_augmentation_options = [
-        preprocessor_builder.build(step)
-        for step in train_config.data_augmentation_options
-    ]
-    data_augmentation_fn = functools.partial(
-        augment_input_data, data_augmentation_options=data_augmentation_options)
+    def transform_and_pad_input_data_fn(tensor_dict):
+      """Combines transform and pad operation."""
+      data_augmentation_options = [
+          preprocessor_builder.build(step)
+          for step in train_config.data_augmentation_options
+      ]
+      data_augmentation_fn = functools.partial(
+          augment_input_data,
+          data_augmentation_options=data_augmentation_options)
+      model = model_builder.build(model_config, is_training=True)
+      image_resizer_config = config_util.get_image_resizer_config(model_config)
+      image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+      transform_data_fn = functools.partial(
+          transform_input_data, model_preprocess_fn=model.preprocess,
+          image_resizer_fn=image_resizer_fn,
+          num_classes=config_util.get_number_of_classes(model_config),
+          data_augmentation_fn=data_augmentation_fn,
+          merge_multiple_boxes=train_config.merge_multiple_label_boxes,
+          retain_original_image=train_config.retain_original_images)
+
+      tensor_dict = pad_input_data_to_static_shapes(
+          tensor_dict=transform_data_fn(tensor_dict),
+          max_num_boxes=train_input_config.max_number_of_boxes,
+          num_classes=config_util.get_number_of_classes(model_config),
+          spatial_image_shape=config_util.get_spatial_image_size(
+              image_resizer_config))
+      return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
 
-    model = model_builder.build(model_config, is_training=True)
-    image_resizer_config = config_util.get_image_resizer_config(model_config)
-    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-
-    transform_data_fn = functools.partial(
-        transform_input_data, model_preprocess_fn=model.preprocess,
-        image_resizer_fn=image_resizer_fn,
-        num_classes=config_util.get_number_of_classes(model_config),
-        data_augmentation_fn=data_augmentation_fn,
-        retain_original_image=train_config.retain_original_images)
     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
         train_input_config,
-        transform_input_data_fn=transform_data_fn,
-        batch_size=params['batch_size'] if params else train_config.batch_size,
-        max_num_boxes=train_config.max_number_of_boxes,
-        num_classes=config_util.get_number_of_classes(model_config),
-        spatial_image_shape=config_util.get_spatial_image_size(
-            image_resizer_config))
-    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()
-    return (_get_features_dict(input_dict), _get_labels_dict(input_dict))
+        transform_input_data_fn=transform_and_pad_input_data_fn,
+        batch_size=params['batch_size'] if params else train_config.batch_size)
+    return dataset
 
   return _train_input_fn
 
@@ -309,6 +417,8 @@ def create_train_input_fn(train_config, train_input_config,
 def create_eval_input_fn(eval_config, eval_input_config, model_config):
   """Creates an eval `input` function for `Estimator`.
 
+  # TODO(ronnyvotel,rathodv): Allow batch sizes of more than 1 for eval.
+
   Args:
     eval_config: An eval_pb2.EvalConfig.
     eval_input_config: An input_reader_pb2.InputReader.
@@ -325,6 +435,8 @@ def create_eval_input_fn(eval_config, eval_input_config, model_config):
       params: Parameter dictionary passed from the estimator.
 
     Returns:
+      A tf.data.Dataset that holds (features, labels) tuple.
+
       features: Dictionary of feature tensors.
         features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor
           with preprocessed images.
@@ -366,36 +478,41 @@ def create_eval_input_fn(eval_config, eval_input_config, model_config):
       raise TypeError('The `model_config` must be a '
                       'model_pb2.DetectionModel.')
 
-    num_classes = config_util.get_number_of_classes(model_config)
-    model = model_builder.build(model_config, is_training=False)
-    image_resizer_config = config_util.get_image_resizer_config(model_config)
-    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
-
-    transform_data_fn = functools.partial(
-        transform_input_data, model_preprocess_fn=model.preprocess,
-        image_resizer_fn=image_resizer_fn,
-        num_classes=num_classes,
-        data_augmentation_fn=None,
-        retain_original_image=eval_config.retain_original_images)
+    def transform_and_pad_input_data_fn(tensor_dict):
+      """Combines transform and pad operation."""
+      num_classes = config_util.get_number_of_classes(model_config)
+      model = model_builder.build(model_config, is_training=False)
+      image_resizer_config = config_util.get_image_resizer_config(model_config)
+      image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+
+      transform_data_fn = functools.partial(
+          transform_input_data, model_preprocess_fn=model.preprocess,
+          image_resizer_fn=image_resizer_fn,
+          num_classes=num_classes,
+          data_augmentation_fn=None,
+          retain_original_image=eval_config.retain_original_images)
+      tensor_dict = pad_input_data_to_static_shapes(
+          tensor_dict=transform_data_fn(tensor_dict),
+          max_num_boxes=eval_input_config.max_number_of_boxes,
+          num_classes=config_util.get_number_of_classes(model_config),
+          spatial_image_shape=config_util.get_spatial_image_size(
+              image_resizer_config))
+      return (_get_features_dict(tensor_dict), _get_labels_dict(tensor_dict))
     dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
         eval_input_config,
-        transform_input_data_fn=transform_data_fn,
-        batch_size=params.get('batch_size', 1),
-        num_classes=config_util.get_number_of_classes(model_config),
-        spatial_image_shape=config_util.get_spatial_image_size(
-            image_resizer_config))
-    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()
-
-    return (_get_features_dict(input_dict), _get_labels_dict(input_dict))
+        batch_size=1,  # Currently only support batch size of 1 for eval.
+        transform_input_data_fn=transform_and_pad_input_data_fn)
+    return dataset
 
   return _eval_input_fn
 
 
-def create_predict_input_fn(model_config):
+def create_predict_input_fn(model_config, predict_input_config):
   """Creates a predict `input` function for `Estimator`.
 
   Args:
     model_config: A model_pb2.DetectionModel.
+    predict_input_config: An input_reader_pb2.InputReader.
 
   Returns:
     `input_fn` for `Estimator` in PREDICT mode.
@@ -424,7 +541,9 @@ def create_predict_input_fn(model_config):
         num_classes=num_classes,
         data_augmentation_fn=None)
 
-    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)
+    decoder = tf_example_decoder.TfExampleDecoder(
+        load_instance_masks=False,
+        num_additional_channels=predict_input_config.num_additional_channels)
     input_dict = transform_fn(decoder.decode(example))
     images = tf.to_float(input_dict[fields.InputDataFields.image])
     images = tf.expand_dims(images, axis=0)
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index 83266335..009c44c7 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -48,17 +48,30 @@ def _get_configs_for_model(model_name):
       label_map_path=label_map_path)
 
 
+def _make_initializable_iterator(dataset):
+  """Creates an iterator, and initializes tables.
+
+  Args:
+    dataset: A `tf.data.Dataset` object.
+
+  Returns:
+    A `tf.data.Iterator`.
+  """
+  iterator = dataset.make_initializable_iterator()
+  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  return iterator
+
+
 class InputsTest(tf.test.TestCase):
 
   def test_faster_rcnn_resnet50_train_input(self):
     """Tests the training input function for FasterRcnnResnet50."""
     configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
-    configs['train_config'].unpad_groundtruth_tensors = True
     model_config = configs['model']
     model_config.faster_rcnn.num_classes = 37
     train_input_fn = inputs.create_train_input_fn(
         configs['train_config'], configs['train_input_config'], model_config)
-    features, labels = train_input_fn()
+    features, labels = _make_initializable_iterator(train_input_fn()).get_next()
 
     self.assertAllEqual([1, None, None, 3],
                         features[fields.InputDataFields.image].shape.as_list())
@@ -67,17 +80,17 @@ class InputsTest(tf.test.TestCase):
                         features[inputs.HASH_KEY].shape.as_list())
     self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
     self.assertAllEqual(
-        [1, 50, 4],
+        [1, 100, 4],
         labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_boxes].dtype)
     self.assertAllEqual(
-        [1, 50, model_config.faster_rcnn.num_classes],
+        [1, 100, model_config.faster_rcnn.num_classes],
         labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_classes].dtype)
     self.assertAllEqual(
-        [1, 50],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_weights].dtype)
@@ -89,8 +102,7 @@ class InputsTest(tf.test.TestCase):
     model_config.faster_rcnn.num_classes = 37
     eval_input_fn = inputs.create_eval_input_fn(
         configs['eval_config'], configs['eval_input_config'], model_config)
-    features, labels = eval_input_fn()
-
+    features, labels = _make_initializable_iterator(eval_input_fn()).get_next()
     self.assertAllEqual([1, None, None, 3],
                         features[fields.InputDataFields.image].shape.as_list())
     self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
@@ -102,27 +114,27 @@ class InputsTest(tf.test.TestCase):
     self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())
     self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
     self.assertAllEqual(
-        [1, None, 4],
+        [1, 100, 4],
         labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_boxes].dtype)
     self.assertAllEqual(
-        [1, None, model_config.faster_rcnn.num_classes],
+        [1, 100, model_config.faster_rcnn.num_classes],
         labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_classes].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_area].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_area].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())
     self.assertEqual(
         tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())
     self.assertEqual(
         tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
@@ -135,7 +147,7 @@ class InputsTest(tf.test.TestCase):
     batch_size = configs['train_config'].batch_size
     train_input_fn = inputs.create_train_input_fn(
         configs['train_config'], configs['train_input_config'], model_config)
-    features, labels = train_input_fn()
+    features, labels = _make_initializable_iterator(train_input_fn()).get_next()
 
     self.assertAllEqual([batch_size, 300, 300, 3],
                         features[fields.InputDataFields.image].shape.as_list())
@@ -149,17 +161,17 @@ class InputsTest(tf.test.TestCase):
     self.assertEqual(tf.int32,
                      labels[fields.InputDataFields.num_groundtruth_boxes].dtype)
     self.assertAllEqual(
-        [batch_size, 50, 4],
+        [batch_size, 100, 4],
         labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_boxes].dtype)
     self.assertAllEqual(
-        [batch_size, 50, model_config.ssd.num_classes],
+        [batch_size, 100, model_config.ssd.num_classes],
         labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_classes].dtype)
     self.assertAllEqual(
-        [batch_size, 50],
+        [batch_size, 100],
         labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_weights].dtype)
@@ -171,8 +183,7 @@ class InputsTest(tf.test.TestCase):
     model_config.ssd.num_classes = 37
     eval_input_fn = inputs.create_eval_input_fn(
         configs['eval_config'], configs['eval_input_config'], model_config)
-    features, labels = eval_input_fn()
-
+    features, labels = _make_initializable_iterator(eval_input_fn()).get_next()
     self.assertAllEqual([1, 300, 300, 3],
                         features[fields.InputDataFields.image].shape.as_list())
     self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
@@ -184,27 +195,27 @@ class InputsTest(tf.test.TestCase):
     self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())
     self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
     self.assertAllEqual(
-        [1, None, 4],
+        [1, 100, 4],
         labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_boxes].dtype)
     self.assertAllEqual(
-        [1, None, model_config.ssd.num_classes],
+        [1, 100, model_config.ssd.num_classes],
         labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_classes].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_area].shape.as_list())
     self.assertEqual(tf.float32,
                      labels[fields.InputDataFields.groundtruth_area].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())
     self.assertEqual(
         tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)
     self.assertAllEqual(
-        [1, None],
+        [1, 100],
         labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())
     self.assertEqual(
         tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
@@ -213,7 +224,8 @@ class InputsTest(tf.test.TestCase):
     """Tests the predict input function."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
     predict_input_fn = inputs.create_predict_input_fn(
-        model_config=configs['model'])
+        model_config=configs['model'],
+        predict_input_config=configs['eval_input_config'])
     serving_input_receiver = predict_input_fn()
 
     image = serving_input_receiver.features[fields.InputDataFields.image]
@@ -223,6 +235,23 @@ class InputsTest(tf.test.TestCase):
     self.assertEqual(tf.float32, image.dtype)
     self.assertEqual(tf.string, receiver_tensors.dtype)
 
+  def test_predict_input_with_additional_channels(self):
+    """Tests the predict input function with additional channels."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    configs['eval_input_config'].num_additional_channels = 2
+    predict_input_fn = inputs.create_predict_input_fn(
+        model_config=configs['model'],
+        predict_input_config=configs['eval_input_config'])
+    serving_input_receiver = predict_input_fn()
+
+    image = serving_input_receiver.features[fields.InputDataFields.image]
+    receiver_tensors = serving_input_receiver.receiver_tensors[
+        inputs.SERVING_FED_EXAMPLE_KEY]
+    # RGB + 2 additional channels = 5 channels.
+    self.assertEqual([1, 300, 300, 5], image.shape.as_list())
+    self.assertEqual(tf.float32, image.dtype)
+    self.assertEqual(tf.string, receiver_tensors.dtype)
+
   def test_error_with_bad_train_config(self):
     """Tests that a TypeError is raised with improper train config."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
@@ -597,5 +626,93 @@ class DataTransformationFnTest(tf.test.TestCase):
                         (np_image + 5) * 2)
 
 
+class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
+
+  def test_pad_images_boxes_and_classes(self):
+    input_tensor_dict = {
+        fields.InputDataFields.image:
+            tf.placeholder(tf.float32, [None, None, 3]),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.placeholder(tf.float32, [None, 4]),
+        fields.InputDataFields.groundtruth_classes:
+            tf.placeholder(tf.int32, [None, 3]),
+        fields.InputDataFields.true_image_shape: tf.placeholder(tf.int32, [3]),
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[5, 6])
+
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.image].shape.as_list(),
+        [5, 6, 3])
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.true_image_shape]
+        .shape.as_list(), [3])
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.groundtruth_boxes]
+        .shape.as_list(), [3, 4])
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.groundtruth_classes]
+        .shape.as_list(), [3, 3])
+
+  def test_do_not_pad_dynamic_images(self):
+    input_tensor_dict = {
+        fields.InputDataFields.image:
+            tf.placeholder(tf.float32, [None, None, 3]),
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[None, None])
+
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.image].shape.as_list(),
+        [None, None, 3])
+
+  def test_images_and_additional_channels(self):
+    input_tensor_dict = {
+        fields.InputDataFields.image:
+            tf.placeholder(tf.float32, [None, None, 3]),
+        fields.InputDataFields.image_additional_channels:
+            tf.placeholder(tf.float32, [None, None, 2]),
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[5, 6])
+
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.image].shape.as_list(),
+        [5, 6, 5])
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.image_additional_channels]
+        .shape.as_list(), [5, 6, 2])
+
+  def test_keypoints(self):
+    input_tensor_dict = {
+        fields.InputDataFields.groundtruth_keypoints:
+            tf.placeholder(tf.float32, [None, 16, 4]),
+        fields.InputDataFields.groundtruth_keypoint_visibilities:
+            tf.placeholder(tf.bool, [None, 16]),
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[5, 6])
+
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.groundtruth_keypoints]
+        .shape.as_list(), [3, 16, 4])
+    self.assertAllEqual(
+        padded_tensor_dict[
+            fields.InputDataFields.groundtruth_keypoint_visibilities]
+        .shape.as_list(), [3, 16])
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 6315d4df..57af6faf 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -253,7 +253,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                second_stage_mask_prediction_loss_weight=1.0,
                hard_example_miner=None,
                parallel_iterations=16,
-               add_summaries=True):
+               add_summaries=True,
+               use_matmul_crop_and_resize=False):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -360,6 +361,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         in parallel for calls to tf.map_fn.
       add_summaries: boolean (default: True) controlling whether summary ops
         should be added to tensorflow graph.
+      use_matmul_crop_and_resize: Force the use of matrix multiplication based
+        crop and resize instead of standard tf.image.crop_and_resize while
+        computing second stage input feature maps.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
@@ -446,6 +450,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._second_stage_cls_loss_weight = second_stage_classification_loss_weight
     self._second_stage_mask_loss_weight = (
         second_stage_mask_prediction_loss_weight)
+    self._use_matmul_crop_and_resize = use_matmul_crop_and_resize
     self._hard_example_miner = hard_example_miner
     self._parallel_iterations = parallel_iterations
 
@@ -1429,11 +1434,26 @@ class FasterRCNNMetaArch(model.DetectionModel):
           tf.range(start=0, limit=proposals_shape[0]), 1)
       return tf.reshape(ones_mat * multiplier, [-1])
 
-    cropped_regions = tf.image.crop_and_resize(
-        features_to_crop,
-        self._flatten_first_two_dimensions(proposal_boxes_normalized),
-        get_box_inds(proposal_boxes_normalized),
-        (self._initial_crop_size, self._initial_crop_size))
+    if self._use_matmul_crop_and_resize:
+      def _single_image_crop_and_resize(inputs):
+        single_image_features_to_crop, proposal_boxes_normalized = inputs
+        return ops.matmul_crop_and_resize(
+            tf.expand_dims(single_image_features_to_crop, 0),
+            proposal_boxes_normalized,
+            [self._initial_crop_size, self._initial_crop_size])
+
+      cropped_regions = self._flatten_first_two_dimensions(
+          shape_utils.static_or_dynamic_map_fn(
+              _single_image_crop_and_resize,
+              elems=[features_to_crop, proposal_boxes_normalized],
+              dtype=tf.float32,
+              parallel_iterations=self._parallel_iterations))
+    else:
+      cropped_regions = tf.image.crop_and_resize(
+          features_to_crop,
+          self._flatten_first_two_dimensions(proposal_boxes_normalized),
+          get_box_inds(proposal_boxes_normalized),
+          (self._initial_crop_size, self._initial_crop_size))
     return slim.max_pool2d(
         cropped_regions,
         [self._maxpool_kernel_size, self._maxpool_kernel_size],
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 00c70540..a1927b0d 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -152,7 +152,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
                    softmax_second_stage_classification_loss=True,
                    predict_masks=False,
                    pad_to_max_dimension=None,
-                   masks_are_class_agnostic=False):
+                   masks_are_class_agnostic=False,
+                   use_matmul_crop_and_resize=False):
 
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
@@ -287,7 +288,9 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         second_stage_classification_loss_weight,
         'second_stage_classification_loss':
         second_stage_classification_loss,
-        'hard_example_miner': hard_example_miner}
+        'hard_example_miner': hard_example_miner,
+        'use_matmul_crop_and_resize': use_matmul_crop_and_resize
+    }
 
     return self._get_model(
         self._get_second_stage_box_predictor(
@@ -465,14 +468,16 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       for key in expected_shapes:
         self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
-  def test_predict_gives_correct_shapes_in_train_mode_both_stages(self):
+  def _test_predict_gives_correct_shapes_in_train_mode_both_stages(
+      self, use_matmul_crop_and_resize=False):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
           is_training=True,
           number_of_stages=2,
           second_stage_batch_size=7,
-          predict_masks=False)
+          predict_masks=False,
+          use_matmul_crop_and_resize=use_matmul_crop_and_resize)
 
       batch_size = 2
       image_size = 10
@@ -535,6 +540,13 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
             tensor_dict_out['rpn_objectness_predictions_with_background'].shape,
             (2, num_anchors_out, 2))
 
+  def test_predict_gives_correct_shapes_in_train_mode_both_stages(self):
+    self._test_predict_gives_correct_shapes_in_train_mode_both_stages()
+
+  def test_predict_gives_correct_shapes_in_train_mode_matmul_crop_resize(self):
+    self._test_predict_gives_correct_shapes_in_train_mode_both_stages(
+        use_matmul_crop_and_resize=True)
+
   def _test_postprocess_first_stage_only_inference_mode(
       self, pad_to_max_dimension=None):
     model = self._build_model(
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 36554493..4e39070d 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -76,7 +76,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                second_stage_classification_loss,
                hard_example_miner,
                parallel_iterations=16,
-               add_summaries=True):
+               add_summaries=True,
+               use_matmul_crop_and_resize=False):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -159,14 +160,17 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         in parallel for calls to tf.map_fn.
       add_summaries: boolean (default: True) controlling whether summary ops
         should be added to tensorflow graph.
+      use_matmul_crop_and_resize: Force the use of matrix multiplication based
+        crop and resize instead of standard tf.image.crop_and_resize while
+        computing second stage input feature maps.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
       ValueError: If first_stage_anchor_generator is not of type
         grid_anchor_generator.GridAnchorGenerator.
     """
-    # TODO(rathodv): add_summaries is currently unused. Respect that directive
-    # in the future.
+    # TODO(rathodv): add_summaries and crop_and_resize_fn is currently
+    # unused. Respect that directive in the future.
     super(RFCNMetaArch, self).__init__(
         is_training,
         num_classes,
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index ffbe9148..2be5f6ce 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -480,12 +480,16 @@ class SSDMetaArch(model.DetectionModel):
     with tf.name_scope('Postprocessor'):
       preprocessed_images = prediction_dict['preprocessed_inputs']
       box_encodings = prediction_dict['box_encodings']
+      box_encodings = tf.identity(box_encodings, 'raw_box_encodings')
       class_predictions = prediction_dict['class_predictions_with_background']
       detection_boxes, detection_keypoints = self._batch_decode(box_encodings)
+      detection_boxes = tf.identity(detection_boxes, 'raw_box_locations')
       detection_boxes = tf.expand_dims(detection_boxes, axis=2)
 
       detection_scores_with_background = self._score_conversion_fn(
           class_predictions)
+      detection_scores_with_background = tf.identity(
+          detection_scores_with_background, 'raw_box_scores')
       detection_scores = tf.slice(detection_scores_with_background, [0, 0, 1],
                                   [-1, -1, -1])
       additional_fields = None
diff --git a/research/object_detection/metrics/io_utils.py b/research/object_detection/metrics/io_utils.py
new file mode 100644
index 00000000..900584de
--- /dev/null
+++ b/research/object_detection/metrics/io_utils.py
@@ -0,0 +1,34 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Common IO utils used in offline metric computation.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import csv
+
+
+def write_csv(fid, metrics):
+  """Writes metrics key-value pairs to CSV file.
+
+  Args:
+    fid: File identifier of an opened file.
+    metrics: A dictionary with metrics to be written.
+  """
+  metrics_writer = csv.writer(fid, delimiter=',')
+  for metric_name, metric_value in metrics.items():
+    metrics_writer.writerow([metric_name, str(metric_value)])
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation.py b/research/object_detection/metrics/oid_od_challenge_evaluation.py
new file mode 100644
index 00000000..6e6e7ac3
--- /dev/null
+++ b/research/object_detection/metrics/oid_od_challenge_evaluation.py
@@ -0,0 +1,128 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Runs evaluation using OpenImages groundtruth and predictions.
+
+Example usage:
+python models/research/object_detection/metrics/oid_od_challenge_evaluation.py \
+    --input_annotations_boxes=/path/to/input/annotations-human-bbox.csv \
+    --input_annotations_labels=/path/to/input/annotations-label.csv \
+    --input_class_labelmap=/path/to/input/class_labelmap.pbtxt \
+    --input_predictions=/path/to/input/predictions.csv \
+    --output_metrics=/path/to/output/metric.csv \
+
+CSVs with bounding box annotations and image label (including the image URLs)
+can be downloaded from the Open Images Challenge website:
+https://storage.googleapis.com/openimages/web/challenge.html
+The format of the input csv and the metrics itself are described on the
+challenge website.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import pandas as pd
+from google.protobuf import text_format
+
+from object_detection.metrics import io_utils
+from object_detection.metrics import oid_od_challenge_evaluation_utils as utils
+from object_detection.protos import string_int_label_map_pb2
+from object_detection.utils import object_detection_evaluation
+
+
+def _load_labelmap(labelmap_path):
+  """Loads labelmap from the labelmap path.
+
+  Args:
+    labelmap_path: Path to the labelmap.
+
+  Returns:
+    A dictionary mapping class name to class numerical id
+    A list with dictionaries, one dictionary per category.
+  """
+
+  label_map = string_int_label_map_pb2.StringIntLabelMap()
+  with open(labelmap_path, 'r') as fid:
+    label_map_string = fid.read()
+    text_format.Merge(label_map_string, label_map)
+  labelmap_dict = {}
+  categories = []
+  for item in label_map.item:
+    labelmap_dict[item.name] = item.id
+    categories.append({'id': item.id, 'name': item.name})
+  return labelmap_dict, categories
+
+
+def main(parsed_args):
+  all_box_annotations = pd.read_csv(parsed_args.input_annotations_boxes)
+  all_label_annotations = pd.read_csv(parsed_args.input_annotations_labels)
+  all_label_annotations.rename(
+      columns={'Confidence': 'ConfidenceImageLabel'}, inplace=True)
+  all_annotations = pd.concat([all_box_annotations, all_label_annotations])
+
+  class_label_map, categories = _load_labelmap(parsed_args.input_class_labelmap)
+  challenge_evaluator = (
+      object_detection_evaluation.OpenImagesDetectionChallengeEvaluator(
+          categories))
+
+  for _, groundtruth in enumerate(all_annotations.groupby('ImageID')):
+    image_id, image_groundtruth = groundtruth
+    groundtruth_dictionary = utils.build_groundtruth_boxes_dictionary(
+        image_groundtruth, class_label_map)
+    challenge_evaluator.add_single_ground_truth_image_info(
+        image_id, groundtruth_dictionary)
+
+  all_predictions = pd.read_csv(parsed_args.input_predictions)
+  for _, prediction_data in enumerate(all_predictions.groupby('ImageID')):
+    image_id, image_predictions = prediction_data
+    prediction_dictionary = utils.build_predictions_dictionary(
+        image_predictions, class_label_map)
+    challenge_evaluator.add_single_detected_image_info(image_id,
+                                                       prediction_dictionary)
+
+  metrics = challenge_evaluator.evaluate()
+
+  with open(parsed_args.output_metrics, 'w') as fid:
+    io_utils.write_csv(fid, metrics)
+
+
+if __name__ == '__main__':
+
+  parser = argparse.ArgumentParser(
+      description='Evaluate Open Images Object Detection Challenge predictions.'
+  )
+  parser.add_argument(
+      '--input_annotations_boxes',
+      required=True,
+      help='File with groundtruth boxes annotations.')
+  parser.add_argument(
+      '--input_annotations_labels',
+      required=True,
+      help='File with groundtruth labels annotations')
+  parser.add_argument(
+      '--input_predictions',
+      required=True,
+      help="""File with detection predictions; NOTE: no postprocessing is
+      applied in the evaluation script.""")
+  parser.add_argument(
+      '--input_class_labelmap',
+      required=True,
+      help='Open Images Challenge labelmap.')
+  parser.add_argument(
+      '--output_metrics', required=True, help='Output file with csv metrics')
+
+  args = parser.parse_args()
+  main(args)
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py b/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py
new file mode 100644
index 00000000..bfd3ee3f
--- /dev/null
+++ b/research/object_detection/metrics/oid_od_challenge_evaluation_utils.py
@@ -0,0 +1,90 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Converts data from CSV to the OpenImagesDetectionChallengeEvaluator format.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from object_detection.core import standard_fields
+
+
+def build_groundtruth_boxes_dictionary(data, class_label_map):
+  """Builds a groundtruth dictionary from groundtruth data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the groundtruth data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+
+  Returns:
+    A dictionary with keys suitable for passing to
+    OpenImagesDetectionChallengeEvaluator.add_single_ground_truth_image_info:
+        standard_fields.InputDataFields.groundtruth_boxes: float32 numpy array
+          of shape [num_boxes, 4] containing `num_boxes` groundtruth boxes of
+          the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        standard_fields.InputDataFields.groundtruth_classes: integer numpy array
+          of shape [num_boxes] containing 1-indexed groundtruth classes for the
+          boxes.
+        standard_fields.InputDataFields.verified_labels: integer 1D numpy array
+          containing all classes for which labels are verified.
+        standard_fields.InputDataFields.groundtruth_group_of: Optional length
+          M numpy boolean array denoting whether a groundtruth box contains a
+          group of instances.
+  """
+  data_boxes = data[data.ConfidenceImageLabel.isnull()]
+  data_labels = data[data.XMin.isnull()]
+
+  return {
+      standard_fields.InputDataFields.groundtruth_boxes:
+          data_boxes[['YMin', 'XMin', 'YMax', 'XMax']].as_matrix(),
+      standard_fields.InputDataFields.groundtruth_classes:
+          data_boxes['LabelName'].map(lambda x: class_label_map[x]).as_matrix(),
+      standard_fields.InputDataFields.groundtruth_group_of:
+          data_boxes['IsGroupOf'].as_matrix().astype(int),
+      standard_fields.InputDataFields.groundtruth_image_classes:
+          data_labels['LabelName'].map(lambda x: class_label_map[x])
+          .as_matrix(),
+  }
+
+
+def build_predictions_dictionary(data, class_label_map):
+  """Builds a predictions dictionary from predictions data in CSV file.
+
+  Args:
+    data: Pandas DataFrame with the predictions data for a single image.
+    class_label_map: Class labelmap from string label name to an integer.
+
+  Returns:
+    Dictionary with keys suitable for passing to
+    OpenImagesDetectionChallengeEvaluator.add_single_detected_image_info:
+        standard_fields.DetectionResultFields.detection_boxes: float32 numpy
+          array of shape [num_boxes, 4] containing `num_boxes` detection boxes
+          of the format [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        standard_fields.DetectionResultFields.detection_scores: float32 numpy
+          array of shape [num_boxes] containing detection scores for the boxes.
+        standard_fields.DetectionResultFields.detection_classes: integer numpy
+          array of shape [num_boxes] containing 1-indexed detection classes for
+          the boxes.
+
+  """
+  return {
+      standard_fields.DetectionResultFields.detection_boxes:
+          data[['YMin', 'XMin', 'YMax', 'XMax']].as_matrix(),
+      standard_fields.DetectionResultFields.detection_classes:
+          data['LabelName'].map(lambda x: class_label_map[x]).as_matrix(),
+      standard_fields.DetectionResultFields.detection_scores:
+          data['Score'].as_matrix()
+  }
diff --git a/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py
new file mode 100644
index 00000000..a7a1a585
--- /dev/null
+++ b/research/object_detection/metrics/oid_od_challenge_evaluation_utils_test.py
@@ -0,0 +1,103 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for oid_od_challenge_evaluation_util."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import pandas as pd
+import tensorflow as tf
+from object_detection.core import standard_fields
+from object_detection.metrics import oid_od_challenge_evaluation_utils as utils
+
+
+class OidOdChallengeEvaluationUtilTest(tf.test.TestCase):
+
+  def testBuildGroundtruthDictionary(self):
+    np_data = pd.DataFrame(
+        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 1, None], [
+            'fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0, None
+        ], ['fe58ec1b06db2bb7', '/m/04bcr3', None, None, None, None, None, 1], [
+            'fe58ec1b06db2bb7', '/m/083vt', None, None, None, None, None, 0
+        ], ['fe58ec1b06db2bb7', '/m/02gy9n', None, None, None, None, None, 1]],
+        columns=[
+            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'IsGroupOf',
+            'ConfidenceImageLabel'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    groundtruth_dictionary = utils.build_groundtruth_boxes_dictionary(
+        np_data, class_label_map)
+
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_boxes in
+                    groundtruth_dictionary)
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_classes in
+                    groundtruth_dictionary)
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_group_of in
+                    groundtruth_dictionary)
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_image_classes in
+                    groundtruth_dictionary)
+
+    self.assertAllEqual(
+        np.array([1, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_classes])
+    self.assertAllEqual(
+        np.array([1, 0]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_group_of])
+
+    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2]])
+
+    self.assertNDArrayNear(
+        expected_boxes_data, groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_boxes], 1e-5)
+    self.assertAllEqual(
+        np.array([1, 2, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_image_classes])
+
+  def testBuildPredictionDictionary(self):
+    np_data = pd.DataFrame(
+        [['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.3, 0.5, 0.6, 0.1], [
+            'fe58ec1b06db2bb7', '/m/02gy9n', 0.1, 0.2, 0.3, 0.4, 0.2
+        ], ['fe58ec1b06db2bb7', '/m/04bcr3', 0.0, 0.1, 0.2, 0.3, 0.3]],
+        columns=[
+            'ImageID', 'LabelName', 'XMin', 'XMax', 'YMin', 'YMax', 'Score'
+        ])
+    class_label_map = {'/m/04bcr3': 1, '/m/083vt': 2, '/m/02gy9n': 3}
+    prediction_dictionary = utils.build_predictions_dictionary(
+        np_data, class_label_map)
+
+    self.assertTrue(standard_fields.DetectionResultFields.detection_boxes in
+                    prediction_dictionary)
+    self.assertTrue(standard_fields.DetectionResultFields.detection_classes in
+                    prediction_dictionary)
+    self.assertTrue(standard_fields.DetectionResultFields.detection_scores in
+                    prediction_dictionary)
+
+    self.assertAllEqual(
+        np.array([1, 3, 1]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_classes])
+    expected_boxes_data = np.array([[0.5, 0.0, 0.6, 0.3], [0.3, 0.1, 0.4, 0.2],
+                                    [0.2, 0.0, 0.3, 0.1]])
+    self.assertNDArrayNear(
+        expected_boxes_data, prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_boxes], 1e-5)
+    self.assertNDArrayNear(
+        np.array([0.1, 0.2, 0.3]), prediction_dictionary[
+            standard_fields.DetectionResultFields.detection_scores], 1e-5)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
index 3c8bb54a..20b93e66 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation.py
@@ -15,8 +15,8 @@
 r"""Runs evaluation using OpenImages groundtruth and predictions.
 
 Example usage:
-  python third_party/tensorflow_models/object_detection/\
-  metrics/oid_vrd_challenge_evaluation.py \
+python \
+models/research/object_detection/metrics/oid_vrd_challenge_evaluation.py \
     --input_annotations_boxes=/path/to/input/annotations-human-bbox.csv \
     --input_annotations_labels=/path/to/input/annotations-label.csv \
     --input_class_labelmap=/path/to/input/class_labelmap.pbtxt \
@@ -39,6 +39,7 @@ import argparse
 import pandas as pd
 from google.protobuf import text_format
 
+from object_detection.metrics import io_utils
 from object_detection.metrics import oid_vrd_challenge_evaluation_utils as utils
 from object_detection.protos import string_int_label_map_pb2
 from object_detection.utils import vrd_evaluation
@@ -109,12 +110,14 @@ def main(parsed_args):
     phrase_evaluator.add_single_detected_image_info(image_id,
                                                     prediction_dictionary)
 
-  relation_metrics = relation_evaluator.evaluate()
-  phrase_metrics = phrase_evaluator.evaluate()
+  relation_metrics = relation_evaluator.evaluate(
+      relationships=_swap_labelmap_dict(relationship_label_map))
+  phrase_metrics = phrase_evaluator.evaluate(
+      relationships=_swap_labelmap_dict(relationship_label_map))
 
   with open(parsed_args.output_metrics, 'w') as fid:
-    utils.write_csv(fid, relation_metrics)
-    utils.write_csv(fid, phrase_metrics)
+    io_utils.write_csv(fid, relation_metrics)
+    io_utils.write_csv(fid, phrase_metrics)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py
index 8c834775..34be018c 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils.py
@@ -18,7 +18,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import csv
 import numpy as np
 from object_detection.core import standard_fields
 from object_detection.utils import vrd_evaluation
@@ -58,18 +57,21 @@ def build_groundtruth_vrd_dictionary(data, class_label_map,
   boxes['object'] = data_boxes[['YMin2', 'XMin2', 'YMax2', 'XMax2']].as_matrix()
 
   labels = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.label_data_type)
-  labels['subject'] = data_boxes['LabelName1'].map(lambda x: class_label_map[x])
-  labels['object'] = data_boxes['LabelName2'].map(lambda x: class_label_map[x])
+  labels['subject'] = data_boxes['LabelName1'].map(
+      lambda x: class_label_map[x]).as_matrix()
+  labels['object'] = data_boxes['LabelName2'].map(
+      lambda x: class_label_map[x]).as_matrix()
   labels['relation'] = data_boxes['RelationshipLabel'].map(
-      lambda x: relationship_label_map[x])
+      lambda x: relationship_label_map[x]).as_matrix()
 
   return {
       standard_fields.InputDataFields.groundtruth_boxes:
           boxes,
       standard_fields.InputDataFields.groundtruth_classes:
           labels,
-      standard_fields.InputDataFields.verified_labels:
-          data_labels['LabelName'].map(lambda x: class_label_map[x]),
+      standard_fields.InputDataFields.groundtruth_image_classes:
+          data_labels['LabelName'].map(lambda x: class_label_map[x])
+          .as_matrix(),
   }
 
 
@@ -106,10 +108,12 @@ def build_predictions_vrd_dictionary(data, class_label_map,
   boxes['object'] = data_boxes[['YMin2', 'XMin2', 'YMax2', 'XMax2']].as_matrix()
 
   labels = np.zeros(data_boxes.shape[0], dtype=vrd_evaluation.label_data_type)
-  labels['subject'] = data_boxes['LabelName1'].map(lambda x: class_label_map[x])
-  labels['object'] = data_boxes['LabelName2'].map(lambda x: class_label_map[x])
+  labels['subject'] = data_boxes['LabelName1'].map(
+      lambda x: class_label_map[x]).as_matrix()
+  labels['object'] = data_boxes['LabelName2'].map(
+      lambda x: class_label_map[x]).as_matrix()
   labels['relation'] = data_boxes['RelationshipLabel'].map(
-      lambda x: relationship_label_map[x])
+      lambda x: relationship_label_map[x]).as_matrix()
 
   return {
       standard_fields.DetectionResultFields.detection_boxes:
@@ -119,15 +123,3 @@ def build_predictions_vrd_dictionary(data, class_label_map,
       standard_fields.DetectionResultFields.detection_scores:
           data_boxes['Score'].as_matrix()
   }
-
-
-def write_csv(fid, metrics):
-  """Writes metrics key-value pairs to CSV file.
-
-  Args:
-    fid: File identifier of an opened file.
-    metrics: A dictionary with metrics to be written.
-  """
-  metrics_writer = csv.writer(fid, delimiter=',')
-  for metric_name, metric_value in metrics.items():
-    metrics_writer.writerow([metric_name, str(metric_value)])
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
index 49ce0898..73818288 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
@@ -66,7 +66,7 @@ class OidVrdChallengeEvaluationUtilsTest(tf.test.TestCase):
                     groundtruth_dictionary)
     self.assertTrue(standard_fields.InputDataFields.groundtruth_classes in
                     groundtruth_dictionary)
-    self.assertTrue(standard_fields.InputDataFields.verified_labels in
+    self.assertTrue(standard_fields.InputDataFields.groundtruth_image_classes in
                     groundtruth_dictionary)
 
     self.assertAllEqual(
@@ -87,8 +87,8 @@ class OidVrdChallengeEvaluationUtilsTest(tf.test.TestCase):
           expected_vrd_data[field], groundtruth_dictionary[
               standard_fields.InputDataFields.groundtruth_boxes][field], 1e-5)
     self.assertAllEqual(
-        np.array([1, 2, 3]),
-        groundtruth_dictionary[standard_fields.InputDataFields.verified_labels])
+        np.array([1, 2, 3]), groundtruth_dictionary[
+            standard_fields.InputDataFields.groundtruth_image_classes])
 
   def testBuildPredictionDictionary(self):
     np_data = pd.DataFrame(
diff --git a/research/object_detection/metrics/tf_example_parser.py b/research/object_detection/metrics/tf_example_parser.py
index 22a28e8a..9a5f130f 100644
--- a/research/object_detection/metrics/tf_example_parser.py
+++ b/research/object_detection/metrics/tf_example_parser.py
@@ -114,7 +114,7 @@ class TfExampleDetectionAndGTParser(data_parser.DataToNumpyParser):
             Int64Parser(fields.TfExampleFields.object_difficult),
         fields.InputDataFields.groundtruth_group_of:
             Int64Parser(fields.TfExampleFields.object_group_of),
-        fields.InputDataFields.verified_labels:
+        fields.InputDataFields.groundtruth_image_classes:
             Int64Parser(fields.TfExampleFields.image_class_label),
     }
 
@@ -136,6 +136,8 @@ class TfExampleDetectionAndGTParser(data_parser.DataToNumpyParser):
       groundtruth group of flag (optional, None if not specified).
       fields.InputDataFields.groundtruth_difficult - a numpy array containing
       groundtruth difficult flag (optional, None if not specified).
+      fields.InputDataFields.groundtruth_image_classes - a numpy array
+      containing groundtruth image-level labels.
       fields.DetectionResultFields.detection_boxes - a numpy array containing
       detection boxes.
       fields.DetectionResultFields.detection_classes - a numpy array containing
diff --git a/research/object_detection/metrics/tf_example_parser_test.py b/research/object_detection/metrics/tf_example_parser_test.py
index 99a64d5f..7d265cc2 100644
--- a/research/object_detection/metrics/tf_example_parser_test.py
+++ b/research/object_detection/metrics/tf_example_parser_test.py
@@ -125,7 +125,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
     results_dict = parser.parse(example)
     self.assertIsNotNone(results_dict)
     np_testing.assert_equal(
-        verified_labels, results_dict[fields.InputDataFields.verified_labels])
+        verified_labels,
+        results_dict[fields.InputDataFields.groundtruth_image_classes])
 
   def testParseString(self):
     string_val = 'abc'
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 973ca259..6531dcdd 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -99,7 +99,7 @@ def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
   """Unstacks all tensors in `tensor_dict` along 0th dimension.
 
   Unstacks tensor from the tensor dict along 0th dimension and returns a
-  tensor_dict containing values that are lists of unstacked tensors.
+  tensor_dict containing values that are lists of unstacked, unpadded tensors.
 
   Tensors in the `tensor_dict` are expected to be of one of the three shapes:
   1. [batch_size]
@@ -244,8 +244,9 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
     preprocessed_images = features[fields.InputDataFields.image]
     prediction_dict = detection_model.predict(
         preprocessed_images, features[fields.InputDataFields.true_image_shape])
-    detections = detection_model.postprocess(
-        prediction_dict, features[fields.InputDataFields.true_image_shape])
+    if mode in (tf.estimator.ModeKeys.EVAL, tf.estimator.ModeKeys.PREDICT):
+      detections = detection_model.postprocess(
+          prediction_dict, features[fields.InputDataFields.true_image_shape])
 
     if mode == tf.estimator.ModeKeys.TRAIN:
       if train_config.fine_tune_checkpoint and hparams.load_pretrained:
@@ -399,7 +400,8 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
             keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)
         scaffold = tf.train.Scaffold(saver=saver)
 
-    if use_tpu:
+    # EVAL executes on CPU, so use regular non-TPU EstimatorSpec.
+    if use_tpu and mode != tf.estimator.ModeKeys.EVAL:
       return tf.contrib.tpu.TPUEstimatorSpec(
           mode=mode,
           scaffold_fn=scaffold_fn,
@@ -490,6 +492,7 @@ def create_estimator_and_inputs(run_config,
       hparams,
       train_steps=train_steps,
       eval_steps=eval_steps,
+      retain_original_images_in_eval=False if use_tpu else True,
       **kwargs)
   model_config = configs['model']
   train_config = configs['train_config']
@@ -519,8 +522,10 @@ def create_estimator_and_inputs(run_config,
       eval_config=eval_config,
       eval_input_config=train_input_config,
       model_config=model_config)
-  predict_input_fn = create_predict_input_fn(model_config=model_config)
+  predict_input_fn = create_predict_input_fn(
+      model_config=model_config, predict_input_config=eval_input_config)
 
+  tf.logging.info('create_estimator_and_inputs: use_tpu %s', use_tpu)
   model_fn = model_fn_creator(detection_model_fn, configs, hparams, use_tpu)
   if use_tpu_estimator:
     estimator = tf.contrib.tpu.TPUEstimator(
@@ -530,6 +535,7 @@ def create_estimator_and_inputs(run_config,
         eval_batch_size=num_shards * 1 if use_tpu else 1,
         use_tpu=use_tpu,
         config=run_config,
+        # TODO(lzc): Remove conditional after CMLE moves to TF 1.9
         params=params if params else {})
   else:
     estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
diff --git a/research/object_detection/model_lib_test.py b/research/object_detection/model_lib_test.py
index ec571e05..e2a5fc7d 100644
--- a/research/object_detection/model_lib_test.py
+++ b/research/object_detection/model_lib_test.py
@@ -72,6 +72,20 @@ def _get_configs_for_model(model_name):
   return configs
 
 
+def _make_initializable_iterator(dataset):
+  """Creates an iterator, and initializes tables.
+
+  Args:
+    dataset: A `tf.data.Dataset` object.
+
+  Returns:
+    A `tf.data.Iterator`.
+  """
+  iterator = dataset.make_initializable_iterator()
+  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  return iterator
+
+
 class ModelLibTest(tf.test.TestCase):
 
   @classmethod
@@ -84,24 +98,24 @@ class ModelLibTest(tf.test.TestCase):
     train_config = configs['train_config']
     with tf.Graph().as_default():
       if mode == 'train':
-        features, labels = inputs.create_train_input_fn(
-            configs['train_config'],
-            configs['train_input_config'],
-            configs['model'])()
+        features, labels = _make_initializable_iterator(
+            inputs.create_train_input_fn(configs['train_config'],
+                                         configs['train_input_config'],
+                                         configs['model'])()).get_next()
         model_mode = tf.estimator.ModeKeys.TRAIN
         batch_size = train_config.batch_size
       elif mode == 'eval':
-        features, labels = inputs.create_eval_input_fn(
-            configs['eval_config'],
-            configs['eval_input_config'],
-            configs['model'])()
+        features, labels = _make_initializable_iterator(
+            inputs.create_eval_input_fn(configs['eval_config'],
+                                        configs['eval_input_config'],
+                                        configs['model'])()).get_next()
         model_mode = tf.estimator.ModeKeys.EVAL
         batch_size = 1
       elif mode == 'eval_on_train':
-        features, labels = inputs.create_eval_input_fn(
-            configs['eval_config'],
-            configs['train_input_config'],
-            configs['model'])()
+        features, labels = _make_initializable_iterator(
+            inputs.create_eval_input_fn(configs['eval_config'],
+                                        configs['train_input_config'],
+                                        configs['model'])()).get_next()
         model_mode = tf.estimator.ModeKeys.EVAL
         batch_size = 1
 
@@ -116,20 +130,21 @@ class ModelLibTest(tf.test.TestCase):
 
       self.assertIsNotNone(estimator_spec.loss)
       self.assertIsNotNone(estimator_spec.predictions)
-      if class_agnostic:
-        self.assertNotIn('detection_classes', estimator_spec.predictions)
-      else:
-        detection_classes = estimator_spec.predictions['detection_classes']
-        self.assertEqual(batch_size, detection_classes.shape.as_list()[0])
-        self.assertEqual(tf.float32, detection_classes.dtype)
-      detection_boxes = estimator_spec.predictions['detection_boxes']
-      detection_scores = estimator_spec.predictions['detection_scores']
-      num_detections = estimator_spec.predictions['num_detections']
-      self.assertEqual(batch_size, detection_boxes.shape.as_list()[0])
-      self.assertEqual(tf.float32, detection_boxes.dtype)
-      self.assertEqual(batch_size, detection_scores.shape.as_list()[0])
-      self.assertEqual(tf.float32, detection_scores.dtype)
-      self.assertEqual(tf.float32, num_detections.dtype)
+      if mode == 'eval' or mode == 'eval_on_train':
+        if class_agnostic:
+          self.assertNotIn('detection_classes', estimator_spec.predictions)
+        else:
+          detection_classes = estimator_spec.predictions['detection_classes']
+          self.assertEqual(batch_size, detection_classes.shape.as_list()[0])
+          self.assertEqual(tf.float32, detection_classes.dtype)
+        detection_boxes = estimator_spec.predictions['detection_boxes']
+        detection_scores = estimator_spec.predictions['detection_scores']
+        num_detections = estimator_spec.predictions['num_detections']
+        self.assertEqual(batch_size, detection_boxes.shape.as_list()[0])
+        self.assertEqual(tf.float32, detection_boxes.dtype)
+        self.assertEqual(batch_size, detection_scores.shape.as_list()[0])
+        self.assertEqual(tf.float32, detection_scores.dtype)
+        self.assertEqual(tf.float32, num_detections.dtype)
       if model_mode == tf.estimator.ModeKeys.TRAIN:
         self.assertIsNotNone(estimator_spec.train_op)
       return estimator_spec
@@ -138,10 +153,10 @@ class ModelLibTest(tf.test.TestCase):
     model_config = configs['model']
 
     with tf.Graph().as_default():
-      features, _ = inputs.create_eval_input_fn(
-          configs['eval_config'],
-          configs['eval_input_config'],
-          configs['model'])()
+      features, _ = _make_initializable_iterator(
+          inputs.create_eval_input_fn(configs['eval_config'],
+                                      configs['eval_input_config'],
+                                      configs['model'])()).get_next()
       detection_model_fn = functools.partial(
           model_builder.build, model_config=model_config, is_training=False)
 
diff --git a/research/object_detection/model_main.py b/research/object_detection/model_main.py
index b4bfcf32..2082c848 100644
--- a/research/object_detection/model_main.py
+++ b/research/object_detection/model_main.py
@@ -40,7 +40,12 @@ flags.DEFINE_string(
     'checkpoint_dir', None, 'Path to directory holding a checkpoint.  If '
     '`checkpoint_dir` is provided, this binary operates in eval-only mode, '
     'writing resulting metrics to `model_dir`.')
-
+flags.DEFINE_boolean(
+    'run_once', False, 'If running in eval-only mode, whether to run just '
+    'one round of eval vs running continuously (default).'
+)
+flags.DEFINE_boolean('eval_training_data', False,
+                     'If training data should be evaluated for this job.')
 FLAGS = flags.FLAGS
 
 
@@ -64,10 +69,20 @@ def main(unused_argv):
   eval_steps = train_and_eval_dict['eval_steps']
 
   if FLAGS.checkpoint_dir:
-    estimator.evaluate(eval_input_fn,
-                       eval_steps,
-                       checkpoint_path=tf.train.latest_checkpoint(
-                           FLAGS.checkpoint_dir))
+    if FLAGS.eval_training_data:
+      name = 'training_data'
+      input_fn = eval_on_train_input_fn
+    else:
+      name = 'validation_data'
+      input_fn = eval_input_fn
+    if FLAGS.run_once:
+      estimator.evaluate(input_fn,
+                         eval_steps,
+                         checkpoint_path=tf.train.latest_checkpoint(
+                             FLAGS.checkpoint_dir))
+    else:
+      model_lib.continuous_eval(estimator, FLAGS.model_dir, input_fn,
+                                eval_steps, train_steps, name)
   else:
     train_spec, eval_specs = model_lib.create_train_and_eval_specs(
         train_input_fn,
diff --git a/research/object_detection/model_tpu_main.py b/research/object_detection/model_tpu_main.py
index 50f8fb9a..cb9f78ce 100644
--- a/research/object_detection/model_tpu_main.py
+++ b/research/object_detection/model_tpu_main.py
@@ -25,7 +25,6 @@ from __future__ import print_function
 from absl import flags
 import tensorflow as tf
 
-from tensorflow.contrib.tpu.python.tpu import tpu_config
 
 from object_detection import model_hparams
 from object_detection import model_lib
@@ -81,17 +80,17 @@ def main(unused_argv):
   flags.mark_flag_as_required('pipeline_config_path')
 
   tpu_cluster_resolver = (
-      tf.contrib.cluster_resolver.python.training.TPUClusterResolver(
-          tpu_names=[FLAGS.tpu_name],
+      tf.contrib.cluster_resolver.TPUClusterResolver(
+          tpu=[FLAGS.tpu_name],
           zone=FLAGS.tpu_zone,
           project=FLAGS.gcp_project))
   tpu_grpc_url = tpu_cluster_resolver.get_master()
 
-  config = tpu_config.RunConfig(
+  config = tf.contrib.tpu.RunConfig(
       master=tpu_grpc_url,
       evaluation_master=tpu_grpc_url,
       model_dir=FLAGS.model_dir,
-      tpu_config=tpu_config.TPUConfig(
+      tpu_config=tf.contrib.tpu.TPUConfig(
           iterations_per_loop=FLAGS.iterations_per_loop,
           num_shards=FLAGS.num_shards))
 
diff --git a/research/object_detection/protos/faster_rcnn.proto b/research/object_detection/protos/faster_rcnn.proto
index 797874cb..dcc4d387 100644
--- a/research/object_detection/protos/faster_rcnn.proto
+++ b/research/object_detection/protos/faster_rcnn.proto
@@ -137,6 +137,11 @@ message FasterRcnn {
   // a control dependency on tf.GraphKeys.UPDATE_OPS for train/loss op in order
   // to update the batch norm moving average parameters.
   optional bool inplace_batchnorm_update = 30 [default = false];
+
+  // Force the use of matrix multiplication based crop and resize instead of
+  // standard tf.image.crop_and_resize while computing second stage input
+  // feature maps.
+  optional bool use_matmul_crop_and_resize = 31 [default = false];
 }
 
 
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index a0000460..bc9036c9 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -37,32 +37,54 @@ message InputReader {
   // Buffer size to be used when shuffling file names.
   optional uint32 filenames_shuffle_buffer_size = 12 [default = 100];
 
+  // The number of times a data source is read. If set to zero, the data source
+  // will be reused indefinitely.
+  optional uint32 num_epochs = 5 [default=0];
+
+  // Number of file shards to read in parallel.
+  optional uint32 num_readers = 6 [default=64];
+
+  // Number of batches to produce in parallel. If this is run on a 2x2 TPU set
+  // this to 8.
+  optional uint32 num_parallel_batches = 19 [default=8];
+
+  // Number of batches to prefetch. Prefetch decouples input pipeline and
+  // model so they can be pipelined resulting in higher throughput. Set this
+  // to a small constant and increment linearly until the improvements become
+  // marginal or you exceed your cpu memory budget. Setting this to -1,
+  // automatically tunes this value for you.
+  optional int32 num_prefetch_batches = 20 [default=2];
+
   // Maximum number of records to keep in reader queue.
-  optional uint32 queue_capacity = 3 [default=2000];
+  optional uint32 queue_capacity = 3 [default=2000, deprecated=true];
 
   // Minimum number of records to keep in reader queue. A large value is needed
   // to generate a good random shuffle.
-  optional uint32 min_after_dequeue = 4 [default=1000];
+  optional uint32 min_after_dequeue = 4 [default=1000, deprecated=true];
 
-  // The number of times a data source is read. If set to zero, the data source
-  // will be reused indefinitely.
-  optional uint32 num_epochs = 5 [default=0];
-
-  // Number of reader instances to create.
-  optional uint32 num_readers = 6 [default=32];
 
   // Number of records to read from each reader at once.
   optional uint32 read_block_length = 15 [default=32];
 
   // Number of decoded records to prefetch before batching.
-  optional uint32 prefetch_size = 13 [default = 512];
+  optional uint32 prefetch_size = 13 [default = 512, deprecated=true];
 
   // Number of parallel decode ops to apply.
-  optional uint32 num_parallel_map_calls = 14 [default = 64];
+  optional uint32 num_parallel_map_calls = 14 [default = 64, deprecated=true];
+
+  // If positive, TfExampleDecoder will try to decode rasters of additional
+  // channels from tf.Examples.
+  optional int32 num_additional_channels = 18 [default = 0];
 
   // Number of groundtruth keypoints per object.
   optional uint32 num_keypoints = 16 [default = 0];
 
+  // Maximum number of boxes to pad to during training.
+  // Set this to at least the maximum amount of boxes in the input data.
+  // Otherwise, it may cause "Data loss: Attempted to pad to a smaller size
+  // than the input element" errors.
+  optional int32 max_number_of_boxes = 21 [default=100];
+
   // Whether to load groundtruth instance masks.
   optional bool load_instance_masks = 7 [default = false];
 
diff --git a/research/object_detection/protos/losses.proto b/research/object_detection/protos/losses.proto
index f0898db2..a3e5bd34 100644
--- a/research/object_detection/protos/losses.proto
+++ b/research/object_detection/protos/losses.proto
@@ -43,7 +43,7 @@ message WeightedL2LocalizationLoss {
 
 // SmoothL1 (Huber) location loss.
 // The smooth L1_loss is defined elementwise as .5 x^2 if |x| <= delta and
-// 0.5 x^2 + delta * (|x|-delta) otherwise, where x is the difference between
+// delta * (|x|-0.5*delta) otherwise, where x is the difference between
 // predictions and target.
 message WeightedSmoothL1LocalizationLoss {
   // DEPRECATED, do not use.
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index 01a188eb..fb826c3f 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -96,7 +96,7 @@ message TrainConfig {
   // Set this to at least the maximum amount of boxes in the input data.
   // Otherwise, it may cause "Data loss: Attempted to pad to a smaller size
   // than the input element" errors.
-  optional int32 max_number_of_boxes = 20 [default=100];
+  optional int32 max_number_of_boxes = 20 [default=100, deprecated=true];
 
   // Whether to remove padding along `num_boxes` dimension of the groundtruth
   // tensors.
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
index 6c8f263b..3a6fcc89 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
@@ -107,6 +107,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -120,21 +121,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config b/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
index 020e5ba4..6ebd586b 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
@@ -105,6 +105,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -119,21 +120,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
index 3303fd8a..55f5f8d3 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
@@ -105,6 +105,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -118,21 +119,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
index a8b6a84e..38784bbe 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
@@ -105,6 +105,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -118,21 +119,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
index 1d8012ff..f1b6fe95 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
@@ -105,6 +105,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -119,21 +120,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
index 268bd2ba..5d9819ed 100644
--- a/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
@@ -120,6 +120,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -133,22 +134,20 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   load_instance_masks: true
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_mask_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/rfcn_resnet101_pets.config b/research/object_detection/samples/configs/rfcn_resnet101_pets.config
index 95757388..6d6d92b6 100644
--- a/research/object_detection/samples/configs/rfcn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/rfcn_resnet101_pets.config
@@ -102,6 +102,7 @@ train_config: {
   gradient_clipping_by_norm: 10.0
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -115,21 +116,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_inception_v2_pets.config b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
index 79f91f97..42f4ebbd 100644
--- a/research/object_detection/samples/configs/ssd_inception_v2_pets.config
+++ b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
@@ -150,6 +150,7 @@ train_config: {
   }
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -168,21 +169,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_inception_v3_pets.config b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
index 1c3a676c..3fff911c 100644
--- a/research/object_detection/samples/configs/ssd_inception_v3_pets.config
+++ b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
@@ -150,6 +150,7 @@ train_config: {
   }
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -167,21 +168,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
index 7186b88e..a0f4ca40 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
@@ -151,6 +151,7 @@ train_config: {
   }
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -170,21 +171,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
index 72a0b5d9..2464782c 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
@@ -155,6 +155,7 @@ train_config: {
   }
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
   from_detection_checkpoint: true
+  load_all_detection_checkpoint_vars: true
   # Note: The below line limits the training process to 200K steps, which we
   # empirically found to be sufficient enough to train the pets dataset. This
   # effectively bypasses the learning rate schedule (the learning rate will
@@ -172,21 +173,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 2000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  num_examples: 1101
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/train.py b/research/object_detection/train.py
index 37b5cf0a..a4a568fc 100644
--- a/research/object_detection/train.py
+++ b/research/object_detection/train.py
@@ -51,7 +51,6 @@ from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.utils import config_util
-from object_detection.utils import dataset_util
 
 tf.logging.set_verbosity(tf.logging.INFO)
 
@@ -117,7 +116,7 @@ def main(_):
       is_training=True)
 
   def get_next(config):
-    return dataset_util.make_initializable_iterator(
+    return dataset_builder.make_initializable_iterator(
         dataset_builder.build(config)).get_next()
 
   create_input_dict_fn = functools.partial(get_next, input_config)
diff --git a/research/object_detection/trainer.py b/research/object_detection/trainer.py
index caa7187d..a725e842 100644
--- a/research/object_detection/trainer.py
+++ b/research/object_detection/trainer.py
@@ -109,7 +109,8 @@ def get_inputs(input_queue,
     image_keys: a list of string keys for the images.
     locations_list: a list of tensors of shape [num_boxes, 4]
       containing the corners of the groundtruth boxes.
-    classes_list: a list of padded one-hot tensors containing target classes.
+    classes_list: a list of padded one-hot (or K-hot) float32 tensors containing
+      target classes.
     masks_list: a list of 3-D float tensors of shape [num_boxes, image_height,
       image_width] containing instance masks for objects if present in the
       input_queue. Else returns None.
@@ -141,6 +142,7 @@ def get_inputs(input_queue,
     if merge_multiple_label_boxes:
       location_gt, classes_gt, _ = util_ops.merge_boxes_with_multiple_labels(
           location_gt, classes_gt, num_classes)
+      classes_gt = tf.cast(classes_gt, tf.float32)
     elif use_multiclass_scores:
       classes_gt = tf.cast(read_data[fields.InputDataFields.multiclass_scores],
                            tf.float32)
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index d7ca1e98..e7835223 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -357,6 +357,12 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
       _update_mask_type(configs, value)
     elif key == "eval_with_moving_averages":
       _update_use_moving_averages(configs, value)
+    elif key == "train_shuffle":
+      _update_shuffle(configs["train_input_config"], value)
+    elif key == "eval_shuffle":
+      _update_shuffle(configs["eval_input_config"], value)
+    elif key == "retain_original_images_in_eval":
+      _update_retain_original_images(configs["eval_config"], value)
     elif _is_generic_key(key):
       _update_generic(configs, key, value)
     else:
@@ -654,3 +660,28 @@ def _update_use_moving_averages(configs, use_moving_averages):
       should be loaded during evaluation.
   """
   configs["eval_config"].use_moving_averages = use_moving_averages
+
+
+def _update_shuffle(input_config, shuffle):
+  """Updates input configuration to reflect a new shuffle configuration.
+
+  The input_config object is updated in place, and hence not returned.
+
+  Args:
+    input_config: A input_reader_pb2.InputReader.
+    shuffle: Whether or not to shuffle the input data before reading.
+  """
+  input_config.shuffle = shuffle
+
+
+def _update_retain_original_images(eval_config, retain_original_images):
+  """Updates eval config with option to retain original images.
+
+  The eval_config object is updated in place, and hence not returned.
+
+  Args:
+    eval_config: A eval_pb2.EvalConfig.
+    retain_original_images: Boolean indicating whether to retain original images
+      in eval mode.
+  """
+  eval_config.retain_original_images = retain_original_images
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index f528d453..cbb42157 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -547,7 +547,7 @@ class ConfigUtilTest(tf.test.TestCase):
         configs, eval_with_moving_averages=True)
     self.assertEqual(True, configs["eval_config"].use_moving_averages)
 
-  def  test_get_image_resizer_config(self):
+  def  testGetImageResizerConfig(self):
     """Tests that number of classes can be retrieved."""
     model_config = model_pb2.DetectionModel()
     model_config.faster_rcnn.image_resizer.fixed_shape_resizer.height = 100
@@ -556,14 +556,14 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(image_resizer_config.fixed_shape_resizer.height, 100)
     self.assertEqual(image_resizer_config.fixed_shape_resizer.width, 300)
 
-  def test_get_spatial_image_size_from_fixed_shape_resizer_config(self):
+  def testGetSpatialImageSizeFromFixedShapeResizerConfig(self):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     image_resizer_config.fixed_shape_resizer.height = 100
     image_resizer_config.fixed_shape_resizer.width = 200
     image_shape = config_util.get_spatial_image_size(image_resizer_config)
     self.assertAllEqual(image_shape, [100, 200])
 
-  def test_get_spatial_image_size_from_aspect_preserving_resizer_config(self):
+  def testGetSpatialImageSizeFromAspectPreservingResizerConfig(self):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100
     image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600
@@ -571,13 +571,62 @@ class ConfigUtilTest(tf.test.TestCase):
     image_shape = config_util.get_spatial_image_size(image_resizer_config)
     self.assertAllEqual(image_shape, [600, 600])
 
-  def test_get_spatial_image_size_from_aspect_preserving_resizer_dynamic(self):
+  def testGetSpatialImageSizeFromAspectPreservingResizerDynamic(self):
     image_resizer_config = image_resizer_pb2.ImageResizer()
     image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100
     image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600
     image_shape = config_util.get_spatial_image_size(image_resizer_config)
     self.assertAllEqual(image_shape, [-1, -1])
 
+  def testEvalShuffle(self):
+    """Tests that `eval_shuffle` keyword arguments are applied correctly."""
+    original_shuffle = True
+    desired_shuffle = False
+
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_input_reader.shuffle = original_shuffle
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, eval_shuffle=desired_shuffle)
+    eval_shuffle = configs["eval_input_config"].shuffle
+    self.assertEqual(desired_shuffle, eval_shuffle)
+
+  def testTrainShuffle(self):
+    """Tests that `train_shuffle` keyword arguments are applied correctly."""
+    original_shuffle = True
+    desired_shuffle = False
+
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.train_input_reader.shuffle = original_shuffle
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, train_shuffle=desired_shuffle)
+    train_shuffle = configs["train_input_config"].shuffle
+    self.assertEqual(desired_shuffle, train_shuffle)
+
+  def testOverWriteRetainOriginalImages(self):
+    """Tests that `train_shuffle` keyword arguments are applied correctly."""
+    original_retain_original_images = True
+    desired_retain_original_images = False
+
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.retain_original_images = (
+        original_retain_original_images)
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, retain_original_images_in_eval=desired_retain_original_images)
+    retain_original_images = configs["eval_config"].retain_original_images
+    self.assertEqual(desired_retain_original_images, retain_original_images)
+
 
 if __name__ == "__main__":
   tf.test.main()
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index efc59bcb..77b46a2d 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -72,7 +72,7 @@ def recursive_parse_xml_to_dict(xml):
   Returns:
     Python dictionary holding XML contents.
   """
-  if not len(xml):
+  if not xml:
     return {xml.tag: xml.text}
   result = {}
   for child in xml:
@@ -86,61 +86,3 @@ def recursive_parse_xml_to_dict(xml):
   return {xml.tag: result}
 
 
-def make_initializable_iterator(dataset):
-  """Creates an iterator, and initializes tables.
-
-  This is useful in cases where make_one_shot_iterator wouldn't work because
-  the graph contains a hash table that needs to be initialized.
-
-  Args:
-    dataset: A `tf.data.Dataset` object.
-
-  Returns:
-    A `tf.data.Iterator`.
-  """
-  iterator = dataset.make_initializable_iterator()
-  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
-  return iterator
-
-
-def read_dataset(file_read_func, decode_func, input_files, config):
-  """Reads a dataset, and handles repetition and shuffling.
-
-  Args:
-    file_read_func: Function to use in tf.data.Dataset.interleave, to read
-      every individual file into a tf.data.Dataset.
-    decode_func: Function to apply to all records.
-    input_files: A list of file paths to read.
-    config: A input_reader_builder.InputReader object.
-
-  Returns:
-    A tf.data.Dataset based on config.
-  """
-  # Shard, shuffle, and read files.
-  filenames = tf.gfile.Glob(input_files)
-  num_readers = config.num_readers
-  if num_readers > len(filenames):
-    num_readers = len(filenames)
-    tf.logging.warning('num_readers has been reduced to %d to match input file '
-                       'shards.' % num_readers)
-  filename_dataset = tf.data.Dataset.from_tensor_slices(tf.unstack(filenames))
-  if config.shuffle:
-    filename_dataset = filename_dataset.shuffle(
-        config.filenames_shuffle_buffer_size)
-  elif num_readers > 1:
-    tf.logging.warning('`shuffle` is false, but the input data stream is '
-                       'still slightly shuffled since `num_readers` > 1.')
-
-  filename_dataset = filename_dataset.repeat(config.num_epochs or None)
-
-  records_dataset = filename_dataset.apply(
-      tf.contrib.data.parallel_interleave(
-          file_read_func,
-          cycle_length=num_readers,
-          block_length=config.read_block_length,
-          sloppy=config.shuffle))
-  if config.shuffle:
-    records_dataset = records_dataset.shuffle(config.shuffle_buffer_size)
-  tensor_dataset = records_dataset.map(
-      decode_func, num_parallel_calls=config.num_parallel_map_calls)
-  return tensor_dataset.prefetch(config.prefetch_size)
diff --git a/research/object_detection/utils/dataset_util_test.py b/research/object_detection/utils/dataset_util_test.py
index a74d4dca..99cfb2cd 100644
--- a/research/object_detection/utils/dataset_util_test.py
+++ b/research/object_detection/utils/dataset_util_test.py
@@ -16,39 +16,13 @@
 """Tests for object_detection.utils.dataset_util."""
 
 import os
-import numpy as np
 import tensorflow as tf
 
-from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
 
 class DatasetUtilTest(tf.test.TestCase):
 
-  def setUp(self):
-    self._path_template = os.path.join(self.get_temp_dir(), 'examples_%s.txt')
-
-    for i in range(5):
-      path = self._path_template % i
-      with tf.gfile.Open(path, 'wb') as f:
-        f.write('\n'.join([str(i + 1), str((i + 1) * 10)]))
-
-    self._shuffle_path_template = os.path.join(self.get_temp_dir(),
-                                               'shuffle_%s.txt')
-    for i in range(2):
-      path = self._shuffle_path_template % i
-      with tf.gfile.Open(path, 'wb') as f:
-        f.write('\n'.join([str(i)] * 5))
-
-  def _get_dataset_next(self, files, config, batch_size):
-    def decode_func(value):
-      return [tf.string_to_number(value, out_type=tf.int32)]
-
-    dataset = dataset_util.read_dataset(
-        tf.data.TextLineDataset, decode_func, files, config)
-    dataset = dataset.batch(batch_size)
-    return dataset.make_one_shot_iterator().get_next()
-
   def test_read_examples_list(self):
     example_list_data = """example1 1\nexample2 2"""
     example_list_path = os.path.join(self.get_temp_dir(), 'examples.txt')
@@ -58,84 +32,6 @@ class DatasetUtilTest(tf.test.TestCase):
     examples = dataset_util.read_examples_list(example_list_path)
     self.assertListEqual(['example1', 'example2'], examples)
 
-  def test_make_initializable_iterator_with_hashTable(self):
-    keys = [1, 0, -1]
-    dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])
-    table = tf.contrib.lookup.HashTable(
-        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
-            keys=keys,
-            values=list(reversed(keys))),
-        default_value=100)
-    dataset = dataset.map(table.lookup)
-    data = dataset_util.make_initializable_iterator(dataset).get_next()
-    init = tf.tables_initializer()
-
-    with self.test_session() as sess:
-      sess.run(init)
-      self.assertAllEqual(sess.run(data), [-1, 100, 1, 100])
-
-  def test_read_dataset(self):
-    config = input_reader_pb2.InputReader()
-    config.num_readers = 1
-    config.shuffle = False
-
-    data = self._get_dataset_next([self._path_template % '*'], config,
-                                  batch_size=20)
-    with self.test_session() as sess:
-      self.assertAllEqual(sess.run(data),
-                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
-                            30, 4, 40, 5, 50]])
-
-  def test_reduce_num_reader(self):
-    config = input_reader_pb2.InputReader()
-    config.num_readers = 10
-    config.shuffle = False
-
-    data = self._get_dataset_next([self._path_template % '*'], config,
-                                  batch_size=20)
-    with self.test_session() as sess:
-      self.assertAllEqual(sess.run(data),
-                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
-                            30, 4, 40, 5, 50]])
-
-  def test_enable_shuffle(self):
-    config = input_reader_pb2.InputReader()
-    config.num_readers = 1
-    config.shuffle = True
-
-    data = self._get_dataset_next(
-        [self._shuffle_path_template % '*'], config, batch_size=10)
-    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
-
-    with self.test_session() as sess:
-      self.assertTrue(
-          np.any(np.not_equal(sess.run(data), expected_non_shuffle_output)))
-
-  def test_disable_shuffle_(self):
-    config = input_reader_pb2.InputReader()
-    config.num_readers = 1
-    config.shuffle = False
-
-    data = self._get_dataset_next(
-        [self._shuffle_path_template % '*'], config, batch_size=10)
-    expected_non_shuffle_output = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]
-
-    with self.test_session() as sess:
-      self.assertAllEqual(sess.run(data), [expected_non_shuffle_output])
-
-  def test_read_dataset_single_epoch(self):
-    config = input_reader_pb2.InputReader()
-    config.num_epochs = 1
-    config.num_readers = 1
-    config.shuffle = False
-
-    data = self._get_dataset_next([self._path_template % '0'], config,
-                                  batch_size=30)
-    with self.test_session() as sess:
-      # First batch will retrieve as much as it can, second batch will fail.
-      self.assertAllEqual(sess.run(data), [[1, 10]])
-      self.assertRaises(tf.errors.OutOfRangeError, sess.run, data)
-
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 8fa43f6e..5a72d009 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -90,9 +90,6 @@ def cosine_decay_with_warmup(global_step,
     ValueError: if warmup_learning_rate is larger than learning_rate_base,
       or if warmup_steps is larger than total_steps.
   """
-  if learning_rate_base < warmup_learning_rate:
-    raise ValueError('learning_rate_base must be larger '
-                     'or equal to warmup_learning_rate.')
   if total_steps < warmup_steps:
     raise ValueError('total_steps must be larger or equal to '
                      'warmup_steps.')
@@ -104,6 +101,9 @@ def cosine_decay_with_warmup(global_step,
     learning_rate = tf.where(global_step > warmup_steps + hold_base_rate_steps,
                              learning_rate, learning_rate_base)
   if warmup_steps > 0:
+    if learning_rate_base < warmup_learning_rate:
+      raise ValueError('learning_rate_base must be larger or equal to '
+                       'warmup_learning_rate.')
     slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
     warmup_rate = slope * tf.cast(global_step,
                                   tf.float32) + warmup_learning_rate
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 3952d613..8a38d8c2 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -525,8 +525,8 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
         standard_fields.InputDataFields.groundtruth_classes: integer numpy array
           of shape [num_boxes] containing 1-indexed groundtruth classes for the
           boxes.
-        standard_fields.InputDataFields.verified_labels: integer 1D numpy array
-          containing all classes for which labels are verified.
+        standard_fields.InputDataFields.groundtruth_image_classes: integer 1D
+          numpy array containing all classes for which labels are verified.
         standard_fields.InputDataFields.groundtruth_group_of: Optional length
           M numpy boolean array denoting whether a groundtruth box contains a
           group of instances.
@@ -541,7 +541,7 @@ class OpenImagesDetectionChallengeEvaluator(OpenImagesDetectionEvaluator):
         self._label_id_offset)
     self._evaluatable_labels[image_id] = np.unique(
         np.concatenate(((groundtruth_dict.get(
-            standard_fields.InputDataFields.verified_labels,
+            standard_fields.InputDataFields.groundtruth_image_classes,
             np.array([], dtype=int)) - self._label_id_offset),
                         groundtruth_classes)))
 
@@ -839,6 +839,9 @@ class ObjectDetectionEvaluation(object):
       if self.use_weighted_mean_ap:
         all_scores = np.append(all_scores, scores)
         all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)
+      print 'Scores and tpfp per class label: {}'.format(class_index)
+      print tp_fp_labels
+      print scores
       precision, recall = metrics.compute_precision_recall(
           scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])
       self.precisions_per_class.append(precision)
diff --git a/research/object_detection/utils/object_detection_evaluation_test.py b/research/object_detection/utils/object_detection_evaluation_test.py
index 51f56ed1..108db3f4 100644
--- a/research/object_detection/utils/object_detection_evaluation_test.py
+++ b/research/object_detection/utils/object_detection_evaluation_test.py
@@ -131,7 +131,7 @@ class OpenImagesDetectionChallengeEvaluatorTest(tf.test.TestCase):
                 groundtruth_class_labels,
             standard_fields.InputDataFields.groundtruth_group_of:
                 groundtruth_is_group_of_list,
-            standard_fields.InputDataFields.verified_labels:
+            standard_fields.InputDataFields.groundtruth_image_classes:
                 groundtruth_verified_labels,
         })
     image_key = 'img2'
diff --git a/research/object_detection/utils/variables_helper_test.py b/research/object_detection/utils/variables_helper_test.py
index b6d58bb6..94585d71 100644
--- a/research/object_detection/utils/variables_helper_test.py
+++ b/research/object_detection/utils/variables_helper_test.py
@@ -129,80 +129,88 @@ class FreezeGradientsMatchingRegexTest(tf.test.TestCase):
 class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
 
   def test_return_all_variables_from_checkpoint(self):
-    variables = [
-        tf.Variable(1.0, name='weights'),
-        tf.Variable(1.0, name='biases')
-    ]
-    checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
-    init_op = tf.global_variables_initializer()
-    saver = tf.train.Saver(variables)
-    with self.test_session() as sess:
-      sess.run(init_op)
-      saver.save(sess, checkpoint_path)
-    out_variables = variables_helper.get_variables_available_in_checkpoint(
-        variables, checkpoint_path)
+    with tf.Graph().as_default():
+      variables = [
+          tf.Variable(1.0, name='weights'),
+          tf.Variable(1.0, name='biases')
+      ]
+      checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
+      init_op = tf.global_variables_initializer()
+      saver = tf.train.Saver(variables)
+      with self.test_session() as sess:
+        sess.run(init_op)
+        saver.save(sess, checkpoint_path)
+      out_variables = variables_helper.get_variables_available_in_checkpoint(
+          variables, checkpoint_path)
     self.assertItemsEqual(out_variables, variables)
 
   def test_return_variables_available_in_checkpoint(self):
-    checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
-    weight_variable = tf.Variable(1.0, name='weights')
-    global_step = tf.train.get_or_create_global_step()
-    graph1_variables = [
-        weight_variable,
-        global_step
-    ]
-    init_op = tf.global_variables_initializer()
-    saver = tf.train.Saver(graph1_variables)
-    with self.test_session() as sess:
-      sess.run(init_op)
-      saver.save(sess, checkpoint_path)
-
-    graph2_variables = graph1_variables + [tf.Variable(1.0, name='biases')]
-    out_variables = variables_helper.get_variables_available_in_checkpoint(
-        graph2_variables, checkpoint_path, include_global_step=False)
+    checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
+    with tf.Graph().as_default():
+      weight_variable = tf.Variable(1.0, name='weights')
+      global_step = tf.train.get_or_create_global_step()
+      graph1_variables = [
+          weight_variable,
+          global_step
+      ]
+      init_op = tf.global_variables_initializer()
+      saver = tf.train.Saver(graph1_variables)
+      with self.test_session() as sess:
+        sess.run(init_op)
+        saver.save(sess, checkpoint_path)
+
+    with tf.Graph().as_default():
+      graph2_variables = graph1_variables + [tf.Variable(1.0, name='biases')]
+      out_variables = variables_helper.get_variables_available_in_checkpoint(
+          graph2_variables, checkpoint_path, include_global_step=False)
     self.assertItemsEqual(out_variables, [weight_variable])
 
   def test_return_variables_available_an_checkpoint_with_dict_inputs(self):
-    checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
-    graph1_variables = [
-        tf.Variable(1.0, name='ckpt_weights'),
-    ]
-    init_op = tf.global_variables_initializer()
-    saver = tf.train.Saver(graph1_variables)
-    with self.test_session() as sess:
-      sess.run(init_op)
-      saver.save(sess, checkpoint_path)
+    checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
+    with tf.Graph().as_default():
+      graph1_variables = [
+          tf.Variable(1.0, name='ckpt_weights'),
+      ]
+      init_op = tf.global_variables_initializer()
+      saver = tf.train.Saver(graph1_variables)
+      with self.test_session() as sess:
+        sess.run(init_op)
+        saver.save(sess, checkpoint_path)
+
+    with tf.Graph().as_default():
+      graph2_variables_dict = {
+          'ckpt_weights': tf.Variable(1.0, name='weights'),
+          'ckpt_biases': tf.Variable(1.0, name='biases')
+      }
+      out_variables = variables_helper.get_variables_available_in_checkpoint(
+          graph2_variables_dict, checkpoint_path)
 
-    graph2_variables_dict = {
-        'ckpt_weights': tf.Variable(1.0, name='weights'),
-        'ckpt_biases': tf.Variable(1.0, name='biases')
-    }
-    out_variables = variables_helper.get_variables_available_in_checkpoint(
-        graph2_variables_dict, checkpoint_path)
     self.assertTrue(isinstance(out_variables, dict))
     self.assertItemsEqual(out_variables.keys(), ['ckpt_weights'])
     self.assertTrue(out_variables['ckpt_weights'].op.name == 'weights')
 
   def test_return_variables_with_correct_sizes(self):
-    checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
-    bias_variable = tf.Variable(3.0, name='biases')
-    global_step = tf.train.get_or_create_global_step()
-    graph1_variables = [
-        tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='weights'),
-        bias_variable,
-        global_step
-    ]
-    init_op = tf.global_variables_initializer()
-    saver = tf.train.Saver(graph1_variables)
-    with self.test_session() as sess:
-      sess.run(init_op)
-      saver.save(sess, checkpoint_path)
-
-    graph2_variables = [
-        tf.Variable([1.0, 2.0], name='weights'),  # Note the new variable shape.
-        bias_variable,
-        global_step
-    ]
+    checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
+    with tf.Graph().as_default():
+      bias_variable = tf.Variable(3.0, name='biases')
+      global_step = tf.train.get_or_create_global_step()
+      graph1_variables = [
+          tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='weights'),
+          bias_variable,
+          global_step
+      ]
+      init_op = tf.global_variables_initializer()
+      saver = tf.train.Saver(graph1_variables)
+      with self.test_session() as sess:
+        sess.run(init_op)
+        saver.save(sess, checkpoint_path)
+
+    with tf.Graph().as_default():
+      graph2_variables = [
+          tf.Variable([1.0, 2.0], name='weights'),  # New variable shape.
+          bias_variable,
+          global_step
+      ]
 
     out_variables = variables_helper.get_variables_available_in_checkpoint(
         graph2_variables, checkpoint_path, include_global_step=True)
diff --git a/research/object_detection/utils/vrd_evaluation.py b/research/object_detection/utils/vrd_evaluation.py
index 06931101..a56903ef 100644
--- a/research/object_detection/utils/vrd_evaluation.py
+++ b/research/object_detection/utils/vrd_evaluation.py
@@ -128,7 +128,7 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
           structures shape [M, 1], representing  the class labels of the
           corresponding bounding boxes and possibly additional classes (see
           datatype label_data_type above).
-        standard_fields.InputDataFields.verified_labels: numpy array
+        standard_fields.InputDataFields.groundtruth_image_classes: numpy array
           of shape [K] containing verified labels.
     Raises:
       ValueError: On adding groundtruth for an image more than once.
@@ -152,8 +152,8 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       all_classes.append(groundtruth_class_tuples[field])
     groudtruth_positive_classes = np.unique(np.concatenate(all_classes))
     verified_labels = groundtruth_dict.get(
-        standard_fields.InputDataFields.verified_labels, np.array(
-            [], dtype=int))
+        standard_fields.InputDataFields.groundtruth_image_classes,
+        np.array([], dtype=int))
     self._evaluatable_labels[image_id] = np.unique(
         np.concatenate((verified_labels, groudtruth_positive_classes)))
 
@@ -184,17 +184,18 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
         standard_fields.DetectionResultFields.detection_classes]
     detection_box_tuples = detections_dict[
         standard_fields.DetectionResultFields.detection_boxes]
+    negative_selector = np.zeros(num_detections, dtype=bool)
     selector = np.ones(num_detections, dtype=bool)
 
     # Only check boxable labels
     for field in detection_box_tuples.dtype.fields:
       # Verify if one of the labels is negative (this is sure FP)
-      selector |= np.isin(detection_class_tuples[field],
-                          self._negative_labels[image_id])
+      negative_selector |= np.isin(detection_class_tuples[field],
+                                   self._negative_labels[image_id])
       # Verify if all labels are verified
-      selector |= np.isin(detection_class_tuples[field],
+      selector &= np.isin(detection_class_tuples[field],
                           self._evaluatable_labels[image_id])
-
+    selector |= negative_selector
     self._evaluation.add_single_detected_image_info(
         image_key=image_id,
         detected_box_tuples=self._process_detection_boxes(
diff --git a/research/object_detection/utils/vrd_evaluation_test.py b/research/object_detection/utils/vrd_evaluation_test.py
index 52ce9683..f833521b 100644
--- a/research/object_detection/utils/vrd_evaluation_test.py
+++ b/research/object_detection/utils/vrd_evaluation_test.py
@@ -39,7 +39,7 @@ class VRDRelationDetectionEvaluatorTest(tf.test.TestCase):
                 groundtruth_box_tuples1,
             standard_fields.InputDataFields.groundtruth_classes:
                 groundtruth_class_tuples1,
-            standard_fields.InputDataFields.verified_labels:
+            standard_fields.InputDataFields.groundtruth_image_classes:
                 groundtruth_verified_labels1
         })
 
@@ -71,11 +71,12 @@ class VRDRelationDetectionEvaluatorTest(tf.test.TestCase):
 
     image_key = 'img1'
     detected_box_tuples = np.array(
-        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2])],
+        [([0, 0.3, 1, 1], [1.1, 1, 2, 2]), ([0, 0, 1, 1], [1, 1, 2, 2]),
+         ([0.5, 0, 1, 1], [1, 1, 3, 3])],
         dtype=vrd_evaluation.vrd_box_data_type)
     detected_class_tuples = np.array(
-        [(1, 2, 5), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)
-    detected_scores = np.array([0.7, 0.8], dtype=float)
+        [(1, 2, 5), (1, 2, 3), (1, 6, 3)], dtype=vrd_evaluation.label_data_type)
+    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)
     self.vrd_eval.add_single_detected_image_info(
         image_key, {
             standard_fields.DetectionResultFields.detection_boxes:
@@ -121,7 +122,7 @@ class VRDPhraseDetectionEvaluatorTest(tf.test.TestCase):
                 groundtruth_box_tuples1,
             standard_fields.InputDataFields.groundtruth_classes:
                 groundtruth_class_tuples1,
-            standard_fields.InputDataFields.verified_labels:
+            standard_fields.InputDataFields.groundtruth_image_classes:
                 groundtruth_verified_labels1
         })
 
@@ -154,11 +155,12 @@ class VRDPhraseDetectionEvaluatorTest(tf.test.TestCase):
     image_key = 'img1'
     detected_box_tuples = np.array(
         [([0, 0.3, 0.5, 0.5], [0.3, 0.3, 1.0, 1.0]),
-         ([0, 0, 1.2, 1.2], [0.0, 0.0, 2.0, 2.0])],
+         ([0, 0, 1.2, 1.2], [0.0, 0.0, 2.0, 2.0]),
+         ([0.5, 0, 1, 1], [1, 1, 3, 3])],
         dtype=vrd_evaluation.vrd_box_data_type)
     detected_class_tuples = np.array(
-        [(1, 2, 5), (1, 2, 3)], dtype=vrd_evaluation.label_data_type)
-    detected_scores = np.array([0.7, 0.8], dtype=float)
+        [(1, 2, 5), (1, 2, 3), (1, 6, 3)], dtype=vrd_evaluation.label_data_type)
+    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)
     self.vrd_eval.add_single_detected_image_info(
         image_key, {
             standard_fields.DetectionResultFields.detection_boxes:
