commit 1efe98bb8e8d98bbffc703a90d88df15fc2ce906
Author: Zhichao Lu <lzc@google.com>
Date:   Fri Feb 9 17:13:36 2018 -0800

    Merged commit includes the following changes:
    185215255  by Zhichao Lu:
    
        Stop populating image/object/class/text field when generating COCO tf record.
    
    --
    185213306  by Zhichao Lu:
    
        Use the params batch size and not the one from train_config in input_fn
    
    --
    185209081  by Zhichao Lu:
    
        Handle the case when there are no ground-truth masks for an image.
    
    --
    185195531  by Zhichao Lu:
    
        Remove unstack and stack operations on features from third_party/object_detection/model.py.
    
    --
    185195017  by Zhichao Lu:
    
        Matrix multiplication based gather op implementation.
    
    --
    185187744  by Zhichao Lu:
    
        Fix eval_util minor issue.
    
    --
    185098733  by Zhichao Lu:
    
        Internal change
    
    185076656  by Zhichao Lu:
    
        Increment the amount of boxes for coco17.
    
    --
    185074199  by Zhichao Lu:
    
        Add config for SSD Resnet50 v1 with FPN.
    
    --
    185060199  by Zhichao Lu:
    
        Fix a bug in clear_detections.
        This method set detection_keys to an empty dictionary instead of an empty set. I've refactored so that this method and the constructor use the same code path.
    
    --
    185031359  by Zhichao Lu:
    
        Eval TPU trained models continuously.
    
    --
    185016591  by Zhichao Lu:
    
        Use TPUEstimatorSpec for TPU
    
    --
    185013651  by Zhichao Lu:
    
        Add PreprocessorCache to record and duplicate augmentations.
    
    --
    184921763  by Zhichao Lu:
    
        Minor fixes for object detection.
    
    --
    184920610  by Zhichao Lu:
    
        Adds a model builder test for "embedded_ssd_mobilenet_v1" feature extractor.
    
    --
    184919284  by Zhichao Lu:
    
        Added unit tests for TPU, with optional training / eval.
    
    --
    184915910  by Zhichao Lu:
    
        Update third_party g3 doc with Mask RCNN detection models.
    
    --
    184914085  by Zhichao Lu:
    
        Slight change to WeightSharedConvolutionalBoxPredictor implementation to make things match more closely with RetinaNet.  Specifically we now construct the box encoding and class predictor towers separately rather than having them share weights until penultimate layer.
    
    --
    184913786  by Zhichao Lu:
    
        Plumbs SSD Resnet V1 with FPN models into model builder.
    
    --
    184910030  by Zhichao Lu:
    
        Add coco metrics to evaluator.
    
    --
    184897758  by Zhichao Lu:
    
        Merge changes from github.
    
    --
    184888736  by Zhichao Lu:
    
        Ensure groundtruth_weights are always 1-D.
    
    --
    184887256  by Zhichao Lu:
    
        Introduce an option to add summaries in the model so it can be turned off when necessary.
    
    --
    184865559  by Zhichao Lu:
    
        Updating inputs so that a dictionary of tensors is returned from input_fn. Moving unbatch/unpad to model.py.
        Also removing source_id key from features dictionary, and replacing with an integer hash.
    
    --
    184859205  by Zhichao Lu:
    
        This CL is trying to hide those differences by making the default settings work with the public code.
    
    --
    184769779  by Zhichao Lu:
    
        Pass groundtruth weights into ssd meta architecture all the way to target assigner.
    
        This will allow training ssd models with padded groundtruth tensors.
    
    --
    184767117  by Zhichao Lu:
    
        * Add `params` arg to make all input fns work with TPUEstimator
        * Add --master
        * Output eval results
    
    --
    184766244  by Zhichao Lu:
    
        Update create_coco_tf_record to include category indices
    
    --
    184752937  by Zhichao Lu:
    
        Create a third_party version of TPU compatible mobilenet_v2_focal_loss coco config.
    
    --
    184750174  by Zhichao Lu:
    
        A few small fixes for multiscale anchor generator and a test.
    
    --
    184746581  by Zhichao Lu:
    
        Update jupyter notebook to show mask if provided by model.
    
    --
    184728646  by Zhichao Lu:
    
        Adding a few more tests to make sure decoding with/without label maps performs as expected.
    
    --
    184624154  by Zhichao Lu:
    
        Add an object detection binary for TPU.
    
    --
    184622118  by Zhichao Lu:
    
        Batch, transform, and unbatch in the tflearn interface.
    
    --
    184595064  by Zhichao Lu:
    
        Add support for training grayscale models.
    
    --
    184532026  by Zhichao Lu:
    
        Change dataset_builder.build to perform optional batching using tf.data.Dataset API
    
    --
    184330239  by Zhichao Lu:
    
        Add augment_input_data and transform_input_data helper functions to third_party/tensorflow_models/object_detection/inputs.py
    
    --
    184328681  by Zhichao Lu:
    
        Use an internal rgb to gray method that can be quantized.
    
    --
    184327909  by Zhichao Lu:
    
        Helper function to return padding shapes to use with Dataset.padded_batch.
    
    --
    184326291  by Zhichao Lu:
    
        Added decode_func for specialized decoding.
    
    --
    184314676  by Zhichao Lu:
    
        Add unstack_batch method to inputs.py.
    
        This will enable us to convert batched tensors to lists of tensors. This is compatible with OD API that consumes groundtruth batch as a list of tensors.
    
    --
    184281269  by Zhichao Lu:
    
        Internal test target changes.
    
    --
    184192851  by Zhichao Lu:
    
        Adding `Estimator` interface for object detection.
    
    --
    184187885  by Zhichao Lu:
    
        Add config_util functions to help with input pipeline.
    
        1. function to return expected shapes from the resizer config
        2. function to extract image_resizer_config from model_config.
    
    --
    184139892  by Zhichao Lu:
    
        Adding support for depthwise SSD (ssd-lite) and depthwise box predictions.
    
    --
    184089891  by Zhichao Lu:
    
        Fix third_party faster rcnn resnet101 coco config.
    
    --
    184083378  by Zhichao Lu:
    
        In the case when there is no object/weights field in tf.Example proto, return a default weight of 1.0 for all boxes.
    
    --
    
    PiperOrigin-RevId: 185215255

diff --git a/research/object_detection/BUILD b/research/object_detection/BUILD
index 6cda848d..e688edfe 100644
--- a/research/object_detection/BUILD
+++ b/research/object_detection/BUILD
@@ -4,10 +4,14 @@ package(
     default_visibility = ["//visibility:public"],
 )
 
+load("//learning/brain/contrib/learn/tpu:tpu.bzl", "cloud_tpu_py_binaries")
+
 licenses(["notice"])
 
 # Apache 2.0
 
+exports_files(["LICENSE"])
+
 py_library(
     name = "inputs",
     srcs = [
@@ -15,11 +19,14 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow/models/research/object_detection:trainer",
         "//tensorflow/models/research/object_detection/builders:dataset_builder",
+        "//tensorflow/models/research/object_detection/builders:image_resizer_builder",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
         "//tensorflow/models/research/object_detection/builders:preprocessor_builder",
         "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
         "//tensorflow/models/research/object_detection/protos:train_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:config_util",
         "//tensorflow/models/research/object_detection/utils:dataset_util",
         "//tensorflow/models/research/object_detection/utils:ops",
     ],
@@ -44,6 +51,109 @@ py_test(
     ],
 )
 
+py_binary(
+    name = "model",
+    srcs = [
+        "model.py",
+    ],
+    deps = [
+        ":inputs",
+        ":model_hparams",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection:eval_util",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
+        "//tensorflow/models/research/object_detection/builders:optimizer_builder",
+        "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/utils:variables_helper",
+        "//tensorflow/models/research/object_detection/utils:visualization_utils",
+    ],
+)
+
+py_library(
+    name = "model_hparams",
+    srcs = [
+        "model_hparams.py",
+    ],
+    deps = [
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "model_test",
+    timeout = "long",
+    srcs = [
+        "model_test.py",
+    ],
+    data = [
+        "//tensorflow/models/research/object_detection/data:pet_label_map.pbtxt",
+        "//tensorflow/models/research/object_detection/samples/configs:faster_rcnn_resnet50_pets.config",
+        "//tensorflow/models/research/object_detection/samples/configs:ssd_inception_v2_pets.config",
+        "//tensorflow/models/research/object_detection/test_data:pets_examples.record",
+    ],
+    deps = [
+        ":inputs",
+        ":model",
+        ":model_hparams",
+        ":model_test_util",
+        "//mock",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/utils:ops",
+    ],
+)
+
+MODEL_TPU_DEPS = [
+    ":inputs",
+    ":model",
+    ":model_hparams",
+    "//tensorflow",
+    "//tensorflow/models/research/object_detection:eval_util",
+    "//tensorflow/models/research/object_detection/builders:model_builder",
+    "//tensorflow/models/research/object_detection/builders:optimizer_builder",
+    "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
+    "//tensorflow/models/research/object_detection/utils:config_util",
+    "//tensorflow/models/research/object_detection/utils:label_map_util",
+    "//tensorflow/models/research/object_detection/utils:ops",
+    "//tensorflow/models/research/object_detection/utils:variables_helper",
+    "//tensorflow/models/research/object_detection/utils:visualization_utils",
+]
+
+cloud_tpu_py_binaries(
+    name = "model_tpu",
+    srcs = [
+        "model_tpu.py",
+    ],
+    main = "model_tpu.py",
+    deps = MODEL_TPU_DEPS,
+)
+
+py_library(
+    name = "model_tpu_lib",
+    srcs = [
+        "model_tpu.py",
+    ],
+    deps = MODEL_TPU_DEPS,
+)
+
+py_library(
+    name = "model_test_util",
+    srcs = [
+        "model_test_util.py",
+    ],
+    deps = [
+        ":model",
+        ":model_hparams",
+        "//tensorflow",
+    ],
+)
+
 py_binary(
     name = "train",
     srcs = [
@@ -113,6 +223,7 @@ py_library(
         "//tensorflow/models/research/object_detection:eval_util",
         "//tensorflow/models/research/object_detection/core:prefetcher",
         "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/metrics:coco_evaluation",
         "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
         "//tensorflow/models/research/object_detection/utils:object_detection_evaluation",
     ],
diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index b0a8a78c..9e971552 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -69,6 +69,8 @@ Extras:
       Supported object detection evaluation protocols</a><br>
   * <a href='g3doc/oid_inference_and_evaluation.md'>
       Inference and evaluation on the Open Images dataset</a><br>
+  * <a href='g3doc/instance_segmentation.md'>
+      Run an instance segmentation model
 
 ## Getting Help
 
@@ -77,7 +79,7 @@ API, create a new question on [StackOverflow](https://stackoverflow.com/) with
 the tags "tensorflow" and "object-detection".
 
 Please report bugs (actually broken code, not usage questions) to the
-tensorflow/models Github
+tensorflow/models GitHub
 [issue tracker](https://github.com/tensorflow/models/issues), prefixing the
 issue name with "object_detection".
 
@@ -85,6 +87,15 @@ issue name with "object_detection".
 
 ## Release information
 
+### February 9, 2018
+
+We now support instance segmentation!!  In this API update we support a number of instance segmentation models similar to those discussed in the [Mask R-CNN paper](https://arxiv.org/abs/1703.06870). For further details refer to
+[our slides](http://presentations.cocodataset.org/Places17-GMRI.pdf) from the 2017 Coco + Places Workshop.
+Refer to the section on [Running an Instance Segmentation Model](g3doc/instance_segmentation.md) for instructions on how to configure a model
+that predicts masks in addition to object bounding boxes.
+
+<b>Thanks to contributors</b>: Alireza Fathi, Zhichao Lu, Vivek Rathod, Ronny Votel, Jonathan Huang
+
 ### November 17, 2017
 
 As a part of the Open Images V3 release we have released:
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
index f261cb8c..ea8c266c 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -16,8 +16,8 @@
 
 Generates grid anchors on the fly corresponding to multiple CNN layers as
 described in:
-"Focal Loss for Dense Object Detection"
-T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar (https://arxiv.org/abs/1708.02002)
+"Focal Loss for Dense Object Detection" (https://arxiv.org/abs/1708.02002)
+T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
 """
 
 from object_detection.anchor_generators import grid_anchor_generator
@@ -77,11 +77,15 @@ class MultiscaleGridAnchorGenerator(object):
       a list of integers, one for each expected feature map to be passed to
       the Generate function.
     """
-    return self._aspect_ratios * self._scales_per_octave
+    return len(self._anchor_grid_info) * [
+        len(self._aspect_ratios) * self._scales_per_octave]
 
   def generate(self, feature_map_shape_list, im_height, im_width):
     """Generates a collection of bounding boxes to be used as anchors.
 
+    Currently we require the input image shape to be statically defined.  That
+    is, im_height and im_width should be integers rather than tensors.
+
     Args:
       feature_map_shape_list: list of pairs of convnet layer resolutions in the
         format [(height_0, width_0), (height_1, width_1), ...]. For example,
@@ -92,8 +96,12 @@ class MultiscaleGridAnchorGenerator(object):
 
     Returns:
       boxes: a BoxList holding a collection of N anchor boxes
+    Raises:
+      ValueError: if im_height and im_width are not integers.
     """
-
+    if not isinstance(im_height, int) or not isinstance(im_width, int):
+      raise ValueError('MultiscaleGridAnchorGenerator currently requires '
+                       'input image shape to be statically defined.')
     anchor_grid_list = []
     for feat_shape, grid_info in zip(feature_map_shape_list,
                                      self._anchor_grid_info):
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
index 9bf31f30..dd9b8970 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -46,6 +46,30 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       anchor_corners_out = anchor_corners.eval()
       self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
+  def test_num_anchors_per_location(self):
+    min_level = 5
+    max_level = 6
+    anchor_scale = 4.0
+    aspect_ratios = [1.0, 2.0]
+    scales_per_octave = 3
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+    self.assertEqual(anchor_generator.num_anchors_per_location(), [6, 6])
+
+  def test_construct_single_anchor_fails_with_tensor_image_size(self):
+    min_level = 5
+    max_level = 5
+    anchor_scale = 4.0
+    aspect_ratios = [1.0]
+    scales_per_octave = 1
+    im_height = tf.constant(64)
+    im_width = tf.constant(64)
+    feature_map_shape_list = [(2, 2)]
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+    with self.assertRaises(ValueError):
+      anchor_generator.generate(feature_map_shape_list, im_height, im_width)
+
   def test_construct_single_anchor_with_odd_input_dimension(self):
 
     def graph_fn():
diff --git a/research/object_detection/builders/BUILD b/research/object_detection/builders/BUILD
index 7f267281..0bb3458e 100644
--- a/research/object_detection/builders/BUILD
+++ b/research/object_detection/builders/BUILD
@@ -32,6 +32,7 @@ py_library(
         "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
         "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
         "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_resnet_v1_fpn_feature_extractor",
         "//tensorflow/models/research/object_detection/protos:model_py_pb2",
     ],
 )
@@ -44,6 +45,7 @@ py_test(
         "//tensorflow",
         "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
         "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/models:embedded_ssd_mobilenet_v1_feature_extractor",
         "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
         "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
         "//tensorflow/models/research/object_detection/models:faster_rcnn_nas_feature_extractor",
@@ -51,6 +53,7 @@ py_test(
         "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
         "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
         "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_resnet_v1_fpn_feature_extractor",
         "//tensorflow/models/research/object_detection/protos:model_py_pb2",
     ],
 )
diff --git a/research/object_detection/builders/anchor_generator_builder.py b/research/object_detection/builders/anchor_generator_builder.py
index 072ca376..5bd3c84f 100644
--- a/research/object_detection/builders/anchor_generator_builder.py
+++ b/research/object_detection/builders/anchor_generator_builder.py
@@ -83,10 +83,10 @@ def build(anchor_generator_config):
       'anchor_generator_oneof') == 'multiscale_anchor_generator':
     cfg = anchor_generator_config.multiscale_anchor_generator
     return multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator(
-        cfg.min_lvl,
-        cfg.max_lvl,
+        cfg.min_level,
+        cfg.max_level,
         cfg.anchor_scale,
-        cfg.aspect_ratios,
+        [float(aspect_ratio) for aspect_ratio in cfg.aspect_ratios],
         cfg.scales_per_octave
     )
   else:
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index ecc1eca1..5112c028 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -22,6 +22,7 @@ import tensorflow as tf
 from google.protobuf import text_format
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.anchor_generators import multiple_grid_anchor_generator
+from object_detection.anchor_generators import multiscale_grid_anchor_generator
 from object_detection.builders import anchor_generator_builder
 from object_detection.protos import anchor_generator_pb2
 
@@ -252,6 +253,31 @@ class AnchorGeneratorBuilderTest(tf.test.TestCase):
     with self.assertRaises(ValueError):
       anchor_generator_builder.build(anchor_generator_proto)
 
+  def test_build_multiscale_anchor_generator_custom_aspect_ratios(self):
+    anchor_generator_text_proto = """
+      multiscale_anchor_generator {
+        aspect_ratios: [1.0]
+      }
+    """
+    anchor_generator_proto = anchor_generator_pb2.AnchorGenerator()
+    text_format.Merge(anchor_generator_text_proto, anchor_generator_proto)
+    anchor_generator_object = anchor_generator_builder.build(
+        anchor_generator_proto)
+    self.assertTrue(isinstance(anchor_generator_object,
+                               multiscale_grid_anchor_generator.
+                               MultiscaleGridAnchorGenerator))
+    print anchor_generator_object._anchor_grid_info
+    for level, anchor_grid_info in zip(
+        range(3, 8), anchor_generator_object._anchor_grid_info):
+      self.assertEqual(set(anchor_grid_info.keys()), set(['level', 'info']))
+      self.assertTrue(level, anchor_grid_info['level'])
+      self.assertEqual(len(anchor_grid_info['info']), 4)
+      self.assertAllClose(anchor_grid_info['info'][0], [2**0, 2**0.5])
+      self.assertTrue(anchor_grid_info['info'][1], 1.0)
+      self.assertAllClose(anchor_grid_info['info'][2],
+                          [4.0 * 2**level, 4.0 * 2**level])
+      self.assertAllClose(anchor_grid_info['info'][3], [2**level, 2**level])
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 38eb1c23..50bb2238 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -64,7 +64,9 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes):
         kernel_size=conv_box_predictor.kernel_size,
         box_code_size=conv_box_predictor.box_code_size,
         apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,
-        class_prediction_bias_init=conv_box_predictor.class_prediction_bias_init
+        class_prediction_bias_init=(conv_box_predictor.
+                                    class_prediction_bias_init),
+        use_depthwise=conv_box_predictor.use_depthwise
     )
     return box_predictor_object
 
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index c754cb79..398e4c22 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -83,6 +83,7 @@ class ConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
         box_code_size: 3
         apply_sigmoid_to_scores: true
         class_prediction_bias_init: 4.0
+        use_depthwise: true
       }
     """
     conv_hyperparams_text_proto = """
@@ -118,6 +119,7 @@ class ConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertAlmostEqual(box_predictor._class_prediction_bias_init, 4.0)
     self.assertEqual(box_predictor.num_classes, 10)
     self.assertFalse(box_predictor._is_training)
+    self.assertTrue(box_predictor._use_depthwise)
 
   def test_construct_default_conv_box_predictor(self):
     box_predictor_text_proto = """
@@ -148,6 +150,7 @@ class ConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertFalse(box_predictor._apply_sigmoid_to_scores)
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
+    self.assertFalse(box_predictor._use_depthwise)
 
 
 class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 869a070d..de9be429 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -24,18 +24,93 @@ that wraps the build function.
 
 import tensorflow as tf
 
+from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
 
-def build(input_reader_config, num_workers=1, worker_index=0):
-  """Builds a tf.data.Dataset based on the InputReader config.
+def _get_padding_shapes(dataset, max_num_boxes, num_classes,
+                        spatial_image_shape):
+  """Returns shapes to pad dataset tensors to before batching.
+
+  Args:
+    dataset: tf.data.Dataset object.
+    max_num_boxes: Max number of groundtruth boxes needed to computes shapes for
+      padding.
+    num_classes: Number of classes in the dataset needed to compute shapes for
+      padding.
+    spatial_image_shape: A list of two integers of the form [height, width]
+      containing expected spatial shape of the imaage.
+
+  Returns:
+    A dictionary keyed by fields.InputDataFields containing padding shapes for
+    tensors in the dataset.
+  """
+  height, width = spatial_image_shape
+  padding_shapes = {
+      fields.InputDataFields.image: [height, width, 3],
+      fields.InputDataFields.source_id: [],
+      fields.InputDataFields.filename: [],
+      fields.InputDataFields.key: [],
+      fields.InputDataFields.groundtruth_difficult: [max_num_boxes],
+      fields.InputDataFields.groundtruth_boxes: [max_num_boxes, 4],
+      fields.InputDataFields.groundtruth_classes: [
+          max_num_boxes, num_classes
+      ],
+      fields.InputDataFields.groundtruth_instance_masks: [max_num_boxes, height,
+                                                          width],
+      fields.InputDataFields.groundtruth_is_crowd: [max_num_boxes],
+      fields.InputDataFields.groundtruth_group_of: [max_num_boxes],
+      fields.InputDataFields.groundtruth_area: [max_num_boxes],
+      fields.InputDataFields.groundtruth_weights: [max_num_boxes],
+      fields.InputDataFields.num_groundtruth_boxes: [],
+      fields.InputDataFields.groundtruth_label_types: [max_num_boxes],
+      fields.InputDataFields.groundtruth_label_scores: [max_num_boxes],
+      fields.InputDataFields.true_image_shape: [3]
+  }
+  if fields.InputDataFields.groundtruth_keypoints in dataset.output_shapes:
+    tensor_shape = dataset.output_shapes[fields.InputDataFields.
+                                         groundtruth_keypoints]
+    padding_shape = [max_num_boxes, tensor_shape[1].value,
+                     tensor_shape[2].value]
+    padding_shapes[fields.InputDataFields.groundtruth_keypoints] = padding_shape
+  if (fields.InputDataFields.groundtruth_keypoint_visibilities
+      in dataset.output_shapes):
+    tensor_shape = dataset.output_shapes[fields.InputDataFields.
+                                         groundtruth_keypoint_visibilities]
+    padding_shape = [max_num_boxes, tensor_shape[1].value]
+    padding_shapes[fields.InputDataFields.
+                   groundtruth_keypoint_visibilities] = padding_shape
+  return {tensor_key: padding_shapes[tensor_key]
+          for tensor_key, _ in dataset.output_shapes.items()}
+
+
+def build(input_reader_config, transform_input_data_fn=None, num_workers=1,
+          worker_index=0, batch_size=1, max_num_boxes=None, num_classes=None,
+          spatial_image_shape=None):
+  """Builds a tf.data.Dataset.
+
+  Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all
+  records. Optionally, if `batch_size` > 1 and `max_num_boxes`, `num_classes`
+  and `spatial_image_shape` are not None, returns a padded batched
+  tf.data.Dataset.
 
   Args:
     input_reader_config: A input_reader_pb2.InputReader object.
-    num_workers: Number of workers / shards.
-    worker_index: Id for the current worker.
+    transform_input_data_fn: Function to apply to all records, or None if
+      no extra decoding is required.
+    num_workers: Number of workers (tpu shard).
+    worker_index: Id for the current worker (tpu shard).
+    batch_size: Batch size. If not None, returns a padded batch dataset.
+    max_num_boxes: Max number of groundtruth boxes needed to computes shapes for
+      padding. This is only used if batch_size is greater than 1.
+    num_classes: Number of classes in the dataset needed to compute shapes for
+      padding. This is only used if batch_size is greater than 1.
+    spatial_image_shape: a list of two integers of the form [height, width]
+      containing expected spatial shape of the image after applying
+      transform_input_data_fn. This is needed to compute shapes for padding and
+      only used if batch_size is greater than 1.
 
   Returns:
     A tf.data.Dataset based on the input_reader_config.
@@ -43,6 +118,8 @@ def build(input_reader_config, num_workers=1, worker_index=0):
   Raises:
     ValueError: On invalid input reader proto.
     ValueError: If no input paths are specified.
+    ValueError: If batch_size > 1 and any of (max_num_boxes, num_classes,
+      spatial_image_shape) is None.
   """
   if not isinstance(input_reader_config, input_reader_pb2.InputReader):
     raise ValueError('input_reader_config not of type '
@@ -62,8 +139,29 @@ def build(input_reader_config, num_workers=1, worker_index=0):
         instance_mask_type=input_reader_config.mask_type,
         label_map_proto_file=label_map_proto_file)
 
-    return dataset_util.read_dataset(
-        tf.data.TFRecordDataset, decoder.decode, config.input_path[:],
+    def process_fn(value):
+      processed = decoder.decode(value)
+      if transform_input_data_fn is not None:
+        return transform_input_data_fn(processed)
+      return processed
+
+    dataset = dataset_util.read_dataset(
+        tf.data.TFRecordDataset, process_fn, config.input_path[:],
         input_reader_config, num_workers, worker_index)
 
+    if batch_size > 1:
+      if num_classes is None:
+        raise ValueError('`num_classes` must be set when batch_size > 1.')
+      if max_num_boxes is None:
+        raise ValueError('`max_num_boxes` must be set when batch_size > 1.')
+      if spatial_image_shape is None:
+        raise ValueError('`spatial_image_shape` must be set when batch_size > '
+                         '1 .')
+      padding_shapes = _get_padding_shapes(dataset, max_num_boxes, num_classes,
+                                           spatial_image_shape)
+      dataset = dataset.apply(
+          tf.contrib.data.padded_batch_and_drop_remainder(batch_size,
+                                                          padding_shapes))
+    return dataset
+
   raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index 7dcd6caf..24c99d7d 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -130,18 +130,92 @@ class DatasetBuilderTest(tf.test.TestCase):
     with sv.prepare_or_wait_for_session() as sess:
       sv.start_queue_runners(sess)
       output_dict = sess.run(tensor_dict)
+    self.assertAllEqual(
+        (1, 4, 5),
+        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
 
-    self.assertEquals((4, 5, 3),
-                      output_dict[fields.InputDataFields.image].shape)
-    self.assertEquals([2],
-                      output_dict[fields.InputDataFields.groundtruth_classes])
-    self.assertEquals(
-        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+  def test_build_tf_record_input_reader_with_batch_size_two(self):
+    tf_record_path = self.create_tf_record()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+
+    def one_hot_class_encoding_fn(tensor_dict):
+      tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(
+          tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
+      return tensor_dict
+
+    tensor_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(
+            input_reader_proto,
+            transform_input_data_fn=one_hot_class_encoding_fn,
+            batch_size=2,
+            max_num_boxes=2,
+            num_classes=3,
+            spatial_image_shape=[4, 5])).get_next()
+
+    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    with sv.prepare_or_wait_for_session() as sess:
+      sv.start_queue_runners(sess)
+      output_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual([2, 4, 5, 3],
+                        output_dict[fields.InputDataFields.image].shape)
+    self.assertAllEqual([2, 2, 3],
+                        output_dict[fields.InputDataFields.groundtruth_classes].
+                        shape)
+    self.assertAllEqual([2, 2, 4],
+                        output_dict[fields.InputDataFields.groundtruth_boxes].
+                        shape)
     self.assertAllEqual(
-        [0.0, 0.0, 1.0, 1.0],
-        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+        [[[0.0, 0.0, 1.0, 1.0],
+          [0.0, 0.0, 0.0, 0.0]],
+         [[0.0, 0.0, 1.0, 1.0],
+          [0.0, 0.0, 0.0, 0.0]]],
+        output_dict[fields.InputDataFields.groundtruth_boxes])
+
+  def test_build_tf_record_input_reader_with_batch_size_two_and_masks(self):
+    tf_record_path = self.create_tf_record()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      load_instance_masks: true
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+
+    def one_hot_class_encoding_fn(tensor_dict):
+      tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(
+          tensor_dict[fields.InputDataFields.groundtruth_classes] - 1, depth=3)
+      return tensor_dict
+
+    tensor_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(
+            input_reader_proto,
+            transform_input_data_fn=one_hot_class_encoding_fn,
+            batch_size=2,
+            max_num_boxes=2,
+            num_classes=3,
+            spatial_image_shape=[4, 5])).get_next()
+
+    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    with sv.prepare_or_wait_for_session() as sess:
+      sv.start_queue_runners(sess)
+      output_dict = sess.run(tensor_dict)
+
     self.assertAllEqual(
-        (1, 4, 5),
+        [2, 2, 4, 5],
         output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
 
   def test_raises_error_with_no_input_paths(self):
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index f9a600ca..2dbceaec 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -79,19 +79,32 @@ def build(image_resizer_config):
             keep_aspect_ratio_config.max_dimension):
       raise ValueError('min_dimension > max_dimension')
     method = _tf_resize_method(keep_aspect_ratio_config.resize_method)
-    return functools.partial(
+    image_resizer_fn = functools.partial(
         preprocessor.resize_to_range,
         min_dimension=keep_aspect_ratio_config.min_dimension,
         max_dimension=keep_aspect_ratio_config.max_dimension,
         method=method,
         pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension)
-  if image_resizer_config.WhichOneof(
+    if not keep_aspect_ratio_config.convert_to_grayscale:
+      return image_resizer_fn
+  elif image_resizer_config.WhichOneof(
       'image_resizer_oneof') == 'fixed_shape_resizer':
     fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer
     method = _tf_resize_method(fixed_shape_resizer_config.resize_method)
-    return functools.partial(
+    image_resizer_fn = functools.partial(
         preprocessor.resize_image,
         new_height=fixed_shape_resizer_config.height,
         new_width=fixed_shape_resizer_config.width,
         method=method)
-  raise ValueError('Invalid image resizer option.')
+    if not fixed_shape_resizer_config.convert_to_grayscale:
+      return image_resizer_fn
+  else:
+    raise ValueError('Invalid image resizer option.')
+
+  def grayscale_image_resizer(image):
+    [resized_image, resized_image_shape] = image_resizer_fn(image)
+    grayscale_image = preprocessor.rgb_to_gray(resized_image)
+    grayscale_image_shape = tf.concat([resized_image_shape[:-1], [1]], 0)
+    return [grayscale_image, grayscale_image_shape]
+
+  return functools.partial(grayscale_image_resizer)
diff --git a/research/object_detection/builders/matcher_builder.py b/research/object_detection/builders/matcher_builder.py
index 6ec49da9..d334f435 100644
--- a/research/object_detection/builders/matcher_builder.py
+++ b/research/object_detection/builders/matcher_builder.py
@@ -45,7 +45,9 @@ def build(matcher_config):
         matched_threshold=matched_threshold,
         unmatched_threshold=unmatched_threshold,
         negatives_lower_than_unmatched=matcher.negatives_lower_than_unmatched,
-        force_match_for_each_row=matcher.force_match_for_each_row)
+        force_match_for_each_row=matcher.force_match_for_each_row,
+        use_matmul_gather=matcher.use_matmul_gather)
   if matcher_config.WhichOneof('matcher_oneof') == 'bipartite_matcher':
-    return bipartite_matcher.GreedyBipartiteMatcher()
+    matcher = matcher_config.bipartite_matcher
+    return bipartite_matcher.GreedyBipartiteMatcher(matcher.use_matmul_gather)
   raise ValueError('Empty matcher.')
diff --git a/research/object_detection/builders/matcher_builder_test.py b/research/object_detection/builders/matcher_builder_test.py
index c4275aae..66854491 100644
--- a/research/object_detection/builders/matcher_builder_test.py
+++ b/research/object_detection/builders/matcher_builder_test.py
@@ -62,6 +62,7 @@ class MatcherBuilderTest(tf.test.TestCase):
         unmatched_threshold: 0.3
         negatives_lower_than_unmatched: false
         force_match_for_each_row: true
+        use_matmul_gather: true
       }
     """
     matcher_proto = matcher_pb2.Matcher()
@@ -72,6 +73,7 @@ class MatcherBuilderTest(tf.test.TestCase):
     self.assertAlmostEqual(matcher_object._unmatched_threshold, 0.3)
     self.assertFalse(matcher_object._negatives_lower_than_unmatched)
     self.assertTrue(matcher_object._force_match_for_each_row)
+    self.assertTrue(matcher_object._use_matmul_gather)
 
   def test_build_bipartite_matcher(self):
     matcher_text_proto = """
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 7e2019ca..c48f2fdc 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -31,6 +31,7 @@ from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extr
 from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
 from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
+from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
 from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
@@ -42,6 +43,9 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_inception_v2': SSDInceptionV2FeatureExtractor,
     'ssd_inception_v3': SSDInceptionV3FeatureExtractor,
     'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,
+    'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
+    'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
+    'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
     'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,
 }
 
@@ -62,13 +66,14 @@ FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
 }
 
 
-def build(model_config, is_training):
+def build(model_config, is_training, add_summaries=True):
   """Builds a DetectionModel based on the model config.
 
   Args:
     model_config: A model.proto object containing the config for the desired
       DetectionModel.
     is_training: True if this model is being built for training purposes.
+    add_summaries: Whether to add tensorflow summaries in the model graph.
 
   Returns:
     DetectionModel based on the config.
@@ -80,9 +85,10 @@ def build(model_config, is_training):
     raise ValueError('model_config not of type model_pb2.DetectionModel.')
   meta_architecture = model_config.WhichOneof('model')
   if meta_architecture == 'ssd':
-    return _build_ssd_model(model_config.ssd, is_training)
+    return _build_ssd_model(model_config.ssd, is_training, add_summaries)
   if meta_architecture == 'faster_rcnn':
-    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training)
+    return _build_faster_rcnn_model(model_config.faster_rcnn, is_training,
+                                    add_summaries)
   raise ValueError('Unknown meta architecture: {}'.format(meta_architecture))
 
 
@@ -107,6 +113,7 @@ def _build_ssd_feature_extractor(feature_extractor_config, is_training,
   pad_to_multiple = feature_extractor_config.pad_to_multiple
   batch_norm_trainable = feature_extractor_config.batch_norm_trainable
   use_explicit_padding = feature_extractor_config.use_explicit_padding
+  use_depthwise = feature_extractor_config.use_depthwise
   conv_hyperparams = hyperparams_builder.build(
       feature_extractor_config.conv_hyperparams, is_training)
 
@@ -117,16 +124,17 @@ def _build_ssd_feature_extractor(feature_extractor_config, is_training,
   return feature_extractor_class(is_training, depth_multiplier, min_depth,
                                  pad_to_multiple, conv_hyperparams,
                                  batch_norm_trainable, reuse_weights,
-                                 use_explicit_padding)
+                                 use_explicit_padding, use_depthwise)
 
 
-def _build_ssd_model(ssd_config, is_training):
+def _build_ssd_model(ssd_config, is_training, add_summaries):
   """Builds an SSD detection model based on the model config.
 
   Args:
     ssd_config: A ssd.proto object containing the config for the desired
       SSDMetaArch.
     is_training: True if this model is being built for training purposes.
+    add_summaries: Whether to add tf summaries in the model.
 
   Returns:
     SSDMetaArch based on the config.
@@ -173,7 +181,8 @@ def _build_ssd_model(ssd_config, is_training):
       classification_weight,
       localization_weight,
       normalize_loss_by_num_matches,
-      hard_example_miner)
+      hard_example_miner,
+      add_summaries=add_summaries)
 
 
 def _build_faster_rcnn_feature_extractor(
@@ -207,7 +216,7 @@ def _build_faster_rcnn_feature_extractor(
       batch_norm_trainable, reuse_weights)
 
 
-def _build_faster_rcnn_model(frcnn_config, is_training):
+def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
   """Builds a Faster R-CNN or R-FCN detection model based on the model config.
 
   Builds R-FCN model if the second_stage_box_predictor in the config is of type
@@ -215,8 +224,9 @@ def _build_faster_rcnn_model(frcnn_config, is_training):
 
   Args:
     frcnn_config: A faster_rcnn.proto object containing the config for the
-    desired FasterRCNNMetaArch or RFCNMetaArch.
+      desired FasterRCNNMetaArch or RFCNMetaArch.
     is_training: True if this model is being built for training purposes.
+    add_summaries: Whether to add tf summaries in the model.
 
   Returns:
     FasterRCNNMetaArch based on the config.
@@ -312,7 +322,8 @@ def _build_faster_rcnn_model(frcnn_config, is_training):
       second_stage_classification_loss,
       'second_stage_classification_loss_weight':
       second_stage_classification_loss_weight,
-      'hard_example_miner': hard_example_miner}
+      'hard_example_miner': hard_example_miner,
+      'add_summaries': add_summaries}
 
   if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):
     return rfcn_meta_arch.RFCNMetaArch(
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index 5e217094..fd3098b2 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -26,12 +26,14 @@ from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extr
 from object_detection.models import faster_rcnn_inception_v2_feature_extractor as frcnn_inc_v2
 from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
+from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
+from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
 from object_detection.protos import model_pb2
 
-FEATURE_EXTRACTOR_MAPS = {
+FRCNN_RESNET_FEAT_MAPS = {
     'faster_rcnn_resnet50':
     frcnn_resnet_v1.FasterRCNNResnet50FeatureExtractor,
     'faster_rcnn_resnet101':
@@ -40,6 +42,15 @@ FEATURE_EXTRACTOR_MAPS = {
     frcnn_resnet_v1.FasterRCNNResnet152FeatureExtractor
 }
 
+SSD_RESNET_V1_FPN_FEAT_MAPS = {
+    'ssd_resnet50_v1_fpn':
+    ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
+    'ssd_resnet101_v1_fpn':
+    ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
+    'ssd_resnet152_v1_fpn':
+    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor
+}
+
 
 class ModelBuilderTest(tf.test.TestCase):
 
@@ -197,6 +208,87 @@ class ModelBuilderTest(tf.test.TestCase):
     self.assertIsInstance(model._feature_extractor,
                           SSDInceptionV3FeatureExtractor)
 
+  def test_create_ssd_resnet_v1_fpn_model_from_config(self):
+    model_text_proto = """
+      ssd {
+        feature_extractor {
+          type: 'ssd_resnet50_v1_fpn'
+          conv_hyperparams {
+            regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+          }
+          batch_norm_trainable: true
+        }
+        box_coder {
+          faster_rcnn_box_coder {
+          }
+        }
+        matcher {
+          argmax_matcher {
+          }
+        }
+        similarity_calculator {
+          iou_similarity {
+          }
+        }
+        anchor_generator {
+          multiscale_anchor_generator {
+            aspect_ratios: [1.0, 2.0, 0.5]
+            scales_per_octave: 2
+          }
+        }
+        image_resizer {
+          fixed_shape_resizer {
+            height: 320
+            width: 320
+          }
+        }
+        box_predictor {
+          weight_shared_convolutional_box_predictor {
+            depth: 32
+            conv_hyperparams {
+              regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+            }
+            num_layers_before_predictor: 1
+          }
+        }
+        loss {
+          classification_loss {
+            weighted_sigmoid_focal {
+              alpha: 0.25
+              gamma: 2.0
+            }
+          }
+          localization_loss {
+            weighted_smooth_l1 {
+            }
+          }
+          classification_weight: 1.0
+          localization_weight: 1.0
+        }
+      }"""
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+
+    for extractor_type, extractor_class in SSD_RESNET_V1_FPN_FEAT_MAPS.items():
+      model_proto.ssd.feature_extractor.type = extractor_type
+      model = model_builder.build(model_proto, is_training=True)
+      self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+      self.assertIsInstance(model._feature_extractor, extractor_class)
+
   def test_create_ssd_mobilenet_v1_model_from_config(self):
     model_text_proto = """
       ssd {
@@ -270,6 +362,78 @@ class ModelBuilderTest(tf.test.TestCase):
                           SSDMobileNetV1FeatureExtractor)
     self.assertTrue(model._feature_extractor._batch_norm_trainable)
 
+  def test_create_embedded_ssd_mobilenet_v1_model_from_config(self):
+    model_text_proto = """
+      ssd {
+        feature_extractor {
+          type: 'embedded_ssd_mobilenet_v1'
+          conv_hyperparams {
+            regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+          }
+          batch_norm_trainable: true
+        }
+        box_coder {
+          faster_rcnn_box_coder {
+          }
+        }
+        matcher {
+          argmax_matcher {
+          }
+        }
+        similarity_calculator {
+          iou_similarity {
+          }
+        }
+        anchor_generator {
+          ssd_anchor_generator {
+            aspect_ratios: 1.0
+          }
+        }
+        image_resizer {
+          fixed_shape_resizer {
+            height: 256
+            width: 256
+          }
+        }
+        box_predictor {
+          convolutional_box_predictor {
+            conv_hyperparams {
+              regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+            }
+          }
+        }
+        loss {
+          classification_loss {
+            weighted_softmax {
+            }
+          }
+          localization_loss {
+            weighted_smooth_l1 {
+            }
+          }
+        }
+      }"""
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+    model = self.create_model(model_proto)
+    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+    self.assertIsInstance(model._feature_extractor,
+                          EmbeddedSSDMobileNetV1FeatureExtractor)
+
   def test_create_faster_rcnn_resnet_v1_models_from_config(self):
     model_text_proto = """
       faster_rcnn {
@@ -331,7 +495,7 @@ class ModelBuilderTest(tf.test.TestCase):
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
       model_proto.faster_rcnn.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, faster_rcnn_meta_arch.FasterRCNNMetaArch)
@@ -730,7 +894,7 @@ class ModelBuilderTest(tf.test.TestCase):
       }"""
     model_proto = model_pb2.DetectionModel()
     text_format.Merge(model_text_proto, model_proto)
-    for extractor_type, extractor_class in FEATURE_EXTRACTOR_MAPS.items():
+    for extractor_type, extractor_class in FRCNN_RESNET_FEAT_MAPS.items():
       model_proto.faster_rcnn.feature_extractor.type = extractor_type
       model = model_builder.build(model_proto, is_training=True)
       self.assertIsInstance(model, rfcn_meta_arch.RFCNMetaArch)
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index cccaba99..15e92bb9 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -19,15 +19,14 @@ import tensorflow as tf
 from object_detection.utils import learning_schedules
 
 
-def build(optimizer_config, global_summaries):
+def build(optimizer_config):
   """Create optimizer based on config.
 
   Args:
     optimizer_config: A Optimizer proto message.
-    global_summaries: A set to attach learning rate summary to.
 
   Returns:
-    An optimizer.
+    An optimizer and a list of variables for summary.
 
   Raises:
     ValueError: when using an unsupported input data type.
@@ -35,24 +34,30 @@ def build(optimizer_config, global_summaries):
   optimizer_type = optimizer_config.WhichOneof('optimizer')
   optimizer = None
 
+  summary_vars = []
   if optimizer_type == 'rms_prop_optimizer':
     config = optimizer_config.rms_prop_optimizer
+    learning_rate = _create_learning_rate(config.learning_rate)
+    summary_vars.append(learning_rate)
     optimizer = tf.train.RMSPropOptimizer(
-        _create_learning_rate(config.learning_rate, global_summaries),
+        learning_rate,
         decay=config.decay,
         momentum=config.momentum_optimizer_value,
         epsilon=config.epsilon)
 
   if optimizer_type == 'momentum_optimizer':
     config = optimizer_config.momentum_optimizer
+    learning_rate = _create_learning_rate(config.learning_rate)
+    summary_vars.append(learning_rate)
     optimizer = tf.train.MomentumOptimizer(
-        _create_learning_rate(config.learning_rate, global_summaries),
+        learning_rate,
         momentum=config.momentum_optimizer_value)
 
   if optimizer_type == 'adam_optimizer':
     config = optimizer_config.adam_optimizer
-    optimizer = tf.train.AdamOptimizer(
-        _create_learning_rate(config.learning_rate, global_summaries))
+    learning_rate = _create_learning_rate(config.learning_rate)
+    summary_vars.append(learning_rate)
+    optimizer = tf.train.AdamOptimizer(learning_rate)
 
   if optimizer is None:
     raise ValueError('Optimizer %s not supported.' % optimizer_type)
@@ -61,15 +66,14 @@ def build(optimizer_config, global_summaries):
     optimizer = tf.contrib.opt.MovingAverageOptimizer(
         optimizer, average_decay=optimizer_config.moving_average_decay)
 
-  return optimizer
+  return optimizer, summary_vars
 
 
-def _create_learning_rate(learning_rate_config, global_summaries):
+def _create_learning_rate(learning_rate_config):
   """Create optimizer learning rate based on config.
 
   Args:
     learning_rate_config: A LearningRate proto message.
-    global_summaries: A set to attach learning rate summary to.
 
   Returns:
     A learning rate.
@@ -81,7 +85,7 @@ def _create_learning_rate(learning_rate_config, global_summaries):
   learning_rate_type = learning_rate_config.WhichOneof('learning_rate')
   if learning_rate_type == 'constant_learning_rate':
     config = learning_rate_config.constant_learning_rate
-    learning_rate = config.learning_rate
+    learning_rate = tf.constant(config.learning_rate, dtype=tf.float32)
 
   if learning_rate_type == 'exponential_decay_learning_rate':
     config = learning_rate_config.exponential_decay_learning_rate
@@ -115,5 +119,4 @@ def _create_learning_rate(learning_rate_config, global_summaries):
   if learning_rate is None:
     raise ValueError('Learning_rate %s not supported.' % learning_rate_type)
 
-  global_summaries.add(tf.summary.scalar('Learning_Rate', learning_rate))
   return learning_rate
diff --git a/research/object_detection/builders/optimizer_builder_test.py b/research/object_detection/builders/optimizer_builder_test.py
index dff5a6cd..faebb05c 100644
--- a/research/object_detection/builders/optimizer_builder_test.py
+++ b/research/object_detection/builders/optimizer_builder_test.py
@@ -31,12 +31,13 @@ class LearningRateBuilderTest(tf.test.TestCase):
         learning_rate: 0.004
       }
     """
-    global_summaries = set([])
     learning_rate_proto = optimizer_pb2.LearningRate()
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
-        learning_rate_proto, global_summaries)
-    self.assertAlmostEqual(learning_rate, 0.004)
+        learning_rate_proto)
+    with self.test_session():
+      learning_rate_out = learning_rate.eval()
+    self.assertAlmostEqual(learning_rate_out, 0.004)
 
   def testBuildExponentialDecayLearningRate(self):
     learning_rate_text_proto = """
@@ -47,11 +48,10 @@ class LearningRateBuilderTest(tf.test.TestCase):
         staircase: false
       }
     """
-    global_summaries = set([])
     learning_rate_proto = optimizer_pb2.LearningRate()
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
     self.assertTrue(isinstance(learning_rate, tf.Tensor))
 
   def testBuildManualStepLearningRate(self):
@@ -67,11 +67,10 @@ class LearningRateBuilderTest(tf.test.TestCase):
         }
       }
     """
-    global_summaries = set([])
     learning_rate_proto = optimizer_pb2.LearningRate()
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
     self.assertTrue(isinstance(learning_rate, tf.Tensor))
 
   def testBuildCosineDecayLearningRate(self):
@@ -83,22 +82,19 @@ class LearningRateBuilderTest(tf.test.TestCase):
         warmup_steps: 1000
       }
     """
-    global_summaries = set([])
     learning_rate_proto = optimizer_pb2.LearningRate()
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     learning_rate = optimizer_builder._create_learning_rate(
-        learning_rate_proto, global_summaries)
+        learning_rate_proto)
     self.assertTrue(isinstance(learning_rate, tf.Tensor))
 
   def testRaiseErrorOnEmptyLearningRate(self):
     learning_rate_text_proto = """
     """
-    global_summaries = set([])
     learning_rate_proto = optimizer_pb2.LearningRate()
     text_format.Merge(learning_rate_text_proto, learning_rate_proto)
     with self.assertRaises(ValueError):
-      optimizer_builder._create_learning_rate(
-          learning_rate_proto, global_summaries)
+      optimizer_builder._create_learning_rate(learning_rate_proto)
 
 
 class OptimizerBuilderTest(tf.test.TestCase):
@@ -119,10 +115,9 @@ class OptimizerBuilderTest(tf.test.TestCase):
       }
       use_moving_average: false
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(isinstance(optimizer, tf.train.RMSPropOptimizer))
 
   def testBuildMomentumOptimizer(self):
@@ -137,10 +132,9 @@ class OptimizerBuilderTest(tf.test.TestCase):
       }
       use_moving_average: false
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(isinstance(optimizer, tf.train.MomentumOptimizer))
 
   def testBuildAdamOptimizer(self):
@@ -154,10 +148,9 @@ class OptimizerBuilderTest(tf.test.TestCase):
       }
       use_moving_average: false
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(isinstance(optimizer, tf.train.AdamOptimizer))
 
   def testBuildMovingAverageOptimizer(self):
@@ -171,10 +164,9 @@ class OptimizerBuilderTest(tf.test.TestCase):
       }
       use_moving_average: True
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(
         isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
 
@@ -190,10 +182,9 @@ class OptimizerBuilderTest(tf.test.TestCase):
       use_moving_average: True
       moving_average_decay: 0.2
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
-    optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
+    optimizer, _ = optimizer_builder.build(optimizer_proto)
     self.assertTrue(
         isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
     # TODO: Find a way to not depend on the private members.
@@ -202,11 +193,10 @@ class OptimizerBuilderTest(tf.test.TestCase):
   def testBuildEmptyOptimizer(self):
     optimizer_text_proto = """
     """
-    global_summaries = set([])
     optimizer_proto = optimizer_pb2.Optimizer()
     text_format.Merge(optimizer_text_proto, optimizer_proto)
     with self.assertRaises(ValueError):
-      optimizer_builder.build(optimizer_proto, global_summaries)
+      optimizer_builder.build(optimizer_proto)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index 9263925e..b9ef09c0 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -83,6 +83,7 @@ PREPROCESSING_FUNCTION_MAP = {
     'random_jitter_boxes': preprocessor.random_jitter_boxes,
     'random_crop_to_aspect_ratio': preprocessor.random_crop_to_aspect_ratio,
     'random_black_patches': preprocessor.random_black_patches,
+    'rgb_to_gray': preprocessor.rgb_to_gray,
     'scale_boxes_to_pixel_coordinates': (
         preprocessor.scale_boxes_to_pixel_coordinates),
     'subtract_channel_mean': preprocessor.subtract_channel_mean,
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index cc2789aa..f03e3b28 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -379,6 +379,16 @@ class PreprocessorBuilderTest(tf.test.TestCase):
                             'new_width': 100,
                             'method': tf.image.ResizeMethod.BICUBIC})
 
+  def test_build_rgb_to_gray(self):
+    preprocessor_text_proto = """
+    rgb_to_gray {}
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Merge(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.rgb_to_gray)
+    self.assertEqual(args, {})
+
   def test_build_subtract_channel_mean(self):
     preprocessor_text_proto = """
     subtract_channel_mean {
diff --git a/research/object_detection/core/BUILD b/research/object_detection/core/BUILD
index af8e33ef..c1fbc308 100644
--- a/research/object_detection/core/BUILD
+++ b/research/object_detection/core/BUILD
@@ -123,6 +123,7 @@ py_library(
         "matcher.py",
     ],
     deps = [
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
@@ -160,12 +161,20 @@ py_library(
         ":box_list",
         ":box_list_ops",
         ":keypoint_ops",
+        ":preprocessor_cache",
         ":standard_fields",
         "//tensorflow",
         "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
+py_library(
+    name = "preprocessor_cache",
+    srcs = [
+        "preprocessor_cache.py",
+    ],
+)
+
 py_test(
     name = "preprocessor_test",
     srcs = [
@@ -173,6 +182,7 @@ py_test(
     ],
     deps = [
         ":preprocessor",
+        ":preprocessor_cache",
         "//tensorflow",
     ],
 )
diff --git a/research/object_detection/core/__init__.py b/research/object_detection/core/__init__.py
index e69de29b..8b137891 100644
--- a/research/object_detection/core/__init__.py
+++ b/research/object_detection/core/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 36059702..296b73ce 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -102,7 +102,7 @@ class BoxPredictor(object):
         return self._predict(image_features, num_predictions_per_location,
                              **params)
     return self._predict(image_features, num_predictions_per_location,
-                           **params)
+                         **params)
 
   # TODO: num_predictions_per_location could be moved to constructor.
   # This is currently only used by ConvolutionalBoxPredictor.
@@ -582,7 +582,8 @@ class ConvolutionalBoxPredictor(BoxPredictor):
                kernel_size,
                box_code_size,
                apply_sigmoid_to_scores=False,
-               class_prediction_bias_init=0.0):
+               class_prediction_bias_init=0.0,
+               use_depthwise=False):
     """Constructor.
 
     Args:
@@ -611,6 +612,8 @@ class ConvolutionalBoxPredictor(BoxPredictor):
         class_predictions.
       class_prediction_bias_init: constant value to initialize bias of the last
         conv2d layer before class prediction.
+      use_depthwise: Whether to use depthwise convolutions for prediction
+        steps. Default is False.
 
     Raises:
       ValueError: if min_depth > max_depth.
@@ -628,6 +631,7 @@ class ConvolutionalBoxPredictor(BoxPredictor):
     self._dropout_keep_prob = dropout_keep_prob
     self._apply_sigmoid_to_scores = apply_sigmoid_to_scores
     self._class_prediction_bias_init = class_prediction_bias_init
+    self._use_depthwise = use_depthwise
 
   def _predict(self, image_features, num_predictions_per_location_list):
     """Computes encoded object locations and corresponding confidences.
@@ -683,17 +687,38 @@ class ConvolutionalBoxPredictor(BoxPredictor):
                   net, depth, [1, 1], scope='Conv2d_%d_1x1_%d' % (i, depth))
           with slim.arg_scope([slim.conv2d], activation_fn=None,
                               normalizer_fn=None, normalizer_params=None):
-            box_encodings = slim.conv2d(
-                net, num_predictions_per_location * self._box_code_size,
-                [self._kernel_size, self._kernel_size],
-                scope='BoxEncodingPredictor')
+            if self._use_depthwise:
+              box_encodings = slim.separable_conv2d(
+                  net, None, [self._kernel_size, self._kernel_size],
+                  padding='SAME', depth_multiplier=1, stride=1,
+                  rate=1, scope='BoxEncodingPredictor_depthwise')
+              box_encodings = slim.conv2d(
+                  box_encodings,
+                  num_predictions_per_location * self._box_code_size, [1, 1],
+                  scope='BoxEncodingPredictor')
+            else:
+              box_encodings = slim.conv2d(
+                  net, num_predictions_per_location * self._box_code_size,
+                  [self._kernel_size, self._kernel_size],
+                  scope='BoxEncodingPredictor')
             if self._use_dropout:
               net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
-            class_predictions_with_background = slim.conv2d(
-                net, num_predictions_per_location * num_class_slots,
-                [self._kernel_size, self._kernel_size], scope='ClassPredictor',
-                biases_initializer=tf.constant_initializer(
-                    self._class_prediction_bias_init))
+            if self._use_depthwise:
+              class_predictions_with_background = slim.separable_conv2d(
+                  net, None, [self._kernel_size, self._kernel_size],
+                  padding='SAME', depth_multiplier=1, stride=1,
+                  rate=1, scope='ClassPredictor_depthwise')
+              class_predictions_with_background = slim.conv2d(
+                  class_predictions_with_background,
+                  num_predictions_per_location * num_class_slots,
+                  [1, 1], scope='ClassPredictor')
+            else:
+              class_predictions_with_background = slim.conv2d(
+                  net, num_predictions_per_location * num_class_slots,
+                  [self._kernel_size, self._kernel_size],
+                  scope='ClassPredictor',
+                  biases_initializer=tf.constant_initializer(
+                      self._class_prediction_bias_init))
             if self._apply_sigmoid_to_scores:
               class_predictions_with_background = tf.sigmoid(
                   class_predictions_with_background)
@@ -729,7 +754,8 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
   Defines the box predictor as defined in
   https://arxiv.org/abs/1708.02002. This class differs from
   ConvolutionalBoxPredictor in that it shares weights and biases while
-  predicting from different feature maps.
+  predicting from different feature maps.  Separate multi-layer towers are
+  constructed for the box encoding and class predictors respectively.
   """
 
   def __init__(self,
@@ -811,22 +837,35 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
       with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
                              reuse=tf.AUTO_REUSE):
         num_class_slots = self.num_classes + 1
-        net = image_feature
+        box_encodings_net = image_feature
+        class_predictions_net = image_feature
         with slim.arg_scope(self._conv_hyperparams):
           for i in range(self._num_layers_before_predictor):
-            net = slim.conv2d(net,
-                              self._depth,
-                              [self._kernel_size, self._kernel_size],
-                              stride=1,
-                              padding='SAME',
-                              scope='conv2d_{}'.format(i))
+            box_encodings_net = slim.conv2d(
+                box_encodings_net,
+                self._depth,
+                [self._kernel_size, self._kernel_size],
+                stride=1,
+                padding='SAME',
+                scope='BoxEncodingPredictionTower/conv2d_{}'.format(i))
           box_encodings = slim.conv2d(
-              net, num_predictions_per_location * self._box_code_size,
+              box_encodings_net,
+              num_predictions_per_location * self._box_code_size,
               [self._kernel_size, self._kernel_size],
               activation_fn=None, stride=1, padding='SAME',
               scope='BoxEncodingPredictor')
+
+          for i in range(self._num_layers_before_predictor):
+            class_predictions_net = slim.conv2d(
+                class_predictions_net,
+                self._depth,
+                [self._kernel_size, self._kernel_size],
+                stride=1,
+                padding='SAME',
+                scope='ClassPredictionTower/conv2d_{}'.format(i))
           class_predictions_with_background = slim.conv2d(
-              net, num_predictions_per_location * num_class_slots,
+              class_predictions_net,
+              num_predictions_per_location * num_class_slots,
               [self._kernel_size, self._kernel_size],
               activation_fn=None, stride=1, padding='SAME',
               biases_initializer=tf.constant_initializer(
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index e076f441..39c52993 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -316,9 +316,69 @@ class ConvolutionalBoxPredictorTest(test_case.TestCase):
            [tf.shape(box_encodings), tf.shape(objectness_predictions)],
            feed_dict={image_features:
                       np.random.rand(4, resolution, resolution, 64)})
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
       self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
       self.assertAllEqual(objectness_predictions_shape,
                           [4, expected_num_anchors, 1])
+    expected_variable_set = set([
+        'BoxPredictor/Conv2d_0_1x1_32/biases',
+        'BoxPredictor/Conv2d_0_1x1_32/weights',
+        'BoxPredictor/BoxEncodingPredictor/biases',
+        'BoxPredictor/BoxEncodingPredictor/weights',
+        'BoxPredictor/ClassPredictor/biases',
+        'BoxPredictor/ClassPredictor/weights'])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_use_depthwise_convolution(self):
+    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
+    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+        is_training=False,
+        num_classes=0,
+        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+        min_depth=0,
+        max_depth=32,
+        num_layers_before_predictor=1,
+        dropout_keep_prob=0.8,
+        kernel_size=1,
+        box_code_size=4,
+        use_dropout=True,
+        use_depthwise=True
+    )
+    box_predictions = conv_box_predictor.predict(
+        [image_features], num_predictions_per_location=[5],
+        scope='BoxPredictor')
+    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+    objectness_predictions = box_predictions[
+        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    init_op = tf.global_variables_initializer()
+
+    resolution = 32
+    expected_num_anchors = resolution*resolution*5
+    with self.test_session() as sess:
+      sess.run(init_op)
+      (box_encodings_shape,
+       objectness_predictions_shape) = sess.run(
+           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           feed_dict={image_features:
+                      np.random.rand(4, resolution, resolution, 64)})
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
+    self.assertAllEqual(objectness_predictions_shape,
+                        [4, expected_num_anchors, 1])
+    expected_variable_set = set([
+        'BoxPredictor/Conv2d_0_1x1_32/biases',
+        'BoxPredictor/Conv2d_0_1x1_32/weights',
+        'BoxPredictor/BoxEncodingPredictor_depthwise/biases',
+        'BoxPredictor/BoxEncodingPredictor_depthwise/depthwise_weights',
+        'BoxPredictor/BoxEncodingPredictor/biases',
+        'BoxPredictor/BoxEncodingPredictor/weights',
+        'BoxPredictor/ClassPredictor_depthwise/biases',
+        'BoxPredictor/ClassPredictor_depthwise/depthwise_weights',
+        'BoxPredictor/ClassPredictor/biases',
+        'BoxPredictor/ClassPredictor/weights'])
+    self.assertEqual(expected_variable_set, actual_variable_set)
 
 
 class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
@@ -440,14 +500,26 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
 
     with self.test_session(graph=tf.Graph()):
       graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
-               tf.random_uniform([4, 32, 32, 3], dtype=tf.float32))
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
       actual_variable_set = set(
           [var.op.name for var in tf.trainable_variables()])
     expected_variable_set = set([
-        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_0/weights',
-        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_0/biases',
-        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_1/weights',
-        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_1/biases',
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictionTower/conv2d_1/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/biases'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
          'BoxEncodingPredictor/weights'),
         ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
@@ -489,6 +561,5 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
       self.assertAllEqual(objectness_predictions_shape,
                           [4, expected_num_anchors, 1])
 
-
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index ddded899..4c0a9c81 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -36,6 +36,8 @@ from abc import abstractmethod
 
 import tensorflow as tf
 
+from object_detection.utils import ops
+
 
 class Match(object):
   """Class to store results from the matcher.
@@ -44,7 +46,7 @@ class Match(object):
   convenient methods to query the matching results.
   """
 
-  def __init__(self, match_results):
+  def __init__(self, match_results, use_matmul_gather=False):
     """Constructs a Match object.
 
     Args:
@@ -52,6 +54,8 @@ class Match(object):
         meaning that column i is matched with row match_results[i].
         (2) match_results[i]=-1, meaning that column i is not matched.
         (3) match_results[i]=-2, meaning that column i is ignored.
+      use_matmul_gather: Use matrix multiplication based gather instead of
+        standard tf.gather. (Default: False).
 
     Raises:
       ValueError: if match_results does not have rank 1 or is not an
@@ -63,6 +67,9 @@ class Match(object):
       raise ValueError('match_results should be an int32 or int64 scalar '
                        'tensor')
     self._match_results = match_results
+    self._gather_op = tf.gather
+    if use_matmul_gather:
+      self._gather_op = ops.matmul_gather_on_zeroth_axis
 
   @property
   def match_results(self):
@@ -163,7 +170,7 @@ class Match(object):
       row_indices: int32 tensor of shape [K] with row indices.
     """
     return self._reshape_and_cast(
-        tf.gather(self._match_results, self.matched_column_indices()))
+        self._gather_op(self._match_results, self.matched_column_indices()))
 
   def _reshape_and_cast(self, t):
     return tf.cast(tf.reshape(t, [-1]), tf.int32)
@@ -193,7 +200,7 @@ class Match(object):
     input_tensor = tf.concat([tf.stack([ignored_value, unmatched_value]),
                               input_tensor], axis=0)
     gather_indices = tf.maximum(self.match_results + 2, 0)
-    gathered_tensor = tf.gather(input_tensor, gather_indices)
+    gathered_tensor = self._gather_op(input_tensor, gather_indices)
     return gathered_tensor
 
 
@@ -202,6 +209,16 @@ class Matcher(object):
   """
   __metaclass__ = ABCMeta
 
+  def __init__(self, use_matmul_gather=False):
+    """Constructs a Matcher.
+
+    Args:
+      use_matmul_gather: Force constructed match objects to use matrix
+        multiplication based gather instead of standard tf.gather.
+        (Default: False).
+    """
+    self._use_matmul_gather = use_matmul_gather
+
   def match(self, similarity_matrix, scope=None, **params):
     """Computes matches among row and column indices and returns the result.
 
@@ -219,7 +236,8 @@ class Matcher(object):
       A Match object with the results of matching.
     """
     with tf.name_scope(scope, 'Match', [similarity_matrix, params]) as scope:
-      return Match(self._match(similarity_matrix, **params))
+      return Match(self._match(similarity_matrix, **params),
+                   self._use_matmul_gather)
 
   @abstractmethod
   def _match(self, similarity_matrix, **params):
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index db6158cd..05607834 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -172,5 +172,21 @@ class MatchTest(tf.test.TestCase):
       gathered_tensor_out = gathered_tensor.eval()
     self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
 
+  def test_multidimensional_gather_based_on_match_with_matmul_gather_op(self):
+    match_results = tf.constant([1, -1, -2])
+    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
+                               dtype=tf.float32)
+    expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]
+    match = matcher.Match(match_results, use_matmul_gather=True)
+    gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                  unmatched_value=tf.zeros(4),
+                                                  ignored_value=tf.zeros(4))
+    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    with self.test_session() as sess:
+      self.assertTrue(
+          all([op.name is not 'Gather' for op in sess.graph.get_operations()]))
+      gathered_tensor_out = gathered_tensor.eval()
+    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index 7182bbe4..852586aa 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -236,7 +236,8 @@ class DetectionModel(object):
                           groundtruth_boxes_list,
                           groundtruth_classes_list,
                           groundtruth_masks_list=None,
-                          groundtruth_keypoints_list=None):
+                          groundtruth_keypoints_list=None,
+                          groundtruth_weights_list=None):
     """Provide groundtruth tensors.
 
     Args:
@@ -257,10 +258,15 @@ class DetectionModel(object):
         shape [num_boxes, num_keypoints, 2] containing keypoints.
         Keypoints are assumed to be provided in normalized coordinates and
         missing keypoints should be encoded as NaN.
+      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape
+        [num_boxes] containing weights for groundtruth boxes.
     """
     self._groundtruth_lists[fields.BoxListFields.boxes] = groundtruth_boxes_list
     self._groundtruth_lists[
         fields.BoxListFields.classes] = groundtruth_classes_list
+    if groundtruth_weights_list:
+      self._groundtruth_lists[fields.BoxListFields.
+                              weights] = groundtruth_weights_list
     if groundtruth_masks_list:
       self._groundtruth_lists[
           fields.BoxListFields.masks] = groundtruth_masks_list
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 0b671b3d..a0bcb422 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -35,6 +35,27 @@ in each row there is a box with [ymin xmin ymax xmax].
 Boxes are in normalized coordinates meaning
 their coordinate values range in [0, 1]
 
+To preprocess multiple images with the same operations in cases where
+nondeterministic operations are used, a preprocessor_cache.PreprocessorCache
+object can be passed into the preprocess function or individual operations.
+All nondeterministic operations except random_jitter_boxes support caching.
+E.g.
+Let tensor_dict{1,2,3,4,5} be copies of the same inputs.
+Let preprocess_options contain nondeterministic operation(s) excluding
+random_jitter_boxes.
+
+cache1 = preprocessor_cache.PreprocessorCache()
+cache2 = preprocessor_cache.PreprocessorCache()
+a = preprocess(tensor_dict1, preprocess_options, preprocess_vars_cache=cache1)
+b = preprocess(tensor_dict2, preprocess_options, preprocess_vars_cache=cache1)
+c = preprocess(tensor_dict3, preprocess_options, preprocess_vars_cache=cache2)
+d = preprocess(tensor_dict4, preprocess_options, preprocess_vars_cache=cache2)
+e = preprocess(tensor_dict5, preprocess_options)
+
+Then correspondings tensors of object pairs (a,b) and (c,d)
+are guaranteed to be equal element-wise, but the equality of any other object
+pair cannot be determined.
+
 Important Note: In tensor_dict, images is a rank 4 tensor, but preprocessing
 functions receive a rank 3 tensor for processing the image. Thus, inside the
 preprocess function we squeeze the image to become a rank 3 tensor and then
@@ -42,6 +63,8 @@ we pass it to the functions. At the end of the preprocess we expand the image
 back to rank 4.
 """
 
+import functools
+import inspect
 import sys
 import tensorflow as tf
 
@@ -50,45 +73,79 @@ from tensorflow.python.ops import control_flow_ops
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import keypoint_ops
+from object_detection.core import preprocessor_cache
 from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
 
 
-def _apply_with_random_selector(x, func, num_cases):
+def _apply_with_random_selector(x,
+                                func,
+                                num_cases,
+                                preprocess_vars_cache=None,
+                                key=''):
   """Computes func(x, sel), with sel sampled from [0...num_cases-1].
 
+  If both preprocess_vars_cache AND key are the same between two calls, sel will
+  be the same value in both calls.
+
   Args:
     x: input Tensor.
     func: Python function to apply.
     num_cases: Python int32, number of cases to sample sel from.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+    key: variable identifier for preprocess_vars_cache.
 
   Returns:
     The result of func(x, sel), where func receives the value of the
     selector as a python integer, but sel is sampled dynamically.
   """
-  rand_sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)
+  generator_func = functools.partial(
+      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)
+  rand_sel = _get_or_create_preprocess_rand_vars(
+      generator_func, preprocessor_cache.PreprocessorCache.SELECTOR,
+      preprocess_vars_cache, key)
+
   # Pass the real x only to one of the func calls.
   return control_flow_ops.merge([func(
       control_flow_ops.switch(x, tf.equal(rand_sel, case))[1], case)
                                  for case in range(num_cases)])[0]
 
 
-def _apply_with_random_selector_tuples(x, func, num_cases):
+def _apply_with_random_selector_tuples(x,
+                                       func,
+                                       num_cases,
+                                       preprocess_vars_cache=None,
+                                       key=''):
   """Computes func(x, sel), with sel sampled from [0...num_cases-1].
 
+  If both preprocess_vars_cache AND key are the same between two calls, sel will
+  be the same value in both calls.
+
   Args:
     x: A tuple of input tensors.
     func: Python function to apply.
     num_cases: Python int32, number of cases to sample sel from.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+    key: variable identifier for preprocess_vars_cache.
 
   Returns:
     The result of func(x, sel), where func receives the value of the
     selector as a python integer, but sel is sampled dynamically.
   """
   num_inputs = len(x)
-  rand_sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)
-  # Pass the real x only to one of the func calls.
+  generator_func = functools.partial(
+      tf.random_uniform, [], maxval=num_cases, dtype=tf.int32)
+  rand_sel = _get_or_create_preprocess_rand_vars(
+      generator_func, preprocessor_cache.PreprocessorCache.SELECTOR_TUPLES,
+      preprocess_vars_cache, key)
 
+  # Pass the real x only to one of the func calls.
   tuples = [list() for t in x]
   for case in range(num_cases):
     new_x = [control_flow_ops.switch(t, tf.equal(rand_sel, case))[1] for t in x]
@@ -101,6 +158,37 @@ def _apply_with_random_selector_tuples(x, func, num_cases):
   return tuple(tuples)
 
 
+def _get_or_create_preprocess_rand_vars(generator_func,
+                                        function_id,
+                                        preprocess_vars_cache,
+                                        key=''):
+  """Returns a tensor stored in preprocess_vars_cache or using generator_func.
+
+  If the tensor was previously generated and appears in the PreprocessorCache,
+  the previously generated tensor will be returned. Otherwise, a new tensor
+  is generated using generator_func and stored in the cache.
+
+  Args:
+    generator_func: A 0-argument function that generates a tensor.
+    function_id: identifier for the preprocessing function used.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
+    key: identifier for the variable stored.
+  Returns:
+    The generated tensor.
+  """
+  if preprocess_vars_cache is not None:
+    var = preprocess_vars_cache.get(function_id, key)
+    if var is None:
+      var = generator_func()
+      preprocess_vars_cache.update(function_id, key, var)
+  else:
+    var = generator_func()
+  return var
+
+
 def _random_integer(minval, maxval, seed):
   """Returns a random 0-D tensor between minval and maxval.
 
@@ -116,6 +204,40 @@ def _random_integer(minval, maxval, seed):
       [], minval=minval, maxval=maxval, dtype=tf.int32, seed=seed)
 
 
+# TODO: This method is needed because the current
+# tf.image.rgb_to_grayscale method does not support quantization. Replace with
+# tf.image.rgb_to_grayscale after quantization support is added.
+def _rgb_to_grayscale(images, name=None):
+  """Converts one or more images from RGB to Grayscale.
+
+  Outputs a tensor of the same `DType` and rank as `images`.  The size of the
+  last dimension of the output is 1, containing the Grayscale value of the
+  pixels.
+
+  Args:
+    images: The RGB tensor to convert. Last dimension must have size 3 and
+      should contain RGB values.
+    name: A name for the operation (optional).
+
+  Returns:
+    The converted grayscale image(s).
+  """
+  with tf.name_scope(name, 'rgb_to_grayscale', [images]) as name:
+    images = tf.convert_to_tensor(images, name='images')
+    # Remember original dtype to so we can convert back if needed
+    orig_dtype = images.dtype
+    flt_image = tf.image.convert_image_dtype(images, tf.float32)
+
+    # Reference for converting between RGB and grayscale.
+    # https://en.wikipedia.org/wiki/Luma_%28video%29
+    rgb_weights = [0.2989, 0.5870, 0.1140]
+    rank_1 = tf.expand_dims(tf.rank(images) - 1, 0)
+    gray_float = tf.reduce_sum(
+        flt_image * rgb_weights, rank_1, keepdims=True)
+    gray_float.set_shape(images.get_shape()[:-1].concatenate([1]))
+    return tf.image.convert_image_dtype(gray_float, orig_dtype, name=name)
+
+
 def normalize_image(image, original_minval, original_maxval, target_minval,
                     target_maxval):
   """Normalizes pixel values in the image.
@@ -313,7 +435,8 @@ def random_horizontal_flip(image,
                            masks=None,
                            keypoints=None,
                            keypoint_flip_permutation=None,
-                           seed=None):
+                           seed=None,
+                           preprocess_vars_cache=None):
   """Randomly flips the image and detections horizontally.
 
   The probability of flipping the image is 50%.
@@ -334,6 +457,10 @@ def random_horizontal_flip(image,
     keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip
                                permutation.
     seed: random seed
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
@@ -365,7 +492,12 @@ def random_horizontal_flip(image,
   with tf.name_scope('RandomHorizontalFlip', values=[image, boxes]):
     result = []
     # random variable defining whether to do flip or not
-    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_a_flip_random = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.HORIZONTAL_FLIP,
+        preprocess_vars_cache)
+    do_a_flip_random = tf.greater(do_a_flip_random, 0.5)
 
     # flip image
     image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)
@@ -400,7 +532,8 @@ def random_vertical_flip(image,
                          masks=None,
                          keypoints=None,
                          keypoint_flip_permutation=None,
-                         seed=None):
+                         seed=None,
+                         preprocess_vars_cache=None):
   """Randomly flips the image and detections vertically.
 
   The probability of flipping the image is 50%.
@@ -421,6 +554,10 @@ def random_vertical_flip(image,
     keypoint_flip_permutation: rank 1 int32 tensor containing the keypoint flip
                                permutation.
     seed: random seed
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
@@ -452,7 +589,11 @@ def random_vertical_flip(image,
   with tf.name_scope('RandomVerticalFlip', values=[image, boxes]):
     result = []
     # random variable defining whether to do flip or not
-    do_a_flip_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_a_flip_random = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.VERTICAL_FLIP,
+        preprocess_vars_cache)
+    do_a_flip_random = tf.greater(do_a_flip_random, 0.5)
 
     # flip image
     image = tf.cond(do_a_flip_random, lambda: _flip_image(image), lambda: image)
@@ -486,7 +627,8 @@ def random_rotation90(image,
                       boxes=None,
                       masks=None,
                       keypoints=None,
-                      seed=None):
+                      seed=None,
+                      preprocess_vars_cache=None):
   """Randomly rotates the image and detections 90 degrees counter-clockwise.
 
   The probability of rotating the image is 50%. This can be combined with
@@ -508,6 +650,10 @@ def random_rotation90(image,
                [num_instances, num_keypoints, 2]. The keypoints are in y-x
                normalized coordinates.
     seed: random seed
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
@@ -533,7 +679,11 @@ def random_rotation90(image,
     result = []
 
     # random variable defining whether to rotate by 90 degrees or not
-    do_a_rot90_random = tf.greater(tf.random_uniform([], seed=seed), 0.5)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_a_rot90_random = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.ROTATION90,
+        preprocess_vars_cache)
+    do_a_rot90_random = tf.greater(do_a_rot90_random, 0.5)
 
     # flip image
     image = tf.cond(do_a_rot90_random, lambda: _rot90_image(image),
@@ -563,7 +713,11 @@ def random_rotation90(image,
     return tuple(result)
 
 
-def random_pixel_value_scale(image, minval=0.9, maxval=1.1, seed=None):
+def random_pixel_value_scale(image,
+                             minval=0.9,
+                             maxval=1.1,
+                             seed=None,
+                             preprocess_vars_cache=None):
   """Scales each value in the pixels of the image.
 
      This function scales each pixel independent of the other ones.
@@ -576,17 +730,24 @@ def random_pixel_value_scale(image, minval=0.9, maxval=1.1, seed=None):
     minval: lower ratio of scaling pixel values.
     maxval: upper ratio of scaling pixel values.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
   """
   with tf.name_scope('RandomPixelValueScale', values=[image]):
-    color_coef = tf.random_uniform(
-        tf.shape(image),
-        minval=minval,
-        maxval=maxval,
-        dtype=tf.float32,
-        seed=seed)
+    generator_func = functools.partial(
+        tf.random_uniform, tf.shape(image),
+        minval=minval, maxval=maxval,
+        dtype=tf.float32, seed=seed)
+    color_coef = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.PIXEL_VALUE_SCALE,
+        preprocess_vars_cache)
+
     image = tf.multiply(image, color_coef)
     image = tf.clip_by_value(image, 0.0, 1.0)
 
@@ -597,7 +758,8 @@ def random_image_scale(image,
                        masks=None,
                        min_scale_ratio=0.5,
                        max_scale_ratio=2.0,
-                       seed=None):
+                       seed=None,
+                       preprocess_vars_cache=None):
   """Scales the image size.
 
   Args:
@@ -608,6 +770,10 @@ def random_image_scale(image,
     min_scale_ratio: minimum scaling ratio.
     max_scale_ratio: maximum scaling ratio.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -619,10 +785,14 @@ def random_image_scale(image,
     image_shape = tf.shape(image)
     image_height = image_shape[0]
     image_width = image_shape[1]
-    size_coef = tf.random_uniform([],
-                                  minval=min_scale_ratio,
-                                  maxval=max_scale_ratio,
-                                  dtype=tf.float32, seed=seed)
+    generator_func = functools.partial(
+        tf.random_uniform, [],
+        minval=min_scale_ratio, maxval=max_scale_ratio,
+        dtype=tf.float32, seed=seed)
+    size_coef = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.IMAGE_SCALE,
+        preprocess_vars_cache)
+
     image_newysize = tf.to_int32(
         tf.multiply(tf.to_float(image_height), size_coef))
     image_newxsize = tf.to_int32(
@@ -637,7 +807,10 @@ def random_image_scale(image,
     return tuple(result)
 
 
-def random_rgb_to_gray(image, probability=0.1, seed=None):
+def random_rgb_to_gray(image,
+                       probability=0.1,
+                       seed=None,
+                       preprocess_vars_cache=None):
   """Changes the image from RGB to Grayscale with the given probability.
 
   Args:
@@ -646,18 +819,25 @@ def random_rgb_to_gray(image, probability=0.1, seed=None):
     probability: the probability of returning a grayscale image.
             The probability should be a number between [0, 1].
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
   """
   def _image_to_gray(image):
-    image_gray1 = tf.image.rgb_to_grayscale(image)
+    image_gray1 = _rgb_to_grayscale(image)
     image_gray3 = tf.image.grayscale_to_rgb(image_gray1)
     return image_gray3
 
   with tf.name_scope('RandomRGBtoGray', values=[image]):
-    # random variable defining whether to do flip or not
-    do_gray_random = tf.random_uniform([], seed=seed)
+    # random variable defining whether to change to grayscale or not
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_gray_random = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.RGB_TO_GRAY,
+        preprocess_vars_cache)
 
     image = tf.cond(
         tf.greater(do_gray_random, probability), lambda: image,
@@ -666,7 +846,10 @@ def random_rgb_to_gray(image, probability=0.1, seed=None):
   return image
 
 
-def random_adjust_brightness(image, max_delta=0.2):
+def random_adjust_brightness(image,
+                             max_delta=0.2,
+                             seed=None,
+                             preprocess_vars_cache=None):
   """Randomly adjusts brightness.
 
   Makes sure the output image is still between 0 and 1.
@@ -675,18 +858,34 @@ def random_adjust_brightness(image, max_delta=0.2):
     image: rank 3 float32 tensor contains 1 image -> [height, width, channels]
            with pixel values varying between [0, 1].
     max_delta: how much to change the brightness. A value between [0, 1).
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
     boxes: boxes which is the same shape as input boxes.
   """
   with tf.name_scope('RandomAdjustBrightness', values=[image]):
-    image = tf.image.random_brightness(image, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
+                                       -max_delta, max_delta, seed=seed)
+    delta = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.ADJUST_BRIGHTNESS,
+        preprocess_vars_cache)
+
+    image = tf.image.adjust_brightness(image, delta)
     image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
     return image
 
 
-def random_adjust_contrast(image, min_delta=0.8, max_delta=1.25):
+def random_adjust_contrast(image,
+                           min_delta=0.8,
+                           max_delta=1.25,
+                           seed=None,
+                           preprocess_vars_cache=None):
   """Randomly adjusts contrast.
 
   Makes sure the output image is still between 0 and 1.
@@ -698,17 +897,31 @@ def random_adjust_contrast(image, min_delta=0.8, max_delta=1.25):
     max_delta: how much to change the contrast. Contrast will change with a
                value between min_delta and max_delta. This value will be
                multiplied to the current contrast of the image.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
   """
   with tf.name_scope('RandomAdjustContrast', values=[image]):
-    image = tf.image.random_contrast(image, min_delta, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
+                                       min_delta, max_delta, seed=seed)
+    contrast_factor = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.ADJUST_CONTRAST,
+        preprocess_vars_cache)
+    image = tf.image.adjust_contrast(image, contrast_factor)
     image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
     return image
 
 
-def random_adjust_hue(image, max_delta=0.02):
+def random_adjust_hue(image,
+                      max_delta=0.02,
+                      seed=None,
+                      preprocess_vars_cache=None):
   """Randomly adjusts hue.
 
   Makes sure the output image is still between 0 and 1.
@@ -717,17 +930,31 @@ def random_adjust_hue(image, max_delta=0.02):
     image: rank 3 float32 tensor contains 1 image -> [height, width, channels]
            with pixel values varying between [0, 1].
     max_delta: change hue randomly with a value between 0 and max_delta.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
   """
   with tf.name_scope('RandomAdjustHue', values=[image]):
-    image = tf.image.random_hue(image, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
+                                       -max_delta, max_delta, seed=seed)
+    delta = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.ADJUST_HUE,
+        preprocess_vars_cache)
+    image = tf.image.adjust_hue(image, delta)
     image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
     return image
 
 
-def random_adjust_saturation(image, min_delta=0.8, max_delta=1.25):
+def random_adjust_saturation(image,
+                             min_delta=0.8,
+                             max_delta=1.25,
+                             seed=None,
+                             preprocess_vars_cache=None):
   """Randomly adjusts saturation.
 
   Makes sure the output image is still between 0 and 1.
@@ -739,17 +966,28 @@ def random_adjust_saturation(image, min_delta=0.8, max_delta=1.25):
     max_delta: how much to change the saturation. Saturation will change with a
                value between min_delta and max_delta. This value will be
                multiplied to the current saturation of the image.
+    seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
   """
   with tf.name_scope('RandomAdjustSaturation', values=[image]):
-    image = tf.image.random_saturation(image, min_delta, max_delta)
+    generator_func = functools.partial(tf.random_uniform, [],
+                                       min_delta, max_delta, seed=seed)
+    saturation_factor = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.ADJUST_SATURATION,
+        preprocess_vars_cache)
+    image = tf.image.adjust_saturation(image, saturation_factor)
     image = tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)
     return image
 
 
-def random_distort_color(image, color_ordering=0):
+def random_distort_color(image, color_ordering=0, preprocess_vars_cache=None):
   """Randomly distorts color.
 
   Randomly distorts color using a combination of brightness, hue, contrast
@@ -759,6 +997,10 @@ def random_distort_color(image, color_ordering=0):
     image: rank 3 float32 tensor contains 1 image -> [height, width, channels]
            with pixel values varying between [0, 1].
     color_ordering: Python int, a type of distortion (valid values: 0, 1).
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same shape as input image.
@@ -768,20 +1010,34 @@ def random_distort_color(image, color_ordering=0):
   """
   with tf.name_scope('RandomDistortColor', values=[image]):
     if color_ordering == 0:
-      image = tf.image.random_brightness(image, max_delta=32. / 255.)
-      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
-      image = tf.image.random_hue(image, max_delta=0.2)
-      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
+      image = random_adjust_brightness(
+          image, max_delta=32. / 255.,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_saturation(
+          image, min_delta=0.5, max_delta=1.5,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_hue(
+          image, max_delta=0.2,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_contrast(
+          image, min_delta=0.5, max_delta=1.5,
+          preprocess_vars_cache=preprocess_vars_cache)
+
     elif color_ordering == 1:
-      image = tf.image.random_brightness(image, max_delta=32. / 255.)
-      image = tf.image.random_contrast(image, lower=0.5, upper=1.5)
-      image = tf.image.random_saturation(image, lower=0.5, upper=1.5)
-      image = tf.image.random_hue(image, max_delta=0.2)
+      image = random_adjust_brightness(
+          image, max_delta=32. / 255.,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_contrast(
+          image, min_delta=0.5, max_delta=1.5,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_saturation(
+          image, min_delta=0.5, max_delta=1.5,
+          preprocess_vars_cache=preprocess_vars_cache)
+      image = random_adjust_hue(
+          image, max_delta=0.2,
+          preprocess_vars_cache=preprocess_vars_cache)
     else:
       raise ValueError('color_ordering must be in {0, 1}')
-
-    # The random_* ops do not necessarily clamp.
-    image = tf.clip_by_value(image, 0.0, 1.0)
     return image
 
 
@@ -846,7 +1102,8 @@ def _strict_random_crop_image(image,
                               min_object_covered=1.0,
                               aspect_ratio_range=(0.75, 1.33),
                               area_range=(0.1, 1.0),
-                              overlap_thresh=0.3):
+                              overlap_thresh=0.3,
+                              preprocess_vars_cache=None):
   """Performs random crop.
 
   Note: boxes will be clipped to the crop. Keypoint coordinates that are
@@ -879,6 +1136,10 @@ def _strict_random_crop_image(image,
                 original image.
     overlap_thresh: minimum overlap thresh with new cropped
                     image to keep the box.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -901,7 +1162,8 @@ def _strict_random_crop_image(image,
         tf.clip_by_value(
             boxes, clip_value_min=0.0, clip_value_max=1.0), 1)
 
-    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(
+    generator_func = functools.partial(
+        tf.image.sample_distorted_bounding_box,
         image_shape,
         bounding_boxes=boxes_expanded,
         min_object_covered=min_object_covered,
@@ -910,6 +1172,13 @@ def _strict_random_crop_image(image,
         max_attempts=100,
         use_image_if_no_bounding_boxes=True)
 
+    # for ssd cropping, each value of min_object_covered has its own
+    # cached random variable
+    sample_distorted_bounding_box = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.STRICT_CROP_IMAGE,
+        preprocess_vars_cache, key=min_object_covered)
+
     im_box_begin, im_box_size, im_box = sample_distorted_bounding_box
 
     new_image = tf.slice(image, im_box_begin, im_box_size)
@@ -985,7 +1254,8 @@ def random_crop_image(image,
                       area_range=(0.1, 1.0),
                       overlap_thresh=0.3,
                       random_coef=0.0,
-                      seed=None):
+                      seed=None,
+                      preprocess_vars_cache=None):
   """Randomly crops the image.
 
   Given the input image and its bounding boxes, this op randomly
@@ -1030,6 +1300,10 @@ def random_crop_image(image,
                  cropped image, and if it is 1.0, we will always get the
                  original image.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: Image shape will be [new_height, new_width, channels].
@@ -1057,13 +1331,17 @@ def random_crop_image(image,
         min_object_covered=min_object_covered,
         aspect_ratio_range=aspect_ratio_range,
         area_range=area_range,
-        overlap_thresh=overlap_thresh)
+        overlap_thresh=overlap_thresh,
+        preprocess_vars_cache=preprocess_vars_cache)
 
   # avoids tf.cond to make faster RCNN training on borg. See b/140057645.
   if random_coef < sys.float_info.min:
     result = strict_random_crop_image_fn()
   else:
-    do_a_crop_random = tf.random_uniform([], seed=seed)
+    generator_func = functools.partial(tf.random_uniform, [], seed=seed)
+    do_a_crop_random = _get_or_create_preprocess_rand_vars(
+        generator_func, preprocessor_cache.PreprocessorCache.CROP_IMAGE,
+        preprocess_vars_cache)
     do_a_crop_random = tf.greater(do_a_crop_random, random_coef)
 
     outputs = [image, boxes, labels]
@@ -1085,7 +1363,8 @@ def random_pad_image(image,
                      min_image_size=None,
                      max_image_size=None,
                      pad_color=None,
-                     seed=None):
+                     seed=None,
+                     preprocess_vars_cache=None):
   """Randomly pads the image.
 
   This function randomly pads the image with zeros. The final size of the
@@ -1111,8 +1390,11 @@ def random_pad_image(image,
     pad_color: padding color. A rank 1 tensor of [3] with dtype=tf.float32.
                if set as None, it will be set to average color of the input
                image.
-
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: Image shape will be [new_height, new_width, channels].
@@ -1156,6 +1438,12 @@ def random_pad_image(image,
       lambda: _random_integer(0, target_width - image_width, seed),
       lambda: tf.constant(0, dtype=tf.int32))
 
+  gen_func = lambda: (target_height, target_width, offset_height, offset_width)
+  params = _get_or_create_preprocess_rand_vars(
+      gen_func, preprocessor_cache.PreprocessorCache.PAD_IMAGE,
+      preprocess_vars_cache)
+  target_height, target_width, offset_height, offset_width = params
+
   new_image = tf.image.pad_to_bounding_box(
       image,
       offset_height=offset_height,
@@ -1201,7 +1489,8 @@ def random_crop_pad_image(image,
                           min_padded_size_ratio=(1.0, 1.0),
                           max_padded_size_ratio=(2.0, 2.0),
                           pad_color=None,
-                          seed=None):
+                          seed=None,
+                          preprocess_vars_cache=None):
   """Randomly crops and pads the image.
 
   Given an input image and its bounding boxes, this op first randomly crops
@@ -1242,6 +1531,10 @@ def random_crop_pad_image(image,
                if set as None, it will be set to average color of the randomly
                cropped image.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     padded_image: padded image.
@@ -1264,7 +1557,8 @@ def random_crop_pad_image(image,
       area_range=area_range,
       overlap_thresh=overlap_thresh,
       random_coef=random_coef,
-      seed=seed)
+      seed=seed,
+      preprocess_vars_cache=preprocess_vars_cache)
 
   cropped_image, cropped_boxes, cropped_labels = result[:3]
 
@@ -1281,7 +1575,8 @@ def random_crop_pad_image(image,
       min_image_size=min_image_size,
       max_image_size=max_image_size,
       pad_color=pad_color,
-      seed=seed)
+      seed=seed,
+      preprocess_vars_cache=preprocess_vars_cache)
 
   cropped_padded_output = (padded_image, padded_boxes, cropped_labels)
 
@@ -1300,7 +1595,8 @@ def random_crop_to_aspect_ratio(image,
                                 keypoints=None,
                                 aspect_ratio=1.0,
                                 overlap_thresh=0.3,
-                                seed=None):
+                                seed=None,
+                                preprocess_vars_cache=None):
   """Randomly crops an image to the specified aspect ratio.
 
   Randomly crops the a portion of the image such that the crop is of the
@@ -1332,6 +1628,10 @@ def random_crop_to_aspect_ratio(image,
     overlap_thresh: minimum overlap thresh with new cropped
                     image to keep the box.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -1375,6 +1675,13 @@ def random_crop_to_aspect_ratio(image,
     # offset_height is randomly chosen from [0, offset_height - target_height)
     offset_height = _random_integer(0, orig_height - target_height + 1, seed)
     offset_width = _random_integer(0, orig_width - target_width + 1, seed)
+
+    generator_func = lambda: (offset_height, offset_width)
+    offset_height, offset_width = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.CROP_TO_ASPECT_RATIO,
+        preprocess_vars_cache)
+
     new_image = tf.image.crop_to_bounding_box(
         image, offset_height, offset_width, target_height, target_width)
 
@@ -1437,7 +1744,8 @@ def random_pad_to_aspect_ratio(image,
                                aspect_ratio=1.0,
                                min_padded_size_ratio=(1.0, 1.0),
                                max_padded_size_ratio=(2.0, 2.0),
-                               seed=None):
+                               seed=None,
+                               preprocess_vars_cache=None):
   """Randomly zero pads an image to the specified aspect ratio.
 
   Pads the image so that the resulting image will have the specified aspect
@@ -1465,6 +1773,10 @@ def random_pad_to_aspect_ratio(image,
     max_padded_size_ratio: max ratio of padded image height and width to the
                            input image's height and width.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -1511,7 +1823,13 @@ def random_pad_to_aspect_ratio(image,
 
     min_scale = tf.maximum(min_height / target_height, min_width / target_width)
     max_scale = tf.minimum(max_height / target_height, max_width / target_width)
-    scale = tf.random_uniform([], min_scale, max_scale, seed=seed)
+
+    generator_func = functools.partial(tf.random_uniform, [],
+                                       min_scale, max_scale, seed=seed)
+    scale = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.PAD_TO_ASPECT_RATIO,
+        preprocess_vars_cache)
 
     target_height = scale * target_height
     target_width = scale * target_width
@@ -1550,7 +1868,8 @@ def random_black_patches(image,
                          max_black_patches=10,
                          probability=0.5,
                          size_to_image_ratio=0.1,
-                         random_seed=None):
+                         random_seed=None,
+                         preprocess_vars_cache=None):
   """Randomly adds some black patches to the image.
 
   This op adds up to max_black_patches square black patches of a fixed size
@@ -1567,15 +1886,20 @@ def random_black_patches(image,
                          box_size = size_to_image_ratio *
                                     min(image_width, image_height)
     random_seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image
   """
-  def add_black_patch_to_image(image):
+  def add_black_patch_to_image(image, idx):
     """Function for adding one patch to the image.
 
     Args:
       image: image
+      idx: counter for number of patches that could have been added
 
     Returns:
       image with a randomly added black box
@@ -1587,10 +1911,19 @@ def random_black_patches(image,
         tf.multiply(
             tf.minimum(tf.to_float(image_height), tf.to_float(image_width)),
             size_to_image_ratio))
-    normalized_y_min = tf.random_uniform(
-        [], minval=0.0, maxval=(1.0 - size_to_image_ratio), seed=random_seed)
-    normalized_x_min = tf.random_uniform(
-        [], minval=0.0, maxval=(1.0 - size_to_image_ratio), seed=random_seed)
+
+    generator_func = functools.partial(tf.random_uniform, [], minval=0.0,
+                                       maxval=(1.0 - size_to_image_ratio),
+                                       seed=random_seed)
+    normalized_y_min = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,
+        preprocess_vars_cache, key=str(idx) + 'y')
+    normalized_x_min = _get_or_create_preprocess_rand_vars(
+        generator_func,
+        preprocessor_cache.PreprocessorCache.ADD_BLACK_PATCH,
+        preprocess_vars_cache, key=str(idx) + 'x')
+
     y_min = tf.to_int32(normalized_y_min * tf.to_float(image_height))
     x_min = tf.to_int32(normalized_x_min * tf.to_float(image_width))
     black_box = tf.ones([box_size, box_size, 3], dtype=tf.float32)
@@ -1600,13 +1933,17 @@ def random_black_patches(image,
     return image
 
   with tf.name_scope('RandomBlackPatchInImage', values=[image]):
-    for _ in range(max_black_patches):
-      random_prob = tf.random_uniform(
-          [], minval=0.0, maxval=1.0, dtype=tf.float32, seed=random_seed)
+    for idx in range(max_black_patches):
+      generator_func = functools.partial(tf.random_uniform, [],
+                                         minval=0.0, maxval=1.0,
+                                         dtype=tf.float32, seed=random_seed)
+      random_prob = _get_or_create_preprocess_rand_vars(
+          generator_func,
+          preprocessor_cache.PreprocessorCache.BLACK_PATCHES,
+          preprocess_vars_cache, key=idx)
       image = tf.cond(
           tf.greater(random_prob, probability), lambda: image,
-          lambda: add_black_patch_to_image(image))
-
+          functools.partial(add_black_patch_to_image, image=image, idx=idx))
     return image
 
 
@@ -1624,12 +1961,16 @@ def image_to_float(image):
     return image
 
 
-def random_resize_method(image, target_size):
+def random_resize_method(image, target_size, preprocess_vars_cache=None):
   """Uses a random resize method to resize the image to target size.
 
   Args:
     image: a rank 3 tensor.
     target_size: a list of [target_height, target_width]
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     resized image.
@@ -1638,7 +1979,9 @@ def random_resize_method(image, target_size):
   resized_image = _apply_with_random_selector(
       image,
       lambda x, method: tf.image.resize_images(x, target_size, method),
-      num_cases=4)
+      num_cases=4,
+      preprocess_vars_cache=preprocess_vars_cache,
+      key=preprocessor_cache.PreprocessorCache.RESIZE_METHOD)
 
   return resized_image
 
@@ -2000,7 +2343,7 @@ def rgb_to_gray(image):
   Returns:
     image: A single channel grayscale image -> [image, height, 1].
   """
-  return tf.image.rgb_to_grayscale(image)
+  return _rgb_to_grayscale(image)
 
 
 def ssd_random_crop(image,
@@ -2014,7 +2357,8 @@ def ssd_random_crop(image,
                     area_range=((0.1, 1.0),) * 7,
                     overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),
                     random_coef=(0.15,) * 7,
-                    seed=None):
+                    seed=None,
+                    preprocess_vars_cache=None):
   """Random crop preprocessing with default parameters as in SSD paper.
 
   Liu et al., SSD: Single shot multibox detector.
@@ -2048,6 +2392,10 @@ def ssd_random_crop(image,
                  cropped image, and if it is 1.0, we will always get the
                  original image.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -2100,14 +2448,17 @@ def ssd_random_crop(image,
         area_range=area_range[index],
         overlap_thresh=overlap_thresh[index],
         random_coef=random_coef[index],
-        seed=seed)
+        seed=seed,
+        preprocess_vars_cache=preprocess_vars_cache)
 
   result = _apply_with_random_selector_tuples(
       tuple(
           t for t in (image, boxes, labels, label_scores, masks, keypoints)
           if t is not None),
       random_crop_selector,
-      num_cases=len(min_object_covered))
+      num_cases=len(min_object_covered),
+      preprocess_vars_cache=preprocess_vars_cache,
+      key=preprocessor_cache.PreprocessorCache.SSD_CROP_SELECTOR_ID)
   return result
 
 
@@ -2123,7 +2474,8 @@ def ssd_random_crop_pad(image,
                         min_padded_size_ratio=((1.0, 1.0),) * 6,
                         max_padded_size_ratio=((2.0, 2.0),) * 6,
                         pad_color=(None,) * 6,
-                        seed=None):
+                        seed=None,
+                        preprocess_vars_cache=None):
   """Random crop preprocessing with default parameters as in SSD paper.
 
   Liu et al., SSD: Single shot multibox detector.
@@ -2159,6 +2511,10 @@ def ssd_random_crop_pad(image,
                if set as None, it will be set to average color of the randomly
                cropped image.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: Image shape will be [new_height, new_width, channels].
@@ -2188,12 +2544,15 @@ def ssd_random_crop_pad(image,
         min_padded_size_ratio=min_padded_size_ratio[index],
         max_padded_size_ratio=max_padded_size_ratio[index],
         pad_color=pad_color[index],
-        seed=seed)
+        seed=seed,
+        preprocess_vars_cache=preprocess_vars_cache)
 
   return _apply_with_random_selector_tuples(
       tuple(t for t in (image, boxes, labels, label_scores) if t is not None),
       random_crop_pad_selector,
-      num_cases=len(min_object_covered))
+      num_cases=len(min_object_covered),
+      preprocess_vars_cache=preprocess_vars_cache,
+      key=preprocessor_cache.PreprocessorCache.SSD_CROP_PAD_SELECTOR_ID)
 
 
 def ssd_random_crop_fixed_aspect_ratio(
@@ -2208,7 +2567,8 @@ def ssd_random_crop_fixed_aspect_ratio(
     area_range=((0.1, 1.0),) * 7,
     overlap_thresh=(0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0),
     random_coef=(0.15,) * 7,
-    seed=None):
+    seed=None,
+    preprocess_vars_cache=None):
   """Random crop preprocessing with default parameters as in SSD paper.
 
   Liu et al., SSD: Single shot multibox detector.
@@ -2245,6 +2605,10 @@ def ssd_random_crop_fixed_aspect_ratio(
                  cropped image, and if it is 1.0, we will always get the
                  original image.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -2263,7 +2627,8 @@ def ssd_random_crop_fixed_aspect_ratio(
 
   crop_result = ssd_random_crop(
       image, boxes, labels, label_scores, masks, keypoints, min_object_covered,
-      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed)
+      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,
+      preprocess_vars_cache)
   i = 3
   new_image, new_boxes, new_labels = crop_result[:i]
   new_label_scores = None
@@ -2285,7 +2650,8 @@ def ssd_random_crop_fixed_aspect_ratio(
       new_masks,
       new_keypoints,
       aspect_ratio=aspect_ratio,
-      seed=seed)
+      seed=seed,
+      preprocess_vars_cache=preprocess_vars_cache)
 
   return result
 
@@ -2305,7 +2671,8 @@ def ssd_random_crop_pad_fixed_aspect_ratio(
     random_coef=(0.15,) * 7,
     min_padded_size_ratio=(1.0, 1.0),
     max_padded_size_ratio=(2.0, 2.0),
-    seed=None):
+    seed=None,
+    preprocess_vars_cache=None):
   """Random crop and pad preprocessing with default parameters as in SSD paper.
 
   Liu et al., SSD: Single shot multibox detector.
@@ -2348,6 +2715,10 @@ def ssd_random_crop_pad_fixed_aspect_ratio(
     max_padded_size_ratio: max ratio of padded image height and width to the
                            input image's height and width.
     seed: random seed.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     image: image which is the same rank as input image.
@@ -2364,7 +2735,8 @@ def ssd_random_crop_pad_fixed_aspect_ratio(
   """
   crop_result = ssd_random_crop(
       image, boxes, labels, label_scores, masks, keypoints, min_object_covered,
-      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed)
+      aspect_ratio_range, area_range, overlap_thresh, random_coef, seed,
+      preprocess_vars_cache)
   i = 3
   new_image, new_boxes, new_labels = crop_result[:i]
   new_label_scores = None
@@ -2386,7 +2758,8 @@ def ssd_random_crop_pad_fixed_aspect_ratio(
       aspect_ratio=aspect_ratio,
       min_padded_size_ratio=min_padded_size_ratio,
       max_padded_size_ratio=max_padded_size_ratio,
-      seed=seed)
+      seed=seed,
+      preprocess_vars_cache=preprocess_vars_cache)
 
   result = list(result)
   if new_label_scores is not None:
@@ -2534,7 +2907,10 @@ def get_default_func_arg_map(include_label_scores=False,
   return prep_func_arg_map
 
 
-def preprocess(tensor_dict, preprocess_options, func_arg_map=None):
+def preprocess(tensor_dict,
+               preprocess_options,
+               func_arg_map=None,
+               preprocess_vars_cache=None):
   """Preprocess images and bounding boxes.
 
   Various types of preprocessing (to be implemented) based on the
@@ -2559,6 +2935,10 @@ def preprocess(tensor_dict, preprocess_options, func_arg_map=None):
                         their values.
     func_arg_map: mapping from preprocessing functions to arguments that they
                   expect to receive and return.
+    preprocess_vars_cache: PreprocessorCache object that records previously
+                           performed augmentations. Updated in-place. If this
+                           function is called multiple times with the same
+                           non-null cache, it will perform deterministically.
 
   Returns:
     tensor_dict: which contains the preprocessed images, bounding boxes, etc.
@@ -2598,6 +2978,9 @@ def preprocess(tensor_dict, preprocess_options, func_arg_map=None):
       return tensor_dict[key] if key is not None else None
 
     args = [get_arg(a) for a in arg_names]
+    if (preprocess_vars_cache is not None and
+        'preprocess_vars_cache' in inspect.getargspec(func).args):
+      params['preprocess_vars_cache'] = preprocess_vars_cache
     results = func(*args, **params)
     if not isinstance(results, (list, tuple)):
       results = (results,)
diff --git a/research/object_detection/core/preprocessor_cache.py b/research/object_detection/core/preprocessor_cache.py
new file mode 100644
index 00000000..2822a2ba
--- /dev/null
+++ b/research/object_detection/core/preprocessor_cache.py
@@ -0,0 +1,102 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Records previous preprocessing operations and allows them to be repeated.
+
+Used with object_detection.core.preprocessor. Passing a PreprocessorCache
+into individual data augmentation functions or the general preprocess() function
+will store all randomly generated variables in the PreprocessorCache. When
+a preprocessor function is called multiple times with the same
+PreprocessorCache object, that function will perform the same augmentation
+on all calls.
+"""
+
+from collections import defaultdict
+
+
+class PreprocessorCache(object):
+  """Dictionary wrapper storing random variables generated during preprocessing.
+  """
+
+  # Constant keys representing different preprocessing functions
+  ROTATION90 = 'rotation90'
+  HORIZONTAL_FLIP = 'horizontal_flip'
+  VERTICAL_FLIP = 'vertical_flip'
+  PIXEL_VALUE_SCALE = 'pixel_value_scale'
+  IMAGE_SCALE = 'image_scale'
+  RGB_TO_GRAY = 'rgb_to_gray'
+  ADJUST_BRIGHTNESS = 'adjust_brightness'
+  ADJUST_CONTRAST = 'adjust_contrast'
+  ADJUST_HUE = 'adjust_hue'
+  ADJUST_SATURATION = 'adjust_saturation'
+  DISTORT_COLOR = 'distort_color'
+  STRICT_CROP_IMAGE = 'strict_crop_image'
+  CROP_IMAGE = 'crop_image'
+  PAD_IMAGE = 'pad_image'
+  CROP_TO_ASPECT_RATIO = 'crop_to_aspect_ratio'
+  RESIZE_METHOD = 'resize_method'
+  PAD_TO_ASPECT_RATIO = 'pad_to_aspect_ratio'
+  BLACK_PATCHES = 'black_patches'
+  ADD_BLACK_PATCH = 'add_black_patch'
+  SELECTOR = 'selector'
+  SELECTOR_TUPLES = 'selector_tuples'
+  SSD_CROP_SELECTOR_ID = 'ssd_crop_selector_id'
+  SSD_CROP_PAD_SELECTOR_ID = 'ssd_crop_pad_selector_id'
+
+  # 23 permitted function ids
+  _VALID_FNS = [ROTATION90, HORIZONTAL_FLIP, VERTICAL_FLIP, PIXEL_VALUE_SCALE,
+                IMAGE_SCALE, RGB_TO_GRAY, ADJUST_BRIGHTNESS, ADJUST_CONTRAST,
+                ADJUST_HUE, ADJUST_SATURATION, DISTORT_COLOR, STRICT_CROP_IMAGE,
+                CROP_IMAGE, PAD_IMAGE, CROP_TO_ASPECT_RATIO, RESIZE_METHOD,
+                PAD_TO_ASPECT_RATIO, BLACK_PATCHES, ADD_BLACK_PATCH, SELECTOR,
+                SELECTOR_TUPLES, SSD_CROP_SELECTOR_ID, SSD_CROP_PAD_SELECTOR_ID]
+
+  def __init__(self):
+    self._history = defaultdict(dict)
+
+  def clear(self):
+    """Resets cache."""
+    self._history = {}
+
+  def get(self, function_id, key):
+    """Gets stored value given a function id and key.
+
+    Args:
+      function_id: identifier for the preprocessing function used.
+      key: identifier for the variable stored.
+    Returns:
+      value: the corresponding value, expected to be a tensor or
+             nested structure of tensors.
+    Raises:
+      ValueError: if function_id is not one of the 23 valid function ids.
+    """
+    if function_id not in self._VALID_FNS:
+      raise ValueError('Function id not recognized: %s.' % str(function_id))
+    return self._history[function_id].get(key)
+
+  def update(self, function_id, key, value):
+    """Adds a value to the dictionary.
+
+    Args:
+      function_id: identifier for the preprocessing function used.
+      key: identifier for the variable stored.
+      value: the value to store, expected to be a tensor or nested structure
+             of tensors.
+    Raises:
+      ValueError: if function_id is not one of the 23 valid function ids.
+    """
+    if function_id not in self._VALID_FNS:
+      raise ValueError('Function id not recognized: %s.' % str(function_id))
+    self._history[function_id][key] = value
+
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 08a01bb1..2d7d4e2d 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -21,6 +21,7 @@ import six
 import tensorflow as tf
 
 from object_detection.core import preprocessor
+from object_detection.core import preprocessor_cache
 from object_detection.core import standard_fields as fields
 
 if six.PY2:
@@ -290,6 +291,15 @@ class PreprocessorTest(tf.test.TestCase):
   def expectedLabelsAfterThresholdingWithMissingScore(self):
     return tf.constant([2], dtype=tf.float32)
 
+  def testRgbToGrayscale(self):
+    images = self.createTestImages()
+    grayscale_images = preprocessor._rgb_to_grayscale(images)
+    expected_images = tf.image.rgb_to_grayscale(images)
+    with self.test_session() as sess:
+      (grayscale_images, expected_images) = sess.run(
+          [grayscale_images, expected_images])
+      self.assertAllEqual(expected_images, grayscale_images)
+
   def testNormalizeImage(self):
     preprocess_options = [(preprocessor.normalize_image, {
         'original_minval': 0,
@@ -435,6 +445,55 @@ class PreprocessorTest(tf.test.TestCase):
       rotated_mask, expected_mask = sess.run([rotated_mask, expected_mask])
       self.assertAllEqual(rotated_mask.flatten(), expected_mask.flatten())
 
+  def _testPreprocessorCache(self,
+                             preprocess_options,
+                             test_boxes=False,
+                             test_masks=False,
+                             test_keypoints=False,
+                             num_runs=4):
+    cache = preprocessor_cache.PreprocessorCache()
+    images = self.createTestImages()
+    boxes = self.createTestBoxes()
+    classes = self.createTestLabels()
+    masks = self.createTestMasks()
+    keypoints = self.createTestKeypoints()
+    preprocessor_arg_map = preprocessor.get_default_func_arg_map(
+        include_instance_masks=test_masks, include_keypoints=test_keypoints)
+    out = []
+    for i in range(num_runs):
+      tensor_dict = {
+          fields.InputDataFields.image: images,
+      }
+      num_outputs = 1
+      if test_boxes:
+        tensor_dict[fields.InputDataFields.groundtruth_boxes] = boxes
+        tensor_dict[fields.InputDataFields.groundtruth_classes] = classes
+        num_outputs += 1
+      if test_masks:
+        tensor_dict[fields.InputDataFields.groundtruth_instance_masks] = masks
+        num_outputs += 1
+      if test_keypoints:
+        tensor_dict[fields.InputDataFields.groundtruth_keypoints] = keypoints
+        num_outputs += 1
+      out.append(preprocessor.preprocess(
+          tensor_dict, preprocess_options, preprocessor_arg_map, cache))
+
+    with self.test_session() as sess:
+      to_run = []
+      for i in range(num_runs):
+        to_run.append(out[i][fields.InputDataFields.image])
+        if test_boxes:
+          to_run.append(out[i][fields.InputDataFields.groundtruth_boxes])
+        if test_masks:
+          to_run.append(
+              out[i][fields.InputDataFields.groundtruth_instance_masks])
+        if test_keypoints:
+          to_run.append(out[i][fields.InputDataFields.groundtruth_keypoints])
+
+      out_array = sess.run(to_run)
+      for i in range(num_outputs, len(out_array)):
+        self.assertAllClose(out_array[i], out_array[i - num_outputs])
+
   def testRandomHorizontalFlip(self):
     preprocess_options = [(preprocessor.random_horizontal_flip, {})]
     images = self.expectedImagesAfterNormalization()
@@ -491,6 +550,16 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(boxes_, boxes_expected_)
       self.assertAllClose(images_diff_, images_diff_expected_)
 
+  def testRandomHorizontalFlipWithCache(self):
+    keypoint_flip_permutation = self.createKeypointFlipPermutation()
+    preprocess_options = [
+        (preprocessor.random_horizontal_flip,
+         {'keypoint_flip_permutation': keypoint_flip_permutation})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRunRandomHorizontalFlipWithMaskAndKeypoints(self):
     preprocess_options = [(preprocessor.random_horizontal_flip, {})]
     image_height = 3
@@ -578,6 +647,16 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(boxes_, boxes_expected_)
       self.assertAllClose(images_diff_, images_diff_expected_)
 
+  def testRandomVerticalFlipWithCache(self):
+    keypoint_flip_permutation = self.createKeypointFlipPermutation()
+    preprocess_options = [
+        (preprocessor.random_vertical_flip,
+         {'keypoint_flip_permutation': keypoint_flip_permutation})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRunRandomVerticalFlipWithMaskAndKeypoints(self):
     preprocess_options = [(preprocessor.random_vertical_flip, {})]
     image_height = 3
@@ -665,6 +744,13 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(boxes_, boxes_expected_)
       self.assertAllClose(images_diff_, images_diff_expected_)
 
+  def testRandomRotation90WithCache(self):
+    preprocess_options = [(preprocessor.random_rotation90, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRunRandomRotation90WithMaskAndKeypoints(self):
     preprocess_options = [(preprocessor.random_rotation90, {})]
     image_height = 3
@@ -716,6 +802,20 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(values_greater_, values_true_)
       self.assertAllClose(values_less_, values_true_)
 
+  def testRandomPixelValueScaleWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_pixel_value_scale, {}))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomImageScale(self):
     preprocess_options = [(preprocessor.random_image_scale, {})]
     images_original = self.createTestImages()
@@ -736,6 +836,13 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertTrue(
           images_original_shape_[2] * 2.0 >= images_scaled_shape_[2])
 
+  def testRandomImageScaleWithCache(self):
+    preprocess_options = [(preprocessor.random_image_scale, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomRGBtoGray(self):
     preprocess_options = [(preprocessor.random_rgb_to_gray, {})]
     images_original = self.createTestImages()
@@ -769,6 +876,14 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(images_g_diff_, image_zero1_)
       self.assertAllClose(images_b_diff_, image_zero1_)
 
+  def testRandomRGBtoGrayWithCache(self):
+    preprocess_options = [(
+        preprocessor.random_rgb_to_gray, {'probability': 0.5})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomAdjustBrightness(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -789,6 +904,20 @@ class PreprocessorTest(tf.test.TestCase):
           [image_original_shape, image_bright_shape])
       self.assertAllEqual(image_original_shape_, image_bright_shape_)
 
+  def testRandomAdjustBrightnessWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_adjust_brightness, {}))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomAdjustContrast(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -809,6 +938,20 @@ class PreprocessorTest(tf.test.TestCase):
           [image_original_shape, image_contrast_shape])
       self.assertAllEqual(image_original_shape_, image_contrast_shape_)
 
+  def testRandomAdjustContrastWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_adjust_contrast, {}))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomAdjustHue(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -829,6 +972,20 @@ class PreprocessorTest(tf.test.TestCase):
           [image_original_shape, image_hue_shape])
       self.assertAllEqual(image_original_shape_, image_hue_shape_)
 
+  def testRandomAdjustHueWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_adjust_hue, {}))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomDistortColor(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -849,6 +1006,20 @@ class PreprocessorTest(tf.test.TestCase):
           [images_original_shape, images_distorted_color_shape])
       self.assertAllEqual(images_original_shape_, images_distorted_color_shape_)
 
+  def testRandomDistortColorWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_distort_color, {}))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=False,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomJitterBoxes(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.random_jitter_boxes, {}))
@@ -900,6 +1071,21 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
       self.assertAllEqual(images_rank_, distorted_images_rank_)
 
+  def testRandomCropImageWithCache(self):
+    preprocess_options = [(preprocessor.random_rgb_to_gray,
+                           {'probability': 0.5}),
+                          (preprocessor.normalize_image, {
+                              'original_minval': 0,
+                              'original_maxval': 255,
+                              'target_minval': 0,
+                              'target_maxval': 1,
+                          }),
+                          (preprocessor.random_crop_image, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRandomCropImageGrayscale(self):
     preprocessing_options = [(preprocessor.rgb_to_gray, {}),
                              (preprocessor.normalize_image, {
@@ -1446,6 +1632,13 @@ class PreprocessorTest(tf.test.TestCase):
            self.expectedKeypointsAfterThresholding()])
       self.assertAllClose(retained_keypoints_, expected_keypoints_)
 
+  def testRandomCropToAspectRatioWithCache(self):
+    preprocess_options = [(preprocessor.random_crop_to_aspect_ratio, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testRunRandomCropToAspectRatioWithMasks(self):
     image = self.createColorfulTestImage()
     boxes = self.createTestBoxes()
@@ -1536,6 +1729,13 @@ class PreprocessorTest(tf.test.TestCase):
         self.assertAllClose(distorted_keypoints_.flatten(),
                             expected_keypoints.flatten())
 
+  def testRandomPadToAspectRatioWithCache(self):
+    preprocess_options = [(preprocessor.random_pad_to_aspect_ratio, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRunRandomPadToAspectRatioWithMasks(self):
     image = self.createColorfulTestImage()
     boxes = self.createTestBoxes()
@@ -1624,6 +1824,17 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllClose(distorted_keypoints_.flatten(),
                           expected_keypoints.flatten())
 
+  def testRandomPadImageWithCache(self):
+    preprocess_options = [(preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1,}), (preprocessor.random_pad_image, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRandomPadImage(self):
     preprocessing_options = [(preprocessor.normalize_image, {
         'original_minval': 0,
@@ -1670,6 +1881,17 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
           padded_boxes_[:, 3] - padded_boxes_[:, 1])))
 
+  def testRandomCropPadImageWithCache(self):
+    preprocess_options = [(preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1,}), (preprocessor.random_crop_pad_image, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRandomCropPadImageWithRandomCoefOne(self):
     preprocessing_options = [(preprocessor.normalize_image, {
         'original_minval': 0,
@@ -1788,6 +2010,22 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertEqual(images_shape_[1], padded_images_shape_[1])
       self.assertEqual(2 * images_shape_[2], padded_images_shape_[2])
 
+  def testRandomBlackPatchesWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_black_patches, {
+        'size_to_image_ratio': 0.5
+    }))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRandomBlackPatches(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -1812,6 +2050,22 @@ class PreprocessorTest(tf.test.TestCase):
           [images_shape, blacked_images_shape])
       self.assertAllEqual(images_shape_, blacked_images_shape_)
 
+  def testRandomResizeMethodWithCache(self):
+    preprocess_options = []
+    preprocess_options.append((preprocessor.normalize_image, {
+        'original_minval': 0,
+        'original_maxval': 255,
+        'target_minval': 0,
+        'target_maxval': 1
+    }))
+    preprocess_options.append((preprocessor.random_resize_method, {
+        'target_size': (75, 150)
+    }))
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=True,
+                                test_keypoints=True)
+
   def testRandomResizeMethod(self):
     preprocessing_options = []
     preprocessing_options.append((preprocessor.normalize_image, {
@@ -2144,6 +2398,20 @@ class PreprocessorTest(tf.test.TestCase):
 
       self.assertAllEqual([0, 1, 1, 0, 1], one_hot)
 
+  def testSSDRandomCropWithCache(self):
+    preprocess_options = [
+        (preprocessor.normalize_image, {
+            'original_minval': 0,
+            'original_maxval': 255,
+            'target_minval': 0,
+            'target_maxval': 1
+        }),
+        (preprocessor.ssd_random_crop, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def testSSDRandomCrop(self):
     preprocessing_options = [
         (preprocessor.normalize_image, {
@@ -2216,6 +2484,20 @@ class PreprocessorTest(tf.test.TestCase):
       self.assertAllEqual(boxes_rank_, distorted_boxes_rank_)
       self.assertAllEqual(images_rank_, distorted_images_rank_)
 
+  def testSSDRandomCropFixedAspectRatioWithCache(self):
+    preprocess_options = [
+        (preprocessor.normalize_image, {
+            'original_minval': 0,
+            'original_maxval': 255,
+            'target_minval': 0,
+            'target_maxval': 1
+        }),
+        (preprocessor.ssd_random_crop_fixed_aspect_ratio, {})]
+    self._testPreprocessorCache(preprocess_options,
+                                test_boxes=True,
+                                test_masks=False,
+                                test_keypoints=False)
+
   def _testSSDRandomCropFixedAspectRatio(self,
                                          include_label_scores,
                                          include_instance_masks,
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index e8418a2a..1ea8ce09 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -58,6 +58,9 @@ class InputDataFields(object):
     groundtruth_keypoint_visibilities: ground truth keypoint visibilities.
     groundtruth_label_scores: groundtruth label scores.
     groundtruth_weights: groundtruth weight factor for bounding boxes.
+    num_groundtruth_boxes: number of groundtruth boxes.
+    true_image_shapes: true shapes of images in the resized images, as resized
+      images can be padded with zeros.
   """
   image = 'image'
   original_image = 'original_image'
@@ -81,6 +84,8 @@ class InputDataFields(object):
   groundtruth_keypoint_visibilities = 'groundtruth_keypoint_visibilities'
   groundtruth_label_scores = 'groundtruth_label_scores'
   groundtruth_weights = 'groundtruth_weights'
+  num_groundtruth_boxes = 'num_groundtruth_boxes'
+  true_image_shape = 'true_image_shape'
 
 
 class DetectionResultFields(object):
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index f519ceb7..70832792 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -389,7 +389,8 @@ def create_target_assigner(reference, stage=None,
 def batch_assign_targets(target_assigner,
                          anchors_batch,
                          gt_box_batch,
-                         gt_class_targets_batch):
+                         gt_class_targets_batch,
+                         gt_weights_batch=None):
   """Batched assignment of classification and regression targets.
 
   Args:
@@ -402,6 +403,8 @@ def batch_assign_targets(target_assigner,
       each tensor has shape [num_gt_boxes_i, classification_target_size] and
       num_gt_boxes_i is the number of boxes in the ith boxlist of
       gt_box_batch.
+    gt_weights_batch: A list of 1-D tf.float32 tensors of shape
+      [num_boxes] containing weights for groundtruth boxes.
 
   Returns:
     batch_cls_targets: a tensor with shape [batch_size, num_anchors,
@@ -435,11 +438,13 @@ def batch_assign_targets(target_assigner,
   reg_targets_list = []
   reg_weights_list = []
   match_list = []
-  for anchors, gt_boxes, gt_class_targets in zip(
-      anchors_batch, gt_box_batch, gt_class_targets_batch):
+  if gt_weights_batch is None:
+    gt_weights_batch = [None] * len(gt_class_targets_batch)
+  for anchors, gt_boxes, gt_class_targets, gt_weights in zip(
+      anchors_batch, gt_box_batch, gt_class_targets_batch, gt_weights_batch):
     (cls_targets, cls_weights, reg_targets,
      reg_weights, match) = target_assigner.assign(
-         anchors, gt_boxes, gt_class_targets)
+         anchors, gt_boxes, gt_class_targets, gt_weights)
     cls_targets_list.append(cls_targets)
     cls_weights_list.append(cls_weights)
     reg_targets_list.append(reg_targets)
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index 6de12ea1..b626c3e8 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -632,6 +632,81 @@ class BatchTargetAssignerTest(test_case.TestCase):
     self.assertAllClose(reg_targets_out, exp_reg_targets)
     self.assertAllClose(reg_weights_out, exp_reg_weights)
 
+  def test_batch_assign_multiclass_targets_with_padded_groundtruth(self):
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
+                 groundtruth_boxlist2, class_targets1, class_targets2,
+                 groundtruth_weights1, groundtruth_weights2):
+      box_list1 = box_list.BoxList(groundtruth_boxlist1)
+      box_list2 = box_list.BoxList(groundtruth_boxlist2)
+      gt_box_batch = [box_list1, box_list2]
+      gt_class_targets = [class_targets1, class_targets2]
+      gt_weights = [groundtruth_weights1, groundtruth_weights2]
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      multiclass_target_assigner = self._get_multi_class_target_assigner(
+          num_classes=3)
+      (cls_targets, cls_weights, reg_targets, reg_weights,
+       _) = targetassigner.batch_assign_targets(
+           multiclass_target_assigner, anchors_boxlist, gt_box_batch,
+           gt_class_targets, gt_weights)
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2],
+                                     [0., 0., 0., 0.]], dtype=np.float32)
+    groundtruth_weights1 = np.array([1, 0], dtype=np.float32)
+    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],
+                                     [0.015789, 0.0985, 0.55789, 0.3842],
+                                     [0, 0, 0, 0]],
+                                    dtype=np.float32)
+    groundtruth_weights2 = np.array([1, 1, 0], dtype=np.float32)
+    class_targets1 = np.array([[0, 1, 0, 0], [0, 0, 0, 0]], dtype=np.float32)
+    class_targets2 = np.array([[0, 0, 0, 1],
+                               [0, 0, 1, 0],
+                               [0, 0, 0, 0]], dtype=np.float32)
+
+    anchor_means = np.array([[0, 0, .25, .25],
+                             [0, .25, 1, 1],
+                             [0, .1, .5, .5],
+                             [.75, .75, 1, 1]], dtype=np.float32)
+    anchor_stddevs = np.array([[.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1]], dtype=np.float32)
+
+    exp_reg_targets = [[[0, 0, -0.5, -0.5],
+                        [0, 0, 0, 0],
+                        [0, 0, 0, 0,],
+                        [0, 0, 0, 0,],],
+                       [[0, 0, 0, 0,],
+                        [0, 0.01231521, 0, 0],
+                        [0.15789001, -0.01500003, 0.57889998, -1.15799987],
+                        [0, 0, 0, 0]]]
+    exp_cls_weights = [[1, 1, 1, 1],
+                       [1, 1, 1, 1]]
+    exp_cls_targets = [[[0, 1, 0, 0],
+                        [1, 0, 0, 0],
+                        [1, 0, 0, 0],
+                        [1, 0, 0, 0]],
+                       [[1, 0, 0, 0],
+                        [0, 0, 0, 1],
+                        [0, 0, 1, 0],
+                        [1, 0, 0, 0]]]
+    exp_reg_weights = [[1, 0, 0, 0],
+                       [0, 1, 1, 0]]
+
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_boxlist1,
+                                                groundtruth_boxlist2,
+                                                class_targets1,
+                                                class_targets2,
+                                                groundtruth_weights1,
+                                                groundtruth_weights2])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+
   def test_batch_assign_multidimensional_targets(self):
     def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
                  groundtruth_boxlist2, class_targets1, class_targets2):
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index a56fe86d..17b3e1a6 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -134,7 +134,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         self.items_to_handlers[
             fields.InputDataFields.groundtruth_instance_masks] = (
                 slim_example_decoder.ItemHandlerCallback(
-                    ['image/object/mask'], self._decode_png_instance_masks))
+                    ['image/object/mask', 'image/height', 'image/width'],
+                    self._decode_png_instance_masks))
       else:
         raise ValueError('Did not recognize the `instance_mask_type` option.')
     if label_map_proto_file:
@@ -178,10 +179,15 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         [None, 4] containing box corners.
       fields.InputDataFields.groundtruth_classes - 1D int64 tensor of shape
         [None] containing classes for the boxes.
+      fields.InputDataFields.groundtruth_weights - 1D float32 tensor of
+        shape [None] indicating the weights of groundtruth boxes.
+      fields.InputDataFields.num_groundtruth_boxes - int32 scalar indicating
+        the number of groundtruth_boxes.
       fields.InputDataFields.groundtruth_area - 1D float32 tensor of shape
         [None] containing containing object mask area in pixel squared.
       fields.InputDataFields.groundtruth_is_crowd - 1D bool tensor of shape
         [None] indicating if the boxes enclose a crowd.
+
     Optional:
       fields.InputDataFields.groundtruth_difficult - 1D bool tensor of shape
         [None] indicating if the boxes represent `difficult` instances.
@@ -189,8 +195,6 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         [None] indicating if the boxes represent `group_of` instances.
       fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
         shape [None, None, None] containing instance masks.
-      fields.InputDataFields.groundtruth_weights - 1D float32 tensor of
-        shape [None] indicating the weights of groundtruth boxes.
     """
     serialized_example = tf.reshape(tf_example_string_tensor, shape=[])
     decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,
@@ -201,6 +205,20 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     is_crowd = fields.InputDataFields.groundtruth_is_crowd
     tensor_dict[is_crowd] = tf.cast(tensor_dict[is_crowd], dtype=tf.bool)
     tensor_dict[fields.InputDataFields.image].set_shape([None, None, 3])
+    tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.shape(
+        tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]
+
+    def default_groundtruth_weights():
+      return tf.ones(
+          [tf.shape(tensor_dict[fields.InputDataFields.groundtruth_boxes])[0]],
+          dtype=tf.float32)
+
+    tensor_dict[fields.InputDataFields.groundtruth_weights] = tf.cond(
+        tf.greater(
+            tf.shape(
+                tensor_dict[fields.InputDataFields.groundtruth_weights])[0],
+            0), lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],
+        default_groundtruth_weights)
     return tensor_dict
 
   def _reshape_instance_masks(self, keys_to_tensors):
@@ -247,6 +265,11 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       return image
 
     png_masks = keys_to_tensors['image/object/mask']
+    height = keys_to_tensors['image/height']
+    width = keys_to_tensors['image/width']
     if isinstance(png_masks, tf.SparseTensor):
       png_masks = tf.sparse_tensor_to_dense(png_masks, default_value='')
-    return tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32)
+    return tf.cond(
+        tf.greater(tf.size(png_masks), 0),
+        lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),
+        lambda: tf.zeros(tf.to_int32(tf.stack([0, height, width]))))
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 23af3acc..888c5454 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -58,7 +58,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))
 
   def testDecodeJpegImage(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     decoded_jpeg = self._DecodeImage(encoded_jpeg)
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -79,7 +79,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertEqual('image_id', tensor_dict[fields.InputDataFields.source_id])
 
   def testDecodeImageKeyAndFilename(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     example = tf.train.Example(features=tf.train.Features(feature={
         'image/encoded': self._BytesFeature(encoded_jpeg),
@@ -97,7 +97,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertEqual('filename', tensor_dict[fields.InputDataFields.filename])
 
   def testDecodePngImage(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_png = self._EncodeImage(image_tensor, encoding_type='png')
     decoded_png = self._DecodeImage(encoded_png, encoding_type='png')
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -147,8 +147,32 @@ class TfExampleDecoderTest(tf.test.TestCase):
         decoded_masks,
         tensor_dict[fields.InputDataFields.groundtruth_instance_masks])
 
+  def testDecodeEmptyPngInstanceMasks(self):
+    image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_masks = []
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded': self._BytesFeature(encoded_jpeg),
+                'image/format': self._BytesFeature('jpeg'),
+                'image/object/mask': self._BytesFeature(encoded_masks),
+                'image/height': self._Int64Feature([10]),
+                'image/width': self._Int64Feature([10]),
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+      self.assertAllEqual(
+          tensor_dict[fields.InputDataFields.groundtruth_instance_masks].shape,
+          [0, 10, 10])
+
   def testDecodeBoundingBox(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
@@ -175,9 +199,39 @@ class TfExampleDecoderTest(tf.test.TestCase):
                                 bbox_ymaxs, bbox_xmaxs]).transpose()
     self.assertAllEqual(expected_boxes,
                         tensor_dict[fields.InputDataFields.groundtruth_boxes])
+    self.assertAllEqual(
+        2, tensor_dict[fields.InputDataFields.num_groundtruth_boxes])
+
+  def testDecodeDefaultGroundtruthWeights(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    example = tf.train.Example(features=tf.train.Features(feature={
+        'image/encoded': self._BytesFeature(encoded_jpeg),
+        'image/format': self._BytesFeature('jpeg'),
+        'image/object/bbox/ymin': self._FloatFeature(bbox_ymins),
+        'image/object/bbox/xmin': self._FloatFeature(bbox_xmins),
+        'image/object/bbox/ymax': self._FloatFeature(bbox_ymaxs),
+        'image/object/bbox/xmax': self._FloatFeature(bbox_xmaxs),
+    })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder()
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes].
+                         get_shape().as_list()), [None, 4])
+
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllClose(tensor_dict[fields.InputDataFields.groundtruth_weights],
+                        np.ones(2, dtype=np.float32))
 
   def testDecodeObjectLabel(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     bbox_classes = [0, 1]
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -199,8 +253,89 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  def testDecodeObjectLabelNoText(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_classes = [1, 2]
+    example = tf.train.Example(features=tf.train.Features(feature={
+        'image/encoded': self._BytesFeature(encoded_jpeg),
+        'image/format': self._BytesFeature('jpeg'),
+        'image/object/class/label': self._Int64Feature(bbox_classes),
+    })).SerializeToString()
+    label_map_string = """
+      item {
+        id:1
+        name:'cat'
+      }
+      item {
+        id:2
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[
+        fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                        [None])
+
+    init = tf.tables_initializer()
+    with self.test_session() as sess:
+      sess.run(init)
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(bbox_classes,
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+
+  def testDecodeObjectLabelUnrecognizedName(self):
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_classes_text = ['cat', 'cheetah']
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    self._BytesFeature(encoded_jpeg),
+                'image/format':
+                    self._BytesFeature('jpeg'),
+                'image/object/class/text':
+                    self._BytesFeature(bbox_classes_text),
+            })).SerializeToString()
+
+    label_map_string = """
+      item {
+        id:2
+        name:'cat'
+      }
+      item {
+        id:1
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
+                         .get_shape().as_list()), [None])
+
+    with self.test_session() as sess:
+      sess.run(tf.tables_initializer())
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual([2, -1],
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+
   def testDecodeObjectLabelWithMapping(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     bbox_classes_text = ['cat', 'dog']
     example = tf.train.Example(
@@ -242,7 +377,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectArea(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     object_area = [100., 174.]
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -263,7 +398,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                         tensor_dict[fields.InputDataFields.groundtruth_area])
 
   def testDecodeObjectIsCrowd(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     object_is_crowd = [0, 1]
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -286,7 +421,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                             fields.InputDataFields.groundtruth_is_crowd])
 
   def testDecodeObjectDifficult(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     object_difficult = [0, 1]
     example = tf.train.Example(features=tf.train.Features(feature={
@@ -309,7 +444,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
                             fields.InputDataFields.groundtruth_difficult])
 
   def testDecodeObjectGroupOf(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     object_group_of = [0, 1]
     example = tf.train.Example(features=tf.train.Features(
@@ -333,7 +468,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
         tensor_dict[fields.InputDataFields.groundtruth_group_of])
 
   def testDecodeObjectWeight(self):
-    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
     object_weights = [0.75, 1.0]
     example = tf.train.Example(features=tf.train.Features(
@@ -362,7 +497,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     image_width = 3
 
     # Randomly generate image.
-    image_tensor = np.random.randint(255, size=(image_height,
+    image_tensor = np.random.randint(256, size=(image_height,
                                                 image_width,
                                                 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -413,7 +548,7 @@ class TfExampleDecoderTest(tf.test.TestCase):
     image_height = 5
     image_width = 3
     # Randomly generate image.
-    image_tensor = np.random.randint(255, size=(image_height,
+    image_tensor = np.random.randint(256, size=(image_height,
                                                 image_width,
                                                 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
diff --git a/research/object_detection/dataset_tools/__init__.py b/research/object_detection/dataset_tools/__init__.py
index 8b137891..e69de29b 100644
--- a/research/object_detection/dataset_tools/__init__.py
+++ b/research/object_detection/dataset_tools/__init__.py
@@ -1 +0,0 @@
-
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index 066b8d05..9928443d 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -87,13 +87,12 @@ def create_tf_example(image,
       to the format expected by the Tensorflow Object Detection API (which is
       which is [ymin, xmin, ymax, xmax] with coordinates normalized relative
       to image size).
-    image_dir: Directory containing the image files.
+    image_dir: directory containing the image files.
     category_index: a dict containing COCO category information keyed
       by the 'id' field of each category.  See the
       label_map_util.create_category_index function.
     include_masks: Whether to include instance segmentations masks
       (PNG encoded) in the result. default: False.
-
   Returns:
     example: The converted tf.Example
     num_annotations_skipped: Number of (invalid) annotations that were ignored.
@@ -104,6 +103,7 @@ def create_tf_example(image,
   image_height = image['height']
   image_width = image['width']
   filename = image['file_name']
+  image_id = image['id']
 
   full_path = os.path.join(image_dir, filename)
   with tf.gfile.GFile(full_path, 'rb') as fid:
@@ -118,6 +118,7 @@ def create_tf_example(image,
   ymax = []
   is_crowd = []
   category_names = []
+  category_ids = []
   area = []
   encoded_mask_png = []
   num_annotations_skipped = 0
@@ -135,12 +136,13 @@ def create_tf_example(image,
     ymax.append(float(y + height) / image_height)
     is_crowd.append(object_annotations['iscrowd'])
     category_id = int(object_annotations['category_id'])
+    category_ids.append(category_id)
     category_names.append(category_index[category_id]['name'].encode('utf8'))
     area.append(object_annotations['area'])
 
     if include_masks:
-      run_len_encoding = mask.frPyObjects(
-          object_annotations['segmentation'], image_height, image_width)
+      run_len_encoding = mask.frPyObjects(object_annotations['segmentation'],
+                                          image_height, image_width)
       binary_mask = mask.decode(run_len_encoding)
       if not object_annotations['iscrowd']:
         binary_mask = np.amax(binary_mask, axis=2)
@@ -148,31 +150,41 @@ def create_tf_example(image,
       output_io = io.BytesIO()
       pil_image.save(output_io, format='PNG')
       encoded_mask_png.append(output_io.getvalue())
-
   feature_dict = {
-      'image/height': dataset_util.int64_feature(image_height),
-      'image/width': dataset_util.int64_feature(image_width),
-      'image/filename': dataset_util.bytes_feature(
-          filename.encode('utf8')),
-      'image/source_id': dataset_util.bytes_feature(
-          filename.encode('utf8')),
-      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
-      'image/encoded': dataset_util.bytes_feature(encoded_jpg),
-      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
-      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),
-      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),
-      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),
-      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
-      'image/object/class/text': dataset_util.bytes_list_feature(
-          category_names),
-      'image/object/is_crowd': dataset_util.int64_list_feature(is_crowd),
-      'image/object/area': dataset_util.float_list_feature(area),
+      'image/height':
+          dataset_util.int64_feature(image_height),
+      'image/width':
+          dataset_util.int64_feature(image_width),
+      'image/filename':
+          dataset_util.bytes_feature(filename.encode('utf8')),
+      'image/source_id':
+          dataset_util.bytes_feature(str(image_id).encode('utf8')),
+      'image/key/sha256':
+          dataset_util.bytes_feature(key.encode('utf8')),
+      'image/encoded':
+          dataset_util.bytes_feature(encoded_jpg),
+      'image/format':
+          dataset_util.bytes_feature('jpeg'.encode('utf8')),
+      'image/object/bbox/xmin':
+          dataset_util.float_list_feature(xmin),
+      'image/object/bbox/xmax':
+          dataset_util.float_list_feature(xmax),
+      'image/object/bbox/ymin':
+          dataset_util.float_list_feature(ymin),
+      'image/object/bbox/ymax':
+          dataset_util.float_list_feature(ymax),
+      'image/object/class/label':
+          dataset_util.int64_list_feature(category_ids),
+      'image/object/is_crowd':
+          dataset_util.int64_list_feature(is_crowd),
+      'image/object/area':
+          dataset_util.float_list_feature(area),
   }
   if include_masks:
     feature_dict['image/object/mask'] = (
         dataset_util.bytes_list_feature(encoded_mask_png))
   example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
-  return example, num_annotations_skipped
+  return key, example, num_annotations_skipped
 
 
 def _create_tf_record_from_coco_annotations(
@@ -217,7 +229,7 @@ def _create_tf_record_from_coco_annotations(
       if idx % 100 == 0:
         tf.logging.info('On image %d of %d', idx, len(images))
       annotations_list = annotations_index[image['id']]
-      tf_example, num_annotations_skipped = create_tf_example(
+      _, tf_example, num_annotations_skipped = create_tf_example(
           image, annotations_list, image_dir, category_index, include_masks)
       total_num_annotations_skipped += num_annotations_skipped
       writer.write(tf_example.SerializeToString())
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record_test.py b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
index 23bcb14d..45697eef 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Test for create_coco_tf_record.py."""
 
 import io
@@ -52,26 +51,34 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         'id': 11,
     }
 
-    annotations_list = [
-        {
-            'area': .5,
-            'iscrowd': False,
-            'image_id': 11,
-            'bbox': [64, 64, 128, 128],
-            'category_id': 2,
-            'id': 1000,
-        }
-    ]
+    annotations_list = [{
+        'area': .5,
+        'iscrowd': False,
+        'image_id': 11,
+        'bbox': [64, 64, 128, 128],
+        'category_id': 2,
+        'id': 1000,
+    }]
 
     image_dir = tmp_dir
     category_index = {
-        1: {'name': 'dog', 'id': 1},
-        2: {'name': 'cat', 'id': 2},
-        3: {'name': 'human', 'id': 3}
+        1: {
+            'name': 'dog',
+            'id': 1
+        },
+        2: {
+            'name': 'cat',
+            'id': 2
+        },
+        3: {
+            'name': 'human',
+            'id': 3
+        }
     }
 
-    example, num_annotations_skipped = create_coco_tf_record.create_tf_example(
-        image, annotations_list, image_dir, category_index)
+    (_, example,
+     num_annotations_skipped) = create_coco_tf_record.create_tf_example(
+         image, annotations_list, image_dir, category_index)
 
     self.assertEqual(num_annotations_skipped, 0)
     self._assertProtoEqual(
@@ -83,7 +90,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         [image_file_name])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [image_file_name])
+        [str(image['id'])])
     self._assertProtoEqual(
         example.features.feature['image/format'].bytes_list.value, ['jpeg'])
     self._assertProtoEqual(
@@ -98,9 +105,6 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/ymax'].float_list.value,
         [0.75])
-    self._assertProtoEqual(
-        example.features.feature['image/object/class/text'].bytes_list.value,
-        ['cat'])
 
   def test_create_tf_example_with_instance_masks(self):
     image_file_name = 'tmp_image.jpg'
@@ -117,26 +121,27 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         'id': 11,
     }
 
-    annotations_list = [
-        {
-            'area': .5,
-            'iscrowd': False,
-            'image_id': 11,
-            'bbox': [0, 0, 8, 8],
-            'segmentation': [[4, 0, 0, 0, 0, 4],
-                             [8, 4, 4, 8, 8, 8]],
-            'category_id': 1,
-            'id': 1000,
-        }
-    ]
+    annotations_list = [{
+        'area': .5,
+        'iscrowd': False,
+        'image_id': 11,
+        'bbox': [0, 0, 8, 8],
+        'segmentation': [[4, 0, 0, 0, 0, 4], [8, 4, 4, 8, 8, 8]],
+        'category_id': 1,
+        'id': 1000,
+    }]
 
     image_dir = tmp_dir
     category_index = {
-        1: {'name': 'dog', 'id': 1},
+        1: {
+            'name': 'dog',
+            'id': 1
+        },
     }
 
-    example, num_annotations_skipped = create_coco_tf_record.create_tf_example(
-        image, annotations_list, image_dir, category_index, include_masks=True)
+    (_, example,
+     num_annotations_skipped) = create_coco_tf_record.create_tf_example(
+         image, annotations_list, image_dir, category_index, include_masks=True)
 
     self.assertEqual(num_annotations_skipped, 0)
     self._assertProtoEqual(
@@ -148,7 +153,7 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
         [image_file_name])
     self._assertProtoEqual(
         example.features.feature['image/source_id'].bytes_list.value,
-        [image_file_name])
+        [str(image['id'])])
     self._assertProtoEqual(
         example.features.feature['image/format'].bytes_list.value, ['jpeg'])
     self._assertProtoEqual(
@@ -163,24 +168,20 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
     self._assertProtoEqual(
         example.features.feature['image/object/bbox/ymax'].float_list.value,
         [1])
-    self._assertProtoEqual(
-        example.features.feature['image/object/class/text'].bytes_list.value,
-        ['dog'])
-    encoded_mask_pngs = [io.BytesIO(encoded_masks)
-                         for encoded_masks in example.features.feature[
-                             'image/object/mask'].bytes_list.value]
-    pil_masks = [np.array(PIL.Image.open(encoded_mask_png))
-                 for encoded_mask_png in encoded_mask_pngs]
+    encoded_mask_pngs = [
+        io.BytesIO(encoded_masks) for encoded_masks in example.features.feature[
+            'image/object/mask'].bytes_list.value
+    ]
+    pil_masks = [
+        np.array(PIL.Image.open(encoded_mask_png))
+        for encoded_mask_png in encoded_mask_pngs
+    ]
     self.assertTrue(len(pil_masks) == 1)
     self.assertAllEqual(pil_masks[0],
-                        [[1, 1, 1, 0, 0, 0, 0, 0],
-                         [1, 1, 0, 0, 0, 0, 0, 0],
-                         [1, 0, 0, 0, 0, 0, 0, 0],
-                         [0, 0, 0, 0, 0, 0, 0, 0],
-                         [0, 0, 0, 0, 0, 0, 0, 1],
-                         [0, 0, 0, 0, 0, 0, 1, 1],
-                         [0, 0, 0, 0, 0, 1, 1, 1],
-                         [0, 0, 0, 0, 1, 1, 1, 1]])
+                        [[1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0],
+                         [1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1],
+                         [0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 96e30870..fa8d7b60 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -509,6 +509,11 @@ def result_dict_for_single_example(image,
     detection_masks = detections[detection_fields.detection_masks][0]
     # TODO: This should be done in model's postprocess
     # function ideally.
+    num_detections = tf.to_int32(detections[detection_fields.num_detections][0])
+    detection_boxes = tf.slice(
+        detection_boxes, begin=[0, 0], size=[num_detections, -1])
+    detection_masks = tf.slice(
+        detection_masks, begin=[0, 0, 0], size=[num_detections, -1, -1])
     detection_masks_reframed = ops.reframe_box_masks_to_image_masks(
         detection_masks, detection_boxes, image_shape[1], image_shape[2])
     detection_masks_reframed = tf.cast(
diff --git a/research/object_detection/evaluator.py b/research/object_detection/evaluator.py
index b2bd50ed..a97bf02a 100644
--- a/research/object_detection/evaluator.py
+++ b/research/object_detection/evaluator.py
@@ -24,6 +24,7 @@ import tensorflow as tf
 from object_detection import eval_util
 from object_detection.core import prefetcher
 from object_detection.core import standard_fields as fields
+from object_detection.metrics import coco_evaluation
 from object_detection.utils import object_detection_evaluation
 
 # A dictionary of metric names to classes that implement the metric. The classes
@@ -39,7 +40,11 @@ EVAL_METRICS_CLASS_DICT = {
     'weighted_pascal_voc_instance_segmentation_metrics':
         object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,
     'open_images_detection_metrics':
-        object_detection_evaluation.OpenImagesDetectionEvaluator
+        object_detection_evaluation.OpenImagesDetectionEvaluator,
+    'coco_detection_metrics':
+        coco_evaluation.CocoDetectionEvaluator,
+    'coco_mask_metrics':
+        coco_evaluation.CocoMaskEvaluator,
 }
 
 EVAL_DEFAULT_METRIC = 'pascal_voc_detection_metrics'
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index dac1f1d8..baf29944 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -19,6 +19,7 @@ import os
 import tempfile
 import tensorflow as tf
 from google.protobuf import text_format
+from tensorflow.core.protobuf import saver_pb2
 from tensorflow.python import pywrap_tensorflow
 from tensorflow.python.client import session
 from tensorflow.python.framework import graph_util
@@ -354,16 +355,22 @@ def _export_inference_graph(input_type,
 
   if graph_hook_fn: graph_hook_fn()
 
+  saver_kwargs = {}
   if use_moving_averages:
-    temp_checkpoint_file = tempfile.NamedTemporaryFile()
+    # This check is to be compatible with both version of SaverDef.
+    if os.path.isfile(trained_checkpoint_prefix):
+      saver_kwargs['write_version'] = saver_pb2.SaverDef.V1
+      temp_checkpoint_prefix = tempfile.NamedTemporaryFile().name
+    else:
+      temp_checkpoint_prefix = tempfile.mkdtemp()
     replace_variable_values_with_moving_averages(
         tf.get_default_graph(), trained_checkpoint_prefix,
-        temp_checkpoint_file.name)
-    checkpoint_to_use = temp_checkpoint_file.name
+        temp_checkpoint_prefix)
+    checkpoint_to_use = temp_checkpoint_prefix
   else:
     checkpoint_to_use = trained_checkpoint_prefix
 
-  saver = tf.train.Saver()
+  saver = tf.train.Saver(**saver_kwargs)
   input_saver_def = saver.as_saver_def()
 
   _write_graph_and_checkpoint(
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index d6d31e0d..03d629c2 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -23,7 +23,7 @@ In the table below, we list each such pre-trained model including:
 * detector performance on subset of the COCO validation set or Open Images test split as measured by the dataset-specific mAP measure.
   Here, higher is better, and we only report bounding box mAP rounded to the
   nearest integer.
-* Output types (currently only `Boxes`)
+* Output types (`Boxes`, and `Masks` if applicable )
 
 You can un-tar each tar.gz file via, e.g.,:
 
@@ -55,7 +55,7 @@ Some remarks on frozen inference graphs:
   a detector (and discarding the part past that point), which negatively impacts
   standard mAP metrics.
 * Our frozen inference graphs are generated using the
-  [v1.4.0](https://github.com/tensorflow/tensorflow/tree/v1.4.0)
+  [v1.5.0](https://github.com/tensorflow/tensorflow/tree/v1.5.0)
   release version of Tensorflow and we do not guarantee that these will work
   with other versions; this being said, each frozen inference graph can be
   regenerated using your current version of Tensorflow by re-running the
@@ -69,16 +69,20 @@ Some remarks on frozen inference graphs:
 | ------------ | :--------------: | :--------------: | :-------------: |
 | [ssd_mobilenet_v1_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2017_11_17.tar.gz) | 30 | 21 | Boxes |
 | [ssd_inception_v2_coco](http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2017_11_17.tar.gz) | 42 | 24 | Boxes |
-| [faster_rcnn_inception_v2_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2017_11_08.tar.gz) | 58 | 28 | Boxes |
-| [faster_rcnn_resnet50_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2017_11_08.tar.gz) | 89 | 30 | Boxes |
-| [faster_rcnn_resnet50_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_lowproposals_coco_2017_11_08.tar.gz) | 64 |  | Boxes |
-| [rfcn_resnet101_coco](http://download.tensorflow.org/models/object_detection/rfcn_resnet101_coco_2017_11_08.tar.gz)  | 92 | 30 | Boxes |
-| [faster_rcnn_resnet101_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2017_11_08.tar.gz) | 106 | 32 | Boxes |
-| [faster_rcnn_resnet101_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_lowproposals_coco_2017_11_08.tar.gz) | 82 |  | Boxes |
-| [faster_rcnn_inception_resnet_v2_atrous_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_2017_11_08.tar.gz) | 620 | 37 | Boxes |
-| [faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco_2017_11_08.tar.gz) | 241 |  | Boxes |
-| [faster_rcnn_nas](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_coco_2017_11_08.tar.gz) | 1833 | 43 | Boxes |
-| [faster_rcnn_nas_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_lowproposals_coco_2017_11_08.tar.gz) | 540 |  | Boxes |
+| [faster_rcnn_inception_v2_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz) | 58 | 28 | Boxes |
+| [faster_rcnn_resnet50_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz) | 89 | 30 | Boxes |
+| [faster_rcnn_resnet50_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_lowproposals_coco_2018_01_28.tar.gz) | 64 |  | Boxes |
+| [rfcn_resnet101_coco](http://download.tensorflow.org/models/object_detection/rfcn_resnet101_coco_2018_01_28.tar.gz)  | 92 | 30 | Boxes |
+| [faster_rcnn_resnet101_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_coco_2018_01_28.tar.gz) | 106 | 32 | Boxes |
+| [faster_rcnn_resnet101_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_lowproposals_coco_2018_01_28.tar.gz) | 82 |  | Boxes |
+| [faster_rcnn_inception_resnet_v2_atrous_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.tar.gz) | 620 | 37 | Boxes |
+| [faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco_2018_01_28.tar.gz) | 241 |  | Boxes |
+| [faster_rcnn_nas](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_coco_2018_01_28.tar.gz) | 1833 | 43 | Boxes |
+| [faster_rcnn_nas_lowproposals_coco](http://download.tensorflow.org/models/object_detection/faster_rcnn_nas_lowproposals_coco_2018_01_28.tar.gz) | 540 |  | Boxes |
+| [mask_rcnn_inception_resnet_v2_atrous_coco](http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28.tar.gz) | 771 | 36 | Masks |
+| [mask_rcnn_inception_v2_coco](http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz) | 79 | 25 | Masks |
+| [mask_rcnn_resnet101_atrous_coco](http://download.tensorflow.org/models/object_detection/mask_rcnn_resnet101_atrous_coco_2018_01_28.tar.gz) | 470 | 33 | Masks |
+| [mask_rcnn_resnet50_atrous_coco](http://download.tensorflow.org/models/object_detection/mask_rcnn_resnet50_atrous_coco_2018_01_28.tar.gz) | 343 | 29 | Masks |
 
 
 
@@ -86,14 +90,14 @@ Some remarks on frozen inference graphs:
 
 Model name                                                                                                                                                        | Speed (ms) | Pascal mAP@0.5 (ms) | Outputs
 ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :-------------: | :-----:
-[faster_rcnn_resnet101_kitti](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2017_11_08.tar.gz) | 79  | 87              | Boxes
+[faster_rcnn_resnet101_kitti](http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet101_kitti_2018_01_28.tar.gz) | 79  | 87              | Boxes
 
 ## Open Images-trained models {#open-images-models}
 
 Model name                                                                                                                                                        | Speed (ms) | Open Images mAP@0.5[^2] | Outputs
 ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :-------------: | :-----:
-[faster_rcnn_inception_resnet_v2_atrous_oid](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_2017_11_08.tar.gz) | 727 | 37              | Boxes
-[faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2017_11_08.tar.gz) | 347  |               | Boxes
+[faster_rcnn_inception_resnet_v2_atrous_oid](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid_2018_01_28.tar.gz) | 727 | 37              | Boxes
+[faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid](http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_resnet_v2_atrous_lowproposals_oid_2018_01_28.tar.gz) | 347  |               | Boxes
 
 
 [^1]: See [MSCOCO evaluation protocol](http://cocodataset.org/#detections-eval).
diff --git a/research/object_detection/g3doc/img/kites_with_segment_overlay.png b/research/object_detection/g3doc/img/kites_with_segment_overlay.png
new file mode 100644
index 00000000..a52e57de
Binary files /dev/null and b/research/object_detection/g3doc/img/kites_with_segment_overlay.png differ
diff --git a/research/object_detection/g3doc/instance_segmentation.md b/research/object_detection/g3doc/instance_segmentation.md
new file mode 100644
index 00000000..8ebf7d8c
--- /dev/null
+++ b/research/object_detection/g3doc/instance_segmentation.md
@@ -0,0 +1,105 @@
+## Run an Instance Segmentation Model
+
+For some applications it isn't adequate enough to localize an object with a
+simple bounding box. For instance, you might want to segment an object region
+once it is detected. This class of problems is called **instance segmentation**.
+
+<p align="center">
+  <img src="img/kites_with_segment_overlay.png" width=676 height=450>
+</p>
+
+### Materializing data for instance segmentation {#materializing-instance-seg}
+
+Instance segmentation is an extension of object detection, where a binary mask
+(i.e. object vs. background) is associated with every bounding box. This allows
+for more fine-grained information about the extent of the object within the box.
+To train an instance segmentation model, a groundtruth mask must be supplied for
+every groundtruth bounding box. In additional to the proto fields listed in the
+section titled [Using your own dataset](using_your_own_dataset.md), one must
+also supply `image/object/mask`, which can either be a repeated list of
+single-channel encoded PNG strings, or a single dense 3D binary tensor where
+masks corresponding to each object are stacked along the first dimension. Each
+is described in more detail below.
+
+#### PNG Instance Segmentation Masks
+
+Instance segmentation masks can be supplied as serialized PNG images.
+
+```shell
+image/object/mask = ["\x89PNG\r\n\x1A\n\x00\x00\x00\rIHDR\...", ...]
+```
+
+These masks are whole-image masks, one for each object instance. The spatial
+dimensions of each mask must agree with the image. Each mask has only a single
+channel, and the pixel values are either 0 (background) or 1 (object mask).
+**PNG masks are the preferred parameterization since they offer considerable
+space savings compared to dense numerical masks.**
+
+#### Dense Numerical Instance Segmentation Masks
+
+Masks can also be specified via a dense numerical tensor.
+
+```shell
+image/object/mask = [0.0, 0.0, 1.0, 1.0, 0.0, ...]
+```
+
+For an image with dimensions `H` x `W` and `num_boxes` groundtruth boxes, the
+mask corresponds to a [`num_boxes`, `H`, `W`] float32 tensor, flattened into a
+single vector of shape `num_boxes` * `H` * `W`. In TensorFlow, examples are read
+in row-major format, so the elements are organized as:
+
+```shell
+... mask 0 row 0 ... mask 0 row 1 ... // ... mask 0 row H-1 ... mask 1 row 0 ...
+```
+
+where each row has W contiguous binary values.
+
+To see an example tf-records with mask labels, see the examples under the
+[Preparing Inputs](preparing_inputs.md) section.
+
+### Pre-existing config files
+
+We provide four instance segmentation config files that you can use to train
+your own models:
+
+1.  <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config" target=_blank>mask_rcnn_inception_resnet_v2_atrous_coco</a>
+1.  <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config" target=_blank>mask_rcnn_resnet101_atrous_coco</a>
+1.  <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config" target=_blank>mask_rcnn_resnet50_atrous_coco</a>
+1.  <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config" target=_blank>mask_rcnn_inception_v2_coco</a>
+
+For more details see the [detection model zoo](detection_model_zoo.md).
+
+### Updating a Faster R-CNN config file
+
+Currently, the only supported instance segmentation model is [Mask
+R-CNN](https://arxiv.org/abs/1703.06870), which requires Faster R-CNN as the
+backbone object detector.
+
+Once you have a baseline Faster R-CNN pipeline configuration, you can make the
+following modifications in order to convert it into a Mask R-CNN model.
+
+1.  Within `train_input_reader` and `eval_input_reader`, set
+    `load_instance_masks` to `True`. If using PNG masks, set `mask_type` to
+    `PNG_MASKS`, otherwise you can leave it as the default 'NUMERICAL_MASKS'.
+1.  Within the `faster_rcnn` config, use a `MaskRCNNBoxPredictor` as the
+    `second_stage_box_predictor`.
+1.  Within the `MaskRCNNBoxPredictor` message, set `predict_instance_masks` to
+    `True`. You must also define `conv_hyperparams`.
+1.  Within the `faster_rcnn` message, set `number_of_stages` to `3`.
+1.  Add instance segmentation metrics to the set of metrics:
+    `'coco_mask_metrics'`.
+1.  Update the `input_path`s to point at your data.
+
+Please refer to the section on [Running the pets dataset](running_pets.md) for
+additional details.
+
+> Note: The mask prediction branch consists of a sequence of convolution layers.
+> You can set the number of convolution layers and their depth as follows:
+>
+> 1.  Within the `MaskRCNNBoxPredictor` message, set the
+>     `mask_prediction_conv_depth` to your value of interest. The default value
+>     is 256. If you set it to `0` (recommended), the depth is computed
+>     automatically based on the number of classes in the dataset.
+> 1.  Within the `MaskRCNNBoxPredictor` message, set the
+>     `mask_prediction_num_conv_layers` to your value of interest. The default
+>     value is 2.
diff --git a/research/object_detection/g3doc/running_pets.md b/research/object_detection/g3doc/running_pets.md
index 6ae91799..013c5579 100644
--- a/research/object_detection/g3doc/running_pets.md
+++ b/research/object_detection/g3doc/running_pets.md
@@ -319,6 +319,9 @@ instance segmentation pipeline. Everything above that was mentioned about object
 detection holds true for instance segmentation. Instance segmentation consists
 of an object detection model with an additional head that predicts the object
 mask inside each predicted box once we remove the training and other details.
+Please refer to the section on [Running an Instance Segmentation
+Model](instance_segmentation.md) for instructions on how to configure a model
+that predicts masks in addition to object bounding boxes.
 
 ## What's Next
 
diff --git a/research/object_detection/g3doc/using_your_own_dataset.md b/research/object_detection/g3doc/using_your_own_dataset.md
index c403930e..d989d32e 100644
--- a/research/object_detection/g3doc/using_your_own_dataset.md
+++ b/research/object_detection/g3doc/using_your_own_dataset.md
@@ -103,7 +103,7 @@ FLAGS = flags.FLAGS
 
 
 def create_tf_example(example):
-  # TODO(user): Populate the following variables from your example.
+  # TODO: Populate the following variables from your example.
   height = None # Image height
   width = None # Image width
   filename = None # Filename of the image. Empty if image is not from file
@@ -139,7 +139,7 @@ def create_tf_example(example):
 def main(_):
   writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
 
-  # TODO(user): Write code to read in your dataset to examples variable
+  # TODO: Write code to read in your dataset to examples variable
 
   for example in examples:
     tf_example = create_tf_example(example)
@@ -155,3 +155,7 @@ if __name__ == '__main__':
 
 Note: You may notice additional fields in some other datasets. They are
 currently unused by the API and are optional.
+
+Note: Please refer to the section on [Running an Instance Segmentation
+Model](instance_segmentation.md) for instructions on how to configure a model
+that predicts masks in addition to object bounding boxes.
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index b2a23744..3255877f 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -21,56 +21,183 @@ from __future__ import print_function
 import functools
 
 import tensorflow as tf
-from object_detection import trainer
 from object_detection.builders import dataset_builder
+from object_detection.builders import image_resizer_builder
+from object_detection.builders import model_builder
 from object_detection.builders import preprocessor_builder
-from object_detection.core import prefetcher
+from object_detection.core import preprocessor
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import eval_pb2
 from object_detection.protos import input_reader_pb2
+from object_detection.protos import model_pb2
 from object_detection.protos import train_pb2
+from object_detection.utils import config_util
 from object_detection.utils import dataset_util
 from object_detection.utils import ops as util_ops
 
-FEATURES_IMAGE = 'images'
-FEATURES_KEY = 'key'
+HASH_KEY = 'hash'
+HASH_BINS = 1 << 31
 SERVING_FED_EXAMPLE_KEY = 'serialized_example'
 
 
-def create_train_input_fn(num_classes, train_config, train_input_config):
+def transform_input_data(tensor_dict,
+                         model_preprocess_fn,
+                         image_resizer_fn,
+                         num_classes,
+                         data_augmentation_fn=None,
+                         merge_multiple_boxes=False,
+                         retain_original_image=False):
+  """A single function that is responsible for all input data transformations.
+
+  Data transformation functions are applied in the following order.
+  1. data_augmentation_fn (optional): applied on tensor_dict.
+  2. model_preprocess_fn: applied only on image tensor in tensor_dict.
+  3. image_resizer_fn: applied only on instance mask tensor in tensor_dict.
+  4. one_hot_encoding: applied to classes tensor in tensor_dict.
+  5. merge_multiple_boxes (optional): when groundtruth boxes are exactly the
+     same they can be merged into a single box with an associated k-hot class
+     label.
+
+  Args:
+    tensor_dict: dictionary containing input tensors keyed by
+      fields.InputDataFields.
+    model_preprocess_fn: model's preprocess function to apply on image tensor.
+      This function must take in a 4-D float tensor and return a 4-D preprocess
+      float tensor and a tensor containing the true image shape.
+    image_resizer_fn: image resizer function to apply on groundtruth instance
+      masks. This function must take a 4-D float tensor of image and a 4-D
+      tensor of instances masks and return resized version of these along with
+      the true shapes.
+    num_classes: number of max classes to one-hot (or k-hot) encode the class
+      labels.
+    data_augmentation_fn: (optional) data augmentation function to apply on
+      input `tensor_dict`.
+    merge_multiple_boxes: (optional) whether to merge multiple groundtruth boxes
+      and classes for a given image if the boxes are exactly the same.
+    retain_original_image: (optional) whether to retain original image in the
+      output dictionary.
+
+  Returns:
+    A dictionary keyed by fields.InputDataFields containing the tensors obtained
+    after applying all the transformations.
+  """
+  if retain_original_image:
+    tensor_dict[fields.InputDataFields.
+                original_image] = tensor_dict[fields.InputDataFields.image]
+
+  # Apply data augmentation ops.
+  if data_augmentation_fn is not None:
+    tensor_dict = data_augmentation_fn(tensor_dict)
+
+  # Apply model preprocessing ops and resize instance masks.
+  image = tf.expand_dims(
+      tf.to_float(tensor_dict[fields.InputDataFields.image]), axis=0)
+  preprocessed_resized_image, true_image_shape = model_preprocess_fn(image)
+  tensor_dict[fields.InputDataFields.image] = tf.squeeze(
+      preprocessed_resized_image, axis=0)
+  tensor_dict[fields.InputDataFields.true_image_shape] = tf.squeeze(
+      true_image_shape, axis=0)
+  if fields.InputDataFields.groundtruth_instance_masks in tensor_dict:
+    masks = tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
+    _, resized_masks, _ = image_resizer_fn(image, masks)
+    tensor_dict[fields.InputDataFields.
+                groundtruth_instance_masks] = resized_masks
+
+  # Transform groundtruth classes to one hot encodings.
+  label_offset = 1
+  zero_indexed_groundtruth_classes = tensor_dict[
+      fields.InputDataFields.groundtruth_classes] - label_offset
+  tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.one_hot(
+      zero_indexed_groundtruth_classes, num_classes)
+
+  if merge_multiple_boxes:
+    merged_boxes, merged_classes, _ = util_ops.merge_boxes_with_multiple_labels(
+        tensor_dict[fields.InputDataFields.groundtruth_boxes],
+        zero_indexed_groundtruth_classes, num_classes)
+    tensor_dict[fields.InputDataFields.groundtruth_boxes] = merged_boxes
+    tensor_dict[fields.InputDataFields.groundtruth_classes] = merged_classes
+
+  return tensor_dict
+
+
+def augment_input_data(tensor_dict, data_augmentation_options):
+  """Applies data augmentation ops to input tensors.
+
+  Args:
+    tensor_dict: A dictionary of input tensors keyed by fields.InputDataFields.
+    data_augmentation_options: A list of tuples, where each tuple contains a
+      function and a dictionary that contains arguments and their values.
+      Usually, this is the output of core/preprocessor.build.
+
+  Returns:
+    A dictionary of tensors obtained by applying data augmentation ops to the
+    input tensor dictionary.
+  """
+  tensor_dict[fields.InputDataFields.image] = tf.expand_dims(
+      tf.to_float(tensor_dict[fields.InputDataFields.image]), 0)
+
+  include_instance_masks = (fields.InputDataFields.groundtruth_instance_masks
+                            in tensor_dict)
+  include_keypoints = (fields.InputDataFields.groundtruth_keypoints
+                       in tensor_dict)
+  tensor_dict = preprocessor.preprocess(
+      tensor_dict, data_augmentation_options,
+      func_arg_map=preprocessor.get_default_func_arg_map(
+          include_instance_masks=include_instance_masks,
+          include_keypoints=include_keypoints))
+  tensor_dict[fields.InputDataFields.image] = tf.squeeze(
+      tensor_dict[fields.InputDataFields.image], axis=0)
+  return tensor_dict
+
+
+def create_train_input_fn(train_config, train_input_config,
+                          model_config):
   """Creates a train `input` function for `Estimator`.
 
   Args:
-    num_classes: Number of classes, which does not include a background
-      category.
     train_config: A train_pb2.TrainConfig.
     train_input_config: An input_reader_pb2.InputReader.
+    model_config: A model_pb2.DetectionModel.
 
   Returns:
     `input_fn` for `Estimator` in TRAIN mode.
   """
 
-  def _train_input_fn():
+  def _train_input_fn(params=None):
     """Returns `features` and `labels` tensor dictionaries for training.
 
+    Args:
+      params: Parameter dictionary passed from the estimator.
+
     Returns:
       features: Dictionary of feature tensors.
-        features['images'] is a list of N [1, H, W, C] float32 tensors,
-          where N is the number of images in a batch.
-        features['key'] is a list of N string tensors, each representing a
-          unique identifier for the image.
+        features[fields.InputDataFields.image] is a [batch_size, H, W, C]
+          float32 tensor with preprocessed images.
+        features[HASH_KEY] is a [batch_size] int32 tensor representing unique
+          identifiers for the images.
+        features[fields.InputDataFields.true_image_shape] is a [batch_size, 3]
+          int32 tensor representing the true image shapes, as preprocessed
+          images could be padded.
       labels: Dictionary of groundtruth tensors.
-        labels['locations_list'] is a list of N [num_boxes, 4] float32 tensors
-          containing the corners of the groundtruth boxes.
-        labels['classes_list'] is a list of N [num_boxes, num_classes] float32
-          padded one-hot tensors of classes.
-        labels['masks_list'] is a list of N [num_boxes, H, W] float32 tensors
-          containing only binary values, which represent instance masks for
-          objects if present in the dataset. Else returns None.
-        labels[fields.InputDataFields.groundtruth_weights] is a list of N
-          [num_boxes] float32 tensors containing groundtruth weights for the
-          boxes.
+        labels[fields.InputDataFields.num_groundtruth_boxes] is a [batch_size]
+          int32 tensor indicating the number of groundtruth boxes.
+        labels[fields.InputDataFields.groundtruth_boxes] is a
+          [batch_size, num_boxes, 4] float32 tensor containing the corners of
+          the groundtruth boxes.
+        labels[fields.InputDataFields.groundtruth_classes] is a
+          [batch_size, num_boxes, num_classes] float32 one-hot tensor of
+          classes.
+        labels[fields.InputDataFields.groundtruth_weights] is a
+          [batch_size, num_boxes] float32 tensor containing groundtruth weights
+          for the boxes.
+        -- Optional --
+        labels[fields.InputDataFields.groundtruth_instance_masks] is a
+          [batch_size, num_boxes, H, W] float32 tensor containing only binary
+          values, which represent instance masks for objects.
+        labels[fields.InputDataFields.groundtruth_keypoints] is a
+          [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing
+          keypoints for each box.
 
     Raises:
       TypeError: if the `train_config` or `train_input_config` are not of the
@@ -82,164 +209,226 @@ def create_train_input_fn(num_classes, train_config, train_input_config):
     if not isinstance(train_input_config, input_reader_pb2.InputReader):
       raise TypeError('The `train_input_config` must be a '
                       'input_reader_pb2.InputReader.')
-
-    def get_next(config):
-      return dataset_util.make_initializable_iterator(
-          dataset_builder.build(config)).get_next()
-
-    create_tensor_dict_fn = functools.partial(get_next, train_input_config)
+    if not isinstance(model_config, model_pb2.DetectionModel):
+      raise TypeError('The `model_config` must be a '
+                      'model_pb2.DetectionModel.')
 
     data_augmentation_options = [
         preprocessor_builder.build(step)
         for step in train_config.data_augmentation_options
     ]
-
-    input_queue = trainer.create_input_queue(
-        batch_size_per_clone=train_config.batch_size,
-        create_tensor_dict_fn=create_tensor_dict_fn,
-        batch_queue_capacity=train_config.batch_queue_capacity,
-        num_batch_queue_threads=train_config.num_batch_queue_threads,
-        prefetch_queue_capacity=train_config.prefetch_queue_capacity,
-        data_augmentation_options=data_augmentation_options)
-
-    (images_tuple, image_keys, locations_tuple, classes_tuple, masks_tuple,
-     keypoints_tuple, weights_tuple) = (trainer.get_inputs(
-         input_queue=input_queue, num_classes=num_classes))
-
+    data_augmentation_fn = functools.partial(
+        augment_input_data, data_augmentation_options=data_augmentation_options)
+
+    model = model_builder.build(model_config, is_training=True)
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+
+    transform_data_fn = functools.partial(
+        transform_input_data, model_preprocess_fn=model.preprocess,
+        image_resizer_fn=image_resizer_fn,
+        num_classes=config_util.get_number_of_classes(model_config),
+        data_augmentation_fn=data_augmentation_fn)
+    dataset = dataset_builder.build(
+        train_input_config,
+        transform_input_data_fn=transform_data_fn,
+        batch_size=params['batch_size'] if params else train_config.batch_size,
+        max_num_boxes=train_config.max_number_of_boxes,
+        num_classes=config_util.get_number_of_classes(model_config),
+        spatial_image_shape=config_util.get_spatial_image_size(
+            image_resizer_config))
+    tensor_dict = dataset_util.make_initializable_iterator(dataset).get_next()
+
+    hash_from_source_id = tf.string_to_hash_bucket_fast(
+        tensor_dict[fields.InputDataFields.source_id], HASH_BINS)
     features = {
-        FEATURES_IMAGE: list(images_tuple),
-        FEATURES_KEY: list(image_keys)
+        fields.InputDataFields.image: tensor_dict[fields.InputDataFields.image],
+        HASH_KEY: tf.cast(hash_from_source_id, tf.int32),
+        fields.InputDataFields.true_image_shape: tensor_dict[
+            fields.InputDataFields.true_image_shape]
     }
+
     labels = {
-        'locations_list': list(locations_tuple),
-        'classes_list': list(classes_tuple)
+        fields.InputDataFields.num_groundtruth_boxes: tensor_dict[
+            fields.InputDataFields.num_groundtruth_boxes],
+        fields.InputDataFields.groundtruth_boxes: tensor_dict[
+            fields.InputDataFields.groundtruth_boxes],
+        fields.InputDataFields.groundtruth_classes: tensor_dict[
+            fields.InputDataFields.groundtruth_classes],
+        fields.InputDataFields.groundtruth_weights: tensor_dict[
+            fields.InputDataFields.groundtruth_weights]
     }
-
-    # Make sure that there are no tuple elements with None.
-    if all(masks is not None for masks in masks_tuple):
-      labels['masks_list'] = list(masks_tuple)
-    if all(keypoints is not None for keypoints in keypoints_tuple):
-      labels['keypoints_list'] = list(keypoints_tuple)
-    if all((elem is not None for elem in weights_tuple)):
-      labels[fields.InputDataFields.groundtruth_weights] = list(weights_tuple)
+    if fields.InputDataFields.groundtruth_keypoints in tensor_dict:
+      labels[fields.InputDataFields.groundtruth_keypoints] = tensor_dict[
+          fields.InputDataFields.groundtruth_keypoints]
+    if fields.InputDataFields.groundtruth_instance_masks in tensor_dict:
+      labels[fields.InputDataFields.groundtruth_instance_masks] = tensor_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
 
     return features, labels
 
   return _train_input_fn
 
 
-def create_eval_input_fn(num_classes, eval_config, eval_input_config):
+def create_eval_input_fn(eval_config, eval_input_config, model_config):
   """Creates an eval `input` function for `Estimator`.
 
   Args:
-    num_classes: Number of classes, which does not include a background
-      category.
     eval_config: An eval_pb2.EvalConfig.
     eval_input_config: An input_reader_pb2.InputReader.
+    model_config: A model_pb2.DetectionModel.
 
   Returns:
     `input_fn` for `Estimator` in EVAL mode.
   """
 
-  def _eval_input_fn():
+  def _eval_input_fn(params=None):
     """Returns `features` and `labels` tensor dictionaries for evaluation.
 
+    Args:
+      params: Parameter dictionary passed from the estimator.
+
     Returns:
       features: Dictionary of feature tensors.
-        features['images'] is a [1, H, W, C] float32 tensor.
-        features['key'] is a string tensor representing a unique identifier for
-          the image.
+        features[fields.InputDataFields.image] is a [1, H, W, C] float32 tensor
+          with preprocessed images.
+        features[HASH_KEY] is a [1] int32 tensor representing unique
+          identifiers for the images.
+        features[fields.InputDataFields.true_image_shape] is a [1, 3]
+          int32 tensor representing the true image shapes, as preprocessed
+          images could be padded.
+        features[fields.InputDataFields.original_image] is a [1, H', W', C]
+          float32 tensor with the original image.
       labels: Dictionary of groundtruth tensors.
-        labels['locations_list'] is a list of 1 [num_boxes, 4] float32 tensors
-          containing the corners of the groundtruth boxes.
-        labels['classes_list'] is a list of 1 [num_boxes, num_classes] float32
-          padded one-hot tensors of classes.
-        labels['masks_list'] is an (optional) list of 1 [num_boxes, H, W]
-          float32 tensors containing only binary values, which represent
-          instance masks for objects if present in the dataset. Else returns
-          None.
-        labels['image_id_list'] is a list of 1 string tensors containing the
-          original image id.
-        labels['area_list'] is a list of 1 [num_boxes] float32 tensors
-          containing object mask area in pixels squared.
-        labels['is_crowd_list'] is a list of 1 [num_boxes] bool tensors
-          indicating if the boxes enclose a crowd.
-        labels['difficult_list'] is a list of 1 [num_boxes] bool tensors
-          indicating if the boxes represent `difficult` instances.
+        labels[fields.InputDataFields.groundtruth_boxes] is a [1, num_boxes, 4]
+          float32 tensor containing the corners of the groundtruth boxes.
+        labels[fields.InputDataFields.groundtruth_classes] is a
+          [num_boxes, num_classes] float32 one-hot tensor of classes.
+        labels[fields.InputDataFields.groundtruth_area] is a [1, num_boxes]
+          float32 tensor containing object areas.
+        labels[fields.InputDataFields.groundtruth_is_crowd] is a [1, num_boxes]
+          bool tensor indicating if the boxes enclose a crowd.
+        labels[fields.InputDataFields.groundtruth_difficult] is a [1, num_boxes]
+          int32 tensor indicating if the boxes represent difficult instances.
+        -- Optional --
+        labels[fields.InputDataFields.groundtruth_instance_masks] is a
+          [1, num_boxes, H, W] float32 tensor containing only binary values,
+          which represent instance masks for objects.
 
     Raises:
       TypeError: if the `eval_config` or `eval_input_config` are not of the
         correct type.
     """
+    del params
     if not isinstance(eval_config, eval_pb2.EvalConfig):
       raise TypeError('For eval mode, the `eval_config` must be a '
-                      'eval_pb2.EvalConfig.')
+                      'train_pb2.EvalConfig.')
     if not isinstance(eval_input_config, input_reader_pb2.InputReader):
       raise TypeError('The `eval_input_config` must be a '
                       'input_reader_pb2.InputReader.')
+    if not isinstance(model_config, model_pb2.DetectionModel):
+      raise TypeError('The `model_config` must be a '
+                      'model_pb2.DetectionModel.')
+
+    num_classes = config_util.get_number_of_classes(model_config)
+    model = model_builder.build(model_config, is_training=False)
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
+
+    transform_data_fn = functools.partial(
+        transform_input_data, model_preprocess_fn=model.preprocess,
+        image_resizer_fn=image_resizer_fn,
+        num_classes=num_classes,
+        data_augmentation_fn=None,
+        retain_original_image=True)
+    dataset = dataset_builder.build(eval_input_config,
+                                    transform_input_data_fn=transform_data_fn)
+    input_dict = dataset_util.make_initializable_iterator(dataset).get_next()
+
+    hash_from_source_id = tf.string_to_hash_bucket_fast(
+        input_dict[fields.InputDataFields.source_id], HASH_BINS)
+    features = {
+        fields.InputDataFields.image:
+            input_dict[fields.InputDataFields.image],
+        fields.InputDataFields.original_image:
+            input_dict[fields.InputDataFields.original_image],
+        HASH_KEY: tf.cast(hash_from_source_id, tf.int32),
+        fields.InputDataFields.true_image_shape:
+            input_dict[fields.InputDataFields.true_image_shape]
+    }
 
-    input_dict = dataset_util.make_initializable_iterator(
-        dataset_builder.build(eval_input_config)).get_next()
-    prefetch_queue = prefetcher.prefetch(input_dict, capacity=500)
-    input_dict = prefetch_queue.dequeue()
-    original_image = tf.to_float(
-        tf.expand_dims(input_dict[fields.InputDataFields.image], 0))
-    features = {}
-    features[FEATURES_IMAGE] = original_image
-    features[FEATURES_KEY] = input_dict[fields.InputDataFields.source_id]
-
-    labels = {}
-    labels['locations_list'] = [
-        input_dict[fields.InputDataFields.groundtruth_boxes]
-    ]
-    classes_gt = tf.cast(input_dict[fields.InputDataFields.groundtruth_classes],
-                         tf.int32)
-    classes_gt -= 1  # Remove the label id offset.
-    labels['classes_list'] = [
-        util_ops.padded_one_hot_encoding(
-            indices=classes_gt, depth=num_classes, left_pad=0)
-    ]
-    labels['image_id_list'] = [input_dict[fields.InputDataFields.source_id]]
-    labels['area_list'] = [input_dict[fields.InputDataFields.groundtruth_area]]
-    labels['is_crowd_list'] = [
-        input_dict[fields.InputDataFields.groundtruth_is_crowd]
-    ]
-    labels['difficult_list'] = [
-        input_dict[fields.InputDataFields.groundtruth_difficult]
-    ]
+    labels = {
+        fields.InputDataFields.groundtruth_boxes:
+            input_dict[fields.InputDataFields.groundtruth_boxes],
+        fields.InputDataFields.groundtruth_classes:
+            input_dict[fields.InputDataFields.groundtruth_classes],
+        fields.InputDataFields.groundtruth_area:
+            input_dict[fields.InputDataFields.groundtruth_area],
+        fields.InputDataFields.groundtruth_is_crowd:
+            input_dict[fields.InputDataFields.groundtruth_is_crowd],
+        fields.InputDataFields.groundtruth_difficult:
+            tf.cast(input_dict[fields.InputDataFields.groundtruth_difficult],
+                    tf.int32)
+    }
     if fields.InputDataFields.groundtruth_instance_masks in input_dict:
-      labels['masks_list'] = [
-          input_dict[fields.InputDataFields.groundtruth_instance_masks]
-      ]
+      labels[fields.InputDataFields.groundtruth_instance_masks] = input_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
+
+    # Add a batch dimension to the tensors.
+    features = {
+        key: tf.expand_dims(features[key], axis=0)
+        for key, feature in features.items()
+    }
+    labels = {
+        key: tf.expand_dims(labels[key], axis=0)
+        for key, label in labels.items()
+    }
 
     return features, labels
 
   return _eval_input_fn
 
 
-def create_predict_input_fn():
+def create_predict_input_fn(model_config):
   """Creates a predict `input` function for `Estimator`.
 
+  Args:
+    model_config: A model_pb2.DetectionModel.
+
   Returns:
     `input_fn` for `Estimator` in PREDICT mode.
   """
 
-  def _predict_input_fn():
+  def _predict_input_fn(params=None):
     """Decodes serialized tf.Examples and returns `ServingInputReceiver`.
 
+    Args:
+      params: Parameter dictionary passed from the estimator.
+
     Returns:
       `ServingInputReceiver`.
     """
+    del params
     example = tf.placeholder(dtype=tf.string, shape=[], name='input_feature')
 
-    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)
+    num_classes = config_util.get_number_of_classes(model_config)
+    model = model_builder.build(model_config, is_training=False)
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    image_resizer_fn = image_resizer_builder.build(image_resizer_config)
 
-    input_dict = decoder.decode(example)
+    transform_fn = functools.partial(
+        transform_input_data, model_preprocess_fn=model.preprocess,
+        image_resizer_fn=image_resizer_fn,
+        num_classes=num_classes,
+        data_augmentation_fn=None)
+
+    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)
+    input_dict = transform_fn(decoder.decode(example))
     images = tf.to_float(input_dict[fields.InputDataFields.image])
     images = tf.expand_dims(images, axis=0)
 
     return tf.estimator.export.ServingInputReceiver(
-        features={FEATURES_IMAGE: images},
+        features={fields.InputDataFields.image: images},
         receiver_tensors={SERVING_FED_EXAMPLE_KEY: example})
 
   return _predict_input_fn
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index bea0fad9..3c3c36ed 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -18,11 +18,14 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import functools
 import os
 
+import numpy as np
 import tensorflow as tf
 
 from object_detection import inputs
+from object_detection.core import preprocessor
 from object_detection.core import standard_fields as fields
 from object_detection.utils import config_util
 
@@ -52,148 +55,516 @@ def _get_configs_for_model(model_name):
 
 class InputsTest(tf.test.TestCase):
 
-  def _assert_training_inputs(self, features, labels, num_classes, batch_size):
-    self.assertEqual(batch_size, len(features['images']))
-    self.assertEqual(batch_size, len(features['key']))
-    self.assertEqual(batch_size, len(labels['locations_list']))
-    self.assertEqual(batch_size, len(labels['classes_list']))
-    for i in range(batch_size):
-      image = features['images'][i]
-      key = features['key'][i]
-      locations_list = labels['locations_list'][i]
-      classes_list = labels['classes_list'][i]
-      weights_list = labels[fields.InputDataFields.groundtruth_weights][i]
-      self.assertEqual([1, None, None, 3], image.shape.as_list())
-      self.assertEqual(tf.float32, image.dtype)
-      self.assertEqual(tf.string, key.dtype)
-      self.assertEqual([None, 4], locations_list.shape.as_list())
-      self.assertEqual(tf.float32, locations_list.dtype)
-      self.assertEqual([None, num_classes], classes_list.shape.as_list())
-      self.assertEqual(tf.float32, classes_list.dtype)
-      self.assertEqual([None], weights_list.shape.as_list())
-      self.assertEqual(tf.float32, weights_list.dtype)
-
-  def _assert_eval_inputs(self, features, labels, num_classes):
-    self.assertEqual(1, len(labels['locations_list']))
-    self.assertEqual(1, len(labels['classes_list']))
-    self.assertEqual(1, len(labels['image_id_list']))
-    self.assertEqual(1, len(labels['area_list']))
-    self.assertEqual(1, len(labels['is_crowd_list']))
-    self.assertEqual(1, len(labels['difficult_list']))
-    image = features['images']
-    key = features['key']
-    locations_list = labels['locations_list'][0]
-    classes_list = labels['classes_list'][0]
-    image_id_list = labels['image_id_list'][0]
-    area_list = labels['area_list'][0]
-    is_crowd_list = labels['is_crowd_list'][0]
-    difficult_list = labels['difficult_list'][0]
-    self.assertEqual([1, None, None, 3], image.shape.as_list())
-    self.assertEqual(tf.float32, image.dtype)
-    self.assertEqual(tf.string, key.dtype)
-    self.assertEqual([None, 4], locations_list.shape.as_list())
-    self.assertEqual(tf.float32, locations_list.dtype)
-    self.assertEqual([None, num_classes], classes_list.shape.as_list())
-    self.assertEqual(tf.float32, classes_list.dtype)
-    self.assertEqual(tf.string, image_id_list.dtype)
-    self.assertEqual(tf.float32, area_list.dtype)
-    self.assertEqual(tf.bool, is_crowd_list.dtype)
-    self.assertEqual(tf.int64, difficult_list.dtype)
-
   def test_faster_rcnn_resnet50_train_input(self):
     """Tests the training input function for FasterRcnnResnet50."""
     configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
-    classes = 37
-    batch_size = configs['train_config'].batch_size
+    configs['train_config'].unpad_groundtruth_tensors = True
+    model_config = configs['model']
+    model_config.faster_rcnn.num_classes = 37
     train_input_fn = inputs.create_train_input_fn(
-        classes, configs['train_config'], configs['train_input_config'])
+        configs['train_config'], configs['train_input_config'], model_config)
     features, labels = train_input_fn()
-    self._assert_training_inputs(features, labels, classes, batch_size)
+
+    self.assertAllEqual([None, None, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual([],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [None, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [None, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [None],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_weights].dtype)
 
   def test_faster_rcnn_resnet50_eval_input(self):
     """Tests the eval input function for FasterRcnnResnet50."""
     configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
-    classes = 37
-    eval_input_fn = inputs.create_eval_input_fn(classes, configs['eval_config'],
-                                                configs['eval_input_config'])
+    model_config = configs['model']
+    model_config.faster_rcnn.num_classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(
+        configs['eval_config'], configs['eval_input_config'], model_config)
     features, labels = eval_input_fn()
-    self._assert_eval_inputs(features, labels, classes)
+
+    self.assertAllEqual([1, None, None, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual(
+        [1, None, None, 3],
+        features[fields.InputDataFields.original_image].shape.as_list())
+    self.assertEqual(tf.uint8,
+                     features[fields.InputDataFields.original_image].dtype)
+    self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [1, None, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [1, None, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_area].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_area].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())
+    self.assertEqual(
+        tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())
+    self.assertEqual(
+        tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
 
   def test_ssd_inceptionV2_train_input(self):
     """Tests the training input function for SSDInceptionV2."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
+    model_config = configs['model']
+    model_config.ssd.num_classes = 37
     batch_size = configs['train_config'].batch_size
     train_input_fn = inputs.create_train_input_fn(
-        classes, configs['train_config'], configs['train_input_config'])
+        configs['train_config'], configs['train_input_config'], model_config)
     features, labels = train_input_fn()
-    self._assert_training_inputs(features, labels, classes, batch_size)
+
+    self.assertAllEqual([batch_size, 300, 300, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual([batch_size],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [batch_size],
+        labels[fields.InputDataFields.num_groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.int32,
+                     labels[fields.InputDataFields.num_groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [batch_size, 50, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [batch_size, 50, model_config.ssd.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [batch_size, 50],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_weights].dtype)
 
   def test_ssd_inceptionV2_eval_input(self):
     """Tests the eval input function for SSDInceptionV2."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
-    eval_input_fn = inputs.create_eval_input_fn(classes, configs['eval_config'],
-                                                configs['eval_input_config'])
+    model_config = configs['model']
+    model_config.ssd.num_classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(
+        configs['eval_config'], configs['eval_input_config'], model_config)
     features, labels = eval_input_fn()
-    self._assert_eval_inputs(features, labels, classes)
+
+    self.assertAllEqual([1, 300, 300, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual(
+        [1, None, None, 3],
+        features[fields.InputDataFields.original_image].shape.as_list())
+    self.assertEqual(tf.uint8,
+                     features[fields.InputDataFields.original_image].dtype)
+    self.assertAllEqual([1], features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [1, None, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [1, None, model_config.ssd.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_area].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_area].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_is_crowd].shape.as_list())
+    self.assertEqual(
+        tf.bool, labels[fields.InputDataFields.groundtruth_is_crowd].dtype)
+    self.assertAllEqual(
+        [1, None],
+        labels[fields.InputDataFields.groundtruth_difficult].shape.as_list())
+    self.assertEqual(
+        tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
 
   def test_predict_input(self):
     """Tests the predict input function."""
-    predict_input_fn = inputs.create_predict_input_fn()
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    predict_input_fn = inputs.create_predict_input_fn(
+        model_config=configs['model'])
     serving_input_receiver = predict_input_fn()
 
-    image = serving_input_receiver.features['images']
+    image = serving_input_receiver.features[fields.InputDataFields.image]
     receiver_tensors = serving_input_receiver.receiver_tensors[
-        'serialized_example']
-    self.assertEqual([1, None, None, 3], image.shape.as_list())
+        inputs.SERVING_FED_EXAMPLE_KEY]
+    self.assertEqual([1, 300, 300, 3], image.shape.as_list())
     self.assertEqual(tf.float32, image.dtype)
     self.assertEqual(tf.string, receiver_tensors.dtype)
 
   def test_error_with_bad_train_config(self):
     """Tests that a TypeError is raised with improper train config."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
+    configs['model'].ssd.num_classes = 37
     train_input_fn = inputs.create_train_input_fn(
-        num_classes=classes,
         train_config=configs['eval_config'],  # Expecting `TrainConfig`.
-        train_input_config=configs['train_input_config'])
+        train_input_config=configs['train_input_config'],
+        model_config=configs['model'])
     with self.assertRaises(TypeError):
       train_input_fn()
 
   def test_error_with_bad_train_input_config(self):
     """Tests that a TypeError is raised with improper train input config."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
+    configs['model'].ssd.num_classes = 37
+    train_input_fn = inputs.create_train_input_fn(
+        train_config=configs['train_config'],
+        train_input_config=configs['model'],  # Expecting `InputReader`.
+        model_config=configs['model'])
+    with self.assertRaises(TypeError):
+      train_input_fn()
+
+  def test_error_with_bad_train_model_config(self):
+    """Tests that a TypeError is raised with improper train model config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    configs['model'].ssd.num_classes = 37
     train_input_fn = inputs.create_train_input_fn(
-        num_classes=classes,
         train_config=configs['train_config'],
-        train_input_config=configs['model'])  # Expecting `InputReader`.
+        train_input_config=configs['train_input_config'],
+        model_config=configs['train_config'])  # Expecting `DetectionModel`.
     with self.assertRaises(TypeError):
       train_input_fn()
 
   def test_error_with_bad_eval_config(self):
     """Tests that a TypeError is raised with improper eval config."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
+    configs['model'].ssd.num_classes = 37
     eval_input_fn = inputs.create_eval_input_fn(
-        num_classes=classes,
         eval_config=configs['train_config'],  # Expecting `EvalConfig`.
-        eval_input_config=configs['eval_input_config'])
+        eval_input_config=configs['eval_input_config'],
+        model_config=configs['model'])
     with self.assertRaises(TypeError):
       eval_input_fn()
 
   def test_error_with_bad_eval_input_config(self):
     """Tests that a TypeError is raised with improper eval input config."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
-    classes = 37
+    configs['model'].ssd.num_classes = 37
     eval_input_fn = inputs.create_eval_input_fn(
-        num_classes=classes,
         eval_config=configs['eval_config'],
-        eval_input_config=configs['model'])  # Expecting `InputReader`.
+        eval_input_config=configs['model'],  # Expecting `InputReader`.
+        model_config=configs['model'])
     with self.assertRaises(TypeError):
       eval_input_fn()
 
+  def test_error_with_bad_eval_model_config(self):
+    """Tests that a TypeError is raised with improper eval model config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    configs['model'].ssd.num_classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(
+        eval_config=configs['eval_config'],
+        eval_input_config=configs['eval_input_config'],
+        model_config=configs['eval_config'])  # Expecting `DetectionModel`.
+    with self.assertRaises(TypeError):
+      eval_input_fn()
+
+
+class DataAugmentationFnTest(tf.test.TestCase):
+
+  def test_apply_image_and_box_augmentation(self):
+    data_augmentation_options = [
+        (preprocessor.resize_image, {
+            'new_height': 20,
+            'new_width': 20,
+            'method': tf.image.ResizeMethod.NEAREST_NEIGHBOR
+        }),
+        (preprocessor.scale_boxes_to_pixel_coordinates, {}),
+    ]
+    data_augmentation_fn = functools.partial(
+        inputs.augment_input_data,
+        data_augmentation_options=data_augmentation_options)
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[.5, .5, 1., 1.]], np.float32))
+    }
+    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)
+    with self.test_session() as sess:
+      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)
+
+    self.assertAllEqual(
+        augmented_tensor_dict_out[fields.InputDataFields.image].shape,
+        [20, 20, 3]
+    )
+    self.assertAllClose(
+        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_boxes],
+        [[10, 10, 20, 20]]
+    )
+
+  def test_include_masks_in_data_augmentation(self):
+    data_augmentation_options = [
+        (preprocessor.resize_image, {
+            'new_height': 20,
+            'new_width': 20,
+            'method': tf.image.ResizeMethod.NEAREST_NEIGHBOR
+        })
+    ]
+    data_augmentation_fn = functools.partial(
+        inputs.augment_input_data,
+        data_augmentation_options=data_augmentation_options)
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_instance_masks:
+            tf.constant(np.zeros([2, 10, 10], np.uint8))
+    }
+    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)
+    with self.test_session() as sess:
+      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)
+
+    self.assertAllEqual(
+        augmented_tensor_dict_out[fields.InputDataFields.image].shape,
+        [20, 20, 3])
+    self.assertAllEqual(augmented_tensor_dict_out[
+        fields.InputDataFields.groundtruth_instance_masks].shape, [2, 20, 20])
+
+  def test_include_keypoints_in_data_augmentation(self):
+    data_augmentation_options = [
+        (preprocessor.resize_image, {
+            'new_height': 20,
+            'new_width': 20,
+            'method': tf.image.ResizeMethod.NEAREST_NEIGHBOR
+        }),
+        (preprocessor.scale_boxes_to_pixel_coordinates, {}),
+    ]
+    data_augmentation_fn = functools.partial(
+        inputs.augment_input_data,
+        data_augmentation_options=data_augmentation_options)
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(10, 10, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[.5, .5, 1., 1.]], np.float32)),
+        fields.InputDataFields.groundtruth_keypoints:
+            tf.constant(np.array([[[0.5, 1.0], [0.5, 0.5]]], np.float32))
+    }
+    augmented_tensor_dict = data_augmentation_fn(tensor_dict=tensor_dict)
+    with self.test_session() as sess:
+      augmented_tensor_dict_out = sess.run(augmented_tensor_dict)
+
+    self.assertAllEqual(
+        augmented_tensor_dict_out[fields.InputDataFields.image].shape,
+        [20, 20, 3]
+    )
+    self.assertAllClose(
+        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_boxes],
+        [[10, 10, 20, 20]]
+    )
+    self.assertAllClose(
+        augmented_tensor_dict_out[fields.InputDataFields.groundtruth_keypoints],
+        [[[10, 20], [10, 10]]]
+    )
+
+
+def _fake_model_preprocessor_fn(image):
+  return (image, tf.expand_dims(tf.shape(image)[1:], axis=0))
+
+
+def _fake_image_resizer_fn(image, mask):
+  return (image, mask, tf.shape(image))
+
+
+class DataTransformationFnTest(tf.test.TestCase):
+
+  def test_returns_correct_class_label_encodings(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[0, 0, 1, 1], [.5, .5, 1, 1]], np.float32)),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes)
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_classes],
+        [[0, 0, 1], [1, 0, 0]])
+
+  def test_returns_correct_merged_boxes(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_boxes:
+            tf.constant(np.array([[.5, .5, 1, 1], [.5, .5, 1, 1]], np.float32)),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes,
+        merge_multiple_boxes=True)
+
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_boxes],
+        [[.5, .5, 1., 1.]])
+    self.assertAllClose(
+        transformed_inputs[fields.InputDataFields.groundtruth_classes],
+        [[1, 0, 1]])
+
+  def test_returns_resized_masks(self):
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np.random.rand(4, 4, 3).astype(np.float32)),
+        fields.InputDataFields.groundtruth_instance_masks:
+            tf.constant(np.random.rand(2, 4, 4).astype(np.float32)),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+    def fake_image_resizer_fn(image, masks):
+      resized_image = tf.image.resize_images(image, [8, 8])
+      resized_masks = tf.transpose(
+          tf.image.resize_images(tf.transpose(masks, [1, 2, 0]), [8, 8]),
+          [2, 0, 1])
+      return resized_image, resized_masks, tf.shape(resized_image)
+
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=fake_image_resizer_fn,
+        num_classes=num_classes)
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllEqual(transformed_inputs[
+        fields.InputDataFields.groundtruth_instance_masks].shape, [2, 8, 8])
+
+  def test_applies_model_preprocess_fn_to_image_tensor(self):
+    np_image = np.random.randint(256, size=(4, 4, 3))
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np_image),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+    def fake_model_preprocessor_fn(image):
+      return (image / 255., tf.expand_dims(tf.shape(image)[1:], axis=0))
+
+    num_classes = 3
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes)
+
+    with self.test_session() as sess:
+      transformed_inputs = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+    self.assertAllClose(transformed_inputs[fields.InputDataFields.image],
+                        np_image / 255.)
+    self.assertAllClose(transformed_inputs[fields.InputDataFields.
+                                           true_image_shape],
+                        [4, 4, 3])
+
+  def test_applies_data_augmentation_fn_to_tensor_dict(self):
+    np_image = np.random.randint(256, size=(4, 4, 3))
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np_image),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+    def add_one_data_augmentation_fn(tensor_dict):
+      return {key: value + 1 for key, value in tensor_dict.items()}
+
+    num_classes = 4
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=_fake_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes,
+        data_augmentation_fn=add_one_data_augmentation_fn)
+    with self.test_session() as sess:
+      augmented_tensor_dict = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+
+    self.assertAllEqual(augmented_tensor_dict[fields.InputDataFields.image],
+                        np_image + 1)
+    self.assertAllEqual(
+        augmented_tensor_dict[fields.InputDataFields.groundtruth_classes],
+        [[0, 0, 0, 1], [0, 1, 0, 0]])
+
+  def test_applies_data_augmentation_fn_before_model_preprocess_fn(self):
+    np_image = np.random.randint(256, size=(4, 4, 3))
+    tensor_dict = {
+        fields.InputDataFields.image:
+            tf.constant(np_image),
+        fields.InputDataFields.groundtruth_classes:
+            tf.constant(np.array([3, 1], np.int32))
+    }
+    def mul_two_model_preprocessor_fn(image):
+      return (image * 2, tf.expand_dims(tf.shape(image)[1:], axis=0))
+    def add_five_to_image_data_augmentation_fn(tensor_dict):
+      tensor_dict[fields.InputDataFields.image] += 5
+      return tensor_dict
+
+    num_classes = 4
+    input_transformation_fn = functools.partial(
+        inputs.transform_input_data,
+        model_preprocess_fn=mul_two_model_preprocessor_fn,
+        image_resizer_fn=_fake_image_resizer_fn,
+        num_classes=num_classes,
+        data_augmentation_fn=add_five_to_image_data_augmentation_fn)
+    with self.test_session() as sess:
+      augmented_tensor_dict = sess.run(
+          input_transformation_fn(tensor_dict=tensor_dict))
+
+    self.assertAllEqual(augmented_tensor_dict[fields.InputDataFields.image],
+                        (np_image + 5) * 2)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/matchers/argmax_matcher.py b/research/object_detection/matchers/argmax_matcher.py
index 2407293b..d397ff41 100644
--- a/research/object_detection/matchers/argmax_matcher.py
+++ b/research/object_detection/matchers/argmax_matcher.py
@@ -55,7 +55,8 @@ class ArgMaxMatcher(matcher.Matcher):
                matched_threshold,
                unmatched_threshold=None,
                negatives_lower_than_unmatched=True,
-               force_match_for_each_row=False):
+               force_match_for_each_row=False,
+               use_matmul_gather=False):
     """Construct ArgMaxMatcher.
 
     Args:
@@ -74,11 +75,15 @@ class ArgMaxMatcher(matcher.Matcher):
         at least one column (which is not guaranteed otherwise if the
         matched_threshold is high). Defaults to False. See
         argmax_matcher_test.testMatcherForceMatch() for an example.
+      use_matmul_gather: Force constructed match objects to use matrix
+        multiplication based gather instead of standard tf.gather.
+        (Default: False).
 
     Raises:
       ValueError: if unmatched_threshold is set but matched_threshold is not set
         or if unmatched_threshold > matched_threshold.
     """
+    super(ArgMaxMatcher, self).__init__(use_matmul_gather=use_matmul_gather)
     if (matched_threshold is None) and (unmatched_threshold is not None):
       raise ValueError('Need to also define matched_threshold when'
                        'unmatched_threshold is defined')
diff --git a/research/object_detection/matchers/bipartite_matcher.py b/research/object_detection/matchers/bipartite_matcher.py
index 640b21e7..f995e35a 100644
--- a/research/object_detection/matchers/bipartite_matcher.py
+++ b/research/object_detection/matchers/bipartite_matcher.py
@@ -24,6 +24,17 @@ from object_detection.core import matcher
 class GreedyBipartiteMatcher(matcher.Matcher):
   """Wraps a Tensorflow greedy bipartite matcher."""
 
+  def __init__(self, use_matmul_gather=False):
+    """Constructs a Matcher.
+
+    Args:
+      use_matmul_gather: Force constructed match objects to use matrix
+        multiplication based gather instead of standard tf.gather.
+        (Default: False).
+    """
+    super(GreedyBipartiteMatcher, self).__init__(
+        use_matmul_gather=use_matmul_gather)
+
   def _match(self, similarity_matrix, num_valid_rows=-1):
     """Bipartite matches a collection rows and columns. A greedy bi-partite.
 
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 5d4a099b..488a55fa 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -251,7 +251,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                second_stage_classification_loss,
                second_stage_mask_prediction_loss_weight=1.0,
                hard_example_miner=None,
-               parallel_iterations=16):
+               parallel_iterations=16,
+               add_summaries=True):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -355,12 +356,17 @@ class FasterRCNNMetaArch(model.DetectionModel):
       hard_example_miner:  A losses.HardExampleMiner object (can be None).
       parallel_iterations: (Optional) The number of iterations allowed to run
         in parallel for calls to tf.map_fn.
+      add_summaries: boolean (default: True) controlling whether summary ops
+        should be added to tensorflow graph.
+
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
         training time.
       ValueError: If first_stage_anchor_generator is not of type
         grid_anchor_generator.GridAnchorGenerator.
     """
+    # TODO: add_summaries is currently unused. Respect that directive
+    # in the future.
     super(FasterRCNNMetaArch, self).__init__(num_classes=num_classes)
 
     if is_training and second_stage_batch_size > first_stage_max_proposals:
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 411410b1..219dae3b 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -75,7 +75,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                second_stage_classification_loss_weight,
                second_stage_classification_loss,
                hard_example_miner,
-               parallel_iterations=16):
+               parallel_iterations=16,
+               add_summaries=True):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -155,11 +156,16 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       hard_example_miner:  A losses.HardExampleMiner object (can be None).
       parallel_iterations: (Optional) The number of iterations allowed to run
         in parallel for calls to tf.map_fn.
+      add_summaries: boolean (default: True) controlling whether summary ops
+        should be added to tensorflow graph.
+
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
       ValueError: If first_stage_anchor_generator is not of type
         grid_anchor_generator.GridAnchorGenerator.
     """
+    # TODO: add_summaries is currently unused. Respect that directive
+    # in the future.
     super(RFCNMetaArch, self).__init__(
         is_training,
         num_classes,
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 235db969..d76cc91b 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -44,7 +44,8 @@ class SSDFeatureExtractor(object):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """Constructor.
 
     Args:
@@ -61,6 +62,7 @@ class SSDFeatureExtractor(object):
       reuse_weights: whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
     """
     self._is_training = is_training
     self._depth_multiplier = depth_multiplier
@@ -70,6 +72,7 @@ class SSDFeatureExtractor(object):
     self._batch_norm_trainable = batch_norm_trainable
     self._reuse_weights = reuse_weights
     self._use_explicit_padding = use_explicit_padding
+    self._use_depthwise = use_depthwise
 
   @abstractmethod
   def preprocess(self, resized_inputs):
@@ -130,7 +133,7 @@ class SSDMetaArch(model.DetectionModel):
                add_summaries=True):
     """SSDMetaArch Constructor.
 
-    TODO: group NMS parameters + score converter into
+    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
     a class and loss parameters into a class and write config protos for
     postprocessing and losses.
 
@@ -330,7 +333,8 @@ class SSDMetaArch(model.DetectionModel):
       feature_maps = self._feature_extractor.extract_features(
           preprocessed_inputs)
     feature_map_spatial_dims = self._get_feature_map_spatial_dims(feature_maps)
-    image_shape = tf.shape(preprocessed_inputs)
+    image_shape = shape_utils.combined_static_and_dynamic_shape(
+        preprocessed_inputs)
     self._anchors = self._anchor_generator.generate(
         feature_map_spatial_dims,
         im_height=image_shape[1],
@@ -472,11 +476,14 @@ class SSDMetaArch(model.DetectionModel):
       keypoints = None
       if self.groundtruth_has_field(fields.BoxListFields.keypoints):
         keypoints = self.groundtruth_lists(fields.BoxListFields.keypoints)
+      weights = None
+      if self.groundtruth_has_field(fields.BoxListFields.weights):
+        weights = self.groundtruth_lists(fields.BoxListFields.weights)
       (batch_cls_targets, batch_cls_weights, batch_reg_targets,
        batch_reg_weights, match_list) = self._assign_targets(
            self.groundtruth_lists(fields.BoxListFields.boxes),
            self.groundtruth_lists(fields.BoxListFields.classes),
-           keypoints)
+           keypoints, weights)
       if self._add_summaries:
         self._summarize_input(
             self.groundtruth_lists(fields.BoxListFields.boxes), match_list)
@@ -539,7 +546,8 @@ class SSDMetaArch(model.DetectionModel):
                                               'NegativeAnchorLossCDF')
 
   def _assign_targets(self, groundtruth_boxes_list, groundtruth_classes_list,
-                      groundtruth_keypoints_list=None):
+                      groundtruth_keypoints_list=None,
+                      groundtruth_weights_list=None):
     """Assign groundtruth targets.
 
     Adds a background class to each one-hot encoding of groundtruth classes
@@ -556,6 +564,8 @@ class SSDMetaArch(model.DetectionModel):
         index assumed to map to the first non-background class.
       groundtruth_keypoints_list: (optional) a list of 3-D tensors of shape
         [num_boxes, num_keypoints, 2]
+      groundtruth_weights_list: A list of 1-D tf.float32 tensors of shape
+        [num_boxes] containing weights for groundtruth boxes.
 
     Returns:
       batch_cls_targets: a tensor with shape [batch_size, num_anchors,
@@ -582,7 +592,7 @@ class SSDMetaArch(model.DetectionModel):
         boxlist.add_field(fields.BoxListFields.keypoints, keypoints)
     return target_assigner.batch_assign_targets(
         self._target_assigner, self.anchors, groundtruth_boxlists,
-        groundtruth_classes_with_background_list)
+        groundtruth_classes_with_background_list, groundtruth_weights_list)
 
   def _summarize_input(self, groundtruth_boxes_list, match_list):
     """Creates tensorflow summaries for the input boxes and anchors.
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index 9d63ba3b..e77aa883 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -24,13 +24,17 @@ from object_detection.utils import object_detection_evaluation
 class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
   """Class to evaluate COCO detection metrics."""
 
-  def __init__(self, categories, all_metrics_per_category=False):
+  def __init__(self,
+               categories,
+               include_metrics_per_category=False,
+               all_metrics_per_category=False):
     """Constructor.
 
     Args:
       categories: A list of dicts, each of which has the following keys -
         'id': (required) an integer id uniquely identifying this category.
         'name': (required) string representing category name e.g., 'cat', 'dog'.
+      include_metrics_per_category: If True, include metrics for each category.
       all_metrics_per_category: Whether to include all the summary metrics for
         each category in per_category_ap. Be careful with setting it to true if
         you have more than handful of categories, because it will pollute
@@ -45,6 +49,7 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
     self._category_id_set = set([cat['id'] for cat in self._categories])
     self._annotation_id = 1
     self._metrics = None
+    self._include_metrics_per_category = include_metrics_per_category
     self._all_metrics_per_category = all_metrics_per_category
 
   def clear(self):
@@ -166,7 +171,8 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
       'DetectionBoxes_Recall/AR@100 (large)': average recall for large objects
         with 100 detections.
 
-      2. per_category_ap: category specific results with keys of the form:
+      2. per_category_ap: if include_metrics_per_category is True, category
+      specific results with keys of the form:
       'Precision mAP ByCategory/category' (without the supercategory part if
       no supercategories exist). For backward compatibility
       'PerformanceByCategory' is included in the output regardless of
@@ -183,6 +189,7 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
     box_evaluator = coco_tools.COCOEvalWrapper(
         coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)
     box_metrics, box_per_category_ap = box_evaluator.ComputeMetrics(
+        include_metrics_per_category=self._include_metrics_per_category,
         all_metrics_per_category=self._all_metrics_per_category)
     box_metrics.update(box_per_category_ap)
     box_metrics = {'DetectionBoxes_'+ key: value
@@ -253,9 +260,10 @@ class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
                     'DetectionBoxes_Recall/AR@100 (large)',
                     'DetectionBoxes_Recall/AR@100 (medium)',
                     'DetectionBoxes_Recall/AR@100 (small)']
-    for category_dict in self._categories:
-      metric_names.append('DetectionBoxes_PerformanceByCategory/mAP/' +
-                          category_dict['name'])
+    if self._include_metrics_per_category:
+      for category_dict in self._categories:
+        metric_names.append('DetectionBoxes_PerformanceByCategory/mAP/' +
+                            category_dict['name'])
 
     def first_value_func():
       self._metrics = self.evaluate()
@@ -289,13 +297,14 @@ def _check_mask_type_and_value(array_name, masks):
 class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
   """Class to evaluate COCO detection metrics."""
 
-  def __init__(self, categories):
+  def __init__(self, categories, include_metrics_per_category=False):
     """Constructor.
 
     Args:
       categories: A list of dicts, each of which has the following keys -
         'id': (required) an integer id uniquely identifying this category.
         'name': (required) string representing category name e.g., 'cat', 'dog'.
+      include_metrics_per_category: If True, include metrics for each category.
     """
     super(CocoMaskEvaluator, self).__init__(categories)
     self._image_id_to_mask_shape_map = {}
@@ -304,6 +313,7 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
     self._detection_masks_list = []
     self._category_id_set = set([cat['id'] for cat in self._categories])
     self._annotation_id = 1
+    self._include_metrics_per_category = include_metrics_per_category
 
   def clear(self):
     """Clears the state to prepare for a fresh evaluation."""
@@ -438,7 +448,8 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
       'Recall/AR@100 (large)': average recall for large objects with 100
         detections
 
-      2. per_category_ap: category specific results with keys of the form:
+      2. per_category_ap: if include_metrics_per_category is True, category
+      specific results with keys of the form:
       'Precision mAP ByCategory/category' (without the supercategory part if
       no supercategories exist). For backward compatibility
       'PerformanceByCategory' is included in the output regardless of
@@ -458,7 +469,8 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
     mask_evaluator = coco_tools.COCOEvalWrapper(
         coco_wrapped_groundtruth, coco_wrapped_detection_masks,
         agnostic_mode=False, iou_type='segm')
-    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics()
+    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics(
+        include_metrics_per_category=self._include_metrics_per_category)
     mask_metrics.update(mask_per_category_ap)
     mask_metrics = {'DetectionMasks_'+ key: value
                     for key, value in mask_metrics.iteritems()}
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
index 7a007d7b..2aca297f 100644
--- a/research/object_detection/metrics/coco_evaluation_test.py
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -12,13 +12,12 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Tests for image.understanding.object_detection.metrics.coco_evaluation."""
+"""Tests for tensorflow_models.object_detection.metrics.coco_evaluation."""
 
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import math
 import numpy as np
 import tensorflow as tf
 from object_detection.core import standard_fields
@@ -87,43 +86,6 @@ class CocoDetectionEvaluationTest(tf.test.TestCase):
     metrics = coco_evaluator.evaluate()
     self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
 
-  def testReturnAllMetricsPerCategory(self):
-    """Tests that mAP is calculated correctly on GT and Detections."""
-    category_list = [{'id': 0, 'name': 'person'}]
-    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(
-        category_list, all_metrics_per_category=True)
-    coco_evaluator.add_single_ground_truth_image_info(
-        image_id='image1',
-        groundtruth_dict={
-            standard_fields.InputDataFields.groundtruth_boxes:
-            np.array([[100., 100., 200., 200.]]),
-            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
-        })
-    coco_evaluator.add_single_detected_image_info(
-        image_id='image1',
-        detections_dict={
-            standard_fields.DetectionResultFields.detection_boxes:
-            np.array([[100., 100., 200., 200.]]),
-            standard_fields.DetectionResultFields.detection_scores:
-            np.array([.8]),
-            standard_fields.DetectionResultFields.detection_classes:
-            np.array([1])
-        })
-    metrics = coco_evaluator.evaluate()
-    expected_metrics = [
-        'DetectionBoxes_Recall AR@10 ByCategory/person',
-        'DetectionBoxes_Precision mAP (medium) ByCategory/person',
-        'DetectionBoxes_Precision mAP ByCategory/person',
-        'DetectionBoxes_Precision mAP@.50IOU ByCategory/person',
-        'DetectionBoxes_Precision mAP (small) ByCategory/person',
-        'DetectionBoxes_Precision mAP (large) ByCategory/person',
-        'DetectionBoxes_Recall AR@1 ByCategory/person',
-        'DetectionBoxes_Precision mAP@.75IOU ByCategory/person',
-        'DetectionBoxes_Recall AR@100 ByCategory/person',
-        'DetectionBoxes_Recall AR@100 (medium) ByCategory/person',
-        'DetectionBoxes_Recall AR@100 (large) ByCategory/person']
-    self.assertTrue(set(expected_metrics).issubset(set(metrics)))
-
   def testRejectionOnDuplicateGroundtruth(self):
     """Tests that groundtruth cannot be added more than once for an image."""
     categories = [{'id': 1, 'name': 'cat'},
@@ -279,12 +241,6 @@ class CocoEvaluationPyFuncTest(tf.test.TestCase):
     self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100 (medium)'],
                            -1.0)
     self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100 (small)'], 1.0)
-    self.assertAlmostEqual(metrics[
-        'DetectionBoxes_PerformanceByCategory/mAP/dog'], 1.0)
-    self.assertAlmostEqual(metrics[
-        'DetectionBoxes_PerformanceByCategory/mAP/cat'], 1.0)
-    self.assertTrue(math.isnan(metrics[
-        'DetectionBoxes_PerformanceByCategory/mAP/person']))
     self.assertFalse(coco_evaluator._groundtruth_list)
     self.assertFalse(coco_evaluator._detection_boxes_list)
     self.assertFalse(coco_evaluator._image_ids)
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 851b19f8..5380cb7c 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -189,14 +189,18 @@ class COCOEvalWrapper(cocoeval.COCOeval):
     """Returns list of valid category ids."""
     return self.params.catIds
 
-  def ComputeMetrics(self, all_metrics_per_category=False):
+  def ComputeMetrics(self,
+                     include_metrics_per_category=False,
+                     all_metrics_per_category=False):
     """Computes detection metrics.
 
     Args:
+      include_metrics_per_category: If True, will include metrics per category.
       all_metrics_per_category: If true, include all the summery metrics for
         each category in per_category_ap. Be careful with setting it to true if
         you have more than handful of categories, because it will pollute
         your mldash.
+
     Returns:
       1. summary_metrics: a dictionary holding:
         'Precision/mAP': mean average precision over classes averaged over IOU
@@ -225,6 +229,9 @@ class COCOEvalWrapper(cocoeval.COCOeval):
         output regardless of all_metrics_per_category.
         If evaluating class-agnostic mode, per_category_ap is an empty
         dictionary.
+
+    Raises:
+      ValueError: If category_stats does not exist.
     """
     self.evaluate()
     self.accumulate()
@@ -244,6 +251,10 @@ class COCOEvalWrapper(cocoeval.COCOeval):
         ('Recall/AR@100 (medium)', self.stats[10]),
         ('Recall/AR@100 (large)', self.stats[11])
     ])
+    if not include_metrics_per_category:
+      return summary_metrics, {}
+    if not hasattr(self, 'category_stats'):
+      raise ValueError('Category stats do not exist')
     per_category_ap = OrderedDict([])
     if self.GetAgnosticMode():
       return summary_metrics, per_category_ap
diff --git a/research/object_detection/metrics/coco_tools_test.py b/research/object_detection/metrics/coco_tools_test.py
index c2d1b60c..c845ba11 100644
--- a/research/object_detection/metrics/coco_tools_test.py
+++ b/research/object_detection/metrics/coco_tools_test.py
@@ -12,7 +12,7 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Tests for google3.image.understanding.object_detection.metrics.coco_tools."""
+"""Tests for tensorflow_model.object_detection.metrics.coco_tools."""
 import json
 import os
 import re
diff --git a/research/object_detection/model.py b/research/object_detection/model.py
new file mode 100644
index 00000000..4ec73913
--- /dev/null
+++ b/research/object_detection/model.py
@@ -0,0 +1,490 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Creates and runs `Experiment` for object detection model.
+
+This uses the TF.learn framework to define and run an object detection model
+wrapped in an `Estimator`.
+Note that this module is only compatible with SSD Meta architecture at the
+moment.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+import os
+
+import tensorflow as tf
+
+from google.protobuf import text_format
+from tensorflow.contrib.learn.python.learn import learn_runner
+from tensorflow.contrib.tpu.python.tpu import tpu_optimizer
+from object_detection import eval_util
+from object_detection import inputs
+from object_detection import model_hparams
+from object_detection.builders import model_builder
+from object_detection.builders import optimizer_builder
+from object_detection.core import standard_fields as fields
+from object_detection.metrics import coco_evaluation
+from object_detection.utils import config_util
+from object_detection.utils import label_map_util
+from object_detection.utils import shape_utils
+from object_detection.utils import variables_helper
+from object_detection.utils import visualization_utils as vis_utils
+
+tf.flags.DEFINE_string('model_dir', None, 'Path to output model directory '
+                       'where event and checkpoint files will be written.')
+tf.flags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '
+                       'file.')
+tf.flags.DEFINE_integer('num_train_steps', 500000, 'Number of train steps.')
+tf.flags.DEFINE_integer('num_eval_steps', 10000, 'Number of train steps.')
+FLAGS = tf.flags.FLAGS
+
+
+def _get_groundtruth_data(detection_model, class_agnostic):
+  """Extracts groundtruth data from detection_model.
+
+  Args:
+    detection_model: A `DetectionModel` object.
+    class_agnostic: Whether the detections are class_agnostic.
+
+  Returns:
+    A tuple of:
+    groundtruth: Dictionary with the following fields:
+      'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in
+        normalized coordinates.
+      'groundtruth_classes': [num_boxes] int64 tensor of 1-indexed classes.
+      'groundtruth_masks': 3D float32 tensor of instance masks (if provided in
+        groundtruth)
+    class_agnostic: Boolean indicating whether detections are class agnostic.
+  """
+  input_data_fields = fields.InputDataFields()
+  groundtruth_boxes = detection_model.groundtruth_lists(
+      fields.BoxListFields.boxes)[0]
+  # For class-agnostic models, groundtruth one-hot encodings collapse to all
+  # ones.
+  if class_agnostic:
+    groundtruth_boxes_shape = tf.shape(groundtruth_boxes)
+    groundtruth_classes_one_hot = tf.ones([groundtruth_boxes_shape[0], 1])
+  else:
+    groundtruth_classes_one_hot = detection_model.groundtruth_lists(
+        fields.BoxListFields.classes)[0]
+  label_id_offset = 1  # Applying label id offset (b/63711816)
+  groundtruth_classes = (
+      tf.argmax(groundtruth_classes_one_hot, axis=1) + label_id_offset)
+  groundtruth = {
+      input_data_fields.groundtruth_boxes: groundtruth_boxes,
+      input_data_fields.groundtruth_classes: groundtruth_classes
+  }
+  if detection_model.groundtruth_has_field(fields.BoxListFields.masks):
+    groundtruth[input_data_fields.groundtruth_instance_masks] = (
+        detection_model.groundtruth_lists(fields.BoxListFields.masks)[0])
+  return groundtruth
+
+
+def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
+  """Unstacks all tensors in `tensor_dict` along 0th dimension.
+
+  Unstacks tensor from the tensor dict along 0th dimension and returns a
+  tensor_dict containing values that are lists of unstacked tensors.
+
+  Tensors in the `tensor_dict` are expected to be of one of the three shapes:
+  1. [batch_size]
+  2. [batch_size, height, width, channels]
+  3. [batch_size, num_boxes, d1, d2, ... dn]
+
+  When unpad_tensors is set to true, unstacked tensors of form 3 above are
+  sliced along the `num_boxes` dimension using the value in tensor
+  field.InputDataFields.num_groundtruth_boxes.
+
+  Note that this function has a static list of input data fields and has to be
+  kept in sync with the InputDataFields defined in core/standard_fields.py
+
+  Args:
+    tensor_dict: A dictionary of batched groundtruth tensors.
+    unpad_groundtruth_tensors: Whether to remove padding along `num_boxes`
+      dimension of the groundtruth tensors.
+
+  Returns:
+    A dictionary where the keys are from fields.InputDataFields and values are
+    a list of unstacked (optionally unpadded) tensors.
+
+  Raises:
+    ValueError: If unpad_tensors is True and `tensor_dict` does not contain
+      `num_groundtruth_boxes` tensor.
+  """
+  unbatched_tensor_dict = {key: tf.unstack(tensor)
+                           for key, tensor in tensor_dict.items()}
+  if unpad_groundtruth_tensors:
+    if (fields.InputDataFields.num_groundtruth_boxes not in
+        unbatched_tensor_dict):
+      raise ValueError('`num_groundtruth_boxes` not found in tensor_dict. '
+                       'Keys available: {}'.format(
+                           unbatched_tensor_dict.keys()))
+    unbatched_unpadded_tensor_dict = {}
+    unpad_keys = set([
+        # List of input data fields that are padded along the num_boxes
+        # dimension. This list has to be kept in sync with InputDataFields in
+        # standard_fields.py.
+        fields.InputDataFields.groundtruth_instance_masks,
+        fields.InputDataFields.groundtruth_classes,
+        fields.InputDataFields.groundtruth_boxes,
+        fields.InputDataFields.groundtruth_keypoints,
+        fields.InputDataFields.groundtruth_group_of,
+        fields.InputDataFields.groundtruth_difficult,
+        fields.InputDataFields.groundtruth_is_crowd,
+        fields.InputDataFields.groundtruth_area,
+        fields.InputDataFields.groundtruth_weights
+    ]).intersection(set(unbatched_tensor_dict.keys()))
+
+    for key in unpad_keys:
+      unpadded_tensor_list = []
+      for num_gt, padded_tensor in zip(
+          unbatched_tensor_dict[fields.InputDataFields.num_groundtruth_boxes],
+          unbatched_tensor_dict[key]):
+        tensor_shape = shape_utils.combined_static_and_dynamic_shape(
+            padded_tensor)
+        slice_begin = tf.zeros([len(tensor_shape)], dtype=tf.int32)
+        slice_size = tf.stack(
+            [num_gt] + [-1 if dim is None else dim for dim in tensor_shape[1:]])
+        unpadded_tensor = tf.slice(padded_tensor, slice_begin, slice_size)
+        unpadded_tensor_list.append(unpadded_tensor)
+      unbatched_unpadded_tensor_dict[key] = unpadded_tensor_list
+    unbatched_tensor_dict.update(unbatched_unpadded_tensor_dict)
+
+  return unbatched_tensor_dict
+
+
+def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
+  """Creates a model function for `Estimator`.
+
+  Args:
+    detection_model_fn: Function that returns a `DetectionModel` instance.
+    configs: Dictionary of pipeline config objects.
+    hparams: `HParams` object.
+    use_tpu: Boolean indicating whether model should be constructed for
+        use on TPU.
+
+  Returns:
+    `model_fn` for `Estimator`.
+  """
+  train_config = configs['train_config']
+  eval_input_config = configs['eval_input_config']
+
+  def model_fn(features, labels, mode, params=None):
+    """Constructs the object detection model.
+
+    Args:
+      features: Dictionary of feature tensors, returned from `input_fn`.
+      labels: Dictionary of groundtruth tensors if mode is TRAIN or EVAL,
+        otherwise None.
+      mode: Mode key from tf.estimator.ModeKeys.
+      params: Parameter dictionary passed from the estimator.
+
+    Returns:
+      An `EstimatorSpec` that encapsulates the model and its serving
+        configurations.
+    """
+    params = params or {}
+    total_loss, train_op, detections, export_outputs = None, None, None, None
+    is_training = mode == tf.estimator.ModeKeys.TRAIN
+    detection_model = detection_model_fn(is_training=is_training,
+                                         add_summaries=(not use_tpu))
+    scaffold_fn = None
+
+    if mode == tf.estimator.ModeKeys.TRAIN:
+      labels = unstack_batch(
+          labels,
+          unpad_groundtruth_tensors=train_config.unpad_groundtruth_tensors)
+    elif mode == tf.estimator.ModeKeys.EVAL:
+      labels = unstack_batch(labels, unpad_groundtruth_tensors=False)
+
+    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
+      gt_boxes_list = labels[fields.InputDataFields.groundtruth_boxes]
+      gt_classes_list = labels[fields.InputDataFields.groundtruth_classes]
+      gt_masks_list = None
+      if fields.InputDataFields.groundtruth_instance_masks in labels:
+        gt_masks_list = labels[
+            fields.InputDataFields.groundtruth_instance_masks]
+      gt_keypoints_list = None
+      if fields.InputDataFields.groundtruth_keypoints in labels:
+        gt_keypoints_list = labels[fields.InputDataFields.groundtruth_keypoints]
+      detection_model.provide_groundtruth(
+          groundtruth_boxes_list=gt_boxes_list,
+          groundtruth_classes_list=gt_classes_list,
+          groundtruth_masks_list=gt_masks_list,
+          groundtruth_keypoints_list=gt_keypoints_list)
+
+    preprocessed_images = features[fields.InputDataFields.image]
+    prediction_dict = detection_model.predict(
+        preprocessed_images, features[fields.InputDataFields.true_image_shape])
+    detections = detection_model.postprocess(
+        prediction_dict, features[fields.InputDataFields.true_image_shape])
+
+    if mode == tf.estimator.ModeKeys.TRAIN:
+      if train_config.fine_tune_checkpoint and hparams.load_pretrained:
+        asg_map = detection_model.restore_map(
+            from_detection_checkpoint=train_config.from_detection_checkpoint,
+            load_all_detection_checkpoint_vars=(
+                train_config.load_all_detection_checkpoint_vars))
+        available_var_map = (
+            variables_helper.get_variables_available_in_checkpoint(
+                asg_map, train_config.fine_tune_checkpoint,
+                include_global_step=False))
+        if use_tpu:
+          def tpu_scaffold():
+            tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
+                                          available_var_map)
+            return tf.train.Scaffold()
+          scaffold_fn = tpu_scaffold
+        else:
+          tf.train.init_from_checkpoint(train_config.fine_tune_checkpoint,
+                                        available_var_map)
+
+    if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
+      losses_dict = detection_model.loss(
+          prediction_dict, features[fields.InputDataFields.true_image_shape])
+      losses = [loss_tensor for loss_tensor in losses_dict.itervalues()]
+      total_loss = tf.add_n(losses, name='total_loss')
+
+    if mode == tf.estimator.ModeKeys.TRAIN:
+      global_step = tf.train.get_or_create_global_step()
+      training_optimizer, optimizer_summary_vars = optimizer_builder.build(
+          train_config.optimizer)
+
+      if use_tpu:
+        training_optimizer = tpu_optimizer.CrossShardOptimizer(
+            training_optimizer)
+
+      # Optionally freeze some layers by setting their gradients to be zero.
+      trainable_variables = None
+      if train_config.freeze_variables:
+        trainable_variables = tf.contrib.framework.filter_variables(
+            tf.trainable_variables(),
+            exclude_patterns=train_config.freeze_variables)
+
+      clip_gradients_value = None
+      if train_config.gradient_clipping_by_norm > 0:
+        clip_gradients_value = train_config.gradient_clipping_by_norm
+
+      if not use_tpu:
+        for var in optimizer_summary_vars:
+          tf.summary.scalar(var.op.name, var)
+      summaries = [] if use_tpu else None
+      train_op = tf.contrib.layers.optimize_loss(
+          loss=total_loss,
+          global_step=global_step,
+          learning_rate=None,
+          clip_gradients=clip_gradients_value,
+          optimizer=training_optimizer,
+          variables=trainable_variables,
+          summaries=summaries,
+          name='')  # Preventing scope prefix on all variables.
+
+    if mode == tf.estimator.ModeKeys.PREDICT:
+      export_outputs = {
+          tf.saved_model.signature_constants.PREDICT_METHOD_NAME:
+              tf.estimator.export.PredictOutput(detections)
+      }
+
+    eval_metric_ops = None
+    if mode == tf.estimator.ModeKeys.EVAL:
+      # Detection summaries during eval.
+      class_agnostic = (fields.DetectionResultFields.detection_classes
+                        not in detections)
+      groundtruth = _get_groundtruth_data(detection_model, class_agnostic)
+      eval_dict = eval_util.result_dict_for_single_example(
+          tf.expand_dims(features[fields.InputDataFields.original_image][0], 0),
+          features[inputs.HASH_KEY][0],
+          detections,
+          groundtruth,
+          class_agnostic=class_agnostic,
+          scale_to_absolute=False)
+
+      if class_agnostic:
+        category_index = label_map_util.create_class_agnostic_category_index()
+      else:
+        category_index = label_map_util.create_category_index_from_labelmap(
+            eval_input_config.label_map_path)
+      detection_and_groundtruth = vis_utils.draw_side_by_side_evaluation_image(
+          eval_dict, category_index, max_boxes_to_draw=20, min_score_thresh=0.2)
+      if not use_tpu:
+        tf.summary.image('Detections_Left_Groundtruth_Right',
+                         detection_and_groundtruth)
+
+      # Eval metrics on a single image.
+      detection_fields = fields.DetectionResultFields()
+      input_data_fields = fields.InputDataFields()
+      coco_evaluator = coco_evaluation.CocoDetectionEvaluator(
+          category_index.values())
+      eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(
+          image_id=eval_dict[input_data_fields.key],
+          groundtruth_boxes=eval_dict[input_data_fields.groundtruth_boxes],
+          groundtruth_classes=eval_dict[input_data_fields.groundtruth_classes],
+          detection_boxes=eval_dict[detection_fields.detection_boxes],
+          detection_scores=eval_dict[detection_fields.detection_scores],
+          detection_classes=eval_dict[detection_fields.detection_classes])
+
+    if use_tpu:
+      return tf.contrib.tpu.TPUEstimatorSpec(
+          mode=mode,
+          scaffold_fn=scaffold_fn,
+          predictions=detections,
+          loss=total_loss,
+          train_op=train_op,
+          eval_metrics=eval_metric_ops,
+          export_outputs=export_outputs)
+    else:
+      return tf.estimator.EstimatorSpec(
+          mode=mode,
+          predictions=detections,
+          loss=total_loss,
+          train_op=train_op,
+          eval_metric_ops=eval_metric_ops,
+          export_outputs=export_outputs)
+
+  return model_fn
+
+
+def _build_experiment_fn(train_steps, eval_steps):
+  """Returns a function that creates an `Experiment`."""
+
+  def build_experiment(run_config, hparams):
+    """Builds an `Experiment` from configuration and hyperparameters.
+
+    Args:
+      run_config: A `RunConfig`.
+      hparams: A `HParams`.
+
+    Returns:
+      An `Experiment` object.
+    """
+    return populate_experiment(run_config, hparams, FLAGS.pipeline_config_path,
+                               train_steps, eval_steps)
+
+  return build_experiment
+
+
+def populate_experiment(run_config,
+                        hparams,
+                        pipeline_config_path,
+                        train_steps=None,
+                        eval_steps=None,
+                        model_fn_creator=create_model_fn,
+                        **kwargs):
+  """Populates an `Experiment` object.
+
+  Args:
+    run_config: A `RunConfig`.
+    hparams: A `HParams`.
+    pipeline_config_path: A path to a pipeline config file.
+    train_steps: Number of training steps. If None, the number of training steps
+      is set from the `TrainConfig` proto.
+    eval_steps: Number of evaluation steps per evaluation cycle. If None, the
+      number of evaluation steps is set from the `EvalConfig` proto.
+    model_fn_creator: A function that creates a `model_fn` for `Estimator`.
+      Follows the signature:
+
+      * Args:
+        * `detection_model_fn`: Function that returns `DetectionModel` instance.
+        * `configs`: Dictionary of pipeline config objects.
+        * `hparams`: `HParams` object.
+      * Returns:
+        `model_fn` for `Estimator`.
+
+    **kwargs: Additional keyword arguments for configuration override.
+
+  Returns:
+    An `Experiment` that defines all aspects of training, evaluation, and
+    export.
+  """
+  configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+  configs = config_util.merge_external_params_with_configs(
+      configs,
+      hparams,
+      train_steps=train_steps,
+      eval_steps=eval_steps,
+      **kwargs)
+  model_config = configs['model']
+  train_config = configs['train_config']
+  train_input_config = configs['train_input_config']
+  eval_config = configs['eval_config']
+  eval_input_config = configs['eval_input_config']
+
+  if train_steps is None:
+    train_steps = train_config.num_steps if train_config.num_steps else None
+
+  if eval_steps is None:
+    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+
+  detection_model_fn = functools.partial(
+      model_builder.build, model_config=model_config)
+
+  # Create the input functions for TRAIN/EVAL.
+  train_input_fn = inputs.create_train_input_fn(
+      train_config=train_config,
+      train_input_config=train_input_config,
+      model_config=model_config)
+  eval_input_fn = inputs.create_eval_input_fn(
+      eval_config=eval_config,
+      eval_input_config=eval_input_config,
+      model_config=model_config)
+
+  export_strategies = [
+      tf.contrib.learn.utils.saved_model_export_utils.make_export_strategy(
+          serving_input_fn=inputs.create_predict_input_fn(
+              model_config=model_config))
+  ]
+
+  estimator = tf.estimator.Estimator(
+      model_fn=model_fn_creator(detection_model_fn, configs, hparams),
+      config=run_config)
+
+  if run_config.is_chief:
+    # Store the final pipeline config for traceability.
+    pipeline_config_final = config_util.create_pipeline_proto_from_configs(
+        configs)
+    pipeline_config_final_path = os.path.join(estimator.model_dir,
+                                              'pipeline.config')
+    config_text = text_format.MessageToString(pipeline_config_final)
+    with tf.gfile.Open(pipeline_config_final_path, 'wb') as f:
+      tf.logging.info('Writing as-run pipeline config file to %s',
+                      pipeline_config_final_path)
+      f.write(config_text)
+
+  return tf.contrib.learn.Experiment(
+      estimator=estimator,
+      train_input_fn=train_input_fn,
+      eval_input_fn=eval_input_fn,
+      train_steps=train_steps,
+      eval_steps=eval_steps,
+      export_strategies=export_strategies,
+      eval_delay_secs=120,)
+
+
+def main(unused_argv):
+  tf.flags.mark_flag_as_required('model_dir')
+  tf.flags.mark_flag_as_required('pipeline_config_path')
+  config = tf.contrib.learn.RunConfig(model_dir=FLAGS.model_dir)
+  learn_runner.run(
+      experiment_fn=_build_experiment_fn(FLAGS.num_train_steps,
+                                         FLAGS.num_eval_steps),
+      run_config=config,
+      hparams=model_hparams.create_hparams())
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/object_detection/model_hparams.py b/research/object_detection/model_hparams.py
new file mode 100644
index 00000000..b0d12fce
--- /dev/null
+++ b/research/object_detection/model_hparams.py
@@ -0,0 +1,44 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Hyperparameters for the object detection model in TF.learn.
+
+This file consolidates and documents the hyperparameters used by the model.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def create_hparams(hparams_overrides=None):
+  """Returns hyperparameters, including any flag value overrides.
+
+  Args:
+    hparams_overrides: Optional hparams overrides, represented as a
+      string containing comma-separated hparam_name=value pairs.
+
+  Returns:
+    The hyperparameters as a tf.HParams object.
+  """
+  hparams = tf.contrib.training.HParams(
+      # Whether a fine tuning checkpoint (provided in the pipeline config)
+      # should be loaded for training.
+      load_pretrained=True)
+  # Override any of the preceding hyperparameter values.
+  if hparams_overrides:
+    hparams = hparams.parse(hparams_overrides)
+  return hparams
diff --git a/research/object_detection/model_test.py b/research/object_detection/model_test.py
new file mode 100644
index 00000000..6674ce73
--- /dev/null
+++ b/research/object_detection/model_test.py
@@ -0,0 +1,266 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for object detection model."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+import os
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection import inputs
+from object_detection import model
+from object_detection import model_hparams
+from object_detection import model_test_util
+from object_detection.builders import model_builder
+from object_detection.core import standard_fields as fields
+from object_detection.utils import config_util
+
+FLAGS = tf.flags.FLAGS
+
+MODEL_NAME_FOR_TEST = model_test_util.SSD_INCEPTION_MODEL_NAME
+
+
+def _get_data_path():
+  """Returns an absolute path to TFRecord file."""
+  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, 'test_data',
+                      'pets_examples.record')
+
+
+def _get_labelmap_path():
+  """Returns an absolute path to label map file."""
+  return os.path.join(FLAGS.test_srcdir, model_test_util.PATH_BASE, 'data',
+                      'pet_label_map.pbtxt')
+
+
+def _get_configs_for_model(model_name):
+  """Returns configurations for model."""
+  filename = model_test_util.GetPipelineConfigPath(model_name)
+  data_path = _get_data_path()
+  label_map_path = _get_labelmap_path()
+  configs = config_util.get_configs_from_pipeline_file(filename)
+  configs = config_util.merge_external_params_with_configs(
+      configs,
+      train_input_path=data_path,
+      eval_input_path=data_path,
+      label_map_path=label_map_path)
+  return configs
+
+
+def setUpModule():
+  model_test_util.InitializeFlags(MODEL_NAME_FOR_TEST)
+
+
+class ModelTflearnTest(tf.test.TestCase):
+
+  @classmethod
+  def setUpClass(cls):
+    tf.reset_default_graph()
+
+  def _assert_outputs_for_train_eval(self, configs, mode, class_agnostic=False):
+    model_config = configs['model']
+    train_config = configs['train_config']
+    with tf.Graph().as_default():
+      if mode == tf.estimator.ModeKeys.TRAIN:
+        features, labels = inputs.create_train_input_fn(
+            configs['train_config'],
+            configs['train_input_config'],
+            configs['model'])()
+        batch_size = train_config.batch_size
+      else:
+        features, labels = inputs.create_eval_input_fn(
+            configs['eval_config'],
+            configs['eval_input_config'],
+            configs['model'])()
+        batch_size = 1
+
+      detection_model_fn = functools.partial(
+          model_builder.build, model_config=model_config, is_training=True)
+
+      hparams = model_hparams.create_hparams(
+          hparams_overrides='load_pretrained=false')
+
+      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)
+      estimator_spec = model_fn(features, labels, mode)
+
+      self.assertIsNotNone(estimator_spec.loss)
+      self.assertIsNotNone(estimator_spec.predictions)
+      if class_agnostic:
+        self.assertNotIn('detection_classes', estimator_spec.predictions)
+      else:
+        detection_classes = estimator_spec.predictions['detection_classes']
+        self.assertEqual(batch_size, detection_classes.shape.as_list()[0])
+        self.assertEqual(tf.float32, detection_classes.dtype)
+      detection_boxes = estimator_spec.predictions['detection_boxes']
+      detection_scores = estimator_spec.predictions['detection_scores']
+      num_detections = estimator_spec.predictions['num_detections']
+      self.assertEqual(batch_size, detection_boxes.shape.as_list()[0])
+      self.assertEqual(tf.float32, detection_boxes.dtype)
+      self.assertEqual(batch_size, detection_scores.shape.as_list()[0])
+      self.assertEqual(tf.float32, detection_scores.dtype)
+      self.assertEqual(tf.float32, num_detections.dtype)
+      if mode == tf.estimator.ModeKeys.TRAIN:
+        self.assertIsNotNone(estimator_spec.train_op)
+      return estimator_spec
+
+  def _assert_outputs_for_predict(self, configs):
+    model_config = configs['model']
+
+    with tf.Graph().as_default():
+      features, _ = inputs.create_eval_input_fn(
+          configs['eval_config'],
+          configs['eval_input_config'],
+          configs['model'])()
+      detection_model_fn = functools.partial(
+          model_builder.build, model_config=model_config, is_training=False)
+
+      hparams = model_hparams.create_hparams(
+          hparams_overrides='load_pretrained=false')
+
+      model_fn = model.create_model_fn(detection_model_fn, configs, hparams)
+      estimator_spec = model_fn(features, None, tf.estimator.ModeKeys.PREDICT)
+
+      self.assertIsNone(estimator_spec.loss)
+      self.assertIsNone(estimator_spec.train_op)
+      self.assertIsNotNone(estimator_spec.predictions)
+      self.assertIsNotNone(estimator_spec.export_outputs)
+      self.assertIn(tf.saved_model.signature_constants.PREDICT_METHOD_NAME,
+                    estimator_spec.export_outputs)
+
+  def testModelFnInTrainMode(self):
+    """Tests the model function in TRAIN mode."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
+    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.TRAIN)
+
+  def testModelFnInEvalMode(self):
+    """Tests the model function in EVAL mode."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
+    self._assert_outputs_for_train_eval(configs, tf.estimator.ModeKeys.EVAL)
+
+  def testModelFnInPredictMode(self):
+    """Tests the model function in PREDICT mode."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
+    self._assert_outputs_for_predict(configs)
+
+  def testExperiment(self):
+    """Tests that the `Experiment` object is constructed correctly."""
+    experiment = model_test_util.BuildExperiment()
+    model_dir = experiment.estimator.model_dir
+    pipeline_config_path = os.path.join(model_dir, 'pipeline.config')
+    self.assertTrue(tf.gfile.Exists(pipeline_config_path))
+
+
+class UnbatchTensorsTest(tf.test.TestCase):
+
+  def test_unbatch_without_unpadding(self):
+    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])
+    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, None, None])
+    groundtruth_classes_placeholder = tf.placeholder(tf.float32,
+                                                     [2, None, None])
+    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, None])
+
+    tensor_dict = {
+        fields.InputDataFields.image:
+            image_placeholder,
+        fields.InputDataFields.groundtruth_boxes:
+            groundtruth_boxes_placeholder,
+        fields.InputDataFields.groundtruth_classes:
+            groundtruth_classes_placeholder,
+        fields.InputDataFields.groundtruth_weights:
+            groundtruth_weights_placeholder
+    }
+    unbatched_tensor_dict = model.unstack_batch(
+        tensor_dict, unpad_groundtruth_tensors=False)
+
+    with self.test_session() as sess:
+      unbatched_tensor_dict_out = sess.run(
+          unbatched_tensor_dict,
+          feed_dict={
+              image_placeholder:
+                  np.random.rand(2, 4, 4, 3).astype(np.float32),
+              groundtruth_boxes_placeholder:
+                  np.random.rand(2, 5, 4).astype(np.float32),
+              groundtruth_classes_placeholder:
+                  np.random.rand(2, 5, 6).astype(np.float32),
+              groundtruth_weights_placeholder:
+                  np.random.rand(2, 5).astype(np.float32)
+          })
+    for image_out in unbatched_tensor_dict_out[fields.InputDataFields.image]:
+      self.assertAllEqual(image_out.shape, [4, 4, 3])
+    for groundtruth_boxes_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_boxes]:
+      self.assertAllEqual(groundtruth_boxes_out.shape, [5, 4])
+    for groundtruth_classes_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_classes]:
+      self.assertAllEqual(groundtruth_classes_out.shape, [5, 6])
+    for groundtruth_weights_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_weights]:
+      self.assertAllEqual(groundtruth_weights_out.shape, [5])
+
+  def test_unbatch_and_unpad_groundtruth_tensors(self):
+    image_placeholder = tf.placeholder(tf.float32, [2, None, None, None])
+    groundtruth_boxes_placeholder = tf.placeholder(tf.float32, [2, 5, None])
+    groundtruth_classes_placeholder = tf.placeholder(tf.float32, [2, 5, None])
+    groundtruth_weights_placeholder = tf.placeholder(tf.float32, [2, 5])
+    num_groundtruth_placeholder = tf.placeholder(tf.int32, [2])
+
+    tensor_dict = {
+        fields.InputDataFields.image:
+            image_placeholder,
+        fields.InputDataFields.groundtruth_boxes:
+            groundtruth_boxes_placeholder,
+        fields.InputDataFields.groundtruth_classes:
+            groundtruth_classes_placeholder,
+        fields.InputDataFields.groundtruth_weights:
+            groundtruth_weights_placeholder,
+        fields.InputDataFields.num_groundtruth_boxes:
+            num_groundtruth_placeholder
+    }
+    unbatched_tensor_dict = model.unstack_batch(
+        tensor_dict, unpad_groundtruth_tensors=True)
+    with self.test_session() as sess:
+      unbatched_tensor_dict_out = sess.run(
+          unbatched_tensor_dict,
+          feed_dict={
+              image_placeholder:
+                  np.random.rand(2, 4, 4, 3).astype(np.float32),
+              groundtruth_boxes_placeholder:
+                  np.random.rand(2, 5, 4).astype(np.float32),
+              groundtruth_classes_placeholder:
+                  np.random.rand(2, 5, 6).astype(np.float32),
+              groundtruth_weights_placeholder:
+                  np.random.rand(2, 5).astype(np.float32),
+              num_groundtruth_placeholder:
+                  np.array([3, 3], np.int32)
+          })
+    for image_out in unbatched_tensor_dict_out[fields.InputDataFields.image]:
+      self.assertAllEqual(image_out.shape, [4, 4, 3])
+    for groundtruth_boxes_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_boxes]:
+      self.assertAllEqual(groundtruth_boxes_out.shape, [3, 4])
+    for groundtruth_classes_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_classes]:
+      self.assertAllEqual(groundtruth_classes_out.shape, [3, 6])
+    for groundtruth_weights_out in unbatched_tensor_dict_out[
+        fields.InputDataFields.groundtruth_weights]:
+      self.assertAllEqual(groundtruth_weights_out.shape, [3])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/model_test_util.py b/research/object_detection/model_test_util.py
new file mode 100644
index 00000000..871a3b8c
--- /dev/null
+++ b/research/object_detection/model_test_util.py
@@ -0,0 +1,54 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Common utils for tests for object detection tflearn model."""
+
+from __future__ import absolute_import
+
+import os
+import tempfile
+import tensorflow as tf
+
+
+from object_detection import model
+from object_detection import model_hparams
+
+FLAGS = tf.flags.FLAGS
+
+FASTER_RCNN_MODEL_NAME = 'faster_rcnn_resnet50_pets'
+SSD_INCEPTION_MODEL_NAME = 'ssd_inception_v2_pets'
+PATH_BASE = 'google3/third_party/tensorflow_models/object_detection/'
+
+
+def GetPipelineConfigPath(model_name):
+  """Returns path to the local pipeline config file."""
+  return os.path.join(FLAGS.test_srcdir, PATH_BASE, 'samples', 'configs',
+                      model_name + '.config')
+
+
+def InitializeFlags(model_name_for_test):
+  FLAGS.model_dir = tempfile.mkdtemp()
+  FLAGS.pipeline_config_path = GetPipelineConfigPath(model_name_for_test)
+
+
+def BuildExperiment():
+  """Builds an Experiment object for testing purposes."""
+  run_config = tf.contrib.learn.RunConfig()
+  hparams = model_hparams.create_hparams(
+      hparams_overrides='load_pretrained=false')
+
+  # pylint: disable=protected-access
+  experiment_fn = model._build_experiment_fn(10, 10)
+  # pylint: enable=protected-access
+  return experiment_fn(run_config, hparams)
diff --git a/research/object_detection/model_tpu.py b/research/object_detection/model_tpu.py
new file mode 100644
index 00000000..cb2407f5
--- /dev/null
+++ b/research/object_detection/model_tpu.py
@@ -0,0 +1,262 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Creates and runs `Estimator` for object detection model on TPUs.
+
+This uses the TPUEstimator API to define and run a model in TRAIN/EVAL modes.
+"""
+# pylint: enable=line-too-long
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+import os
+
+import tensorflow as tf
+
+from tensorflow.contrib.tpu.python.tpu import tpu_config
+from tensorflow.contrib.tpu.python.tpu import tpu_estimator
+from tensorflow.contrib.training.python.training import evaluation
+
+from object_detection import inputs
+from object_detection import model
+from object_detection import model_hparams
+from object_detection.builders import model_builder
+from object_detection.utils import config_util
+
+tf.flags.DEFINE_bool('use_tpu', True, 'Use TPUs rather than plain CPUs')
+
+# Cloud TPU Cluster Resolvers
+tf.flags.DEFINE_string(
+    'gcp_project',
+    default=None,
+    help='Project name for the Cloud TPU-enabled project. If not specified, we '
+    'will attempt to automatically detect the GCE project from metadata.')
+tf.flags.DEFINE_string(
+    'tpu_zone',
+    default=None,
+    help='GCE zone where the Cloud TPU is located in. If not specified, we '
+    'will attempt to automatically detect the GCE project from metadata.')
+tf.flags.DEFINE_string(
+    'tpu_name',
+    default=None,
+    help='Name of the Cloud TPU for Cluster Resolvers. You must specify either '
+    'this flag or --master.')
+
+tf.flags.DEFINE_string(
+    'master', default=None,
+    help='GRPC URL of the master (e.g. grpc://ip.address.of.tpu:8470). You '
+    'must specify either this flag or --tpu_name.')
+
+tf.flags.DEFINE_integer('num_shards', 8, 'Number of shards (TPU cores).')
+tf.flags.DEFINE_integer('iterations_per_loop', 100,
+                        'Number of iterations per TPU training loop.')
+# For mode=train_and_eval, evaluation occurs after training is finished.
+# Note: independently of steps_per_checkpoint, estimator will save the most
+# recent checkpoint every 10 minutes by default for train_and_eval
+tf.flags.DEFINE_string('mode', 'train_and_eval',
+                       'Mode to run: train, eval, train_and_eval')
+tf.flags.DEFINE_integer('train_batch_size', 32 * 8, 'Batch size for training.')
+
+# For EVAL.
+tf.flags.DEFINE_integer('min_eval_interval_secs', 180,
+                        'Minimum seconds between evaluations.')
+tf.flags.DEFINE_integer(
+    'eval_timeout_secs', None,
+    'Maximum seconds between checkpoints before evaluation terminates.')
+
+FLAGS = tf.flags.FLAGS
+
+
+def create_estimator(run_config,
+                     hparams,
+                     pipeline_config_path,
+                     train_steps=None,
+                     eval_steps=None,
+                     train_batch_size=None,
+                     model_fn_creator=model.create_model_fn,
+                     use_tpu=False,
+                     num_shards=1,
+                     params=None,
+                     **kwargs):
+  """Creates an `Estimator` object.
+
+  Args:
+    run_config: A `RunConfig`.
+    hparams: A `HParams`.
+    pipeline_config_path: A path to a pipeline config file.
+    train_steps: Number of training steps. If None, the number of training steps
+      is set from the `TrainConfig` proto.
+    eval_steps: Number of evaluation steps per evaluation cycle. If None, the
+      number of evaluation steps is set from the `EvalConfig` proto.
+    train_batch_size: Training batch size. If none, use batch size from
+      `TrainConfig` proto.
+    model_fn_creator: A function that creates a `model_fn` for `Estimator`.
+      Follows the signature:
+
+      * Args:
+        * `detection_model_fn`: Function that returns `DetectionModel` instance.
+        * `configs`: Dictionary of pipeline config objects.
+        * `hparams`: `HParams` object.
+      * Returns:
+        `model_fn` for `Estimator`.
+
+    use_tpu: Boolean, whether training and evaluation should run on TPU.
+    num_shards: Number of shards (TPU cores).
+    params: Parameter dictionary passed from the estimator.
+    **kwargs: Additional keyword arguments for configuration override.
+
+  Returns:
+    Estimator: A estimator object used for training and evaluation
+    train_input_fn: Input function for the training loop
+    eval_input_fn: Input function for the evaluation run
+    train_steps: Number of training steps either from arg `train_steps` or
+      `TrainConfig` proto
+    eval_steps: Number of evaluation steps either from arg `eval_steps` or
+      `EvalConfig` proto
+  """
+  configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+  configs = config_util.merge_external_params_with_configs(
+      configs,
+      hparams,
+      train_steps=train_steps,
+      eval_steps=eval_steps,
+      batch_size=train_batch_size,
+      **kwargs)
+  model_config = configs['model']
+  train_config = configs['train_config']
+  train_input_config = configs['train_input_config']
+  eval_config = configs['eval_config']
+  eval_input_config = configs['eval_input_config']
+
+  if params is None:
+    params = {}
+
+  if train_steps is None:
+    train_steps = train_config.num_steps if train_config.num_steps else None
+
+  if eval_steps is None:
+    eval_steps = eval_config.num_examples if eval_config.num_examples else None
+
+  detection_model_fn = functools.partial(
+      model_builder.build, model_config=model_config)
+
+  # Create the input functions for TRAIN/EVAL.
+  train_input_fn = inputs.create_train_input_fn(
+      train_config=train_config,
+      train_input_config=train_input_config,
+      model_config=model_config)
+  eval_input_fn = inputs.create_eval_input_fn(
+      eval_config=eval_config,
+      eval_input_config=eval_input_config,
+      model_config=model_config)
+
+  estimator = tpu_estimator.TPUEstimator(
+      model_fn=model_fn_creator(detection_model_fn, configs, hparams,
+                                use_tpu),
+      train_batch_size=train_config.batch_size,
+      # For each core, only batch size 1 is supported for eval.
+      eval_batch_size=num_shards * 1 if use_tpu else 1,
+      use_tpu=use_tpu,
+      config=run_config,
+      params=params)
+  return estimator, train_input_fn, eval_input_fn, train_steps, eval_steps
+
+
+def main(unused_argv):
+  tf.flags.mark_flag_as_required('model_dir')
+  tf.flags.mark_flag_as_required('pipeline_config_path')
+
+  if FLAGS.master is None and FLAGS.tpu_name is None:
+    raise RuntimeError('You must specify either --master or --tpu_name.')
+
+  if FLAGS.master is not None:
+    if FLAGS.tpu_name is not None:
+      tf.logging.warn('Both --master and --tpu_name are set. Ignoring '
+                      '--tpu_name and using --master.')
+    tpu_grpc_url = FLAGS.master
+  else:
+    tpu_cluster_resolver = (
+        tf.contrib.cluster_resolver.python.training.TPUClusterResolver(
+            tpu_names=[FLAGS.tpu_name],
+            zone=FLAGS.tpu_zone,
+            project=FLAGS.gcp_project))
+    tpu_grpc_url = tpu_cluster_resolver.get_master()
+
+  config = tpu_config.RunConfig(
+      master=tpu_grpc_url,
+      evaluation_master=tpu_grpc_url,
+      model_dir=FLAGS.model_dir,
+      tpu_config=tpu_config.TPUConfig(
+          iterations_per_loop=FLAGS.iterations_per_loop,
+          num_shards=FLAGS.num_shards))
+  params = {}
+  estimator, train_input_fn, eval_input_fn, train_steps, eval_steps = (
+      create_estimator(
+          config,
+          model_hparams.create_hparams(),
+          FLAGS.pipeline_config_path,
+          train_steps=FLAGS.num_train_steps,
+          eval_steps=FLAGS.num_eval_steps,
+          train_batch_size=FLAGS.train_batch_size,
+          use_tpu=FLAGS.use_tpu,
+          num_shards=FLAGS.num_shards,
+          params=params))
+
+  if FLAGS.mode in ['train', 'train_and_eval']:
+    estimator.train(input_fn=train_input_fn, max_steps=train_steps)
+
+  if FLAGS.mode == 'train_and_eval':
+    # Eval one time.
+    eval_results = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)
+    tf.logging.info('Eval results: %s' % eval_results)
+
+  # Continuously evaluating.
+  if FLAGS.mode == 'eval':
+    def terminate_eval():
+      tf.logging.info('Terminating eval after %d seconds of no checkpoints' %
+                      FLAGS.eval_timeout_secs)
+      return True
+
+    # Run evaluation when there's a new checkpoint.
+    for ckpt in evaluation.checkpoints_iterator(
+        FLAGS.model_dir,
+        min_interval_secs=FLAGS.min_eval_interval_secs,
+        timeout=FLAGS.eval_timeout_secs,
+        timeout_fn=terminate_eval):
+
+      tf.logging.info('Starting to evaluate.')
+      try:
+        eval_results = estimator.evaluate(
+            input_fn=eval_input_fn,
+            steps=eval_steps,
+            checkpoint_path=ckpt)
+        tf.logging.info('Eval results: %s' % eval_results)
+
+        # Terminate eval job when final checkpoint is reached
+        current_step = int(os.path.basename(ckpt).split('-')[1])
+        if current_step >= train_steps:
+          tf.logging.info(
+              'Evaluation finished after training step %d' % current_step)
+          break
+
+      except tf.errors.NotFoundError:
+        tf.logging.info(
+            'Checkpoint %s no longer exists, skipping checkpoint' % ckpt)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/object_detection/models/BUILD b/research/object_detection/models/BUILD
index ef00cab6..6ed8f06c 100644
--- a/research/object_detection/models/BUILD
+++ b/research/object_detection/models/BUILD
@@ -119,6 +119,7 @@ py_library(
 
 py_test(
     name = "ssd_resnet_v1_fpn_feature_extractor_test",
+    timeout = "long",
     srcs = ["ssd_resnet_v1_fpn_feature_extractor_test.py"],
     deps = [
         ":ssd_resnet_v1_fpn_feature_extractor",
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index b2ccbceb..00049a76 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -52,7 +52,8 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """MobileNetV1 Feature Extractor for Embedded-friendly SSD Models.
 
     Args:
@@ -69,6 +70,7 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
 
     Raises:
       ValueError: upon invalid `pad_to_multiple` values.
@@ -80,7 +82,7 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
     super(EmbeddedSSDMobileNetV1FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
         conv_hyperparams, batch_norm_trainable, reuse_weights,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
 
   def extract_features(self, preprocessed_inputs):
     """Extract features from preprocessed inputs.
@@ -119,6 +121,7 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
         'layer_depth': [-1, -1, 512, 256, 256],
         'conv_kernel_size': [-1, -1, 3, 3, 2],
         'use_explicit_padding': self._use_explicit_padding,
+        'use_depthwise': self._use_depthwise,
     }
 
     with slim.arg_scope(self._conv_hyperparams):
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index 5cf0bdce..9714a31c 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -36,7 +36,8 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """InceptionV2 Feature Extractor for SSD Models.
 
     Args:
@@ -53,11 +54,12 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
     """
     super(SSDInceptionV2FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
         conv_hyperparams, batch_norm_trainable, reuse_weights,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -92,6 +94,7 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         'from_layer': ['Mixed_4c', 'Mixed_5c', '', '', '', ''],
         'layer_depth': [-1, -1, 512, 256, 256, 128],
         'use_explicit_padding': self._use_explicit_padding,
+        'use_depthwise': self._use_depthwise,
     }
 
     with slim.arg_scope(self._conv_hyperparams):
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index 987c020c..1ea07a91 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -36,7 +36,8 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """InceptionV3 Feature Extractor for SSD Models.
 
     Args:
@@ -53,11 +54,12 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
     """
     super(SSDInceptionV3FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
         conv_hyperparams, batch_norm_trainable, reuse_weights,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -92,6 +94,7 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         'from_layer': ['Mixed_5d', 'Mixed_6e', 'Mixed_7c', '', '', ''],
         'layer_depth': [-1, -1, -1, 512, 256, 128],
         'use_explicit_padding': self._use_explicit_padding,
+        'use_depthwise': self._use_depthwise,
     }
 
     with slim.arg_scope(self._conv_hyperparams):
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 1785d04c..29bf092c 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -37,7 +37,8 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """MobileNetV1 Feature Extractor for SSD Models.
 
     Args:
@@ -54,11 +55,12 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
     """
     super(SSDMobileNetV1FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
         conv_hyperparams, batch_norm_trainable, reuse_weights,
-        use_explicit_padding)
+        use_explicit_padding, use_depthwise)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -94,6 +96,7 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                        '', ''],
         'layer_depth': [-1, -1, 512, 256, 256, 128],
         'use_explicit_padding': self._use_explicit_padding,
+        'use_depthwise': self._use_depthwise,
     }
 
     with slim.arg_scope(self._conv_hyperparams):
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index b043c2c2..34961eb9 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -25,9 +25,11 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                conv_hyperparams,
                resnet_base_fn,
                resnet_scope_name,
+               fpn_scope_name,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """SSD FPN feature extractor based on Resnet v1 architecture.
 
     Args:
@@ -39,7 +41,9 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         width dimensions to.
       conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
       resnet_base_fn: base resnet network to use.
-      resnet_scope_name: scope name to construct resnet
+      resnet_scope_name: scope name under which to construct resnet
+      fpn_scope_name: scope name under which to construct the feature pyramid
+        network.
       batch_norm_trainable: Whether to update batch norm parameters during
         training or not. When training with a small batch size
         (e.g. 1), it is desirable to disable batch norm update and use
@@ -47,6 +51,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
 
     Raises:
       ValueError: On supplying invalid arguments for unused arguments.
@@ -62,6 +67,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       raise ValueError('Explicit padding is not a valid option.')
     self._resnet_base_fn = resnet_base_fn
     self._resnet_scope_name = resnet_scope_name
+    self._fpn_scope_name = fpn_scope_name
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -124,6 +130,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
             scope=scope)
       image_features = self._filter_features(image_features)
       last_feature_map = image_features['block4']
+    with tf.variable_scope(self._fpn_scope_name, reuse=self._reuse_weights):
       with slim.arg_scope(self._conv_hyperparams):
         for i in range(5, 7):
           last_feature_map = slim.conv2d(
@@ -154,7 +161,8 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """Resnet50 v1 FPN Feature Extractor for SSD Models.
 
     Args:
@@ -170,11 +178,12 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
-        features. Default is False.
+        features. Default is False. UNUSED currently.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
     """
     super(SSDResnet50V1FpnFeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, resnet_v1.resnet_v1_50, 'resnet_v1_50_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_50, 'resnet_v1_50', 'fpn',
         batch_norm_trainable, reuse_weights, use_explicit_padding)
 
 
@@ -188,7 +197,8 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """Resnet101 v1 FPN Feature Extractor for SSD Models.
 
     Args:
@@ -204,11 +214,12 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
-        features. Default is False.
+        features. Default is False. UNUSED currently.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
     """
     super(SSDResnet101V1FpnFeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, resnet_v1.resnet_v1_101, 'resnet_v1_101_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_101, 'resnet_v1_101', 'fpn',
         batch_norm_trainable, reuse_weights, use_explicit_padding)
 
 
@@ -222,7 +233,8 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                conv_hyperparams,
                batch_norm_trainable=True,
                reuse_weights=None,
-               use_explicit_padding=False):
+               use_explicit_padding=False,
+               use_depthwise=False):
     """Resnet152 v1 FPN Feature Extractor for SSD Models.
 
     Args:
@@ -238,9 +250,10 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
-        features. Default is False.
+        features. Default is False. UNUSED currently.
+      use_depthwise: Whether to use depthwise convolutions. UNUSED currently.
     """
     super(SSDResnet152V1FpnFeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, resnet_v1.resnet_v1_152, 'resnet_v1_152_fpn',
+        conv_hyperparams, resnet_v1.resnet_v1_152, 'resnet_v1_152', 'fpn',
         batch_norm_trainable, reuse_weights, use_explicit_padding)
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
index 4b917283..83f8f80e 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
@@ -7,7 +7,7 @@ from object_detection.models import ssd_resnet_v1_fpn_feature_extractor_testbase
 
 class SSDResnet50V1FeatureExtractorTest(
     ssd_resnet_v1_fpn_feature_extractor_testbase.
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet50v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
@@ -19,13 +19,13 @@ class SSDResnet50V1FeatureExtractorTest(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
         conv_hyperparams, batch_norm_trainable)
 
-  def _scope_name(self):
-    return 'resnet_v1_50_fpn'
+  def _resnet_scope_name(self):
+    return 'resnet_v1_50'
 
 
 class SSDResnet101V1FeatureExtractorTest(
     ssd_resnet_v1_fpn_feature_extractor_testbase.
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet101v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
@@ -38,13 +38,13 @@ class SSDResnet101V1FeatureExtractorTest(
             is_training, depth_multiplier, min_depth, pad_to_multiple,
             conv_hyperparams, batch_norm_trainable))
 
-  def _scope_name(self):
-    return 'resnet_v1_101_fpn'
+  def _resnet_scope_name(self):
+    return 'resnet_v1_101'
 
 
 class SSDResnet152V1FeatureExtractorTest(
     ssd_resnet_v1_fpn_feature_extractor_testbase.
-    SSDResnetFeatureExtractorTestBase):
+    SSDResnetFPNFeatureExtractorTestBase):
   """SSDResnet152v1Fpn feature extractor test."""
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
@@ -57,8 +57,8 @@ class SSDResnet152V1FeatureExtractorTest(
             is_training, depth_multiplier, min_depth, pad_to_multiple,
             conv_hyperparams, batch_norm_trainable))
 
-  def _scope_name(self):
-    return 'resnet_v1_152_fpn'
+  def _resnet_scope_name(self):
+    return 'resnet_v1_152'
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index c640c742..d5478823 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -1,18 +1,23 @@
 """Tests for ssd resnet v1 FPN feature extractors."""
 import abc
 import numpy as np
+import tensorflow as tf
 
 from object_detection.models import ssd_feature_extractor_test
 
 
-class SSDResnetFeatureExtractorTestBase(
+class SSDResnetFPNFeatureExtractorTestBase(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
   """Helper test class for SSD Resnet v1 FPN feature extractors."""
 
   @abc.abstractmethod
-  def _scope_name(self):
+  def _resnet_scope_name(self):
     pass
 
+  @abc.abstractmethod
+  def _fpn_scope_name(self):
+    return 'fpn'
+
   def test_extract_features_returns_correct_shapes_256(self):
     image_height = 256
     image_width = 256
@@ -73,5 +78,16 @@ class SSDResnetFeatureExtractorTestBase(
   def test_variables_only_created_in_scope(self):
     depth_multiplier = 1
     pad_to_multiple = 1
-    self.check_feature_extractor_variables_under_scope(
-        depth_multiplier, pad_to_multiple, self._scope_name())
+    g = tf.Graph()
+    with g.as_default():
+      feature_extractor = self._create_feature_extractor(
+          depth_multiplier, pad_to_multiple)
+      preprocessed_inputs = tf.placeholder(tf.float32, (4, None, None, 3))
+      feature_extractor.extract_features(preprocessed_inputs)
+      variables = g.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+      for variable in variables:
+        self.assertTrue(
+            variable.name.startswith(self._resnet_scope_name())
+            or variable.name.startswith(self._fpn_scope_name()))
+
+
diff --git a/research/object_detection/object_detection_tutorial.ipynb b/research/object_detection/object_detection_tutorial.ipynb
index 6e251ff4..8e66be79 100644
--- a/research/object_detection/object_detection_tutorial.ipynb
+++ b/research/object_detection/object_detection_tutorial.ipynb
@@ -35,6 +35,7 @@
     "from io import StringIO\n",
     "from matplotlib import pyplot as plt\n",
     "from PIL import Image\n",
+    "from object_detection.utils import ops as utils_ops\n",
     "\n",
     "if tf.__version__ < '1.4.0':\n",
     "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n"
@@ -223,6 +224,59 @@
     "IMAGE_SIZE = (12, 8)"
    ]
   },
+  {
+   "cell_type": "code",
+   "execution_count": null,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "def run_inference_for_single_image(image, graph):\n",
+    "  with graph.as_default():\n",
+    "    with tf.Session() as sess:\n",
+    "      # Get handles to input and output tensors\n",
+    "      ops = tf.get_default_graph().get_operations()\n",
+    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
+    "      tensor_dict = {}\n",
+    "      for key in [\n",
+    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
+    "          'detection_classes', 'detection_masks'\n",
+    "      ]:\n",
+    "        tensor_name = key + ':0'\n",
+    "        if tensor_name in all_tensor_names:\n",
+    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
+    "              tensor_name)\n",
+    "      if 'detection_masks' in tensor_dict:\n",
+    "        # The following processing is only for single image\n",
+    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
+    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
+    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
+    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
+    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
+    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
+    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
+    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
+    "        detection_masks_reframed = tf.cast(\n",
+    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
+    "        # Follow the convention by adding back the batch dimension\n",
+    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
+    "            detection_masks_reframed, 0)\n",
+    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
+    "\n",
+    "      # Run inference\n",
+    "      output_dict = sess.run(tensor_dict,\n",
+    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
+    "\n",
+    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
+    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
+    "      output_dict['detection_classes'] = output_dict[\n",
+    "          'detection_classes'][0].astype(np.uint8)\n",
+    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
+    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
+    "      if 'detection_masks' in output_dict:\n",
+    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
+    "  return output_dict"
+   ]
+  },
   {
    "cell_type": "code",
    "execution_count": null,
@@ -231,39 +285,27 @@
    },
    "outputs": [],
    "source": [
-    "with detection_graph.as_default():\n",
-    "  with tf.Session(graph=detection_graph) as sess:\n",
-    "    # Definite input and output Tensors for detection_graph\n",
-    "    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
-    "    # Each box represents a part of the image where a particular object was detected.\n",
-    "    detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
-    "    # Each score represent how level of confidence for each of the objects.\n",
-    "    # Score is shown on the result image, together with the class label.\n",
-    "    detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
-    "    detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
-    "    num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
-    "    for image_path in TEST_IMAGE_PATHS:\n",
-    "      image = Image.open(image_path)\n",
-    "      # the array based representation of the image will be used later in order to prepare the\n",
-    "      # result image with boxes and labels on it.\n",
-    "      image_np = load_image_into_numpy_array(image)\n",
-    "      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
-    "      image_np_expanded = np.expand_dims(image_np, axis=0)\n",
-    "      # Actual detection.\n",
-    "      (boxes, scores, classes, num) = sess.run(\n",
-    "          [detection_boxes, detection_scores, detection_classes, num_detections],\n",
-    "          feed_dict={image_tensor: image_np_expanded})\n",
-    "      # Visualization of the results of a detection.\n",
-    "      vis_util.visualize_boxes_and_labels_on_image_array(\n",
-    "          image_np,\n",
-    "          np.squeeze(boxes),\n",
-    "          np.squeeze(classes).astype(np.int32),\n",
-    "          np.squeeze(scores),\n",
-    "          category_index,\n",
-    "          use_normalized_coordinates=True,\n",
-    "          line_thickness=8)\n",
-    "      plt.figure(figsize=IMAGE_SIZE)\n",
-    "      plt.imshow(image_np)"
+    "for image_path in TEST_IMAGE_PATHS:\n",
+    "  image = Image.open(image_path)\n",
+    "  # the array based representation of the image will be used later in order to prepare the\n",
+    "  # result image with boxes and labels on it.\n",
+    "  image_np = load_image_into_numpy_array(image)\n",
+    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
+    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
+    "  # Actual detection.\n",
+    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
+    "  # Visualization of the results of a detection.\n",
+    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
+    "      image_np,\n",
+    "      output_dict['detection_boxes'],\n",
+    "      output_dict['detection_classes'],\n",
+    "      output_dict['detection_scores'],\n",
+    "      category_index,\n",
+    "      instance_masks=output_dict.get('detection_masks'),\n",
+    "      use_normalized_coordinates=True,\n",
+    "      line_thickness=8)\n",
+    "  plt.figure(figsize=IMAGE_SIZE)\n",
+    "  plt.imshow(image_np)"
    ]
   },
   {
@@ -275,6 +317,9 @@
   }
  ],
  "metadata": {
+  "colab": {
+   "version": "0.3.2"
+  },
   "kernelspec": {
    "display_name": "Python 2",
    "language": "python",
diff --git a/research/object_detection/protos/argmax_matcher.proto b/research/object_detection/protos/argmax_matcher.proto
index 88c50318..947fcb98 100644
--- a/research/object_detection/protos/argmax_matcher.proto
+++ b/research/object_detection/protos/argmax_matcher.proto
@@ -22,4 +22,8 @@ message ArgMaxMatcher {
 
   // Whether to ensure each row is matched to at least one column.
   optional bool force_match_for_each_row = 5 [default = false];
+
+  // Force constructed match objects to use matrix multiplication based gather
+  // instead of standard tf.gather
+  optional bool use_matmul_gather = 6 [default = false];
 }
diff --git a/research/object_detection/protos/bipartite_matcher.proto b/research/object_detection/protos/bipartite_matcher.proto
index 7e5a9e5c..175ecdd1 100644
--- a/research/object_detection/protos/bipartite_matcher.proto
+++ b/research/object_detection/protos/bipartite_matcher.proto
@@ -5,4 +5,7 @@ package object_detection.protos;
 // Configuration proto for bipartite matcher. See
 // matchers/bipartite_matcher.py for details.
 message BipartiteMatcher {
+  // Force constructed match objects to use matrix multiplication based gather
+  // instead of standard tf.gather
+  optional bool use_matmul_gather = 6 [default = false];
 }
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index f9310fcd..f40c5ded 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -52,6 +52,9 @@ message ConvolutionalBoxPredictor {
   optional bool apply_sigmoid_to_scores = 9 [default = false];
 
   optional float class_prediction_bias_init = 10 [default = 0.0];
+
+  // Whether to use depthwise separable convolution for box predictor layers.
+  optional bool use_depthwise = 11 [default = false];
 }
 
 // Configuration proto for weight shared convolutional box predictor.
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index 8e8eec7f..e431fc9d 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -34,6 +34,9 @@ message KeepAspectRatioResizer {
   // [max_dimension, max_dimension]. Note that the zeros are padded to the
   // bottom and the right of the resized image.
   optional bool pad_to_max_dimension = 4 [default = false];
+
+  // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
+  optional bool convert_to_grayscale = 5 [default = false];
 }
 
 // Configuration proto for image resizer that resizes to a fixed shape.
@@ -46,4 +49,7 @@ message FixedShapeResizer {
 
   // Desired method when resizing image.
   optional ResizeType resize_method = 3 [default = BILINEAR];
+
+  // Whether to also resize the image channels from 3 to 1 (RGB to grayscale).
+  optional bool convert_to_grayscale = 4 [default = false];
 }
diff --git a/research/object_detection/protos/multiscale_anchor_generator.proto b/research/object_detection/protos/multiscale_anchor_generator.proto
index 50869793..6f6789a1 100644
--- a/research/object_detection/protos/multiscale_anchor_generator.proto
+++ b/research/object_detection/protos/multiscale_anchor_generator.proto
@@ -19,4 +19,5 @@ message MultiscaleAnchorGenerator {
   repeated float aspect_ratios = 4;
 
   // Number of intermediate scale each scale octave
+  optional int32 scales_per_octave = 5 [default = 2];
 }
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index 9b3f25f1..606377c9 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -32,6 +32,7 @@ message PreprocessingStep {
     SSDRandomCropPadFixedAspectRatio ssd_random_crop_pad_fixed_aspect_ratio = 24;
     RandomVerticalFlip random_vertical_flip = 25;
     RandomRotation90 random_rotation90 = 26;
+    RGBtoGray rgb_to_gray = 27;
   }
 }
 
@@ -236,6 +237,11 @@ message RandomResizeMethod {
   optional float target_width = 2;
 }
 
+// Converts the RGB image to a grayscale image. This also converts the image
+// depth from 3 to 1, unlike RandomRGBtoGray which does not change the image
+// depth.
+message RGBtoGray {}
+
 // Scales boxes from normalized coordinates to pixel coordinates.
 message ScaleBoxesToPixelCoordinates {
 }
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index d88a4fd2..403d8aae 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -86,4 +86,8 @@ message SsdFeatureExtractor {
   // Whether to use explicit padding when extracting SSD multiresolution
   // features. Note that this does not apply to the base feature extractor.
   optional bool use_explicit_padding = 7 [default=false];
+
+  // Whether to use depthwise separable convolutions for to extract additional
+  // feature maps added by SSD.
+  optional bool use_depthwise = 8 [default=false];
 }
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index 02f4cec3..a16f00fb 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -78,4 +78,14 @@ message TrainConfig {
   // Setting this option to false is very useful while debugging the model and
   // losses.
   optional bool add_regularization_loss = 18 [default=true];
+
+  // Maximum number of boxes used during training.
+  // Set this to at least the maximum amount of boxes in the input data.
+  // Otherwise, it may cause "Data loss: Attempted to pad to a smaller size
+  // than the input element" errors.
+  optional int32 max_number_of_boxes = 20 [default=50];
+
+  // Whether to remove padding along `num_boxes` dimension of the groundtruth
+  // tensors.
+  optional bool unpad_groundtruth_tensors = 21 [default=true];
 }
diff --git a/research/object_detection/samples/configs/BUILD b/research/object_detection/samples/configs/BUILD
index 652a929f..2580dd89 100644
--- a/research/object_detection/samples/configs/BUILD
+++ b/research/object_detection/samples/configs/BUILD
@@ -7,4 +7,5 @@ licenses(["notice"])
 exports_files([
     "faster_rcnn_resnet50_pets.config",
     "ssd_inception_v2_pets.config",
+    "ssd_mobilenet_v1_focal_loss_pets.config",
 ])
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
index e9e271b0..56c04171 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
@@ -1,5 +1,8 @@
-# Faster R-CNN with Resnet-101 (v1)
-# Trained on COCO, initialized from Imagenet classification checkpoint
+# Faster R-CNN with Resnet-101 (v1), configuration for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
 
 model {
   faster_rcnn {
@@ -104,7 +107,8 @@ train_config: {
     use_moving_average: false
   }
   gradient_clipping_by_norm: 10.0
-  fine_tune_checkpoint: "/namespace/vale-project/models/classification/imagenet/resnet_v1_101/msra_model.ckpt"
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
   data_augmentation_options {
     random_horizontal_flip {
     }
@@ -112,32 +116,24 @@ train_config: {
 }
 
 train_input_reader: {
-  label_map_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/raw/mscoco_label_map.pbtxt"
-  external_input_reader {
-    [object_detection.protos.GoogleInputReader.google_input_reader] {
-      ss_table_input_reader: {
-        input_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/example_sstables/mscoco_alltasks_trainvalminusminival2014-?????-of-00101"
-        data_type: TF_EXAMPLE
-      }
-    }
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
   }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
 
 eval_config: {
-  metrics_set: "coco_detection_metrics"
-  use_moving_averages: false
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
 }
 
 eval_input_reader: {
-  label_map_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/raw/mscoco_label_map.pbtxt"
-  shuffle: false
-  num_epochs: 1
-  external_input_reader {
-    [object_detection.protos.GoogleInputReader.google_input_reader] {
-      ss_table_input_reader: {
-        input_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/example_sstables/mscoco_alltasks_minival2014-?????-of-00020"
-        data_type: TF_EXAMPLE
-      }
-    }
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
   }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
 }
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config
new file mode 100644
index 00000000..50987a49
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config
@@ -0,0 +1,192 @@
+# SSD with Mobilenet v1 configuration for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+# TPU-compatible
+
+model {
+  ssd {
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  max_number_of_boxes: 50
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
new file mode 100644
index 00000000..7186b88e
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
@@ -0,0 +1,192 @@
+# SSD with Mobilenet v1, configured for Oxford-IIIT Pets Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+# TPU-compatible
+
+model {
+  ssd {
+    num_classes: 37
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  max_number_of_boxes: 50
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 2000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/trainer.py b/research/object_detection/trainer.py
index 227748df..8663e9aa 100644
--- a/research/object_detection/trainer.py
+++ b/research/object_detection/trainer.py
@@ -251,8 +251,10 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
     update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, first_clone_scope)
 
     with tf.device(deploy_config.optimizer_device()):
-      training_optimizer = optimizer_builder.build(train_config.optimizer,
-                                                   global_summaries)
+      training_optimizer, optimizer_summary_vars = optimizer_builder.build(
+          train_config.optimizer)
+      for var in optimizer_summary_vars:
+        tf.summary.scalar(var.op.name, var)
 
     sync_optimizer = None
     if train_config.sync_replicas:
diff --git a/research/object_detection/utils/BUILD b/research/object_detection/utils/BUILD
index 332a9a86..30ffb528 100644
--- a/research/object_detection/utils/BUILD
+++ b/research/object_detection/utils/BUILD
@@ -27,6 +27,7 @@ py_library(
         "//pyglib/logging",
         "//tensorflow",
         "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
         "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
         "//tensorflow/models/research/object_detection/protos:model_py_pb2",
         "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
@@ -231,6 +232,7 @@ py_test(
     deps = [
         ":config_util",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
         "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
         "//tensorflow/models/research/object_detection/protos:model_py_pb2",
         "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 3470d212..6c8bf246 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -25,6 +25,51 @@ from object_detection.protos import pipeline_pb2
 from object_detection.protos import train_pb2
 
 
+def get_image_resizer_config(model_config):
+  """Returns the image resizer config from a model config.
+
+  Args:
+    model_config: A model_pb2.DetectionModel.
+
+  Returns:
+    An image_resizer_pb2.ImageResizer.
+
+  Raises:
+    ValueError: If the model type is not recognized.
+  """
+  meta_architecture = model_config.WhichOneof("model")
+  if meta_architecture == "faster_rcnn":
+    return model_config.faster_rcnn.image_resizer
+  if meta_architecture == "ssd":
+    return model_config.ssd.image_resizer
+
+  raise ValueError("Unknown model type: {}".format(meta_architecture))
+
+
+def get_spatial_image_size(image_resizer_config):
+  """Returns expected spatial size of the output image from a given config.
+
+  Args:
+    image_resizer_config: An image_resizer_pb2.ImageResizer.
+
+  Returns:
+    A list of two integers of the form [height, width]. `height` and `width` are
+    set  -1 if they cannot be determined during graph construction.
+
+  Raises:
+    ValueError: If the model type is not recognized.
+  """
+  if image_resizer_config.HasField("fixed_shape_resizer"):
+    return [image_resizer_config.fixed_shape_resizer.height,
+            image_resizer_config.fixed_shape_resizer.width]
+  if image_resizer_config.HasField("keep_aspect_ratio_resizer"):
+    if image_resizer_config.keep_aspect_ratio_resizer.pad_to_max_dimension:
+      return [image_resizer_config.keep_aspect_ratio_resizer.max_dimension] * 2
+    else:
+      return [-1, -1]
+  raise ValueError("Unknown image resizer type.")
+
+
 def get_configs_from_pipeline_file(pipeline_config_path):
   """Reads configuration from a pipeline_pb2.TrainEvalPipelineConfig.
 
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index e93e8281..bd65edea 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -21,6 +21,7 @@ import tensorflow as tf
 from google.protobuf import text_format
 
 from object_detection.protos import eval_pb2
+from object_detection.protos import image_resizer_pb2
 from object_detection.protos import input_reader_pb2
 from object_detection.protos import model_pb2
 from object_detection.protos import pipeline_pb2
@@ -415,6 +416,37 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(new_mask_type, configs["train_input_config"].mask_type)
     self.assertEqual(new_mask_type, configs["eval_input_config"].mask_type)
 
+  def  test_get_image_resizer_config(self):
+    """Tests that number of classes can be retrieved."""
+    model_config = model_pb2.DetectionModel()
+    model_config.faster_rcnn.image_resizer.fixed_shape_resizer.height = 100
+    model_config.faster_rcnn.image_resizer.fixed_shape_resizer.width = 300
+    image_resizer_config = config_util.get_image_resizer_config(model_config)
+    self.assertEqual(image_resizer_config.fixed_shape_resizer.height, 100)
+    self.assertEqual(image_resizer_config.fixed_shape_resizer.width, 300)
+
+  def test_get_spatial_image_size_from_fixed_shape_resizer_config(self):
+    image_resizer_config = image_resizer_pb2.ImageResizer()
+    image_resizer_config.fixed_shape_resizer.height = 100
+    image_resizer_config.fixed_shape_resizer.width = 200
+    image_shape = config_util.get_spatial_image_size(image_resizer_config)
+    self.assertAllEqual(image_shape, [100, 200])
+
+  def test_get_spatial_image_size_from_aspect_preserving_resizer_config(self):
+    image_resizer_config = image_resizer_pb2.ImageResizer()
+    image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100
+    image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600
+    image_resizer_config.keep_aspect_ratio_resizer.pad_to_max_dimension = True
+    image_shape = config_util.get_spatial_image_size(image_resizer_config)
+    self.assertAllEqual(image_shape, [600, 600])
+
+  def test_get_spatial_image_size_from_aspect_preserving_resizer_dynamic(self):
+    image_resizer_config = image_resizer_pb2.ImageResizer()
+    image_resizer_config.keep_aspect_ratio_resizer.min_dimension = 100
+    image_resizer_config.keep_aspect_ratio_resizer.max_dimension = 600
+    image_shape = config_util.get_spatial_image_size(image_resizer_config)
+    self.assertAllEqual(image_shape, [-1, -1])
+
 
 if __name__ == "__main__":
   tf.test.main()
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 14583527..7c757a16 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -94,9 +94,8 @@ def cosine_decay_with_warmup(global_step,
     raise ValueError('total_steps must be larger or equal to '
                      'warmup_steps.')
   learning_rate = 0.5 * learning_rate_base * (
-      1 + tf.cos(np.pi * tf.cast(
-          global_step - warmup_steps, tf.float32
-      ) / float(total_steps - warmup_steps)))
+      1 + tf.cos(np.pi * (tf.cast(global_step, tf.float32) - warmup_steps
+                         ) / float(total_steps - warmup_steps)))
   if warmup_steps > 0:
     slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
     pre_cosine_learning_rate = slope * tf.cast(
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 76ba5ff1..e9116eec 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -475,6 +475,7 @@ class ObjectDetectionEvaluation(object):
         nms_iou_threshold=nms_iou_threshold,
         nms_max_output_boxes=nms_max_output_boxes)
     self.num_class = num_groundtruth_classes
+    self.use_weighted_mean_ap = use_weighted_mean_ap
     self.label_id_offset = label_id_offset
 
     self.groundtruth_boxes = {}
@@ -485,6 +486,9 @@ class ObjectDetectionEvaluation(object):
     self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)
     self.num_gt_imgs_per_class = np.zeros(self.num_class, dtype=int)
 
+    self._initialize_detections()
+
+  def _initialize_detections(self):
     self.detection_keys = set()
     self.scores_per_class = [[] for _ in range(self.num_class)]
     self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]
@@ -495,17 +499,8 @@ class ObjectDetectionEvaluation(object):
     self.recalls_per_class = []
     self.corloc_per_class = np.ones(self.num_class, dtype=float)
 
-    self.use_weighted_mean_ap = use_weighted_mean_ap
-
   def clear_detections(self):
-    self.detection_keys = {}
-    self.scores_per_class = [[] for _ in range(self.num_class)]
-    self.tp_fp_labels_per_class = [[] for _ in range(self.num_class)]
-    self.num_images_correctly_detected_per_class = np.zeros(self.num_class)
-    self.average_precision_per_class = np.zeros(self.num_class, dtype=float)
-    self.precisions_per_class = []
-    self.recalls_per_class = []
-    self.corloc_per_class = np.ones(self.num_class, dtype=float)
+    self._initialize_detections()
 
   def add_single_ground_truth_image_info(self,
                                          image_key,
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 374c691b..348a0254 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -232,8 +232,10 @@ def padded_one_hot_encoding(indices, depth, left_pad):
     raise ValueError('`left_pad` must be a non-negative integer.')
   if depth == 0:
     return None
-  if len(indices.get_shape().as_list()) != 1:
-    raise ValueError('`indices` must have rank 1')
+
+  rank = len(indices.get_shape().as_list())
+  if rank != 1:
+    raise ValueError('`indices` must have rank 1, but has rank=%s' % rank)
 
   def one_hot_and_pad():
     one_hot = tf.cast(tf.one_hot(tf.cast(indices, tf.int64), depth,
@@ -792,3 +794,28 @@ def nearest_neighbor_upsampling(input_tensor, scale):
   resized_tensor = tf.tile(data_reshaped, [1, 1, scale, 1, scale, 1])
   resized_tensor = tf.reshape(resized_tensor, shape_after_tile)
   return resized_tensor
+
+
+def matmul_gather_on_zeroth_axis(params, indices, scope=None):
+  """Matrix multiplication based implementation of tf.gather on zeroth axis.
+
+  TODO(rathodv, jonathanhuang): enable sparse matmul option.
+
+  Args:
+    params: A float32 Tensor. The tensor from which to gather values.
+      Must be at least rank 1.
+    indices: A Tensor. Must be one of the following types: int32, int64.
+      Must be in range [0, params.shape[0])
+    scope: A name for the operation (optional).
+
+  Returns:
+    A Tensor. Has the same type as params. Values from params gathered
+    from indices given by indices, with shape indices.shape + params.shape[1:].
+  """
+  with tf.name_scope(scope, 'MatMulGather'):
+    index_range = params.shape[0]
+    params2d = tf.reshape(params, [index_range, -1])
+    indicator_matrix = tf.one_hot(indices, index_range)
+    gathered_result_flattened = tf.matmul(indicator_matrix, params2d)
+    return tf.reshape(gathered_result_flattened,
+                      indices.shape.concatenate(params.shape[1:]))
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 97ec48e9..805ce3af 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -1179,5 +1179,52 @@ class NearestNeighborUpsamplingTest(test_case.TestCase):
     self.assertAllClose(custom_op_output, tf_op_output)
 
 
+class MatmulGatherOnZerothAxis(test_case.TestCase):
+
+  def test_gather_2d(self):
+
+    def graph_fn(params, indices):
+      return ops.matmul_gather_on_zeroth_axis(params, indices)
+
+    params = np.array([[1, 2, 3, 4],
+                       [5, 6, 7, 8],
+                       [9, 10, 11, 12],
+                       [0, 1, 0, 0]], dtype=np.float32)
+    indices = np.array([2, 2, 1])
+    expected_output = np.array([[9, 10, 11, 12], [9, 10, 11, 12], [5, 6, 7, 8]])
+    gather_output = self.execute(graph_fn, [params, indices])
+    self.assertAllClose(gather_output, expected_output)
+
+  def test_gather_3d(self):
+
+    def graph_fn(params, indices):
+      return ops.matmul_gather_on_zeroth_axis(params, indices)
+
+    params = np.array([[[1, 2], [3, 4]],
+                       [[5, 6], [7, 8]],
+                       [[9, 10], [11, 12]],
+                       [[0, 1], [0, 0]]], dtype=np.float32)
+    indices = np.array([0, 3, 1])
+    expected_output = np.array([[[1, 2], [3, 4]],
+                                [[0, 1], [0, 0]],
+                                [[5, 6], [7, 8]]])
+    gather_output = self.execute(graph_fn, [params, indices])
+    self.assertAllClose(gather_output, expected_output)
+
+  def test_gather_with_many_indices(self):
+
+    def graph_fn(params, indices):
+      return ops.matmul_gather_on_zeroth_axis(params, indices)
+
+    params = np.array([[1, 2, 3, 4],
+                       [5, 6, 7, 8],
+                       [9, 10, 11, 12],
+                       [0, 1, 0, 0]], dtype=np.float32)
+    indices = np.array([0, 0, 0, 0, 0, 0])
+    expected_output = np.array(6*[[1, 2, 3, 4]])
+    gather_output = self.execute(graph_fn, [params, indices])
+    self.assertAllClose(gather_output, expected_output)
+
+
 if __name__ == '__main__':
   tf.test.main()
