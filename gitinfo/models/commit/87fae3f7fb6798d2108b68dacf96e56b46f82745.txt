commit 87fae3f7fb6798d2108b68dacf96e56b46f82745
Author: Andrew M. Dai <adai@google.com>
Date:   Tue Feb 27 18:32:44 2018 -0800

    Added new MaskGAN model.

diff --git a/CODEOWNERS b/CODEOWNERS
index e08e7d3c..eb2e12c4 100644
--- a/CODEOWNERS
+++ b/CODEOWNERS
@@ -18,6 +18,7 @@
 /research/learning_to_remember_rare_events/ @lukaszkaiser @ofirnachum
 /research/lfads/ @jazcollins @susillo
 /research/lm_1b/ @oriolvinyals @panyx0718
+/research/maskgan/ @a-dai
 /research/namignizer/ @knathanieltucker
 /research/neural_gpu/ @lukaszkaiser
 /research/neural_programmer/ @arvind2505
diff --git a/research/README.md b/research/README.md
index b4cb6b84..985411be 100644
--- a/research/README.md
+++ b/research/README.md
@@ -38,6 +38,7 @@ installation](https://www.tensorflow.org/install).
 -   [lfads](lfads): sequential variational autoencoder for analyzing
     neuroscience data.
 -   [lm_1b](lm_1b): language modeling on the one billion word benchmark.
+-   [maskgan](maskgan): text generation with GANs.
 -   [namignizer](namignizer): recognize and generate names.
 -   [neural_gpu](neural_gpu): highly parallel neural computer.
 -   [neural_programmer](neural_programmer): neural network augmented with logic
diff --git a/research/maskgan/README.md b/research/maskgan/README.md
new file mode 100644
index 00000000..0b7eb025
--- /dev/null
+++ b/research/maskgan/README.md
@@ -0,0 +1,90 @@
+# MaskGAN: Better Text Generation via Filling in the ______
+
+Code for [*MaskGAN: Better Text Generation via Filling in the
+______*](https://arxiv.org/abs/1801.07736) published at ICLR 2018.
+
+## Requirements
+
+*   TensorFlow >= v1.3
+
+## Instructions
+
+Warning: The open-source version of this code is still in the process of being
+tested. Pretraining may not work correctly.
+
+For training on PTB:
+
+1. (Optional) Pretrain a LM on PTB and store the checkpoint in /tmp/pretrain-lm/.
+Instructions WIP.
+
+2. (Optional) Run MaskGAN in MLE pretraining mode:
+
+```bash
+python train_mask_gan.py \
+ --data_dir='/tmp/ptb' \
+ --batch_size=20 \
+ --sequence_length=20 \
+ --base_directory='/tmp/maskGAN' \
+ --hparams="gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.00074876,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=1,gen_learning_rate_decay=0.95" \
+ --mode='TRAIN' \
+ --max_steps=100000 \
+ --language_model_ckpt_dir=/tmp/pretrain-lm/ \
+ --generator_model='seq2seq_vd' \
+ --discriminator_model='rnn_zaremba' \
+ --is_present_rate=0.5 \
+ --summaries_every=10 \
+ --print_every=250 \
+ --max_num_to_print=3 \
+ --gen_training_strategy=cross_entropy \
+ --seq2seq_share_embedding
+```
+
+3. Run MaskGAN in GAN mode:
+```bash
+python train_mask_gan.py \
+ --data_dir='/tmp/ptb' \
+ --batch_size=128 \
+ --sequence_length=20 \
+ --base_directory='/tmp/maskGAN' \
+ --mask_strategy=contiguous \
+ --maskgan_ckpt='/tmp/maskGAN' \
+ --hparams="gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.000038877,gen_learning_rate_decay=1.0,gen_full_learning_rate_steps=2000000,gen_vd_keep_prob=0.33971,rl_discount_rate=0.89072,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=2,dis_pretrain_learning_rate=0.005,critic_learning_rate=5.1761e-7,dis_vd_keep_prob=0.71940" \
+ --mode='TRAIN' \
+ --max_steps=100000 \
+ --generator_model='seq2seq_vd' \
+ --discriminator_model='seq2seq_vd' \
+ --is_present_rate=0.5 \
+ --summaries_every=250 \
+ --print_every=250 \
+ --max_num_to_print=3 \
+ --gen_training_strategy='reinforce' \
+ --seq2seq_share_embedding=true \
+ --baseline_method=critic \
+ --attention_option=luong
+```
+
+4. Generate samples:
+```bash
+python generate_samples.py \
+ --data_dir /tmp/ptb/ \
+ --data_set=ptb \
+ --batch_size=256 \
+ --sequence_length=20 \
+ --base_directory /tmp/imdbsample/ \
+ --hparams="gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,gen_vd_keep_prob=0.33971" \
+ --generator_model=seq2seq_vd \
+ --discriminator_model=seq2seq_vd \
+ --is_present_rate=0.0 \
+ --maskgan_ckpt=/tmp/maskGAN \
+ --seq2seq_share_embedding=True \
+ --dis_share_embedding=True \
+ --attention_option=luong \
+ --mask_strategy=contiguous \
+ --baseline_method=critic \
+ --number_epochs=4
+```
+
+## Contact for Issues
+
+*   Liam Fedus, @liamb315 <liam.fedus@gmail.com>
+*   Andrew M. Dai, @a-dai <adai@google.com>
diff --git a/research/maskgan/data/__init__.py b/research/maskgan/data/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/data/imdb_loader.py b/research/maskgan/data/imdb_loader.py
new file mode 100644
index 00000000..8169b333
--- /dev/null
+++ b/research/maskgan/data/imdb_loader.py
@@ -0,0 +1,136 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""IMDB data loader and helpers."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+# Dependency imports
+import numpy as np
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+tf.app.flags.DEFINE_boolean('prefix_label', True, 'Vocabulary file.')
+
+np.set_printoptions(precision=3)
+np.set_printoptions(suppress=True)
+
+EOS_INDEX = 88892
+
+
+def _read_words(filename, use_prefix=True):
+  all_words = []
+  sequence_example = tf.train.SequenceExample()
+  for r in tf.python_io.tf_record_iterator(filename):
+    sequence_example.ParseFromString(r)
+
+    if FLAGS.prefix_label and use_prefix:
+      label = sequence_example.context.feature['class'].int64_list.value[0]
+      review_words = [EOS_INDEX + 1 + label]
+    else:
+      review_words = []
+    review_words.extend([
+        f.int64_list.value[0]
+        for f in sequence_example.feature_lists.feature_list['token_id'].feature
+    ])
+    all_words.append(review_words)
+  return all_words
+
+
+def build_vocab(vocab_file):
+  word_to_id = {}
+
+  with tf.gfile.GFile(vocab_file, 'r') as f:
+    index = 0
+    for word in f:
+      word_to_id[word.strip()] = index
+      index += 1
+    word_to_id['<eos>'] = EOS_INDEX
+
+  return word_to_id
+
+
+def imdb_raw_data(data_path=None):
+  """Load IMDB raw data from data directory "data_path".
+  Reads IMDB tf record files containing integer ids,
+  and performs mini-batching of the inputs.
+  Args:
+    data_path: string path to the directory where simple-examples.tgz has
+      been extracted.
+  Returns:
+    tuple (train_data, valid_data)
+    where each of the data objects can be passed to IMDBIterator.
+  """
+
+  train_path = os.path.join(data_path, 'train_lm.tfrecords')
+  valid_path = os.path.join(data_path, 'test_lm.tfrecords')
+
+  train_data = _read_words(train_path)
+  valid_data = _read_words(valid_path)
+  return train_data, valid_data
+
+
+def imdb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):
+  """Iterate on the raw IMDB data.
+
+  This generates batch_size pointers into the raw IMDB data, and allows
+  minibatch iteration along these pointers.
+
+  Args:
+    raw_data: one of the raw data outputs from imdb_raw_data.
+    batch_size: int, the batch size.
+    num_steps: int, the number of unrolls.
+
+  Yields:
+    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].
+    The second element of the tuple is the same data time-shifted to the
+    right by one. The third is a set of weights with 1 indicating a word was
+    present and 0 not.
+
+  Raises:
+    ValueError: if batch_size or num_steps are too high.
+  """
+  del epoch_size_override
+  data_len = len(raw_data)
+  num_batches = data_len // batch_size - 1
+
+  for batch in range(num_batches):
+    x = np.zeros([batch_size, num_steps], dtype=np.int32)
+    y = np.zeros([batch_size, num_steps], dtype=np.int32)
+    w = np.zeros([batch_size, num_steps], dtype=np.float)
+
+    for i in range(batch_size):
+      data_index = batch * batch_size + i
+      example = raw_data[data_index]
+
+      if len(example) > num_steps:
+        final_x = example[:num_steps]
+        final_y = example[1:(num_steps + 1)]
+        w[i] = 1
+
+      else:
+        to_fill_in = num_steps - len(example)
+        final_x = example + [EOS_INDEX] * to_fill_in
+        final_y = final_x[1:] + [EOS_INDEX]
+        w[i] = [1] * len(example) + [0] * to_fill_in
+
+      x[i] = final_x
+      y[i] = final_y
+
+    yield (x, y, w)
diff --git a/research/maskgan/data/ptb_loader.py b/research/maskgan/data/ptb_loader.py
new file mode 100644
index 00000000..43105952
--- /dev/null
+++ b/research/maskgan/data/ptb_loader.py
@@ -0,0 +1,123 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""PTB data loader and helpers."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import os
+# Dependency imports
+import numpy as np
+
+import tensorflow as tf
+
+EOS_INDEX = 0
+
+
+def _read_words(filename):
+  with tf.gfile.GFile(filename, "r") as f:
+    return f.read().decode("utf-8").replace("\n", "<eos>").split()
+
+
+def build_vocab(filename):
+  data = _read_words(filename)
+
+  counter = collections.Counter(data)
+  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))
+
+  words, _ = list(zip(*count_pairs))
+  word_to_id = dict(zip(words, range(len(words))))
+  print("<eos>:", word_to_id["<eos>"])
+  global EOS_INDEX
+  EOS_INDEX = word_to_id["<eos>"]
+
+  return word_to_id
+
+
+def _file_to_word_ids(filename, word_to_id):
+  data = _read_words(filename)
+  return [word_to_id[word] for word in data if word in word_to_id]
+
+
+def ptb_raw_data(data_path=None):
+  """Load PTB raw data from data directory "data_path".
+  Reads PTB text files, converts strings to integer ids,
+  and performs mini-batching of the inputs.
+  The PTB dataset comes from Tomas Mikolov's webpage:
+  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz
+  Args:
+    data_path: string path to the directory where simple-examples.tgz has
+      been extracted.
+  Returns:
+    tuple (train_data, valid_data, test_data, vocabulary)
+    where each of the data objects can be passed to PTBIterator.
+  """
+
+  train_path = os.path.join(data_path, "ptb.train.txt")
+  valid_path = os.path.join(data_path, "ptb.valid.txt")
+  test_path = os.path.join(data_path, "ptb.test.txt")
+
+  word_to_id = build_vocab(train_path)
+  train_data = _file_to_word_ids(train_path, word_to_id)
+  valid_data = _file_to_word_ids(valid_path, word_to_id)
+  test_data = _file_to_word_ids(test_path, word_to_id)
+  vocabulary = len(word_to_id)
+  return train_data, valid_data, test_data, vocabulary
+
+
+def ptb_iterator(raw_data, batch_size, num_steps, epoch_size_override=None):
+  """Iterate on the raw PTB data.
+
+  This generates batch_size pointers into the raw PTB data, and allows
+  minibatch iteration along these pointers.
+
+  Args:
+    raw_data: one of the raw data outputs from ptb_raw_data.
+    batch_size: int, the batch size.
+    num_steps: int, the number of unrolls.
+
+  Yields:
+    Pairs of the batched data, each a matrix of shape [batch_size, num_steps].
+    The second element of the tuple is the same data time-shifted to the
+    right by one.
+
+  Raises:
+    ValueError: if batch_size or num_steps are too high.
+  """
+  raw_data = np.array(raw_data, dtype=np.int32)
+
+  data_len = len(raw_data)
+  batch_len = data_len // batch_size
+  data = np.full([batch_size, batch_len], EOS_INDEX, dtype=np.int32)
+  for i in range(batch_size):
+    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]
+
+  if epoch_size_override:
+    epoch_size = epoch_size_override
+  else:
+    epoch_size = (batch_len - 1) // num_steps
+
+  if epoch_size == 0:
+    raise ValueError("epoch_size == 0, decrease batch_size or num_steps")
+
+  # print("Number of batches per epoch: %d" % epoch_size)
+  for i in range(epoch_size):
+    x = data[:, i * num_steps:(i + 1) * num_steps]
+    y = data[:, i * num_steps + 1:(i + 1) * num_steps + 1]
+    w = np.ones_like(x)
+    yield (x, y, w)
diff --git a/research/maskgan/generate_samples.py b/research/maskgan/generate_samples.py
new file mode 100644
index 00000000..8b4e417d
--- /dev/null
+++ b/research/maskgan/generate_samples.py
@@ -0,0 +1,281 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Generate samples from the MaskGAN.
+
+Launch command:
+  python generate_samples.py
+  --data_dir=/tmp/data/imdb  --data_set=imdb
+  --batch_size=256 --sequence_length=20 --base_directory=/tmp/imdb
+  --hparams="gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,
+  gen_vd_keep_prob=1.0" --generator_model=seq2seq_vd
+  --discriminator_model=seq2seq_vd --is_present_rate=0.5
+  --maskgan_ckpt=/tmp/model.ckpt-45494
+  --seq2seq_share_embedding=True --dis_share_embedding=True
+  --attention_option=luong --mask_strategy=contiguous --baseline_method=critic
+  --number_epochs=4
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from functools import partial
+import os
+# Dependency imports
+
+import numpy as np
+
+import tensorflow as tf
+
+import train_mask_gan
+from data import imdb_loader
+from data import ptb_loader
+
+# Data.
+from model_utils import helper
+from model_utils import model_utils
+
+SAMPLE_TRAIN = 'TRAIN'
+SAMPLE_VALIDATION = 'VALIDATION'
+
+## Sample Generation.
+## Binary and setup FLAGS.
+tf.app.flags.DEFINE_enum('sample_mode', 'TRAIN',
+                         [SAMPLE_TRAIN, SAMPLE_VALIDATION],
+                         'Dataset to sample from.')
+tf.app.flags.DEFINE_string('output_path', '/tmp', 'Model output directory.')
+tf.app.flags.DEFINE_boolean(
+    'output_masked_logs', False,
+    'Whether to display for human evaluation (show masking).')
+tf.app.flags.DEFINE_integer('number_epochs', 1,
+                            'The number of epochs to produce.')
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def get_iterator(data):
+  """Return the data iterator."""
+  if FLAGS.data_set == 'ptb':
+    iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,
+                                       FLAGS.sequence_length,
+                                       FLAGS.epoch_size_override)
+  elif FLAGS.data_set == 'imdb':
+    iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,
+                                         FLAGS.sequence_length)
+  return iterator
+
+
+def convert_to_human_readable(id_to_word, arr, p, max_num_to_print):
+  """Convert a np.array of indices into words using id_to_word dictionary.
+  Return max_num_to_print results.
+  """
+
+  assert arr.ndim == 2
+
+  samples = []
+  for sequence_id in xrange(min(len(arr), max_num_to_print)):
+    sample = []
+    for i, index in enumerate(arr[sequence_id, :]):
+      if p[sequence_id, i] == 1:
+        sample.append(str(id_to_word[index]))
+      else:
+        sample.append('*' + str(id_to_word[index]))
+    buffer_str = ' '.join(sample)
+    samples.append(buffer_str)
+  return samples
+
+
+def write_unmasked_log(log, id_to_word, sequence_eval):
+  """Helper function for logging evaluated sequences without mask."""
+  indices_arr = np.asarray(sequence_eval)
+  samples = helper.convert_to_human_readable(id_to_word, indices_arr,
+                                             FLAGS.batch_size)
+  for sample in samples:
+    log.write(sample + '\n')
+  log.flush()
+  return samples
+
+
+def write_masked_log(log, id_to_word, sequence_eval, present_eval):
+  indices_arr = np.asarray(sequence_eval)
+  samples = convert_to_human_readable(id_to_word, indices_arr, present_eval,
+                                      FLAGS.batch_size)
+  for sample in samples:
+    log.write(sample + '\n')
+  log.flush()
+  return samples
+
+
+def generate_logs(sess, model, log, id_to_word, feed):
+  """Impute Sequences using the model for a particular feed and send it to
+  logs.
+  """
+  # Impute Sequences.
+  [p, inputs_eval, sequence_eval] = sess.run(
+      [model.present, model.inputs, model.fake_sequence], feed_dict=feed)
+
+  # Add the 0th time-step for coherence.
+  first_token = np.expand_dims(inputs_eval[:, 0], axis=1)
+  sequence_eval = np.concatenate((first_token, sequence_eval), axis=1)
+
+  # 0th token always present.
+  p = np.concatenate((np.ones((FLAGS.batch_size, 1)), p), axis=1)
+
+  if FLAGS.output_masked_logs:
+    samples = write_masked_log(log, id_to_word, sequence_eval, p)
+  else:
+    samples = write_unmasked_log(log, id_to_word, sequence_eval)
+  return samples
+
+
+def generate_samples(hparams, data, id_to_word, log_dir, output_file):
+  """"Generate samples.
+
+    Args:
+      hparams:  Hyperparameters for the MaskGAN.
+      data: Data to evaluate.
+      id_to_word: Dictionary of indices to words.
+      log_dir: Log directory.
+      output_file:  Output file for the samples.
+  """
+  # Boolean indicating operational mode.
+  is_training = False
+
+  # Set a random seed to keep fixed mask.
+  np.random.seed(0)
+
+  with tf.Graph().as_default():
+    # Construct the model.
+    model = train_mask_gan.create_MaskGAN(hparams, is_training)
+
+    ## Retrieve the initial savers.
+    init_savers = model_utils.retrieve_init_savers(hparams)
+
+    ## Initial saver function to supervisor.
+    init_fn = partial(model_utils.init_fn, init_savers)
+
+    is_chief = FLAGS.task == 0
+
+    # Create the supervisor.  It will take care of initialization, summaries,
+    # checkpoints, and recovery.
+    sv = tf.Supervisor(
+        logdir=log_dir,
+        is_chief=is_chief,
+        saver=model.saver,
+        global_step=model.global_step,
+        recovery_wait_secs=30,
+        summary_op=None,
+        init_fn=init_fn)
+
+    # Get an initialized, and possibly recovered session.  Launch the
+    # services: Checkpointing, Summaries, step counting.
+    #
+    # When multiple replicas of this program are running the services are
+    # only launched by the 'chief' replica.
+    with sv.managed_session(
+        FLAGS.master, start_standard_services=False) as sess:
+
+      # Generator statefulness over the epoch.
+      [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(
+          [model.eval_initial_state, model.fake_gen_initial_state])
+
+      for n in xrange(FLAGS.number_epochs):
+        print('Epoch number: %d' % n)
+        # print('Percent done: %.2f' % float(n) / float(FLAGS.number_epochs))
+        iterator = get_iterator(data)
+        for x, y, _ in iterator:
+          if FLAGS.eval_language_model:
+            is_present_rate = 0.
+          else:
+            is_present_rate = FLAGS.is_present_rate
+          tf.logging.info(
+              'Evaluating on is_present_rate=%.3f.' % is_present_rate)
+
+          model_utils.assign_percent_real(sess, model.percent_real_update,
+                                          model.new_rate, is_present_rate)
+
+          # Randomly mask out tokens.
+          p = model_utils.generate_mask()
+
+          eval_feed = {model.inputs: x, model.targets: y, model.present: p}
+
+          if FLAGS.data_set == 'ptb':
+            # Statefulness for *evaluation* Generator.
+            for i, (c, h) in enumerate(model.eval_initial_state):
+              eval_feed[c] = gen_initial_state_eval[i].c
+              eval_feed[h] = gen_initial_state_eval[i].h
+
+            # Statefulness for the Generator.
+            for i, (c, h) in enumerate(model.fake_gen_initial_state):
+              eval_feed[c] = fake_gen_initial_state_eval[i].c
+              eval_feed[h] = fake_gen_initial_state_eval[i].h
+
+          [gen_initial_state_eval, fake_gen_initial_state_eval, _] = sess.run(
+              [
+                  model.eval_final_state, model.fake_gen_final_state,
+                  model.global_step
+              ],
+              feed_dict=eval_feed)
+
+          generate_logs(sess, model, output_file, id_to_word, eval_feed)
+      output_file.close()
+      print('Closing output_file.')
+      return
+
+
+def main(_):
+  hparams = train_mask_gan.create_hparams()
+  log_dir = FLAGS.base_directory
+
+  tf.gfile.MakeDirs(FLAGS.output_path)
+  output_file = tf.gfile.GFile(
+      os.path.join(FLAGS.output_path, 'reviews.txt'), mode='w')
+
+  # Load data set.
+  if FLAGS.data_set == 'ptb':
+    raw_data = ptb_loader.ptb_raw_data(FLAGS.data_dir)
+    train_data, valid_data, _, _ = raw_data
+  elif FLAGS.data_set == 'imdb':
+    raw_data = imdb_loader.imdb_raw_data(FLAGS.data_dir)
+    train_data, valid_data = raw_data
+  else:
+    raise NotImplementedError
+
+  # Generating more data on train set.
+  if FLAGS.sample_mode == SAMPLE_TRAIN:
+    data_set = train_data
+  elif FLAGS.sample_mode == SAMPLE_VALIDATION:
+    data_set = valid_data
+  else:
+    raise NotImplementedError
+
+  # Dictionary and reverse dictionry.
+  if FLAGS.data_set == 'ptb':
+    word_to_id = ptb_loader.build_vocab(
+        os.path.join(FLAGS.data_dir, 'ptb.train.txt'))
+  elif FLAGS.data_set == 'imdb':
+    word_to_id = imdb_loader.build_vocab(
+        os.path.join(FLAGS.data_dir, 'vocab.txt'))
+  id_to_word = {v: k for k, v in word_to_id.iteritems()}
+
+  FLAGS.vocab_size = len(id_to_word)
+  print('Vocab size: %d' % FLAGS.vocab_size)
+
+  generate_samples(hparams, data_set, id_to_word, log_dir, output_file)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/maskgan/losses/__init__.py b/research/maskgan/losses/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/losses/losses.py b/research/maskgan/losses/losses.py
new file mode 100644
index 00000000..38d0e7b4
--- /dev/null
+++ b/research/maskgan/losses/losses.py
@@ -0,0 +1,186 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Losses for Generator and Discriminator."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def discriminator_loss(predictions, labels, missing_tokens):
+  """Discriminator loss based on predictions and labels.
+
+  Args:
+    predictions:  Discriminator linear predictions Tensor of shape [batch_size,
+      sequence_length]
+    labels: Labels for predictions, Tensor of shape [batch_size,
+      sequence_length]
+    missing_tokens:  Indicator for the missing tokens.  Evaluate the loss only
+      on the tokens that were missing.
+
+  Returns:
+    loss:  Scalar tf.float32 loss.
+
+  """
+  loss = tf.losses.sigmoid_cross_entropy(labels,
+                                         predictions,
+                                         weights=missing_tokens)
+  loss = tf.Print(
+      loss, [loss, labels, missing_tokens],
+      message='loss, labels, missing_tokens',
+      summarize=25,
+      first_n=25)
+  return loss
+
+
+def cross_entropy_loss_matrix(gen_labels, gen_logits):
+  """Computes the cross entropy loss for G.
+
+  Args:
+    gen_labels:  Labels for the correct token.
+    gen_logits: Generator logits.
+
+  Returns:
+    loss_matrix:  Loss matrix of shape [batch_size, sequence_length].
+  """
+  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
+      labels=gen_labels, logits=gen_logits)
+  return cross_entropy_loss
+
+
+def GAN_loss_matrix(dis_predictions):
+  """Computes the cross entropy loss for G.
+
+  Args:
+    dis_predictions:  Discriminator predictions.
+
+  Returns:
+    loss_matrix: Loss matrix of shape [batch_size, sequence_length].
+  """
+  eps = tf.constant(1e-7, tf.float32)
+  gan_loss_matrix = -tf.log(dis_predictions + eps)
+  return gan_loss_matrix
+
+
+def generator_GAN_loss(predictions):
+  """Generator GAN loss based on Discriminator predictions."""
+  return -tf.log(tf.reduce_mean(predictions))
+
+
+def generator_blended_forward_loss(gen_logits, gen_labels, dis_predictions,
+                                   is_real_input):
+  """Computes the masked-loss for G.  This will be a blend of cross-entropy
+  loss where the true label is known and GAN loss where the true label has been
+  masked.
+
+  Args:
+    gen_logits: Generator logits.
+    gen_labels:  Labels for the correct token.
+    dis_predictions:  Discriminator predictions.
+    is_real_input:  Tensor indicating whether the label is present.
+
+  Returns:
+    loss: Scalar tf.float32 total loss.
+  """
+  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
+      labels=gen_labels, logits=gen_logits)
+  gan_loss = -tf.log(dis_predictions)
+  loss_matrix = tf.where(is_real_input, cross_entropy_loss, gan_loss)
+  return tf.reduce_mean(loss_matrix)
+
+
+def wasserstein_generator_loss(gen_logits, gen_labels, dis_values,
+                               is_real_input):
+  """Computes the masked-loss for G.  This will be a blend of cross-entropy
+  loss where the true label is known and GAN loss where the true label is
+  missing.
+
+  Args:
+    gen_logits:  Generator logits.
+    gen_labels:  Labels for the correct token.
+    dis_values:  Discriminator values Tensor of shape [batch_size,
+      sequence_length].
+    is_real_input:  Tensor indicating whether the label is present.
+
+  Returns:
+    loss: Scalar tf.float32 total loss.
+  """
+  cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
+      labels=gen_labels, logits=gen_logits)
+  # Maximize the dis_values (minimize the negative)
+  gan_loss = -dis_values
+  loss_matrix = tf.where(is_real_input, cross_entropy_loss, gan_loss)
+  loss = tf.reduce_mean(loss_matrix)
+  return loss
+
+
+def wasserstein_discriminator_loss(real_values, fake_values):
+  """Wasserstein discriminator loss.
+
+  Args:
+    real_values: Value given by the Wasserstein Discriminator to real data.
+    fake_values: Value given by the Wasserstein Discriminator to fake data.
+
+  Returns:
+    loss:  Scalar tf.float32 loss.
+
+  """
+  real_avg = tf.reduce_mean(real_values)
+  fake_avg = tf.reduce_mean(fake_values)
+
+  wasserstein_loss = real_avg - fake_avg
+  return wasserstein_loss
+
+
+def wasserstein_discriminator_loss_intrabatch(values, is_real_input):
+  """Wasserstein discriminator loss.  This is an odd variant where the value
+  difference is between the real tokens and the fake tokens within a single
+  batch.
+
+  Args:
+    values: Value given by the Wasserstein Discriminator of shape [batch_size,
+      sequence_length] to an imputed batch (real and fake).
+    is_real_input: tf.bool Tensor of shape [batch_size, sequence_length]. If
+      true, it indicates that the label is known.
+
+  Returns:
+    wasserstein_loss:  Scalar tf.float32 loss.
+
+  """
+  zero_tensor = tf.constant(0., dtype=tf.float32, shape=[])
+
+  present = tf.cast(is_real_input, tf.float32)
+  missing = tf.cast(1 - present, tf.float32)
+
+  # Counts for real and fake tokens.
+  real_count = tf.reduce_sum(present)
+  fake_count = tf.reduce_sum(missing)
+
+  # Averages for real and fake token values.
+  real = tf.mul(values, present)
+  fake = tf.mul(values, missing)
+  real_avg = tf.reduce_sum(real) / real_count
+  fake_avg = tf.reduce_sum(fake) / fake_count
+
+  # If there are no real or fake entries in the batch, we assign an average
+  # value of zero.
+  real_avg = tf.where(tf.equal(real_count, 0), zero_tensor, real_avg)
+  fake_avg = tf.where(tf.equal(fake_count, 0), zero_tensor, fake_avg)
+
+  wasserstein_loss = real_avg - fake_avg
+  return wasserstein_loss
diff --git a/research/maskgan/model_utils/__init__.py b/research/maskgan/model_utils/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/model_utils/helper.py b/research/maskgan/model_utils/helper.py
new file mode 100644
index 00000000..2913eac0
--- /dev/null
+++ b/research/maskgan/model_utils/helper.py
@@ -0,0 +1,157 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Random helper functions for converting between indices and one-hot encodings
+as well as printing/logging helpers.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+
+def variable_summaries(var, name):
+  """Attach a lot of summaries to a Tensor."""
+  mean = tf.reduce_mean(var)
+  tf.summary.scalar('mean/' + name, mean)
+  with tf.name_scope('stddev'):
+    stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))
+  tf.summary.scalar('sttdev/' + name, stddev)
+  tf.summary.scalar('max/' + name, tf.reduce_max(var))
+  tf.summary.scalar('min/' + name, tf.reduce_min(var))
+  tf.summary.histogram(name, var)
+
+
+def zip_seq_pred_crossent(id_to_word, sequences, predictions, cross_entropy):
+  """Zip together the sequences, predictions, cross entropy."""
+  indices = convert_to_indices(sequences)
+
+  batch_of_metrics = []
+
+  for ind_batch, pred_batch, crossent_batch in zip(indices, predictions,
+                                                   cross_entropy):
+    metrics = []
+
+    for index, pred, crossent in zip(ind_batch, pred_batch, crossent_batch):
+      metrics.append([str(id_to_word[index]), pred, crossent])
+
+    batch_of_metrics.append(metrics)
+  return batch_of_metrics
+
+
+def print_and_log(log, id_to_word, sequence_eval, max_num_to_print=5):
+  """Helper function for printing and logging evaluated sequences."""
+  indices_eval = convert_to_indices(sequence_eval)
+  indices_arr = np.asarray(indices_eval)
+  samples = convert_to_human_readable(id_to_word, indices_arr, max_num_to_print)
+
+  for i, sample in enumerate(samples):
+    print('Sample', i, '. ', sample)
+    log.write('\nSample ' + str(i) + '. ' + sample)
+  log.write('\n')
+  print('\n')
+  log.flush()
+
+
+def convert_to_human_readable(id_to_word, arr, max_num_to_print):
+  """Convert a np.array of indices into words using id_to_word dictionary.
+  Return max_num_to_print results.
+  """
+  assert arr.ndim == 2
+
+  samples = []
+  for sequence_id in xrange(min(len(arr), max_num_to_print)):
+    buffer_str = ' '.join(
+        [str(id_to_word[index]) for index in arr[sequence_id, :]])
+    samples.append(buffer_str)
+  return samples
+
+
+def index_to_vocab_array(indices, vocab_size, sequence_length):
+  """Convert the indices into an array with vocab_size one-hot encoding."""
+
+  # Extract properties of the indices.
+  num_batches = len(indices)
+  shape = list(indices.shape)
+  shape.append(vocab_size)
+
+  # Construct the vocab_size array.
+  new_arr = np.zeros(shape)
+
+  for n in xrange(num_batches):
+    indices_batch = indices[n]
+    new_arr_batch = new_arr[n]
+
+    # We map all indices greater than the vocabulary size to an unknown
+    # character.
+    indices_batch = np.where(indices_batch < vocab_size, indices_batch,
+                             vocab_size - 1)
+
+    # Convert indices to vocab_size dimensions.
+    new_arr_batch[np.arange(sequence_length), indices_batch] = 1
+  return new_arr
+
+
+def convert_to_indices(sequences):
+  """Convert a list of size [batch_size, sequence_length, vocab_size] to
+  a list of size [batch_size, sequence_length] where the vocab element is
+  denoted by the index.
+  """
+  batch_of_indices = []
+
+  for sequence in sequences:
+    indices = []
+    for embedding in sequence:
+      indices.append(np.argmax(embedding))
+    batch_of_indices.append(indices)
+  return batch_of_indices
+
+
+def convert_and_zip(id_to_word, sequences, predictions):
+  """Helper function for printing or logging.  Retrieves list of sequences
+  and predictions and zips them together.
+  """
+  indices = convert_to_indices(sequences)
+
+  batch_of_indices_predictions = []
+
+  for index_batch, pred_batch in zip(indices, predictions):
+    indices_predictions = []
+
+    for index, pred in zip(index_batch, pred_batch):
+      indices_predictions.append([str(id_to_word[index]), pred])
+    batch_of_indices_predictions.append(indices_predictions)
+  return batch_of_indices_predictions
+
+
+def recursive_length(item):
+  """Recursively determine the total number of elements in nested list."""
+  if type(item) == list:
+    return sum(recursive_length(subitem) for subitem in item)
+  else:
+    return 1.
+
+
+def percent_correct(real_sequence, fake_sequences):
+  """Determine the percent of tokens correctly generated within a batch."""
+  identical = 0.
+  for fake_sequence in fake_sequences:
+    for real, fake in zip(real_sequence, fake_sequence):
+      if real == fake:
+        identical += 1.
+  return identical / recursive_length(fake_sequences)
diff --git a/research/maskgan/model_utils/model_construction.py b/research/maskgan/model_utils/model_construction.py
new file mode 100644
index 00000000..8dfa1df3
--- /dev/null
+++ b/research/maskgan/model_utils/model_construction.py
@@ -0,0 +1,234 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Model construction."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+
+import tensorflow as tf
+from models import bidirectional
+from models import bidirectional_vd
+
+from models import bidirectional_zaremba
+from models import cnn
+from models import critic_vd
+from models import feedforward
+from models import rnn
+from models import rnn_nas
+from models import rnn_vd
+from models import rnn_zaremba
+from models import seq2seq
+from models import seq2seq_nas
+from models import seq2seq_vd
+from models import seq2seq_zaremba
+
+FLAGS = tf.app.flags.FLAGS
+
+
+# TODO(adai): IMDB labels placeholder to model.
+def create_generator(hparams,
+                     inputs,
+                     targets,
+                     present,
+                     is_training,
+                     is_validating,
+                     reuse=None):
+  """Create the Generator model specified by the FLAGS and hparams.
+
+  Args;
+    hparams:  Hyperparameters for the MaskGAN.
+    inputs:  tf.int32 Tensor of the sequence input of shape [batch_size,
+      sequence_length].
+    present:  tf.bool Tensor indicating the presence or absence of the token
+      of shape [batch_size, sequence_length].
+    is_training:  Whether the model is training.
+    is_validating:  Whether the model is being run in validation mode for
+      calculating the perplexity.
+    reuse (Optional):  Whether to reuse the model.
+
+  Returns:
+    Tuple of the (sequence, logits, log_probs) of the Generator.   Sequence
+      and logits have shape [batch_size, sequence_length, vocab_size].  The
+      log_probs will have shape [batch_size, sequence_length].  Log_probs
+      corresponds to the log probability of selecting the words.
+  """
+  if FLAGS.generator_model == 'rnn':
+    (sequence, logits, log_probs, initial_state, final_state) = rnn.generator(
+        hparams,
+        inputs,
+        targets,
+        present,
+        is_training=is_training,
+        is_validating=is_validating,
+        reuse=reuse)
+  elif FLAGS.generator_model == 'rnn_zaremba':
+    (sequence, logits, log_probs, initial_state,
+     final_state) = rnn_zaremba.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  elif FLAGS.generator_model == 'seq2seq':
+    (sequence, logits, log_probs, initial_state,
+     final_state) = seq2seq.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  elif FLAGS.generator_model == 'seq2seq_zaremba':
+    (sequence, logits, log_probs, initial_state,
+     final_state) = seq2seq_zaremba.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  elif FLAGS.generator_model == 'rnn_nas':
+    (sequence, logits, log_probs, initial_state,
+     final_state) = rnn_nas.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  elif FLAGS.generator_model == 'seq2seq_nas':
+    (sequence, logits, log_probs, initial_state,
+     final_state) = seq2seq_nas.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  elif FLAGS.generator_model == 'seq2seq_vd':
+    (sequence, logits, log_probs, initial_state, final_state,
+     encoder_states) = seq2seq_vd.generator(
+         hparams,
+         inputs,
+         targets,
+         present,
+         is_training=is_training,
+         is_validating=is_validating,
+         reuse=reuse)
+  else:
+    raise NotImplementedError
+  return (sequence, logits, log_probs, initial_state, final_state,
+          encoder_states)
+
+
+def create_discriminator(hparams,
+                         sequence,
+                         is_training,
+                         reuse=None,
+                         initial_state=None,
+                         inputs=None,
+                         present=None):
+  """Create the Discriminator model specified by the FLAGS and hparams.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    sequence:  tf.int32 Tensor sequence of shape [batch_size, sequence_length]
+    is_training:  Whether the model is training.
+    reuse (Optional):  Whether to reuse the model.
+
+  Returns:
+    predictions:  tf.float32 Tensor of predictions of shape [batch_size,
+      sequence_length]
+  """
+  if FLAGS.discriminator_model == 'cnn':
+    predictions = cnn.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'fnn':
+    predictions = feedforward.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'rnn':
+    predictions = rnn.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'bidirectional':
+    predictions = bidirectional.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'bidirectional_zaremba':
+    predictions = bidirectional_zaremba.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'seq2seq_vd':
+    predictions = seq2seq_vd.discriminator(
+        hparams,
+        inputs,
+        present,
+        sequence,
+        is_training=is_training,
+        reuse=reuse)
+  elif FLAGS.discriminator_model == 'rnn_zaremba':
+    predictions = rnn_zaremba.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'rnn_nas':
+    predictions = rnn_nas.discriminator(
+        hparams, sequence, is_training=is_training, reuse=reuse)
+  elif FLAGS.discriminator_model == 'rnn_vd':
+    predictions = rnn_vd.discriminator(
+        hparams,
+        sequence,
+        is_training=is_training,
+        reuse=reuse,
+        initial_state=initial_state)
+  elif FLAGS.discriminator_model == 'bidirectional_vd':
+    predictions = bidirectional_vd.discriminator(
+        hparams,
+        sequence,
+        is_training=is_training,
+        reuse=reuse,
+        initial_state=initial_state)
+  else:
+    raise NotImplementedError
+  return predictions
+
+
+def create_critic(hparams, sequence, is_training, reuse=None):
+  """Create the Critic model specified by the FLAGS and hparams.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    sequence:  tf.int32 Tensor sequence of shape [batch_size, sequence_length]
+    is_training:  Whether the model is training.
+    reuse (Optional):  Whether to reuse the model.
+
+  Returns:
+    values:  tf.float32 Tensor of predictions of shape [batch_size,
+      sequence_length]
+  """
+  if FLAGS.baseline_method == 'critic':
+    if FLAGS.discriminator_model == 'seq2seq_vd':
+      values = critic_vd.critic_seq2seq_vd_derivative(
+          hparams, sequence, is_training, reuse=reuse)
+    else:
+      raise NotImplementedError
+  else:
+    raise NotImplementedError
+  return values
diff --git a/research/maskgan/model_utils/model_losses.py b/research/maskgan/model_utils/model_losses.py
new file mode 100644
index 00000000..d63a63b0
--- /dev/null
+++ b/research/maskgan/model_utils/model_losses.py
@@ -0,0 +1,327 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Model loss construction."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+import numpy as np
+
+import tensorflow as tf
+
+# Useful for REINFORCE baseline.
+from losses import losses
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def create_dis_loss(fake_predictions, real_predictions, targets_present):
+  """Compute Discriminator loss across real/fake."""
+
+  missing = tf.cast(targets_present, tf.int32)
+  missing = 1 - missing
+  missing = tf.cast(missing, tf.bool)
+
+  real_labels = tf.ones([FLAGS.batch_size, FLAGS.sequence_length])
+  dis_loss_real = tf.losses.sigmoid_cross_entropy(
+      real_labels, real_predictions, weights=missing)
+  dis_loss_fake = tf.losses.sigmoid_cross_entropy(
+      targets_present, fake_predictions, weights=missing)
+
+  dis_loss = (dis_loss_fake + dis_loss_real) / 2.
+  return dis_loss, dis_loss_fake, dis_loss_real
+
+
+def create_critic_loss(cumulative_rewards, estimated_values, present):
+  """Compute Critic loss in estimating the value function.  This should be an
+  estimate only for the missing elements."""
+  missing = tf.cast(present, tf.int32)
+  missing = 1 - missing
+  missing = tf.cast(missing, tf.bool)
+
+  loss = tf.losses.mean_squared_error(
+      labels=cumulative_rewards, predictions=estimated_values, weights=missing)
+  return loss
+
+
+def create_masked_cross_entropy_loss(targets, present, logits):
+  """Calculate the cross entropy loss matrices for the masked tokens."""
+  cross_entropy_losses = losses.cross_entropy_loss_matrix(targets, logits)
+
+  # Zeros matrix.
+  zeros_losses = tf.zeros(
+      shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=tf.float32)
+
+  missing_ce_loss = tf.where(present, zeros_losses, cross_entropy_losses)
+
+  return missing_ce_loss
+
+
+def calculate_reinforce_objective(hparams,
+                                  log_probs,
+                                  dis_predictions,
+                                  present,
+                                  estimated_values=None):
+  """Calculate the REINFORCE objectives.  The REINFORCE objective should
+  only be on the tokens that were missing.  Specifically, the final Generator
+  reward should be based on the Discriminator predictions on missing tokens.
+  The log probaibilities should be only for missing tokens and the baseline
+  should be calculated only on the missing tokens.
+
+  For this model, we optimize the reward is the log of the *conditional*
+  probability the Discriminator assigns to the distribution.  Specifically, for
+  a Discriminator D which outputs probability of real, given the past context,
+
+    r_t = log D(x_t|x_0,x_1,...x_{t-1})
+
+  And the policy for Generator G is the log-probability of taking action x2
+  given the past context.
+
+
+  Args:
+    hparams:  MaskGAN hyperparameters.
+    log_probs:  tf.float32 Tensor of log probailities of the tokens selected by
+      the Generator.  Shape [batch_size, sequence_length].
+    dis_predictions:  tf.float32 Tensor of the predictions from the
+      Discriminator.  Shape [batch_size, sequence_length].
+    present:  tf.bool Tensor indicating which tokens are present.  Shape
+      [batch_size, sequence_length].
+    estimated_values:  tf.float32 Tensor of estimated state values of tokens.
+      Shape [batch_size, sequence_length]
+
+  Returns:
+    final_gen_objective:  Final REINFORCE objective for the sequence.
+    rewards:  tf.float32 Tensor of rewards for sequence of shape [batch_size,
+      sequence_length]
+    advantages: tf.float32 Tensor of advantages for sequence of shape
+      [batch_size, sequence_length]
+    baselines:  tf.float32 Tensor of baselines for sequence of shape
+      [batch_size, sequence_length]
+    maintain_averages_op:  ExponentialMovingAverage apply average op to
+      maintain the baseline.
+  """
+  # Final Generator objective.
+  final_gen_objective = 0.
+  gamma = hparams.rl_discount_rate
+  eps = 1e-7
+
+  # Generator rewards are log-probabilities.
+  eps = tf.constant(1e-7, tf.float32)
+  dis_predictions = tf.nn.sigmoid(dis_predictions)
+  rewards = tf.log(dis_predictions + eps)
+
+  # Apply only for missing elements.
+  zeros = tf.zeros_like(present, dtype=tf.float32)
+  log_probs = tf.where(present, zeros, log_probs)
+  rewards = tf.where(present, zeros, rewards)
+
+  # Unstack Tensors into lists.
+  rewards_list = tf.unstack(rewards, axis=1)
+  log_probs_list = tf.unstack(log_probs, axis=1)
+  missing = 1. - tf.cast(present, tf.float32)
+  missing_list = tf.unstack(missing, axis=1)
+
+  # Cumulative Discounted Returns.  The true value function V*(s).
+  cumulative_rewards = []
+  for t in xrange(FLAGS.sequence_length):
+    cum_value = tf.zeros(shape=[FLAGS.batch_size])
+    for s in xrange(t, FLAGS.sequence_length):
+      cum_value += missing_list[s] * np.power(gamma, (s - t)) * rewards_list[s]
+    cumulative_rewards.append(cum_value)
+  cumulative_rewards = tf.stack(cumulative_rewards, axis=1)
+
+  ## REINFORCE with different baselines.
+  # We create a separate critic functionality for the Discriminator.  This
+  # will need to operate unidirectionally and it may take in the past context.
+  if FLAGS.baseline_method == 'critic':
+
+    # Critic loss calculated from the estimated value function \hat{V}(s)
+    # versus the true value function V*(s).
+    critic_loss = create_critic_loss(cumulative_rewards, estimated_values,
+                                     present)
+
+    # Baselines are coming from the critic's estimated state values.
+    baselines = tf.unstack(estimated_values, axis=1)
+
+    ## Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
+    advantages = []
+    for t in xrange(FLAGS.sequence_length):
+      log_probability = log_probs_list[t]
+      cum_advantage = tf.zeros(shape=[FLAGS.batch_size])
+
+      for s in xrange(t, FLAGS.sequence_length):
+        cum_advantage += missing_list[s] * np.power(gamma,
+                                                    (s - t)) * rewards_list[s]
+      cum_advantage -= baselines[t]
+      # Clip advantages.
+      cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping,
+                                       FLAGS.advantage_clipping)
+      advantages.append(missing_list[t] * cum_advantage)
+      final_gen_objective += tf.multiply(
+          log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))
+
+    maintain_averages_op = None
+    baselines = tf.stack(baselines, axis=1)
+    advantages = tf.stack(advantages, axis=1)
+
+  # Split the batch into half.  Use half for MC estimates for REINFORCE.
+  # Use the other half to establish a baseline.
+  elif FLAGS.baseline_method == 'dis_batch':
+    # TODO(liamfedus):  Recheck.
+    [rewards_half, baseline_half] = tf.split(
+        rewards, num_or_size_splits=2, axis=0)
+    [log_probs_half, _] = tf.split(log_probs, num_or_size_splits=2, axis=0)
+    [reward_present_half, baseline_present_half] = tf.split(
+        present, num_or_size_splits=2, axis=0)
+
+    # Unstack to lists.
+    baseline_list = tf.unstack(baseline_half, axis=1)
+    baseline_missing = 1. - tf.cast(baseline_present_half, tf.float32)
+    baseline_missing_list = tf.unstack(baseline_missing, axis=1)
+
+    baselines = []
+    for t in xrange(FLAGS.sequence_length):
+      # Calculate baseline only for missing tokens.
+      num_missing = tf.reduce_sum(baseline_missing_list[t])
+
+      avg_baseline = tf.reduce_sum(
+          baseline_missing_list[t] * baseline_list[t], keep_dims=True) / (
+              num_missing + eps)
+      baseline = tf.tile(avg_baseline, multiples=[FLAGS.batch_size / 2])
+      baselines.append(baseline)
+
+    # Unstack to lists.
+    rewards_list = tf.unstack(rewards_half, axis=1)
+    log_probs_list = tf.unstack(log_probs_half, axis=1)
+    reward_missing = 1. - tf.cast(reward_present_half, tf.float32)
+    reward_missing_list = tf.unstack(reward_missing, axis=1)
+
+    ## Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
+    advantages = []
+    for t in xrange(FLAGS.sequence_length):
+      log_probability = log_probs_list[t]
+      cum_advantage = tf.zeros(shape=[FLAGS.batch_size / 2])
+
+      for s in xrange(t, FLAGS.sequence_length):
+        cum_advantage += reward_missing_list[s] * np.power(gamma, (s - t)) * (
+            rewards_list[s] - baselines[s])
+      # Clip advantages.
+      cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping,
+                                       FLAGS.advantage_clipping)
+      advantages.append(reward_missing_list[t] * cum_advantage)
+      final_gen_objective += tf.multiply(
+          log_probability,
+          reward_missing_list[t] * tf.stop_gradient(cum_advantage))
+
+    # Cumulative Discounted Returns.  The true value function V*(s).
+    cumulative_rewards = []
+    for t in xrange(FLAGS.sequence_length):
+      cum_value = tf.zeros(shape=[FLAGS.batch_size / 2])
+      for s in xrange(t, FLAGS.sequence_length):
+        cum_value += reward_missing_list[s] * np.power(gamma, (
+            s - t)) * rewards_list[s]
+      cumulative_rewards.append(cum_value)
+    cumulative_rewards = tf.stack(cumulative_rewards, axis=1)
+
+    rewards = rewards_half
+    critic_loss = None
+    maintain_averages_op = None
+    baselines = tf.stack(baselines, axis=1)
+    advantages = tf.stack(advantages, axis=1)
+
+  # Exponential Moving Average baseline.
+  elif FLAGS.baseline_method == 'ema':
+    # TODO(liamfedus): Recheck.
+    # Lists of rewards and Log probabilities of the actions taken only for
+    # missing tokens.
+    ema = tf.train.ExponentialMovingAverage(decay=hparams.baseline_decay)
+    maintain_averages_op = ema.apply(rewards_list)
+
+    baselines = []
+    for r in rewards_list:
+      baselines.append(ema.average(r))
+
+    ## Calculate the Advantages, A(s,a) = Q(s,a) - \hat{V}(s).
+    advantages = []
+    for t in xrange(FLAGS.sequence_length):
+      log_probability = log_probs_list[t]
+
+      # Calculate the forward advantage only on the missing tokens.
+      cum_advantage = tf.zeros(shape=[FLAGS.batch_size])
+      for s in xrange(t, FLAGS.sequence_length):
+        cum_advantage += missing_list[s] * np.power(gamma, (s - t)) * (
+            rewards_list[s] - baselines[s])
+      # Clip advantages.
+      cum_advantage = tf.clip_by_value(cum_advantage, -FLAGS.advantage_clipping,
+                                       FLAGS.advantage_clipping)
+      advantages.append(missing_list[t] * cum_advantage)
+      final_gen_objective += tf.multiply(
+          log_probability, missing_list[t] * tf.stop_gradient(cum_advantage))
+
+    critic_loss = None
+    baselines = tf.stack(baselines, axis=1)
+    advantages = tf.stack(advantages, axis=1)
+
+  elif FLAGS.baseline_method is None:
+    num_missing = tf.reduce_sum(missing)
+    final_gen_objective += tf.reduce_sum(rewards) / (num_missing + eps)
+    baselines = tf.zeros_like(rewards)
+    critic_loss = None
+    maintain_averages_op = None
+    advantages = cumulative_rewards
+
+  else:
+    raise NotImplementedError
+
+  return [
+      final_gen_objective, log_probs, rewards, advantages, baselines,
+      maintain_averages_op, critic_loss, cumulative_rewards
+  ]
+
+
+def calculate_log_perplexity(logits, targets, present):
+  """Calculate the average log perplexity per *missing* token.
+
+  Args:
+    logits:  tf.float32 Tensor of the logits of shape [batch_size,
+      sequence_length, vocab_size].
+    targets:  tf.int32 Tensor of the sequence target of shape [batch_size,
+      sequence_length].
+    present:  tf.bool Tensor indicating the presence or absence of the token
+      of shape [batch_size, sequence_length].
+
+  Returns:
+    avg_log_perplexity:  Scalar indicating the average log perplexity per
+      missing token in the batch.
+  """
+  # logits = tf.Print(logits, [logits], message='logits:', summarize=50)
+  # targets = tf.Print(targets, [targets], message='targets:', summarize=50)
+  eps = 1e-12
+  logits = tf.reshape(logits, [-1, FLAGS.vocab_size])
+
+  # Only calculate log-perplexity on missing tokens.
+  weights = tf.cast(present, tf.float32)
+  weights = 1. - weights
+  weights = tf.reshape(weights, [-1])
+  num_missing = tf.reduce_sum(weights)
+
+  log_perplexity = tf.contrib.legacy_seq2seq.sequence_loss_by_example(
+      [logits], [tf.reshape(targets, [-1])], [weights])
+
+  avg_log_perplexity = tf.reduce_sum(log_perplexity) / (num_missing + eps)
+  return avg_log_perplexity
diff --git a/research/maskgan/model_utils/model_optimization.py b/research/maskgan/model_utils/model_optimization.py
new file mode 100644
index 00000000..caae271f
--- /dev/null
+++ b/research/maskgan/model_utils/model_optimization.py
@@ -0,0 +1,194 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Model optimization."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def create_dis_pretrain_op(hparams, dis_loss, global_step):
+  """Create a train op for pretraining."""
+  with tf.name_scope('pretrain_generator'):
+    optimizer = tf.train.AdamOptimizer(hparams.dis_pretrain_learning_rate)
+    dis_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('dis')
+    ]
+    if FLAGS.dis_update_share_embedding and FLAGS.dis_share_embedding:
+      shared_embedding = [
+          v for v in tf.trainable_variables()
+          if v.op.name == 'gen/decoder/rnn/embedding'
+      ][0]
+      dis_vars.append(shared_embedding)
+    dis_grads = tf.gradients(dis_loss, dis_vars)
+    dis_grads_clipped, _ = tf.clip_by_global_norm(dis_grads,
+                                                  FLAGS.grad_clipping)
+    dis_pretrain_op = optimizer.apply_gradients(
+        zip(dis_grads_clipped, dis_vars), global_step=global_step)
+    return dis_pretrain_op
+
+
+def create_gen_pretrain_op(hparams, cross_entropy_loss, global_step):
+  """Create a train op for pretraining."""
+  with tf.name_scope('pretrain_generator'):
+    optimizer = tf.train.AdamOptimizer(hparams.gen_pretrain_learning_rate)
+    gen_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('gen')
+    ]
+    gen_grads = tf.gradients(cross_entropy_loss, gen_vars)
+    gen_grads_clipped, _ = tf.clip_by_global_norm(gen_grads,
+                                                  FLAGS.grad_clipping)
+    gen_pretrain_op = optimizer.apply_gradients(
+        zip(gen_grads_clipped, gen_vars), global_step=global_step)
+    return gen_pretrain_op
+
+
+def create_gen_train_op(hparams, learning_rate, gen_loss, global_step, mode):
+  """Create Generator train op."""
+  del hparams
+  with tf.name_scope('train_generator'):
+    if FLAGS.generator_optimizer == 'sgd':
+      gen_optimizer = tf.train.GradientDescentOptimizer(learning_rate)
+    elif FLAGS.generator_optimizer == 'adam':
+      gen_optimizer = tf.train.AdamOptimizer(learning_rate)
+    else:
+      raise NotImplementedError
+    gen_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('gen')
+    ]
+    print('Optimizing Generator vars.')
+    for v in gen_vars:
+      print(v)
+    if mode == 'MINIMIZE':
+      gen_grads = tf.gradients(gen_loss, gen_vars)
+    elif mode == 'MAXIMIZE':
+      gen_grads = tf.gradients(-gen_loss, gen_vars)
+    else:
+      raise ValueError("Must be one of 'MINIMIZE' or 'MAXIMIZE'")
+    gen_grads_clipped, _ = tf.clip_by_global_norm(gen_grads,
+                                                  FLAGS.grad_clipping)
+    gen_train_op = gen_optimizer.apply_gradients(
+        zip(gen_grads_clipped, gen_vars), global_step=global_step)
+    return gen_train_op, gen_grads_clipped, gen_vars
+
+
+def create_reinforce_gen_train_op(hparams, learning_rate, final_gen_reward,
+                                  averages_op, global_step):
+  """Create the Generator train_op when using REINFORCE.
+
+  Args:
+    hparams:  MaskGAN hyperparameters.
+    learning_rate:  tf.Variable scalar learning rate.
+    final_gen_objective:  Scalar final REINFORCE objective for the sequence.
+    averages_op:  ExponentialMovingAverage apply average op to
+      maintain the baseline.
+    global_step:  global_step tf.Variable.
+
+  Returns:
+    gen_train_op: Generator training op.
+  """
+  del hparams
+  with tf.name_scope('train_generator'):
+    if FLAGS.generator_optimizer == 'sgd':
+      gen_optimizer = tf.train.GradientDescentOptimizer(learning_rate)
+    elif FLAGS.generator_optimizer == 'adam':
+      gen_optimizer = tf.train.AdamOptimizer(learning_rate)
+    else:
+      raise NotImplementedError
+    gen_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('gen')
+    ]
+    print('\nOptimizing Generator vars:')
+    for v in gen_vars:
+      print(v)
+
+    # Maximize reward.
+    gen_grads = tf.gradients(-final_gen_reward, gen_vars)
+    gen_grads_clipped, _ = tf.clip_by_global_norm(gen_grads,
+                                                  FLAGS.grad_clipping)
+    maximize_op = gen_optimizer.apply_gradients(
+        zip(gen_grads_clipped, gen_vars), global_step=global_step)
+
+    # Group maintain averages op.
+    if averages_op:
+      gen_train_op = tf.group(maximize_op, averages_op)
+    else:
+      gen_train_op = maximize_op
+
+    return [gen_train_op, gen_grads, gen_vars]
+
+
+def create_dis_train_op(hparams, dis_loss, global_step):
+  """Create Discriminator train op."""
+  with tf.name_scope('train_discriminator'):
+    dis_optimizer = tf.train.AdamOptimizer(hparams.dis_learning_rate)
+    dis_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('dis')
+    ]
+    if FLAGS.dis_update_share_embedding and FLAGS.dis_share_embedding:
+      shared_embedding = [
+          v for v in tf.trainable_variables()
+          if v.op.name == 'gen/decoder/rnn/embedding'
+      ][0]
+      dis_vars.append(shared_embedding)
+    print('\nOptimizing Discriminator vars:')
+    for v in dis_vars:
+      print(v)
+    dis_grads = tf.gradients(dis_loss, dis_vars)
+    dis_grads_clipped, _ = tf.clip_by_global_norm(dis_grads,
+                                                  FLAGS.grad_clipping)
+    dis_train_op = dis_optimizer.apply_gradients(
+        zip(dis_grads_clipped, dis_vars), global_step=global_step)
+    return dis_train_op, dis_grads_clipped, dis_vars
+
+
+def create_critic_train_op(hparams, critic_loss, global_step):
+  """Create Discriminator train op."""
+  with tf.name_scope('train_critic'):
+    critic_optimizer = tf.train.AdamOptimizer(hparams.critic_learning_rate)
+    output_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('critic')
+    ]
+
+    if FLAGS.critic_update_dis_vars:
+      if FLAGS.discriminator_model == 'bidirectional_vd':
+        critic_vars = [
+            v for v in tf.trainable_variables()
+            if v.op.name.startswith('dis/rnn')
+        ]
+      elif FLAGS.discriminator_model == 'seq2seq_vd':
+        critic_vars = [
+            v for v in tf.trainable_variables()
+            if v.op.name.startswith('dis/decoder/rnn/multi_rnn_cell')
+        ]
+      critic_vars.extend(output_vars)
+    else:
+      critic_vars = output_vars
+    print('\nOptimizing Critic vars:')
+    for v in critic_vars:
+      print(v)
+    critic_grads = tf.gradients(critic_loss, critic_vars)
+    critic_grads_clipped, _ = tf.clip_by_global_norm(critic_grads,
+                                                     FLAGS.grad_clipping)
+    critic_train_op = critic_optimizer.apply_gradients(
+        zip(critic_grads_clipped, critic_vars), global_step=global_step)
+    return critic_train_op, critic_grads_clipped, critic_vars
diff --git a/research/maskgan/model_utils/model_utils.py b/research/maskgan/model_utils/model_utils.py
new file mode 100644
index 00000000..0e318358
--- /dev/null
+++ b/research/maskgan/model_utils/model_utils.py
@@ -0,0 +1,291 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Model utilities."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+import numpy as np
+
+import tensorflow as tf
+from model_utils import variable_mapping
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def generate_mask():
+  """Generate the mask to be fed into the model."""
+  if FLAGS.mask_strategy == 'random':
+    p = np.random.choice(
+        [True, False],
+        size=[FLAGS.batch_size, FLAGS.sequence_length],
+        p=[FLAGS.is_present_rate, 1. - FLAGS.is_present_rate])
+
+  elif FLAGS.mask_strategy == 'contiguous':
+    masked_length = int((1 - FLAGS.is_present_rate) * FLAGS.sequence_length) - 1
+    # Determine location to start masking.
+    start_mask = np.random.randint(
+        1, FLAGS.sequence_length - masked_length + 1, size=FLAGS.batch_size)
+    p = np.full([FLAGS.batch_size, FLAGS.sequence_length], True, dtype=bool)
+
+    # Create contiguous masked section to be False.
+    for i, index in enumerate(start_mask):
+      p[i, index:index + masked_length] = False
+
+  else:
+    raise NotImplementedError
+
+  return p
+
+
+def assign_percent_real(session, percent_real_update, new_rate, current_rate):
+  """Run assign operation where the we load the current_rate of percent
+  real into a Tensorflow variable.
+
+  Args:
+    session:  Current tf.Session.
+    percent_real_update: tf.assign operation.
+    new_rate: tf.placeholder for the new rate.
+    current_rate: Percent of tokens that are currently real.  Fake tokens
+      are the ones being imputed by the Generator.
+  """
+  session.run(percent_real_update, feed_dict={new_rate: current_rate})
+
+
+def assign_learning_rate(session, lr_update, lr_placeholder, new_lr):
+  """Run assign operation where the we load the current_rate of percent
+  real into a Tensorflow variable.
+
+  Args:
+    session:  Current tf.Session.
+    lr_update: tf.assign operation.
+    lr_placeholder: tf.placeholder for the new learning rate.
+    new_lr: New learning rate to use.
+  """
+  session.run(lr_update, feed_dict={lr_placeholder: new_lr})
+
+
+def clip_weights(variables, c_lower, c_upper):
+  """Clip a list of weights to be within a certain range.
+
+  Args:
+    variables: List of tf.Variable weights.
+    c_lower: Lower bound for weights.
+    c_upper: Upper bound for weights.
+  """
+  clip_ops = []
+
+  for var in variables:
+    clipped_var = tf.clip_by_value(var, c_lower, c_upper)
+
+    clip_ops.append(tf.assign(var, clipped_var))
+  return tf.group(*clip_ops)
+
+
+def retrieve_init_savers(hparams):
+  """Retrieve a dictionary of all the initial savers for the models.
+
+  Args:
+    hparams:  MaskGAN hyperparameters.
+  """
+  ## Dictionary of init savers.
+  init_savers = {}
+
+  ## Load Generator weights from MaskGAN checkpoint.
+  if FLAGS.maskgan_ckpt:
+    gen_vars = [
+        v for v in tf.trainable_variables() if v.op.name.startswith('gen')
+    ]
+    init_saver = tf.train.Saver(var_list=gen_vars)
+    init_savers['init_saver'] = init_saver
+
+    ## Load the Discriminator weights from the MaskGAN checkpoint if
+    # the weights are compatible.
+    if FLAGS.discriminator_model == 'seq2seq_vd':
+      dis_variable_maps = variable_mapping.dis_seq2seq_vd(hparams)
+      dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)
+      init_savers['dis_init_saver'] = dis_init_saver
+
+  ## Load weights from language model checkpoint.
+  if FLAGS.language_model_ckpt_dir:
+    if FLAGS.maskgan_ckpt is None:
+      ## Generator Variables/Savers.
+      if FLAGS.generator_model == 'rnn_nas':
+        gen_variable_maps = variable_mapping.rnn_nas(hparams, model='gen')
+        gen_init_saver = tf.train.Saver(var_list=gen_variable_maps)
+        init_savers['gen_init_saver'] = gen_init_saver
+
+      elif FLAGS.generator_model == 'seq2seq_nas':
+        # Encoder.
+        gen_encoder_variable_maps = variable_mapping.gen_encoder_seq2seq_nas(
+            hparams)
+        gen_encoder_init_saver = tf.train.Saver(
+            var_list=gen_encoder_variable_maps)
+        # Decoder.
+        gen_decoder_variable_maps = variable_mapping.gen_decoder_seq2seq_nas(
+            hparams)
+        gen_decoder_init_saver = tf.train.Saver(
+            var_list=gen_decoder_variable_maps)
+        init_savers['gen_encoder_init_saver'] = gen_encoder_init_saver
+        init_savers['gen_decoder_init_saver'] = gen_decoder_init_saver
+
+      # seq2seq_vd derived from the same code base as seq2seq_zaremba.
+      elif (FLAGS.generator_model == 'seq2seq_zaremba' or
+            FLAGS.generator_model == 'seq2seq_vd'):
+        # Encoder.
+        gen_encoder_variable_maps = variable_mapping.gen_encoder_seq2seq(
+            hparams)
+        gen_encoder_init_saver = tf.train.Saver(
+            var_list=gen_encoder_variable_maps)
+        # Decoder.
+        gen_decoder_variable_maps = variable_mapping.gen_decoder_seq2seq(
+            hparams)
+        gen_decoder_init_saver = tf.train.Saver(
+            var_list=gen_decoder_variable_maps)
+        init_savers['gen_encoder_init_saver'] = gen_encoder_init_saver
+        init_savers['gen_decoder_init_saver'] = gen_decoder_init_saver
+
+      else:
+        raise NotImplementedError
+
+    ## Discriminator Variables/Savers.
+    if FLAGS.discriminator_model == 'rnn_nas':
+      dis_variable_maps = variable_mapping.rnn_nas(hparams, model='dis')
+      dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)
+      init_savers['dis_init_saver'] = dis_init_saver
+
+    # rnn_vd derived from the same code base as rnn_zaremba.
+    elif (FLAGS.discriminator_model == 'rnn_zaremba' or
+          FLAGS.discriminator_model == 'rnn_vd'):
+      dis_variable_maps = variable_mapping.rnn_zaremba(hparams, model='dis')
+      dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)
+      init_savers['dis_init_saver'] = dis_init_saver
+
+    elif (FLAGS.discriminator_model == 'bidirectional_zaremba' or
+          FLAGS.discriminator_model == 'bidirectional_vd'):
+      dis_fwd_variable_maps = variable_mapping.dis_fwd_bidirectional(hparams)
+      dis_bwd_variable_maps = variable_mapping.dis_bwd_bidirectional(hparams)
+      # Savers for the forward/backward Discriminator components.
+      dis_fwd_init_saver = tf.train.Saver(var_list=dis_fwd_variable_maps)
+      dis_bwd_init_saver = tf.train.Saver(var_list=dis_bwd_variable_maps)
+      init_savers['dis_fwd_init_saver'] = dis_fwd_init_saver
+      init_savers['dis_bwd_init_saver'] = dis_bwd_init_saver
+
+    elif FLAGS.discriminator_model == 'cnn':
+      dis_variable_maps = variable_mapping.cnn()
+      dis_init_saver = tf.train.Saver(var_list=dis_variable_maps)
+      init_savers['dis_init_saver'] = dis_init_saver
+
+    elif FLAGS.discriminator_model == 'seq2seq_vd':
+      # Encoder.
+      dis_encoder_variable_maps = variable_mapping.dis_encoder_seq2seq(hparams)
+      dis_encoder_init_saver = tf.train.Saver(
+          var_list=dis_encoder_variable_maps)
+      # Decoder.
+      dis_decoder_variable_maps = variable_mapping.dis_decoder_seq2seq(hparams)
+      dis_decoder_init_saver = tf.train.Saver(
+          var_list=dis_decoder_variable_maps)
+      init_savers['dis_encoder_init_saver'] = dis_encoder_init_saver
+      init_savers['dis_decoder_init_saver'] = dis_decoder_init_saver
+
+  return init_savers
+
+
+def init_fn(init_savers, sess):
+  """The init_fn to be passed to the Supervisor.
+
+  Args:
+    init_savers:  Dictionary of init_savers.  'init_saver_name': init_saver.
+    sess:  tf.Session.
+  """
+  ## Load Generator weights from MaskGAN checkpoint.
+  if FLAGS.maskgan_ckpt:
+    print('Restoring Generator from %s.' % FLAGS.maskgan_ckpt)
+    tf.logging.info('Restoring Generator from %s.' % FLAGS.maskgan_ckpt)
+    print('Asserting Generator is a seq2seq-variant.')
+    tf.logging.info('Asserting Generator is a seq2seq-variant.')
+    assert FLAGS.generator_model.startswith('seq2seq')
+    init_saver = init_savers['init_saver']
+    init_saver.restore(sess, FLAGS.maskgan_ckpt)
+
+    ## Load the Discriminator weights from the MaskGAN checkpoint if
+    # the weights are compatible.
+    if FLAGS.discriminator_model == 'seq2seq_vd':
+      print('Restoring Discriminator from %s.' % FLAGS.maskgan_ckpt)
+      tf.logging.info('Restoring Discriminator from %s.' % FLAGS.maskgan_ckpt)
+      dis_init_saver = init_savers['dis_init_saver']
+      dis_init_saver.restore(sess, FLAGS.maskgan_ckpt)
+
+  ## Load weights from language model checkpoint.
+  if FLAGS.language_model_ckpt_dir:
+    if FLAGS.maskgan_ckpt is None:
+      ## Generator Models.
+      if FLAGS.generator_model == 'rnn_nas':
+        load_ckpt = tf.train.latest_checkpoint(FLAGS.language_model_ckpt_dir)
+        print('Restoring Generator from %s.' % load_ckpt)
+        tf.logging.info('Restoring Generator from %s.' % load_ckpt)
+        gen_init_saver = init_savers['gen_init_saver']
+        gen_init_saver.restore(sess, load_ckpt)
+
+      elif FLAGS.generator_model.startswith('seq2seq'):
+        load_ckpt = tf.train.latest_checkpoint(FLAGS.language_model_ckpt_dir)
+        print('Restoring Generator from %s.' % load_ckpt)
+        tf.logging.info('Restoring Generator from %s.' % load_ckpt)
+        gen_encoder_init_saver = init_savers['gen_encoder_init_saver']
+        gen_decoder_init_saver = init_savers['gen_decoder_init_saver']
+        gen_encoder_init_saver.restore(sess, load_ckpt)
+        gen_decoder_init_saver.restore(sess, load_ckpt)
+
+    ## Discriminator Models.
+    if (FLAGS.discriminator_model == 'rnn_nas' or
+        FLAGS.discriminator_model == 'rnn_zaremba' or
+        FLAGS.discriminator_model == 'rnn_vd' or
+        FLAGS.discriminator_model == 'cnn'):
+      load_ckpt = tf.train.latest_checkpoint(FLAGS.language_model_ckpt_dir)
+      print('Restoring Discriminator from %s.' % load_ckpt)
+      tf.logging.info('Restoring Discriminator from %s.' % load_ckpt)
+      dis_init_saver = init_savers['dis_init_saver']
+      dis_init_saver.restore(sess, load_ckpt)
+
+    elif (FLAGS.discriminator_model == 'bidirectional_zaremba' or
+          FLAGS.discriminator_model == 'bidirectional_vd'):
+      assert FLAGS.language_model_ckpt_dir_reversed is not None, (
+          'Need a reversed directory to fill in the backward components.')
+      load_fwd_ckpt = tf.train.latest_checkpoint(FLAGS.language_model_ckpt_dir)
+      load_bwd_ckpt = tf.train.latest_checkpoint(
+          FLAGS.language_model_ckpt_dir_reversed)
+      print('Restoring Discriminator from %s and %s.' % (load_fwd_ckpt,
+                                                         load_bwd_ckpt))
+      tf.logging.info('Restoring Discriminator from %s and %s.' %
+                      (load_fwd_ckpt, load_bwd_ckpt))
+      dis_fwd_init_saver = init_savers['dis_fwd_init_saver']
+      dis_bwd_init_saver = init_savers['dis_bwd_init_saver']
+      dis_fwd_init_saver.restore(sess, load_fwd_ckpt)
+      dis_bwd_init_saver.restore(sess, load_bwd_ckpt)
+
+    elif FLAGS.discriminator_model == 'seq2seq_vd':
+      load_ckpt = tf.train.latest_checkpoint(FLAGS.language_model_ckpt_dir)
+      print('Restoring Discriminator from %s.' % load_ckpt)
+      tf.logging.info('Restoring Discriminator from %s.' % load_ckpt)
+      dis_encoder_init_saver = init_savers['dis_encoder_init_saver']
+      dis_decoder_init_saver = init_savers['dis_decoder_init_saver']
+      dis_encoder_init_saver.restore(sess, load_ckpt)
+      dis_decoder_init_saver.restore(sess, load_ckpt)
+
+  else:
+    return
diff --git a/research/maskgan/model_utils/n_gram.py b/research/maskgan/model_utils/n_gram.py
new file mode 100644
index 00000000..f83c0c60
--- /dev/null
+++ b/research/maskgan/model_utils/n_gram.py
@@ -0,0 +1,64 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""We calculate n-Grams from the training text. We will use this as an
+evaluation metric."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+
+def hash_function(input_tuple):
+  """Hash function for a tuple."""
+  return hash(input_tuple)
+
+
+def find_all_ngrams(dataset, n):
+  """Generate a list of all ngrams."""
+  return zip(*[dataset[i:] for i in xrange(n)])
+
+
+def construct_ngrams_dict(ngrams_list):
+  """Construct a ngram dictionary which maps an ngram tuple to the number
+  of times it appears in the text."""
+  counts = {}
+
+  for t in ngrams_list:
+    key = hash_function(t)
+    if key in counts:
+      counts[key] += 1
+    else:
+      counts[key] = 1
+  return counts
+
+
+def percent_unique_ngrams_in_train(train_ngrams_dict, gen_ngrams_dict):
+  """Compute the percent of ngrams generated by the model that are
+  present in the training text and are unique."""
+
+  # *Total* number of n-grams produced by the generator.
+  total_ngrams_produced = 0
+
+  for _, value in gen_ngrams_dict.iteritems():
+    total_ngrams_produced += value
+
+  # The unique ngrams in the training set.
+  unique_ngrams_in_train = 0.
+
+  for key, _ in gen_ngrams_dict.iteritems():
+    if key in train_ngrams_dict:
+      unique_ngrams_in_train += 1
+  return float(unique_ngrams_in_train) / float(total_ngrams_produced)
diff --git a/research/maskgan/model_utils/variable_mapping.py b/research/maskgan/model_utils/variable_mapping.py
new file mode 100644
index 00000000..abfb0b9e
--- /dev/null
+++ b/research/maskgan/model_utils/variable_mapping.py
@@ -0,0 +1,773 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def rnn_nas(hparams, model):
+  assert model == 'gen' or model == 'dis'
+
+  # This logic is only valid for rnn_zaremba
+  if model == 'gen':
+    assert FLAGS.generator_model == 'rnn_nas'
+    assert hparams.gen_num_layers == 2
+
+  if model == 'dis':
+    assert FLAGS.discriminator_model == 'rnn_nas'
+    assert hparams.dis_num_layers == 2
+
+  # Output variables only for the Generator.  Discriminator output biases
+  # will begin randomly initialized.
+  if model == 'gen':
+    softmax_b = [
+        v for v in tf.trainable_variables() if v.op.name == 'gen/rnn/softmax_b'
+    ][0]
+
+  # Common elements to Generator and Discriminator.
+  embedding = [
+      v for v in tf.trainable_variables()
+      if v.op.name == str(model) + '/rnn/embedding'
+  ][0]
+  lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat'
+  ][0]
+  lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == str(model) +
+      '/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+  lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat'
+  ][0]
+  lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == str(model) +
+      '/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+
+  # Dictionary mapping.
+  if model == 'gen':
+    variable_mapping = {
+        'Model/embeddings/input_embedding':
+            embedding,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat':
+            lstm_w_0,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat':
+            lstm_b_0,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat':
+            lstm_w_1,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat':
+            lstm_b_1,
+        'Model/softmax_b':
+            softmax_b
+    }
+  else:
+    variable_mapping = {
+        'Model/embeddings/input_embedding':
+            embedding,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat':
+            lstm_w_0,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat':
+            lstm_b_0,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat':
+            lstm_w_1,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat':
+            lstm_b_1
+    }
+
+  return variable_mapping
+
+
+def cnn():
+  """Variable mapping for the CNN embedding.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_var.
+  """
+  # This logic is only valid for cnn
+  assert FLAGS.discriminator_model == 'cnn'
+
+  # Retrieve CNN embedding.
+  embedding = [
+      v for v in tf.trainable_variables() if v.op.name == 'dis/embedding'
+  ][0]
+
+  # Variable mapping.
+  variable_mapping = {'Model/embedding': embedding}
+
+  return variable_mapping
+
+
+def rnn_zaremba(hparams, model):
+  """Returns the PTB Variable name to MaskGAN Variable dictionary mapping.  This
+  is a highly restrictive function just for testing.  This will need to be
+  generalized.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    model:  Model type, one of ['gen', 'dis'].
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_var.
+  """
+  assert model == 'gen' or model == 'dis'
+
+  # This logic is only valid for rnn_zaremba
+  if model == 'gen':
+    assert FLAGS.generator_model == 'rnn_zaremba'
+    assert hparams.gen_num_layers == 2
+
+  if model == 'dis':
+    assert (FLAGS.discriminator_model == 'rnn_zaremba' or
+            FLAGS.discriminator_model == 'rnn_vd')
+    assert hparams.dis_num_layers == 2
+
+  # Output variables only for the Generator.  Discriminator output weights
+  # and biases will begin randomly initialized.
+  if model == 'gen':
+    softmax_w = [
+        v for v in tf.trainable_variables() if v.op.name == 'gen/rnn/softmax_w'
+    ][0]
+    softmax_b = [
+        v for v in tf.trainable_variables() if v.op.name == 'gen/rnn/softmax_b'
+    ][0]
+
+  # Common elements to Generator and Discriminator.
+  if not FLAGS.dis_share_embedding or model != 'dis':
+    embedding = [
+        v for v in tf.trainable_variables()
+        if v.op.name == str(model) + '/rnn/embedding'
+    ][0]
+  lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      str(model) + '/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  # Dictionary mapping.
+  if model == 'gen':
+    variable_mapping = {
+        'Model/embedding': embedding,
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': lstm_w_0,
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': lstm_b_0,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': lstm_w_1,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': lstm_b_1,
+        'Model/softmax_w': softmax_w,
+        'Model/softmax_b': softmax_b
+    }
+  else:
+    if FLAGS.dis_share_embedding:
+      variable_mapping = {
+          'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': lstm_w_0,
+          'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': lstm_b_0,
+          'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': lstm_w_1,
+          'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': lstm_b_1
+      }
+    else:
+      variable_mapping = {
+          'Model/embedding': embedding,
+          'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': lstm_w_0,
+          'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': lstm_b_0,
+          'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': lstm_w_1,
+          'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': lstm_b_1
+      }
+
+  return variable_mapping
+
+
+def gen_encoder_seq2seq_nas(hparams):
+  """Returns the NAS Variable name to MaskGAN Variable
+  dictionary mapping.  This is a highly restrictive function just for testing.
+  This is for the *unidirecitional* seq2seq_nas encoder.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_varself.
+  """
+  assert FLAGS.generator_model == 'seq2seq_nas'
+  assert hparams.gen_num_layers == 2
+  ## Encoder forward variables.
+
+  if not FLAGS.seq2seq_share_embedding:
+    encoder_embedding = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'gen/encoder/rnn/embedding'
+    ][0]
+  encoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat'
+  ][0]
+  encoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+  encoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat'
+  ][0]
+  encoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+
+  if not FLAGS.seq2seq_share_embedding:
+    variable_mapping = {
+        'Model/embeddings/input_embedding':
+            encoder_embedding,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat':
+            encoder_lstm_w_0,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat':
+            encoder_lstm_b_0,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat':
+            encoder_lstm_w_1,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat':
+            encoder_lstm_b_1
+    }
+  else:
+    variable_mapping = {
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat':
+            encoder_lstm_w_0,
+        'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat':
+            encoder_lstm_b_0,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat':
+            encoder_lstm_w_1,
+        'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat':
+            encoder_lstm_b_1
+    }
+  return variable_mapping
+
+
+def gen_decoder_seq2seq_nas(hparams):
+  assert FLAGS.generator_model == 'seq2seq_nas'
+  assert hparams.gen_num_layers == 2
+
+  decoder_embedding = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'gen/decoder/rnn/embedding'
+  ][0]
+  decoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat'
+  ][0]
+  decoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+  decoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat'
+  ][0]
+  decoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat'
+  ][0]
+
+  decoder_softmax_b = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'gen/decoder/rnn/softmax_b'
+  ][0]
+
+  variable_mapping = {
+      'Model/embeddings/input_embedding':
+          decoder_embedding,
+      'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_h_mat':
+          decoder_lstm_w_0,
+      'Model/RNN/GenericMultiRNNCell/Cell0/Alien/rnn_builder/big_inputs_mat':
+          decoder_lstm_b_0,
+      'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_h_mat':
+          decoder_lstm_w_1,
+      'Model/RNN/GenericMultiRNNCell/Cell1/Alien/rnn_builder/big_inputs_mat':
+          decoder_lstm_b_1,
+      'Model/softmax_b':
+          decoder_softmax_b
+  }
+
+  return variable_mapping
+
+
+def gen_encoder_seq2seq(hparams):
+  """Returns the PTB Variable name to MaskGAN Variable
+  dictionary mapping.  This is a highly restrictive function just for testing.
+  This is foe the *unidirecitional* seq2seq_zaremba encoder.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_varself.
+  """
+  assert (FLAGS.generator_model == 'seq2seq_zaremba' or
+          FLAGS.generator_model == 'seq2seq_vd')
+  assert hparams.gen_num_layers == 2
+
+  ## Encoder forward variables.
+  if not FLAGS.seq2seq_share_embedding:
+    encoder_embedding = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'gen/encoder/rnn/embedding'
+    ][0]
+  encoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  encoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  if FLAGS.data_set == 'ptb':
+    model_str = 'Model'
+  else:
+    model_str = 'model'
+
+  if not FLAGS.seq2seq_share_embedding:
+    variable_mapping = {
+        str(model_str) + '/embedding':
+            encoder_embedding,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+            encoder_lstm_w_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+            encoder_lstm_b_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+            encoder_lstm_w_1,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+            encoder_lstm_b_1
+    }
+  else:
+    variable_mapping = {
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+            encoder_lstm_w_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+            encoder_lstm_b_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+            encoder_lstm_w_1,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+            encoder_lstm_b_1
+    }
+  return variable_mapping
+
+
+def gen_decoder_seq2seq(hparams):
+  assert (FLAGS.generator_model == 'seq2seq_zaremba' or
+          FLAGS.generator_model == 'seq2seq_vd')
+  assert hparams.gen_num_layers == 2
+
+  decoder_embedding = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'gen/decoder/rnn/embedding'
+  ][0]
+  decoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  decoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+  decoder_softmax_b = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'gen/decoder/rnn/softmax_b'
+  ][0]
+
+  if FLAGS.data_set == 'ptb':
+    model_str = 'Model'
+  else:
+    model_str = 'model'
+
+  variable_mapping = {
+      str(model_str) + '/embedding':
+          decoder_embedding,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+          decoder_lstm_w_0,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+          decoder_lstm_b_0,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+          decoder_lstm_w_1,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+          decoder_lstm_b_1,
+      str(model_str) + '/softmax_b':
+          decoder_softmax_b
+  }
+  return variable_mapping
+
+
+def dis_fwd_bidirectional(hparams):
+  """Returns the *forward* PTB Variable name to MaskGAN Variable dictionary
+  mapping.  This is a highly restrictive function just for testing. This is for
+  the bidirectional_zaremba discriminator.
+
+  Args:
+    FLAGS:  Flags for the model.
+    hparams:  Hyperparameters for the MaskGAN.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_varself.
+  """
+  assert (FLAGS.discriminator_model == 'bidirectional_zaremba' or
+          FLAGS.discriminator_model == 'bidirectional_vd')
+  assert hparams.dis_num_layers == 2
+
+  # Forward Discriminator Elements.
+  if not FLAGS.dis_share_embedding:
+    embedding = [
+        v for v in tf.trainable_variables() if v.op.name == 'dis/embedding'
+    ][0]
+  fw_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  fw_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/fw/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  fw_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  fw_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/fw/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+  if FLAGS.dis_share_embedding:
+    variable_mapping = {
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': fw_lstm_w_0,
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': fw_lstm_b_0,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': fw_lstm_w_1,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': fw_lstm_b_1
+    }
+  else:
+    variable_mapping = {
+        'Model/embedding': embedding,
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': fw_lstm_w_0,
+        'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': fw_lstm_b_0,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': fw_lstm_w_1,
+        'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': fw_lstm_b_1
+    }
+  return variable_mapping
+
+
+def dis_bwd_bidirectional(hparams):
+  """Returns the *backward* PTB Variable name to MaskGAN Variable dictionary
+  mapping.  This is a highly restrictive function just for testing. This is for
+  the bidirectional_zaremba discriminator.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_varself.
+  """
+  assert (FLAGS.discriminator_model == 'bidirectional_zaremba' or
+          FLAGS.discriminator_model == 'bidirectional_vd')
+  assert hparams.dis_num_layers == 2
+
+  # Backward Discriminator Elements.
+  bw_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  bw_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/bw/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  bw_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  bw_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name == 'dis/rnn/bw/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  variable_mapping = {
+      'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights': bw_lstm_w_0,
+      'Model/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases': bw_lstm_b_0,
+      'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights': bw_lstm_w_1,
+      'Model/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases': bw_lstm_b_1
+  }
+  return variable_mapping
+
+
+def dis_encoder_seq2seq(hparams):
+  """Returns the PTB Variable name to MaskGAN Variable
+  dictionary mapping.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+
+  Returns:
+    variable_mapping:  Dictionary with Key: ckpt_name, Value: model_varself.
+  """
+  assert FLAGS.discriminator_model == 'seq2seq_vd'
+  assert hparams.dis_num_layers == 2
+
+  ## Encoder forward variables.
+  encoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  encoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  if FLAGS.data_set == 'ptb':
+    model_str = 'Model'
+  else:
+    model_str = 'model'
+
+  variable_mapping = {
+      str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+          encoder_lstm_w_0,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+          encoder_lstm_b_0,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+          encoder_lstm_w_1,
+      str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+          encoder_lstm_b_1
+  }
+  return variable_mapping
+
+
+def dis_decoder_seq2seq(hparams):
+  assert FLAGS.discriminator_model == 'seq2seq_vd'
+  assert hparams.dis_num_layers == 2
+
+  if not FLAGS.dis_share_embedding:
+    decoder_embedding = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'dis/decoder/rnn/embedding'
+    ][0]
+  decoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  decoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  if FLAGS.data_set == 'ptb':
+    model_str = 'Model'
+  else:
+    model_str = 'model'
+
+  if not FLAGS.dis_share_embedding:
+    variable_mapping = {
+        str(model_str) + '/embedding':
+            decoder_embedding,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+            decoder_lstm_w_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+            decoder_lstm_b_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+            decoder_lstm_w_1,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+            decoder_lstm_b_1
+    }
+  else:
+    variable_mapping = {
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+            decoder_lstm_w_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+            decoder_lstm_b_0,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+            decoder_lstm_w_1,
+        str(model_str) + '/RNN/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+            decoder_lstm_b_1,
+    }
+  return variable_mapping
+
+
+def dis_seq2seq_vd(hparams):
+  assert FLAGS.discriminator_model == 'seq2seq_vd'
+  assert hparams.dis_num_layers == 2
+
+  if not FLAGS.dis_share_embedding:
+    decoder_embedding = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'dis/decoder/rnn/embedding'
+    ][0]
+
+  ## Encoder variables.
+  encoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  encoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  encoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  ## Attention.
+  if FLAGS.attention_option is not None:
+    decoder_attention_keys = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'dis/decoder/attention_keys/weights'
+    ][0]
+    decoder_attention_construct_weights = [
+        v for v in tf.trainable_variables()
+        if v.op.name == 'dis/decoder/rnn/attention_construct/weights'
+    ][0]
+
+  ## Decoder.
+  decoder_lstm_w_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_0 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases'
+  ][0]
+  decoder_lstm_w_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights'
+  ][0]
+  decoder_lstm_b_1 = [
+      v for v in tf.trainable_variables()
+      if v.op.name ==
+      'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases'
+  ][0]
+
+  # Standard variable mappings.
+  variable_mapping = {
+      'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+          encoder_lstm_w_0,
+      'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+          encoder_lstm_b_0,
+      'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+          encoder_lstm_w_1,
+      'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+          encoder_lstm_b_1,
+      'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights':
+          decoder_lstm_w_0,
+      'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/biases':
+          decoder_lstm_b_0,
+      'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/weights':
+          decoder_lstm_w_1,
+      'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/biases':
+          decoder_lstm_b_1
+  }
+
+  # Optional variable mappings.
+  if not FLAGS.dis_share_embedding:
+    variable_mapping['gen/decoder/rnn/embedding'] = decoder_embedding
+  if FLAGS.attention_option is not None:
+    variable_mapping[
+        'gen/decoder/attention_keys/weights'] = decoder_attention_keys
+    variable_mapping[
+        'gen/decoder/rnn/attention_construct/weights'] = decoder_attention_construct_weights
+
+  return variable_mapping
diff --git a/research/maskgan/models/__init__.py b/research/maskgan/models/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/models/attention_utils.py b/research/maskgan/models/attention_utils.py
new file mode 100644
index 00000000..4bd9e41d
--- /dev/null
+++ b/research/maskgan/models/attention_utils.py
@@ -0,0 +1,477 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Attention-based decoder functions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from tensorflow.python.framework import function
+
+__all__ = [
+    "prepare_attention", "attention_decoder_fn_train",
+    "attention_decoder_fn_inference"
+]
+
+
+def attention_decoder_fn_train(encoder_state,
+                               attention_keys,
+                               attention_values,
+                               attention_score_fn,
+                               attention_construct_fn,
+                               name=None):
+  """Attentional decoder function for `dynamic_rnn_decoder` during training.
+
+  The `attention_decoder_fn_train` is a training function for an
+  attention-based sequence-to-sequence model. It should be used when
+  `dynamic_rnn_decoder` is in the training mode.
+
+  The `attention_decoder_fn_train` is called with a set of the user arguments
+  and returns the `decoder_fn`, which can be passed to the
+  `dynamic_rnn_decoder`, such that
+
+  ```
+  dynamic_fn_train = attention_decoder_fn_train(encoder_state)
+  outputs_train, state_train = dynamic_rnn_decoder(
+      decoder_fn=dynamic_fn_train, ...)
+  ```
+
+  Further usage can be found in the `kernel_tests/seq2seq_test.py`.
+
+  Args:
+    encoder_state: The encoded state to initialize the `dynamic_rnn_decoder`.
+    attention_keys: to be compared with target states.
+    attention_values: to be used to construct context vectors.
+    attention_score_fn: to compute similarity between key and target states.
+    attention_construct_fn: to build attention states.
+    name: (default: `None`) NameScope for the decoder function;
+      defaults to "simple_decoder_fn_train"
+
+  Returns:
+    A decoder function with the required interface of `dynamic_rnn_decoder`
+    intended for training.
+  """
+  with tf.name_scope(name, "attention_decoder_fn_train", [
+      encoder_state, attention_keys, attention_values, attention_score_fn,
+      attention_construct_fn
+  ]):
+    pass
+
+  def decoder_fn(time, cell_state, cell_input, cell_output, context_state):
+    """Decoder function used in the `dynamic_rnn_decoder` for training.
+
+    Args:
+      time: positive integer constant reflecting the current timestep.
+      cell_state: state of RNNCell.
+      cell_input: input provided by `dynamic_rnn_decoder`.
+      cell_output: output of RNNCell.
+      context_state: context state provided by `dynamic_rnn_decoder`.
+
+    Returns:
+      A tuple (done, next state, next input, emit output, next context state)
+      where:
+
+      done: `None`, which is used by the `dynamic_rnn_decoder` to indicate
+      that `sequence_lengths` in `dynamic_rnn_decoder` should be used.
+
+      next state: `cell_state`, this decoder function does not modify the
+      given state.
+
+      next input: `cell_input`, this decoder function does not modify the
+      given input. The input could be modified when applying e.g. attention.
+
+      emit output: `cell_output`, this decoder function does not modify the
+      given output.
+
+      next context state: `context_state`, this decoder function does not
+      modify the given context state. The context state could be modified when
+      applying e.g. beam search.
+    """
+    with tf.name_scope(
+        name, "attention_decoder_fn_train",
+        [time, cell_state, cell_input, cell_output, context_state]):
+      if cell_state is None:  # first call, return encoder_state
+        cell_state = encoder_state
+
+        # init attention
+        attention = _init_attention(encoder_state)
+      else:
+        # construct attention
+        attention = attention_construct_fn(cell_output, attention_keys,
+                                           attention_values)
+        cell_output = attention
+
+      # combine cell_input and attention
+      next_input = tf.concat([cell_input, attention], 1)
+
+      return (None, cell_state, next_input, cell_output, context_state)
+
+  return decoder_fn
+
+
+def attention_decoder_fn_inference(output_fn,
+                                   encoder_state,
+                                   attention_keys,
+                                   attention_values,
+                                   attention_score_fn,
+                                   attention_construct_fn,
+                                   embeddings,
+                                   start_of_sequence_id,
+                                   end_of_sequence_id,
+                                   maximum_length,
+                                   num_decoder_symbols,
+                                   dtype=tf.int32,
+                                   name=None):
+  """Attentional decoder function for `dynamic_rnn_decoder` during inference.
+
+  The `attention_decoder_fn_inference` is a simple inference function for a
+  sequence-to-sequence model. It should be used when `dynamic_rnn_decoder` is
+  in the inference mode.
+
+  The `attention_decoder_fn_inference` is called with user arguments
+  and returns the `decoder_fn`, which can be passed to the
+  `dynamic_rnn_decoder`, such that
+
+  ```
+  dynamic_fn_inference = attention_decoder_fn_inference(...)
+  outputs_inference, state_inference = dynamic_rnn_decoder(
+      decoder_fn=dynamic_fn_inference, ...)
+  ```
+
+  Further usage can be found in the `kernel_tests/seq2seq_test.py`.
+
+  Args:
+    output_fn: An output function to project your `cell_output` onto class
+    logits.
+
+    An example of an output function;
+
+    ```
+      tf.variable_scope("decoder") as varscope
+        output_fn = lambda x: tf.contrib.layers.linear(x, num_decoder_symbols,
+                                            scope=varscope)
+
+        outputs_train, state_train = seq2seq.dynamic_rnn_decoder(...)
+        logits_train = output_fn(outputs_train)
+
+        varscope.reuse_variables()
+        logits_inference, state_inference = seq2seq.dynamic_rnn_decoder(
+            output_fn=output_fn, ...)
+    ```
+
+    If `None` is supplied it will act as an identity function, which
+    might be wanted when using the RNNCell `OutputProjectionWrapper`.
+
+    encoder_state: The encoded state to initialize the `dynamic_rnn_decoder`.
+    attention_keys: to be compared with target states.
+    attention_values: to be used to construct context vectors.
+    attention_score_fn: to compute similarity between key and target states.
+    attention_construct_fn: to build attention states.
+    embeddings: The embeddings matrix used for the decoder sized
+    `[num_decoder_symbols, embedding_size]`.
+    start_of_sequence_id: The start of sequence ID in the decoder embeddings.
+    end_of_sequence_id: The end of sequence ID in the decoder embeddings.
+    maximum_length: The maximum allowed of time steps to decode.
+    num_decoder_symbols: The number of classes to decode at each time step.
+    dtype: (default: `tf.int32`) The default data type to use when
+    handling integer objects.
+    name: (default: `None`) NameScope for the decoder function;
+      defaults to "attention_decoder_fn_inference"
+
+  Returns:
+    A decoder function with the required interface of `dynamic_rnn_decoder`
+    intended for inference.
+  """
+  with tf.name_scope(name, "attention_decoder_fn_inference", [
+      output_fn, encoder_state, attention_keys, attention_values,
+      attention_score_fn, attention_construct_fn, embeddings,
+      start_of_sequence_id, end_of_sequence_id, maximum_length,
+      num_decoder_symbols, dtype
+  ]):
+    start_of_sequence_id = tf.convert_to_tensor(start_of_sequence_id, dtype)
+    end_of_sequence_id = tf.convert_to_tensor(end_of_sequence_id, dtype)
+    maximum_length = tf.convert_to_tensor(maximum_length, dtype)
+    num_decoder_symbols = tf.convert_to_tensor(num_decoder_symbols, dtype)
+    encoder_info = tf.contrib.framework.nest.flatten(encoder_state)[0]
+    batch_size = encoder_info.get_shape()[0].value
+    if output_fn is None:
+      output_fn = lambda x: x
+    if batch_size is None:
+      batch_size = tf.shape(encoder_info)[0]
+
+  def decoder_fn(time, cell_state, cell_input, cell_output, context_state):
+    """Decoder function used in the `dynamic_rnn_decoder` for inference.
+
+    The main difference between this decoder function and the `decoder_fn` in
+    `attention_decoder_fn_train` is how `next_cell_input` is calculated. In
+    decoder function we calculate the next input by applying an argmax across
+    the feature dimension of the output from the decoder. This is a
+    greedy-search approach. (Bahdanau et al., 2014) & (Sutskever et al., 2014)
+    use beam-search instead.
+
+    Args:
+      time: positive integer constant reflecting the current timestep.
+      cell_state: state of RNNCell.
+      cell_input: input provided by `dynamic_rnn_decoder`.
+      cell_output: output of RNNCell.
+      context_state: context state provided by `dynamic_rnn_decoder`.
+
+    Returns:
+      A tuple (done, next state, next input, emit output, next context state)
+      where:
+
+      done: A boolean vector to indicate which sentences has reached a
+      `end_of_sequence_id`. This is used for early stopping by the
+      `dynamic_rnn_decoder`. When `time>=maximum_length` a boolean vector with
+      all elements as `true` is returned.
+
+      next state: `cell_state`, this decoder function does not modify the
+      given state.
+
+      next input: The embedding from argmax of the `cell_output` is used as
+      `next_input`.
+
+      emit output: If `output_fn is None` the supplied `cell_output` is
+      returned, else the `output_fn` is used to update the `cell_output`
+      before calculating `next_input` and returning `cell_output`.
+
+      next context state: `context_state`, this decoder function does not
+      modify the given context state. The context state could be modified when
+      applying e.g. beam search.
+
+    Raises:
+      ValueError: if cell_input is not None.
+
+    """
+    with tf.name_scope(
+        name, "attention_decoder_fn_inference",
+        [time, cell_state, cell_input, cell_output, context_state]):
+      if cell_input is not None:
+        raise ValueError(
+            "Expected cell_input to be None, but saw: %s" % cell_input)
+      if cell_output is None:
+        # invariant that this is time == 0
+        next_input_id = tf.ones(
+            [
+                batch_size,
+            ], dtype=dtype) * (
+                start_of_sequence_id)
+        done = tf.zeros(
+            [
+                batch_size,
+            ], dtype=tf.bool)
+        cell_state = encoder_state
+        cell_output = tf.zeros([num_decoder_symbols], dtype=tf.float32)
+        cell_input = tf.gather(embeddings, next_input_id)
+
+        # init attention
+        attention = _init_attention(encoder_state)
+      else:
+        # construct attention
+        attention = attention_construct_fn(cell_output, attention_keys,
+                                           attention_values)
+        cell_output = attention
+
+        # argmax decoder
+        cell_output = output_fn(cell_output)  # logits
+        next_input_id = tf.cast(tf.argmax(cell_output, 1), dtype=dtype)
+        done = tf.equal(next_input_id, end_of_sequence_id)
+        cell_input = tf.gather(embeddings, next_input_id)
+
+      # combine cell_input and attention
+      next_input = tf.concat([cell_input, attention], 1)
+
+      # if time > maxlen, return all true vector
+      done = tf.cond(
+          tf.greater(time, maximum_length),
+          lambda: tf.ones([
+              batch_size,], dtype=tf.bool), lambda: done)
+      return (done, cell_state, next_input, cell_output, context_state)
+
+  return decoder_fn
+
+
+## Helper functions ##
+def prepare_attention(attention_states, attention_option, num_units,
+                      reuse=None):
+  """Prepare keys/values/functions for attention.
+
+  Args:
+    attention_states: hidden states to attend over.
+    attention_option: how to compute attention, either "luong" or "bahdanau".
+    num_units: hidden state dimension.
+    reuse: whether to reuse variable scope.
+
+  Returns:
+    attention_keys: to be compared with target states.
+    attention_values: to be used to construct context vectors.
+    attention_score_fn: to compute similarity between key and target states.
+    attention_construct_fn: to build attention states.
+  """
+  # Prepare attention keys / values from attention_states
+  with tf.variable_scope("attention_keys", reuse=reuse) as scope:
+    attention_keys = tf.contrib.layers.linear(
+        attention_states, num_units, biases_initializer=None, scope=scope)
+  attention_values = attention_states
+
+  # Attention score function
+  attention_score_fn = _create_attention_score_fn("attention_score", num_units,
+                                                  attention_option, reuse)
+  # Attention construction function
+  attention_construct_fn = _create_attention_construct_fn(
+      "attention_construct", num_units, attention_score_fn, reuse)
+
+  return (attention_keys, attention_values, attention_score_fn,
+          attention_construct_fn)
+
+
+def _init_attention(encoder_state):
+  """Initialize attention. Handling both LSTM and GRU.
+
+  Args:
+    encoder_state: The encoded state to initialize the `dynamic_rnn_decoder`.
+
+  Returns:
+    attn: initial zero attention vector.
+  """
+
+  # Multi- vs single-layer
+  # TODO(thangluong): is this the best way to check?
+  if isinstance(encoder_state, tuple):
+    top_state = encoder_state[-1]
+  else:
+    top_state = encoder_state
+
+  # LSTM vs GRU
+  if isinstance(top_state, tf.contrib.rnn.LSTMStateTuple):
+    attn = tf.zeros_like(top_state.h)
+  else:
+    attn = tf.zeros_like(top_state)
+
+  return attn
+
+
+def _create_attention_construct_fn(name, num_units, attention_score_fn, reuse):
+  """Function to compute attention vectors.
+
+  Args:
+    name: to label variables.
+    num_units: hidden state dimension.
+    attention_score_fn: to compute similarity between key and target states.
+    reuse: whether to reuse variable scope.
+
+  Returns:
+    attention_construct_fn: to build attention states.
+  """
+
+  def construct_fn(attention_query, attention_keys, attention_values):
+    with tf.variable_scope(name, reuse=reuse) as scope:
+      context = attention_score_fn(attention_query, attention_keys,
+                                   attention_values)
+      concat_input = tf.concat([attention_query, context], 1)
+      attention = tf.contrib.layers.linear(
+          concat_input, num_units, biases_initializer=None, scope=scope)
+      return attention
+
+  return construct_fn
+
+
+# keys: [batch_size, attention_length, attn_size]
+# query: [batch_size, 1, attn_size]
+# return weights [batch_size, attention_length]
+@function.Defun(func_name="attn_add_fun", noinline=True)
+def _attn_add_fun(v, keys, query):
+  return tf.reduce_sum(v * tf.tanh(keys + query), [2])
+
+
+@function.Defun(func_name="attn_mul_fun", noinline=True)
+def _attn_mul_fun(keys, query):
+  return tf.reduce_sum(keys * query, [2])
+
+
+def _create_attention_score_fn(name,
+                               num_units,
+                               attention_option,
+                               reuse,
+                               dtype=tf.float32):
+  """Different ways to compute attention scores.
+
+  Args:
+    name: to label variables.
+    num_units: hidden state dimension.
+    attention_option: how to compute attention, either "luong" or "bahdanau".
+      "bahdanau": additive (Bahdanau et al., ICLR'2015)
+      "luong": multiplicative (Luong et al., EMNLP'2015)
+    reuse: whether to reuse variable scope.
+    dtype: (default: `tf.float32`) data type to use.
+
+  Returns:
+    attention_score_fn: to compute similarity between key and target states.
+  """
+  with tf.variable_scope(name, reuse=reuse):
+    if attention_option == "bahdanau":
+      query_w = tf.get_variable("attnW", [num_units, num_units], dtype=dtype)
+      score_v = tf.get_variable("attnV", [num_units], dtype=dtype)
+
+    def attention_score_fn(query, keys, values):
+      """Put attention masks on attention_values using attention_keys and query.
+
+      Args:
+        query: A Tensor of shape [batch_size, num_units].
+        keys: A Tensor of shape [batch_size, attention_length, num_units].
+        values: A Tensor of shape [batch_size, attention_length, num_units].
+
+      Returns:
+        context_vector: A Tensor of shape [batch_size, num_units].
+
+      Raises:
+        ValueError: if attention_option is neither "luong" or "bahdanau".
+
+
+      """
+      if attention_option == "bahdanau":
+        # transform query
+        query = tf.matmul(query, query_w)
+
+        # reshape query: [batch_size, 1, num_units]
+        query = tf.reshape(query, [-1, 1, num_units])
+
+        # attn_fun
+        scores = _attn_add_fun(score_v, keys, query)
+      elif attention_option == "luong":
+        # reshape query: [batch_size, 1, num_units]
+        query = tf.reshape(query, [-1, 1, num_units])
+
+        # attn_fun
+        scores = _attn_mul_fun(keys, query)
+      else:
+        raise ValueError("Unknown attention option %s!" % attention_option)
+
+      # Compute alignment weights
+      #   scores: [batch_size, length]
+      #   alignments: [batch_size, length]
+      # TODO(thangluong): not normalize over padding positions.
+      alignments = tf.nn.softmax(scores)
+
+      # Now calculate the attention-weighted vector.
+      alignments = tf.expand_dims(alignments, 2)
+      context_vector = tf.reduce_sum(alignments * values, [1])
+      context_vector.set_shape([None, num_units])
+
+      return context_vector
+
+    return attention_score_fn
diff --git a/research/maskgan/models/bidirectional.py b/research/maskgan/models/bidirectional.py
new file mode 100644
index 00000000..1e6b3fe4
--- /dev/null
+++ b/research/maskgan/models/bidirectional.py
@@ -0,0 +1,75 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple bidirectional model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+# ZoneoutWrapper.
+from regularization import zoneout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the bidirectional Discriminator graph."""
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('dis', reuse=reuse):
+    cell_fwd = tf.contrib.rnn.LayerNormBasicLSTMCell(
+        hparams.dis_rnn_size, forget_bias=1.0, reuse=reuse)
+    cell_bwd = tf.contrib.rnn.LayerNormBasicLSTMCell(
+        hparams.dis_rnn_size, forget_bias=1.0, reuse=reuse)
+    if FLAGS.zoneout_drop_prob > 0.0:
+      cell_fwd = zoneout.ZoneoutWrapper(
+          cell_fwd,
+          zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+          is_training=is_training)
+      cell_bwd = zoneout.ZoneoutWrapper(
+          cell_bwd,
+          zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+          is_training=is_training)
+
+    state_fwd = cell_fwd.zero_state(FLAGS.batch_size, tf.float32)
+    state_bwd = cell_bwd.zero_state(FLAGS.batch_size, tf.float32)
+
+    if not FLAGS.dis_share_embedding:
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+    rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+    rnn_inputs = tf.unstack(rnn_inputs, axis=1)
+
+    with tf.variable_scope('rnn') as vs:
+      outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(
+          cell_fwd, cell_bwd, rnn_inputs, state_fwd, state_bwd, scope=vs)
+
+      # Prediction is linear output for Discriminator.
+      predictions = tf.contrib.layers.linear(outputs, 1, scope=vs)
+
+      predictions = tf.transpose(predictions, [1, 0, 2])
+      return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/bidirectional_vd.py b/research/maskgan/models/bidirectional_vd.py
new file mode 100644
index 00000000..469af9da
--- /dev/null
+++ b/research/maskgan/models/bidirectional_vd.py
@@ -0,0 +1,116 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple bidirectional model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from regularization import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams,
+                  sequence,
+                  is_training,
+                  reuse=None,
+                  initial_state=None):
+  """Define the Discriminator graph."""
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('dis', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.dis_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,
+            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)
+
+    cell_fwd = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    cell_bwd = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    # print initial_state
+    # print cell_fwd.zero_state(FLAGS.batch_size, tf.float32)
+    if initial_state:
+      state_fwd = [[tf.identity(x) for x in inner_initial_state]
+                   for inner_initial_state in initial_state]
+      state_bwd = cell_bwd.zero_state(FLAGS.batch_size, tf.float32)
+    else:
+      state_fwd = cell_fwd.zero_state(FLAGS.batch_size, tf.float32)
+      state_bwd = cell_bwd.zero_state(FLAGS.batch_size, tf.float32)
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    if is_training:
+      output_mask = make_mask(hparams.dis_vd_keep_prob,
+                              2 * hparams.dis_rnn_size)
+
+    if not FLAGS.dis_share_embedding:
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+    rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+    rnn_inputs = tf.unstack(rnn_inputs, axis=1)
+
+    with tf.variable_scope('rnn') as vs:
+      outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(
+          cell_fwd, cell_bwd, rnn_inputs, state_fwd, state_bwd, scope=vs)
+
+      if is_training:
+        outputs *= output_mask
+
+      # Prediction is linear output for Discriminator.
+      predictions = tf.contrib.layers.linear(outputs, 1, scope=vs)
+      predictions = tf.transpose(predictions, [1, 0, 2])
+
+  if FLAGS.baseline_method == 'critic':
+    with tf.variable_scope('critic', reuse=reuse) as critic_scope:
+      values = tf.contrib.layers.linear(outputs, 1, scope=critic_scope)
+      values = tf.transpose(values, [1, 0, 2])
+
+    return tf.squeeze(predictions, axis=2), tf.squeeze(values, axis=2)
+
+  else:
+    return tf.squeeze(predictions, axis=2), None
diff --git a/research/maskgan/models/bidirectional_zaremba.py b/research/maskgan/models/bidirectional_zaremba.py
new file mode 100644
index 00000000..b0683d7c
--- /dev/null
+++ b/research/maskgan/models/bidirectional_zaremba.py
@@ -0,0 +1,83 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple bidirectional model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the bidirectional Discriminator graph."""
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('dis', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and FLAGS.keep_prob < 1:
+
+      def attn_cell():
+        return tf.contrib.rnn.DropoutWrapper(
+            lstm_cell(), output_keep_prob=FLAGS.keep_prob)
+
+    cell_fwd = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    cell_bwd = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    state_fwd = cell_fwd.zero_state(FLAGS.batch_size, tf.float32)
+    state_bwd = cell_bwd.zero_state(FLAGS.batch_size, tf.float32)
+
+    if not FLAGS.dis_share_embedding:
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+    rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+    if is_training and FLAGS.keep_prob < 1:
+      rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+    rnn_inputs = tf.unstack(rnn_inputs, axis=1)
+
+    with tf.variable_scope('rnn') as vs:
+      outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(
+          cell_fwd, cell_bwd, rnn_inputs, state_fwd, state_bwd, scope=vs)
+
+      # Prediction is linear output for Discriminator.
+      predictions = tf.contrib.layers.linear(outputs, 1, scope=vs)
+
+      predictions = tf.transpose(predictions, [1, 0, 2])
+      return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/cnn.py b/research/maskgan/models/cnn.py
new file mode 100644
index 00000000..ca682deb
--- /dev/null
+++ b/research/maskgan/models/cnn.py
@@ -0,0 +1,93 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple CNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the Discriminator graph."""
+  del is_training
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        "If you wish to share Discriminator/Generator embeddings, they must be"
+        " same dimension.")
+    with tf.variable_scope("gen/rnn", reuse=True):
+      embedding = tf.get_variable("embedding",
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  dis_filter_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]
+
+  with tf.variable_scope("dis", reuse=reuse):
+    if not FLAGS.dis_share_embedding:
+      embedding = tf.get_variable("embedding",
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+    cnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+    # Create a convolution layer for each filter size
+    conv_outputs = []
+    for filter_size in dis_filter_sizes:
+      with tf.variable_scope("conv-%s" % filter_size):
+        # Convolution Layer
+        filter_shape = [
+            filter_size, hparams.dis_rnn_size, hparams.dis_num_filters
+        ]
+        W = tf.get_variable(
+            name="W", initializer=tf.truncated_normal(filter_shape, stddev=0.1))
+        b = tf.get_variable(
+            name="b",
+            initializer=tf.constant(0.1, shape=[hparams.dis_num_filters]))
+        conv = tf.nn.conv1d(
+            cnn_inputs, W, stride=1, padding="SAME", name="conv")
+
+        # Apply nonlinearity
+        h = tf.nn.relu(tf.nn.bias_add(conv, b), name="relu")
+
+        conv_outputs.append(h)
+
+    # Combine all the pooled features
+    dis_num_filters_total = hparams.dis_num_filters * len(dis_filter_sizes)
+
+    h_conv = tf.concat(conv_outputs, axis=2)
+    h_conv_flat = tf.reshape(h_conv, [-1, dis_num_filters_total])
+
+    # Add dropout
+    with tf.variable_scope("dropout"):
+      h_drop = tf.nn.dropout(h_conv_flat, FLAGS.keep_prob)
+
+    with tf.variable_scope("fully_connected"):
+      fc = tf.contrib.layers.fully_connected(
+          h_drop, num_outputs=dis_num_filters_total / 2)
+
+    # Final (unnormalized) scores and predictions
+    with tf.variable_scope("output"):
+      W = tf.get_variable(
+          "W",
+          shape=[dis_num_filters_total / 2, 1],
+          initializer=tf.contrib.layers.xavier_initializer())
+      b = tf.get_variable(name="b", initializer=tf.constant(0.1, shape=[1]))
+      predictions = tf.nn.xw_plus_b(fc, W, b, name="predictions")
+      predictions = tf.reshape(
+          predictions, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  return predictions
diff --git a/research/maskgan/models/critic_vd.py b/research/maskgan/models/critic_vd.py
new file mode 100644
index 00000000..3d6739fc
--- /dev/null
+++ b/research/maskgan/models/critic_vd.py
@@ -0,0 +1,107 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Critic model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from regularization import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def critic_seq2seq_vd_derivative(hparams, sequence, is_training, reuse=None):
+  """Define the Critic graph which is derived from the seq2seq_vd
+  Discriminator.  This will be initialized with the same parameters as the
+  language model and will share the forward RNN components with the
+  Discriminator.   This estimates the V(s_t), where the state
+  s_t = x_0,...,x_t-1.
+  """
+  assert FLAGS.discriminator_model == 'seq2seq_vd'
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+  else:
+    with tf.variable_scope('dis/decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+  with tf.variable_scope(
+      'dis/decoder/rnn/multi_rnn_cell', reuse=True) as dis_scope:
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=True)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.dis_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,
+            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)
+
+    cell_critic = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+  with tf.variable_scope('critic', reuse=reuse):
+    state_dis = cell_critic.zero_state(FLAGS.batch_size, tf.float32)
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    if is_training:
+      output_mask = make_mask(hparams.dis_vd_keep_prob, hparams.dis_rnn_size)
+
+    with tf.variable_scope('rnn') as vs:
+      values = []
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        if t == 0:
+          rnn_in = tf.zeros_like(rnn_inputs[:, 0])
+        else:
+          rnn_in = rnn_inputs[:, t - 1]
+        rnn_out, state_dis = cell_critic(rnn_in, state_dis, scope=dis_scope)
+
+        if is_training:
+          rnn_out *= output_mask
+
+        # Prediction is linear output for Discriminator.
+        value = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+
+        values.append(value)
+  values = tf.stack(values, axis=1)
+  return tf.squeeze(values, axis=2)
diff --git a/research/maskgan/models/evaluation_utils.py b/research/maskgan/models/evaluation_utils.py
new file mode 100644
index 00000000..fc2a3a16
--- /dev/null
+++ b/research/maskgan/models/evaluation_utils.py
@@ -0,0 +1,280 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Evaluation utilities."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import Counter
+# Dependency imports
+import numpy as np
+from scipy.special import expit
+
+import tensorflow as tf
+
+from model_utils import helper
+from model_utils import n_gram
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def print_and_log_losses(log, step, is_present_rate, avg_dis_loss,
+                         avg_gen_loss):
+  """Prints and logs losses to the log file.
+
+  Args:
+    log: GFile for logs.
+    step: Global step.
+    is_present_rate: Current masking rate.
+    avg_dis_loss: List of Discriminator losses.
+    avg_gen_loss: List of Generator losses.
+  """
+  print('global_step: %d' % step)
+  print(' is_present_rate: %.3f' % is_present_rate)
+  print(' D train loss: %.5f' % np.mean(avg_dis_loss))
+  print(' G train loss: %.5f' % np.mean(avg_gen_loss))
+  log.write('\nglobal_step: %d\n' % step)
+  log.write((' is_present_rate: %.3f\n' % is_present_rate))
+  log.write(' D train loss: %.5f\n' % np.mean(avg_dis_loss))
+  log.write(' G train loss: %.5f\n' % np.mean(avg_gen_loss))
+
+
+def print_and_log(log, id_to_word, sequence_eval, max_num_to_print=5):
+  """Helper function for printing and logging evaluated sequences."""
+  indices_arr = np.asarray(sequence_eval)
+  samples = helper.convert_to_human_readable(id_to_word, indices_arr,
+                                             max_num_to_print)
+
+  for i, sample in enumerate(samples):
+    print('Sample', i, '. ', sample)
+    log.write('\nSample ' + str(i) + '. ' + sample)
+  log.write('\n')
+  print('\n')
+  log.flush()
+  return samples
+
+
+def zip_seq_pred_crossent(id_to_word, sequences, predictions, cross_entropy):
+  """Zip together the sequences, predictions, cross entropy."""
+  indices = np.asarray(sequences)
+
+  batch_of_metrics = []
+
+  for ind_batch, pred_batch, crossent_batch in zip(indices, predictions,
+                                                   cross_entropy):
+    metrics = []
+
+    for index, pred, crossent in zip(ind_batch, pred_batch, crossent_batch):
+      metrics.append([str(id_to_word[index]), pred, crossent])
+
+    batch_of_metrics.append(metrics)
+  return batch_of_metrics
+
+
+def zip_metrics(indices, *args):
+  """Zip together the indices matrices with the provided metrics matrices."""
+  batch_of_metrics = []
+  for metrics_batch in zip(indices, *args):
+
+    metrics = []
+    for m in zip(*metrics_batch):
+      metrics.append(m)
+    batch_of_metrics.append(metrics)
+  return batch_of_metrics
+
+
+def print_formatted(present, id_to_word, log, batch_of_tuples):
+  """Print and log metrics."""
+  num_cols = len(batch_of_tuples[0][0])
+  repeat_float_format = '{:<12.3f} '
+  repeat_str_format = '{:<13}'
+
+  format_str = ''.join(
+      ['[{:<1}]  {:<20}',
+       str(repeat_float_format * (num_cols - 1))])
+
+  # TODO(liamfedus): Generalize the logging. This is sloppy.
+  header_format_str = ''.join(
+      ['[{:<1}]  {:<20}',
+       str(repeat_str_format * (num_cols - 1))])
+  header_str = header_format_str.format('p', 'Word', 'p(real)', 'log-perp',
+                                        'log(p(a))', 'r', 'R=V*(s)', 'b=V(s)',
+                                        'A(a,s)')
+
+  for i, batch in enumerate(batch_of_tuples):
+    print(' Sample: %d' % i)
+    log.write(' Sample %d.\n' % i)
+    print('  ', header_str)
+    log.write('  ' + str(header_str) + '\n')
+
+    for j, t in enumerate(batch):
+      t = list(t)
+      t[0] = id_to_word[t[0]]
+      buffer_str = format_str.format(int(present[i][j]), *t)
+      print('  ', buffer_str)
+      log.write('  ' + str(buffer_str) + '\n')
+  log.flush()
+
+
+def generate_RL_logs(sess, model, log, id_to_word, feed):
+  """Generate complete logs while running with REINFORCE."""
+  # Impute Sequences.
+  [
+      p,
+      fake_sequence_eval,
+      fake_predictions_eval,
+      _,
+      fake_cross_entropy_losses_eval,
+      _,
+      fake_log_probs_eval,
+      fake_rewards_eval,
+      fake_baselines_eval,
+      cumulative_rewards_eval,
+      fake_advantages_eval,
+  ] = sess.run(
+      [
+          model.present,
+          model.fake_sequence,
+          model.fake_predictions,
+          model.real_predictions,
+          model.fake_cross_entropy_losses,
+          model.fake_logits,
+          model.fake_log_probs,
+          model.fake_rewards,
+          model.fake_baselines,
+          model.cumulative_rewards,
+          model.fake_advantages,
+      ],
+      feed_dict=feed)
+
+  indices = np.asarray(fake_sequence_eval)
+
+  # Convert Discriminator linear layer to probability.
+  fake_prob_eval = expit(fake_predictions_eval)
+
+  # Add metrics.
+  fake_tuples = zip_metrics(indices, fake_prob_eval,
+                            fake_cross_entropy_losses_eval, fake_log_probs_eval,
+                            fake_rewards_eval, cumulative_rewards_eval,
+                            fake_baselines_eval, fake_advantages_eval)
+
+  # real_tuples = zip_metrics(indices, )
+
+  # Print forward sequences.
+  tuples_to_print = fake_tuples[:FLAGS.max_num_to_print]
+  print_formatted(p, id_to_word, log, tuples_to_print)
+
+  print('Samples')
+  log.write('Samples\n')
+  samples = print_and_log(log, id_to_word, fake_sequence_eval,
+                          FLAGS.max_num_to_print)
+  return samples
+
+
+def generate_logs(sess, model, log, id_to_word, feed):
+  """Impute Sequences using the model for a particular feed and send it to
+  logs."""
+  # Impute Sequences.
+  [
+      p, sequence_eval, fake_predictions_eval, fake_cross_entropy_losses_eval,
+      fake_logits_eval
+  ] = sess.run(
+      [
+          model.present, model.fake_sequence, model.fake_predictions,
+          model.fake_cross_entropy_losses, model.fake_logits
+      ],
+      feed_dict=feed)
+
+  # Convert Discriminator linear layer to probability.
+  fake_prob_eval = expit(fake_predictions_eval)
+
+  # Forward Masked Tuples.
+  fake_tuples = zip_seq_pred_crossent(id_to_word, sequence_eval, fake_prob_eval,
+                                      fake_cross_entropy_losses_eval)
+
+  tuples_to_print = fake_tuples[:FLAGS.max_num_to_print]
+
+  if FLAGS.print_verbose:
+    print('fake_logits_eval')
+    print(fake_logits_eval)
+
+  for i, batch in enumerate(tuples_to_print):
+    print(' Sample %d.' % i)
+    log.write(' Sample %d.\n' % i)
+    for j, pred in enumerate(batch):
+      buffer_str = ('[{:<1}]  {:<20}  {:<7.3f} {:<7.3f}').format(
+          int(p[i][j]), pred[0], pred[1], pred[2])
+      print('  ', buffer_str)
+      log.write('  ' + str(buffer_str) + '\n')
+  log.flush()
+
+  print('Samples')
+  log.write('Samples\n')
+  samples = print_and_log(log, id_to_word, sequence_eval,
+                          FLAGS.max_num_to_print)
+  return samples
+
+
+def create_merged_ngram_dictionaries(indices, n):
+  """Generate a single dictionary for the full batch.
+
+  Args:
+    indices:  List of lists of indices.
+    n:  Degree of n-grams.
+
+  Returns:
+    Dictionary of hashed(n-gram tuples) to counts in the batch of indices.
+  """
+  ngram_dicts = []
+
+  for ind in indices:
+    ngrams = n_gram.find_all_ngrams(ind, n=n)
+    ngram_counts = n_gram.construct_ngrams_dict(ngrams)
+    ngram_dicts.append(ngram_counts)
+
+  merged_gen_dict = Counter()
+  for ngram_dict in ngram_dicts:
+    merged_gen_dict += Counter(ngram_dict)
+  return merged_gen_dict
+
+
+def sequence_ngram_evaluation(sess, sequence, log, feed, data_ngram_count, n):
+  """Calculates the percent of ngrams produced in the sequence is present in
+  data_ngram_count.
+
+  Args:
+    sess: tf.Session.
+    sequence: Sequence Tensor from the MaskGAN model.
+    log:  gFile log.
+    feed: Feed to evaluate.
+    data_ngram_count:  Dictionary of hashed(n-gram tuples) to counts in the
+      data_set.
+
+  Returns:
+    avg_percent_captured: Percent of produced ngrams that appear in the
+      data_ngram_count.
+  """
+  del log
+  # Impute sequence.
+  [sequence_eval] = sess.run([sequence], feed_dict=feed)
+  indices = sequence_eval
+
+  # Retrieve the counts across the batch of indices.
+  gen_ngram_counts = create_merged_ngram_dictionaries(
+      indices, n=n)
+  return n_gram.percent_unique_ngrams_in_train(data_ngram_count,
+                                               gen_ngram_counts)
diff --git a/research/maskgan/models/feedforward.py b/research/maskgan/models/feedforward.py
new file mode 100644
index 00000000..d736fdd2
--- /dev/null
+++ b/research/maskgan/models/feedforward.py
@@ -0,0 +1,97 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple FNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the Discriminator graph."""
+  del is_training
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        "If you wish to share Discriminator/Generator embeddings, they must be"
+        " same dimension.")
+    with tf.variable_scope("gen/rnn", reuse=True):
+      embedding = tf.get_variable("embedding",
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope("dis", reuse=reuse):
+    if not FLAGS.dis_share_embedding:
+      embedding = tf.get_variable("embedding",
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+    embeddings = tf.nn.embedding_lookup(embedding, sequence)
+
+    # Input matrices.
+    W = tf.get_variable(
+        "W",
+        initializer=tf.truncated_normal(
+            shape=[3 * hparams.dis_embedding_dim, hparams.dis_hidden_dim],
+            stddev=0.1))
+    b = tf.get_variable(
+        "b", initializer=tf.constant(0.1, shape=[hparams.dis_hidden_dim]))
+
+    # Output matrices.
+    W_out = tf.get_variable(
+        "W_out",
+        initializer=tf.truncated_normal(
+            shape=[hparams.dis_hidden_dim, 1], stddev=0.1))
+    b_out = tf.get_variable("b_out", initializer=tf.constant(0.1, shape=[1]))
+
+    predictions = []
+    for t in xrange(FLAGS.sequence_length):
+      if t > 0:
+        tf.get_variable_scope().reuse_variables()
+
+      inp = embeddings[:, t]
+
+      if t > 0:
+        past_inp = tf.unstack(embeddings[:, 0:t], axis=1)
+        avg_past_inp = tf.add_n(past_inp) / len(past_inp)
+      else:
+        avg_past_inp = tf.zeros_like(inp)
+
+      if t < FLAGS.sequence_length:
+        future_inp = tf.unstack(embeddings[:, t:], axis=1)
+        avg_future_inp = tf.add_n(future_inp) / len(future_inp)
+      else:
+        avg_future_inp = tf.zeros_like(inp)
+
+      # Cumulative input.
+      concat_inp = tf.concat([avg_past_inp, inp, avg_future_inp], axis=1)
+
+      # Hidden activations.
+      hidden = tf.nn.relu(tf.nn.xw_plus_b(concat_inp, W, b, name="scores"))
+
+      # Add dropout
+      with tf.variable_scope("dropout"):
+        hidden = tf.nn.dropout(hidden, FLAGS.keep_prob)
+
+      # Output.
+      output = tf.nn.xw_plus_b(hidden, W_out, b_out, name="output")
+
+      predictions.append(output)
+    predictions = tf.stack(predictions, axis=1)
+    return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/rnn.py b/research/maskgan/models/rnn.py
new file mode 100644
index 00000000..3c6ee6c7
--- /dev/null
+++ b/research/maskgan/models/rnn.py
@@ -0,0 +1,210 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple RNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+# ZoneoutWrapper.
+from regularization import zoneout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph.
+
+    G will now impute tokens that have been masked from the input seqeunce.
+  """
+  tf.logging.warning(
+      'Undirectional generative model is not a useful model for this MaskGAN '
+      'because future context is needed.  Use only for debugging purposes.')
+  init_scale = 0.05
+  initializer = tf.random_uniform_initializer(-init_scale, init_scale)
+
+  with tf.variable_scope('gen', reuse=reuse, initializer=initializer):
+
+    def lstm_cell():
+      return tf.contrib.rnn.LayerNormBasicLSTMCell(
+          hparams.gen_rnn_size, reuse=reuse)
+
+    attn_cell = lstm_cell
+    if FLAGS.zoneout_drop_prob > 0.0:
+
+      def attn_cell():
+        return zoneout.ZoneoutWrapper(
+            lstm_cell(),
+            zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+            is_training=is_training)
+
+    cell_gen = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    initial_state = cell_gen.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+      softmax_w = tf.get_variable('softmax_w',
+                                  [hparams.gen_rnn_size, FLAGS.vocab_size])
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the model is the first token to provide context.  The
+        # model will then predict token t > 0.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          state_gen = initial_state
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the target at the last time-step was present, read in the real.
+        # If the target at the last time-step was not present, read in the fake.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+          fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+
+          # Use teacher forcing.
+          if (is_training and
+              FLAGS.gen_training_strategy == 'cross_entropy') or is_validating:
+            rnn_inp = real_rnn_inp
+          else:
+            # Note that targets_t-1 == inputs_(t)
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+        logit = tf.matmul(rnn_out, softmax_w) + softmax_b
+
+        # Real sample.
+        real = targets[:, t]
+
+        # Fake sample.
+        categorical = tf.contrib.distributions.Categorical(logits=logit)
+        fake = categorical.sample()
+        log_prob = categorical.log_prob(fake)
+
+        # Output for Generator will either be generated or the target.
+        # If present:   Return real.
+        # If not present:  Return fake.
+        output = tf.where(targets_present[:, t], real, fake)
+
+        # Append to lists.
+        sequence.append(output)
+        logits.append(logit)
+        log_probs.append(log_prob)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state_gen = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = rnn_inputs[:, t]
+
+        # RNN.
+        rnn_out, real_state_gen = cell_gen(rnn_inp, real_state_gen)
+
+      final_state = real_state_gen
+
+  return (tf.stack(sequence, axis=1), tf.stack(logits, axis=1), tf.stack(
+      log_probs, axis=1), initial_state, final_state)
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the Discriminator graph.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    FLAGS: Current flags.
+    sequence:  [FLAGS.batch_size, FLAGS.sequence_length]
+    is_training:
+    reuse
+
+  Returns:
+    predictions:
+  """
+  tf.logging.warning(
+      'Undirectional Discriminative model is not a useful model for this '
+      'MaskGAN because future context is needed.  Use only for debugging '
+      'purposes.')
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('dis', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.LayerNormBasicLSTMCell(
+          hparams.dis_rnn_size, reuse=reuse)
+
+    attn_cell = lstm_cell
+    if FLAGS.zoneout_drop_prob > 0.0:
+
+      def attn_cell():
+        return zoneout.ZoneoutWrapper(
+            lstm_cell(),
+            zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+            is_training=is_training)
+
+    cell_dis = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+    state_dis = cell_dis.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn') as vs:
+      predictions = []
+      if not FLAGS.dis_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = rnn_inputs[:, t]
+        rnn_out, state_dis = cell_dis(rnn_in, state_dis)
+
+        # Prediction is linear output for Discriminator.
+        pred = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+
+        predictions.append(pred)
+  predictions = tf.stack(predictions, axis=1)
+  return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/rnn_nas.py b/research/maskgan/models/rnn_nas.py
new file mode 100644
index 00000000..f5985f52
--- /dev/null
+++ b/research/maskgan/models/rnn_nas.py
@@ -0,0 +1,233 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple RNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import tensorflow as tf
+
+# NAS Code..
+from nas_utils import configs
+from nas_utils import custom_cell
+from nas_utils import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def get_config():
+  return configs.AlienConfig2()
+
+
+LSTMTuple = collections.namedtuple('LSTMTuple', ['c', 'h'])
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph.
+
+    G will now impute tokens that have been masked from the input seqeunce.
+  """
+  tf.logging.info(
+      'Undirectional generative model is not a useful model for this MaskGAN '
+      'because future context is needed.  Use only for debugging purposes.')
+  config = get_config()
+  config.keep_prob = [hparams.gen_nas_keep_prob_0, hparams.gen_nas_keep_prob_1]
+  configs.print_config(config)
+
+  init_scale = config.init_scale
+  initializer = tf.random_uniform_initializer(-init_scale, init_scale)
+
+  with tf.variable_scope('gen', reuse=reuse, initializer=initializer):
+    # Neural architecture search cell.
+    cell = custom_cell.Alien(config.hidden_size)
+
+    if is_training:
+      [h2h_masks, _, _,
+       output_mask] = variational_dropout.generate_variational_dropout_masks(
+           hparams, config.keep_prob)
+    else:
+      output_mask = None
+
+    cell_gen = custom_cell.GenericMultiRNNCell([cell] * config.num_layers)
+    initial_state = cell_gen.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+      softmax_w = tf.matrix_transpose(embedding)
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the model is the first token to provide context.  The
+        # model will then predict token t > 0.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          state_gen = initial_state
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+          fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or (is_training and
+                               FLAGS.gen_training_strategy == 'cross_entropy'):
+            rnn_inp = real_rnn_inp
+          else:
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        if is_training:
+          state_gen = list(state_gen)
+          for layer_num, per_layer_state in enumerate(state_gen):
+            per_layer_state = LSTMTuple(
+                per_layer_state[0], per_layer_state[1] * h2h_masks[layer_num])
+            state_gen[layer_num] = per_layer_state
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+
+        if is_training:
+          rnn_out = output_mask * rnn_out
+
+        logit = tf.matmul(rnn_out, softmax_w) + softmax_b
+
+        # Real sample.
+        real = targets[:, t]
+
+        categorical = tf.contrib.distributions.Categorical(logits=logit)
+        fake = categorical.sample()
+        log_prob = categorical.log_prob(fake)
+
+        # Output for Generator will either be generated or the input.
+        #
+        # If present:   Return real.
+        # If not present:  Return fake.
+        output = tf.where(targets_present[:, t], real, fake)
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state_gen = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = rnn_inputs[:, t]
+
+        # RNN.
+        rnn_out, real_state_gen = cell_gen(rnn_inp, real_state_gen)
+
+      final_state = real_state_gen
+
+  return (tf.stack(sequence, axis=1), tf.stack(logits, axis=1), tf.stack(
+      log_probs, axis=1), initial_state, final_state)
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the Discriminator graph."""
+  tf.logging.info(
+      'Undirectional Discriminative model is not a useful model for this '
+      'MaskGAN because future context is needed.  Use only for debugging '
+      'purposes.')
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  config = get_config()
+  config.keep_prob = [hparams.dis_nas_keep_prob_0, hparams.dis_nas_keep_prob_1]
+  configs.print_config(config)
+
+  with tf.variable_scope('dis', reuse=reuse):
+    # Neural architecture search cell.
+    cell = custom_cell.Alien(config.hidden_size)
+
+    if is_training:
+      [h2h_masks, _, _,
+       output_mask] = variational_dropout.generate_variational_dropout_masks(
+           hparams, config.keep_prob)
+    else:
+      output_mask = None
+
+    cell_dis = custom_cell.GenericMultiRNNCell([cell] * config.num_layers)
+    state_dis = cell_dis.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn') as vs:
+      predictions = []
+      if not FLAGS.dis_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = rnn_inputs[:, t]
+
+        if is_training:
+          state_dis = list(state_dis)
+          for layer_num, per_layer_state in enumerate(state_dis):
+            per_layer_state = LSTMTuple(
+                per_layer_state[0], per_layer_state[1] * h2h_masks[layer_num])
+            state_dis[layer_num] = per_layer_state
+
+        # RNN.
+        rnn_out, state_dis = cell_dis(rnn_in, state_dis)
+
+        if is_training:
+          rnn_out = output_mask * rnn_out
+
+        # Prediction is linear output for Discriminator.
+        pred = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+
+        predictions.append(pred)
+  predictions = tf.stack(predictions, axis=1)
+  return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/rnn_vd.py b/research/maskgan/models/rnn_vd.py
new file mode 100644
index 00000000..938d520b
--- /dev/null
+++ b/research/maskgan/models/rnn_vd.py
@@ -0,0 +1,117 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple RNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+from regularization import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def discriminator(hparams,
+                  sequence,
+                  is_training,
+                  reuse=None,
+                  initial_state=None):
+  """Define the Discriminator graph."""
+  tf.logging.info(
+      'Undirectional Discriminative model is not a useful model for this '
+      'MaskGAN because future context is needed.  Use only for debugging '
+      'purposes.')
+  sequence = tf.cast(sequence, tf.int32)
+
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('dis', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.dis_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,
+            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)
+
+    cell_dis = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    if initial_state:
+      state_dis = [[tf.identity(x) for x in inner_initial_state]
+                   for inner_initial_state in initial_state]
+    else:
+      state_dis = cell_dis.zero_state(FLAGS.batch_size, tf.float32)
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    if is_training:
+      output_mask = make_mask(hparams.dis_vd_keep_prob, hparams.dis_rnn_size)
+
+    with tf.variable_scope('rnn') as vs:
+      predictions, rnn_outs = [], []
+
+      if not FLAGS.dis_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = rnn_inputs[:, t]
+        rnn_out, state_dis = cell_dis(rnn_in, state_dis)
+
+        if is_training:
+          rnn_out *= output_mask
+
+        # Prediction is linear output for Discriminator.
+        pred = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+        predictions.append(pred)
+        rnn_outs.append(rnn_out)
+
+  predictions = tf.stack(predictions, axis=1)
+
+  if FLAGS.baseline_method == 'critic':
+    with tf.variable_scope('critic', reuse=reuse) as critic_scope:
+      rnn_outs = tf.stack(rnn_outs, axis=1)
+      values = tf.contrib.layers.linear(rnn_outs, 1, scope=critic_scope)
+    return tf.squeeze(predictions, axis=2), tf.squeeze(values, axis=2)
+
+  else:
+    return tf.squeeze(predictions, axis=2), None
diff --git a/research/maskgan/models/rnn_zaremba.py b/research/maskgan/models/rnn_zaremba.py
new file mode 100644
index 00000000..1c34ffde
--- /dev/null
+++ b/research/maskgan/models/rnn_zaremba.py
@@ -0,0 +1,195 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple RNN model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph.
+
+    G will now impute tokens that have been masked from the input seqeunce.
+  """
+  tf.logging.warning(
+      'Undirectional generative model is not a useful model for this MaskGAN '
+      'because future context is needed.  Use only for debugging purposes.')
+  init_scale = 0.05
+  initializer = tf.random_uniform_initializer(-init_scale, init_scale)
+  with tf.variable_scope('gen', reuse=reuse, initializer=initializer):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(hparams.gen_rnn_size,
+                                          forget_bias=0.0,
+                                          state_is_tuple=True,
+                                          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and FLAGS.keep_prob < 1:
+
+      def attn_cell():
+        return tf.contrib.rnn.DropoutWrapper(
+            lstm_cell(), output_keep_prob=FLAGS.keep_prob)
+
+    cell_gen = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    initial_state = cell_gen.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+      softmax_w = tf.get_variable('softmax_w',
+                                  [hparams.gen_rnn_size, FLAGS.vocab_size])
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      fake = None
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the model is the first token to provide context.  The
+        # model will then predict token t > 0.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          state_gen = initial_state
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+          fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or (is_training and
+                               FLAGS.gen_training_strategy == 'cross_entropy'):
+            rnn_inp = real_rnn_inp
+          else:
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+        logit = tf.matmul(rnn_out, softmax_w) + softmax_b
+
+        # Real sample.
+        real = targets[:, t]
+
+        categorical = tf.contrib.distributions.Categorical(logits=logit)
+        fake = categorical.sample()
+        log_prob = categorical.log_prob(fake)
+
+        # Output for Generator will either be generated or the input.
+        #
+        # If present:   Return real.
+        # If not present:  Return fake.
+        output = tf.where(targets_present[:, t], real, fake)
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state_gen = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = rnn_inputs[:, t]
+
+        # RNN.
+        rnn_out, real_state_gen = cell_gen(rnn_inp, real_state_gen)
+
+      final_state = real_state_gen
+
+  return (tf.stack(sequence, axis=1), tf.stack(logits, axis=1), tf.stack(
+      log_probs, axis=1), initial_state, final_state)
+
+
+def discriminator(hparams, sequence, is_training, reuse=None):
+  """Define the Discriminator graph."""
+  tf.logging.warning(
+      'Undirectional Discriminative model is not a useful model for this '
+      'MaskGAN because future context is needed.  Use only for debugging '
+      'purposes.')
+  sequence = tf.cast(sequence, tf.int32)
+
+  with tf.variable_scope('dis', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(hparams.dis_rnn_size,
+                                          forget_bias=0.0,
+                                          state_is_tuple=True,
+                                          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and FLAGS.keep_prob < 1:
+
+      def attn_cell():
+        return tf.contrib.rnn.DropoutWrapper(
+            lstm_cell(), output_keep_prob=FLAGS.keep_prob)
+
+    cell_dis = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    state_dis = cell_dis.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn') as vs:
+      predictions = []
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = rnn_inputs[:, t]
+        rnn_out, state_dis = cell_dis(rnn_in, state_dis)
+
+        # Prediction is linear output for Discriminator.
+        pred = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+
+        predictions.append(pred)
+  predictions = tf.stack(predictions, axis=1)
+  return tf.squeeze(predictions, axis=2)
diff --git a/research/maskgan/models/rollout.py b/research/maskgan/models/rollout.py
new file mode 100644
index 00000000..b93828c2
--- /dev/null
+++ b/research/maskgan/models/rollout.py
@@ -0,0 +1,383 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Rollout RNN model definitions which call rnn_zaremba code."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+
+import tensorflow as tf
+
+from losses import losses
+from model_utils import helper
+from model_utils import model_construction
+from model_utils import model_losses
+from model_utils import model_optimization
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def create_rollout_MaskGAN(hparams, is_training):
+  """Create the MaskGAN model.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    is_training:  Boolean indicating operational mode (train/inference).
+      evaluated with a teacher forcing regime.
+
+  Return:
+    model:  Namedtuple for specifying the MaskGAN."""
+  global_step = tf.Variable(0, name='global_step', trainable=False)
+
+  new_learning_rate = tf.placeholder(tf.float32, [], name='new_learning_rate')
+  learning_rate = tf.Variable(0.0, name='learning_rate', trainable=False)
+  learning_rate_update = tf.assign(learning_rate, new_learning_rate)
+
+  new_rate = tf.placeholder(tf.float32, [], name='new_rate')
+  percent_real_var = tf.Variable(0.0, trainable=False)
+  percent_real_update = tf.assign(percent_real_var, new_rate)
+
+  ## Placeholders.
+  inputs = tf.placeholder(
+      tf.int32, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  present = tf.placeholder(
+      tf.bool, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  inv_present = tf.placeholder(
+      tf.bool, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  ## Rollout Generator.
+  fwd_gen_rollouts = rollout_generator(
+      hparams, inputs, present, is_training=is_training, is_validating=False)
+  inv_gen_rollouts = rollout_generator(
+      hparams,
+      inputs,
+      inv_present,
+      is_training=is_training,
+      is_validating=False,
+      reuse=True)
+
+  ## Rollout Discriminator.
+  fwd_dis_rollouts = rollout_discriminator(
+      hparams, fwd_gen_rollouts, is_training=is_training)
+  inv_dis_rollouts = rollout_discriminator(
+      hparams, inv_gen_rollouts, is_training=is_training, reuse=True)
+
+  ## Discriminator Loss.
+  [dis_loss, dis_loss_pred, dis_loss_inv_pred] = rollout_discriminator_loss(
+      fwd_dis_rollouts, present, inv_dis_rollouts, inv_present)
+
+  ## Average log-perplexity for only missing words.  However, to do this,
+  # the logits are still computed using teacher forcing, that is, the ground
+  # truth tokens are fed in at each time point to be valid.
+  # TODO(liamfedus): Fix the naming convention.
+  with tf.variable_scope('gen_rollout'):
+    _, fwd_eval_logits, _ = model_construction.create_generator(
+        hparams,
+        inputs,
+        present,
+        is_training=False,
+        is_validating=True,
+        reuse=True)
+
+  avg_log_perplexity = model_losses.calculate_log_perplexity(
+      fwd_eval_logits, inputs, present)
+
+  ## Generator Loss.
+  # 1.  Cross Entropy losses on missing tokens.
+  [fwd_cross_entropy_losses,
+   inv_cross_entropy_losses] = rollout_masked_cross_entropy_loss(
+       inputs, present, inv_present, fwd_gen_rollouts, inv_gen_rollouts)
+
+  # 2.  GAN losses on missing tokens.
+  [fwd_RL_loss,
+   fwd_RL_statistics, fwd_averages_op] = rollout_reinforce_objective(
+       hparams, fwd_gen_rollouts, fwd_dis_rollouts, present)
+  [inv_RL_loss,
+   inv_RL_statistics, inv_averages_op] = rollout_reinforce_objective(
+       hparams, inv_gen_rollouts, inv_dis_rollouts, inv_present)
+
+  # TODO(liamfedus):  Generalize this to use all logs.
+  [fwd_sequence, fwd_logits, fwd_log_probs] = fwd_gen_rollouts[-1]
+  [inv_sequence, inv_logits, inv_log_probs] = inv_gen_rollouts[-1]
+
+  # TODO(liamfedus):  Generalize this to use all logs.
+  fwd_predictions = fwd_dis_rollouts[-1]
+  inv_predictions = inv_dis_rollouts[-1]
+
+  # TODO(liamfedus):  Generalize this to use all logs.
+  [fwd_log_probs, fwd_rewards, fwd_advantages,
+   fwd_baselines] = fwd_RL_statistics[-1]
+  [inv_log_probs, inv_rewards, inv_advantages,
+   inv_baselines] = inv_RL_statistics[-1]
+
+  ## Pre-training.
+  if FLAGS.gen_pretrain_steps:
+    # TODO(liamfedus): Rewrite this.
+    fwd_cross_entropy_loss = tf.reduce_mean(fwd_cross_entropy_losses)
+    gen_pretrain_op = model_optimization.create_gen_pretrain_op(
+        hparams, fwd_cross_entropy_loss, global_step)
+  else:
+    gen_pretrain_op = tf.no_op('gen_pretrain_no_op')
+  if FLAGS.dis_pretrain_steps:
+    dis_pretrain_op = model_optimization.create_dis_pretrain_op(
+        hparams, dis_loss, global_step)
+  else:
+    dis_pretrain_op = tf.no_op('dis_pretrain_no_op')
+
+  ##  Generator Train Op.
+  # 1.  Cross-Entropy.
+  if FLAGS.gen_training_strategy == 'cross_entropy':
+    gen_loss = tf.reduce_mean(
+        fwd_cross_entropy_losses + inv_cross_entropy_losses) / 2.
+    [gen_train_op, gen_grads,
+     gen_vars] = model_optimization.create_gen_train_op(
+         hparams, learning_rate, gen_loss, global_step, mode='MINIMIZE')
+
+  # 2.  GAN (REINFORCE)
+  elif FLAGS.gen_training_strategy == 'reinforce':
+    gen_loss = (fwd_RL_loss + inv_RL_loss) / 2.
+    [gen_train_op, gen_grads,
+     gen_vars] = model_optimization.create_reinforce_gen_train_op(
+         hparams, learning_rate, gen_loss, fwd_averages_op, inv_averages_op,
+         global_step)
+
+  else:
+    raise NotImplementedError
+
+  ## Discriminator Train Op.
+  dis_train_op, dis_grads, dis_vars = model_optimization.create_dis_train_op(
+      hparams, dis_loss, global_step)
+
+  ## Summaries.
+  with tf.name_scope('general'):
+    tf.summary.scalar('percent_real', percent_real_var)
+    tf.summary.scalar('learning_rate', learning_rate)
+
+  with tf.name_scope('generator_losses'):
+    tf.summary.scalar('gen_loss', tf.reduce_mean(gen_loss))
+    tf.summary.scalar('gen_loss_fwd_cross_entropy',
+                      tf.reduce_mean(fwd_cross_entropy_losses))
+    tf.summary.scalar('gen_loss_inv_cross_entropy',
+                      tf.reduce_mean(inv_cross_entropy_losses))
+
+  with tf.name_scope('REINFORCE'):
+    with tf.name_scope('objective'):
+      tf.summary.scalar('fwd_RL_loss', tf.reduce_mean(fwd_RL_loss))
+      tf.summary.scalar('inv_RL_loss', tf.reduce_mean(inv_RL_loss))
+
+    with tf.name_scope('rewards'):
+      helper.variable_summaries(fwd_rewards, 'fwd_rewards')
+      helper.variable_summaries(inv_rewards, 'inv_rewards')
+
+    with tf.name_scope('advantages'):
+      helper.variable_summaries(fwd_advantages, 'fwd_advantages')
+      helper.variable_summaries(inv_advantages, 'inv_advantages')
+
+    with tf.name_scope('baselines'):
+      helper.variable_summaries(fwd_baselines, 'fwd_baselines')
+      helper.variable_summaries(inv_baselines, 'inv_baselines')
+
+    with tf.name_scope('log_probs'):
+      helper.variable_summaries(fwd_log_probs, 'fwd_log_probs')
+      helper.variable_summaries(inv_log_probs, 'inv_log_probs')
+
+  with tf.name_scope('discriminator_losses'):
+    tf.summary.scalar('dis_loss', dis_loss)
+    tf.summary.scalar('dis_loss_fwd_sequence', dis_loss_pred)
+    tf.summary.scalar('dis_loss_inv_sequence', dis_loss_inv_pred)
+
+  with tf.name_scope('logits'):
+    helper.variable_summaries(fwd_logits, 'fwd_logits')
+    helper.variable_summaries(inv_logits, 'inv_logits')
+
+  for v, g in zip(gen_vars, gen_grads):
+    helper.variable_summaries(v, v.op.name)
+    helper.variable_summaries(g, 'grad/' + v.op.name)
+
+  for v, g in zip(dis_vars, dis_grads):
+    helper.variable_summaries(v, v.op.name)
+    helper.variable_summaries(g, 'grad/' + v.op.name)
+
+  merge_summaries_op = tf.summary.merge_all()
+
+  # Model saver.
+  saver = tf.train.Saver(keep_checkpoint_every_n_hours=1, max_to_keep=5)
+
+  # Named tuple that captures elements of the MaskGAN model.
+  Model = collections.namedtuple('Model', [
+      'inputs', 'present', 'inv_present', 'percent_real_update', 'new_rate',
+      'fwd_sequence', 'fwd_logits', 'fwd_rewards', 'fwd_advantages',
+      'fwd_log_probs', 'fwd_predictions', 'fwd_cross_entropy_losses',
+      'inv_sequence', 'inv_logits', 'inv_rewards', 'inv_advantages',
+      'inv_log_probs', 'inv_predictions', 'inv_cross_entropy_losses',
+      'avg_log_perplexity', 'dis_loss', 'gen_loss', 'dis_train_op',
+      'gen_train_op', 'gen_pretrain_op', 'dis_pretrain_op',
+      'merge_summaries_op', 'global_step', 'new_learning_rate',
+      'learning_rate_update', 'saver'
+  ])
+
+  model = Model(
+      inputs, present, inv_present, percent_real_update, new_rate, fwd_sequence,
+      fwd_logits, fwd_rewards, fwd_advantages, fwd_log_probs, fwd_predictions,
+      fwd_cross_entropy_losses, inv_sequence, inv_logits, inv_rewards,
+      inv_advantages, inv_log_probs, inv_predictions, inv_cross_entropy_losses,
+      avg_log_perplexity, dis_loss, gen_loss, dis_train_op, gen_train_op,
+      gen_pretrain_op, dis_pretrain_op, merge_summaries_op, global_step,
+      new_learning_rate, learning_rate_update, saver)
+  return model
+
+
+def rollout_generator(hparams,
+                      inputs,
+                      input_present,
+                      is_training,
+                      is_validating,
+                      reuse=None):
+  """Define the Generator graph which does rollouts.
+
+    G will now impute tokens that have been masked from the input seqeunce.
+  """
+  rollouts = []
+
+  with tf.variable_scope('gen_rollout'):
+    for n in xrange(FLAGS.num_rollouts):
+      if n > 0:
+        # TODO(liamfedus): Why is it necessary here to manually set reuse?
+        reuse = True
+        tf.get_variable_scope().reuse_variables()
+
+      [sequence, logits, log_probs] = model_construction.create_generator(
+          hparams,
+          inputs,
+          input_present,
+          is_training,
+          is_validating,
+          reuse=reuse)
+
+      rollouts.append([sequence, logits, log_probs])
+
+  # Length assertion.
+  assert len(rollouts) == FLAGS.num_rollouts
+
+  return rollouts
+
+
+def rollout_discriminator(hparams, gen_rollouts, is_training, reuse=None):
+  """Define the Discriminator graph which does rollouts.
+
+    G will now impute tokens that have been masked from the input seqeunce.
+  """
+  rollout_predictions = []
+
+  with tf.variable_scope('dis_rollout'):
+    for n, rollout in enumerate(gen_rollouts):
+      if n > 0:
+        # TODO(liamfedus): Why is it necessary here to manually set reuse?
+        reuse = True
+        tf.get_variable_scope().reuse_variables()
+
+      [sequence, _, _] = rollout
+
+      predictions = model_construction.create_discriminator(
+          hparams, sequence, is_training=is_training, reuse=reuse)
+
+      # Predictions for each rollout.
+      rollout_predictions.append(predictions)
+
+  # Length assertion.
+  assert len(rollout_predictions) == FLAGS.num_rollouts
+
+  return rollout_predictions
+
+
+def rollout_reinforce_objective(hparams, gen_rollouts, dis_rollouts, present):
+  cumulative_gen_objective = 0.
+  cumulative_averages_op = []
+  cumulative_statistics = []
+
+  assert len(gen_rollouts) == len(dis_rollouts)
+
+  for gen_rollout, dis_rollout in zip(gen_rollouts, dis_rollouts):
+    [_, _, log_probs] = gen_rollout
+    dis_predictions = dis_rollout
+
+    [
+        final_gen_objective, log_probs, rewards, advantages, baselines,
+        maintain_averages_op
+    ] = model_losses.calculate_reinforce_objective(hparams, log_probs,
+                                                   dis_predictions, present)
+
+    # Accumulate results.
+    cumulative_gen_objective += final_gen_objective
+    cumulative_averages_op.append(maintain_averages_op)
+    cumulative_statistics.append([log_probs, rewards, advantages, baselines])
+
+  # Group all the averaging operations.
+  cumulative_averages_op = tf.group(*cumulative_averages_op)
+  cumulative_gen_objective /= FLAGS.num_rollouts
+  [log_probs, rewards, advantages, baselines] = cumulative_statistics[-1]
+
+  # Length assertion.
+  assert len(cumulative_statistics) == FLAGS.num_rollouts
+
+  return [
+      cumulative_gen_objective, cumulative_statistics, cumulative_averages_op
+  ]
+
+
+def rollout_masked_cross_entropy_loss(inputs, present, inv_present,
+                                      fwd_rollouts, inv_rollouts):
+  cumulative_fwd_cross_entropy_losses = tf.zeros(
+      shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  cumulative_inv_cross_entropy_losses = tf.zeros(
+      shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  for fwd_rollout, inv_rollout in zip(fwd_rollouts, inv_rollouts):
+    [_, fwd_logits, _] = fwd_rollout
+    [_, inv_logits, _] = inv_rollout
+
+    [fwd_cross_entropy_losses,
+     inv_cross_entropy_losses] = model_losses.create_masked_cross_entropy_loss(
+         inputs, present, inv_present, fwd_logits, inv_logits)
+
+    cumulative_fwd_cross_entropy_losses = tf.add(
+        cumulative_fwd_cross_entropy_losses, fwd_cross_entropy_losses)
+    cumulative_inv_cross_entropy_losses = tf.add(
+        cumulative_inv_cross_entropy_losses, inv_cross_entropy_losses)
+
+  return [
+      cumulative_fwd_cross_entropy_losses, cumulative_inv_cross_entropy_losses
+  ]
+
+
+def rollout_discriminator_loss(fwd_rollouts, present, inv_rollouts,
+                               inv_present):
+
+  dis_loss = 0
+  dis_loss_pred = 0
+  dis_loss_inv_pred = 0
+
+  for fwd_predictions, inv_predictions in zip(fwd_rollouts, inv_rollouts):
+    dis_loss_pred += losses.discriminator_loss(fwd_predictions, present)
+    dis_loss_inv_pred += losses.discriminator_loss(inv_predictions, inv_present)
+
+  dis_loss_pred /= FLAGS.num_rollouts
+  dis_loss_inv_pred /= FLAGS.num_rollouts
+
+  dis_loss = (dis_loss_pred + dis_loss_inv_pred) / 2.
+  return [dis_loss, dis_loss_pred, dis_loss_inv_pred]
diff --git a/research/maskgan/models/seq2seq.py b/research/maskgan/models/seq2seq.py
new file mode 100644
index 00000000..e8574757
--- /dev/null
+++ b/research/maskgan/models/seq2seq.py
@@ -0,0 +1,277 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple seq2seq model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from models import attention_utils
+
+# ZoneoutWrapper.
+from regularization import zoneout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def transform_input_with_is_missing_token(inputs, targets_present):
+  """Transforms the inputs to have missing tokens when it's masked out.  The
+  mask is for the targets, so therefore, to determine if an input at time t is
+  masked, we have to check if the target at time t - 1 is masked out.
+
+  e.g.
+    inputs = [a, b, c, d]
+    targets = [b, c, d, e]
+    targets_present = [1, 0, 1, 0]
+
+  then,
+    transformed_input = [a, b, <missing>, d]
+
+  Args:
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the word.
+
+  Returns:
+    transformed_input:  tf.int32 Tensor of shape [batch_size, sequence_length]
+      which takes on value of inputs when the input is present and takes on
+      value=vocab_size to indicate a missing token.
+  """
+  # To fill in if the input is missing.
+  input_missing = tf.constant(
+      FLAGS.vocab_size,
+      dtype=tf.int32,
+      shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  # The 0th input will always be present to MaskGAN.
+  zeroth_input_present = tf.constant(True, tf.bool, shape=[FLAGS.batch_size, 1])
+
+  # Input present mask.
+  inputs_present = tf.concat(
+      [zeroth_input_present, targets_present[:, :-1]], axis=1)
+
+  transformed_input = tf.where(inputs_present, inputs, input_missing)
+  return transformed_input
+
+
+def gen_encoder(hparams, inputs, targets_present, is_training, reuse=None):
+  """Define the Encoder graph."""
+  # We will use the same variable from the decoder.
+  if FLAGS.seq2seq_share_embedding:
+    with tf.variable_scope('decoder/rnn'):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('encoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.LayerNormBasicLSTMCell(
+          hparams.gen_rnn_size, reuse=reuse)
+
+    attn_cell = lstm_cell
+    if FLAGS.zoneout_drop_prob > 0.0:
+
+      def attn_cell():
+        return zoneout.ZoneoutWrapper(
+            lstm_cell(),
+            zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+            is_training=is_training)
+
+    cell = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    initial_state = cell.zero_state(FLAGS.batch_size, tf.float32)
+
+    # Add a missing token for inputs not present.
+    real_inputs = inputs
+    masked_inputs = transform_input_with_is_missing_token(
+        inputs, targets_present)
+
+    with tf.variable_scope('rnn'):
+      hidden_states = []
+
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size + 1, hparams.gen_rnn_size])
+
+      real_rnn_inputs = tf.nn.embedding_lookup(embedding, real_inputs)
+      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)
+
+      state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = masked_rnn_inputs[:, t]
+        rnn_out, state = cell(rnn_inp, state)
+        hidden_states.append(rnn_out)
+      final_masked_state = state
+      hidden_states = tf.stack(hidden_states, axis=1)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        # RNN.
+        rnn_inp = real_rnn_inputs[:, t]
+        rnn_out, real_state = cell(rnn_inp, real_state)
+      final_state = real_state
+
+  return (hidden_states, final_masked_state), initial_state, final_state
+
+
+def gen_decoder(hparams,
+                inputs,
+                targets,
+                targets_present,
+                encoding_state,
+                is_training,
+                is_validating,
+                reuse=None):
+  """Define the Decoder graph. The Decoder will now impute tokens that
+      have been masked from the input seqeunce.
+  """
+  gen_decoder_rnn_size = hparams.gen_rnn_size
+
+  with tf.variable_scope('decoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.LayerNormBasicLSTMCell(
+          gen_decoder_rnn_size, reuse=reuse)
+
+    attn_cell = lstm_cell
+    if FLAGS.zoneout_drop_prob > 0.0:
+
+      def attn_cell():
+        return zoneout.ZoneoutWrapper(
+            lstm_cell(),
+            zoneout_drop_prob=FLAGS.zoneout_drop_prob,
+            is_training=is_training)
+
+    cell_gen = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    # Hidden encoder states.
+    hidden_vector_encodings = encoding_state[0]
+
+    # Carry forward the final state tuple from the encoder.
+    # State tuples.
+    state_gen = encoding_state[1]
+
+    if FLAGS.attention_option is not None:
+      (attention_keys, attention_values, _,
+       attention_construct_fn) = attention_utils.prepare_attention(
+           hidden_vector_encodings,
+           FLAGS.attention_option,
+           num_units=gen_decoder_rnn_size,
+           reuse=reuse)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, gen_decoder_rnn_size])
+      softmax_w = tf.get_variable('softmax_w',
+                                  [gen_decoder_rnn_size, FLAGS.vocab_size])
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the Decoder.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+          fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or (is_training and
+                               FLAGS.gen_training_strategy == 'cross_entropy'):
+            rnn_inp = real_rnn_inp
+          else:
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+
+        if FLAGS.attention_option is not None:
+          rnn_out = attention_construct_fn(rnn_out, attention_keys,
+                                           attention_values)
+        #   # TODO(liamfedus): Assert not "monotonic" attention_type.
+        #   # TODO(liamfedus): FLAGS.attention_type.
+        #   context_state = revised_attention_utils._empty_state()
+        #   rnn_out, context_state = attention_construct_fn(
+        #       rnn_out, attention_keys, attention_values, context_state, t)
+        logit = tf.matmul(rnn_out, softmax_w) + softmax_b
+
+        # Output for Decoder.
+        # If input is present:   Return real at t+1.
+        # If input is not present:  Return fake for t+1.
+        real = targets[:, t]
+
+        categorical = tf.contrib.distributions.Categorical(logits=logit)
+        fake = categorical.sample()
+        log_prob = categorical.log_prob(fake)
+
+        output = tf.where(targets_present[:, t], real, fake)
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+
+  return (tf.stack(sequence, axis=1), tf.stack(logits, axis=1), tf.stack(
+      log_probs, axis=1))
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph."""
+  with tf.variable_scope('gen', reuse=reuse):
+    encoder_states, initial_state, final_state = gen_encoder(
+        hparams, inputs, targets_present, is_training=is_training, reuse=reuse)
+    stacked_sequence, stacked_logits, stacked_log_probs = gen_decoder(
+        hparams,
+        inputs,
+        targets,
+        targets_present,
+        encoder_states,
+        is_training=is_training,
+        is_validating=is_validating,
+        reuse=reuse)
+    return (stacked_sequence, stacked_logits, stacked_log_probs, initial_state,
+            final_state)
diff --git a/research/maskgan/models/seq2seq_nas.py b/research/maskgan/models/seq2seq_nas.py
new file mode 100644
index 00000000..58683cd9
--- /dev/null
+++ b/research/maskgan/models/seq2seq_nas.py
@@ -0,0 +1,332 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple seq2seq model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import tensorflow as tf
+
+from models import attention_utils
+
+# NAS Code..
+from nas_utils import configs
+from nas_utils import custom_cell
+from nas_utils import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def get_config():
+  return configs.AlienConfig2()
+
+
+LSTMTuple = collections.namedtuple('LSTMTuple', ['c', 'h'])
+
+
+def transform_input_with_is_missing_token(inputs, targets_present):
+  """Transforms the inputs to have missing tokens when it's masked out.  The
+  mask is for the targets, so therefore, to determine if an input at time t is
+  masked, we have to check if the target at time t - 1 is masked out.
+
+  e.g.
+    inputs = [a, b, c, d]
+    targets = [b, c, d, e]
+    targets_present = [1, 0, 1, 0]
+
+  then,
+    transformed_input = [a, b, <missing>, d]
+
+  Args:
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the word.
+
+  Returns:
+    transformed_input:  tf.int32 Tensor of shape [batch_size, sequence_length]
+      which takes on value of inputs when the input is present and takes on
+      value=vocab_size to indicate a missing token.
+  """
+  # To fill in if the input is missing.
+  input_missing = tf.constant(
+      FLAGS.vocab_size,
+      dtype=tf.int32,
+      shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  # The 0th input will always be present to MaskGAN.
+  zeroth_input_present = tf.constant(True, tf.bool, shape=[FLAGS.batch_size, 1])
+
+  # Input present mask.
+  inputs_present = tf.concat(
+      [zeroth_input_present, targets_present[:, :-1]], axis=1)
+
+  transformed_input = tf.where(inputs_present, inputs, input_missing)
+  return transformed_input
+
+
+def gen_encoder(hparams, inputs, targets_present, is_training, reuse=None):
+  """Define the Encoder graph.
+
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the target.
+    is_training:  Boolean indicating operational mode (train/inference).
+    reuse (Optional):   Whether to reuse the variables.
+
+  Returns:
+    Tuple of (hidden_states, final_state).
+  """
+  config = get_config()
+  configs.print_config(config)
+  # We will use the same variable from the decoder.
+  if FLAGS.seq2seq_share_embedding:
+    with tf.variable_scope('decoder/rnn'):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('encoder', reuse=reuse):
+    # Neural architecture search cell.
+    cell = custom_cell.Alien(config.hidden_size)
+
+    if is_training:
+      [h2h_masks, h2i_masks, _,
+       output_mask] = variational_dropout.generate_variational_dropout_masks(
+           hparams, config.keep_prob)
+    else:
+      h2i_masks, output_mask = None, None
+
+    cell = custom_cell.GenericMultiRNNCell([cell] * config.num_layers)
+
+    initial_state = cell.zero_state(FLAGS.batch_size, tf.float32)
+
+    # Add a missing token for inputs not present.
+    real_inputs = inputs
+    masked_inputs = transform_input_with_is_missing_token(
+        inputs, targets_present)
+
+    with tf.variable_scope('rnn'):
+      hidden_states = []
+
+      # Split the embedding into two parts so that we can load the PTB
+      # weights into one part of the Variable.
+      if not FLAGS.seq2seq_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.gen_rnn_size])
+      missing_embedding = tf.get_variable('missing_embedding',
+                                          [1, hparams.gen_rnn_size])
+      embedding = tf.concat([embedding, missing_embedding], axis=0)
+
+      real_rnn_inputs = tf.nn.embedding_lookup(embedding, real_inputs)
+      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        masked_rnn_inputs = tf.nn.dropout(masked_rnn_inputs, FLAGS.keep_prob)
+
+      state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = masked_rnn_inputs[:, t]
+
+        if is_training:
+          state = list(state)
+          for layer_num, per_layer_state in enumerate(state):
+            per_layer_state = LSTMTuple(
+                per_layer_state[0], per_layer_state[1] * h2h_masks[layer_num])
+            state[layer_num] = per_layer_state
+
+        rnn_out, state = cell(rnn_inp, state, h2i_masks)
+
+        if is_training:
+          rnn_out = output_mask * rnn_out
+
+        hidden_states.append(rnn_out)
+      final_masked_state = state
+      hidden_states = tf.stack(hidden_states, axis=1)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        # RNN.
+        rnn_inp = real_rnn_inputs[:, t]
+        rnn_out, real_state = cell(rnn_inp, real_state)
+      final_state = real_state
+
+  return (hidden_states, final_masked_state), initial_state, final_state
+
+
+def gen_decoder(hparams,
+                inputs,
+                targets,
+                targets_present,
+                encoding_state,
+                is_training,
+                is_validating,
+                reuse=None):
+  """Define the Decoder graph. The Decoder will now impute tokens that
+      have been masked from the input seqeunce.
+  """
+  config = get_config()
+  gen_decoder_rnn_size = hparams.gen_rnn_size
+
+  if FLAGS.seq2seq_share_embedding:
+    with tf.variable_scope('decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, gen_decoder_rnn_size])
+
+  with tf.variable_scope('decoder', reuse=reuse):
+    # Neural architecture search cell.
+    cell = custom_cell.Alien(config.hidden_size)
+
+    if is_training:
+      [h2h_masks, _, _,
+       output_mask] = variational_dropout.generate_variational_dropout_masks(
+           hparams, config.keep_prob)
+    else:
+      output_mask = None
+
+    cell_gen = custom_cell.GenericMultiRNNCell([cell] * config.num_layers)
+
+    # Hidden encoder states.
+    hidden_vector_encodings = encoding_state[0]
+
+    # Carry forward the final state tuple from the encoder.
+    # State tuples.
+    state_gen = encoding_state[1]
+
+    if FLAGS.attention_option is not None:
+      (attention_keys, attention_values, _,
+       attention_construct_fn) = attention_utils.prepare_attention(
+           hidden_vector_encodings,
+           FLAGS.attention_option,
+           num_units=gen_decoder_rnn_size,
+           reuse=reuse)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+
+      if not FLAGS.seq2seq_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, gen_decoder_rnn_size])
+      softmax_w = tf.matrix_transpose(embedding)
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the Decoder.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+          fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or (is_training and
+                               FLAGS.gen_training_strategy == 'cross_entropy'):
+            rnn_inp = real_rnn_inp
+          else:
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        if is_training:
+          state_gen = list(state_gen)
+          for layer_num, per_layer_state in enumerate(state_gen):
+            per_layer_state = LSTMTuple(
+                per_layer_state[0], per_layer_state[1] * h2h_masks[layer_num])
+            state_gen[layer_num] = per_layer_state
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+
+        if is_training:
+          rnn_out = output_mask * rnn_out
+
+        if FLAGS.attention_option is not None:
+          rnn_out = attention_construct_fn(rnn_out, attention_keys,
+                                           attention_values)
+        #   # TODO(liamfedus): Assert not "monotonic" attention_type.
+        #   # TODO(liamfedus): FLAGS.attention_type.
+        #   context_state = revised_attention_utils._empty_state()
+        #   rnn_out, context_state = attention_construct_fn(
+        #       rnn_out, attention_keys, attention_values, context_state, t)
+        logit = tf.matmul(rnn_out, softmax_w) + softmax_b
+
+        # Output for Decoder.
+        # If input is present:   Return real at t+1.
+        # If input is not present:  Return fake for t+1.
+        real = targets[:, t]
+
+        categorical = tf.contrib.distributions.Categorical(logits=logit)
+        fake = categorical.sample()
+        log_prob = categorical.log_prob(fake)
+
+        output = tf.where(targets_present[:, t], real, fake)
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+
+  return (tf.stack(sequence, axis=1), tf.stack(logits, axis=1), tf.stack(
+      log_probs, axis=1))
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph."""
+  with tf.variable_scope('gen', reuse=reuse):
+    encoder_states, initial_state, final_state = gen_encoder(
+        hparams, inputs, targets_present, is_training=is_training, reuse=reuse)
+    stacked_sequence, stacked_logits, stacked_log_probs = gen_decoder(
+        hparams,
+        inputs,
+        targets,
+        targets_present,
+        encoder_states,
+        is_training=is_training,
+        is_validating=is_validating,
+        reuse=reuse)
+    return (stacked_sequence, stacked_logits, stacked_log_probs, initial_state,
+            final_state)
diff --git a/research/maskgan/models/seq2seq_vd.py b/research/maskgan/models/seq2seq_vd.py
new file mode 100644
index 00000000..faee97a4
--- /dev/null
+++ b/research/maskgan/models/seq2seq_vd.py
@@ -0,0 +1,608 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple seq2seq model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from models import attention_utils
+from regularization import variational_dropout
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def transform_input_with_is_missing_token(inputs, targets_present):
+  """Transforms the inputs to have missing tokens when it's masked out.  The
+  mask is for the targets, so therefore, to determine if an input at time t is
+  masked, we have to check if the target at time t - 1 is masked out.
+
+  e.g.
+    inputs = [a, b, c, d]
+    targets = [b, c, d, e]
+    targets_present = [1, 0, 1, 0]
+
+  which computes,
+    inputs_present = [1, 1, 0, 1]
+
+  and outputs,
+    transformed_input = [a, b, <missing>, d]
+
+  Args:
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the word.
+
+  Returns:
+    transformed_input:  tf.int32 Tensor of shape [batch_size, sequence_length]
+      which takes on value of inputs when the input is present and takes on
+      value=vocab_size to indicate a missing token.
+  """
+  # To fill in if the input is missing.
+  input_missing = tf.constant(
+      FLAGS.vocab_size,
+      dtype=tf.int32,
+      shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  # The 0th input will always be present to MaskGAN.
+  zeroth_input_present = tf.constant(True, tf.bool, shape=[FLAGS.batch_size, 1])
+
+  # Input present mask.
+  inputs_present = tf.concat(
+      [zeroth_input_present, targets_present[:, :-1]], axis=1)
+
+  transformed_input = tf.where(inputs_present, inputs, input_missing)
+  return transformed_input
+
+
+# TODO(adai): IMDB labels placeholder to encoder.
+def gen_encoder(hparams, inputs, targets_present, is_training, reuse=None):
+  """Define the Encoder graph.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the target.
+    is_training:  Boolean indicating operational mode (train/inference).
+    reuse (Optional):   Whether to reuse the variables.
+
+  Returns:
+    Tuple of (hidden_states, final_state).
+  """
+  # We will use the same variable from the decoder.
+  if FLAGS.seq2seq_share_embedding:
+    with tf.variable_scope('decoder/rnn'):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('encoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.gen_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.gen_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.gen_rnn_size,
+            hparams.gen_vd_keep_prob, hparams.gen_vd_keep_prob)
+
+    cell = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    initial_state = cell.zero_state(FLAGS.batch_size, tf.float32)
+
+    # Add a missing token for inputs not present.
+    real_inputs = inputs
+    masked_inputs = transform_input_with_is_missing_token(
+        inputs, targets_present)
+
+    with tf.variable_scope('rnn') as scope:
+      hidden_states = []
+
+      # Split the embedding into two parts so that we can load the PTB
+      # weights into one part of the Variable.
+      if not FLAGS.seq2seq_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.gen_rnn_size])
+      missing_embedding = tf.get_variable('missing_embedding',
+                                          [1, hparams.gen_rnn_size])
+      embedding = tf.concat([embedding, missing_embedding], axis=0)
+
+      # TODO(adai): Perhaps append IMDB labels placeholder to input at
+      # each time point.
+      real_rnn_inputs = tf.nn.embedding_lookup(embedding, real_inputs)
+      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)
+
+      state = initial_state
+
+      def make_mask(keep_prob, units):
+        random_tensor = keep_prob
+        # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+        random_tensor += tf.random_uniform(
+            tf.stack([FLAGS.batch_size, 1, units]))
+        return tf.floor(random_tensor) / keep_prob
+
+      if is_training:
+        output_mask = make_mask(hparams.gen_vd_keep_prob, hparams.gen_rnn_size)
+
+      hidden_states, state = tf.nn.dynamic_rnn(
+          cell, masked_rnn_inputs, initial_state=state, scope=scope)
+      if is_training:
+        hidden_states *= output_mask
+
+      final_masked_state = state
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state = initial_state
+      _, real_state = tf.nn.dynamic_rnn(
+          cell, real_rnn_inputs, initial_state=real_state, scope=scope)
+      final_state = real_state
+
+  return (hidden_states, final_masked_state), initial_state, final_state
+
+
+# TODO(adai): IMDB labels placeholder to encoder.
+def gen_encoder_cnn(hparams, inputs, targets_present, is_training, reuse=None):
+  """Define the CNN Encoder graph."""
+  del reuse
+  sequence = transform_input_with_is_missing_token(inputs, targets_present)
+
+  # TODO(liamfedus): Make this a hyperparameter.
+  dis_filter_sizes = [3, 4, 5, 6, 7, 8, 9, 10, 15, 20]
+
+  # Keeping track of l2 regularization loss (optional)
+  # l2_loss = tf.constant(0.0)
+
+  with tf.variable_scope('encoder', reuse=True):
+    with tf.variable_scope('rnn'):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  cnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+  # Create a convolution layer for each filter size
+  conv_outputs = []
+  for filter_size in dis_filter_sizes:
+    with tf.variable_scope('conv-%s' % filter_size):
+      # Convolution Layer
+      filter_shape = [
+          filter_size, hparams.gen_rnn_size, hparams.dis_num_filters
+      ]
+      W = tf.get_variable(
+          name='W', initializer=tf.truncated_normal(filter_shape, stddev=0.1))
+      b = tf.get_variable(
+          name='b',
+          initializer=tf.constant(0.1, shape=[hparams.dis_num_filters]))
+      conv = tf.nn.conv1d(cnn_inputs, W, stride=1, padding='SAME', name='conv')
+
+      # Apply nonlinearity
+      h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')
+
+      conv_outputs.append(h)
+
+  # Combine all the pooled features
+  dis_num_filters_total = hparams.dis_num_filters * len(dis_filter_sizes)
+
+  h_conv = tf.concat(conv_outputs, axis=2)
+  h_conv_flat = tf.reshape(h_conv, [-1, dis_num_filters_total])
+
+  # Add dropout
+  if is_training:
+    with tf.variable_scope('dropout'):
+      h_conv_flat = tf.nn.dropout(h_conv_flat, hparams.gen_vd_keep_prob)
+
+  # Final (unnormalized) scores and predictions
+  with tf.variable_scope('output'):
+    W = tf.get_variable(
+        'W',
+        shape=[dis_num_filters_total, hparams.gen_rnn_size],
+        initializer=tf.contrib.layers.xavier_initializer())
+    b = tf.get_variable(
+        name='b', initializer=tf.constant(0.1, shape=[hparams.gen_rnn_size]))
+    # l2_loss += tf.nn.l2_loss(W)
+    # l2_loss += tf.nn.l2_loss(b)
+    predictions = tf.nn.xw_plus_b(h_conv_flat, W, b, name='predictions')
+    predictions = tf.reshape(
+        predictions,
+        shape=[FLAGS.batch_size, FLAGS.sequence_length, hparams.gen_rnn_size])
+  final_state = tf.reduce_mean(predictions, 1)
+  return predictions, (final_state, final_state)
+
+
+# TODO(adai): IMDB labels placeholder to decoder.
+def gen_decoder(hparams,
+                inputs,
+                targets,
+                targets_present,
+                encoding_state,
+                is_training,
+                is_validating,
+                reuse=None):
+  """Define the Decoder graph. The Decoder will now impute tokens that
+      have been masked from the input seqeunce.
+  """
+  gen_decoder_rnn_size = hparams.gen_rnn_size
+
+  targets = tf.Print(targets, [targets], message='targets', summarize=50)
+  if FLAGS.seq2seq_share_embedding:
+    with tf.variable_scope('decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+
+  with tf.variable_scope('decoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          gen_decoder_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.gen_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.gen_rnn_size,
+            hparams.gen_vd_keep_prob, hparams.gen_vd_keep_prob)
+
+    cell_gen = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    # Hidden encoder states.
+    hidden_vector_encodings = encoding_state[0]
+
+    # Carry forward the final state tuple from the encoder.
+    # State tuples.
+    state_gen = encoding_state[1]
+
+    if FLAGS.attention_option is not None:
+      (attention_keys, attention_values, _,
+       attention_construct_fn) = attention_utils.prepare_attention(
+           hidden_vector_encodings,
+           FLAGS.attention_option,
+           num_units=gen_decoder_rnn_size,
+           reuse=reuse)
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    if is_training:
+      output_mask = make_mask(hparams.gen_vd_keep_prob, hparams.gen_rnn_size)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+
+      if not FLAGS.seq2seq_share_embedding:
+        embedding = tf.get_variable('embedding',
+                                    [FLAGS.vocab_size, hparams.gen_rnn_size])
+      softmax_w = tf.matrix_transpose(embedding)
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+      # TODO(adai): Perhaps append IMDB labels placeholder to input at
+      # each time point.
+
+      rnn_outs = []
+
+      fake = None
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the Decoder.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or FLAGS.gen_training_strategy == 'cross_entropy':
+            rnn_inp = real_rnn_inp
+          else:
+            fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+
+        if FLAGS.attention_option is not None:
+          rnn_out = attention_construct_fn(rnn_out, attention_keys,
+                                           attention_values)
+        if is_training:
+          rnn_out *= output_mask
+
+        rnn_outs.append(rnn_out)
+        if FLAGS.gen_training_strategy != 'cross_entropy':
+          logit = tf.nn.bias_add(tf.matmul(rnn_out, softmax_w), softmax_b)
+
+          # Output for Decoder.
+          # If input is present:   Return real at t+1.
+          # If input is not present:  Return fake for t+1.
+          real = targets[:, t]
+
+          categorical = tf.contrib.distributions.Categorical(logits=logit)
+          if FLAGS.use_gen_mode:
+            fake = categorical.mode()
+          else:
+            fake = categorical.sample()
+          log_prob = categorical.log_prob(fake)
+          output = tf.where(targets_present[:, t], real, fake)
+
+        else:
+          real = targets[:, t]
+          logit = tf.zeros(tf.stack([FLAGS.batch_size, FLAGS.vocab_size]))
+          log_prob = tf.zeros(tf.stack([FLAGS.batch_size]))
+          output = real
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+
+      if FLAGS.gen_training_strategy == 'cross_entropy':
+        logits = tf.nn.bias_add(
+            tf.matmul(
+                tf.reshape(tf.stack(rnn_outs, 1), [-1, gen_decoder_rnn_size]),
+                softmax_w), softmax_b)
+        logits = tf.reshape(logits,
+                            [-1, FLAGS.sequence_length, FLAGS.vocab_size])
+      else:
+        logits = tf.stack(logits, axis=1)
+
+  return (tf.stack(sequence, axis=1), logits, tf.stack(log_probs, axis=1))
+
+
+def dis_encoder(hparams, masked_inputs, is_training, reuse=None,
+                embedding=None):
+  """Define the Discriminator encoder.  Reads in the masked inputs for context
+  and produces the hidden states of the encoder."""
+  with tf.variable_scope('encoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.dis_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,
+            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)
+
+    cell_dis = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    state_dis = cell_dis.zero_state(FLAGS.batch_size, tf.float32)
+
+    with tf.variable_scope('rnn'):
+      hidden_states = []
+
+      missing_embedding = tf.get_variable('missing_embedding',
+                                          [1, hparams.dis_rnn_size])
+      embedding = tf.concat([embedding, missing_embedding], axis=0)
+      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)
+
+      def make_mask(keep_prob, units):
+        random_tensor = keep_prob
+        # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+        random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+        return tf.floor(random_tensor) / keep_prob
+
+      if is_training:
+        output_mask = make_mask(hparams.dis_vd_keep_prob, hparams.dis_rnn_size)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = masked_rnn_inputs[:, t]
+        rnn_out, state_dis = cell_dis(rnn_in, state_dis)
+        if is_training:
+          rnn_out *= output_mask
+        hidden_states.append(rnn_out)
+      final_state = state_dis
+
+  return (tf.stack(hidden_states, axis=1), final_state)
+
+
+def dis_decoder(hparams,
+                sequence,
+                encoding_state,
+                is_training,
+                reuse=None,
+                embedding=None):
+  """Define the Discriminator decoder.  Read in the sequence and predict
+    at each time point."""
+  sequence = tf.cast(sequence, tf.int32)
+
+  with tf.variable_scope('decoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(
+          hparams.dis_rnn_size,
+          forget_bias=0.0,
+          state_is_tuple=True,
+          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and hparams.dis_vd_keep_prob < 1:
+
+      def attn_cell():
+        return variational_dropout.VariationalDropoutWrapper(
+            lstm_cell(), FLAGS.batch_size, hparams.dis_rnn_size,
+            hparams.dis_vd_keep_prob, hparams.dis_vd_keep_prob)
+
+    cell_dis = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.dis_num_layers)],
+        state_is_tuple=True)
+
+    # Hidden encoder states.
+    hidden_vector_encodings = encoding_state[0]
+
+    # Carry forward the final state tuple from the encoder.
+    # State tuples.
+    state = encoding_state[1]
+
+    if FLAGS.attention_option is not None:
+      (attention_keys, attention_values, _,
+       attention_construct_fn) = attention_utils.prepare_attention(
+           hidden_vector_encodings,
+           FLAGS.attention_option,
+           num_units=hparams.dis_rnn_size,
+           reuse=reuse)
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([FLAGS.batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    if is_training:
+      output_mask = make_mask(hparams.dis_vd_keep_prob, hparams.dis_rnn_size)
+
+    with tf.variable_scope('rnn') as vs:
+      predictions = []
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, sequence)
+
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_in = rnn_inputs[:, t]
+        rnn_out, state = cell_dis(rnn_in, state)
+
+        if FLAGS.attention_option is not None:
+          rnn_out = attention_construct_fn(rnn_out, attention_keys,
+                                           attention_values)
+        if is_training:
+          rnn_out *= output_mask
+
+        # Prediction is linear output for Discriminator.
+        pred = tf.contrib.layers.linear(rnn_out, 1, scope=vs)
+        predictions.append(pred)
+
+  predictions = tf.stack(predictions, axis=1)
+  return tf.squeeze(predictions, axis=2)
+
+
+def discriminator(hparams,
+                  inputs,
+                  targets_present,
+                  sequence,
+                  is_training,
+                  reuse=None):
+  """Define the Discriminator graph."""
+  if FLAGS.dis_share_embedding:
+    assert hparams.dis_rnn_size == hparams.gen_rnn_size, (
+        'If you wish to share Discriminator/Generator embeddings, they must be'
+        ' same dimension.')
+    with tf.variable_scope('gen/decoder/rnn', reuse=True):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+  else:
+    # Explicitly share the embedding.
+    with tf.variable_scope('dis/decoder/rnn', reuse=reuse):
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.dis_rnn_size])
+
+  # Mask the input sequence.
+  masked_inputs = transform_input_with_is_missing_token(inputs, targets_present)
+
+  # Confirm masking.
+  masked_inputs = tf.Print(
+      masked_inputs, [inputs, targets_present, masked_inputs, sequence],
+      message='inputs, targets_present, masked_inputs, sequence',
+      summarize=10)
+
+  with tf.variable_scope('dis', reuse=reuse):
+    encoder_states = dis_encoder(
+        hparams,
+        masked_inputs,
+        is_training=is_training,
+        reuse=reuse,
+        embedding=embedding)
+    predictions = dis_decoder(
+        hparams,
+        sequence,
+        encoder_states,
+        is_training=is_training,
+        reuse=reuse,
+        embedding=embedding)
+
+  # if FLAGS.baseline_method == 'critic':
+  #   with tf.variable_scope('critic', reuse=reuse) as critic_scope:
+  #     values = tf.contrib.layers.linear(rnn_outs, 1, scope=critic_scope)
+  #     values = tf.squeeze(values, axis=2)
+  # else:
+  #   values = None
+
+  return predictions
+
+
+# TODO(adai): IMDB labels placeholder to encoder/decoder.
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph."""
+  with tf.variable_scope('gen', reuse=reuse):
+    encoder_states, initial_state, final_state = gen_encoder(
+        hparams, inputs, targets_present, is_training=is_training, reuse=reuse)
+    stacked_sequence, stacked_logits, stacked_log_probs = gen_decoder(
+        hparams,
+        inputs,
+        targets,
+        targets_present,
+        encoder_states,
+        is_training=is_training,
+        is_validating=is_validating,
+        reuse=reuse)
+    return (stacked_sequence, stacked_logits, stacked_log_probs, initial_state,
+            final_state, encoder_states)
diff --git a/research/maskgan/models/seq2seq_zaremba.py b/research/maskgan/models/seq2seq_zaremba.py
new file mode 100644
index 00000000..bf91af8d
--- /dev/null
+++ b/research/maskgan/models/seq2seq_zaremba.py
@@ -0,0 +1,305 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Simple seq2seq model definitions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from models import attention_utils
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def transform_input_with_is_missing_token(inputs, targets_present):
+  """Transforms the inputs to have missing tokens when it's masked out.  The
+  mask is for the targets, so therefore, to determine if an input at time t is
+  masked, we have to check if the target at time t - 1 is masked out.
+
+  e.g.
+    inputs = [a, b, c, d]
+    targets = [b, c, d, e]
+    targets_present = [1, 0, 1, 0]
+
+  then,
+    transformed_input = [a, b, <missing>, d]
+
+  Args:
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the word.
+
+  Returns:
+    transformed_input:  tf.int32 Tensor of shape [batch_size, sequence_length]
+      which takes on value of inputs when the input is present and takes on
+      value=vocab_size to indicate a missing token.
+  """
+  # To fill in if the input is missing.
+  input_missing = tf.constant(FLAGS.vocab_size,
+                              dtype=tf.int32,
+                              shape=[FLAGS.batch_size, FLAGS.sequence_length])
+
+  # The 0th input will always be present to MaskGAN.
+  zeroth_input_present = tf.constant(True, tf.bool, shape=[FLAGS.batch_size, 1])
+
+  # Input present mask.
+  inputs_present = tf.concat(
+      [zeroth_input_present, targets_present[:, :-1]], axis=1)
+
+  transformed_input = tf.where(inputs_present, inputs, input_missing)
+  return transformed_input
+
+
+def gen_encoder(hparams, inputs, targets_present, is_training, reuse=None):
+  """Define the Encoder graph.
+
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    inputs:  tf.int32 Tensor of shape [batch_size, sequence_length] with tokens
+      up to, but not including, vocab_size.
+    targets_present:  tf.bool Tensor of shape [batch_size, sequence_length] with
+      True representing the presence of the target.
+    is_training:  Boolean indicating operational mode (train/inference).
+    reuse (Optional):   Whether to reuse the variables.
+
+  Returns:
+    Tuple of (hidden_states, final_state).
+  """
+  with tf.variable_scope('encoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(hparams.gen_rnn_size,
+                                          forget_bias=0.0,
+                                          state_is_tuple=True,
+                                          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and FLAGS.keep_prob < 1:
+
+      def attn_cell():
+        return tf.contrib.rnn.DropoutWrapper(
+            lstm_cell(), output_keep_prob=FLAGS.keep_prob)
+
+    cell = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    initial_state = cell.zero_state(FLAGS.batch_size, tf.float32)
+
+    # Add a missing token for inputs not present.
+    real_inputs = inputs
+    masked_inputs = transform_input_with_is_missing_token(inputs,
+                                                          targets_present)
+
+    with tf.variable_scope('rnn'):
+      hidden_states = []
+
+      # Split the embedding into two parts so that we can load the PTB
+      # weights into one part of the Variable.
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+      missing_embedding = tf.get_variable('missing_embedding',
+                                          [1, hparams.gen_rnn_size])
+      embedding = tf.concat([embedding, missing_embedding], axis=0)
+
+      real_rnn_inputs = tf.nn.embedding_lookup(embedding, real_inputs)
+      masked_rnn_inputs = tf.nn.embedding_lookup(embedding, masked_inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        masked_rnn_inputs = tf.nn.dropout(masked_rnn_inputs, FLAGS.keep_prob)
+
+      state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        rnn_inp = masked_rnn_inputs[:, t]
+        rnn_out, state = cell(rnn_inp, state)
+        hidden_states.append(rnn_out)
+      final_masked_state = state
+      hidden_states = tf.stack(hidden_states, axis=1)
+
+      # Produce the RNN state had the model operated only
+      # over real data.
+      real_state = initial_state
+      for t in xrange(FLAGS.sequence_length):
+        tf.get_variable_scope().reuse_variables()
+
+        # RNN.
+        rnn_inp = real_rnn_inputs[:, t]
+        rnn_out, real_state = cell(rnn_inp, real_state)
+      final_state = real_state
+
+  return (hidden_states, final_masked_state), initial_state, final_state
+
+
+def gen_decoder(hparams,
+                inputs,
+                targets,
+                targets_present,
+                encoding_state,
+                is_training,
+                is_validating,
+                reuse=None):
+  """Define the Decoder graph. The Decoder will now impute tokens that
+      have been masked from the input seqeunce.
+  """
+  gen_decoder_rnn_size = hparams.gen_rnn_size
+
+  with tf.variable_scope('decoder', reuse=reuse):
+
+    def lstm_cell():
+      return tf.contrib.rnn.BasicLSTMCell(gen_decoder_rnn_size,
+                                          forget_bias=0.0,
+                                          state_is_tuple=True,
+                                          reuse=reuse)
+
+    attn_cell = lstm_cell
+    if is_training and FLAGS.keep_prob < 1:
+
+      def attn_cell():
+        return tf.contrib.rnn.DropoutWrapper(
+            lstm_cell(), output_keep_prob=FLAGS.keep_prob)
+
+    cell_gen = tf.contrib.rnn.MultiRNNCell(
+        [attn_cell() for _ in range(hparams.gen_num_layers)],
+        state_is_tuple=True)
+
+    # Hidden encoder states.
+    hidden_vector_encodings = encoding_state[0]
+
+    # Carry forward the final state tuple from the encoder.
+    # State tuples.
+    state_gen = encoding_state[1]
+
+    if FLAGS.attention_option is not None:
+      (attention_keys, attention_values, _,
+       attention_construct_fn) = attention_utils.prepare_attention(
+           hidden_vector_encodings,
+           FLAGS.attention_option,
+           num_units=gen_decoder_rnn_size,
+           reuse=reuse)
+
+    with tf.variable_scope('rnn'):
+      sequence, logits, log_probs = [], [], []
+
+      embedding = tf.get_variable('embedding',
+                                  [FLAGS.vocab_size, hparams.gen_rnn_size])
+      softmax_w = tf.matrix_transpose(embedding)
+      softmax_b = tf.get_variable('softmax_b', [FLAGS.vocab_size])
+
+      rnn_inputs = tf.nn.embedding_lookup(embedding, inputs)
+
+      if is_training and FLAGS.keep_prob < 1:
+        rnn_inputs = tf.nn.dropout(rnn_inputs, FLAGS.keep_prob)
+
+      rnn_outs = []
+
+      fake = None
+      for t in xrange(FLAGS.sequence_length):
+        if t > 0:
+          tf.get_variable_scope().reuse_variables()
+
+        # Input to the Decoder.
+        if t == 0:
+          # Always provide the real input at t = 0.
+          rnn_inp = rnn_inputs[:, t]
+
+        # If the input is present, read in the input at t.
+        # If the input is not present, read in the previously generated.
+        else:
+          real_rnn_inp = rnn_inputs[:, t]
+
+          # While validating, the decoder should be operating in teacher
+          # forcing regime.  Also, if we're just training with cross_entropy
+          # use teacher forcing.
+          if is_validating or FLAGS.gen_training_strategy == 'cross_entropy':
+            rnn_inp = real_rnn_inp
+          else:
+            fake_rnn_inp = tf.nn.embedding_lookup(embedding, fake)
+            rnn_inp = tf.where(targets_present[:, t - 1], real_rnn_inp,
+                               fake_rnn_inp)
+
+        # RNN.
+        rnn_out, state_gen = cell_gen(rnn_inp, state_gen)
+
+        if FLAGS.attention_option is not None:
+          rnn_out = attention_construct_fn(rnn_out, attention_keys,
+                                           attention_values)
+        rnn_outs.append(rnn_out)
+        if FLAGS.gen_training_strategy != 'cross_entropy':
+          logit = tf.nn.bias_add(tf.matmul(rnn_out, softmax_w), softmax_b)
+
+          # Output for Decoder.
+          # If input is present:   Return real at t+1.
+          # If input is not present:  Return fake for t+1.
+          real = targets[:, t]
+
+          categorical = tf.contrib.distributions.Categorical(logits=logit)
+          fake = categorical.sample()
+          log_prob = categorical.log_prob(fake)
+
+          output = tf.where(targets_present[:, t], real, fake)
+
+        else:
+          batch_size = tf.shape(rnn_out)[0]
+          logit = tf.zeros(tf.stack([batch_size, FLAGS.vocab_size]))
+          log_prob = tf.zeros(tf.stack([batch_size]))
+          output = targets[:, t]
+
+        # Add to lists.
+        sequence.append(output)
+        log_probs.append(log_prob)
+        logits.append(logit)
+      if FLAGS.gen_training_strategy == 'cross_entropy':
+        logits = tf.nn.bias_add(
+            tf.matmul(
+                tf.reshape(tf.stack(rnn_outs, 1), [-1, gen_decoder_rnn_size]),
+                softmax_w), softmax_b)
+        logits = tf.reshape(logits,
+                            [-1, FLAGS.sequence_length, FLAGS.vocab_size])
+      else:
+        logits = tf.stack(logits, axis=1)
+
+  return (tf.stack(sequence, axis=1), logits, tf.stack(log_probs, axis=1))
+
+
+def generator(hparams,
+              inputs,
+              targets,
+              targets_present,
+              is_training,
+              is_validating,
+              reuse=None):
+  """Define the Generator graph."""
+  with tf.variable_scope('gen', reuse=reuse):
+    encoder_states, initial_state, final_state = gen_encoder(
+        hparams, inputs, targets_present, is_training=is_training, reuse=reuse)
+    stacked_sequence, stacked_logits, stacked_log_probs = gen_decoder(
+        hparams,
+        inputs,
+        targets,
+        targets_present,
+        encoder_states,
+        is_training=is_training,
+        is_validating=is_validating,
+        reuse=reuse)
+    return (stacked_sequence, stacked_logits, stacked_log_probs, initial_state,
+            final_state)
diff --git a/research/maskgan/nas_utils/__init__.py b/research/maskgan/nas_utils/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/nas_utils/configs.py b/research/maskgan/nas_utils/configs.py
new file mode 100644
index 00000000..80d867c3
--- /dev/null
+++ b/research/maskgan/nas_utils/configs.py
@@ -0,0 +1,46 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+
+def print_config(config):
+  print("-" * 10, "Configuration Specs", "-" * 10)
+  for item in dir(config):
+    if list(item)[0] != "_":
+      print(item, getattr(config, item))
+  print("-" * 29)
+
+
+class AlienConfig2(object):
+  """Base 8 740 shared embeddings, gets 64.0 (mean: std: min: max: )."""
+  init_scale = 0.05
+  learning_rate = 1.0
+  max_grad_norm = 10
+  num_layers = 2
+  num_steps = 25
+  hidden_size = 740
+  max_epoch = 70
+  max_max_epoch = 250
+  keep_prob = [1 - 0.15, 1 - 0.45]
+  lr_decay = 0.95
+  batch_size = 20
+  vocab_size = 10000
+  weight_decay = 1e-4
+  share_embeddings = True
+  cell = "alien"
+  dropout_type = "variational"
diff --git a/research/maskgan/nas_utils/custom_cell.py b/research/maskgan/nas_utils/custom_cell.py
new file mode 100644
index 00000000..6add7ffa
--- /dev/null
+++ b/research/maskgan/nas_utils/custom_cell.py
@@ -0,0 +1,166 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+import numpy as np
+import tensorflow as tf
+
+flags = tf.flags
+FLAGS = tf.app.flags.FLAGS
+LSTMTuple = collections.namedtuple('LSTMTuple', ['c', 'h'])
+
+
+def cell_depth(num):
+  num /= 2
+  val = np.log2(1 + num)
+  assert abs(val - int(val)) == 0
+  return int(val)
+
+
+class GenericMultiRNNCell(tf.contrib.rnn.RNNCell):
+  """More generic version of MultiRNNCell that allows you to pass in a dropout mask"""
+
+  def __init__(self, cells):
+    """Create a RNN cell composed sequentially of a number of RNNCells.
+
+    Args:
+      cells: list of RNNCells that will be composed in this order.
+      state_is_tuple: If True, accepted and returned states are n-tuples, where
+        `n = len(cells)`.  If False, the states are all
+        concatenated along the column axis.  This latter behavior will soon be
+        deprecated.
+
+    Raises:
+      ValueError: if cells is empty (not allowed), or at least one of the cells
+        returns a state tuple but the flag `state_is_tuple` is `False`.
+    """
+    self._cells = cells
+
+  @property
+  def state_size(self):
+    return tuple(cell.state_size for cell in self._cells)
+
+  @property
+  def output_size(self):
+    return self._cells[-1].output_size
+
+  def __call__(self, inputs, state, input_masks=None, scope=None):
+    """Run this multi-layer cell on inputs, starting from state."""
+    with tf.variable_scope(scope or type(self).__name__):
+      cur_inp = inputs
+      new_states = []
+      for i, cell in enumerate(self._cells):
+        with tf.variable_scope('Cell%d' % i):
+          cur_state = state[i]
+          if input_masks is not None:
+            cur_inp *= input_masks[i]
+          cur_inp, new_state = cell(cur_inp, cur_state)
+          new_states.append(new_state)
+    new_states = tuple(new_states)
+    return cur_inp, new_states
+
+
+class AlienRNNBuilder(tf.contrib.rnn.RNNCell):
+
+  def __init__(self, num_units, params, additional_params, base_size):
+    self.num_units = num_units
+    self.cell_create_index = additional_params[0]
+    self.cell_inject_index = additional_params[1]
+    self.base_size = base_size
+    self.cell_params = params[
+        -2:]  # Cell injection parameters are always the last two
+    params = params[:-2]
+    self.depth = cell_depth(len(params))
+    self.params = params
+    self.units_per_layer = [2**i for i in range(self.depth)
+                           ][::-1]  # start with the biggest layer
+
+  def __call__(self, inputs, state, scope=None):
+    with tf.variable_scope(scope or type(self).__name__):
+      definition1 = ['add', 'elem_mult', 'max']
+      definition2 = [tf.identity, tf.tanh, tf.sigmoid, tf.nn.relu, tf.sin]
+      layer_outputs = [[] for _ in range(self.depth)]
+      with tf.variable_scope('rnn_builder'):
+        curr_index = 0
+        c, h = state
+
+        # Run all dense matrix multiplications at once
+        big_h_mat = tf.get_variable(
+            'big_h_mat', [self.num_units,
+                          self.base_size * self.num_units], tf.float32)
+        big_inputs_mat = tf.get_variable(
+            'big_inputs_mat', [self.num_units,
+                               self.base_size * self.num_units], tf.float32)
+        big_h_output = tf.matmul(h, big_h_mat)
+        big_inputs_output = tf.matmul(inputs, big_inputs_mat)
+        h_splits = tf.split(big_h_output, self.base_size, axis=1)
+        inputs_splits = tf.split(big_inputs_output, self.base_size, axis=1)
+
+        for layer_num, units in enumerate(self.units_per_layer):
+          for unit_num in range(units):
+            with tf.variable_scope(
+                'layer_{}_unit_{}'.format(layer_num, unit_num)):
+              if layer_num == 0:
+                prev1_mat = h_splits[unit_num]
+                prev2_mat = inputs_splits[unit_num]
+              else:
+                prev1_mat = layer_outputs[layer_num - 1][2 * unit_num]
+                prev2_mat = layer_outputs[layer_num - 1][2 * unit_num + 1]
+              if definition1[self.params[curr_index]] == 'add':
+                output = prev1_mat + prev2_mat
+              elif definition1[self.params[curr_index]] == 'elem_mult':
+                output = prev1_mat * prev2_mat
+              elif definition1[self.params[curr_index]] == 'max':
+                output = tf.maximum(prev1_mat, prev2_mat)
+              if curr_index / 2 == self.cell_create_index:  # Take the new cell before the activation
+                new_c = tf.identity(output)
+              output = definition2[self.params[curr_index + 1]](output)
+              if curr_index / 2 == self.cell_inject_index:
+                if definition1[self.cell_params[0]] == 'add':
+                  output += c
+                elif definition1[self.cell_params[0]] == 'elem_mult':
+                  output *= c
+                elif definition1[self.cell_params[0]] == 'max':
+                  output = tf.maximum(output, c)
+                output = definition2[self.cell_params[1]](output)
+              layer_outputs[layer_num].append(output)
+              curr_index += 2
+        new_h = layer_outputs[-1][-1]
+        return new_h, LSTMTuple(new_c, new_h)
+
+  @property
+  def state_size(self):
+    return LSTMTuple(self.num_units, self.num_units)
+
+  @property
+  def output_size(self):
+    return self.num_units
+
+
+class Alien(AlienRNNBuilder):
+  """Base 8 Cell."""
+
+  def __init__(self, num_units):
+    params = [
+        0, 2, 0, 3, 0, 2, 1, 3, 0, 1, 0, 2, 0, 1, 0, 2, 1, 1, 0, 1, 1, 1, 0, 2,
+        1, 0, 0, 1, 1, 1, 0, 1
+    ]
+    additional_params = [12, 8]
+    base_size = 8
+    super(Alien, self).__init__(num_units, params, additional_params, base_size)
diff --git a/research/maskgan/nas_utils/variational_dropout.py b/research/maskgan/nas_utils/variational_dropout.py
new file mode 100644
index 00000000..49cc29f0
--- /dev/null
+++ b/research/maskgan/nas_utils/variational_dropout.py
@@ -0,0 +1,61 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Variational Dropout."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def generate_dropout_masks(keep_prob, shape, amount):
+  masks = []
+  for _ in range(amount):
+    dropout_mask = tf.random_uniform(shape) + (keep_prob)
+    dropout_mask = tf.floor(dropout_mask) / (keep_prob)
+    masks.append(dropout_mask)
+  return masks
+
+
+def generate_variational_dropout_masks(hparams, keep_prob):
+  [batch_size, num_steps, size, num_layers] = [
+      FLAGS.batch_size, FLAGS.sequence_length, hparams.gen_rnn_size,
+      hparams.gen_num_layers
+  ]
+  if len(keep_prob) == 2:
+    emb_keep_prob = keep_prob[0]  # keep prob for embedding matrix
+    h2h_keep_prob = emb_keep_prob  # keep prob for hidden to hidden connections
+    h2i_keep_prob = keep_prob[1]  # keep prob for hidden to input connections
+    out_keep_prob = h2i_keep_prob  # keep probability for output state
+  else:
+    emb_keep_prob = keep_prob[0]  # keep prob for embedding matrix
+    h2h_keep_prob = keep_prob[1]  # keep prob for hidden to hidden connections
+    h2i_keep_prob = keep_prob[2]  # keep prob for hidden to input connections
+    out_keep_prob = keep_prob[3]  # keep probability for output state
+  h2i_masks = []  # Masks for input to recurrent connections
+  h2h_masks = []  # Masks for recurrent to recurrent connections
+
+  # Input word dropout mask
+  emb_masks = generate_dropout_masks(emb_keep_prob, [num_steps, 1], batch_size)
+  output_mask = generate_dropout_masks(out_keep_prob, [batch_size, size], 1)[0]
+  h2i_masks = generate_dropout_masks(h2i_keep_prob, [batch_size, size],
+                                     num_layers)
+  h2h_masks = generate_dropout_masks(h2h_keep_prob, [batch_size, size],
+                                     num_layers)
+  return h2h_masks, h2i_masks, emb_masks, output_mask
diff --git a/research/maskgan/pretrain_mask_gan.py b/research/maskgan/pretrain_mask_gan.py
new file mode 100644
index 00000000..1a9d8ee9
--- /dev/null
+++ b/research/maskgan/pretrain_mask_gan.py
@@ -0,0 +1,231 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Pretraining functions."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+# Dependency imports
+
+import numpy as np
+
+import tensorflow as tf
+
+from data import imdb_loader
+from data import ptb_loader
+
+# Data.
+from model_utils import model_utils
+from models import evaluation_utils
+
+tf.app.flags.DEFINE_integer(
+    'gen_pretrain_steps', None,
+    'The number of steps to pretrain the generator with cross entropy loss.')
+tf.app.flags.DEFINE_integer(
+    'dis_pretrain_steps', None,
+    'The number of steps to pretrain the discriminator.')
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def pretrain_generator(sv, sess, model, data, log, id_to_word,
+                       data_ngram_counts, is_chief):
+  """Pretrain the generator with classic language modeling training."""
+  print('\nPretraining generator for %d steps.' % FLAGS.gen_pretrain_steps)
+  log.write(
+      '\nPretraining generator for %d steps.\n' % FLAGS.gen_pretrain_steps)
+
+  is_pretraining = True
+
+  while is_pretraining:
+
+    costs = 0.
+    iters = 0
+    if FLAGS.data_set == 'ptb':
+      iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,
+                                         FLAGS.sequence_length,
+                                         FLAGS.epoch_size_override)
+    elif FLAGS.data_set == 'imdb':
+      iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,
+                                           FLAGS.sequence_length)
+
+    for x, y, _ in iterator:
+
+      # For pretraining with cross entropy loss, we have all tokens in the
+      # forward sequence present (all True).
+      model_utils.assign_percent_real(sess, model.percent_real_update,
+                                      model.new_rate, 1.0)
+      p = np.ones(shape=[FLAGS.batch_size, FLAGS.sequence_length], dtype=bool)
+
+      pretrain_feed = {model.inputs: x, model.targets: y, model.present: p}
+
+      [losses, cost_eval, _, step] = sess.run(
+          [
+              model.fake_cross_entropy_losses, model.avg_log_perplexity,
+              model.gen_pretrain_op, model.global_step
+          ],
+          feed_dict=pretrain_feed)
+
+      costs += cost_eval
+      iters += FLAGS.sequence_length
+
+      # Calulate rolling perplexity.
+      perplexity = np.exp(costs / iters)
+
+      # Summaries.
+      if is_chief and step % FLAGS.summaries_every == 0:
+        # Graph summaries.
+        summary_str = sess.run(
+            model.merge_summaries_op, feed_dict=pretrain_feed)
+        sv.SummaryComputed(sess, summary_str)
+
+        # Additional summary.
+        for n, data_ngram_count in data_ngram_counts.iteritems():
+          avg_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+              sess, model.fake_sequence, log, pretrain_feed, data_ngram_count,
+              int(n))
+          summary_percent_str = tf.Summary(value=[
+              tf.Summary.Value(
+                  tag='general/%s-grams_percent_correct' % n,
+                  simple_value=avg_percent_captured)
+          ])
+          sv.SummaryComputed(sess, summary_percent_str, global_step=step)
+
+        summary_perplexity_str = tf.Summary(value=[
+            tf.Summary.Value(tag='general/perplexity', simple_value=perplexity)
+        ])
+        sv.SummaryComputed(sess, summary_perplexity_str, global_step=step)
+
+      # Printing and logging
+      if is_chief and step % FLAGS.print_every == 0:
+        print('global_step: %d' % step)
+        print(' generator loss: %.3f' % np.mean(losses))
+        print(' perplexity: %.3f' % perplexity)
+        log.write('global_step: %d\n' % step)
+        log.write(' generator loss: %.3f\n' % np.mean(losses))
+        log.write(' perplexity: %.3f\n' % perplexity)
+
+        for n, data_ngram_count in data_ngram_counts.iteritems():
+          avg_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+              sess, model.fake_sequence, log, pretrain_feed, data_ngram_count,
+              int(n))
+          print(' percent of %s-grams captured: %.3f.\n' %
+                (n, avg_percent_captured))
+          log.write(' percent of %s-grams captured: %.3f.\n\n' %
+                    (n, avg_percent_captured))
+
+        evaluation_utils.generate_logs(sess, model, log, id_to_word,
+                                       pretrain_feed)
+
+      if step >= FLAGS.gen_pretrain_steps:
+        is_pretraining = False
+        break
+  return
+
+
+def pretrain_discriminator(sv, sess, model, data, log, id_to_word,
+                           data_ngram_counts, is_chief):
+  print('\nPretraining discriminator for %d steps.' % FLAGS.dis_pretrain_steps)
+  log.write(
+      '\nPretraining discriminator for %d steps.\n' % FLAGS.dis_pretrain_steps)
+
+  is_pretraining = True
+
+  while is_pretraining:
+
+    cumulative_costs = 0.
+    iters = 0
+    if FLAGS.data_set == 'ptb':
+      iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,
+                                         FLAGS.sequence_length,
+                                         FLAGS.epoch_size_override)
+    elif FLAGS.data_set == 'imdb':
+      iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,
+                                           FLAGS.sequence_length)
+
+    for x, y, _ in iterator:
+      is_present_rate = FLAGS.is_present_rate
+      # is_present_rate = np.random.uniform(low=0.0, high=1.0)
+      model_utils.assign_percent_real(sess, model.percent_real_update,
+                                      model.new_rate, is_present_rate)
+      # Randomly mask out tokens.
+      p = model_utils.generate_mask()
+
+      pretrain_feed = {model.inputs: x, model.targets: y, model.present: p}
+
+      [_, dis_loss_eval, gen_log_perplexity_eval, step] = sess.run(
+          [
+              model.dis_pretrain_op, model.dis_loss, model.avg_log_perplexity,
+              model.global_step
+          ],
+          feed_dict=pretrain_feed)
+
+      cumulative_costs += gen_log_perplexity_eval
+      iters += 1
+
+      # Calulate rolling perplexity.
+      perplexity = np.exp(cumulative_costs / iters)
+
+      # Summaries.
+      if is_chief and step % FLAGS.summaries_every == 0:
+        # Graph summaries.
+        summary_str = sess.run(
+            model.merge_summaries_op, feed_dict=pretrain_feed)
+        sv.SummaryComputed(sess, summary_str)
+
+        # Additional summary.
+        for n, data_ngram_count in data_ngram_counts.iteritems():
+          avg_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+              sess, model.fake_sequence, log, pretrain_feed, data_ngram_count,
+              int(n))
+          summary_percent_str = tf.Summary(value=[
+              tf.Summary.Value(
+                  tag='general/%s-grams_percent_correct' % n,
+                  simple_value=avg_percent_captured)
+          ])
+          sv.SummaryComputed(sess, summary_percent_str, global_step=step)
+
+        summary_perplexity_str = tf.Summary(value=[
+            tf.Summary.Value(tag='general/perplexity', simple_value=perplexity)
+        ])
+        sv.SummaryComputed(sess, summary_perplexity_str, global_step=step)
+
+      # Printing and logging
+      if is_chief and step % FLAGS.print_every == 0:
+        print('global_step: %d' % step)
+        print(' discriminator loss: %.3f' % dis_loss_eval)
+        print(' perplexity: %.3f' % perplexity)
+        log.write('global_step: %d\n' % step)
+        log.write(' discriminator loss: %.3f\n' % dis_loss_eval)
+        log.write(' perplexity: %.3f\n' % perplexity)
+
+        for n, data_ngram_count in data_ngram_counts.iteritems():
+          avg_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+              sess, model.fake_sequence, log, pretrain_feed, data_ngram_count,
+              int(n))
+          print(' percent of %s-grams captured: %.3f.\n' %
+                (n, avg_percent_captured))
+          log.write(' percent of %s-grams captured: %.3f.\n\n' %
+                    (n, avg_percent_captured))
+
+        evaluation_utils.generate_logs(sess, model, log, id_to_word,
+                                       pretrain_feed)
+
+      if step >= FLAGS.dis_pretrain_steps + int(FLAGS.gen_pretrain_steps or 0):
+        is_pretraining = False
+        break
+  return
diff --git a/research/maskgan/regularization/__init__.py b/research/maskgan/regularization/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/maskgan/regularization/variational_dropout.py b/research/maskgan/regularization/variational_dropout.py
new file mode 100644
index 00000000..d67fe52e
--- /dev/null
+++ b/research/maskgan/regularization/variational_dropout.py
@@ -0,0 +1,56 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Variational Dropout Wrapper."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+class VariationalDropoutWrapper(tf.contrib.rnn.RNNCell):
+  """Add variational dropout to a RNN cell."""
+
+  def __init__(self, cell, batch_size, input_size, recurrent_keep_prob,
+               input_keep_prob):
+    self._cell = cell
+    self._recurrent_keep_prob = recurrent_keep_prob
+    self._input_keep_prob = input_keep_prob
+
+    def make_mask(keep_prob, units):
+      random_tensor = keep_prob
+      # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)
+      random_tensor += tf.random_uniform(tf.stack([batch_size, units]))
+      return tf.floor(random_tensor) / keep_prob
+
+    self._recurrent_mask = make_mask(recurrent_keep_prob,
+                                     self._cell.state_size[0])
+    self._input_mask = self._recurrent_mask
+
+  @property
+  def state_size(self):
+    return self._cell.state_size
+
+  @property
+  def output_size(self):
+    return self._cell.output_size
+
+  def __call__(self, inputs, state, scope=None):
+    dropped_inputs = inputs * self._input_mask
+    dropped_state = (state[0], state[1] * self._recurrent_mask)
+    new_h, new_state = self._cell(dropped_inputs, dropped_state, scope)
+    return new_h, new_state
diff --git a/research/maskgan/regularization/zoneout.py b/research/maskgan/regularization/zoneout.py
new file mode 100644
index 00000000..5f9ef3e3
--- /dev/null
+++ b/research/maskgan/regularization/zoneout.py
@@ -0,0 +1,64 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Zoneout Wrapper"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+class ZoneoutWrapper(tf.contrib.rnn.RNNCell):
+  """Add Zoneout to a RNN cell."""
+
+  def __init__(self, cell, zoneout_drop_prob, is_training=True):
+    self._cell = cell
+    self._zoneout_prob = zoneout_drop_prob
+    self._is_training = is_training
+
+  @property
+  def state_size(self):
+    return self._cell.state_size
+
+  @property
+  def output_size(self):
+    return self._cell.output_size
+
+  def __call__(self, inputs, state, scope=None):
+    output, new_state = self._cell(inputs, state, scope)
+    if not isinstance(self._cell.state_size, tuple):
+      new_state = tf.split(value=new_state, num_or_size_splits=2, axis=1)
+      state = tf.split(value=state, num_or_size_splits=2, axis=1)
+    final_new_state = [new_state[0], new_state[1]]
+    if self._is_training:
+      for i, state_element in enumerate(state):
+        random_tensor = 1 - self._zoneout_prob  # keep probability
+        random_tensor += tf.random_uniform(tf.shape(state_element))
+        # 0. if [zoneout_prob, 1.0) and 1. if [1.0, 1.0 + zoneout_prob)
+        binary_tensor = tf.floor(random_tensor)
+        final_new_state[
+            i] = (new_state[i] - state_element) * binary_tensor + state_element
+    else:
+      for i, state_element in enumerate(state):
+        final_new_state[
+            i] = state_element * self._zoneout_prob + new_state[i] * (
+                1 - self._zoneout_prob)
+    if isinstance(self._cell.state_size, tuple):
+      return output, tf.contrib.rnn.LSTMStateTuple(
+          final_new_state[0], final_new_state[1])
+
+    return output, tf.concat([final_new_state[0], final_new_state[1]], 1)
diff --git a/research/maskgan/sample_shuffler.py b/research/maskgan/sample_shuffler.py
new file mode 100644
index 00000000..58c31fb5
--- /dev/null
+++ b/research/maskgan/sample_shuffler.py
@@ -0,0 +1,95 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Shuffle samples for human evaluation.
+
+Local launch command:
+  python sample_shuffler.py
+  --input_ml_path=/tmp/ptb/seq2seq_vd_shareemb_forreal_55_3
+  --input_gan_path=/tmp/ptb/MaskGAN_PTB_ari_avg_56.29_v2.0.0
+  --output_file_name=/tmp/ptb/shuffled_output.txt
+
+  python sample_shuffler.py
+  --input_ml_path=/tmp/generate_samples/MaskGAN_IMDB_Benchmark_87.1_v0.3.0
+  --input_gan_path=/tmp/generate_samples/MaskGAN_IMDB_v1.0.1
+  --output_file_name=/tmp/imdb/shuffled_output.txt
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+# Dependency imports
+import numpy as np
+
+import tensorflow as tf
+
+tf.app.flags.DEFINE_string('input_ml_path', '/tmp', 'Model output directory.')
+tf.app.flags.DEFINE_string('input_gan_path', '/tmp', 'Model output directory.')
+tf.app.flags.DEFINE_string('output_file_name', '/tmp/ptb/shuffled_output.txt',
+                           'Model output file.')
+tf.app.flags.DEFINE_boolean(
+    'output_masked_logs', False,
+    'Whether to display for human evaluation (show masking).')
+tf.app.flags.DEFINE_integer('number_epochs', 1,
+                            'The number of epochs to produce.')
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def shuffle_samples(input_file_1, input_file_2):
+  """Shuffle the examples."""
+  shuffled = []
+
+  # Set a random seed to keep fixed mask.
+  np.random.seed(0)
+
+  for line_1, line_2 in zip(input_file_1, input_file_2):
+    rand = np.random.randint(1, 3)
+    if rand == 1:
+      shuffled.append((rand, line_1, line_2))
+    else:
+      shuffled.append((rand, line_2, line_1))
+  input_file_1.close()
+  input_file_2.close()
+  return shuffled
+
+
+def generate_output(shuffled_tuples, output_file_name):
+  output_file = tf.gfile.GFile(output_file_name, mode='w')
+
+  for tup in shuffled_tuples:
+    formatted_tuple = ('\n{:<1}, {:<1}, {:<1}').format(tup[0], tup[1].rstrip(),
+                                                       tup[2].rstrip())
+    output_file.write(formatted_tuple)
+  output_file.close()
+
+
+def main(_):
+  ml_samples_file = tf.gfile.GFile(
+      os.path.join(FLAGS.input_ml_path, 'reviews.txt'), mode='r')
+  gan_samples_file = tf.gfile.GFile(
+      os.path.join(FLAGS.input_gan_path, 'reviews.txt'), mode='r')
+
+  # Generate shuffled tuples.
+  shuffled_tuples = shuffle_samples(ml_samples_file, gan_samples_file)
+
+  # Output to file.
+  generate_output(shuffled_tuples, FLAGS.output_file_name)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/maskgan/train_mask_gan.py b/research/maskgan/train_mask_gan.py
new file mode 100644
index 00000000..d6d18c83
--- /dev/null
+++ b/research/maskgan/train_mask_gan.py
@@ -0,0 +1,1168 @@
+# Copyright 2017 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Launch example:
+
+[IMDB]
+python train_mask_gan.py --data_dir
+/tmp/imdb  --data_set imdb  --batch_size 128
+--sequence_length 20  --base_directory /tmp/maskGAN_v0.01
+--hparams="gen_rnn_size=650,gen_num_layers=2,dis_rnn_size=650,dis_num_layers=2
+,critic_learning_rate=0.0009756,dis_learning_rate=0.0000585,
+dis_train_iterations=8,gen_learning_rate=0.0016624,
+gen_full_learning_rate_steps=1e9,gen_learning_rate_decay=0.999999,
+rl_discount_rate=0.8835659"  --mode TRAIN  --max_steps 1000000
+--generator_model seq2seq_vd  --discriminator_model seq2seq_vd
+--is_present_rate 0.5  --summaries_every 25  --print_every 25
+ --max_num_to_print=3 --generator_optimizer=adam
+ --seq2seq_share_embedding=True --baseline_method=critic
+ --attention_option=luong --n_gram_eval=4 --mask_strategy=contiguous
+ --gen_training_strategy=reinforce --dis_pretrain_steps=100
+ --perplexity_threshold=1000000
+ --dis_share_embedding=True  --maskgan_ckpt
+ /tmp/model.ckpt-171091
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import collections
+
+from functools import partial
+import os
+import time
+# Dependency imports
+
+import numpy as np
+
+import tensorflow as tf
+
+import pretrain_mask_gan
+from data import imdb_loader
+from data import ptb_loader
+from model_utils import helper
+from model_utils import model_construction
+from model_utils import model_losses
+from model_utils import model_optimization
+
+# Data.
+from model_utils import model_utils
+
+from model_utils import n_gram
+from models import evaluation_utils
+
+from models import rollout
+
+np.set_printoptions(precision=3)
+np.set_printoptions(suppress=True)
+
+MODE_TRAIN = 'TRAIN'
+MODE_TRAIN_EVAL = 'TRAIN_EVAL'
+MODE_VALIDATION = 'VALIDATION'
+MODE_TEST = 'TEST'
+
+## Binary and setup FLAGS.
+tf.app.flags.DEFINE_enum(
+    'mode', 'TRAIN', [MODE_TRAIN, MODE_VALIDATION, MODE_TEST, MODE_TRAIN_EVAL],
+    'What this binary will do.')
+tf.app.flags.DEFINE_string('master', 'local',
+                           """Name of the TensorFlow master to use.""")
+tf.app.flags.DEFINE_string('eval_master', 'local',
+                           """Name prefix of the Tensorflow eval master,
+                    or "local".""")
+tf.app.flags.DEFINE_integer('task', 0,
+                            """Task id of the replica running the training.""")
+tf.app.flags.DEFINE_integer('ps_tasks', 0, """Number of tasks in the ps job.
+                            If 0 no ps job is used.""")
+
+## General FLAGS.
+tf.app.flags.DEFINE_string(
+    'hparams', '', 'Comma separated list of name=value hyperparameter pairs.')
+tf.app.flags.DEFINE_integer('batch_size', 20, 'The batch size.')
+tf.app.flags.DEFINE_integer('vocab_size', 10000, 'The vocabulary size.')
+tf.app.flags.DEFINE_integer('sequence_length', 20, 'The sequence length.')
+tf.app.flags.DEFINE_integer('max_steps', 1000000,
+                            'Maximum number of steps to run.')
+tf.app.flags.DEFINE_string(
+    'mask_strategy', 'random', 'Strategy for masking the words.  Determine the '
+    'characterisitics of how the words are dropped out.  One of '
+    "['contiguous', 'random'].")
+tf.app.flags.DEFINE_float('is_present_rate', 0.5,
+                          'Percent of tokens present in the forward sequence.')
+tf.app.flags.DEFINE_float('is_present_rate_decay', None, 'Decay rate for the '
+                          'percent of words that are real (are present).')
+tf.app.flags.DEFINE_string(
+    'generator_model', 'seq2seq',
+    "Type of Generator model.  One of ['rnn', 'seq2seq', 'seq2seq_zaremba',"
+    "'rnn_zaremba', 'rnn_nas', 'seq2seq_nas']")
+tf.app.flags.DEFINE_string(
+    'attention_option', None,
+    "Attention mechanism.  One of [None, 'luong', 'bahdanau']")
+tf.app.flags.DEFINE_string(
+    'discriminator_model', 'bidirectional',
+    "Type of Discriminator model.  One of ['cnn', 'rnn', 'bidirectional', "
+    "'rnn_zaremba', 'bidirectional_zaremba', 'rnn_nas', 'rnn_vd', 'seq2seq_vd']"
+)
+tf.app.flags.DEFINE_boolean('seq2seq_share_embedding', False,
+                            'Whether to share the '
+                            'embeddings between the encoder and decoder.')
+tf.app.flags.DEFINE_boolean(
+    'dis_share_embedding', False, 'Whether to share the '
+    'embeddings between the generator and discriminator.')
+tf.app.flags.DEFINE_boolean('dis_update_share_embedding', False, 'Whether the '
+                            'discriminator should update the shared embedding.')
+tf.app.flags.DEFINE_boolean('use_gen_mode', False,
+                            'Use the mode of the generator '
+                            'to produce samples.')
+tf.app.flags.DEFINE_boolean('critic_update_dis_vars', False,
+                            'Whether the critic '
+                            'updates the discriminator variables.')
+
+## Training FLAGS.
+tf.app.flags.DEFINE_string(
+    'gen_training_strategy', 'reinforce',
+    "Method for training the Generator. One of ['cross_entropy', 'reinforce']")
+tf.app.flags.DEFINE_string(
+    'generator_optimizer', 'adam',
+    "Type of Generator optimizer.  One of ['sgd', 'adam']")
+tf.app.flags.DEFINE_float('grad_clipping', 10., 'Norm for gradient clipping.')
+tf.app.flags.DEFINE_float('advantage_clipping', 5., 'Clipping for advantages.')
+tf.app.flags.DEFINE_string(
+    'baseline_method', None,
+    "Approach for baseline.  One of ['critic', 'dis_batch', 'ema', None]")
+tf.app.flags.DEFINE_float('perplexity_threshold', 15000,
+                          'Limit for perplexity before terminating job.')
+tf.app.flags.DEFINE_float('zoneout_drop_prob', 0.1,
+                          'Probability for dropping parameter for zoneout.')
+tf.app.flags.DEFINE_float('keep_prob', 0.5,
+                          'Probability for keeping parameter for dropout.')
+
+## Logging and evaluation FLAGS.
+tf.app.flags.DEFINE_integer('print_every', 250,
+                            'Frequency to print and log the '
+                            'outputs of the model.')
+tf.app.flags.DEFINE_integer('max_num_to_print', 5,
+                            'Number of samples to log/print.')
+tf.app.flags.DEFINE_boolean('print_verbose', False, 'Whether to print in full.')
+tf.app.flags.DEFINE_integer('summaries_every', 100,
+                            'Frequency to compute summaries.')
+tf.app.flags.DEFINE_boolean('eval_language_model', False,
+                            'Whether to evaluate on '
+                            'all words as in language modeling.')
+tf.app.flags.DEFINE_float('eval_interval_secs', 60,
+                          'Delay for evaluating model.')
+tf.app.flags.DEFINE_integer(
+    'n_gram_eval', 4, """The degree of the n-grams to use for evaluation.""")
+tf.app.flags.DEFINE_integer(
+    'epoch_size_override', None,
+    'If an integer, this dictates the size of the epochs and will potentially '
+    'not iterate over all the data.')
+tf.app.flags.DEFINE_integer('eval_epoch_size_override', None,
+                            'Number of evaluation steps.')
+
+## Directories and checkpoints.
+tf.app.flags.DEFINE_string('base_directory', '/tmp/maskGAN_v0.00',
+                           'Base directory for the logging, events and graph.')
+tf.app.flags.DEFINE_string('data_set', 'ptb', 'Data set to operate on.  One of'
+                           "['ptb', 'imdb']")
+tf.app.flags.DEFINE_string('data_dir', '/tmp/data/ptb',
+                           'Directory for the training data.')
+tf.app.flags.DEFINE_string(
+    'language_model_ckpt_dir', None,
+    'Directory storing checkpoints to initialize the model.  Pretrained models'
+    'are stored at /tmp/maskGAN/pretrained/')
+tf.app.flags.DEFINE_string(
+    'language_model_ckpt_dir_reversed', None,
+    'Directory storing checkpoints of reversed models to initialize the model.'
+    'Pretrained models stored at'
+    'are stored at  /tmp/PTB/pretrained_reversed')
+tf.app.flags.DEFINE_string(
+    'maskgan_ckpt', None,
+    'Override which checkpoint file to use to restore the '
+    'model.  A pretrained seq2seq_zaremba model is stored at '
+    '/tmp/maskGAN/pretrain/seq2seq_zaremba/train/model.ckpt-64912')
+
+tf.app.flags.DEFINE_boolean('wasserstein_objective', False,
+                            '(DEPRECATED) Whether to use the WGAN training.')
+tf.app.flags.DEFINE_integer('num_rollouts', 1,
+                            'The number of rolled out predictions to make.')
+tf.app.flags.DEFINE_float('c_lower', -0.01, 'Lower bound for weights.')
+tf.app.flags.DEFINE_float('c_upper', 0.01, 'Upper bound for weights.')
+
+FLAGS = tf.app.flags.FLAGS
+
+
+def create_hparams():
+  """Create the hparams object for generic training hyperparameters."""
+  hparams = tf.contrib.training.HParams(
+      gen_num_layers=2,
+      dis_num_layers=2,
+      gen_rnn_size=740,
+      dis_rnn_size=740,
+      gen_learning_rate=5e-4,
+      dis_learning_rate=5e-3,
+      critic_learning_rate=5e-3,
+      dis_train_iterations=1,
+      gen_learning_rate_decay=1.0,
+      gen_full_learning_rate_steps=1e7,
+      baseline_decay=0.999999,
+      rl_discount_rate=0.9,
+      gen_vd_keep_prob=0.5,
+      dis_vd_keep_prob=0.5,
+      dis_pretrain_learning_rate=5e-3,
+      dis_num_filters=128,
+      dis_hidden_dim=128,
+      gen_nas_keep_prob_0=0.85,
+      gen_nas_keep_prob_1=0.55,
+      dis_nas_keep_prob_0=0.85,
+      dis_nas_keep_prob_1=0.55)
+  # Command line flags override any of the preceding hyperparameter values.
+  if FLAGS.hparams:
+    hparams = hparams.parse(FLAGS.hparams)
+  return hparams
+
+
+def create_MaskGAN(hparams, is_training):
+  """Create the MaskGAN model.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    is_training:  Boolean indicating operational mode (train/inference).
+      evaluated with a teacher forcing regime.
+
+  Return:
+    model:  Namedtuple for specifying the MaskGAN.
+  """
+  global_step = tf.Variable(0, name='global_step', trainable=False)
+
+  new_learning_rate = tf.placeholder(tf.float32, [], name='new_learning_rate')
+  learning_rate = tf.Variable(0.0, name='learning_rate', trainable=False)
+  learning_rate_update = tf.assign(learning_rate, new_learning_rate)
+
+  new_rate = tf.placeholder(tf.float32, [], name='new_rate')
+  percent_real_var = tf.Variable(0.0, trainable=False)
+  percent_real_update = tf.assign(percent_real_var, new_rate)
+
+  ## Placeholders.
+  inputs = tf.placeholder(
+      tf.int32, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  targets = tf.placeholder(
+      tf.int32, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  present = tf.placeholder(
+      tf.bool, shape=[FLAGS.batch_size, FLAGS.sequence_length])
+  # TODO(adai): Placeholder for IMDB label.
+
+  ## Real Sequence is the targets.
+  real_sequence = targets
+
+  ## Fakse Sequence from the Generator.
+  # TODO(adai):  Generator must have IMDB labels placeholder.
+  (fake_sequence, fake_logits, fake_log_probs, fake_gen_initial_state,
+   fake_gen_final_state, _) = model_construction.create_generator(
+       hparams,
+       inputs,
+       targets,
+       present,
+       is_training=is_training,
+       is_validating=False)
+  (_, eval_logits, _, eval_initial_state, eval_final_state,
+   _) = model_construction.create_generator(
+       hparams,
+       inputs,
+       targets,
+       present,
+       is_training=False,
+       is_validating=True,
+       reuse=True)
+
+  ## Discriminator.
+  fake_predictions = model_construction.create_discriminator(
+      hparams,
+      fake_sequence,
+      is_training=is_training,
+      inputs=inputs,
+      present=present)
+  real_predictions = model_construction.create_discriminator(
+      hparams,
+      real_sequence,
+      is_training=is_training,
+      reuse=True,
+      inputs=inputs,
+      present=present)
+
+  ## Critic.
+  # The critic will be used to estimate the forward rewards to the Generator.
+  if FLAGS.baseline_method == 'critic':
+    est_state_values = model_construction.create_critic(
+        hparams, fake_sequence, is_training=is_training)
+  else:
+    est_state_values = None
+
+  ## Discriminator Loss.
+  [dis_loss, dis_loss_fake, dis_loss_real] = model_losses.create_dis_loss(
+      fake_predictions, real_predictions, present)
+
+  ## Average log-perplexity for only missing words.  However, to do this,
+  # the logits are still computed using teacher forcing, that is, the ground
+  # truth tokens are fed in at each time point to be valid.
+  avg_log_perplexity = model_losses.calculate_log_perplexity(
+      eval_logits, targets, present)
+
+  ## Generator Objective.
+  # 1.  Cross Entropy losses on missing tokens.
+  fake_cross_entropy_losses = model_losses.create_masked_cross_entropy_loss(
+      targets, present, fake_logits)
+
+  #  2.  GAN REINFORCE losses.
+  [
+      fake_RL_loss, fake_log_probs, fake_rewards, fake_advantages,
+      fake_baselines, fake_averages_op, critic_loss, cumulative_rewards
+  ] = model_losses.calculate_reinforce_objective(
+      hparams, fake_log_probs, fake_predictions, present, est_state_values)
+
+  ## Pre-training.
+  if FLAGS.gen_pretrain_steps:
+    raise NotImplementedError
+    # # TODO(liamfedus): Rewrite this.
+    # fwd_cross_entropy_loss = tf.reduce_mean(fwd_cross_entropy_losses)
+    # gen_pretrain_op = model_optimization.create_gen_pretrain_op(
+    #     hparams, fwd_cross_entropy_loss, global_step)
+  else:
+    gen_pretrain_op = None
+  if FLAGS.dis_pretrain_steps:
+    dis_pretrain_op = model_optimization.create_dis_pretrain_op(
+        hparams, dis_loss, global_step)
+  else:
+    dis_pretrain_op = None
+
+  ##  Generator Train Op.
+  # 1.  Cross-Entropy.
+  if FLAGS.gen_training_strategy == 'cross_entropy':
+    gen_loss = tf.reduce_mean(fake_cross_entropy_losses)
+    [gen_train_op, gen_grads,
+     gen_vars] = model_optimization.create_gen_train_op(
+         hparams, learning_rate, gen_loss, global_step, mode='MINIMIZE')
+
+  # 2.  GAN (REINFORCE)
+  elif FLAGS.gen_training_strategy == 'reinforce':
+    gen_loss = fake_RL_loss
+    [gen_train_op, gen_grads,
+     gen_vars] = model_optimization.create_reinforce_gen_train_op(
+         hparams, learning_rate, gen_loss, fake_averages_op, global_step)
+
+  else:
+    raise NotImplementedError
+
+  ## Discriminator Train Op.
+  dis_train_op, dis_grads, dis_vars = model_optimization.create_dis_train_op(
+      hparams, dis_loss, global_step)
+
+  ## Critic Train Op.
+  if critic_loss is not None:
+    [critic_train_op, _, _] = model_optimization.create_critic_train_op(
+        hparams, critic_loss, global_step)
+    dis_train_op = tf.group(dis_train_op, critic_train_op)
+
+  ## Summaries.
+  with tf.name_scope('general'):
+    tf.summary.scalar('percent_real', percent_real_var)
+    tf.summary.scalar('learning_rate', learning_rate)
+
+  with tf.name_scope('generator_objectives'):
+    tf.summary.scalar('gen_objective', tf.reduce_mean(gen_loss))
+    tf.summary.scalar('gen_loss_cross_entropy',
+                      tf.reduce_mean(fake_cross_entropy_losses))
+
+  with tf.name_scope('REINFORCE'):
+    with tf.name_scope('objective'):
+      tf.summary.scalar('fake_RL_loss', tf.reduce_mean(fake_RL_loss))
+
+    with tf.name_scope('rewards'):
+      helper.variable_summaries(cumulative_rewards, 'rewards')
+
+    with tf.name_scope('advantages'):
+      helper.variable_summaries(fake_advantages, 'advantages')
+
+    with tf.name_scope('baselines'):
+      helper.variable_summaries(fake_baselines, 'baselines')
+
+    with tf.name_scope('log_probs'):
+      helper.variable_summaries(fake_log_probs, 'log_probs')
+
+  with tf.name_scope('discriminator_losses'):
+    tf.summary.scalar('dis_loss', dis_loss)
+    tf.summary.scalar('dis_loss_fake_sequence', dis_loss_fake)
+    tf.summary.scalar('dis_loss_prob_fake_sequence', tf.exp(-dis_loss_fake))
+    tf.summary.scalar('dis_loss_real_sequence', dis_loss_real)
+    tf.summary.scalar('dis_loss_prob_real_sequence', tf.exp(-dis_loss_real))
+
+  if critic_loss is not None:
+    with tf.name_scope('critic_losses'):
+      tf.summary.scalar('critic_loss', critic_loss)
+
+  with tf.name_scope('logits'):
+    helper.variable_summaries(fake_logits, 'fake_logits')
+
+  for v, g in zip(gen_vars, gen_grads):
+    helper.variable_summaries(v, v.op.name)
+    helper.variable_summaries(g, 'grad/' + v.op.name)
+
+  for v, g in zip(dis_vars, dis_grads):
+    helper.variable_summaries(v, v.op.name)
+    helper.variable_summaries(g, 'grad/' + v.op.name)
+
+  merge_summaries_op = tf.summary.merge_all()
+  text_summary_placeholder = tf.placeholder(tf.string)
+  text_summary_op = tf.summary.text('Samples', text_summary_placeholder)
+
+  # Model saver.
+  saver = tf.train.Saver(keep_checkpoint_every_n_hours=1, max_to_keep=5)
+
+  # Named tuple that captures elements of the MaskGAN model.
+  Model = collections.namedtuple('Model', [
+      'inputs', 'targets', 'present', 'percent_real_update', 'new_rate',
+      'fake_sequence', 'fake_logits', 'fake_rewards', 'fake_baselines',
+      'fake_advantages', 'fake_log_probs', 'fake_predictions',
+      'real_predictions', 'fake_cross_entropy_losses', 'fake_gen_initial_state',
+      'fake_gen_final_state', 'eval_initial_state', 'eval_final_state',
+      'avg_log_perplexity', 'dis_loss', 'gen_loss', 'critic_loss',
+      'cumulative_rewards', 'dis_train_op', 'gen_train_op', 'gen_pretrain_op',
+      'dis_pretrain_op', 'merge_summaries_op', 'global_step',
+      'new_learning_rate', 'learning_rate_update', 'saver', 'text_summary_op',
+      'text_summary_placeholder'
+  ])
+
+  model = Model(
+      inputs, targets, present, percent_real_update, new_rate, fake_sequence,
+      fake_logits, fake_rewards, fake_baselines, fake_advantages,
+      fake_log_probs, fake_predictions, real_predictions,
+      fake_cross_entropy_losses, fake_gen_initial_state, fake_gen_final_state,
+      eval_initial_state, eval_final_state, avg_log_perplexity, dis_loss,
+      gen_loss, critic_loss, cumulative_rewards, dis_train_op, gen_train_op,
+      gen_pretrain_op, dis_pretrain_op, merge_summaries_op, global_step,
+      new_learning_rate, learning_rate_update, saver, text_summary_op,
+      text_summary_placeholder)
+  return model
+
+
+def compute_geometric_average(percent_captured):
+  """Compute the geometric average of the n-gram metrics."""
+
+  res = 1.
+  for _, n_gram_percent in percent_captured.iteritems():
+    res *= n_gram_percent
+
+  return np.power(res, 1. / float(len(percent_captured)))
+
+
+def compute_arithmetic_average(percent_captured):
+  """Compute the arithmetic average of the n-gram metrics."""
+  N = len(percent_captured)
+
+  res = 0.
+  for _, n_gram_percent in percent_captured.iteritems():
+    res += n_gram_percent
+
+  return res / float(N)
+
+
+def get_iterator(data):
+  """Return the data iterator."""
+  if FLAGS.data_set == 'ptb':
+    iterator = ptb_loader.ptb_iterator(data, FLAGS.batch_size,
+                                       FLAGS.sequence_length,
+                                       FLAGS.epoch_size_override)
+  elif FLAGS.data_set == 'imdb':
+    iterator = imdb_loader.imdb_iterator(data, FLAGS.batch_size,
+                                         FLAGS.sequence_length)
+  return iterator
+
+
+def train_model(hparams, data, log_dir, log, id_to_word, data_ngram_counts):
+  """Train model.
+
+  Args:
+    hparams: Hyperparameters for the MaskGAN.
+    data: Data to evaluate.
+    log_dir: Directory to save checkpoints.
+    log: Readable log for the experiment.
+    id_to_word: Dictionary of indices to words.
+    data_ngram_counts: Dictionary of hashed(n-gram tuples) to counts in the
+      data_set.
+  """
+  print('Training model.')
+  tf.logging.info('Training model.')
+
+  # Boolean indicating operational mode.
+  is_training = True
+
+  # Write all the information to the logs.
+  log.write('hparams\n')
+  log.write(str(hparams))
+  log.flush()
+
+  is_chief = FLAGS.task == 0
+
+  with tf.Graph().as_default():
+    with tf.device(tf.ReplicaDeviceSetter(FLAGS.ps_tasks)):
+      container_name = ''
+      with tf.container(container_name):
+        # Construct the model.
+        if FLAGS.num_rollouts == 1:
+          model = create_MaskGAN(hparams, is_training)
+        elif FLAGS.num_rollouts > 1:
+          model = rollout.create_rollout_MaskGAN(hparams, is_training)
+        else:
+          raise ValueError
+
+        print('\nTrainable Variables in Graph:')
+        for v in tf.trainable_variables():
+          print(v)
+
+        ## Retrieve the initial savers.
+        init_savers = model_utils.retrieve_init_savers(hparams)
+
+        ## Initial saver function to supervisor.
+        init_fn = partial(model_utils.init_fn, init_savers)
+
+        # Create the supervisor.  It will take care of initialization,
+        # summaries, checkpoints, and recovery.
+        sv = tf.Supervisor(
+            logdir=log_dir,
+            is_chief=is_chief,
+            saver=model.saver,
+            global_step=model.global_step,
+            save_model_secs=60,
+            recovery_wait_secs=30,
+            summary_op=None,
+            init_fn=init_fn)
+
+        # Get an initialized, and possibly recovered session.  Launch the
+        # services: Checkpointing, Summaries, step counting.
+        #
+        # When multiple replicas of this program are running the services are
+        # only launched by the 'chief' replica.
+        with sv.managed_session(FLAGS.master) as sess:
+
+          ## Pretrain the generator.
+          if FLAGS.gen_pretrain_steps:
+            pretrain_mask_gan.pretrain_generator(sv, sess, model, data, log,
+                                                 id_to_word, data_ngram_counts,
+                                                 is_chief)
+
+          ## Pretrain the discriminator.
+          if FLAGS.dis_pretrain_steps:
+            pretrain_mask_gan.pretrain_discriminator(
+                sv, sess, model, data, log, id_to_word, data_ngram_counts,
+                is_chief)
+
+          # Initial indicators for printing and summarizing.
+          print_step_division = -1
+          summary_step_division = -1
+
+          # Run iterative computation in a loop.
+          while not sv.ShouldStop():
+            is_present_rate = FLAGS.is_present_rate
+
+            if FLAGS.is_present_rate_decay is not None:
+              is_present_rate *= (1. - FLAGS.is_present_rate_decay)
+
+            model_utils.assign_percent_real(sess, model.percent_real_update,
+                                            model.new_rate, is_present_rate)
+
+            # GAN training.
+            avg_epoch_gen_loss, avg_epoch_dis_loss = [], []
+            cumulative_costs = 0.
+            gen_iters = 0
+
+            # Generator and Discriminator statefulness initial evaluation.
+            # TODO(liamfedus): Throughout the code I am implicitly assuming
+            # that the Generator and Discriminator are equal sized.
+            [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(
+                [model.eval_initial_state, model.fake_gen_initial_state])
+            dis_initial_state_eval = fake_gen_initial_state_eval
+
+            # Save zeros state to reset later.
+            zeros_state = fake_gen_initial_state_eval
+
+            ## Offset Discriminator.
+            if FLAGS.ps_tasks == 0:
+              dis_offset = 1
+            else:
+              dis_offset = FLAGS.task * 1000 + 1
+            dis_iterator = get_iterator(data)
+
+            for i in range(dis_offset):
+              try:
+                dis_x, dis_y, _ = next(dis_iterator)
+              except StopIteration:
+                dis_iterator = get_iterator(data)
+                dis_initial_state_eval = zeros_state
+                dis_x, dis_y, _ = next(dis_iterator)
+
+              p = model_utils.generate_mask()
+
+              # Construct the train feed.
+              train_feed = {
+                  model.inputs: dis_x,
+                  model.targets: dis_y,
+                  model.present: p
+              }
+
+              if FLAGS.data_set == 'ptb':
+                # Statefulness of the Generator being used for Discriminator.
+                for i, (c, h) in enumerate(model.fake_gen_initial_state):
+                  train_feed[c] = dis_initial_state_eval[i].c
+                  train_feed[h] = dis_initial_state_eval[i].h
+
+                # Determine the state had the Generator run over real data.  We
+                # use this state for the Discriminator.
+                [dis_initial_state_eval] = sess.run(
+                    [model.fake_gen_final_state], train_feed)
+
+            ## Training loop.
+            iterator = get_iterator(data)
+            gen_initial_state_eval = zeros_state
+
+            if FLAGS.ps_tasks > 0:
+              gen_offset = FLAGS.task * 1000 + 1
+              for i in range(gen_offset):
+                try:
+                  next(iterator)
+                except StopIteration:
+                  dis_iterator = get_iterator(data)
+                  dis_initial_state_eval = zeros_state
+                  next(dis_iterator)
+
+            for x, y, _ in iterator:
+              for _ in xrange(hparams.dis_train_iterations):
+                try:
+                  dis_x, dis_y, _ = next(dis_iterator)
+                except StopIteration:
+                  dis_iterator = get_iterator(data)
+                  dis_initial_state_eval = zeros_state
+                  dis_x, dis_y, _ = next(dis_iterator)
+
+                  if FLAGS.data_set == 'ptb':
+                    [dis_initial_state_eval] = sess.run(
+                        [model.fake_gen_initial_state])
+
+                p = model_utils.generate_mask()
+
+                # Construct the train feed.
+                train_feed = {
+                    model.inputs: dis_x,
+                    model.targets: dis_y,
+                    model.present: p
+                }
+
+                # Statefulness for the Discriminator.
+                if FLAGS.data_set == 'ptb':
+                  for i, (c, h) in enumerate(model.fake_gen_initial_state):
+                    train_feed[c] = dis_initial_state_eval[i].c
+                    train_feed[h] = dis_initial_state_eval[i].h
+
+                _, dis_loss_eval, step = sess.run(
+                    [model.dis_train_op, model.dis_loss, model.global_step],
+                    feed_dict=train_feed)
+
+                # Determine the state had the Generator run over real data.
+                # Use this state for the Discriminator.
+                [dis_initial_state_eval] = sess.run(
+                    [model.fake_gen_final_state], train_feed)
+
+              # Randomly mask out tokens.
+              p = model_utils.generate_mask()
+
+              # Construct the train feed.
+              train_feed = {model.inputs: x, model.targets: y, model.present: p}
+
+              # Statefulness for Generator.
+              if FLAGS.data_set == 'ptb':
+                tf.logging.info('Generator is stateful.')
+                print('Generator is stateful.')
+                # Statefulness for *evaluation* Generator.
+                for i, (c, h) in enumerate(model.eval_initial_state):
+                  train_feed[c] = gen_initial_state_eval[i].c
+                  train_feed[h] = gen_initial_state_eval[i].h
+
+                # Statefulness for Generator.
+                for i, (c, h) in enumerate(model.fake_gen_initial_state):
+                  train_feed[c] = fake_gen_initial_state_eval[i].c
+                  train_feed[h] = fake_gen_initial_state_eval[i].h
+
+              # Determine whether to decay learning rate.
+              lr_decay = hparams.gen_learning_rate_decay**max(
+                  step + 1 - hparams.gen_full_learning_rate_steps, 0.0)
+
+              # Assign learning rate.
+              gen_learning_rate = hparams.gen_learning_rate * lr_decay
+              model_utils.assign_learning_rate(sess, model.learning_rate_update,
+                                               model.new_learning_rate,
+                                               gen_learning_rate)
+
+              [_, gen_loss_eval, gen_log_perplexity_eval, step] = sess.run(
+                  [
+                      model.gen_train_op, model.gen_loss,
+                      model.avg_log_perplexity, model.global_step
+                  ],
+                  feed_dict=train_feed)
+
+              cumulative_costs += gen_log_perplexity_eval
+              gen_iters += 1
+
+              # Determine the state had the Generator run over real data.
+              [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(
+                  [model.eval_final_state,
+                   model.fake_gen_final_state], train_feed)
+
+              avg_epoch_dis_loss.append(dis_loss_eval)
+              avg_epoch_gen_loss.append(gen_loss_eval)
+
+              ## Summaries.
+              # Calulate rolling perplexity.
+              perplexity = np.exp(cumulative_costs / gen_iters)
+
+              if is_chief and (step / FLAGS.summaries_every >
+                               summary_step_division):
+                summary_step_division = step / FLAGS.summaries_every
+
+                # Confirm perplexity is not infinite.
+                if (not np.isfinite(perplexity) or
+                    perplexity >= FLAGS.perplexity_threshold):
+                  print('Training raising FloatingPoinError.')
+                  raise FloatingPointError(
+                      'Training infinite perplexity: %.3f' % perplexity)
+
+                # Graph summaries.
+                summary_str = sess.run(
+                    model.merge_summaries_op, feed_dict=train_feed)
+                sv.SummaryComputed(sess, summary_str)
+
+                # Summary:  n-gram
+                avg_percent_captured = {'2': 0., '3': 0., '4': 0.}
+                for n, data_ngram_count in data_ngram_counts.iteritems():
+                  batch_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+                      sess, model.fake_sequence, log, train_feed,
+                      data_ngram_count, int(n))
+                  summary_percent_str = tf.Summary(value=[
+                      tf.Summary.Value(
+                          tag='general/%s-grams_percent_correct' % n,
+                          simple_value=batch_percent_captured)
+                  ])
+                  sv.SummaryComputed(
+                      sess, summary_percent_str, global_step=step)
+
+                # Summary:  geometric_avg
+                geometric_avg = compute_geometric_average(avg_percent_captured)
+                summary_geometric_avg_str = tf.Summary(value=[
+                    tf.Summary.Value(
+                        tag='general/geometric_avg', simple_value=geometric_avg)
+                ])
+                sv.SummaryComputed(
+                    sess, summary_geometric_avg_str, global_step=step)
+
+                # Summary:  arithmetic_avg
+                arithmetic_avg = compute_arithmetic_average(
+                    avg_percent_captured)
+                summary_arithmetic_avg_str = tf.Summary(value=[
+                    tf.Summary.Value(
+                        tag='general/arithmetic_avg',
+                        simple_value=arithmetic_avg)
+                ])
+                sv.SummaryComputed(
+                    sess, summary_arithmetic_avg_str, global_step=step)
+
+                # Summary:  perplexity
+                summary_perplexity_str = tf.Summary(value=[
+                    tf.Summary.Value(
+                        tag='general/perplexity', simple_value=perplexity)
+                ])
+                sv.SummaryComputed(
+                    sess, summary_perplexity_str, global_step=step)
+
+              ## Printing and logging
+              if is_chief and (step / FLAGS.print_every > print_step_division):
+                print_step_division = (step / FLAGS.print_every)
+                print('global_step: %d' % step)
+                print(' perplexity: %.3f' % perplexity)
+                print(' gen_learning_rate: %.6f' % gen_learning_rate)
+                log.write('global_step: %d\n' % step)
+                log.write(' perplexity: %.3f\n' % perplexity)
+                log.write(' gen_learning_rate: %.6f' % gen_learning_rate)
+
+                # Average percent captured for each of the n-grams.
+                avg_percent_captured = {'2': 0., '3': 0., '4': 0.}
+                for n, data_ngram_count in data_ngram_counts.iteritems():
+                  batch_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+                      sess, model.fake_sequence, log, train_feed,
+                      data_ngram_count, int(n))
+                  avg_percent_captured[n] = batch_percent_captured
+                  print(' percent of %s-grams captured: %.3f.' %
+                        (n, batch_percent_captured))
+                  log.write(' percent of %s-grams captured: %.3f.\n' %
+                            (n, batch_percent_captured))
+                geometric_avg = compute_geometric_average(avg_percent_captured)
+                print(' geometric_avg: %.3f.' % geometric_avg)
+                log.write(' geometric_avg: %.3f.' % geometric_avg)
+                arithmetic_avg = compute_arithmetic_average(
+                    avg_percent_captured)
+                print(' arithmetic_avg: %.3f.' % arithmetic_avg)
+                log.write(' arithmetic_avg: %.3f.' % arithmetic_avg)
+
+                evaluation_utils.print_and_log_losses(
+                    log, step, is_present_rate, avg_epoch_dis_loss,
+                    avg_epoch_gen_loss)
+
+                if FLAGS.gen_training_strategy == 'reinforce':
+                  evaluation_utils.generate_RL_logs(sess, model, log,
+                                                    id_to_word, train_feed)
+                else:
+                  evaluation_utils.generate_logs(sess, model, log, id_to_word,
+                                                 train_feed)
+                log.flush()
+
+  log.close()
+
+
+def evaluate_once(data, sv, model, sess, train_dir, log, id_to_word,
+                  data_ngram_counts, eval_saver):
+  """Evaluate model for a number of steps.
+
+  Args:
+    data:  Dataset.
+    sv: Supervisor.
+    model: The GAN model we have just built.
+    sess: A session to use.
+    train_dir: Path to a directory containing checkpoints.
+    log: Evaluation log for evaluation.
+    id_to_word: Dictionary of indices to words.
+    data_ngram_counts: Dictionary of hashed(n-gram tuples) to counts in the
+      data_set.
+    eval_saver:  Evaluation saver.r.
+  """
+  tf.logging.info('Evaluate Once.')
+  # Load the last model checkpoint, or initialize the graph.
+  model_save_path = tf.latest_checkpoint(train_dir)
+  if not model_save_path:
+    tf.logging.warning('No checkpoint yet in: %s', train_dir)
+    return
+
+  tf.logging.info('Starting eval of: %s' % model_save_path)
+  tf.logging.info('Only restoring trainable variables.')
+  eval_saver.restore(sess, model_save_path)
+
+  # Run the requested number of evaluation steps
+  avg_epoch_gen_loss, avg_epoch_dis_loss = [], []
+  cumulative_costs = 0.
+
+  # Average percent captured for each of the n-grams.
+  avg_percent_captured = {'2': 0., '3': 0., '4': 0.}
+
+  # Set a random seed to keep fixed mask.
+  np.random.seed(0)
+  gen_iters = 0
+
+  # Generator statefulness over the epoch.
+  # TODO(liamfedus):  Check this.
+  [gen_initial_state_eval, fake_gen_initial_state_eval] = sess.run(
+      [model.eval_initial_state, model.fake_gen_initial_state])
+
+  if FLAGS.eval_language_model:
+    is_present_rate = 0.
+    tf.logging.info('Overriding is_present_rate=0. for evaluation.')
+    print('Overriding is_present_rate=0. for evaluation.')
+
+  iterator = get_iterator(data)
+
+  for x, y, _ in iterator:
+    if FLAGS.eval_language_model:
+      is_present_rate = 0.
+    else:
+      is_present_rate = FLAGS.is_present_rate
+      tf.logging.info('Evaluating on is_present_rate=%.3f.' % is_present_rate)
+
+    model_utils.assign_percent_real(sess, model.percent_real_update,
+                                    model.new_rate, is_present_rate)
+
+    # Randomly mask out tokens.
+    p = model_utils.generate_mask()
+
+    eval_feed = {model.inputs: x, model.targets: y, model.present: p}
+
+    if FLAGS.data_set == 'ptb':
+      # Statefulness for *evaluation* Generator.
+      for i, (c, h) in enumerate(model.eval_initial_state):
+        eval_feed[c] = gen_initial_state_eval[i].c
+        eval_feed[h] = gen_initial_state_eval[i].h
+
+      # Statefulness for the Generator.
+      for i, (c, h) in enumerate(model.fake_gen_initial_state):
+        eval_feed[c] = fake_gen_initial_state_eval[i].c
+        eval_feed[h] = fake_gen_initial_state_eval[i].h
+
+    [
+        gen_log_perplexity_eval, dis_loss_eval, gen_loss_eval,
+        gen_initial_state_eval, fake_gen_initial_state_eval, step
+    ] = sess.run(
+        [
+            model.avg_log_perplexity, model.dis_loss, model.gen_loss,
+            model.eval_final_state, model.fake_gen_final_state,
+            model.global_step
+        ],
+        feed_dict=eval_feed)
+
+    for n, data_ngram_count in data_ngram_counts.iteritems():
+      batch_percent_captured = evaluation_utils.sequence_ngram_evaluation(
+          sess, model.fake_sequence, log, eval_feed, data_ngram_count, int(n))
+      avg_percent_captured[n] += batch_percent_captured
+
+    cumulative_costs += gen_log_perplexity_eval
+
+    avg_epoch_dis_loss.append(dis_loss_eval)
+    avg_epoch_gen_loss.append(gen_loss_eval)
+
+    gen_iters += 1
+
+  # Calulate rolling metrics.
+  perplexity = np.exp(cumulative_costs / gen_iters)
+  for n, _ in avg_percent_captured.iteritems():
+    avg_percent_captured[n] /= gen_iters
+
+  # Confirm perplexity is not infinite.
+  if not np.isfinite(perplexity) or perplexity >= FLAGS.perplexity_threshold:
+    print('Evaluation raising FloatingPointError.')
+    raise FloatingPointError(
+        'Evaluation infinite perplexity: %.3f' % perplexity)
+
+  ## Printing and logging.
+  evaluation_utils.print_and_log_losses(log, step, is_present_rate,
+                                        avg_epoch_dis_loss, avg_epoch_gen_loss)
+  print(' perplexity: %.3f' % perplexity)
+  log.write(' perplexity: %.3f\n' % perplexity)
+
+  for n, n_gram_percent in avg_percent_captured.iteritems():
+    n = int(n)
+    print(' percent of %d-grams captured: %.3f.' % (n, n_gram_percent))
+    log.write(' percent of %d-grams captured: %.3f.\n' % (n, n_gram_percent))
+
+  samples = evaluation_utils.generate_logs(sess, model, log, id_to_word,
+                                           eval_feed)
+
+  ## Summaries.
+  summary_str = sess.run(model.merge_summaries_op, feed_dict=eval_feed)
+  sv.SummaryComputed(sess, summary_str)
+
+  # Summary: text
+  summary_str = sess.run(model.text_summary_op,
+                         {model.text_summary_placeholder: '\n\n'.join(samples)})
+  sv.SummaryComputed(sess, summary_str, global_step=step)
+
+  # Summary:  n-gram
+  for n, n_gram_percent in avg_percent_captured.iteritems():
+    n = int(n)
+    summary_percent_str = tf.Summary(value=[
+        tf.Summary.Value(
+            tag='general/%d-grams_percent_correct' % n,
+            simple_value=n_gram_percent)
+    ])
+    sv.SummaryComputed(sess, summary_percent_str, global_step=step)
+
+  # Summary:  geometric_avg
+  geometric_avg = compute_geometric_average(avg_percent_captured)
+  summary_geometric_avg_str = tf.Summary(value=[
+      tf.Summary.Value(tag='general/geometric_avg', simple_value=geometric_avg)
+  ])
+  sv.SummaryComputed(sess, summary_geometric_avg_str, global_step=step)
+
+  # Summary:  arithmetic_avg
+  arithmetic_avg = compute_arithmetic_average(avg_percent_captured)
+  summary_arithmetic_avg_str = tf.Summary(value=[
+      tf.Summary.Value(
+          tag='general/arithmetic_avg', simple_value=arithmetic_avg)
+  ])
+  sv.SummaryComputed(sess, summary_arithmetic_avg_str, global_step=step)
+
+  # Summary:  perplexity
+  summary_perplexity_str = tf.Summary(value=[
+      tf.Summary.Value(tag='general/perplexity', simple_value=perplexity)
+  ])
+  sv.SummaryComputed(sess, summary_perplexity_str, global_step=step)
+
+
+def evaluate_model(hparams, data, train_dir, log, id_to_word,
+                   data_ngram_counts):
+  """Evaluate MaskGAN model.
+
+  Args:
+    hparams:  Hyperparameters for the MaskGAN.
+    data: Data to evaluate.
+    train_dir: Path to a directory containing checkpoints.
+    id_to_word: Dictionary of indices to words.
+    data_ngram_counts: Dictionary of hashed(n-gram tuples) to counts in the
+      data_set.
+  """
+  tf.logging.error('Evaluate model.')
+
+  # Boolean indicating operational mode.
+  is_training = False
+
+  if FLAGS.mode == MODE_VALIDATION:
+    logdir = FLAGS.base_directory + '/validation'
+  elif FLAGS.mode == MODE_TRAIN_EVAL:
+    logdir = FLAGS.base_directory + '/train_eval'
+  elif FLAGS.mode == MODE_TEST:
+    logdir = FLAGS.base_directory + '/test'
+  else:
+    raise NotImplementedError
+
+  # Wait for a checkpoint to exist.
+  print(train_dir)
+  print(tf.train.latest_checkpoint(train_dir))
+  while not tf.train.latest_checkpoint(train_dir):
+    tf.logging.error('Waiting for checkpoint...')
+    print('Waiting for checkpoint...')
+    time.sleep(10)
+
+  with tf.Graph().as_default():
+    # Use a separate container for each trial
+    container_name = ''
+    with tf.container(container_name):
+
+      # Construct the model.
+      if FLAGS.num_rollouts == 1:
+        model = create_MaskGAN(hparams, is_training)
+      elif FLAGS.num_rollouts > 1:
+        model = rollout.create_rollout_MaskGAN(hparams, is_training)
+      else:
+        raise ValueError
+
+      # Create the supervisor.  It will take care of initialization, summaries,
+      # checkpoints, and recovery.  We only pass the trainable variables
+      # to load since things like baselines keep batch_size which may not
+      # match between training and evaluation.
+      evaluation_variables = tf.trainable_variables()
+      evaluation_variables.append(model.global_step)
+      eval_saver = tf.train.Saver(var_list=evaluation_variables)
+      sv = tf.Supervisor(logdir=logdir)
+      sess = sv.PrepareSession(FLAGS.eval_master, start_standard_services=False)
+
+      tf.logging.info('Before sv.Loop.')
+      sv.Loop(FLAGS.eval_interval_secs, evaluate_once,
+              (data, sv, model, sess, train_dir, log, id_to_word,
+               data_ngram_counts, eval_saver))
+
+      sv.WaitForStop()
+      tf.logging.info('sv.Stop().')
+      sv.Stop()
+
+
+def main(_):
+  hparams = create_hparams()
+  train_dir = FLAGS.base_directory + '/train'
+
+  # Load data set.
+  if FLAGS.data_set == 'ptb':
+    raw_data = ptb_loader.ptb_raw_data(FLAGS.data_dir)
+    train_data, valid_data, test_data, _ = raw_data
+    valid_data_flat = valid_data
+  elif FLAGS.data_set == 'imdb':
+    raw_data = imdb_loader.imdb_raw_data(FLAGS.data_dir)
+    # TODO(liamfedus): Get an IMDB test partition.
+    train_data, valid_data = raw_data
+    valid_data_flat = [word for review in valid_data for word in review]
+  else:
+    raise NotImplementedError
+
+  if FLAGS.mode == MODE_TRAIN or FLAGS.mode == MODE_TRAIN_EVAL:
+    data_set = train_data
+  elif FLAGS.mode == MODE_VALIDATION:
+    data_set = valid_data
+  elif FLAGS.mode == MODE_TEST:
+    data_set = test_data
+  else:
+    raise NotImplementedError
+
+  # Dictionary and reverse dictionry.
+  if FLAGS.data_set == 'ptb':
+    word_to_id = ptb_loader.build_vocab(
+        os.path.join(FLAGS.data_dir, 'ptb.train.txt'))
+  elif FLAGS.data_set == 'imdb':
+    word_to_id = imdb_loader.build_vocab(
+        os.path.join(FLAGS.data_dir, 'vocab.txt'))
+  id_to_word = {v: k for k, v in word_to_id.iteritems()}
+
+  # Dictionary of Training Set n-gram counts.
+  bigram_tuples = n_gram.find_all_ngrams(valid_data_flat, n=2)
+  trigram_tuples = n_gram.find_all_ngrams(valid_data_flat, n=3)
+  fourgram_tuples = n_gram.find_all_ngrams(valid_data_flat, n=4)
+
+  bigram_counts = n_gram.construct_ngrams_dict(bigram_tuples)
+  trigram_counts = n_gram.construct_ngrams_dict(trigram_tuples)
+  fourgram_counts = n_gram.construct_ngrams_dict(fourgram_tuples)
+  print('Unique %d-grams: %d' % (2, len(bigram_counts)))
+  print('Unique %d-grams: %d' % (3, len(trigram_counts)))
+  print('Unique %d-grams: %d' % (4, len(fourgram_counts)))
+
+  data_ngram_counts = {
+      '2': bigram_counts,
+      '3': trigram_counts,
+      '4': fourgram_counts
+  }
+
+  # TODO(liamfedus):  This was necessary because there was a problem with our
+  # originally trained IMDB models.  The EOS_INDEX was off by one, which means,
+  # two words were mapping to index 86933.  The presence of '</s>' is going
+  # to throw and out of vocabulary error.
+  FLAGS.vocab_size = len(id_to_word)
+  print('Vocab size: %d' % FLAGS.vocab_size)
+
+  tf.gfile.MakeDirs(FLAGS.base_directory)
+
+  if FLAGS.mode == MODE_TRAIN:
+    log = tf.gfile.GFile(
+        os.path.join(FLAGS.base_directory, 'train-log.txt'), mode='w')
+  elif FLAGS.mode == MODE_VALIDATION:
+    log = tf.gfile.GFile(
+        os.path.join(FLAGS.base_directory, 'validation-log.txt'), mode='w')
+  elif FLAGS.mode == MODE_TRAIN_EVAL:
+    log = tf.gfile.GFile(
+        os.path.join(FLAGS.base_directory, 'train_eval-log.txt'), mode='w')
+  else:
+    log = tf.gfile.GFile(
+        os.path.join(FLAGS.base_directory, 'test-log.txt'), mode='w')
+
+  if FLAGS.mode == MODE_TRAIN:
+    train_model(hparams, data_set, train_dir, log, id_to_word,
+                data_ngram_counts)
+
+  elif FLAGS.mode == MODE_VALIDATION:
+    evaluate_model(hparams, data_set, train_dir, log, id_to_word,
+                   data_ngram_counts)
+  elif FLAGS.mode == MODE_TRAIN_EVAL:
+    evaluate_model(hparams, data_set, train_dir, log, id_to_word,
+                   data_ngram_counts)
+
+  elif FLAGS.mode == MODE_TEST:
+    evaluate_model(hparams, data_set, train_dir, log, id_to_word,
+                   data_ngram_counts)
+
+  else:
+    raise NotImplementedError
+
+
+if __name__ == '__main__':
+  tf.app.run()
