commit 83076c16069b961ab82b7df9ca6a731551b0cbf5
Author: Hongkun Yu <hongkuny@google.com>
Date:   Wed Dec 18 21:57:17 2019 -0800

    Fix bert readme. Always use keras_bert path.
    
    PiperOrigin-RevId: 286327318

diff --git a/official/nlp/bert/README.md b/official/nlp/bert/README.md
index 77d4a11d..bfd7a2f7 100644
--- a/official/nlp/bert/README.md
+++ b/official/nlp/bert/README.md
@@ -141,7 +141,7 @@ and unpack it to some directory `$GLUE_DIR`.
 
 ```shell
 export GLUE_DIR=~/glue
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 
 export TASK_NAME=MNLI
 export OUTPUT_DIR=gs://some_bucket/datasets
@@ -172,7 +172,7 @@ The necessary files can be found here:
 ```shell
 export SQUAD_DIR=~/squad
 export SQUAD_VERSION=v1.1
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export OUTPUT_DIR=gs://some_bucket/datasets
 
 python create_finetuning_data.py \
@@ -190,7 +190,7 @@ python create_finetuning_data.py \
 * Cloud Storage
 
 The unzipped pre-trained model files can also be found in the Google Cloud
-Storage folder `gs://cloud-tpu-checkpoints/bert/tf_20`. For example:
+Storage folder `gs://cloud-tpu-checkpoints/bert/keras_bert`. For example:
 
 ```shell
 export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
@@ -221,7 +221,7 @@ For GPU memory of 16GB or smaller, you may try to use `BERT-Base`
 (uncased_L-12_H-768_A-12).
 
 ```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export MODEL_DIR=gs://some_bucket/my_output_dir
 export GLUE_DIR=gs://some_bucket/datasets
 export TASK=MRPC
@@ -246,7 +246,7 @@ To use TPU, you only need to switch distribution strategy type to `tpu` with TPU
 information and use remote storage for model checkpoints.
 
 ```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export TPU_IP_ADDRESS='???'
 export MODEL_DIR=gs://some_bucket/my_output_dir
 export GLUE_DIR=gs://some_bucket/datasets
@@ -278,7 +278,7 @@ For GPU memory of 16GB or smaller, you may try to use `BERT-Base`
 (uncased_L-12_H-768_A-12).
 
 ```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export SQUAD_DIR=gs://some_bucket/datasets
 export MODEL_DIR=gs://some_bucket/my_output_dir
 export SQUAD_VERSION=v1.1
@@ -302,7 +302,7 @@ To use TPU, you need switch distribution strategy type to `tpu` with TPU
 information.
 
 ```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export TPU_IP_ADDRESS='???'
 export MODEL_DIR=gs://some_bucket/my_output_dir
 export SQUAD_DIR=gs://some_bucket/datasets
