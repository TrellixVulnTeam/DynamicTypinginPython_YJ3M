commit e8140fa9a2723ace22146f9dd01fc1c1c7c6ed0e
Author: Chen Chen <chendouble@google.com>
Date:   Wed Feb 12 19:10:15 2020 -0800

    Remove output_activation because it is not used anywhere.
    
    PiperOrigin-RevId: 294813724

diff --git a/official/nlp/modeling/networks/bert_pretrainer.py b/official/nlp/modeling/networks/bert_pretrainer.py
index f3439377..42406103 100644
--- a/official/nlp/modeling/networks/bert_pretrainer.py
+++ b/official/nlp/modeling/networks/bert_pretrainer.py
@@ -33,9 +33,9 @@ class BertPretrainer(tf.keras.Model):
   encoder as described in "BERT: Pre-training of Deep Bidirectional Transformers
   for Language Understanding" (https://arxiv.org/abs/1810.04805).
 
-  The BertTrainer allows a user to pass in a transformer stack, and instantiates
-  the masked language model and classification networks that are used to create
-  the training objectives.
+  The BertPretrainer allows a user to pass in a transformer stack, and
+  instantiates the masked language model and classification networks that are
+  used to create the training objectives.
 
   Attributes:
     network: A transformer network. This network should output a sequence output
@@ -56,7 +56,6 @@ class BertPretrainer(tf.keras.Model):
                num_classes,
                num_token_predictions,
                activation=None,
-               output_activation=None,
                initializer='glorot_uniform',
                output='logits',
                **kwargs):
@@ -66,7 +65,6 @@ class BertPretrainer(tf.keras.Model):
         'num_classes': num_classes,
         'num_token_predictions': num_token_predictions,
         'activation': activation,
-        'output_activation': output_activation,
         'initializer': initializer,
         'output': output,
     }
