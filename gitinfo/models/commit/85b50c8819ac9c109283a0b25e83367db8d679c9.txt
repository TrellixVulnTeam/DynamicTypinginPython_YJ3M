commit 85b50c8819ac9c109283a0b25e83367db8d679c9
Author: Chen Chen <chendouble@google.com>
Date:   Fri Apr 17 11:33:52 2020 -0700

    Add TalkingHeadsAttention to README.
    
    PiperOrigin-RevId: 307082084

diff --git a/official/nlp/modeling/layers/README.md b/official/nlp/modeling/layers/README.md
index 111d69fe..b34c3a46 100644
--- a/official/nlp/modeling/layers/README.md
+++ b/official/nlp/modeling/layers/README.md
@@ -14,6 +14,9 @@ If `from_tensor` and `to_tensor` are the same, then this is self-attention.
 * [CachedAttention](attention.py) implements an attention layer with cache used
 for auto-agressive decoding.
 
+* [TalkingHeadsAttention](talking_heads_attention.py) implements the talking
+heads attention, as decribed in ["Talking-Heads Attention"](https://arxiv.org/abs/2003.02436).
+
 * [Transformer](transformer.py) implements an optionally masked transformer as
 described in ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).
 
