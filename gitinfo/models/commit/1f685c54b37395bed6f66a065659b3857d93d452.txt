commit 1f685c54b37395bed6f66a065659b3857d93d452
Author: Chen Chen <chendouble@google.com>
Date:   Mon Apr 20 16:32:46 2020 -0700

    Set the name when creating layers.PositionEmbedding object.
    
    PiperOrigin-RevId: 307500045

diff --git a/official/nlp/modeling/networks/albert_transformer_encoder.py b/official/nlp/modeling/networks/albert_transformer_encoder.py
index 437f5622..5a2b522f 100644
--- a/official/nlp/modeling/networks/albert_transformer_encoder.py
+++ b/official/nlp/modeling/networks/albert_transformer_encoder.py
@@ -122,7 +122,8 @@ class AlbertTransformerEncoder(network.Network):
     self._position_embedding_layer = layers.PositionEmbedding(
         initializer=initializer,
         use_dynamic_slicing=True,
-        max_sequence_length=max_sequence_length)
+        max_sequence_length=max_sequence_length,
+        name='position_embedding')
     position_embeddings = self._position_embedding_layer(word_embeddings)
 
     type_embeddings = (
diff --git a/official/nlp/modeling/networks/encoder_scaffold.py b/official/nlp/modeling/networks/encoder_scaffold.py
index 12dd61ee..d356ca59 100644
--- a/official/nlp/modeling/networks/encoder_scaffold.py
+++ b/official/nlp/modeling/networks/encoder_scaffold.py
@@ -146,7 +146,8 @@ class EncoderScaffold(network.Network):
       self._position_embedding_layer = layers.PositionEmbedding(
           initializer=embedding_cfg['initializer'],
           use_dynamic_slicing=True,
-          max_sequence_length=embedding_cfg['max_seq_length'])
+          max_sequence_length=embedding_cfg['max_seq_length'],
+          name='position_embedding')
       position_embeddings = self._position_embedding_layer(word_embeddings)
 
       type_embeddings = (
diff --git a/official/nlp/modeling/networks/transformer_encoder.py b/official/nlp/modeling/networks/transformer_encoder.py
index 8b0bd40d..07d209c1 100644
--- a/official/nlp/modeling/networks/transformer_encoder.py
+++ b/official/nlp/modeling/networks/transformer_encoder.py
@@ -118,7 +118,8 @@ class TransformerEncoder(network.Network):
     self._position_embedding_layer = layers.PositionEmbedding(
         initializer=initializer,
         use_dynamic_slicing=True,
-        max_sequence_length=max_sequence_length)
+        max_sequence_length=max_sequence_length,
+        name='position_embedding')
     position_embeddings = self._position_embedding_layer(word_embeddings)
 
     type_embeddings = (
