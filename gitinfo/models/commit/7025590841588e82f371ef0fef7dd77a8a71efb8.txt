commit 7025590841588e82f371ef0fef7dd77a8a71efb8
Author: pkulzc <lzc@google.com>
Date:   Thu Jul 12 17:04:05 2018 -0700

    Object detection Internal Changes. (#4757)
    
    * Merged commit includes the following changes:
    204316992  by Zhichao Lu:
    
        Update docs to prepare inputs
    
    --
    204309254  by Zhichao Lu:
    
        Update running_pets.md to use new binaries and correct a few things in running_on_cloud.md
    
    --
    204306734  by Zhichao Lu:
    
        Move old binaries into legacy folder and add deprecation notice.
    
    --
    204267757  by Zhichao Lu:
    
        Fixing a problem in VRD evaluation with missing ground truth annotations for
        images that do not contain objects from 62 groundtruth classes.
    
    --
    204167430  by Zhichao Lu:
    
        This fixes a flaky losses test failure.
    
    --
    203670721  by Zhichao Lu:
    
        Internal change.
    
    --
    203569388  by Zhichao Lu:
    
        Internal change
    
    203546580  by Zhichao Lu:
    
        * Expand TPU compatibility g3doc with config snippets
        * Change mscoco dataset path in sample configs to the sharded versions
    
    --
    203325694  by Zhichao Lu:
    
        Make merge_multiple_label_boxes work for model_main code path.
    
    --
    203305655  by Zhichao Lu:
    
        Remove the 1x1 conv layer before pooling in MobileNet-v1-PPN feature extractor.
    
    --
    203139608  by Zhichao Lu:
    
        - Support exponential_decay with burnin learning rate schedule.
        - Add the minimum learning rate option.
        - Make the exponential decay start only after the burnin steps.
    
    --
    203068703  by Zhichao Lu:
    
        Modify create_coco_tf_record.py to output sharded files.
    
    --
    203025308  by Zhichao Lu:
    
        Add an option to share the prediction tower in WeightSharedBoxPredictor.
    
    --
    203024942  by Zhichao Lu:
    
        Move ssd mobilenet v1 ppn configs to third party.
    
    --
    202901259  by Zhichao Lu:
    
        Delete obsolete ssd mobilenet v1 focal loss configs and update pets dataset path
    
    --
    202894154  by Zhichao Lu:
    
        Move all TPU compatible ssd mobilenet v1 coco14/pet configs to third party.
    
    --
    202861774  by Zhichao Lu:
    
        Move Retinanet (SSD + FPN + Shared box predictor) configs to third_party.
    
    --
    
    PiperOrigin-RevId: 204316992
    
    * Add original files back.

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index 98ade416..39c7284e 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -39,15 +39,18 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:l291WsrB-hQJ:scholar.go
 
 ## Table of contents
 
+Setup:
+
+  * <a href='g3doc/installation.md'>Installation</a><br>
+
 Quick Start:
 
   * <a href='object_detection_tutorial.ipynb'>
       Quick Start: Jupyter notebook for off-the-shelf inference</a><br>
   * <a href="g3doc/running_pets.md">Quick Start: Training a pet detector</a><br>
 
-Setup:
+Customizing a Pipeline:
 
-  * <a href='g3doc/installation.md'>Installation</a><br>
   * <a href='g3doc/configuring_jobs.md'>
       Configuring an object detection pipeline</a><br>
   * <a href='g3doc/preparing_inputs.md'>Preparing inputs</a><br>
@@ -73,7 +76,7 @@ Extras:
   * <a href='g3doc/instance_segmentation.md'>
       Run an instance segmentation model</a><br>
   * <a href='g3doc/challenge_evaluation.md'>
-      Run the evaluation for the Open Images Challenge 2018.</a><br>
+      Run the evaluation for the Open Images Challenge 2018</a><br>
 
 ## Getting Help
 
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 2f311221..9ac14487 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -87,7 +87,8 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes):
         class_prediction_bias_init=conv_box_predictor.
         class_prediction_bias_init,
         use_dropout=conv_box_predictor.use_dropout,
-        dropout_keep_prob=conv_box_predictor.dropout_keep_probability)
+        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,
+        share_prediction_tower=conv_box_predictor.share_prediction_tower)
     return box_predictor_object
 
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 1ebdcb79..6c586909 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -33,10 +33,13 @@ from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_n
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
+from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
 from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
 from object_detection.protos import model_pb2
 
@@ -45,10 +48,17 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_inception_v2': SSDInceptionV2FeatureExtractor,
     'ssd_inception_v3': SSDInceptionV3FeatureExtractor,
     'ssd_mobilenet_v1': SSDMobileNetV1FeatureExtractor,
+    'ssd_mobilenet_v1_fpn': SSDMobileNetV1FpnFeatureExtractor,
+    'ssd_mobilenet_v1_ppn': SSDMobileNetV1PpnFeatureExtractor,
     'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,
     'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
     'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
     'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
+    'ssd_resnet50_v1_ppn': ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
+    'ssd_resnet101_v1_ppn':
+        ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
+    'ssd_resnet152_v1_ppn':
+        ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor,
     'embedded_ssd_mobilenet_v1': EmbeddedSSDMobileNetV1FeatureExtractor,
 }
 
@@ -327,6 +337,8 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
         second_stage_classification_loss_weight,
         second_stage_localization_loss_weight)
 
+  use_matmul_crop_and_resize = (frcnn_config.use_matmul_crop_and_resize)
+
   common_kwargs = {
       'is_training': is_training,
       'num_classes': num_classes,
@@ -360,7 +372,9 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'second_stage_classification_loss_weight':
       second_stage_classification_loss_weight,
       'hard_example_miner': hard_example_miner,
-      'add_summaries': add_summaries}
+      'add_summaries': add_summaries,
+      'use_matmul_crop_and_resize': use_matmul_crop_and_resize
+  }
 
   if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):
     return rfcn_meta_arch.RFCNMetaArch(
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index 225e1d50..cb744f25 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -28,10 +28,13 @@ from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_n
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
+from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
 from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
 from object_detection.models.ssd_inception_v2_feature_extractor import SSDInceptionV2FeatureExtractor
 from object_detection.models.ssd_inception_v3_feature_extractor import SSDInceptionV3FeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobileNetV1FeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
+from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
 from object_detection.protos import model_pb2
 
@@ -50,7 +53,22 @@ SSD_RESNET_V1_FPN_FEAT_MAPS = {
     'ssd_resnet101_v1_fpn':
     ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
     'ssd_resnet152_v1_fpn':
-    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor
+    ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
+    'ssd_resnet50_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
+    'ssd_resnet101_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
+    'ssd_resnet152_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor
+}
+
+SSD_RESNET_V1_PPN_FEAT_MAPS = {
+    'ssd_resnet50_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
+    'ssd_resnet101_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
+    'ssd_resnet152_v1_ppn':
+    ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor
 }
 
 
@@ -296,6 +314,87 @@ class ModelBuilderTest(tf.test.TestCase):
       self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
       self.assertIsInstance(model._feature_extractor, extractor_class)
 
+  def test_create_ssd_resnet_v1_ppn_model_from_config(self):
+    model_text_proto = """
+      ssd {
+        feature_extractor {
+          type: 'ssd_resnet_v1_50_ppn'
+          conv_hyperparams {
+            regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+          }
+        }
+        box_coder {
+          mean_stddev_box_coder {
+          }
+        }
+        matcher {
+          bipartite_matcher {
+          }
+        }
+        similarity_calculator {
+          iou_similarity {
+          }
+        }
+        anchor_generator {
+          ssd_anchor_generator {
+            aspect_ratios: 1.0
+          }
+        }
+        image_resizer {
+          fixed_shape_resizer {
+            height: 320
+            width: 320
+          }
+        }
+        box_predictor {
+          weight_shared_convolutional_box_predictor {
+            depth: 1024
+            class_prediction_bias_init: -4.6
+            conv_hyperparams {
+              activation: RELU_6,
+              regularizer {
+                l2_regularizer {
+                  weight: 0.0004
+                }
+              }
+              initializer {
+                variance_scaling_initializer {
+                }
+              }
+            }
+            num_layers_before_predictor: 2
+            kernel_size: 1
+          }
+        }
+        loss {
+          classification_loss {
+            weighted_softmax {
+            }
+          }
+          localization_loss {
+            weighted_l2 {
+            }
+          }
+          classification_weight: 1.0
+          localization_weight: 1.0
+        }
+      }"""
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+
+    for extractor_type, extractor_class in SSD_RESNET_V1_PPN_FEAT_MAPS.items():
+      model_proto.ssd.feature_extractor.type = extractor_type
+      model = model_builder.build(model_proto, is_training=True)
+      self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+      self.assertIsInstance(model._feature_extractor, extractor_class)
+
   def test_create_ssd_mobilenet_v1_model_from_config(self):
     model_text_proto = """
       ssd {
@@ -373,6 +472,160 @@ class ModelBuilderTest(tf.test.TestCase):
     self.assertTrue(model._freeze_batchnorm)
     self.assertTrue(model._inplace_batchnorm_update)
 
+  def test_create_ssd_mobilenet_v1_fpn_model_from_config(self):
+    model_text_proto = """
+      ssd {
+        freeze_batchnorm: true
+        inplace_batchnorm_update: true
+        feature_extractor {
+          type: 'ssd_mobilenet_v1_fpn'
+          conv_hyperparams {
+            regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+          }
+        }
+        box_coder {
+          faster_rcnn_box_coder {
+          }
+        }
+        matcher {
+          argmax_matcher {
+          }
+        }
+        similarity_calculator {
+          iou_similarity {
+          }
+        }
+        anchor_generator {
+          ssd_anchor_generator {
+            aspect_ratios: 1.0
+          }
+        }
+        image_resizer {
+          fixed_shape_resizer {
+            height: 320
+            width: 320
+          }
+        }
+        box_predictor {
+          convolutional_box_predictor {
+            conv_hyperparams {
+              regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+            }
+          }
+        }
+        normalize_loc_loss_by_codesize: true
+        loss {
+          classification_loss {
+            weighted_softmax {
+            }
+          }
+          localization_loss {
+            weighted_smooth_l1 {
+            }
+          }
+        }
+      }"""
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+    model = self.create_model(model_proto)
+    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+    self.assertIsInstance(model._feature_extractor,
+                          SSDMobileNetV1FpnFeatureExtractor)
+    self.assertTrue(model._normalize_loc_loss_by_codesize)
+    self.assertTrue(model._freeze_batchnorm)
+    self.assertTrue(model._inplace_batchnorm_update)
+
+  def test_create_ssd_mobilenet_v1_ppn_model_from_config(self):
+    model_text_proto = """
+      ssd {
+        freeze_batchnorm: true
+        inplace_batchnorm_update: true
+        feature_extractor {
+          type: 'ssd_mobilenet_v1_ppn'
+          conv_hyperparams {
+            regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+          }
+        }
+        box_coder {
+          faster_rcnn_box_coder {
+          }
+        }
+        matcher {
+          argmax_matcher {
+          }
+        }
+        similarity_calculator {
+          iou_similarity {
+          }
+        }
+        anchor_generator {
+          ssd_anchor_generator {
+            aspect_ratios: 1.0
+          }
+        }
+        image_resizer {
+          fixed_shape_resizer {
+            height: 320
+            width: 320
+          }
+        }
+        box_predictor {
+          convolutional_box_predictor {
+            conv_hyperparams {
+              regularizer {
+                l2_regularizer {
+                }
+              }
+              initializer {
+                truncated_normal_initializer {
+                }
+              }
+            }
+          }
+        }
+        normalize_loc_loss_by_codesize: true
+        loss {
+          classification_loss {
+            weighted_softmax {
+            }
+          }
+          localization_loss {
+            weighted_smooth_l1 {
+            }
+          }
+        }
+      }"""
+    model_proto = model_pb2.DetectionModel()
+    text_format.Merge(model_text_proto, model_proto)
+    model = self.create_model(model_proto)
+    self.assertIsInstance(model, ssd_meta_arch.SSDMetaArch)
+    self.assertIsInstance(model._feature_extractor,
+                          SSDMobileNetV1PpnFeatureExtractor)
+    self.assertTrue(model._normalize_loc_loss_by_codesize)
+    self.assertTrue(model._freeze_batchnorm)
+    self.assertTrue(model._inplace_batchnorm_update)
+
   def test_create_ssd_mobilenet_v2_model_from_config(self):
     model_text_proto = """
       ssd {
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index e3a437f0..ce64bfe6 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -90,12 +90,15 @@ def _create_learning_rate(learning_rate_config):
 
   if learning_rate_type == 'exponential_decay_learning_rate':
     config = learning_rate_config.exponential_decay_learning_rate
-    learning_rate = tf.train.exponential_decay(
-        config.initial_learning_rate,
+    learning_rate = learning_schedules.exponential_decay_with_burnin(
         tf.train.get_or_create_global_step(),
+        config.initial_learning_rate,
         config.decay_steps,
         config.decay_factor,
-        staircase=config.staircase, name='learning_rate')
+        burnin_learning_rate=config.burnin_learning_rate,
+        burnin_steps=config.burnin_steps,
+        min_learning_rate=config.min_learning_rate,
+        staircase=config.staircase)
 
   if learning_rate_type == 'manual_step_learning_rate':
     config = learning_rate_config.manual_step_learning_rate
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index aed1fd66..6b389d72 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -802,7 +802,8 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                kernel_size=3,
                class_prediction_bias_init=0.0,
                use_dropout=False,
-               dropout_keep_prob=0.8):
+               dropout_keep_prob=0.8,
+               share_prediction_tower=False):
     """Constructor.
 
     Args:
@@ -822,6 +823,8 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
         conv2d layer before class prediction.
       use_dropout: Whether to apply dropout to class prediction head.
       dropout_keep_prob: Probability of keeping activiations.
+      share_prediction_tower: Whether to share the multi-layer tower between box
+        prediction and class prediction heads.
     """
     super(WeightSharedConvolutionalBoxPredictor, self).__init__(is_training,
                                                                 num_classes)
@@ -833,6 +836,7 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
     self._class_prediction_bias_init = class_prediction_bias_init
     self._use_dropout = use_dropout
     self._dropout_keep_prob = dropout_keep_prob
+    self._share_prediction_tower = share_prediction_tower
 
   def _predict(self, image_features, num_predictions_per_location_list):
     """Computes encoded object locations and corresponding confidences.
@@ -912,6 +916,9 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
           box_encodings_net = image_feature
           class_predictions_net = image_feature
           for i in range(self._num_layers_before_predictor):
+            box_prediction_tower_prefix = (
+                'PredictionTower' if self._share_prediction_tower
+                else 'BoxPredictionTower')
             box_encodings_net = slim.conv2d(
                 box_encodings_net,
                 self._depth,
@@ -920,12 +927,12 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
                 padding='SAME',
                 activation_fn=None,
                 normalizer_fn=(tf.identity if apply_batch_norm else None),
-                scope='BoxPredictionTower/conv2d_{}'.format(i))
+                scope='{}/conv2d_{}'.format(box_prediction_tower_prefix, i))
             if apply_batch_norm:
               box_encodings_net = slim.batch_norm(
                   box_encodings_net,
-                  scope='BoxPredictionTower/conv2d_{}/BatchNorm/feature_{}'.
-                  format(i, feature_index))
+                  scope='{}/conv2d_{}/BatchNorm/feature_{}'.
+                  format(box_prediction_tower_prefix, i, feature_index))
             box_encodings_net = tf.nn.relu6(box_encodings_net)
           box_encodings = slim.conv2d(
               box_encodings_net,
@@ -935,22 +942,25 @@ class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
               normalizer_fn=None,
               scope='BoxPredictor')
 
-          for i in range(self._num_layers_before_predictor):
-            class_predictions_net = slim.conv2d(
-                class_predictions_net,
-                self._depth,
-                [self._kernel_size, self._kernel_size],
-                stride=1,
-                padding='SAME',
-                activation_fn=None,
-                normalizer_fn=(tf.identity if apply_batch_norm else None),
-                scope='ClassPredictionTower/conv2d_{}'.format(i))
-            if apply_batch_norm:
-              class_predictions_net = slim.batch_norm(
+          if self._share_prediction_tower:
+            class_predictions_net = box_encodings_net
+          else:
+            for i in range(self._num_layers_before_predictor):
+              class_predictions_net = slim.conv2d(
                   class_predictions_net,
-                  scope='ClassPredictionTower/conv2d_{}/BatchNorm/feature_{}'
-                  .format(i, feature_index))
-            class_predictions_net = tf.nn.relu6(class_predictions_net)
+                  self._depth,
+                  [self._kernel_size, self._kernel_size],
+                  stride=1,
+                  padding='SAME',
+                  activation_fn=None,
+                  normalizer_fn=(tf.identity if apply_batch_norm else None),
+                  scope='ClassPredictionTower/conv2d_{}'.format(i))
+              if apply_batch_norm:
+                class_predictions_net = slim.batch_norm(
+                    class_predictions_net,
+                    scope='ClassPredictionTower/conv2d_{}/BatchNorm/feature_{}'
+                    .format(i, feature_index))
+              class_predictions_net = tf.nn.relu6(class_predictions_net)
           if self._use_dropout:
             class_predictions_net = slim.dropout(
                 class_predictions_net, keep_prob=self._dropout_keep_prob)
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index 2111f662..a5565620 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -720,6 +720,60 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
          'ClassPredictor/biases')])
     self.assertEqual(expected_variable_set, actual_variable_set)
 
+  def test_predictions_share_weights_share_tower_not_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=2,
+          box_code_size=4,
+          share_prediction_tower=True)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Shared prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/BatchNorm/feature_0/beta'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/BatchNorm/feature_1/beta'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/BatchNorm/feature_0/beta'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/BatchNorm/feature_1/beta'),
+        # Box prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/biases'),
+        # Class prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/biases')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
   def test_get_predictions_with_feature_maps_of_dynamic_shape(
       self):
     image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index d3a06b60..173d6e98 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -224,7 +224,7 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
 
   def testEasyExamplesProduceSmallLossComparedToSigmoidXEntropy(self):
     prediction_tensor = tf.constant([[[_logit(0.97)],
-                                      [_logit(0.90)],
+                                      [_logit(0.91)],
                                       [_logit(0.73)],
                                       [_logit(0.27)],
                                       [_logit(0.09)],
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index 9928443d..37033e0a 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -15,6 +15,8 @@
 
 r"""Convert raw COCO dataset to TFRecord for object_detection.
 
+Please note that this tool creates sharded output files.
+
 Example usage:
     python create_coco_tf_record.py --logtostderr \
       --train_image_dir="${TRAIN_IMAGE_DIR}" \
@@ -33,12 +35,14 @@ import hashlib
 import io
 import json
 import os
+import contextlib2
 import numpy as np
 import PIL.Image
 
 from pycocotools import mask
 import tensorflow as tf
 
+from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 
@@ -188,7 +192,7 @@ def create_tf_example(image,
 
 
 def _create_tf_record_from_coco_annotations(
-    annotations_file, image_dir, output_path, include_masks):
+    annotations_file, image_dir, output_path, include_masks, num_shards):
   """Loads COCO annotation json files and converts to tf.Record format.
 
   Args:
@@ -197,8 +201,12 @@ def _create_tf_record_from_coco_annotations(
     output_path: Path to output tf.Record file.
     include_masks: Whether to include instance segmentations masks
       (PNG encoded) in the result. default: False.
+    num_shards: number of output file shards.
   """
-  with tf.gfile.GFile(annotations_file, 'r') as fid:
+  with contextlib2.ExitStack() as tf_record_close_stack, \
+      tf.gfile.GFile(annotations_file, 'r') as fid:
+    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
+        tf_record_close_stack, output_path, num_shards)
     groundtruth_data = json.load(fid)
     images = groundtruth_data['images']
     category_index = label_map_util.create_category_index(
@@ -222,8 +230,6 @@ def _create_tf_record_from_coco_annotations(
     tf.logging.info('%d images are missing annotations.',
                     missing_annotation_count)
 
-    tf.logging.info('writing to output path: %s', output_path)
-    writer = tf.python_io.TFRecordWriter(output_path)
     total_num_annotations_skipped = 0
     for idx, image in enumerate(images):
       if idx % 100 == 0:
@@ -232,8 +238,8 @@ def _create_tf_record_from_coco_annotations(
       _, tf_example, num_annotations_skipped = create_tf_example(
           image, annotations_list, image_dir, category_index, include_masks)
       total_num_annotations_skipped += num_annotations_skipped
-      writer.write(tf_example.SerializeToString())
-    writer.close()
+      shard_idx = idx % num_shards
+      output_tfrecords[shard_idx].write(tf_example.SerializeToString())
     tf.logging.info('Finished writing, skipped %d annotations.',
                     total_num_annotations_skipped)
 
@@ -256,17 +262,20 @@ def main(_):
       FLAGS.train_annotations_file,
       FLAGS.train_image_dir,
       train_output_path,
-      FLAGS.include_masks)
+      FLAGS.include_masks,
+      num_shards=100)
   _create_tf_record_from_coco_annotations(
       FLAGS.val_annotations_file,
       FLAGS.val_image_dir,
       val_output_path,
-      FLAGS.include_masks)
+      FLAGS.include_masks,
+      num_shards=10)
   _create_tf_record_from_coco_annotations(
       FLAGS.testdev_annotations_file,
       FLAGS.test_image_dir,
       testdev_output_path,
-      FLAGS.include_masks)
+      FLAGS.include_masks,
+      num_shards=100)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record_test.py b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
index 45697eef..b89eeeae 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
@@ -15,6 +15,7 @@
 """Test for create_coco_tf_record.py."""
 
 import io
+import json
 import os
 
 import numpy as np
@@ -183,6 +184,62 @@ class CreateCocoTFRecordTest(tf.test.TestCase):
                          [0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 1, 1],
                          [0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 0, 1, 1, 1, 1]])
 
+  def test_create_sharded_tf_record(self):
+    tmp_dir = self.get_temp_dir()
+    image_paths = ['tmp1_image.jpg', 'tmp2_image.jpg']
+    for image_path in image_paths:
+      image_data = np.random.rand(256, 256, 3)
+      save_path = os.path.join(tmp_dir, image_path)
+      image = PIL.Image.fromarray(image_data, 'RGB')
+      image.save(save_path)
+
+    images = [{
+        'file_name': image_paths[0],
+        'height': 256,
+        'width': 256,
+        'id': 11,
+    }, {
+        'file_name': image_paths[1],
+        'height': 256,
+        'width': 256,
+        'id': 12,
+    }]
+
+    annotations = [{
+        'area': .5,
+        'iscrowd': False,
+        'image_id': 11,
+        'bbox': [64, 64, 128, 128],
+        'category_id': 2,
+        'id': 1000,
+    }]
+
+    category_index = [{
+        'name': 'dog',
+        'id': 1
+    }, {
+        'name': 'cat',
+        'id': 2
+    }, {
+        'name': 'human',
+        'id': 3
+    }]
+    groundtruth_data = {'images': images, 'annotations': annotations,
+                        'categories': category_index}
+    annotation_file = os.path.join(tmp_dir, 'annotation.json')
+    with open(annotation_file, 'w') as annotation_fid:
+      json.dump(groundtruth_data, annotation_fid)
+
+    output_path = os.path.join(tmp_dir, 'out.record')
+    create_coco_tf_record._create_tf_record_from_coco_annotations(
+        annotation_file,
+        tmp_dir,
+        output_path,
+        False,
+        2)
+    self.assertTrue(os.path.exists(output_path + '-00000-of-00002'))
+    self.assertTrue(os.path.exists(output_path + '-00001-of-00002'))
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/export_tflite_ssd_graph.py b/research/object_detection/export_tflite_ssd_graph.py
new file mode 100644
index 00000000..4ed07dc6
--- /dev/null
+++ b/research/object_detection/export_tflite_ssd_graph.py
@@ -0,0 +1,137 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Exports an SSD detection model to use with tf-lite.
+
+Outputs file:
+* A tflite compatible frozen graph - $output_directory/tflite_graph.pb
+
+The exported graph has the following input and output nodes.
+
+Inputs:
+'normalized_input_image_tensor': a float32 tensor of shape
+[1, height, width, 3] containing the normalized input image. Note that the
+height and width must be compatible with the height and width configured in
+the fixed_shape_image resizer options in the pipeline config proto.
+
+In floating point Mobilenet model, 'normalized_image_tensor' has values
+between [-1,1). This typically means mapping each pixel (linearly)
+to a value between [-1, 1]. Input image
+values between 0 and 255 are scaled by (1/128.0) and then a value of
+-1 is added to them to ensure the range is [-1,1).
+In quantized Mobilenet model, 'normalized_image_tensor' has values between [0,
+255].
+In general, see the `preprocess` function defined in the feature extractor class
+in the object_detection/models directory.
+
+Outputs:
+If add_postprocessing_op is true: frozen graph adds a
+  TFLite_Detection_PostProcess custom op node has four outputs:
+  detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box
+  locations
+  detection_scores: a float32 tensor of shape [1, num_boxes]
+  with class scores
+  detection_classes: a float32 tensor of shape [1, num_boxes]
+  with class indices
+  num_boxes: a float32 tensor of size 1 containing the number of detected boxes
+else:
+  the graph has two outputs:
+   'raw_outputs/box_encodings': a float32 tensor of shape [1, num_anchors, 4]
+    containing the encoded box predictions.
+   'raw_outputs/class_predictions': a float32 tensor of shape
+    [1, num_anchors, num_classes] containing the class scores for each anchor
+    after applying score conversion.
+
+Example Usage:
+--------------
+python object_detection/export_tflite_ssd_graph \
+    --pipeline_config_path path/to/ssd_mobilenet.config \
+    --trained_checkpoint_prefix path/to/model.ckpt \
+    --output_directory path/to/exported_model_directory
+
+The expected output would be in the directory
+path/to/exported_model_directory (which is created if it does not exist)
+with contents:
+ - tflite_graph.pbtxt
+ - tflite_graph.pb
+Config overrides (see the `config_override` flag) are text protobufs
+(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override
+certain fields in the provided pipeline_config_path.  These are useful for
+making small changes to the inference graph that differ from the training or
+eval config.
+
+Example Usage (in which we change the NMS iou_threshold to be 0.5 and
+NMS score_threshold to be 0.0):
+python object_detection/export_tflite_ssd_graph \
+    --pipeline_config_path path/to/ssd_mobilenet.config \
+    --trained_checkpoint_prefix path/to/model.ckpt \
+    --output_directory path/to/exported_model_directory
+    --config_override " \
+            model{ \
+            ssd{ \
+              post_processing { \
+                batch_non_max_suppression { \
+                        score_threshold: 0.0 \
+                        iou_threshold: 0.5 \
+                } \
+             } \
+          } \
+       } \
+       "
+"""
+
+import tensorflow as tf
+from google.protobuf import text_format
+from object_detection import export_tflite_ssd_graph_lib
+from object_detection.protos import pipeline_pb2
+
+flags = tf.app.flags
+flags.DEFINE_string('output_directory', None, 'Path to write outputs.')
+flags.DEFINE_string(
+    'pipeline_config_path', None,
+    'Path to a pipeline_pb2.TrainEvalPipelineConfig config '
+    'file.')
+flags.DEFINE_string('trained_checkpoint_prefix', None, 'Checkpoint prefix.')
+flags.DEFINE_integer('max_detections', 10,
+                     'Maximum number of detections (boxes) to show.')
+flags.DEFINE_integer('max_classes_per_detection', 1,
+                     'Number of classes to display per detection box.')
+flags.DEFINE_bool('add_postprocessing_op', True,
+                  'Add TFLite custom op for postprocessing to the graph.')
+flags.DEFINE_string(
+    'config_override', '', 'pipeline_pb2.TrainEvalPipelineConfig '
+    'text proto to override pipeline_config_path.')
+
+FLAGS = flags.FLAGS
+
+
+def main(argv):
+  del argv  # Unused.
+  flags.mark_flag_as_required('output_directory')
+  flags.mark_flag_as_required('pipeline_config_path')
+  flags.mark_flag_as_required('trained_checkpoint_prefix')
+
+  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+
+  with tf.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
+    text_format.Merge(f.read(), pipeline_config)
+  text_format.Merge(FLAGS.config_override, pipeline_config)
+  export_tflite_ssd_graph_lib.export_tflite_graph(
+      pipeline_config, FLAGS.trained_checkpoint_prefix, FLAGS.output_directory,
+      FLAGS.add_postprocessing_op, FLAGS.max_detections,
+      FLAGS.max_classes_per_detection)
+
+
+if __name__ == '__main__':
+  tf.app.run(main)
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
new file mode 100644
index 00000000..2e8a00cd
--- /dev/null
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -0,0 +1,277 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Exports an SSD detection model to use with tf-lite.
+
+See export_tflite_ssd_graph.py for usage.
+"""
+import os
+import tempfile
+import numpy as np
+import tensorflow as tf
+from tensorflow.core.framework import attr_value_pb2
+from tensorflow.core.protobuf import saver_pb2
+from tensorflow.tools.graph_transforms import TransformGraph
+from object_detection import exporter
+from object_detection.builders import graph_rewriter_builder
+from object_detection.builders import model_builder
+from object_detection.builders import post_processing_builder
+from object_detection.core import box_list
+
+_DEFAULT_NUM_CHANNELS = 3
+_DEFAULT_NUM_COORD_BOX = 4
+
+
+def get_const_center_size_encoded_anchors(anchors):
+  """Exports center-size encoded anchors as a constant tensor.
+
+  Args:
+    anchors: a float32 tensor of shape [num_anchors, 4] containing the anchor
+      boxes
+
+  Returns:
+    encoded_anchors: a float32 constant tensor of shape [4, num_anchors]
+    containing the anchor boxes.
+  """
+  anchor_boxlist = box_list.BoxList(anchors)
+  y, x, h, w = anchor_boxlist.get_center_coordinates_and_sizes()
+  num_anchors = y.get_shape().as_list()
+
+  with tf.Session() as sess:
+    y_out, x_out, h_out, w_out = sess.run([y, x, h, w])
+  encoded_anchors = tf.constant(
+      np.transpose(np.stack((y_out, x_out, h_out, w_out))),
+      dtype=tf.float32,
+      shape=[num_anchors[0], _DEFAULT_NUM_COORD_BOX],
+      name='anchors')
+  return encoded_anchors
+
+
+def append_postprocessing_op(frozen_graph_def, max_detections,
+                             max_classes_per_detection, nms_score_threshold,
+                             nms_iou_threshold, num_classes, scale_values):
+  """Appends postprocessing custom op.
+
+  Args:
+    frozen_graph_def: Frozen GraphDef for SSD model after freezing the
+      checkpoint
+    max_detections: Maximum number of detections (boxes) to show
+    max_classes_per_detection: Number of classes to display per detection
+    nms_score_threshold: Score threshold used in Non-maximal suppression in
+      post-processing
+    nms_iou_threshold: Intersection-over-union threshold used in Non-maximal
+      suppression in post-processing
+    num_classes: number of classes in SSD detector
+    scale_values: scale values is a dict with following key-value pairs
+      {y_scale: 10, x_scale: 10, h_scale: 5, w_scale: 5} that are used in decode
+      centersize boxes
+
+  Returns:
+    transformed_graph_def: Frozen GraphDef with postprocessing custom op
+    appended
+    TFLite_Detection_PostProcess custom op node has four outputs:
+    detection_boxes: a float32 tensor of shape [1, num_boxes, 4] with box
+    locations
+    detection_scores: a float32 tensor of shape [1, num_boxes]
+    with class scores
+    detection_classes: a float32 tensor of shape [1, num_boxes]
+    with class indices
+    num_boxes: a float32 tensor of size 1 containing the number of detected
+    boxes
+  """
+  new_output = frozen_graph_def.node.add()
+  new_output.op = 'TFLite_Detection_PostProcess'
+  new_output.name = 'TFLite_Detection_PostProcess'
+  new_output.attr['_output_quantized'].CopyFrom(
+      attr_value_pb2.AttrValue(b=True))
+  new_output.attr['max_detections'].CopyFrom(
+      attr_value_pb2.AttrValue(i=max_detections))
+  new_output.attr['max_classes_per_detection'].CopyFrom(
+      attr_value_pb2.AttrValue(i=max_classes_per_detection))
+  new_output.attr['nms_score_threshold'].CopyFrom(
+      attr_value_pb2.AttrValue(f=nms_score_threshold.pop()))
+  new_output.attr['nms_iou_threshold'].CopyFrom(
+      attr_value_pb2.AttrValue(f=nms_iou_threshold.pop()))
+  new_output.attr['num_classes'].CopyFrom(
+      attr_value_pb2.AttrValue(i=num_classes))
+
+  new_output.attr['y_scale'].CopyFrom(
+      attr_value_pb2.AttrValue(f=scale_values['y_scale'].pop()))
+  new_output.attr['x_scale'].CopyFrom(
+      attr_value_pb2.AttrValue(f=scale_values['x_scale'].pop()))
+  new_output.attr['h_scale'].CopyFrom(
+      attr_value_pb2.AttrValue(f=scale_values['h_scale'].pop()))
+  new_output.attr['w_scale'].CopyFrom(
+      attr_value_pb2.AttrValue(f=scale_values['w_scale'].pop()))
+
+  new_output.input.extend(
+      ['raw_outputs/box_encodings', 'raw_outputs/class_predictions', 'anchors'])
+  # Transform the graph to append new postprocessing op
+  input_names = []
+  output_names = ['TFLite_Detection_PostProcess']
+  transforms = ['strip_unused_nodes']
+  transformed_graph_def = TransformGraph(frozen_graph_def, input_names,
+                                         output_names, transforms)
+  return transformed_graph_def
+
+
+def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
+                        add_postprocessing_op, max_detections,
+                        max_classes_per_detection):
+  """Exports a tflite compatible graph and anchors for ssd detection model.
+
+  Anchors are written to a tensor and tflite compatible graph
+  is written to output_dir/tflite_graph.pb.
+
+  Args:
+    pipeline_config: a pipeline.proto object containing the configuration for
+      SSD model to export.
+    trained_checkpoint_prefix: a file prefix for the checkpoint containing the
+      trained parameters of the SSD model.
+    output_dir: A directory to write the tflite graph and anchor file to.
+    add_postprocessing_op: If add_postprocessing_op is true: frozen graph adds a
+      TFLite_Detection_PostProcess custom op
+    max_detections: Maximum number of detections (boxes) to show
+    max_classes_per_detection: Number of classes to display per detection
+
+
+  Raises:
+    ValueError: if the pipeline config contains models other than ssd or uses an
+      fixed_shape_resizer and provides a shape as well.
+  """
+  tf.gfile.MakeDirs(output_dir)
+  if pipeline_config.model.WhichOneof('model') != 'ssd':
+    raise ValueError('Only ssd models are supported in tflite. '
+                     'Found {} in config'.format(
+                         pipeline_config.model.WhichOneof('model')))
+
+  num_classes = pipeline_config.model.ssd.num_classes
+  nms_score_threshold = {
+      pipeline_config.model.ssd.post_processing.batch_non_max_suppression.
+      score_threshold
+  }
+  nms_iou_threshold = {
+      pipeline_config.model.ssd.post_processing.batch_non_max_suppression.
+      iou_threshold
+  }
+  scale_values = {}
+  scale_values['y_scale'] = {
+      pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale
+  }
+  scale_values['x_scale'] = {
+      pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale
+  }
+  scale_values['h_scale'] = {
+      pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale
+  }
+  scale_values['w_scale'] = {
+      pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale
+  }
+
+  image_resizer_config = pipeline_config.model.ssd.image_resizer
+  image_resizer = image_resizer_config.WhichOneof('image_resizer_oneof')
+  num_channels = _DEFAULT_NUM_CHANNELS
+  if image_resizer == 'fixed_shape_resizer':
+    height = image_resizer_config.fixed_shape_resizer.height
+    width = image_resizer_config.fixed_shape_resizer.width
+    if image_resizer_config.fixed_shape_resizer.convert_to_grayscale:
+      num_channels = 1
+    shape = [1, height, width, num_channels]
+  else:
+    raise ValueError(
+        'Only fixed_shape_resizer'
+        'is supported with tflite. Found {}'.format(
+            image_resizer_config.WhichOneof('image_resizer_oneof')))
+
+  image = tf.placeholder(
+      tf.float32, shape=shape, name='normalized_input_image_tensor')
+
+  detection_model = model_builder.build(
+      pipeline_config.model, is_training=False)
+  predicted_tensors = detection_model.predict(image, true_image_shapes=None)
+  # The score conversion occurs before the post-processing custom op
+  _, score_conversion_fn = post_processing_builder.build(
+      pipeline_config.model.ssd.post_processing)
+  class_predictions = score_conversion_fn(
+      predicted_tensors['class_predictions_with_background'])
+
+  with tf.name_scope('raw_outputs'):
+    # 'raw_outputs/box_encodings': a float32 tensor of shape [1, num_anchors, 4]
+    #  containing the encoded box predictions. Note that these are raw
+    #  predictions and no Non-Max suppression is applied on them and
+    #  no decode center size boxes is applied to them.
+    tf.identity(predicted_tensors['box_encodings'], name='box_encodings')
+    # 'raw_outputs/class_predictions': a float32 tensor of shape
+    #  [1, num_anchors, num_classes] containing the class scores for each anchor
+    #  after applying score conversion.
+    tf.identity(class_predictions, name='class_predictions')
+  # 'anchors': a float32 tensor of shape
+  #   [4, num_anchors] containing the anchors as a constant node.
+  tf.identity(
+      get_const_center_size_encoded_anchors(predicted_tensors['anchors']),
+      name='anchors')
+
+  # Add global step to the graph, so we know the training step number when we
+  # evaluate the model.
+  tf.train.get_or_create_global_step()
+
+  # graph rewriter
+  if pipeline_config.HasField('graph_rewriter'):
+    graph_rewriter_config = pipeline_config.graph_rewriter
+    graph_rewriter_fn = graph_rewriter_builder.build(
+        graph_rewriter_config, is_training=False)
+    graph_rewriter_fn()
+
+  # freeze the graph
+  saver_kwargs = {}
+  if pipeline_config.eval_config.use_moving_averages:
+    saver_kwargs['write_version'] = saver_pb2.SaverDef.V1
+    moving_average_checkpoint = tempfile.NamedTemporaryFile()
+    exporter.replace_variable_values_with_moving_averages(
+        tf.get_default_graph(), trained_checkpoint_prefix,
+        moving_average_checkpoint.name)
+    checkpoint_to_use = moving_average_checkpoint.name
+  else:
+    checkpoint_to_use = trained_checkpoint_prefix
+
+  saver = tf.train.Saver(**saver_kwargs)
+  input_saver_def = saver.as_saver_def()
+  frozen_graph_def = exporter.freeze_graph_with_def_protos(
+      input_graph_def=tf.get_default_graph().as_graph_def(),
+      input_saver_def=input_saver_def,
+      input_checkpoint=checkpoint_to_use,
+      output_node_names=','.join([
+          'raw_outputs/box_encodings', 'raw_outputs/class_predictions',
+          'anchors'
+      ]),
+      restore_op_name='save/restore_all',
+      filename_tensor_name='save/Const:0',
+      clear_devices=True,
+      initializer_nodes='')
+
+  # Add new operation to do post processing in a custom op (TF Lite only)
+  if add_postprocessing_op:
+    transformed_graph_def = append_postprocessing_op(
+        frozen_graph_def, max_detections, max_classes_per_detection,
+        nms_score_threshold, nms_iou_threshold, num_classes, scale_values)
+  else:
+    # Return frozen without adding post-processing custom op
+    transformed_graph_def = frozen_graph_def
+
+  binary_graph = os.path.join(output_dir, 'tflite_graph.pb')
+  with tf.gfile.GFile(binary_graph, 'wb') as f:
+    f.write(transformed_graph_def.SerializeToString())
+  txt_graph = os.path.join(output_dir, 'tflite_graph.pbtxt')
+  with tf.gfile.GFile(txt_graph, 'w') as f:
+    f.write(str(transformed_graph_def))
diff --git a/research/object_detection/export_tflite_ssd_graph_lib_test.py b/research/object_detection/export_tflite_ssd_graph_lib_test.py
new file mode 100644
index 00000000..51d25f56
--- /dev/null
+++ b/research/object_detection/export_tflite_ssd_graph_lib_test.py
@@ -0,0 +1,272 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.export_tflite_ssd_graph."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+import os
+import numpy as np
+import six
+import tensorflow as tf
+from object_detection import export_tflite_ssd_graph_lib
+from object_detection.builders import graph_rewriter_builder
+from object_detection.builders import model_builder
+from object_detection.core import model
+from object_detection.protos import graph_rewriter_pb2
+from object_detection.protos import pipeline_pb2
+from object_detection.protos import post_processing_pb2
+
+if six.PY2:
+  import mock  # pylint: disable=g-import-not-at-top
+else:
+  from unittest import mock  # pylint: disable=g-import-not-at-top
+
+
+class FakeModel(model.DetectionModel):
+
+  def __init__(self, add_detection_masks=False):
+    self._add_detection_masks = add_detection_masks
+
+  def preprocess(self, inputs):
+    pass
+
+  def predict(self, preprocessed_inputs, true_image_shapes):
+    features = tf.contrib.slim.conv2d(preprocessed_inputs, 3, 1)
+    with tf.control_dependencies([features]):
+      prediction_tensors = {
+          'box_encodings':
+              tf.constant([[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]],
+                          tf.float32),
+          'class_predictions_with_background':
+              tf.constant([[[0.7, 0.6], [0.9, 0.0]]], tf.float32),
+      }
+    with tf.control_dependencies(
+        [tf.convert_to_tensor(features.get_shape().as_list()[1:3])]):
+      prediction_tensors['anchors'] = tf.constant(
+          [[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 1.0, 1.0]], tf.float32)
+    return prediction_tensors
+
+  def postprocess(self, prediction_tensors, true_image_shapes):
+    pass
+
+  def restore_map(self, checkpoint_path, from_detection_checkpoint):
+    pass
+
+  def loss(self, prediction_dict, true_image_shapes):
+    pass
+
+
+class ExportTfliteGraphTest(tf.test.TestCase):
+
+  def _save_checkpoint_from_mock_model(self,
+                                       checkpoint_path,
+                                       use_moving_averages,
+                                       quantize=False,
+                                       num_channels=3):
+    g = tf.Graph()
+    with g.as_default():
+      mock_model = FakeModel()
+      inputs = tf.placeholder(tf.float32, shape=[1, 10, 10, num_channels])
+      mock_model.predict(inputs, true_image_shapes=None)
+      if use_moving_averages:
+        tf.train.ExponentialMovingAverage(0.0).apply()
+      tf.train.get_or_create_global_step()
+      if quantize:
+        graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
+        graph_rewriter_config.quantization.delay = 500000
+        graph_rewriter_fn = graph_rewriter_builder.build(
+            graph_rewriter_config, is_training=False)
+        graph_rewriter_fn()
+
+      saver = tf.train.Saver()
+      init = tf.global_variables_initializer()
+      with self.test_session() as sess:
+        sess.run(init)
+        saver.save(sess, checkpoint_path)
+
+  def _assert_quant_vars_exists(self, tflite_graph_file):
+    with tf.gfile.Open(tflite_graph_file) as f:
+      graph_string = f.read()
+      print(graph_string)
+      self.assertTrue('quant' in graph_string)
+
+  def _import_graph_and_run_inference(self, tflite_graph_file, num_channels=3):
+    """Imports a tflite graph, runs single inference and returns outputs."""
+    graph = tf.Graph()
+    with graph.as_default():
+      graph_def = tf.GraphDef()
+      with tf.gfile.Open(tflite_graph_file) as f:
+        graph_def.ParseFromString(f.read())
+      tf.import_graph_def(graph_def, name='')
+      input_tensor = graph.get_tensor_by_name('normalized_input_image_tensor:0')
+      box_encodings = graph.get_tensor_by_name('raw_outputs/box_encodings:0')
+      class_predictions = graph.get_tensor_by_name(
+          'raw_outputs/class_predictions:0')
+      with self.test_session(graph) as sess:
+        [box_encodings_np, class_predictions_np] = sess.run(
+            [box_encodings, class_predictions],
+            feed_dict={input_tensor: np.random.rand(1, 10, 10, num_channels)})
+    return box_encodings_np, class_predictions_np
+
+  def _export_graph(self, pipeline_config, num_channels=3):
+    """Exports a tflite graph and an anchor file."""
+    output_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(output_dir, 'model.ckpt')
+    tflite_graph_file = os.path.join(output_dir, 'tflite_graph.pb')
+
+    quantize = pipeline_config.HasField('graph_rewriter')
+    self._save_checkpoint_from_mock_model(
+        trained_checkpoint_prefix,
+        use_moving_averages=pipeline_config.eval_config.use_moving_averages,
+        quantize=quantize,
+        num_channels=num_channels)
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel()
+
+      with tf.Graph().as_default():
+        export_tflite_ssd_graph_lib.export_tflite_graph(
+            pipeline_config=pipeline_config,
+            trained_checkpoint_prefix=trained_checkpoint_prefix,
+            output_dir=output_dir,
+            add_postprocessing_op=False,
+            max_detections=10,
+            max_classes_per_detection=1)
+    return tflite_graph_file
+
+  def test_export_tflite_graph_with_moving_averages(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = True
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+
+    (box_encodings_np, class_predictions_np
+    ) = self._import_graph_and_run_inference(tflite_graph_file)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np, [[[0.7, 0.6], [0.9, 0.0]]])
+
+  def test_export_tflite_graph_without_moving_averages(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    (box_encodings_np, class_predictions_np
+    ) = self._import_graph_and_run_inference(tflite_graph_file)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np, [[[0.7, 0.6], [0.9, 0.0]]])
+
+  def test_export_tflite_graph_grayscale(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    (pipeline_config.model.ssd.image_resizer.fixed_shape_resizer
+    ).convert_to_grayscale = True
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config, num_channels=1)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    (box_encodings_np,
+     class_predictions_np) = self._import_graph_and_run_inference(
+         tflite_graph_file, num_channels=1)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np, [[[0.7, 0.6], [0.9, 0.0]]])
+
+  def test_export_tflite_graph_with_quantization(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.graph_rewriter.quantization.delay = 500000
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    self._assert_quant_vars_exists(tflite_graph_file)
+    (box_encodings_np, class_predictions_np
+    ) = self._import_graph_and_run_inference(tflite_graph_file)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np, [[[0.7, 0.6], [0.9, 0.0]]])
+
+  def test_export_tflite_graph_with_softmax_score_conversion(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.post_processing.score_converter = (
+        post_processing_pb2.PostProcessing.SOFTMAX)
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    (box_encodings_np, class_predictions_np
+    ) = self._import_graph_and_run_inference(tflite_graph_file)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np,
+                        [[[0.524979, 0.475021], [0.710949, 0.28905]]])
+
+  def test_export_tflite_graph_with_sigmoid_score_conversion(self):
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    pipeline_config.eval_config.use_moving_averages = False
+    pipeline_config.model.ssd.post_processing.score_converter = (
+        post_processing_pb2.PostProcessing.SIGMOID)
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = 10
+    pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = 10
+    pipeline_config.model.ssd.num_classes = 2
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.y_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.x_scale = 10.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.height_scale = 5.0
+    pipeline_config.model.ssd.box_coder.faster_rcnn_box_coder.width_scale = 5.0
+    tflite_graph_file = self._export_graph(pipeline_config)
+    self.assertTrue(os.path.exists(tflite_graph_file))
+    (box_encodings_np, class_predictions_np
+    ) = self._import_graph_and_run_inference(tflite_graph_file)
+    self.assertAllClose(box_encodings_np,
+                        [[[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.8, 0.8]]])
+    self.assertAllClose(class_predictions_np,
+                        [[[0.668188, 0.645656], [0.710949, 0.5]]])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/g3doc/evaluation_protocols.md b/research/object_detection/g3doc/evaluation_protocols.md
index ec960058..aa6024c4 100644
--- a/research/object_detection/g3doc/evaluation_protocols.md
+++ b/research/object_detection/g3doc/evaluation_protocols.md
@@ -42,6 +42,27 @@ union based on the object masks instead of object boxes.
 Similar to the weighted pascal voc 2010 detection metric, but computes the
 intersection over union based on the object masks instead of object boxes.
 
+
+## COCO detection metrics
+
+`EvalConfig.metrics_set='coco_detection_metrics'`
+
+The COCO metrics are the official detection metrics used to score the
+[COCO competition](http://cocodataset.org/) and are similar to Pascal VOC
+metrics but have a slightly different implementation and report additional
+statistics such as mAP at IOU thresholds of .5:.95, and precision/recall
+statistics for small, medium, and large objects.
+See the
+[pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI)
+repository for more details.
+
+## COCO mask metrics
+
+`EvalConfig.metrics_set='coco_mask_metrics'`
+
+Similar to the COCO detection metrics, but computes the
+intersection over union based on the object masks instead of object boxes.
+
 ## Open Images V2 detection metric
 
 `EvalConfig.metrics_set='open_images_V2_detection_metrics'`
diff --git a/research/object_detection/g3doc/preparing_inputs.md b/research/object_detection/g3doc/preparing_inputs.md
index d9d290d2..cd51e109 100644
--- a/research/object_detection/g3doc/preparing_inputs.md
+++ b/research/object_detection/g3doc/preparing_inputs.md
@@ -50,8 +50,10 @@ python object_detection/dataset_tools/create_pet_tf_record.py \
     --output_dir=`pwd`
 ```
 
-You should end up with two TFRecord files named `pet_train.record` and
-`pet_val.record` in the `tensorflow/models/research/` directory.
+You should end up with two 10-sharded TFRecord files named
+`pet_faces_train.record-?????-of-00010` and
+`pet_faces_val.record-?????-of-00010` in the `tensorflow/models/research/`
+directory.
 
 The label map for the Pet dataset can be found at
 `object_detection/data/pet_label_map.pbtxt`.
diff --git a/research/object_detection/g3doc/using_your_own_dataset.md b/research/object_detection/g3doc/using_your_own_dataset.md
index 397e394c..5180b4cc 100644
--- a/research/object_detection/g3doc/using_your_own_dataset.md
+++ b/research/object_detection/g3doc/using_your_own_dataset.md
@@ -1,5 +1,7 @@
 # Preparing Inputs
 
+[TOC]
+
 To use your own dataset in Tensorflow Object Detection API, you must convert it
 into the [TFRecord file format](https://www.tensorflow.org/api_guides/python/python_io#tfrecords_format_details).
 This document outlines how to write a script to generate the TFRecord file.
@@ -86,7 +88,7 @@ def create_cat_tf_example(encoded_cat_image_data):
   return tf_example
 ```
 
-## Conversion Script Outline
+## Conversion Script Outline {#conversion-script-outline}
 
 A typical conversion script will look like the following:
 
@@ -159,3 +161,49 @@ currently unused by the API and are optional.
 Note: Please refer to the section on [Running an Instance Segmentation
 Model](instance_segmentation.md) for instructions on how to configure a model
 that predicts masks in addition to object bounding boxes.
+
+## Sharding datasets
+
+When you have more than a few thousand examples, it is beneficial to shard your
+dataset into multiple files:
+
+*   tf.data.Dataset API can read input examples in parallel improving
+    throughput.
+*   tf.data.Dataset API can shuffle the examples better with sharded files which
+    improves performance of the model slightly.
+
+Instead of writing all tf.Example protos to a single file as shown in
+[conversion script outline](#conversion-script-outline), use the snippet below.
+
+```python
+import contextlib2
+from google3.third_party.tensorflow_models.object_detection.dataset_tools import tf_record_creation_util
+
+num_shards=10
+output_filebase='/path/to/train_dataset.record'
+
+with contextlib2.ExitStack() as tf_record_close_stack:
+  output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
+      tf_record_close_stack, output_filebase, num_shards)
+  for index, example in examples:
+    tf_example = create_tf_example(example)
+    output_shard_index = index % num_shards
+    output_tfrecords[output_shard_index].write(tf_example.SerializeToString())
+```
+
+This will produce the following output files
+
+```bash
+/path/to/train_dataset.record-00000-00010
+/path/to/train_dataset.record-00001-00010
+...
+/path/to/train_dataset.record-00009-00010
+```
+
+which can then be used in the config file as below.
+
+```bash
+tf_record_input_reader {
+  input_path: "/path/to/train_dataset.record-?????-of-00010"
+}
+```
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 5afaf851..91de994f 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -132,6 +132,7 @@ def transform_input_data(tensor_dict,
     merged_boxes, merged_classes, _ = util_ops.merge_boxes_with_multiple_labels(
         tensor_dict[fields.InputDataFields.groundtruth_boxes],
         zero_indexed_groundtruth_classes, num_classes)
+    merged_classes = tf.cast(merged_classes, tf.float32)
     tensor_dict[fields.InputDataFields.groundtruth_boxes] = merged_boxes
     tensor_dict[fields.InputDataFields.groundtruth_classes] = merged_classes
 
diff --git a/research/object_detection/legacy/__init__.py b/research/object_detection/legacy/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/object_detection/eval.py b/research/object_detection/legacy/eval.py
similarity index 97%
rename from research/object_detection/eval.py
rename to research/object_detection/legacy/eval.py
index 2a8c7153..13e4cf09 100644
--- a/research/object_detection/eval.py
+++ b/research/object_detection/legacy/eval.py
@@ -47,10 +47,10 @@ import functools
 import os
 import tensorflow as tf
 
-from object_detection import evaluator
 from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
+from object_detection.legacy import evaluator
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
 
@@ -80,6 +80,7 @@ flags.DEFINE_boolean('run_once', False, 'Option to only run a single pass of '
 FLAGS = flags.FLAGS
 
 
+@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')
 def main(unused_argv):
   assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.'
   assert FLAGS.eval_dir, '`eval_dir` is missing.'
diff --git a/research/object_detection/evaluator.py b/research/object_detection/legacy/evaluator.py
similarity index 100%
rename from research/object_detection/evaluator.py
rename to research/object_detection/legacy/evaluator.py
diff --git a/research/object_detection/train.py b/research/object_detection/legacy/train.py
similarity index 98%
rename from research/object_detection/train.py
rename to research/object_detection/legacy/train.py
index a4a568fc..33b9a9cf 100644
--- a/research/object_detection/train.py
+++ b/research/object_detection/legacy/train.py
@@ -46,10 +46,10 @@ import json
 import os
 import tensorflow as tf
 
-from object_detection import trainer
 from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
+from object_detection.legacy import trainer
 from object_detection.utils import config_util
 
 tf.logging.set_verbosity(tf.logging.INFO)
@@ -84,6 +84,7 @@ flags.DEFINE_string('model_config_path', '',
 FLAGS = flags.FLAGS
 
 
+@tf.contrib.framework.deprecated(None, 'Use object_detection/model_main.py.')
 def main(_):
   assert FLAGS.train_dir, '`train_dir` is missing.'
   if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
diff --git a/research/object_detection/trainer.py b/research/object_detection/legacy/trainer.py
similarity index 100%
rename from research/object_detection/trainer.py
rename to research/object_detection/legacy/trainer.py
diff --git a/research/object_detection/trainer_test.py b/research/object_detection/legacy/trainer_test.py
similarity index 99%
rename from research/object_detection/trainer_test.py
rename to research/object_detection/legacy/trainer_test.py
index a4e3e4ef..82e77274 100644
--- a/research/object_detection/trainer_test.py
+++ b/research/object_detection/legacy/trainer_test.py
@@ -19,10 +19,10 @@ import tensorflow as tf
 
 from google.protobuf import text_format
 
-from object_detection import trainer
 from object_detection.core import losses
 from object_detection.core import model
 from object_detection.core import standard_fields as fields
+from object_detection.legacy import trainer
 from object_detection.protos import train_pb2
 
 
diff --git a/research/object_detection/metrics/offline_eval_map_corloc.py b/research/object_detection/metrics/offline_eval_map_corloc.py
index b7b1eb69..ff2efbaf 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc.py
@@ -36,8 +36,8 @@ import os
 import re
 import tensorflow as tf
 
-from object_detection import evaluator
 from object_detection.core import standard_fields
+from object_detection.legacy import evaluator
 from object_detection.metrics import tf_example_parser
 from object_detection.utils import config_util
 from object_detection.utils import label_map_util
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 2c72eeb4..899a4280 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -223,3 +223,69 @@ def fpn_top_down_feature_maps(image_features, depth, scope=None):
         output_feature_map_keys.append('top_down_%s' % image_features[level][0])
       return collections.OrderedDict(
           reversed(zip(output_feature_map_keys, output_feature_maps_list)))
+
+
+def pooling_pyramid_feature_maps(base_feature_map_depth, num_layers,
+                                 image_features):
+  """Generates pooling pyramid feature maps.
+
+  The pooling pyramid feature maps is motivated by
+  multi_resolution_feature_maps. The main difference are that it is simpler and
+  reduces the number of free parameters.
+
+  More specifically:
+   - Instead of using convolutions to shrink the feature map, it uses max
+     pooling, therefore totally gets rid of the parameters in convolution.
+   - By pooling feature from larger map up to a single cell, it generates
+     features in the same feature space.
+   - Instead of independently making box predictions from individual maps, it
+     shares the same classifier across different feature maps, therefore reduces
+     the "mis-calibration" across different scales.
+
+  See go/ppn-detection for more details.
+
+  Args:
+    base_feature_map_depth: Depth of the base feature before the max pooling.
+    num_layers: Number of layers used to make predictions. They are pooled
+      from the base feature.
+    image_features: A dictionary of handles to activation tensors from the
+      feature extractor.
+
+  Returns:
+    feature_maps: an OrderedDict mapping keys (feature map names) to
+      tensors where each tensor has shape [batch, height_i, width_i, depth_i].
+  Raises:
+    ValueError: image_features does not contain exactly one entry
+  """
+  if len(image_features) != 1:
+    raise ValueError('image_features should be a dictionary of length 1.')
+  image_features = image_features[image_features.keys()[0]]
+
+  feature_map_keys = []
+  feature_maps = []
+  feature_map_key = 'Base_Conv2d_1x1_%d' % base_feature_map_depth
+  if base_feature_map_depth > 0:
+    image_features = slim.conv2d(
+        image_features,
+        base_feature_map_depth,
+        [1, 1],  # kernel size
+        padding='SAME', stride=1, scope=feature_map_key)
+    # Add a 1x1 max-pooling node (a no op node) immediately after the conv2d for
+    # TPU v1 compatibility.  Without the following dummy op, TPU runtime
+    # compiler will combine the convolution with one max-pooling below into a
+    # single cycle, so getting the conv2d feature becomes impossible.
+    image_features = slim.max_pool2d(
+        image_features, [1, 1], padding='SAME', stride=1, scope=feature_map_key)
+  feature_map_keys.append(feature_map_key)
+  feature_maps.append(image_features)
+  feature_map = image_features
+  with slim.arg_scope([slim.max_pool2d], padding='SAME', stride=2):
+    for i in range(num_layers - 1):
+      feature_map_key = 'MaxPool2d_%d_2x2' % i
+      feature_map = slim.max_pool2d(
+          feature_map, [2, 2], padding='SAME', scope=feature_map_key)
+      feature_map_keys.append(feature_map_key)
+      feature_maps.append(feature_map)
+  return collections.OrderedDict(
+      [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])
+
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
new file mode 100644
index 00000000..a52b7572
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -0,0 +1,101 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSD MobilenetV1 FPN Feature Extractor."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import context_manager
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets import mobilenet_v1
+
+slim = tf.contrib.slim
+
+
+class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD Feature Extractor using MobilenetV1 FPN features."""
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    with tf.variable_scope('MobilenetV1',
+                           reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(
+          mobilenet_v1.mobilenet_v1_arg_scope(
+              is_training=None, regularize_depthwise=True)):
+        with (slim.arg_scope(self._conv_hyperparams_fn())
+              if self._override_base_feature_extractor_hyperparams
+              else context_manager.IdentityContextManager()):
+          _, image_features = mobilenet_v1.mobilenet_v1_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='Conv2d_13_pointwise',
+              min_depth=self._min_depth,
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+
+      depth_fn = lambda d: max(int(d * self._depth_multiplier), self._min_depth)
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        with tf.variable_scope('fpn', reuse=self._reuse_weights):
+          fpn_features = feature_map_generators.fpn_top_down_feature_maps(
+              [(key, image_features[key])
+               for key in ['Conv2d_5_pointwise', 'Conv2d_11_pointwise',
+                           'Conv2d_13_pointwise']],
+              depth=depth_fn(256))
+          last_feature_map = fpn_features['top_down_Conv2d_13_pointwise']
+          coarse_features = {}
+          for i in range(14, 16):
+            last_feature_map = slim.conv2d(
+                last_feature_map,
+                num_outputs=depth_fn(256),
+                kernel_size=[3, 3],
+                stride=2,
+                padding='SAME',
+                scope='bottom_up_Conv2d_{}'.format(i))
+            coarse_features['bottom_up_Conv2d_{}'.format(i)] = last_feature_map
+    return [fpn_features['top_down_Conv2d_5_pointwise'],
+            fpn_features['top_down_Conv2d_11_pointwise'],
+            fpn_features['top_down_Conv2d_13_pointwise'],
+            coarse_features['bottom_up_Conv2d_14'],
+            coarse_features['bottom_up_Conv2d_15']]
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
new file mode 100644
index 00000000..72771ceb
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
@@ -0,0 +1,172 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for ssd_mobilenet_v1_fpn_feature_extractor."""
+import numpy as np
+import tensorflow as tf
+
+from object_detection.models import ssd_feature_extractor_test
+from object_detection.models import ssd_mobilenet_v1_fpn_feature_extractor
+
+slim = tf.contrib.slim
+
+
+class SsdMobilenetV1FpnFeatureExtractorTest(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                is_training=True, use_explicit_padding=False):
+    """Constructs a new feature extractor.
+
+    Args:
+      depth_multiplier: float depth multiplier for feature extractor
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      is_training: whether the network is in training mode.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
+    Returns:
+      an ssd_meta_arch.SSDFeatureExtractor object.
+    """
+    min_depth = 32
+    return (ssd_mobilenet_v1_fpn_feature_extractor.
+            SSDMobileNetV1FpnFeatureExtractor(
+                is_training,
+                depth_multiplier,
+                min_depth,
+                pad_to_multiple,
+                self.conv_hyperparams_fn,
+                use_explicit_padding=use_explicit_padding))
+
+  def test_extract_features_returns_correct_shapes_256(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),
+                                  (2, 8, 8, 256), (2, 4, 4, 256),
+                                  (2, 2, 2, 256)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_384(self):
+    image_height = 320
+    image_width = 320
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 40, 40, 256), (2, 20, 20, 256),
+                                  (2, 10, 10, 256), (2, 5, 5, 256),
+                                  (2, 3, 3, 256)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_with_dynamic_image_shape(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),
+                                  (2, 8, 8, 256), (2, 4, 4, 256),
+                                  (2, 2, 2, 256)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    pad_to_multiple = 32
+    expected_feature_map_shape = [(2, 40, 40, 256), (2, 20, 20, 256),
+                                  (2, 10, 10, 256), (2, 5, 5, 256),
+                                  (2, 3, 3, 256)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 0.5**12
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 32), (2, 16, 16, 32),
+                                  (2, 8, 8, 32), (2, 4, 4, 32),
+                                  (2, 2, 2, 32)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_raises_error_with_invalid_image_size(self):
+    image_height = 32
+    image_width = 32
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    self.check_extract_features_raises_error_with_invalid_image_size(
+        image_height, image_width, depth_multiplier, pad_to_multiple)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(2, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
+
+  def test_variables_only_created_in_scope(self):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    scope_name = 'MobilenetV1'
+    self.check_feature_extractor_variables_under_scope(
+        depth_multiplier, pad_to_multiple, scope_name)
+
+  def test_fused_batchnorm(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    image_placeholder = tf.placeholder(tf.float32,
+                                       [1, image_height, image_width, 3])
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(image_placeholder)
+    _ = feature_extractor.extract_features(preprocessed_image)
+    self.assertTrue(
+        any(op.type == 'FusedBatchNorm'
+            for op in tf.get_default_graph().get_operations()))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
new file mode 100644
index 00000000..34e9cb4e
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
@@ -0,0 +1,84 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSDFeatureExtractor for MobilenetV1 PPN features."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import context_manager
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets import mobilenet_v1
+
+slim = tf.contrib.slim
+
+
+class SSDMobileNetV1PpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD Feature Extractor using MobilenetV1 PPN features."""
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    with tf.variable_scope('MobilenetV1',
+                           reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(
+          mobilenet_v1.mobilenet_v1_arg_scope(
+              is_training=None, regularize_depthwise=True)):
+        with (slim.arg_scope(self._conv_hyperparams_fn())
+              if self._override_base_feature_extractor_hyperparams
+              else context_manager.IdentityContextManager()):
+          _, image_features = mobilenet_v1.mobilenet_v1_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='Conv2d_13_pointwise',
+              min_depth=self._min_depth,
+              depth_multiplier=self._depth_multiplier,
+              use_explicit_padding=self._use_explicit_padding,
+              scope=scope)
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.pooling_pyramid_feature_maps(
+            base_feature_map_depth=0,
+            num_layers=6,
+            image_features={
+                'image_features': image_features['Conv2d_11_pointwise']
+            })
+    return feature_maps.values()
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
new file mode 100644
index 00000000..1fb17df2
--- /dev/null
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
@@ -0,0 +1,185 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for ssd_mobilenet_v1_ppn_feature_extractor."""
+import numpy as np
+import tensorflow as tf
+
+from object_detection.models import ssd_feature_extractor_test
+from object_detection.models import ssd_mobilenet_v1_ppn_feature_extractor
+
+slim = tf.contrib.slim
+
+
+class SsdMobilenetV1PpnFeatureExtractorTest(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                is_training=True, use_explicit_padding=False):
+    """Constructs a new feature extractor.
+
+    Args:
+      depth_multiplier: float depth multiplier for feature extractor
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      is_training: whether the network is in training mode.
+      use_explicit_padding: Use 'VALID' padding for convolutions, but prepad
+        inputs so that the output dimensions are the same as if 'SAME' padding
+        were used.
+    Returns:
+      an ssd_meta_arch.SSDFeatureExtractor object.
+    """
+    min_depth = 32
+    return (ssd_mobilenet_v1_ppn_feature_extractor.
+            SSDMobileNetV1PpnFeatureExtractor(
+                is_training,
+                depth_multiplier,
+                min_depth,
+                pad_to_multiple,
+                self.conv_hyperparams_fn,
+                use_explicit_padding=use_explicit_padding))
+
+  def test_extract_features_returns_correct_shapes_320(self):
+    image_height = 320
+    image_width = 320
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 512),
+                                  (2, 5, 5, 512), (2, 3, 3, 512),
+                                  (2, 2, 2, 512), (2, 1, 1, 512)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_300(self):
+    image_height = 300
+    image_width = 300
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 19, 19, 512), (2, 10, 10, 512),
+                                  (2, 5, 5, 512), (2, 3, 3, 512),
+                                  (2, 2, 2, 512), (2, 1, 1, 512)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_640(self):
+    image_height = 640
+    image_width = 640
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 40, 40, 512), (2, 20, 20, 512),
+                                  (2, 10, 10, 512), (2, 5, 5, 512),
+                                  (2, 3, 3, 512), (2, 2, 2, 512)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_with_dynamic_image_shape(self):
+    image_height = 320
+    image_width = 320
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 512),
+                                  (2, 5, 5, 512), (2, 3, 3, 512),
+                                  (2, 2, 2, 512), (2, 1, 1, 512)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+    image_height = 299
+    image_width = 299
+    depth_multiplier = 1.0
+    pad_to_multiple = 32
+    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 512),
+                                  (2, 5, 5, 512), (2, 3, 3, 512),
+                                  (2, 2, 2, 512)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 0.5**12
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 16, 16, 32), (2, 8, 8, 32),
+                                  (2, 4, 4, 32), (2, 2, 2, 32),
+                                  (2, 1, 1, 32)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=False)
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape, use_explicit_padding=True)
+
+  def test_extract_features_raises_error_with_invalid_image_size(self):
+    image_height = 32
+    image_width = 32
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    self.check_extract_features_raises_error_with_invalid_image_size(
+        image_height, image_width, depth_multiplier, pad_to_multiple)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(2, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertTrue(np.all(np.less_equal(np.abs(preprocessed_image), 1.0)))
+
+  def test_variables_only_created_in_scope(self):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    scope_name = 'MobilenetV1'
+    self.check_feature_extractor_variables_under_scope(
+        depth_multiplier, pad_to_multiple, scope_name)
+
+  def test_has_fused_batchnorm(self):
+    image_height = 320
+    image_width = 320
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    image_placeholder = tf.placeholder(tf.float32,
+                                       [1, image_height, image_width, 3])
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(image_placeholder)
+    _ = feature_extractor.extract_features(preprocessed_image)
+    self.assertTrue(any(op.type == 'FusedBatchNorm'
+                        for op in tf.get_default_graph().get_operations()))
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
new file mode 100644
index 00000000..13422503
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
@@ -0,0 +1,279 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""SSD feature extractors based on Resnet v1 and PPN architectures."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import context_manager
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets import resnet_v1
+
+slim = tf.contrib.slim
+
+
+class _SSDResnetPpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD feature extractor based on resnet architecture and PPN."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               resnet_base_fn,
+               resnet_scope_name,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               base_feature_map_depth=1024,
+               num_layers=6,
+               override_base_feature_extractor_hyperparams=False,
+               use_bounded_activations=False):
+    """Resnet based PPN Feature Extractor for SSD Models.
+
+    See go/pooling-pyramid for more details about PPN.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      base_feature_map_depth: Depth of the base feature before the max pooling.
+      num_layers: Number of layers used to make predictions. They are pooled
+        from the base feature.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+      use_bounded_activations: Whether or not to use bounded activations for
+        resnet v1 bottleneck residual unit. Bounded activations better lend
+        themselves to quantized inference.
+    """
+    super(_SSDResnetPpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
+        override_base_feature_extractor_hyperparams)
+    self._resnet_base_fn = resnet_base_fn
+    self._resnet_scope_name = resnet_scope_name
+    self._base_feature_map_depth = base_feature_map_depth
+    self._num_layers = num_layers
+    self._use_bounded_activations = use_bounded_activations
+
+  def _filter_features(self, image_features):
+    # TODO(rathodv): Change resnet endpoint to strip scope prefixes instead
+    # of munging the scope here.
+    filtered_image_features = dict({})
+    for key, feature in image_features.items():
+      feature_name = key.split('/')[-1]
+      if feature_name in ['block2', 'block3', 'block4']:
+        filtered_image_features[feature_name] = feature
+    return filtered_image_features
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    VGG style channel mean subtraction as described here:
+    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-mdnge.
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    channel_means = [123.68, 116.779, 103.939]
+    return resized_inputs - [[channel_means]]
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+
+    Raises:
+      ValueError: depth multiplier is not supported.
+    """
+    if self._depth_multiplier != 1.0:
+      raise ValueError('Depth multiplier not supported.')
+
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        129, preprocessed_inputs)
+
+    with tf.variable_scope(
+        self._resnet_scope_name, reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(resnet_v1.resnet_arg_scope()):
+        with (slim.arg_scope(self._conv_hyperparams_fn())
+              if self._override_base_feature_extractor_hyperparams else
+              context_manager.IdentityContextManager()):
+          with slim.arg_scope(
+              [resnet_v1.bottleneck],
+              use_bounded_activations=self._use_bounded_activations):
+            _, activations = self._resnet_base_fn(
+                inputs=ops.pad_to_multiple(preprocessed_inputs,
+                                           self._pad_to_multiple),
+                num_classes=None,
+                is_training=None,
+                global_pool=False,
+                output_stride=None,
+                store_non_strided_activations=True,
+                scope=scope)
+
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.pooling_pyramid_feature_maps(
+            base_feature_map_depth=self._base_feature_map_depth,
+            num_layers=self._num_layers,
+            image_features={
+                'image_features': self._filter_features(activations)['block3']
+            })
+    return feature_maps.values()
+
+
+class SSDResnet50V1PpnFeatureExtractor(_SSDResnetPpnFeatureExtractor):
+  """PPN Resnet50 v1 Feature Extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet50 v1 Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet50V1PpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams_fn, resnet_v1.resnet_v1_50, 'resnet_v1_50',
+        reuse_weights, use_explicit_padding, use_depthwise,
+        override_base_feature_extractor_hyperparams=(
+            override_base_feature_extractor_hyperparams))
+
+
+class SSDResnet101V1PpnFeatureExtractor(_SSDResnetPpnFeatureExtractor):
+  """PPN Resnet101 v1 Feature Extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet101 v1 Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet101V1PpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams_fn, resnet_v1.resnet_v1_101, 'resnet_v1_101',
+        reuse_weights, use_explicit_padding, use_depthwise,
+        override_base_feature_extractor_hyperparams=(
+            override_base_feature_extractor_hyperparams))
+
+
+class SSDResnet152V1PpnFeatureExtractor(_SSDResnetPpnFeatureExtractor):
+  """PPN Resnet152 v1 Feature Extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet152 v1 Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet152V1PpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams_fn, resnet_v1.resnet_v1_152, 'resnet_v1_152',
+        reuse_weights, use_explicit_padding, use_depthwise,
+        override_base_feature_extractor_hyperparams=(
+            override_base_feature_extractor_hyperparams))
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py
new file mode 100644
index 00000000..c47cd120
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py
@@ -0,0 +1,88 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for ssd resnet v1 feature extractors."""
+import tensorflow as tf
+
+from object_detection.models import ssd_resnet_v1_ppn_feature_extractor
+from object_detection.models import ssd_resnet_v1_ppn_feature_extractor_testbase
+
+
+class SSDResnet50V1PpnFeatureExtractorTest(
+    ssd_resnet_v1_ppn_feature_extractor_testbase.
+    SSDResnetPpnFeatureExtractorTestBase):
+  """SSDResnet50v1 feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
+    min_depth = 32
+    is_training = True
+    return ssd_resnet_v1_ppn_feature_extractor.SSDResnet50V1PpnFeatureExtractor(
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        self.conv_hyperparams_fn,
+        use_explicit_padding=use_explicit_padding)
+
+  def _scope_name(self):
+    return 'resnet_v1_50'
+
+
+class SSDResnet101V1PpnFeatureExtractorTest(
+    ssd_resnet_v1_ppn_feature_extractor_testbase.
+    SSDResnetPpnFeatureExtractorTestBase):
+  """SSDResnet101v1 feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
+    min_depth = 32
+    is_training = True
+    return (
+        ssd_resnet_v1_ppn_feature_extractor.SSDResnet101V1PpnFeatureExtractor(
+            is_training,
+            depth_multiplier,
+            min_depth,
+            pad_to_multiple,
+            self.conv_hyperparams_fn,
+            use_explicit_padding=use_explicit_padding))
+
+  def _scope_name(self):
+    return 'resnet_v1_101'
+
+
+class SSDResnet152V1PpnFeatureExtractorTest(
+    ssd_resnet_v1_ppn_feature_extractor_testbase.
+    SSDResnetPpnFeatureExtractorTestBase):
+  """SSDResnet152v1 feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
+                                use_explicit_padding=False):
+    min_depth = 32
+    is_training = True
+    return (
+        ssd_resnet_v1_ppn_feature_extractor.SSDResnet152V1PpnFeatureExtractor(
+            is_training,
+            depth_multiplier,
+            min_depth,
+            pad_to_multiple,
+            self.conv_hyperparams_fn,
+            use_explicit_padding=use_explicit_padding))
+
+  def _scope_name(self):
+    return 'resnet_v1_152'
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py
new file mode 100644
index 00000000..e8d0db5f
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py
@@ -0,0 +1,78 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for ssd resnet v1 feature extractors."""
+import abc
+import numpy as np
+
+from object_detection.models import ssd_feature_extractor_test
+
+
+class SSDResnetPpnFeatureExtractorTestBase(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+  """Helper test class for SSD Resnet PPN feature extractors."""
+
+  @abc.abstractmethod
+  def _scope_name(self):
+    pass
+
+  def test_extract_features_returns_correct_shapes_289(self):
+    image_height = 289
+    image_width = 289
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 19, 19, 1024), (2, 10, 10, 1024),
+                                  (2, 5, 5, 1024), (2, 3, 3, 1024),
+                                  (2, 2, 2, 1024), (2, 1, 1, 1024)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):
+    image_height = 289
+    image_width = 289
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 19, 19, 1024), (2, 10, 10, 1024),
+                                  (2, 5, 5, 1024), (2, 3, 3, 1024),
+                                  (2, 2, 2, 1024), (2, 1, 1, 1024)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_raises_error_with_invalid_image_size(self):
+    image_height = 32
+    image_width = 32
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    self.check_extract_features_raises_error_with_invalid_image_size(
+        image_height, image_width, depth_multiplier, pad_to_multiple)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(4, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertAllClose(preprocessed_image,
+                        test_image - [[123.68, 116.779, 103.939]])
+
+  def test_variables_only_created_in_scope(self):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    self.check_feature_extractor_variables_under_scope(
+        depth_multiplier, pad_to_multiple, self._scope_name())
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index f5ceae68..5dbf47f4 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -4,14 +4,14 @@ package object_detection.protos;
 
 import "object_detection/protos/hyperparams.proto";
 
-
 // Configuration proto for box predictor. See core/box_predictor.py for details.
 message BoxPredictor {
   oneof box_predictor_oneof {
     ConvolutionalBoxPredictor convolutional_box_predictor = 1;
     MaskRCNNBoxPredictor mask_rcnn_box_predictor = 2;
     RfcnBoxPredictor rfcn_box_predictor = 3;
-    WeightSharedConvolutionalBoxPredictor weight_shared_convolutional_box_predictor = 4;
+    WeightSharedConvolutionalBoxPredictor
+        weight_shared_convolutional_box_predictor = 4;
   }
 }
 
@@ -82,11 +82,15 @@ message WeightSharedConvolutionalBoxPredictor {
   // https://arxiv.org/abs/1708.02002 for details.
   optional float class_prediction_bias_init = 10 [default = 0.0];
 
-   // Whether to use dropout for class prediction.
+  // Whether to use dropout for class prediction.
   optional bool use_dropout = 11 [default = false];
 
   // Keep probability for dropout
   optional float dropout_keep_probability = 12 [default = 0.8];
+
+  // Whether to share the multi-layer tower between box prediction and class
+  // prediction heads.
+  optional bool share_prediction_tower = 13 [default = false];
 }
 
 message MaskRCNNBoxPredictor {
@@ -94,7 +98,7 @@ message MaskRCNNBoxPredictor {
   optional Hyperparams fc_hyperparams = 1;
 
   // Whether to use dropout op prior to the both box and class predictions.
-  optional bool use_dropout = 2 [default= false];
+  optional bool use_dropout = 2 [default = false];
 
   // Keep probability for dropout. This is only used if use_dropout is true.
   optional float dropout_keep_probability = 3 [default = 0.5];
@@ -141,13 +145,13 @@ message RfcnBoxPredictor {
   optional int32 num_spatial_bins_width = 3 [default = 3];
 
   // Target depth to reduce the input image features to.
-  optional int32 depth = 4 [default=1024];
+  optional int32 depth = 4 [default = 1024];
 
   // Size of the encoding for the boxes.
   optional int32 box_code_size = 5 [default = 4];
 
   // Size to resize the rfcn crops to.
-  optional int32 crop_height = 6 [default= 12];
+  optional int32 crop_height = 6 [default = 12];
 
-  optional int32 crop_width = 7 [default=12];
+  optional int32 crop_width = 7 [default = 12];
 }
diff --git a/research/object_detection/protos/optimizer.proto b/research/object_detection/protos/optimizer.proto
index 4f788edf..8bbebe45 100644
--- a/research/object_detection/protos/optimizer.proto
+++ b/research/object_detection/protos/optimizer.proto
@@ -61,6 +61,9 @@ message ExponentialDecayLearningRate {
   optional uint32 decay_steps = 2 [default = 4000000];
   optional float decay_factor = 3 [default = 0.95];
   optional bool staircase = 4 [default = true];
+  optional float burnin_learning_rate = 5 [default = 0.0];
+  optional uint32 burnin_steps =  6 [default = 0];
+  optional float min_learning_rate =  7 [default = 0.0];
 }
 
 // Configuration message for a manually defined learning rate schedule.
diff --git a/research/object_detection/samples/cloud/cloud.yml b/research/object_detection/samples/cloud/cloud.yml
index 495876a1..b349a9c7 100644
--- a/research/object_detection/samples/cloud/cloud.yml
+++ b/research/object_detection/samples/cloud/cloud.yml
@@ -1,5 +1,5 @@
 trainingInput:
-  runtimeVersion: "1.0"
+  runtimeVersion: "1.8"
   scaleTier: CUSTOM
   masterType: standard_gpu
   workerCount: 5
diff --git a/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config b/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config
index 1b66d0d5..eb6baf0a 100644
--- a/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config
+++ b/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config
@@ -166,7 +166,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -178,7 +178,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
index c11d852c..e198ed08 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
@@ -120,7 +120,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -134,7 +134,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config
index ef83c9c4..0d650f48 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config
@@ -110,7 +110,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -124,7 +124,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
index 3a6fcc89..cce5fe4a 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_pets.config
@@ -121,7 +121,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -133,7 +133,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config b/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config
index c3469924..9623d877 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_v2_coco.config
@@ -119,7 +119,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -133,7 +133,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config b/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
index 6ebd586b..0b3ddbd0 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config
@@ -120,7 +120,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -132,7 +132,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_nas_coco.config b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
index 99cbb679..61915d12 100644
--- a/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
@@ -121,7 +121,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -134,7 +134,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config
index 50227315..1a34a435 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config
@@ -114,7 +114,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -128,7 +128,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
index b1f5036c..a04b0cb2 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
@@ -113,7 +113,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -127,7 +127,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
index 55f5f8d3..078b9511 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_pets.config
@@ -119,7 +119,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -131,7 +131,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
index 0a159bf0..1d0e7d50 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
@@ -118,7 +118,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -132,7 +132,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
index 38784bbe..757858ac 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet152_pets.config
@@ -119,7 +119,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -131,7 +131,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
index 2fb63eb0..28eb1f2c 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
@@ -118,7 +118,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -132,7 +132,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config b/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
index f1b6fe95..8f9e5776 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet50_pets.config
@@ -120,7 +120,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -132,7 +132,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config
index 1d5059fa..7906bca4 100644
--- a/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config
+++ b/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config
@@ -140,7 +140,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
@@ -156,7 +156,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config b/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config
index dc2ff5e2..22fc1d4f 100644
--- a/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config
+++ b/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config
@@ -139,7 +139,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
@@ -155,7 +155,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config
index fd43cf25..21421384 100644
--- a/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config
@@ -140,7 +140,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
@@ -156,7 +156,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
index 5d9819ed..106e6915 100644
--- a/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
@@ -134,7 +134,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   load_instance_masks: true
@@ -147,7 +147,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_fullbody_with_masks_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config
index c93993a5..04ab5cf6 100644
--- a/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config
@@ -140,7 +140,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
@@ -156,7 +156,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   load_instance_masks: true
diff --git a/research/object_detection/samples/configs/rfcn_resnet101_coco.config b/research/object_detection/samples/configs/rfcn_resnet101_coco.config
index 9e2255ad..541ae8c5 100644
--- a/research/object_detection/samples/configs/rfcn_resnet101_coco.config
+++ b/research/object_detection/samples/configs/rfcn_resnet101_coco.config
@@ -115,7 +115,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -129,7 +129,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/rfcn_resnet101_pets.config b/research/object_detection/samples/configs/rfcn_resnet101_pets.config
index 6d6d92b6..3da5c21e 100644
--- a/research/object_detection/samples/configs/rfcn_resnet101_pets.config
+++ b/research/object_detection/samples/configs/rfcn_resnet101_pets.config
@@ -116,7 +116,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -128,7 +128,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_inception_v2_coco.config b/research/object_detection/samples/configs/ssd_inception_v2_coco.config
index 972ef204..e1d3cd82 100644
--- a/research/object_detection/samples/configs/ssd_inception_v2_coco.config
+++ b/research/object_detection/samples/configs/ssd_inception_v2_coco.config
@@ -167,7 +167,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -181,7 +181,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_inception_v2_pets.config b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
index 42f4ebbd..e36c5e8f 100644
--- a/research/object_detection/samples/configs/ssd_inception_v2_pets.config
+++ b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
@@ -169,7 +169,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -181,7 +181,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_inception_v3_pets.config b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
index 3fff911c..3cf3aa60 100644
--- a/research/object_detection/samples/configs/ssd_inception_v3_pets.config
+++ b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
@@ -168,7 +168,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -180,7 +180,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config
new file mode 100644
index 00000000..3d180719
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync.config
@@ -0,0 +1,198 @@
+# SSD with Mobilenet v1 0.75 depth multiplied feature extractor and focal loss.
+# Trained on COCO14, initialized from Imagenet classification checkpoint
+
+# Achieves 17.5 mAP on COCO14 minival dataset. Doubling the number of training
+# steps gets to 18.4.
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 0.75
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 2048
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 10000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.9
+          total_steps: 10000
+          warmup_learning_rate: 0.3
+          warmup_steps: 300
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
\ No newline at end of file
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config
new file mode 100644
index 00000000..25cc0bf1
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync.config
@@ -0,0 +1,201 @@
+# SSD with Mobilenet v1 with quantized training.
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+# Achieves 18.2 mAP on coco14 minival dataset.
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 0.75
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          random_normal_initializer {
+            stddev: 0.01
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 128
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 50000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: .2
+          total_steps: 50000
+          warmup_learning_rate: 0.06
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
+graph_rewriter {
+  quantization {
+    delay: 48000
+    activation_bits: 8
+    weight_bits: 8
+  }
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config
new file mode 100644
index 00000000..d15fd313
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_0.75_depth_quantized_300x300_pets_sync.config
@@ -0,0 +1,204 @@
+# SSD with Mobilenet v1 0.75 depth multiplied feature extractor, focal loss and
+# quantized training.
+# Trained on IIIT-Oxford pets, initialized from COCO detection checkpoint
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 37
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 0.75
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          center: true,
+          decay: 0.9,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  fine_tune_checkpoint_type: "detection"
+  load_all_detection_checkpoint_vars: true
+  batch_size: 128
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 2000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.2
+          total_steps: 2000
+          warmup_steps: 0
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 1100
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
+graph_rewriter {
+  quantization {
+    delay: 1800
+    activation_bits: 8
+    weight_bits: 8
+  }
+}
\ No newline at end of file
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
similarity index 70%
rename from research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config
rename to research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
index 50987a49..7ce26953 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_coco.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
@@ -1,12 +1,15 @@
-# SSD with Mobilenet v1 configuration for MSCOCO Dataset.
-# Users should configure the fine_tune_checkpoint field in the train config as
-# well as the label_map_path and input_path fields in the train_input_reader and
-# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
-# should be configured.
-# TPU-compatible
+# SSD with Mobilenet v1 feature extractor and focal loss.
+# Trained on COCO14, initialized from Imagenet classification checkpoint
+
+# Achieves 19.3 mAP on COCO14 minival dataset. Doubling the number of training
+# steps gets to 20.6 mAP.
+
+# This config is TPU compatible
 
 model {
   ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
     num_classes: 90
     box_coder {
       faster_rcnn_box_coder {
@@ -23,12 +26,14 @@ model {
         ignore_thresholds: false
         negatives_lower_than_unmatched: true
         force_match_for_each_row: true
+        use_matmul_gather: true
       }
     }
     similarity_calculator {
       iou_similarity {
       }
     }
+    encode_background_as_zeros: true
     anchor_generator {
       ssd_anchor_generator {
         num_layers: 6
@@ -57,6 +62,7 @@ model {
         kernel_size: 1
         box_code_size: 4
         apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
         conv_hyperparams {
           activation: RELU_6,
           regularizer {
@@ -65,8 +71,8 @@ model {
             }
           }
           initializer {
-            truncated_normal_initializer {
-              stddev: 0.03
+            random_normal_initializer {
+              stddev: 0.01
               mean: 0.0
             }
           }
@@ -74,7 +80,7 @@ model {
             train: true,
             scale: true,
             center: true,
-            decay: 0.9997,
+            decay: 0.97,
             epsilon: 0.001,
           }
         }
@@ -101,26 +107,29 @@ model {
           train: true,
           scale: true,
           center: true,
-          decay: 0.9997,
+          decay: 0.97,
           epsilon: 0.001,
         }
       }
+      override_base_feature_extractor_hyperparams: true
     }
     loss {
       classification_loss {
         weighted_sigmoid_focal {
-          alpha: 0.75
+          alpha: 0.75,
           gamma: 2.0
         }
       }
       localization_loss {
         weighted_smooth_l1 {
+          delta: 1.0
         }
       }
       classification_weight: 1.0
       localization_weight: 1.0
     }
     normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
     post_processing {
       batch_non_max_suppression {
         score_threshold: 1e-8
@@ -134,28 +143,12 @@ model {
 }
 
 train_config: {
-  batch_size: 24
-  optimizer {
-    rms_prop_optimizer: {
-      learning_rate: {
-        exponential_decay_learning_rate {
-          initial_learning_rate: 0.004
-          decay_steps: 800720
-          decay_factor: 0.95
-        }
-      }
-      momentum_optimizer_value: 0.9
-      decay: 0.9
-      epsilon: 1.0
-    }
-  }
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
-  from_detection_checkpoint: true
-  # Note: The below line limits the training process to 200K steps, which we
-  # empirically found to be sufficient enough to train the pets dataset. This
-  # effectively bypasses the learning rate schedule (the learning rate will
-  # never decay). Remove the below line to train indefinitely.
-  num_steps: 200000
+  batch_size: 2048
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 10000
   data_augmentation_options {
     random_horizontal_flip {
     }
@@ -164,29 +157,42 @@ train_config: {
     ssd_random_crop {
     }
   }
-  max_number_of_boxes: 50
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.9
+          total_steps: 10000
+          warmup_learning_rate: 0.3
+          warmup_steps: 300
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
   unpad_groundtruth_tensors: false
 }
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
 
 eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
   num_examples: 8000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-}
+}
\ No newline at end of file
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
index 73da9861..a80117b5 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
@@ -172,7 +172,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -186,7 +186,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
index a0f4ca40..4623660f 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_focal_loss_pets.config
@@ -171,7 +171,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
@@ -183,7 +183,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
new file mode 100644
index 00000000..9b6b8a3b
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
@@ -0,0 +1,193 @@
+# SSD with Mobilenet v1 FPN feature extractor, shared box predictor and focal
+# loss (a.k.a Retinanet).
+# See Lin et al, https://arxiv.org/abs/1708.02002
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+# Achieves 29.6 mAP on COCO14 minival dataset. Doubling the number of training
+# steps to 25k gets 31.5 mAP
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      multiscale_anchor_generator {
+        min_level: 3
+        max_level: 7
+        anchor_scale: 4.0
+        aspect_ratios: [1.0, 2.0, 0.5]
+        scales_per_octave: 2
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 640
+        width: 640
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        depth: 256
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            decay: 0.997,
+            epsilon: 0.001,
+          }
+        }
+        num_layers_before_predictor: 4
+        kernel_size: 3
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1_fpn'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          random_normal_initializer {
+            stddev: 0.01
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          decay: 0.997,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.25
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 128
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 12500
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    random_crop_image {
+      min_object_covered: 0.0
+      min_aspect_ratio: 0.75
+      max_aspect_ratio: 3.0
+      min_area: 0.75
+      max_area: 1.0
+      overlap_thresh: 0.0
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: .08
+          total_steps: 12500
+          warmup_learning_rate: .026666
+          warmup_steps: 1000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
\ No newline at end of file
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
index 2464782c..11e91d40 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
@@ -173,19 +173,19 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_train.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
 }
 
 eval_config: {
   metrics_set: "coco_detection_metrics"
-  num_examples: 1101
+  num_examples: 1100
 }
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-?????"
+    input_path: "PATH_TO_BE_CONFIGURED/pet_faces_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config
new file mode 100644
index 00000000..92c9e6ed
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync.config
@@ -0,0 +1,191 @@
+# SSD with Mobilenet v1 PPN feature extractor.
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+# Achieves 19.7 mAP on COCO14 minival dataset.
+
+# This config is TPU compatible.
+
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.15
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+        reduce_boxes_in_lowest_layer: false
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        depth: 512
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true
+            center: true
+            train: true
+            decay: 0.97
+            epsilon: 0.001
+          }
+        }
+        num_layers_before_predictor: 1
+        kernel_size: 1
+        share_prediction_tower: true
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1_ppn'
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          random_normal_initializer {
+            stddev: 0.01
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true
+          center: true
+          decay: 0.97
+          epsilon: 0.001
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.5
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 50000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.7
+          total_steps: 50000
+          warmup_learning_rate: 0.1333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config
new file mode 100644
index 00000000..a79d7726
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_quantized_300x300_coco14_sync.config
@@ -0,0 +1,201 @@
+# SSD with Mobilenet v1 with quantized training.
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+# Achieves 18.2 mAP on coco14 minival dataset.
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          random_normal_initializer {
+            stddev: 0.01
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 128
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 50000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: .2
+          total_steps: 50000
+          warmup_learning_rate: 0.06
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
+
+graph_rewriter {
+  quantization {
+    delay: 48000
+    activation_bits: 8
+    weight_bits: 8
+  }
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config
index cb852989..e1adf269 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config
@@ -172,7 +172,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -186,7 +186,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config b/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
new file mode 100644
index 00000000..748714c7
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
@@ -0,0 +1,193 @@
+# SSD with Resnet 50 v1 FPN feature extractor, shared box predictor and focal
+# loss (a.k.a Retinanet).
+# See Lin et al, https://arxiv.org/abs/1708.02002
+# Trained on COCO, initialized from Imagenet classification checkpoint
+
+# Achieves 35.2 mAP on COCO14 minival dataset. Doubling the number of training
+# steps to 50k gets 36.9 mAP
+
+# This config is TPU compatible
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      multiscale_anchor_generator {
+        min_level: 3
+        max_level: 7
+        anchor_scale: 4.0
+        aspect_ratios: [1.0, 2.0, 0.5]
+        scales_per_octave: 2
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 640
+        width: 640
+      }
+    }
+    box_predictor {
+      weight_shared_convolutional_box_predictor {
+        depth: 256
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.0004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.01
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            scale: true,
+            decay: 0.997,
+            epsilon: 0.001,
+          }
+        }
+        num_layers_before_predictor: 4
+        kernel_size: 3
+      }
+    }
+    feature_extractor {
+      type: 'ssd_resnet50_v1_fpn'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.0004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          scale: true,
+          decay: 0.997,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: true
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.25
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  batch_size: 64
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 8
+  num_steps: 25000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    random_crop_image {
+      min_object_covered: 0.0
+      min_aspect_ratio: 0.75
+      max_aspect_ratio: 3.0
+      min_area: 0.75
+      max_area: 1.0
+      overlap_thresh: 0.0
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: .04
+          total_steps: 25000
+          warmup_learning_rate: .013333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-00000-of-00100"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-00000-of-00010"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
\ No newline at end of file
diff --git a/research/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config b/research/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config
index f35422df..b6c8fb09 100644
--- a/research/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config
+++ b/research/object_detection/samples/configs/ssdlite_mobilenet_v1_coco.config
@@ -174,7 +174,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -188,7 +188,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config b/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config
index b6077f99..09222600 100644
--- a/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config
+++ b/research/object_detection/samples/configs/ssdlite_mobilenet_v2_coco.config
@@ -174,7 +174,7 @@ train_config: {
 
 train_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
@@ -188,7 +188,7 @@ eval_config: {
 
 eval_input_reader: {
   tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
   }
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 5a72d009..d6f1229e 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -23,7 +23,9 @@ def exponential_decay_with_burnin(global_step,
                                   learning_rate_decay_steps,
                                   learning_rate_decay_factor,
                                   burnin_learning_rate=0.0,
-                                  burnin_steps=0):
+                                  burnin_steps=0,
+                                  min_learning_rate=0.0,
+                                  staircase=True):
   """Exponential decay schedule with burn-in period.
 
   In this schedule, learning rate is fixed at burnin_learning_rate
@@ -41,6 +43,8 @@ def exponential_decay_with_burnin(global_step,
       0.0 (which is the default), then the burn-in learning rate is simply
       set to learning_rate_base.
     burnin_steps: number of steps to use burnin learning rate.
+    min_learning_rate: the minimum learning rate.
+    staircase: whether use staircase decay.
 
   Returns:
     a (scalar) float tensor representing learning rate
@@ -49,14 +53,14 @@ def exponential_decay_with_burnin(global_step,
     burnin_learning_rate = learning_rate_base
   post_burnin_learning_rate = tf.train.exponential_decay(
       learning_rate_base,
-      global_step,
+      global_step - burnin_steps,
       learning_rate_decay_steps,
       learning_rate_decay_factor,
-      staircase=True)
-  return tf.where(
+      staircase=staircase)
+  return tf.maximum(tf.where(
       tf.less(tf.cast(global_step, tf.int32), tf.constant(burnin_steps)),
       tf.constant(burnin_learning_rate),
-      post_burnin_learning_rate, name='learning_rate')
+      post_burnin_learning_rate), min_learning_rate, name='learning_rate')
 
 
 def cosine_decay_with_warmup(global_step,
diff --git a/research/object_detection/utils/learning_schedules_test.py b/research/object_detection/utils/learning_schedules_test.py
index 225b79e5..b78ca775 100644
--- a/research/object_detection/utils/learning_schedules_test.py
+++ b/research/object_detection/utils/learning_schedules_test.py
@@ -30,17 +30,19 @@ class LearningSchedulesTest(test_case.TestCase):
       learning_rate_decay_factor = .1
       burnin_learning_rate = .5
       burnin_steps = 2
+      min_learning_rate = .05
       learning_rate = learning_schedules.exponential_decay_with_burnin(
           global_step, learning_rate_base, learning_rate_decay_steps,
-          learning_rate_decay_factor, burnin_learning_rate, burnin_steps)
+          learning_rate_decay_factor, burnin_learning_rate, burnin_steps,
+          min_learning_rate)
       assert learning_rate.op.name.endswith('learning_rate')
       return (learning_rate,)
 
     output_rates = [
-        self.execute(graph_fn, [np.array(i).astype(np.int64)]) for i in range(8)
+        self.execute(graph_fn, [np.array(i).astype(np.int64)]) for i in range(9)
     ]
 
-    exp_rates = [.5, .5, 1, .1, .1, .1, .01, .01]
+    exp_rates = [.5, .5, 1, 1, 1, .1, .1, .1, .05]
     self.assertAllClose(output_rates, exp_rates, rtol=1e-4)
 
   def testCosineDecayWithWarmup(self):
diff --git a/research/object_detection/utils/per_image_vrd_evaluation.py b/research/object_detection/utils/per_image_vrd_evaluation.py
index 41682101..0e204968 100644
--- a/research/object_detection/utils/per_image_vrd_evaluation.py
+++ b/research/object_detection/utils/per_image_vrd_evaluation.py
@@ -137,9 +137,14 @@ class PerImageVRDEvaluation(object):
       result_tp_fp_labels.append(tp_fp_labels)
       result_mapping.append(selector_mapping[sorted_indices])
 
-    result_scores = np.concatenate(result_scores)
-    result_tp_fp_labels = np.concatenate(result_tp_fp_labels)
-    result_mapping = np.concatenate(result_mapping)
+    if result_scores:
+      result_scores = np.concatenate(result_scores)
+      result_tp_fp_labels = np.concatenate(result_tp_fp_labels)
+      result_mapping = np.concatenate(result_mapping)
+    else:
+      result_scores = np.array([], dtype=float)
+      result_tp_fp_labels = np.array([], dtype=bool)
+      result_mapping = np.array([], dtype=int)
 
     sorted_indices = np.argsort(result_scores)
     sorted_indices = sorted_indices[::-1]
diff --git a/research/object_detection/utils/vrd_evaluation.py b/research/object_detection/utils/vrd_evaluation.py
index a56903ef..f11f35ea 100644
--- a/research/object_detection/utils/vrd_evaluation.py
+++ b/research/object_detection/utils/vrd_evaluation.py
@@ -178,6 +178,14 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
           corresponding bounding boxes and possibly additional classes (see
           datatype label_data_type above).
     """
+    if image_id not in self._image_ids:
+      logging.warn('No groundtruth for the image with id %s.', image_id)
+      # Since for the correct work of evaluator it is assumed that groundtruth
+      # is inserted first we make sure to break the code if is it not the case.
+      self._image_ids.update([image_id])
+      self._negative_labels[image_id] = np.array([])
+      self._evaluatable_labels[image_id] = np.array([])
+
     num_detections = detections_dict[
         standard_fields.DetectionResultFields.detection_boxes].shape[0]
     detection_class_tuples = detections_dict[
@@ -186,7 +194,6 @@ class VRDDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
         standard_fields.DetectionResultFields.detection_boxes]
     negative_selector = np.zeros(num_detections, dtype=bool)
     selector = np.ones(num_detections, dtype=bool)
-
     # Only check boxable labels
     for field in detection_box_tuples.dtype.fields:
       # Verify if one of the labels is negative (this is sure FP)
@@ -483,8 +490,9 @@ class _VRDDetectionEvaluation(object):
       groundtruth_box_tuples = self._groundtruth_box_tuples[image_key]
       groundtruth_class_tuples = self._groundtruth_class_tuples[image_key]
     else:
-      groundtruth_box_tuples = np.empty(shape=[0, 4], dtype=float)
-      groundtruth_class_tuples = np.array([], dtype=int)
+      groundtruth_box_tuples = np.empty(
+          shape=[0, 4], dtype=detected_box_tuples.dtype)
+      groundtruth_class_tuples = np.array([], dtype=detected_class_tuples.dtype)
 
     scores, tp_fp_labels, mapping = (
         self._per_image_eval.compute_detection_tp_fp(
