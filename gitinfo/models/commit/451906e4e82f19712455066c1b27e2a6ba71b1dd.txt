commit 451906e4e82f19712455066c1b27e2a6ba71b1dd
Author: pkulzc <lzc@google.com>
Date:   Tue May 26 16:19:59 2020 -0700

    Release MobileDet code and model, and require tf_slim installation for OD API. (#8562)
    
    * Merged commit includes the following changes:
    311933687  by Sergio Guadarrama:
    
        Removes spurios use of tf.compat.v2, which results in spurious tf.compat.v1.compat.v2. Adds basic test to nasnet_utils.
        Replaces all remaining import tensorflow as tf with import tensorflow.compat.v1 as tf
    
    --
    311766063  by Sergio Guadarrama:
    
        Removes explicit tf.compat.v1 in all call sites (we already import tf.compat.v1, so this code was  doing tf.compat.v1.compat.v1). The existing code worked in latest version of tensorflow, 2.2, (and 1.15) but not in 1.14 or in 2.0.0a, this CL fixes it.
    
    --
    311624958  by Sergio Guadarrama:
    
        Updates README that doesn't render properly in github documentation
    
    --
    310980959  by Sergio Guadarrama:
    
        Moves research_models/slim off tf.contrib.slim/layers/framework to tf_slim
    
    --
    310263156  by Sergio Guadarrama:
    
        Adds model breakdown for MobilenetV3
    
    --
    308640516  by Sergio Guadarrama:
    
        Internal change
    
    308244396  by Sergio Guadarrama:
    
        GroupNormalization support for MobilenetV3.
    
    --
    307475800  by Sergio Guadarrama:
    
        Internal change
    
    --
    302077708  by Sergio Guadarrama:
    
        Remove `disable_tf2` behavior from slim py_library targets
    
    --
    301208453  by Sergio Guadarrama:
    
        Automated refactoring to make code Python 3 compatible.
    
    --
    300816672  by Sergio Guadarrama:
    
        Internal change
    
    299433840  by Sergio Guadarrama:
    
        Internal change
    
    299221609  by Sergio Guadarrama:
    
        Explicitly disable Tensorflow v2 behaviors for all TF1.x binaries and tests
    
    --
    299179617  by Sergio Guadarrama:
    
        Internal change
    
    299040784  by Sergio Guadarrama:
    
        Internal change
    
    299036699  by Sergio Guadarrama:
    
        Internal change
    
    298736510  by Sergio Guadarrama:
    
        Internal change
    
    298732599  by Sergio Guadarrama:
    
        Internal change
    
    298729507  by Sergio Guadarrama:
    
        Internal change
    
    298253328  by Sergio Guadarrama:
    
        Internal change
    
    297788346  by Sergio Guadarrama:
    
        Internal change
    
    297785278  by Sergio Guadarrama:
    
        Internal change
    
    297783127  by Sergio Guadarrama:
    
        Internal change
    
    297725870  by Sergio Guadarrama:
    
        Internal change
    
    297721811  by Sergio Guadarrama:
    
        Internal change
    
    297711347  by Sergio Guadarrama:
    
        Internal change
    
    297708059  by Sergio Guadarrama:
    
        Internal change
    
    297701831  by Sergio Guadarrama:
    
        Internal change
    
    297700038  by Sergio Guadarrama:
    
        Internal change
    
    297670468  by Sergio Guadarrama:
    
        Internal change.
    
    --
    297350326  by Sergio Guadarrama:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    297201668  by Sergio Guadarrama:
    
        Explicitly replace "import tensorflow" with "tensorflow.compat.v1" for TF2.x migration
    
    --
    294483372  by Sergio Guadarrama:
    
        Internal change
    
    PiperOrigin-RevId: 311933687
    
    * Merged commit includes the following changes:
    312578615  by Menglong Zhu:
    
        Modify the LSTM feature extractors to be python 3 compatible.
    
    --
    311264357  by Menglong Zhu:
    
        Removes contrib.slim
    
    --
    308957207  by Menglong Zhu:
    
        Automated refactoring to make code Python 3 compatible.
    
    --
    306976470  by yongzhe:
    
        Internal change
    
    306777559  by Menglong Zhu:
    
        Internal change
    
    --
    299232507  by lzyuan:
    
        Internal update.
    
    --
    299221735  by lzyuan:
    
        Add small epsilon on max_range for quantize_op to prevent range collapse.
    
    --
    
    PiperOrigin-RevId: 312578615
    
    * Merged commit includes the following changes:
    310447280  by lzc:
    
        Internal changes.
    
    --
    
    PiperOrigin-RevId: 310447280
    
    Co-authored-by: Sergio Guadarrama <sguada@google.com>
    Co-authored-by: Menglong Zhu <menglong@google.com>

diff --git a/research/lstm_object_detection/README.md b/research/lstm_object_detection/README.md
index edd3b1d8..a696ba3d 100644
--- a/research/lstm_object_detection/README.md
+++ b/research/lstm_object_detection/README.md
@@ -1,6 +1,3 @@
-![TensorFlow Requirement: 1.x](https://img.shields.io/badge/TensorFlow%20Requirement-1.x-brightgreen)
-![TensorFlow 2 Not Supported](https://img.shields.io/badge/TensorFlow%202%20Not%20Supported-%E2%9C%95-red.svg)
-
 # Tensorflow Mobile Video Object Detection
 
 Tensorflow mobile video object detection implementation proposed in the
@@ -35,6 +32,7 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:rLqvkztmWYgJ:scholar.go
 * yinxiao@google.com
 * menglong@google.com
 * yongzhe@google.com
+* lzyuan@google.com
 
 
 ## Table of Contents
diff --git a/research/lstm_object_detection/inputs/seq_dataset_builder.py b/research/lstm_object_detection/inputs/seq_dataset_builder.py
index a2b7afcb..55e24820 100644
--- a/research/lstm_object_detection/inputs/seq_dataset_builder.py
+++ b/research/lstm_object_detection/inputs/seq_dataset_builder.py
@@ -23,7 +23,8 @@ Detection configuration framework, they should define their own builder function
 that wraps the build function.
 """
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
+
 from tensorflow.contrib.training.python.training import sequence_queueing_state_saver as sqss
 from lstm_object_detection.inputs import tf_sequence_example_decoder
 from lstm_object_detection.protos import input_reader_google_pb2
@@ -33,7 +34,7 @@ from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import ops as util_ops
 
-parallel_reader = contrib_slim.parallel_reader
+parallel_reader = slim.parallel_reader
 # TODO(yinxiao): Make the following variable into configurable proto.
 # Padding size for the labeled objects in each frame. Here we assume each
 # frame has a total number of objects less than _PADDING_SIZE.
diff --git a/research/lstm_object_detection/inputs/tf_sequence_example_decoder.py b/research/lstm_object_detection/inputs/tf_sequence_example_decoder.py
index 5ccb40c5..def945b3 100644
--- a/research/lstm_object_detection/inputs/tf_sequence_example_decoder.py
+++ b/research/lstm_object_detection/inputs/tf_sequence_example_decoder.py
@@ -19,11 +19,11 @@ A decoder to decode string tensors containing serialized
 tensorflow.SequenceExample protos.
 """
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 from object_detection.core import data_decoder
 from object_detection.core import standard_fields as fields
 
-tfexample_decoder = contrib_slim.tfexample_decoder
+tfexample_decoder = slim.tfexample_decoder
 
 
 class BoundingBoxSequence(tfexample_decoder.ItemHandler):
diff --git a/research/lstm_object_detection/lstm/lstm_cells.py b/research/lstm_object_detection/lstm/lstm_cells.py
index d40bd1a0..0f3601cb 100644
--- a/research/lstm_object_detection/lstm/lstm_cells.py
+++ b/research/lstm_object_detection/lstm/lstm_cells.py
@@ -15,10 +15,8 @@
 """BottleneckConvLSTMCell implementation."""
 
 import tensorflow.compat.v1 as tf
-
-from tensorflow.contrib import layers as contrib_layers
+import tf_slim as slim
 from tensorflow.contrib import rnn as contrib_rnn
-from tensorflow.contrib import slim
 from tensorflow.contrib.framework.python.ops import variables as contrib_variables
 import lstm_object_detection.lstm.utils as lstm_utils
 
@@ -121,7 +119,7 @@ class BottleneckConvLSTMCell(contrib_rnn.RNNCell):
       if self._pre_bottleneck:
         bottleneck = inputs
       else:
-        bottleneck = contrib_layers.separable_conv2d(
+        bottleneck = slim.separable_conv2d(
             tf.concat([inputs, h], 3),
             self._num_units,
             self._filter_size,
@@ -133,7 +131,7 @@ class BottleneckConvLSTMCell(contrib_rnn.RNNCell):
         if self._viz_gates:
           slim.summaries.add_histogram_summary(bottleneck, 'bottleneck')
 
-      concat = contrib_layers.separable_conv2d(
+      concat = slim.separable_conv2d(
           bottleneck,
           4 * self._num_units,
           self._filter_size,
@@ -243,7 +241,7 @@ class BottleneckConvLSTMCell(contrib_rnn.RNNCell):
       state = tf.reshape(state, [batch_size, height, width, -1])
     with tf.variable_scope('conv_lstm_cell', reuse=tf.AUTO_REUSE):
       scope_name = 'bottleneck_%d' % input_index
-      inputs = contrib_layers.separable_conv2d(
+      inputs = slim.separable_conv2d(
           tf.concat([inputs, state], 3),
           self.output_size[-1],
           self._filter_size,
diff --git a/research/lstm_object_detection/lstm/utils.py b/research/lstm_object_detection/lstm/utils.py
index 5d72e913..47c18921 100644
--- a/research/lstm_object_detection/lstm/utils.py
+++ b/research/lstm_object_detection/lstm/utils.py
@@ -217,7 +217,8 @@ def quantize_op(inputs,
     # While training, collect EMAs of ranges seen, store in min_var, max_var.
     # TFLite requires that 0.0 is always in the [min; max] range.
     range_min = tf.minimum(tf.reduce_min(inputs), 0.0, 'SafeQuantRangeMin')
-    range_max = tf.maximum(tf.reduce_max(inputs), 0.0, 'SafeQuantRangeMax')
+    # We set the lower_bound of max_range to prevent range collapse.
+    range_max = tf.maximum(tf.reduce_max(inputs), 1e-5, 'SafeQuantRangeMax')
     min_val = moving_averages.assign_moving_average(
         min_var, range_min, ema_decay, name='AssignMinEma')
     max_val = moving_averages.assign_moving_average(
diff --git a/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch.py b/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch.py
index 168c36df..22edc97e 100644
--- a/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch.py
+++ b/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch.py
@@ -25,7 +25,6 @@ for details.
 import abc
 import re
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
 
 from object_detection.core import box_list_ops
 from object_detection.core import matcher
@@ -34,8 +33,6 @@ from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 
-slim = contrib_slim
-
 
 class LSTMSSDMetaArch(ssd_meta_arch.SSDMetaArch):
   """LSTM Meta-architecture definition."""
diff --git a/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch_test.py b/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch_test.py
index 057aefb3..0e20f1e7 100644
--- a/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch_test.py
+++ b/research/lstm_object_detection/meta_architectures/lstm_ssd_meta_arch_test.py
@@ -23,7 +23,7 @@ import functools
 
 import numpy as np
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 
 from lstm_object_detection.lstm import lstm_cells
 from lstm_object_detection.meta_architectures import lstm_ssd_meta_arch
@@ -39,8 +39,6 @@ from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
 
-slim = contrib_slim
-
 MAX_TOTAL_NUM_BOXES = 5
 NUM_CLASSES = 1
 
diff --git a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
index e6c13206..5a2d4bd0 100644
--- a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
+++ b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor.py
@@ -16,7 +16,7 @@
 """LSTDInterleavedFeatureExtractor which interleaves multiple MobileNet V2."""
 
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim
+import tf_slim as slim
 
 from tensorflow.python.framework import ops as tf_ops
 from lstm_object_detection.lstm import lstm_cells
@@ -134,9 +134,10 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
     scope_name = self._base_network_scope + '_2'
     with tf.variable_scope(scope_name, reuse=self._reuse_weights) as base_scope:
       if self._low_res:
-        size_small = preprocessed_inputs.get_shape().as_list()[1] / 2
+        height_small = preprocessed_inputs.get_shape().as_list()[1] // 2
+        width_small = preprocessed_inputs.get_shape().as_list()[2] // 2
         inputs_small = tf.image.resize_images(preprocessed_inputs,
-                                              [size_small, size_small])
+                                              [height_small, width_small])
         # Create end point handle for tflite deployment.
         with tf.name_scope(None):
           inputs_small = tf.identity(
@@ -152,7 +153,8 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
           scope=base_scope)
       return net, end_points
 
-  def create_lstm_cell(self, batch_size, output_size, state_saver, state_name):
+  def create_lstm_cell(self, batch_size, output_size, state_saver, state_name,
+                       dtype=tf.float32):
     """Create the LSTM cell, and initialize state if necessary.
 
     Args:
@@ -160,6 +162,8 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
       output_size: output size of the lstm cell, [width, height].
       state_saver: a state saver object with methods `state` and `save_state`.
       state_name: string, the name to use with the state_saver.
+      dtype: dtype to initialize lstm state.
+
     Returns:
       lstm_cell: the lstm cell unit.
       init_state: initial state representations.
@@ -180,7 +184,7 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
         visualize_gates=False)
 
     if state_saver is None:
-      init_state = lstm_cell.init_state('lstm_state', batch_size, tf.float32)
+      init_state = lstm_cell.init_state('lstm_state', batch_size, dtype)
       step = None
     else:
       step = state_saver.state(state_name + '_step')
@@ -222,7 +226,7 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
         33, preprocessed_inputs)
     preprocessed_inputs = ops.pad_to_multiple(
         preprocessed_inputs, self._pad_to_multiple)
-    batch_size = preprocessed_inputs.shape[0].value / unroll_length
+    batch_size = preprocessed_inputs.shape[0].value // unroll_length
     batch_axis = 0
     nets = []
 
@@ -250,7 +254,8 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
       with tf.variable_scope('LSTM', reuse=self._reuse_weights):
         output_size = (large_base_feature_shape[1], large_base_feature_shape[2])
         lstm_cell, init_state, step = self.create_lstm_cell(
-            batch_size, output_size, state_saver, state_name)
+            batch_size, output_size, state_saver, state_name,
+            dtype=preprocessed_inputs.dtype)
 
         nets_seq = [
             tf.split(net, unroll_length, axis=batch_axis) for net in nets
@@ -269,15 +274,16 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
             scope=None)
         self._states_out = states_out
 
-      batcher_ops = None
+      image_features = {}
       if state_saver is not None:
         self._step = state_saver.state(state_name + '_step')
         batcher_ops = [
             state_saver.save_state(state_name + '_c', states_out[-1][0]),
             state_saver.save_state(state_name + '_h', states_out[-1][1]),
             state_saver.save_state(state_name + '_step', self._step + 1)]
-      image_features = {}
-      with tf_ops.control_dependencies(batcher_ops):
+        with tf_ops.control_dependencies(batcher_ops):
+          image_features['layer_19'] = tf.concat(net_seq, 0)
+      else:
         image_features['layer_19'] = tf.concat(net_seq, 0)
 
       # SSD layers.
@@ -289,4 +295,4 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractor(
             insert_1x1_conv=True,
             image_features=image_features,
             pool_residual=True)
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
index e89a6d06..b285f0e4 100644
--- a/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
+++ b/research/lstm_object_detection/models/lstm_ssd_interleaved_mobilenet_v2_feature_extractor_test.py
@@ -15,10 +15,9 @@
 
 """Tests for lstm_ssd_interleaved_mobilenet_v2_feature_extractor."""
 
-import itertools
 import numpy as np
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim
+import tf_slim as slim
 from tensorflow.contrib import training as contrib_training
 
 from lstm_object_detection.models import lstm_ssd_interleaved_mobilenet_v2_feature_extractor
@@ -261,16 +260,16 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractorTest(
     state_channel = 320
     init_state1 = {
         'lstm_state_c': tf.zeros(
-            [image_height/32, image_width/32, state_channel]),
+            [image_height // 32, image_width // 32, state_channel]),
         'lstm_state_h': tf.zeros(
-            [image_height/32, image_width/32, state_channel]),
+            [image_height // 32, image_width // 32, state_channel]),
         'lstm_state_step': tf.zeros([1])
     }
     init_state2 = {
         'lstm_state_c': tf.random_uniform(
-            [image_height/32, image_width/32, state_channel]),
+            [image_height // 32, image_width // 32, state_channel]),
         'lstm_state_h': tf.random_uniform(
-            [image_height/32, image_width/32, state_channel]),
+            [image_height // 32, image_width // 32, state_channel]),
         'lstm_state_step': tf.zeros([1])
     }
     seq = {'dummy': tf.random_uniform([2, 1, 1, 1])}
@@ -326,7 +325,7 @@ class LSTMSSDInterleavedMobilenetV2FeatureExtractorTest(
     image_tensor = np.random.rand(batch_size, image_height, image_width,
                                   3).astype(np.float32)
     feature_maps = self.execute(graph_fn, [image_tensor])
-    for feature_map, expected_shape in itertools.izip(
+    for feature_map, expected_shape in zip(
         feature_maps, expected_feature_map_shapes):
       self.assertAllEqual(feature_map.shape, expected_shape)
 
diff --git a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
index 37e87b1e..cccf740a 100644
--- a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor.py
@@ -16,7 +16,7 @@
 """LSTMSSDFeatureExtractor for MobilenetV1 features."""
 
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 from tensorflow.python.framework import ops as tf_ops
 from lstm_object_detection.lstm import lstm_cells
 from lstm_object_detection.lstm import rnn_decoder
@@ -27,8 +27,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 class LSTMSSDMobileNetV1FeatureExtractor(
     lstm_ssd_meta_arch.LSTMSSDFeatureExtractor):
@@ -85,7 +83,8 @@ class LSTMSSDMobileNetV1FeatureExtractor(
     self._base_network_scope = 'MobilenetV1'
     self._lstm_state_depth = lstm_state_depth
 
-  def create_lstm_cell(self, batch_size, output_size, state_saver, state_name):
+  def create_lstm_cell(self, batch_size, output_size, state_saver, state_name,
+                       dtype=tf.float32):
     """Create the LSTM cell, and initialize state if necessary.
 
     Args:
@@ -93,6 +92,7 @@ class LSTMSSDMobileNetV1FeatureExtractor(
       output_size: output size of the lstm cell, [width, height].
       state_saver: a state saver object with methods `state` and `save_state`.
       state_name: string, the name to use with the state_saver.
+      dtype: dtype to initialize lstm state.
 
     Returns:
       lstm_cell: the lstm cell unit.
@@ -107,7 +107,7 @@ class LSTMSSDMobileNetV1FeatureExtractor(
         visualize_gates=False)
 
     if state_saver is None:
-      init_state = lstm_cell.init_state(state_name, batch_size, tf.float32)
+      init_state = lstm_cell.init_state(state_name, batch_size, dtype)
       step = None
     else:
       step = state_saver.state(state_name + '_step')
@@ -166,11 +166,14 @@ class LSTMSSDMobileNetV1FeatureExtractor(
       with slim.arg_scope(
           [slim.batch_norm], fused=False, is_training=self._is_training):
         # ConvLSTM layers.
-        batch_size = net.shape[0].value / unroll_length
+        batch_size = net.shape[0].value // unroll_length
         with tf.variable_scope('LSTM', reuse=self._reuse_weights) as lstm_scope:
           lstm_cell, init_state, _ = self.create_lstm_cell(
-              batch_size, (net.shape[1].value, net.shape[2].value), state_saver,
-              state_name)
+              batch_size,
+              (net.shape[1].value, net.shape[2].value),
+              state_saver,
+              state_name,
+              dtype=preprocessed_inputs.dtype)
           net_seq = list(tf.split(net, unroll_length))
 
           # Identities added for inputing state tensors externally.
@@ -205,4 +208,4 @@ class LSTMSSDMobileNetV1FeatureExtractor(
               insert_1x1_conv=True,
               image_features=image_features)
 
-    return feature_maps.values()
+    return list(feature_maps.values())
diff --git a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
index 6e8345ee..56ad2745 100644
--- a/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/lstm_object_detection/models/lstm_ssd_mobilenet_v1_feature_extractor_test.py
@@ -17,14 +17,12 @@
 
 import numpy as np
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 from tensorflow.contrib import training as contrib_training
 
 from lstm_object_detection.models import lstm_ssd_mobilenet_v1_feature_extractor as feature_extractor
 from object_detection.models import ssd_feature_extractor_test
 
-slim = contrib_slim
-
 
 class LstmSsdMobilenetV1FeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
diff --git a/research/lstm_object_detection/models/mobilenet_defs.py b/research/lstm_object_detection/models/mobilenet_defs.py
index b66dd824..4f984240 100644
--- a/research/lstm_object_detection/models/mobilenet_defs.py
+++ b/research/lstm_object_detection/models/mobilenet_defs.py
@@ -15,14 +15,11 @@
 """Definitions for modified MobileNet models used in LSTD."""
 
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
-
+import tf_slim as slim
 from nets import mobilenet_v1
 from nets.mobilenet import conv_blocks as mobilenet_convs
 from nets.mobilenet import mobilenet
 
-slim = contrib_slim
-
 
 def mobilenet_v1_lite_def(depth_multiplier, low_res=False):
   """Conv definitions for a lite MobileNet v1 model.
diff --git a/research/lstm_object_detection/trainer.py b/research/lstm_object_detection/trainer.py
index b2f439e0..17ae96c8 100644
--- a/research/lstm_object_detection/trainer.py
+++ b/research/lstm_object_detection/trainer.py
@@ -19,9 +19,15 @@ This file provides a generic training method that can be used to train a
 DetectionModel.
 """
 
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
 import functools
+import six
+from six.moves import range
 import tensorflow.compat.v1 as tf
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 
 from object_detection.builders import optimizer_builder
 from object_detection.core import standard_fields as fields
@@ -29,8 +35,6 @@ from object_detection.utils import ops as util_ops
 from object_detection.utils import variables_helper
 from deployment import model_deploy
 
-slim = contrib_slim
-
 
 def create_input_queue(create_tensor_dict_fn):
   """Sets up reader, prefetcher and returns input queue.
@@ -198,7 +202,7 @@ def get_restore_checkpoint_ops(restore_checkpoints, detection_model,
     available_var_map = (
         variables_helper.get_variables_available_in_checkpoint(
             var_map, restore_checkpoint))
-    for var_name, var in available_var_map.iteritems():
+    for var_name, var in six.iteritems(available_var_map):
       if var in vars_restored:
         tf.logging.info('Variable %s contained in multiple checkpoints',
                      var.op.name)
@@ -210,7 +214,7 @@ def get_restore_checkpoint_ops(restore_checkpoints, detection_model,
     available_ema_var_map = {}
     ckpt_reader = tf.train.NewCheckpointReader(restore_checkpoint)
     ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
-    for var_name, var in available_var_map.iteritems():
+    for var_name, var in six.iteritems(available_var_map):
       var_name_ema = var_name + '/ExponentialMovingAverage'
       if var_name_ema in ckpt_vars_to_shape_map:
         available_ema_var_map[var_name_ema] = var
@@ -218,7 +222,7 @@ def get_restore_checkpoint_ops(restore_checkpoints, detection_model,
         available_ema_var_map[var_name] = var
     available_var_map = available_ema_var_map
     init_saver = tf.train.Saver(available_var_map)
-    if available_var_map.keys():
+    if list(available_var_map.keys()):
       restorers.append(init_saver)
     else:
       tf.logging.info('WARNING: Checkpoint %s has no restorable variables',
diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index 7556f0a2..b6dc9ad0 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -104,6 +104,25 @@ reporting an issue.
 
 ## Release information
 
+### May 19th, 2020
+We have released
+[MobileDets](https://arxiv.org/abs/2004.14525),
+a set of high-performance models for mobile CPUs, DSPs and EdgeTPUs.
+
+* MobileDets outperform MobileNetV3+SSDLite by 1.7 mAP at comparable mobile CPU
+inference latencies. MobileDets also outperform MobileNetV2+SSDLite by 1.9 mAP
+on mobile CPUs, 3.7 mAP on EdgeTPUs and 3.4 mAP on DSPs while running equally
+fast. MobileDets also offer up to 2x speedup over MnasFPN on EdgeTPUs and DSPs.
+
+For each of the three hardware platforms we have released model definition,
+model checkpoints trained on the COCO14 dataset and converted TFLite models in
+fp32 and/or uint8.
+
+<b>Thanks to contributors</b>: Yunyang Xiong, Hanxiao Liu, Suyog Gupta,
+Berkin Akin, Gabriel Bender, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh,
+Bo Chen, Quoc Le, Zhichao Lu.
+
+
 ### May 7th, 2020
 We have released a mobile model with the
 [MnasFPN head](https://arxiv.org/abs/1912.01106).
@@ -119,7 +138,7 @@ the COCO14 dataset and a converted TFLite model.
 
 <b>Thanks to contributors</b>: Bo Chen, Golnaz Ghiasi, Hanxiao Liu,
 Tsung-Yi Lin, Dmitry Kalenichenko, Hartwig Adam, Quoc Le, Zhichao Lu,
-Jonathan Huang.
+Jonathan Huang, Hao Xu.
 
 
 
diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
index 352b4a41..0f340cc9 100644
--- a/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Generates grid anchors on the fly corresponding to multiple CNN layers."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.core import anchor_generator
diff --git a/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
index ff255fb3..bab34b75 100644
--- a/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/flexible_grid_anchor_generator_test.py
@@ -15,7 +15,7 @@
 
 """Tests for anchor_generators.flexible_grid_anchor_generator_test.py."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import flexible_grid_anchor_generator as fg
 from object_detection.utils import test_case
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index 42892563..a31bc879 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -20,7 +20,7 @@ Generates grid anchors on the fly as described in:
 Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import anchor_generator
 from object_detection.core import box_list
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator_test.py b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
index 8de74aa7..292076ea 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.grid_anchor_generator."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.utils import test_case
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index 171396f8..5da24d41 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -25,7 +25,7 @@ Cheng-Yang Fu, Alexander C. Berg
 
 import numpy as np
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.core import anchor_generator
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
index 070d81d3..c9cc507e 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
@@ -17,7 +17,7 @@
 
 import numpy as np
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import multiple_grid_anchor_generator as ag
 from object_detection.utils import test_case
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
index 09f8f779..a3244e1b 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -20,7 +20,7 @@ described in:
 T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.core import anchor_generator
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
index 7afc6e75..82aa8d1d 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -15,7 +15,7 @@
 
 """Tests for anchor_generators.multiscale_grid_anchor_generator_test.py."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.anchor_generators import multiscale_grid_anchor_generator as mg
 from object_detection.utils import test_case
diff --git a/research/object_detection/box_coders/faster_rcnn_box_coder.py b/research/object_detection/box_coders/faster_rcnn_box_coder.py
index af25e21a..e06c1b12 100644
--- a/research/object_detection/box_coders/faster_rcnn_box_coder.py
+++ b/research/object_detection/box_coders/faster_rcnn_box_coder.py
@@ -28,7 +28,7 @@ Faster RCNN box coder follows the coding schema described below:
   See http://arxiv.org/abs/1506.01497 for details.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/faster_rcnn_box_coder_test.py b/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
index 0fed9fec..1cd48279 100644
--- a/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
+++ b/research/object_detection/box_coders/faster_rcnn_box_coder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.box_coder.faster_rcnn_box_coder."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import faster_rcnn_box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/keypoint_box_coder.py b/research/object_detection/box_coders/keypoint_box_coder.py
index fabcc5a8..7bb4bf8b 100644
--- a/research/object_detection/box_coders/keypoint_box_coder.py
+++ b/research/object_detection/box_coders/keypoint_box_coder.py
@@ -35,7 +35,7 @@ to box coordinates):
   anchor-encoded keypoint coordinates.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/keypoint_box_coder_test.py b/research/object_detection/box_coders/keypoint_box_coder_test.py
index 7c10cd2f..5748255c 100644
--- a/research/object_detection/box_coders/keypoint_box_coder_test.py
+++ b/research/object_detection/box_coders/keypoint_box_coder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.box_coder.keypoint_box_coder."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import keypoint_box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/mean_stddev_box_coder_test.py b/research/object_detection/box_coders/mean_stddev_box_coder_test.py
index e4319bea..d94fff11 100644
--- a/research/object_detection/box_coders/mean_stddev_box_coder_test.py
+++ b/research/object_detection/box_coders/mean_stddev_box_coder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.box_coder.mean_stddev_boxcoder."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/square_box_coder.py b/research/object_detection/box_coders/square_box_coder.py
index ee46b689..859320fd 100644
--- a/research/object_detection/box_coders/square_box_coder.py
+++ b/research/object_detection/box_coders/square_box_coder.py
@@ -32,7 +32,7 @@ coder when the objects being detected tend to be square (e.g. faces) and when
 the input images are not distorted via resizing.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/box_coders/square_box_coder_test.py b/research/object_detection/box_coders/square_box_coder_test.py
index d7ef6847..e6bdcb24 100644
--- a/research/object_detection/box_coders/square_box_coder_test.py
+++ b/research/object_detection/box_coders/square_box_coder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.box_coder.square_box_coder."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import square_box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/builders/anchor_generator_builder_test.py b/research/object_detection/builders/anchor_generator_builder_test.py
index 0049f9af..35cdfcae 100644
--- a/research/object_detection/builders/anchor_generator_builder_test.py
+++ b/research/object_detection/builders/anchor_generator_builder_test.py
@@ -24,7 +24,7 @@ import math
 
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.anchor_generators import flexible_grid_anchor_generator
diff --git a/research/object_detection/builders/box_coder_builder_test.py b/research/object_detection/builders/box_coder_builder_test.py
index 286012e9..5db9947c 100644
--- a/research/object_detection/builders/box_coder_builder_test.py
+++ b/research/object_detection/builders/box_coder_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for box_coder_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.box_coders import faster_rcnn_box_coder
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 439efff4..029649d8 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -16,7 +16,7 @@
 """Function to build box predictor from configuration."""
 
 import collections
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.predictors import convolutional_box_predictor
 from object_detection.predictors import convolutional_keras_box_predictor
 from object_detection.predictors import mask_rcnn_box_predictor
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 0835fbb9..72a71b79 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -17,7 +17,7 @@
 """Tests for box_predictor_builder."""
 
 import mock
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
diff --git a/research/object_detection/builders/calibration_builder.py b/research/object_detection/builders/calibration_builder.py
index a99d38b9..4adc170d 100644
--- a/research/object_detection/builders/calibration_builder.py
+++ b/research/object_detection/builders/calibration_builder.py
@@ -15,7 +15,7 @@
 
 """Tensorflow ops to calibrate class predictions and background class."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.utils import shape_utils
 
 
diff --git a/research/object_detection/builders/calibration_builder_test.py b/research/object_detection/builders/calibration_builder_test.py
index e6752635..a077ef4f 100644
--- a/research/object_detection/builders/calibration_builder_test.py
+++ b/research/object_detection/builders/calibration_builder_test.py
@@ -22,7 +22,7 @@ from __future__ import print_function
 import numpy as np
 from scipy import interpolate
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.builders import calibration_builder
 from object_detection.protos import calibration_pb2
 
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
index 30171cce..77208661 100644
--- a/research/object_detection/builders/dataset_builder.py
+++ b/research/object_detection/builders/dataset_builder.py
@@ -27,7 +27,7 @@ from __future__ import division
 from __future__ import print_function
 
 import functools
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.contrib import data as tf_data
 from object_detection.builders import decoder_builder
@@ -118,7 +118,7 @@ def shard_function_for_context(input_context):
 
 
 def build(input_reader_config, batch_size=None, transform_input_data_fn=None,
-          input_context=None):
+          input_context=None, reduce_to_frame_fn=None):
   """Builds a tf.data.Dataset.
 
   Builds a tf.data.Dataset by applying the `transform_input_data_fn` on all
@@ -132,6 +132,8 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None,
     input_context: optional, A tf.distribute.InputContext object used to
       shard filenames and compute per-replica batch_size when this function
       is being called per-replica.
+    reduce_to_frame_fn: Function that extracts frames from tf.SequenceExample
+      type input data.
 
   Returns:
     A tf.data.Dataset based on the input_reader_config.
@@ -151,18 +153,9 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None,
     if not config.input_path:
       raise ValueError('At least one input path must be specified in '
                        '`input_reader_config`.')
-
-    def process_fn(value):
-      """Sets up tf graph that decodes, transforms and pads input data."""
-      processed_tensors = decoder.decode(value)
-      if transform_input_data_fn is not None:
-        processed_tensors = transform_input_data_fn(processed_tensors)
-      return processed_tensors
-
     shard_fn = shard_function_for_context(input_context)
     if input_context is not None:
       batch_size = input_context.get_per_replica_batch_size(batch_size)
-
     dataset = read_dataset(
         functools.partial(tf.data.TFRecordDataset, buffer_size=8 * 1000 * 1000),
         config.input_path[:], input_reader_config, filename_shard_fn=shard_fn)
@@ -170,16 +163,12 @@ def build(input_reader_config, batch_size=None, transform_input_data_fn=None,
       dataset = dataset.shard(input_reader_config.sample_1_of_n_examples, 0)
     # TODO(rathodv): make batch size a required argument once the old binaries
     # are deleted.
-    if batch_size:
-      num_parallel_calls = batch_size * input_reader_config.num_parallel_batches
-    else:
-      num_parallel_calls = input_reader_config.num_parallel_map_calls
-    # TODO(b/123952794): Migrate to V2 function.
-    if hasattr(dataset, 'map_with_legacy_function'):
-      data_map_fn = dataset.map_with_legacy_function
-    else:
-      data_map_fn = dataset.map
-    dataset = data_map_fn(process_fn, num_parallel_calls=num_parallel_calls)
+    dataset = dataset.map(decoder.decode, tf.data.experimental.AUTOTUNE)
+    if reduce_to_frame_fn:
+      dataset = reduce_to_frame_fn(dataset)
+    if transform_input_data_fn is not None:
+      dataset = dataset.map(transform_input_data_fn,
+                            tf.data.experimental.AUTOTUNE)
     if batch_size:
       dataset = dataset.apply(
           tf_data.batch_and_drop_remainder(batch_size))
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
index 5febeee6..741ff3bc 100644
--- a/research/object_detection/builders/dataset_builder_test.py
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -22,17 +22,17 @@ from __future__ import print_function
 import os
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
 from object_detection.builders import dataset_builder
 from object_detection.core import standard_fields as fields
+from object_detection.dataset_tools import seq_example_util
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 from object_detection.utils import test_case
 
-
 # pylint: disable=g-import-not-at-top
 try:
   from tensorflow.contrib import lookup as contrib_lookup
@@ -43,15 +43,17 @@ except ImportError:
 
 
 def get_iterator_next_for_testing(dataset, is_tf2):
+  iterator = dataset.make_initializable_iterator()
+  if not is_tf2:
+    tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  return iterator.get_next()
+
 
-  # In TF2, lookup tables are not supported in one shot iterators, but
-  # initialization is implicit.
-  if is_tf2:
-    return dataset.make_initializable_iterator().get_next()
-  # In TF1, we use one shot iterator because it does not require running
-  # a separate init op.
-  else:
-    return dataset.make_one_shot_iterator().get_next()
+def _get_labelmap_path():
+  """Returns an absolute path to label map file."""
+  parent_path = os.path.dirname(tf.resource_loader.get_data_files_path())
+  return os.path.join(parent_path, 'data',
+                      'pet_label_map.pbtxt')
 
 
 class DatasetBuilderTest(test_case.TestCase):
@@ -111,6 +113,57 @@ class DatasetBuilderTest(test_case.TestCase):
 
     return os.path.join(self.get_temp_dir(), '?????.tfrecord')
 
+  def _make_random_serialized_jpeg_images(self, num_frames, image_height,
+                                          image_width):
+    def graph_fn():
+      images = tf.cast(tf.random.uniform(
+          [num_frames, image_height, image_width, 3],
+          maxval=256,
+          dtype=tf.int32), dtype=tf.uint8)
+      images_list = tf.unstack(images, axis=0)
+      encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+      return encoded_images_list
+
+    encoded_images = self.execute(graph_fn, [])
+    return encoded_images
+
+  def create_tf_record_sequence_example(self):
+    path = os.path.join(self.get_temp_dir(), 'seq_tfrecord')
+    writer = tf.python_io.TFRecordWriter(path)
+
+    num_frames = 4
+    image_height = 4
+    image_width = 5
+    image_source_ids = [str(i) for i in range(num_frames)]
+    with self.test_session():
+      encoded_images = self._make_random_serialized_jpeg_images(
+          num_frames, image_height, image_width)
+      sequence_example_serialized = seq_example_util.make_sequence_example(
+          dataset_name='video_dataset',
+          video_id='video',
+          encoded_images=encoded_images,
+          image_height=image_height,
+          image_width=image_width,
+          image_source_ids=image_source_ids,
+          image_format='JPEG',
+          is_annotated=[[1], [1], [1], [1]],
+          bboxes=[
+              [[]],  # Frame 0.
+              [[0., 0., 1., 1.]],  # Frame 1.
+              [[0., 0., 1., 1.],
+               [0.1, 0.1, 0.2, 0.2]],  # Frame 2.
+              [[]],  # Frame 3.
+          ],
+          label_strings=[
+              [],  # Frame 0.
+              ['Abyssinian'],  # Frame 1.
+              ['Abyssinian', 'american_bulldog'],  # Frame 2.
+              [],  # Frame 3
+          ]).SerializeToString()
+      writer.write(sequence_example_serialized)
+      writer.close()
+    return path
+
   def test_build_tf_record_input_reader(self):
     tf_record_path = self.create_tf_record()
 
@@ -143,6 +196,71 @@ class DatasetBuilderTest(test_case.TestCase):
         [0.0, 0.0, 1.0, 1.0],
         output_dict[fields.InputDataFields.groundtruth_boxes][0][0])
 
+  def get_mock_reduce_to_frame_fn(self):
+    def mock_reduce_to_frame_fn(dataset):
+      def get_frame(tensor_dict):
+        out_tensor_dict = {}
+        out_tensor_dict[fields.InputDataFields.source_id] = (
+            tensor_dict[fields.InputDataFields.source_id][0])
+        return out_tensor_dict
+      return dataset.map(get_frame, tf.data.experimental.AUTOTUNE)
+    return mock_reduce_to_frame_fn
+
+  def test_build_tf_record_input_reader_sequence_example_train(self):
+    tf_record_path = self.create_tf_record_sequence_example()
+    label_map_path = _get_labelmap_path()
+    input_type = 'TF_SEQUENCE_EXAMPLE'
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      input_type: {1}
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path, input_type)
+    input_reader_proto = input_reader_pb2.InputReader()
+    input_reader_proto.label_map_path = label_map_path
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    reduce_to_frame_fn = self.get_mock_reduce_to_frame_fn()
+
+    def graph_fn():
+      return get_iterator_next_for_testing(
+          dataset_builder.build(input_reader_proto, batch_size=1,
+                                reduce_to_frame_fn=reduce_to_frame_fn),
+          self.is_tf2())
+
+    output_dict = self.execute(graph_fn, [])
+
+    self.assertEqual((1,),
+                     output_dict[fields.InputDataFields.source_id].shape)
+
+  def test_build_tf_record_input_reader_sequence_example_test(self):
+    tf_record_path = self.create_tf_record_sequence_example()
+    input_type = 'TF_SEQUENCE_EXAMPLE'
+    label_map_path = _get_labelmap_path()
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      input_type: {1}
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path, input_type)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    input_reader_proto.label_map_path = label_map_path
+    reduce_to_frame_fn = self.get_mock_reduce_to_frame_fn()
+    def graph_fn():
+      return get_iterator_next_for_testing(
+          dataset_builder.build(input_reader_proto, batch_size=1,
+                                reduce_to_frame_fn=reduce_to_frame_fn),
+          self.is_tf2())
+
+    output_dict = self.execute(graph_fn, [])
+
+    self.assertEqual((1,),
+                     output_dict[fields.InputDataFields.source_id].shape)
+
   def test_build_tf_record_input_reader_and_load_instance_masks(self):
     tf_record_path = self.create_tf_record()
 
diff --git a/research/object_detection/builders/decoder_builder.py b/research/object_detection/builders/decoder_builder.py
index d5fde547..d3cac57d 100644
--- a/research/object_detection/builders/decoder_builder.py
+++ b/research/object_detection/builders/decoder_builder.py
@@ -23,6 +23,7 @@ from __future__ import division
 from __future__ import print_function
 
 from object_detection.data_decoders import tf_example_decoder
+from object_detection.data_decoders import tf_sequence_example_decoder
 from object_detection.protos import input_reader_pb2
 
 
@@ -46,16 +47,24 @@ def build(input_reader_config):
     label_map_proto_file = None
     if input_reader_config.HasField('label_map_path'):
       label_map_proto_file = input_reader_config.label_map_path
-    decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=input_reader_config.load_instance_masks,
-        load_multiclass_scores=input_reader_config.load_multiclass_scores,
-        load_context_features=input_reader_config.load_context_features,
-        instance_mask_type=input_reader_config.mask_type,
-        label_map_proto_file=label_map_proto_file,
-        use_display_name=input_reader_config.use_display_name,
-        num_additional_channels=input_reader_config.num_additional_channels,
-        num_keypoints=input_reader_config.num_keypoints)
-
-    return decoder
+    input_type = input_reader_config.input_type
+    if input_type == input_reader_pb2.InputType.TF_EXAMPLE:
+      decoder = tf_example_decoder.TfExampleDecoder(
+          load_instance_masks=input_reader_config.load_instance_masks,
+          load_multiclass_scores=input_reader_config.load_multiclass_scores,
+          load_context_features=input_reader_config.load_context_features,
+          instance_mask_type=input_reader_config.mask_type,
+          label_map_proto_file=label_map_proto_file,
+          use_display_name=input_reader_config.use_display_name,
+          num_additional_channels=input_reader_config.num_additional_channels,
+          num_keypoints=input_reader_config.num_keypoints,
+          expand_hierarchy_labels=input_reader_config.expand_labels_hierarchy)
+      return decoder
+    elif input_type == input_reader_pb2.InputType.TF_SEQUENCE_EXAMPLE:
+      decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder(
+          label_map_proto_file=label_map_proto_file,
+          load_context_features=input_reader_config.load_context_features)
+      return decoder
+    raise ValueError('Unsupported input_type in config.')
 
   raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/decoder_builder_test.py b/research/object_detection/builders/decoder_builder_test.py
index 49d17cfa..767c108e 100644
--- a/research/object_detection/builders/decoder_builder_test.py
+++ b/research/object_detection/builders/decoder_builder_test.py
@@ -19,16 +19,25 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import os
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import decoder_builder
 from object_detection.core import standard_fields as fields
+from object_detection.dataset_tools import seq_example_util
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
 
+def _get_labelmap_path():
+  """Returns an absolute path to label map file."""
+  parent_path = os.path.dirname(tf.resource_loader.get_data_files_path())
+  return os.path.join(parent_path, 'data',
+                      'pet_label_map.pbtxt')
+
+
 class DecoderBuilderTest(tf.test.TestCase):
 
   def _make_serialized_tf_example(self, has_additional_channels=False):
@@ -60,6 +69,50 @@ class DecoderBuilderTest(tf.test.TestCase):
     example = tf.train.Example(features=tf.train.Features(feature=features))
     return example.SerializeToString()
 
+  def _make_random_serialized_jpeg_images(self, num_frames, image_height,
+                                          image_width):
+    images = tf.cast(tf.random.uniform(
+        [num_frames, image_height, image_width, 3],
+        maxval=256,
+        dtype=tf.int32), dtype=tf.uint8)
+    images_list = tf.unstack(images, axis=0)
+    encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+    with tf.Session() as sess:
+      encoded_images = sess.run(encoded_images_list)
+    return encoded_images
+
+  def _make_serialized_tf_sequence_example(self):
+    num_frames = 4
+    image_height = 20
+    image_width = 30
+    image_source_ids = [str(i) for i in range(num_frames)]
+    with self.test_session():
+      encoded_images = self._make_random_serialized_jpeg_images(
+          num_frames, image_height, image_width)
+      sequence_example_serialized = seq_example_util.make_sequence_example(
+          dataset_name='video_dataset',
+          video_id='video',
+          encoded_images=encoded_images,
+          image_height=image_height,
+          image_width=image_width,
+          image_source_ids=image_source_ids,
+          image_format='JPEG',
+          is_annotated=[[1], [1], [1], [1]],
+          bboxes=[
+              [[]],  # Frame 0.
+              [[0., 0., 1., 1.]],  # Frame 1.
+              [[0., 0., 1., 1.],
+               [0.1, 0.1, 0.2, 0.2]],  # Frame 2.
+              [[]],  # Frame 3.
+          ],
+          label_strings=[
+              [],  # Frame 0.
+              ['Abyssinian'],  # Frame 1.
+              ['Abyssinian', 'american_bulldog'],  # Frame 2.
+              [],  # Frame 3
+          ]).SerializeToString()
+    return sequence_example_serialized
+
   def test_build_tf_record_input_reader(self):
     input_reader_text_proto = 'tf_record_input_reader {}'
     input_reader_proto = input_reader_pb2.InputReader()
@@ -82,6 +135,43 @@ class DecoderBuilderTest(tf.test.TestCase):
         [0.0, 0.0, 1.0, 1.0],
         output_dict[fields.InputDataFields.groundtruth_boxes][0])
 
+  def test_build_tf_record_input_reader_sequence_example(self):
+    label_map_path = _get_labelmap_path()
+    input_reader_text_proto = """
+      input_type: TF_SEQUENCE_EXAMPLE
+      tf_record_input_reader {}
+    """
+    input_reader_proto = input_reader_pb2.InputReader()
+    input_reader_proto.label_map_path = label_map_path
+    text_format.Parse(input_reader_text_proto, input_reader_proto)
+
+    decoder = decoder_builder.build(input_reader_proto)
+    tensor_dict = decoder.decode(self._make_serialized_tf_sequence_example())
+
+    with tf.train.MonitoredSession() as sess:
+      output_dict = sess.run(tensor_dict)
+
+    expected_groundtruth_classes = [[-1, -1], [1, -1], [1, 2], [-1, -1]]
+    expected_groundtruth_boxes = [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],
+                                  [[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0]],
+                                  [[0.0, 0.0, 1.0, 1.0], [0.1, 0.1, 0.2, 0.2]],
+                                  [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]]
+    expected_num_groundtruth_boxes = [0, 1, 2, 0]
+
+    self.assertNotIn(
+        fields.InputDataFields.groundtruth_instance_masks, output_dict)
+    # Sequence example images are encoded.
+    self.assertEqual((4,), output_dict[fields.InputDataFields.image].shape)
+    self.assertAllEqual(expected_groundtruth_classes,
+                        output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
+        (4, 2, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllClose(expected_groundtruth_boxes,
+                        output_dict[fields.InputDataFields.groundtruth_boxes])
+    self.assertAllClose(
+        expected_num_groundtruth_boxes,
+        output_dict[fields.InputDataFields.num_groundtruth_boxes])
+
   def test_build_tf_record_input_reader_and_load_instance_masks(self):
     input_reader_text_proto = """
       load_instance_masks: true
diff --git a/research/object_detection/builders/graph_rewriter_builder.py b/research/object_detection/builders/graph_rewriter_builder.py
index ddf8bb12..9cbeb4a1 100644
--- a/research/object_detection/builders/graph_rewriter_builder.py
+++ b/research/object_detection/builders/graph_rewriter_builder.py
@@ -14,11 +14,10 @@
 # ==============================================================================
 """Functions for quantized training and evaluation."""
 
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import layers as contrib_layers
   from tensorflow.contrib import quantize as contrib_quantize
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
@@ -49,7 +48,6 @@ def build(graph_rewriter_config, is_training):
       contrib_quantize.experimental_create_eval_graph(
           input_graph=tf.get_default_graph()
       )
-
-    contrib_layers.summarize_collection('quant_vars')
+    slim.summarize_collection('quant_vars')
 
   return graph_rewrite_fn
diff --git a/research/object_detection/builders/graph_rewriter_builder_test.py b/research/object_detection/builders/graph_rewriter_builder_test.py
index 34a25d30..02692ce9 100644
--- a/research/object_detection/builders/graph_rewriter_builder_test.py
+++ b/research/object_detection/builders/graph_rewriter_builder_test.py
@@ -14,13 +14,14 @@
 # ==============================================================================
 """Tests for graph_rewriter_builder."""
 import mock
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from object_detection.builders import graph_rewriter_builder
 from object_detection.protos import graph_rewriter_pb2
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import layers as contrib_layers
   from tensorflow.contrib import quantize as contrib_quantize
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
@@ -34,7 +35,7 @@ class QuantizationBuilderTest(tf.test.TestCase):
     with mock.patch.object(
         contrib_quantize,
         'experimental_create_training_graph') as mock_quant_fn:
-      with mock.patch.object(contrib_layers,
+      with mock.patch.object(slim,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_proto.quantization.delay = 10
@@ -51,7 +52,7 @@ class QuantizationBuilderTest(tf.test.TestCase):
   def testQuantizationBuilderSetsUpCorrectEvalArguments(self):
     with mock.patch.object(contrib_quantize,
                            'experimental_create_eval_graph') as mock_quant_fn:
-      with mock.patch.object(contrib_layers,
+      with mock.patch.object(slim,
                              'summarize_collection') as mock_summarize_col:
         graph_rewriter_proto = graph_rewriter_pb2.GraphRewriter()
         graph_rewriter_proto.quantization.delay = 10
diff --git a/research/object_detection/builders/hyperparams_builder.py b/research/object_detection/builders/hyperparams_builder.py
index d8a188bf..f34e1112 100644
--- a/research/object_detection/builders/hyperparams_builder.py
+++ b/research/object_detection/builders/hyperparams_builder.py
@@ -14,19 +14,13 @@
 # ==============================================================================
 
 """Builder function to construct tf-slim arg_scope for convolution, fc ops."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.core import freezable_batch_norm
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import context_manager
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import slim
-  from tensorflow.contrib import layers as contrib_layers
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
 # pylint: enable=g-import-not-at-top
 
 
@@ -223,7 +217,7 @@ def build(hyperparams_config, is_training):
     batch_norm_params = _build_batch_norm_params(
         hyperparams_config.batch_norm, is_training)
   if hyperparams_config.HasField('group_norm'):
-    normalizer_fn = contrib_layers.group_norm
+    normalizer_fn = slim.group_norm
   affected_ops = [slim.conv2d, slim.separable_conv2d, slim.conv2d_transpose]
   if hyperparams_config.HasField('op') and (
       hyperparams_config.op == hyperparams_pb2.Hyperparams.FC):
diff --git a/research/object_detection/builders/hyperparams_builder_test.py b/research/object_detection/builders/hyperparams_builder_test.py
index be6d5749..0f92f7d7 100644
--- a/research/object_detection/builders/hyperparams_builder_test.py
+++ b/research/object_detection/builders/hyperparams_builder_test.py
@@ -17,22 +17,14 @@
 """Tests object_detection.core.hyperparams_builder."""
 
 import numpy as np
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from google.protobuf import text_format
 
 from object_detection.builders import hyperparams_builder
 from object_detection.core import freezable_batch_norm
 from object_detection.protos import hyperparams_pb2
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import slim
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
-# pylint: enable=g-import-not-at-top
-
 
 def _get_scope_key(op):
   return getattr(op, '_key_op', str(op))
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index 794af688..1a3f096f 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Builder function for image resizing operations."""
 import functools
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import preprocessor
 from object_detection.protos import image_resizer_pb2
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index 655cbece..62ea5dc9 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Tests for object_detection.builders.image_resizer_builder."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from object_detection.builders import image_resizer_builder
 from object_detection.protos import image_resizer_pb2
diff --git a/research/object_detection/builders/input_reader_builder.py b/research/object_detection/builders/input_reader_builder.py
index e4717a87..0ab9c05b 100644
--- a/research/object_detection/builders/input_reader_builder.py
+++ b/research/object_detection/builders/input_reader_builder.py
@@ -28,20 +28,21 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.data_decoders import tf_example_decoder
+from object_detection.data_decoders import tf_sequence_example_decoder
 from object_detection.protos import input_reader_pb2
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
 # pylint: enable=g-import-not-at-top
 
-parallel_reader = contrib_slim.parallel_reader
+parallel_reader = slim.parallel_reader
 
 
 def build(input_reader_config):
@@ -80,11 +81,18 @@ def build(input_reader_config):
     label_map_proto_file = None
     if input_reader_config.HasField('label_map_path'):
       label_map_proto_file = input_reader_config.label_map_path
-    decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=input_reader_config.load_instance_masks,
-        instance_mask_type=input_reader_config.mask_type,
-        label_map_proto_file=label_map_proto_file,
-        load_context_features=input_reader_config.load_context_features)
-    return decoder.decode(string_tensor)
-
+    input_type = input_reader_config.input_type
+    if input_type == input_reader_pb2.InputType.TF_EXAMPLE:
+      decoder = tf_example_decoder.TfExampleDecoder(
+          load_instance_masks=input_reader_config.load_instance_masks,
+          instance_mask_type=input_reader_config.mask_type,
+          label_map_proto_file=label_map_proto_file,
+          load_context_features=input_reader_config.load_context_features)
+      return decoder.decode(string_tensor)
+    elif input_type == input_reader_pb2.InputType.TF_SEQUENCE_EXAMPLE:
+      decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder(
+          label_map_proto_file=label_map_proto_file,
+          load_context_features=input_reader_config.load_context_features)
+      return decoder.decode(string_tensor)
+    raise ValueError('Unsupported input_type.')
   raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/input_reader_builder_test.py b/research/object_detection/builders/input_reader_builder_test.py
index 69a09c4a..14a8eb81 100644
--- a/research/object_detection/builders/input_reader_builder_test.py
+++ b/research/object_detection/builders/input_reader_builder_test.py
@@ -17,16 +17,24 @@
 
 import os
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
 from object_detection.builders import input_reader_builder
 from object_detection.core import standard_fields as fields
+from object_detection.dataset_tools import seq_example_util
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
 
+def _get_labelmap_path():
+  """Returns an absolute path to label map file."""
+  parent_path = os.path.dirname(tf.resource_loader.get_data_files_path())
+  return os.path.join(parent_path, 'data',
+                      'pet_label_map.pbtxt')
+
+
 class InputReaderBuilderTest(tf.test.TestCase):
 
   def create_tf_record(self):
@@ -54,6 +62,56 @@ class InputReaderBuilderTest(tf.test.TestCase):
 
     return path
 
+  def _make_random_serialized_jpeg_images(self, num_frames, image_height,
+                                          image_width):
+    images = tf.cast(tf.random.uniform(
+        [num_frames, image_height, image_width, 3],
+        maxval=256,
+        dtype=tf.int32), dtype=tf.uint8)
+    images_list = tf.unstack(images, axis=0)
+    encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+    with tf.Session() as sess:
+      encoded_images = sess.run(encoded_images_list)
+    return encoded_images
+
+  def create_tf_record_sequence_example(self):
+    path = os.path.join(self.get_temp_dir(), 'tfrecord')
+    writer = tf.python_io.TFRecordWriter(path)
+    num_frames = 4
+    image_height = 20
+    image_width = 30
+    image_source_ids = [str(i) for i in range(num_frames)]
+    with self.test_session():
+      encoded_images = self._make_random_serialized_jpeg_images(
+          num_frames, image_height, image_width)
+      sequence_example_serialized = seq_example_util.make_sequence_example(
+          dataset_name='video_dataset',
+          video_id='video',
+          encoded_images=encoded_images,
+          image_height=image_height,
+          image_width=image_width,
+          image_source_ids=image_source_ids,
+          image_format='JPEG',
+          is_annotated=[[1], [1], [1], [1]],
+          bboxes=[
+              [[]],  # Frame 0.
+              [[0., 0., 1., 1.]],  # Frame 1.
+              [[0., 0., 1., 1.],
+               [0.1, 0.1, 0.2, 0.2]],  # Frame 2.
+              [[]],  # Frame 3.
+          ],
+          label_strings=[
+              [],  # Frame 0.
+              ['Abyssinian'],  # Frame 1.
+              ['Abyssinian', 'american_bulldog'],  # Frame 2.
+              [],  # Frame 3
+          ]).SerializeToString()
+
+    writer.write(sequence_example_serialized)
+    writer.close()
+
+    return path
+
   def create_tf_record_with_context(self):
     path = os.path.join(self.get_temp_dir(), 'tfrecord')
     writer = tf.python_io.TFRecordWriter(path)
@@ -124,6 +182,46 @@ class InputReaderBuilderTest(tf.test.TestCase):
         [0.0, 0.0, 1.0, 1.0],
         output_dict[fields.InputDataFields.groundtruth_boxes][0])
 
+  def test_build_tf_record_input_reader_sequence_example(self):
+    tf_record_path = self.create_tf_record_sequence_example()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      input_type: TF_SEQUENCE_EXAMPLE
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    input_reader_proto.label_map_path = _get_labelmap_path()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    tensor_dict = input_reader_builder.build(input_reader_proto)
+
+    with tf.train.MonitoredSession() as sess:
+      output_dict = sess.run(tensor_dict)
+
+    expected_groundtruth_classes = [[-1, -1], [1, -1], [1, 2], [-1, -1]]
+    expected_groundtruth_boxes = [[[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]],
+                                  [[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0]],
+                                  [[0.0, 0.0, 1.0, 1.0], [0.1, 0.1, 0.2, 0.2]],
+                                  [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]]
+    expected_num_groundtruth_boxes = [0, 1, 2, 0]
+
+    self.assertNotIn(
+        fields.InputDataFields.groundtruth_instance_masks, output_dict)
+    # sequence example images are encoded
+    self.assertEqual((4,), output_dict[fields.InputDataFields.image].shape)
+    self.assertAllEqual(expected_groundtruth_classes,
+                        output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEqual(
+        (4, 2, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllClose(expected_groundtruth_boxes,
+                        output_dict[fields.InputDataFields.groundtruth_boxes])
+    self.assertAllClose(
+        expected_num_groundtruth_boxes,
+        output_dict[fields.InputDataFields.num_groundtruth_boxes])
+
   def test_build_tf_record_input_reader_with_context(self):
     tf_record_path = self.create_tf_record_with_context()
 
diff --git a/research/object_detection/builders/losses_builder_test.py b/research/object_detection/builders/losses_builder_test.py
index 18c78cd5..b37b7f31 100644
--- a/research/object_detection/builders/losses_builder_test.py
+++ b/research/object_detection/builders/losses_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for losses_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import losses_builder
diff --git a/research/object_detection/builders/matcher_builder_test.py b/research/object_detection/builders/matcher_builder_test.py
index 66854491..451e1f9c 100644
--- a/research/object_detection/builders/matcher_builder_test.py
+++ b/research/object_detection/builders/matcher_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for matcher_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import matcher_builder
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 9f5555a8..d5afb825 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -75,6 +75,9 @@ if tf_version.is_tf1():
   from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
   from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3LargeFeatureExtractor
   from object_detection.models.ssd_mobilenet_v3_feature_extractor import SSDMobileNetV3SmallFeatureExtractor
+  from object_detection.models.ssd_mobiledet_feature_extractor import SSDMobileDetCPUFeatureExtractor
+  from object_detection.models.ssd_mobiledet_feature_extractor import SSDMobileDetDSPFeatureExtractor
+  from object_detection.models.ssd_mobiledet_feature_extractor import SSDMobileDetEdgeTPUFeatureExtractor
   from object_detection.models.ssd_pnasnet_feature_extractor import SSDPNASNetFeatureExtractor
   from object_detection.predictors import rfcn_box_predictor
 # pylint: enable=g-import-not-at-top
@@ -156,6 +159,9 @@ if tf_version.is_tf1():
           EmbeddedSSDMobileNetV1FeatureExtractor,
       'ssd_pnasnet':
           SSDPNASNetFeatureExtractor,
+      'ssd_mobiledet_cpu': SSDMobileDetCPUFeatureExtractor,
+      'ssd_mobiledet_dsp': SSDMobileDetDSPFeatureExtractor,
+      'ssd_mobiledet_edgetpu': SSDMobileDetEdgeTPUFeatureExtractor,
   }
 
   FASTER_RCNN_FEATURE_EXTRACTOR_CLASS_MAP = {
@@ -794,7 +800,25 @@ def object_center_proto_to_params(oc_config):
       object_center_loss_weight=oc_config.object_center_loss_weight,
       heatmap_bias_init=oc_config.heatmap_bias_init,
       min_box_overlap_iou=oc_config.min_box_overlap_iou,
-      max_box_predictions=oc_config.max_box_predictions)
+      max_box_predictions=oc_config.max_box_predictions,
+      use_labeled_classes=oc_config.use_labeled_classes)
+
+
+def mask_proto_to_params(mask_config):
+  """Converts CenterNet.MaskEstimation proto to parameter namedtuple."""
+  loss = losses_pb2.Loss()
+  # Add dummy localization loss to avoid the loss_builder throwing error.
+  loss.localization_loss.weighted_l2.CopyFrom(
+      losses_pb2.WeightedL2LocalizationLoss())
+  loss.classification_loss.CopyFrom(mask_config.classification_loss)
+  classification_loss, _, _, _, _, _, _ = (losses_builder.build(loss))
+  return center_net_meta_arch.MaskParams(
+      classification_loss=classification_loss,
+      task_loss_weight=mask_config.task_loss_weight,
+      mask_height=mask_config.mask_height,
+      mask_width=mask_config.mask_width,
+      score_threshold=mask_config.score_threshold,
+      heatmap_bias_init=mask_config.heatmap_bias_init)
 
 
 def _build_center_net_model(center_net_config, is_training, add_summaries):
@@ -844,6 +868,11 @@ def _build_center_net_model(center_net_config, is_training, add_summaries):
         keypoint_class_id_set.add(kp_params.class_id)
     if len(all_keypoint_indices) > len(set(all_keypoint_indices)):
       raise ValueError('Some keypoint indices are used more than once.')
+
+  mask_params = None
+  if center_net_config.HasField('mask_estimation_task'):
+    mask_params = mask_proto_to_params(center_net_config.mask_estimation_task)
+
   return center_net_meta_arch.CenterNetMetaArch(
       is_training=is_training,
       add_summaries=add_summaries,
@@ -852,7 +881,8 @@ def _build_center_net_model(center_net_config, is_training, add_summaries):
       image_resizer_fn=image_resizer_fn,
       object_center_params=object_center_params,
       object_detection_params=object_detection_params,
-      keypoint_params_dict=keypoint_params_dict)
+      keypoint_params_dict=keypoint_params_dict,
+      mask_params=mask_params)
 
 
 def _build_center_net_feature_extractor(
diff --git a/research/object_detection/builders/model_builder_tf1_test.py b/research/object_detection/builders/model_builder_tf1_test.py
index eedef968..a4d2913f 100644
--- a/research/object_detection/builders/model_builder_tf1_test.py
+++ b/research/object_detection/builders/model_builder_tf1_test.py
@@ -16,7 +16,7 @@
 """Tests for model_builder under TensorFlow 1.X."""
 
 from absl.testing import parameterized
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.builders import model_builder
 from object_detection.builders import model_builder_test
diff --git a/research/object_detection/builders/optimizer_builder.py b/research/object_detection/builders/optimizer_builder.py
index 3576b18c..548b5cdc 100644
--- a/research/object_detection/builders/optimizer_builder.py
+++ b/research/object_detection/builders/optimizer_builder.py
@@ -15,7 +15,7 @@
 
 """Functions to build DetectionModel training optimizers."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 from tensorflow.contrib import opt as tf_opt
diff --git a/research/object_detection/builders/optimizer_builder_tf1_test.py b/research/object_detection/builders/optimizer_builder_tf1_test.py
index aafb53e6..9a6d1e40 100644
--- a/research/object_detection/builders/optimizer_builder_tf1_test.py
+++ b/research/object_detection/builders/optimizer_builder_tf1_test.py
@@ -21,7 +21,7 @@ from __future__ import division
 from __future__ import print_function
 
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/builders/post_processing_builder.py b/research/object_detection/builders/post_processing_builder.py
index 16b6a4c3..18795f58 100644
--- a/research/object_detection/builders/post_processing_builder.py
+++ b/research/object_detection/builders/post_processing_builder.py
@@ -16,7 +16,7 @@
 """Builder function for post processing operations."""
 import functools
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.builders import calibration_builder
 from object_detection.core import post_processing
 from object_detection.protos import post_processing_pb2
@@ -102,7 +102,8 @@ def _build_non_max_suppressor(nms_config):
       soft_nms_sigma=nms_config.soft_nms_sigma,
       use_partitioned_nms=nms_config.use_partitioned_nms,
       use_combined_nms=nms_config.use_combined_nms,
-      change_coordinate_frame=nms_config.change_coordinate_frame)
+      change_coordinate_frame=nms_config.change_coordinate_frame,
+      use_hard_nms=nms_config.use_hard_nms)
 
   return non_max_suppressor_fn
 
diff --git a/research/object_detection/builders/post_processing_builder_test.py b/research/object_detection/builders/post_processing_builder_test.py
index 5514a517..d163aa8f 100644
--- a/research/object_detection/builders/post_processing_builder_test.py
+++ b/research/object_detection/builders/post_processing_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for post_processing_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from object_detection.builders import post_processing_builder
 from object_detection.protos import post_processing_pb2
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index a6723fe1..aa6a6bc9 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -15,7 +15,7 @@
 
 """Builder for preprocessing steps."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import preprocessor
 from object_detection.protos import preprocessor_pb2
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index 28ab8fb9..4c283238 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for preprocessor_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/builders/region_similarity_calculator_builder_test.py b/research/object_detection/builders/region_similarity_calculator_builder_test.py
index ca3a5512..da72e736 100644
--- a/research/object_detection/builders/region_similarity_calculator_builder_test.py
+++ b/research/object_detection/builders/region_similarity_calculator_builder_test.py
@@ -15,7 +15,7 @@
 
 """Tests for region_similarity_calculator_builder."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import region_similarity_calculator_builder
diff --git a/research/object_detection/builders/target_assigner_builder_test.py b/research/object_detection/builders/target_assigner_builder_test.py
index 9ca71b1d..27960021 100644
--- a/research/object_detection/builders/target_assigner_builder_test.py
+++ b/research/object_detection/builders/target_assigner_builder_test.py
@@ -14,7 +14,7 @@
 # limitations under the License.
 # ==============================================================================
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/core/anchor_generator.py b/research/object_detection/core/anchor_generator.py
index 070b1d68..69e29d84 100644
--- a/research/object_detection/core/anchor_generator.py
+++ b/research/object_detection/core/anchor_generator.py
@@ -38,7 +38,7 @@ from abc import abstractmethod
 
 import six
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 class AnchorGenerator(six.with_metaclass(ABCMeta, object)):
diff --git a/research/object_detection/core/balanced_positive_negative_sampler.py b/research/object_detection/core/balanced_positive_negative_sampler.py
index 1d28090a..6e09537d 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler.py
@@ -30,7 +30,7 @@ It also ensures the length of output of the subsample is always batch_size, even
 when number of examples set to True in indicator is less than batch_size.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import minibatch_sampler
 
diff --git a/research/object_detection/core/balanced_positive_negative_sampler_test.py b/research/object_detection/core/balanced_positive_negative_sampler_test.py
index b302b802..10b8ca74 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler_test.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler_test.py
@@ -16,7 +16,7 @@
 """Tests for object_detection.core.balanced_positive_negative_sampler."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import balanced_positive_negative_sampler
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/batch_multiclass_nms_test.py b/research/object_detection/core/batch_multiclass_nms_test.py
index cd0c56bf..d99116a4 100644
--- a/research/object_detection/core/batch_multiclass_nms_test.py
+++ b/research/object_detection/core/batch_multiclass_nms_test.py
@@ -19,7 +19,7 @@ from __future__ import print_function
 from absl.testing import parameterized
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import post_processing
 from object_detection.utils import test_case
 
diff --git a/research/object_detection/core/batcher.py b/research/object_detection/core/batcher.py
index 95714faf..832e2242 100644
--- a/research/object_detection/core/batcher.py
+++ b/research/object_detection/core/batcher.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import collections
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import prefetcher
 from object_detection.utils import tf_version
diff --git a/research/object_detection/core/batcher_tf1_test.py b/research/object_detection/core/batcher_tf1_test.py
index ed648b89..8f443a94 100644
--- a/research/object_detection/core/batcher_tf1_test.py
+++ b/research/object_detection/core/batcher_tf1_test.py
@@ -21,13 +21,11 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.core import batcher
 
-slim = contrib_slim
-
 
 class BatcherTest(tf.test.TestCase):
 
diff --git a/research/object_detection/core/box_coder.py b/research/object_detection/core/box_coder.py
index 82f084d6..c6e54a44 100644
--- a/research/object_detection/core/box_coder.py
+++ b/research/object_detection/core/box_coder.py
@@ -35,7 +35,7 @@ from abc import abstractmethod
 from abc import abstractproperty
 
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import shape_utils
 
diff --git a/research/object_detection/core/box_coder_test.py b/research/object_detection/core/box_coder_test.py
index 5c494813..52765a9d 100644
--- a/research/object_detection/core/box_coder_test.py
+++ b/research/object_detection/core/box_coder_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.core.box_coder."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/core/box_list.py b/research/object_detection/core/box_list.py
index cda75755..7b6b97e9 100644
--- a/research/object_detection/core/box_list.py
+++ b/research/object_detection/core/box_list.py
@@ -34,7 +34,7 @@ Some other notes:
   * Tensors are always provided as (flat) [N, 4] tensors.
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import shape_utils
 
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index 06091a34..159845b6 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -28,7 +28,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.utils import ops
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index 810ed68c..b572dff9 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.core.box_list_ops."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
diff --git a/research/object_detection/core/box_list_test.py b/research/object_detection/core/box_list_test.py
index d6ac68dd..c1389dbf 100644
--- a/research/object_detection/core/box_list_test.py
+++ b/research/object_detection/core/box_list_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.core.box_list."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 91a6647e..342bca83 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -27,7 +27,7 @@ These modules are separated from the main model since the same
 few box predictor architectures are shared across many models.
 """
 from abc import abstractmethod
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 BOX_ENCODINGS = 'box_encodings'
 CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'
diff --git a/research/object_detection/core/class_agnostic_nms_test.py b/research/object_detection/core/class_agnostic_nms_test.py
index 6dc9814d..ed205c51 100644
--- a/research/object_detection/core/class_agnostic_nms_test.py
+++ b/research/object_detection/core/class_agnostic_nms_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Tests for google3.third_party.tensorflow_models.object_detection.core.class_agnostic_nms."""
 from absl.testing import parameterized
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import post_processing
 from object_detection.core import standard_fields as fields
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/freezable_batch_norm.py b/research/object_detection/core/freezable_batch_norm.py
index be82fcd2..7f08fa5d 100644
--- a/research/object_detection/core/freezable_batch_norm.py
+++ b/research/object_detection/core/freezable_batch_norm.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """A freezable batch norm layer that uses Keras batch normalization."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 class FreezableBatchNorm(tf.keras.layers.BatchNormalization):
diff --git a/research/object_detection/core/freezable_batch_norm_test.py b/research/object_detection/core/freezable_batch_norm_test.py
index 32a7f8c2..8379a383 100644
--- a/research/object_detection/core/freezable_batch_norm_test.py
+++ b/research/object_detection/core/freezable_batch_norm_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 from object_detection.core import freezable_batch_norm
diff --git a/research/object_detection/core/keypoint_ops.py b/research/object_detection/core/keypoint_ops.py
index cfcaa529..e321783d 100644
--- a/research/object_detection/core/keypoint_ops.py
+++ b/research/object_detection/core/keypoint_ops.py
@@ -20,7 +20,7 @@ where the last dimension holds rank 2 tensors of the form [y, x] representing
 the coordinates of the keypoint.
 """
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def scale(keypoints, y_scale, x_scale, scope=None):
diff --git a/research/object_detection/core/keypoint_ops_test.py b/research/object_detection/core/keypoint_ops_test.py
index 1af6ef81..695e8fa1 100644
--- a/research/object_detection/core/keypoint_ops_test.py
+++ b/research/object_detection/core/keypoint_ops_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.core.keypoint_ops."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import keypoint_ops
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index 06151bb2..07e7dd3f 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -32,15 +32,11 @@ from __future__ import print_function
 
 import abc
 import six
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
+import tensorflow.compat.v1 as tf
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.utils import ops
 
-slim = contrib_slim
-
 
 class Loss(six.with_metaclass(abc.ABCMeta, object)):
   """Abstract base class for loss functions."""
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index 3fee6d9f..5957052e 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -22,7 +22,7 @@ import math
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.core import losses
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index 60988528..6a09ffa9 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -37,7 +37,7 @@ from __future__ import print_function
 
 import abc
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import ops
 
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index 0b1d53cd..ad640753 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.core.matcher."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import matcher
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/minibatch_sampler.py b/research/object_detection/core/minibatch_sampler.py
index 7628c8df..9a5b0a72 100644
--- a/research/object_detection/core/minibatch_sampler.py
+++ b/research/object_detection/core/minibatch_sampler.py
@@ -36,7 +36,7 @@ from abc import ABCMeta
 from abc import abstractmethod
 
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import ops
 
diff --git a/research/object_detection/core/minibatch_sampler_test.py b/research/object_detection/core/minibatch_sampler_test.py
index 4f148c9a..b3ddadd2 100644
--- a/research/object_detection/core/minibatch_sampler_test.py
+++ b/research/object_detection/core/minibatch_sampler_test.py
@@ -16,7 +16,7 @@
 """Tests for google3.research.vale.object_detection.minibatch_sampler."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import minibatch_sampler
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index 1f768f1d..0430b37b 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -60,7 +60,7 @@ from __future__ import print_function
 
 import abc
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields as fields
 
@@ -68,7 +68,7 @@ from object_detection.core import standard_fields as fields
 # If using a new enough version of TensorFlow, detection models should be a
 # tf module or keras model for tracking.
 try:
-  _BaseClass = tf.Module
+  _BaseClass = tf.keras.layers.Layer
 except AttributeError:
   _BaseClass = object
 
@@ -90,6 +90,8 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
     self._num_classes = num_classes
     self._groundtruth_lists = {}
 
+    super(DetectionModel, self).__init__()
+
   @property
   def num_classes(self):
     return self._num_classes
@@ -295,6 +297,7 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
                           groundtruth_weights_list=None,
                           groundtruth_confidences_list=None,
                           groundtruth_is_crowd_list=None,
+                          groundtruth_group_of_list=None,
                           groundtruth_area_list=None,
                           is_annotated_list=None,
                           groundtruth_labeled_classes=None):
@@ -328,14 +331,16 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
         boxes.
       groundtruth_is_crowd_list: A list of 1-D tf.bool tensors of shape
         [num_boxes] containing is_crowd annotations.
+      groundtruth_group_of_list: A list of 1-D tf.bool tensors of shape
+        [num_boxes] containing group_of annotations.
       groundtruth_area_list: A list of 1-D tf.float32 tensors of shape
         [num_boxes] containing the area (in the original absolute coordinates)
         of the annotations.
       is_annotated_list: A list of scalar tf.bool tensors indicating whether
         images have been labeled or not.
       groundtruth_labeled_classes: A list of 1-D tf.float32 tensors of shape
-        [num_classes], containing label indices (1-indexed) of the classes that
-        are exhaustively annotated.
+        [num_classes], containing label indices encoded as k-hot of the classes
+        that are exhaustively annotated.
     """
     self._groundtruth_lists[fields.BoxListFields.boxes] = groundtruth_boxes_list
     self._groundtruth_lists[
@@ -359,6 +364,9 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
     if groundtruth_is_crowd_list:
       self._groundtruth_lists[
           fields.BoxListFields.is_crowd] = groundtruth_is_crowd_list
+    if groundtruth_group_of_list:
+      self._groundtruth_lists[
+          fields.BoxListFields.group_of] = groundtruth_group_of_list
     if groundtruth_area_list:
       self._groundtruth_lists[
           fields.InputDataFields.groundtruth_area] = groundtruth_area_list
@@ -418,3 +426,20 @@ class DetectionModel(six.with_metaclass(abc.ABCMeta, _BaseClass)):
       A list of update operators.
     """
     pass
+
+  def call(self, images):
+    """Returns detections from a batch of images.
+
+    This method calls the preprocess, predict and postprocess function
+    sequentially and returns the output.
+
+    Args:
+      images: a [batch_size, height, width, channels] float tensor.
+
+    Returns:
+       detetcions: The dict of tensors returned by the postprocess function.
+    """
+
+    preprocessed_images, shapes = self.preprocess(images)
+    prediction_dict = self.predict(preprocessed_images, shapes)
+    return self.postprocess(prediction_dict, shapes)
diff --git a/research/object_detection/core/model_test.py b/research/object_detection/core/model_test.py
new file mode 100644
index 00000000..2bb1ab34
--- /dev/null
+++ b/research/object_detection/core/model_test.py
@@ -0,0 +1,98 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for model API."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow.compat.v1 as tf
+
+from object_detection.core import model
+from object_detection.utils import test_case
+
+
+class FakeModel(model.DetectionModel):
+
+  def __init__(self):
+
+    # sub-networks containing weights of different shapes.
+    self._network1 = tf.keras.Sequential([
+        tf.keras.layers.Conv2D(8, 1)
+    ])
+
+    self._network2 = tf.keras.Sequential([
+        tf.keras.layers.Conv2D(16, 1)
+    ])
+
+    super(FakeModel, self).__init__(num_classes=0)
+
+  def preprocess(self, images):
+    return images, tf.shape(images)
+
+  def predict(self, images, shapes):
+    return {'prediction': self._network2(self._network1(images))}
+
+  def postprocess(self, prediction_dict, shapes):
+    return prediction_dict
+
+  def loss(self):
+    return tf.constant(0.0)
+
+  def updates(self):
+    return []
+
+  def restore_map(self):
+    return {}
+
+  def regularization_losses(self):
+    return []
+
+
+class ModelTest(test_case.TestCase):
+
+  def test_model_call(self):
+
+    detection_model = FakeModel()
+
+    def graph_fn():
+      return detection_model(tf.zeros((1, 128, 128, 3)))
+
+    result = self.execute(graph_fn, [])
+    self.assertEqual(result['prediction'].shape,
+                     (1, 128, 128, 16))
+
+  def test_freeze(self):
+
+    detection_model = FakeModel()
+    detection_model(tf.zeros((1, 128, 128, 3)))
+
+    net1_var_shapes = [tuple(var.get_shape().as_list()) for var in
+                       detection_model._network1.trainable_variables]
+
+    del detection_model
+
+    detection_model = FakeModel()
+    detection_model._network2.trainable = False
+    detection_model(tf.zeros((1, 128, 128, 3)))
+
+    var_shapes = [tuple(var.get_shape().as_list()) for var in
+                  detection_model._network1.trainable_variables]
+
+    self.assertEqual(set(net1_var_shapes), set(var_shapes))
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/core/multiclass_nms_test.py b/research/object_detection/core/multiclass_nms_test.py
index d191e040..80be89da 100644
--- a/research/object_detection/core/multiclass_nms_test.py
+++ b/research/object_detection/core/multiclass_nms_test.py
@@ -15,7 +15,7 @@
 
 """Tests for tensorflow_models.object_detection.core.post_processing."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import post_processing
 from object_detection.core import standard_fields as fields
 from object_detection.utils import test_case
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index 5e594055..e425cd08 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -22,7 +22,7 @@ import collections
 import numpy as np
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
@@ -396,6 +396,7 @@ def multiclass_non_max_suppression(boxes,
                                    use_partitioned_nms=False,
                                    additional_fields=None,
                                    soft_nms_sigma=0.0,
+                                   use_hard_nms=False,
                                    scope=None):
   """Multi-class version of non maximum suppression.
 
@@ -450,6 +451,7 @@ def multiclass_non_max_suppression(boxes,
       `soft_nms_sigma=0.0` (which is default), we fall back to standard (hard)
       NMS.  Soft NMS is currently only supported when pad_to_max_output_size is
       False.
+    use_hard_nms: Enforce the usage of hard NMS.
     scope: name scope.
 
   Returns:
@@ -536,7 +538,7 @@ def multiclass_non_max_suppression(boxes,
         max_selection_size = tf.minimum(max_size_per_class,
                                         boxlist_and_class_scores.num_boxes())
         if (hasattr(tf.image, 'non_max_suppression_with_scores') and
-            tf.compat.forward_compatible(2019, 6, 6)):
+            tf.compat.forward_compatible(2019, 6, 6) and not use_hard_nms):
           (selected_indices, selected_scores
           ) = tf.image.non_max_suppression_with_scores(
               boxlist_and_class_scores.get(),
@@ -852,7 +854,8 @@ def batch_multiclass_non_max_suppression(boxes,
                                          use_class_agnostic_nms=False,
                                          max_classes_per_detection=1,
                                          use_dynamic_map_fn=False,
-                                         use_combined_nms=False):
+                                         use_combined_nms=False,
+                                         use_hard_nms=False):
   """Multi-class version of non maximum suppression that operates on a batch.
 
   This op is similar to `multiclass_non_max_suppression` but operates on a batch
@@ -923,6 +926,7 @@ def batch_multiclass_non_max_suppression(boxes,
       calling this function.
       Masks and additional fields are not supported.
       See argument checks in the code below for unsupported arguments.
+    use_hard_nms: Enforce the usage of hard NMS.
 
   Returns:
     'nmsed_boxes': A [batch_size, max_detections, 4] float32 tensor
@@ -966,12 +970,10 @@ def batch_multiclass_non_max_suppression(boxes,
           'clip_window is not supported by combined_nms unless it is'
           ' [0. 0. 1. 1.] for each image.')
     if additional_fields is not None:
-      tf.logging.warning(
-          'additional_fields is not supported by combined_nms.')
+      tf.logging.warning('additional_fields is not supported by combined_nms.')
     if parallel_iterations != 32:
-      tf.logging.warning(
-          'Number of batch items to be processed in parallel is'
-          ' not configurable by combined_nms.')
+      tf.logging.warning('Number of batch items to be processed in parallel is'
+                         ' not configurable by combined_nms.')
     if max_classes_per_detection > 1:
       tf.logging.warning(
           'max_classes_per_detection is not configurable by combined_nms.')
@@ -1009,7 +1011,7 @@ def batch_multiclass_non_max_suppression(boxes,
   # in _single_image_nms_fn(). The dictionary is thus a sorted version of
   # additional_fields.
   if additional_fields is None:
-    ordered_additional_fields = {}
+    ordered_additional_fields = collections.OrderedDict()
   else:
     ordered_additional_fields = collections.OrderedDict(
         sorted(additional_fields.items(), key=lambda item: item[0]))
@@ -1159,7 +1161,8 @@ def batch_multiclass_non_max_suppression(boxes,
             pad_to_max_output_size=use_static_shapes,
             use_partitioned_nms=use_partitioned_nms,
             additional_fields=per_image_additional_fields,
-            soft_nms_sigma=soft_nms_sigma)
+            soft_nms_sigma=soft_nms_sigma,
+            use_hard_nms=use_hard_nms)
 
       if not use_static_shapes:
         nmsed_boxlist = box_list_ops.pad_or_clip_box_list(
diff --git a/research/object_detection/core/prefetcher.py b/research/object_detection/core/prefetcher.py
index 52a9e3ea..f88fbbd3 100644
--- a/research/object_detection/core/prefetcher.py
+++ b/research/object_detection/core/prefetcher.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Provides functions to prefetch tensors to feed into models."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import tf_version
 if not tf_version.is_tf1():
diff --git a/research/object_detection/core/prefetcher_tf1_test.py b/research/object_detection/core/prefetcher_tf1_test.py
index 8d16a74a..3c827d80 100644
--- a/research/object_detection/core/prefetcher_tf1_test.py
+++ b/research/object_detection/core/prefetcher_tf1_test.py
@@ -19,12 +19,12 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 # pylint: disable=g-bad-import-order,
 from object_detection.core import prefetcher
-from tensorflow.contrib import slim as contrib_slim
-slim = contrib_slim
+import tf_slim as slim
+
 # pylint: disable=g-bad-import-order
 
 
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 703bb025..8b8fdff5 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -74,7 +74,7 @@ import sys
 import six
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.python.ops import control_flow_ops
 from object_detection.core import box_list
@@ -1603,6 +1603,7 @@ def random_crop_image(image,
 
 def random_pad_image(image,
                      boxes,
+                     masks=None,
                      keypoints=None,
                      min_image_size=None,
                      max_image_size=None,
@@ -1625,6 +1626,9 @@ def random_pad_image(image,
            Boxes are in normalized form meaning their coordinates vary
            between [0, 1].
            Each row is in the form of [ymin, xmin, ymax, xmax].
+    masks: (optional) rank 3 float32 tensor with shape
+           [N, height, width] containing instance masks. The masks
+           are of the same height, width as the input `image`.
     keypoints: (optional) rank 3 float32 tensor with shape
                [N, num_keypoints, 2]. The keypoints are in y-x normalized
                coordinates.
@@ -1648,6 +1652,8 @@ def random_pad_image(image,
     boxes: boxes which is the same rank as input boxes. Boxes are in normalized
            form.
 
+    if masks is not None, the function also returns:
+    masks: rank 3 float32 tensor with shape [N, new_height, new_width]
     if keypoints is not None, the function also returns:
     keypoints: rank 3 float32 tensor with shape [N, num_keypoints, 2]
   """
@@ -1728,6 +1734,15 @@ def random_pad_image(image,
 
   result = [new_image, new_boxes]
 
+  if masks is not None:
+    new_masks = tf.image.pad_to_bounding_box(
+        masks[:, :, :, tf.newaxis],
+        offset_height=offset_height,
+        offset_width=offset_width,
+        target_height=target_height,
+        target_width=target_width)[:, :, :, 0]
+    result.append(new_masks)
+
   if keypoints is not None:
     new_keypoints = keypoint_ops.change_coordinate_frame(keypoints, new_window)
     result.append(new_keypoints)
@@ -1737,6 +1752,7 @@ def random_pad_image(image,
 
 def random_absolute_pad_image(image,
                               boxes,
+                              masks=None,
                               keypoints=None,
                               max_height_padding=None,
                               max_width_padding=None,
@@ -1756,6 +1772,9 @@ def random_absolute_pad_image(image,
            Boxes are in normalized form meaning their coordinates vary
            between [0, 1].
            Each row is in the form of [ymin, xmin, ymax, xmax].
+    masks: (optional) rank 3 float32 tensor with shape
+           [N, height, width] containing instance masks. The masks
+           are of the same height, width as the input `image`.
     keypoints: (optional) rank 3 float32 tensor with shape
                [N, num_keypoints, 2]. The keypoints are in y-x normalized
                coordinates.
@@ -1778,6 +1797,10 @@ def random_absolute_pad_image(image,
     image: Image shape will be [new_height, new_width, channels].
     boxes: boxes which is the same rank as input boxes. Boxes are in normalized
            form.
+    if masks is not None, the function also returns:
+    masks: rank 3 float32 tensor with shape [N, new_height, new_width]
+    if keypoints is not None, the function also returns:
+    keypoints: rank 3 float32 tensor with shape [N, num_keypoints, 2]
   """
   min_image_size = tf.shape(image)[:2]
   max_image_size = min_image_size + tf.cast(
@@ -1785,6 +1808,7 @@ def random_absolute_pad_image(image,
   return random_pad_image(
       image,
       boxes,
+      masks=masks,
       keypoints=keypoints,
       min_image_size=min_image_size,
       max_image_size=max_image_size,
@@ -4060,10 +4084,12 @@ def get_default_func_arg_map(include_label_weights=True,
                           groundtruth_keypoint_visibilities),
       random_pad_image:
           (fields.InputDataFields.image,
-           fields.InputDataFields.groundtruth_boxes, groundtruth_keypoints),
+           fields.InputDataFields.groundtruth_boxes, groundtruth_instance_masks,
+           groundtruth_keypoints),
       random_absolute_pad_image:
           (fields.InputDataFields.image,
-           fields.InputDataFields.groundtruth_boxes, groundtruth_keypoints),
+           fields.InputDataFields.groundtruth_boxes, groundtruth_instance_masks,
+           groundtruth_keypoints),
       random_crop_pad_image: (fields.InputDataFields.image,
                               fields.InputDataFields.groundtruth_boxes,
                               fields.InputDataFields.groundtruth_classes,
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 3b4e2b13..a535ce20 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -24,7 +24,7 @@ import numpy as np
 import six
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import preprocessor
 from object_detection.core import preprocessor_cache
@@ -2268,7 +2268,7 @@ class PreprocessorTest(test_case.TestCase, parameterized.TestCase):
     self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
         padded_boxes_[:, 3] - padded_boxes_[:, 1])))
 
-  def testRandomPadImageWithKeypoints(self):
+  def testRandomPadImageWithKeypointsAndMasks(self):
     def graph_fn():
       preprocessing_options = [(preprocessor.normalize_image, {
           'original_minval': 0,
@@ -2280,45 +2280,57 @@ class PreprocessorTest(test_case.TestCase, parameterized.TestCase):
       images = self.createTestImages()
       boxes = self.createTestBoxes()
       labels = self.createTestLabels()
+      masks = self.createTestMasks()
       keypoints, _ = self.createTestKeypoints()
       tensor_dict = {
           fields.InputDataFields.image: images,
           fields.InputDataFields.groundtruth_boxes: boxes,
           fields.InputDataFields.groundtruth_classes: labels,
+          fields.InputDataFields.groundtruth_instance_masks: masks,
           fields.InputDataFields.groundtruth_keypoints: keypoints,
       }
       tensor_dict = preprocessor.preprocess(tensor_dict, preprocessing_options)
       images = tensor_dict[fields.InputDataFields.image]
 
       preprocessing_options = [(preprocessor.random_pad_image, {})]
+      func_arg_map = preprocessor.get_default_func_arg_map(
+          include_instance_masks=True,
+          include_keypoints=True,
+          include_keypoint_visibilities=True)
       padded_tensor_dict = preprocessor.preprocess(tensor_dict,
-                                                   preprocessing_options)
+                                                   preprocessing_options,
+                                                   func_arg_map=func_arg_map)
 
       padded_images = padded_tensor_dict[fields.InputDataFields.image]
       padded_boxes = padded_tensor_dict[
           fields.InputDataFields.groundtruth_boxes]
+      padded_masks = padded_tensor_dict[
+          fields.InputDataFields.groundtruth_instance_masks]
       padded_keypoints = padded_tensor_dict[
           fields.InputDataFields.groundtruth_keypoints]
       boxes_shape = tf.shape(boxes)
       padded_boxes_shape = tf.shape(padded_boxes)
+      padded_masks_shape = tf.shape(padded_masks)
       keypoints_shape = tf.shape(keypoints)
       padded_keypoints_shape = tf.shape(padded_keypoints)
       images_shape = tf.shape(images)
       padded_images_shape = tf.shape(padded_images)
-      return [boxes_shape, padded_boxes_shape, keypoints_shape,
-              padded_keypoints_shape, images_shape, padded_images_shape, boxes,
-              padded_boxes, keypoints, padded_keypoints]
-
-    (boxes_shape_, padded_boxes_shape_, keypoints_shape_,
-     padded_keypoints_shape_, images_shape_, padded_images_shape_, boxes_,
-     padded_boxes_, keypoints_, padded_keypoints_) = self.execute_cpu(graph_fn,
-                                                                      [])
+      return [boxes_shape, padded_boxes_shape, padded_masks_shape,
+              keypoints_shape, padded_keypoints_shape, images_shape,
+              padded_images_shape, boxes, padded_boxes, keypoints,
+              padded_keypoints]
+
+    (boxes_shape_, padded_boxes_shape_, padded_masks_shape_,
+     keypoints_shape_, padded_keypoints_shape_, images_shape_,
+     padded_images_shape_, boxes_, padded_boxes_,
+     keypoints_, padded_keypoints_) = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(boxes_shape_, padded_boxes_shape_)
     self.assertAllEqual(keypoints_shape_, padded_keypoints_shape_)
     self.assertTrue((images_shape_[1] >= padded_images_shape_[1] * 0.5).all)
     self.assertTrue((images_shape_[2] >= padded_images_shape_[2] * 0.5).all)
     self.assertTrue((images_shape_[1] <= padded_images_shape_[1]).all)
     self.assertTrue((images_shape_[2] <= padded_images_shape_[2]).all)
+    self.assertAllEqual(padded_masks_shape_[1:3], padded_images_shape_[1:3])
     self.assertTrue(np.all((boxes_[:, 2] - boxes_[:, 0]) >= (
         padded_boxes_[:, 2] - padded_boxes_[:, 0])))
     self.assertTrue(np.all((boxes_[:, 3] - boxes_[:, 1]) >= (
diff --git a/research/object_detection/core/region_similarity_calculator.py b/research/object_detection/core/region_similarity_calculator.py
index 7b6e1485..fd75d52f 100644
--- a/research/object_detection/core/region_similarity_calculator.py
+++ b/research/object_detection/core/region_similarity_calculator.py
@@ -26,7 +26,7 @@ from abc import ABCMeta
 from abc import abstractmethod
 
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list_ops
 from object_detection.core import standard_fields as fields
diff --git a/research/object_detection/core/region_similarity_calculator_test.py b/research/object_detection/core/region_similarity_calculator_test.py
index 64f6ff39..9f9a10b6 100644
--- a/research/object_detection/core/region_similarity_calculator_test.py
+++ b/research/object_detection/core/region_similarity_calculator_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for region_similarity_calculator."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_list
 from object_detection.core import region_similarity_calculator
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index d1fe3628..df995b4a 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -76,6 +76,9 @@ class InputDataFields(object):
       context_features, used for reshaping.
     valid_context_size: the valid context size, used in filtering the padded
       context features.
+    image_format: format for the images, used to decode
+    image_height: height of images, used to decode
+    image_width: width of images, used to decode
   """
   image = 'image'
   image_additional_channels = 'image_additional_channels'
@@ -112,6 +115,10 @@ class InputDataFields(object):
   context_features = 'context_features'
   context_feature_length = 'context_feature_length'
   valid_context_size = 'valid_context_size'
+  image_timestamps = 'image_timestamps'
+  image_format = 'image_format'
+  image_height = 'image_height'
+  image_width = 'image_width'
 
 
 class DetectionResultFields(object):
@@ -182,6 +189,7 @@ class BoxListFields(object):
   keypoint_visibilities = 'keypoint_visibilities'
   keypoint_heatmaps = 'keypoint_heatmaps'
   is_crowd = 'is_crowd'
+  group_of = 'group_of'
 
 
 class PredictionFields(object):
@@ -279,3 +287,14 @@ class TfExampleFields(object):
   detection_bbox_ymax = 'image/detection/bbox/ymax'
   detection_bbox_xmax = 'image/detection/bbox/xmax'
   detection_score = 'image/detection/score'
+
+# Sequence fields for SequenceExample inputs.
+# All others are considered context fields.
+SEQUENCE_FIELDS = [InputDataFields.image,
+                   InputDataFields.source_id,
+                   InputDataFields.groundtruth_boxes,
+                   InputDataFields.num_groundtruth_boxes,
+                   InputDataFields.groundtruth_classes,
+                   InputDataFields.groundtruth_weights,
+                   InputDataFields.source_id,
+                   InputDataFields.is_annotated]
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index fdbc36e1..3d5453bf 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -37,7 +37,8 @@ from __future__ import print_function
 
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tensorflow.compat.v2 as tf2
 
 from object_detection.box_coders import faster_rcnn_box_coder
 from object_detection.box_coders import mean_stddev_box_coder
@@ -54,6 +55,8 @@ from object_detection.utils import shape_utils
 from object_detection.utils import target_assigner_utils as ta_utils
 
 
+ResizeMethod = tf2.image.ResizeMethod
+
 _DEFAULT_KEYPOINT_OFFSET_STD_DEV = 1.0
 
 
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index a17c4c34..fb0a63bd 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.core.target_assigner."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import keypoint_box_coder
 from object_detection.box_coders import mean_stddev_box_coder
@@ -1230,6 +1230,6 @@ class CreateTargetAssignerTest(tf.test.TestCase):
 
 
 
-
 if __name__ == '__main__':
+  tf.enable_v2_behavior()
   tf.test.main()
diff --git a/research/object_detection/data/snapshot_serengeti_label_map.pbtxt b/research/object_detection/data/snapshot_serengeti_label_map.pbtxt
new file mode 100644
index 00000000..57555d17
--- /dev/null
+++ b/research/object_detection/data/snapshot_serengeti_label_map.pbtxt
@@ -0,0 +1,240 @@
+item {
+  id: 1
+  name: 'human'
+}
+
+item {
+  id: 2
+  name: 'gazelleGrants'
+}
+
+item {
+  id: 3
+  name: 'reedbuck'
+}
+
+item {
+  id: 4
+  name: 'dikDik'
+}
+
+item {
+  id: 5
+  name: 'zebra'
+}
+
+item {
+  id: 6
+  name: 'porcupine'
+}
+
+item {
+  id: 7
+  name: 'gazelleThomsons'
+}
+
+item {
+  id: 8
+  name: 'hyenaSpotted'
+}
+
+item {
+  id: 9
+  name: 'warthog'
+}
+
+item {
+  id: 10
+  name: 'impala'
+}
+
+item {
+  id: 11
+  name: 'elephant'
+}
+
+item {
+  id: 12
+  name: 'giraffe'
+}
+
+item {
+  id: 13
+  name: 'mongoose'
+}
+
+item {
+  id: 14
+  name: 'buffalo'
+}
+
+item {
+  id: 15
+  name: 'hartebeest'
+}
+
+item {
+  id: 16
+  name: 'guineaFowl'
+}
+
+item {
+  id: 17
+  name: 'wildebeest'
+}
+
+item {
+  id: 18
+  name: 'leopard'
+}
+
+item {
+  id: 19
+  name: 'ostrich'
+}
+
+item {
+  id: 20
+  name: 'lionFemale'
+}
+
+item {
+  id: 21
+  name: 'koriBustard'
+}
+
+item {
+  id: 22
+  name: 'otherBird'
+}
+
+item {
+  id: 23
+  name: 'batEaredFox'
+}
+
+item {
+  id: 24
+  name: 'bushbuck'
+}
+
+item {
+  id: 25
+  name: 'jackal'
+}
+
+item {
+  id: 26
+  name: 'cheetah'
+}
+
+item {
+  id: 27
+  name: 'eland'
+}
+
+item {
+  id: 28
+  name: 'aardwolf'
+}
+
+item {
+  id: 29
+  name: 'hippopotamus'
+}
+
+item {
+  id: 30
+  name: 'hyenaStriped'
+}
+
+item {
+  id: 31
+  name: 'aardvark'
+}
+
+item {
+  id: 32
+  name: 'hare'
+}
+
+item {
+  id: 33
+  name: 'baboon'
+}
+
+item {
+  id: 34
+  name: 'vervetMonkey'
+}
+
+item {
+  id: 35
+  name: 'waterbuck'
+}
+
+item {
+  id: 36
+  name: 'secretaryBird'
+}
+
+item {
+  id: 37
+  name: 'serval'
+}
+
+item {
+  id: 38
+  name: 'lionMale'
+}
+
+item {
+  id: 39
+  name: 'topi'
+}
+
+item {
+  id: 40
+  name: 'honeyBadger'
+}
+
+item {
+  id: 41
+  name: 'rodents'
+}
+
+item {
+  id: 42
+  name: 'wildcat'
+}
+
+item {
+  id: 43
+  name: 'civet'
+}
+
+item {
+  id: 44
+  name: 'genet'
+}
+
+item {
+  id: 45
+  name: 'caracal'
+}
+
+item {
+  id: 46
+  name: 'rhinoceros'
+}
+
+item {
+  id: 47
+  name: 'reptiles'
+}
+
+item {
+  id: 48
+  name: 'zorilla'
+}
+
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index c159eeac..bd1fa2c7 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -24,8 +24,8 @@ from __future__ import print_function
 import enum
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+from tf_slim import tfexample_decoder as slim_example_decoder
 from object_detection.core import data_decoder
 from object_detection.core import standard_fields as fields
 from object_detection.protos import input_reader_pb2
@@ -34,12 +34,14 @@ from object_detection.utils import label_map_util
 # pylint: disable=g-import-not-at-top
 try:
   from tensorflow.contrib import lookup as contrib_lookup
-  from tensorflow.contrib.slim import tfexample_decoder as slim_example_decoder
+
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
 # pylint: enable=g-import-not-at-top
 
+_LABEL_OFFSET = 1
+
 
 class Visibility(enum.Enum):
   """Visibility definitions.
@@ -167,7 +169,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
                num_keypoints=0,
                num_additional_channels=0,
                load_multiclass_scores=False,
-               load_context_features=False):
+               load_context_features=False,
+               expand_hierarchy_labels=False):
     """Constructor sets keys_to_features and items_to_handlers.
 
     Args:
@@ -193,12 +196,18 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         boxes.
       load_context_features: Whether to load information from context_features,
         to provide additional context to a detection model for training and/or
-        inference
+        inference.
+      expand_hierarchy_labels: Expands the object and image labels taking into
+        account the provided hierarchy in the label_map_proto_file. For positive
+        classes, the labels are extended to ancestor. For negative classes,
+        the labels are expanded to descendants.
 
     Raises:
       ValueError: If `instance_mask_type` option is not one of
         input_reader_pb2.DEFAULT, input_reader_pb2.NUMERICAL, or
         input_reader_pb2.PNG_MASKS.
+      ValueError: If `expand_labels_hierarchy` is True, but the
+        `label_map_proto_file` is not provided.
     """
     # TODO(rathodv): delete unused `use_display_name` argument once we change
     # other decoders to handle label maps similarly.
@@ -385,6 +394,20 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     self.items_to_handlers[
         fields.InputDataFields.groundtruth_image_classes] = image_label_handler
 
+    self._expand_hierarchy_labels = expand_hierarchy_labels
+    self._ancestors_lut = None
+    self._descendants_lut = None
+    if expand_hierarchy_labels:
+      if label_map_proto_file:
+        ancestors_lut, descendants_lut = (
+            label_map_util.get_label_map_hierarchy_lut(label_map_proto_file,
+                                                       True))
+        self._ancestors_lut = tf.constant(ancestors_lut, dtype=tf.int64)
+        self._descendants_lut = tf.constant(descendants_lut, dtype=tf.int64)
+      else:
+        raise ValueError('In order to expand labels, the label_map_proto_file '
+                         'has to be provided.')
+
   def decode(self, tf_example_string_tensor):
     """Decodes serialized tensorflow example and returns a tensor dictionary.
 
@@ -432,7 +455,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         tensor of shape [None, num_keypoints] containing keypoint visibilites.
       fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
         shape [None, None, None] containing instance masks.
-      fields.InputDataFields.groundtruth_image_classes - 1D uint64 of shape
+      fields.InputDataFields.groundtruth_image_classes - 1D int64 of shape
         [None] containing classes for the boxes.
       fields.InputDataFields.multiclass_scores - 1D float32 tensor of shape
         [None * num_classes] containing flattened multiclass scores for
@@ -484,6 +507,46 @@ class TfExampleDecoder(data_decoder.DataDecoder):
           tensor_dict[gt_kpt_fld],
           np.nan * tf.ones_like(tensor_dict[gt_kpt_fld]))
 
+    if self._expand_hierarchy_labels:
+      input_fields = fields.InputDataFields
+      image_classes, image_confidences = self._expand_image_label_hierarchy(
+          tensor_dict[input_fields.groundtruth_image_classes],
+          tensor_dict[input_fields.groundtruth_image_confidences])
+      tensor_dict[input_fields.groundtruth_image_classes] = image_classes
+      tensor_dict[input_fields.groundtruth_image_confidences] = (
+          image_confidences)
+
+      box_fields = [
+          fields.InputDataFields.groundtruth_group_of,
+          fields.InputDataFields.groundtruth_is_crowd,
+          fields.InputDataFields.groundtruth_difficult,
+          fields.InputDataFields.groundtruth_area,
+          fields.InputDataFields.groundtruth_boxes,
+          fields.InputDataFields.groundtruth_weights,
+      ]
+
+      def expand_field(field_name):
+        return self._expansion_box_field_labels(
+            tensor_dict[input_fields.groundtruth_classes],
+            tensor_dict[field_name])
+
+      # pylint: disable=cell-var-from-loop
+      for field in box_fields:
+        if field in tensor_dict:
+          tensor_dict[field] = tf.cond(
+              tf.size(tensor_dict[field]) > 0, lambda: expand_field(field),
+              lambda: tensor_dict[field])
+      # pylint: enable=cell-var-from-loop
+
+      tensor_dict[input_fields.groundtruth_classes] = (
+          self._expansion_box_field_labels(
+              tensor_dict[input_fields.groundtruth_classes],
+              tensor_dict[input_fields.groundtruth_classes], True))
+
+    if fields.InputDataFields.groundtruth_group_of in tensor_dict:
+      group_of = fields.InputDataFields.groundtruth_group_of
+      tensor_dict[group_of] = tf.cast(tensor_dict[group_of], dtype=tf.bool)
+
     return tensor_dict
 
   def _reshape_keypoints(self, keys_to_tensors):
@@ -633,3 +696,69 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         tf.greater(tf.size(png_masks), 0),
         lambda: tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32),
         lambda: tf.zeros(tf.cast(tf.stack([0, height, width]), dtype=tf.int32)))
+
+  def _expand_image_label_hierarchy(self, image_classes, image_confidences):
+    """Expand image level labels according to the hierarchy.
+
+    Args:
+      image_classes: Int64 tensor with the image level class ids for a sample.
+      image_confidences: Float tensor signaling whether a class id is present in
+        the image (1.0) or not present (0.0).
+
+    Returns:
+      new_image_classes: Int64 tensor equal to expanding image_classes.
+      new_image_confidences: Float tensor equal to expanding image_confidences.
+    """
+
+    def expand_labels(relation_tensor, confidence_value):
+      """Expand to ancestors or descendants depending on arguments."""
+      mask = tf.equal(image_confidences, confidence_value)
+      target_image_classes = tf.boolean_mask(image_classes, mask)
+      expanded_indices = tf.reduce_any((tf.gather(
+          relation_tensor, target_image_classes - _LABEL_OFFSET, axis=0) > 0),
+                                       axis=0)
+      expanded_indices = tf.where(expanded_indices)[:, 0] + _LABEL_OFFSET
+      new_groundtruth_image_classes = (
+          tf.concat([
+              tf.boolean_mask(image_classes, tf.logical_not(mask)),
+              expanded_indices,
+          ],
+                    axis=0))
+      new_groundtruth_image_confidences = (
+          tf.concat([
+              tf.boolean_mask(image_confidences, tf.logical_not(mask)),
+              tf.ones([tf.shape(expanded_indices)[0]],
+                      dtype=image_confidences.dtype) * confidence_value,
+          ],
+                    axis=0))
+      return new_groundtruth_image_classes, new_groundtruth_image_confidences
+
+    image_classes, image_confidences = expand_labels(self._ancestors_lut, 1.0)
+    new_image_classes, new_image_confidences = expand_labels(
+        self._descendants_lut, 0.0)
+    return new_image_classes, new_image_confidences
+
+  def _expansion_box_field_labels(self,
+                                  object_classes,
+                                  object_field,
+                                  copy_class_id=False):
+    """Expand the labels of a specific object field according to the hierarchy.
+
+    Args:
+      object_classes: Int64 tensor with the class id for each element in
+        object_field.
+      object_field: Tensor to be expanded.
+      copy_class_id: Boolean to choose whether to use class id values in the
+        output tensor instead of replicating the original values.
+
+    Returns:
+      A tensor with the result of expanding object_field.
+    """
+    expanded_indices = tf.gather(
+        self._ancestors_lut, object_classes - _LABEL_OFFSET, axis=0)
+    if copy_class_id:
+      new_object_field = tf.where(expanded_indices > 0)[:, 1] + _LABEL_OFFSET
+    else:
+      new_object_field = tf.repeat(
+          object_field, tf.reduce_sum(expanded_indices, axis=1), axis=0)
+    return new_object_field
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 0f3ccce6..9cbed32f 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -17,95 +17,95 @@
 import os
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
 from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
+from object_detection.utils import test_case
 
 
-class TfExampleDecoderTest(tf.test.TestCase):
-
-  def _EncodeImage(self, image_tensor, encoding_type='jpeg'):
-    with self.test_session():
-      if encoding_type == 'jpeg':
-        image_encoded = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
-      elif encoding_type == 'png':
-        image_encoded = tf.image.encode_png(tf.constant(image_tensor)).eval()
-      else:
-        raise ValueError('Invalid encoding type.')
-    return image_encoded
-
-  def _DecodeImage(self, image_encoded, encoding_type='jpeg'):
-    with self.test_session():
-      if encoding_type == 'jpeg':
-        image_decoded = tf.image.decode_jpeg(tf.constant(image_encoded)).eval()
-      elif encoding_type == 'png':
-        image_decoded = tf.image.decode_png(tf.constant(image_encoded)).eval()
-      else:
-        raise ValueError('Invalid encoding type.')
-    return image_decoded
+class TfExampleDecoderTest(test_case.TestCase):
+
+  def _create_encoded_and_decoded_data(self, data, encoding_type):
+    if encoding_type == 'jpeg':
+      encode_fn = tf.image.encode_jpeg
+      decode_fn = tf.image.decode_jpeg
+    elif encoding_type == 'png':
+      encode_fn = tf.image.encode_png
+      decode_fn = tf.image.decode_png
+    else:
+      raise ValueError('Invalid encoding type.')
+
+    def prepare_data_fn():
+      encoded_data = encode_fn(data)
+      decoded_data = decode_fn(encoded_data)
+      return encoded_data, decoded_data
+
+    return self.execute_cpu(prepare_data_fn, [])
 
   def testDecodeAdditionalChannels(self):
-    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
-
-    additional_channel_tensor = np.random.randint(
-        256, size=(4, 5, 1)).astype(np.uint8)
-    encoded_additional_channel = self._EncodeImage(additional_channel_tensor)
-    decoded_additional_channel = self._DecodeImage(encoded_additional_channel)
-
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/additional_channels/encoded':
-                    dataset_util.bytes_list_feature(
-                        [encoded_additional_channel] * 2),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/source_id':
-                    dataset_util.bytes_feature(six.b('image_id')),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        num_additional_channels=2)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
-      self.assertAllEqual(
-          np.concatenate([decoded_additional_channel] * 2, axis=2),
-          tensor_dict[fields.InputDataFields.image_additional_channels])
+    image = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(image, 'jpeg')
+
+    additional_channel = np.random.randint(256, size=(4, 5, 1)).astype(np.uint8)
+    (encoded_additional_channel,
+     decoded_additional_channel) = self._create_encoded_and_decoded_data(
+         additional_channel, 'jpeg')
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/additional_channels/encoded':
+                      dataset_util.bytes_list_feature(
+                          [encoded_additional_channel] * 2),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/source_id':
+                      dataset_util.bytes_feature(six.b('image_id')),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          num_additional_channels=2)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(
+        np.concatenate([decoded_additional_channel] * 2, axis=2),
+        tensor_dict[fields.InputDataFields.image_additional_channels])
 
   def testDecodeJpegImage(self):
-    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
-    decoded_jpeg = self._DecodeImage(encoded_jpeg)
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/source_id':
-                    dataset_util.bytes_feature(six.b('image_id')),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.image].
-                         get_shape().as_list()), [None, None, 3])
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.
-                                     original_image_spatial_shape].
-                         get_shape().as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+    image = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg, decoded_jpeg = self._create_encoded_and_decoded_data(
+        image, 'jpeg')
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/source_id':
+                      dataset_util.bytes_feature(six.b('image_id')),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+      self.assertAllEqual(
+          (output[fields.InputDataFields.image].get_shape().as_list()),
+          [None, None, 3])
+      self.assertAllEqual(
+          (output[fields.InputDataFields.original_image_spatial_shape]
+           .get_shape().as_list()), [2])
+      return output
 
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(decoded_jpeg, tensor_dict[fields.InputDataFields.image])
     self.assertAllEqual([4, 5], tensor_dict[fields.InputDataFields.
                                             original_image_spatial_shape])
@@ -113,50 +113,57 @@ class TfExampleDecoderTest(tf.test.TestCase):
         six.b('image_id'), tensor_dict[fields.InputDataFields.source_id])
 
   def testDecodeImageKeyAndFilename(self):
-    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded': dataset_util.bytes_feature(encoded_jpeg),
-                'image/key/sha256': dataset_util.bytes_feature(six.b('abc')),
-                'image/filename': dataset_util.bytes_feature(six.b('filename'))
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
-
+    image = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(image, 'jpeg')
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/key/sha256':
+                      dataset_util.bytes_feature(six.b('abc')),
+                  'image/filename':
+                      dataset_util.bytes_feature(six.b('filename'))
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertEqual(six.b('abc'), tensor_dict[fields.InputDataFields.key])
     self.assertEqual(
         six.b('filename'), tensor_dict[fields.InputDataFields.filename])
 
   def testDecodePngImage(self):
-    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_png = self._EncodeImage(image_tensor, encoding_type='png')
-    decoded_png = self._DecodeImage(encoded_png, encoding_type='png')
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded': dataset_util.bytes_feature(encoded_png),
-                'image/format': dataset_util.bytes_feature(six.b('png')),
-                'image/source_id': dataset_util.bytes_feature(
-                    six.b('image_id'))
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.image].
-                         get_shape().as_list()), [None, None, 3])
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.
-                                     original_image_spatial_shape].
-                         get_shape().as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+    image = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_png, decoded_png = self._create_encoded_and_decoded_data(
+        image, 'png')
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_png),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('png')),
+                  'image/source_id':
+                      dataset_util.bytes_feature(six.b('image_id'))
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+      self.assertAllEqual(
+          (output[fields.InputDataFields.image].get_shape().as_list()),
+          [None, None, 3])
+      self.assertAllEqual(
+          (output[fields.InputDataFields.original_image_spatial_shape]
+           .get_shape().as_list()), [2])
+      return output
 
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(decoded_png, tensor_dict[fields.InputDataFields.image])
     self.assertAllEqual([4, 5], tensor_dict[fields.InputDataFields.
                                             original_image_spatial_shape])
@@ -164,99 +171,105 @@ class TfExampleDecoderTest(tf.test.TestCase):
         six.b('image_id'), tensor_dict[fields.InputDataFields.source_id])
 
   def testDecodePngInstanceMasks(self):
-    image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    image = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_png, _ = self._create_encoded_and_decoded_data(image, 'png')
     mask_1 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)
     mask_2 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)
-    encoded_png_1 = self._EncodeImage(mask_1, encoding_type='png')
+    encoded_png_1, _ = self._create_encoded_and_decoded_data(mask_1, 'png')
     decoded_png_1 = np.squeeze(mask_1.astype(np.float32))
-    encoded_png_2 = self._EncodeImage(mask_2, encoding_type='png')
+    encoded_png_2, _ = self._create_encoded_and_decoded_data(mask_2, 'png')
     decoded_png_2 = np.squeeze(mask_2.astype(np.float32))
     encoded_masks = [encoded_png_1, encoded_png_2]
     decoded_masks = np.stack([decoded_png_1, decoded_png_2])
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/mask':
-                    dataset_util.bytes_list_feature(encoded_masks)
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_png),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('png')),
+                  'image/object/mask':
+                      dataset_util.bytes_list_feature(encoded_masks)
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_instance_masks=True,
+          instance_mask_type=input_reader_pb2.PNG_MASKS)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(
         decoded_masks,
         tensor_dict[fields.InputDataFields.groundtruth_instance_masks])
 
   def testDecodeEmptyPngInstanceMasks(self):
     image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_png, _ = self._create_encoded_and_decoded_data(image_tensor, 'png')
     encoded_masks = []
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/mask':
-                    dataset_util.bytes_list_feature(encoded_masks),
-                'image/height':
-                    dataset_util.int64_feature(10),
-                'image/width':
-                    dataset_util.int64_feature(10),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
-      self.assertAllEqual(
-          tensor_dict[fields.InputDataFields.groundtruth_instance_masks].shape,
-          [0, 10, 10])
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_png),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('png')),
+                  'image/object/mask':
+                      dataset_util.bytes_list_feature(encoded_masks),
+                  'image/height':
+                      dataset_util.int64_feature(10),
+                  'image/width':
+                      dataset_util.int64_feature(10),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_instance_masks=True,
+          instance_mask_type=input_reader_pb2.PNG_MASKS)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(
+        tensor_dict[fields.InputDataFields.groundtruth_instance_masks].shape,
+        [0, 10, 10])
 
   def testDecodeBoundingBox(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
     bbox_xmaxs = [3.0, 7.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes]
-                         .get_shape().as_list()), [None, 4])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_boxes].get_shape().as_list()),
+                          [None, 4])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     expected_boxes = np.vstack([bbox_ymins, bbox_xmins, bbox_ymaxs,
                                 bbox_xmaxs]).transpose()
     self.assertAllEqual(expected_boxes,
@@ -264,7 +277,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def testDecodeKeypoint(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
@@ -272,40 +286,43 @@ class TfExampleDecoderTest(tf.test.TestCase):
     keypoint_ys = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
     keypoint_xs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
     keypoint_visibility = [1, 2, 0, 1, 0, 2]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-                'image/object/keypoint/y':
-                    dataset_util.float_list_feature(keypoint_ys),
-                'image/object/keypoint/x':
-                    dataset_util.float_list_feature(keypoint_xs),
-                'image/object/keypoint/visibility':
-                    dataset_util.int64_list_feature(keypoint_visibility),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes]
-                         .get_shape().as_list()), [None, 4])
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_keypoints].get_shape()
-         .as_list()), [2, 3, 2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+                  'image/object/keypoint/y':
+                      dataset_util.float_list_feature(keypoint_ys),
+                  'image/object/keypoint/x':
+                      dataset_util.float_list_feature(keypoint_xs),
+                  'image/object/keypoint/visibility':
+                      dataset_util.int64_list_feature(keypoint_visibility),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_boxes].get_shape().as_list()),
+                          [None, 4])
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_keypoints].get_shape().as_list()),
+                          [2, 3, 2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     expected_boxes = np.vstack([bbox_ymins, bbox_xmins, bbox_ymaxs,
                                 bbox_xmaxs]).transpose()
     self.assertAllEqual(expected_boxes,
@@ -326,45 +343,49 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def testDecodeKeypointNoVisibilities(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
     bbox_xmaxs = [3.0, 7.0]
     keypoint_ys = [0.0, 1.0, 2.0, 3.0, 4.0, 5.0]
     keypoint_xs = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-                'image/object/keypoint/y':
-                    dataset_util.float_list_feature(keypoint_ys),
-                'image/object/keypoint/x':
-                    dataset_util.float_list_feature(keypoint_xs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes]
-                         .get_shape().as_list()), [None, 4])
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_keypoints].get_shape()
-         .as_list()), [2, 3, 2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+                  'image/object/keypoint/y':
+                      dataset_util.float_list_feature(keypoint_ys),
+                  'image/object/keypoint/x':
+                      dataset_util.float_list_feature(keypoint_xs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(num_keypoints=3)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_boxes].get_shape().as_list()),
+                          [None, 4])
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_keypoints].get_shape().as_list()),
+                          [2, 3, 2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     expected_boxes = np.vstack([bbox_ymins, bbox_xmins, bbox_ymaxs,
                                 bbox_xmaxs]).transpose()
     self.assertAllEqual(expected_boxes,
@@ -383,553 +404,594 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def testDecodeDefaultGroundtruthWeights(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
     bbox_xmaxs = [3.0, 7.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_boxes]
-                         .get_shape().as_list()), [None, 4])
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_boxes].get_shape().as_list()),
+                          [None, 4])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllClose(tensor_dict[fields.InputDataFields.groundtruth_weights],
                         np.ones(2, dtype=np.float32))
 
   def testDecodeObjectLabel(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes = [0, 1]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(bbox_classes),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [2])
-
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(bbox_classes),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeMultiClassScores(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
     bbox_xmaxs = [3.0, 7.0]
     flattened_multiclass_scores = [100., 50.] + [20., 30.]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/multiclass_scores':
-                    dataset_util.float_list_feature(flattened_multiclass_scores
-                                                   ),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_multiclass_scores=True)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/multiclass_scores':
+                      dataset_util.float_list_feature(
+                          flattened_multiclass_scores),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_multiclass_scores=True)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(flattened_multiclass_scores,
                         tensor_dict[fields.InputDataFields.multiclass_scores])
 
   def testDecodeEmptyMultiClassScores(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
     bbox_xmaxs = [3.0, 7.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_multiclass_scores=True)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
-    self.assertEqual(0,
-                     tensor_dict[fields.InputDataFields.multiclass_scores].size)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_multiclass_scores=True)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
+    self.assertEqual(
+        (0,), tensor_dict[fields.InputDataFields.multiclass_scores].shape)
 
   def testDecodeObjectLabelNoText(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes = [1, 2]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(bbox_classes),
-            })).SerializeToString()
-    label_map_string = """
-      item {
-        id:1
-        name:'cat'
-      }
-      item {
-        id:2
-        name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
 
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [None])
-
-    init = tf.tables_initializer()
-    with self.test_session() as sess:
-      sess.run(init)
-      tensor_dict = sess.run(tensor_dict)
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(bbox_classes),
+              })).SerializeToString()
+      label_map_string = """
+        item {
+          id:1
+          name:'cat'
+        }
+        item {
+          id:2
+          name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [None])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectLabelWithText(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes_text = [six.b('cat'), six.b('dog')]
     # Annotation label gets overridden by labelmap id.
     annotated_bbox_classes = [3, 4]
     expected_bbox_classes = [1, 2]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/text':
-                    dataset_util.bytes_list_feature(bbox_classes_text),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(annotated_bbox_classes),
-            })).SerializeToString()
-    label_map_string = """
-      item {
-        id:1
-        name:'cat'
-      }
-      item {
-        id:2
-        name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
 
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    init = tf.tables_initializer()
-    with self.test_session() as sess:
-      sess.run(init)
-      tensor_dict = sess.run(tensor_dict)
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(annotated_bbox_classes),
+              })).SerializeToString()
+      label_map_string = """
+        item {
+          id:1
+          name:'cat'
+        }
+        item {
+          id:2
+          name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(expected_bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectLabelUnrecognizedName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes_text = [six.b('cat'), six.b('cheetah')]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/text':
-                    dataset_util.bytes_list_feature(bbox_classes_text),
-            })).SerializeToString()
-
-    label_map_string = """
-      item {
-        id:2
-        name:'cat'
-      }
-      item {
-        id:1
-        name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [None])
-
-    with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+              })).SerializeToString()
+
+      label_map_string = """
+        item {
+          id:2
+          name:'cat'
+        }
+        item {
+          id:1
+          name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [None])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual([2, -1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectLabelWithMappingWithDisplayName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes_text = [six.b('cat'), six.b('dog')]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/text':
-                    dataset_util.bytes_list_feature(bbox_classes_text),
-            })).SerializeToString()
-
-    label_map_string = """
-      item {
-        id:3
-        display_name:'cat'
-      }
-      item {
-        id:1
-        display_name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [None])
-
-    with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+              })).SerializeToString()
+
+      label_map_string = """
+        item {
+          id:3
+          display_name:'cat'
+        }
+        item {
+          id:1
+          display_name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [None])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual([3, 1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectLabelUnrecognizedNameWithMappingWithDisplayName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes_text = [six.b('cat'), six.b('cheetah')]
     bbox_classes_id = [5, 6]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/text':
-                    dataset_util.bytes_list_feature(bbox_classes_text),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(bbox_classes_id),
-            })).SerializeToString()
-
-    label_map_string = """
-      item {
-        name:'/m/cat'
-        id:3
-        display_name:'cat'
-      }
-      item {
-        name:'/m/dog'
-        id:1
-        display_name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(bbox_classes_id),
+              })).SerializeToString()
+
+      label_map_string = """
+        item {
+          name:'/m/cat'
+          id:3
+          display_name:'cat'
+        }
+        item {
+          name:'/m/dog'
+          id:1
+          display_name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual([3, -1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectLabelWithMappingWithName(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_classes_text = [six.b('cat'), six.b('dog')]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/class/text':
-                    dataset_util.bytes_list_feature(bbox_classes_text),
-            })).SerializeToString()
-
-    label_map_string = """
-      item {
-        id:3
-        name:'cat'
-      }
-      item {
-        id:1
-        name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [None])
-
-    with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+              })).SerializeToString()
+
+      label_map_string = """
+        item {
+          id:3
+          name:'cat'
+        }
+        item {
+          id:1
+          name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [None])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual([3, 1],
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
   def testDecodeObjectArea(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     object_area = [100., 174.]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/area':
-                    dataset_util.float_list_feature(object_area),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_area]
-                         .get_shape().as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/area':
+                      dataset_util.float_list_feature(object_area),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_area].get_shape().as_list()), [2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(object_area,
                         tensor_dict[fields.InputDataFields.groundtruth_area])
 
   def testDecodeObjectIsCrowd(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     object_is_crowd = [0, 1]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/is_crowd':
-                    dataset_util.int64_list_feature(object_is_crowd),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_is_crowd].get_shape()
-         .as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/is_crowd':
+                      dataset_util.int64_list_feature(object_is_crowd),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_is_crowd].get_shape().as_list()),
+                          [2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(
         [bool(item) for item in object_is_crowd],
         tensor_dict[fields.InputDataFields.groundtruth_is_crowd])
 
   def testDecodeObjectDifficult(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     object_difficult = [0, 1]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/difficult':
-                    dataset_util.int64_list_feature(object_difficult),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_difficult].get_shape()
-         .as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/difficult':
+                      dataset_util.int64_list_feature(object_difficult),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_difficult].get_shape().as_list()),
+                          [2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(
         [bool(item) for item in object_difficult],
         tensor_dict[fields.InputDataFields.groundtruth_difficult])
 
   def testDecodeObjectGroupOf(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     object_group_of = [0, 1]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/group_of':
-                    dataset_util.int64_list_feature(object_group_of),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_group_of].get_shape()
-         .as_list()), [2])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
 
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/group_of':
+                      dataset_util.int64_list_feature(object_group_of),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_group_of].get_shape().as_list()),
+                          [2])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(
         [bool(item) for item in object_group_of],
         tensor_dict[fields.InputDataFields.groundtruth_group_of])
 
   def testDecodeObjectWeight(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     object_weights = [0.75, 1.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/object/weight':
-                    dataset_util.float_list_feature(object_weights),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_weights]
-                         .get_shape().as_list()), [None])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/weight':
+                      dataset_util.float_list_feature(object_weights),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
+
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_weights].get_shape().as_list()),
+                          [None])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(object_weights,
                         tensor_dict[fields.InputDataFields.groundtruth_weights])
 
   def testDecodeClassConfidence(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     class_confidence = [0.0, 1.0, 0.0]
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/class/confidence':
-                    dataset_util.float_list_feature(class_confidence),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
 
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_image_confidences]
-         .get_shape().as_list()), [3])
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/class/confidence':
+                      dataset_util.float_list_feature(class_confidence),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      output = example_decoder.decode(tf.convert_to_tensor(example))
 
+      self.assertAllEqual(
+          (output[fields.InputDataFields.groundtruth_image_confidences]
+           .get_shape().as_list()), [3])
+      return output
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllEqual(
         class_confidence,
         tensor_dict[fields.InputDataFields.groundtruth_image_confidences])
@@ -942,7 +1004,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
     # Randomly generate image.
     image_tensor = np.random.randint(
         256, size=(image_height, image_width, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
 
     # Randomly generate instance segmentation masks.
     instance_masks = (
@@ -954,35 +1017,37 @@ class TfExampleDecoderTest(tf.test.TestCase):
     object_classes = np.random.randint(
         100, size=(num_instances)).astype(np.int64)
 
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/height':
-                    dataset_util.int64_feature(image_height),
-                'image/width':
-                    dataset_util.int64_feature(image_width),
-                'image/object/mask':
-                    dataset_util.float_list_feature(instance_masks_flattened),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(object_classes)
-            })).SerializeToString()
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_instance_masks=True)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/height':
+                      dataset_util.int64_feature(image_height),
+                  'image/width':
+                      dataset_util.int64_feature(image_width),
+                  'image/object/mask':
+                      dataset_util.float_list_feature(instance_masks_flattened),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(object_classes)
+              })).SerializeToString()
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_instance_masks=True)
+      output = example_decoder.decode(tf.convert_to_tensor(example))
 
-    self.assertAllEqual(
-        (tensor_dict[fields.InputDataFields.groundtruth_instance_masks]
-         .get_shape().as_list()), [4, 5, 3])
+      self.assertAllEqual(
+          (output[fields.InputDataFields.groundtruth_instance_masks].get_shape(
+          ).as_list()), [4, 5, 3])
 
-    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
-                         .get_shape().as_list()), [4])
+      self.assertAllEqual((output[
+          fields.InputDataFields.groundtruth_classes].get_shape().as_list()),
+                          [4])
+      return output
 
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+    tensor_dict = self.execute_cpu(graph_fn, [])
 
     self.assertAllEqual(
         instance_masks.astype(np.float32),
@@ -997,7 +1062,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
     # Randomly generate image.
     image_tensor = np.random.randint(
         256, size=(image_height, image_width, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
 
     # Randomly generate instance segmentation masks.
     instance_masks = (
@@ -1009,76 +1075,83 @@ class TfExampleDecoderTest(tf.test.TestCase):
     object_classes = np.random.randint(
         100, size=(num_instances)).astype(np.int64)
 
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/height':
-                    dataset_util.int64_feature(image_height),
-                'image/width':
-                    dataset_util.int64_feature(image_width),
-                'image/object/mask':
-                    dataset_util.float_list_feature(instance_masks_flattened),
-                'image/object/class/label':
-                    dataset_util.int64_list_feature(object_classes)
-            })).SerializeToString()
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/height':
+                      dataset_util.int64_feature(image_height),
+                  'image/width':
+                      dataset_util.int64_feature(image_width),
+                  'image/object/mask':
+                      dataset_util.float_list_feature(instance_masks_flattened),
+                  'image/object/class/label':
+                      dataset_util.int64_list_feature(object_classes)
+              })).SerializeToString()
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertTrue(
         fields.InputDataFields.groundtruth_instance_masks not in tensor_dict)
 
   def testDecodeImageLabels(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded': dataset_util.bytes_feature(encoded_jpeg),
-                'image/format': dataset_util.bytes_feature(six.b('jpeg')),
-                'image/class/label': dataset_util.int64_list_feature([1, 2]),
-            })).SerializeToString()
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
+
+    def graph_fn_1():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded': dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format': dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/class/label': dataset_util.int64_list_feature([1, 2]),
+              })).SerializeToString()
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn_1, [])
     self.assertTrue(
         fields.InputDataFields.groundtruth_image_classes in tensor_dict)
     self.assertAllEqual(
         tensor_dict[fields.InputDataFields.groundtruth_image_classes],
         np.array([1, 2]))
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/class/text':
-                    dataset_util.bytes_list_feature(
-                        [six.b('dog'), six.b('cat')]),
-            })).SerializeToString()
-    label_map_string = """
-      item {
-        id:3
-        name:'cat'
-      }
-      item {
-        id:1
-        name:'dog'
-      }
-    """
-    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
-    with tf.gfile.Open(label_map_path, 'wb') as f:
-      f.write(label_map_string)
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        label_map_proto_file=label_map_path)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      sess.run(tf.tables_initializer())
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn_2():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/class/text':
+                      dataset_util.bytes_list_feature(
+                          [six.b('dog'), six.b('cat')]),
+              })).SerializeToString()
+      label_map_string = """
+        item {
+          id:3
+          name:'cat'
+        }
+        item {
+          id:1
+          name:'dog'
+        }
+      """
+      label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+      with tf.gfile.Open(label_map_path, 'wb') as f:
+        f.write(label_map_string)
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn_2, [])
     self.assertTrue(
         fields.InputDataFields.groundtruth_image_classes in tensor_dict)
     self.assertAllEqual(
@@ -1087,7 +1160,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def testDecodeContextFeatures(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
@@ -1095,32 +1169,34 @@ class TfExampleDecoderTest(tf.test.TestCase):
     num_features = 8
     context_feature_length = 10
     context_features = np.random.random(num_features*context_feature_length)
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/context_features':
-                    dataset_util.float_list_feature(context_features),
-                'image/context_feature_length':
-                    dataset_util.int64_feature(context_feature_length),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder(
-        load_context_features=True)
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/context_features':
+                      dataset_util.float_list_feature(context_features),
+                  'image/context_feature_length':
+                      dataset_util.int64_feature(context_feature_length),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          load_context_features=True)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertAllClose(
         context_features.reshape(num_features, context_feature_length),
         tensor_dict[fields.InputDataFields.context_features])
@@ -1130,7 +1206,8 @@ class TfExampleDecoderTest(tf.test.TestCase):
 
   def testContextFeaturesNotAvailableByDefault(self):
     image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
-    encoded_jpeg = self._EncodeImage(image_tensor)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
     bbox_ymins = [0.0, 4.0]
     bbox_xmins = [1.0, 5.0]
     bbox_ymaxs = [2.0, 6.0]
@@ -1138,34 +1215,136 @@ class TfExampleDecoderTest(tf.test.TestCase):
     num_features = 10
     context_feature_length = 10
     context_features = np.random.random(num_features*context_feature_length)
-    example = tf.train.Example(
-        features=tf.train.Features(
-            feature={
-                'image/encoded':
-                    dataset_util.bytes_feature(encoded_jpeg),
-                'image/format':
-                    dataset_util.bytes_feature(six.b('jpeg')),
-                'image/context_features':
-                    dataset_util.float_list_feature(context_features),
-                'image/context_feature_length':
-                    dataset_util.int64_feature(context_feature_length),
-                'image/object/bbox/ymin':
-                    dataset_util.float_list_feature(bbox_ymins),
-                'image/object/bbox/xmin':
-                    dataset_util.float_list_feature(bbox_xmins),
-                'image/object/bbox/ymax':
-                    dataset_util.float_list_feature(bbox_ymaxs),
-                'image/object/bbox/xmax':
-                    dataset_util.float_list_feature(bbox_xmaxs),
-            })).SerializeToString()
-
-    example_decoder = tf_example_decoder.TfExampleDecoder()
-    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
-    with self.test_session() as sess:
-      tensor_dict = sess.run(tensor_dict)
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/context_features':
+                      dataset_util.float_list_feature(context_features),
+                  'image/context_feature_length':
+                      dataset_util.int64_feature(context_feature_length),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder()
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
     self.assertNotIn(fields.InputDataFields.context_features,
                      tensor_dict)
 
+  def testExpandLabels(self):
+    label_map_string = """
+      item {
+        id:1
+        name:'cat'
+        ancestor_ids: 2
+      }
+      item {
+        id:2
+        name:'animal'
+        descendant_ids: 1
+      }
+      item {
+        id:3
+        name:'man'
+        ancestor_ids: 5
+      }
+      item {
+        id:4
+        name:'woman'
+        display_name:'woman'
+        ancestor_ids: 5
+      }
+      item {
+        id:5
+        name:'person'
+        descendant_ids: 3
+        descendant_ids: 4
+      }
+    """
+
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+    image_tensor = np.random.randint(256, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg, _ = self._create_encoded_and_decoded_data(
+        image_tensor, 'jpeg')
+    bbox_ymins = [0.0, 4.0]
+    bbox_xmins = [1.0, 5.0]
+    bbox_ymaxs = [2.0, 6.0]
+    bbox_xmaxs = [3.0, 7.0]
+    bbox_classes_text = [six.b('cat'), six.b('cat')]
+    bbox_group_of = [0, 1]
+    image_class_text = [six.b('cat'), six.b('person')]
+    image_confidence = [1.0, 0.0]
+
+    def graph_fn():
+      example = tf.train.Example(
+          features=tf.train.Features(
+              feature={
+                  'image/encoded':
+                      dataset_util.bytes_feature(encoded_jpeg),
+                  'image/format':
+                      dataset_util.bytes_feature(six.b('jpeg')),
+                  'image/object/bbox/ymin':
+                      dataset_util.float_list_feature(bbox_ymins),
+                  'image/object/bbox/xmin':
+                      dataset_util.float_list_feature(bbox_xmins),
+                  'image/object/bbox/ymax':
+                      dataset_util.float_list_feature(bbox_ymaxs),
+                  'image/object/bbox/xmax':
+                      dataset_util.float_list_feature(bbox_xmaxs),
+                  'image/object/class/text':
+                      dataset_util.bytes_list_feature(bbox_classes_text),
+                  'image/object/group_of':
+                      dataset_util.int64_list_feature(bbox_group_of),
+                  'image/class/text':
+                      dataset_util.bytes_list_feature(image_class_text),
+                  'image/class/confidence':
+                      dataset_util.float_list_feature(image_confidence),
+              })).SerializeToString()
+
+      example_decoder = tf_example_decoder.TfExampleDecoder(
+          label_map_proto_file=label_map_path, expand_hierarchy_labels=True)
+      return example_decoder.decode(tf.convert_to_tensor(example))
+
+    tensor_dict = self.execute_cpu(graph_fn, [])
+
+    boxes = np.vstack([bbox_ymins, bbox_xmins, bbox_ymaxs,
+                       bbox_xmaxs]).transpose()
+    expected_boxes = np.stack(
+        [boxes[0, :], boxes[0, :], boxes[1, :], boxes[1, :]], axis=0)
+    expected_boxes_class = np.array([1, 2, 1, 2])
+    expected_boxes_group_of = np.array([0, 0, 1, 1])
+    expected_image_class = np.array([1, 2, 3, 4, 5])
+    expected_image_confidence = np.array([1.0, 1.0, 0.0, 0.0, 0.0])
+    self.assertAllEqual(expected_boxes,
+                        tensor_dict[fields.InputDataFields.groundtruth_boxes])
+    self.assertAllEqual(expected_boxes_class,
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertAllEqual(
+        expected_boxes_group_of,
+        tensor_dict[fields.InputDataFields.groundtruth_group_of])
+    self.assertAllEqual(
+        expected_image_class,
+        tensor_dict[fields.InputDataFields.groundtruth_image_classes])
+    self.assertAllEqual(
+        expected_image_confidence,
+        tensor_dict[fields.InputDataFields.groundtruth_image_confidences])
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/data_decoders/tf_sequence_example_decoder.py b/research/object_detection/data_decoders/tf_sequence_example_decoder.py
new file mode 100644
index 00000000..1565a910
--- /dev/null
+++ b/research/object_detection/data_decoders/tf_sequence_example_decoder.py
@@ -0,0 +1,314 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Sequence example decoder for object detection."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from six.moves import zip
+import tensorflow.compat.v1 as tf
+from tf_slim import tfexample_decoder as slim_example_decoder
+
+from object_detection.core import data_decoder
+from object_detection.core import standard_fields as fields
+from object_detection.utils import label_map_util
+
+# pylint: disable=g-import-not-at-top
+try:
+  from tensorflow.contrib import lookup as contrib_lookup
+except ImportError:
+  # TF 2.0 doesn't ship with contrib.
+  pass
+# pylint: enable=g-import-not-at-top
+
+
+class _ClassTensorHandler(slim_example_decoder.Tensor):
+  """An ItemHandler to fetch class ids from class text."""
+
+  def __init__(self,
+               tensor_key,
+               label_map_proto_file,
+               shape_keys=None,
+               shape=None,
+               default_value=''):
+    """Initializes the LookupTensor handler.
+
+    Simply calls a vocabulary (most often, a label mapping) lookup.
+
+    Args:
+      tensor_key: the name of the `TFExample` feature to read the tensor from.
+      label_map_proto_file: File path to a text format LabelMapProto message
+        mapping class text to id.
+      shape_keys: Optional name or list of names of the TF-Example feature in
+        which the tensor shape is stored. If a list, then each corresponds to
+        one dimension of the shape.
+      shape: Optional output shape of the `Tensor`. If provided, the `Tensor` is
+        reshaped accordingly.
+      default_value: The value used when the `tensor_key` is not found in a
+        particular `TFExample`.
+
+    Raises:
+      ValueError: if both `shape_keys` and `shape` are specified.
+    """
+    name_to_id = label_map_util.get_label_map_dict(
+        label_map_proto_file, use_display_name=False)
+    # We use a default_value of -1, but we expect all labels to be contained
+    # in the label map.
+    try:
+      # Dynamically try to load the tf v2 lookup, falling back to contrib
+      lookup = tf.compat.v2.lookup
+      hash_table_class = tf.compat.v2.lookup.StaticHashTable
+    except AttributeError:
+      lookup = contrib_lookup
+      hash_table_class = contrib_lookup.HashTable
+    name_to_id_table = hash_table_class(
+        initializer=lookup.KeyValueTensorInitializer(
+            keys=tf.constant(list(name_to_id.keys())),
+            values=tf.constant(list(name_to_id.values()), dtype=tf.int64)),
+        default_value=-1)
+
+    self._name_to_id_table = name_to_id_table
+    super(_ClassTensorHandler, self).__init__(tensor_key, shape_keys, shape,
+                                              default_value)
+
+  def tensors_to_item(self, keys_to_tensors):
+    unmapped_tensor = super(_ClassTensorHandler,
+                            self).tensors_to_item(keys_to_tensors)
+    return self._name_to_id_table.lookup(unmapped_tensor)
+
+
+class TfSequenceExampleDecoder(data_decoder.DataDecoder):
+  """Tensorflow Sequence Example proto decoder for Object Detection.
+
+  Sequence examples contain sequences of images which share common
+  features. The structure of TfSequenceExamples can be seen in
+  dataset_tools/seq_example_util.py
+
+  For the TFODAPI, the following fields are required:
+    Shared features:
+      'image/format'
+      'image/height'
+      'image/width'
+
+    Features with an entry for each image, where bounding box features can
+    be empty lists if the image does not contain any objects:
+      'image/encoded'
+      'image/source_id'
+      'region/bbox/xmin'
+      'region/bbox/xmax'
+      'region/bbox/ymin'
+      'region/bbox/ymax'
+      'region/label/string'
+
+  Optionally, the sequence example can include context_features for use in
+  Context R-CNN (see https://arxiv.org/abs/1912.03538):
+    'image/context_features'
+    'image/context_feature_length'
+  """
+
+  def __init__(self,
+               label_map_proto_file,
+               load_context_features=False,
+               use_display_name=False,
+               fully_annotated=False):
+    """Constructs `TfSequenceExampleDecoder` object.
+
+    Args:
+      label_map_proto_file: a file path to a
+        object_detection.protos.StringIntLabelMap proto. The
+        label map will be used to map IDs of 'region/label/string'.
+        It is assumed that 'region/label/string' will be in the data.
+      load_context_features: Whether to load information from context_features,
+        to provide additional context to a detection model for training and/or
+        inference
+      use_display_name: whether or not to use the `display_name` for label
+        mapping (instead of `name`).  Only used if label_map_proto_file is
+        provided.
+      fully_annotated: If True, will assume that every frame (whether it has
+        boxes or not), has been fully annotated. If False, a
+        'region/is_annotated' field must be provided in the dataset which
+        indicates which frames have annotations. Default False.
+    """
+    # Specifies how the tf.SequenceExamples are decoded.
+    self._context_keys_to_features = {
+        'image/format': tf.FixedLenFeature((), tf.string, default_value='jpeg'),
+        'image/height': tf.FixedLenFeature((), tf.int64),
+        'image/width': tf.FixedLenFeature((), tf.int64),
+    }
+    self._sequence_keys_to_feature_lists = {
+        'image/encoded': tf.FixedLenSequenceFeature([], dtype=tf.string),
+        'image/source_id': tf.FixedLenSequenceFeature([], dtype=tf.string),
+        'region/bbox/xmin': tf.VarLenFeature(dtype=tf.float32),
+        'region/bbox/xmax': tf.VarLenFeature(dtype=tf.float32),
+        'region/bbox/ymin': tf.VarLenFeature(dtype=tf.float32),
+        'region/bbox/ymax': tf.VarLenFeature(dtype=tf.float32),
+        'region/label/string': tf.VarLenFeature(dtype=tf.string),
+        'region/label/confidence': tf.VarLenFeature(dtype=tf.float32),
+    }
+
+    self._items_to_handlers = {
+        # Context.
+        fields.InputDataFields.image_height:
+            slim_example_decoder.Tensor('image/height'),
+        fields.InputDataFields.image_width:
+            slim_example_decoder.Tensor('image/width'),
+
+        # Sequence.
+        fields.InputDataFields.num_groundtruth_boxes:
+            slim_example_decoder.NumBoxesSequence('region/bbox/xmin'),
+        fields.InputDataFields.groundtruth_boxes:
+            slim_example_decoder.BoundingBoxSequence(
+                prefix='region/bbox/', default_value=0.0),
+        fields.InputDataFields.groundtruth_weights:
+            slim_example_decoder.Tensor('region/label/confidence'),
+    }
+
+    # If the dataset is sparsely annotated, parse sequence features which
+    # indicate which frames have been labeled.
+    if not fully_annotated:
+      self._sequence_keys_to_feature_lists['region/is_annotated'] = (
+          tf.FixedLenSequenceFeature([], dtype=tf.int64))
+      self._items_to_handlers[fields.InputDataFields.is_annotated] = (
+          slim_example_decoder.Tensor('region/is_annotated'))
+
+    self._items_to_handlers[fields.InputDataFields.image] = (
+        slim_example_decoder.Tensor('image/encoded'))
+    self._items_to_handlers[fields.InputDataFields.source_id] = (
+        slim_example_decoder.Tensor('image/source_id'))
+
+    label_handler = _ClassTensorHandler(
+        'region/label/string', label_map_proto_file, default_value='')
+
+    self._items_to_handlers[
+        fields.InputDataFields.groundtruth_classes] = label_handler
+
+    if load_context_features:
+      self._context_keys_to_features['image/context_features'] = (
+          tf.VarLenFeature(dtype=tf.float32))
+      self._items_to_handlers[fields.InputDataFields.context_features] = (
+          slim_example_decoder.ItemHandlerCallback(
+              ['image/context_features', 'image/context_feature_length'],
+              self._reshape_context_features))
+
+      self._context_keys_to_features['image/context_feature_length'] = (
+          tf.FixedLenFeature((), tf.int64))
+      self._items_to_handlers[fields.InputDataFields.context_feature_length] = (
+          slim_example_decoder.Tensor('image/context_feature_length'))
+    self._fully_annotated = fully_annotated
+
+  def decode(self, tf_seq_example_string_tensor):
+    """Decodes serialized `tf.SequenceExample`s and returns a tensor dictionary.
+
+    Args:
+      tf_seq_example_string_tensor: a string tensor holding a serialized
+        `tf.SequenceExample`.
+
+    Returns:
+      A list of dictionaries with (at least) the following tensors:
+      fields.InputDataFields.source_id: a [num_frames] string tensor with a
+        unique ID for each frame.
+      fields.InputDataFields.num_groundtruth_boxes: a [num_frames] int32 tensor
+        specifying the number of boxes in each frame.
+      fields.InputDataFields.groundtruth_boxes: a [num_frames, num_boxes, 4]
+        float32 tensor with bounding boxes for each frame. Note that num_boxes
+        is the maximum boxes seen in any individual frame. Any frames with fewer
+        boxes are padded with 0.0.
+      fields.InputDataFields.groundtruth_classes: a [num_frames, num_boxes]
+        int32 tensor with class indices for each box in each frame.
+      fields.InputDataFields.groundtruth_weights: a [num_frames, num_boxes]
+        float32 tensor with weights of the groundtruth boxes.
+      fields.InputDataFields.is_annotated: a [num_frames] bool tensor specifying
+        whether the image was annotated or not. If False, the corresponding
+        entries in the groundtruth tensor will be ignored.
+      fields.InputDataFields.context_features - 1D float32 tensor of shape
+        [context_feature_length * num_context_features]
+      fields.InputDataFields.context_feature_length - int32 tensor specifying
+        the length of each feature in context_features
+      fields.InputDataFields.image: a [num_frames] string tensor with
+        the encoded images.
+    """
+    serialized_example = tf.reshape(tf_seq_example_string_tensor, shape=[])
+    decoder = slim_example_decoder.TFSequenceExampleDecoder(
+        self._context_keys_to_features, self._sequence_keys_to_feature_lists,
+        self._items_to_handlers)
+    keys = decoder.list_items()
+    tensors = decoder.decode(serialized_example, items=keys)
+    tensor_dict = dict(list(zip(keys, tensors)))
+    tensor_dict[fields.InputDataFields.groundtruth_boxes].set_shape(
+        [None, None, 4])
+    tensor_dict[fields.InputDataFields.num_groundtruth_boxes] = tf.cast(
+        tensor_dict[fields.InputDataFields.num_groundtruth_boxes],
+        dtype=tf.int32)
+    tensor_dict[fields.InputDataFields.groundtruth_classes] = tf.cast(
+        tensor_dict[fields.InputDataFields.groundtruth_classes], dtype=tf.int32)
+    tensor_dict[fields.InputDataFields.original_image_spatial_shape] = tf.cast(
+        tf.stack([
+            tensor_dict[fields.InputDataFields.image_height],
+            tensor_dict[fields.InputDataFields.image_width]
+        ]),
+        dtype=tf.int32)
+    tensor_dict.pop(fields.InputDataFields.image_height)
+    tensor_dict.pop(fields.InputDataFields.image_width)
+
+    def default_groundtruth_weights():
+      """Produces weights of 1.0 for each valid box, and 0.0 otherwise."""
+      num_boxes_per_frame = tensor_dict[
+          fields.InputDataFields.num_groundtruth_boxes]
+      max_num_boxes = tf.reduce_max(num_boxes_per_frame)
+      num_boxes_per_frame_tiled = tf.tile(
+          tf.expand_dims(num_boxes_per_frame, axis=-1),
+          multiples=tf.stack([1, max_num_boxes]))
+      range_tiled = tf.tile(
+          tf.expand_dims(tf.range(max_num_boxes), axis=0),
+          multiples=tf.stack([tf.shape(num_boxes_per_frame)[0], 1]))
+      return tf.cast(
+          tf.greater(num_boxes_per_frame_tiled, range_tiled), tf.float32)
+
+    tensor_dict[fields.InputDataFields.groundtruth_weights] = tf.cond(
+        tf.greater(
+            tf.size(tensor_dict[fields.InputDataFields.groundtruth_weights]),
+            0), lambda: tensor_dict[fields.InputDataFields.groundtruth_weights],
+        default_groundtruth_weights)
+
+    if self._fully_annotated:
+      tensor_dict[fields.InputDataFields.is_annotated] = tf.ones_like(
+          tensor_dict[fields.InputDataFields.num_groundtruth_boxes],
+          dtype=tf.bool)
+    else:
+      tensor_dict[fields.InputDataFields.is_annotated] = tf.cast(
+          tensor_dict[fields.InputDataFields.is_annotated], dtype=tf.bool)
+
+    return tensor_dict
+
+  def _reshape_context_features(self, keys_to_tensors):
+    """Reshape context features.
+
+    The instance context_features are reshaped to
+      [num_context_features, context_feature_length]
+
+    Args:
+      keys_to_tensors: a dictionary from keys to tensors.
+
+    Returns:
+      A 2-D float tensor of shape [num_context_features, context_feature_length]
+    """
+    context_feature_length = keys_to_tensors['image/context_feature_length']
+    to_shape = tf.cast(tf.stack([-1, context_feature_length]), tf.int32)
+    context_features = keys_to_tensors['image/context_features']
+    if isinstance(context_features, tf.SparseTensor):
+      context_features = tf.sparse_tensor_to_dense(context_features)
+    context_features = tf.reshape(context_features, to_shape)
+    return context_features
diff --git a/research/object_detection/data_decoders/tf_sequence_example_decoder_test.py b/research/object_detection/data_decoders/tf_sequence_example_decoder_test.py
new file mode 100644
index 00000000..2ea1c616
--- /dev/null
+++ b/research/object_detection/data_decoders/tf_sequence_example_decoder_test.py
@@ -0,0 +1,173 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for tf_sequence_example_decoder.py."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import numpy as np
+import tensorflow.compat.v1 as tf
+
+from object_detection.core import standard_fields as fields
+from object_detection.data_decoders import tf_sequence_example_decoder
+from object_detection.dataset_tools import seq_example_util
+from object_detection.utils import test_case
+
+
+class TfSequenceExampleDecoderTest(test_case.TestCase):
+
+  def _create_label_map(self, path):
+    label_map_text = """
+      item {
+        name: "dog"
+        id: 1
+      }
+      item {
+        name: "cat"
+        id: 2
+      }
+      item {
+        name: "panda"
+        id: 4
+      }
+    """
+    with tf.gfile.Open(path, 'wb') as f:
+      f.write(label_map_text)
+
+  def _make_random_serialized_jpeg_images(self, num_frames, image_height,
+                                          image_width):
+    def graph_fn():
+      images = tf.cast(tf.random.uniform(
+          [num_frames, image_height, image_width, 3],
+          maxval=256,
+          dtype=tf.int32), dtype=tf.uint8)
+      images_list = tf.unstack(images, axis=0)
+      return [tf.io.encode_jpeg(image) for image in images_list]
+    encoded_images = self.execute(graph_fn, [])
+    return encoded_images
+
+  def test_decode_sequence_example(self):
+    num_frames = 4
+    image_height = 20
+    image_width = 30
+
+    expected_groundtruth_boxes = [
+        [[0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0]],
+        [[0.2, 0.2, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]],
+        [[0.0, 0.0, 1.0, 1.0], [0.1, 0.1, 0.2, 0.2]],
+        [[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]
+    ]
+    expected_groundtruth_classes = [
+        [-1, -1],
+        [-1, 1],
+        [1, 2],
+        [-1, -1]
+    ]
+
+    flds = fields.InputDataFields
+    encoded_images = self._make_random_serialized_jpeg_images(
+        num_frames, image_height, image_width)
+
+    def graph_fn():
+      label_map_proto_file = os.path.join(self.get_temp_dir(), 'labelmap.pbtxt')
+      self._create_label_map(label_map_proto_file)
+      decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder(
+          label_map_proto_file=label_map_proto_file)
+      sequence_example_serialized = seq_example_util.make_sequence_example(
+          dataset_name='video_dataset',
+          video_id='video',
+          encoded_images=encoded_images,
+          image_height=image_height,
+          image_width=image_width,
+          image_format='JPEG',
+          image_source_ids=[str(i) for i in range(num_frames)],
+          is_annotated=[[1], [1], [1], [1]],
+          bboxes=[
+              [[0., 0., 1., 1.]],  # Frame 0.
+              [[0.2, 0.2, 1., 1.],
+               [0., 0., 1., 1.]],  # Frame 1.
+              [[0., 0., 1., 1.],  # Frame 2.
+               [0.1, 0.1, 0.2, 0.2]],
+              [[]],  # Frame 3.
+          ],
+          label_strings=[
+              ['fox'],  # Frame 0. Fox will be filtered out.
+              ['fox', 'dog'],  # Frame 1. Fox will be filtered out.
+              ['dog', 'cat'],  # Frame 2.
+              [],  # Frame 3
+          ]).SerializeToString()
+
+      example_string_tensor = tf.convert_to_tensor(sequence_example_serialized)
+      return decoder.decode(example_string_tensor)
+
+    tensor_dict_out = self.execute(graph_fn, [])
+    self.assertAllClose(expected_groundtruth_boxes,
+                        tensor_dict_out[flds.groundtruth_boxes])
+    self.assertAllEqual(expected_groundtruth_classes,
+                        tensor_dict_out[flds.groundtruth_classes])
+
+  def test_decode_sequence_example_negative_clip(self):
+    num_frames = 4
+    image_height = 20
+    image_width = 30
+
+    expected_groundtruth_boxes = -1 * np.ones((4, 0, 4))
+    expected_groundtruth_classes = -1 * np.ones((4, 0))
+
+    flds = fields.InputDataFields
+
+    encoded_images = self._make_random_serialized_jpeg_images(
+        num_frames, image_height, image_width)
+
+    def graph_fn():
+      sequence_example_serialized = seq_example_util.make_sequence_example(
+          dataset_name='video_dataset',
+          video_id='video',
+          encoded_images=encoded_images,
+          image_height=image_height,
+          image_width=image_width,
+          image_format='JPEG',
+          image_source_ids=[str(i) for i in range(num_frames)],
+          bboxes=[
+              [[]],
+              [[]],
+              [[]],
+              [[]]
+          ],
+          label_strings=[
+              [],
+              [],
+              [],
+              []
+          ]).SerializeToString()
+      example_string_tensor = tf.convert_to_tensor(sequence_example_serialized)
+
+      label_map_proto_file = os.path.join(self.get_temp_dir(), 'labelmap.pbtxt')
+      self._create_label_map(label_map_proto_file)
+      decoder = tf_sequence_example_decoder.TfSequenceExampleDecoder(
+          label_map_proto_file=label_map_proto_file)
+      return decoder.decode(example_string_tensor)
+
+    tensor_dict_out = self.execute(graph_fn, [])
+    self.assertAllClose(expected_groundtruth_boxes,
+                        tensor_dict_out[flds.groundtruth_boxes])
+    self.assertAllEqual(expected_groundtruth_classes,
+                        tensor_dict_out[flds.groundtruth_classes])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index aee40f62..51ed3891 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -40,7 +40,7 @@ import numpy as np
 import PIL.Image
 
 from pycocotools import mask
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record_test.py b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
index 756fb61b..0bcc8be9 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
@@ -21,7 +21,7 @@ import os
 import numpy as np
 import PIL.Image
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import create_coco_tf_record
 
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record.py b/research/object_detection/dataset_tools/create_kitti_tf_record.py
index c612db99..fe4f13ec 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record.py
@@ -42,7 +42,7 @@ import os
 
 import numpy as np
 import PIL.Image as pil
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
index 1e3097a0..606c684e 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
@@ -20,7 +20,7 @@ import os
 import numpy as np
 import PIL.Image
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import create_kitti_tf_record
 
diff --git a/research/object_detection/dataset_tools/create_oid_tf_record.py b/research/object_detection/dataset_tools/create_oid_tf_record.py
index 26d9699c..9b35765b 100644
--- a/research/object_detection/dataset_tools/create_oid_tf_record.py
+++ b/research/object_detection/dataset_tools/create_oid_tf_record.py
@@ -40,7 +40,7 @@ import os
 
 import contextlib2
 import pandas as pd
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import oid_tfrecord_creation
 from object_detection.dataset_tools import tf_record_creation_util
diff --git a/research/object_detection/dataset_tools/create_pascal_tf_record.py b/research/object_detection/dataset_tools/create_pascal_tf_record.py
index 813071c9..8d79a339 100644
--- a/research/object_detection/dataset_tools/create_pascal_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pascal_tf_record.py
@@ -32,7 +32,7 @@ import os
 
 from lxml import etree
 import PIL.Image
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
diff --git a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
index 982652ed..c751a139 100644
--- a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
@@ -20,7 +20,7 @@ import os
 import numpy as np
 import PIL.Image
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import create_pascal_tf_record
 
diff --git a/research/object_detection/dataset_tools/create_pet_tf_record.py b/research/object_detection/dataset_tools/create_pet_tf_record.py
index 9b3b55c6..78524b50 100644
--- a/research/object_detection/dataset_tools/create_pet_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pet_tf_record.py
@@ -37,7 +37,7 @@ import contextlib2
 from lxml import etree
 import numpy as np
 import PIL.Image
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py
index 76898ac7..ca010c5b 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion_test.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import oid_hierarchical_labels_expansion
 
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation.py b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
index 12f9b6e0..a4618a6a 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.utils import dataset_util
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
index e0520fb0..b1e945f4 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation_test.py
@@ -16,7 +16,7 @@
 
 import pandas as pd
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import oid_tfrecord_creation
 
diff --git a/research/object_detection/dataset_tools/seq_example_util.py b/research/object_detection/dataset_tools/seq_example_util.py
new file mode 100644
index 00000000..84573ec7
--- /dev/null
+++ b/research/object_detection/dataset_tools/seq_example_util.py
@@ -0,0 +1,264 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Common utility for object detection tf.train.SequenceExamples."""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow.compat.v1 as tf
+
+
+def context_float_feature(ndarray):
+  """Converts a numpy float array to a context float feature.
+
+  Args:
+    ndarray: A numpy float array.
+
+  Returns:
+    A context float feature.
+  """
+  feature = tf.train.Feature()
+  for val in ndarray:
+    feature.float_list.value.append(val)
+  return feature
+
+
+def context_int64_feature(ndarray):
+  """Converts a numpy array to a context int64 feature.
+
+  Args:
+    ndarray: A numpy int64 array.
+
+  Returns:
+    A context int64 feature.
+  """
+  feature = tf.train.Feature()
+  for val in ndarray:
+    feature.int64_list.value.append(val)
+  return feature
+
+
+def context_bytes_feature(ndarray):
+  """Converts a numpy bytes array to a context bytes feature.
+
+  Args:
+    ndarray: A numpy bytes array.
+
+  Returns:
+    A context bytes feature.
+  """
+  feature = tf.train.Feature()
+  for val in ndarray:
+    if isinstance(val, np.ndarray):
+      val = val.tolist()
+    feature.bytes_list.value.append(tf.compat.as_bytes(val))
+  return feature
+
+
+def sequence_float_feature(ndarray):
+  """Converts a numpy float array to a sequence float feature.
+
+  Args:
+    ndarray: A numpy float array.
+
+  Returns:
+    A sequence float feature.
+  """
+  feature_list = tf.train.FeatureList()
+  for row in ndarray:
+    feature = feature_list.feature.add()
+    if row.size:
+      feature.float_list.value[:] = row
+  return feature_list
+
+
+def sequence_int64_feature(ndarray):
+  """Converts a numpy int64 array to a sequence int64 feature.
+
+  Args:
+    ndarray: A numpy int64 array.
+
+  Returns:
+    A sequence int64 feature.
+  """
+  feature_list = tf.train.FeatureList()
+  for row in ndarray:
+    feature = feature_list.feature.add()
+    if row.size:
+      feature.int64_list.value[:] = row
+  return feature_list
+
+
+def sequence_bytes_feature(ndarray):
+  """Converts a bytes float array to a sequence bytes feature.
+
+  Args:
+    ndarray: A numpy bytes array.
+
+  Returns:
+    A sequence bytes feature.
+  """
+  feature_list = tf.train.FeatureList()
+  for row in ndarray:
+    if isinstance(row, np.ndarray):
+      row = row.tolist()
+    feature = feature_list.feature.add()
+    if row:
+      row = [tf.compat.as_bytes(val) for val in row]
+      feature.bytes_list.value[:] = row
+  return feature_list
+
+
+def boxes_to_box_components(bboxes):
+  """Converts a list of numpy arrays (boxes) to box components.
+
+  Args:
+    bboxes: A numpy array of bounding boxes.
+
+  Returns:
+    Bounding box component lists.
+  """
+  ymin_list = []
+  xmin_list = []
+  ymax_list = []
+  xmax_list = []
+  for bbox in bboxes:
+    bbox = np.array(bbox).astype(np.float32)
+    ymin, xmin, ymax, xmax = np.split(bbox, 4, axis=1)
+    ymin_list.append(np.reshape(ymin, [-1]))
+    xmin_list.append(np.reshape(xmin, [-1]))
+    ymax_list.append(np.reshape(ymax, [-1]))
+    xmax_list.append(np.reshape(xmax, [-1]))
+  return ymin_list, xmin_list, ymax_list, xmax_list
+
+
+def make_sequence_example(dataset_name,
+                          video_id,
+                          encoded_images,
+                          image_height,
+                          image_width,
+                          image_format=None,
+                          image_source_ids=None,
+                          timestamps=None,
+                          is_annotated=None,
+                          bboxes=None,
+                          label_strings=None,
+                          detection_bboxes=None,
+                          detection_classes=None,
+                          detection_scores=None):
+  """Constructs tf.SequenceExamples.
+
+  Args:
+    dataset_name: String with dataset name.
+    video_id: String with video id.
+    encoded_images: A [num_frames] list (or numpy array) of encoded image
+      frames.
+    image_height: Height of the images.
+    image_width: Width of the images.
+    image_format: Format of encoded images.
+    image_source_ids: (Optional) A [num_frames] list of unique string ids for
+      each image.
+    timestamps: (Optional) A [num_frames] list (or numpy array) array with image
+      timestamps.
+    is_annotated: (Optional) A [num_frames] list (or numpy array) array
+      in which each element indicates whether the frame has been annotated
+      (1) or not (0).
+    bboxes: (Optional) A list (with num_frames elements) of [num_boxes_i, 4]
+      numpy float32 arrays holding boxes for each frame.
+    label_strings: (Optional) A list (with num_frames_elements) of [num_boxes_i]
+      numpy string arrays holding object string labels for each frame.
+    detection_bboxes: (Optional) A list (with num_frames elements) of
+      [num_boxes_i, 4] numpy float32 arrays holding prediction boxes for each
+      frame.
+    detection_classes: (Optional) A list (with num_frames_elements) of
+      [num_boxes_i] numpy int64 arrays holding predicted classes for each frame.
+    detection_scores: (Optional) A list (with num_frames_elements) of
+      [num_boxes_i] numpy float32 arrays holding predicted object scores for
+      each frame.
+
+  Returns:
+    A tf.train.SequenceExample.
+  """
+  num_frames = len(encoded_images)
+  image_encoded = np.expand_dims(encoded_images, axis=-1)
+  if timestamps is None:
+    timestamps = np.arange(num_frames)
+  image_timestamps = np.expand_dims(timestamps, axis=-1)
+
+  # Context fields.
+  context_dict = {
+      'example/dataset_name': context_bytes_feature([dataset_name]),
+      'clip/start/timestamp': context_int64_feature([image_timestamps[0][0]]),
+      'clip/end/timestamp': context_int64_feature([image_timestamps[-1][0]]),
+      'clip/frames': context_int64_feature([num_frames]),
+      'image/channels': context_int64_feature([3]),
+      'image/height': context_int64_feature([image_height]),
+      'image/width': context_int64_feature([image_width]),
+      'clip/media_id': context_bytes_feature([video_id])
+  }
+
+  # Sequence fields.
+  feature_list = {
+      'image/encoded': sequence_bytes_feature(image_encoded),
+      'image/timestamp': sequence_int64_feature(image_timestamps),
+  }
+
+  # Add optional fields.
+  if image_format is not None:
+    context_dict['image/format'] = context_bytes_feature([image_format])
+  if image_source_ids is not None:
+    feature_list['image/source_id'] = sequence_bytes_feature(image_source_ids)
+  if bboxes is not None:
+    bbox_ymin, bbox_xmin, bbox_ymax, bbox_xmax = boxes_to_box_components(bboxes)
+    feature_list['region/bbox/xmin'] = sequence_float_feature(bbox_xmin)
+    feature_list['region/bbox/xmax'] = sequence_float_feature(bbox_xmax)
+    feature_list['region/bbox/ymin'] = sequence_float_feature(bbox_ymin)
+    feature_list['region/bbox/ymax'] = sequence_float_feature(bbox_ymax)
+    if is_annotated is None:
+      is_annotated = np.ones(num_frames, dtype=np.int64)
+    is_annotated = np.expand_dims(is_annotated, axis=-1)
+    feature_list['region/is_annotated'] = sequence_int64_feature(is_annotated)
+
+  if label_strings is not None:
+    feature_list['region/label/string'] = sequence_bytes_feature(
+        label_strings)
+
+  if detection_bboxes is not None:
+    det_bbox_ymin, det_bbox_xmin, det_bbox_ymax, det_bbox_xmax = (
+        boxes_to_box_components(detection_bboxes))
+    feature_list['predicted/region/bbox/xmin'] = sequence_float_feature(
+        det_bbox_xmin)
+    feature_list['predicted/region/bbox/xmax'] = sequence_float_feature(
+        det_bbox_xmax)
+    feature_list['predicted/region/bbox/ymin'] = sequence_float_feature(
+        det_bbox_ymin)
+    feature_list['predicted/region/bbox/ymax'] = sequence_float_feature(
+        det_bbox_ymax)
+  if detection_classes is not None:
+    feature_list['predicted/region/label/index'] = sequence_int64_feature(
+        detection_classes)
+  if detection_scores is not None:
+    feature_list['predicted/region/label/confidence'] = sequence_float_feature(
+        detection_scores)
+
+  context = tf.train.Features(feature=context_dict)
+  feature_lists = tf.train.FeatureLists(feature_list=feature_list)
+
+  sequence_example = tf.train.SequenceExample(
+      context=context,
+      feature_lists=feature_lists)
+  return sequence_example
diff --git a/research/object_detection/dataset_tools/seq_example_util_test.py b/research/object_detection/dataset_tools/seq_example_util_test.py
new file mode 100644
index 00000000..81fd4f54
--- /dev/null
+++ b/research/object_detection/dataset_tools/seq_example_util_test.py
@@ -0,0 +1,359 @@
+# Lint as: python2, python3
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for object_detection.utils.seq_example_util."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import six
+import tensorflow.compat.v1 as tf
+
+from object_detection.dataset_tools import seq_example_util
+
+
+class SeqExampleUtilTest(tf.test.TestCase):
+
+  def test_make_unlabeled_example(self):
+    num_frames = 5
+    image_height = 100
+    image_width = 200
+    dataset_name = b'unlabeled_dataset'
+    video_id = b'video_000'
+    images = tf.cast(tf.random.uniform(
+        [num_frames, image_height, image_width, 3],
+        maxval=256,
+        dtype=tf.int32), dtype=tf.uint8)
+    image_source_ids = [str(idx) for idx in range(num_frames)]
+    images_list = tf.unstack(images, axis=0)
+    encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+    with tf.Session() as sess:
+      encoded_images = sess.run(encoded_images_list)
+    seq_example = seq_example_util.make_sequence_example(
+        dataset_name=dataset_name,
+        video_id=video_id,
+        encoded_images=encoded_images,
+        image_height=image_height,
+        image_width=image_width,
+        image_format='JPEG',
+        image_source_ids=image_source_ids)
+
+    context_feature_dict = seq_example.context.feature
+    self.assertEqual(
+        dataset_name,
+        context_feature_dict['example/dataset_name'].bytes_list.value[0])
+    self.assertEqual(
+        0,
+        context_feature_dict['clip/start/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        num_frames - 1,
+        context_feature_dict['clip/end/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        num_frames,
+        context_feature_dict['clip/frames'].int64_list.value[0])
+    self.assertEqual(
+        3,
+        context_feature_dict['image/channels'].int64_list.value[0])
+    self.assertEqual(
+        b'JPEG',
+        context_feature_dict['image/format'].bytes_list.value[0])
+    self.assertEqual(
+        image_height,
+        context_feature_dict['image/height'].int64_list.value[0])
+    self.assertEqual(
+        image_width,
+        context_feature_dict['image/width'].int64_list.value[0])
+    self.assertEqual(
+        video_id,
+        context_feature_dict['clip/media_id'].bytes_list.value[0])
+
+    seq_feature_dict = seq_example.feature_lists.feature_list
+    self.assertLen(
+        seq_feature_dict['image/encoded'].feature[:],
+        num_frames)
+    timestamps = [
+        feature.int64_list.value[0] for feature
+        in seq_feature_dict['image/timestamp'].feature]
+    self.assertAllEqual(list(range(num_frames)), timestamps)
+    source_ids = [
+        feature.bytes_list.value[0] for feature
+        in seq_feature_dict['image/source_id'].feature]
+    self.assertAllEqual(
+        [six.ensure_binary(str(idx)) for idx in range(num_frames)],
+        source_ids)
+
+  def test_make_labeled_example(self):
+    num_frames = 2
+    image_height = 100
+    image_width = 200
+    dataset_name = b'unlabeled_dataset'
+    video_id = b'video_000'
+    labels = [b'dog', b'cat']
+    images = tf.cast(tf.random.uniform(
+        [num_frames, image_height, image_width, 3],
+        maxval=256,
+        dtype=tf.int32), dtype=tf.uint8)
+    images_list = tf.unstack(images, axis=0)
+    encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+    with tf.Session() as sess:
+      encoded_images = sess.run(encoded_images_list)
+    timestamps = [100000, 110000]
+    is_annotated = [1, 0]
+    bboxes = [
+        np.array([[0., 0., 0., 0.],
+                  [0., 0., 1., 1.]], dtype=np.float32),
+        np.zeros([0, 4], dtype=np.float32)
+    ]
+    label_strings = [
+        np.array(labels),
+        np.array([])
+    ]
+
+    seq_example = seq_example_util.make_sequence_example(
+        dataset_name=dataset_name,
+        video_id=video_id,
+        encoded_images=encoded_images,
+        image_height=image_height,
+        image_width=image_width,
+        timestamps=timestamps,
+        is_annotated=is_annotated,
+        bboxes=bboxes,
+        label_strings=label_strings)
+
+    context_feature_dict = seq_example.context.feature
+    self.assertEqual(
+        dataset_name,
+        context_feature_dict['example/dataset_name'].bytes_list.value[0])
+    self.assertEqual(
+        timestamps[0],
+        context_feature_dict['clip/start/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        timestamps[-1],
+        context_feature_dict['clip/end/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        num_frames,
+        context_feature_dict['clip/frames'].int64_list.value[0])
+
+    seq_feature_dict = seq_example.feature_lists.feature_list
+    self.assertLen(
+        seq_feature_dict['image/encoded'].feature[:],
+        num_frames)
+    actual_timestamps = [
+        feature.int64_list.value[0] for feature
+        in seq_feature_dict['image/timestamp'].feature]
+    self.assertAllEqual(timestamps, actual_timestamps)
+    # Frame 0.
+    self.assertAllEqual(
+        is_annotated[0],
+        seq_feature_dict['region/is_annotated'].feature[0].int64_list.value[0])
+    self.assertAllClose(
+        [0., 0.],
+        seq_feature_dict['region/bbox/ymin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0., 0.],
+        seq_feature_dict['region/bbox/xmin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0., 1.],
+        seq_feature_dict['region/bbox/ymax'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0., 1.],
+        seq_feature_dict['region/bbox/xmax'].feature[0].float_list.value[:])
+    self.assertAllEqual(
+        labels,
+        seq_feature_dict['region/label/string'].feature[0].bytes_list.value[:])
+
+    # Frame 1.
+    self.assertAllEqual(
+        is_annotated[1],
+        seq_feature_dict['region/is_annotated'].feature[1].int64_list.value[0])
+    self.assertAllClose(
+        [],
+        seq_feature_dict['region/bbox/ymin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict['region/bbox/xmin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict['region/bbox/ymax'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict['region/bbox/xmax'].feature[1].float_list.value[:])
+    self.assertAllEqual(
+        [],
+        seq_feature_dict['region/label/string'].feature[1].bytes_list.value[:])
+
+  def test_make_labeled_example_with_predictions(self):
+    num_frames = 2
+    image_height = 100
+    image_width = 200
+    dataset_name = b'unlabeled_dataset'
+    video_id = b'video_000'
+    images = tf.cast(tf.random.uniform(
+        [num_frames, image_height, image_width, 3],
+        maxval=256,
+        dtype=tf.int32), dtype=tf.uint8)
+    images_list = tf.unstack(images, axis=0)
+    encoded_images_list = [tf.io.encode_jpeg(image) for image in images_list]
+    with tf.Session() as sess:
+      encoded_images = sess.run(encoded_images_list)
+    bboxes = [
+        np.array([[0., 0., 0.75, 0.75],
+                  [0., 0., 1., 1.]], dtype=np.float32),
+        np.array([[0., 0.25, 0.5, 0.75]], dtype=np.float32)
+    ]
+    label_strings = [
+        np.array(['cat', 'frog']),
+        np.array(['cat'])
+    ]
+    detection_bboxes = [
+        np.array([[0., 0., 0.75, 0.75]], dtype=np.float32),
+        np.zeros([0, 4], dtype=np.float32)
+    ]
+    detection_classes = [
+        np.array([5], dtype=np.int64),
+        np.array([], dtype=np.int64)
+    ]
+    detection_scores = [
+        np.array([0.9], dtype=np.float32),
+        np.array([], dtype=np.float32)
+    ]
+
+    seq_example = seq_example_util.make_sequence_example(
+        dataset_name=dataset_name,
+        video_id=video_id,
+        encoded_images=encoded_images,
+        image_height=image_height,
+        image_width=image_width,
+        bboxes=bboxes,
+        label_strings=label_strings,
+        detection_bboxes=detection_bboxes,
+        detection_classes=detection_classes,
+        detection_scores=detection_scores)
+
+    context_feature_dict = seq_example.context.feature
+    self.assertEqual(
+        dataset_name,
+        context_feature_dict['example/dataset_name'].bytes_list.value[0])
+    self.assertEqual(
+        0,
+        context_feature_dict['clip/start/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        1,
+        context_feature_dict['clip/end/timestamp'].int64_list.value[0])
+    self.assertEqual(
+        num_frames,
+        context_feature_dict['clip/frames'].int64_list.value[0])
+
+    seq_feature_dict = seq_example.feature_lists.feature_list
+    self.assertLen(
+        seq_feature_dict['image/encoded'].feature[:],
+        num_frames)
+    actual_timestamps = [
+        feature.int64_list.value[0] for feature
+        in seq_feature_dict['image/timestamp'].feature]
+    self.assertAllEqual([0, 1], actual_timestamps)
+    # Frame 0.
+    self.assertAllEqual(
+        1,
+        seq_feature_dict['region/is_annotated'].feature[0].int64_list.value[0])
+    self.assertAllClose(
+        [0., 0.],
+        seq_feature_dict['region/bbox/ymin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0., 0.],
+        seq_feature_dict['region/bbox/xmin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0.75, 1.],
+        seq_feature_dict['region/bbox/ymax'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0.75, 1.],
+        seq_feature_dict['region/bbox/xmax'].feature[0].float_list.value[:])
+    self.assertAllEqual(
+        ['cat', 'frog'],
+        seq_feature_dict['region/label/string'].feature[0].bytes_list.value[:])
+    self.assertAllClose(
+        [0.],
+        seq_feature_dict[
+            'predicted/region/bbox/ymin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0.],
+        seq_feature_dict[
+            'predicted/region/bbox/xmin'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0.75],
+        seq_feature_dict[
+            'predicted/region/bbox/ymax'].feature[0].float_list.value[:])
+    self.assertAllClose(
+        [0.75],
+        seq_feature_dict[
+            'predicted/region/bbox/xmax'].feature[0].float_list.value[:])
+    self.assertAllEqual(
+        [5],
+        seq_feature_dict[
+            'predicted/region/label/index'].feature[0].int64_list.value[:])
+    self.assertAllClose(
+        [0.9],
+        seq_feature_dict[
+            'predicted/region/label/confidence'].feature[0].float_list.value[:])
+
+    # Frame 1.
+    self.assertAllEqual(
+        1,
+        seq_feature_dict['region/is_annotated'].feature[1].int64_list.value[0])
+    self.assertAllClose(
+        [0.0],
+        seq_feature_dict['region/bbox/ymin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [0.25],
+        seq_feature_dict['region/bbox/xmin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [0.5],
+        seq_feature_dict['region/bbox/ymax'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [0.75],
+        seq_feature_dict['region/bbox/xmax'].feature[1].float_list.value[:])
+    self.assertAllEqual(
+        ['cat'],
+        seq_feature_dict['region/label/string'].feature[1].bytes_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict[
+            'predicted/region/bbox/ymin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict[
+            'predicted/region/bbox/xmin'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict[
+            'predicted/region/bbox/ymax'].feature[1].float_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict[
+            'predicted/region/bbox/xmax'].feature[1].float_list.value[:])
+    self.assertAllEqual(
+        [],
+        seq_feature_dict[
+            'predicted/region/label/index'].feature[1].int64_list.value[:])
+    self.assertAllClose(
+        [],
+        seq_feature_dict[
+            'predicted/region/label/confidence'].feature[1].float_list.value[:])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util.py b/research/object_detection/dataset_tools/tf_record_creation_util.py
index 781a0640..e54bcbce 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util_test.py b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
index d37bcbe8..2873a6d1 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util_test.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util_test.py
@@ -23,7 +23,7 @@ import os
 import contextlib2
 import six
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.dataset_tools import tf_record_creation_util
 
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 08b7f3ad..e2d1255b 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -24,9 +24,10 @@ import time
 
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+
+import tf_slim as slim
 
-from tensorflow.contrib import slim
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import keypoint_ops
@@ -553,11 +554,15 @@ def _resize_detection_masks(args):
 
 
 def _resize_groundtruth_masks(args):
-  mask, image_shape = args
+  """Resizes groundgtruth masks to the original image size."""
+  mask, true_image_shape, original_image_shape = args
+  true_height = true_image_shape[0]
+  true_width = true_image_shape[1]
+  mask = mask[:, :true_height, :true_width]
   mask = tf.expand_dims(mask, 3)
   mask = tf.image.resize_images(
       mask,
-      image_shape,
+      original_image_shape,
       method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
       align_corners=True)
   return tf.cast(tf.squeeze(mask, 3), tf.uint8)
@@ -689,7 +694,7 @@ def result_dict_for_batched_example(images,
 
   Args:
     images: A single 4D uint8 image tensor of shape [batch_size, H, W, C].
-    keys: A [batch_size] string tensor with image identifier.
+    keys: A [batch_size] string/int tensor with image identifier.
     detections: A dictionary of detections, returned from
       DetectionModel.postprocess().
     groundtruth: (Optional) Dictionary of groundtruth items, with fields:
@@ -711,6 +716,8 @@ def result_dict_for_batched_example(images,
         2] float32 tensor with keypoints (Optional).
       'groundtruth_keypoint_visibilities': [batch_size, max_number_of_boxes,
         num_keypoints] bool tensor with keypoint visibilities (Optional).
+      'groundtruth_labeled_classes': [batch_size, num_classes] int64
+        tensor of 1-indexed classes. (Optional)
     class_agnostic: Boolean indicating whether the detections are class-agnostic
       (i.e. binary). Default False.
     scale_to_absolute: Boolean indicating whether boxes and keypoints should be
@@ -762,6 +769,8 @@ def result_dict_for_batched_example(images,
       tensor with keypoints (Optional).
     'groundtruth_keypoint_visibilities': [batch_size, num_boxes, num_keypoints]
       bool tensor with keypoint visibilities (Optional).
+    'groundtruth_labeled_classes': [batch_size, num_classes]  int64 tensor
+      of 1-indexed classes. (Optional)
     'num_groundtruth_boxes': [batch_size] tensor containing the maximum number
       of groundtruth boxes per image.
 
@@ -871,7 +880,7 @@ def result_dict_for_batched_example(images,
       groundtruth[input_data_fields.groundtruth_instance_masks] = (
           shape_utils.static_or_dynamic_map_fn(
               _resize_groundtruth_masks,
-              elems=[masks, original_image_spatial_shapes],
+              elems=[masks, true_image_shapes, original_image_spatial_shapes],
               dtype=tf.uint8))
 
     output_dict.update(groundtruth)
diff --git a/research/object_detection/eval_util_test.py b/research/object_detection/eval_util_test.py
index 7006e13e..f2f66405 100644
--- a/research/object_detection/eval_util_test.py
+++ b/research/object_detection/eval_util_test.py
@@ -23,7 +23,7 @@ from absl.testing import parameterized
 import numpy as np
 import six
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import eval_util
 from object_detection.core import standard_fields as fields
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 662cf1f1..bcb5c40b 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -104,7 +104,7 @@ python export_inference_graph.py \
               } \
             }"
 """
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from object_detection import exporter
 from object_detection.protos import pipeline_pb2
diff --git a/research/object_detection/export_tflite_ssd_graph.py b/research/object_detection/export_tflite_ssd_graph.py
index 88fe1812..2127ca08 100644
--- a/research/object_detection/export_tflite_ssd_graph.py
+++ b/research/object_detection/export_tflite_ssd_graph.py
@@ -92,7 +92,7 @@ python object_detection/export_tflite_ssd_graph.py \
        "
 """
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from object_detection import export_tflite_ssd_graph_lib
 from object_detection.protos import pipeline_pb2
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index 548818c6..229daab0 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -20,7 +20,7 @@ See export_tflite_ssd_graph.py for usage.
 import os
 import tempfile
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.core.framework import attr_value_pb2
 from tensorflow.core.framework import types_pb2
 from tensorflow.core.protobuf import saver_pb2
diff --git a/research/object_detection/export_tflite_ssd_graph_lib_test.py b/research/object_detection/export_tflite_ssd_graph_lib_test.py
index cb83e7b7..5b6082d1 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib_test.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib_test.py
@@ -20,7 +20,9 @@ from __future__ import print_function
 import os
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from tensorflow.core.framework import types_pb2
 from object_detection import export_tflite_ssd_graph_lib
 from object_detection import exporter
@@ -32,11 +34,6 @@ from object_detection.protos import pipeline_pb2
 from object_detection.protos import post_processing_pb2
 
 # pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import slim as contrib_slim
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
 
 if six.PY2:
   import mock
@@ -54,7 +51,7 @@ class FakeModel(model.DetectionModel):
     pass
 
   def predict(self, preprocessed_inputs, true_image_shapes):
-    features = contrib_slim.conv2d(preprocessed_inputs, 3, 1)
+    features = slim.conv2d(preprocessed_inputs, 3, 1)
     with tf.control_dependencies([features]):
       prediction_tensors = {
           'box_encodings':
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 62c0ee4a..676e34de 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -16,7 +16,8 @@
 """Functions to export object detection inference graph."""
 import os
 import tempfile
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from tensorflow.core.protobuf import saver_pb2
 from tensorflow.python.tools import freeze_graph  # pylint: disable=g-direct-tensorflow-import
 from object_detection.builders import graph_rewriter_builder
@@ -28,7 +29,6 @@ from object_detection.utils import shape_utils
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim
   from tensorflow.contrib import tfprof as contrib_tfprof
   from tensorflow.contrib.quantize.python import graph_matcher
 except ImportError:
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index e26e164e..babe41d1 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import os
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from tensorflow.python.framework import dtypes
 from tensorflow.python.ops import array_ops
@@ -42,7 +42,7 @@ else:
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -1092,7 +1092,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     g = tf.Graph()
     with g.as_default():
       x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
-      x_conv = contrib_slim.conv2d(x, 8, 1)
+      x_conv = slim.conv2d(x, 8, 1)
       y = array_ops.placeholder(dtypes.float32, shape=(8, 20, 20, 8))
       s = ops.nearest_neighbor_upsampling(x_conv, 2)
       t = s + y
@@ -1137,7 +1137,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     g = tf.Graph()
     with g.as_default():
       x = array_ops.placeholder(dtypes.float32, shape=(8, 10, 10, 8))
-      x_conv = contrib_slim.conv2d(x, 8, 1)
+      x_conv = slim.conv2d(x, 8, 1)
       s = ops.nearest_neighbor_upsampling(x_conv, 2)
       t = s[:, :19, :19, :]
 
diff --git a/research/object_detection/g3doc/detection_model_zoo.md b/research/object_detection/g3doc/detection_model_zoo.md
index 686f6f8c..b13fe6a3 100644
--- a/research/object_detection/g3doc/detection_model_zoo.md
+++ b/research/object_detection/g3doc/detection_model_zoo.md
@@ -109,14 +109,21 @@ Note: If you download the tar.gz file of quantized models and un-tar, you will g
 
 Model name                                                                                                                                                                | Pixel 1 Latency (ms) | COCO mAP | Outputs
 ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :------: | :-----:
-[ssd_mobilenet_v2_mnasfpn_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_06.tar.gz) | 183                  | 26.6     | Boxes
-[ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz)                                       | 119                  | 22.6     | Boxes
-[ssd_mobilenet_v3_small_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz)                                       | 43                   | 15.4     | Boxes
+[ssd_mobiledet_cpu_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_cpu_320x320_coco_2020_05_19.tar.gz)                                      |  113  |  24.0  |  Boxes
+[ssd_mobilenet_v2_mnasfpn_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_18.tar.gz)  |  183  |  26.6  |  Boxes
+[ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz)                                        |  119  |  22.6  |  Boxes
+[ssd_mobilenet_v3_small_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_small_coco_2020_01_14.tar.gz)                                        |  43   |  15.4  |  Boxes
 
 ### Pixel4 Edge TPU models
-Model name                                                                                                                          | Pixel 4  Edge TPU Latency (ms) | COCO mAP | Outputs
+Model name                                                                                                                          | Pixel 4  Edge TPU Latency (ms) | COCO mAP (fp32/uint8) | Outputs
 ----------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :------: | :-----:
-[ssd_mobilenet_edgetpu_coco](https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/ssdlite_mobilenet_edgetpu_coco_quant.tar.gz) | 6.6                  | 24.3     | Boxes
+[ssd_mobiledet_edgetpu_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_edgetpu_320x320_coco_2020_05_19.tar.gz)  |  6.9  |  25.9/25.6  |  Boxes
+[ssd_mobilenet_edgetpu_coco](https://storage.cloud.google.com/mobilenet_edgetpu/checkpoints/ssdlite_mobilenet_edgetpu_coco_quant.tar.gz)       |  6.6  |  -/24.3     |  Boxes
+
+### Pixel4 DSP models
+Model name                                                                                                                          | Pixel 4  DSP Latency (ms) | COCO mAP (fp32/uint8) | Outputs
+----------------------------------------------------------------------------------------------------------------------------------- | :------------------: | :------: | :-----:
+[ssd_mobiledet_dsp_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobiledet_dsp_320x320_coco_2020_05_19.tar.gz)  |  12.3  |  28.9/28.8  |  Boxes
 
 ## Kitti-trained models
 
diff --git a/research/object_detection/g3doc/faq.md b/research/object_detection/g3doc/faq.md
index faa94b39..c0ca503f 100644
--- a/research/object_detection/g3doc/faq.md
+++ b/research/object_detection/g3doc/faq.md
@@ -9,7 +9,7 @@ groundtruth boxes in the dataset. If an image is encountered with more
 bounding boxes, the excess boxes will be clipped.
 
 ## Q: AttributeError: 'module' object has no attribute 'BackupHandler'
-A: This BackupHandler (tf.contrib.slim.tfexample_decoder.BackupHandler) was
+A: This BackupHandler (tf_slim.tfexample_decoder.BackupHandler) was
 introduced in tensorflow 1.5.0 so runing with earlier versions may cause this
 issue. It now has been replaced by
 object_detection.data_decoders.tf_example_decoder.BackupHandler. Whoever sees
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index fb13ca02..05c89180 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -8,7 +8,8 @@ Tensorflow Object Detection API depends on the following libraries:
 *   Python-tk
 *   Pillow 1.0
 *   lxml
-*   tf Slim (which is included in the "tensorflow/models/research/" checkout)
+*   tf-slim (https://github.com/google-research/tf-slim)
+*   slim (which is included in the "tensorflow/models/research/" checkout)
 *   Jupyter notebook
 *   Matplotlib
 *   Tensorflow (1.15.0)
@@ -29,23 +30,25 @@ pip install tensorflow-gpu
 
 The remaining libraries can be installed on Ubuntu 16.04 using via apt-get:
 
-``` bash
+```bash
 sudo apt-get install protobuf-compiler python-pil python-lxml python-tk
 pip install --user Cython
 pip install --user contextlib2
 pip install --user jupyter
 pip install --user matplotlib
+pip install --user tf_slim
 ```
 
 Alternatively, users can install dependencies using pip:
 
-``` bash
+```bash
 pip install --user Cython
 pip install --user contextlib2
 pip install --user pillow
 pip install --user lxml
 pip install --user jupyter
 pip install --user matplotlib
+pip install --user tf_slim
 ```
 
 <!-- common_typos_disable -->
@@ -161,6 +164,10 @@ to avoid running this manually, you can add it as a new line to the end of your
 tensorflow/models/research on your system. After updating ~/.bashrc file you
 can run the following command:
 
+Note: Some of the functions defined in tensorflow/models/research/slim has been
+moved to [tf-slim](https://github.com/google-research/tf-slim), so installing
+tf_slim is required now.
+
 ``` bash
 source ~/.bashrc
 ```
diff --git a/research/object_detection/inference/detection_inference.py b/research/object_detection/inference/detection_inference.py
index ccd8b3ae..b395cd7e 100644
--- a/research/object_detection/inference/detection_inference.py
+++ b/research/object_detection/inference/detection_inference.py
@@ -15,7 +15,7 @@
 """Utility functions for detection inference."""
 from __future__ import division
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 
diff --git a/research/object_detection/inference/detection_inference_test.py b/research/object_detection/inference/detection_inference_test.py
index 31cd3e9b..6d35f2b6 100644
--- a/research/object_detection/inference/detection_inference_test.py
+++ b/research/object_detection/inference/detection_inference_test.py
@@ -19,7 +19,7 @@ import os
 import numpy as np
 from PIL import Image
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 
 from object_detection.core import standard_fields
diff --git a/research/object_detection/inference/infer_detections.py b/research/object_detection/inference/infer_detections.py
index a251009e..3579142f 100644
--- a/research/object_detection/inference/infer_detections.py
+++ b/research/object_detection/inference/infer_detections.py
@@ -35,7 +35,7 @@ metrics).
 """
 
 import itertools
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.inference import detection_inference
 
 tf.flags.DEFINE_string('input_tfrecord_paths', None,
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 74b2c61c..7512a56b 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 import functools
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.builders import dataset_builder
 from object_detection.builders import image_resizer_builder
 from object_detection.builders import model_builder
@@ -189,10 +189,21 @@ def transform_input_data(tensor_dict,
   Returns:
     A dictionary keyed by fields.InputDataFields containing the tensors obtained
     after applying all the transformations.
+
+  Raises:
+    KeyError: If both groundtruth_labeled_classes and groundtruth_image_classes
+      are provided by the decoder in tensor_dict since both fields are
+      considered to contain the same information.
   """
   out_tensor_dict = tensor_dict.copy()
 
   labeled_classes_field = fields.InputDataFields.groundtruth_labeled_classes
+  image_classes_field = fields.InputDataFields.groundtruth_image_classes
+  if (labeled_classes_field in out_tensor_dict and
+      image_classes_field in out_tensor_dict):
+    raise KeyError('groundtruth_labeled_classes and groundtruth_image_classes'
+                   'are provided by the decoder, but only one should be set.')
+
   if labeled_classes_field in out_tensor_dict:
     # tf_example_decoder casts unrecognized labels to -1. Remove these
     # unrecognized labels before converting labeled_classes to k-hot vector.
@@ -201,6 +212,10 @@ def transform_input_data(tensor_dict,
     out_tensor_dict[labeled_classes_field] = _convert_labeled_classes_to_k_hot(
         out_tensor_dict[labeled_classes_field], num_classes)
 
+  if image_classes_field in out_tensor_dict:
+    out_tensor_dict[labeled_classes_field] = _convert_labeled_classes_to_k_hot(
+        out_tensor_dict[image_classes_field], num_classes)
+
   if fields.InputDataFields.multiclass_scores in out_tensor_dict:
     out_tensor_dict[
         fields.InputDataFields
@@ -475,6 +490,9 @@ def pad_input_data_to_static_shapes(tensor_dict,
   if fields.InputDataFields.context_feature_length in tensor_dict:
     padding_shapes[fields.InputDataFields.context_feature_length] = []
 
+  if fields.InputDataFields.is_annotated in tensor_dict:
+    padding_shapes[fields.InputDataFields.is_annotated] = []
+
   padded_tensor_dict = {}
   for tensor_name in tensor_dict:
     padded_tensor_dict[tensor_name] = shape_utils.pad_or_clip_nd(
@@ -551,6 +569,7 @@ def _get_labels_dict(input_dict):
       fields.InputDataFields.groundtruth_instance_masks,
       fields.InputDataFields.groundtruth_area,
       fields.InputDataFields.groundtruth_is_crowd,
+      fields.InputDataFields.groundtruth_group_of,
       fields.InputDataFields.groundtruth_difficult,
       fields.InputDataFields.groundtruth_keypoint_visibilities,
       fields.InputDataFields.groundtruth_keypoint_weights,
@@ -700,6 +719,8 @@ def train_input(train_config, train_input_config,
       labels[fields.InputDataFields.groundtruth_visibilities] is a
         [batch_size, num_boxes, num_keypoints] bool tensor containing
         groundtruth visibilities for each keypoint.
+      labels[fields.InputDataFields.groundtruth_labeled_classes] is a
+        [batch_size, num_classes] float32 k-hot tensor of classes.
 
   Raises:
     TypeError: if the `train_config`, `train_input_config` or `model_config`
@@ -760,12 +781,14 @@ def train_input(train_config, train_input_config,
     include_source_id = train_input_config.include_source_id
     return (_get_features_dict(tensor_dict, include_source_id),
             _get_labels_dict(tensor_dict))
+  reduce_to_frame_fn = get_reduce_to_frame_fn(train_input_config, True)
 
   dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
       train_input_config,
       transform_input_data_fn=transform_and_pad_input_data_fn,
       batch_size=params['batch_size'] if params else train_config.batch_size,
-      input_context=input_context)
+      input_context=input_context,
+      reduce_to_frame_fn=reduce_to_frame_fn)
   return dataset
 
 
@@ -834,6 +857,11 @@ def eval_input(eval_config, eval_input_config, model_config,
       labels[fields.InputDataFields.groundtruth_visibilities] is a
         [batch_size, num_boxes, num_keypoints] bool tensor containing
         groundtruth visibilities for each keypoint.
+      labels[fields.InputDataFields.groundtruth_group_of] is a [1, num_boxes]
+        bool tensor indicating if the box covers more than 5 instances of the
+        same class which heavily occlude each other.
+      labels[fields.InputDataFields.groundtruth_labeled_classes] is a
+        [num_boxes, num_classes] float32 k-hot tensor of classes.
 
   Raises:
     TypeError: if the `eval_config`, `eval_input_config` or `model_config`
@@ -894,10 +922,14 @@ def eval_input(eval_config, eval_input_config, model_config,
     include_source_id = eval_input_config.include_source_id
     return (_get_features_dict(tensor_dict, include_source_id),
             _get_labels_dict(tensor_dict))
+
+  reduce_to_frame_fn = get_reduce_to_frame_fn(eval_input_config, False)
+
   dataset = INPUT_BUILDER_UTIL_MAP['dataset_build'](
       eval_input_config,
       batch_size=params['batch_size'] if params else eval_config.batch_size,
-      transform_input_data_fn=transform_and_pad_input_data_fn)
+      transform_input_data_fn=transform_and_pad_input_data_fn,
+      reduce_to_frame_fn=reduce_to_frame_fn)
   return dataset
 
 
@@ -953,3 +985,74 @@ def create_predict_input_fn(model_config, predict_input_config):
         receiver_tensors={SERVING_FED_EXAMPLE_KEY: example})
 
   return _predict_input_fn
+
+
+def get_reduce_to_frame_fn(input_reader_config, is_training):
+  """Returns a function reducing sequence tensors to single frame tensors.
+
+  If the input type is not TF_SEQUENCE_EXAMPLE, the tensors are passed through
+  this function unchanged. Otherwise, when in training mode, a single frame is
+  selected at random from the sequence example, and the tensors for that frame
+  are converted to single frame tensors, with all associated context features.
+  In evaluation mode all frames are converted to single frame tensors with
+  copied context tensors. After the sequence example tensors are converted into
+  one or many single frame tensors, the images from each frame are decoded.
+
+  Args:
+    input_reader_config: An input_reader_pb2.InputReader.
+    is_training: Whether we are in training mode.
+
+  Returns:
+    `reduce_to_frame_fn` for the dataset builder
+  """
+  if input_reader_config.input_type != (
+      input_reader_pb2.InputType.TF_SEQUENCE_EXAMPLE):
+    return lambda d: d
+  else:
+    def reduce_to_frame(dataset):
+      """Returns a function reducing sequence tensors to single frame tensors.
+
+      Args:
+        dataset: A tf dataset containing sequence tensors.
+
+      Returns:
+        A tf dataset containing single frame tensors.
+      """
+      if is_training:
+        def get_single_frame(tensor_dict):
+          """Returns a random frame from a sequence.
+
+          Picks a random frame and returns slices of sequence tensors
+          corresponding to the random frame. Returns non-sequence tensors
+          unchanged.
+
+          Args:
+            tensor_dict: A dictionary containing sequence tensors.
+
+          Returns:
+            Tensors for a single random frame within the sequence.
+          """
+          num_frames = tf.cast(
+              tf.shape(tensor_dict[fields.InputDataFields.source_id])[0],
+              dtype=tf.int32)
+          frame_index = tf.random.uniform((), minval=0, maxval=num_frames,
+                                          dtype=tf.int32)
+          out_tensor_dict = {}
+          for key in tensor_dict:
+            if key in fields.SEQUENCE_FIELDS:
+              # Slice random frame from sequence tensors
+              out_tensor_dict[key] = tensor_dict[key][frame_index]
+            else:
+              # Copy all context tensors.
+              out_tensor_dict[key] = tensor_dict[key]
+          return out_tensor_dict
+        dataset = dataset.map(get_single_frame, tf.data.experimental.AUTOTUNE)
+      else:
+        dataset = dataset.map(util_ops.tile_context_tensors,
+                              tf.data.experimental.AUTOTUNE)
+        dataset = dataset.unbatch()
+      # Decode frame here as SequenceExample tensors contain encoded images.
+      dataset = dataset.map(util_ops.decode_image,
+                            tf.data.experimental.AUTOTUNE)
+      return dataset
+    return reduce_to_frame
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index a450fcbb..78e268b2 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -24,7 +24,7 @@ from absl import logging
 from absl.testing import parameterized
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import inputs
 from object_detection.core import preprocessor
@@ -53,6 +53,25 @@ def _get_configs_for_model(model_name):
       configs, kwargs_dict=override_dict)
 
 
+def _get_configs_for_model_sequence_example(model_name):
+  """Returns configurations for model."""
+  fname = os.path.join(tf.resource_loader.get_data_files_path(),
+                       'test_data/' + model_name + '.config')
+  label_map_path = os.path.join(tf.resource_loader.get_data_files_path(),
+                                'data/snapshot_serengeti_label_map.pbtxt')
+  data_path = os.path.join(
+      tf.resource_loader.get_data_files_path(),
+      'test_data/snapshot_serengeti_sequence_examples.record')
+  configs = config_util.get_configs_from_pipeline_file(fname)
+  override_dict = {
+      'train_input_path': data_path,
+      'eval_input_path': data_path,
+      'label_map_path': label_map_path
+  }
+  return config_util.merge_external_params_with_configs(
+      configs, kwargs_dict=override_dict)
+
+
 def _make_initializable_iterator(dataset):
   """Creates an iterator, and initializes tables.
 
@@ -62,7 +81,7 @@ def _make_initializable_iterator(dataset):
   Returns:
     A `tf.data.Iterator`.
   """
-  iterator = dataset.make_initializable_iterator()
+  iterator = tf.data.make_initializable_iterator(dataset)
   tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
   return iterator
 
@@ -205,6 +224,85 @@ class InputsTest(test_case.TestCase, parameterized.TestCase):
     self.assertEqual(
         tf.int32, labels[fields.InputDataFields.groundtruth_difficult].dtype)
 
+  def test_context_rcnn_resnet50_train_input_with_sequence_example(
+      self, train_batch_size=8):
+    """Tests the training input function for FasterRcnnResnet50."""
+    configs = _get_configs_for_model_sequence_example(
+        'context_rcnn_camera_trap')
+    model_config = configs['model']
+    train_config = configs['train_config']
+    train_config.batch_size = train_batch_size
+    train_input_fn = inputs.create_train_input_fn(
+        train_config, configs['train_input_config'], model_config)
+    features, labels = _make_initializable_iterator(train_input_fn()).get_next()
+
+    self.assertAllEqual([train_batch_size, 640, 640, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual([train_batch_size],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [train_batch_size, 100, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [train_batch_size, 100, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [train_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_weights].dtype)
+    self.assertAllEqual(
+        [train_batch_size, 100, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_confidences].shape.as_list())
+    self.assertEqual(
+        tf.float32,
+        labels[fields.InputDataFields.groundtruth_confidences].dtype)
+
+  def test_context_rcnn_resnet50_eval_input_with_sequence_example(
+      self, eval_batch_size=8):
+    """Tests the eval input function for FasterRcnnResnet50."""
+    configs = _get_configs_for_model_sequence_example(
+        'context_rcnn_camera_trap')
+    model_config = configs['model']
+    eval_config = configs['eval_config']
+    eval_config.batch_size = eval_batch_size
+    eval_input_fn = inputs.create_eval_input_fn(
+        eval_config, configs['eval_input_configs'][0], model_config)
+    features, labels = _make_initializable_iterator(eval_input_fn()).get_next()
+    self.assertAllEqual([eval_batch_size, 640, 640, 3],
+                        features[fields.InputDataFields.image].shape.as_list())
+    self.assertEqual(tf.float32, features[fields.InputDataFields.image].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 640, 640, 3],
+        features[fields.InputDataFields.original_image].shape.as_list())
+    self.assertEqual(tf.uint8,
+                     features[fields.InputDataFields.original_image].dtype)
+    self.assertAllEqual([eval_batch_size],
+                        features[inputs.HASH_KEY].shape.as_list())
+    self.assertEqual(tf.int32, features[inputs.HASH_KEY].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100, 4],
+        labels[fields.InputDataFields.groundtruth_boxes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_boxes].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100, model_config.faster_rcnn.num_classes],
+        labels[fields.InputDataFields.groundtruth_classes].shape.as_list())
+    self.assertEqual(tf.float32,
+                     labels[fields.InputDataFields.groundtruth_classes].dtype)
+    self.assertAllEqual(
+        [eval_batch_size, 100],
+        labels[fields.InputDataFields.groundtruth_weights].shape.as_list())
+    self.assertEqual(
+        tf.float32,
+        labels[fields.InputDataFields.groundtruth_weights].dtype)
+
   def test_ssd_inceptionV2_train_input(self):
     """Tests the training input function for SSDInceptionV2."""
     configs = _get_configs_for_model('ssd_inception_v2_pets')
diff --git a/research/object_detection/legacy/eval.py b/research/object_detection/legacy/eval.py
index 1830803b..9a7d8c43 100644
--- a/research/object_detection/legacy/eval.py
+++ b/research/object_detection/legacy/eval.py
@@ -44,9 +44,8 @@ Example usage:
 """
 import functools
 import os
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-
+import tensorflow.compat.v1 as tf
+from tensorflow.python.util.deprecation import deprecated
 from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
@@ -81,7 +80,7 @@ flags.DEFINE_boolean(
 FLAGS = flags.FLAGS
 
 
-@contrib_framework.deprecated(None, 'Use object_detection/model_main.py.')
+@deprecated(None, 'Use object_detection/model_main.py.')
 def main(unused_argv):
   assert FLAGS.checkpoint_dir, '`checkpoint_dir` is missing.'
   assert FLAGS.eval_dir, '`eval_dir` is missing.'
diff --git a/research/object_detection/legacy/evaluator.py b/research/object_detection/legacy/evaluator.py
index ac7565da..feeb7188 100644
--- a/research/object_detection/legacy/evaluator.py
+++ b/research/object_detection/legacy/evaluator.py
@@ -19,7 +19,7 @@ DetectionModel.
 """
 
 import logging
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import eval_util
 from object_detection.core import prefetcher
diff --git a/research/object_detection/legacy/train.py b/research/object_detection/legacy/train.py
index 61bc457a..61577376 100644
--- a/research/object_detection/legacy/train.py
+++ b/research/object_detection/legacy/train.py
@@ -44,8 +44,9 @@ Example usage:
 import functools
 import json
 import os
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
+import tensorflow.compat.v1 as tf
+from tensorflow.python.util.deprecation import deprecated
+
 
 from object_detection.builders import dataset_builder
 from object_detection.builders import graph_rewriter_builder
@@ -85,7 +86,7 @@ flags.DEFINE_string('model_config_path', '',
 FLAGS = flags.FLAGS
 
 
-@contrib_framework.deprecated(None, 'Use object_detection/model_main.py.')
+@deprecated(None, 'Use object_detection/model_main.py.')
 def main(_):
   assert FLAGS.train_dir, '`train_dir` is missing.'
   if FLAGS.task == 0: tf.gfile.MakeDirs(FLAGS.train_dir)
diff --git a/research/object_detection/legacy/trainer.py b/research/object_detection/legacy/trainer.py
index ccef2618..21f8973d 100644
--- a/research/object_detection/legacy/trainer.py
+++ b/research/object_detection/legacy/trainer.py
@@ -21,8 +21,8 @@ DetectionModel.
 
 import functools
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.builders import optimizer_builder
 from object_detection.builders import preprocessor_builder
@@ -33,8 +33,6 @@ from object_detection.utils import ops as util_ops
 from object_detection.utils import variables_helper
 from deployment import model_deploy
 
-slim = contrib_slim
-
 
 def create_input_queue(batch_size_per_clone, create_tensor_dict_fn,
                        batch_queue_capacity, num_batch_queue_threads,
diff --git a/research/object_detection/legacy/trainer_test.py b/research/object_detection/legacy/trainer_test.py
index 2b00f170..3a5d0730 100644
--- a/research/object_detection/legacy/trainer_test.py
+++ b/research/object_detection/legacy/trainer_test.py
@@ -15,10 +15,9 @@
 
 """Tests for object_detection.trainer."""
 
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from google.protobuf import text_format
-from tensorflow.contrib import layers as contrib_layers
 
 from object_detection.core import losses
 from object_detection.core import model
@@ -90,10 +89,9 @@ class FakeDetectionModel(model.DetectionModel):
       prediction_dict: a dictionary holding prediction tensors to be
         passed to the Loss or Postprocess functions.
     """
-    flattened_inputs = contrib_layers.flatten(preprocessed_inputs)
-    class_prediction = contrib_layers.fully_connected(flattened_inputs,
-                                                      self._num_classes)
-    box_prediction = contrib_layers.fully_connected(flattened_inputs, 4)
+    flattened_inputs = slim.flatten(preprocessed_inputs)
+    class_prediction = slim.fully_connected(flattened_inputs, self._num_classes)
+    box_prediction = slim.fully_connected(flattened_inputs, 4)
 
     return {
         'class_predictions_with_background': tf.reshape(
diff --git a/research/object_detection/matchers/argmax_matcher.py b/research/object_detection/matchers/argmax_matcher.py
index 2ddbfc34..a347decb 100644
--- a/research/object_detection/matchers/argmax_matcher.py
+++ b/research/object_detection/matchers/argmax_matcher.py
@@ -26,7 +26,7 @@ This matcher is used in Fast(er)-RCNN.
 Note: matchers are used in TargetAssigners. There is a create_target_assigner
 factory function for popular implementations.
 """
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import matcher
 from object_detection.utils import shape_utils
diff --git a/research/object_detection/matchers/argmax_matcher_test.py b/research/object_detection/matchers/argmax_matcher_test.py
index 4b73efb8..9305f0a8 100644
--- a/research/object_detection/matchers/argmax_matcher_test.py
+++ b/research/object_detection/matchers/argmax_matcher_test.py
@@ -16,7 +16,7 @@
 """Tests for object_detection.matchers.argmax_matcher."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.matchers import argmax_matcher
 from object_detection.utils import test_case
diff --git a/research/object_detection/matchers/bipartite_matcher.py b/research/object_detection/matchers/bipartite_matcher.py
index ec27baf4..f62afe09 100644
--- a/research/object_detection/matchers/bipartite_matcher.py
+++ b/research/object_detection/matchers/bipartite_matcher.py
@@ -15,7 +15,7 @@
 
 """Bipartite matcher implementation."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.contrib.image.python.ops import image_ops
 from object_detection.core import matcher
diff --git a/research/object_detection/matchers/bipartite_matcher_test.py b/research/object_detection/matchers/bipartite_matcher_test.py
index 9dabaaa9..1617cbbc 100644
--- a/research/object_detection/matchers/bipartite_matcher_test.py
+++ b/research/object_detection/matchers/bipartite_matcher_test.py
@@ -16,7 +16,7 @@
 """Tests for object_detection.core.bipartite_matcher."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.matchers import bipartite_matcher
 from object_detection.utils import test_case
diff --git a/research/object_detection/meta_architectures/context_rcnn_meta_arch_tf1_test.py b/research/object_detection/meta_architectures/context_rcnn_meta_arch_tf1_test.py
new file mode 100644
index 00000000..47d7624d
--- /dev/null
+++ b/research/object_detection/meta_architectures/context_rcnn_meta_arch_tf1_test.py
@@ -0,0 +1,539 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for google3.third_party.tensorflow_models.object_detection.meta_architectures.context_meta_arch."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+from absl.testing import parameterized
+import mock
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
+from google.protobuf import text_format
+
+from object_detection.anchor_generators import grid_anchor_generator
+from object_detection.builders import box_predictor_builder
+from object_detection.builders import hyperparams_builder
+from object_detection.builders import post_processing_builder
+from object_detection.core import balanced_positive_negative_sampler as sampler
+from object_detection.core import losses
+from object_detection.core import post_processing
+from object_detection.core import standard_fields as fields
+from object_detection.core import target_assigner
+from object_detection.meta_architectures import context_rcnn_meta_arch
+from object_detection.meta_architectures import faster_rcnn_meta_arch
+from object_detection.protos import box_predictor_pb2
+from object_detection.protos import hyperparams_pb2
+from object_detection.protos import post_processing_pb2
+from object_detection.utils import ops
+from object_detection.utils import test_case
+from object_detection.utils import test_utils
+from object_detection.utils import tf_version
+
+
+class FakeFasterRCNNFeatureExtractor(
+    faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):
+  """Fake feature extractor to use in tests."""
+
+  def __init__(self):
+    super(FakeFasterRCNNFeatureExtractor, self).__init__(
+        is_training=False,
+        first_stage_features_stride=32,
+        reuse_weights=None,
+        weight_decay=0.0)
+
+  def preprocess(self, resized_inputs):
+    return tf.identity(resized_inputs)
+
+  def _extract_proposal_features(self, preprocessed_inputs, scope):
+    with tf.variable_scope('mock_model'):
+      proposal_features = 0 * slim.conv2d(
+          preprocessed_inputs, num_outputs=3, kernel_size=1, scope='layer1')
+      return proposal_features, {}
+
+  def _extract_box_classifier_features(self, proposal_feature_maps, scope):
+    with tf.variable_scope('mock_model'):
+      return 0 * slim.conv2d(
+          proposal_feature_maps, num_outputs=3, kernel_size=1, scope='layer2')
+
+
+class FakeFasterRCNNKerasFeatureExtractor(
+    faster_rcnn_meta_arch.FasterRCNNKerasFeatureExtractor):
+  """Fake feature extractor to use in tests."""
+
+  def __init__(self):
+    super(FakeFasterRCNNKerasFeatureExtractor, self).__init__(
+        is_training=False, first_stage_features_stride=32, weight_decay=0.0)
+
+  def preprocess(self, resized_inputs):
+    return tf.identity(resized_inputs)
+
+  def get_proposal_feature_extractor_model(self, name):
+
+    class ProposalFeatureExtractor(tf.keras.Model):
+      """Dummy proposal feature extraction."""
+
+      def __init__(self, name):
+        super(ProposalFeatureExtractor, self).__init__(name=name)
+        self.conv = None
+
+      def build(self, input_shape):
+        self.conv = tf.keras.layers.Conv2D(
+            3, kernel_size=1, padding='SAME', name='layer1')
+
+      def call(self, inputs):
+        return self.conv(inputs)
+
+    return ProposalFeatureExtractor(name=name)
+
+  def get_box_classifier_feature_extractor_model(self, name):
+    return tf.keras.Sequential([
+        tf.keras.layers.Conv2D(
+            3, kernel_size=1, padding='SAME', name=name + '_layer2')
+    ])
+
+
+class ContextRCNNMetaArchTest(test_case.TestCase, parameterized.TestCase):
+
+  def _get_model(self, box_predictor, **common_kwargs):
+    return context_rcnn_meta_arch.ContextRCNNMetaArch(
+        initial_crop_size=3,
+        maxpool_kernel_size=1,
+        maxpool_stride=1,
+        second_stage_mask_rcnn_box_predictor=box_predictor,
+        attention_bottleneck_dimension=10,
+        attention_temperature=0.2,
+        **common_kwargs)
+
+  def _build_arg_scope_with_hyperparams(self, hyperparams_text_proto,
+                                        is_training):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    return hyperparams_builder.build(hyperparams, is_training=is_training)
+
+  def _build_keras_layer_hyperparams(self, hyperparams_text_proto):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    return hyperparams_builder.KerasLayerHyperparams(hyperparams)
+
+  def _get_second_stage_box_predictor_text_proto(self,
+                                                 share_box_across_classes=False
+                                                ):
+    share_box_field = 'true' if share_box_across_classes else 'false'
+    box_predictor_text_proto = """
+      mask_rcnn_box_predictor {{
+        fc_hyperparams {{
+          op: FC
+          activation: NONE
+          regularizer {{
+            l2_regularizer {{
+              weight: 0.0005
+            }}
+          }}
+          initializer {{
+            variance_scaling_initializer {{
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }}
+          }}
+        }}
+        share_box_across_classes: {share_box_across_classes}
+      }}
+    """.format(share_box_across_classes=share_box_field)
+    return box_predictor_text_proto
+
+  def _get_box_classifier_features_shape(self,
+                                         image_size,
+                                         batch_size,
+                                         max_num_proposals,
+                                         initial_crop_size,
+                                         maxpool_stride,
+                                         num_features):
+    return (batch_size * max_num_proposals,
+            initial_crop_size/maxpool_stride,
+            initial_crop_size/maxpool_stride,
+            num_features)
+
+  def _get_second_stage_box_predictor(self,
+                                      num_classes,
+                                      is_training,
+                                      predict_masks,
+                                      masks_are_class_agnostic,
+                                      share_box_across_classes=False,
+                                      use_keras=False):
+    box_predictor_proto = box_predictor_pb2.BoxPredictor()
+    text_format.Merge(
+        self._get_second_stage_box_predictor_text_proto(
+            share_box_across_classes), box_predictor_proto)
+    if predict_masks:
+      text_format.Merge(
+          self._add_mask_to_second_stage_box_predictor_text_proto(
+              masks_are_class_agnostic), box_predictor_proto)
+
+    if use_keras:
+      return box_predictor_builder.build_keras(
+          hyperparams_builder.KerasLayerHyperparams,
+          inplace_batchnorm_update=False,
+          freeze_batchnorm=False,
+          box_predictor_config=box_predictor_proto,
+          num_classes=num_classes,
+          num_predictions_per_location_list=None,
+          is_training=is_training)
+    else:
+      return box_predictor_builder.build(
+          hyperparams_builder.build,
+          box_predictor_proto,
+          num_classes=num_classes,
+          is_training=is_training)
+
+  def _build_model(self,
+                   is_training,
+                   number_of_stages,
+                   second_stage_batch_size,
+                   first_stage_max_proposals=8,
+                   num_classes=2,
+                   hard_mining=False,
+                   softmax_second_stage_classification_loss=True,
+                   predict_masks=False,
+                   pad_to_max_dimension=None,
+                   masks_are_class_agnostic=False,
+                   use_matmul_crop_and_resize=False,
+                   clip_anchors_to_image=False,
+                   use_matmul_gather_in_matcher=False,
+                   use_static_shapes=False,
+                   calibration_mapping_value=None,
+                   share_box_across_classes=False,
+                   return_raw_detections_during_predict=False):
+    use_keras = tf_version.is_tf2()
+    def image_resizer_fn(image, masks=None):
+      """Fake image resizer function."""
+      resized_inputs = []
+      resized_image = tf.identity(image)
+      if pad_to_max_dimension is not None:
+        resized_image = tf.image.pad_to_bounding_box(image, 0, 0,
+                                                     pad_to_max_dimension,
+                                                     pad_to_max_dimension)
+      resized_inputs.append(resized_image)
+      if masks is not None:
+        resized_masks = tf.identity(masks)
+        if pad_to_max_dimension is not None:
+          resized_masks = tf.image.pad_to_bounding_box(
+              tf.transpose(masks, [1, 2, 0]), 0, 0, pad_to_max_dimension,
+              pad_to_max_dimension)
+          resized_masks = tf.transpose(resized_masks, [2, 0, 1])
+        resized_inputs.append(resized_masks)
+      resized_inputs.append(tf.shape(image))
+      return resized_inputs
+
+    # anchors in this test are designed so that a subset of anchors are inside
+    # the image and a subset of anchors are outside.
+    first_stage_anchor_scales = (0.001, 0.005, 0.1)
+    first_stage_anchor_aspect_ratios = (0.5, 1.0, 2.0)
+    first_stage_anchor_strides = (1, 1)
+    first_stage_anchor_generator = grid_anchor_generator.GridAnchorGenerator(
+        first_stage_anchor_scales,
+        first_stage_anchor_aspect_ratios,
+        anchor_stride=first_stage_anchor_strides)
+    first_stage_target_assigner = target_assigner.create_target_assigner(
+        'FasterRCNN',
+        'proposal',
+        use_matmul_gather=use_matmul_gather_in_matcher)
+
+    if use_keras:
+      fake_feature_extractor = FakeFasterRCNNKerasFeatureExtractor()
+    else:
+      fake_feature_extractor = FakeFasterRCNNFeatureExtractor()
+
+    first_stage_box_predictor_hyperparams_text_proto = """
+      op: CONV
+      activation: RELU
+      regularizer {
+        l2_regularizer {
+          weight: 0.00004
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.03
+        }
+      }
+    """
+    if use_keras:
+      first_stage_box_predictor_arg_scope_fn = (
+          self._build_keras_layer_hyperparams(
+              first_stage_box_predictor_hyperparams_text_proto))
+    else:
+      first_stage_box_predictor_arg_scope_fn = (
+          self._build_arg_scope_with_hyperparams(
+              first_stage_box_predictor_hyperparams_text_proto, is_training))
+
+    first_stage_box_predictor_kernel_size = 3
+    first_stage_atrous_rate = 1
+    first_stage_box_predictor_depth = 512
+    first_stage_minibatch_size = 3
+    first_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+        positive_fraction=0.5, is_static=use_static_shapes)
+
+    first_stage_nms_score_threshold = -1.0
+    first_stage_nms_iou_threshold = 1.0
+    first_stage_max_proposals = first_stage_max_proposals
+    first_stage_non_max_suppression_fn = functools.partial(
+        post_processing.batch_multiclass_non_max_suppression,
+        score_thresh=first_stage_nms_score_threshold,
+        iou_thresh=first_stage_nms_iou_threshold,
+        max_size_per_class=first_stage_max_proposals,
+        max_total_size=first_stage_max_proposals,
+        use_static_shapes=use_static_shapes)
+
+    first_stage_localization_loss_weight = 1.0
+    first_stage_objectness_loss_weight = 1.0
+
+    post_processing_config = post_processing_pb2.PostProcessing()
+    post_processing_text_proto = """
+      score_converter: IDENTITY
+      batch_non_max_suppression {
+        score_threshold: -20.0
+        iou_threshold: 1.0
+        max_detections_per_class: 5
+        max_total_detections: 5
+        use_static_shapes: """ + '{}'.format(use_static_shapes) + """
+      }
+    """
+    if calibration_mapping_value:
+      calibration_text_proto = """
+      calibration_config {
+        function_approximation {
+          x_y_pairs {
+            x_y_pair {
+              x: 0.0
+              y: %f
+            }
+            x_y_pair {
+              x: 1.0
+              y: %f
+              }}}}""" % (calibration_mapping_value, calibration_mapping_value)
+      post_processing_text_proto = (
+          post_processing_text_proto + ' ' + calibration_text_proto)
+    text_format.Merge(post_processing_text_proto, post_processing_config)
+    second_stage_non_max_suppression_fn, second_stage_score_conversion_fn = (
+        post_processing_builder.build(post_processing_config))
+
+    second_stage_target_assigner = target_assigner.create_target_assigner(
+        'FasterRCNN',
+        'detection',
+        use_matmul_gather=use_matmul_gather_in_matcher)
+    second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+        positive_fraction=1.0, is_static=use_static_shapes)
+
+    second_stage_localization_loss_weight = 1.0
+    second_stage_classification_loss_weight = 1.0
+    if softmax_second_stage_classification_loss:
+      second_stage_classification_loss = (
+          losses.WeightedSoftmaxClassificationLoss())
+    else:
+      second_stage_classification_loss = (
+          losses.WeightedSigmoidClassificationLoss())
+
+    hard_example_miner = None
+    if hard_mining:
+      hard_example_miner = losses.HardExampleMiner(
+          num_hard_examples=1,
+          iou_threshold=0.99,
+          loss_type='both',
+          cls_loss_weight=second_stage_classification_loss_weight,
+          loc_loss_weight=second_stage_localization_loss_weight,
+          max_negatives_per_positive=None)
+
+    crop_and_resize_fn = (
+        ops.matmul_crop_and_resize
+        if use_matmul_crop_and_resize else ops.native_crop_and_resize)
+    common_kwargs = {
+        'is_training':
+            is_training,
+        'num_classes':
+            num_classes,
+        'image_resizer_fn':
+            image_resizer_fn,
+        'feature_extractor':
+            fake_feature_extractor,
+        'number_of_stages':
+            number_of_stages,
+        'first_stage_anchor_generator':
+            first_stage_anchor_generator,
+        'first_stage_target_assigner':
+            first_stage_target_assigner,
+        'first_stage_atrous_rate':
+            first_stage_atrous_rate,
+        'first_stage_box_predictor_arg_scope_fn':
+            first_stage_box_predictor_arg_scope_fn,
+        'first_stage_box_predictor_kernel_size':
+            first_stage_box_predictor_kernel_size,
+        'first_stage_box_predictor_depth':
+            first_stage_box_predictor_depth,
+        'first_stage_minibatch_size':
+            first_stage_minibatch_size,
+        'first_stage_sampler':
+            first_stage_sampler,
+        'first_stage_non_max_suppression_fn':
+            first_stage_non_max_suppression_fn,
+        'first_stage_max_proposals':
+            first_stage_max_proposals,
+        'first_stage_localization_loss_weight':
+            first_stage_localization_loss_weight,
+        'first_stage_objectness_loss_weight':
+            first_stage_objectness_loss_weight,
+        'second_stage_target_assigner':
+            second_stage_target_assigner,
+        'second_stage_batch_size':
+            second_stage_batch_size,
+        'second_stage_sampler':
+            second_stage_sampler,
+        'second_stage_non_max_suppression_fn':
+            second_stage_non_max_suppression_fn,
+        'second_stage_score_conversion_fn':
+            second_stage_score_conversion_fn,
+        'second_stage_localization_loss_weight':
+            second_stage_localization_loss_weight,
+        'second_stage_classification_loss_weight':
+            second_stage_classification_loss_weight,
+        'second_stage_classification_loss':
+            second_stage_classification_loss,
+        'hard_example_miner':
+            hard_example_miner,
+        'crop_and_resize_fn':
+            crop_and_resize_fn,
+        'clip_anchors_to_image':
+            clip_anchors_to_image,
+        'use_static_shapes':
+            use_static_shapes,
+        'resize_masks':
+            True,
+        'return_raw_detections_during_predict':
+            return_raw_detections_during_predict
+    }
+
+    return self._get_model(
+        self._get_second_stage_box_predictor(
+            num_classes=num_classes,
+            is_training=is_training,
+            use_keras=use_keras,
+            predict_masks=predict_masks,
+            masks_are_class_agnostic=masks_are_class_agnostic,
+            share_box_across_classes=share_box_across_classes), **common_kwargs)
+
+  @mock.patch.object(context_rcnn_meta_arch, 'context_rcnn_lib')
+  def test_prediction_mock(self, mock_context_rcnn_lib):
+    """Mocks the context_rcnn_lib module to test the prediction.
+
+    Using mock object so that we can ensure compute_box_context_attention is
+    called in side the prediction function.
+
+    Args:
+      mock_context_rcnn_lib: mock module for the context_rcnn_lib.
+    """
+    model = self._build_model(
+        is_training=False,
+        number_of_stages=2,
+        second_stage_batch_size=6,
+        num_classes=42)
+    mock_tensor = tf.ones([2, 8, 3, 3, 3], tf.float32)
+
+    mock_context_rcnn_lib.compute_box_context_attention.return_value = mock_tensor
+    inputs_shape = (2, 20, 20, 3)
+    inputs = tf.cast(
+        tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32),
+        dtype=tf.float32)
+    preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+    context_features = tf.random_uniform((2, 20, 10),
+                                         minval=0,
+                                         maxval=255,
+                                         dtype=tf.float32)
+    valid_context_size = tf.random_uniform((2,),
+                                           minval=0,
+                                           maxval=10,
+                                           dtype=tf.int32)
+    features = {
+        fields.InputDataFields.context_features: context_features,
+        fields.InputDataFields.valid_context_size: valid_context_size
+    }
+
+    side_inputs = model.get_side_inputs(features)
+
+    _ = model.predict(preprocessed_inputs, true_image_shapes, **side_inputs)
+    mock_context_rcnn_lib.compute_box_context_attention.assert_called_once()
+
+  @parameterized.named_parameters(
+      {'testcase_name': 'static_shapes', 'static_shapes': True},
+      {'testcase_name': 'nostatic_shapes', 'static_shapes': False},
+      )
+  def test_prediction_end_to_end(self, static_shapes):
+    """Runs prediction end to end and test the shape of the results."""
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2,
+          second_stage_batch_size=6,
+          use_matmul_crop_and_resize=static_shapes,
+          clip_anchors_to_image=static_shapes,
+          use_matmul_gather_in_matcher=static_shapes,
+          use_static_shapes=static_shapes,
+          num_classes=42)
+
+    def graph_fn():
+      inputs_shape = (2, 20, 20, 3)
+      inputs = tf.cast(
+          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32),
+          dtype=tf.float32)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      context_features = tf.random_uniform((2, 20, 10),
+                                           minval=0,
+                                           maxval=255,
+                                           dtype=tf.float32)
+      valid_context_size = tf.random_uniform((2,),
+                                             minval=0,
+                                             maxval=10,
+                                             dtype=tf.int32)
+      features = {
+          fields.InputDataFields.context_features: context_features,
+          fields.InputDataFields.valid_context_size: valid_context_size
+      }
+
+      side_inputs = model.get_side_inputs(features)
+
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes,
+                                      **side_inputs)
+      return (prediction_dict['rpn_box_predictor_features'],
+              prediction_dict['rpn_box_encodings'],
+              prediction_dict['refined_box_encodings'],
+              prediction_dict['proposal_boxes_normalized'],
+              prediction_dict['proposal_boxes'])
+    execute_fn = self.execute if static_shapes else self.execute_cpu
+    (rpn_box_predictor_features, rpn_box_encodings, refined_box_encodings,
+     proposal_boxes_normalized, proposal_boxes) = execute_fn(graph_fn, [],
+                                                             graph=g)
+    self.assertAllEqual(rpn_box_predictor_features.shape, [2, 20, 20, 512])
+    self.assertAllEqual(rpn_box_encodings.shape, [2, 3600, 4])
+    self.assertAllEqual(refined_box_encodings.shape, [16, 42, 4])
+    self.assertAllEqual(proposal_boxes_normalized.shape, [2, 8, 4])
+    self.assertAllEqual(proposal_boxes.shape, [2, 8, 4])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index a4fb1580..2b6c093a 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -96,7 +96,8 @@ configured in the meta architecture:
 from __future__ import print_function
 import abc
 import functools
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.builders import box_predictor_builder
@@ -112,14 +113,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from object_detection.utils import variables_helper
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import framework as contrib_framework
-  from tensorflow.contrib import slim as contrib_slim
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
-# pylint: enable=g-import-not-at-top
 
 _UNINITIALIZED_FEATURE_EXTRACTOR = '__uninitialized__'
 
@@ -566,10 +559,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
       self._first_stage_box_predictor_arg_scope_fn = (
           first_stage_box_predictor_arg_scope_fn)
       def rpn_box_predictor_feature_extractor(rpn_features_to_crop):
-        with contrib_slim.arg_scope(
-            self._first_stage_box_predictor_arg_scope_fn()):
+        with slim.arg_scope(self._first_stage_box_predictor_arg_scope_fn()):
           reuse = tf.get_variable_scope().reuse
-          return contrib_slim.conv2d(
+          return slim.conv2d(
               rpn_features_to_crop,
               self._first_stage_box_predictor_depth,
               kernel_size=[
@@ -2805,7 +2797,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           self.first_stage_feature_extractor_scope,
           self.second_stage_feature_extractor_scope
       ]
-    feature_extractor_variables = contrib_framework.filter_variables(
+    feature_extractor_variables = slim.filter_variables(
         variables_to_restore, include_patterns=include_patterns)
     return {var.op.name: var for var in feature_extractor_variables}
 
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index c061b1a7..6c830b32 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -23,48 +23,63 @@ from __future__ import print_function
 from absl.testing import parameterized
 import numpy as np
 from six.moves import range
-from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch_test_lib
+from object_detection.utils import test_utils
 
 
 class FasterRCNNMetaArchTest(
     faster_rcnn_meta_arch_test_lib.FasterRCNNMetaArchTestBase,
     parameterized.TestCase):
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_postprocess_second_stage_only_inference_mode_with_masks(
-      self, use_keras=False):
-    model = self._build_model(
-        is_training=False, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6)
+  def test_postprocess_second_stage_only_inference_mode_with_masks(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2, second_stage_batch_size=6)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
-    proposal_boxes = tf.constant(
-        [[[1, 1, 2, 3],
-          [0, 0, 1, 1],
-          [.5, .5, .6, .6],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
-         [[2, 3, 6, 8],
-          [1, 2, 5, 3],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
-    num_proposals = tf.constant([3, 2], dtype=tf.int32)
-    refined_box_encodings = tf.zeros(
-        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
-    class_predictions_with_background = tf.ones(
-        [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
-
-    mask_height = 2
-    mask_width = 2
-    mask_predictions = 30. * tf.ones(
-        [total_num_padded_proposals, model.num_classes,
-         mask_height, mask_width], dtype=tf.float32)
+    def graph_fn():
+      proposal_boxes = tf.constant(
+          [[[1, 1, 2, 3],
+            [0, 0, 1, 1],
+            [.5, .5, .6, .6],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+           [[2, 3, 6, 8],
+            [1, 2, 5, 3],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
+      num_proposals = tf.constant([3, 2], dtype=tf.int32)
+      refined_box_encodings = tf.zeros(
+          [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
+      class_predictions_with_background = tf.ones(
+          [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+      mask_height = 2
+      mask_width = 2
+      mask_predictions = 30. * tf.ones(
+          [total_num_padded_proposals, model.num_classes,
+           mask_height, mask_width], dtype=tf.float32)
+
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      detections = model.postprocess({
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'num_proposals': num_proposals,
+          'proposal_boxes': proposal_boxes,
+          'image_shape': image_shape,
+          'mask_predictions': mask_predictions
+      }, true_image_shapes)
+      return (detections['detection_boxes'],
+              detections['detection_scores'],
+              detections['detection_classes'],
+              detections['num_detections'],
+              detections['detection_masks'])
+    (detection_boxes, detection_scores, detection_classes,
+     num_detections, detection_masks) = self.execute_cpu(graph_fn, [], graph=g)
     exp_detection_masks = np.array([[[[1, 1], [1, 1]],
                                      [[1, 1], [1, 1]],
                                      [[1, 1], [1, 1]],
@@ -75,62 +90,63 @@ class FasterRCNNMetaArchTest(
                                      [[1, 1], [1, 1]],
                                      [[1, 1], [1, 1]],
                                      [[0, 0], [0, 0]]]])
-
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    detections = model.postprocess({
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'num_proposals': num_proposals,
-        'proposal_boxes': proposal_boxes,
-        'image_shape': image_shape,
-        'mask_predictions': mask_predictions
-    }, true_image_shapes)
-    with self.test_session() as sess:
-      detections_out = sess.run(detections)
-      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
-      self.assertAllClose(detections_out['detection_scores'],
-                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
-      self.assertAllClose(detections_out['detection_classes'],
-                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
-      self.assertAllClose(detections_out['num_detections'], [5, 4])
-      self.assertAllClose(detections_out['detection_masks'],
-                          exp_detection_masks)
-      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
-      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
-
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_postprocess_second_stage_only_inference_mode_with_calibration(
-      self, use_keras=False):
-    model = self._build_model(
-        is_training=False, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6,
-        calibration_mapping_value=0.5)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    self.assertAllClose(detection_scores,
+                        [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
+    self.assertAllClose(detection_classes,
+                        [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+    self.assertAllClose(num_detections, [5, 4])
+    self.assertAllClose(detection_masks, exp_detection_masks)
+    self.assertTrue(np.amax(detection_masks <= 1.0))
+    self.assertTrue(np.amin(detection_masks >= 0.0))
+
+  def test_postprocess_second_stage_only_inference_mode_with_calibration(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2, second_stage_batch_size=6,
+          calibration_mapping_value=0.5)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
-    proposal_boxes = tf.constant(
-        [[[1, 1, 2, 3],
-          [0, 0, 1, 1],
-          [.5, .5, .6, .6],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
-         [[2, 3, 6, 8],
-          [1, 2, 5, 3],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
-    num_proposals = tf.constant([3, 2], dtype=tf.int32)
-    refined_box_encodings = tf.zeros(
-        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
-    class_predictions_with_background = tf.ones(
-        [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
-
-    mask_height = 2
-    mask_width = 2
-    mask_predictions = 30. * tf.ones(
-        [total_num_padded_proposals, model.num_classes,
-         mask_height, mask_width], dtype=tf.float32)
+    def graph_fn():
+      proposal_boxes = tf.constant(
+          [[[1, 1, 2, 3],
+            [0, 0, 1, 1],
+            [.5, .5, .6, .6],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+           [[2, 3, 6, 8],
+            [1, 2, 5, 3],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
+      num_proposals = tf.constant([3, 2], dtype=tf.int32)
+      refined_box_encodings = tf.zeros(
+          [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
+      class_predictions_with_background = tf.ones(
+          [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+      mask_height = 2
+      mask_width = 2
+      mask_predictions = 30. * tf.ones(
+          [total_num_padded_proposals, model.num_classes,
+           mask_height, mask_width], dtype=tf.float32)
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      detections = model.postprocess({
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'num_proposals': num_proposals,
+          'proposal_boxes': proposal_boxes,
+          'image_shape': image_shape,
+          'mask_predictions': mask_predictions
+      }, true_image_shapes)
+      return (detections['detection_boxes'],
+              detections['detection_scores'],
+              detections['detection_classes'],
+              detections['num_detections'],
+              detections['detection_masks'])
+    (detection_boxes, detection_scores, detection_classes,
+     num_detections, detection_masks) = self.execute_cpu(graph_fn, [], graph=g)
     exp_detection_masks = np.array([[[[1, 1], [1, 1]],
                                      [[1, 1], [1, 1]],
                                      [[1, 1], [1, 1]],
@@ -142,281 +158,176 @@ class FasterRCNNMetaArchTest(
                                      [[1, 1], [1, 1]],
                                      [[0, 0], [0, 0]]]])
 
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    detections = model.postprocess({
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'num_proposals': num_proposals,
-        'proposal_boxes': proposal_boxes,
-        'image_shape': image_shape,
-        'mask_predictions': mask_predictions
-    }, true_image_shapes)
-    with self.test_session() as sess:
-      detections_out = sess.run(detections)
-      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
-      # All scores map to 0.5, except for the final one, which is pruned.
-      self.assertAllClose(detections_out['detection_scores'],
-                          [[0.5, 0.5, 0.5, 0.5, 0.5],
-                           [0.5, 0.5, 0.5, 0.5, 0.0]])
-      self.assertAllClose(detections_out['detection_classes'],
-                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
-      self.assertAllClose(detections_out['num_detections'], [5, 4])
-      self.assertAllClose(detections_out['detection_masks'],
-                          exp_detection_masks)
-      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
-      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
-
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_postprocess_second_stage_only_inference_mode_with_shared_boxes(
-      self, use_keras=False):
-    model = self._build_model(
-        is_training=False, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    # All scores map to 0.5, except for the final one, which is pruned.
+    self.assertAllClose(detection_scores,
+                        [[0.5, 0.5, 0.5, 0.5, 0.5],
+                         [0.5, 0.5, 0.5, 0.5, 0.0]])
+    self.assertAllClose(detection_classes,
+                        [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+    self.assertAllClose(num_detections, [5, 4])
+    self.assertAllClose(detection_masks,
+                        exp_detection_masks)
+    self.assertTrue(np.amax(detection_masks <= 1.0))
+    self.assertTrue(np.amin(detection_masks >= 0.0))
+
+  def test_postprocess_second_stage_only_inference_mode_with_shared_boxes(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2, second_stage_batch_size=6)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
-    proposal_boxes = tf.constant(
-        [[[1, 1, 2, 3],
-          [0, 0, 1, 1],
-          [.5, .5, .6, .6],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
-         [[2, 3, 6, 8],
-          [1, 2, 5, 3],
-          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
-    num_proposals = tf.constant([3, 2], dtype=tf.int32)
-
-    # This has 1 box instead of one for each class.
-    refined_box_encodings = tf.zeros(
-        [total_num_padded_proposals, 1, 4], dtype=tf.float32)
-    class_predictions_with_background = tf.ones(
-        [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
-
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    detections = model.postprocess({
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'num_proposals': num_proposals,
-        'proposal_boxes': proposal_boxes,
-        'image_shape': image_shape,
-    }, true_image_shapes)
-    with self.test_session() as sess:
-      detections_out = sess.run(detections)
-      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
-      self.assertAllClose(detections_out['detection_scores'],
-                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
-      self.assertAllClose(detections_out['detection_classes'],
-                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
-      self.assertAllClose(detections_out['num_detections'], [5, 4])
+    def graph_fn():
+      proposal_boxes = tf.constant(
+          [[[1, 1, 2, 3],
+            [0, 0, 1, 1],
+            [.5, .5, .6, .6],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+           [[2, 3, 6, 8],
+            [1, 2, 5, 3],
+            4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]], dtype=tf.float32)
+      num_proposals = tf.constant([3, 2], dtype=tf.int32)
+
+      # This has 1 box instead of one for each class.
+      refined_box_encodings = tf.zeros(
+          [total_num_padded_proposals, 1, 4], dtype=tf.float32)
+      class_predictions_with_background = tf.ones(
+          [total_num_padded_proposals, model.num_classes+1], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      detections = model.postprocess({
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'num_proposals': num_proposals,
+          'proposal_boxes': proposal_boxes,
+          'image_shape': image_shape,
+      }, true_image_shapes)
+      return (detections['detection_boxes'],
+              detections['detection_scores'],
+              detections['detection_classes'],
+              detections['num_detections'])
+    (detection_boxes, detection_scores, detection_classes,
+     num_detections) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    self.assertAllClose(detection_scores,
+                        [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
+    self.assertAllClose(detection_classes,
+                        [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+    self.assertAllClose(num_detections, [5, 4])
 
   @parameterized.parameters(
-      {'masks_are_class_agnostic': False, 'use_keras': True},
-      {'masks_are_class_agnostic': True, 'use_keras': True},
-      {'masks_are_class_agnostic': False, 'use_keras': False},
-      {'masks_are_class_agnostic': True, 'use_keras': False},
+      {'masks_are_class_agnostic': False},
+      {'masks_are_class_agnostic': True},
   )
   def test_predict_correct_shapes_in_inference_mode_three_stages_with_masks(
-      self, masks_are_class_agnostic, use_keras):
+      self, masks_are_class_agnostic):
     batch_size = 2
     image_size = 10
-    max_num_proposals = 8
-    initial_crop_size = 3
-    maxpool_stride = 1
-
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-    expected_num_anchors = image_size * image_size * 3 * 3
-    expected_shapes = {
-        'rpn_box_predictor_features':
-        (2, image_size, image_size, 512),
-        'rpn_features_to_crop': (2, image_size, image_size, 3),
-        'image_shape': (4,),
-        'rpn_box_encodings': (2, expected_num_anchors, 4),
-        'rpn_objectness_predictions_with_background':
-        (2, expected_num_anchors, 2),
-        'anchors': (expected_num_anchors, 4),
-        'refined_box_encodings': (2 * max_num_proposals, 2, 4),
-        'class_predictions_with_background': (2 * max_num_proposals, 2 + 1),
-        'num_proposals': (2,),
-        'proposal_boxes': (2, max_num_proposals, 4),
-        'proposal_boxes_normalized': (2, max_num_proposals, 4),
-        'box_classifier_features':
-        self._get_box_classifier_features_shape(image_size,
-                                                batch_size,
-                                                max_num_proposals,
-                                                initial_crop_size,
-                                                maxpool_stride,
-                                                3),
-        'feature_maps': [(2, image_size, image_size, 512)]
-    }
-
-    for input_shape in input_shapes:
-      test_graph = tf.Graph()
-      with test_graph.as_default():
-        model = self._build_model(
-            is_training=False,
-            use_keras=use_keras,
-            number_of_stages=3,
-            second_stage_batch_size=2,
-            predict_masks=True,
-            masks_are_class_agnostic=masks_are_class_agnostic)
-        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
-        _, true_image_shapes = model.preprocess(preprocessed_inputs)
-        result_tensor_dict = model.predict(preprocessed_inputs,
-                                           true_image_shapes)
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=test_graph) as sess:
-        sess.run(init_op)
-        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
-            preprocessed_inputs:
-            np.zeros((batch_size, image_size, image_size, 3))})
-      self.assertEqual(
-          set(tensor_dict_out.keys()),
-          set(expected_shapes.keys()).union(
-              set([
-                  'detection_boxes', 'detection_scores',
-                  'detection_multiclass_scores', 'detection_classes',
-                  'detection_masks', 'num_detections', 'mask_predictions',
-                  'raw_detection_boxes', 'raw_detection_scores',
-                  'detection_anchor_indices', 'final_anchors',
-              ])))
-      for key in expected_shapes:
-        if isinstance(tensor_dict_out[key], list):
-          continue
-        self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
-      self.assertAllEqual(tensor_dict_out['detection_boxes'].shape, [2, 5, 4])
-      self.assertAllEqual(tensor_dict_out['detection_masks'].shape,
-                          [2, 5, 14, 14])
-      self.assertAllEqual(tensor_dict_out['detection_classes'].shape, [2, 5])
-      self.assertAllEqual(tensor_dict_out['detection_scores'].shape, [2, 5])
-      self.assertAllEqual(tensor_dict_out['num_detections'].shape, [2])
-      num_classes = 1 if masks_are_class_agnostic else 2
-      self.assertAllEqual(tensor_dict_out['mask_predictions'].shape,
-                          [10, num_classes, 14, 14])
-
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False},
-  )
-  def test_raw_detection_boxes_and_anchor_indices_correct(self, use_keras):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=3,
+          second_stage_batch_size=2,
+          predict_masks=True,
+          masks_are_class_agnostic=masks_are_class_agnostic)
+    def graph_fn():
+      shape = [tf.random_uniform([], minval=batch_size, maxval=batch_size + 1,
+                                 dtype=tf.int32),
+               tf.random_uniform([], minval=image_size, maxval=image_size + 1,
+                                 dtype=tf.int32),
+               tf.random_uniform([], minval=image_size, maxval=image_size + 1,
+                                 dtype=tf.int32),
+               3]
+      image = tf.zeros(shape)
+      _, true_image_shapes = model.preprocess(image)
+      detections = model.predict(image, true_image_shapes)
+      return (detections['detection_boxes'], detections['detection_classes'],
+              detections['detection_scores'], detections['num_detections'],
+              detections['detection_masks'], detections['mask_predictions'])
+    (detection_boxes, detection_scores, detection_classes,
+     num_detections, detection_masks,
+     mask_predictions) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    self.assertAllEqual(detection_masks.shape,
+                        [2, 5, 14, 14])
+    self.assertAllEqual(detection_classes.shape, [2, 5])
+    self.assertAllEqual(detection_scores.shape, [2, 5])
+    self.assertAllEqual(num_detections.shape, [2])
+    num_classes = 1 if masks_are_class_agnostic else 2
+    self.assertAllEqual(mask_predictions.shape,
+                        [10, num_classes, 14, 14])
+
+  def test_raw_detection_boxes_and_anchor_indices_correct(self):
     batch_size = 2
     image_size = 10
-    max_num_proposals = 8
-    initial_crop_size = 3
-    maxpool_stride = 1
 
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-    expected_num_anchors = image_size * image_size * 3 * 3
-    expected_shapes = {
-        'rpn_box_predictor_features':
-        (batch_size, image_size, image_size, 512),
-        'rpn_features_to_crop': (batch_size, image_size, image_size, 3),
-        'image_shape': (4,),
-        'rpn_box_encodings': (batch_size, expected_num_anchors, 4),
-        'rpn_objectness_predictions_with_background':
-        (batch_size, expected_num_anchors, 2),
-        'anchors': (expected_num_anchors, 4),
-        'refined_box_encodings': (batch_size * max_num_proposals, 1, 4),
-        'class_predictions_with_background':
-            (batch_size * max_num_proposals, 2 + 1),
-        'num_proposals': (batch_size,),
-        'proposal_boxes': (batch_size, max_num_proposals, 4),
-        'proposal_boxes_normalized': (batch_size, max_num_proposals, 4),
-        'box_classifier_features':
-        self._get_box_classifier_features_shape(image_size,
-                                                batch_size,
-                                                max_num_proposals,
-                                                initial_crop_size,
-                                                maxpool_stride,
-                                                3),
-        'feature_maps': [(batch_size, image_size, image_size, 3)],
-        'raw_detection_feature_map_indices': (batch_size, max_num_proposals, 1),
-        'raw_detection_boxes': (batch_size, max_num_proposals, 1, 4),
-        'final_anchors': (batch_size, max_num_proposals, 4)
-    }
-
-    for input_shape in input_shapes:
-      test_graph = tf.Graph()
-      with test_graph.as_default():
-        model = self._build_model(
-            is_training=False,
-            use_keras=use_keras,
-            number_of_stages=2,
-            second_stage_batch_size=2,
-            share_box_across_classes=True,
-            return_raw_detections_during_predict=True)
-        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
-        _, true_image_shapes = model.preprocess(preprocessed_inputs)
-        predict_tensor_dict = model.predict(preprocessed_inputs,
-                                            true_image_shapes)
-        postprocess_tensor_dict = model.postprocess(predict_tensor_dict,
-                                                    true_image_shapes)
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=test_graph) as sess:
-        sess.run(init_op)
-        [predict_dict_out, postprocess_dict_out] = sess.run(
-            [predict_tensor_dict, postprocess_tensor_dict], feed_dict={
-                preprocessed_inputs:
-                    np.zeros((batch_size, image_size, image_size, 3))})
-      self.assertEqual(
-          set(predict_dict_out.keys()),
-          set(expected_shapes.keys()))
-      for key in expected_shapes:
-        if isinstance(predict_dict_out[key], list):
-          continue
-        self.assertAllEqual(predict_dict_out[key].shape, expected_shapes[key])
-      # Verify that the raw detections from predict and postprocess are the
-      # same.
-      self.assertAllClose(
-          np.squeeze(predict_dict_out['raw_detection_boxes']),
-          postprocess_dict_out['raw_detection_boxes'])
-      # Verify that the raw detection boxes at detection anchor indices are the
-      # same as the postprocessed detections.
-      for i in range(batch_size):
-        num_detections_per_image = int(
-            postprocess_dict_out['num_detections'][i])
-        detection_boxes_per_image = postprocess_dict_out[
-            'detection_boxes'][i][:num_detections_per_image]
-        detection_anchor_indices_per_image = postprocess_dict_out[
-            'detection_anchor_indices'][i][:num_detections_per_image]
-        raw_detections_per_image = np.squeeze(predict_dict_out[
-            'raw_detection_boxes'][i])
-        raw_detections_at_anchor_indices = raw_detections_per_image[
-            detection_anchor_indices_per_image]
-        self.assertAllClose(detection_boxes_per_image,
-                            raw_detections_at_anchor_indices)
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2,
+          second_stage_batch_size=2,
+          share_box_across_classes=True,
+          return_raw_detections_during_predict=True)
+    def graph_fn():
+      shape = [tf.random_uniform([], minval=batch_size, maxval=batch_size + 1,
+                                 dtype=tf.int32),
+               tf.random_uniform([], minval=image_size, maxval=image_size + 1,
+                                 dtype=tf.int32),
+               tf.random_uniform([], minval=image_size, maxval=image_size + 1,
+                                 dtype=tf.int32),
+               3]
+      image = tf.zeros(shape)
+      _, true_image_shapes = model.preprocess(image)
+      predict_tensor_dict = model.predict(image, true_image_shapes)
+      detections = model.postprocess(predict_tensor_dict, true_image_shapes)
+      return (detections['detection_boxes'],
+              detections['num_detections'],
+              detections['detection_anchor_indices'],
+              detections['raw_detection_boxes'],
+              predict_tensor_dict['raw_detection_boxes'])
+    (detection_boxes, num_detections, detection_anchor_indices,
+     raw_detection_boxes,
+     predict_raw_detection_boxes) = self.execute_cpu(graph_fn, [], graph=g)
+
+    # Verify that the raw detections from predict and postprocess are the
+    # same.
+    self.assertAllClose(
+        np.squeeze(predict_raw_detection_boxes), raw_detection_boxes)
+    # Verify that the raw detection boxes at detection anchor indices are the
+    # same as the postprocessed detections.
+    for i in range(batch_size):
+      num_detections_per_image = int(num_detections[i])
+      detection_boxes_per_image = detection_boxes[i][
+          :num_detections_per_image]
+      detection_anchor_indices_per_image = detection_anchor_indices[i][
+          :num_detections_per_image]
+      raw_detections_per_image = np.squeeze(raw_detection_boxes[i])
+      raw_detections_at_anchor_indices = raw_detections_per_image[
+          detection_anchor_indices_per_image]
+      self.assertAllClose(detection_boxes_per_image,
+                          raw_detections_at_anchor_indices)
 
   @parameterized.parameters(
-      {'masks_are_class_agnostic': False, 'use_keras': True},
-      {'masks_are_class_agnostic': True, 'use_keras': True},
-      {'masks_are_class_agnostic': False, 'use_keras': False},
-      {'masks_are_class_agnostic': True, 'use_keras': False},
+      {'masks_are_class_agnostic': False},
+      {'masks_are_class_agnostic': True},
   )
   def test_predict_gives_correct_shapes_in_train_mode_both_stages_with_masks(
-      self, masks_are_class_agnostic, use_keras):
-    test_graph = tf.Graph()
-    with test_graph.as_default():
+      self, masks_are_class_agnostic):
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
           is_training=True,
-          use_keras=use_keras,
           number_of_stages=3,
           second_stage_batch_size=7,
           predict_masks=True,
           masks_are_class_agnostic=masks_are_class_agnostic)
-      batch_size = 2
-      image_size = 10
-      max_num_proposals = 7
-      initial_crop_size = 3
-      maxpool_stride = 1
-
+    batch_size = 2
+    image_size = 10
+    max_num_proposals = 7
+    def graph_fn():
       image_shape = (batch_size, image_size, image_size, 3)
       preprocessed_inputs = tf.zeros(image_shape, dtype=tf.float32)
       groundtruth_boxes_list = [
@@ -437,141 +348,80 @@ class FasterRCNNMetaArchTest(
           groundtruth_weights_list=groundtruth_weights_list)
 
       result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
-      mask_shape_1 = 1 if masks_are_class_agnostic else model._num_classes
-      expected_shapes = {
-          'rpn_box_predictor_features': (2, image_size, image_size, 512),
-          'rpn_features_to_crop': (2, image_size, image_size, 3),
-          'image_shape': (4,),
-          'refined_box_encodings': (2 * max_num_proposals, 2, 4),
-          'class_predictions_with_background': (2 * max_num_proposals, 2 + 1),
-          'num_proposals': (2,),
-          'proposal_boxes': (2, max_num_proposals, 4),
-          'proposal_boxes_normalized': (2, max_num_proposals, 4),
-          'box_classifier_features':
-              self._get_box_classifier_features_shape(
-                  image_size, batch_size, max_num_proposals, initial_crop_size,
-                  maxpool_stride, 3),
-          'mask_predictions': (2 * max_num_proposals, mask_shape_1, 14, 14),
-          'feature_maps': [(2, image_size, image_size, 512)]
-      }
-
-      init_op = tf.global_variables_initializer()
-      with self.test_session(graph=test_graph) as sess:
-        sess.run(init_op)
-        tensor_dict_out = sess.run(result_tensor_dict)
-        self.assertEqual(
-            set(tensor_dict_out.keys()),
-            set(expected_shapes.keys()).union(
-                set([
-                    'rpn_box_encodings',
-                    'rpn_objectness_predictions_with_background',
-                    'anchors',
-                    'final_anchors',
-                ])))
-        for key in expected_shapes:
-          if isinstance(tensor_dict_out[key], list):
-            continue
-          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
-
-        anchors_shape_out = tensor_dict_out['anchors'].shape
-        self.assertLen(anchors_shape_out, 2)
-        self.assertEqual(4, anchors_shape_out[1])
-        num_anchors_out = anchors_shape_out[0]
-        self.assertAllEqual(tensor_dict_out['rpn_box_encodings'].shape,
-                            (2, num_anchors_out, 4))
-        self.assertAllEqual(
-            tensor_dict_out['rpn_objectness_predictions_with_background'].shape,
-            (2, num_anchors_out, 2))
+      return result_tensor_dict['mask_predictions']
+    mask_shape_1 = 1 if masks_are_class_agnostic else model._num_classes
+    mask_out = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllEqual(mask_out.shape,
+                        (2 * max_num_proposals, mask_shape_1, 14, 14))
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_postprocess_third_stage_only_inference_mode(self, use_keras=False):
-    num_proposals_shapes = [(2), (None)]
-    refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]
-    class_predictions_with_background_shapes = [(16, 3), (None, 3)]
-    proposal_boxes_shapes = [(2, 8, 4), (None, 8, 4)]
+  def test_postprocess_third_stage_only_inference_mode(self):
     batch_size = 2
     initial_crop_size = 3
     maxpool_stride = 1
     height = initial_crop_size // maxpool_stride
     width = initial_crop_size // maxpool_stride
     depth = 3
-    image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
-    for (num_proposals_shape, refined_box_encoding_shape,
-         class_predictions_with_background_shape,
-         proposal_boxes_shape) in zip(num_proposals_shapes,
-                                      refined_box_encodings_shapes,
-                                      class_predictions_with_background_shapes,
-                                      proposal_boxes_shapes):
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model = self._build_model(
-            is_training=False, use_keras=use_keras, number_of_stages=3,
-            second_stage_batch_size=6, predict_masks=True)
-        total_num_padded_proposals = batch_size * model.max_num_proposals
-        proposal_boxes = np.array(
-            [[[1, 1, 2, 3],
-              [0, 0, 1, 1],
-              [.5, .5, .6, .6],
-              4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
-             [[2, 3, 6, 8],
-              [1, 2, 5, 3],
-              4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]])
-        num_proposals = np.array([3, 2], dtype=np.int32)
-        refined_box_encodings = np.zeros(
-            [total_num_padded_proposals, model.num_classes, 4])
-        class_predictions_with_background = np.ones(
-            [total_num_padded_proposals, model.num_classes+1])
-
-        num_proposals_placeholder = tf.placeholder(tf.int32,
-                                                   shape=num_proposals_shape)
-        refined_box_encodings_placeholder = tf.placeholder(
-            tf.float32, shape=refined_box_encoding_shape)
-        class_predictions_with_background_placeholder = tf.placeholder(
-            tf.float32, shape=class_predictions_with_background_shape)
-        proposal_boxes_placeholder = tf.placeholder(
-            tf.float32, shape=proposal_boxes_shape)
-        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))
-        _, true_image_shapes = model.preprocess(
-            tf.zeros(image_shape_placeholder))
-        detections = model.postprocess({
-            'refined_box_encodings': refined_box_encodings_placeholder,
-            'class_predictions_with_background':
-            class_predictions_with_background_placeholder,
-            'num_proposals': num_proposals_placeholder,
-            'proposal_boxes': proposal_boxes_placeholder,
-            'image_shape': image_shape_placeholder,
-            'detection_boxes': tf.zeros([2, 5, 4]),
-            'detection_masks': tf.zeros([2, 5, 14, 14]),
-            'detection_scores': tf.zeros([2, 5]),
-            'detection_classes': tf.zeros([2, 5]),
-            'num_detections': tf.zeros([2]),
-            'detection_features': tf.zeros([2, 5, width, height, depth])
-        }, true_image_shapes)
-      with self.test_session(graph=tf_graph) as sess:
-        detections_out = sess.run(
-            detections,
-            feed_dict={
-                refined_box_encodings_placeholder: refined_box_encodings,
-                class_predictions_with_background_placeholder:
-                class_predictions_with_background,
-                num_proposals_placeholder: num_proposals,
-                proposal_boxes_placeholder: proposal_boxes,
-                image_shape_placeholder: image_shape
-            })
-      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
-      self.assertAllEqual(detections_out['detection_masks'].shape,
-                          [2, 5, 14, 14])
-      self.assertAllClose(detections_out['detection_scores'].shape, [2, 5])
-      self.assertAllClose(detections_out['detection_classes'].shape, [2, 5])
-      self.assertAllClose(detections_out['num_detections'].shape, [2])
-      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
-      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
-      self.assertAllEqual(detections_out['detection_features'].shape,
-                          [2, 5, width, height, depth])
-      self.assertGreaterEqual(np.amax(detections_out['detection_features']), 0)
+
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False, number_of_stages=3,
+          second_stage_batch_size=6, predict_masks=True)
+    total_num_padded_proposals = batch_size * model.max_num_proposals
+    def graph_fn(images_shape, num_proposals, proposal_boxes,
+                 refined_box_encodings, class_predictions_with_background):
+      _, true_image_shapes = model.preprocess(
+          tf.zeros(images_shape))
+      detections = model.postprocess({
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+          class_predictions_with_background,
+          'num_proposals': num_proposals,
+          'proposal_boxes': proposal_boxes,
+          'image_shape': images_shape,
+          'detection_boxes': tf.zeros([2, 5, 4]),
+          'detection_masks': tf.zeros([2, 5, 14, 14]),
+          'detection_scores': tf.zeros([2, 5]),
+          'detection_classes': tf.zeros([2, 5]),
+          'num_detections': tf.zeros([2]),
+          'detection_features': tf.zeros([2, 5, width, height, depth])
+      }, true_image_shapes)
+      return (detections['detection_boxes'], detections['detection_masks'],
+              detections['detection_scores'], detections['detection_classes'],
+              detections['num_detections'],
+              detections['detection_features'])
+    images_shape = np.array((2, 36, 48, 3), dtype=np.int32)
+    proposal_boxes = np.array(
+        [[[1, 1, 2, 3],
+          [0, 0, 1, 1],
+          [.5, .5, .6, .6],
+          4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+         [[2, 3, 6, 8],
+          [1, 2, 5, 3],
+          4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]])
+    num_proposals = np.array([3, 2], dtype=np.int32)
+    refined_box_encodings = np.zeros(
+        [total_num_padded_proposals, model.num_classes, 4])
+    class_predictions_with_background = np.ones(
+        [total_num_padded_proposals, model.num_classes+1])
+
+    (detection_boxes, detection_masks, detection_scores, detection_classes,
+     num_detections,
+     detection_features) = self.execute_cpu(graph_fn,
+                                            [images_shape, num_proposals,
+                                             proposal_boxes,
+                                             refined_box_encodings,
+                                             class_predictions_with_background],
+                                            graph=g)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    self.assertAllEqual(detection_masks.shape, [2, 5, 14, 14])
+    self.assertAllClose(detection_scores.shape, [2, 5])
+    self.assertAllClose(detection_classes.shape, [2, 5])
+    self.assertAllClose(num_detections.shape, [2])
+    self.assertTrue(np.amax(detection_masks <= 1.0))
+    self.assertTrue(np.amin(detection_masks >= 0.0))
+    self.assertAllEqual(detection_features.shape,
+                        [2, 5, width, height, depth])
+    self.assertGreaterEqual(np.amax(detection_features), 0)
 
   def _get_box_classifier_features_shape(self,
                                          image_size,
@@ -585,46 +435,40 @@ class FasterRCNNMetaArchTest(
             initial_crop_size // maxpool_stride,
             num_features)
 
-  @parameterized.parameters({'use_keras': True}, {'use_keras': False})
-  def test_output_final_box_features(self, use_keras):
-    model = self._build_model(
-        is_training=False,
-        use_keras=use_keras,
-        number_of_stages=2,
-        second_stage_batch_size=6,
-        output_final_box_features=True)
+  def test_output_final_box_features(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2,
+          second_stage_batch_size=6,
+          output_final_box_features=True)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
-    proposal_boxes = tf.constant([[[1, 1, 2, 3], [0, 0, 1, 1], [.5, .5, .6, .6],
-                                   4 * [0], 4 * [0], 4 * [0], 4 * [0], 4 * [0]],
-                                  [[2, 3, 6, 8], [1, 2, 5, 3], 4 * [0], 4 * [0],
-                                   4 * [0], 4 * [0], 4 * [0], 4 * [0]]],
-                                 dtype=tf.float32)
-    num_proposals = tf.constant([3, 2], dtype=tf.int32)
-    refined_box_encodings = tf.zeros(
-        [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
-    class_predictions_with_background = tf.ones(
-        [total_num_padded_proposals, model.num_classes + 1], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
-
-    mask_height = 2
-    mask_width = 2
-    mask_predictions = 30. * tf.ones([
-        total_num_padded_proposals, model.num_classes, mask_height, mask_width
-    ],
-                                     dtype=tf.float32)
-    exp_detection_masks = np.array([[[[1, 1], [1, 1]], [[1, 1], [1, 1]],
-                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
-                                     [[1, 1], [1, 1]]],
-                                    [[[1, 1], [1, 1]], [[1, 1], [1, 1]],
-                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
-                                     [[0, 0], [0, 0]]]])
-
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-
-    # It should fail due to no rpn_features_to_crop in the input dict.
-    with self.assertRaises(ValueError):
+    def graph_fn():
+      proposal_boxes = tf.constant([[[1, 1, 2, 3], [0, 0, 1, 1],
+                                     [.5, .5, .6, .6], 4 * [0], 4 * [0],
+                                     4 * [0], 4 * [0], 4 * [0]],
+                                    [[2, 3, 6, 8], [1, 2, 5, 3], 4 * [0],
+                                     4 * [0], 4 * [0], 4 * [0], 4 * [0],
+                                     4 * [0]]],
+                                   dtype=tf.float32)
+      num_proposals = tf.constant([3, 2], dtype=tf.int32)
+      refined_box_encodings = tf.zeros(
+          [total_num_padded_proposals, model.num_classes, 4], dtype=tf.float32)
+      class_predictions_with_background = tf.ones(
+          [total_num_padded_proposals, model.num_classes + 1], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 36, 48, 3], dtype=tf.int32)
+
+      mask_height = 2
+      mask_width = 2
+      mask_predictions = 30. * tf.ones([
+          total_num_padded_proposals, model.num_classes, mask_height, mask_width
+      ],
+                                       dtype=tf.float32)
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      rpn_features_to_crop = tf.ones((batch_size, mask_height, mask_width, 3),
+                                     tf.float32)
       detections = model.postprocess(
           {
               'refined_box_encodings':
@@ -638,44 +482,31 @@ class FasterRCNNMetaArchTest(
               'image_shape':
                   image_shape,
               'mask_predictions':
-                  mask_predictions
+                  mask_predictions,
+              'rpn_features_to_crop':
+                  rpn_features_to_crop
           }, true_image_shapes)
+      self.assertIn('detection_features', detections)
+      return (detections['detection_boxes'], detections['detection_scores'],
+              detections['detection_classes'], detections['num_detections'],
+              detections['detection_masks'])
+    (detection_boxes, detection_scores, detection_classes, num_detections,
+     detection_masks) = self.execute_cpu(graph_fn, [], graph=g)
+    exp_detection_masks = np.array([[[[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]]],
+                                    [[[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[1, 1], [1, 1]], [[1, 1], [1, 1]],
+                                     [[0, 0], [0, 0]]]])
 
-    rpn_features_to_crop = tf.ones((batch_size, mask_height, mask_width, 3),
-                                   tf.float32)
-    detections = model.postprocess(
-        {
-            'refined_box_encodings':
-                refined_box_encodings,
-            'class_predictions_with_background':
-                class_predictions_with_background,
-            'num_proposals':
-                num_proposals,
-            'proposal_boxes':
-                proposal_boxes,
-            'image_shape':
-                image_shape,
-            'mask_predictions':
-                mask_predictions,
-            'rpn_features_to_crop':
-                rpn_features_to_crop
-        }, true_image_shapes)
-
-    with self.test_session() as sess:
-      init_op = tf.global_variables_initializer()
-      sess.run(init_op)
-      detections_out = sess.run(detections)
-      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
-      self.assertAllClose(detections_out['detection_scores'],
-                          [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
-      self.assertAllClose(detections_out['detection_classes'],
-                          [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
-      self.assertAllClose(detections_out['num_detections'], [5, 4])
-      self.assertAllClose(detections_out['detection_masks'],
-                          exp_detection_masks)
-      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
-      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
-      self.assertIn('detection_features', detections_out)
+    self.assertAllEqual(detection_boxes.shape, [2, 5, 4])
+    self.assertAllClose(detection_scores,
+                        [[1, 1, 1, 1, 1], [1, 1, 1, 1, 0]])
+    self.assertAllClose(detection_classes,
+                        [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
+    self.assertAllClose(num_detections, [5, 4])
+    self.assertAllClose(detection_masks,
+                        exp_detection_masks)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 99297262..beead134 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -19,7 +19,7 @@ from absl.testing import parameterized
 
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.anchor_generators import grid_anchor_generator
@@ -37,10 +37,11 @@ from object_detection.protos import post_processing_pb2
 from object_detection.utils import ops
 from object_detection.utils import test_case
 from object_detection.utils import test_utils
+from object_detection.utils import tf_version
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -65,13 +66,13 @@ class FakeFasterRCNNFeatureExtractor(
 
   def _extract_proposal_features(self, preprocessed_inputs, scope):
     with tf.variable_scope('mock_model'):
-      proposal_features = 0 * contrib_slim.conv2d(
+      proposal_features = 0 * slim.conv2d(
           preprocessed_inputs, num_outputs=3, kernel_size=1, scope='layer1')
       return proposal_features, {}
 
   def _extract_box_classifier_features(self, proposal_feature_maps, scope):
     with tf.variable_scope('mock_model'):
-      return 0 * contrib_slim.conv2d(
+      return 0 * slim.conv2d(
           proposal_feature_maps, num_outputs=3, kernel_size=1, scope='layer2')
 
 
@@ -219,7 +220,6 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    is_training,
                    number_of_stages,
                    second_stage_batch_size,
-                   use_keras=False,
                    first_stage_max_proposals=8,
                    num_classes=2,
                    hard_mining=False,
@@ -235,7 +235,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                    share_box_across_classes=False,
                    return_raw_detections_during_predict=False,
                    output_final_box_features=False):
-
+    use_keras = tf_version.is_tf2()
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
       resized_inputs = []
@@ -456,27 +456,26 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
             share_box_across_classes=share_box_across_classes), **common_kwargs)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'use_keras': True},
-      {'use_static_shapes': False, 'use_keras': False},
-      {'use_static_shapes': True, 'use_keras': True},
-      {'use_static_shapes': True, 'use_keras': False},
+      {'use_static_shapes': False},
+      {'use_static_shapes': True},
   )
   def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
-      self, use_static_shapes=False, use_keras=False):
+      self, use_static_shapes=False):
     batch_size = 2
     height = 10
     width = 12
     input_image_shape = (batch_size, height, width, 3)
 
-    def graph_fn(images):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
           is_training=False,
-          use_keras=use_keras,
           number_of_stages=1,
           second_stage_batch_size=2,
           clip_anchors_to_image=use_static_shapes,
           use_static_shapes=use_static_shapes)
+    def graph_fn(images):
+      """Function to construct tf graph for the test."""
+
       preprocessed_inputs, true_image_shapes = model.preprocess(images)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       return (prediction_dict['rpn_box_predictor_features'],
@@ -503,9 +502,9 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     }
 
     if use_static_shapes:
-      results = self.execute(graph_fn, [images])
+      results = self.execute(graph_fn, [images], graph=g)
     else:
-      results = self.execute_cpu(graph_fn, [images])
+      results = self.execute_cpu(graph_fn, [images], graph=g)
 
     self.assertAllEqual(results[0].shape,
                         expected_output_shapes['rpn_box_predictor_features'])
@@ -529,101 +528,75 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))
     self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_regularization_losses(
-      self, use_keras=False):
-    test_graph = tf.Graph()
-    with test_graph.as_default():
+  def test_regularization_losses(self):
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
-          is_training=True, use_keras=use_keras,
-          number_of_stages=1, second_stage_batch_size=2,)
+          is_training=True, number_of_stages=1, second_stage_batch_size=2)
+    def graph_fn():
       batch_size = 2
       height = 10
       width = 12
       input_image_shape = (batch_size, height, width, 3)
-      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      preprocessed_inputs = tf.placeholder(
-          dtype=tf.float32, shape=(batch_size, None, None, 3))
-      model.predict(preprocessed_inputs, true_image_shapes)
+      image, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
+      model.predict(image, true_image_shapes)
 
       reg_losses = tf.math.add_n(model.regularization_losses())
+      return reg_losses
+    reg_losses = self.execute(graph_fn, [], graph=g)
+    self.assertGreaterEqual(reg_losses, 0)
 
-      init_op = tf.global_variables_initializer()
-      with self.test_session(graph=test_graph) as sess:
-        sess.run(init_op)
-        self.assertGreaterEqual(sess.run(reg_losses), 0)
+  def test_predict_gives_valid_anchors_in_training_mode_first_stage_only(self):
+    expected_output_keys = set([
+        'rpn_box_predictor_features', 'rpn_features_to_crop', 'image_shape',
+        'rpn_box_encodings', 'rpn_objectness_predictions_with_background',
+        'anchors', 'feature_maps'])
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_predict_gives_valid_anchors_in_training_mode_first_stage_only(
-      self, use_keras=False):
-    test_graph = tf.Graph()
-    with test_graph.as_default():
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
-          is_training=True, use_keras=use_keras,
-          number_of_stages=1, second_stage_batch_size=2,)
-      batch_size = 2
-      height = 10
-      width = 12
-      input_image_shape = (batch_size, height, width, 3)
-      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
-      preprocessed_inputs = tf.placeholder(
-          dtype=tf.float32, shape=(batch_size, None, None, 3))
-      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+          is_training=True, number_of_stages=1, second_stage_batch_size=2,)
+
+    batch_size = 2
+    height = 10
+    width = 12
+    input_image_shape = (batch_size, height, width, 3)
+    def graph_fn():
+      image, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
+      prediction_dict = model.predict(image, true_image_shapes)
+      self.assertEqual(set(prediction_dict.keys()), expected_output_keys)
+      return (prediction_dict['image_shape'], prediction_dict['anchors'],
+              prediction_dict['rpn_box_encodings'],
+              prediction_dict['rpn_objectness_predictions_with_background'])
+
+    (image_shape, anchors, rpn_box_encodings,
+     rpn_objectness_predictions_with_background) = self.execute(graph_fn, [],
+                                                                graph=g)
+    # At training time, anchors that exceed image bounds are pruned.  Thus
+    # the `expected_num_anchors` in the above inference mode test is now
+    # a strict upper bound on the number of anchors.
+    num_anchors_strict_upper_bound = height * width * 3 * 3
+    self.assertAllEqual(image_shape, input_image_shape)
+    self.assertTrue(len(anchors.shape) == 2 and anchors.shape[1] == 4)
+    num_anchors_out = anchors.shape[0]
+    self.assertLess(num_anchors_out, num_anchors_strict_upper_bound)
 
-      expected_output_keys = set([
-          'rpn_box_predictor_features', 'rpn_features_to_crop', 'image_shape',
-          'rpn_box_encodings', 'rpn_objectness_predictions_with_background',
-          'anchors', 'feature_maps'])
-      # At training time, anchors that exceed image bounds are pruned.  Thus
-      # the `expected_num_anchors` in the above inference mode test is now
-      # a strict upper bound on the number of anchors.
-      num_anchors_strict_upper_bound = height * width * 3 * 3
+    self.assertTrue(np.all(np.greater_equal(anchors, 0)))
+    self.assertTrue(np.all(np.less_equal(anchors[:, 0], height)))
+    self.assertTrue(np.all(np.less_equal(anchors[:, 1], width)))
+    self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))
+    self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))
 
-      init_op = tf.global_variables_initializer()
-      with self.test_session(graph=test_graph) as sess:
-        sess.run(init_op)
-        prediction_out = sess.run(prediction_dict,
-                                  feed_dict={
-                                      preprocessed_inputs:
-                                      np.zeros(input_image_shape)
-                                  })
-
-        self.assertEqual(set(prediction_out.keys()), expected_output_keys)
-        self.assertAllEqual(prediction_out['image_shape'], input_image_shape)
-
-        # Check that anchors have less than the upper bound and
-        # are clipped to window.
-        anchors = prediction_out['anchors']
-        self.assertTrue(len(anchors.shape) == 2 and anchors.shape[1] == 4)
-        num_anchors_out = anchors.shape[0]
-        self.assertLess(num_anchors_out, num_anchors_strict_upper_bound)
-
-        self.assertTrue(np.all(np.greater_equal(anchors, 0)))
-        self.assertTrue(np.all(np.less_equal(anchors[:, 0], height)))
-        self.assertTrue(np.all(np.less_equal(anchors[:, 1], width)))
-        self.assertTrue(np.all(np.less_equal(anchors[:, 2], height)))
-        self.assertTrue(np.all(np.less_equal(anchors[:, 3], width)))
-
-        self.assertAllEqual(prediction_out['rpn_box_encodings'].shape,
-                            (batch_size, num_anchors_out, 4))
-        self.assertAllEqual(
-            prediction_out['rpn_objectness_predictions_with_background'].shape,
-            (batch_size, num_anchors_out, 2))
+    self.assertAllEqual(rpn_box_encodings.shape,
+                        (batch_size, num_anchors_out, 4))
+    self.assertAllEqual(
+        rpn_objectness_predictions_with_background.shape,
+        (batch_size, num_anchors_out, 2))
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'use_keras': True},
-      {'use_static_shapes': False, 'use_keras': False},
-      {'use_static_shapes': True, 'use_keras': True},
-      {'use_static_shapes': True, 'use_keras': False},
+      {'use_static_shapes': False},
+      {'use_static_shapes': True},
   )
   def test_predict_correct_shapes_in_inference_mode_two_stages(
-      self, use_static_shapes=False, use_keras=False):
+      self, use_static_shapes):
 
     def compare_results(results, expected_output_shapes):
       """Checks if the shape of the predictions are as expected."""
@@ -661,22 +634,32 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     initial_crop_size = 3
     maxpool_stride = 1
 
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    def graph_fn_tpu(images):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
           is_training=False,
-          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=2,
           predict_masks=False,
           use_matmul_crop_and_resize=use_static_shapes,
           clip_anchors_to_image=use_static_shapes,
           use_static_shapes=use_static_shapes)
+    def graph_fn():
+      """A function with TF compute."""
+      if use_static_shapes:
+        images = tf.random_uniform((batch_size, image_size, image_size, 3))
+      else:
+        images = tf.random_uniform((tf.random_uniform([],
+                                                      minval=batch_size,
+                                                      maxval=batch_size + 1,
+                                                      dtype=tf.int32),
+                                    tf.random_uniform([],
+                                                      minval=image_size,
+                                                      maxval=image_size + 1,
+                                                      dtype=tf.int32),
+                                    tf.random_uniform([],
+                                                      minval=image_size,
+                                                      maxval=image_size + 1,
+                                                      dtype=tf.int32), 3))
       preprocessed_inputs, true_image_shapes = model.preprocess(images)
       prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
       return (prediction_dict['rpn_box_predictor_features'],
@@ -692,7 +675,6 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               prediction_dict['proposal_boxes_normalized'],
               prediction_dict['box_classifier_features'],
               prediction_dict['final_anchors'])
-
     expected_num_anchors = image_size * image_size * 3 * 3
     expected_shapes = {
         'rpn_box_predictor_features':
@@ -720,58 +702,27 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     }
 
     if use_static_shapes:
-      input_shape = (batch_size, image_size, image_size, 3)
-      images = np.zeros(input_shape, dtype=np.float32)
-      results = self.execute(graph_fn_tpu, [images])
-      compare_results(results, expected_shapes)
+      results = self.execute(graph_fn, [], graph=g)
     else:
-      for input_shape in input_shapes:
-        test_graph = tf.Graph()
-        with test_graph.as_default():
-          model = self._build_model(
-              is_training=False,
-              use_keras=use_keras,
-              number_of_stages=2,
-              second_stage_batch_size=2,
-              predict_masks=False)
-          preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
-          _, true_image_shapes = model.preprocess(preprocessed_inputs)
-          result_tensor_dict = model.predict(
-              preprocessed_inputs, true_image_shapes)
-          init_op = tf.global_variables_initializer()
-        with self.test_session(graph=test_graph) as sess:
-          sess.run(init_op)
-          tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
-              preprocessed_inputs:
-              np.zeros((batch_size, image_size, image_size, 3))})
-        self.assertEqual(set(tensor_dict_out.keys()),
-                         set(expected_shapes.keys()))
-        for key in expected_shapes:
-          if isinstance(tensor_dict_out[key], list):
-            continue
-          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
+      results = self.execute_cpu(graph_fn, [], graph=g)
+    compare_results(results, expected_shapes)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'use_keras': True},
-      {'use_static_shapes': False, 'use_keras': False},
-      {'use_static_shapes': True, 'use_keras': True},
-      {'use_static_shapes': True, 'use_keras': False},
+      {'use_static_shapes': False},
+      {'use_static_shapes': True},
   )
   def test_predict_gives_correct_shapes_in_train_mode_both_stages(
       self,
-      use_static_shapes=False,
-      use_keras=False):
+      use_static_shapes=False):
     batch_size = 2
     image_size = 10
     max_num_proposals = 7
     initial_crop_size = 3
     maxpool_stride = 1
 
-    def graph_fn(images, gt_boxes, gt_classes, gt_weights):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
           is_training=True,
-          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=7,
           predict_masks=False,
@@ -779,13 +730,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           clip_anchors_to_image=use_static_shapes,
           use_static_shapes=use_static_shapes)
 
+    def graph_fn(images, gt_boxes, gt_classes, gt_weights):
+      """Function to construct tf graph for the test."""
       preprocessed_inputs, true_image_shapes = model.preprocess(images)
       model.provide_groundtruth(
           groundtruth_boxes_list=tf.unstack(gt_boxes),
           groundtruth_classes_list=tf.unstack(gt_classes),
           groundtruth_weights_list=tf.unstack(gt_weights))
       result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
-      updates = model.updates()
       return (result_tensor_dict['refined_box_encodings'],
               result_tensor_dict['class_predictions_with_background'],
               result_tensor_dict['proposal_boxes'],
@@ -795,7 +747,6 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
               result_tensor_dict['rpn_objectness_predictions_with_background'],
               result_tensor_dict['rpn_features_to_crop'],
               result_tensor_dict['rpn_box_predictor_features'],
-              updates,
               result_tensor_dict['final_anchors'],
              )
 
@@ -815,10 +766,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     ])
     if use_static_shapes:
       results = self.execute(graph_fn,
-                             [images, gt_boxes, gt_classes, gt_weights])
+                             [images, gt_boxes, gt_classes, gt_weights],
+                             graph=g)
     else:
       results = self.execute_cpu(graph_fn,
-                                 [images, gt_boxes, gt_classes, gt_weights])
+                                 [images, gt_boxes, gt_classes, gt_weights],
+                                 graph=g)
 
     expected_shapes = {
         'rpn_box_predictor_features': (2, image_size, image_size, 512),
@@ -854,42 +807,24 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                         expected_shapes['rpn_features_to_crop'])
     self.assertAllEqual(results[8].shape,
                         expected_shapes['rpn_box_predictor_features'])
-    self.assertAllEqual(results[10].shape,
+    self.assertAllEqual(results[9].shape,
                         expected_shapes['final_anchors'])
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'pad_to_max_dimension': None,
-       'use_keras': True},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None,
-       'use_keras': True},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
-       'use_keras': True},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
-       'use_keras': True},
-      {'use_static_shapes': False, 'pad_to_max_dimension': None,
-       'use_keras': False},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None,
-       'use_keras': False},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
-       'use_keras': False},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
-       'use_keras': False}
+      {'use_static_shapes': False, 'pad_to_max_dimension': None},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56,},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56},
   )
   def test_postprocess_first_stage_only_inference_mode(
       self, use_static_shapes=False,
-      pad_to_max_dimension=None,
-      use_keras=False):
+      pad_to_max_dimension=None):
     batch_size = 2
     first_stage_max_proposals = 4 if use_static_shapes else 8
 
-    def graph_fn(images,
-                 rpn_box_encodings,
-                 rpn_objectness_predictions_with_background,
-                 rpn_features_to_crop,
-                 anchors):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
-          is_training=False, use_keras=use_keras,
+          is_training=False,
           number_of_stages=1, second_stage_batch_size=6,
           use_matmul_crop_and_resize=use_static_shapes,
           clip_anchors_to_image=use_static_shapes,
@@ -897,6 +832,13 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           use_matmul_gather_in_matcher=use_static_shapes,
           first_stage_max_proposals=first_stage_max_proposals,
           pad_to_max_dimension=pad_to_max_dimension)
+
+    def graph_fn(images,
+                 rpn_box_encodings,
+                 rpn_objectness_predictions_with_background,
+                 rpn_features_to_crop,
+                 anchors):
+      """Function to construct tf graph for the test."""
       preprocessed_images, true_image_shapes = model.preprocess(images)
       proposals = model.postprocess({
           'rpn_box_encodings': rpn_box_encodings,
@@ -935,12 +877,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       results = self.execute(graph_fn,
                              [images, rpn_box_encodings,
                               rpn_objectness_predictions_with_background,
-                              rpn_features_to_crop, anchors])
+                              rpn_features_to_crop, anchors], graph=g)
     else:
       results = self.execute_cpu(graph_fn,
                                  [images, rpn_box_encodings,
                                   rpn_objectness_predictions_with_background,
-                                  rpn_features_to_crop, anchors])
+                                  rpn_features_to_crop, anchors], graph=g)
 
     expected_proposal_boxes = [
         [[0, 0, .5, .5], [.5, .5, 1, 1], [0, .5, .5, 1], [.5, 0, 1.0, .5]]
@@ -973,55 +915,67 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllClose(results[4], expected_raw_scores)
 
   def _test_postprocess_first_stage_only_train_mode(self,
-                                                    use_keras=False,
                                                     pad_to_max_dimension=None):
-    model = self._build_model(
-        is_training=True, use_keras=use_keras,
-        number_of_stages=1, second_stage_batch_size=2,
-        pad_to_max_dimension=pad_to_max_dimension)
+
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=True,
+          number_of_stages=1, second_stage_batch_size=2,
+          pad_to_max_dimension=pad_to_max_dimension)
     batch_size = 2
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size, anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant([
-        [[-10, 13],
-         [-10, 12],
-         [-10, 11],
-         [-10, 10]],
-        [[-10, 13],
-         [-10, 12],
-         [-10, 11],
-         [-10, 10]]], dtype=tf.float32)
-    rpn_features_to_crop = tf.ones((batch_size, 8, 8, 10), dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
-    groundtruth_boxes_list = [
-        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
-        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
-                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]
-    groundtruth_weights_list = [
-        tf.constant([1, 1], dtype=tf.float32),
-        tf.constant([1, 1], dtype=tf.float32)
-    ]
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(
-        groundtruth_boxes_list,
-        groundtruth_classes_list,
-        groundtruth_weights_list=groundtruth_weights_list)
-    proposals = model.postprocess({
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'rpn_features_to_crop': rpn_features_to_crop,
-        'anchors': anchors,
-        'image_shape': image_shape}, true_image_shapes)
+
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size, anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant([
+          [[-10, 13],
+           [-10, 12],
+           [-10, 11],
+           [-10, 10]],
+          [[-10, 13],
+           [-10, 12],
+           [-10, 11],
+           [-10, 10]]], dtype=tf.float32)
+      rpn_features_to_crop = tf.ones((batch_size, 8, 8, 10), dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
+          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]
+      groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]],
+                                              dtype=tf.float32),
+                                  tf.constant([[1, 0], [1, 0]],
+                                              dtype=tf.float32)]
+      groundtruth_weights_list = [
+          tf.constant([1, 1], dtype=tf.float32),
+          tf.constant([1, 1], dtype=tf.float32)
+      ]
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(
+          groundtruth_boxes_list,
+          groundtruth_classes_list,
+          groundtruth_weights_list=groundtruth_weights_list)
+      proposals = model.postprocess({
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'rpn_features_to_crop': rpn_features_to_crop,
+          'anchors': anchors,
+          'image_shape': image_shape}, true_image_shapes)
+      return (proposals['detection_boxes'], proposals['detection_scores'],
+              proposals['num_detections'],
+              proposals['detection_multiclass_scores'],
+              proposals['raw_detection_boxes'],
+              proposals['raw_detection_scores'])
+
     expected_proposal_boxes = [
         [[0, 0, .5, .5], [.5, .5, 1, 1]], [[0, .5, .5, 1], [.5, 0, 1, .5]]]
     expected_proposal_scores = [[1, 1],
@@ -1034,94 +988,57 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
                                     [0.5, 0., 1., 0.5], [0.5, 0.5, 1., 1.]]]
     expected_raw_scores = [[[0., 1.], [0., 1.], [0., 1.], [0., 1.]],
                            [[0., 1.], [0., 1.], [0., 1.], [0., 1.]]]
-    expected_output_keys = set([
-        'detection_boxes', 'detection_scores', 'detection_multiclass_scores',
-        'num_detections', 'raw_detection_boxes', 'raw_detection_scores'
-    ])
-    self.assertEqual(set(proposals.keys()), expected_output_keys)
-
-    with self.test_session() as sess:
-      proposals_out = sess.run(proposals)
-      for image_idx in range(batch_size):
-        num_detections = int(proposals_out['num_detections'][image_idx])
-        boxes = proposals_out['detection_boxes'][
-            image_idx][:num_detections, :].tolist()
-        scores = proposals_out['detection_scores'][
-            image_idx][:num_detections].tolist()
-        multiclass_scores = proposals_out['detection_multiclass_scores'][
-            image_idx][:num_detections, :].tolist()
-        expected_boxes = expected_proposal_boxes[image_idx]
-        expected_scores = expected_proposal_scores[image_idx]
-        expected_multiclass_scores = expected_proposal_multiclass_scores[
-            image_idx]
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(boxes, expected_boxes))
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(scores, expected_scores))
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(multiclass_scores,
-                                               expected_multiclass_scores))
-
-    self.assertAllClose(proposals_out['raw_detection_boxes'],
-                        expected_raw_proposal_boxes)
-    self.assertAllClose(proposals_out['raw_detection_scores'],
-                        expected_raw_scores)
-
-  @parameterized.named_parameters({
-      'testcase_name': 'keras',
-      'use_keras': True
-  }, {
-      'testcase_name': 'slim',
-      'use_keras': False
-  })
-  def test_postprocess_first_stage_only_train_mode(self, use_keras=False):
-    self._test_postprocess_first_stage_only_train_mode(use_keras=use_keras)
+
+    (proposal_boxes, proposal_scores, batch_num_detections,
+     batch_multiclass_scores, raw_detection_boxes,
+     raw_detection_scores) = self.execute_cpu(graph_fn, [], graph=g)
+    for image_idx in range(batch_size):
+      num_detections = int(batch_num_detections[image_idx])
+      boxes = proposal_boxes[image_idx][:num_detections, :].tolist()
+      scores = proposal_scores[image_idx][:num_detections].tolist()
+      multiclass_scores = batch_multiclass_scores[
+          image_idx][:num_detections, :].tolist()
+      expected_boxes = expected_proposal_boxes[image_idx]
+      expected_scores = expected_proposal_scores[image_idx]
+      expected_multiclass_scores = expected_proposal_multiclass_scores[
+          image_idx]
+      self.assertTrue(
+          test_utils.first_rows_close_as_set(boxes, expected_boxes))
+      self.assertTrue(
+          test_utils.first_rows_close_as_set(scores, expected_scores))
+      self.assertTrue(
+          test_utils.first_rows_close_as_set(multiclass_scores,
+                                             expected_multiclass_scores))
+
+    self.assertAllClose(raw_detection_boxes, expected_raw_proposal_boxes)
+    self.assertAllClose(raw_detection_scores, expected_raw_scores)
 
   @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
+      {'pad_to_max_dimension': 56},
+      {'pad_to_max_dimension': None}
   )
   def test_postprocess_first_stage_only_train_mode_padded_image(
-      self, use_keras=False):
-    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56,
-                                                       use_keras=use_keras)
+      self, pad_to_max_dimension):
+    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'pad_to_max_dimension': None,
-       'use_keras': True},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None,
-       'use_keras': True},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
-       'use_keras': True},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
-       'use_keras': True},
-      {'use_static_shapes': False, 'pad_to_max_dimension': None,
-       'use_keras': False},
-      {'use_static_shapes': True, 'pad_to_max_dimension': None,
-       'use_keras': False},
-      {'use_static_shapes': False, 'pad_to_max_dimension': 56,
-       'use_keras': False},
-      {'use_static_shapes': True, 'pad_to_max_dimension': 56,
-       'use_keras': False}
+      {'use_static_shapes': False, 'pad_to_max_dimension': None},
+      {'use_static_shapes': True, 'pad_to_max_dimension': None},
+      {'use_static_shapes': False, 'pad_to_max_dimension': 56},
+      {'use_static_shapes': True, 'pad_to_max_dimension': 56},
   )
   def test_postprocess_second_stage_only_inference_mode(
       self, use_static_shapes=False,
-      pad_to_max_dimension=None,
-      use_keras=False):
+      pad_to_max_dimension=None):
     batch_size = 2
     num_classes = 2
     image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
     first_stage_max_proposals = 8
     total_num_padded_proposals = batch_size * first_stage_max_proposals
 
-    def graph_fn(images,
-                 refined_box_encodings,
-                 class_predictions_with_background,
-                 num_proposals,
-                 proposal_boxes):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
-          is_training=False, use_keras=use_keras,
+          is_training=False,
           number_of_stages=2,
           second_stage_batch_size=6,
           use_matmul_crop_and_resize=use_static_shapes,
@@ -1129,6 +1046,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           use_static_shapes=use_static_shapes,
           use_matmul_gather_in_matcher=use_static_shapes,
           pad_to_max_dimension=pad_to_max_dimension)
+    def graph_fn(images,
+                 refined_box_encodings,
+                 class_predictions_with_background,
+                 num_proposals,
+                 proposal_boxes):
+      """Function to construct tf graph for the test."""
       _, true_image_shapes = model.preprocess(images)
       detections = model.postprocess({
           'refined_box_encodings': refined_box_encodings,
@@ -1164,12 +1087,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
       results = self.execute(graph_fn,
                              [images, refined_box_encodings,
                               class_predictions_with_background,
-                              num_proposals, proposal_boxes])
+                              num_proposals, proposal_boxes], graph=g)
     else:
       results = self.execute_cpu(graph_fn,
                                  [images, refined_box_encodings,
                                   class_predictions_with_background,
-                                  num_proposals, proposal_boxes])
+                                  num_proposals, proposal_boxes], graph=g)
     # Note that max_total_detections=5 in the NMS config.
     expected_num_detections = [5, 4]
     expected_detection_classes = [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]]
@@ -1217,424 +1140,426 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     if not use_static_shapes:
       self.assertAllEqual(results[1].shape, [2, 5, 4])
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_preprocess_preserves_input_shapes(self, use_keras=False):
-    image_shapes = [(3, None, None, 3),
-                    (None, 10, 10, 3),
-                    (None, None, None, 3)]
-    for image_shape in image_shapes:
-      model = self._build_model(
-          is_training=False, use_keras=use_keras,
-          number_of_stages=2, second_stage_batch_size=6)
-      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
-      preprocessed_inputs, _ = model.preprocess(image_placeholder)
-      self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
+  def test_preprocess_preserves_dynamic_input_shapes(self):
+    width = tf.random.uniform([], minval=5, maxval=10, dtype=tf.int32)
+    batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+    shape = tf.stack([batch, 5, width, 3])
+    image = tf.random.uniform(shape)
+    model = self._build_model(
+        is_training=False, number_of_stages=2, second_stage_batch_size=6)
+    preprocessed_inputs, _ = model.preprocess(image)
+    self.assertTrue(
+        preprocessed_inputs.shape.is_compatible_with([None, 5, None, 3]))
+
+  def test_preprocess_preserves_static_input_shapes(self):
+    shape = tf.stack([2, 5, 5, 3])
+    image = tf.random.uniform(shape)
+    model = self._build_model(
+        is_training=False, number_of_stages=2, second_stage_batch_size=6)
+    preprocessed_inputs, _ = model.preprocess(image)
+    self.assertTrue(
+        preprocessed_inputs.shape.is_compatible_with([2, 5, 5, 3]))
 
   # TODO(rathodv): Split test into two - with and without masks.
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_first_stage_only_mode(self, use_keras=False):
-    model = self._build_model(
-        is_training=True, use_keras=use_keras,
-        number_of_stages=1, second_stage_batch_size=6)
+  def test_loss_first_stage_only_mode(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=True,
+          number_of_stages=1, second_stage_batch_size=6)
     batch_size = 2
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant([
-        [[-10, 13],
-         [10, -10],
-         [10, -11],
-         [-10, 12]],
-        [[10, -10],
-         [-10, 13],
-         [-10, 12],
-         [10, -11]]], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
-
-    groundtruth_boxes_list = [
-        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
-        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
-                                tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant([
+          [[-10, 13],
+           [10, -10],
+           [10, -11],
+           [-10, 12]],
+          [[10, -10],
+           [-10, 13],
+           [-10, 12],
+           [10, -11]]], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
+          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)]
+      groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]],
+                                              dtype=tf.float32),
+                                  tf.constant([[1, 0], [1, 0]],
+                                              dtype=tf.float32)]
 
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/objectness_loss'], 0)
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
       self.assertNotIn('Loss/BoxClassifierLoss/localization_loss',
-                       loss_dict_out)
+                       loss_dict)
       self.assertNotIn('Loss/BoxClassifierLoss/classification_loss',
-                       loss_dict_out)
+                       loss_dict)
+      return (loss_dict['Loss/RPNLoss/localization_loss'],
+              loss_dict['Loss/RPNLoss/objectness_loss'])
+    loc_loss, obj_loss = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllClose(loc_loss, 0)
+    self.assertAllClose(obj_loss, 0)
 
   # TODO(rathodv): Split test into two - with and without masks.
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_full(self, use_keras=False):
-    model = self._build_model(
-        is_training=True, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6)
+  def test_loss_full(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=True,
+          number_of_stages=2, second_stage_batch_size=6)
     batch_size = 3
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant(
-        [[[-10, 13], [10, -10], [10, -11], [-10, 12]], [[10, -10], [-10, 13], [
-            -10, 12
-        ], [10, -11]], [[10, -10], [-10, 13], [-10, 12], [10, -11]]],
-        dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
-
-    num_proposals = tf.constant([6, 6, 6], dtype=tf.int32)
-    proposal_boxes = tf.constant(
-        3 * [[[0, 0, 16, 16], [0, 16, 16, 32], [16, 0, 32, 16],
-              [16, 16, 32, 32], [0, 0, 16, 16], [0, 16, 16, 32]]],
-        dtype=tf.float32)
-    refined_box_encodings = tf.zeros(
-        (batch_size * model.max_num_proposals,
-         model.num_classes,
-         BOX_CODE_SIZE), dtype=tf.float32)
-    class_predictions_with_background = tf.constant(
-        [
-            [-10, 10, -10],  # first image
-            [10, -10, -10],
-            [10, -10, -10],
-            [-10, -10, 10],
-            [-10, 10, -10],
-            [10, -10, -10],
-            [10, -10, -10],  # second image
-            [-10, 10, -10],
-            [-10, 10, -10],
-            [10, -10, -10],
-            [10, -10, -10],
-            [-10, 10, -10],
-            [10, -10, -10],  # third image
-            [-10, 10, -10],
-            [-10, 10, -10],
-            [10, -10, -10],
-            [10, -10, -10],
-            [-10, 10, -10]
-        ],
-        dtype=tf.float32)
-
-    mask_predictions_logits = 20 * tf.ones((batch_size *
-                                            model.max_num_proposals,
-                                            model.num_classes,
-                                            14, 14),
-                                           dtype=tf.float32)
-
-    groundtruth_boxes_list = [
-        tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
-        tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32),
-        tf.constant([[0, .5, .5, 1], [.5, 0, 1, 1]], dtype=tf.float32)
-    ]
-    groundtruth_classes_list = [
-        tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
-        tf.constant([[1, 0], [1, 0]], dtype=tf.float32),
-        tf.constant([[1, 0], [0, 1]], dtype=tf.float32)
-    ]
-
-    # Set all elements of groundtruth mask to 1.0. In this case all proposal
-    # crops of the groundtruth masks should return a mask that covers the entire
-    # proposal. Thus, if mask_predictions_logits element values are all greater
-    # than 20, the loss should be zero.
-    groundtruth_masks_list = [
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
-        tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32)
-    ]
-    groundtruth_weights_list = [
-        tf.constant([1, 1], dtype=tf.float32),
-        tf.constant([1, 1], dtype=tf.float32),
-        tf.constant([1, 0], dtype=tf.float32)
-    ]
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors,
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'proposal_boxes': proposal_boxes,
-        'num_proposals': num_proposals,
-        'mask_predictions': mask_predictions_logits
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(
-        groundtruth_boxes_list,
-        groundtruth_classes_list,
-        groundtruth_masks_list,
-        groundtruth_weights_list=groundtruth_weights_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/objectness_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/classification_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant(
+          [[[-10, 13], [10, -10], [10, -11], [-10, 12]],
+           [[10, -10], [-10, 13], [-10, 12], [10, -11]],
+           [[10, -10], [-10, 13], [-10, 12], [10, -11]]],
+          dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_full_zero_padded_proposals(self, use_keras=False):
-    model = self._build_model(
-        is_training=True, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6)
-    batch_size = 1
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant([
-        [[-10, 13],
-         [10, -10],
-         [10, -11],
-         [10, -12]],], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+      num_proposals = tf.constant([6, 6, 6], dtype=tf.int32)
+      proposal_boxes = tf.constant(
+          3 * [[[0, 0, 16, 16], [0, 16, 16, 32], [16, 0, 32, 16],
+                [16, 16, 32, 32], [0, 0, 16, 16], [0, 16, 16, 32]]],
+          dtype=tf.float32)
+      refined_box_encodings = tf.zeros(
+          (batch_size * model.max_num_proposals,
+           model.num_classes,
+           BOX_CODE_SIZE), dtype=tf.float32)
+      class_predictions_with_background = tf.constant(
+          [
+              [-10, 10, -10],  # first image
+              [10, -10, -10],
+              [10, -10, -10],
+              [-10, -10, 10],
+              [-10, 10, -10],
+              [10, -10, -10],
+              [10, -10, -10],  # second image
+              [-10, 10, -10],
+              [-10, 10, -10],
+              [10, -10, -10],
+              [10, -10, -10],
+              [-10, 10, -10],
+              [10, -10, -10],  # third image
+              [-10, 10, -10],
+              [-10, 10, -10],
+              [10, -10, -10],
+              [10, -10, -10],
+              [-10, 10, -10]
+          ],
+          dtype=tf.float32)
 
-    # box_classifier_batch_size is 6, but here we assume that the number of
-    # actual proposals (not counting zero paddings) is fewer (3).
-    num_proposals = tf.constant([3], dtype=tf.int32)
-    proposal_boxes = tf.constant(
-        [[[0, 0, 16, 16],
-          [0, 16, 16, 32],
-          [16, 0, 32, 16],
-          [0, 0, 0, 0],  # begin paddings
-          [0, 0, 0, 0],
-          [0, 0, 0, 0]]], dtype=tf.float32)
-
-    refined_box_encodings = tf.zeros(
-        (batch_size * model.max_num_proposals,
-         model.num_classes,
-         BOX_CODE_SIZE), dtype=tf.float32)
-    class_predictions_with_background = tf.constant(
-        [[-10, 10, -10],
-         [10, -10, -10],
-         [10, -10, -10],
-         [0, 0, 0],  # begin paddings
-         [0, 0, 0],
-         [0, 0, 0]], dtype=tf.float32)
-
-    mask_predictions_logits = 20 * tf.ones((batch_size *
-                                            model.max_num_proposals,
-                                            model.num_classes,
-                                            14, 14),
-                                           dtype=tf.float32)
-
-    groundtruth_boxes_list = [
-        tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1, 0]], dtype=tf.float32)]
-
-    # Set all elements of groundtruth mask to 1.0. In this case all proposal
-    # crops of the groundtruth masks should return a mask that covers the entire
-    # proposal. Thus, if mask_predictions_logits element values are all greater
-    # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
-                                                   dtype=tf.float32)]
-
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors,
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'proposal_boxes': proposal_boxes,
-        'num_proposals': num_proposals,
-        'mask_predictions': mask_predictions_logits
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list,
-                              groundtruth_masks_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/objectness_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/classification_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
+      mask_predictions_logits = 20 * tf.ones((batch_size *
+                                              model.max_num_proposals,
+                                              model.num_classes,
+                                              14, 14),
+                                             dtype=tf.float32)
+
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
+          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32),
+          tf.constant([[0, .5, .5, 1], [.5, 0, 1, 1]], dtype=tf.float32)
+      ]
+      groundtruth_classes_list = [
+          tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
+          tf.constant([[1, 0], [1, 0]], dtype=tf.float32),
+          tf.constant([[1, 0], [0, 1]], dtype=tf.float32)
+      ]
+
+      # Set all elements of groundtruth mask to 1.0. In this case all proposal
+      # crops of the groundtruth masks should return a mask that covers the
+      # entire proposal. Thus, if mask_predictions_logits element values are all
+      # greater than 20, the loss should be zero.
+      groundtruth_masks_list = [
+          tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
+          tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32),
+          tf.convert_to_tensor(np.ones((2, 32, 32)), dtype=tf.float32)
+      ]
+      groundtruth_weights_list = [
+          tf.constant([1, 1], dtype=tf.float32),
+          tf.constant([1, 1], dtype=tf.float32),
+          tf.constant([1, 0], dtype=tf.float32)
+      ]
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors,
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'proposal_boxes': proposal_boxes,
+          'num_proposals': num_proposals,
+          'mask_predictions': mask_predictions_logits
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(
+          groundtruth_boxes_list,
+          groundtruth_classes_list,
+          groundtruth_masks_list,
+          groundtruth_weights_list=groundtruth_weights_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
+      return (loss_dict['Loss/RPNLoss/localization_loss'],
+              loss_dict['Loss/RPNLoss/objectness_loss'],
+              loss_dict['Loss/BoxClassifierLoss/localization_loss'],
+              loss_dict['Loss/BoxClassifierLoss/classification_loss'],
+              loss_dict['Loss/BoxClassifierLoss/mask_loss'])
+    (rpn_loc_loss, rpn_obj_loss, box_loc_loss, box_cls_loss,
+     box_mask_loss) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllClose(rpn_loc_loss, 0)
+    self.assertAllClose(rpn_obj_loss, 0)
+    self.assertAllClose(box_loc_loss, 0)
+    self.assertAllClose(box_cls_loss, 0)
+    self.assertAllClose(box_mask_loss, 0)
+
+  def test_loss_full_zero_padded_proposals(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=True, number_of_stages=2, second_stage_batch_size=6)
+    batch_size = 1
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant([
+          [[-10, 13],
+           [10, -10],
+           [10, -11],
+           [10, -12]],], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+
+      # box_classifier_batch_size is 6, but here we assume that the number of
+      # actual proposals (not counting zero paddings) is fewer (3).
+      num_proposals = tf.constant([3], dtype=tf.int32)
+      proposal_boxes = tf.constant(
+          [[[0, 0, 16, 16],
+            [0, 16, 16, 32],
+            [16, 0, 32, 16],
+            [0, 0, 0, 0],  # begin paddings
+            [0, 0, 0, 0],
+            [0, 0, 0, 0]]], dtype=tf.float32)
+
+      refined_box_encodings = tf.zeros(
+          (batch_size * model.max_num_proposals,
+           model.num_classes,
+           BOX_CODE_SIZE), dtype=tf.float32)
+      class_predictions_with_background = tf.constant(
+          [[-10, 10, -10],
+           [10, -10, -10],
+           [10, -10, -10],
+           [0, 0, 0],  # begin paddings
+           [0, 0, 0],
+           [0, 0, 0]], dtype=tf.float32)
+
+      mask_predictions_logits = 20 * tf.ones((batch_size *
+                                              model.max_num_proposals,
+                                              model.num_classes,
+                                              14, 14),
+                                             dtype=tf.float32)
+
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]
+      groundtruth_classes_list = [tf.constant([[1, 0]], dtype=tf.float32)]
+
+      # Set all elements of groundtruth mask to 1.0. In this case all proposal
+      # crops of the groundtruth masks should return a mask that covers the
+      # entire proposal. Thus, if mask_predictions_logits element values are all
+      # greater than 20, the loss should be zero.
+      groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
+                                                     dtype=tf.float32)]
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_full_multiple_label_groundtruth(self, use_keras=False):
-    model = self._build_model(
-        is_training=True, use_keras=use_keras,
-        number_of_stages=2, second_stage_batch_size=6,
-        softmax_second_stage_classification_loss=False)
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors,
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'proposal_boxes': proposal_boxes,
+          'num_proposals': num_proposals,
+          'mask_predictions': mask_predictions_logits
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list,
+                                groundtruth_masks_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
+      return (loss_dict['Loss/RPNLoss/localization_loss'],
+              loss_dict['Loss/RPNLoss/objectness_loss'],
+              loss_dict['Loss/BoxClassifierLoss/localization_loss'],
+              loss_dict['Loss/BoxClassifierLoss/classification_loss'],
+              loss_dict['Loss/BoxClassifierLoss/mask_loss'])
+    (rpn_loc_loss, rpn_obj_loss, box_loc_loss, box_cls_loss,
+     box_mask_loss) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllClose(rpn_loc_loss, 0)
+    self.assertAllClose(rpn_obj_loss, 0)
+    self.assertAllClose(box_loc_loss, 0)
+    self.assertAllClose(box_cls_loss, 0)
+    self.assertAllClose(box_mask_loss, 0)
+
+  def test_loss_full_multiple_label_groundtruth(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(
+          is_training=True,
+          number_of_stages=2, second_stage_batch_size=6,
+          softmax_second_stage_classification_loss=False)
     batch_size = 1
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant([
-        [[-10, 13],
-         [10, -10],
-         [10, -11],
-         [10, -12]],], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant([
+          [[-10, 13],
+           [10, -10],
+           [10, -11],
+           [10, -12]],], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+
+      # box_classifier_batch_size is 6, but here we assume that the number of
+      # actual proposals (not counting zero paddings) is fewer (3).
+      num_proposals = tf.constant([3], dtype=tf.int32)
+      proposal_boxes = tf.constant(
+          [[[0, 0, 16, 16],
+            [0, 16, 16, 32],
+            [16, 0, 32, 16],
+            [0, 0, 0, 0],  # begin paddings
+            [0, 0, 0, 0],
+            [0, 0, 0, 0]]], dtype=tf.float32)
+
+      # second_stage_localization_loss should only be computed for predictions
+      # that match groundtruth. For multiple label groundtruth boxes, the loss
+      # should only be computed once for the label with the smaller index.
+      refined_box_encodings = tf.constant(
+          [[[0, 0, 0, 0], [1, 1, -1, -1]],
+           [[1, 1, -1, -1], [1, 1, 1, 1]],
+           [[1, 1, -1, -1], [1, 1, 1, 1]],
+           [[1, 1, -1, -1], [1, 1, 1, 1]],
+           [[1, 1, -1, -1], [1, 1, 1, 1]],
+           [[1, 1, -1, -1], [1, 1, 1, 1]]], dtype=tf.float32)
+      class_predictions_with_background = tf.constant(
+          [[-100, 100, 100],
+           [100, -100, -100],
+           [100, -100, -100],
+           [0, 0, 0],  # begin paddings
+           [0, 0, 0],
+           [0, 0, 0]], dtype=tf.float32)
+
+      mask_predictions_logits = 20 * tf.ones((batch_size *
+                                              model.max_num_proposals,
+                                              model.num_classes,
+                                              14, 14),
+                                             dtype=tf.float32)
+
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]
+      # Box contains two ground truth labels.
+      groundtruth_classes_list = [tf.constant([[1, 1]], dtype=tf.float32)]
+
+      # Set all elements of groundtruth mask to 1.0. In this case all proposal
+      # crops of the groundtruth masks should return a mask that covers the
+      # entire proposal. Thus, if mask_predictions_logits element values are all
+      # greater than 20, the loss should be zero.
+      groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
+                                                     dtype=tf.float32)]
 
-    # box_classifier_batch_size is 6, but here we assume that the number of
-    # actual proposals (not counting zero paddings) is fewer (3).
-    num_proposals = tf.constant([3], dtype=tf.int32)
-    proposal_boxes = tf.constant(
-        [[[0, 0, 16, 16],
-          [0, 16, 16, 32],
-          [16, 0, 32, 16],
-          [0, 0, 0, 0],  # begin paddings
-          [0, 0, 0, 0],
-          [0, 0, 0, 0]]], dtype=tf.float32)
-
-    # second_stage_localization_loss should only be computed for predictions
-    # that match groundtruth. For multiple label groundtruth boxes, the loss
-    # should only be computed once for the label with the smaller index.
-    refined_box_encodings = tf.constant(
-        [[[0, 0, 0, 0], [1, 1, -1, -1]],
-         [[1, 1, -1, -1], [1, 1, 1, 1]],
-         [[1, 1, -1, -1], [1, 1, 1, 1]],
-         [[1, 1, -1, -1], [1, 1, 1, 1]],
-         [[1, 1, -1, -1], [1, 1, 1, 1]],
-         [[1, 1, -1, -1], [1, 1, 1, 1]]], dtype=tf.float32)
-    class_predictions_with_background = tf.constant(
-        [[-100, 100, 100],
-         [100, -100, -100],
-         [100, -100, -100],
-         [0, 0, 0],  # begin paddings
-         [0, 0, 0],
-         [0, 0, 0]], dtype=tf.float32)
-
-    mask_predictions_logits = 20 * tf.ones((batch_size *
-                                            model.max_num_proposals,
-                                            model.num_classes,
-                                            14, 14),
-                                           dtype=tf.float32)
-
-    groundtruth_boxes_list = [
-        tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]
-    # Box contains two ground truth labels.
-    groundtruth_classes_list = [tf.constant([[1, 1]], dtype=tf.float32)]
-
-    # Set all elements of groundtruth mask to 1.0. In this case all proposal
-    # crops of the groundtruth masks should return a mask that covers the entire
-    # proposal. Thus, if mask_predictions_logits element values are all greater
-    # than 20, the loss should be zero.
-    groundtruth_masks_list = [tf.convert_to_tensor(np.ones((1, 32, 32)),
-                                                   dtype=tf.float32)]
-
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors,
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'proposal_boxes': proposal_boxes,
-        'num_proposals': num_proposals,
-        'mask_predictions': mask_predictions_logits
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list,
-                              groundtruth_masks_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/RPNLoss/objectness_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/localization_loss'], 0)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/classification_loss'], 0)
-      self.assertAllClose(loss_dict_out['Loss/BoxClassifierLoss/mask_loss'], 0)
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors,
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'proposal_boxes': proposal_boxes,
+          'num_proposals': num_proposals,
+          'mask_predictions': mask_predictions_logits
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list,
+                                groundtruth_masks_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
+      return (loss_dict['Loss/RPNLoss/localization_loss'],
+              loss_dict['Loss/RPNLoss/objectness_loss'],
+              loss_dict['Loss/BoxClassifierLoss/localization_loss'],
+              loss_dict['Loss/BoxClassifierLoss/classification_loss'],
+              loss_dict['Loss/BoxClassifierLoss/mask_loss'])
+    (rpn_loc_loss, rpn_obj_loss, box_loc_loss, box_cls_loss,
+     box_mask_loss) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllClose(rpn_loc_loss, 0)
+    self.assertAllClose(rpn_obj_loss, 0)
+    self.assertAllClose(box_loc_loss, 0)
+    self.assertAllClose(box_cls_loss, 0)
+    self.assertAllClose(box_mask_loss, 0)
 
   @parameterized.parameters(
-      {'use_static_shapes': False, 'shared_boxes': False, 'use_keras': True},
-      {'use_static_shapes': False, 'shared_boxes': True, 'use_keras': True},
-      {'use_static_shapes': True, 'shared_boxes': False, 'use_keras': True},
-      {'use_static_shapes': True, 'shared_boxes': True, 'use_keras': True},
-      {'use_static_shapes': False, 'shared_boxes': False, 'use_keras': False},
-      {'use_static_shapes': False, 'shared_boxes': True, 'use_keras': False},
-      {'use_static_shapes': True, 'shared_boxes': False, 'use_keras': False},
-      {'use_static_shapes': True, 'shared_boxes': True, 'use_keras': False},
+      {'use_static_shapes': False, 'shared_boxes': False},
+      {'use_static_shapes': False, 'shared_boxes': True},
+      {'use_static_shapes': True, 'shared_boxes': False},
+      {'use_static_shapes': True, 'shared_boxes': True},
   )
   def test_loss_full_zero_padded_proposals_nonzero_loss_with_two_images(
-      self, use_static_shapes=False, shared_boxes=False, use_keras=False):
+      self, use_static_shapes=False, shared_boxes=False):
     batch_size = 2
     first_stage_max_proposals = 8
     second_stage_batch_size = 6
     num_classes = 2
-    def graph_fn(anchors, rpn_box_encodings,
-                 rpn_objectness_predictions_with_background, images,
-                 num_proposals, proposal_boxes, refined_box_encodings,
-                 class_predictions_with_background, groundtruth_boxes,
-                 groundtruth_classes):
-      """Function to construct tf graph for the test."""
+    with test_utils.GraphContextOrNone() as g:
       model = self._build_model(
-          is_training=True, use_keras=use_keras,
+          is_training=True,
           number_of_stages=2,
           second_stage_batch_size=second_stage_batch_size,
           first_stage_max_proposals=first_stage_max_proposals,
@@ -1643,6 +1568,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           clip_anchors_to_image=use_static_shapes,
           use_static_shapes=use_static_shapes)
 
+    def graph_fn(anchors, rpn_box_encodings,
+                 rpn_objectness_predictions_with_background, images,
+                 num_proposals, proposal_boxes, refined_box_encodings,
+                 class_predictions_with_background, groundtruth_boxes,
+                 groundtruth_classes):
+      """Function to construct tf graph for the test."""
       prediction_dict = {
           'rpn_box_encodings': rpn_box_encodings,
           'rpn_objectness_predictions_with_background':
@@ -1741,7 +1672,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
         images, num_proposals, proposal_boxes, refined_box_encodings,
         class_predictions_with_background, groundtruth_boxes,
         groundtruth_classes
-    ])
+    ], graph=g)
 
     exp_loc_loss = (-5 * np.log(.8) - 0.5) / 3.0
 
@@ -1750,231 +1681,222 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     self.assertAllClose(results[2], exp_loc_loss, rtol=1e-4, atol=1e-4)
     self.assertAllClose(results[3], 0.0)
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_with_hard_mining(self, use_keras=False):
-    model = self._build_model(is_training=True,
-                              use_keras=use_keras,
-                              number_of_stages=2,
-                              second_stage_batch_size=None,
-                              first_stage_max_proposals=6,
-                              hard_mining=True)
+  def test_loss_with_hard_mining(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(is_training=True,
+                                number_of_stages=2,
+                                second_stage_batch_size=None,
+                                first_stage_max_proposals=6,
+                                hard_mining=True)
     batch_size = 1
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant(
-        [[[-10, 13],
-          [-10, 12],
-          [10, -11],
-          [10, -12]]], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
-
-    # box_classifier_batch_size is 6, but here we assume that the number of
-    # actual proposals (not counting zero paddings) is fewer (3).
-    num_proposals = tf.constant([3], dtype=tf.int32)
-    proposal_boxes = tf.constant(
-        [[[0, 0, 16, 16],
-          [0, 16, 16, 32],
-          [16, 0, 32, 16],
-          [0, 0, 0, 0],  # begin paddings
-          [0, 0, 0, 0],
-          [0, 0, 0, 0]]], dtype=tf.float32)
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant(
+          [[[-10, 13],
+            [-10, 12],
+            [10, -11],
+            [10, -12]]], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+
+      # box_classifier_batch_size is 6, but here we assume that the number of
+      # actual proposals (not counting zero paddings) is fewer (3).
+      num_proposals = tf.constant([3], dtype=tf.int32)
+      proposal_boxes = tf.constant(
+          [[[0, 0, 16, 16],
+            [0, 16, 16, 32],
+            [16, 0, 32, 16],
+            [0, 0, 0, 0],  # begin paddings
+            [0, 0, 0, 0],
+            [0, 0, 0, 0]]], dtype=tf.float32)
+
+      refined_box_encodings = tf.zeros(
+          (batch_size * model.max_num_proposals,
+           model.num_classes,
+           BOX_CODE_SIZE), dtype=tf.float32)
+      class_predictions_with_background = tf.constant(
+          [[-10, 10, -10],  # first image
+           [-10, -10, 10],
+           [10, -10, -10],
+           [0, 0, 0],  # begin paddings
+           [0, 0, 0],
+           [0, 0, 0]], dtype=tf.float32)
+
+      # The first groundtruth box is 4/5 of the anchor size in both directions
+      # experiencing a loss of:
+      # 2 * SmoothL1(5 * log(4/5)) / num_proposals
+      #   = 2 * (abs(5 * log(1/2)) - .5) / 3
+      # The second groundtruth box is 46/50 of the anchor size in both
+      # directions experiencing a loss of:
+      # 2 * SmoothL1(5 * log(42/50)) / num_proposals
+      #   = 2 * (.5(5 * log(.92))^2 - .5) / 3.
+      # Since the first groundtruth box experiences greater loss, and we have
+      # set num_hard_examples=1 in the HardMiner, the final localization loss
+      # corresponds to that of the first groundtruth box.
+      groundtruth_boxes_list = [
+          tf.constant([[0.05, 0.05, 0.45, 0.45],
+                       [0.02, 0.52, 0.48, 0.98],], dtype=tf.float32)]
+      groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]],
+                                              dtype=tf.float32)]
 
-    refined_box_encodings = tf.zeros(
-        (batch_size * model.max_num_proposals,
-         model.num_classes,
-         BOX_CODE_SIZE), dtype=tf.float32)
-    class_predictions_with_background = tf.constant(
-        [[-10, 10, -10],  # first image
-         [-10, -10, 10],
-         [10, -10, -10],
-         [0, 0, 0],  # begin paddings
-         [0, 0, 0],
-         [0, 0, 0]], dtype=tf.float32)
-
-    # The first groundtruth box is 4/5 of the anchor size in both directions
-    # experiencing a loss of:
-    # 2 * SmoothL1(5 * log(4/5)) / num_proposals
-    #   = 2 * (abs(5 * log(1/2)) - .5) / 3
-    # The second groundtruth box is 46/50 of the anchor size in both directions
-    # experiencing a loss of:
-    # 2 * SmoothL1(5 * log(42/50)) / num_proposals
-    #   = 2 * (.5(5 * log(.92))^2 - .5) / 3.
-    # Since the first groundtruth box experiences greater loss, and we have
-    # set num_hard_examples=1 in the HardMiner, the final localization loss
-    # corresponds to that of the first groundtruth box.
-    groundtruth_boxes_list = [
-        tf.constant([[0.05, 0.05, 0.45, 0.45],
-                     [0.02, 0.52, 0.48, 0.98],], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32)]
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors,
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'proposal_boxes': proposal_boxes,
+          'num_proposals': num_proposals
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
+      return (loss_dict['Loss/BoxClassifierLoss/localization_loss'],
+              loss_dict['Loss/BoxClassifierLoss/classification_loss'])
+    loc_loss, cls_loss = self.execute_cpu(graph_fn, [], graph=g)
     exp_loc_loss = 2 * (-5 * np.log(.8) - 0.5) / 3.0
-
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors,
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'proposal_boxes': proposal_boxes,
-        'num_proposals': num_proposals
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/localization_loss'], exp_loc_loss)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/classification_loss'], 0)
-
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_loss_with_hard_mining_and_losses_mask(self, use_keras=False):
-    model = self._build_model(is_training=True,
-                              use_keras=use_keras,
-                              number_of_stages=2,
-                              second_stage_batch_size=None,
-                              first_stage_max_proposals=6,
-                              hard_mining=True)
+    self.assertAllClose(loc_loss, exp_loc_loss)
+    self.assertAllClose(cls_loss, 0)
+
+  def test_loss_with_hard_mining_and_losses_mask(self):
+    with test_utils.GraphContextOrNone() as g:
+      model = self._build_model(is_training=True,
+                                number_of_stages=2,
+                                second_stage_batch_size=None,
+                                first_stage_max_proposals=6,
+                                hard_mining=True)
     batch_size = 2
     number_of_proposals = 3
-    anchors = tf.constant(
-        [[0, 0, 16, 16],
-         [0, 16, 16, 32],
-         [16, 0, 32, 16],
-         [16, 16, 32, 32]], dtype=tf.float32)
-    rpn_box_encodings = tf.zeros(
-        [batch_size,
-         anchors.get_shape().as_list()[0],
-         BOX_CODE_SIZE], dtype=tf.float32)
-    # use different numbers for the objectness category to break ties in
-    # order of boxes returned by NMS
-    rpn_objectness_predictions_with_background = tf.constant(
-        [[[-10, 13],
-          [-10, 12],
-          [10, -11],
-          [10, -12]],
-         [[-10, 13],
-          [-10, 12],
-          [10, -11],
-          [10, -12]]], dtype=tf.float32)
-    image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
-
-    # box_classifier_batch_size is 6, but here we assume that the number of
-    # actual proposals (not counting zero paddings) is fewer (3).
-    num_proposals = tf.constant([number_of_proposals, number_of_proposals],
-                                dtype=tf.int32)
-    proposal_boxes = tf.constant(
-        [[[0, 0, 16, 16],  # first image
-          [0, 16, 16, 32],
-          [16, 0, 32, 16],
-          [0, 0, 0, 0],  # begin paddings
-          [0, 0, 0, 0],
-          [0, 0, 0, 0]],
-         [[0, 0, 16, 16],  # second image
-          [0, 16, 16, 32],
-          [16, 0, 32, 16],
-          [0, 0, 0, 0],  # begin paddings
-          [0, 0, 0, 0],
-          [0, 0, 0, 0]]], dtype=tf.float32)
-
-    refined_box_encodings = tf.zeros(
-        (batch_size * model.max_num_proposals,
-         model.num_classes,
-         BOX_CODE_SIZE), dtype=tf.float32)
-    class_predictions_with_background = tf.constant(
-        [[-10, 10, -10],  # first image
-         [-10, -10, 10],
-         [10, -10, -10],
-         [0, 0, 0],  # begin paddings
-         [0, 0, 0],
-         [0, 0, 0],
-         [-10, 10, -10],  # second image
-         [-10, -10, 10],
-         [10, -10, -10],
-         [0, 0, 0],  # begin paddings
-         [0, 0, 0],
-         [0, 0, 0]], dtype=tf.float32)
+    def graph_fn():
+      """A function with TF compute."""
+      anchors = tf.constant(
+          [[0, 0, 16, 16],
+           [0, 16, 16, 32],
+           [16, 0, 32, 16],
+           [16, 16, 32, 32]], dtype=tf.float32)
+      rpn_box_encodings = tf.zeros(
+          [batch_size,
+           anchors.get_shape().as_list()[0],
+           BOX_CODE_SIZE], dtype=tf.float32)
+      # use different numbers for the objectness category to break ties in
+      # order of boxes returned by NMS
+      rpn_objectness_predictions_with_background = tf.constant(
+          [[[-10, 13],
+            [-10, 12],
+            [10, -11],
+            [10, -12]],
+           [[-10, 13],
+            [-10, 12],
+            [10, -11],
+            [10, -12]]], dtype=tf.float32)
+      image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+
+      # box_classifier_batch_size is 6, but here we assume that the number of
+      # actual proposals (not counting zero paddings) is fewer (3).
+      num_proposals = tf.constant([number_of_proposals, number_of_proposals],
+                                  dtype=tf.int32)
+      proposal_boxes = tf.constant(
+          [[[0, 0, 16, 16],  # first image
+            [0, 16, 16, 32],
+            [16, 0, 32, 16],
+            [0, 0, 0, 0],  # begin paddings
+            [0, 0, 0, 0],
+            [0, 0, 0, 0]],
+           [[0, 0, 16, 16],  # second image
+            [0, 16, 16, 32],
+            [16, 0, 32, 16],
+            [0, 0, 0, 0],  # begin paddings
+            [0, 0, 0, 0],
+            [0, 0, 0, 0]]], dtype=tf.float32)
+
+      refined_box_encodings = tf.zeros(
+          (batch_size * model.max_num_proposals,
+           model.num_classes,
+           BOX_CODE_SIZE), dtype=tf.float32)
+      class_predictions_with_background = tf.constant(
+          [[-10, 10, -10],  # first image
+           [-10, -10, 10],
+           [10, -10, -10],
+           [0, 0, 0],  # begin paddings
+           [0, 0, 0],
+           [0, 0, 0],
+           [-10, 10, -10],  # second image
+           [-10, -10, 10],
+           [10, -10, -10],
+           [0, 0, 0],  # begin paddings
+           [0, 0, 0],
+           [0, 0, 0]], dtype=tf.float32)
+
+      # The first groundtruth box is 4/5 of the anchor size in both directions
+      # experiencing a loss of:
+      # 2 * SmoothL1(5 * log(4/5)) / (num_proposals * batch_size)
+      #   = 2 * (abs(5 * log(1/2)) - .5) / 3
+      # The second groundtruth box is 46/50 of the anchor size in both
+      # directions experiencing a loss of:
+      # 2 * SmoothL1(5 * log(42/50)) / (num_proposals * batch_size)
+      #   = 2 * (.5(5 * log(.92))^2 - .5) / 3.
+      # Since the first groundtruth box experiences greater loss, and we have
+      # set num_hard_examples=1 in the HardMiner, the final localization loss
+      # corresponds to that of the first groundtruth box.
+      groundtruth_boxes_list = [
+          tf.constant([[0.05, 0.05, 0.45, 0.45],
+                       [0.02, 0.52, 0.48, 0.98]], dtype=tf.float32),
+          tf.constant([[0.05, 0.05, 0.45, 0.45],
+                       [0.02, 0.52, 0.48, 0.98]], dtype=tf.float32)]
+      groundtruth_classes_list = [
+          tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
+          tf.constant([[1, 0], [0, 1]], dtype=tf.float32)]
+      is_annotated_list = [tf.constant(True, dtype=tf.bool),
+                           tf.constant(False, dtype=tf.bool)]
 
-    # The first groundtruth box is 4/5 of the anchor size in both directions
-    # experiencing a loss of:
-    # 2 * SmoothL1(5 * log(4/5)) / (num_proposals * batch_size)
-    #   = 2 * (abs(5 * log(1/2)) - .5) / 3
-    # The second groundtruth box is 46/50 of the anchor size in both directions
-    # experiencing a loss of:
-    # 2 * SmoothL1(5 * log(42/50)) / (num_proposals * batch_size)
-    #   = 2 * (.5(5 * log(.92))^2 - .5) / 3.
-    # Since the first groundtruth box experiences greater loss, and we have
-    # set num_hard_examples=1 in the HardMiner, the final localization loss
-    # corresponds to that of the first groundtruth box.
-    groundtruth_boxes_list = [
-        tf.constant([[0.05, 0.05, 0.45, 0.45],
-                     [0.02, 0.52, 0.48, 0.98]], dtype=tf.float32),
-        tf.constant([[0.05, 0.05, 0.45, 0.45],
-                     [0.02, 0.52, 0.48, 0.98]], dtype=tf.float32)]
-    groundtruth_classes_list = [
-        tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
-        tf.constant([[1, 0], [0, 1]], dtype=tf.float32)]
-    is_annotated_list = [tf.constant(True, dtype=tf.bool),
-                         tf.constant(False, dtype=tf.bool)]
+      prediction_dict = {
+          'rpn_box_encodings': rpn_box_encodings,
+          'rpn_objectness_predictions_with_background':
+          rpn_objectness_predictions_with_background,
+          'image_shape': image_shape,
+          'anchors': anchors,
+          'refined_box_encodings': refined_box_encodings,
+          'class_predictions_with_background':
+              class_predictions_with_background,
+          'proposal_boxes': proposal_boxes,
+          'num_proposals': num_proposals
+      }
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list,
+                                is_annotated_list=is_annotated_list)
+      loss_dict = model.loss(prediction_dict, true_image_shapes)
+      return (loss_dict['Loss/BoxClassifierLoss/localization_loss'],
+              loss_dict['Loss/BoxClassifierLoss/classification_loss'])
     exp_loc_loss = (2 * (-5 * np.log(.8) - 0.5) /
                     (number_of_proposals * batch_size))
+    loc_loss, cls_loss = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllClose(loc_loss, exp_loc_loss)
+    self.assertAllClose(cls_loss, 0)
 
-    prediction_dict = {
-        'rpn_box_encodings': rpn_box_encodings,
-        'rpn_objectness_predictions_with_background':
-        rpn_objectness_predictions_with_background,
-        'image_shape': image_shape,
-        'anchors': anchors,
-        'refined_box_encodings': refined_box_encodings,
-        'class_predictions_with_background': class_predictions_with_background,
-        'proposal_boxes': proposal_boxes,
-        'num_proposals': num_proposals
-    }
-    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
-    model.provide_groundtruth(groundtruth_boxes_list,
-                              groundtruth_classes_list,
-                              is_annotated_list=is_annotated_list)
-    loss_dict = model.loss(prediction_dict, true_image_shapes)
-
-    with self.test_session() as sess:
-      loss_dict_out = sess.run(loss_dict)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/localization_loss'], exp_loc_loss)
-      self.assertAllClose(loss_dict_out[
-          'Loss/BoxClassifierLoss/classification_loss'], 0)
-
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_restore_map_for_classification_ckpt(self, use_keras=False):
+  def test_restore_map_for_classification_ckpt(self):
+    if tf_version.is_tf2(): self.skipTest('Skipping TF1 only test.')
     # Define mock tensorflow classification graph and save variables.
     test_graph_classification = tf.Graph()
     with test_graph_classification.as_default():
       image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
       with tf.variable_scope('mock_model'):
-        net = contrib_slim.conv2d(
-            image, num_outputs=3, kernel_size=1, scope='layer1')
-        contrib_slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
+        net = slim.conv2d(image, num_outputs=3, kernel_size=1, scope='layer1')
+        slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
@@ -1988,7 +1910,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
       model = self._build_model(
-          is_training=False, use_keras=use_keras,
+          is_training=False,
           number_of_stages=2, second_stage_batch_size=6)
 
       inputs_shape = (2, 20, 20, 3)
@@ -2006,16 +1928,14 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           self.assertNotIn(model.first_stage_feature_extractor_scope, var)
           self.assertNotIn(model.second_stage_feature_extractor_scope, var)
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_restore_map_for_detection_ckpt(self, use_keras=False):
+  def test_restore_map_for_detection_ckpt(self):
+    if tf_version.is_tf2(): self.skipTest('Skipping TF1 only test.')
+    # Define mock tensorflow classification graph and save variables.
     # Define first detection graph and save variables.
     test_graph_detection1 = tf.Graph()
     with test_graph_detection1.as_default():
       model = self._build_model(
-          is_training=False, use_keras=use_keras,
+          is_training=False,
           number_of_stages=2, second_stage_batch_size=6)
       inputs_shape = (2, 20, 20, 3)
       inputs = tf.cast(tf.random_uniform(
@@ -2034,7 +1954,7 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
     # Define second detection graph and restore variables.
     test_graph_detection2 = tf.Graph()
     with test_graph_detection2.as_default():
-      model2 = self._build_model(is_training=False, use_keras=use_keras,
+      model2 = self._build_model(is_training=False,
                                  number_of_stages=2,
                                  second_stage_batch_size=6, num_classes=42)
 
@@ -2059,16 +1979,12 @@ class FasterRCNNMetaArchTestBase(test_case.TestCase, parameterized.TestCase):
           self.assertNotIn(
               six.b(model2.second_stage_feature_extractor_scope), var)
 
-  @parameterized.parameters(
-      {'use_keras': True},
-      {'use_keras': False}
-  )
-  def test_load_all_det_checkpoint_vars(self, use_keras=False):
+  def test_load_all_det_checkpoint_vars(self):
+    if tf_version.is_tf2(): self.skipTest('Skipping TF1 only test.')
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
       model = self._build_model(
           is_training=False,
-          use_keras=use_keras,
           number_of_stages=2,
           second_stage_batch_size=6,
           num_classes=42)
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index bd692cd4..1228a4b9 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -38,7 +38,7 @@ for an example.
 See notes in the documentation of Faster R-CNN meta-architecture as they all
 apply here.
 """
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_predictor
 from object_detection.meta_architectures import faster_rcnn_meta_arch
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch_test.py b/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
index d99782cc..9e279bdf 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.meta_architectures.rfcn_meta_arch."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch_test_lib
 from object_detection.meta_architectures import rfcn_meta_arch
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 4b518135..d401b0de 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -18,7 +18,7 @@ General tensorflow implementation of convolutional Multibox/SSD detection
 models.
 """
 import abc
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.python.util.deprecation import deprecated_args
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
@@ -34,7 +34,7 @@ from object_detection.utils import visualization_utils
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -601,10 +601,10 @@ class SSDMetaArch(model.DetectionModel):
     if self._feature_extractor.is_keras_model:
       feature_maps = self._feature_extractor(preprocessed_inputs)
     else:
-      with contrib_slim.arg_scope(
-          [contrib_slim.batch_norm],
-          is_training=(self._is_training and not self._freeze_batchnorm),
-          updates_collections=batchnorm_updates_collections):
+      with slim.arg_scope([slim.batch_norm],
+                          is_training=(self._is_training and
+                                       not self._freeze_batchnorm),
+                          updates_collections=batchnorm_updates_collections):
         with tf.variable_scope(None, self._extract_features_scope,
                                [preprocessed_inputs]):
           feature_maps = self._feature_extractor.extract_features(
@@ -622,10 +622,10 @@ class SSDMetaArch(model.DetectionModel):
     if self._box_predictor.is_keras_model:
       predictor_results_dict = self._box_predictor(feature_maps)
     else:
-      with contrib_slim.arg_scope(
-          [contrib_slim.batch_norm],
-          is_training=(self._is_training and not self._freeze_batchnorm),
-          updates_collections=batchnorm_updates_collections):
+      with slim.arg_scope([slim.batch_norm],
+                          is_training=(self._is_training and
+                                       not self._freeze_batchnorm),
+                          updates_collections=batchnorm_updates_collections):
         predictor_results_dict = self._box_predictor.predict(
             feature_maps, self._anchor_generator.num_anchors_per_location())
     predictions_dict = {
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 8eef1ee7..585eb177 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -25,7 +25,7 @@ from absl.testing import parameterized
 import numpy as np
 import six
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch_test_lib
@@ -34,7 +34,7 @@ from object_detection.utils import test_utils
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -43,10 +43,6 @@ except ImportError:
 keras = tf.keras.layers
 
 
-@parameterized.parameters(
-    {'use_keras': False},
-    {'use_keras': True},
-)
 class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                       parameterized.TestCase):
 
@@ -59,7 +55,6 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       expected_loss_weights=model_pb2.DetectionModel().ssd.loss.NONE,
       min_num_negative_samples=1,
       desired_negative_sampling_ratio=3,
-      use_keras=False,
       predict_mask=False,
       use_static_shapes=False,
       nms_max_size_per_class=5,
@@ -74,7 +69,6 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         expected_loss_weights=expected_loss_weights,
         min_num_negative_samples=min_num_negative_samples,
         desired_negative_sampling_ratio=desired_negative_sampling_ratio,
-        use_keras=use_keras,
         predict_mask=predict_mask,
         use_static_shapes=use_static_shapes,
         nms_max_size_per_class=nms_max_size_per_class,
@@ -82,82 +76,61 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         return_raw_detections_during_predict=(
             return_raw_detections_during_predict))
 
-  def test_preprocess_preserves_shapes_with_dynamic_input_image(
-      self, use_keras):
-    image_shapes = [(3, None, None, 3),
-                    (None, 10, 10, 3),
-                    (None, None, None, 3)]
-    model, _, _, _ = self._create_model(use_keras=use_keras)
-    for image_shape in image_shapes:
-      image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
-      preprocessed_inputs, _ = model.preprocess(image_placeholder)
-      self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
-
-  def test_preprocess_preserves_shape_with_static_input_image(self, use_keras):
-    def graph_fn(input_image):
-      model, _, _, _ = self._create_model(use_keras=use_keras)
-      return model.preprocess(input_image)
-    input_image = np.random.rand(2, 3, 3, 3).astype(np.float32)
-    preprocessed_inputs, _ = self.execute(graph_fn, [input_image])
-    self.assertAllEqual(preprocessed_inputs.shape, [2, 3, 3, 3])
-
-  def test_predict_result_shapes_on_image_with_dynamic_shape(self, use_keras):
-    batch_size = 3
-    image_size = 2
-    input_shapes = [(None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, num_classes, num_anchors, code_size = self._create_model(
-            use_keras=use_keras)
-        preprocessed_input_placeholder = tf.placeholder(tf.float32,
-                                                        shape=input_shape)
-        prediction_dict = model.predict(
-            preprocessed_input_placeholder, true_image_shapes=None)
-
-        self.assertIn('box_encodings', prediction_dict)
-        self.assertIn('class_predictions_with_background', prediction_dict)
-        self.assertIn('feature_maps', prediction_dict)
-        self.assertIn('anchors', prediction_dict)
-        self.assertIn('final_anchors', prediction_dict)
-
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        prediction_out = sess.run(prediction_dict,
-                                  feed_dict={
-                                      preprocessed_input_placeholder:
-                                      np.random.uniform(
-                                          size=(batch_size, 2, 2, 3))})
-      expected_box_encodings_shape_out = (batch_size, num_anchors, code_size)
-      expected_class_predictions_with_background_shape_out = (batch_size,
-                                                              num_anchors,
-                                                              num_classes + 1)
-
-      self.assertAllEqual(prediction_out['box_encodings'].shape,
-                          expected_box_encodings_shape_out)
-      self.assertAllEqual(prediction_out['final_anchors'].shape,
-                          (batch_size, num_anchors, 4))
-      self.assertAllEqual(
-          prediction_out['class_predictions_with_background'].shape,
-          expected_class_predictions_with_background_shape_out)
-
-  def test_predict_result_shapes_on_image_with_static_shape(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, code_size = self._create_model(
-          use_keras=use_keras)
+  def test_preprocess_preserves_shapes_with_dynamic_input_image(self):
+    width = tf.random.uniform([], minval=5, maxval=10, dtype=tf.int32)
+    batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+    shape = tf.stack([batch, 5, width, 3])
+    image = tf.random.uniform(shape)
+    model, _, _, _ = self._create_model()
+    preprocessed_inputs, _ = model.preprocess(image)
+    self.assertTrue(
+        preprocessed_inputs.shape.is_compatible_with([None, 5, None, 3]))
+
+  def test_preprocess_preserves_shape_with_static_input_image(self):
+    image = tf.random.uniform([2, 3, 3, 3])
+    model, _, _, _ = self._create_model()
+    preprocessed_inputs, _ = model.preprocess(image)
+    self.assertTrue(preprocessed_inputs.shape.is_compatible_with([2, 3, 3, 3]))
+
+  def test_predict_result_shapes_on_image_with_dynamic_shape(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, code_size = self._create_model()
+
+    def graph_fn():
+      size = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      shape = tf.stack([batch, size, size, 3])
+      image = tf.random.uniform(shape)
+      prediction_dict = model.predict(image, true_image_shapes=None)
+      self.assertIn('box_encodings', prediction_dict)
+      self.assertIn('class_predictions_with_background', prediction_dict)
+      self.assertIn('feature_maps', prediction_dict)
+      self.assertIn('anchors', prediction_dict)
+      self.assertIn('final_anchors', prediction_dict)
+      return (prediction_dict['box_encodings'],
+              prediction_dict['final_anchors'],
+              prediction_dict['class_predictions_with_background'],
+              tf.constant(num_anchors), batch)
+    (box_encodings_out, final_anchors, class_predictions_with_background,
+     num_anchors, batch_size) = self.execute_cpu(graph_fn, [], graph=g)
+    self.assertAllEqual(box_encodings_out.shape,
+                        (batch_size, num_anchors, code_size))
+    self.assertAllEqual(final_anchors.shape,
+                        (batch_size, num_anchors, code_size))
+    self.assertAllEqual(
+        class_predictions_with_background.shape,
+        (batch_size, num_anchors, num_classes + 1))
+
+  def test_predict_result_shapes_on_image_with_static_shape(self):
+
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, code_size = self._create_model()
 
     def graph_fn(input_image):
-      model, _, _, _ = self._create_model()
       predictions = model.predict(input_image, true_image_shapes=None)
       return (predictions['box_encodings'],
               predictions['class_predictions_with_background'],
-              predictions['feature_maps'],
-              predictions['anchors'], predictions['final_anchors'])
+              predictions['final_anchors'])
     batch_size = 3
     image_size = 2
     channels = 3
@@ -166,26 +139,23 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     expected_box_encodings_shape = (batch_size, num_anchors, code_size)
     expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
     final_anchors_shape = (batch_size, num_anchors, 4)
-    (box_encodings, class_predictions, _, _, final_anchors) = self.execute(
-        graph_fn, [input_image])
+    (box_encodings, class_predictions, final_anchors) = self.execute(
+        graph_fn, [input_image], graph=g)
     self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
     self.assertAllEqual(class_predictions.shape,
                         expected_class_predictions_shape)
     self.assertAllEqual(final_anchors.shape, final_anchors_shape)
 
-  def test_predict_with_raw_output_fields(self, use_keras):
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, code_size = self._create_model(
-          use_keras=use_keras)
+  def test_predict_with_raw_output_fields(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, code_size = self._create_model(
+          return_raw_detections_during_predict=True)
 
     def graph_fn(input_image):
-      model, _, _, _ = self._create_model(
-          return_raw_detections_during_predict=True)
       predictions = model.predict(input_image, true_image_shapes=None)
       return (predictions['box_encodings'],
               predictions['class_predictions_with_background'],
-              predictions['feature_maps'],
-              predictions['anchors'], predictions['final_anchors'],
+              predictions['final_anchors'],
               predictions['raw_detection_boxes'],
               predictions['raw_detection_feature_map_indices'])
     batch_size = 3
@@ -197,9 +167,9 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
     final_anchors_shape = (batch_size, num_anchors, 4)
     expected_raw_detection_boxes_shape = (batch_size, num_anchors, 4)
-    (box_encodings, class_predictions, _, _, final_anchors, raw_detection_boxes,
+    (box_encodings, class_predictions, final_anchors, raw_detection_boxes,
      raw_detection_feature_map_indices) = self.execute(
-         graph_fn, [input_image])
+         graph_fn, [input_image], graph=g)
     self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
     self.assertAllEqual(class_predictions.shape,
                         expected_class_predictions_shape)
@@ -209,47 +179,52 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     self.assertAllEqual(raw_detection_feature_map_indices,
                         np.zeros((batch_size, num_anchors)))
 
-  def test_raw_detection_boxes_agree_predict_postprocess(self, use_keras):
-    batch_size = 2
-    image_size = 2
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, _, _, _ = self._create_model(
-            use_keras=use_keras, return_raw_detections_during_predict=True)
-        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
-        preprocessed_inputs, true_image_shapes = model.preprocess(
-            input_placeholder)
-        prediction_dict = model.predict(preprocessed_inputs,
-                                        true_image_shapes)
-        raw_detection_boxes_predict = prediction_dict['raw_detection_boxes']
-        detections = model.postprocess(prediction_dict, true_image_shapes)
-        raw_detection_boxes_postprocess = detections['raw_detection_boxes']
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        raw_detection_boxes_predict_out, raw_detection_boxes_postprocess_out = (
-            sess.run(
-                [raw_detection_boxes_predict, raw_detection_boxes_postprocess],
-                feed_dict={
-                    input_placeholder:
-                        np.random.uniform(size=(batch_size, 2, 2, 3))}))
+  def test_raw_detection_boxes_agree_predict_postprocess(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, _, _, _ = self._create_model(
+          return_raw_detections_during_predict=True)
+    def graph_fn():
+      size = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      shape = tf.stack([batch, size, size, 3])
+      image = tf.random.uniform(shape)
+      preprocessed_inputs, true_image_shapes = model.preprocess(
+          image)
+      prediction_dict = model.predict(preprocessed_inputs,
+                                      true_image_shapes)
+      raw_detection_boxes_predict = prediction_dict['raw_detection_boxes']
+      detections = model.postprocess(prediction_dict, true_image_shapes)
+      raw_detection_boxes_postprocess = detections['raw_detection_boxes']
+      return raw_detection_boxes_predict, raw_detection_boxes_postprocess
+    (raw_detection_boxes_predict_out,
+     raw_detection_boxes_postprocess_out) = self.execute_cpu(graph_fn, [],
+                                                             graph=g)
+    self.assertAllEqual(raw_detection_boxes_predict_out,
+                        raw_detection_boxes_postprocess_out)
 
-      self.assertAllEqual(raw_detection_boxes_predict_out,
-                          raw_detection_boxes_postprocess_out)
+  def test_postprocess_results_are_correct(self):
 
-  def test_postprocess_results_are_correct(self, use_keras):
-    batch_size = 2
-    image_size = 2
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
+    with test_utils.GraphContextOrNone() as g:
+      model, _, _, _ = self._create_model()
+
+    def graph_fn():
+      size = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      shape = tf.stack([batch, size, size, 3])
+      image = tf.random.uniform(shape)
+      preprocessed_inputs, true_image_shapes = model.preprocess(
+          image)
+      prediction_dict = model.predict(preprocessed_inputs,
+                                      true_image_shapes)
+      detections = model.postprocess(prediction_dict, true_image_shapes)
+      return [
+          batch, detections['detection_boxes'], detections['detection_scores'],
+          detections['detection_classes'],
+          detections['detection_multiclass_scores'],
+          detections['num_detections'], detections['raw_detection_boxes'],
+          detections['raw_detection_scores'],
+          detections['detection_anchor_indices']
+      ]
 
     expected_boxes = [
         [
@@ -274,64 +249,37 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
 
-    raw_detection_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
-                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
-                           [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
-                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
-    raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
-                            [[0, 0], [0, 0], [0, 0], [0, 0]]]
-    detection_anchor_indices_sets = [[0, 1, 2], [0, 1, 2]]
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, _, _, _ = self._create_model(use_keras=use_keras)
-        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
-        preprocessed_inputs, true_image_shapes = model.preprocess(
-            input_placeholder)
-        prediction_dict = model.predict(preprocessed_inputs,
-                                        true_image_shapes)
-        detections = model.postprocess(prediction_dict, true_image_shapes)
-        self.assertIn('detection_boxes', detections)
-        self.assertIn('detection_scores', detections)
-        self.assertIn('detection_multiclass_scores', detections)
-        self.assertIn('detection_classes', detections)
-        self.assertIn('num_detections', detections)
-        self.assertIn('raw_detection_boxes', detections)
-        self.assertIn('raw_detection_scores', detections)
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        detections_out = sess.run(detections,
-                                  feed_dict={
-                                      input_placeholder:
-                                      np.random.uniform(
-                                          size=(batch_size, 2, 2, 3))})
-      for image_idx in range(batch_size):
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(
-                detections_out['detection_boxes'][image_idx].tolist(),
-                expected_boxes[image_idx]))
-      self.assertAllClose(detections_out['detection_scores'], expected_scores)
-      self.assertAllClose(detections_out['detection_classes'], expected_classes)
-      self.assertAllClose(detections_out['detection_multiclass_scores'],
-                          expected_multiclass_scores)
-      self.assertAllClose(detections_out['num_detections'],
-                          expected_num_detections)
-      self.assertAllEqual(detections_out['raw_detection_boxes'],
-                          raw_detection_boxes)
-      self.assertAllEqual(detections_out['raw_detection_scores'],
-                          raw_detection_scores)
-      for idx in range(batch_size):
-        self.assertSameElements(detections_out['detection_anchor_indices'][idx],
-                                detection_anchor_indices_sets[idx])
-
-  def test_postprocess_results_are_correct_static(self, use_keras):
-    with tf.Graph().as_default():
-      _, _, _, _ = self._create_model(use_keras=use_keras)
-    def graph_fn(input_image):
+    expected_raw_detection_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                     [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
+                                    [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
+                                     [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
+    expected_raw_detection_scores = [[[0, 0], [0, 0], [0, 0], [0, 0]],
+                                     [[0, 0], [0, 0], [0, 0], [0, 0]]]
+    expected_detection_anchor_indices = [[0, 1, 2], [0, 1, 2]]
+    (batch, detection_boxes, detection_scores, detection_classes,
+     detection_multiclass_scores, num_detections, raw_detection_boxes,
+     raw_detection_scores, detection_anchor_indices) = self.execute_cpu(
+         graph_fn, [], graph=g)
+    for image_idx in range(batch):
+      self.assertTrue(
+          test_utils.first_rows_close_as_set(
+              detection_boxes[image_idx].tolist(), expected_boxes[image_idx]))
+      self.assertSameElements(detection_anchor_indices[image_idx],
+                              expected_detection_anchor_indices[image_idx])
+    self.assertAllClose(detection_scores, expected_scores)
+    self.assertAllClose(detection_classes, expected_classes)
+    self.assertAllClose(detection_multiclass_scores, expected_multiclass_scores)
+    self.assertAllClose(num_detections, expected_num_detections)
+    self.assertAllEqual(raw_detection_boxes, expected_raw_detection_boxes)
+    self.assertAllEqual(raw_detection_scores,
+                        expected_raw_detection_scores)
+
+  def test_postprocess_results_are_correct_static(self):
+    with test_utils.GraphContextOrNone() as g:
       model, _, _, _ = self._create_model(use_static_shapes=True,
                                           nms_max_size_per_class=4)
+
+    def graph_fn(input_image):
       preprocessed_inputs, true_image_shapes = model.preprocess(input_image)
       prediction_dict = model.predict(preprocessed_inputs,
                                       true_image_shapes)
@@ -340,11 +288,6 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
               detections['detection_classes'], detections['num_detections'],
               detections['detection_multiclass_scores'])
 
-    batch_size = 2
-    image_size = 2
-    channels = 3
-    input_image = np.random.rand(batch_size, image_size, image_size,
-                                 channels).astype(np.float32)
     expected_boxes = [
         [
             [0, 0, .5, .5],
@@ -364,10 +307,15 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                   [[0, 0], [0, 0], [0, 0], [0, 0]]]
     expected_classes = [[0, 0, 0, 0], [0, 0, 0, 0]]
     expected_num_detections = np.array([3, 3])
-
+    batch_size = 2
+    image_size = 2
+    channels = 3
+    input_image = np.random.rand(batch_size, image_size, image_size,
+                                 channels).astype(np.float32)
     (detection_boxes, detection_scores, detection_classes,
      num_detections, detection_multiclass_scores) = self.execute(graph_fn,
-                                                                 [input_image])
+                                                                 [input_image],
+                                                                 graph=g)
     for image_idx in range(batch_size):
       self.assertTrue(test_utils.first_rows_close_as_set(
           detection_boxes[image_idx][
@@ -387,92 +335,43 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     self.assertAllClose(num_detections,
                         expected_num_detections)
 
-  def test_postprocess_results_are_correct_with_calibration(self, use_keras):
-    batch_size = 2
-    image_size = 2
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
-                    (batch_size, None, None, 3),
-                    (None, None, None, 3)]
-
-    expected_boxes = [
-        [
-            [0, 0, .5, .5],
-            [0, .5, .5, 1],
-            [.5, 0, 1, .5],
-            [0, 0, 0, 0],  # pruned prediction
-            [0, 0, 0, 0]
-        ],  # padding
-        [
-            [0, 0, .5, .5],
-            [0, .5, .5, 1],
-            [.5, 0, 1, .5],
-            [0, 0, 0, 0],  # pruned prediction
-            [0, 0, 0, 0]
-        ]
-    ]  # padding
+  def test_postprocess_results_are_correct_with_calibration(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, _, _, _ = self._create_model(calibration_mapping_value=0.5)
+
+    def graph_fn():
+      size = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      batch = tf.random.uniform([], minval=2, maxval=3, dtype=tf.int32)
+      shape = tf.stack([batch, size, size, 3])
+      image = tf.random.uniform(shape)
+      preprocessed_inputs, true_image_shapes = model.preprocess(
+          image)
+      prediction_dict = model.predict(preprocessed_inputs,
+                                      true_image_shapes)
+      detections = model.postprocess(prediction_dict, true_image_shapes)
+      return detections['detection_scores'], detections['raw_detection_scores']
     # Calibration mapping value below is set to map all scores to 0.5, except
     # for the last two detections in each batch (see expected number of
     # detections below.
     expected_scores = [[0.5, 0.5, 0.5, 0., 0.], [0.5, 0.5, 0.5, 0., 0.]]
-    expected_classes = [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]
-    expected_num_detections = np.array([3, 3])
-
-    raw_detection_boxes = [[[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
-                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]],
-                           [[0., 0., 0.5, 0.5], [0., 0.5, 0.5, 1.],
-                            [0.5, 0., 1., 0.5], [1., 1., 1.5, 1.5]]]
-    raw_detection_scores = [[[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]],
-                            [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]]
-
-    for input_shape in input_shapes:
-      tf_graph = tf.Graph()
-      with tf_graph.as_default():
-        model, _, _, _ = self._create_model(use_keras=use_keras,
-                                            calibration_mapping_value=0.5)
-        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
-        preprocessed_inputs, true_image_shapes = model.preprocess(
-            input_placeholder)
-        prediction_dict = model.predict(preprocessed_inputs,
-                                        true_image_shapes)
-        detections = model.postprocess(prediction_dict, true_image_shapes)
-        self.assertIn('detection_boxes', detections)
-        self.assertIn('detection_scores', detections)
-        self.assertIn('detection_classes', detections)
-        self.assertIn('num_detections', detections)
-        self.assertIn('raw_detection_boxes', detections)
-        self.assertIn('raw_detection_scores', detections)
-        init_op = tf.global_variables_initializer()
-      with self.test_session(graph=tf_graph) as sess:
-        sess.run(init_op)
-        detections_out = sess.run(detections,
-                                  feed_dict={
-                                      input_placeholder:
-                                      np.random.uniform(
-                                          size=(batch_size, 2, 2, 3))})
-      for image_idx in range(batch_size):
-        self.assertTrue(
-            test_utils.first_rows_close_as_set(
-                detections_out['detection_boxes'][image_idx].tolist(),
-                expected_boxes[image_idx]))
-      self.assertAllClose(detections_out['detection_scores'], expected_scores)
-      self.assertAllClose(detections_out['detection_classes'], expected_classes)
-      self.assertAllClose(detections_out['num_detections'],
-                          expected_num_detections)
-      self.assertAllEqual(detections_out['raw_detection_boxes'],
-                          raw_detection_boxes)
-      self.assertAllEqual(detections_out['raw_detection_scores'],
-                          raw_detection_scores)
-
-  def test_loss_results_are_correct(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
+    expected_raw_detection_scores = [
+        [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]],
+        [[0.5, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 0.5]]
+    ]
+    detection_scores, raw_detection_scores = self.execute_cpu(graph_fn, [],
+                                                              graph=g)
+    self.assertAllClose(detection_scores, expected_scores)
+    self.assertAllEqual(raw_detection_scores, expected_raw_detection_scores)
+
+  def test_loss_results_are_correct(self):
+
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, _ = self._create_model(
+          apply_hard_mining=False)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(apply_hard_mining=False)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(preprocessed_tensor,
@@ -482,37 +381,35 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                                'Loss/localization_loss'),
               self._get_value_for_matching_key(loss_dict,
                                                'Loss/classification_loss'))
-
     batch_size = 2
     preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
     groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
     groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
     groundtruth_classes1 = np.array([[1]], dtype=np.float32)
     groundtruth_classes2 = np.array([[1]], dtype=np.float32)
+    (localization_loss, classification_loss) = self.execute(
+        graph_fn, [
+            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
+            groundtruth_classes1, groundtruth_classes2
+        ],
+        graph=g)
+
     expected_localization_loss = 0.0
     expected_classification_loss = (batch_size * num_anchors
                                     * (num_classes+1) * np.log(2.0))
-    (localization_loss,
-     classification_loss) = self.execute(graph_fn, [preprocessed_input,
-                                                    groundtruth_boxes1,
-                                                    groundtruth_boxes2,
-                                                    groundtruth_classes1,
-                                                    groundtruth_classes2])
+
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_loss_results_are_correct_with_normalize_by_codesize_true(
-      self, use_keras):
+  def test_loss_results_are_correct_with_normalize_by_codesize_true(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, _, _, _ = self._create_model(
+          apply_hard_mining=False, normalize_loc_loss_by_codesize=True)
 
-    with tf.Graph().as_default():
-      _, _, _, _ = self._create_model(use_keras=use_keras)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(apply_hard_mining=False,
-                                          normalize_loc_loss_by_codesize=True,
-                                          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(preprocessed_tensor,
@@ -532,18 +429,16 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                                 groundtruth_boxes1,
                                                 groundtruth_boxes2,
                                                 groundtruth_classes1,
-                                                groundtruth_classes2])
+                                                groundtruth_classes2], graph=g)
     self.assertAllClose(localization_loss, expected_localization_loss)
 
-  def test_loss_results_are_correct_with_hard_example_mining(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
+  def test_loss_results_are_correct_with_hard_example_mining(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, _ = self._create_model()
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model()
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(preprocessed_tensor,
@@ -567,24 +462,20 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         graph_fn, [
             preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
             groundtruth_classes1, groundtruth_classes2
-        ])
+        ], graph=g)
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_loss_results_are_correct_without_add_background_class(
-      self, use_keras):
+  def test_loss_results_are_correct_without_add_background_class(self):
 
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(
-          add_background_class=False, use_keras=use_keras)
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, _ = self._create_model(
+          apply_hard_mining=False, add_background_class=False)
 
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(
-          apply_hard_mining=False, add_background_class=False,
-          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(
@@ -606,16 +497,16 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         graph_fn, [
             preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
             groundtruth_classes1, groundtruth_classes2
-        ])
+        ], graph=g)
 
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
 
-  def test_loss_results_are_correct_with_losses_mask(self, use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
+  def test_loss_results_are_correct_with_losses_mask(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, num_anchors, _ = self._create_model(
+          apply_hard_mining=False)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_boxes3, groundtruth_classes1, groundtruth_classes2,
                  groundtruth_classes3):
@@ -625,7 +516,6 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                   groundtruth_classes3]
       is_annotated_list = [tf.constant(True), tf.constant(True),
                            tf.constant(False)]
-      model, _, _, _ = self._create_model(apply_hard_mining=False)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list,
                                 is_annotated_list=is_annotated_list)
@@ -657,19 +547,22 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
                                                     groundtruth_boxes3,
                                                     groundtruth_classes1,
                                                     groundtruth_classes2,
-                                                    groundtruth_classes3])
+                                                    groundtruth_classes3],
+                                         graph=g)
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_restore_map_for_detection_ckpt(self, use_keras):
-    model, _, _, _ = self._create_model(use_keras=use_keras)
+  def test_restore_map_for_detection_ckpt(self):
+    # TODO(rathodv): Support TF2.X
+    if self.is_tf2(): return
+    model, _, _, _ = self._create_model()
     model.predict(tf.constant(np.array([[[[0, 0], [1, 1]], [[1, 0], [0, 1]]]],
                                        dtype=np.float32)),
                   true_image_shapes=None)
     init_op = tf.global_variables_initializer()
     saver = tf.train.Saver()
     save_path = self.get_temp_dir()
-    with self.test_session() as sess:
+    with self.session() as sess:
       sess.run(init_op)
       saved_model_path = saver.save(sess, save_path)
       var_map = model.restore_map(
@@ -681,27 +574,22 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       for var in sess.run(tf.report_uninitialized_variables()):
         self.assertNotIn('FeatureExtractor', var)
 
-  def test_restore_map_for_classification_ckpt(self, use_keras):
+  def test_restore_map_for_classification_ckpt(self):
+    # TODO(rathodv): Support TF2.X
+    if self.is_tf2(): return
     # Define mock tensorflow classification graph and save variables.
     test_graph_classification = tf.Graph()
     with test_graph_classification.as_default():
       image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
-      if use_keras:
-        with tf.name_scope('mock_model'):
-          layer_one = keras.Conv2D(32, kernel_size=1, name='layer1')
-          net = layer_one(image)
-          layer_two = keras.Conv2D(3, kernel_size=1, name='layer2')
-          layer_two(net)
-      else:
-        with tf.variable_scope('mock_model'):
-          net = contrib_slim.conv2d(
-              image, num_outputs=32, kernel_size=1, scope='layer1')
-          contrib_slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
+
+      with tf.variable_scope('mock_model'):
+        net = slim.conv2d(image, num_outputs=32, kernel_size=1, scope='layer1')
+        slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
       save_path = self.get_temp_dir()
-      with self.test_session(graph=test_graph_classification) as sess:
+      with self.session(graph=test_graph_classification) as sess:
         sess.run(init_op)
         saved_model_path = saver.save(sess, save_path)
 
@@ -709,7 +597,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
     # classification checkpoint.
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model(use_keras=use_keras)
+      model, _, _, _ = self._create_model()
       inputs_shape = [2, 2, 2, 3]
       inputs = tf.cast(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32), dtype=tf.float32)
@@ -721,15 +609,17 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       self.assertNotIn('another_variable', var_map)
       self.assertIsInstance(var_map, dict)
       saver = tf.train.Saver(var_map)
-      with self.test_session(graph=test_graph_detection) as sess:
+      with self.session(graph=test_graph_detection) as sess:
         saver.restore(sess, saved_model_path)
         for var in sess.run(tf.report_uninitialized_variables()):
           self.assertNotIn(six.ensure_binary('FeatureExtractor'), var)
 
-  def test_load_all_det_checkpoint_vars(self, use_keras):
+  def test_load_all_det_checkpoint_vars(self):
+    # TODO(rathodv): Support TF2.X
+    if self.is_tf2(): return
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model(use_keras=use_keras)
+      model, _, _, _ = self._create_model()
       inputs_shape = [2, 2, 2, 3]
       inputs = tf.cast(
           tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32),
@@ -744,20 +634,15 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
       self.assertIsInstance(var_map, dict)
       self.assertIn('another_variable', var_map)
 
-  def test_loss_results_are_correct_with_random_example_sampling(
-      self,
-      use_keras):
-
-    with tf.Graph().as_default():
-      _, num_classes, _, _ = self._create_model(
-          random_example_sampling=True, use_keras=use_keras)
+  def test_loss_results_are_correct_with_random_example_sampling(self):
+    with test_utils.GraphContextOrNone() as g:
+      model, num_classes, _, _ = self._create_model(
+          random_example_sampling=True)
 
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(random_example_sampling=True,
-                                          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(
@@ -785,7 +670,7 @@ class SsdMetaArchTest(ssd_meta_arch_test_lib.SSDMetaArchTestBase,
         graph_fn, [
             preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
             groundtruth_classes1, groundtruth_classes2
-        ])
+        ], graph=g)
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
index 1e9b1e84..0991388b 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test_lib.py
@@ -15,7 +15,7 @@
 """Helper functions for SSD models meta architecture tests."""
 
 import functools
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 
 from object_detection.builders import post_processing_builder
@@ -32,10 +32,11 @@ from object_detection.protos import model_pb2
 from object_detection.utils import ops
 from object_detection.utils import test_case
 from object_detection.utils import test_utils
+from object_detection.utils import tf_version
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import slim as contrib_slim
+  import tf_slim as slim
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -60,7 +61,7 @@ class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
 
   def extract_features(self, preprocessed_inputs):
     with tf.variable_scope('mock_model'):
-      features = contrib_slim.conv2d(
+      features = slim.conv2d(
           inputs=preprocessed_inputs,
           num_outputs=32,
           kernel_size=1,
@@ -132,7 +133,6 @@ class SSDMetaArchTestBase(test_case.TestCase):
       expected_loss_weights=model_pb2.DetectionModel().ssd.loss.NONE,
       min_num_negative_samples=1,
       desired_negative_sampling_ratio=3,
-      use_keras=False,
       predict_mask=False,
       use_static_shapes=False,
       nms_max_size_per_class=5,
@@ -141,6 +141,7 @@ class SSDMetaArchTestBase(test_case.TestCase):
     is_training = False
     num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
+    use_keras = tf_version.is_tf2()
     if use_keras:
       mock_box_predictor = test_utils.MockKerasBoxPredictor(
           is_training, num_classes, add_background_class=add_background_class)
@@ -256,7 +257,3 @@ class SSDMetaArchTestBase(test_case.TestCase):
       if key.endswith(suffix):
         return dictionary[key]
     raise ValueError('key not found {}'.format(suffix))
-
-
-if __name__ == '__main__':
-  tf.test.main()
diff --git a/research/object_detection/metrics/calibration_evaluation.py b/research/object_detection/metrics/calibration_evaluation.py
index 928c16ad..e3fc4b05 100644
--- a/research/object_detection/metrics/calibration_evaluation.py
+++ b/research/object_detection/metrics/calibration_evaluation.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_list
diff --git a/research/object_detection/metrics/calibration_evaluation_test.py b/research/object_detection/metrics/calibration_evaluation_test.py
index 422567e0..375978d8 100644
--- a/research/object_detection/metrics/calibration_evaluation_test.py
+++ b/research/object_detection/metrics/calibration_evaluation_test.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import standard_fields
 from object_detection.metrics import calibration_evaluation
 
diff --git a/research/object_detection/metrics/calibration_metrics.py b/research/object_detection/metrics/calibration_metrics.py
index a94f4600..611c81c3 100644
--- a/research/object_detection/metrics/calibration_metrics.py
+++ b/research/object_detection/metrics/calibration_metrics.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.python.ops import metrics_impl
 
 
diff --git a/research/object_detection/metrics/calibration_metrics_test.py b/research/object_detection/metrics/calibration_metrics_test.py
index 4518293c..54793fca 100644
--- a/research/object_detection/metrics/calibration_metrics_test.py
+++ b/research/object_detection/metrics/calibration_metrics_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.metrics import calibration_metrics
 
 
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
index bf2f0b1f..7a962457 100644
--- a/research/object_detection/metrics/coco_evaluation.py
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -19,7 +19,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.metrics import coco_tools
@@ -1096,27 +1096,24 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
                     for key, value in mask_metrics.items()}
     return mask_metrics
 
-  def get_estimator_eval_metric_ops(self, eval_dict):
-    """Returns a dictionary of eval metric ops.
+  def add_eval_dict(self, eval_dict):
+    """Observes an evaluation result dict for a single example.
 
-    Note that once value_op is called, the detections and groundtruth added via
-    update_op are cleared.
+    When executing eagerly, once all observations have been observed by this
+    method you can use `.evaluate()` to get the final metrics.
+
+    When using `tf.estimator.Estimator` for evaluation this function is used by
+    `get_estimator_eval_metric_ops()` to construct the metric update op.
 
     Args:
-      eval_dict: A dictionary that holds tensors for evaluating object detection
-        performance. For single-image evaluation, this dictionary may be
-        produced from eval_util.result_dict_for_single_example(). If multi-image
-        evaluation, `eval_dict` should contain the fields
-        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
-        properly unpad the tensors from the batch.
+      eval_dict: A dictionary that holds tensors for evaluating an object
+        detection model, returned from
+        eval_util.result_dict_for_single_example().
 
     Returns:
-      a dictionary of metric names to tuple of value_op and update_op that can
-      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
-      update ops  must be run together and similarly all value ops must be run
-      together to guarantee correct behaviour.
+      None when executing eagerly, or an update_op that can be used to update
+      the eval metrics in `tf.estimator.EstimatorSpec`.
     """
-
     def update_op(image_id_batched, groundtruth_boxes_batched,
                   groundtruth_classes_batched,
                   groundtruth_instance_masks_batched,
@@ -1203,13 +1200,34 @@ class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
             tf.shape(detection_scores)[1:2],
             multiples=tf.shape(detection_scores)[0:1])
 
-    update_op = tf.py_func(update_op, [
+    return tf.py_func(update_op, [
         image_id, groundtruth_boxes, groundtruth_classes,
         groundtruth_instance_masks, groundtruth_is_crowd,
         num_gt_boxes_per_image, detection_scores, detection_classes,
         detection_masks, num_det_boxes_per_image
     ], [])
 
+  def get_estimator_eval_metric_ops(self, eval_dict):
+    """Returns a dictionary of eval metric ops.
+
+    Note that once value_op is called, the detections and groundtruth added via
+    update_op are cleared.
+
+    Args:
+      eval_dict: A dictionary that holds tensors for evaluating object detection
+        performance. For single-image evaluation, this dictionary may be
+        produced from eval_util.result_dict_for_single_example(). If multi-image
+        evaluation, `eval_dict` should contain the fields
+        'num_groundtruth_boxes_per_image' and 'num_det_boxes_per_image' to
+        properly unpad the tensors from the batch.
+
+    Returns:
+      a dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in tf.estimator.EstimatorSpec. Note that all
+      update ops  must be run together and similarly all value ops must be run
+      together to guarantee correct behaviour.
+    """
+    update_op = self.add_eval_dict(eval_dict)
     metric_names = ['DetectionMasks_Precision/mAP',
                     'DetectionMasks_Precision/mAP@.50IOU',
                     'DetectionMasks_Precision/mAP@.75IOU',
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
index c23e44d6..aed6047f 100644
--- a/research/object_detection/metrics/coco_evaluation_test.py
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import standard_fields
 from object_detection.metrics import coco_evaluation
 
@@ -1440,6 +1440,66 @@ class CocoMaskEvaluationTest(tf.test.TestCase):
 
 class CocoMaskEvaluationPyFuncTest(tf.test.TestCase):
 
+  def testAddEvalDict(self):
+    coco_evaluator = coco_evaluation.CocoMaskEvaluator(_get_categories_list())
+    image_id = tf.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
+    groundtruth_masks = tf.placeholder(tf.uint8, shape=(None, None, None))
+    detection_scores = tf.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.placeholder(tf.float32, shape=(None))
+    detection_masks = tf.placeholder(tf.uint8, shape=(None, None, None))
+
+    input_data_fields = standard_fields.InputDataFields
+    detection_fields = standard_fields.DetectionResultFields
+    eval_dict = {
+        input_data_fields.key: image_id,
+        input_data_fields.groundtruth_boxes: groundtruth_boxes,
+        input_data_fields.groundtruth_classes: groundtruth_classes,
+        input_data_fields.groundtruth_instance_masks: groundtruth_masks,
+        detection_fields.detection_scores: detection_scores,
+        detection_fields.detection_classes: detection_classes,
+        detection_fields.detection_masks: detection_masks,
+    }
+    update_op = coco_evaluator.add_eval_dict(eval_dict)
+    with self.test_session() as sess:
+      sess.run(
+          update_op,
+          feed_dict={
+              image_id:
+                  'image1',
+              groundtruth_boxes:
+                  np.array([[100., 100., 200., 200.], [50., 50., 100., 100.]]),
+              groundtruth_classes:
+                  np.array([1, 2]),
+              groundtruth_masks:
+                  np.stack([
+                      np.pad(
+                          np.ones([100, 100], dtype=np.uint8), ((10, 10),
+                                                                (10, 10)),
+                          mode='constant'),
+                      np.pad(
+                          np.ones([50, 50], dtype=np.uint8), ((0, 70), (0, 70)),
+                          mode='constant')
+                  ]),
+              detection_scores:
+                  np.array([.9, .8]),
+              detection_classes:
+                  np.array([2, 1]),
+              detection_masks:
+                  np.stack([
+                      np.pad(
+                          np.ones([50, 50], dtype=np.uint8), ((0, 70), (0, 70)),
+                          mode='constant'),
+                      np.pad(
+                          np.ones([100, 100], dtype=np.uint8), ((10, 10),
+                                                                (10, 10)),
+                          mode='constant'),
+                  ])
+          })
+      self.assertLen(coco_evaluator._groundtruth_list, 2)
+      self.assertLen(coco_evaluator._detection_masks_list, 2)
+
   def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
     coco_evaluator = coco_evaluation.CocoMaskEvaluator(_get_categories_list())
     image_id = tf.placeholder(tf.string, shape=())
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
index 60b70f38..f2379f65 100644
--- a/research/object_detection/metrics/coco_tools.py
+++ b/research/object_detection/metrics/coco_tools.py
@@ -54,7 +54,7 @@ from pycocotools import mask
 
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import json_utils
 
diff --git a/research/object_detection/metrics/coco_tools_test.py b/research/object_detection/metrics/coco_tools_test.py
index 269147c2..f2c3ce0a 100644
--- a/research/object_detection/metrics/coco_tools_test.py
+++ b/research/object_detection/metrics/coco_tools_test.py
@@ -20,7 +20,7 @@ import numpy as np
 
 from pycocotools import mask
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.metrics import coco_tools
 
diff --git a/research/object_detection/metrics/offline_eval_map_corloc.py b/research/object_detection/metrics/offline_eval_map_corloc.py
index b5514be1..69ecaeaa 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc.py
@@ -34,7 +34,7 @@ Example usage:
 import csv
 import os
 import re
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.legacy import evaluator
diff --git a/research/object_detection/metrics/offline_eval_map_corloc_test.py b/research/object_detection/metrics/offline_eval_map_corloc_test.py
index 68ac3893..9641dfb2 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc_test.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Tests for utilities in offline_eval_map_corloc binary."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.metrics import offline_eval_map_corloc as offline_eval
 
diff --git a/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
index de68461c..94a1da03 100644
--- a/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
+++ b/research/object_detection/metrics/oid_challenge_evaluation_utils_test.py
@@ -25,7 +25,7 @@ import numpy as np
 import pandas as pd
 from pycocotools import mask as coco_mask
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.metrics import oid_challenge_evaluation_utils as utils
diff --git a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
index 73818288..04547bbe 100644
--- a/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
+++ b/research/object_detection/metrics/oid_vrd_challenge_evaluation_utils_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 import numpy as np
 import pandas as pd
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import standard_fields
 from object_detection.metrics import oid_vrd_challenge_evaluation_utils as utils
 from object_detection.utils import vrd_evaluation
diff --git a/research/object_detection/metrics/tf_example_parser_test.py b/research/object_detection/metrics/tf_example_parser_test.py
index 57792b69..c195c737 100644
--- a/research/object_detection/metrics/tf_example_parser_test.py
+++ b/research/object_detection/metrics/tf_example_parser_test.py
@@ -16,7 +16,7 @@
 
 import numpy as np
 import numpy.testing as np_testing
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields as fields
 from object_detection.metrics import tf_example_parser
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index ba0f0b4b..57912515 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -22,7 +22,9 @@ import copy
 import functools
 import os
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 
 from object_detection import eval_util
 from object_detection import exporter as exporter_lib
@@ -40,11 +42,8 @@ from object_detection.utils import visualization_utils as vis_utils
 
 # pylint: disable=g-import-not-at-top
 try:
-  from tensorflow.contrib import framework as contrib_framework
-  from tensorflow.contrib import layers as contrib_layers
   from tensorflow.contrib import learn as contrib_learn
   from tensorflow.contrib import tpu as contrib_tpu
-  from tensorflow.contrib import training as contrib_training
 except ImportError:
   # TF 2.0 doesn't ship with contrib.
   pass
@@ -95,6 +94,10 @@ def _prepare_groundtruth_for_eval(detection_model, class_agnostic,
         of groundtruth boxes per image..
       'groundtruth_keypoints': [batch_size, num_boxes, num_keypoints, 2] float32
         tensor of keypoints (if provided in groundtruth).
+      'groundtruth_group_of': [batch_size, num_boxes] bool tensor indicating
+        group_of annotations (if provided in groundtruth).
+      'groundtruth_labeled_classes': [batch_size, num_classes] int64
+        tensor of 1-indexed classes.
     class_agnostic: Boolean indicating whether detections are class agnostic.
   """
   input_data_fields = fields.InputDataFields()
@@ -138,6 +141,29 @@ def _prepare_groundtruth_for_eval(detection_model, class_agnostic,
         detection_model.groundtruth_lists(
             fields.BoxListFields.keypoint_visibilities))
 
+  if detection_model.groundtruth_has_field(fields.BoxListFields.group_of):
+    groundtruth[input_data_fields.groundtruth_group_of] = tf.stack(
+        detection_model.groundtruth_lists(fields.BoxListFields.group_of))
+
+  if detection_model.groundtruth_has_field(
+      fields.InputDataFields.groundtruth_labeled_classes):
+    labeled_classes_list = detection_model.groundtruth_lists(
+        fields.InputDataFields.groundtruth_labeled_classes)
+    labeled_classes = [
+        tf.where(x)[:, 0] + label_id_offset for x in labeled_classes_list
+    ]
+    if len(labeled_classes) > 1:
+      num_classes = labeled_classes_list[0].shape[0]
+      padded_labeled_classes = []
+      for x in labeled_classes:
+        padding = num_classes - tf.shape(x)[0]
+        padded_labeled_classes.append(tf.pad(x, [[0, padding]]))
+      groundtruth[input_data_fields.groundtruth_labeled_classes] = tf.stack(
+          padded_labeled_classes)
+    else:
+      groundtruth[input_data_fields.groundtruth_labeled_classes] = tf.stack(
+          labeled_classes)
+
   groundtruth[input_data_fields.num_groundtruth_boxes] = (
       tf.tile([max_number_of_boxes], multiples=[groundtruth_boxes_shape[0]]))
   return groundtruth
@@ -213,6 +239,7 @@ def unstack_batch(tensor_dict, unpad_groundtruth_tensors=True):
         unpadded_tensor = tf.slice(padded_tensor, slice_begin, slice_size)
         unpadded_tensor_list.append(unpadded_tensor)
       unbatched_unpadded_tensor_dict[key] = unpadded_tensor_list
+
     unbatched_tensor_dict.update(unbatched_unpadded_tensor_dict)
 
   return unbatched_tensor_dict
@@ -252,6 +279,9 @@ def provide_groundtruth(model, labels):
   gt_is_crowd_list = None
   if fields.InputDataFields.groundtruth_is_crowd in labels:
     gt_is_crowd_list = labels[fields.InputDataFields.groundtruth_is_crowd]
+  gt_group_of_list = None
+  if fields.InputDataFields.groundtruth_group_of in labels:
+    gt_group_of_list = labels[fields.InputDataFields.groundtruth_group_of]
   gt_area_list = None
   if fields.InputDataFields.groundtruth_area in labels:
     gt_area_list = labels[fields.InputDataFields.groundtruth_area]
@@ -269,6 +299,7 @@ def provide_groundtruth(model, labels):
       groundtruth_keypoint_visibilities_list=gt_keypoint_visibilities_list,
       groundtruth_weights_list=gt_weights_list,
       groundtruth_is_crowd_list=gt_is_crowd_list,
+      groundtruth_group_of_list=gt_group_of_list,
       groundtruth_area_list=gt_area_list)
 
 
@@ -447,7 +478,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       exclude_variables = (
           train_config.freeze_variables
           if train_config.freeze_variables else None)
-      trainable_variables = contrib_framework.filter_variables(
+      trainable_variables = slim.filter_variables(
           tf.trainable_variables(),
           include_patterns=include_variables,
           exclude_patterns=exclude_variables)
@@ -462,7 +493,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False,
       summaries = [] if use_tpu else None
       if train_config.summarize_gradients:
         summaries = ['gradients', 'gradient_norm', 'global_gradient_norm']
-      train_op = contrib_layers.optimize_loss(
+      train_op = slim.optimizers.optimize_loss(
           loss=total_loss,
           global_step=global_step,
           learning_rate=None,
@@ -826,7 +857,48 @@ def create_train_and_eval_specs(train_input_fn,
   return train_spec, eval_specs
 
 
-def continuous_eval(estimator, model_dir, input_fn, train_steps, name):
+def _evaluate_checkpoint(estimator,
+                         input_fn,
+                         checkpoint_path,
+                         name,
+                         max_retries=0):
+  """Evaluates a checkpoint.
+
+  Args:
+    estimator: Estimator object to use for evaluation.
+    input_fn: Input function to use for evaluation.
+    checkpoint_path: Path of the checkpoint to evaluate.
+    name: Namescope for eval summary.
+    max_retries: Maximum number of times to retry the evaluation on encountering
+      a tf.errors.InvalidArgumentError. If negative, will always retry the
+      evaluation.
+
+  Returns:
+    Estimator evaluation results.
+  """
+  always_retry = True if max_retries < 0 else False
+  retries = 0
+  while always_retry or retries <= max_retries:
+    try:
+      return estimator.evaluate(
+          input_fn=input_fn,
+          steps=None,
+          checkpoint_path=checkpoint_path,
+          name=name)
+    except tf.errors.InvalidArgumentError as e:
+      if always_retry or retries < max_retries:
+        tf.logging.info('Retrying checkpoint evaluation after exception: %s', e)
+        retries += 1
+      else:
+        raise e
+
+
+def continuous_eval(estimator,
+                    model_dir,
+                    input_fn,
+                    train_steps,
+                    name,
+                    max_retries=0):
   """Perform continuous evaluation on checkpoints written to a model directory.
 
   Args:
@@ -836,20 +908,27 @@ def continuous_eval(estimator, model_dir, input_fn, train_steps, name):
     train_steps: Number of training steps. This is used to infer the last
       checkpoint and stop evaluation loop.
     name: Namescope for eval summary.
+    max_retries: Maximum number of times to retry the evaluation on encountering
+      a tf.errors.InvalidArgumentError. If negative, will always retry the
+      evaluation.
   """
 
   def terminate_eval():
     tf.logging.info('Terminating eval after 180 seconds of no checkpoints')
     return True
 
-  for ckpt in contrib_training.checkpoints_iterator(
+  for ckpt in tf.train.checkpoints_iterator(
       model_dir, min_interval_secs=180, timeout=None,
       timeout_fn=terminate_eval):
 
     tf.logging.info('Starting Evaluation.')
     try:
-      eval_results = estimator.evaluate(
-          input_fn=input_fn, steps=None, checkpoint_path=ckpt, name=name)
+      eval_results = _evaluate_checkpoint(
+          estimator=estimator,
+          input_fn=input_fn,
+          checkpoint_path=ckpt,
+          name=name,
+          max_retries=max_retries)
       tf.logging.info('Eval results: %s' % eval_results)
 
       # Terminate eval job when final checkpoint is reached
diff --git a/research/object_detection/model_lib_test.py b/research/object_detection/model_lib_test.py
index c64b202c..ae14ad84 100644
--- a/research/object_detection/model_lib_test.py
+++ b/research/object_detection/model_lib_test.py
@@ -22,7 +22,7 @@ import functools
 import os
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.contrib.tpu.python.tpu import tpu_config
 from tensorflow.contrib.tpu.python.tpu import tpu_estimator
@@ -42,11 +42,18 @@ MODEL_NAME_FOR_TEST = 'ssd_inception_v2_pets'
 # Model for testing keypoints.
 MODEL_NAME_FOR_KEYPOINTS_TEST = 'ssd_mobilenet_v1_fpp'
 
+# Model for testing tfSequenceExample inputs.
+MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST = 'context_rcnn_camera_trap'
 
-def _get_data_path():
+
+def _get_data_path(model_name):
   """Returns an absolute path to TFRecord file."""
-  return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
-                      'pets_examples.record')
+  if model_name == MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST:
+    return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
+                        'snapshot_serengeti_sequence_examples.record')
+  else:
+    return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
+                        'pets_examples.record')
 
 
 def get_pipeline_config_path(model_name):
@@ -54,6 +61,9 @@ def get_pipeline_config_path(model_name):
   if model_name == MODEL_NAME_FOR_KEYPOINTS_TEST:
     return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
                         model_name + '.config')
+  elif model_name == MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST:
+    return os.path.join(tf.resource_loader.get_data_files_path(), 'test_data',
+                        model_name + '.config')
   else:
     return os.path.join(tf.resource_loader.get_data_files_path(), 'samples',
                         'configs', model_name + '.config')
@@ -71,12 +81,20 @@ def _get_keypoints_labelmap_path():
                       'face_person_with_keypoints_label_map.pbtxt')
 
 
+def _get_sequence_example_labelmap_path():
+  """Returns an absolute path to label map file."""
+  return os.path.join(tf.resource_loader.get_data_files_path(), 'data',
+                      'snapshot_serengeti_label_map.pbtxt')
+
+
 def _get_configs_for_model(model_name):
   """Returns configurations for model."""
   filename = get_pipeline_config_path(model_name)
-  data_path = _get_data_path()
+  data_path = _get_data_path(model_name)
   if model_name == MODEL_NAME_FOR_KEYPOINTS_TEST:
     label_map_path = _get_keypoints_labelmap_path()
+  elif model_name == MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST:
+    label_map_path = _get_sequence_example_labelmap_path()
   else:
     label_map_path = _get_labelmap_path()
   configs = config_util.get_configs_from_pipeline_file(filename)
@@ -99,7 +117,7 @@ def _make_initializable_iterator(dataset):
   Returns:
     A `tf.data.Iterator`.
   """
-  iterator = dataset.make_initializable_iterator()
+  iterator = tf.data.make_initializable_iterator(dataset)
   tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
   return iterator
 
@@ -199,6 +217,11 @@ class ModelLibTest(tf.test.TestCase):
     configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
     self._assert_model_fn_for_train_eval(configs, 'train')
 
+  def test_model_fn_in_train_mode_sequences(self):
+    """Tests the model function in TRAIN mode."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST)
+    self._assert_model_fn_for_train_eval(configs, 'train')
+
   def test_model_fn_in_train_mode_freeze_all_variables(self):
     """Tests model_fn TRAIN mode with all variables frozen."""
     configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
@@ -229,6 +252,11 @@ class ModelLibTest(tf.test.TestCase):
     configs = _get_configs_for_model(MODEL_NAME_FOR_TEST)
     self._assert_model_fn_for_train_eval(configs, 'eval')
 
+  def test_model_fn_in_eval_mode_sequences(self):
+    """Tests the model function in EVAL mode."""
+    configs = _get_configs_for_model(MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST)
+    self._assert_model_fn_for_train_eval(configs, 'eval')
+
   def test_model_fn_in_keypoints_eval_mode(self):
     """Tests the model function in EVAL mode with keypoints config."""
     configs = _get_configs_for_model(MODEL_NAME_FOR_KEYPOINTS_TEST)
@@ -270,6 +298,27 @@ class ModelLibTest(tf.test.TestCase):
     self.assertIn('eval_input_fns', train_and_eval_dict)
     self.assertIn('eval_on_train_input_fn', train_and_eval_dict)
 
+  def test_create_estimator_and_inputs_sequence_example(self):
+    """Tests that Estimator and input function are constructed correctly."""
+    run_config = tf.estimator.RunConfig()
+    hparams = model_hparams.create_hparams(
+        hparams_overrides='load_pretrained=false')
+    pipeline_config_path = get_pipeline_config_path(
+        MODEL_NAME_FOR_SEQUENCE_EXAMPLE_TEST)
+    train_steps = 20
+    train_and_eval_dict = model_lib.create_estimator_and_inputs(
+        run_config,
+        hparams,
+        pipeline_config_path,
+        train_steps=train_steps)
+    estimator = train_and_eval_dict['estimator']
+    train_steps = train_and_eval_dict['train_steps']
+    self.assertIsInstance(estimator, tf.estimator.Estimator)
+    self.assertEqual(20, train_steps)
+    self.assertIn('train_input_fn', train_and_eval_dict)
+    self.assertIn('eval_input_fns', train_and_eval_dict)
+    self.assertIn('eval_on_train_input_fn', train_and_eval_dict)
+
   def test_create_estimator_with_default_train_eval_steps(self):
     """Tests that number of train/eval defaults to config values."""
     run_config = tf.estimator.RunConfig()
diff --git a/research/object_detection/model_lib_v2.py b/research/object_detection/model_lib_v2.py
index b99f0666..ab1fbdc1 100644
--- a/research/object_detection/model_lib_v2.py
+++ b/research/object_detection/model_lib_v2.py
@@ -22,7 +22,7 @@ import copy
 import os
 import time
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import eval_util
 from object_detection import inputs
@@ -101,6 +101,10 @@ def _compute_losses_and_predictions_dicts(
           instance masks for objects.
         labels[fields.InputDataFields.groundtruth_keypoints] is a
           float32 tensor containing keypoints for each box.
+        labels[fields.InputDataFields.groundtruth_group_of] is a tf.bool tensor
+          containing group_of annotations.
+        labels[fields.InputDataFields.groundtruth_labeled_classes] is a float32
+          k-hot tensor of classes.
     add_regularization_loss: Whether or not to include the model's
       regularization loss in the losses dictionary.
 
@@ -199,6 +203,8 @@ def eager_train_step(detection_model,
         labels[fields.InputDataFields.groundtruth_keypoints] is a
           [batch_size, num_boxes, num_keypoints, 2] float32 tensor containing
           keypoints for each box.
+        labels[fields.InputDataFields.groundtruth_labeled_classes] is a float32
+          k-hot tensor of classes.
     unpad_groundtruth_tensors: A parameter passed to unstack_batch.
     optimizer: The training optimizer that will update the variables.
     learning_rate: The learning rate tensor for the current training step.
diff --git a/research/object_detection/model_lib_v2_test.py b/research/object_detection/model_lib_v2_test.py
index 20fce37a..d2eff82f 100644
--- a/research/object_detection/model_lib_v2_test.py
+++ b/research/object_detection/model_lib_v2_test.py
@@ -23,7 +23,7 @@ import tempfile
 
 import numpy as np
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import inputs
 from object_detection import model_hparams
diff --git a/research/object_detection/model_main.py b/research/object_detection/model_main.py
index 5e8db1e5..2636ad4b 100644
--- a/research/object_detection/model_main.py
+++ b/research/object_detection/model_main.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 from absl import flags
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection import model_hparams
 from object_detection import model_lib
@@ -53,6 +53,11 @@ flags.DEFINE_boolean(
     'run_once', False, 'If running in eval-only mode, whether to run just '
     'one round of eval vs running continuously (default).'
 )
+flags.DEFINE_integer(
+    'max_eval_retries', 0, 'If running continuous eval, the maximum number of '
+    'retries upon encountering tf.errors.InvalidArgumentError. If negative, '
+    'will always retry the evaluation.'
+)
 FLAGS = flags.FLAGS
 
 
@@ -91,7 +96,7 @@ def main(unused_argv):
                              FLAGS.checkpoint_dir))
     else:
       model_lib.continuous_eval(estimator, FLAGS.checkpoint_dir, input_fn,
-                                train_steps, name)
+                                train_steps, name, FLAGS.max_eval_retries)
   else:
     train_spec, eval_specs = model_lib.create_train_and_eval_specs(
         train_input_fn,
diff --git a/research/object_detection/model_tpu_main.py b/research/object_detection/model_tpu_main.py
index 94d7e9f1..a1229eb1 100644
--- a/research/object_detection/model_tpu_main.py
+++ b/research/object_detection/model_tpu_main.py
@@ -23,7 +23,7 @@ from __future__ import division
 from __future__ import print_function
 
 from absl import flags
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 from object_detection import model_hparams
@@ -85,6 +85,11 @@ flags.DEFINE_string(
     'where event and checkpoint files will be written.')
 flags.DEFINE_string('pipeline_config_path', None, 'Path to pipeline config '
                     'file.')
+flags.DEFINE_integer(
+    'max_eval_retries', 0, 'If running continuous eval, the maximum number of '
+    'retries upon encountering tf.errors.InvalidArgumentError. If negative, '
+    'will always retry the evaluation.'
+)
 
 FLAGS = tf.flags.FLAGS
 
@@ -142,7 +147,7 @@ def main(unused_argv):
       # Currently only a single eval input is allowed.
       input_fn = eval_input_fns[0]
     model_lib.continuous_eval(estimator, FLAGS.model_dir, input_fn, train_steps,
-                              name)
+                              name, FLAGS.max_eval_retries)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index 8e272720..ac1886e0 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -16,8 +16,8 @@
 
 """Embedded-friendly SSDFeatureExtractor for MobilenetV1 features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -25,8 +25,6 @@ from object_detection.utils import context_manager
 from object_detection.utils import ops
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 class EmbeddedSSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """Embedded-friendly SSD Feature Extractor using MobilenetV1 features.
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
index 1fee66c8..fd7e0454 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for embedded_ssd_mobilenet_v1_feature_extractor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import embedded_ssd_mobilenet_v1_feature_extractor
 from object_detection.models import ssd_feature_extractor_test
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
index 79847356..a94aa207 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
@@ -22,15 +22,13 @@ as well as
 Huang et al. (https://arxiv.org/abs/1611.10012)
 """
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.utils import variables_helper
 from nets import inception_resnet_v2
 
-slim = contrib_slim
-
 
 class FasterRCNNInceptionResnetV2FeatureExtractor(
     faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
index 1d9f088f..8b5351c8 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for models.faster_rcnn_inception_resnet_v2_feature_extractor."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_inception_resnet_v2_feature_extractor as frcnn_inc_res
 
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
index a9f15306..9196871b 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor.py
@@ -25,7 +25,7 @@ Huang et al. (https://arxiv.org/abs/1611.10012)
 # Skip pylint for this file because it times out
 # pylint: skip-file
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.models.keras_models import inception_resnet_v2
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py
index f4df2a00..c8227603 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_keras_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for models.faster_rcnn_inception_resnet_v2_keras_feature_extractor."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_inception_resnet_v2_keras_feature_extractor as frcnn_inc_res
 
diff --git a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
index 3152c7af..549ad6bb 100644
--- a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor.py
@@ -18,14 +18,12 @@
 See "Rethinking the Inception Architecture for Computer Vision"
 https://arxiv.org/abs/1512.00567
 """
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from nets import inception_v2
 
-slim = contrib_slim
-
 
 def _batch_norm_arg_scope(list_ops,
                           use_batch_norm=True,
diff --git a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
index 6b5bc2f9..600c699c 100644
--- a/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_inception_v2_feature_extractor_test.py
@@ -16,7 +16,7 @@
 """Tests for faster_rcnn_inception_v2_feature_extractor."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_inception_v2_feature_extractor as faster_rcnn_inception_v2
 
diff --git a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
index 98e0f561..aa37848b 100644
--- a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor.py
@@ -16,15 +16,13 @@
 """Mobilenet v1 Faster R-CNN implementation."""
 import numpy as np
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 def _get_mobilenet_conv_no_last_stride_defs(conv_depth_ratio_in_percentage):
   if conv_depth_ratio_in_percentage not in [25, 50, 75, 100]:
diff --git a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
index fcefe616..39d6d234 100644
--- a/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_mobilenet_v1_feature_extractor_test.py
@@ -16,7 +16,7 @@
 """Tests for faster_rcnn_mobilenet_v1_feature_extractor."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_mobilenet_v1_feature_extractor as faster_rcnn_mobilenet_v1
 
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
index e765af11..b1f5e1e6 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor.py
@@ -26,17 +26,15 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.utils import variables_helper
 from nets.nasnet import nasnet
 from nets.nasnet import nasnet_utils
 
-arg_scope = contrib_framework.arg_scope
-slim = contrib_slim
+arg_scope = slim.arg_scope
 
 
 def nasnet_large_arg_scope_for_detection(is_batch_norm_training=False):
diff --git a/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
index cecfc4f8..4f7e5bed 100644
--- a/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_nas_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for models.faster_rcnn_nas_feature_extractor."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
 
diff --git a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
index 5263839b..7f4ff7e8 100644
--- a/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_pnas_feature_extractor.py
@@ -24,17 +24,15 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.utils import variables_helper
 from nets.nasnet import nasnet_utils
 from nets.nasnet import pnasnet
 
-arg_scope = contrib_framework.arg_scope
-slim = contrib_slim
+arg_scope = slim.arg_scope
 
 
 def pnasnet_large_arg_scope_for_detection(is_batch_norm_training=False):
diff --git a/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
index 6bb36804..46b822fd 100644
--- a/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_pnas_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for models.faster_rcnn_pnas_feature_extractor."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
 
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
index a72ce6b3..30cd9d42 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
@@ -24,15 +24,13 @@ the MSRA provided checkpoints
 (see https://github.com/KaimingHe/deep-residual-networks), e.g., with
 same preprocessing, batch norm scaling, etc.
 """
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from nets import resnet_utils
 from nets import resnet_v1
 
-slim = contrib_slim
-
 
 class FasterRCNNResnetV1FeatureExtractor(
     faster_rcnn_meta_arch.FasterRCNNFeatureExtractor):
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
index 876235e1..0b5055a0 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor_test.py
@@ -16,7 +16,7 @@
 """Tests for object_detection.models.faster_rcnn_resnet_v1_feature_extractor."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as faster_rcnn_resnet_v1
 
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 2af3ba66..87d15e96 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -31,11 +31,10 @@ import collections
 import functools
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
-slim = contrib_slim
 
 # Activation bound used for TPU v1. Activations will be clipped to
 # [-ACTIVATION_BOUND, ACTIVATION_BOUND] when training with
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index 30a68188..49ba09bd 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -18,7 +18,7 @@
 from absl.testing import parameterized
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
index beaabb81..cf7f9572 100644
--- a/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/base_models/original_mobilenet_v2.py
@@ -91,7 +91,7 @@ from __future__ import print_function
 
 import warnings
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 Model = tf.keras.Model
 Input = tf.keras.layers.Input
diff --git a/research/object_detection/models/keras_models/hourglass_network_test.py b/research/object_detection/models/keras_models/hourglass_network_test.py
index b168feae..2e05eb99 100644
--- a/research/object_detection/models/keras_models/hourglass_network_test.py
+++ b/research/object_detection/models/keras_models/hourglass_network_test.py
@@ -16,7 +16,7 @@
 
 from absl.testing import parameterized
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models.keras_models import hourglass_network as hourglass
 
diff --git a/research/object_detection/models/keras_models/inception_resnet_v2.py b/research/object_detection/models/keras_models/inception_resnet_v2.py
index ec99e3e9..9ecdfa26 100644
--- a/research/object_detection/models/keras_models/inception_resnet_v2.py
+++ b/research/object_detection/models/keras_models/inception_resnet_v2.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import freezable_batch_norm
 
diff --git a/research/object_detection/models/keras_models/inception_resnet_v2_test.py b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
index e3bcaded..5706e679 100644
--- a/research/object_detection/models/keras_models/inception_resnet_v2_test.py
+++ b/research/object_detection/models/keras_models/inception_resnet_v2_test.py
@@ -33,7 +33,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models.keras_models import inception_resnet_v2
 from object_detection.utils import test_case
diff --git a/research/object_detection/models/keras_models/mobilenet_v1.py b/research/object_detection/models/keras_models/mobilenet_v1.py
index bac9b852..71c20e2c 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import freezable_batch_norm
 from object_detection.models.keras_models import model_utils
diff --git a/research/object_detection/models/keras_models/mobilenet_v1_test.py b/research/object_detection/models/keras_models/mobilenet_v1_test.py
index 7563ff00..72cc1f14 100644
--- a/research/object_detection/models/keras_models/mobilenet_v1_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v1_test.py
@@ -32,7 +32,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/models/keras_models/mobilenet_v2.py b/research/object_detection/models/keras_models/mobilenet_v2.py
index 7f8ed508..b534cfbb 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import freezable_batch_norm
 from object_detection.models.keras_models import model_utils
diff --git a/research/object_detection/models/keras_models/mobilenet_v2_test.py b/research/object_detection/models/keras_models/mobilenet_v2_test.py
index e64f08a6..cfdd1197 100644
--- a/research/object_detection/models/keras_models/mobilenet_v2_test.py
+++ b/research/object_detection/models/keras_models/mobilenet_v2_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/models/keras_models/model_utils.py b/research/object_detection/models/keras_models/model_utils.py
index 1576fe95..77f3cbd1 100644
--- a/research/object_detection/models/keras_models/model_utils.py
+++ b/research/object_detection/models/keras_models/model_utils.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 # This is to specify the custom config of model structures. For example,
 # ConvDefs(conv_name='conv_pw_12', filters=512) for Mobilenet V1 is to specify
diff --git a/research/object_detection/models/keras_models/resnet_v1.py b/research/object_detection/models/keras_models/resnet_v1.py
index 12b71123..d5426ad6 100644
--- a/research/object_detection/models/keras_models/resnet_v1.py
+++ b/research/object_detection/models/keras_models/resnet_v1.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import freezable_batch_norm
 from object_detection.models.keras_models import model_utils
diff --git a/research/object_detection/models/keras_models/resnet_v1_test.py b/research/object_detection/models/keras_models/resnet_v1_test.py
index 43df4156..7b0c2a8e 100644
--- a/research/object_detection/models/keras_models/resnet_v1_test.py
+++ b/research/object_detection/models/keras_models/resnet_v1_test.py
@@ -22,7 +22,7 @@ object detection. To verify the consistency of the two models, we compare:
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index d1332e47..913a9f6a 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -24,10 +24,10 @@ from abc import abstractmethod
 
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from google.protobuf import text_format
-from tensorflow.contrib import slim as contrib_slim
+
 from object_detection.builders import hyperparams_builder
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
@@ -59,7 +59,7 @@ class SsdFeatureExtractorTestBase(test_case.TestCase):
     return hyperparams_builder.KerasLayerHyperparams(conv_hyperparams)
 
   def conv_hyperparams_fn(self):
-    with contrib_slim.arg_scope([]) as sc:
+    with slim.arg_scope([]) as sc:
       return sc
 
   @abstractmethod
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index d8ce0798..c782bb25 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -15,8 +15,8 @@
 # ==============================================================================
 
 """SSDFeatureExtractor for InceptionV2 features."""
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -24,8 +24,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import inception_v2
 
-slim = contrib_slim
-
 
 class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using InceptionV2 features."""
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py b/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
index 4eb32e4d..34921609 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.models.ssd_inception_v2_feature_extractor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_inception_v2_feature_extractor
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index f62cd34e..0fa7f78d 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -15,8 +15,8 @@
 # ==============================================================================
 
 """SSDFeatureExtractor for InceptionV3 features."""
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -24,8 +24,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import inception_v3
 
-slim = contrib_slim
-
 
 class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using InceptionV3 features."""
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py b/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
index 6927314f..1e706c1e 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.models.ssd_inception_v3_feature_extractor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_inception_v3_feature_extractor
diff --git a/research/object_detection/models/ssd_mobiledet_feature_extractor.py b/research/object_detection/models/ssd_mobiledet_feature_extractor.py
new file mode 100644
index 00000000..33d7e053
--- /dev/null
+++ b/research/object_detection/models/ssd_mobiledet_feature_extractor.py
@@ -0,0 +1,492 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""SSDFeatureExtractor for MobileDet features."""
+
+import functools
+import numpy as np
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+
+
+BACKBONE_WEIGHT_DECAY = 4e-5
+
+
+def _scale_filters(filters, multiplier, base=8):
+  """Scale the filters accordingly to (multiplier, base)."""
+  round_half_up = int(int(filters) * multiplier / base + 0.5)
+  result = int(round_half_up * base)
+  return max(result, base)
+
+
+def _swish6(h):
+  with tf.name_scope('swish6'):
+    return h * tf.nn.relu6(h + np.float32(3)) * np.float32(1. / 6.)
+
+
+def _conv(h, filters, kernel_size, strides=1,
+          normalizer_fn=slim.batch_norm, activation_fn=tf.nn.relu6):
+  if activation_fn is None:
+    raise ValueError('Activation function cannot be None. Use tf.identity '
+                     'instead to better support quantized training.')
+  return slim.conv2d(
+      h,
+      filters,
+      kernel_size,
+      stride=strides,
+      activation_fn=activation_fn,
+      normalizer_fn=normalizer_fn,
+      weights_initializer=tf.initializers.he_normal(),
+      weights_regularizer=slim.l2_regularizer(BACKBONE_WEIGHT_DECAY),
+      padding='SAME')
+
+
+def _separable_conv(
+    h, filters, kernel_size, strides=1, activation_fn=tf.nn.relu6):
+  """Separable convolution layer."""
+  if activation_fn is None:
+    raise ValueError('Activation function cannot be None. Use tf.identity '
+                     'instead to better support quantized training.')
+  # Depthwise variant of He initialization derived under the principle proposed
+  # in the original paper. Note the original He normalization was designed for
+  # full convolutions and calling tf.initializers.he_normal() can over-estimate
+  # the fan-in of a depthwise kernel by orders of magnitude.
+  stddev = (2.0 / kernel_size**2)**0.5 / .87962566103423978
+  depthwise_initializer = tf.initializers.truncated_normal(stddev=stddev)
+  return slim.separable_conv2d(
+      h,
+      filters,
+      kernel_size,
+      stride=strides,
+      activation_fn=activation_fn,
+      normalizer_fn=slim.batch_norm,
+      weights_initializer=depthwise_initializer,
+      pointwise_initializer=tf.initializers.he_normal(),
+      weights_regularizer=slim.l2_regularizer(BACKBONE_WEIGHT_DECAY),
+      padding='SAME')
+
+
+def _squeeze_and_excite(h, hidden_dim, activation_fn=tf.nn.relu6):
+  with tf.variable_scope(None, default_name='SqueezeExcite'):
+    height, width = h.shape[1], h.shape[2]
+    u = slim.avg_pool2d(h, [height, width], stride=1, padding='VALID')
+    u = _conv(u, hidden_dim, 1,
+              normalizer_fn=None, activation_fn=activation_fn)
+    u = _conv(u, h.shape[-1], 1,
+              normalizer_fn=None, activation_fn=tf.nn.sigmoid)
+    return u * h
+
+
+def _inverted_bottleneck_no_expansion(
+    h, filters, activation_fn=tf.nn.relu6,
+    kernel_size=3, strides=1, use_se=False):
+  """Inverted bottleneck layer without the first 1x1 expansion convolution."""
+  with tf.variable_scope(None, default_name='IBNNoExpansion'):
+    # Setting filters to None will make _separable_conv a depthwise conv.
+    h = _separable_conv(
+        h, None, kernel_size, strides=strides, activation_fn=activation_fn)
+    if use_se:
+      hidden_dim = _scale_filters(h.shape[-1], 0.25)
+      h = _squeeze_and_excite(h, hidden_dim, activation_fn=activation_fn)
+    h = _conv(h, filters, 1, activation_fn=tf.identity)
+    return h
+
+
+def _inverted_bottleneck(
+    h, filters, activation_fn=tf.nn.relu6,
+    kernel_size=3, expansion=8, strides=1, use_se=False, residual=True):
+  """Inverted bottleneck layer."""
+  with tf.variable_scope(None, default_name='IBN'):
+    shortcut = h
+    expanded_filters = int(h.shape[-1]) * expansion
+    if expansion <= 1:
+      raise ValueError('Expansion factor must be greater than 1.')
+    h = _conv(h, expanded_filters, 1, activation_fn=activation_fn)
+    # Setting filters to None will make _separable_conv a depthwise conv.
+    h = _separable_conv(h, None, kernel_size, strides=strides,
+                        activation_fn=activation_fn)
+    if use_se:
+      hidden_dim = _scale_filters(expanded_filters, 0.25)
+      h = _squeeze_and_excite(h, hidden_dim, activation_fn=activation_fn)
+    h = _conv(h, filters, 1, activation_fn=tf.identity)
+    if residual:
+      h = h + shortcut
+    return h
+
+
+def _fused_conv(
+    h, filters, activation_fn=tf.nn.relu6,
+    kernel_size=3, expansion=8, strides=1, use_se=False, residual=True):
+  """Fused convolution layer."""
+  with tf.variable_scope(None, default_name='FusedConv'):
+    shortcut = h
+    expanded_filters = int(h.shape[-1]) * expansion
+    if expansion <= 1:
+      raise ValueError('Expansion factor must be greater than 1.')
+    h = _conv(h, expanded_filters, kernel_size, strides=strides,
+              activation_fn=activation_fn)
+    if use_se:
+      hidden_dim = _scale_filters(expanded_filters, 0.25)
+      h = _squeeze_and_excite(h, hidden_dim, activation_fn=activation_fn)
+    h = _conv(h, filters, 1, activation_fn=tf.identity)
+    if residual:
+      h = h + shortcut
+    return h
+
+
+def _tucker_conv(
+    h, filters, activation_fn=tf.nn.relu6,
+    kernel_size=3, input_rank_ratio=0.25, output_rank_ratio=0.25,
+    strides=1, residual=True):
+  """Tucker convolution layer (generalized bottleneck)."""
+  with tf.variable_scope(None, default_name='TuckerConv'):
+    shortcut = h
+    input_rank = _scale_filters(h.shape[-1], input_rank_ratio)
+    h = _conv(h, input_rank, 1, activation_fn=activation_fn)
+    output_rank = _scale_filters(filters, output_rank_ratio)
+    h = _conv(h, output_rank, kernel_size, strides=strides,
+              activation_fn=activation_fn)
+    h = _conv(h, filters, 1, activation_fn=tf.identity)
+    if residual:
+      h = h + shortcut
+    return h
+
+
+def mobiledet_cpu_backbone(h, multiplier=1.0):
+  """Build a MobileDet CPU backbone."""
+  def _scale(filters):
+    return _scale_filters(filters, multiplier)
+  ibn = functools.partial(
+      _inverted_bottleneck, use_se=True, activation_fn=_swish6)
+
+  endpoints = {}
+  h = _conv(h, _scale(16), 3, strides=2, activation_fn=_swish6)
+  h = _inverted_bottleneck_no_expansion(
+      h, _scale(8), use_se=True, activation_fn=_swish6)
+  endpoints['C1'] = h
+  h = ibn(h, _scale(16), expansion=4, strides=2, residual=False)
+  endpoints['C2'] = h
+  h = ibn(h, _scale(32), expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(32), expansion=4)
+  h = ibn(h, _scale(32), expansion=4)
+  h = ibn(h, _scale(32), expansion=4)
+  endpoints['C3'] = h
+  h = ibn(h, _scale(72), kernel_size=5, expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(72), expansion=8)
+  h = ibn(h, _scale(72), kernel_size=5, expansion=4)
+  h = ibn(h, _scale(72), expansion=4)
+  h = ibn(h, _scale(72), expansion=8, residual=False)
+  h = ibn(h, _scale(72), expansion=8)
+  h = ibn(h, _scale(72), expansion=8)
+  h = ibn(h, _scale(72), expansion=8)
+  endpoints['C4'] = h
+  h = ibn(h, _scale(104), kernel_size=5, expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(104), kernel_size=5, expansion=4)
+  h = ibn(h, _scale(104), kernel_size=5, expansion=4)
+  h = ibn(h, _scale(104), expansion=4)
+  h = ibn(h, _scale(144), expansion=8, residual=False)
+  endpoints['C5'] = h
+  return endpoints
+
+
+def mobiledet_dsp_backbone(h, multiplier=1.0):
+  """Build a MobileDet DSP backbone."""
+  def _scale(filters):
+    return _scale_filters(filters, multiplier)
+
+  ibn = functools.partial(_inverted_bottleneck, activation_fn=tf.nn.relu6)
+  fused = functools.partial(_fused_conv, activation_fn=tf.nn.relu6)
+  tucker = functools.partial(_tucker_conv, activation_fn=tf.nn.relu6)
+
+  endpoints = {}
+  h = _conv(h, _scale(32), 3, strides=2, activation_fn=tf.nn.relu6)
+  h = _inverted_bottleneck_no_expansion(
+      h, _scale(24), activation_fn=tf.nn.relu6)
+  endpoints['C1'] = h
+  h = fused(h, _scale(32), expansion=4, strides=2, residual=False)
+  h = fused(h, _scale(32), expansion=4)
+  h = ibn(h, _scale(32), expansion=4)
+  h = tucker(h, _scale(32), input_rank_ratio=0.25, output_rank_ratio=0.75)
+  endpoints['C2'] = h
+  h = fused(h, _scale(64), expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(64), expansion=4)
+  h = fused(h, _scale(64), expansion=4)
+  h = fused(h, _scale(64), expansion=4)
+  endpoints['C3'] = h
+  h = fused(h, _scale(120), expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(120), expansion=4)
+  h = ibn(h, _scale(120), expansion=8)
+  h = ibn(h, _scale(120), expansion=8)
+  h = fused(h, _scale(144), expansion=8, residual=False)
+  h = ibn(h, _scale(144), expansion=8)
+  h = ibn(h, _scale(144), expansion=8)
+  h = ibn(h, _scale(144), expansion=8)
+  endpoints['C4'] = h
+  h = ibn(h, _scale(160), expansion=4, strides=2, residual=False)
+  h = ibn(h, _scale(160), expansion=4)
+  h = fused(h, _scale(160), expansion=4)
+  h = tucker(h, _scale(160), input_rank_ratio=0.75, output_rank_ratio=0.75)
+  h = ibn(h, _scale(240), expansion=8, residual=False)
+  endpoints['C5'] = h
+  return endpoints
+
+
+def mobiledet_edgetpu_backbone(h, multiplier=1.0):
+  """Build a MobileDet EdgeTPU backbone."""
+  def _scale(filters):
+    return _scale_filters(filters, multiplier)
+
+  ibn = functools.partial(_inverted_bottleneck, activation_fn=tf.nn.relu6)
+  fused = functools.partial(_fused_conv, activation_fn=tf.nn.relu6)
+  tucker = functools.partial(_tucker_conv, activation_fn=tf.nn.relu6)
+
+  endpoints = {}
+  h = _conv(h, _scale(32), 3, strides=2, activation_fn=tf.nn.relu6)
+  h = tucker(h, _scale(16),
+             input_rank_ratio=0.25, output_rank_ratio=0.75, residual=False)
+  endpoints['C1'] = h
+  h = fused(h, _scale(16), expansion=8, strides=2, residual=False)
+  h = fused(h, _scale(16), expansion=4)
+  h = fused(h, _scale(16), expansion=8)
+  h = fused(h, _scale(16), expansion=4)
+  endpoints['C2'] = h
+  h = fused(h, _scale(40), expansion=8, kernel_size=5, strides=2,
+            residual=False)
+  h = fused(h, _scale(40), expansion=4)
+  h = fused(h, _scale(40), expansion=4)
+  h = fused(h, _scale(40), expansion=4)
+  endpoints['C3'] = h
+  h = ibn(h, _scale(72), expansion=8, strides=2, residual=False)
+  h = ibn(h, _scale(72), expansion=8)
+  h = fused(h, _scale(72), expansion=4)
+  h = fused(h, _scale(72), expansion=4)
+  h = ibn(h, _scale(96), expansion=8, kernel_size=5, residual=False)
+  h = ibn(h, _scale(96), expansion=8, kernel_size=5)
+  h = ibn(h, _scale(96), expansion=8)
+  h = ibn(h, _scale(96), expansion=8)
+  endpoints['C4'] = h
+  h = ibn(h, _scale(120), expansion=8, kernel_size=5, strides=2, residual=False)
+  h = ibn(h, _scale(120), expansion=8)
+  h = ibn(h, _scale(120), expansion=4, kernel_size=5)
+  h = ibn(h, _scale(120), expansion=8)
+  h = ibn(h, _scale(384), expansion=8, kernel_size=5, residual=False)
+  endpoints['C5'] = h
+  return endpoints
+
+
+class SSDMobileDetFeatureExtractorBase(ssd_meta_arch.SSDFeatureExtractor):
+  """Base class of SSD feature extractor using MobileDet features."""
+
+  def __init__(self,
+               backbone_fn,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               scope_name='MobileDet'):
+    """MobileDet Feature Extractor for SSD Models.
+
+    Reference:
+      https://arxiv.org/abs/2004.14525
+
+    Args:
+      backbone_fn: function to construct the MobileDet backbone.
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: Integer, minimum feature extractor depth (number of filters).
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the base
+        feature extractor.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features.
+      use_depthwise: Whether to use depthwise convolutions in the SSD head.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+      scope_name: scope name (string) of network variables.
+    """
+    if use_explicit_padding:
+      raise NotImplementedError(
+          'Explicit padding is not yet supported in MobileDet backbones.')
+
+    super(SSDMobileDetFeatureExtractorBase, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams
+    )
+    self._backbone_fn = backbone_fn
+    self._scope_name = scope_name
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1]. The preprocessing assumes an input
+    value range of [0, 255].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+    padded_inputs = ops.pad_to_multiple(
+        preprocessed_inputs, self._pad_to_multiple)
+
+    feature_map_layout = {
+        'from_layer': ['C4', 'C5', '', '', '', ''],
+        # Do not specify the layer depths (number of filters) for C4 and C5, as
+        # their values are determined based on the backbone.
+        'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_depthwise': self._use_depthwise,
+        'use_explicit_padding': self._use_explicit_padding,
+    }
+
+    with tf.variable_scope(self._scope_name, reuse=self._reuse_weights):
+      with slim.arg_scope([slim.batch_norm],
+                          is_training=self._is_training,
+                          epsilon=0.01, decay=0.99, center=True, scale=True):
+        endpoints = self._backbone_fn(
+            padded_inputs,
+            multiplier=self._depth_multiplier)
+
+      image_features = {'C4': endpoints['C4'], 'C5': endpoints['C5']}
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
+
+    return list(feature_maps.values())
+
+
+class SSDMobileDetCPUFeatureExtractor(SSDMobileDetFeatureExtractorBase):
+  """MobileDet-CPU feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               scope_name='MobileDetCPU'):
+    super(SSDMobileDetCPUFeatureExtractor, self).__init__(
+        backbone_fn=mobiledet_cpu_backbone,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams,
+        scope_name=scope_name)
+
+
+class SSDMobileDetDSPFeatureExtractor(SSDMobileDetFeatureExtractorBase):
+  """MobileDet-DSP feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               scope_name='MobileDetDSP'):
+    super(SSDMobileDetDSPFeatureExtractor, self).__init__(
+        backbone_fn=mobiledet_dsp_backbone,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams,
+        scope_name=scope_name)
+
+
+class SSDMobileDetEdgeTPUFeatureExtractor(SSDMobileDetFeatureExtractorBase):
+  """MobileDet-EdgeTPU feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False,
+               scope_name='MobileDetEdgeTPU'):
+    super(SSDMobileDetEdgeTPUFeatureExtractor, self).__init__(
+        backbone_fn=mobiledet_edgetpu_backbone,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=override_base_feature_extractor_hyperparams,
+        scope_name=scope_name)
diff --git a/research/object_detection/models/ssd_mobiledet_feature_extractor_test.py b/research/object_detection/models/ssd_mobiledet_feature_extractor_test.py
new file mode 100644
index 00000000..c2c1ef69
--- /dev/null
+++ b/research/object_detection/models/ssd_mobiledet_feature_extractor_test.py
@@ -0,0 +1,153 @@
+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for ssd_mobiledet_feature_extractor."""
+
+import tensorflow.compat.v1 as tf
+
+from tensorflow.contrib import quantize as contrib_quantize
+from object_detection.models import ssd_feature_extractor_test
+from object_detection.models import ssd_mobiledet_feature_extractor
+
+
+class SSDMobileDetFeatureExtractorTest(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+
+  def _create_feature_extractor(self,
+                                feature_extractor_cls,
+                                is_training=False,
+                                depth_multiplier=1.0,
+                                pad_to_multiple=1,
+                                use_explicit_padding=False,
+                                use_keras=False):
+    """Constructs a new MobileDet feature extractor.
+
+    Args:
+      feature_extractor_cls: feature extractor class.
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      use_explicit_padding: If True, we will use 'VALID' padding for
+        convolutions, but prepad inputs so that the output dimensions are the
+        same as if 'SAME' padding were used.
+      use_keras: if True builds a keras-based feature extractor, if False builds
+        a slim-based one.
+
+    Returns:
+      an ssd_meta_arch.SSDMobileDetFeatureExtractor object.
+    """
+    min_depth = 32
+    return feature_extractor_cls(
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        self.conv_hyperparams_fn,
+        use_explicit_padding=use_explicit_padding)
+
+  def test_mobiledet_cpu_returns_correct_shapes(self):
+    expected_feature_map_shapes = [(2, 40, 20, 72),
+                                   (2, 20, 10, 144),
+                                   (2, 10, 5, 512),
+                                   (2, 5, 3, 256),
+                                   (2, 3, 2, 256),
+                                   (2, 2, 1, 128)]
+    feature_extractor = self._create_feature_extractor(
+        ssd_mobiledet_feature_extractor.SSDMobileDetCPUFeatureExtractor)
+    image = tf.random.normal((2, 640, 320, 3))
+    feature_maps = feature_extractor.extract_features(image)
+
+    self.assertEqual(len(expected_feature_map_shapes), len(feature_maps))
+    for expected_shape, x in zip(expected_feature_map_shapes, feature_maps):
+      self.assertTrue(x.shape.is_compatible_with(expected_shape))
+
+  def test_mobiledet_dsp_returns_correct_shapes(self):
+    expected_feature_map_shapes = [(2, 40, 20, 144),
+                                   (2, 20, 10, 240),
+                                   (2, 10, 5, 512),
+                                   (2, 5, 3, 256),
+                                   (2, 3, 2, 256),
+                                   (2, 2, 1, 128)]
+    feature_extractor = self._create_feature_extractor(
+        ssd_mobiledet_feature_extractor.SSDMobileDetDSPFeatureExtractor)
+    image = tf.random.normal((2, 640, 320, 3))
+    feature_maps = feature_extractor.extract_features(image)
+
+    self.assertEqual(len(expected_feature_map_shapes), len(feature_maps))
+    for expected_shape, x in zip(expected_feature_map_shapes, feature_maps):
+      self.assertTrue(x.shape.is_compatible_with(expected_shape))
+
+  def test_mobiledet_edgetpu_returns_correct_shapes(self):
+    expected_feature_map_shapes = [(2, 40, 20, 96),
+                                   (2, 20, 10, 384),
+                                   (2, 10, 5, 512),
+                                   (2, 5, 3, 256),
+                                   (2, 3, 2, 256),
+                                   (2, 2, 1, 128)]
+    feature_extractor = self._create_feature_extractor(
+        ssd_mobiledet_feature_extractor.SSDMobileDetEdgeTPUFeatureExtractor)
+    image = tf.random.normal((2, 640, 320, 3))
+    feature_maps = feature_extractor.extract_features(image)
+
+    self.assertEqual(len(expected_feature_map_shapes), len(feature_maps))
+    for expected_shape, x in zip(expected_feature_map_shapes, feature_maps):
+      self.assertTrue(x.shape.is_compatible_with(expected_shape))
+
+  def _check_quantization(self, model_fn):
+    checkpoint_dir = self.get_temp_dir()
+
+    with tf.Graph().as_default() as training_graph:
+      model_fn(is_training=True)
+      contrib_quantize.experimental_create_training_graph(training_graph)
+      with self.session(graph=training_graph) as sess:
+        sess.run(tf.global_variables_initializer())
+        tf.train.Saver().save(sess, checkpoint_dir)
+
+    with tf.Graph().as_default() as eval_graph:
+      model_fn(is_training=False)
+      contrib_quantize.experimental_create_eval_graph(eval_graph)
+      with self.session(graph=eval_graph) as sess:
+        tf.train.Saver().restore(sess, checkpoint_dir)
+
+  def test_mobiledet_cpu_quantization(self):
+    def model_fn(is_training):
+      feature_extractor = self._create_feature_extractor(
+          ssd_mobiledet_feature_extractor.SSDMobileDetCPUFeatureExtractor,
+          is_training=is_training)
+      image = tf.random.normal((2, 320, 320, 3))
+      feature_extractor.extract_features(image)
+    self._check_quantization(model_fn)
+
+  def test_mobiledet_dsp_quantization(self):
+    def model_fn(is_training):
+      feature_extractor = self._create_feature_extractor(
+          ssd_mobiledet_feature_extractor.SSDMobileDetDSPFeatureExtractor,
+          is_training=is_training)
+      image = tf.random.normal((2, 320, 320, 3))
+      feature_extractor.extract_features(image)
+    self._check_quantization(model_fn)
+
+  def test_mobiledet_edgetpu_quantization(self):
+    def model_fn(is_training):
+      feature_extractor = self._create_feature_extractor(
+          ssd_mobiledet_feature_extractor.SSDMobileDetEdgeTPUFeatureExtractor,
+          is_training=is_training)
+      image = tf.random.normal((2, 320, 320, 3))
+      feature_extractor.extract_features(image)
+    self._check_quantization(model_fn)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_test.py
index ef40a1f5..18612202 100644
--- a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_test.py
@@ -14,14 +14,11 @@
 # ==============================================================================
 """Tests for ssd_mobilenet_edgetpu_feature_extractor."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
-from tensorflow.contrib import slim as contrib_slim
 from object_detection.models import ssd_mobilenet_edgetpu_feature_extractor
 from object_detection.models import ssd_mobilenet_edgetpu_feature_extractor_testbase
 
-slim = contrib_slim
-
 
 class SsdMobilenetEdgeTPUFeatureExtractorTest(
     ssd_mobilenet_edgetpu_feature_extractor_testbase
diff --git a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
index 10c607bc..ce3290f8 100644
--- a/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_mobilenet_edgetpu_feature_extractor_testbase.py
@@ -17,7 +17,7 @@
 import abc
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 4b173c17..7fdcdac5 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -16,8 +16,8 @@
 
 """SSDFeatureExtractor for MobilenetV1 features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -26,8 +26,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using MobilenetV1 features."""
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index 11f32e44..eaf8776a 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -20,15 +20,12 @@ Keras-based Mobilenet V1 feature extractors in SSD.
 from absl.testing import parameterized
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v1_feature_extractor
 from object_detection.models import ssd_mobilenet_v1_keras_feature_extractor
 
-slim = contrib_slim
-
 
 @parameterized.parameters(
     {'use_keras': False},
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
index 32b5261e..37f8eb83 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -23,8 +23,8 @@ from __future__ import print_function
 import copy
 import functools
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -33,8 +33,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 # A modified config of mobilenet v1 that makes it more detection friendly,
 def _create_modified_mobilenet_config():
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
index 7c24e03a..131afed8 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor_test.py
@@ -20,15 +20,12 @@ Keras-based Mobilenet V1 FPN feature extractors in SSD.
 """
 from absl.testing import parameterized
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v1_fpn_feature_extractor
 from object_detection.models import ssd_mobilenet_v1_fpn_keras_feature_extractor
 
-slim = contrib_slim
-
 
 @parameterized.parameters(
     {'use_keras': False},
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
index 9357b7ce..53d3fdbd 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_keras_feature_extractor.py
@@ -21,7 +21,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
diff --git a/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
index 4455d012..679dc25d 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_keras_feature_extractor.py
@@ -16,8 +16,7 @@
 
 """SSDFeatureExtractor for Keras MobilenetV1 features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -25,8 +24,6 @@ from object_detection.models.keras_models import mobilenet_v1
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 
-slim = contrib_slim
-
 
 class SSDMobileNetV1KerasFeatureExtractor(
     ssd_meta_arch.SSDKerasFeatureExtractor):
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
index 0910acd7..85f6a559 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor.py
@@ -16,8 +16,8 @@
 
 """SSDFeatureExtractor for MobilenetV1 PPN features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -26,8 +26,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 class SSDMobileNetV1PpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using MobilenetV1 PPN features."""
diff --git a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
index 9ca7a289..c5a9cd80 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_ppn_feature_extractor_test.py
@@ -15,14 +15,11 @@
 
 """Tests for ssd_mobilenet_v1_ppn_feature_extractor."""
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v1_ppn_feature_extractor
 
-slim = contrib_slim
-
 
 class SsdMobilenetV1PpnFeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
index 4629b5fb..e3a37e16 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
@@ -16,8 +16,8 @@
 
 """SSDFeatureExtractor for MobilenetV2 features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -27,8 +27,6 @@ from object_detection.utils import shape_utils
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
-slim = contrib_slim
-
 
 class SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using MobilenetV2 features."""
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
index 8cd1de9f..40eee93d 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor_test.py
@@ -17,15 +17,12 @@
 from absl.testing import parameterized
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v2_feature_extractor
 from object_detection.models import ssd_mobilenet_v2_keras_feature_extractor
 
-slim = contrib_slim
-
 
 @parameterized.parameters(
     {'use_keras': False},
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
index dd24731f..65cdcc85 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor.py
@@ -23,8 +23,8 @@ from __future__ import print_function
 import copy
 import functools
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -34,8 +34,6 @@ from object_detection.utils import shape_utils
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
-slim = contrib_slim
-
 
 # A modified config of mobilenet v2 that makes it more detection friendly.
 def _create_modified_mobilenet_config():
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
index 6a8b76e2..f5bb42b6 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_feature_extractor_test.py
@@ -20,15 +20,12 @@ Keras-based Mobilenet V2 FPN feature extractors in SSD.
 """
 from absl.testing import parameterized
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v2_fpn_feature_extractor
 from object_detection.models import ssd_mobilenet_v2_fpn_keras_feature_extractor
 
-slim = contrib_slim
-
 
 @parameterized.parameters(
     {
diff --git a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
index 1d528dcf..f01bec9c 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_fpn_keras_feature_extractor.py
@@ -21,7 +21,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
diff --git a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
index d3c80588..e9260cd7 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_keras_feature_extractor.py
@@ -16,7 +16,7 @@
 
 """SSDFeatureExtractor for MobilenetV2 features."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
diff --git a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py
index 96b91a73..be1d55a0 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor.py
@@ -22,16 +22,15 @@ from __future__ import print_function
 import collections
 import functools
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
-slim = contrib_slim
 
 Block = collections.namedtuple(
     'Block', ['inputs', 'output_level', 'kernel_size', 'expansion_size'])
diff --git a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py
index 222eb549..dd9aae97 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_mnasfpn_feature_extractor_test.py
@@ -15,14 +15,11 @@
 # ==============================================================================
 """Tests for ssd_mobilenet_v2_nas_fpn_feature_extractor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
-from tensorflow.contrib import slim as contrib_slim
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_mobilenet_v2_mnasfpn_feature_extractor as mnasfpn_feature_extractor
 
-slim = contrib_slim
-
 
 class SsdMobilenetV2MnasFPNFeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
index 39e4d4d8..cc85fdcc 100644
--- a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor.py
@@ -14,8 +14,8 @@
 # ==============================================================================
 """SSDFeatureExtractor for MobileNetV3 features."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -25,8 +25,6 @@ from object_detection.utils import shape_utils
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v3
 
-slim = contrib_slim
-
 
 class SSDMobileNetV3FeatureExtractorBase(ssd_meta_arch.SSDFeatureExtractor):
   """Base class of SSD feature extractor using MobilenetV3 features."""
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py
index 6ddde4b9..38621744 100644
--- a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_test.py
@@ -14,14 +14,14 @@
 # ==============================================================================
 """Tests for ssd_mobilenet_v3_feature_extractor."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.models import ssd_mobilenet_v3_feature_extractor
 from object_detection.models import ssd_mobilenet_v3_feature_extractor_testbase
 
 
-slim = contrib_slim
+slim = slim
 
 
 class SsdMobilenetV3LargeFeatureExtractorTest(
diff --git a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py
index d2d0e098..d5ba60f2 100644
--- a/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_mobilenet_v3_feature_extractor_testbase.py
@@ -17,15 +17,11 @@
 import abc
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 
 
-slim = contrib_slim
-
-
 class _SsdMobilenetV3FeatureExtractorTestBase(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
   """Base class for MobilenetV3 tests."""
diff --git a/research/object_detection/models/ssd_pnasnet_feature_extractor.py b/research/object_detection/models/ssd_pnasnet_feature_extractor.py
index 232602e1..802c8394 100644
--- a/research/object_detection/models/ssd_pnasnet_feature_extractor.py
+++ b/research/object_detection/models/ssd_pnasnet_feature_extractor.py
@@ -19,8 +19,8 @@
 Based on PNASNet ImageNet model: https://arxiv.org/abs/1712.00559
 """
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -29,8 +29,6 @@ from object_detection.utils import ops
 from object_detection.utils import variables_helper
 from nets.nasnet import pnasnet
 
-slim = contrib_slim
-
 
 def pnasnet_large_arg_scope_for_detection(is_batch_norm_training=False):
   """Defines the default arg scope for the PNASNet Large for object detection.
diff --git a/research/object_detection/models/ssd_pnasnet_feature_extractor_test.py b/research/object_detection/models/ssd_pnasnet_feature_extractor_test.py
index 9dfafe52..1f2fb0f8 100644
--- a/research/object_detection/models/ssd_pnasnet_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_pnasnet_feature_extractor_test.py
@@ -15,14 +15,11 @@
 
 """Tests for ssd_pnas_feature_extractor."""
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 from object_detection.models import ssd_pnasnet_feature_extractor
 
-slim = contrib_slim
-
 
 class SsdPnasNetFeatureExtractorTest(
     ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index d7cbd245..fc1827a1 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -23,8 +23,8 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -33,8 +33,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import resnet_v1
 
-slim = contrib_slim
-
 
 class SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD FPN feature extractor based on Resnet v1 architecture."""
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
index 27644a57..ddd4b081 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
@@ -13,7 +13,7 @@
 # limitations under the License.
 # ==============================================================================
 """Tests for ssd resnet v1 FPN feature extractors."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor_testbase
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
index 00356c45..c3854444 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -22,7 +22,7 @@ import abc
 from absl.testing import parameterized
 import numpy as np
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
index 0f4f1fa2..6de9ae3e 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_keras_feature_extractor.py
@@ -22,7 +22,7 @@ from __future__ import print_function
 
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
index a13794c3..cbb34e26 100644
--- a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor.py
@@ -15,8 +15,8 @@
 # ==============================================================================
 """SSD feature extractors based on Resnet v1 and PPN architectures."""
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
@@ -25,8 +25,6 @@ from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from nets import resnet_v1
 
-slim = contrib_slim
-
 
 class _SSDResnetPpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD feature extractor based on resnet architecture and PPN."""
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py
index c47cd120..bfcb74cf 100644
--- a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_test.py
@@ -13,7 +13,7 @@
 # limitations under the License.
 # ==============================================================================
 """Tests for ssd resnet v1 feature extractors."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_resnet_v1_ppn_feature_extractor
 from object_detection.models import ssd_resnet_v1_ppn_feature_extractor_testbase
diff --git a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py
index 3857fc70..ba80c662 100644
--- a/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py
+++ b/research/object_detection/models/ssd_resnet_v1_ppn_feature_extractor_testbase.py
@@ -15,7 +15,7 @@
 """Tests for ssd resnet v1 feature extractors."""
 import abc
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.models import ssd_feature_extractor_test
 
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
index d710a124..44b47533 100644
--- a/research/object_detection/predictors/convolutional_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -21,14 +21,12 @@ from __future__ import print_function
 import functools
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from object_detection.core import box_predictor
 from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
 
-slim = contrib_slim
-
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
     box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
diff --git a/research/object_detection/predictors/convolutional_box_predictor_test.py b/research/object_detection/predictors/convolutional_box_predictor_test.py
index a5edb55e..eb608e1e 100644
--- a/research/object_detection/predictors/convolutional_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_box_predictor_test.py
@@ -24,7 +24,7 @@ from absl.testing import parameterized
 import numpy as np
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor.py b/research/object_detection/predictors/convolutional_keras_box_predictor.py
index 4aeabbfb..630c6803 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor.py
@@ -22,7 +22,7 @@ from __future__ import print_function
 import collections
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import box_predictor
 from object_detection.utils import shape_utils
diff --git a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
index c3ad8392..5db7e962 100644
--- a/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_keras_box_predictor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.predictors.convolutional_keras_box_predictor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
diff --git a/research/object_detection/predictors/heads/box_head.py b/research/object_detection/predictors/heads/box_head.py
index 95c03cc1..6535e9b2 100644
--- a/research/object_detection/predictors/heads/box_head.py
+++ b/research/object_detection/predictors/heads/box_head.py
@@ -20,13 +20,11 @@ All the box prediction heads have a predict function that receives the
 `features` as the first argument and returns `box_encodings`.
 """
 import functools
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.predictors.heads import head
 
-slim = contrib_slim
-
 
 class MaskRCNNBoxHead(head.Head):
   """Box prediction head.
diff --git a/research/object_detection/predictors/heads/box_head_test.py b/research/object_detection/predictors/heads/box_head_test.py
index 34df8f43..dd69115e 100644
--- a/research/object_detection/predictors/heads/box_head_test.py
+++ b/research/object_detection/predictors/heads/box_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.box_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/class_head.py b/research/object_detection/predictors/heads/class_head.py
index e779d33d..60485931 100644
--- a/research/object_detection/predictors/heads/class_head.py
+++ b/research/object_detection/predictors/heads/class_head.py
@@ -20,13 +20,11 @@ All the class prediction heads have a predict function that receives the
 `features` as the first argument and returns class predictions with background.
 """
 import functools
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.predictors.heads import head
 
-slim = contrib_slim
-
 
 class MaskRCNNClassHead(head.Head):
   """Mask RCNN class prediction head.
diff --git a/research/object_detection/predictors/heads/class_head_test.py b/research/object_detection/predictors/heads/class_head_test.py
index 4680524e..eaadcdc3 100644
--- a/research/object_detection/predictors/heads/class_head_test.py
+++ b/research/object_detection/predictors/heads/class_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.class_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/head.py b/research/object_detection/predictors/heads/head.py
index cd1607ae..d2780319 100644
--- a/research/object_detection/predictors/heads/head.py
+++ b/research/object_detection/predictors/heads/head.py
@@ -36,7 +36,7 @@ Mask RCNN box predictor.
 """
 from abc import abstractmethod
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 class Head(object):
diff --git a/research/object_detection/predictors/heads/keras_box_head.py b/research/object_detection/predictors/heads/keras_box_head.py
index 3c1c1145..b8def7fc 100644
--- a/research/object_detection/predictors/heads/keras_box_head.py
+++ b/research/object_detection/predictors/heads/keras_box_head.py
@@ -19,7 +19,7 @@ Contains Box prediction head classes for different meta architectures.
 All the box prediction heads have a _predict function that receives the
 `features` as the first argument and returns `box_encodings`.
 """
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.predictors.heads import head
 
diff --git a/research/object_detection/predictors/heads/keras_box_head_test.py b/research/object_detection/predictors/heads/keras_box_head_test.py
index 929b5f97..1dcf7ce3 100644
--- a/research/object_detection/predictors/heads/keras_box_head_test.py
+++ b/research/object_detection/predictors/heads/keras_box_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.box_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/keras_class_head.py b/research/object_detection/predictors/heads/keras_class_head.py
index 632fef6c..988ebb2e 100644
--- a/research/object_detection/predictors/heads/keras_class_head.py
+++ b/research/object_detection/predictors/heads/keras_class_head.py
@@ -19,7 +19,7 @@ Contains Class prediction head classes for different meta architectures.
 All the class prediction heads have a predict function that receives the
 `features` as the first argument and returns class predictions with background.
 """
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.predictors.heads import head
 
diff --git a/research/object_detection/predictors/heads/keras_class_head_test.py b/research/object_detection/predictors/heads/keras_class_head_test.py
index 1c339ec8..4a25efc3 100644
--- a/research/object_detection/predictors/heads/keras_class_head_test.py
+++ b/research/object_detection/predictors/heads/keras_class_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.class_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/keras_mask_head.py b/research/object_detection/predictors/heads/keras_mask_head.py
index e8c5973b..3b65cc4b 100644
--- a/research/object_detection/predictors/heads/keras_mask_head.py
+++ b/research/object_detection/predictors/heads/keras_mask_head.py
@@ -26,7 +26,7 @@ from __future__ import print_function
 
 import math
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.predictors.heads import head
 from object_detection.utils import ops
diff --git a/research/object_detection/predictors/heads/keras_mask_head_test.py b/research/object_detection/predictors/heads/keras_mask_head_test.py
index 46baeb17..4cdce7a1 100644
--- a/research/object_detection/predictors/heads/keras_mask_head_test.py
+++ b/research/object_detection/predictors/heads/keras_mask_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.mask_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/keypoint_head.py b/research/object_detection/predictors/heads/keypoint_head.py
index d72e561a..79a4d4be 100644
--- a/research/object_detection/predictors/heads/keypoint_head.py
+++ b/research/object_detection/predictors/heads/keypoint_head.py
@@ -28,11 +28,10 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.predictors.heads import head
-slim = contrib_slim
 
 
 class MaskRCNNKeypointHead(head.Head):
diff --git a/research/object_detection/predictors/heads/keypoint_head_test.py b/research/object_detection/predictors/heads/keypoint_head_test.py
index 626d59c4..0dc4c6f7 100644
--- a/research/object_detection/predictors/heads/keypoint_head_test.py
+++ b/research/object_detection/predictors/heads/keypoint_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.keypoint_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/heads/mask_head.py b/research/object_detection/predictors/heads/mask_head.py
index ee13d68b..ca0a694f 100644
--- a/research/object_detection/predictors/heads/mask_head.py
+++ b/research/object_detection/predictors/heads/mask_head.py
@@ -26,14 +26,12 @@ from __future__ import print_function
 
 import math
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from object_detection.predictors.heads import head
 from object_detection.utils import ops
 
-slim = contrib_slim
-
 
 class MaskRCNNMaskHead(head.Head):
   """Mask RCNN mask prediction head.
diff --git a/research/object_detection/predictors/heads/mask_head_test.py b/research/object_detection/predictors/heads/mask_head_test.py
index ae46d6ad..d3bd6819 100644
--- a/research/object_detection/predictors/heads/mask_head_test.py
+++ b/research/object_detection/predictors/heads/mask_head_test.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """Tests for object_detection.predictors.heads.mask_head."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor.py b/research/object_detection/predictors/mask_rcnn_box_predictor.py
index b7ee156f..26ff65da 100644
--- a/research/object_detection/predictors/mask_rcnn_box_predictor.py
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor.py
@@ -14,11 +14,8 @@
 # ==============================================================================
 
 """Mask R-CNN Box Predictor."""
-from tensorflow.contrib import slim as contrib_slim
-
 from object_detection.core import box_predictor
 
-slim = contrib_slim
 
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor_test.py b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
index 77042b5b..4733e7a5 100644
--- a/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.predictors.mask_rcnn_box_predictor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
diff --git a/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py b/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py
index 03cad615..fbffe44e 100644
--- a/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py
+++ b/research/object_detection/predictors/mask_rcnn_keras_box_predictor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.predictors.mask_rcnn_box_predictor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
diff --git a/research/object_detection/predictors/rfcn_box_predictor.py b/research/object_detection/predictors/rfcn_box_predictor.py
index a10c5f9d..c5cf7acb 100644
--- a/research/object_detection/predictors/rfcn_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_box_predictor.py
@@ -14,12 +14,11 @@
 # ==============================================================================
 
 """RFCN Box Predictor."""
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from object_detection.core import box_predictor
 from object_detection.utils import ops
 
-slim = contrib_slim
 
 BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
 CLASS_PREDICTIONS_WITH_BACKGROUND = (
diff --git a/research/object_detection/predictors/rfcn_box_predictor_test.py b/research/object_detection/predictors/rfcn_box_predictor_test.py
index 104246d0..7a484c08 100644
--- a/research/object_detection/predictors/rfcn_box_predictor_test.py
+++ b/research/object_detection/predictors/rfcn_box_predictor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.predictors.rfcn_box_predictor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/predictors/rfcn_keras_box_predictor.py b/research/object_detection/predictors/rfcn_keras_box_predictor.py
index 3329e0a6..094e665f 100644
--- a/research/object_detection/predictors/rfcn_keras_box_predictor.py
+++ b/research/object_detection/predictors/rfcn_keras_box_predictor.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 
 """RFCN Box Predictor."""
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.core import box_predictor
 from object_detection.utils import ops
 
diff --git a/research/object_detection/predictors/rfcn_keras_box_predictor_test.py b/research/object_detection/predictors/rfcn_keras_box_predictor_test.py
index 30a5bb59..d8cc01e4 100644
--- a/research/object_detection/predictors/rfcn_keras_box_predictor_test.py
+++ b/research/object_detection/predictors/rfcn_keras_box_predictor_test.py
@@ -15,7 +15,7 @@
 
 """Tests for object_detection.predictors.rfcn_box_predictor."""
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index f3048888..2d9deda1 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -24,7 +24,14 @@ enum InstanceMaskType {
   PNG_MASKS = 2;        // Encoded PNG masks.
 }
 
-// Next id: 29
+// Input type format: whether inputs are TfExamples or TfSequenceExamples.
+enum InputType {
+  INPUT_DEFAULT = 0;          // Default implementation, currently TF_EXAMPLE
+  TF_EXAMPLE = 1;  // TfExample input
+  TF_SEQUENCE_EXAMPLE = 2;        // TfSequenceExample Input
+}
+
+// Next id: 31
 message InputReader {
   // Name of input reader. Typically used to describe the dataset that is read
   // by this input reader.
@@ -119,15 +126,22 @@ message InputReader {
   // Whether to include the source_id string in the input features.
   optional bool include_source_id = 27 [default = false];
 
+  // Whether input data type is tf.Examples or tf.SequenceExamples
+  optional InputType input_type = 30 [default = TF_EXAMPLE];
+
   oneof input_reader {
     TFRecordInputReader tf_record_input_reader = 8;
     ExternalInputReader external_input_reader = 9;
   }
 
 
+  // Expand labels to ancestors or descendants in the hierarchy for
+  // for positive and negative labels, respectively.
+  optional bool expand_labels_hierarchy = 29 [default = false];
 }
 
-// An input reader that reads TF Example protos from local TFRecord files.
+// An input reader that reads TF Example or TF Sequence Example protos from
+// local TFRecord files.
 message TFRecordInputReader {
   // Path(s) to `TFRecordFile`s.
   repeated string input_path = 1;
diff --git a/research/object_detection/protos/optimizer.proto b/research/object_detection/protos/optimizer.proto
index 7ce2302b..ef1cad68 100644
--- a/research/object_detection/protos/optimizer.proto
+++ b/research/object_detection/protos/optimizer.proto
@@ -37,7 +37,7 @@ message MomentumOptimizer {
 message AdamOptimizer {
   optional LearningRate learning_rate = 1;
   // Default value for epsilon (1e-8) matches default value in
-  // tf.compat.v1.train.AdamOptimizer. This differs from tf2 default of 1e-7
+  // tf.train.AdamOptimizer. This differs from tf2 default of 1e-7
   // in tf.keras.optimizers.Adam .
   optional float epsilon = 2 [default = 1e-8];
 }
@@ -67,8 +67,8 @@ message ExponentialDecayLearningRate {
   optional float decay_factor = 3 [default = 0.95];
   optional bool staircase = 4 [default = true];
   optional float burnin_learning_rate = 5 [default = 0.0];
-  optional uint32 burnin_steps =  6 [default = 0];
-  optional float min_learning_rate =  7 [default = 0.0];
+  optional uint32 burnin_steps = 6 [default = 0];
+  optional float min_learning_rate = 7 [default = 0.0];
 }
 
 // Configuration message for a manually defined learning rate schedule.
diff --git a/research/object_detection/protos/post_processing.proto b/research/object_detection/protos/post_processing.proto
index ab12bb53..80f75b18 100644
--- a/research/object_detection/protos/post_processing.proto
+++ b/research/object_detection/protos/post_processing.proto
@@ -49,6 +49,14 @@ message BatchNonMaxSuppression {
   // Whether to change coordinate frame of the boxlist to be relative to
   // window's frame.
   optional bool change_coordinate_frame = 12 [default = true];
+
+  // Use hard NMS. Note that even if this field is set false, the behavior of
+  // NMS will be equivalent to hard NMS; This field when set to true forces the
+  // tf.image.non_max_suppression function to be called instead
+  // of tf.image.non_max_suppression_with_scores and can be used to
+  // export models for older versions of TF.
+  optional bool use_hard_nms = 13 [default = false];
+
 }
 
 // Configuration proto for post-processing predicted boxes and
diff --git a/research/object_detection/protos/string_int_label_map.proto b/research/object_detection/protos/string_int_label_map.proto
index 3b79e624..c9095a9d 100644
--- a/research/object_detection/protos/string_int_label_map.proto
+++ b/research/object_detection/protos/string_int_label_map.proto
@@ -33,6 +33,11 @@ message StringIntLabelMapItem {
     optional string label = 2;
   }
   repeated KeypointMap keypoints = 4;
+
+  // Label ids for the elements that are connected in the hierarchy with the
+  // current element. Value should correspond to another label id element.
+  repeated int32 ancestor_ids = 5;
+  repeated int32 descendant_ids = 6;
 };
 
 message StringIntLabelMap {
diff --git a/research/object_detection/samples/configs/ssdlite_mobiledet_cpu_320x320_coco_sync_4x4.config b/research/object_detection/samples/configs/ssdlite_mobiledet_cpu_320x320_coco_sync_4x4.config
new file mode 100644
index 00000000..71c7fa5f
--- /dev/null
+++ b/research/object_detection/samples/configs/ssdlite_mobiledet_cpu_320x320_coco_sync_4x4.config
@@ -0,0 +1,202 @@
+# SSDLite with MobileDet-CPU feature extractor.
+# Reference: Xiong & Liu et al., https://arxiv.org/abs/2004.14525
+# Trained on COCO, initialized from scratch.
+#
+# 0.45B MulAdds, 4.21M Parameters. Latency is 113ms on Pixel 1 CPU.
+# Achieves 24.1 mAP on COCO14 minival dataset.
+# Achieves 23.5 mAP on COCO17 val dataset.
+#
+# This config is TPU compatible.
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        use_depthwise: true
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobiledet_cpu'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_depthwise: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: false
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 400000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.8
+          total_steps: 400000
+          warmup_learning_rate: 0.13333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_epochs: 1
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+}
diff --git a/research/object_detection/samples/configs/ssdlite_mobiledet_dsp_320x320_coco_sync_4x4.config b/research/object_detection/samples/configs/ssdlite_mobiledet_dsp_320x320_coco_sync_4x4.config
new file mode 100644
index 00000000..a2ac3a24
--- /dev/null
+++ b/research/object_detection/samples/configs/ssdlite_mobiledet_dsp_320x320_coco_sync_4x4.config
@@ -0,0 +1,202 @@
+# SSDLite with MobileDet-DSP feature extractor.
+# Reference: Xiong & Liu et al., https://arxiv.org/abs/2004.14525
+# Trained on COCO, initialized from scratch.
+#
+# 2.82B MulAdds, 7.16M Parameters. Latency is 12.3ms on Pixel 4 DSP.
+# Achieves 28.9 mAP on COCO14 minival dataset.
+# Achieves 28.4 mAP on COCO17 val dataset.
+#
+# This config is TPU compatible.
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        use_depthwise: true
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobiledet_dsp'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_depthwise: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: false
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 400000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.8
+          total_steps: 400000
+          warmup_learning_rate: 0.13333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_epochs: 1
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+}
diff --git a/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config b/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config
new file mode 100644
index 00000000..cc85b9f3
--- /dev/null
+++ b/research/object_detection/samples/configs/ssdlite_mobiledet_edgetpu_320x320_coco_sync_4x4.config
@@ -0,0 +1,202 @@
+# SSDLite with MobileDet-EdgeTPU feature extractor.
+# Reference: Xiong & Liu et al., https://arxiv.org/abs/2004.14525
+# Trained on COCO, initialized from scratch.
+#
+# 1.53B MulAdds, 4.20M Parameters. Latency is 6.9ms on Pixel 4 EdgeTPU.
+# Achieves 26.0 mAP on COCO14 minival dataset.
+# Achieves 25.2 mAP on COCO17 val dataset.
+#
+# This config is TPU compatible.
+
+model {
+  ssd {
+    inplace_batchnorm_update: true
+    freeze_batchnorm: false
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+        use_matmul_gather: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    encode_background_as_zeros: true
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 320
+        width: 320
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        use_depthwise: true
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        class_prediction_bias_init: -4.6
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            random_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.97,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_mobiledet_edgetpu'
+      min_depth: 16
+      depth_multiplier: 1.0
+      use_depthwise: true
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.97,
+          epsilon: 0.001,
+        }
+      }
+      override_base_feature_extractor_hyperparams: false
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid_focal {
+          alpha: 0.75,
+          gamma: 2.0
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+          delta: 1.0
+        }
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    normalize_loc_loss_by_codesize: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+        use_static_shapes: true
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 512
+  sync_replicas: true
+  startup_delay_steps: 0
+  replicas_to_aggregate: 32
+  num_steps: 400000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.8
+          total_steps: 400000
+          warmup_learning_rate: 0.13333
+          warmup_steps: 2000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+
+train_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100"
+  }
+}
+
+eval_config: {
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  num_examples: 8000
+}
+
+eval_input_reader: {
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_epochs: 1
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010"
+  }
+}
diff --git a/research/object_detection/test_data/context_rcnn_camera_trap.config b/research/object_detection/test_data/context_rcnn_camera_trap.config
new file mode 100644
index 00000000..4b1ca82c
--- /dev/null
+++ b/research/object_detection/test_data/context_rcnn_camera_trap.config
@@ -0,0 +1,161 @@
+
+# Context R-CNN unit test configuration using sequence example sample data with
+# context_features created from the Snapshot Serengeti Dataset and stored in
+# snapshot_serengeti_sequence_examples.record.
+# This model uses attention into contextual features within the Faster R-CNN
+# object detection framework to improve object detection performance.
+# See https://arxiv.org/abs/1912.03538 for more information.
+
+model {
+  faster_rcnn {
+    num_classes: 48
+    image_resizer {
+      fixed_shape_resizer {
+        height: 640
+        width: 640
+      }
+    }
+    feature_extractor {
+      type: "faster_rcnn_resnet101"
+      first_stage_features_stride: 16
+      batch_norm_trainable: true
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        height_stride: 16
+        width_stride: 16
+        scales: 0.25
+        scales: 0.5
+        scales: 1.0
+        scales: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+      }
+    }
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.00999999977648
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.699999988079
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        share_box_across_classes: true
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.600000023842
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+    use_matmul_crop_and_resize: true
+    clip_anchors_to_image: true
+    use_matmul_gather_in_matcher: true
+    use_static_balanced_label_sampler: true
+    use_static_shapes: true
+    context_config {
+      max_num_context_features: 2000
+      context_feature_length: 2057
+    }
+  }
+}
+train_config {
+  batch_size: 1
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  sync_replicas: true
+  optimizer {
+    momentum_optimizer {
+      learning_rate {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0
+          schedule {
+            step: 2000
+            learning_rate: 0.00200000009499
+          }
+          schedule {
+            step: 200000
+            learning_rate: 0.000199999994948
+          }
+          schedule {
+            step: 300000
+            learning_rate: 1.99999994948e-05
+          }
+          warmup: true
+        }
+      }
+      momentum_optimizer_value: 0.899999976158
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  num_steps: 500000
+  replicas_to_aggregate: 8
+  max_number_of_boxes: 100
+  unpad_groundtruth_tensors: false
+}
+train_input_reader {
+  label_map_path: ""
+  tf_record_input_reader {
+    input_path: ""
+  }
+  load_context_features: true
+  input_type: TF_SEQUENCE_EXAMPLE
+}
+eval_config {
+  max_evals: 50
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
+  batch_size: 1
+}
+eval_input_reader {
+  label_map_path: ""
+  shuffle: false
+  num_epochs: 1
+  tf_record_input_reader {
+    input_path: ""
+  }
+  load_context_features: true
+  input_type: TF_SEQUENCE_EXAMPLE
+}
diff --git a/research/object_detection/test_data/snapshot_serengeti_sequence_examples.record b/research/object_detection/test_data/snapshot_serengeti_sequence_examples.record
new file mode 100644
index 00000000..865ddd15
Binary files /dev/null and b/research/object_detection/test_data/snapshot_serengeti_sequence_examples.record differ
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu.py b/research/object_detection/tpu_exporters/export_saved_model_tpu.py
index 018461af..7560d138 100644
--- a/research/object_detection/tpu_exporters/export_saved_model_tpu.py
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu.py
@@ -17,7 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.tpu_exporters import export_saved_model_tpu_lib
 
 flags = tf.app.flags
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
index 147d09ad..e97561c0 100644
--- a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib.py
@@ -17,7 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 # pylint: disable=g-direct-tensorflow-import
diff --git a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py
index 1d696036..4bbffed3 100644
--- a/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py
+++ b/research/object_detection/tpu_exporters/export_saved_model_tpu_lib_test.py
@@ -22,7 +22,7 @@ import os
 
 from absl.testing import parameterized
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.tpu_exporters import export_saved_model_tpu_lib
 
diff --git a/research/object_detection/tpu_exporters/faster_rcnn.py b/research/object_detection/tpu_exporters/faster_rcnn.py
index 854a3e8c..6736b465 100644
--- a/research/object_detection/tpu_exporters/faster_rcnn.py
+++ b/research/object_detection/tpu_exporters/faster_rcnn.py
@@ -18,7 +18,7 @@ from __future__ import division
 from __future__ import print_function
 
 # pylint: disable=protected-access
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 # pylint: disable=g-import-not-at-top
 # Checking TF version, because this module relies on TPUPartitionedCall
diff --git a/research/object_detection/tpu_exporters/ssd.py b/research/object_detection/tpu_exporters/ssd.py
index c36d8695..7469ce8f 100644
--- a/research/object_detection/tpu_exporters/ssd.py
+++ b/research/object_detection/tpu_exporters/ssd.py
@@ -17,7 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 # pylint: disable=g-import-not-at-top
 # Checking TF version, because this module relies on TPUPartitionedCall
diff --git a/research/object_detection/tpu_exporters/utils.py b/research/object_detection/tpu_exporters/utils.py
index f9bf566d..cd169795 100644
--- a/research/object_detection/tpu_exporters/utils.py
+++ b/research/object_detection/tpu_exporters/utils.py
@@ -17,7 +17,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def bfloat16_to_float32(tensor):
diff --git a/research/object_detection/tpu_exporters/utils_test.py b/research/object_detection/tpu_exporters/utils_test.py
index ec8922c3..3750a40d 100644
--- a/research/object_detection/tpu_exporters/utils_test.py
+++ b/research/object_detection/tpu_exporters/utils_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.tpu_exporters import utils
 
diff --git a/research/object_detection/utils/autoaugment_utils.py b/research/object_detection/utils/autoaugment_utils.py
index 1060550b..962c4cbc 100644
--- a/research/object_detection/utils/autoaugment_utils.py
+++ b/research/object_detection/utils/autoaugment_utils.py
@@ -22,7 +22,7 @@ from __future__ import print_function
 import inspect
 import math
 import six
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 # pylint: disable=g-import-not-at-top
 try:
diff --git a/research/object_detection/utils/category_util.py b/research/object_detection/utils/category_util.py
index 251e5847..edd98c36 100644
--- a/research/object_detection/utils/category_util.py
+++ b/research/object_detection/utils/category_util.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 
 import csv
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def load_categories_from_csv_file(csv_path):
diff --git a/research/object_detection/utils/category_util_test.py b/research/object_detection/utils/category_util_test.py
index ab14d138..ba32919e 100644
--- a/research/object_detection/utils/category_util_test.py
+++ b/research/object_detection/utils/category_util_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 
 import os
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import category_util
 
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index a4215496..71185a5a 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index 725551ca..cd5f87d8 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import os
 
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 
diff --git a/research/object_detection/utils/context_manager_test.py b/research/object_detection/utils/context_manager_test.py
index b8da6b10..b14dd1bd 100644
--- a/research/object_detection/utils/context_manager_test.py
+++ b/research/object_detection/utils/context_manager_test.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.utils import context_manager
 
 
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index e8403eae..65e7f7fe 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def int64_feature(value):
diff --git a/research/object_detection/utils/dataset_util_test.py b/research/object_detection/utils/dataset_util_test.py
index 779c8bfe..6fa86a24 100644
--- a/research/object_detection/utils/dataset_util_test.py
+++ b/research/object_detection/utils/dataset_util_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import dataset_util
 
diff --git a/research/object_detection/utils/json_utils_test.py b/research/object_detection/utils/json_utils_test.py
index bd68ef99..d4b2303f 100644
--- a/research/object_detection/utils/json_utils_test.py
+++ b/research/object_detection/utils/json_utils_test.py
@@ -15,7 +15,7 @@
 """Tests for google3.image.understanding.object_detection.utils.json_utils."""
 import os
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import json_utils
 
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index 516744bc..37c823a8 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -18,14 +18,18 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import collections
 import logging
 
+import numpy as np
 from six import string_types
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from google.protobuf import text_format
 from object_detection.protos import string_int_label_map_pb2
 
+_LABEL_OFFSET = 1
+
 
 def _validate_label_map(label_map):
   """Checks if a label map is valid.
@@ -216,6 +220,59 @@ def get_label_map_dict(label_map_path_or_proto,
   return label_map_dict
 
 
+def get_label_map_hierarchy_lut(label_map_path_or_proto,
+                                include_identity=False):
+  """Reads a label map and returns ancestors and descendants in the hierarchy.
+
+  The function returns the ancestors and descendants as separate look up tables
+   (LUT) numpy arrays of shape [max_id, max_id] where lut[i,j] = 1 when there is
+   a hierarchical relationship between class i and j.
+
+  Args:
+    label_map_path_or_proto: path to StringIntLabelMap proto text file or the
+      proto itself.
+    include_identity: Boolean to indicate whether to include a class element
+      among its ancestors and descendants. Setting this will result in the lut
+      diagonal being set to 1.
+
+  Returns:
+    ancestors_lut: Look up table with the ancestors.
+    descendants_lut: Look up table with the descendants.
+  """
+  if isinstance(label_map_path_or_proto, string_types):
+    label_map = load_labelmap(label_map_path_or_proto)
+  else:
+    _validate_label_map(label_map_path_or_proto)
+    label_map = label_map_path_or_proto
+
+  hierarchy_dict = {
+      'ancestors': collections.defaultdict(list),
+      'descendants': collections.defaultdict(list)
+  }
+  max_id = -1
+  for item in label_map.item:
+    max_id = max(max_id, item.id)
+    for ancestor in item.ancestor_ids:
+      hierarchy_dict['ancestors'][item.id].append(ancestor)
+    for descendant in item.descendant_ids:
+      hierarchy_dict['descendants'][item.id].append(descendant)
+
+  def get_graph_relations_tensor(graph_relations):
+    graph_relations_tensor = np.zeros([max_id, max_id])
+    for id_val, ids_related in graph_relations.items():
+      id_val = int(id_val) - _LABEL_OFFSET
+      for id_related in ids_related:
+        id_related -= _LABEL_OFFSET
+        graph_relations_tensor[id_val, id_related] = 1
+    if include_identity:
+      graph_relations_tensor += np.eye(max_id)
+    return graph_relations_tensor
+
+  ancestors_lut = get_graph_relations_tensor(hierarchy_dict['ancestors'])
+  descendants_lut = get_graph_relations_tensor(hierarchy_dict['descendants'])
+  return ancestors_lut, descendants_lut
+
+
 def create_categories_from_labelmap(label_map_path, use_display_name=True):
   """Reads a label map and returns categories list compatible with eval.
 
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index b0de44e2..969f3258 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -19,8 +19,9 @@ from __future__ import division
 from __future__ import print_function
 
 import os
+import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from google.protobuf import text_format
 from object_detection.protos import string_int_label_map_pb2
@@ -38,6 +39,22 @@ class LabelMapUtilTest(tf.test.TestCase):
       item.display_name = str(i)
     return label_map_proto
 
+  def _generate_label_map_with_hierarchy(self, num_classes, ancestors_dict,
+                                         descendants_dict):
+    label_map_proto = string_int_label_map_pb2.StringIntLabelMap()
+    for i in range(1, num_classes + 1):
+      item = label_map_proto.item.add()
+      item.id = i
+      item.name = 'label_' + str(i)
+      item.display_name = str(i)
+      if i in ancestors_dict:
+        for anc_i in ancestors_dict[i]:
+          item.ancestor_ids.append(anc_i)
+      if i in descendants_dict:
+        for desc_i in descendants_dict[i]:
+          item.descendant_ids.append(desc_i)
+    return label_map_proto
+
   def test_get_label_map_dict(self):
     label_map_string = """
       item {
@@ -404,6 +421,37 @@ class LabelMapUtilTest(tf.test.TestCase):
         }
     }, label_map_util.create_category_index_from_labelmap(label_map_path))
 
+  def test_get_label_map_hierarchy_lut(self):
+    num_classes = 5
+    ancestors = {2: [1, 3], 5: [1]}
+    descendants = {1: [2], 5: [1, 2]}
+    label_map = self._generate_label_map_with_hierarchy(num_classes, ancestors,
+                                                        descendants)
+    gt_hierarchy_dict_lut = {
+        'ancestors':
+            np.array([
+                [1, 0, 0, 0, 0],
+                [1, 1, 1, 0, 0],
+                [0, 0, 1, 0, 0],
+                [0, 0, 0, 1, 0],
+                [1, 0, 0, 0, 1],
+            ]),
+        'descendants':
+            np.array([
+                [1, 1, 0, 0, 0],
+                [0, 1, 0, 0, 0],
+                [0, 0, 1, 0, 0],
+                [0, 0, 0, 1, 0],
+                [1, 1, 0, 0, 1],
+            ]),
+    }
+    ancestors_lut, descendants_lut = (
+        label_map_util.get_label_map_hierarchy_lut(label_map, True))
+    np.testing.assert_array_equal(gt_hierarchy_dict_lut['ancestors'],
+                                  ancestors_lut)
+    np.testing.assert_array_equal(gt_hierarchy_dict_lut['descendants'],
+                                  descendants_lut)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/learning_schedules.py b/research/object_detection/utils/learning_schedules.py
index 079bdb18..167be22f 100644
--- a/research/object_detection/utils/learning_schedules.py
+++ b/research/object_detection/utils/learning_schedules.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 import numpy as np
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def exponential_decay_with_burnin(global_step,
diff --git a/research/object_detection/utils/learning_schedules_test.py b/research/object_detection/utils/learning_schedules_test.py
index 4fa55a93..2b6012d1 100644
--- a/research/object_detection/utils/learning_schedules_test.py
+++ b/research/object_detection/utils/learning_schedules_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import learning_schedules
 from object_detection.utils import test_case
diff --git a/research/object_detection/utils/metrics_test.py b/research/object_detection/utils/metrics_test.py
index d369d7ad..b393b70c 100644
--- a/research/object_detection/utils/metrics_test.py
+++ b/research/object_detection/utils/metrics_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import metrics
 
diff --git a/research/object_detection/utils/model_util.py b/research/object_detection/utils/model_util.py
index b2378993..6a46265c 100644
--- a/research/object_detection/utils/model_util.py
+++ b/research/object_detection/utils/model_util.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def extract_submodel(model, inputs, outputs, name=None):
diff --git a/research/object_detection/utils/model_util_test.py b/research/object_detection/utils/model_util_test.py
index ad8cf0eb..c505464c 100644
--- a/research/object_detection/utils/model_util_test.py
+++ b/research/object_detection/utils/model_util_test.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import model_util
 
diff --git a/research/object_detection/utils/np_box_list_ops_test.py b/research/object_detection/utils/np_box_list_ops_test.py
index 1e10c9e6..32fabc9f 100644
--- a/research/object_detection/utils/np_box_list_ops_test.py
+++ b/research/object_detection/utils/np_box_list_ops_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_box_list
 from object_detection.utils import np_box_list_ops
diff --git a/research/object_detection/utils/np_box_list_test.py b/research/object_detection/utils/np_box_list_test.py
index 5112dfe5..cfb90b56 100644
--- a/research/object_detection/utils/np_box_list_test.py
+++ b/research/object_detection/utils/np_box_list_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_box_list
 
diff --git a/research/object_detection/utils/np_box_mask_list_ops_test.py b/research/object_detection/utils/np_box_mask_list_ops_test.py
index 88a090ee..b9b42d61 100644
--- a/research/object_detection/utils/np_box_mask_list_ops_test.py
+++ b/research/object_detection/utils/np_box_mask_list_ops_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_box_mask_list
 from object_detection.utils import np_box_mask_list_ops
diff --git a/research/object_detection/utils/np_box_mask_list_test.py b/research/object_detection/utils/np_box_mask_list_test.py
index 60c300df..5f4d6aac 100644
--- a/research/object_detection/utils/np_box_mask_list_test.py
+++ b/research/object_detection/utils/np_box_mask_list_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_box_mask_list
 
diff --git a/research/object_detection/utils/np_box_ops_test.py b/research/object_detection/utils/np_box_ops_test.py
index c484489b..81547cbc 100644
--- a/research/object_detection/utils/np_box_ops_test.py
+++ b/research/object_detection/utils/np_box_ops_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_box_ops
 
diff --git a/research/object_detection/utils/np_mask_ops_test.py b/research/object_detection/utils/np_mask_ops_test.py
index 5df77d48..a53b801b 100644
--- a/research/object_detection/utils/np_mask_ops_test.py
+++ b/research/object_detection/utils/np_mask_ops_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import np_mask_ops
 
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index d180282c..e0b51101 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -40,7 +40,7 @@ import numpy as np
 import six
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.utils import label_map_util
@@ -815,8 +815,11 @@ class OpenImagesChallengeEvaluator(OpenImagesDetectionEvaluator):
         metric_prefix=metrics_prefix)
 
     self._evaluatable_labels = {}
-    self._expected_keys.add(
-        standard_fields.InputDataFields.groundtruth_image_classes)
+    # Only one of the two has to be provided, but both options are given
+    # for compatibility with previous codebase.
+    self._expected_keys.update([
+        standard_fields.InputDataFields.groundtruth_image_classes,
+        standard_fields.InputDataFields.groundtruth_labeled_classes])
 
   def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
     """Adds groundtruth for a single image to be used for evaluation.
@@ -841,14 +844,18 @@ class OpenImagesChallengeEvaluator(OpenImagesDetectionEvaluator):
     """
     super(OpenImagesChallengeEvaluator,
           self).add_single_ground_truth_image_info(image_id, groundtruth_dict)
+    input_fields = standard_fields.InputDataFields
     groundtruth_classes = (
-        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -
+        groundtruth_dict[input_fields.groundtruth_classes] -
         self._label_id_offset)
+    image_classes = np.array([], dtype=int)
+    if input_fields.groundtruth_image_classes in groundtruth_dict:
+      image_classes = groundtruth_dict[input_fields.groundtruth_image_classes]
+    elif input_fields.groundtruth_labeled_classes in groundtruth_dict:
+      image_classes = groundtruth_dict[input_fields.groundtruth_labeled_classes]
+    image_classes -= self._label_id_offset
     self._evaluatable_labels[image_id] = np.unique(
-        np.concatenate(((groundtruth_dict.get(
-            standard_fields.InputDataFields.groundtruth_image_classes,
-            np.array([], dtype=int)) - self._label_id_offset),
-                        groundtruth_classes)))
+        np.concatenate((image_classes, groundtruth_classes)))
 
   def add_single_detected_image_info(self, image_id, detections_dict):
     """Adds detections for a single image to be used for evaluation.
diff --git a/research/object_detection/utils/object_detection_evaluation_test.py b/research/object_detection/utils/object_detection_evaluation_test.py
index 957826a2..5b2b5c80 100644
--- a/research/object_detection/utils/object_detection_evaluation_test.py
+++ b/research/object_detection/utils/object_detection_evaluation_test.py
@@ -22,7 +22,7 @@ from absl.testing import parameterized
 import numpy as np
 import six
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection import eval_util
 from object_detection.core import standard_fields
 from object_detection.utils import object_detection_evaluation
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 5bdf866a..f5988158 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -24,20 +24,13 @@ import six
 
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
 from object_detection.utils import spatial_transform_ops as spatial_ops
 from object_detection.utils import static_shape
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import framework as contrib_framework
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
-# pylint: enable=g-import-not-at-top
 
 matmul_crop_and_resize = spatial_ops.matmul_crop_and_resize
 multilevel_roi_align = spatial_ops.multilevel_roi_align
@@ -595,7 +588,7 @@ def normalize_to_target(inputs,
       initial_norm = depth * [target_norm_value]
     else:
       initial_norm = target_norm_value
-    target_norm = contrib_framework.model_variable(
+    target_norm = slim.model_variable(
         name='weights',
         dtype=tf.float32,
         initializer=tf.constant(initial_norm, dtype=tf.float32),
@@ -833,7 +826,10 @@ def reframe_box_masks_to_image_masks(box_masks, boxes, image_height,
       boxes = tf.reshape(boxes, [-1, 2, 2])
       min_corner = tf.expand_dims(reference_boxes[:, 0:2], 1)
       max_corner = tf.expand_dims(reference_boxes[:, 2:4], 1)
-      transformed_boxes = (boxes - min_corner) / (max_corner - min_corner)
+      denom = max_corner - min_corner
+      # Prevent a divide by zero.
+      denom = tf.math.maximum(denom, 1e-4)
+      transformed_boxes = (boxes - min_corner) / denom
       return tf.reshape(transformed_boxes, [-1, 4])
 
     box_masks_expanded = tf.expand_dims(box_masks, axis=3)
@@ -841,6 +837,9 @@ def reframe_box_masks_to_image_masks(box_masks, boxes, image_height,
     unit_boxes = tf.concat(
         [tf.zeros([num_boxes, 2]), tf.ones([num_boxes, 2])], axis=1)
     reverse_boxes = transform_boxes_relative_to_boxes(unit_boxes, boxes)
+
+    # TODO(vighneshb) Use matmul_crop_and_resize so that the output shape
+    # is static. This will help us run and test on TPUs.
     return tf.image.crop_and_resize(
         image=box_masks_expanded,
         boxes=reverse_boxes,
@@ -1042,28 +1041,30 @@ def fpn_feature_levels(num_levels, unit_scale_index, image_ratio, boxes):
   return levels
 
 
-def bfloat16_to_float32_nested(tensor_nested):
+def bfloat16_to_float32_nested(input_nested):
   """Convert float32 tensors in a nested structure to bfloat16.
 
   Args:
-    tensor_nested: A Python dict, values being Tensor or Python list/tuple of
-      Tensor.
+    input_nested: A Python dict, values being Tensor or Python list/tuple of
+      Tensor or Non-Tensor.
 
   Returns:
     A Python dict with the same structure as `tensor_dict`,
     with all bfloat16 tensors converted to float32.
   """
-  if isinstance(tensor_nested, tf.Tensor):
-    if tensor_nested.dtype == tf.bfloat16:
-      return tf.cast(tensor_nested, dtype=tf.float32)
+  if isinstance(input_nested, tf.Tensor):
+    if input_nested.dtype == tf.bfloat16:
+      return tf.cast(input_nested, dtype=tf.float32)
     else:
-      return tensor_nested
-  elif isinstance(tensor_nested, (list, tuple)):
-    out_tensor_dict = [bfloat16_to_float32_nested(t) for t in tensor_nested]
-  elif isinstance(tensor_nested, dict):
+      return input_nested
+  elif isinstance(input_nested, (list, tuple)):
+    out_tensor_dict = [bfloat16_to_float32_nested(t) for t in input_nested]
+  elif isinstance(input_nested, dict):
     out_tensor_dict = {
-        k: bfloat16_to_float32_nested(v) for k, v in tensor_nested.items()
+        k: bfloat16_to_float32_nested(v) for k, v in input_nested.items()
     }
+  else:
+    return input_nested
   return out_tensor_dict
 
 
@@ -1101,3 +1102,28 @@ EqualizationLossConfig = collections.namedtuple('EqualizationLossConfig',
                                                 ['weight', 'exclude_prefixes'])
 
 
+
+
+def tile_context_tensors(tensor_dict):
+  """Tiles context fields to have num_frames along 0-th dimension."""
+
+  num_frames = tf.shape(tensor_dict[fields.InputDataFields.image])[0]
+
+  for key in tensor_dict:
+    if key not in fields.SEQUENCE_FIELDS:
+      original_tensor = tensor_dict[key]
+      tensor_shape = shape_utils.combined_static_and_dynamic_shape(
+          original_tensor)
+      tensor_dict[key] = tf.tile(
+          tf.expand_dims(original_tensor, 0),
+          tf.stack([num_frames] + [1] * len(tensor_shape), axis=0))
+  return tensor_dict
+
+
+def decode_image(tensor_dict):
+  """Decodes images in a tensor dict."""
+
+  tensor_dict[fields.InputDataFields.image] = tf.io.decode_image(
+      tensor_dict[fields.InputDataFields.image], channels=3)
+  tensor_dict[fields.InputDataFields.image].set_shape([None, None, 3])
+  return tensor_dict
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index a903fecc..a7a6f8df 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -21,80 +21,75 @@ from __future__ import print_function
 import numpy as np
 import six
 from six.moves import range
-import tensorflow as tf
-
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from object_detection.core import standard_fields as fields
 from object_detection.utils import ops
 from object_detection.utils import test_case
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import framework as contrib_framework
-  from tensorflow.contrib import slim
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
-# pylint: enable=g-import-not-at-top
-
 
-class NormalizedToImageCoordinatesTest(tf.test.TestCase):
+class NormalizedToImageCoordinatesTest(test_case.TestCase):
 
   def test_normalized_to_image_coordinates(self):
-    normalized_boxes = tf.placeholder(tf.float32, shape=(None, 1, 4))
     normalized_boxes_np = np.array([[[0.0, 0.0, 1.0, 1.0]],
                                     [[0.5, 0.5, 1.0, 1.0]]])
-    image_shape = tf.convert_to_tensor([1, 4, 4, 3], dtype=tf.int32)
-    absolute_boxes = ops.normalized_to_image_coordinates(normalized_boxes,
-                                                         image_shape,
-                                                         parallel_iterations=2)
+
+    def graph_fn(normalized_boxes):
+      image_shape = tf.convert_to_tensor([1, 4, 4, 3], dtype=tf.int32)
+      absolute_boxes = ops.normalized_to_image_coordinates(
+          normalized_boxes, image_shape, parallel_iterations=2)
+      return absolute_boxes
 
     expected_boxes = np.array([[[0, 0, 4, 4]],
                                [[2, 2, 4, 4]]])
-    with self.test_session() as sess:
-      absolute_boxes = sess.run(absolute_boxes,
-                                feed_dict={normalized_boxes:
-                                           normalized_boxes_np})
 
+    absolute_boxes = self.execute(graph_fn, [normalized_boxes_np])
     self.assertAllEqual(absolute_boxes, expected_boxes)
 
 
-class ReduceSumTrailingDimensions(tf.test.TestCase):
+class ReduceSumTrailingDimensions(test_case.TestCase):
 
   def test_reduce_sum_trailing_dimensions(self):
-    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None])
-    reduced_tensor = ops.reduce_sum_trailing_dimensions(input_tensor, ndims=2)
-    with self.test_session() as sess:
-      reduced_np = sess.run(reduced_tensor,
-                            feed_dict={input_tensor: np.ones((2, 2, 2),
-                                                             np.float32)})
+
+    def graph_fn(input_tensor):
+      reduced_tensor = ops.reduce_sum_trailing_dimensions(input_tensor, ndims=2)
+      return reduced_tensor
+
+    reduced_np = self.execute(graph_fn, [np.ones((2, 2, 2), np.float32)])
     self.assertAllClose(reduced_np, 2 * np.ones((2, 2), np.float32))
 
 
-class MeshgridTest(tf.test.TestCase):
+class MeshgridTest(test_case.TestCase):
 
   def test_meshgrid_numpy_comparison(self):
     """Tests meshgrid op with vectors, for which it should match numpy."""
+
     x = np.arange(4)
     y = np.arange(6)
+
+    def graph_fn():
+      xgrid, ygrid = ops.meshgrid(x, y)
+      return xgrid, ygrid
+
     exp_xgrid, exp_ygrid = np.meshgrid(x, y)
-    xgrid, ygrid = ops.meshgrid(x, y)
-    with self.test_session() as sess:
-      xgrid_output, ygrid_output = sess.run([xgrid, ygrid])
-      self.assertAllEqual(xgrid_output, exp_xgrid)
-      self.assertAllEqual(ygrid_output, exp_ygrid)
+    xgrid_output, ygrid_output = self.execute(graph_fn, [])
+    self.assertAllEqual(xgrid_output, exp_xgrid)
+    self.assertAllEqual(ygrid_output, exp_ygrid)
 
   def test_meshgrid_multidimensional(self):
     np.random.seed(18)
     x = np.random.rand(4, 1, 2).astype(np.float32)
     y = np.random.rand(2, 3).astype(np.float32)
 
-    xgrid, ygrid = ops.meshgrid(x, y)
-
     grid_shape = list(y.shape) + list(x.shape)
-    self.assertEqual(xgrid.get_shape().as_list(), grid_shape)
-    self.assertEqual(ygrid.get_shape().as_list(), grid_shape)
-    with self.test_session() as sess:
-      xgrid_output, ygrid_output = sess.run([xgrid, ygrid])
+
+    def graph_fn():
+      xgrid, ygrid = ops.meshgrid(x, y)
+      self.assertEqual(xgrid.get_shape().as_list(), grid_shape)
+      self.assertEqual(ygrid.get_shape().as_list(), grid_shape)
+      return xgrid, ygrid
+
+    xgrid_output, ygrid_output = self.execute(graph_fn, [])
 
     # Check the shape of the output grids
     self.assertEqual(xgrid_output.shape, tuple(grid_shape))
@@ -111,110 +106,150 @@ class MeshgridTest(tf.test.TestCase):
       self.assertEqual(ygrid_output[yind + xind], y[yind])
 
 
-class OpsTestFixedPadding(tf.test.TestCase):
+class OpsTestFixedPadding(test_case.TestCase):
 
   def test_3x3_kernel(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.fixed_padding(tensor, 3)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.fixed_padding(tensor, 3)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 4, 4, 1), padded_tensor_out.shape)
 
   def test_5x5_kernel(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.fixed_padding(tensor, 5)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.fixed_padding(tensor, 5)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)
 
   def test_3x3_atrous_kernel(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.fixed_padding(tensor, 3, 2)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.fixed_padding(tensor, 3, 2)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)
 
 
-class OpsTestPadToMultiple(tf.test.TestCase):
+class OpsTestPadToMultiple(test_case.TestCase):
 
   def test_zero_padding(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.pad_to_multiple(tensor, 1)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.pad_to_multiple(tensor, 1)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 2, 2, 1), padded_tensor_out.shape)
 
   def test_no_padding(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.pad_to_multiple(tensor, 2)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.pad_to_multiple(tensor, 2)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 2, 2, 1), padded_tensor_out.shape)
 
   def test_non_square_padding(self):
-    tensor = tf.constant([[[[0.], [0.]]]])
-    padded_tensor = ops.pad_to_multiple(tensor, 2)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]]]])
+      padded_tensor = ops.pad_to_multiple(tensor, 2)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 2, 2, 1), padded_tensor_out.shape)
 
   def test_padding(self):
-    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
-    padded_tensor = ops.pad_to_multiple(tensor, 4)
-    with self.test_session() as sess:
-      padded_tensor_out = sess.run(padded_tensor)
+
+    def graph_fn():
+      tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+      padded_tensor = ops.pad_to_multiple(tensor, 4)
+      return padded_tensor
+
+    padded_tensor_out = self.execute(graph_fn, [])
     self.assertEqual((1, 4, 4, 1), padded_tensor_out.shape)
 
 
-class OpsTestPaddedOneHotEncoding(tf.test.TestCase):
+class OpsTestPaddedOneHotEncoding(test_case.TestCase):
 
   def test_correct_one_hot_tensor_with_no_pad(self):
-    indices = tf.constant([1, 2, 3, 5])
-    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=0)
+
+    def graph_fn():
+      indices = tf.constant([1, 2, 3, 5])
+      one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=0)
+      return one_hot_tensor
+
     expected_tensor = np.array([[0, 1, 0, 0, 0, 0],
                                 [0, 0, 1, 0, 0, 0],
                                 [0, 0, 0, 1, 0, 0],
                                 [0, 0, 0, 0, 0, 1]], np.float32)
-    with self.test_session() as sess:
-      out_one_hot_tensor = sess.run(one_hot_tensor)
-      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
-                          atol=1e-10)
+
+    # Executing on CPU only because output shape is not constant.
+    out_one_hot_tensor = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
+                        atol=1e-10)
 
   def test_correct_one_hot_tensor_with_pad_one(self):
-    indices = tf.constant([1, 2, 3, 5])
-    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=1)
+
+    def graph_fn():
+      indices = tf.constant([1, 2, 3, 5])
+      one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=1)
+      return one_hot_tensor
+
     expected_tensor = np.array([[0, 0, 1, 0, 0, 0, 0],
                                 [0, 0, 0, 1, 0, 0, 0],
                                 [0, 0, 0, 0, 1, 0, 0],
                                 [0, 0, 0, 0, 0, 0, 1]], np.float32)
-    with self.test_session() as sess:
-      out_one_hot_tensor = sess.run(one_hot_tensor)
-      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
-                          atol=1e-10)
+    # Executing on CPU only because output shape is not constant.
+    out_one_hot_tensor = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
+                        atol=1e-10)
 
   def test_correct_one_hot_tensor_with_pad_three(self):
-    indices = tf.constant([1, 2, 3, 5])
-    one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=3)
+
+    def graph_fn():
+      indices = tf.constant([1, 2, 3, 5])
+      one_hot_tensor = ops.padded_one_hot_encoding(indices, depth=6, left_pad=3)
+      return one_hot_tensor
+
     expected_tensor = np.array([[0, 0, 0, 0, 1, 0, 0, 0, 0],
                                 [0, 0, 0, 0, 0, 1, 0, 0, 0],
                                 [0, 0, 0, 0, 0, 0, 1, 0, 0],
                                 [0, 0, 0, 0, 0, 0, 0, 0, 1]], np.float32)
-    with self.test_session() as sess:
-      out_one_hot_tensor = sess.run(one_hot_tensor)
-      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
-                          atol=1e-10)
+
+    # executing on CPU only because output shape is not constant.
+    out_one_hot_tensor = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
+                        atol=1e-10)
 
   def test_correct_padded_one_hot_tensor_with_empty_indices(self):
+
     depth = 6
     pad = 2
-    indices = tf.constant([])
-    one_hot_tensor = ops.padded_one_hot_encoding(
-        indices, depth=depth, left_pad=pad)
+
+    def graph_fn():
+      indices = tf.constant([])
+      one_hot_tensor = ops.padded_one_hot_encoding(
+          indices, depth=depth, left_pad=pad)
+      return one_hot_tensor
+
     expected_tensor = np.zeros((0, depth + pad))
-    with self.test_session() as sess:
-      out_one_hot_tensor = sess.run(one_hot_tensor)
-      self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
-                          atol=1e-10)
+    # executing on CPU only because output shape is not constant.
+    out_one_hot_tensor = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(out_one_hot_tensor, expected_tensor, rtol=1e-10,
+                        atol=1e-10)
 
   def test_return_none_on_zero_depth(self):
     indices = tf.constant([1, 2, 3, 4, 5])
@@ -242,28 +277,27 @@ class OpsTestPaddedOneHotEncoding(tf.test.TestCase):
       ops.padded_one_hot_encoding(indices, depth=0.1, left_pad=2)
 
 
-class OpsDenseToSparseBoxesTest(tf.test.TestCase):
+class OpsDenseToSparseBoxesTest(test_case.TestCase):
 
   def test_return_all_boxes_when_all_input_boxes_are_valid(self):
     num_classes = 4
     num_valid_boxes = 3
     code_size = 4
-    dense_location_placeholder = tf.placeholder(tf.float32,
-                                                shape=(num_valid_boxes,
-                                                       code_size))
-    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))
-    box_locations, box_classes = ops.dense_to_sparse_boxes(
-        dense_location_placeholder, dense_num_boxes_placeholder, num_classes)
-    feed_dict = {dense_location_placeholder: np.random.uniform(
-        size=[num_valid_boxes, code_size]),
-                 dense_num_boxes_placeholder: np.array([1, 0, 0, 2],
-                                                       dtype=np.int32)}
-
-    expected_box_locations = feed_dict[dense_location_placeholder]
+
+    def graph_fn(dense_location, dense_num_boxes):
+      box_locations, box_classes = ops.dense_to_sparse_boxes(
+          dense_location, dense_num_boxes, num_classes)
+      return box_locations, box_classes
+
+    dense_location_np = np.random.uniform(size=[num_valid_boxes, code_size])
+    dense_num_boxes_np = np.array([1, 0, 0, 2], dtype=np.int32)
+
+    expected_box_locations = dense_location_np
     expected_box_classses = np.array([0, 3, 3])
-    with self.test_session() as sess:
-      box_locations, box_classes = sess.run([box_locations, box_classes],
-                                            feed_dict=feed_dict)
+
+    # Executing on CPU only since output shape is not constant.
+    box_locations, box_classes = self.execute_cpu(
+        graph_fn, [dense_location_np, dense_num_boxes_np])
 
     self.assertAllClose(box_locations, expected_box_locations, rtol=1e-6,
                         atol=1e-6)
@@ -275,29 +309,27 @@ class OpsDenseToSparseBoxesTest(tf.test.TestCase):
     num_boxes = 10
     code_size = 4
 
-    dense_location_placeholder = tf.placeholder(tf.float32, shape=(num_boxes,
-                                                                   code_size))
-    dense_num_boxes_placeholder = tf.placeholder(tf.int32, shape=(num_classes))
-    box_locations, box_classes = ops.dense_to_sparse_boxes(
-        dense_location_placeholder, dense_num_boxes_placeholder, num_classes)
-    feed_dict = {dense_location_placeholder: np.random.uniform(
-        size=[num_boxes, code_size]),
-                 dense_num_boxes_placeholder: np.array([1, 0, 0, 2],
-                                                       dtype=np.int32)}
-
-    expected_box_locations = (feed_dict[dense_location_placeholder]
-                              [:num_valid_boxes])
+    def graph_fn(dense_location, dense_num_boxes):
+      box_locations, box_classes = ops.dense_to_sparse_boxes(
+          dense_location, dense_num_boxes, num_classes)
+      return box_locations, box_classes
+
+    dense_location_np = np.random.uniform(size=[num_boxes, code_size])
+    dense_num_boxes_np = np.array([1, 0, 0, 2], dtype=np.int32)
+
+    expected_box_locations = dense_location_np[:num_valid_boxes]
     expected_box_classses = np.array([0, 3, 3])
-    with self.test_session() as sess:
-      box_locations, box_classes = sess.run([box_locations, box_classes],
-                                            feed_dict=feed_dict)
+
+    # Executing on CPU only since output shape is not constant.
+    box_locations, box_classes = self.execute_cpu(
+        graph_fn, [dense_location_np, dense_num_boxes_np])
 
     self.assertAllClose(box_locations, expected_box_locations, rtol=1e-6,
                         atol=1e-6)
     self.assertAllEqual(box_classes, expected_box_classses)
 
 
-class OpsTestIndicesToDenseVector(tf.test.TestCase):
+class OpsTestIndicesToDenseVector(test_case.TestCase):
 
   def test_indices_to_dense_vector(self):
     size = 10000
@@ -307,13 +339,14 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
     expected_output = np.zeros(size, dtype=np.float32)
     expected_output[rand_indices] = 1.
 
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+    def graph_fn():
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator)
-      self.assertAllEqual(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
   def test_indices_to_dense_vector_size_at_inference(self):
     size = 5000
@@ -324,16 +357,15 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
     expected_output = np.zeros(size, dtype=np.float32)
     expected_output[rand_indices] = 1.
 
-    tf_all_indices = tf.placeholder(tf.int32)
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(tf_rand_indices,
-                                            tf.shape(tf_all_indices)[0])
-    feed_dict = {tf_all_indices: all_indices}
+    def graph_fn(tf_all_indices):
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(tf_rand_indices,
+                                              tf.shape(tf_all_indices)[0])
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator, feed_dict=feed_dict)
-      self.assertAllEqual(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [all_indices])
+    self.assertAllEqual(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
   def test_indices_to_dense_vector_int(self):
     size = 500
@@ -343,14 +375,15 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
     expected_output = np.zeros(size, dtype=np.int64)
     expected_output[rand_indices] = 1
 
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(
-        tf_rand_indices, size, 1, dtype=tf.int64)
+    def graph_fn():
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(
+          tf_rand_indices, size, 1, dtype=tf.int64)
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator)
-      self.assertAllEqual(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
   def test_indices_to_dense_vector_custom_values(self):
     size = 100
@@ -362,17 +395,18 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
     expected_output = np.float32(np.ones(size) * default_value)
     expected_output[rand_indices] = indices_value
 
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(
-        tf_rand_indices,
-        size,
-        indices_value=indices_value,
-        default_value=default_value)
+    def graph_fn():
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(
+          tf_rand_indices,
+          size,
+          indices_value=indices_value,
+          default_value=default_value)
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator)
-      self.assertAllClose(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [])
+    self.assertAllClose(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
   def test_indices_to_dense_vector_all_indices_as_input(self):
     size = 500
@@ -381,13 +415,14 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
 
     expected_output = np.ones(size, dtype=np.float32)
 
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+    def graph_fn():
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator)
-      self.assertAllEqual(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
   def test_indices_to_dense_vector_empty_indices_as_input(self):
     size = 500
@@ -395,55 +430,58 @@ class OpsTestIndicesToDenseVector(tf.test.TestCase):
 
     expected_output = np.zeros(size, dtype=np.float32)
 
-    tf_rand_indices = tf.constant(rand_indices)
-    indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+    def graph_fn():
+      tf_rand_indices = tf.constant(rand_indices)
+      indicator = ops.indices_to_dense_vector(tf_rand_indices, size)
+      return indicator
 
-    with self.test_session() as sess:
-      output = sess.run(indicator)
-      self.assertAllEqual(output, expected_output)
-      self.assertEqual(output.dtype, expected_output.dtype)
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(output, expected_output)
+    self.assertEqual(output.dtype, expected_output.dtype)
 
 
-class GroundtruthFilterTest(tf.test.TestCase):
+class GroundtruthFilterTest(test_case.TestCase):
 
   def test_filter_groundtruth(self):
-    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_label_types = tf.placeholder(tf.string, shape=(None,))
-    input_confidences = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
-    input_tensors = {
-        fields.InputDataFields.image: input_image,
-        fields.InputDataFields.groundtruth_boxes: input_boxes,
-        fields.InputDataFields.groundtruth_classes: input_classes,
-        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
-        fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_label_types: input_label_types,
-        fields.InputDataFields.groundtruth_confidences: input_confidences,
-    }
-    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
-
-    image_tensor = np.random.rand(224, 224, 3)
-    feed_dict = {
-        input_image: image_tensor,
-        input_boxes:
-        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes: np.array([1, 2], dtype=np.int32),
-        input_is_crowd: np.array([False, True], dtype=np.bool),
-        input_area: np.array([32, 48], dtype=np.float32),
-        input_difficult: np.array([True, False], dtype=np.bool),
-        input_label_types:
-        np.array(['APPROPRIATE', 'INCORRECT'], dtype=np.string_),
-        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
-        valid_indices: np.array([0], dtype=np.int32),
-    }
+
+    def graph_fn(input_image, input_boxes, input_classes, input_is_crowd,
+                 input_area, input_difficult, input_label_types,
+                 input_confidences, valid_indices):
+      input_tensors = {
+          fields.InputDataFields.image: input_image,
+          fields.InputDataFields.groundtruth_boxes: input_boxes,
+          fields.InputDataFields.groundtruth_classes: input_classes,
+          fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
+          fields.InputDataFields.groundtruth_area: input_area,
+          fields.InputDataFields.groundtruth_difficult: input_difficult,
+          fields.InputDataFields.groundtruth_label_types: input_label_types,
+          fields.InputDataFields.groundtruth_confidences: input_confidences,
+      }
+
+      output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
+      return output_tensors
+
+    input_image = np.random.rand(224, 224, 3)
+    input_boxes = np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]],
+                           dtype=np.float32)
+    input_classes = np.array([1, 2], dtype=np.int32)
+    input_is_crowd = np.array([False, True], dtype=np.bool)
+    input_area = np.array([32, 48], dtype=np.float32)
+    input_difficult = np.array([True, False], dtype=np.bool)
+    input_label_types = np.array(['APPROPRIATE', 'INCORRECT'],
+                                 dtype=np.string_)
+    input_confidences = np.array([0.99, 0.5], dtype=np.float32)
+    valid_indices = np.array([0], dtype=np.int32)
+
+    # Strings are not supported on TPU.
+    output_tensors = self.execute_cpu(
+        graph_fn,
+        [input_image, input_boxes, input_classes, input_is_crowd, input_area,
+         input_difficult, input_label_types, input_confidences, valid_indices]
+    )
+
     expected_tensors = {
-        fields.InputDataFields.image: image_tensor,
+        fields.InputDataFields.image: input_image,
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
         fields.InputDataFields.groundtruth_classes: [1],
         fields.InputDataFields.groundtruth_is_crowd: [False],
@@ -452,35 +490,24 @@ class GroundtruthFilterTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_label_types: [six.b('APPROPRIATE')],
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
-      for key in [fields.InputDataFields.image,
-                  fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd,
-                  fields.InputDataFields.groundtruth_label_types]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.image,
+                fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd,
+                fields.InputDataFields.groundtruth_label_types]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_missing_fields(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes: input_boxes,
-        fields.InputDataFields.groundtruth_classes: input_classes
-    }
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
-
-    feed_dict = {
-        input_boxes:
-        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes:
-        np.array([1, 2], dtype=np.int32),
-        valid_indices:
-        np.array([0], dtype=np.int32)
-    }
+
+    input_boxes = np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]],
+                           dtype=np.float)
+    input_classes = np.array([1, 2], dtype=np.int32)
+    valid_indices = np.array([0], dtype=np.int32)
+
     expected_tensors = {
         fields.InputDataFields.groundtruth_boxes:
         [[0.2, 0.4, 0.1, 0.8]],
@@ -488,42 +515,46 @@ class GroundtruthFilterTest(tf.test.TestCase):
         [1]
     }
 
-    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
-      for key in [fields.InputDataFields.groundtruth_boxes]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    def graph_fn(input_boxes, input_classes, valid_indices):
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes: input_boxes,
+          fields.InputDataFields.groundtruth_classes: input_classes
+      }
+      output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
+      return output_tensors
+
+    output_tensors = self.execute(graph_fn, [input_boxes, input_classes,
+                                             valid_indices])
+
+    for key in [fields.InputDataFields.groundtruth_boxes]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_empty_fields(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_confidences = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes: input_boxes,
-        fields.InputDataFields.groundtruth_classes: input_classes,
-        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
-        fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_confidences: input_confidences,
-    }
-    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
-
-    feed_dict = {
-        input_boxes:
-        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes: np.array([1, 2], dtype=np.int32),
-        input_is_crowd: np.array([False, True], dtype=np.bool),
-        input_area: np.array([], dtype=np.float32),
-        input_difficult: np.array([], dtype=np.float32),
-        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
-        valid_indices: np.array([0], dtype=np.int32)
-    }
+
+    def graph_fn(input_boxes, input_classes, input_is_crowd, input_area,
+                 input_difficult, input_confidences, valid_indices):
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes: input_boxes,
+          fields.InputDataFields.groundtruth_classes: input_classes,
+          fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
+          fields.InputDataFields.groundtruth_area: input_area,
+          fields.InputDataFields.groundtruth_difficult: input_difficult,
+          fields.InputDataFields.groundtruth_confidences: input_confidences,
+      }
+      output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
+      return output_tensors
+
+    input_boxes = np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]],
+                           dtype=np.float)
+    input_classes = np.array([1, 2], dtype=np.int32)
+    input_is_crowd = np.array([False, True], dtype=np.bool)
+    input_area = np.array([], dtype=np.float32)
+    input_difficult = np.array([], dtype=np.float32)
+    input_confidences = np.array([0.99, 0.5], dtype=np.float32)
+    valid_indices = np.array([0], dtype=np.int32)
+
     expected_tensors = {
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
         fields.InputDataFields.groundtruth_classes: [1],
@@ -532,92 +563,87 @@ class GroundtruthFilterTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_difficult: [],
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
-      for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    output_tensors = self.execute(graph_fn, [
+        input_boxes, input_classes, input_is_crowd, input_area,
+        input_difficult, input_confidences, valid_indices])
+
+    for key in [fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
   def test_filter_with_empty_groundtruth_boxes(self):
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_confidences = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes: input_boxes,
-        fields.InputDataFields.groundtruth_classes: input_classes,
-        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
-        fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_confidences: input_confidences,
-    }
-    output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
-
-    feed_dict = {
-        input_boxes: np.array([], dtype=np.float).reshape(0, 4),
-        input_classes: np.array([], dtype=np.int32),
-        input_is_crowd: np.array([], dtype=np.bool),
-        input_area: np.array([], dtype=np.float32),
-        input_difficult: np.array([], dtype=np.float32),
-        input_confidences: np.array([], dtype=np.float32),
-        valid_indices: np.array([], dtype=np.int32),
-    }
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
-      for key in input_tensors:
-        if key == fields.InputDataFields.groundtruth_boxes:
-          self.assertAllEqual([0, 4], output_tensors[key].shape)
-        else:
-          self.assertAllEqual([0], output_tensors[key].shape)
-
 
-class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
+    def graph_fn(input_boxes, input_classes, input_is_crowd, input_area,
+                 input_difficult, input_confidences, valid_indices):
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes: input_boxes,
+          fields.InputDataFields.groundtruth_classes: input_classes,
+          fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
+          fields.InputDataFields.groundtruth_area: input_area,
+          fields.InputDataFields.groundtruth_difficult: input_difficult,
+          fields.InputDataFields.groundtruth_confidences: input_confidences,
+      }
+      output_tensors = ops.retain_groundtruth(input_tensors, valid_indices)
+      return output_tensors
+
+    input_boxes = np.array([], dtype=np.float).reshape(0, 4)
+    input_classes = np.array([], dtype=np.int32)
+    input_is_crowd = np.array([], dtype=np.bool)
+    input_area = np.array([], dtype=np.float32)
+    input_difficult = np.array([], dtype=np.float32)
+    input_confidences = np.array([], dtype=np.float32)
+    valid_indices = np.array([], dtype=np.int32)
+
+    output_tensors = self.execute(graph_fn, [input_boxes, input_classes,
+                                             input_is_crowd, input_area,
+                                             input_difficult,
+                                             input_confidences,
+                                             valid_indices])
+    for key in output_tensors:
+      if key == fields.InputDataFields.groundtruth_boxes:
+        self.assertAllEqual([0, 4], output_tensors[key].shape)
+      else:
+        self.assertAllEqual([0], output_tensors[key].shape)
+
+
+class RetainGroundTruthWithPositiveClasses(test_case.TestCase):
 
   def test_filter_groundtruth_with_positive_classes(self):
-    input_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-    input_boxes = tf.placeholder(tf.float32, shape=(None, 4))
-    input_classes = tf.placeholder(tf.int32, shape=(None,))
-    input_is_crowd = tf.placeholder(tf.bool, shape=(None,))
-    input_area = tf.placeholder(tf.float32, shape=(None,))
-    input_difficult = tf.placeholder(tf.float32, shape=(None,))
-    input_label_types = tf.placeholder(tf.string, shape=(None,))
-    input_confidences = tf.placeholder(tf.float32, shape=(None,))
-    valid_indices = tf.placeholder(tf.int32, shape=(None,))
-    input_tensors = {
-        fields.InputDataFields.image: input_image,
-        fields.InputDataFields.groundtruth_boxes: input_boxes,
-        fields.InputDataFields.groundtruth_classes: input_classes,
-        fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
-        fields.InputDataFields.groundtruth_area: input_area,
-        fields.InputDataFields.groundtruth_difficult: input_difficult,
-        fields.InputDataFields.groundtruth_label_types: input_label_types,
-        fields.InputDataFields.groundtruth_confidences: input_confidences,
-    }
-    output_tensors = ops.retain_groundtruth_with_positive_classes(input_tensors)
-
-    image_tensor = np.random.rand(224, 224, 3)
-    feed_dict = {
-        input_image: image_tensor,
-        input_boxes:
-        np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]], dtype=np.float),
-        input_classes: np.array([1, 0], dtype=np.int32),
-        input_is_crowd: np.array([False, True], dtype=np.bool),
-        input_area: np.array([32, 48], dtype=np.float32),
-        input_difficult: np.array([True, False], dtype=np.bool),
-        input_label_types:
-        np.array(['APPROPRIATE', 'INCORRECT'], dtype=np.string_),
-        input_confidences: np.array([0.99, 0.5], dtype=np.float32),
-        valid_indices: np.array([0], dtype=np.int32),
-    }
+
+    def graph_fn(input_image, input_boxes, input_classes, input_is_crowd,
+                 input_area, input_difficult, input_label_types,
+                 input_confidences):
+      input_tensors = {
+          fields.InputDataFields.image: input_image,
+          fields.InputDataFields.groundtruth_boxes: input_boxes,
+          fields.InputDataFields.groundtruth_classes: input_classes,
+          fields.InputDataFields.groundtruth_is_crowd: input_is_crowd,
+          fields.InputDataFields.groundtruth_area: input_area,
+          fields.InputDataFields.groundtruth_difficult: input_difficult,
+          fields.InputDataFields.groundtruth_label_types: input_label_types,
+          fields.InputDataFields.groundtruth_confidences: input_confidences,
+      }
+      output_tensors = ops.retain_groundtruth_with_positive_classes(
+          input_tensors)
+      return output_tensors
+
+    input_image = np.random.rand(224, 224, 3)
+    input_boxes = np.array([[0.2, 0.4, 0.1, 0.8], [0.2, 0.4, 1.0, 0.8]],
+                           dtype=np.float)
+    input_classes = np.array([1, 0], dtype=np.int32)
+    input_is_crowd = np.array([False, True], dtype=np.bool)
+    input_area = np.array([32, 48], dtype=np.float32)
+    input_difficult = np.array([True, False], dtype=np.bool)
+    input_label_types = np.array(['APPROPRIATE', 'INCORRECT'],
+                                 dtype=np.string_)
+    input_confidences = np.array([0.99, 0.5], dtype=np.float32)
+
     expected_tensors = {
-        fields.InputDataFields.image: image_tensor,
+        fields.InputDataFields.image: input_image,
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
         fields.InputDataFields.groundtruth_classes: [1],
         fields.InputDataFields.groundtruth_is_crowd: [False],
@@ -626,51 +652,70 @@ class RetainGroundTruthWithPositiveClasses(tf.test.TestCase):
         fields.InputDataFields.groundtruth_label_types: [six.b('APPROPRIATE')],
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors, feed_dict=feed_dict)
-      for key in [fields.InputDataFields.image,
-                  fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd,
-                  fields.InputDataFields.groundtruth_label_types]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
+    # Executing on CPU because string types are not supported on TPU.
+    output_tensors = self.execute_cpu(graph_fn,
+                                      [input_image, input_boxes,
+                                       input_classes, input_is_crowd,
+                                       input_area,
+                                       input_difficult, input_label_types,
+                                       input_confidences])
 
-class ReplaceNaNGroundtruthLabelScoresWithOnes(tf.test.TestCase):
+    for key in [fields.InputDataFields.image,
+                fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd,
+                fields.InputDataFields.groundtruth_label_types]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
+
+
+class ReplaceNaNGroundtruthLabelScoresWithOnes(test_case.TestCase):
 
   def test_replace_nan_groundtruth_label_scores_with_ones(self):
-    label_scores = tf.constant([np.nan, 1.0, np.nan])
-    output_tensor = ops.replace_nan_groundtruth_label_scores_with_ones(
-        label_scores)
+
+    def graph_fn():
+      label_scores = tf.constant([np.nan, 1.0, np.nan])
+      output_tensor = ops.replace_nan_groundtruth_label_scores_with_ones(
+          label_scores)
+      return output_tensor
+
     expected_tensor = [1.0, 1.0, 1.0]
-    with self.test_session():
-      output_tensor = output_tensor.eval()
-      self.assertAllClose(expected_tensor, output_tensor)
+    output_tensor = self.execute(graph_fn, [])
+    self.assertAllClose(expected_tensor, output_tensor)
 
   def test_input_equals_output_when_no_nans(self):
+
     input_label_scores = [0.5, 1.0, 1.0]
-    label_scores_tensor = tf.constant(input_label_scores)
-    output_label_scores = ops.replace_nan_groundtruth_label_scores_with_ones(
-        label_scores_tensor)
-    with self.test_session():
-      output_label_scores = output_label_scores.eval()
-      self.assertAllClose(input_label_scores, output_label_scores)
+    def graph_fn():
+      label_scores_tensor = tf.constant(input_label_scores)
+      output_label_scores = ops.replace_nan_groundtruth_label_scores_with_ones(
+          label_scores_tensor)
+      return output_label_scores
+
+    output_label_scores = self.execute(graph_fn, [])
+
+    self.assertAllClose(input_label_scores, output_label_scores)
 
 
-class GroundtruthFilterWithCrowdBoxesTest(tf.test.TestCase):
+class GroundtruthFilterWithCrowdBoxesTest(test_case.TestCase):
 
   def test_filter_groundtruth_with_crowd_boxes(self):
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[0.1, 0.2, 0.6, 0.8], [0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes: [1, 2],
-        fields.InputDataFields.groundtruth_is_crowd: [True, False],
-        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
-        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
-    }
+
+    def graph_fn():
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes:
+          [[0.1, 0.2, 0.6, 0.8], [0.2, 0.4, 0.1, 0.8]],
+          fields.InputDataFields.groundtruth_classes: [1, 2],
+          fields.InputDataFields.groundtruth_is_crowd: [True, False],
+          fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+          fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
+      }
+      output_tensors = ops.filter_groundtruth_with_crowd_boxes(
+          input_tensors)
+      return output_tensors
 
     expected_tensors = {
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
@@ -680,30 +725,32 @@ class GroundtruthFilterWithCrowdBoxesTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
 
-    output_tensors = ops.filter_groundtruth_with_crowd_boxes(
-        input_tensors)
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors)
-      for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    output_tensors = self.execute(graph_fn, [])
+    for key in [fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
 
-class GroundtruthFilterWithNanBoxTest(tf.test.TestCase):
+class GroundtruthFilterWithNanBoxTest(test_case.TestCase):
 
   def test_filter_groundtruth_with_nan_box_coordinates(self):
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[np.nan, np.nan, np.nan, np.nan], [0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes: [1, 2],
-        fields.InputDataFields.groundtruth_is_crowd: [False, True],
-        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
-        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
-    }
+
+    def graph_fn():
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes:
+          [[np.nan, np.nan, np.nan, np.nan], [0.2, 0.4, 0.1, 0.8]],
+          fields.InputDataFields.groundtruth_classes: [1, 2],
+          fields.InputDataFields.groundtruth_is_crowd: [False, True],
+          fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+          fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
+      }
+      output_tensors = ops.filter_groundtruth_with_nan_box_coordinates(
+          input_tensors)
+      return output_tensors
 
     expected_tensors = {
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
@@ -713,30 +760,30 @@ class GroundtruthFilterWithNanBoxTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
 
-    output_tensors = ops.filter_groundtruth_with_nan_box_coordinates(
-        input_tensors)
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors)
-      for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    output_tensors = self.execute(graph_fn, [])
+    for key in [fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
 
-class GroundtruthFilterWithUnrecognizedClassesTest(tf.test.TestCase):
+class GroundtruthFilterWithUnrecognizedClassesTest(test_case.TestCase):
 
   def test_filter_unrecognized_classes(self):
-    input_tensors = {
-        fields.InputDataFields.groundtruth_boxes:
-        [[.3, .3, .5, .7], [0.2, 0.4, 0.1, 0.8]],
-        fields.InputDataFields.groundtruth_classes: [-1, 2],
-        fields.InputDataFields.groundtruth_is_crowd: [False, True],
-        fields.InputDataFields.groundtruth_area: [100.0, 238.7],
-        fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
-    }
+    def graph_fn():
+      input_tensors = {
+          fields.InputDataFields.groundtruth_boxes:
+          [[.3, .3, .5, .7], [0.2, 0.4, 0.1, 0.8]],
+          fields.InputDataFields.groundtruth_classes: [-1, 2],
+          fields.InputDataFields.groundtruth_is_crowd: [False, True],
+          fields.InputDataFields.groundtruth_area: [100.0, 238.7],
+          fields.InputDataFields.groundtruth_confidences: [0.5, 0.99],
+      }
+      output_tensors = ops.filter_unrecognized_classes(input_tensors)
+      return output_tensors
 
     expected_tensors = {
         fields.InputDataFields.groundtruth_boxes: [[0.2, 0.4, 0.1, 0.8]],
@@ -746,28 +793,30 @@ class GroundtruthFilterWithUnrecognizedClassesTest(tf.test.TestCase):
         fields.InputDataFields.groundtruth_confidences: [0.99],
     }
 
-    output_tensors = ops.filter_unrecognized_classes(input_tensors)
-    with self.test_session() as sess:
-      output_tensors = sess.run(output_tensors)
-      for key in [fields.InputDataFields.groundtruth_boxes,
-                  fields.InputDataFields.groundtruth_area,
-                  fields.InputDataFields.groundtruth_confidences]:
-        self.assertAllClose(expected_tensors[key], output_tensors[key])
-      for key in [fields.InputDataFields.groundtruth_classes,
-                  fields.InputDataFields.groundtruth_is_crowd]:
-        self.assertAllEqual(expected_tensors[key], output_tensors[key])
+    output_tensors = self.execute(graph_fn, [])
+    for key in [fields.InputDataFields.groundtruth_boxes,
+                fields.InputDataFields.groundtruth_area,
+                fields.InputDataFields.groundtruth_confidences]:
+      self.assertAllClose(expected_tensors[key], output_tensors[key])
+    for key in [fields.InputDataFields.groundtruth_classes,
+                fields.InputDataFields.groundtruth_is_crowd]:
+      self.assertAllEqual(expected_tensors[key], output_tensors[key])
 
 
-class OpsTestNormalizeToTarget(tf.test.TestCase):
+class OpsTestNormalizeToTarget(test_case.TestCase):
 
   def test_create_normalize_to_target(self):
+
+    if self.is_tf2():
+      self.skipTest('Skipping as variable names not supported in eager mode.')
+
     inputs = tf.random_uniform([5, 10, 12, 3])
     target_norm_value = 4.0
     dim = 3
     with self.test_session():
       output = ops.normalize_to_target(inputs, target_norm_value, dim)
       self.assertEqual(output.op.name, 'NormalizeToTarget/mul')
-      var_name = contrib_framework.get_variables()[0].name
+      var_name = slim.get_variables()[0].name
       self.assertEqual(var_name, 'NormalizeToTarget/weights:0')
 
   def test_invalid_dim(self):
@@ -788,94 +837,125 @@ class OpsTestNormalizeToTarget(tf.test.TestCase):
       ops.normalize_to_target(inputs, target_norm_value, dim)
 
   def test_correct_output_shape(self):
-    inputs = tf.random_uniform([5, 10, 12, 3])
-    target_norm_value = 4.0
-    dim = 3
-    with self.test_session():
+
+    if self.is_tf2():
+      self.skipTest('normalize_to_target not supported in eager mode because,'
+                    ' it requires creating variables.')
+
+    inputs = np.random.uniform(size=(5, 10, 12, 3)).astype(np.float32)
+    def graph_fn(inputs):
+      target_norm_value = 4.0
+      dim = 3
       output = ops.normalize_to_target(inputs, target_norm_value, dim)
-      self.assertEqual(output.get_shape().as_list(),
-                       inputs.get_shape().as_list())
+      return output
+
+    # Executing on CPU since creating a variable inside a conditional is not
+    # supported.
+    outputs = self.execute_cpu(graph_fn, [inputs])
+    self.assertEqual(outputs.shape, inputs.shape)
 
   def test_correct_initial_output_values(self):
-    inputs = tf.constant([[[[3, 4], [7, 24]],
-                           [[5, -12], [-1, 0]]]], tf.float32)
-    target_norm_value = 10.0
-    dim = 3
+
+    if self.is_tf2():
+      self.skipTest('normalize_to_target not supported in eager mode because,'
+                    ' it requires creating variables.')
+    def graph_fn():
+      inputs = tf.constant([[[[3, 4], [7, 24]],
+                             [[5, -12], [-1, 0]]]], tf.float32)
+      target_norm_value = 10.0
+      dim = 3
+      normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,
+                                                  dim)
+      return normalized_inputs
+
     expected_output = [[[[30/5.0, 40/5.0], [70/25.0, 240/25.0]],
                         [[50/13.0, -120/13.0], [-10, 0]]]]
-    with self.test_session() as sess:
+    # Executing on CPU since creating a variable inside a conditional is not
+    # supported.
+    output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(output, expected_output)
+
+  def test_multiple_target_norm_values(self):
+
+    if self.is_tf2():
+      self.skipTest('normalize_to_target not supported in eager mode because,'
+                    ' it requires creating variables.')
+
+    def graph_fn():
+      inputs = tf.constant([[[[3, 4], [7, 24]],
+                             [[5, -12], [-1, 0]]]], tf.float32)
+      target_norm_value = [10.0, 20.0]
+      dim = 3
       normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,
                                                   dim)
-      sess.run(tf.global_variables_initializer())
-      output = normalized_inputs.eval()
-      self.assertAllClose(output, expected_output)
+      return normalized_inputs
 
-  def test_multiple_target_norm_values(self):
-    inputs = tf.constant([[[[3, 4], [7, 24]],
-                           [[5, -12], [-1, 0]]]], tf.float32)
-    target_norm_value = [10.0, 20.0]
-    dim = 3
     expected_output = [[[[30/5.0, 80/5.0], [70/25.0, 480/25.0]],
                         [[50/13.0, -240/13.0], [-10, 0]]]]
-    with self.test_session() as sess:
-      normalized_inputs = ops.normalize_to_target(inputs, target_norm_value,
-                                                  dim)
-      sess.run(tf.global_variables_initializer())
-      output = normalized_inputs.eval()
-      self.assertAllClose(output, expected_output)
+
+    # Executing on CPU since creating a variable inside a conditional is not
+    # supported.
+    output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(output, expected_output)
 
 
-class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
+class OpsTestPositionSensitiveCropRegions(test_case.TestCase):
 
   def test_position_sensitive(self):
     num_spatial_bins = [3, 2]
     image_shape = [3, 2, 6]
 
-    # First channel is 1's, second channel is 2's, etc.
-    image = tf.constant(
-        list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
-    boxes = tf.random_uniform((2, 4))
-
     # The result for both boxes should be [[1, 2], [3, 4], [5, 6]]
     # before averaging.
     expected_output = np.array([3.5, 3.5]).reshape([2, 1, 1, 1])
 
     for crop_size_mult in range(1, 3):
       crop_size = [3 * crop_size_mult, 2 * crop_size_mult]
-      ps_crop_and_pool = ops.position_sensitive_crop_regions(
-          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
-      with self.test_session() as sess:
-        output = sess.run(ps_crop_and_pool)
-        self.assertAllClose(output, expected_output)
+      def graph_fn():
+        # First channel is 1's, second channel is 2's, etc.
+        image = tf.constant(
+            list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
+        boxes = tf.random_uniform((2, 4))
+
+        # pylint:disable=cell-var-from-loop
+        ps_crop_and_pool = ops.position_sensitive_crop_regions(
+            image, boxes, crop_size, num_spatial_bins, global_pool=True)
+        return ps_crop_and_pool
+
+      output = self.execute(graph_fn, [])
+      self.assertAllClose(output, expected_output)
 
   def test_position_sensitive_with_equal_channels(self):
     num_spatial_bins = [2, 2]
     image_shape = [3, 3, 4]
     crop_size = [2, 2]
 
-    image = tf.constant(
-        list(range(1, 3 * 3 + 1)), dtype=tf.float32, shape=[3, 3, 1])
-    tiled_image = tf.tile(image, [1, 1, image_shape[2]])
-    boxes = tf.random_uniform((3, 4))
-    box_ind = tf.constant([0, 0, 0], dtype=tf.int32)
-
-    # All channels are equal so position-sensitive crop and resize should
-    # work as the usual crop and resize for just one channel.
-    crop = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), boxes,
-                                    box_ind, crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
-
-    ps_crop_and_pool = ops.position_sensitive_crop_regions(
-        tiled_image,
-        boxes,
-        crop_size,
-        num_spatial_bins,
-        global_pool=True)
-
-    with self.test_session() as sess:
-      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))
-      self.assertAllClose(output, expected_output)
+    def graph_fn():
+      image = tf.constant(
+          list(range(1, 3 * 3 + 1)), dtype=tf.float32, shape=[3, 3, 1])
+      tiled_image = tf.tile(image, [1, 1, image_shape[2]])
+      boxes = tf.random_uniform((3, 4))
+      box_ind = tf.constant([0, 0, 0], dtype=tf.int32)
+
+      # All channels are equal so position-sensitive crop and resize should
+      # work as the usual crop and resize for just one channel.
+      crop = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), boxes,
+                                      box_ind, crop_size)
+      crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+
+      ps_crop_and_pool = ops.position_sensitive_crop_regions(
+          tiled_image,
+          boxes,
+          crop_size,
+          num_spatial_bins,
+          global_pool=True)
+
+      return crop_and_pool, ps_crop_and_pool
+
+    # Crop and resize op is not supported in TPUs.
+    expected_output, output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(output, expected_output)
 
   def test_raise_value_error_on_num_bins_less_than_one(self):
     num_spatial_bins = [1, -1]
@@ -907,24 +987,22 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     image_shape = [1, 1, 5]
     crop_size = [2, 2]
 
-    image = tf.constant(1, dtype=tf.float32, shape=image_shape)
-    boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
+    def graph_fn():
+      image = tf.constant(1, dtype=tf.float32, shape=image_shape)
+      boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
+
+      return ops.position_sensitive_crop_regions(
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
     with self.assertRaisesRegexp(
         ValueError, 'Dimension size must be evenly divisible by 4 but is 5'):
-      ops.position_sensitive_crop_regions(
-          image, boxes, crop_size, num_spatial_bins, global_pool=True)
+      self.execute(graph_fn, [])
 
   def test_position_sensitive_with_global_pool_false(self):
     num_spatial_bins = [3, 2]
     image_shape = [3, 2, 6]
     num_boxes = 2
 
-    # First channel is 1's, second channel is 2's, etc.
-    image = tf.constant(
-        list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
-    boxes = tf.random_uniform((num_boxes, 4))
-
     expected_output = []
 
     # Expected output, when crop_size = [3, 2].
@@ -946,10 +1024,19 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
 
     for crop_size_mult in range(1, 3):
       crop_size = [3 * crop_size_mult, 2 * crop_size_mult]
-      ps_crop = ops.position_sensitive_crop_regions(
-          image, boxes, crop_size, num_spatial_bins, global_pool=False)
-      with self.test_session() as sess:
-        output = sess.run(ps_crop)
+      # First channel is 1's, second channel is 2's, etc.
+
+      def graph_fn():
+        # pylint:disable=cell-var-from-loop
+        image = tf.constant(
+            list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
+        boxes = tf.random_uniform((num_boxes, 4))
+
+        ps_crop = ops.position_sensitive_crop_regions(
+            image, boxes, crop_size, num_spatial_bins, global_pool=False)
+        return ps_crop
+
+      output = self.execute(graph_fn, [])
       self.assertAllClose(output, expected_output[crop_size_mult - 1])
 
   def test_position_sensitive_with_global_pool_false_and_do_global_pool(self):
@@ -957,11 +1044,6 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     image_shape = [3, 2, 6]
     num_boxes = 2
 
-    # First channel is 1's, second channel is 2's, etc.
-    image = tf.constant(
-        list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
-    boxes = tf.random_uniform((num_boxes, 4))
-
     expected_output = []
 
     # Expected output, when crop_size = [3, 2].
@@ -988,16 +1070,22 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     for crop_size_mult in range(1, 3):
       crop_size = [3 * crop_size_mult, 2 * crop_size_mult]
 
-      # Perform global_pooling after running the function with
-      # global_pool=False.
-      ps_crop = ops.position_sensitive_crop_regions(
-          image, boxes, crop_size, num_spatial_bins, global_pool=False)
-      ps_crop_and_pool = tf.reduce_mean(
-          ps_crop, reduction_indices=(1, 2), keepdims=True)
-
-      with self.test_session() as sess:
-        output = sess.run(ps_crop_and_pool)
-
+      def graph_fn():
+        # pylint:disable=cell-var-from-loop
+        # First channel is 1's, second channel is 2's, etc.
+        image = tf.constant(
+            list(range(1, 3 * 2 + 1)) * 6, dtype=tf.float32, shape=image_shape)
+        boxes = tf.random_uniform((num_boxes, 4))
+
+        # Perform global_pooling after running the function with
+        # global_pool=False.
+        ps_crop = ops.position_sensitive_crop_regions(
+            image, boxes, crop_size, num_spatial_bins, global_pool=False)
+        ps_crop_and_pool = tf.reduce_mean(
+            ps_crop, reduction_indices=(1, 2), keepdims=True)
+        return ps_crop_and_pool
+
+      output = self.execute(graph_fn, [])
       self.assertAllEqual(output, expected_output[crop_size_mult - 1])
 
   def test_raise_value_error_on_non_square_block_size(self):
@@ -1014,42 +1102,39 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
           image, boxes, crop_size, num_spatial_bins, global_pool=False)
 
 
-class OpsTestBatchPositionSensitiveCropRegions(tf.test.TestCase):
+class OpsTestBatchPositionSensitiveCropRegions(test_case.TestCase):
 
   def test_position_sensitive_with_single_bin(self):
     num_spatial_bins = [1, 1]
     image_shape = [2, 3, 3, 4]
     crop_size = [2, 2]
 
-    image = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((2, 3, 4))
-    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
+    def graph_fn():
+      image = tf.random_uniform(image_shape)
+      boxes = tf.random_uniform((2, 3, 4))
+      box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
 
-    # When a single bin is used, position-sensitive crop and pool should be
-    # the same as non-position sensitive crop and pool.
-    crop = tf.image.crop_and_resize(image, tf.reshape(boxes, [-1, 4]), box_ind,
-                                    crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
-    crop_and_pool = tf.reshape(crop_and_pool, [2, 3, 1, 1, 4])
+      # When a single bin is used, position-sensitive crop and pool should be
+      # the same as non-position sensitive crop and pool.
+      crop = tf.image.crop_and_resize(image,
+                                      tf.reshape(boxes, [-1, 4]), box_ind,
+                                      crop_size)
+      crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+      crop_and_pool = tf.reshape(crop_and_pool, [2, 3, 1, 1, 4])
 
-    ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
-        image, boxes, crop_size, num_spatial_bins, global_pool=True)
+      ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
+      return crop_and_pool, ps_crop_and_pool
 
-    with self.test_session() as sess:
-      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))
-      self.assertAllClose(output, expected_output)
+    # Crop and resize is not supported on TPUs.
+    expected_output, output = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(output, expected_output)
 
   def test_position_sensitive_with_global_pool_false_and_known_boxes(self):
     num_spatial_bins = [2, 2]
     image_shape = [2, 2, 2, 4]
     crop_size = [2, 2]
 
-    images = tf.constant(
-        list(range(1, 2 * 2 * 4 + 1)) * 2, dtype=tf.float32, shape=image_shape)
-
-    # First box contains whole image, and second box contains only first row.
-    boxes = tf.constant(np.array([[[0., 0., 1., 1.]],
-                                  [[0., 0., 0.5, 1.]]]), dtype=tf.float32)
     # box_ind = tf.constant([0, 1], dtype=tf.int32)
 
     expected_output = []
@@ -1069,105 +1154,147 @@ class OpsTestBatchPositionSensitiveCropRegions(tf.test.TestCase):
     )
     expected_output = np.stack(expected_output, axis=0)
 
-    ps_crop = ops.batch_position_sensitive_crop_regions(
-        images, boxes, crop_size, num_spatial_bins, global_pool=False)
+    def graph_fn():
+      images = tf.constant(
+          list(range(1, 2 * 2 * 4 + 1)) * 2, dtype=tf.float32,
+          shape=image_shape)
 
-    with self.test_session() as sess:
-      output = sess.run(ps_crop)
-      self.assertAllEqual(output, expected_output)
+      # First box contains whole image, and second box contains only first row.
+      boxes = tf.constant(np.array([[[0., 0., 1., 1.]],
+                                    [[0., 0., 0.5, 1.]]]), dtype=tf.float32)
+
+      ps_crop = ops.batch_position_sensitive_crop_regions(
+          images, boxes, crop_size, num_spatial_bins, global_pool=False)
+      return ps_crop
+
+    output = self.execute(graph_fn, [])
+    self.assertAllEqual(output, expected_output)
 
   def test_position_sensitive_with_global_pool_false_and_single_bin(self):
     num_spatial_bins = [1, 1]
     image_shape = [2, 3, 3, 4]
     crop_size = [1, 1]
 
-    images = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((2, 3, 4))
-    # box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
+    def graph_fn():
+      images = tf.random_uniform(image_shape)
+      boxes = tf.random_uniform((2, 3, 4))
+      # box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
 
-    # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),
-    # the outputs are the same whatever the global_pool value is.
-    ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
-        images, boxes, crop_size, num_spatial_bins, global_pool=True)
-    ps_crop = ops.batch_position_sensitive_crop_regions(
-        images, boxes, crop_size, num_spatial_bins, global_pool=False)
+      # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),
+      # the outputs are the same whatever the global_pool value is.
+      ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
+          images, boxes, crop_size, num_spatial_bins, global_pool=True)
+      ps_crop = ops.batch_position_sensitive_crop_regions(
+          images, boxes, crop_size, num_spatial_bins, global_pool=False)
+      return ps_crop_and_pool, ps_crop
 
-    with self.test_session() as sess:
-      pooled_output, unpooled_output = sess.run((ps_crop_and_pool, ps_crop))
-      self.assertAllClose(pooled_output, unpooled_output)
+    pooled_output, unpooled_output = self.execute(graph_fn, [])
+    self.assertAllClose(pooled_output, unpooled_output)
 
 
-class ReframeBoxMasksToImageMasksTest(tf.test.TestCase):
+# The following tests are only executed on CPU because the output
+# shape is not constant.
+class ReframeBoxMasksToImageMasksTest(test_case.TestCase):
 
   def testZeroImageOnEmptyMask(self):
-    box_masks = tf.constant([[[0, 0],
-                              [0, 0]]], dtype=tf.float32)
-    boxes = tf.constant([[0.0, 0.0, 1.0, 1.0]], dtype=tf.float32)
-    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
-                                                       image_height=4,
-                                                       image_width=4)
     np_expected_image_masks = np.array([[[0, 0, 0, 0],
                                          [0, 0, 0, 0],
                                          [0, 0, 0, 0],
                                          [0, 0, 0, 0]]], dtype=np.float32)
-    with self.test_session() as sess:
-      np_image_masks = sess.run(image_masks)
-      self.assertAllClose(np_image_masks, np_expected_image_masks)
+    def graph_fn():
+      box_masks = tf.constant([[[0, 0],
+                                [0, 0]]], dtype=tf.float32)
+      boxes = tf.constant([[0.0, 0.0, 1.0, 1.0]], dtype=tf.float32)
+      image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                         image_height=4,
+                                                         image_width=4)
+      return image_masks
+
+    np_image_masks = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(np_image_masks, np_expected_image_masks)
 
   def testZeroBoxMasks(self):
-    box_masks = tf.zeros([0, 3, 3], dtype=tf.float32)
-    boxes = tf.zeros([0, 4], dtype=tf.float32)
-    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
-                                                       image_height=4,
-                                                       image_width=4)
-    with self.test_session() as sess:
-      np_image_masks = sess.run(image_masks)
-      self.assertAllEqual(np_image_masks.shape, np.array([0, 4, 4]))
+
+    def graph_fn():
+      box_masks = tf.zeros([0, 3, 3], dtype=tf.float32)
+      boxes = tf.zeros([0, 4], dtype=tf.float32)
+      image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                         image_height=4,
+                                                         image_width=4)
+      return image_masks
+
+    np_image_masks = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(np_image_masks.shape, np.array([0, 4, 4]))
+
+  def testBoxWithZeroArea(self):
+
+    def graph_fn():
+      box_masks = tf.zeros([1, 3, 3], dtype=tf.float32)
+      boxes = tf.constant([[0.1, 0.2, 0.1, 0.7]], dtype=tf.float32)
+      image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                         image_height=4,
+                                                         image_width=4)
+      return image_masks
+
+    np_image_masks = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(np_image_masks.shape, np.array([1, 4, 4]))
 
   def testMaskIsCenteredInImageWhenBoxIsCentered(self):
-    box_masks = tf.constant([[[1, 1],
-                              [1, 1]]], dtype=tf.float32)
-    boxes = tf.constant([[0.25, 0.25, 0.75, 0.75]], dtype=tf.float32)
-    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
-                                                       image_height=4,
-                                                       image_width=4)
+
+    def graph_fn():
+      box_masks = tf.constant([[[1, 1],
+                                [1, 1]]], dtype=tf.float32)
+      boxes = tf.constant([[0.25, 0.25, 0.75, 0.75]], dtype=tf.float32)
+      image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                         image_height=4,
+                                                         image_width=4)
+      return image_masks
+
     np_expected_image_masks = np.array([[[0, 0, 0, 0],
                                          [0, 1, 1, 0],
                                          [0, 1, 1, 0],
                                          [0, 0, 0, 0]]], dtype=np.float32)
-    with self.test_session() as sess:
-      np_image_masks = sess.run(image_masks)
-      self.assertAllClose(np_image_masks, np_expected_image_masks)
+    np_image_masks = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(np_image_masks, np_expected_image_masks)
 
   def testMaskOffCenterRemainsOffCenterInImage(self):
-    box_masks = tf.constant([[[1, 0],
-                              [0, 1]]], dtype=tf.float32)
-    boxes = tf.constant([[0.25, 0.5, 0.75, 1.0]], dtype=tf.float32)
-    image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
-                                                       image_height=4,
-                                                       image_width=4)
+
+    def graph_fn():
+      box_masks = tf.constant([[[1, 0],
+                                [0, 1]]], dtype=tf.float32)
+      boxes = tf.constant([[0.25, 0.5, 0.75, 1.0]], dtype=tf.float32)
+      image_masks = ops.reframe_box_masks_to_image_masks(box_masks, boxes,
+                                                         image_height=4,
+                                                         image_width=4)
+      return image_masks
+
     np_expected_image_masks = np.array([[[0, 0, 0, 0],
                                          [0, 0, 0.6111111, 0.16666669],
                                          [0, 0, 0.3888889, 0.83333337],
                                          [0, 0, 0, 0]]], dtype=np.float32)
-    with self.test_session() as sess:
-      np_image_masks = sess.run(image_masks)
-      self.assertAllClose(np_image_masks, np_expected_image_masks)
+    np_image_masks = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(np_image_masks, np_expected_image_masks)
 
 
-class MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):
+class MergeBoxesWithMultipleLabelsTest(test_case.TestCase):
 
   def testMergeBoxesWithMultipleLabels(self):
-    boxes = tf.constant(
-        [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],
-         [0.25, 0.25, 0.75, 0.75]],
-        dtype=tf.float32)
-    class_indices = tf.constant([0, 4, 2], dtype=tf.int32)
-    class_confidences = tf.constant([0.8, 0.2, 0.1], dtype=tf.float32)
-    num_classes = 5
-    merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
-        ops.merge_boxes_with_multiple_labels(
-            boxes, class_indices, class_confidences, num_classes))
+
+    def graph_fn():
+      boxes = tf.constant(
+          [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],
+           [0.25, 0.25, 0.75, 0.75]],
+          dtype=tf.float32)
+      class_indices = tf.constant([0, 4, 2], dtype=tf.int32)
+      class_confidences = tf.constant([0.8, 0.2, 0.1], dtype=tf.float32)
+      num_classes = 5
+      merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
+          ops.merge_boxes_with_multiple_labels(
+              boxes, class_indices, class_confidences, num_classes))
+
+      return (merged_boxes, merged_classes, merged_confidences,
+              merged_box_indices)
+
     expected_merged_boxes = np.array(
         [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75]], dtype=np.float32)
     expected_merged_classes = np.array(
@@ -1175,28 +1302,32 @@ class MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):
     expected_merged_confidences = np.array(
         [[0.8, 0, 0.1, 0, 0], [0, 0, 0, 0, 0.2]], dtype=np.float32)
     expected_merged_box_indices = np.array([0, 1], dtype=np.int32)
-    with self.test_session() as sess:
-      (np_merged_boxes, np_merged_classes, np_merged_confidences,
-       np_merged_box_indices) = sess.run(
-           [merged_boxes, merged_classes, merged_confidences,
-            merged_box_indices])
-      self.assertAllClose(np_merged_boxes, expected_merged_boxes)
-      self.assertAllClose(np_merged_classes, expected_merged_classes)
-      self.assertAllClose(np_merged_confidences, expected_merged_confidences)
-      self.assertAllClose(np_merged_box_indices, expected_merged_box_indices)
+
+    # Running on CPU only as tf.unique is not supported on TPU.
+    (np_merged_boxes, np_merged_classes, np_merged_confidences,
+     np_merged_box_indices) = self.execute_cpu(graph_fn, [])
+    self.assertAllClose(np_merged_boxes, expected_merged_boxes)
+    self.assertAllClose(np_merged_classes, expected_merged_classes)
+    self.assertAllClose(np_merged_confidences, expected_merged_confidences)
+    self.assertAllClose(np_merged_box_indices, expected_merged_box_indices)
 
   def testMergeBoxesWithMultipleLabelsCornerCase(self):
-    boxes = tf.constant(
-        [[0, 0, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1],
-         [1, 1, 1, 1], [1, 0, 1, 1], [0, 1, 1, 1], [0, 0, 1, 1]],
-        dtype=tf.float32)
-    class_indices = tf.constant([0, 1, 2, 3, 2, 1, 0, 3], dtype=tf.int32)
-    class_confidences = tf.constant([0.1, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4, 0.6],
-                                    dtype=tf.float32)
-    num_classes = 4
-    merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
-        ops.merge_boxes_with_multiple_labels(
-            boxes, class_indices, class_confidences, num_classes))
+
+    def graph_fn():
+      boxes = tf.constant(
+          [[0, 0, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1],
+           [1, 1, 1, 1], [1, 0, 1, 1], [0, 1, 1, 1], [0, 0, 1, 1]],
+          dtype=tf.float32)
+      class_indices = tf.constant([0, 1, 2, 3, 2, 1, 0, 3], dtype=tf.int32)
+      class_confidences = tf.constant([0.1, 0.9, 0.2, 0.8, 0.3, 0.7, 0.4, 0.6],
+                                      dtype=tf.float32)
+      num_classes = 4
+      merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
+          ops.merge_boxes_with_multiple_labels(
+              boxes, class_indices, class_confidences, num_classes))
+      return (merged_boxes, merged_classes, merged_confidences,
+              merged_box_indices)
+
     expected_merged_boxes = np.array(
         [[0, 0, 1, 1], [0, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1]],
         dtype=np.float32)
@@ -1207,35 +1338,42 @@ class MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):
         [[0.1, 0, 0, 0.6], [0.4, 0.9, 0, 0],
          [0, 0.7, 0.2, 0], [0, 0, 0.3, 0.8]], dtype=np.float32)
     expected_merged_box_indices = np.array([0, 1, 2, 3], dtype=np.int32)
-    with self.test_session() as sess:
-      (np_merged_boxes, np_merged_classes, np_merged_confidences,
-       np_merged_box_indices) = sess.run(
-           [merged_boxes, merged_classes, merged_confidences,
-            merged_box_indices])
-      self.assertAllClose(np_merged_boxes, expected_merged_boxes)
-      self.assertAllClose(np_merged_classes, expected_merged_classes)
-      self.assertAllClose(np_merged_confidences, expected_merged_confidences)
-      self.assertAllClose(np_merged_box_indices, expected_merged_box_indices)
+
+    # Running on CPU only as tf.unique is not supported on TPU.
+    (np_merged_boxes, np_merged_classes, np_merged_confidences,
+     np_merged_box_indices) = self.execute_cpu(graph_fn, [])
+
+    self.assertAllClose(np_merged_boxes, expected_merged_boxes)
+    self.assertAllClose(np_merged_classes, expected_merged_classes)
+    self.assertAllClose(np_merged_confidences, expected_merged_confidences)
+    self.assertAllClose(np_merged_box_indices, expected_merged_box_indices)
 
   def testMergeBoxesWithEmptyInputs(self):
-    boxes = tf.zeros([0, 4], dtype=tf.float32)
-    class_indices = tf.constant([], dtype=tf.int32)
-    class_confidences = tf.constant([], dtype=tf.float32)
-    num_classes = 5
-    merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
-        ops.merge_boxes_with_multiple_labels(
-            boxes, class_indices, class_confidences, num_classes))
-    with self.test_session() as sess:
-      (np_merged_boxes, np_merged_classes, np_merged_confidences,
-       np_merged_box_indices) = sess.run(
-           [merged_boxes, merged_classes, merged_confidences,
-            merged_box_indices])
-      self.assertAllEqual(np_merged_boxes.shape, [0, 4])
-      self.assertAllEqual(np_merged_classes.shape, [0, 5])
-      self.assertAllEqual(np_merged_confidences.shape, [0, 5])
-      self.assertAllEqual(np_merged_box_indices.shape, [0])
+
+    def graph_fn():
+      boxes = tf.zeros([0, 4], dtype=tf.float32)
+      class_indices = tf.constant([], dtype=tf.int32)
+      class_confidences = tf.constant([], dtype=tf.float32)
+      num_classes = 5
+      merged_boxes, merged_classes, merged_confidences, merged_box_indices = (
+          ops.merge_boxes_with_multiple_labels(
+              boxes, class_indices, class_confidences, num_classes))
+      return (merged_boxes, merged_classes, merged_confidences,
+              merged_box_indices)
+
+    # Running on CPU only as tf.unique is not supported on TPU.
+    (np_merged_boxes, np_merged_classes, np_merged_confidences,
+     np_merged_box_indices) = self.execute_cpu(graph_fn, [])
+    self.assertAllEqual(np_merged_boxes.shape, [0, 4])
+    self.assertAllEqual(np_merged_classes.shape, [0, 5])
+    self.assertAllEqual(np_merged_confidences.shape, [0, 5])
+    self.assertAllEqual(np_merged_box_indices.shape, [0])
 
   def testMergeBoxesWithMultipleLabelsUsesInt64(self):
+
+    if self.is_tf2():
+      self.skipTest('Getting op names is not supported in eager mode.')
+
     boxes = tf.constant(
         [[0.25, 0.25, 0.75, 0.75], [0.0, 0.0, 0.5, 0.75],
          [0.25, 0.25, 0.75, 0.75]],
@@ -1345,20 +1483,18 @@ class MatmulGatherOnZerothAxis(test_case.TestCase):
     self.assertAllClose(gather_output, expected_output)
 
   def test_gather_with_dynamic_shape_input(self):
-    params_placeholder = tf.placeholder(tf.float32, shape=[None, 4])
-    indices_placeholder = tf.placeholder(tf.int32, shape=[None])
-    gather_result = ops.matmul_gather_on_zeroth_axis(
-        params_placeholder, indices_placeholder)
+
+    def graph_fn(params, indices):
+      return ops.matmul_gather_on_zeroth_axis(params, indices)
+
     params = np.array([[1, 2, 3, 4],
                        [5, 6, 7, 8],
                        [9, 10, 11, 12],
                        [0, 1, 0, 0]], dtype=np.float32)
     indices = np.array([0, 0, 0, 0, 0, 0])
     expected_output = np.array(6*[[1, 2, 3, 4]])
-    with self.test_session() as sess:
-      gather_output = sess.run(gather_result, feed_dict={
-          params_placeholder: params, indices_placeholder: indices})
-      self.assertAllClose(gather_output, expected_output)
+    gather_output = self.execute(graph_fn, [params, indices])
+    self.assertAllClose(gather_output, expected_output)
 
 
 class FpnFeatureLevelsTest(test_case.TestCase):
@@ -1419,22 +1555,27 @@ class TestBfloat16ToFloat32(test_case.TestCase):
 class TestGatherWithPaddingValues(test_case.TestCase):
 
   def test_gather_with_padding_values(self):
-    indices = tf.constant([1, -1, 0, -1])
-    input_tensor = tf.constant([[0, 0, 0.1, 0.1], [0, 0, 0.2, 0.2]],
-                               dtype=tf.float32)
     expected_gathered_tensor = [
         [0, 0, 0.2, 0.2],
         [0, 0, 0, 0],
         [0, 0, 0.1, 0.1],
         [0, 0, 0, 0],
     ]
-    gathered_tensor = ops.gather_with_padding_values(
-        input_tensor,
-        indices=indices,
-        padding_value=tf.zeros_like(input_tensor[0]))
-    self.assertEqual(gathered_tensor.dtype, tf.float32)
-    with self.test_session():
-      gathered_tensor_np = gathered_tensor.eval()
+
+    def graph_fn():
+      indices = tf.constant([1, -1, 0, -1])
+      input_tensor = tf.constant([[0, 0, 0.1, 0.1], [0, 0, 0.2, 0.2]],
+                                 dtype=tf.float32)
+
+      gathered_tensor = ops.gather_with_padding_values(
+          input_tensor,
+          indices=indices,
+          padding_value=tf.zeros_like(input_tensor[0]))
+      self.assertEqual(gathered_tensor.dtype, tf.float32)
+
+      return gathered_tensor
+
+    gathered_tensor_np = self.execute(graph_fn, [])
     self.assertAllClose(expected_gathered_tensor, gathered_tensor_np)
 
 
diff --git a/research/object_detection/utils/patch_ops.py b/research/object_detection/utils/patch_ops.py
index 0d1524c8..d76b8357 100644
--- a/research/object_detection/utils/patch_ops.py
+++ b/research/object_detection/utils/patch_ops.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def get_patch_mask(y, x, patch_size, image_shape):
diff --git a/research/object_detection/utils/patch_ops_test.py b/research/object_detection/utils/patch_ops_test.py
index 2dc57251..c5385e33 100644
--- a/research/object_detection/utils/patch_ops_test.py
+++ b/research/object_detection/utils/patch_ops_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 
 from absl.testing import parameterized
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import patch_ops
 from object_detection.utils import test_case
diff --git a/research/object_detection/utils/per_image_evaluation_test.py b/research/object_detection/utils/per_image_evaluation_test.py
index 17566dae..02e66862 100644
--- a/research/object_detection/utils/per_image_evaluation_test.py
+++ b/research/object_detection/utils/per_image_evaluation_test.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import per_image_evaluation
 
diff --git a/research/object_detection/utils/per_image_vrd_evaluation_test.py b/research/object_detection/utils/per_image_vrd_evaluation_test.py
index dd9830fc..5aa1b1ef 100644
--- a/research/object_detection/utils/per_image_vrd_evaluation_test.py
+++ b/research/object_detection/utils/per_image_vrd_evaluation_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import per_image_vrd_evaluation
 
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index 254719bc..e21c2435 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import static_shape
 
@@ -69,17 +69,18 @@ def pad_tensor(t, length):
       is an integer, the first dimension of padded_t is set to length
       statically.
   """
-  t_rank = tf.rank(t)
-  t_shape = tf.shape(t)
-  t_d0 = t_shape[0]
-  pad_d0 = tf.expand_dims(length - t_d0, 0)
-  pad_shape = tf.cond(
-      tf.greater(t_rank, 1), lambda: tf.concat([pad_d0, t_shape[1:]], 0),
-      lambda: tf.expand_dims(length - t_d0, 0))
-  padded_t = tf.concat([t, tf.zeros(pad_shape, dtype=t.dtype)], 0)
-  if not _is_tensor(length):
-    padded_t = _set_dim_0(padded_t, length)
-  return padded_t
+
+  # Computing the padding statically makes the operation work with XLA.
+  rank = len(t.get_shape())
+  paddings = [[0 for _ in range(2)] for _ in range(rank)]
+  t_d0 = tf.shape(t)[0]
+
+  if isinstance(length, int) or len(length.get_shape()) == 0:  # pylint:disable=g-explicit-length-test
+    paddings[0][1] = length - t_d0
+  else:
+    paddings[0][1] = length[0] - t_d0
+
+  return tf.pad(t, paddings)
 
 
 def clip_tensor(t, length):
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index 8372b5b4..de4951df 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -20,216 +20,215 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import shape_utils
+from object_detection.utils import test_case
 
-# pylint: disable=g-import-not-at-top
-try:
-  from tensorflow.contrib import framework as contrib_framework
-except ImportError:
-  # TF 2.0 doesn't ship with contrib.
-  pass
-# pylint: enable=g-import-not-at-top
 
-
-class UtilTest(tf.test.TestCase):
+class UtilTest(test_case.TestCase):
 
   def test_pad_tensor_using_integer_input(self):
-    t1 = tf.constant([1], dtype=tf.int32)
-    pad_t1 = shape_utils.pad_tensor(t1, 2)
-    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
-    pad_t2 = shape_utils.pad_tensor(t2, 2)
 
-    self.assertEqual(2, pad_t1.get_shape()[0])
-    self.assertEqual(2, pad_t2.get_shape()[0])
+    print('........pad tensor using interger input.')
+    def graph_fn():
+      t1 = tf.constant([1], dtype=tf.int32)
+      pad_t1 = shape_utils.pad_tensor(t1, 2)
+      t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
+      pad_t2 = shape_utils.pad_tensor(t2, 2)
+
+      return pad_t1, pad_t2
+
+    pad_t1_result, pad_t2_result = self.execute(graph_fn, [])
 
-    with self.test_session() as sess:
-      pad_t1_result, pad_t2_result = sess.run([pad_t1, pad_t2])
-      self.assertAllEqual([1, 0], pad_t1_result)
-      self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)
+    self.assertAllEqual([1, 0], pad_t1_result)
+    self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)
 
   def test_pad_tensor_using_tensor_input(self):
-    t1 = tf.constant([1], dtype=tf.int32)
-    pad_t1 = shape_utils.pad_tensor(t1, tf.constant(2))
-    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
-    pad_t2 = shape_utils.pad_tensor(t2, tf.constant(2))
 
-    with self.test_session() as sess:
-      pad_t1_result, pad_t2_result = sess.run([pad_t1, pad_t2])
-      self.assertAllEqual([1, 0], pad_t1_result)
-      self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)
+    def graph_fn():
+      t1 = tf.constant([1], dtype=tf.int32)
+      pad_t1 = shape_utils.pad_tensor(t1, tf.constant(2))
+      t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
+      pad_t2 = shape_utils.pad_tensor(t2, tf.constant(2))
+
+      return pad_t1, pad_t2
+
+    pad_t1_result, pad_t2_result = self.execute(graph_fn, [])
+    self.assertAllEqual([1, 0], pad_t1_result)
+    self.assertAllClose([[0.1, 0.2], [0, 0]], pad_t2_result)
 
   def test_clip_tensor_using_integer_input(self):
-    t1 = tf.constant([1, 2, 3], dtype=tf.int32)
-    clip_t1 = shape_utils.clip_tensor(t1, 2)
-    t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
-    clip_t2 = shape_utils.clip_tensor(t2, 2)
 
-    self.assertEqual(2, clip_t1.get_shape()[0])
-    self.assertEqual(2, clip_t2.get_shape()[0])
+    def graph_fn():
+      t1 = tf.constant([1, 2, 3], dtype=tf.int32)
+      clip_t1 = shape_utils.clip_tensor(t1, 2)
+      t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
+      clip_t2 = shape_utils.clip_tensor(t2, 2)
 
-    with self.test_session() as sess:
-      clip_t1_result, clip_t2_result = sess.run([clip_t1, clip_t2])
-      self.assertAllEqual([1, 2], clip_t1_result)
-      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)
+      self.assertEqual(2, clip_t1.get_shape()[0])
+      self.assertEqual(2, clip_t2.get_shape()[0])
+
+      return clip_t1, clip_t2
+
+    clip_t1_result, clip_t2_result = self.execute(graph_fn, [])
+    self.assertAllEqual([1, 2], clip_t1_result)
+    self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)
 
   def test_clip_tensor_using_tensor_input(self):
-    t1 = tf.constant([1, 2, 3], dtype=tf.int32)
-    clip_t1 = shape_utils.clip_tensor(t1, tf.constant(2))
-    t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
-    clip_t2 = shape_utils.clip_tensor(t2, tf.constant(2))
 
-    with self.test_session() as sess:
-      clip_t1_result, clip_t2_result = sess.run([clip_t1, clip_t2])
-      self.assertAllEqual([1, 2], clip_t1_result)
-      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)
+    def graph_fn():
+      t1 = tf.constant([1, 2, 3], dtype=tf.int32)
+      clip_t1 = shape_utils.clip_tensor(t1, tf.constant(2))
+      t2 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
+      clip_t2 = shape_utils.clip_tensor(t2, tf.constant(2))
+
+      return clip_t1, clip_t2
+
+    clip_t1_result, clip_t2_result = self.execute(graph_fn, [])
+    self.assertAllEqual([1, 2], clip_t1_result)
+    self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], clip_t2_result)
 
   def test_pad_or_clip_tensor_using_integer_input(self):
-    t1 = tf.constant([1], dtype=tf.int32)
-    tt1 = shape_utils.pad_or_clip_tensor(t1, 2)
-    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
-    tt2 = shape_utils.pad_or_clip_tensor(t2, 2)
-
-    t3 = tf.constant([1, 2, 3], dtype=tf.int32)
-    tt3 = shape_utils.clip_tensor(t3, 2)
-    t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
-    tt4 = shape_utils.clip_tensor(t4, 2)
-
-    self.assertEqual(2, tt1.get_shape()[0])
-    self.assertEqual(2, tt2.get_shape()[0])
-    self.assertEqual(2, tt3.get_shape()[0])
-    self.assertEqual(2, tt4.get_shape()[0])
-
-    with self.test_session() as sess:
-      tt1_result, tt2_result, tt3_result, tt4_result = sess.run(
-          [tt1, tt2, tt3, tt4])
-      self.assertAllEqual([1, 0], tt1_result)
-      self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)
-      self.assertAllEqual([1, 2], tt3_result)
-      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)
+
+    def graph_fn():
+      t1 = tf.constant([1], dtype=tf.int32)
+      tt1 = shape_utils.pad_or_clip_tensor(t1, 2)
+      t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
+      tt2 = shape_utils.pad_or_clip_tensor(t2, 2)
+
+      t3 = tf.constant([1, 2, 3], dtype=tf.int32)
+      tt3 = shape_utils.clip_tensor(t3, 2)
+      t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
+      tt4 = shape_utils.clip_tensor(t4, 2)
+
+      self.assertEqual(2, tt1.get_shape()[0])
+      self.assertEqual(2, tt2.get_shape()[0])
+      self.assertEqual(2, tt3.get_shape()[0])
+      self.assertEqual(2, tt4.get_shape()[0])
+
+      return tt1, tt2, tt3, tt4
+
+    tt1_result, tt2_result, tt3_result, tt4_result = self.execute(graph_fn, [])
+    self.assertAllEqual([1, 0], tt1_result)
+    self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)
+    self.assertAllEqual([1, 2], tt3_result)
+    self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)
 
   def test_pad_or_clip_tensor_using_tensor_input(self):
-    t1 = tf.constant([1], dtype=tf.int32)
-    tt1 = shape_utils.pad_or_clip_tensor(t1, tf.constant(2))
-    t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
-    tt2 = shape_utils.pad_or_clip_tensor(t2, tf.constant(2))
-
-    t3 = tf.constant([1, 2, 3], dtype=tf.int32)
-    tt3 = shape_utils.clip_tensor(t3, tf.constant(2))
-    t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
-    tt4 = shape_utils.clip_tensor(t4, tf.constant(2))
-
-    with self.test_session() as sess:
-      tt1_result, tt2_result, tt3_result, tt4_result = sess.run(
-          [tt1, tt2, tt3, tt4])
-      self.assertAllEqual([1, 0], tt1_result)
-      self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)
-      self.assertAllEqual([1, 2], tt3_result)
-      self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)
-
-  def test_combines_static_dynamic_shape(self):
-    tensor = tf.placeholder(tf.float32, shape=(None, 2, 3))
-    combined_shape = shape_utils.combined_static_and_dynamic_shape(
-        tensor)
-    self.assertTrue(contrib_framework.is_tensor(combined_shape[0]))
-    self.assertListEqual(combined_shape[1:], [2, 3])
+
+    def graph_fn():
+      t1 = tf.constant([1], dtype=tf.int32)
+      tt1 = shape_utils.pad_or_clip_tensor(t1, tf.constant(2))
+      t2 = tf.constant([[0.1, 0.2]], dtype=tf.float32)
+      tt2 = shape_utils.pad_or_clip_tensor(t2, tf.constant(2))
+
+      t3 = tf.constant([1, 2, 3], dtype=tf.int32)
+      tt3 = shape_utils.clip_tensor(t3, tf.constant(2))
+      t4 = tf.constant([[0.1, 0.2], [0.2, 0.4], [0.5, 0.8]], dtype=tf.float32)
+      tt4 = shape_utils.clip_tensor(t4, tf.constant(2))
+
+      return tt1, tt2, tt3, tt4
+
+    tt1_result, tt2_result, tt3_result, tt4_result = self.execute(graph_fn, [])
+    self.assertAllEqual([1, 0], tt1_result)
+    self.assertAllClose([[0.1, 0.2], [0, 0]], tt2_result)
+    self.assertAllEqual([1, 2], tt3_result)
+    self.assertAllClose([[0.1, 0.2], [0.2, 0.4]], tt4_result)
+
+  def test_combined_static_dynamic_shape(self):
+
+    for n in [2, 3, 4]:
+      tensor = tf.zeros((n, 2, 3))
+      combined_shape = shape_utils.combined_static_and_dynamic_shape(
+          tensor)
+      self.assertListEqual(combined_shape[1:], [2, 3])
 
   def test_pad_or_clip_nd_tensor(self):
-    tensor_placeholder = tf.placeholder(tf.float32, [None, 5, 4, 7])
-    output_tensor = shape_utils.pad_or_clip_nd(
-        tensor_placeholder, [None, 3, 5, tf.constant(6)])
 
-    self.assertAllEqual(output_tensor.shape.as_list(), [None, 3, 5, None])
+    def graph_fn(input_tensor):
+      output_tensor = shape_utils.pad_or_clip_nd(
+          input_tensor, [None, 3, 5, tf.constant(6)])
 
-    with self.test_session() as sess:
-      output_tensor_np = sess.run(
-          output_tensor,
-          feed_dict={
-              tensor_placeholder: np.random.rand(2, 5, 4, 7),
-          })
+      return output_tensor
 
-    self.assertAllEqual(output_tensor_np.shape, [2, 3, 5, 6])
+    for n in [2, 3, 4, 5]:
+      input_np = np.zeros((n, 5, 4, 7))
+      output_tensor_np = self.execute(graph_fn, [input_np])
+      self.assertAllEqual(output_tensor_np.shape[1:], [3, 5, 6])
 
 
-class StaticOrDynamicMapFnTest(tf.test.TestCase):
+class StaticOrDynamicMapFnTest(test_case.TestCase):
 
   def test_with_dynamic_shape(self):
+
     def fn(input_tensor):
       return tf.reduce_sum(input_tensor)
-    input_tensor = tf.placeholder(tf.float32, shape=(None, 2))
-    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
-
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
-    self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
-
-    with self.test_session() as sess:
-      result1 = sess.run(
-          map_fn_output, feed_dict={
-              input_tensor: [[1, 2], [3, 1], [0, 4]]})
-      result2 = sess.run(
-          map_fn_output, feed_dict={
-              input_tensor: [[-1, 1], [0, 9]]})
-      self.assertAllEqual(result1, [3, 4, 4])
-      self.assertAllEqual(result2, [0, 9])
+
+    def graph_fn(input_tensor):
+      return shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
+
+    # The input has different shapes, but due to how self.execute()
+    # works, the shape is known at graph compile time.
+    result1 = self.execute(
+        graph_fn, [np.array([[1, 2], [3, 1], [0, 4]]),])
+    result2 = self.execute(
+        graph_fn, [np.array([[-1, 1], [0, 9]]),])
+    self.assertAllEqual(result1, [3, 4, 4])
+    self.assertAllEqual(result2, [0, 9])
 
   def test_with_static_shape(self):
     def fn(input_tensor):
       return tf.reduce_sum(input_tensor)
-    input_tensor = tf.constant([[1, 2], [3, 1], [0, 4]], dtype=tf.float32)
-    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
-    self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
+    def graph_fn():
+      input_tensor = tf.constant([[1, 2], [3, 1], [0, 4]], dtype=tf.float32)
+      return shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
 
-    with self.test_session() as sess:
-      result = sess.run(map_fn_output)
-      self.assertAllEqual(result, [3, 4, 4])
+    result = self.execute(graph_fn, [])
+    self.assertAllEqual(result, [3, 4, 4])
 
   def test_with_multiple_dynamic_shapes(self):
     def fn(elems):
       input_tensor, scalar_index_tensor = elems
       return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])
 
-    input_tensor = tf.placeholder(tf.float32, shape=(None, 3))
-    scalar_index_tensor = tf.placeholder(tf.int32, shape=(None, 1))
-    map_fn_output = shape_utils.static_or_dynamic_map_fn(
-        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
-
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
-    self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
-
-    with self.test_session() as sess:
-      result1 = sess.run(
-          map_fn_output, feed_dict={
-              input_tensor: [[1, 2, 3], [4, 5, -1], [0, 6, 9]],
-              scalar_index_tensor: [[0], [2], [1]],
-          })
-      result2 = sess.run(
-          map_fn_output, feed_dict={
-              input_tensor: [[-1, 1, 0], [3, 9, 30]],
-              scalar_index_tensor: [[1], [0]]
-          })
-      self.assertAllEqual(result1, [1, -1, 6])
-      self.assertAllEqual(result2, [1, 3])
+    def graph_fn(input_tensor, scalar_index_tensor):
+      map_fn_output = shape_utils.static_or_dynamic_map_fn(
+          fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
+      return map_fn_output
+
+    # The input has different shapes, but due to how self.execute()
+    # works, the shape is known at graph compile time.
+
+    result1 = self.execute(
+        graph_fn, [
+            np.array([[1, 2, 3], [4, 5, -1], [0, 6, 9]]),
+            np.array([[0], [2], [1]]),
+        ])
+    result2 = self.execute(
+        graph_fn, [
+            np.array([[-1, 1, 0], [3, 9, 30]]),
+            np.array([[1], [0]])
+        ])
+    self.assertAllEqual(result1, [1, -1, 6])
+    self.assertAllEqual(result2, [1, 3])
 
   def test_with_multiple_static_shapes(self):
     def fn(elems):
       input_tensor, scalar_index_tensor = elems
       return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])
 
-    input_tensor = tf.constant([[1, 2, 3], [4, 5, -1], [0, 6, 9]],
-                               dtype=tf.float32)
-    scalar_index_tensor = tf.constant([[0], [2], [1]], dtype=tf.int32)
-    map_fn_output = shape_utils.static_or_dynamic_map_fn(
-        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
+    def graph_fn():
+      input_tensor = tf.constant([[1, 2, 3], [4, 5, -1], [0, 6, 9]],
+                                 dtype=tf.float32)
+      scalar_index_tensor = tf.constant([[0], [2], [1]], dtype=tf.int32)
+      map_fn_output = shape_utils.static_or_dynamic_map_fn(
+          fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
+      return map_fn_output
 
-    op_names = [op.name for op in tf.get_default_graph().get_operations()]
-    self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
-
-    with self.test_session() as sess:
-      result = sess.run(map_fn_output)
-      self.assertAllEqual(result, [1, -1, 6])
+    result = self.execute(graph_fn, [])
+    self.assertAllEqual(result, [1, -1, 6])
 
   def test_fails_with_nested_input(self):
     def fn(input_tensor):
@@ -242,7 +241,7 @@ class StaticOrDynamicMapFnTest(tf.test.TestCase):
           fn, [input_tensor1, [input_tensor2]], dtype=tf.float32)
 
 
-class CheckMinImageShapeTest(tf.test.TestCase):
+class CheckMinImageShapeTest(test_case.TestCase):
 
   def test_check_min_image_dim_static_shape(self):
     input_tensor = tf.constant(np.zeros([1, 42, 42, 3]))
@@ -253,125 +252,151 @@ class CheckMinImageShapeTest(tf.test.TestCase):
       _ = shape_utils.check_min_image_dim(64, input_tensor)
 
   def test_check_min_image_dim_dynamic_shape(self):
-    input_placeholder = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    image_tensor = shape_utils.check_min_image_dim(33, input_placeholder)
 
-    with self.test_session() as sess:
-      sess.run(image_tensor,
-               feed_dict={input_placeholder: np.zeros([1, 42, 42, 3])})
-      with self.assertRaises(tf.errors.InvalidArgumentError):
-        sess.run(image_tensor,
-                 feed_dict={input_placeholder: np.zeros([1, 32, 32, 3])})
+    def graph_fn(input_tensor):
+      return shape_utils.check_min_image_dim(33, input_tensor)
+
+    self.execute(graph_fn,
+                 [np.zeros([1, 42, 42, 3])])
+    self.assertRaises(
+        ValueError, self.execute,
+        graph_fn, np.zeros([1, 32, 32, 3])
+    )
 
 
-class AssertShapeEqualTest(tf.test.TestCase):
+class AssertShapeEqualTest(test_case.TestCase):
 
   def test_unequal_static_shape_raises_exception(self):
     shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
     shape_b = tf.constant(np.zeros([4, 2, 3, 1]))
-    with self.assertRaisesRegexp(
-        ValueError, 'Unequal shapes'):
-      shape_utils.assert_shape_equal(
-          shape_utils.combined_static_and_dynamic_shape(shape_a),
-          shape_utils.combined_static_and_dynamic_shape(shape_b))
+    self.assertRaisesRegex(
+        ValueError, 'Unequal shapes',
+        shape_utils.assert_shape_equal,
+        shape_utils.combined_static_and_dynamic_shape(shape_a),
+        shape_utils.combined_static_and_dynamic_shape(shape_b)
+    )
 
   def test_equal_static_shape_succeeds(self):
-    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
-    shape_b = tf.constant(np.zeros([4, 2, 2, 1]))
-    with self.test_session() as sess:
-      op = shape_utils.assert_shape_equal(
+
+    def graph_fn():
+      shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+      shape_b = tf.constant(np.zeros([4, 2, 2, 1]))
+
+      shape_utils.assert_shape_equal(
           shape_utils.combined_static_and_dynamic_shape(shape_a),
           shape_utils.combined_static_and_dynamic_shape(shape_b))
-      sess.run(op)
+
+      return tf.constant(0)
+
+    self.execute(graph_fn, [])
 
   def test_unequal_dynamic_shape_raises_tf_assert(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    op = shape_utils.assert_shape_equal(
-        shape_utils.combined_static_and_dynamic_shape(tensor_a),
-        shape_utils.combined_static_and_dynamic_shape(tensor_b))
-    with self.test_session() as sess:
-      with self.assertRaises(tf.errors.InvalidArgumentError):
-        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
-                                tensor_b: np.zeros([1, 4, 4, 3])})
+
+    def graph_fn(tensor_a, tensor_b):
+      shape_utils.assert_shape_equal(
+          shape_utils.combined_static_and_dynamic_shape(tensor_a),
+          shape_utils.combined_static_and_dynamic_shape(tensor_b))
+      return tf.constant(0)
+
+    self.assertRaises(ValueError,
+                      self.execute, graph_fn,
+                      [np.zeros([1, 2, 2, 3]), np.zeros([1, 4, 4, 3])])
 
   def test_equal_dynamic_shape_succeeds(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
-    op = shape_utils.assert_shape_equal(
-        shape_utils.combined_static_and_dynamic_shape(tensor_a),
-        shape_utils.combined_static_and_dynamic_shape(tensor_b))
-    with self.test_session() as sess:
-      sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
-                              tensor_b: np.zeros([1, 2, 2, 3])})
+
+    def graph_fn(tensor_a, tensor_b):
+      shape_utils.assert_shape_equal(
+          shape_utils.combined_static_and_dynamic_shape(tensor_a),
+          shape_utils.combined_static_and_dynamic_shape(tensor_b)
+      )
+
+      return tf.constant(0)
+
+    self.execute(graph_fn, [np.zeros([1, 2, 2, 3]),
+                            np.zeros([1, 2, 2, 3])])
 
   def test_unequal_static_shape_along_first_dim_raises_exception(self):
     shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
     shape_b = tf.constant(np.zeros([6, 2, 3, 1]))
-    with self.assertRaisesRegexp(
-        ValueError, 'Unequal first dimension'):
-      shape_utils.assert_shape_equal_along_first_dimension(
-          shape_utils.combined_static_and_dynamic_shape(shape_a),
-          shape_utils.combined_static_and_dynamic_shape(shape_b))
+
+    self.assertRaisesRegexp(
+        ValueError, 'Unequal first dimension',
+        shape_utils.assert_shape_equal_along_first_dimension,
+        shape_utils.combined_static_and_dynamic_shape(shape_a),
+        shape_utils.combined_static_and_dynamic_shape(shape_b)
+    )
 
   def test_equal_static_shape_along_first_dim_succeeds(self):
-    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
-    shape_b = tf.constant(np.zeros([4, 7, 2]))
-    with self.test_session() as sess:
-      op = shape_utils.assert_shape_equal_along_first_dimension(
+
+    def graph_fn():
+      shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+      shape_b = tf.constant(np.zeros([4, 7, 2]))
+      shape_utils.assert_shape_equal_along_first_dimension(
           shape_utils.combined_static_and_dynamic_shape(shape_a),
           shape_utils.combined_static_and_dynamic_shape(shape_b))
-      sess.run(op)
+      return tf.constant(0)
+
+    self.execute(graph_fn, [])
 
   def test_unequal_dynamic_shape_along_first_dim_raises_tf_assert(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[None, None, 3])
-    op = shape_utils.assert_shape_equal_along_first_dimension(
-        shape_utils.combined_static_and_dynamic_shape(tensor_a),
-        shape_utils.combined_static_and_dynamic_shape(tensor_b))
-    with self.test_session() as sess:
-      with self.assertRaises(tf.errors.InvalidArgumentError):
-        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
-                                tensor_b: np.zeros([2, 4, 3])})
+
+    def graph_fn(tensor_a, tensor_b):
+      shape_utils.assert_shape_equal_along_first_dimension(
+          shape_utils.combined_static_and_dynamic_shape(tensor_a),
+          shape_utils.combined_static_and_dynamic_shape(tensor_b))
+
+      return tf.constant(0)
+
+    self.assertRaises(ValueError,
+                      self.execute, graph_fn,
+                      [np.zeros([1, 2, 2, 3]), np.zeros([2, 4, 3])])
 
   def test_equal_dynamic_shape_along_first_dim_succeeds(self):
-    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
-    tensor_b = tf.placeholder(tf.float32, shape=[None])
-    op = shape_utils.assert_shape_equal_along_first_dimension(
-        shape_utils.combined_static_and_dynamic_shape(tensor_a),
-        shape_utils.combined_static_and_dynamic_shape(tensor_b))
-    with self.test_session() as sess:
-      sess.run(op, feed_dict={tensor_a: np.zeros([5, 2, 2, 3]),
-                              tensor_b: np.zeros([5])})
+
+    def graph_fn(tensor_a, tensor_b):
+      shape_utils.assert_shape_equal_along_first_dimension(
+          shape_utils.combined_static_and_dynamic_shape(tensor_a),
+          shape_utils.combined_static_and_dynamic_shape(tensor_b))
+      return tf.constant(0)
+
+    self.execute(graph_fn, [np.zeros([5, 2, 2, 3]), np.zeros([5])])
 
 
-class FlattenExpandDimensionTest(tf.test.TestCase):
+class FlattenExpandDimensionTest(test_case.TestCase):
 
   def test_flatten_given_dims(self):
-    inputs = tf.random_uniform([5, 2, 10, 10, 3])
-    actual_flattened = shape_utils.flatten_dimensions(inputs, first=1, last=3)
-    expected_flattened = tf.reshape(inputs, [5, 20, 10, 3])
-    with self.test_session() as sess:
-      (actual_flattened_np,
-       expected_flattened_np) = sess.run([actual_flattened, expected_flattened])
+
+    def graph_fn():
+      inputs = tf.random_uniform([5, 2, 10, 10, 3])
+      actual_flattened = shape_utils.flatten_dimensions(inputs, first=1, last=3)
+      expected_flattened = tf.reshape(inputs, [5, 20, 10, 3])
+
+      return actual_flattened, expected_flattened
+
+    (actual_flattened_np,
+     expected_flattened_np) = self.execute(graph_fn, [])
     self.assertAllClose(expected_flattened_np, actual_flattened_np)
 
   def test_raises_value_error_incorrect_dimensions(self):
     inputs = tf.random_uniform([5, 2, 10, 10, 3])
-    with self.assertRaises(ValueError):
-      shape_utils.flatten_dimensions(inputs, first=0, last=6)
+    self.assertRaises(ValueError,
+                      shape_utils.flatten_dimensions, inputs,
+                      first=0, last=6)
 
   def test_flatten_first_two_dimensions(self):
-    inputs = tf.constant(
-        [
-            [[1, 2], [3, 4]],
-            [[5, 6], [7, 8]],
-            [[9, 10], [11, 12]]
-        ], dtype=tf.int32)
-    flattened_tensor = shape_utils.flatten_first_n_dimensions(
-        inputs, 2)
-    with self.test_session() as sess:
-      flattened_tensor_out = sess.run(flattened_tensor)
+
+    def graph_fn():
+      inputs = tf.constant(
+          [
+              [[1, 2], [3, 4]],
+              [[5, 6], [7, 8]],
+              [[9, 10], [11, 12]]
+          ], dtype=tf.int32)
+      flattened_tensor = shape_utils.flatten_first_n_dimensions(
+          inputs, 2)
+      return flattened_tensor
+
+    flattened_tensor_out = self.execute(graph_fn, [])
 
     expected_output = [[1, 2],
                        [3, 4],
@@ -382,20 +407,23 @@ class FlattenExpandDimensionTest(tf.test.TestCase):
     self.assertAllEqual(expected_output, flattened_tensor_out)
 
   def test_expand_first_dimension(self):
-    inputs = tf.constant(
-        [
-            [1, 2],
-            [3, 4],
-            [5, 6],
-            [7, 8],
-            [9, 10],
-            [11, 12]
-        ], dtype=tf.int32)
-    dims = [3, 2]
-    expanded_tensor = shape_utils.expand_first_dimension(
-        inputs, dims)
-    with self.test_session() as sess:
-      expanded_tensor_out = sess.run(expanded_tensor)
+
+    def graph_fn():
+      inputs = tf.constant(
+          [
+              [1, 2],
+              [3, 4],
+              [5, 6],
+              [7, 8],
+              [9, 10],
+              [11, 12]
+          ], dtype=tf.int32)
+      dims = [3, 2]
+      expanded_tensor = shape_utils.expand_first_dimension(
+          inputs, dims)
+      return expanded_tensor
+
+    expanded_tensor_out = self.execute(graph_fn, [])
 
     expected_output = [
         [[1, 2], [3, 4]],
@@ -404,19 +432,20 @@ class FlattenExpandDimensionTest(tf.test.TestCase):
     self.assertAllEqual(expected_output, expanded_tensor_out)
 
   def test_expand_first_dimension_with_incompatible_dims(self):
-    inputs_default = tf.constant(
-        [
-            [[1, 2]],
-            [[3, 4]],
-            [[5, 6]],
-        ], dtype=tf.int32)
-    inputs = tf.placeholder_with_default(inputs_default, [None, 1, 2])
-    dims = [3, 2]
-    expanded_tensor = shape_utils.expand_first_dimension(
-        inputs, dims)
-    with self.test_session() as sess:
-      with self.assertRaises(tf.errors.InvalidArgumentError):
-        sess.run(expanded_tensor)
+
+    def graph_fn():
+      inputs = tf.constant(
+          [
+              [[1, 2]],
+              [[3, 4]],
+              [[5, 6]],
+          ], dtype=tf.int32)
+      dims = [3, 2]
+      expanded_tensor = shape_utils.expand_first_dimension(
+          inputs, dims)
+      return expanded_tensor
+
+    self.assertRaises(ValueError, self.execute, graph_fn, [])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/utils/spatial_transform_ops.py b/research/object_detection/utils/spatial_transform_ops.py
index b0dee876..95aaf967 100644
--- a/research/object_detection/utils/spatial_transform_ops.py
+++ b/research/object_detection/utils/spatial_transform_ops.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 def _coordinate_vector_1d(start, end, size, align_endpoints):
diff --git a/research/object_detection/utils/spatial_transform_ops_test.py b/research/object_detection/utils/spatial_transform_ops_test.py
index 6d12968e..d1845664 100644
--- a/research/object_detection/utils/spatial_transform_ops_test.py
+++ b/research/object_detection/utils/spatial_transform_ops_test.py
@@ -20,7 +20,7 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import spatial_transform_ops as spatial_ops
 from object_detection.utils import test_case
diff --git a/research/object_detection/utils/static_shape_test.py b/research/object_detection/utils/static_shape_test.py
index fd677917..95366e05 100644
--- a/research/object_detection/utils/static_shape_test.py
+++ b/research/object_detection/utils/static_shape_test.py
@@ -19,7 +19,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import static_shape
 
diff --git a/research/object_detection/utils/target_assigner_utils.py b/research/object_detection/utils/target_assigner_utils.py
index 9a2707d6..ca7918f3 100644
--- a/research/object_detection/utils/target_assigner_utils.py
+++ b/research/object_detection/utils/target_assigner_utils.py
@@ -14,7 +14,7 @@
 # ==============================================================================
 """Utility functions used by target assigner."""
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import shape_utils
 
diff --git a/research/object_detection/utils/target_assigner_utils_test.py b/research/object_detection/utils/target_assigner_utils_test.py
index 6fca5510..b895cca0 100644
--- a/research/object_detection/utils/target_assigner_utils_test.py
+++ b/research/object_detection/utils/target_assigner_utils_test.py
@@ -15,7 +15,7 @@
 """Tests for utils.target_assigner_utils."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.utils import target_assigner_utils as ta_utils
 from object_detection.utils import test_case
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
index 4cc6978b..3839cbd0 100644
--- a/research/object_detection/utils/test_case.py
+++ b/research/object_detection/utils/test_case.py
@@ -18,8 +18,9 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.python import tf2  # pylint: disable=import-outside-toplevel
+from object_detection.utils import tf_version
 if not tf2.enabled():
   from tensorflow.contrib import tpu as contrib_tpu  # pylint: disable=g-import-not-at-top, line-too-long
 
@@ -58,9 +59,9 @@ class TestCase(tf.test.TestCase):
 
   def is_tf2(self):
     """Returns whether TF2 is enabled."""
-    return tf2.enabled()
+    return tf_version.is_tf2()
 
-  def execute_tpu_tf1(self, compute_fn, inputs):
+  def execute_tpu_tf1(self, compute_fn, inputs, graph=None):
     """Executes compute_fn on TPU with Tensorflow 1.X.
 
     Args:
@@ -68,11 +69,13 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single numpy array.
     """
-    with self.test_session(graph=tf.Graph()) as sess:
+    with self.session(graph=(graph or tf.Graph())) as sess:
       placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
       def wrap_graph_fn(*args, **kwargs):
         results = compute_fn(*args, **kwargs)
@@ -117,7 +120,7 @@ class TestCase(tf.test.TestCase):
     tf.tpu.experimental.shutdown_tpu_system()
     return self.maybe_extract_single_output(outputs)
 
-  def execute_cpu_tf1(self, compute_fn, inputs):
+  def execute_cpu_tf1(self, compute_fn, inputs, graph=None):
     """Executes compute_fn on CPU with Tensorflow 1.X.
 
     Args:
@@ -125,13 +128,15 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single numpy array.
     """
     if self.is_tf2():
       raise ValueError('Required version Tenforflow 1.X is not available.')
-    with self.test_session(graph=tf.Graph()) as sess:
+    with self.session(graph=(graph or tf.Graph())) as sess:
       placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
       results = compute_fn(*placeholders)
       if (not (isinstance(results, dict) or isinstance(results, tf.Tensor)) and
@@ -163,7 +168,7 @@ class TestCase(tf.test.TestCase):
       return compute_fn(*tf_inputs)
     return self.maybe_extract_single_output(run())
 
-  def execute_cpu(self, compute_fn, inputs):
+  def execute_cpu(self, compute_fn, inputs, graph=None):
     """Executes compute_fn on CPU.
 
     Depending on the underlying TensorFlow installation (build deps) runs in
@@ -174,6 +179,8 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single tensor.
@@ -181,9 +188,9 @@ class TestCase(tf.test.TestCase):
     if self.is_tf2():
       return self.execute_cpu_tf2(compute_fn, inputs)
     else:
-      return self.execute_cpu_tf1(compute_fn, inputs)
+      return self.execute_cpu_tf1(compute_fn, inputs, graph)
 
-  def execute_tpu(self, compute_fn, inputs):
+  def execute_tpu(self, compute_fn, inputs, graph=None):
     """Executes compute_fn on TPU.
 
     Depending on the underlying TensorFlow installation (build deps) runs in
@@ -194,6 +201,8 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single tensor.
@@ -203,7 +212,7 @@ class TestCase(tf.test.TestCase):
     if self.is_tf2():
       return self.execute_tpu_tf2(compute_fn, inputs)
     else:
-      return self.execute_tpu_tf1(compute_fn, inputs)
+      return self.execute_tpu_tf1(compute_fn, inputs, graph)
 
   def execute_tf2(self, compute_fn, inputs):
     """Runs compute_fn with TensorFlow 2.0.
@@ -226,7 +235,7 @@ class TestCase(tf.test.TestCase):
     else:
       return self.execute_cpu_tf2(compute_fn, inputs)
 
-  def execute_tf1(self, compute_fn, inputs):
+  def execute_tf1(self, compute_fn, inputs, graph=None):
     """Runs compute_fn with TensorFlow 1.X.
 
     Executes on TPU if available, otherwise executes on CPU.
@@ -236,6 +245,8 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single tensor.
@@ -243,11 +254,11 @@ class TestCase(tf.test.TestCase):
     if self.is_tf2():
       raise ValueError('Required version Tenforflow 1.X is not available.')
     if self.has_tpu():
-      return self.execute_tpu_tf1(compute_fn, inputs)
+      return self.execute_tpu_tf1(compute_fn, inputs, graph)
     else:
-      return self.execute_cpu_tf1(compute_fn, inputs)
+      return self.execute_cpu_tf1(compute_fn, inputs, graph)
 
-  def execute(self, compute_fn, inputs):
+  def execute(self, compute_fn, inputs, graph=None):
     """Runs compute_fn with inputs and returns results.
 
     * Executes in either TF1.X or TF2.X style based on the TensorFlow version.
@@ -258,6 +269,8 @@ class TestCase(tf.test.TestCase):
         of input numpy tensors, performs computation and returns output numpy
         tensors.
       inputs: a list of numpy arrays to feed input to the `compute_fn`.
+      graph: (optional) If not None, provided `graph` is used for computation
+        instead of a brand new tf.Graph().
 
     Returns:
       A list of numpy arrays or a single tensor.
@@ -267,6 +280,6 @@ class TestCase(tf.test.TestCase):
     elif not self.has_tpu() and tf2.enabled():
       return self.execute_cpu_tf2(compute_fn, inputs)
     elif self.has_tpu() and not tf2.enabled():
-      return self.execute_tpu_tf1(compute_fn, inputs)
+      return self.execute_tpu_tf1(compute_fn, inputs, graph)
     else:
-      return self.execute_cpu_tf1(compute_fn, inputs)
+      return self.execute_cpu_tf1(compute_fn, inputs, graph)
diff --git a/research/object_detection/utils/test_case_test.py b/research/object_detection/utils/test_case_test.py
index a7a8d84b..e20a4108 100644
--- a/research/object_detection/utils/test_case_test.py
+++ b/research/object_detection/utils/test_case_test.py
@@ -15,7 +15,7 @@
 """Tests for google3.third_party.tensorflow_models.object_detection.utils.test_case."""
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from object_detection.utils import test_case
 
 
diff --git a/research/object_detection/utils/test_utils.py b/research/object_detection/utils/test_utils.py
index 535378b0..f7e92c0b 100644
--- a/research/object_detection/utils/test_utils.py
+++ b/research/object_detection/utils/test_utils.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import numpy as np
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import anchor_generator
 from object_detection.core import box_coder
@@ -29,6 +29,7 @@ from object_detection.core import box_list
 from object_detection.core import box_predictor
 from object_detection.core import matcher
 from object_detection.utils import shape_utils
+from object_detection.utils import tf_version
 
 # Default size (both width and height) used for testing mask predictions.
 DEFAULT_MASK_SIZE = 5
@@ -233,3 +234,40 @@ def first_rows_close_as_set(a, b, k=None, rtol=1e-6, atol=1e-6):
       np.allclose(entry_a, entry_b, rtol, atol)
       for (entry_a, entry_b) in zip(a_sorted, b_sorted)
   ])
+
+
+class GraphContextOrNone(object):
+  """A new Graph context for TF1.X and None for TF2.X.
+
+  This is useful to write model tests that work with both TF1.X and TF2.X.
+
+  Example test using this pattern:
+
+  class ModelTest(test_case.TestCase):
+    def test_model(self):
+      with test_utils.GraphContextOrNone() as g:
+        model = Model()
+      def compute_fn():
+        out = model.predict()
+        return out['detection_boxes']
+      boxes = self.execute(compute_fn, [], graph=g)
+      self.assertAllClose(boxes, expected_boxes)
+  """
+
+  def __init__(self):
+    if tf_version.is_tf2():
+      self.graph = None
+    else:
+      self.graph = tf.Graph().as_default()
+
+  def __enter__(self):
+    if tf_version.is_tf2():
+      return None
+    else:
+      return self.graph.__enter__()
+
+  def __exit__(self, ttype, value, traceback):
+    if tf_version.is_tf2():
+      return False
+    else:
+      return self.graph.__exit__(ttype, value, traceback)
diff --git a/research/object_detection/utils/test_utils_test.py b/research/object_detection/utils/test_utils_test.py
index bcb14eab..f274a2e6 100644
--- a/research/object_detection/utils/test_utils_test.py
+++ b/research/object_detection/utils/test_utils_test.py
@@ -20,12 +20,13 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
+from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
 
-class TestUtilsTest(tf.test.TestCase):
+class TestUtilsTest(test_case.TestCase):
 
   def test_diagonal_gradient_image(self):
     """Tests if a good pyramid image is created."""
@@ -67,10 +68,10 @@ class TestUtilsTest(tf.test.TestCase):
     self.assertAllEqual(boxes[:, 0] < boxes[:, 2], true_column)
     self.assertAllEqual(boxes[:, 1] < boxes[:, 3], true_column)
 
-    self.assertTrue(boxes[:, 0].min() >= 0)
-    self.assertTrue(boxes[:, 1].min() >= 0)
-    self.assertTrue(boxes[:, 2].max() <= max_height)
-    self.assertTrue(boxes[:, 3].max() <= max_width)
+    self.assertGreaterEqual(boxes[:, 0].min(), 0)
+    self.assertGreaterEqual(boxes[:, 1].min(), 0)
+    self.assertLessEqual(boxes[:, 2].max(), max_height)
+    self.assertLessEqual(boxes[:, 3].max(), max_width)
 
   def test_first_rows_close_as_set(self):
     a = [1, 2, 3, 0, 0]
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 0b1198d4..327f3b67 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -23,9 +23,9 @@ from __future__ import print_function
 import logging
 import re
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
-from tensorflow.contrib import slim
 from tensorflow.python.ops import variables as tf_variables
 
 
@@ -47,6 +47,8 @@ def filter_variables(variables, filter_regex_list, invert=False):
   Returns:
     a list of filtered variables.
   """
+  if tf.executing_eagerly():
+    raise ValueError('Accessing variables is not supported in eager mode.')
   kept_vars = []
   variables_to_ignore_patterns = list([fre for fre in filter_regex_list if fre])
   for var in variables:
@@ -72,6 +74,8 @@ def multiply_gradients_matching_regex(grads_and_vars, regex_list, multiplier):
   Returns:
     grads_and_vars: A list of gradient to variable pairs (tuples).
   """
+  if tf.executing_eagerly():
+    raise ValueError('Accessing variables is not supported in eager mode.')
   variables = [pair[1] for pair in grads_and_vars]
   matching_vars = filter_variables(variables, regex_list, invert=True)
   for var in matching_vars:
@@ -93,6 +97,8 @@ def freeze_gradients_matching_regex(grads_and_vars, regex_list):
     grads_and_vars: A list of gradient to variable pairs (tuples) that do not
       contain the variables and gradients matching the regex.
   """
+  if tf.executing_eagerly():
+    raise ValueError('Accessing variables is not supported in eager mode.')
   variables = [pair[1] for pair in grads_and_vars]
   matching_vars = filter_variables(variables, regex_list, invert=True)
   kept_grads_and_vars = [pair for pair in grads_and_vars
@@ -123,6 +129,8 @@ def get_variables_available_in_checkpoint(variables,
   Raises:
     ValueError: if `variables` is not a list or dict.
   """
+  if tf.executing_eagerly():
+    raise ValueError('Accessing variables is not supported in eager mode.')
   if isinstance(variables, list):
     variable_names_map = {}
     for variable in variables:
@@ -170,6 +178,8 @@ def get_global_variables_safely():
   Returns:
     The result of tf.global_variables()
   """
+  if tf.executing_eagerly():
+    raise ValueError('Accessing variables is not supported in eager mode.')
   with tf.init_scope():
     if tf.executing_eagerly():
       raise ValueError("Global variables collection is not tracked when "
diff --git a/research/object_detection/utils/variables_helper_test.py b/research/object_detection/utils/variables_helper_test.py
index 058c799e..44e72d0d 100644
--- a/research/object_detection/utils/variables_helper_test.py
+++ b/research/object_detection/utils/variables_helper_test.py
@@ -21,12 +21,13 @@ from __future__ import print_function
 
 import os
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
+from object_detection.utils import test_case
 from object_detection.utils import variables_helper
 
 
-class FilterVariablesTest(tf.test.TestCase):
+class FilterVariablesTest(test_case.TestCase):
 
   def _create_variables(self):
     return [tf.Variable(1.0, name='FeatureExtractor/InceptionV3/weights'),
@@ -37,26 +38,26 @@ class FilterVariablesTest(tf.test.TestCase):
   def test_return_all_variables_when_empty_regex(self):
     variables = self._create_variables()
     out_variables = variables_helper.filter_variables(variables, [''])
-    self.assertItemsEqual(out_variables, variables)
+    self.assertCountEqual(out_variables, variables)
 
   def test_return_variables_which_do_not_match_single_regex(self):
     variables = self._create_variables()
     out_variables = variables_helper.filter_variables(variables,
                                                       ['FeatureExtractor/.*'])
-    self.assertItemsEqual(out_variables, variables[2:])
+    self.assertCountEqual(out_variables, variables[2:])
 
   def test_return_variables_which_do_not_match_any_regex_in_list(self):
     variables = self._create_variables()
     out_variables = variables_helper.filter_variables(variables, [
         'FeatureExtractor.*biases', 'StackProposalGenerator.*biases'
     ])
-    self.assertItemsEqual(out_variables, [variables[0], variables[2]])
+    self.assertCountEqual(out_variables, [variables[0], variables[2]])
 
   def test_return_variables_matching_empty_regex_list(self):
     variables = self._create_variables()
     out_variables = variables_helper.filter_variables(
         variables, [''], invert=True)
-    self.assertItemsEqual(out_variables, [])
+    self.assertCountEqual(out_variables, [])
 
   def test_return_variables_matching_some_regex_in_list(self):
     variables = self._create_variables()
@@ -64,7 +65,7 @@ class FilterVariablesTest(tf.test.TestCase):
         variables,
         ['FeatureExtractor.*biases', 'StackProposalGenerator.*biases'],
         invert=True)
-    self.assertItemsEqual(out_variables, [variables[1], variables[3]])
+    self.assertCountEqual(out_variables, [variables[1], variables[3]])
 
 
 class MultiplyGradientsMatchingRegexTest(tf.test.TestCase):
@@ -90,7 +91,7 @@ class MultiplyGradientsMatchingRegexTest(tf.test.TestCase):
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
-      self.assertItemsEqual(output, exp_output)
+      self.assertCountEqual(output, exp_output)
 
   def test_multiply_all_bias_variables(self):
     grads_and_vars = self._create_grads_and_vars()
@@ -103,10 +104,10 @@ class MultiplyGradientsMatchingRegexTest(tf.test.TestCase):
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
-      self.assertItemsEqual(output, exp_output)
+      self.assertCountEqual(output, exp_output)
 
 
-class FreezeGradientsMatchingRegexTest(tf.test.TestCase):
+class FreezeGradientsMatchingRegexTest(test_case.TestCase):
 
   def _create_grads_and_vars(self):
     return [(tf.constant(1.0),
@@ -128,10 +129,10 @@ class FreezeGradientsMatchingRegexTest(tf.test.TestCase):
     with self.test_session() as sess:
       sess.run(init_op)
       output = sess.run(grads_and_vars)
-      self.assertItemsEqual(output, exp_output)
+      self.assertCountEqual(output, exp_output)
 
 
-class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
+class GetVariablesAvailableInCheckpointTest(test_case.TestCase):
 
   def test_return_all_variables_from_checkpoint(self):
     with tf.Graph().as_default():
@@ -147,7 +148,7 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
         saver.save(sess, checkpoint_path)
       out_variables = variables_helper.get_variables_available_in_checkpoint(
           variables, checkpoint_path)
-    self.assertItemsEqual(out_variables, variables)
+    self.assertCountEqual(out_variables, variables)
 
   def test_return_all_variables_from_checkpoint_with_partition(self):
     with tf.Graph().as_default():
@@ -165,7 +166,7 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
         saver.save(sess, checkpoint_path)
       out_variables = variables_helper.get_variables_available_in_checkpoint(
           variables, checkpoint_path)
-    self.assertItemsEqual(out_variables, variables)
+    self.assertCountEqual(out_variables, variables)
 
   def test_return_variables_available_in_checkpoint(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
@@ -186,7 +187,7 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
       graph2_variables = graph1_variables + [tf.Variable(1.0, name='biases')]
       out_variables = variables_helper.get_variables_available_in_checkpoint(
           graph2_variables, checkpoint_path, include_global_step=False)
-    self.assertItemsEqual(out_variables, [weight_variable])
+    self.assertCountEqual(out_variables, [weight_variable])
 
   def test_return_variables_available_an_checkpoint_with_dict_inputs(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
@@ -208,9 +209,9 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
       out_variables = variables_helper.get_variables_available_in_checkpoint(
           graph2_variables_dict, checkpoint_path)
 
-    self.assertTrue(isinstance(out_variables, dict))
-    self.assertItemsEqual(list(out_variables.keys()), ['ckpt_weights'])
-    self.assertTrue(out_variables['ckpt_weights'].op.name == 'weights')
+    self.assertIsInstance(out_variables, dict)
+    self.assertCountEqual(list(out_variables.keys()), ['ckpt_weights'])
+    self.assertEqual(out_variables['ckpt_weights'].op.name, 'weights')
 
   def test_return_variables_with_correct_sizes(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'model.ckpt')
@@ -237,7 +238,7 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
 
     out_variables = variables_helper.get_variables_available_in_checkpoint(
         graph2_variables, checkpoint_path, include_global_step=True)
-    self.assertItemsEqual(out_variables, [bias_variable, global_step])
+    self.assertCountEqual(out_variables, [bias_variable, global_step])
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index f978123e..058a47b1 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -36,7 +36,7 @@ import PIL.ImageFont as ImageFont
 import six
 from six.moves import range
 from six.moves import zip
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import keypoint_ops
 from object_detection.core import standard_fields as fields
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index 6f017297..2909a54a 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -25,15 +25,28 @@ import numpy as np
 import PIL.Image as Image
 import six
 from six.moves import range
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields as fields
+from object_detection.utils import test_case
 from object_detection.utils import visualization_utils
 
 _TESTDATA_PATH = 'object_detection/test_images'
 
 
-class VisualizationUtilsTest(tf.test.TestCase):
+def get_iterator_next_for_testing(dataset, is_tf2):
+
+  # In TF2, lookup tables are not supported in one shot iterators, but
+  # initialization is implicit.
+  if is_tf2:
+    return dataset.make_initializable_iterator().get_next()
+  # In TF1, we use one shot iterator because it does not require running
+  # a separate init op.
+  else:
+    return dataset.make_one_shot_iterator().get_next()
+
+
+class VisualizationUtilsTest(test_case.TestCase):
 
   def test_get_prime_multiplier_for_color_randomness(self):
     # Show that default multipler is not 1 and does not divide the total number
@@ -151,13 +164,12 @@ class VisualizationUtilsTest(tf.test.TestCase):
   def test_draw_bounding_boxes_on_image_tensors(self):
     """Tests that bounding box utility produces reasonable results."""
     category_index = {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}}
-
     fname = os.path.join(_TESTDATA_PATH, 'image1.jpg')
     image_np = np.array(Image.open(fname))
     images_np = np.stack((image_np, image_np), axis=0)
     original_image_shape = [[636, 512], [636, 512]]
 
-    with tf.Graph().as_default():
+    def graph_fn():
       images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
       image_shape = tf.constant(original_image_shape, dtype=tf.int32)
       boxes = tf.constant([[[0.4, 0.25, 0.75, 0.75], [0.5, 0.3, 0.6, 0.9]],
@@ -178,22 +190,20 @@ class VisualizationUtilsTest(tf.test.TestCase):
               keypoints=keypoints,
               min_score_thresh=0.2,
               keypoint_edges=keypoint_edges))
-
-      with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
-
-        # Write output images for visualization.
-        images_with_boxes_np = sess.run(images_with_boxes)
-        self.assertEqual(images_np.shape[0], images_with_boxes_np.shape[0])
-        self.assertEqual(images_np.shape[3], images_with_boxes_np.shape[3])
-        self.assertEqual(
-            tuple(original_image_shape[0]), images_with_boxes_np.shape[1:3])
-        for i in range(images_with_boxes_np.shape[0]):
-          img_name = 'image_' + str(i) + '.png'
-          output_file = os.path.join(self.get_temp_dir(), img_name)
-          logging.info('Writing output image %d to %s', i, output_file)
-          image_pil = Image.fromarray(images_with_boxes_np[i, ...])
-          image_pil.save(output_file)
+      return images_with_boxes
+
+    # Write output images for visualization.
+    images_with_boxes_np = self.execute(graph_fn, [])
+    self.assertEqual(images_np.shape[0], images_with_boxes_np.shape[0])
+    self.assertEqual(images_np.shape[3], images_with_boxes_np.shape[3])
+    self.assertEqual(
+        tuple(original_image_shape[0]), images_with_boxes_np.shape[1:3])
+    for i in range(images_with_boxes_np.shape[0]):
+      img_name = 'image_' + str(i) + '.png'
+      output_file = os.path.join(self.get_temp_dir(), img_name)
+      logging.info('Writing output image %d to %s', i, output_file)
+      image_pil = Image.fromarray(images_with_boxes_np[i, ...])
+      image_pil.save(output_file)
 
   def test_draw_bounding_boxes_on_image_tensors_with_track_ids(self):
     """Tests that bounding box utility produces reasonable results."""
@@ -204,7 +214,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
     images_np = np.stack((image_np, image_np), axis=0)
     original_image_shape = [[636, 512], [636, 512]]
 
-    with tf.Graph().as_default():
+    def graph_fn():
       images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
       image_shape = tf.constant(original_image_shape, dtype=tf.int32)
       boxes = tf.constant([[[0.4, 0.25, 0.75, 0.75],
@@ -227,22 +237,20 @@ class VisualizationUtilsTest(tf.test.TestCase):
               true_image_shape=image_shape,
               track_ids=track_ids,
               min_score_thresh=0.2))
-
-      with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
-
-        # Write output images for visualization.
-        images_with_boxes_np = sess.run(images_with_boxes)
-        self.assertEqual(images_np.shape[0], images_with_boxes_np.shape[0])
-        self.assertEqual(images_np.shape[3], images_with_boxes_np.shape[3])
-        self.assertEqual(
-            tuple(original_image_shape[0]), images_with_boxes_np.shape[1:3])
-        for i in range(images_with_boxes_np.shape[0]):
-          img_name = 'image_with_track_ids_' + str(i) + '.png'
-          output_file = os.path.join(self.get_temp_dir(), img_name)
-          logging.info('Writing output image %d to %s', i, output_file)
-          image_pil = Image.fromarray(images_with_boxes_np[i, ...])
-          image_pil.save(output_file)
+      return images_with_boxes
+
+    # Write output images for visualization.
+    images_with_boxes_np = self.execute(graph_fn, [])
+    self.assertEqual(images_np.shape[0], images_with_boxes_np.shape[0])
+    self.assertEqual(images_np.shape[3], images_with_boxes_np.shape[3])
+    self.assertEqual(
+        tuple(original_image_shape[0]), images_with_boxes_np.shape[1:3])
+    for i in range(images_with_boxes_np.shape[0]):
+      img_name = 'image_with_track_ids_' + str(i) + '.png'
+      output_file = os.path.join(self.get_temp_dir(), img_name)
+      logging.info('Writing output image %d to %s', i, output_file)
+      image_pil = Image.fromarray(images_with_boxes_np[i, ...])
+      image_pil.save(output_file)
 
   def test_draw_bounding_boxes_on_image_tensors_with_additional_channels(self):
     """Tests the case where input image tensor has more than 3 channels."""
@@ -250,7 +258,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
     image_np = self.create_test_image_with_five_channels()
     images_np = np.stack((image_np, image_np), axis=0)
 
-    with tf.Graph().as_default():
+    def graph_fn():
       images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
       boxes = tf.constant(0, dtype=tf.float32, shape=[2, 0, 4])
       classes = tf.constant(0, dtype=tf.int64, shape=[2, 0])
@@ -264,11 +272,10 @@ class VisualizationUtilsTest(tf.test.TestCase):
               category_index,
               min_score_thresh=0.2))
 
-      with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
+      return images_with_boxes
 
-        final_images_np = sess.run(images_with_boxes)
-        self.assertEqual((2, 100, 200, 3), final_images_np.shape)
+    final_images_np = self.execute(graph_fn, [])
+    self.assertEqual((2, 100, 200, 3), final_images_np.shape)
 
   def test_draw_bounding_boxes_on_image_tensors_grayscale(self):
     """Tests the case where input image tensor has one channel."""
@@ -276,7 +283,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
     image_np = self.create_test_grayscale_image()
     images_np = np.stack((image_np, image_np), axis=0)
 
-    with tf.Graph().as_default():
+    def graph_fn():
       images_tensor = tf.constant(value=images_np, dtype=tf.uint8)
       image_shape = tf.constant([[100, 200], [100, 200]], dtype=tf.int32)
       boxes = tf.constant(0, dtype=tf.float32, shape=[2, 0, 4])
@@ -293,11 +300,10 @@ class VisualizationUtilsTest(tf.test.TestCase):
               true_image_shape=image_shape,
               min_score_thresh=0.2))
 
-      with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
+      return images_with_boxes
 
-        final_images_np = sess.run(images_with_boxes)
-        self.assertEqual((2, 100, 200, 3), final_images_np.shape)
+    final_images_np = self.execute(graph_fn, [])
+    self.assertEqual((2, 100, 200, 3), final_images_np.shape)
 
   def test_draw_keypoints_on_image(self):
     test_image = self.create_colorful_test_image()
@@ -407,7 +413,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
     heatmap2 = np.asarray([[0, 1],
                            [1, 0]], dtype=np.float)
     heatmaps = np.stack([heatmap1, heatmap2], axis=0)
-    with tf.Graph().as_default():
+    def graph_fn():
       image_tensor = tf.constant(test_image, dtype=tf.uint8)
       image_tensor = tf.expand_dims(image_tensor, axis=0)
       heatmaps_tensor = tf.expand_dims(
@@ -417,34 +423,38 @@ class VisualizationUtilsTest(tf.test.TestCase):
           heatmaps=heatmaps_tensor,
           apply_sigmoid=False)
 
-      with self.test_session() as sess:
-        sess.run(tf.global_variables_initializer())
+      return output_image
 
-        output_image_np = sess.run(output_image)
-      self.assertAllEqual(
-          output_image_np,
-          np.expand_dims(
-              np.array([[[240, 248, 255], [127, 255, 0]],
-                        [[127, 255, 0], [240, 248, 255]]]),
-              axis=0))
+    output_image_np = self.execute(graph_fn, [])
+    self.assertAllEqual(
+        output_image_np,
+        np.expand_dims(
+            np.array([[[240, 248, 255], [127, 255, 0]],
+                      [[127, 255, 0], [240, 248, 255]]]),
+            axis=0))
 
   def test_add_cdf_image_summary(self):
-    values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
-    visualization_utils.add_cdf_image_summary(values, 'PositiveAnchorLoss')
-    cdf_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
-    with self.test_session():
-      cdf_image_summary.eval()
+    def graph_fn():
+      values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
+      visualization_utils.add_cdf_image_summary(values, 'PositiveAnchorLoss')
+      cdf_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
+      return cdf_image_summary
+    self.execute(graph_fn, [])
 
   def test_add_hist_image_summary(self):
-    values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
-    bins = [0.01 * i for i in range(101)]
-    visualization_utils.add_hist_image_summary(values, bins,
-                                               'ScoresDistribution')
-    hist_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
-    with self.test_session():
-      hist_image_summary.eval()
+    def graph_fn():
+      values = [0.1, 0.2, 0.3, 0.4, 0.42, 0.44, 0.46, 0.48, 0.50]
+      bins = [0.01 * i for i in range(101)]
+      visualization_utils.add_hist_image_summary(values, bins,
+                                                 'ScoresDistribution')
+      hist_image_summary = tf.get_collection(key=tf.GraphKeys.SUMMARIES)[0]
+      return hist_image_summary
+    self.execute(graph_fn, [])
 
   def test_eval_metric_ops(self):
+    if self.is_tf2():
+      self.skipTest('This test is only compatible with Tensorflow 1.X, '
+                    'estimator eval ops are not supported in Tensorflow 2.')
     category_index = {1: {'id': 1, 'name': 'dog'}, 2: {'id': 2, 'name': 'cat'}}
     max_examples_to_draw = 4
     metric_op_base = 'Detections_Left_Groundtruth_Right'
diff --git a/research/object_detection/utils/vrd_evaluation_test.py b/research/object_detection/utils/vrd_evaluation_test.py
index 5c90e2d5..e6ea93eb 100644
--- a/research/object_detection/utils/vrd_evaluation_test.py
+++ b/research/object_detection/utils/vrd_evaluation_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from object_detection.core import standard_fields
 from object_detection.utils import vrd_evaluation
diff --git a/research/slim/BUILD b/research/slim/BUILD
index 1ba7522a..b88d1683 100644
--- a/research/slim/BUILD
+++ b/research/slim/BUILD
@@ -37,8 +37,9 @@ sh_binary(
 py_binary(
     name = "build_imagenet_data",
     srcs = ["datasets/build_imagenet_data.py"],
-    python_version = "PY2",
+    python_version = "PY3",
     deps = [
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         "//third_party/py/six",
         # "//tensorflow",
@@ -59,8 +60,10 @@ py_library(
 py_library(
     name = "download_and_convert_flowers",
     srcs = ["datasets/download_and_convert_flowers.py"],
+    srcs_version = "PY2AND3",
     deps = [
         ":dataset_utils",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -79,10 +82,12 @@ py_library(
 py_library(
     name = "download_and_convert_visualwakewords_lib",
     srcs = ["datasets/download_and_convert_visualwakewords_lib.py"],
+    srcs_version = "PY2AND3",
     deps = [
         ":dataset_utils",
         "//third_party/py/PIL:pil",
         "//third_party/py/contextlib2",
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -90,6 +95,7 @@ py_library(
 py_library(
     name = "download_and_convert_visualwakewords",
     srcs = ["datasets/download_and_convert_visualwakewords.py"],
+    srcs_version = "PY2AND3",
     deps = [
         ":download_and_convert_visualwakewords_lib",
         # "//tensorflow",
@@ -99,12 +105,13 @@ py_library(
 py_binary(
     name = "download_and_convert_data",
     srcs = ["download_and_convert_data.py"],
-    python_version = "PY2",
+    python_version = "PY3",
     deps = [
         ":download_and_convert_cifar10",
         ":download_and_convert_flowers",
         ":download_and_convert_mnist",
         ":download_and_convert_visualwakewords",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
     ],
 )
@@ -115,7 +122,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -125,7 +132,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -136,7 +143,7 @@ py_library(
         ":dataset_utils",
         "//third_party/py/six",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -146,7 +153,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -156,7 +163,7 @@ py_library(
     deps = [
         ":dataset_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -177,22 +184,20 @@ py_library(
     srcs = ["deployment/model_deploy.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "model_deploy_test",
     srcs = ["deployment/model_deploy_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":model_deploy",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -201,7 +206,7 @@ py_library(
     srcs = ["preprocessing/cifarnet_preprocessing.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -219,7 +224,7 @@ py_library(
     srcs = ["preprocessing/lenet_preprocessing.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -228,7 +233,7 @@ py_library(
     srcs = ["preprocessing/vgg_preprocessing.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -240,7 +245,7 @@ py_library(
         ":inception_preprocessing",
         ":lenet_preprocessing",
         ":vgg_preprocessing",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -273,20 +278,20 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "alexnet_test",
     size = "medium",
     srcs = ["nets/alexnet_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":alexnet",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -295,7 +300,7 @@ py_library(
     srcs = ["nets/cifarnet.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -306,20 +311,19 @@ py_library(
         # "//numpy",
         "//third_party/py/six",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
-        # "//tensorflow/contrib/util:util_py",
+        "//third_party/py/tf_slim:slim",
+        # "//tensorflow/python:tensor_util",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "cyclegan_test",
     srcs = ["nets/cyclegan_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":cyclegan",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
     ],
 )
@@ -330,18 +334,18 @@ py_library(
     deps = [
         "//third_party/py/six",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "dcgan_test",
     srcs = ["nets/dcgan_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":dcgan",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         "//third_party/py/six",
         # "//tensorflow",
     ],
@@ -355,20 +359,22 @@ py_library(
         ":i3d_utils",
         ":s3dg",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "i3d_test",
     size = "large",
     srcs = ["nets/i3d_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":i3d",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
+        "//third_party/py/six",
         # "//tensorflow",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -379,8 +385,7 @@ py_library(
     deps = [
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -403,7 +408,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -414,7 +419,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -425,7 +430,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -436,7 +441,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -447,7 +452,7 @@ py_library(
     deps = [
         ":inception_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -457,22 +462,22 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "inception_v1_test",
     size = "large",
     srcs = ["nets/inception_v1_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -484,52 +489,53 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "inception_v3_test",
     size = "large",
     srcs = ["nets/inception_v3_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "inception_v4_test",
     size = "large",
     srcs = ["nets/inception_v4_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "inception_resnet_v2_test",
     size = "large",
     srcs = ["nets/inception_resnet_v2_test.py"],
-    python_version = "PY2",
-    shard_count = 3,
+    shard_count = 4,
     srcs_version = "PY2AND3",
     deps = [
         ":inception",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -538,7 +544,7 @@ py_library(
     srcs = ["nets/lenet.py"],
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -548,8 +554,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/layers:layers_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -562,7 +567,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -573,8 +578,7 @@ py_library(
     deps = [
         ":mobilenet_common",
         # "//tensorflow",
-        # "//tensorflow/contrib/layers:layers_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -586,7 +590,7 @@ py_library(
         ":mobilenet_common",
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -597,18 +601,22 @@ py_test(  # py2and3_test
     deps = [
         ":mobilenet",
         ":mobilenet_common",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         "//third_party/py/six",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
 py_test(  # py2and3_test
     name = "mobilenet_v3_test",
     srcs = ["nets/mobilenet/mobilenet_v3_test.py"],
+    shard_count = 2,
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
+        "//testing/pybase:parameterized",
         "//third_party/py/absl/testing:absltest",
         # "//tensorflow",
     ],
@@ -623,46 +631,49 @@ py_library(
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "mobilenet_v1_test",
     size = "large",
     srcs = ["nets/mobilenet_v1_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":mobilenet_v1",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
 py_binary(
     name = "mobilenet_v1_train",
     srcs = ["nets/mobilenet_v1_train.py"],
-    python_version = "PY2",
+    python_version = "PY3",
     deps = [
         ":dataset_factory",
         ":mobilenet_v1",
         ":preprocessing_factory",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/quantize:quantize_graph",
-        # "//tensorflow/contrib/slim",
     ],
 )
 
 py_binary(
     name = "mobilenet_v1_eval",
     srcs = ["nets/mobilenet_v1_eval.py"],
-    python_version = "PY2",
+    python_version = "PY3",
+    srcs_version = "PY3",
     deps = [
         ":dataset_factory",
         ":mobilenet_v1",
         ":preprocessing_factory",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/quantize:quantize_graph",
-        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -672,8 +683,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -684,36 +694,34 @@ py_library(
     deps = [
         ":nasnet_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/training:training_py",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "nasnet_utils_test",
     size = "medium",
     srcs = ["nets/nasnet/nasnet_utils_test.py"],
-    python_version = "PY2",
     srcs_version = "PY2AND3",
     deps = [
         ":nasnet_utils",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "nasnet_test",
     size = "large",
     srcs = ["nets/nasnet/nasnet_test.py"],
-    python_version = "PY2",
     shard_count = 10,
     srcs_version = "PY2AND3",
     deps = [
         ":nasnet",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -725,23 +733,22 @@ py_library(
         ":nasnet",
         ":nasnet_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/training:training_py",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "pnasnet_test",
     size = "large",
     srcs = ["nets/nasnet/pnasnet_test.py"],
-    python_version = "PY2",
     shard_count = 4,
     srcs_version = "PY2AND3",
     deps = [
         ":pnasnet",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -751,7 +758,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -762,8 +769,9 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":overfeat",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -773,8 +781,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -784,8 +791,9 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":pix2pix",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -795,7 +803,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -806,24 +814,24 @@ py_library(
     deps = [
         ":resnet_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "resnet_v1_test",
     size = "medium",
     timeout = "long",
     srcs = ["nets/resnet_v1_test.py"],
-    python_version = "PY2",
     shard_count = 2,
     srcs_version = "PY2AND3",
     deps = [
         ":resnet_utils",
         ":resnet_v1",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -834,23 +842,23 @@ py_library(
     deps = [
         ":resnet_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "resnet_v2_test",
     size = "medium",
     srcs = ["nets/resnet_v2_test.py"],
-    python_version = "PY2",
     shard_count = 2,
     srcs_version = "PY2AND3",
     deps = [
         ":resnet_utils",
         ":resnet_v2",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//numpy",
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -861,20 +869,20 @@ py_library(
     deps = [
         ":i3d_utils",
         # "//tensorflow",
-        # "//tensorflow/contrib/framework:framework_py",
-        # "//tensorflow/contrib/layers:layers_py",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "s3dg_test",
     size = "large",
     srcs = ["nets/s3dg_test.py"],
-    python_version = "PY2",
     shard_count = 3,
     srcs_version = "PY2AND3",
     deps = [
         ":s3dg",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
+        "//third_party/py/six",
         # "//tensorflow",
     ],
 )
@@ -885,7 +893,7 @@ py_library(
     srcs_version = "PY2AND3",
     deps = [
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -896,8 +904,9 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":vgg",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -906,7 +915,7 @@ py_library(
     srcs = ["nets/nets_factory.py"],
     deps = [
         ":nets",
-        # "//tensorflow/contrib/slim",
+        "//third_party/py/tf_slim:slim",
     ],
 )
 
@@ -918,6 +927,7 @@ py_test(  # py2and3_test
     srcs_version = "PY2AND3",
     deps = [
         ":nets_factory",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
     ],
 )
@@ -929,6 +939,7 @@ pytype_strict_binary(
     deps = [
         ":nets_factory",
         ":preprocessing_factory",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         "//third_party/py/absl:app",
         "//third_party/py/absl/flags",
         # "//tensorflow",
@@ -946,8 +957,8 @@ py_library(
         ":nets_factory",
         ":preprocessing_factory",
         # "//tensorflow",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/quantize:quantize_graph",
-        # "//tensorflow/contrib/slim",
     ],
 )
 
@@ -956,9 +967,10 @@ py_binary(
     srcs = ["train_image_classifier.py"],
     # WARNING: not supported in bazel; will be commented out by copybara.
     # paropts = ["--compress"],
-    python_version = "PY2",
+    python_version = "PY3",
     deps = [
         ":train_image_classifier_lib",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
     ],
 )
 
@@ -970,17 +982,18 @@ py_library(
         ":nets_factory",
         ":preprocessing_factory",
         # "//tensorflow",
+        "//third_party/py/tf_slim:slim",
         # "//tensorflow/contrib/quantize:quantize_graph",
-        # "//tensorflow/contrib/slim",
     ],
 )
 
 py_binary(
     name = "eval_image_classifier",
     srcs = ["eval_image_classifier.py"],
-    python_version = "PY2",
+    python_version = "PY3",
     deps = [
         ":eval_image_classifier_lib",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
     ],
 )
 
@@ -989,8 +1002,11 @@ py_binary(
     srcs = ["export_inference_graph.py"],
     # WARNING: not supported in bazel; will be commented out by copybara.
     # paropts = ["--compress"],
-    python_version = "PY2",
-    deps = [":export_inference_graph_lib"],
+    python_version = "PY3",
+    deps = [
+        ":export_inference_graph_lib",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
+    ],
 )
 
 py_library(
@@ -1001,22 +1017,22 @@ py_library(
         ":nets_factory",
         # "//tensorflow",
         # "//tensorflow/contrib/quantize:quantize_graph",
-        # "//tensorflow/contrib/slim",
         # "//tensorflow/python:platform",
     ],
 )
 
-py_test(
+py_test(  # py2and3_test
     name = "export_inference_graph_test",
     size = "medium",
     srcs = ["export_inference_graph_test.py"],
-    python_version = "PY2",
+    python_version = "PY3",
     srcs_version = "PY2AND3",
     tags = [
         "manual",
     ],
     deps = [
         ":export_inference_graph_lib",
+        "//learning/brain/public:disable_tf2",  # build_cleaner: keep; go/disable_tf2
         # "//tensorflow",
         # "//tensorflow/python:platform",
     ],
diff --git a/research/slim/README.md b/research/slim/README.md
index 60a81b93..df327bd9 100644
--- a/research/slim/README.md
+++ b/research/slim/README.md
@@ -1,15 +1,9 @@
-![TensorFlow Requirement: 1.x](https://img.shields.io/badge/TensorFlow%20Requirement-1.x-brightgreen)
-![TensorFlow 2 Not Supported](https://img.shields.io/badge/TensorFlow%202%20Not%20Supported-%E2%9C%95-red.svg)
-
 # TensorFlow-Slim image classification model library
-
-[TF-slim](https://github.com/tensorflow/models/tree/master/research/slim)
-is a new lightweight high-level API of TensorFlow (`tensorflow.contrib.slim`)
-for defining, training and evaluating complex
-models. This directory contains
-code for training and evaluating several widely used Convolutional Neural
-Network (CNN) image classification models using TF-slim.
-It contains scripts that will allow
+This directory contains code for training and evaluating several
+widely used Convolutional Neural Network (CNN) image classification
+models using
+[tf_slim](https://github.com/google-research/tf-slim/tree/master/tf_slim).
+It contains scripts that allow
 you to train models from scratch or fine-tune them from pre-trained network
 weights. It also contains code for downloading standard image datasets,
 converting them
@@ -18,15 +12,12 @@ data reading and queueing utilities. You can easily train any model on any of
 these datasets, as we demonstrate below. We've also included a
 [jupyter notebook](https://github.com/tensorflow/models/blob/master/research/slim/slim_walkthrough.ipynb),
 which provides working examples of how to use TF-Slim for image classification.
-For developing or modifying your own models, see also the [main TF-Slim page](https://github.com/tensorflow/models/tree/master/research/slim).
+For developing or modifying your own models, see also the [main TF-Slim page](https://github.com/google-research/tf-slim/tree/master/tf_slim).
 
 ## Contacts
 
 Maintainers of TF-slim:
-
-* Sergio Guadarrama, GitHub: [sguada](https://github.com/sguada)
-* Nathan Silberman,
-  GitHub: [nathansilberman](https://github.com/nathansilberman)
+* Sergio Guadarrama, github: [sguada](https://github.com/sguada)
 
 ## Citation
 "TensorFlow-Slim image classification model library"
@@ -52,12 +43,12 @@ prerequisite packages.
 
 ## Installing latest version of TF-slim
 
-TF-Slim is available as `tf.contrib.slim` via TensorFlow 1.0. To test that your
+TF-Slim is available as `tf_slim` package. To test that your
 installation is working, execute the following command; it should run without
 raising any errors.
 
 ```
-python -c "import tensorflow.contrib.slim as slim; eval = slim.evaluation.evaluate_once"
+python -c "import tf_slim as slim; eval = slim.evaluation.evaluate_once"
 ```
 
 ## Installing the TF-slim image models library
@@ -143,7 +134,7 @@ download can take several hours, and could use up to 500GB.
 ## Creating a TF-Slim Dataset Descriptor.
 
 Once the TFRecord files have been created, you can easily define a Slim
-[Dataset](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/contrib/slim/python/slim/data/dataset.py),
+[Dataset](https://github.com/google-research/tf-slim/master/tf_slim/data/dataset.py),
 which stores pointers to the data file, as well as various other pieces of
 metadata, such as the class labels, the train/test split, and how to parse the
 TFExample protos. We have included the TF-Slim Dataset descriptors
@@ -156,14 +147,15 @@ and
 [VisualWakeWords](https://github.com/tensorflow/models/blob/master/research/slim/datasets/visualwakewords.py),
 An example of how to load data using a TF-Slim dataset descriptor using a
 TF-Slim
-[DatasetDataProvider](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py)
+[DatasetDataProvider](https://github.com/google-research/tf-slim/tree/master/tf_slim/data/dataset_data_provider.py)
 is found below:
 
 ```python
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from datasets import flowers
 
-slim = tf.contrib.slim
+
 
 # Selects the 'validation' dataset.
 dataset = flowers.get_split('validation', DATA_DIR)
@@ -205,10 +197,10 @@ you will not need to interact with the script again.
 DATA_DIR=$HOME/imagenet-data
 
 # build the preprocessing script.
-bazel build slim/download_and_preprocess_imagenet
+bazel build slim/download_and_convert_imagenet
 
 # run it
-bazel-bin/slim/download_and_preprocess_imagenet "${DATA_DIR}"
+bazel-bin/slim/download_and_convert_imagenet "${DATA_DIR}"
 ```
 
 The final line of the output script should read:
@@ -414,7 +406,7 @@ $ python eval_image_classifier.py \
     --model_name=inception_v3
 ```
 
-See the [evaluation module example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim#evaluation-loop)
+See the [evaluation module example](https://github.com/google-research/tf-slim#evaluation-loop)
 for an example of how to evaluate a model at multiple checkpoints during or after the training.
 
 # Exporting the Inference Graph
diff --git a/research/slim/datasets/build_imagenet_data.py b/research/slim/datasets/build_imagenet_data.py
index 572bcd33..c35741ea 100644
--- a/research/slim/datasets/build_imagenet_data.py
+++ b/research/slim/datasets/build_imagenet_data.py
@@ -94,7 +94,7 @@ import threading
 
 import numpy as np
 from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 
 tf.app.flags.DEFINE_string('train_directory', '/tmp/',
diff --git a/research/slim/datasets/cifar10.py b/research/slim/datasets/cifar10.py
index a28e0096..0593afa1 100644
--- a/research/slim/datasets/cifar10.py
+++ b/research/slim/datasets/cifar10.py
@@ -23,13 +23,11 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from datasets import dataset_utils
 
-slim = contrib_slim
-
 _FILE_PATTERN = 'cifar10_%s.tfrecord'
 
 SPLITS_TO_SIZES = {'train': 50000, 'test': 10000}
diff --git a/research/slim/datasets/dataset_utils.py b/research/slim/datasets/dataset_utils.py
index 47e27d19..9d285ecb 100644
--- a/research/slim/datasets/dataset_utils.py
+++ b/research/slim/datasets/dataset_utils.py
@@ -23,7 +23,7 @@ import tarfile
 import zipfile
 
 from six.moves import urllib
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 LABELS_FILENAME = 'labels.txt'
 
diff --git a/research/slim/datasets/download_and_convert_cifar10.py b/research/slim/datasets/download_and_convert_cifar10.py
index f23618e7..48fb6e24 100644
--- a/research/slim/datasets/download_and_convert_cifar10.py
+++ b/research/slim/datasets/download_and_convert_cifar10.py
@@ -33,7 +33,7 @@ import tarfile
 import numpy as np
 from six.moves import cPickle
 from six.moves import urllib
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from datasets import dataset_utils
 
diff --git a/research/slim/datasets/download_and_convert_flowers.py b/research/slim/datasets/download_and_convert_flowers.py
index 7976e387..7bbaf09c 100644
--- a/research/slim/datasets/download_and_convert_flowers.py
+++ b/research/slim/datasets/download_and_convert_flowers.py
@@ -32,7 +32,9 @@ import os
 import random
 import sys
 
-import tensorflow as tf
+from six.moves import range
+from six.moves import zip
+import tensorflow.compat.v1 as tf
 
 from datasets import dataset_utils
 
@@ -189,7 +191,8 @@ def run(dataset_dir):
 
   dataset_utils.download_and_uncompress_tarball(_DATA_URL, dataset_dir)
   photo_filenames, class_names = _get_filenames_and_classes(dataset_dir)
-  class_names_to_ids = dict(zip(class_names, range(len(class_names))))
+  class_names_to_ids = dict(
+      list(zip(class_names, list(range(len(class_names))))))
 
   # Divide into train and test:
   random.seed(_RANDOM_SEED)
@@ -204,7 +207,8 @@ def run(dataset_dir):
                    dataset_dir)
 
   # Finally, write the labels file:
-  labels_to_class_names = dict(zip(range(len(class_names)), class_names))
+  labels_to_class_names = dict(
+      list(zip(list(range(len(class_names))), class_names)))
   dataset_utils.write_label_file(labels_to_class_names, dataset_dir)
 
   _clean_up_temporary_files(dataset_dir)
diff --git a/research/slim/datasets/download_and_convert_mnist.py b/research/slim/datasets/download_and_convert_mnist.py
index d6ae8743..74b86acd 100644
--- a/research/slim/datasets/download_and_convert_mnist.py
+++ b/research/slim/datasets/download_and_convert_mnist.py
@@ -32,7 +32,7 @@ import sys
 
 import numpy as np
 from six.moves import urllib
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from datasets import dataset_utils
 
diff --git a/research/slim/datasets/download_and_convert_visualwakewords.py b/research/slim/datasets/download_and_convert_visualwakewords.py
index dc6aa69f..2deab85c 100644
--- a/research/slim/datasets/download_and_convert_visualwakewords.py
+++ b/research/slim/datasets/download_and_convert_visualwakewords.py
@@ -77,17 +77,17 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from datasets import download_and_convert_visualwakewords_lib
 
-tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
+tf.logging.set_verbosity(tf.logging.INFO)
 
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'coco_dirname', 'coco_dataset',
     'A subdirectory in visualwakewords dataset directory'
     'containing the coco dataset')
 
-FLAGS = tf.compat.v1.app.flags.FLAGS
+FLAGS = tf.app.flags.FLAGS
 
 
 def run(dataset_dir, small_object_area_threshold, foreground_class_of_interest):
diff --git a/research/slim/datasets/download_and_convert_visualwakewords_lib.py b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
index 4c3d2004..996f38bd 100644
--- a/research/slim/datasets/download_and_convert_visualwakewords_lib.py
+++ b/research/slim/datasets/download_and_convert_visualwakewords_lib.py
@@ -32,26 +32,27 @@ import contextlib2
 
 import PIL.Image
 
-import tensorflow as tf
+import six
+import tensorflow.compat.v1 as tf
 
 from datasets import dataset_utils
 
-tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)
+tf.logging.set_verbosity(tf.logging.INFO)
 
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'coco_train_url',
     'http://images.cocodataset.org/zips/train2014.zip',
     'Link to zip file containing coco training data')
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'coco_validation_url',
     'http://images.cocodataset.org/zips/val2014.zip',
     'Link to zip file containing coco validation data')
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'coco_annotations_url',
     'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',
     'Link to zip file containing coco annotation data')
 
-FLAGS = tf.compat.v1.app.flags.FLAGS
+FLAGS = tf.app.flags.FLAGS
 
 
 def download_coco_dataset(dataset_dir):
@@ -201,7 +202,7 @@ def create_tf_record_for_visualwakewords_dataset(annotations_file, image_dir,
     groundtruth_data = json.load(fid)
     images = groundtruth_data['images']
     annotations_index = groundtruth_data['annotations']
-    annotations_index = {int(k): v for k, v in annotations_index.iteritems()}
+    annotations_index = {int(k): v for k, v in six.iteritems(annotations_index)}
     # convert 'unicode' key to 'int' key after we parse the json file
 
     for idx, image in enumerate(images):
diff --git a/research/slim/datasets/flowers.py b/research/slim/datasets/flowers.py
index afbfc13b..5aa1fbcf 100644
--- a/research/slim/datasets/flowers.py
+++ b/research/slim/datasets/flowers.py
@@ -23,13 +23,11 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from datasets import dataset_utils
 
-slim = contrib_slim
-
 _FILE_PATTERN = 'flowers_%s_*.tfrecord'
 
 SPLITS_TO_SIZES = {'train': 3320, 'validation': 350}
diff --git a/research/slim/datasets/imagenet.py b/research/slim/datasets/imagenet.py
index 9b648a2d..cd7a19e5 100644
--- a/research/slim/datasets/imagenet.py
+++ b/research/slim/datasets/imagenet.py
@@ -34,13 +34,11 @@ from __future__ import print_function
 
 import os
 from six.moves import urllib
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from datasets import dataset_utils
 
-slim = contrib_slim
-
 # TODO(nsilberman): Add tfrecord file type once the script is updated.
 _FILE_PATTERN = '%s-*'
 
diff --git a/research/slim/datasets/mnist.py b/research/slim/datasets/mnist.py
index faa6799f..73bfbe0a 100644
--- a/research/slim/datasets/mnist.py
+++ b/research/slim/datasets/mnist.py
@@ -23,13 +23,11 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from datasets import dataset_utils
 
-slim = contrib_slim
-
 _FILE_PATTERN = 'mnist_%s.tfrecord'
 
 _SPLITS_TO_SIZES = {'train': 60000, 'test': 10000}
diff --git a/research/slim/datasets/visualwakewords.py b/research/slim/datasets/visualwakewords.py
index 41d4e673..5db77b2f 100644
--- a/research/slim/datasets/visualwakewords.py
+++ b/research/slim/datasets/visualwakewords.py
@@ -28,14 +28,12 @@ from __future__ import division
 from __future__ import print_function
 
 import os
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from datasets import dataset_utils
 
 
-slim = contrib_slim
-
 _FILE_PATTERN = '%s.record-*'
 
 _SPLITS_TO_SIZES = {
diff --git a/research/slim/deployment/model_deploy.py b/research/slim/deployment/model_deploy.py
index ad5005a4..49128ec2 100644
--- a/research/slim/deployment/model_deploy.py
+++ b/research/slim/deployment/model_deploy.py
@@ -101,10 +101,8 @@ from __future__ import print_function
 
 import collections
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 __all__ = ['create_clones',
diff --git a/research/slim/deployment/model_deploy_test.py b/research/slim/deployment/model_deploy_test.py
index c9a1c07c..d2f77bbb 100644
--- a/research/slim/deployment/model_deploy_test.py
+++ b/research/slim/deployment/model_deploy_test.py
@@ -19,14 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
-from deployment import model_deploy
+import tf_slim as slim
 
-slim = contrib_slim
+from deployment import model_deploy
 
 
 class DeploymentConfigTest(tf.test.TestCase):
@@ -511,9 +508,8 @@ class DeployTest(tf.test.TestCase):
 
       with tf.Session() as sess:
         sess.run(tf.global_variables_initializer())
-        moving_mean = contrib_framework.get_variables_by_name('moving_mean')[0]
-        moving_variance = contrib_framework.get_variables_by_name(
-            'moving_variance')[0]
+        moving_mean = slim.get_variables_by_name('moving_mean')[0]
+        moving_variance = slim.get_variables_by_name('moving_variance')[0]
         initial_loss = sess.run(model.total_loss)
         initial_mean, initial_variance = sess.run([moving_mean,
                                                    moving_variance])
@@ -539,8 +535,8 @@ class DeployTest(tf.test.TestCase):
       # clone function creates a fully_connected layer with a regularizer loss.
       def ModelFn():
         inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)
-        reg = contrib_layers.l2_regularizer(0.001)
-        contrib_layers.fully_connected(inputs, 30, weights_regularizer=reg)
+        reg = slim.l2_regularizer(0.001)
+        slim.fully_connected(inputs, 30, weights_regularizer=reg)
 
       model = model_deploy.deploy(
           deploy_config, ModelFn,
@@ -558,8 +554,8 @@ class DeployTest(tf.test.TestCase):
       # clone function creates a fully_connected layer with a regularizer loss.
       def ModelFn():
         inputs = tf.constant(1.0, shape=(10, 20), dtype=tf.float32)
-        reg = contrib_layers.l2_regularizer(0.001)
-        contrib_layers.fully_connected(inputs, 30, weights_regularizer=reg)
+        reg = slim.l2_regularizer(0.001)
+        slim.fully_connected(inputs, 30, weights_regularizer=reg)
 
       # No optimizer here, it's an eval.
       model = model_deploy.deploy(deploy_config, ModelFn)
diff --git a/research/slim/download_and_convert_data.py b/research/slim/download_and_convert_data.py
index e935780c..f6c011bc 100644
--- a/research/slim/download_and_convert_data.py
+++ b/research/slim/download_and_convert_data.py
@@ -39,22 +39,22 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from datasets import download_and_convert_cifar10
 from datasets import download_and_convert_flowers
 from datasets import download_and_convert_mnist
 from datasets import download_and_convert_visualwakewords
 
-FLAGS = tf.compat.v1.app.flags.FLAGS
+FLAGS = tf.app.flags.FLAGS
 
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'dataset_name',
     None,
     'The name of the dataset to convert, one of "flowers", "cifar10", "mnist", "visualwakewords"'
     )
 
-tf.compat.v1.app.flags.DEFINE_string(
+tf.app.flags.DEFINE_string(
     'dataset_dir',
     None,
     'The directory where the output TFRecords and temporary files are saved.')
@@ -91,4 +91,4 @@ def main(_):
         'dataset_name [%s] was not recognized.' % FLAGS.dataset_name)
 
 if __name__ == '__main__':
-  tf.compat.v1.app.run()
+  tf.app.run()
diff --git a/research/slim/eval_image_classifier.py b/research/slim/eval_image_classifier.py
index 12ea9c0f..cf63d99e 100644
--- a/research/slim/eval_image_classifier.py
+++ b/research/slim/eval_image_classifier.py
@@ -19,16 +19,15 @@ from __future__ import division
 from __future__ import print_function
 
 import math
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from tensorflow.contrib import quantize as contrib_quantize
-from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import nets_factory
 from preprocessing import preprocessing_factory
 
-slim = contrib_slim
-
 tf.app.flags.DEFINE_integer(
     'batch_size', 100, 'The number of samples in each batch.')
 
diff --git a/research/slim/export_inference_graph.py b/research/slim/export_inference_graph.py
index ba5fb1de..f0feec5e 100644
--- a/research/slim/export_inference_graph.py
+++ b/research/slim/export_inference_graph.py
@@ -57,17 +57,15 @@ from __future__ import division
 from __future__ import print_function
 import os
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 from tensorflow.contrib import quantize as contrib_quantize
-from tensorflow.contrib import slim as contrib_slim
+
 
 from tensorflow.python.platform import gfile
 from datasets import dataset_factory
 from nets import nets_factory
 
 
-slim = contrib_slim
-
 tf.app.flags.DEFINE_string(
     'model_name', 'inception_v3', 'The name of the architecture to save.')
 
diff --git a/research/slim/export_inference_graph_test.py b/research/slim/export_inference_graph_test.py
index 42474f22..39ea22a6 100644
--- a/research/slim/export_inference_graph_test.py
+++ b/research/slim/export_inference_graph_test.py
@@ -22,7 +22,7 @@ from __future__ import print_function
 import os
 
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.python.platform import gfile
 import export_inference_graph
diff --git a/research/slim/nets/alexnet.py b/research/slim/nets/alexnet.py
index 9733d795..84977554 100644
--- a/research/slim/nets/alexnet.py
+++ b/research/slim/nets/alexnet.py
@@ -36,20 +36,18 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 
 
 def alexnet_v2_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
-                      biases_initializer=tf.compat.v1.constant_initializer(0.1),
+                      biases_initializer=tf.constant_initializer(0.1),
                       weights_regularizer=slim.l2_regularizer(weight_decay)):
     with slim.arg_scope([slim.conv2d], padding='SAME'):
       with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
@@ -97,7 +95,7 @@ def alexnet_v2(inputs,
       or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.compat.v1.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
+  with tf.variable_scope(scope, 'alexnet_v2', [inputs]) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -116,7 +114,7 @@ def alexnet_v2(inputs,
       with slim.arg_scope(
           [slim.conv2d],
           weights_initializer=trunc_normal(0.005),
-          biases_initializer=tf.compat.v1.constant_initializer(0.1)):
+          biases_initializer=tf.constant_initializer(0.1)):
         net = slim.conv2d(net, 4096, [5, 5], padding='VALID',
                           scope='fc6')
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
@@ -137,7 +135,7 @@ def alexnet_v2(inputs,
               num_classes, [1, 1],
               activation_fn=None,
               normalizer_fn=None,
-              biases_initializer=tf.compat.v1.zeros_initializer(),
+              biases_initializer=tf.zeros_initializer(),
               scope='fc8')
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
diff --git a/research/slim/nets/alexnet_test.py b/research/slim/nets/alexnet_test.py
index b6fcdf4e..af0d669e 100644
--- a/research/slim/nets/alexnet_test.py
+++ b/research/slim/nets/alexnet_test.py
@@ -17,13 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import alexnet
 
-slim = contrib_slim
-
 
 class AlexnetV2Test(tf.test.TestCase):
 
@@ -156,7 +154,7 @@ class AlexnetV2Test(tf.test.TestCase):
       logits, _ = alexnet.alexnet_v2(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.compat.v1.get_variable_scope().reuse_variables()
+      tf.get_variable_scope().reuse_variables()
       eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = alexnet.alexnet_v2(eval_inputs, is_training=False,
@@ -173,7 +171,7 @@ class AlexnetV2Test(tf.test.TestCase):
     with self.test_session() as sess:
       inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = alexnet.alexnet_v2(inputs)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/nets/cifarnet.py b/research/slim/nets/cifarnet.py
index 1dae82cd..e9a9310c 100644
--- a/research/slim/nets/cifarnet.py
+++ b/research/slim/nets/cifarnet.py
@@ -18,13 +18,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     stddev=stddev)
 
 
@@ -63,7 +61,7 @@ def cifarnet(images, num_classes=10, is_training=False,
   """
   end_points = {}
 
-  with tf.compat.v1.variable_scope(scope, 'CifarNet', [images]):
+  with tf.variable_scope(scope, 'CifarNet', [images]):
     net = slim.conv2d(images, 64, [5, 5], scope='conv1')
     end_points['conv1'] = net
     net = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
@@ -87,7 +85,7 @@ def cifarnet(images, num_classes=10, is_training=False,
     logits = slim.fully_connected(
         net,
         num_classes,
-        biases_initializer=tf.compat.v1.zeros_initializer(),
+        biases_initializer=tf.zeros_initializer(),
         weights_initializer=trunc_normal(1 / 192.0),
         weights_regularizer=None,
         activation_fn=None,
@@ -111,12 +109,12 @@ def cifarnet_arg_scope(weight_decay=0.004):
   """
   with slim.arg_scope(
       [slim.conv2d],
-      weights_initializer=tf.compat.v1.truncated_normal_initializer(
+      weights_initializer=tf.truncated_normal_initializer(
           stddev=5e-2),
       activation_fn=tf.nn.relu):
     with slim.arg_scope(
         [slim.fully_connected],
-        biases_initializer=tf.compat.v1.constant_initializer(0.1),
+        biases_initializer=tf.constant_initializer(0.1),
         weights_initializer=trunc_normal(0.04),
         weights_regularizer=slim.l2_regularizer(weight_decay),
         activation_fn=tf.nn.relu) as sc:
diff --git a/research/slim/nets/cyclegan.py b/research/slim/nets/cyclegan.py
index 7c642371..d82aeced 100644
--- a/research/slim/nets/cyclegan.py
+++ b/research/slim/nets/cyclegan.py
@@ -19,12 +19,9 @@ from __future__ import print_function
 
 import numpy as np
 from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
-from tensorflow.contrib import util as contrib_util
-
-layers = contrib_layers
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+from tensorflow.python.framework import tensor_util
 
 
 def cyclegan_arg_scope(instance_norm_center=True,
@@ -55,13 +52,13 @@ def cyclegan_arg_scope(instance_norm_center=True,
 
   weights_regularizer = None
   if weight_decay and weight_decay > 0.0:
-    weights_regularizer = layers.l2_regularizer(weight_decay)
+    weights_regularizer = slim.l2_regularizer(weight_decay)
 
-  with contrib_framework.arg_scope(
-      [layers.conv2d],
-      normalizer_fn=layers.instance_norm,
+  with slim.arg_scope(
+      [slim.conv2d],
+      normalizer_fn=slim.instance_norm,
       normalizer_params=instance_norm_params,
-      weights_initializer=tf.compat.v1.random_normal_initializer(
+      weights_initializer=tf.random_normal_initializer(
           0, weights_init_stddev),
       weights_regularizer=weights_regularizer) as sc:
     return sc
@@ -91,7 +88,7 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
   Raises:
     ValueError: if `method` is not recognized.
   """
-  with tf.compat.v1.variable_scope('upconv'):
+  with tf.variable_scope('upconv'):
     net_shape = tf.shape(input=net)
     height = net_shape[1]
     width = net_shape[2]
@@ -106,19 +103,19 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
           net, [stride[0] * height, stride[1] * width],
           method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
       net = tf.pad(tensor=net, paddings=spatial_pad_1, mode=pad_mode)
-      net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
+      net = slim.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
     elif method == 'bilinear_upsample_conv':
-      net = tf.compat.v1.image.resize_bilinear(
+      net = tf.image.resize_bilinear(
           net, [stride[0] * height, stride[1] * width],
           align_corners=align_corners)
       net = tf.pad(tensor=net, paddings=spatial_pad_1, mode=pad_mode)
-      net = layers.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
+      net = slim.conv2d(net, num_outputs, kernel_size=[3, 3], padding='valid')
     elif method == 'conv2d_transpose':
       # This corrects 1 pixel offset for images with even width and height.
       # conv2d is left aligned and conv2d_transpose is right aligned for even
       # sized images (while doing 'SAME' padding).
       # Note: This doesn't reflect actual model in paper.
-      net = layers.conv2d_transpose(
+      net = slim.conv2d_transpose(
           net, num_outputs, kernel_size=[3, 3], stride=stride, padding='valid')
       net = net[:, 1:, 1:, :]
     else:
@@ -129,7 +126,7 @@ def cyclegan_upsample(net, num_outputs, stride, method='conv2d_transpose',
 
 def _dynamic_or_static_shape(tensor):
   shape = tf.shape(input=tensor)
-  static_shape = contrib_util.constant_value(shape)
+  static_shape = tensor_util.constant_value(shape)
   return static_shape if static_shape is not None else shape
 
 
@@ -201,47 +198,46 @@ def cyclegan_generator_resnet(images,
       dtype=np.int32)
   spatial_pad_3 = np.array([[0, 0], [3, 3], [3, 3], [0, 0]])
 
-  with contrib_framework.arg_scope(arg_scope_fn()):
+  with slim.arg_scope(arg_scope_fn()):
 
     ###########
     # Encoder #
     ###########
-    with tf.compat.v1.variable_scope('input'):
+    with tf.variable_scope('input'):
       # 7x7 input stage
       net = tf.pad(tensor=images, paddings=spatial_pad_3, mode='REFLECT')
-      net = layers.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')
+      net = slim.conv2d(net, num_filters, kernel_size=[7, 7], padding='VALID')
       end_points['encoder_0'] = net
 
-    with tf.compat.v1.variable_scope('encoder'):
-      with contrib_framework.arg_scope([layers.conv2d],
-                                       kernel_size=kernel_size,
-                                       stride=2,
-                                       activation_fn=tf.nn.relu,
-                                       padding='VALID'):
+    with tf.variable_scope('encoder'):
+      with slim.arg_scope([slim.conv2d],
+                          kernel_size=kernel_size,
+                          stride=2,
+                          activation_fn=tf.nn.relu,
+                          padding='VALID'):
 
         net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
-        net = layers.conv2d(net, num_filters * 2)
+        net = slim.conv2d(net, num_filters * 2)
         end_points['encoder_1'] = net
         net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
-        net = layers.conv2d(net, num_filters * 4)
+        net = slim.conv2d(net, num_filters * 4)
         end_points['encoder_2'] = net
 
     ###################
     # Residual Blocks #
     ###################
-    with tf.compat.v1.variable_scope('residual_blocks'):
-      with contrib_framework.arg_scope([layers.conv2d],
-                                       kernel_size=kernel_size,
-                                       stride=1,
-                                       activation_fn=tf.nn.relu,
-                                       padding='VALID'):
+    with tf.variable_scope('residual_blocks'):
+      with slim.arg_scope([slim.conv2d],
+                          kernel_size=kernel_size,
+                          stride=1,
+                          activation_fn=tf.nn.relu,
+                          padding='VALID'):
         for block_id in xrange(num_resnet_blocks):
-          with tf.compat.v1.variable_scope('block_{}'.format(block_id)):
+          with tf.variable_scope('block_{}'.format(block_id)):
             res_net = tf.pad(tensor=net, paddings=paddings, mode='REFLECT')
-            res_net = layers.conv2d(res_net, num_filters * 4)
+            res_net = slim.conv2d(res_net, num_filters * 4)
             res_net = tf.pad(tensor=res_net, paddings=paddings, mode='REFLECT')
-            res_net = layers.conv2d(res_net, num_filters * 4,
-                                    activation_fn=None)
+            res_net = slim.conv2d(res_net, num_filters * 4, activation_fn=None)
             net += res_net
 
             end_points['resnet_block_%d' % block_id] = net
@@ -249,24 +245,24 @@ def cyclegan_generator_resnet(images,
     ###########
     # Decoder #
     ###########
-    with tf.compat.v1.variable_scope('decoder'):
+    with tf.variable_scope('decoder'):
 
-      with contrib_framework.arg_scope([layers.conv2d],
-                                       kernel_size=kernel_size,
-                                       stride=1,
-                                       activation_fn=tf.nn.relu):
+      with slim.arg_scope([slim.conv2d],
+                          kernel_size=kernel_size,
+                          stride=1,
+                          activation_fn=tf.nn.relu):
 
-        with tf.compat.v1.variable_scope('decoder1'):
+        with tf.variable_scope('decoder1'):
           net = upsample_fn(net, num_outputs=num_filters * 2, stride=[2, 2])
         end_points['decoder1'] = net
 
-        with tf.compat.v1.variable_scope('decoder2'):
+        with tf.variable_scope('decoder2'):
           net = upsample_fn(net, num_outputs=num_filters, stride=[2, 2])
         end_points['decoder2'] = net
 
-    with tf.compat.v1.variable_scope('output'):
+    with tf.variable_scope('output'):
       net = tf.pad(tensor=net, paddings=spatial_pad_3, mode='REFLECT')
-      logits = layers.conv2d(
+      logits = slim.conv2d(
           net,
           num_outputs, [7, 7],
           activation_fn=None,
diff --git a/research/slim/nets/cyclegan_test.py b/research/slim/nets/cyclegan_test.py
index 96f0e248..e7baf389 100644
--- a/research/slim/nets/cyclegan_test.py
+++ b/research/slim/nets/cyclegan_test.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from nets import cyclegan
 
@@ -31,7 +31,7 @@ class CycleganTest(tf.test.TestCase):
     img_batch = tf.zeros([2, 32, 32, 3])
     model_output, _ = cyclegan.cyclegan_generator_resnet(img_batch)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       sess.run(model_output)
 
   def _test_generator_graph_helper(self, shape):
@@ -50,13 +50,13 @@ class CycleganTest(tf.test.TestCase):
 
   def test_generator_unknown_batch_dim(self):
     """Check that generator can take unknown batch dimension inputs."""
-    img = tf.compat.v1.placeholder(tf.float32, shape=[None, 32, None, 3])
+    img = tf.placeholder(tf.float32, shape=[None, 32, None, 3])
     output_imgs, _ = cyclegan.cyclegan_generator_resnet(img)
 
     self.assertAllEqual([None, 32, None, 3], output_imgs.shape.as_list())
 
   def _input_and_output_same_shape_helper(self, kernel_size):
-    img_batch = tf.compat.v1.placeholder(tf.float32, shape=[None, 32, 32, 3])
+    img_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])
     output_img_batch, _ = cyclegan.cyclegan_generator_resnet(
         img_batch, kernel_size=kernel_size)
 
@@ -79,7 +79,7 @@ class CycleganTest(tf.test.TestCase):
     self.assertRaisesRegexp(
         ValueError, 'The input height must be a multiple of 4.',
         cyclegan.cyclegan_generator_resnet,
-        tf.compat.v1.placeholder(tf.float32, shape=[None, height, 32, 3]))
+        tf.placeholder(tf.float32, shape=[None, height, 32, 3]))
 
   def test_error_if_height_not_multiple_of_four_height29(self):
     self._error_if_height_not_multiple_of_four_helper(29)
@@ -94,7 +94,7 @@ class CycleganTest(tf.test.TestCase):
     self.assertRaisesRegexp(
         ValueError, 'The input width must be a multiple of 4.',
         cyclegan.cyclegan_generator_resnet,
-        tf.compat.v1.placeholder(tf.float32, shape=[None, 32, width, 3]))
+        tf.placeholder(tf.float32, shape=[None, 32, width, 3]))
 
   def test_error_if_width_not_multiple_of_four_width29(self):
     self._error_if_width_not_multiple_of_four_helper(29)
diff --git a/research/slim/nets/dcgan.py b/research/slim/nets/dcgan.py
index 598b642a..7cf47ed1 100644
--- a/research/slim/nets/dcgan.py
+++ b/research/slim/nets/dcgan.py
@@ -20,10 +20,8 @@ from __future__ import print_function
 from math import log
 
 from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def _validate_image_inputs(inputs):
@@ -82,7 +80,7 @@ def discriminator(inputs,
   inp_shape = inputs.get_shape().as_list()[1]
 
   end_points = {}
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, values=[inputs], reuse=reuse) as scope:
     with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
       with slim.arg_scope([slim.conv2d],
@@ -157,7 +155,7 @@ def generator(inputs,
 
   end_points = {}
   num_layers = int(log(final_size, 2)) - 1
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, values=[inputs], reuse=reuse) as scope:
     with slim.arg_scope([normalizer_fn], **normalizer_fn_args):
       with slim.arg_scope([slim.conv2d_transpose],
diff --git a/research/slim/nets/dcgan_test.py b/research/slim/nets/dcgan_test.py
index 53fd9fb7..f654ec1a 100644
--- a/research/slim/nets/dcgan_test.py
+++ b/research/slim/nets/dcgan_test.py
@@ -19,7 +19,7 @@ from __future__ import division
 from __future__ import print_function
 
 from six.moves import xrange  # pylint: disable=redefined-builtin
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from nets import dcgan
 
@@ -27,19 +27,19 @@ from nets import dcgan
 class DCGANTest(tf.test.TestCase):
 
   def test_generator_run(self):
-    tf.compat.v1.set_random_seed(1234)
+    tf.set_random_seed(1234)
     noise = tf.random.normal([100, 64])
     image, _ = dcgan.generator(noise)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       image.eval()
 
   def test_generator_graph(self):
-    tf.compat.v1.set_random_seed(1234)
+    tf.set_random_seed(1234)
     # Check graph construction for a number of image size/depths and batch
     # sizes.
     for i, batch_size in zip(xrange(3, 7), xrange(3, 8)):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       final_size = 2 ** i
       noise = tf.random.normal([batch_size, 64])
       image, end_points = dcgan.generator(
@@ -74,14 +74,14 @@ class DCGANTest(tf.test.TestCase):
     image = tf.random.uniform([5, 32, 32, 3], -1, 1)
     output, _ = dcgan.discriminator(image)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output.eval()
 
   def test_discriminator_graph(self):
     # Check graph construction for a number of image size/depths and batch
     # sizes.
     for i, batch_size in zip(xrange(1, 6), xrange(3, 8)):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       img_w = 2 ** i
       image = tf.random.uniform([batch_size, img_w, img_w, 3], -1, 1)
       output, end_points = dcgan.discriminator(
@@ -103,7 +103,7 @@ class DCGANTest(tf.test.TestCase):
     with self.assertRaises(ValueError):
       dcgan.discriminator(wrong_dim_img)
 
-    spatially_undefined_shape = tf.compat.v1.placeholder(
+    spatially_undefined_shape = tf.placeholder(
         tf.float32, [5, 32, None, 3])
     with self.assertRaises(ValueError):
       dcgan.discriminator(spatially_undefined_shape)
diff --git a/research/slim/nets/i3d.py b/research/slim/nets/i3d.py
index 28974ead..c4782d41 100644
--- a/research/slim/nets/i3d.py
+++ b/research/slim/nets/i3d.py
@@ -24,16 +24,14 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import i3d_utils
 from nets import s3dg
 
-slim = contrib_slim
-
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 conv3d_spatiotemporal = i3d_utils.conv3d_spatiotemporal
 
@@ -152,12 +150,12 @@ def i3d(inputs,
       activation.
   """
   # Final pooling and prediction
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV1', [inputs, num_classes], reuse=reuse) as scope:
     with slim.arg_scope(
         [slim.batch_norm, slim.dropout], is_training=is_training):
       net, end_points = i3d_base(inputs, scope=scope)
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         kernel_size = i3d_utils.reduced_kernel_size_3d(net, [2, 7, 7])
         net = slim.avg_pool3d(
             net, kernel_size, stride=1, scope='AvgPool_0a_7x7')
diff --git a/research/slim/nets/i3d_test.py b/research/slim/nets/i3d_test.py
index 307233cb..e69a3b55 100644
--- a/research/slim/nets/i3d_test.py
+++ b/research/slim/nets/i3d_test.py
@@ -18,7 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import six
+import tensorflow.compat.v1 as tf
 
 from nets import i3d
 
@@ -55,7 +56,7 @@ class I3DTest(tf.test.TestCase):
                           'Mixed_3c', 'MaxPool_4a_3x3', 'Mixed_4b', 'Mixed_4c',
                           'Mixed_4d', 'Mixed_4e', 'Mixed_4f', 'MaxPool_5a_2x2',
                           'Mixed_5b', 'Mixed_5c']
-    self.assertItemsEqual(end_points.keys(), expected_endpoints)
+    self.assertItemsEqual(list(end_points.keys()), expected_endpoints)
 
   def testBuildOnlyUptoFinalEndpoint(self):
     batch_size = 5
@@ -100,8 +101,9 @@ class I3DTest(tf.test.TestCase):
                         'Mixed_5b': [5, 8, 7, 7, 832],
                         'Mixed_5c': [5, 8, 7, 7, 1024]}
 
-    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    self.assertItemsEqual(
+        list(endpoints_shapes.keys()), list(end_points.keys()))
+    for endpoint_name, expected_shape in six.iteritems(endpoints_shapes):
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
                            expected_shape)
@@ -140,7 +142,7 @@ class I3DTest(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
diff --git a/research/slim/nets/i3d_utils.py b/research/slim/nets/i3d_utils.py
index 05df3016..515298cf 100644
--- a/research/slim/nets/i3d_utils.py
+++ b/research/slim/nets/i3d_utils.py
@@ -19,15 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
-
-# Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to
-# more update-to-date tf.contrib.* API.
-add_arg_scope = contrib_framework.add_arg_scope
-layers = contrib_layers
+add_arg_scope = slim.add_arg_scope
+layers = slim.layers
 
 
 def center_initializer():
@@ -228,13 +224,13 @@ def inception_block_v1_3d(inputs,
   """
   use_gating = self_gating_fn is not None
 
-  with tf.compat.v1.variable_scope(scope):
-    with tf.compat.v1.variable_scope('Branch_0'):
+  with tf.variable_scope(scope):
+    with tf.variable_scope('Branch_0'):
       branch_0 = layers.conv3d(
           inputs, num_outputs_0_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       if use_gating:
         branch_0 = self_gating_fn(branch_0, scope='Conv2d_0a_1x1')
-    with tf.compat.v1.variable_scope('Branch_1'):
+    with tf.variable_scope('Branch_1'):
       branch_1 = layers.conv3d(
           inputs, num_outputs_1_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       branch_1 = conv3d_spatiotemporal(
@@ -242,7 +238,7 @@ def inception_block_v1_3d(inputs,
           scope='Conv2d_0b_3x3')
       if use_gating:
         branch_1 = self_gating_fn(branch_1, scope='Conv2d_0b_3x3')
-    with tf.compat.v1.variable_scope('Branch_2'):
+    with tf.variable_scope('Branch_2'):
       branch_2 = layers.conv3d(
           inputs, num_outputs_2_0a, [1, 1, 1], scope='Conv2d_0a_1x1')
       branch_2 = conv3d_spatiotemporal(
@@ -250,7 +246,7 @@ def inception_block_v1_3d(inputs,
           scope='Conv2d_0b_3x3')
       if use_gating:
         branch_2 = self_gating_fn(branch_2, scope='Conv2d_0b_3x3')
-    with tf.compat.v1.variable_scope('Branch_3'):
+    with tf.variable_scope('Branch_3'):
       branch_3 = layers.max_pool3d(inputs, [3, 3, 3], scope='MaxPool_0a_3x3')
       branch_3 = layers.conv3d(
           branch_3, num_outputs_3_0b, [1, 1, 1], scope='Conv2d_0b_1x1')
diff --git a/research/slim/nets/inception_resnet_v2.py b/research/slim/nets/inception_resnet_v2.py
index cff58953..aa3b54ab 100644
--- a/research/slim/nets/inception_resnet_v2.py
+++ b/research/slim/nets/inception_resnet_v2.py
@@ -25,21 +25,19 @@ from __future__ import division
 from __future__ import print_function
 
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 35x35 resnet block."""
-  with tf.compat.v1.variable_scope(scope, 'Block35', [net], reuse=reuse):
-    with tf.compat.v1.variable_scope('Branch_0'):
+  with tf.variable_scope(scope, 'Block35', [net], reuse=reuse):
+    with tf.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 32, 1, scope='Conv2d_1x1')
-    with tf.compat.v1.variable_scope('Branch_1'):
+    with tf.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 32, 3, scope='Conv2d_0b_3x3')
-    with tf.compat.v1.variable_scope('Branch_2'):
+    with tf.variable_scope('Branch_2'):
       tower_conv2_0 = slim.conv2d(net, 32, 1, scope='Conv2d_0a_1x1')
       tower_conv2_1 = slim.conv2d(tower_conv2_0, 48, 3, scope='Conv2d_0b_3x3')
       tower_conv2_2 = slim.conv2d(tower_conv2_1, 64, 3, scope='Conv2d_0c_3x3')
@@ -59,10 +57,10 @@ def block35(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
 def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 17x17 resnet block."""
-  with tf.compat.v1.variable_scope(scope, 'Block17', [net], reuse=reuse):
-    with tf.compat.v1.variable_scope('Branch_0'):
+  with tf.variable_scope(scope, 'Block17', [net], reuse=reuse):
+    with tf.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')
-    with tf.compat.v1.variable_scope('Branch_1'):
+    with tf.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 128, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 160, [1, 7],
                                   scope='Conv2d_0b_1x7')
@@ -85,10 +83,10 @@ def block17(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
 
 def block8(net, scale=1.0, activation_fn=tf.nn.relu, scope=None, reuse=None):
   """Builds the 8x8 resnet block."""
-  with tf.compat.v1.variable_scope(scope, 'Block8', [net], reuse=reuse):
-    with tf.compat.v1.variable_scope('Branch_0'):
+  with tf.variable_scope(scope, 'Block8', [net], reuse=reuse):
+    with tf.variable_scope('Branch_0'):
       tower_conv = slim.conv2d(net, 192, 1, scope='Conv2d_1x1')
-    with tf.compat.v1.variable_scope('Branch_1'):
+    with tf.variable_scope('Branch_1'):
       tower_conv1_0 = slim.conv2d(net, 192, 1, scope='Conv2d_0a_1x1')
       tower_conv1_1 = slim.conv2d(tower_conv1_0, 224, [1, 3],
                                   scope='Conv2d_0b_1x3')
@@ -155,7 +153,7 @@ def inception_resnet_v2_base(inputs,
     end_points[name] = net
     return name == final_endpoint
 
-  with tf.compat.v1.variable_scope(scope, 'InceptionResnetV2', [inputs]):
+  with tf.variable_scope(scope, 'InceptionResnetV2', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='SAME'):
       # 149 x 149 x 32
@@ -188,20 +186,20 @@ def inception_resnet_v2_base(inputs,
       if add_and_check_final('MaxPool_5a_3x3', net): return net, end_points
 
       # 35 x 35 x 320
-      with tf.compat.v1.variable_scope('Mixed_5b'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_5b'):
+        with tf.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 96, 1, scope='Conv2d_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           tower_conv1_0 = slim.conv2d(net, 48, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1_0, 64, 5,
                                       scope='Conv2d_0b_5x5')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           tower_conv2_0 = slim.conv2d(net, 64, 1, scope='Conv2d_0a_1x1')
           tower_conv2_1 = slim.conv2d(tower_conv2_0, 96, 3,
                                       scope='Conv2d_0b_3x3')
           tower_conv2_2 = slim.conv2d(tower_conv2_1, 96, 3,
                                       scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           tower_pool = slim.avg_pool2d(net, 3, stride=1, padding='SAME',
                                        scope='AvgPool_0a_3x3')
           tower_pool_1 = slim.conv2d(tower_pool, 64, 1,
@@ -218,12 +216,12 @@ def inception_resnet_v2_base(inputs,
       # 33 x 33 x 1088 if output_stride == 16
       use_atrous = output_stride == 8
 
-      with tf.compat.v1.variable_scope('Mixed_6a'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_6a'):
+        with tf.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 384, 3, stride=1 if use_atrous else 2,
                                    padding=padding,
                                    scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           tower_conv1_0 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1_0, 256, 3,
                                       scope='Conv2d_0b_3x3')
@@ -231,7 +229,7 @@ def inception_resnet_v2_base(inputs,
                                       stride=1 if use_atrous else 2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           tower_pool = slim.max_pool2d(net, 3, stride=1 if use_atrous else 2,
                                        padding=padding,
                                        scope='MaxPool_1a_3x3')
@@ -251,25 +249,25 @@ def inception_resnet_v2_base(inputs,
                          'PreAuxlogits end_point for now.')
 
       # 8 x 8 x 2080
-      with tf.compat.v1.variable_scope('Mixed_7a'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_7a'):
+        with tf.variable_scope('Branch_0'):
           tower_conv = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv_1 = slim.conv2d(tower_conv, 384, 3, stride=2,
                                      padding=padding,
                                      scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           tower_conv1 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv1_1 = slim.conv2d(tower_conv1, 288, 3, stride=2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           tower_conv2 = slim.conv2d(net, 256, 1, scope='Conv2d_0a_1x1')
           tower_conv2_1 = slim.conv2d(tower_conv2, 288, 3,
                                       scope='Conv2d_0b_3x3')
           tower_conv2_2 = slim.conv2d(tower_conv2_1, 320, 3, stride=2,
                                       padding=padding,
                                       scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           tower_pool = slim.max_pool2d(net, 3, stride=2,
                                        padding=padding,
                                        scope='MaxPool_1a_3x3')
@@ -320,7 +318,7 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
   """
   end_points = {}
 
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionResnetV2', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
@@ -329,7 +327,7 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
                                                  activation_fn=activation_fn)
 
       if create_aux_logits and num_classes:
-        with tf.compat.v1.variable_scope('AuxLogits'):
+        with tf.variable_scope('AuxLogits'):
           aux = end_points['PreAuxLogits']
           aux = slim.avg_pool2d(aux, 5, stride=3, padding='VALID',
                                 scope='Conv2d_1a_3x3')
@@ -341,7 +339,7 @@ def inception_resnet_v2(inputs, num_classes=1001, is_training=True,
                                      scope='Logits')
           end_points['AuxLogits'] = aux
 
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
         # can be set to False to disable pooling here (as in resnet_*()).
         kernel_size = net.get_shape()[1:3]
@@ -372,7 +370,7 @@ def inception_resnet_v2_arg_scope(
     batch_norm_decay=0.9997,
     batch_norm_epsilon=0.001,
     activation_fn=tf.nn.relu,
-    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
+    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
     batch_norm_scale=False):
   """Returns the scope with the default parameters for inception_resnet_v2.
 
diff --git a/research/slim/nets/inception_resnet_v2_test.py b/research/slim/nets/inception_resnet_v2_test.py
index 348c44d3..d1cf3d2f 100644
--- a/research/slim/nets/inception_resnet_v2_test.py
+++ b/research/slim/nets/inception_resnet_v2_test.py
@@ -17,8 +17,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception
 
@@ -117,7 +117,7 @@ class InceptionTest(tf.test.TestCase):
         if endpoint != 'PreAuxLogits':
           self.assertTrue(out_tensor.op.name.startswith(
               'InceptionResnetV2/' + endpoint))
-        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
+        self.assertItemsEqual(endpoints[:index + 1], end_points.keys())
 
   def testBuildAndCheckAllEndPointsUptoPreAuxLogits(self):
     batch_size = 5
@@ -204,15 +204,15 @@ class InceptionTest(tf.test.TestCase):
     with self.test_session():
       inputs = tf.random.uniform((batch_size, height, width, 3))
       # Force all Variables to reside on the device.
-      with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
+      with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
         inception.inception_resnet_v2(inputs, num_classes)
-      with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
+      with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
         inception.inception_resnet_v2(inputs, num_classes)
-      for v in tf.compat.v1.get_collection(
-          tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+      for v in tf.get_collection(
+          tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
         self.assertDeviceEqual(v.device, '/cpu:0')
-      for v in tf.compat.v1.get_collection(
-          tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+      for v in tf.get_collection(
+          tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
         self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testHalfSizeImages(self):
@@ -248,7 +248,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 330, 400
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(tf.float32, (batch_size, None, None, 3))
+      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
       logits, end_points = inception.inception_resnet_v2(
           inputs, num_classes, create_aux_logits=False)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
@@ -256,7 +256,7 @@ class InceptionTest(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_7b_1x1']
       images = tf.random.uniform((batch_size, height, width, 3))
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       logits_out, pre_pool_out = sess.run([logits, pre_pool],
                                           {inputs: images.eval()})
       self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
@@ -267,13 +267,13 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
       logits, _ = inception.inception_resnet_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionResnetV2/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
       images = tf.random.uniform((batch_size, height, width, 3))
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -287,7 +287,7 @@ class InceptionTest(tf.test.TestCase):
                                                 num_classes,
                                                 is_training=False)
       predictions = tf.argmax(input=logits, axis=1)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -305,32 +305,32 @@ class InceptionTest(tf.test.TestCase):
                                                 is_training=False,
                                                 reuse=True)
       predictions = tf.argmax(input=logits, axis=1)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
-    with contrib_slim.arg_scope(inception.inception_resnet_v2_arg_scope()):
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    with slim.arg_scope(inception.inception_resnet_v2_arg_scope()):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
-    with contrib_slim.arg_scope(
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    with slim.arg_scope(
         inception.inception_resnet_v2_arg_scope(batch_norm_scale=True)):
       inception.inception_resnet_v2(inputs, num_classes, is_training=False)
 
     gamma_names = set(
         v.op.name
-        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
+        for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_utils.py b/research/slim/nets/inception_utils.py
index 493a684c..af128594 100644
--- a/research/slim/nets/inception_utils.py
+++ b/research/slim/nets/inception_utils.py
@@ -24,10 +24,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def inception_arg_scope(
@@ -36,7 +34,7 @@ def inception_arg_scope(
     batch_norm_decay=0.9997,
     batch_norm_epsilon=0.001,
     activation_fn=tf.nn.relu,
-    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
+    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
     batch_norm_scale=False):
   """Defines the default arg scope for inception models.
 
diff --git a/research/slim/nets/inception_v1.py b/research/slim/nets/inception_v1.py
index b84104af..9a439539 100644
--- a/research/slim/nets/inception_v1.py
+++ b/research/slim/nets/inception_v1.py
@@ -18,15 +18,13 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception_utils
 
-slim = contrib_slim
-
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 
 
@@ -62,7 +60,7 @@ def inception_v1_base(inputs,
     ValueError: if final_endpoint is not set to one of the predefined values.
   """
   end_points = {}
-  with tf.compat.v1.variable_scope(scope, 'InceptionV1', [inputs]):
+  with tf.variable_scope(scope, 'InceptionV1', [inputs]):
     with slim.arg_scope(
         [slim.conv2d, slim.fully_connected],
         weights_initializer=trunc_normal(0.01)):
@@ -97,16 +95,16 @@ def inception_v1_base(inputs,
             return net, end_points
 
         end_point = 'Mixed_3b'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 96, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 16, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 32, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -115,16 +113,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_3c'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 192, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -138,16 +136,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4b'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 96, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 208, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 16, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 48, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -156,16 +154,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4c'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 112, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 24, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -174,16 +172,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4d'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 256, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 24, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -192,16 +190,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4e'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 112, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 144, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 288, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 64, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -210,16 +208,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_4f'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -233,16 +231,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_5b'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 256, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 320, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 32, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0a_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -251,16 +249,16 @@ def inception_v1_base(inputs,
         if final_endpoint == end_point: return net, end_points
 
         end_point = 'Mixed_5c'
-        with tf.compat.v1.variable_scope(end_point):
-          with tf.compat.v1.variable_scope('Branch_0'):
+        with tf.variable_scope(end_point):
+          with tf.variable_scope('Branch_0'):
             branch_0 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')
-          with tf.compat.v1.variable_scope('Branch_1'):
+          with tf.variable_scope('Branch_1'):
             branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')
             branch_1 = slim.conv2d(branch_1, 384, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_2'):
+          with tf.variable_scope('Branch_2'):
             branch_2 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')
             branch_2 = slim.conv2d(branch_2, 128, [3, 3], scope='Conv2d_0b_3x3')
-          with tf.compat.v1.variable_scope('Branch_3'):
+          with tf.variable_scope('Branch_3'):
             branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
             branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
           net = tf.concat(
@@ -316,12 +314,12 @@ def inception_v1(inputs,
       activation.
   """
   # Final pooling and prediction
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v1_base(inputs, scope=scope)
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(
diff --git a/research/slim/nets/inception_v1_test.py b/research/slim/nets/inception_v1_test.py
index ce0fca42..94f0c330 100644
--- a/research/slim/nets/inception_v1_test.py
+++ b/research/slim/nets/inception_v1_test.py
@@ -19,13 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception
 
-slim = contrib_slim
-
 
 class InceptionV1Test(tf.test.TestCase):
 
@@ -172,13 +170,13 @@ class InceptionV1Test(tf.test.TestCase):
                            expected_shape)
 
   def testUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v1(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
@@ -186,18 +184,18 @@ class InceptionV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v1(inputs, num_classes,
                                                   global_pool=True)
@@ -206,7 +204,7 @@ class InceptionV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -215,7 +213,7 @@ class InceptionV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -223,7 +221,7 @@ class InceptionV1Test(tf.test.TestCase):
     images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -238,7 +236,7 @@ class InceptionV1Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -255,7 +253,7 @@ class InceptionV1Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
@@ -267,32 +265,32 @@ class InceptionV1Test(tf.test.TestCase):
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v1_arg_scope()):
       inception.inception_v1(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v1_arg_scope(batch_norm_scale=True)):
       inception.inception_v1(inputs, num_classes, is_training=False)
 
     gamma_names = set(
         v.op.name
-        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
+        for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v2.py b/research/slim/nets/inception_v2.py
index 859c901a..f9f2cd70 100644
--- a/research/slim/nets/inception_v2.py
+++ b/research/slim/nets/inception_v2.py
@@ -18,15 +18,13 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception_utils
 
-slim = contrib_slim
-
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 
 
@@ -95,7 +93,7 @@ def inception_v2_base(inputs,
     )
 
   concat_dim = 3 if data_format == 'NHWC' else 1
-  with tf.compat.v1.variable_scope(scope, 'InceptionV2', [inputs]):
+  with tf.variable_scope(scope, 'InceptionV2', [inputs]):
     with slim.arg_scope(
         [slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
         stride=1,
@@ -170,17 +168,17 @@ def inception_v2_base(inputs,
       # 28 x 28 x 192
       # Inception module.
       end_point = 'Mixed_3b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -189,7 +187,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(32), [1, 1],
@@ -201,17 +199,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 256
       end_point = 'Mixed_3c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -220,7 +218,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(64), [1, 1],
@@ -232,15 +230,15 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 28 x 28 x 320
       end_point = 'Mixed_4a'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(160), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -249,7 +247,7 @@ def inception_v2_base(inputs,
               branch_1, depth(96), [3, 3], scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(
               branch_1, depth(96), [3, 3], stride=2, scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(
               net, [3, 3], stride=2, scope='MaxPool_1a_3x3')
         net = tf.concat(axis=concat_dim, values=[branch_0, branch_1, branch_2])
@@ -257,17 +255,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(224), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(64), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(
               branch_1, depth(96), [3, 3], scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -276,7 +274,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -288,17 +286,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(128), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(96), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -307,7 +305,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(128), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -319,17 +317,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4d'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -338,7 +336,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(160), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(96), [1, 1],
@@ -350,17 +348,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_4e'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(96), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(160), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -369,7 +367,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(192), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(96), [1, 1],
@@ -381,15 +379,15 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 14 x 14 x 576
       end_point = 'Mixed_5a'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(
               net, depth(128), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(192), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -398,7 +396,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(branch_1, depth(256), [3, 3], stride=2,
                                  scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2,
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(
@@ -407,17 +405,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 7 x 7 x 1024
       end_point = 'Mixed_5b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(352), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(160), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -426,7 +424,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -438,17 +436,17 @@ def inception_v2_base(inputs,
         if end_point == final_endpoint: return net, end_points
       # 7 x 7 x 1024
       end_point = 'Mixed_5c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(352), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
               scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(320), [3, 3],
                                  scope='Conv2d_0b_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(
               net, depth(192), [1, 1],
               weights_initializer=trunc_normal(0.09),
@@ -457,7 +455,7 @@ def inception_v2_base(inputs,
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(224), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.max_pool2d(net, [3, 3], scope='MaxPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(128), [1, 1],
@@ -528,14 +526,14 @@ def inception_v2(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
 
   # Final pooling and prediction
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV2', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
       net, end_points = inception_v2_base(
           inputs, scope=scope, min_depth=min_depth,
           depth_multiplier=depth_multiplier)
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(
diff --git a/research/slim/nets/inception_v2_test.py b/research/slim/nets/inception_v2_test.py
index 089a64de..bcfac2e8 100644
--- a/research/slim/nets/inception_v2_test.py
+++ b/research/slim/nets/inception_v2_test.py
@@ -19,13 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception
 
-slim = contrib_slim
-
 
 class InceptionV2Test(tf.test.TestCase):
 
@@ -284,13 +282,13 @@ class InceptionV2Test(tf.test.TestCase):
                            expected_shape)
 
   def testUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v2(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
@@ -298,18 +296,18 @@ class InceptionV2Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v2(inputs, num_classes,
                                                   global_pool=True)
@@ -318,7 +316,7 @@ class InceptionV2Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_5c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -327,7 +325,7 @@ class InceptionV2Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v2(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV2/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -335,7 +333,7 @@ class InceptionV2Test(tf.test.TestCase):
     images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -350,7 +348,7 @@ class InceptionV2Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -367,7 +365,7 @@ class InceptionV2Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
@@ -379,32 +377,32 @@ class InceptionV2Test(tf.test.TestCase):
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v2_arg_scope()):
       inception.inception_v2(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 224, 224
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v2_arg_scope(batch_norm_scale=True)):
       inception.inception_v2(inputs, num_classes, is_training=False)
 
     gamma_names = set(
         v.op.name
-        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
+        for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v3.py b/research/slim/nets/inception_v3.py
index 7c9fd7d9..9fc20b53 100644
--- a/research/slim/nets/inception_v3.py
+++ b/research/slim/nets/inception_v3.py
@@ -18,15 +18,13 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception_utils
 
-slim = contrib_slim
-
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 
 
@@ -100,7 +98,7 @@ def inception_v3_base(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.compat.v1.variable_scope(scope, 'InceptionV3', [inputs]):
+  with tf.variable_scope(scope, 'InceptionV3', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='VALID'):
       # 299 x 299 x 3
@@ -145,20 +143,20 @@ def inception_v3_base(inputs,
                         stride=1, padding='SAME'):
       # mixed: 35 x 35 x 256.
       end_point = 'Mixed_5b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv2d_0b_5x5')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(32), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -168,21 +166,21 @@ def inception_v3_base(inputs,
 
       # mixed_1: 35 x 35 x 288.
       end_point = 'Mixed_5c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0b_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv_1_0c_5x5')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1],
                                  scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -192,20 +190,20 @@ def inception_v3_base(inputs,
 
       # mixed_2: 35 x 35 x 288.
       end_point = 'Mixed_5d'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(48), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(64), [5, 5],
                                  scope='Conv2d_0b_5x5')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_2 = slim.conv2d(branch_2, depth(96), [3, 3],
                                  scope='Conv2d_0c_3x3')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(64), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -215,17 +213,17 @@ def inception_v3_base(inputs,
 
       # mixed_3: 17 x 17 x 768.
       end_point = 'Mixed_6a'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(384), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(64), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3],
                                  scope='Conv2d_0b_3x3')
           branch_1 = slim.conv2d(branch_1, depth(96), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_1x1')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -234,16 +232,16 @@ def inception_v3_base(inputs,
 
       # mixed4: 17 x 17 x 768.
       end_point = 'Mixed_6b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(128), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(128), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(128), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(128), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -253,7 +251,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -263,16 +261,16 @@ def inception_v3_base(inputs,
 
       # mixed_5: 17 x 17 x 768.
       end_point = 'Mixed_6c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -282,7 +280,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -291,16 +289,16 @@ def inception_v3_base(inputs,
       if end_point == final_endpoint: return net, end_points
       # mixed_6: 17 x 17 x 768.
       end_point = 'Mixed_6d'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(160), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(160), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(160), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -310,7 +308,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -320,16 +318,16 @@ def inception_v3_base(inputs,
 
       # mixed_7: 17 x 17 x 768.
       end_point = 'Mixed_6e'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],
                                  scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, depth(192), [7, 1],
                                  scope='Conv2d_0c_7x1')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [7, 1],
                                  scope='Conv2d_0b_7x1')
@@ -339,7 +337,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0d_7x1')
           branch_2 = slim.conv2d(branch_2, depth(192), [1, 7],
                                  scope='Conv2d_0e_1x7')
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(branch_3, depth(192), [1, 1],
                                  scope='Conv2d_0b_1x1')
@@ -349,12 +347,12 @@ def inception_v3_base(inputs,
 
       # mixed_8: 8 x 8 x 1280.
       end_point = 'Mixed_7a'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, depth(320), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(192), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [1, 7],
                                  scope='Conv2d_0b_1x7')
@@ -362,7 +360,7 @@ def inception_v3_base(inputs,
                                  scope='Conv2d_0c_7x1')
           branch_1 = slim.conv2d(branch_1, depth(192), [3, 3], stride=2,
                                  padding='VALID', scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -370,22 +368,22 @@ def inception_v3_base(inputs,
       if end_point == final_endpoint: return net, end_points
       # mixed_9: 8 x 8 x 2048.
       end_point = 'Mixed_7b'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0b_3x1')])
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
           branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
@@ -395,22 +393,22 @@ def inception_v3_base(inputs,
 
       # mixed_10: 8 x 8 x 2048.
       end_point = 'Mixed_7c'
-      with tf.compat.v1.variable_scope(end_point):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope(end_point):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, depth(320), [1, 1], scope='Conv2d_0a_1x1')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, depth(384), [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = tf.concat(axis=3, values=[
               slim.conv2d(branch_1, depth(384), [1, 3], scope='Conv2d_0b_1x3'),
               slim.conv2d(branch_1, depth(384), [3, 1], scope='Conv2d_0c_3x1')])
-        with tf.compat.v1.variable_scope('Branch_2'):
+        with tf.variable_scope('Branch_2'):
           branch_2 = slim.conv2d(net, depth(448), [1, 1], scope='Conv2d_0a_1x1')
           branch_2 = slim.conv2d(
               branch_2, depth(384), [3, 3], scope='Conv2d_0b_3x3')
           branch_2 = tf.concat(axis=3, values=[
               slim.conv2d(branch_2, depth(384), [1, 3], scope='Conv2d_0c_1x3'),
               slim.conv2d(branch_2, depth(384), [3, 1], scope='Conv2d_0d_3x1')])
-        with tf.compat.v1.variable_scope('Branch_3'):
+        with tf.variable_scope('Branch_3'):
           branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')
           branch_3 = slim.conv2d(
               branch_3, depth(192), [1, 1], scope='Conv2d_0b_1x1')
@@ -486,7 +484,7 @@ def inception_v3(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV3', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
@@ -499,7 +497,7 @@ def inception_v3(inputs,
         with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                             stride=1, padding='SAME'):
           aux_logits = end_points['Mixed_6e']
-          with tf.compat.v1.variable_scope('AuxLogits'):
+          with tf.variable_scope('AuxLogits'):
             aux_logits = slim.avg_pool2d(
                 aux_logits, [5, 5], stride=3, padding='VALID',
                 scope='AvgPool_1a_5x5')
@@ -522,7 +520,7 @@ def inception_v3(inputs,
             end_points['AuxLogits'] = aux_logits
 
       # Final pooling and prediction
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(
diff --git a/research/slim/nets/inception_v3_test.py b/research/slim/nets/inception_v3_test.py
index 1bc2c13f..500d02c3 100644
--- a/research/slim/nets/inception_v3_test.py
+++ b/research/slim/nets/inception_v3_test.py
@@ -19,13 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception
 
-slim = contrib_slim
-
 
 class InceptionV3Test(tf.test.TestCase):
 
@@ -89,7 +87,7 @@ class InceptionV3Test(tf.test.TestCase):
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
             'InceptionV3/' + endpoint))
-        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
+        self.assertItemsEqual(endpoints[:index + 1], end_points.keys())
 
   def testBuildAndCheckAllEndPointsUptoMixed7c(self):
     batch_size = 5
@@ -223,31 +221,31 @@ class InceptionV3Test(tf.test.TestCase):
                          [batch_size, 3, 3, 2048])
 
   def testUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 2
     height, width = 299, 299
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v3(inputs, num_classes)
       self.assertListEqual(logits.get_shape().as_list(),
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 8, 2048])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 1
     height, width = 330, 400
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = inception.inception_v3(inputs, num_classes,
                                                   global_pool=True)
@@ -255,7 +253,7 @@ class InceptionV3Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7c']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 11, 2048])
 
@@ -264,7 +262,7 @@ class InceptionV3Test(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
 
-    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = inception.inception_v3(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('InceptionV3/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -272,7 +270,7 @@ class InceptionV3Test(tf.test.TestCase):
     images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -287,7 +285,7 @@ class InceptionV3Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -305,7 +303,7 @@ class InceptionV3Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
@@ -317,32 +315,32 @@ class InceptionV3Test(tf.test.TestCase):
                                        spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(inception.inception_v3_arg_scope()):
       inception.inception_v3(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
     with slim.arg_scope(
         inception.inception_v3_arg_scope(batch_norm_scale=True)):
       inception.inception_v3(inputs, num_classes, is_training=False)
 
     gamma_names = set(
         v.op.name
-        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
+        for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/inception_v4.py b/research/slim/nets/inception_v4.py
index 7c6678c5..32bd78a0 100644
--- a/research/slim/nets/inception_v4.py
+++ b/research/slim/nets/inception_v4.py
@@ -24,31 +24,29 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception_utils
 
-slim = contrib_slim
-
 
 def block_inception_a(inputs, scope=None, reuse=None):
   """Builds Inception-A block for Inception v4 network."""
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.compat.v1.variable_scope(
+    with tf.variable_scope(
         scope, 'BlockInceptionA', [inputs], reuse=reuse):
-      with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.compat.v1.variable_scope('Branch_1'):
+      with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')
-      with tf.compat.v1.variable_scope('Branch_2'):
+      with tf.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')
         branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')
-      with tf.compat.v1.variable_scope('Branch_3'):
+      with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -59,17 +57,17 @@ def block_reduction_a(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.compat.v1.variable_scope(
+    with tf.variable_scope(
         scope, 'BlockReductionA', [inputs], reuse=reuse):
-      with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID',
                                scope='Conv2d_1a_3x3')
-      with tf.compat.v1.variable_scope('Branch_1'):
+      with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')
         branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.compat.v1.variable_scope('Branch_2'):
+      with tf.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -80,21 +78,21 @@ def block_inception_b(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.compat.v1.variable_scope(
+    with tf.variable_scope(
         scope, 'BlockInceptionB', [inputs], reuse=reuse):
-      with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.compat.v1.variable_scope('Branch_1'):
+      with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')
         branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')
-      with tf.compat.v1.variable_scope('Branch_2'):
+      with tf.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')
         branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')
         branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')
         branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')
-      with tf.compat.v1.variable_scope('Branch_3'):
+      with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -105,19 +103,19 @@ def block_reduction_b(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.compat.v1.variable_scope(
+    with tf.variable_scope(
         scope, 'BlockReductionB', [inputs], reuse=reuse):
-      with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')
         branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.compat.v1.variable_scope('Branch_1'):
+      with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')
         branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')
         branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,
                                padding='VALID', scope='Conv2d_1a_3x3')
-      with tf.compat.v1.variable_scope('Branch_2'):
+      with tf.variable_scope('Branch_2'):
         branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',
                                    scope='MaxPool_1a_3x3')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])
@@ -128,23 +126,23 @@ def block_inception_c(inputs, scope=None, reuse=None):
   # By default use stride=1 and SAME padding
   with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],
                       stride=1, padding='SAME'):
-    with tf.compat.v1.variable_scope(
+    with tf.variable_scope(
         scope, 'BlockInceptionC', [inputs], reuse=reuse):
-      with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Branch_0'):
         branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')
-      with tf.compat.v1.variable_scope('Branch_1'):
+      with tf.variable_scope('Branch_1'):
         branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
         branch_1 = tf.concat(axis=3, values=[
             slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'),
             slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])
-      with tf.compat.v1.variable_scope('Branch_2'):
+      with tf.variable_scope('Branch_2'):
         branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')
         branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')
         branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')
         branch_2 = tf.concat(axis=3, values=[
             slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'),
             slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])
-      with tf.compat.v1.variable_scope('Branch_3'):
+      with tf.variable_scope('Branch_3'):
         branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')
         branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')
       return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])
@@ -176,7 +174,7 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
     end_points[name] = net
     return name == final_endpoint
 
-  with tf.compat.v1.variable_scope(scope, 'InceptionV4', [inputs]):
+  with tf.variable_scope(scope, 'InceptionV4', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],
                         stride=1, padding='SAME'):
       # 299 x 299 x 3
@@ -191,23 +189,23 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
       net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')
       if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points
       # 147 x 147 x 64
-      with tf.compat.v1.variable_scope('Mixed_3a'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_3a'):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_0a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID',
                                  scope='Conv2d_0a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1])
         if add_and_check_final('Mixed_3a', net): return net, end_points
 
       # 73 x 73 x 160
-      with tf.compat.v1.variable_scope('Mixed_4a'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_4a'):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
           branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding='VALID',
                                  scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')
           branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope='Conv2d_0b_1x7')
           branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')
@@ -217,11 +215,11 @@ def inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):
         if add_and_check_final('Mixed_4a', net): return net, end_points
 
       # 71 x 71 x 192
-      with tf.compat.v1.variable_scope('Mixed_5a'):
-        with tf.compat.v1.variable_scope('Branch_0'):
+      with tf.variable_scope('Mixed_5a'):
+        with tf.variable_scope('Branch_0'):
           branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding='VALID',
                                  scope='Conv2d_1a_3x3')
-        with tf.compat.v1.variable_scope('Branch_1'):
+        with tf.variable_scope('Branch_1'):
           branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',
                                      scope='MaxPool_1a_3x3')
         net = tf.concat(axis=3, values=[branch_0, branch_1])
@@ -286,7 +284,7 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
     end_points: the set of end_points from the inception model.
   """
   end_points = {}
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV4', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
@@ -296,7 +294,7 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
                           stride=1, padding='SAME'):
         # Auxiliary Head logits
         if create_aux_logits and num_classes:
-          with tf.compat.v1.variable_scope('AuxLogits'):
+          with tf.variable_scope('AuxLogits'):
             # 17 x 17 x 1024
             aux_logits = end_points['Mixed_6h']
             aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,
@@ -316,7 +314,7 @@ def inception_v4(inputs, num_classes=1001, is_training=True,
         # Final pooling and prediction
         # TODO(sguada,arnoegw): Consider adding a parameter global_pool which
         # can be set to False to disable pooling here (as in resnet_*()).
-        with tf.compat.v1.variable_scope('Logits'):
+        with tf.variable_scope('Logits'):
           # 8 x 8 x 1536
           kernel_size = net.get_shape()[1:3]
           if kernel_size.is_fully_defined():
diff --git a/research/slim/nets/inception_v4_test.py b/research/slim/nets/inception_v4_test.py
index bc8df478..1589299e 100644
--- a/research/slim/nets/inception_v4_test.py
+++ b/research/slim/nets/inception_v4_test.py
@@ -17,8 +17,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import inception
 
@@ -147,7 +147,7 @@ class InceptionTest(tf.test.TestCase):
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
             'InceptionV4/' + endpoint))
-        self.assertItemsEqual(all_endpoints[:index+1], end_points.keys())
+        self.assertItemsEqual(all_endpoints[:index + 1], end_points.keys())
 
   def testVariablesSetDevice(self):
     batch_size = 5
@@ -155,15 +155,15 @@ class InceptionTest(tf.test.TestCase):
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
     # Force all Variables to reside on the device.
-    with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
+    with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
       inception.inception_v4(inputs, num_classes)
-    with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
+    with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
       inception.inception_v4(inputs, num_classes)
-    for v in tf.compat.v1.get_collection(
-        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+    for v in tf.get_collection(
+        tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
       self.assertDeviceEqual(v.device, '/cpu:0')
-    for v in tf.compat.v1.get_collection(
-        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+    for v in tf.get_collection(
+        tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
       self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testHalfSizeImages(self):
@@ -197,7 +197,7 @@ class InceptionTest(tf.test.TestCase):
     height, width = 350, 400
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(tf.float32, (batch_size, None, None, 3))
+      inputs = tf.placeholder(tf.float32, (batch_size, None, None, 3))
       logits, end_points = inception.inception_v4(
           inputs, num_classes, create_aux_logits=False)
       self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
@@ -205,7 +205,7 @@ class InceptionTest(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Mixed_7d']
       images = tf.random.uniform((batch_size, height, width, 3))
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       logits_out, pre_pool_out = sess.run([logits, pre_pool],
                                           {inputs: images.eval()})
       self.assertTupleEqual(logits_out.shape, (batch_size, num_classes))
@@ -216,13 +216,13 @@ class InceptionTest(tf.test.TestCase):
     height, width = 299, 299
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
       logits, _ = inception.inception_v4(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('InceptionV4/Logits'))
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
       images = tf.random.uniform((batch_size, height, width, 3))
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -236,7 +236,7 @@ class InceptionTest(tf.test.TestCase):
                                          num_classes,
                                          is_training=False)
       predictions = tf.argmax(input=logits, axis=1)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -254,32 +254,32 @@ class InceptionTest(tf.test.TestCase):
                                          is_training=False,
                                          reuse=True)
       predictions = tf.argmax(input=logits, axis=1)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
   def testNoBatchNormScaleByDefault(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
-    with contrib_slim.arg_scope(inception.inception_v4_arg_scope()):
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    with slim.arg_scope(inception.inception_v4_arg_scope()):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
-    self.assertEqual(tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'), [])
+    self.assertEqual(tf.global_variables('.*/BatchNorm/gamma:0$'), [])
 
   def testBatchNormScale(self):
     height, width = 299, 299
     num_classes = 1000
-    inputs = tf.compat.v1.placeholder(tf.float32, (1, height, width, 3))
-    with contrib_slim.arg_scope(
+    inputs = tf.placeholder(tf.float32, (1, height, width, 3))
+    with slim.arg_scope(
         inception.inception_v4_arg_scope(batch_norm_scale=True)):
       inception.inception_v4(inputs, num_classes, is_training=False)
 
     gamma_names = set(
         v.op.name
-        for v in tf.compat.v1.global_variables('.*/BatchNorm/gamma:0$'))
+        for v in tf.global_variables('.*/BatchNorm/gamma:0$'))
     self.assertGreater(len(gamma_names), 0)
-    for v in tf.compat.v1.global_variables('.*/BatchNorm/moving_mean:0$'):
+    for v in tf.global_variables('.*/BatchNorm/moving_mean:0$'):
       self.assertIn(v.op.name[:-len('moving_mean')] + 'gamma', gamma_names)
 
 
diff --git a/research/slim/nets/lenet.py b/research/slim/nets/lenet.py
index 9f269d27..350c401d 100644
--- a/research/slim/nets/lenet.py
+++ b/research/slim/nets/lenet.py
@@ -18,10 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def lenet(images, num_classes=10, is_training=False,
@@ -59,7 +57,7 @@ def lenet(images, num_classes=10, is_training=False,
   """
   end_points = {}
 
-  with tf.compat.v1.variable_scope(scope, 'LeNet', [images]):
+  with tf.variable_scope(scope, 'LeNet', [images]):
     net = end_points['conv1'] = slim.conv2d(images, 32, [5, 5], scope='conv1')
     net = end_points['pool1'] = slim.max_pool2d(net, [2, 2], 2, scope='pool1')
     net = end_points['conv2'] = slim.conv2d(net, 64, [5, 5], scope='conv2')
@@ -93,6 +91,6 @@ def lenet_arg_scope(weight_decay=0.0):
   with slim.arg_scope(
       [slim.conv2d, slim.fully_connected],
       weights_regularizer=slim.l2_regularizer(weight_decay),
-      weights_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.1),
+      weights_initializer=tf.truncated_normal_initializer(stddev=0.1),
       activation_fn=tf.nn.relu) as sc:
     return sc
diff --git a/research/slim/nets/mobilenet/README.md b/research/slim/nets/mobilenet/README.md
index 677ece77..6ec7fad8 100644
--- a/research/slim/nets/mobilenet/README.md
+++ b/research/slim/nets/mobilenet/README.md
@@ -6,7 +6,7 @@ This folder contains building code for
 definition for each model is located in [mobilenet_v2.py](mobilenet_v2.py) and
 [mobilenet_v3.py](mobilenet_v3.py) respectively.
 
-For MobilenetV1 please refer to this [page](../mobilenet_v1.md)
+For MobilenetV1 please refer to [this page](../mobilenet_v1.md)
 
 We have also introduced a family of MobileNets customized for the Edge TPU
 accelerator found in
@@ -61,20 +61,20 @@ on CPU, we find that they are much more performant on GPU/DSP.
 
 | Imagenet Checkpoint | MACs (M) | Params (M) | Top1 | Pixel 1 | Pixel 2 | Pixel 3 |
 | ------------------ | -------- | ---------- | ---- | ------- | ------- | ------- |
-| [Large dm=1 (float)]   | 217      | 5.4        | 75.2 | 51.2    | 61      | 44      |
-| [Large dm=1 (8-bit)] | 217      | 5.4        | 73.9 | 44      | 42.5    | 32      |
+| [Large dm=1 (float)]    | 217      | 5.4        | 75.2 | 51.2    | 61      | 44      |
+| [Large dm=1 (8-bit)]    | 217      | 5.4        | 73.9 | 44      | 42.5    | 32      |
 | [Large dm=0.75 (float)] | 155      | 4.0        | 73.3 | 39.8    | 48      | 34      |
-| [Small dm=1 (float)]   | 66       | 2.9        | 67.5 | 15.8    | 19.4    | 14.4    |
-| [Small dm=1 (8-bit)]   | 66       | 2.9        | 64.9 | 15.5    | 15      | 10.7    |
+| [Small dm=1 (float)]    | 66       | 2.9        | 67.5 | 15.8    | 19.4    | 14.4    |
+| [Small dm=1 (8-bit)]    | 66       | 2.9        | 64.9 | 15.5    | 15      | 10.7    |
 | [Small dm=0.75 (float)] | 44       | 2.4        | 65.4 | 12.8    | 15.9    | 11.6    |
 
 #### Minimalistic checkpoints:
 
 | Imagenet Checkpoint | MACs (M) | Params (M) | Top1 | Pixel 1 | Pixel 2 | Pixel 3 |
 | -------------- | -------- | ---------- | ---- | ------- | ------- | ------- |
-| [Large minimalistic (float)]   | 209      | 3.9        | 72.3 | 44.1    | 51      | 35      |
+| [Large minimalistic (float)]        | 209      | 3.9        | 72.3 | 44.1    | 51      | 35      |
 | [Large minimalistic (8-bit)][lm8]   | 209      | 3.9        | 71.3 | 37      | 35      | 27      |
-| [Small minimalistic (float)]   | 65       | 2.0        | 61.9 | 12.2    | 15.1    | 11      |
+| [Small minimalistic (float)]        | 65       | 2.0        | 61.9 | 12.2    | 15.1    | 11      |
 
 #### Edge TPU checkpoints:
 
@@ -103,36 +103,87 @@ tool.
 
 ### Mobilenet V2 Imagenet Checkpoints
 
-Classification Checkpoint                                                                                  | MACs (M) | Parameters (M) | Top 1 Accuracy | Top 5 Accuracy | Mobile CPU (ms) Pixel 1
----------------------------------------------------------------------------------------------------------- | -------- | -------------- | -------------- | -------------- | -----------------------
-[mobilenet_v2_1.4_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz)   | 582      | 6.06           | 75.0           | 92.5           | 138.0
-[mobilenet_v2_1.3_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.3_224.tgz)   | 509      | 5.34           | 74.4           | 92.1           | 123.0
-[mobilenet_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz)   | 300      | 3.47           | 71.8           | 91.0           | 73.8
-[mobilenet_v2_1.0_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_192.tgz)   | 221      | 3.47           | 70.7           | 90.1           | 55.1
-[mobilenet_v2_1.0_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_160.tgz)   | 154      | 3.47           | 68.8           | 89.0           | 40.2
-[mobilenet_v2_1.0_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_128.tgz)   | 99       | 3.47           | 65.3           | 86.9           | 27.6
-[mobilenet_v2_1.0_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_96.tgz)     | 56       | 3.47           | 60.3           | 83.2           | 17.6
-[mobilenet_v2_0.75_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz) | 209      | 2.61           | 69.8           | 89.6           | 55.8
-[mobilenet_v2_0.75_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_192.tgz) | 153      | 2.61           | 68.7           | 88.9           | 41.6
-[mobilenet_v2_0.75_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_160.tgz) | 107      | 2.61           | 66.4           | 87.3           | 30.4
-[mobilenet_v2_0.75_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_128.tgz) | 69       | 2.61           | 63.2           | 85.3           | 21.9
-[mobilenet_v2_0.75_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_96.tgz)   | 39       | 2.61           | 58.8           | 81.6           | 14.2
-[mobilenet_v2_0.5_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz)   | 97       | 1.95           | 65.4           | 86.4           | 28.7
-[mobilenet_v2_0.5_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_192.tgz)   | 71       | 1.95           | 63.9           | 85.4           | 21.1
-[mobilenet_v2_0.5_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_160.tgz)   | 50       | 1.95           | 61.0           | 83.2           | 14.9
-[mobilenet_v2_0.5_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_128.tgz)   | 32       | 1.95           | 57.7           | 80.8           | 9.9
-[mobilenet_v2_0.5_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_96.tgz)     | 18       | 1.95           | 51.2           | 75.8           | 6.4
-[mobilenet_v2_0.35_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz) | 59       | 1.66           | 60.3           | 82.9           | 19.7
-[mobilenet_v2_0.35_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_192.tgz) | 43       | 1.66           | 58.2           | 81.2           | 14.6
-[mobilenet_v2_0.35_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_160.tgz) | 30       | 1.66           | 55.7           | 79.1           | 10.5
-[mobilenet_v2_0.35_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_128.tgz) | 20       | 1.66           | 50.8           | 75.0           | 6.9
-[mobilenet_v2_0.35_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_96.tgz)   | 11       | 1.66           | 45.5           | 70.4           | 4.5
+Classification Checkpoint                   | Quantized                                                               | MACs (M) | Parameters (M) | Top 1 Accuracy | Top 5 Accuracy | Mobile CPU (ms) Pixel 1
+------------------------------------------------------------------------------------------------------|-------------- | -------- | -------------- | -------------- | -------------- | -----------------------
+[float_v2_1.4_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.4_224.tgz)  | [uint8][quantized_v2_1.4_224]  | 582      | 6.06           | 75.0           | 92.5           | 138.0
+[float_v2_1.3_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.3_224.tgz) |[uint8][quantized_v2_1.3_224]   | 509      | 5.34           | 74.4           | 92.1           | 123.0
+[float_v2_1.0_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_224.tgz)  |[uint8][quantized_v2_1.0_224]  | 300      | 3.47           | 71.8           | 91.0           | 73.8
+[float_v2_1.0_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_192.tgz)  |[uint8][quantized_v2_1.0_192]  | 221      | 3.47           | 70.7           | 90.1           | 55.1
+[float_v2_1.0_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_160.tgz) | [uint8][quantized_v2_1.0_160]  | 154      | 3.47           | 68.8           | 89.0           | 40.2
+[float_v2_1.0_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_128.tgz) |  [uint8][quantized_v2_1.0_128]  | 99       | 3.47           | 65.3           | 86.9           | 27.6
+[float_v2_1.0_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_1.0_96.tgz)    | [uint8][quantized_v2_1.0_96]   | 56       | 3.47           | 60.3           | 83.2           | 17.6
+[float_v2_0.75_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_224.tgz)|[uint8][quantized_v2_0.75_224] | 209      | 2.61           | 69.8           | 89.6           | 55.8
+[float_v2_0.75_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_192.tgz)|[uint8][quantized_v2_0.75_192] | 153      | 2.61           | 68.7           | 88.9           | 41.6
+[float_v2_0.75_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_160.tgz)| [uint8][quantized_v2_0.75_160] | 107      | 2.61           | 66.4           | 87.3           | 30.4
+[float_v2_0.75_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_128.tgz)| [uint8][quantized_v2_0.75_128] | 69       | 2.61           | 63.2           | 85.3           | 21.9
+[float_v2_0.75_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.75_96.tgz)  |[uint8][quantized_v2_0.75_96]  | 39       | 2.61           | 58.8           | 81.6           | 14.2
+[float_v2_0.5_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_224.tgz)  |[uint8][quantized_v2_0.5_224] | 97       | 1.95           | 65.4           | 86.4           | 28.7
+[float_v2_0.5_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_192.tgz)  |[uint8][quantized_v2_0.5_192]  | 71       | 1.95           | 63.9           | 85.4           | 21.1
+[float_v2_0.5_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_160.tgz)  |[uint8][quantized_v2_0.5_160]  | 50       | 1.95           | 61.0           | 83.2           | 14.9
+[float_v2_0.5_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_128.tgz)  |[uint8][quantized_v2_0.5_128]  | 32       | 1.95           | 57.7           | 80.8           | 9.9
+[float_v2_0.5_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.5_96.tgz)    |[uint8][quantized_v2_0.5_96]   | 18       | 1.95           | 51.2           | 75.8           | 6.4
+[float_v2_0.35_224](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_224.tgz)|[uint8][quantized_v2_0.35_224] | 59       | 1.66           | 60.3           | 82.9           | 19.7
+[float_v2_0.35_192](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_192.tgz)| [uint8][quantized_v2_0.35_192] | 43       | 1.66           | 58.2           | 81.2           | 14.6
+[float_v2_0.35_160](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_160.tgz)|[uint8][quantized_v2_0.35_160] | 30       | 1.66           | 55.7           | 79.1           | 10.5
+[float_v2_0.35_128](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_128.tgz)|[uint8][quantized_v2_0.35_128] | 20       | 1.66           | 50.8           | 75.0           | 6.9
+[float_v2_0.35_96](https://storage.googleapis.com/mobilenet_v2/checkpoints/mobilenet_v2_0.35_96.tgz)  |[uint8][quantized_v2_0.35_96]  | 11       | 1.66           | 45.5           | 70.4           | 4.5
+
+[quantized_v2_1.4_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_140.tgz
+[quantized_v2_1.3_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_130.tgz
+[quantized_v2_1.0_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_100.tgz
+[quantized_v2_1.0_192]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_192_100.tgz
+[quantized_v2_1.0_160]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_160_100.tgz
+[quantized_v2_1.0_128]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_128_100.tgz
+[quantized_v2_1.0_96]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_96_100.tgz
+[quantized_v2_0.75_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_75.tgz
+[quantized_v2_0.75_192]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_192_75.tgz
+[quantized_v2_0.75_160]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_160_75.tgz
+[quantized_v2_0.75_128]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_128_75.tgz
+[quantized_v2_0.75_96]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_96_75.tgz
+[quantized_v2_0.5_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_50.tgz
+[quantized_v2_0.5_192]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_192_50.tgz
+[quantized_v2_0.5_160]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_160_50.tgz
+[quantized_v2_0.5_128]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_128_50.tgz
+[quantized_v2_0.5_96]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_96_50.tgz
+[quantized_v2_0.35_224]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_224_35.tgz
+[quantized_v2_0.35_192]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_192_35.tgz
+[quantized_v2_0.35_160]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_160_35.tgz
+[quantized_v2_0.35_128]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_128_35.tgz
+[quantized_v2_0.35_96]: https://storage.googleapis.com/mobilenet_v2/checkpoints/quantized_v2_96_35.tgz
+
+
+
+
+
 
 ## Training
 
 ### V3
+The following configuration, achieves 74.6% using 8 GPU setup and 75.2% using
+2x2 TPU setup.
+
+
+Final Top 1 Accuracy        |      74.6         |           |
+----------------------------|------------------|-----------|
+learning_rate               |   0.16     |Total learning rate. (Per clone learning rate is 0.02) |
+rmsprop_momentum            |   0.9      |    |
+rmsprop_decay               |   0.9      |    |
+rmsprop_epsilon             |   0.002    |    |
+learning_rate_decay_factor  |   0.99     |   |
+optimizer                   |   RMSProp  | |
+warmup_epochs               |   5        | Slim uses per clone epoch, so the the flag value is 0.6  |
+num_epochs_per_decay        |   3        | Slim uses per clone epoch, so the  flag value is 0.375 |
+batch_size (per chip)       |   192      |                   |
+moving_average_decay        |   0.9999   |                   |
+weight_decay                |   1e-5     |                   |
+init_stddev                 |    0.008   |                   |
+dropout_keep_prob           |   0.8      |                   |
+bn_moving_average_decay     |   0.997    |                   |
+bn_epsilon                  |      0.001 |                   |
+label_smoothing             |   0.1      |                   |
+
+
 
-TODO: Add V3 hyperparameters
 
 ### V2
 
diff --git a/research/slim/nets/mobilenet/conv_blocks.py b/research/slim/nets/mobilenet/conv_blocks.py
index 85b07916..ce39da5c 100644
--- a/research/slim/nets/mobilenet/conv_blocks.py
+++ b/research/slim/nets/mobilenet/conv_blocks.py
@@ -16,10 +16,8 @@
 import contextlib
 import functools
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def _fixed_padding(inputs, kernel_size, rate=1):
@@ -80,8 +78,8 @@ def _split_divisible(num, num_ways, divisible_by=8):
 def _v1_compatible_scope_naming(scope):
   """v1 compatible scope naming."""
   if scope is None:  # Create uniqified separable blocks.
-    with tf.compat.v1.variable_scope(None, default_name='separable') as s, \
-         tf.compat.v1.name_scope(s.original_name_scope):
+    with tf.variable_scope(None, default_name='separable') as s, \
+         tf.name_scope(s.original_name_scope):
       yield ''
   else:
     # We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.
@@ -300,8 +298,8 @@ def expanded_conv(input_tensor,
   if depthwise_activation_fn is not None:
     dw_defaults['activation_fn'] = depthwise_activation_fn
   # pylint: disable=g-backslash-continuation
-  with tf.compat.v1.variable_scope(scope, default_name='expanded_conv') as s, \
-       tf.compat.v1.name_scope(s.original_name_scope), \
+  with tf.variable_scope(scope, default_name='expanded_conv') as s, \
+       tf.name_scope(s.original_name_scope), \
       slim.arg_scope((slim.conv2d,), **conv_defaults), \
        slim.arg_scope((slim.separable_conv2d,), **dw_defaults):
     prev_depth = input_tensor.get_shape().as_list()[3]
@@ -434,7 +432,7 @@ def squeeze_excite(input_tensor,
   Returns:
     Gated input_tensor. (e.g. X * SE(X))
   """
-  with tf.compat.v1.variable_scope('squeeze_excite'):
+  with tf.variable_scope('squeeze_excite'):
     if squeeze_input_tensor is None:
       squeeze_input_tensor = input_tensor
     input_size = input_tensor.shape.as_list()[1:3]
diff --git a/research/slim/nets/mobilenet/mobilenet.py b/research/slim/nets/mobilenet/mobilenet.py
index 908dad43..55912d31 100644
--- a/research/slim/nets/mobilenet/mobilenet.py
+++ b/research/slim/nets/mobilenet/mobilenet.py
@@ -22,10 +22,8 @@ import contextlib
 import copy
 import os
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 @slim.add_arg_scope
@@ -306,8 +304,8 @@ def mobilenet_base(  # pylint: disable=invalid-name
 
 @contextlib.contextmanager
 def _scope_all(scope, default_scope=None):
-  with tf.compat.v1.variable_scope(scope, default_name=default_scope) as s,\
-       tf.compat.v1.name_scope(s.original_name_scope):
+  with tf.variable_scope(scope, default_name=default_scope) as s,\
+       tf.name_scope(s.original_name_scope):
     yield s
 
 
@@ -318,6 +316,7 @@ def mobilenet(inputs,
               reuse=None,
               scope='Mobilenet',
               base_only=False,
+              use_reduce_mean_for_pooling=False,
               **mobilenet_args):
   """Mobilenet model for classification, supports both V1 and V2.
 
@@ -337,6 +336,8 @@ def mobilenet(inputs,
     scope: Optional variable_scope.
     base_only: if True will only create the base of the network (no pooling
     and no logits).
+    use_reduce_mean_for_pooling: if True use the reduce_mean for pooling. If
+    True use the global_pool function that provides some optimization.
     **mobilenet_args: passed to mobilenet_base verbatim.
       - conv_defs: list of conv defs
       - multiplier: Float multiplier for the depth (number of channels)
@@ -363,7 +364,7 @@ def mobilenet(inputs,
   if len(input_shape) != 4:
     raise ValueError('Expected rank 4 input, was: %d' % len(input_shape))
 
-  with tf.compat.v1.variable_scope(scope, 'Mobilenet', reuse=reuse) as scope:
+  with tf.variable_scope(scope, 'Mobilenet', reuse=reuse) as scope:
     inputs = tf.identity(inputs, 'input')
     net, end_points = mobilenet_base(inputs, scope=scope, **mobilenet_args)
     if base_only:
@@ -371,8 +372,8 @@ def mobilenet(inputs,
 
     net = tf.identity(net, name='embedding')
 
-    with tf.compat.v1.variable_scope('Logits'):
-      net = global_pool(net)
+    with tf.variable_scope('Logits'):
+      net = global_pool(net, use_reduce_mean_for_pooling)
       end_points['global_pool'] = net
       if not num_classes:
         return net, end_points
@@ -384,7 +385,7 @@ def mobilenet(inputs,
           num_classes, [1, 1],
           activation_fn=None,
           normalizer_fn=None,
-          biases_initializer=tf.compat.v1.zeros_initializer(),
+          biases_initializer=tf.zeros_initializer(),
           scope='Conv2d_1c_1x1')
 
       logits = tf.squeeze(logits, [1, 2])
@@ -396,7 +397,9 @@ def mobilenet(inputs,
   return logits, end_points
 
 
-def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d):
+def global_pool(input_tensor,
+                use_reduce_mean_for_pooling=False,
+                pool_op=tf.nn.avg_pool2d):
   """Applies avg pool to produce 1x1 output.
 
   NOTE: This function is funcitonally equivalenet to reduce_mean, but it has
@@ -404,24 +407,29 @@ def global_pool(input_tensor, pool_op=tf.compat.v2.nn.avg_pool2d):
 
   Args:
     input_tensor: input tensor
+    use_reduce_mean_for_pooling: if True use reduce_mean for pooling
     pool_op: pooling op (avg pool is default)
   Returns:
     a tensor batch_size x 1 x 1 x depth.
   """
-  shape = input_tensor.get_shape().as_list()
-  if shape[1] is None or shape[2] is None:
-    kernel_size = tf.convert_to_tensor(value=[
-        1,
-        tf.shape(input=input_tensor)[1],
-        tf.shape(input=input_tensor)[2], 1
-    ])
+  if use_reduce_mean_for_pooling:
+    return tf.reduce_mean(
+        input_tensor, [1, 2], keepdims=True, name='ReduceMean')
   else:
-    kernel_size = [1, shape[1], shape[2], 1]
-  output = pool_op(
-      input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')
-  # Recover output shape, for unknown shape.
-  output.set_shape([None, 1, 1, None])
-  return output
+    shape = input_tensor.get_shape().as_list()
+    if shape[1] is None or shape[2] is None:
+      kernel_size = tf.convert_to_tensor(value=[
+          1,
+          tf.shape(input=input_tensor)[1],
+          tf.shape(input=input_tensor)[2], 1
+      ])
+    else:
+      kernel_size = [1, shape[1], shape[2], 1]
+    output = pool_op(
+        input_tensor, ksize=kernel_size, strides=[1, 1, 1, 1], padding='VALID')
+    # Recover output shape, for unknown shape.
+    output.set_shape([None, 1, 1, None])
+    return output
 
 
 def training_scope(is_training=True,
@@ -432,7 +440,7 @@ def training_scope(is_training=True,
   """Defines Mobilenet training scope.
 
   Usage:
-     with tf.contrib.slim.arg_scope(mobilenet.training_scope()):
+     with slim.arg_scope(mobilenet.training_scope()):
        logits, endpoints = mobilenet_v2.mobilenet(input_tensor)
 
      # the network created will be trainble with dropout/batch norm
@@ -462,7 +470,7 @@ def training_scope(is_training=True,
   if stddev < 0:
     weight_intitializer = slim.initializers.xavier_initializer()
   else:
-    weight_intitializer = tf.compat.v1.truncated_normal_initializer(
+    weight_intitializer = tf.truncated_normal_initializer(
         stddev=stddev)
 
   # Set weight_decay for weights in Conv and FC layers.
diff --git a/research/slim/nets/mobilenet/mobilenet_example.ipynb b/research/slim/nets/mobilenet/mobilenet_example.ipynb
index b0b36891..6bbfec6f 100644
--- a/research/slim/nets/mobilenet/mobilenet_example.ipynb
+++ b/research/slim/nets/mobilenet/mobilenet_example.ipynb
@@ -227,7 +227,7 @@
       },
       "outputs": [],
       "source": [
-        "import tensorflow as tf\n",
+        "import tensorflow.compat.v1 as tf\n",
         "from nets.mobilenet import mobilenet_v2\n",
         "\n",
         "tf.reset_default_graph()\n",
diff --git a/research/slim/nets/mobilenet/mobilenet_v2.py b/research/slim/nets/mobilenet/mobilenet_v2.py
index 7a06fc24..aba27f31 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2.py
@@ -27,14 +27,12 @@ from __future__ import print_function
 import copy
 import functools
 
-import tensorflow as tf
-from tensorflow.contrib import layers as contrib_layers
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
-slim = contrib_slim
 op = lib.op
 
 expand_input = ops.expand_input_by_factor
@@ -86,18 +84,17 @@ V2_DEF = dict(
 # Mobilenet v2 Definition with group normalization.
 V2_DEF_GROUP_NORM = copy.deepcopy(V2_DEF)
 V2_DEF_GROUP_NORM['defaults'] = {
-    (contrib_slim.conv2d, contrib_slim.fully_connected,
-     contrib_slim.separable_conv2d): {
-        'normalizer_fn': contrib_layers.group_norm,  # pylint: disable=C0330
+    (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
+        'normalizer_fn': slim.group_norm,  # pylint: disable=C0330
         'activation_fn': tf.nn.relu6,  # pylint: disable=C0330
     },  # pylint: disable=C0330
     (ops.expanded_conv,): {
         'expansion_size': ops.expand_input_by_factor(6),
         'split_expansion': 1,
-        'normalizer_fn': contrib_layers.group_norm,
+        'normalizer_fn': slim.group_norm,
         'residual': True
     },
-    (contrib_slim.conv2d, contrib_slim.separable_conv2d): {
+    (slim.conv2d, slim.separable_conv2d): {
         'padding': 'SAME'
     }
 }
@@ -119,7 +116,7 @@ def mobilenet(input_tensor,
   Inference mode is created by default. To create training use training_scope
   below.
 
-  with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
+  with slim.arg_scope(mobilenet_v2.training_scope()):
      logits, endpoints = mobilenet_v2.mobilenet(input_tensor)
 
   Args:
@@ -215,7 +212,7 @@ def mobilenet_base_group_norm(input_tensor, depth_multiplier=1.0, **kwargs):
   """Creates base of the mobilenet (no pooling and no logits) ."""
   kwargs['conv_defs'] = V2_DEF_GROUP_NORM
   kwargs['conv_defs']['defaults'].update({
-      (contrib_layers.group_norm,): {
+      (slim.group_norm,): {
           'groups': kwargs.pop('groups', 8)
       }
   })
@@ -227,11 +224,9 @@ def training_scope(**kwargs):
   """Defines MobilenetV2 training scope.
 
   Usage:
-     with tf.contrib.slim.arg_scope(mobilenet_v2.training_scope()):
+     with slim.arg_scope(mobilenet_v2.training_scope()):
        logits, endpoints = mobilenet_v2.mobilenet(input_tensor)
 
-  with slim.
-
   Args:
     **kwargs: Passed to mobilenet.training_scope. The following parameters
     are supported:
diff --git a/research/slim/nets/mobilenet/mobilenet_v2_test.py b/research/slim/nets/mobilenet/mobilenet_v2_test.py
index 11ab0eb9..444c53ed 100644
--- a/research/slim/nets/mobilenet/mobilenet_v2_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v2_test.py
@@ -19,16 +19,13 @@ from __future__ import division
 from __future__ import print_function
 import copy
 from six.moves import range
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet
 from nets.mobilenet import mobilenet_v2
 
 
-slim = contrib_slim
-
-
 def find_ops(optype):
   """Find ops of a given type in graphdef or a graph.
 
@@ -37,19 +34,16 @@ def find_ops(optype):
   Returns:
      List of operations.
   """
-  gd = tf.compat.v1.get_default_graph()
+  gd = tf.get_default_graph()
   return [var for var in gd.get_operations() if var.type == optype]
 
 
 class MobilenetV2Test(tf.test.TestCase):
 
-  def setUp(self):
-    tf.compat.v1.reset_default_graph()
-
   def testCreation(self):
     spec = dict(mobilenet_v2.V2_DEF)
     _, ep = mobilenet.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=spec)
     num_convs = len(find_ops('Conv2D'))
 
@@ -66,7 +60,7 @@ class MobilenetV2Test(tf.test.TestCase):
   def testCreationNoClasses(self):
     spec = copy.deepcopy(mobilenet_v2.V2_DEF)
     net, ep = mobilenet.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=spec,
         num_classes=None)
     self.assertIs(net, ep['global_pool'])
@@ -74,9 +68,9 @@ class MobilenetV2Test(tf.test.TestCase):
   def testImageSizes(self):
     for input_size, output_size in [(224, 7), (192, 6), (160, 5),
                                     (128, 4), (96, 3)]:
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       _, ep = mobilenet_v2.mobilenet(
-          tf.compat.v1.placeholder(tf.float32, (10, input_size, input_size, 3)))
+          tf.placeholder(tf.float32, (10, input_size, input_size, 3)))
 
       self.assertEqual(ep['layer_18/output'].get_shape().as_list()[1:3],
                        [output_size] * 2)
@@ -87,7 +81,7 @@ class MobilenetV2Test(tf.test.TestCase):
         (ops.expanded_conv,): dict(split_expansion=2),
     }
     _, _ = mobilenet.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=spec)
     num_convs = len(find_ops('Conv2D'))
     # All but 3 op has 3 conv operatore, the remainign 3 have one
@@ -96,16 +90,16 @@ class MobilenetV2Test(tf.test.TestCase):
 
   def testWithOutputStride8(self):
     out, _ = mobilenet.mobilenet_base(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=8,
         scope='MobilenetV2')
     self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])
 
   def testDivisibleBy(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     mobilenet_v2.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         divisible_by=16,
         min_depth=32)
@@ -115,12 +109,12 @@ class MobilenetV2Test(tf.test.TestCase):
                              1001], s)
 
   def testDivisibleByWithArgScope(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     # Verifies that depth_multiplier arg scope actually works
     # if no default min_depth is provided.
     with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):
       mobilenet_v2.mobilenet(
-          tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 2)),
+          tf.placeholder(tf.float32, (10, 224, 224, 2)),
           conv_defs=mobilenet_v2.V2_DEF,
           depth_multiplier=0.1)
       s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops('Conv2D')]
@@ -128,12 +122,12 @@ class MobilenetV2Test(tf.test.TestCase):
       self.assertSameElements(s, [32, 192, 128, 1001])
 
   def testFineGrained(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     # Verifies that depth_multiplier arg scope actually works
     # if no default min_depth is provided.
 
     mobilenet_v2.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 2)),
+        tf.placeholder(tf.float32, (10, 224, 224, 2)),
         conv_defs=mobilenet_v2.V2_DEF,
         depth_multiplier=0.01,
         finegrain_classification_mode=True)
@@ -143,19 +137,19 @@ class MobilenetV2Test(tf.test.TestCase):
     self.assertSameElements(s, [8, 48, 1001, 1280])
 
   def testMobilenetBase(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     # Verifies that mobilenet_base returns pre-pooling layer.
     with slim.arg_scope((mobilenet.depth_multiplier,), min_depth=32):
       net, _ = mobilenet_v2.mobilenet_base(
-          tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+          tf.placeholder(tf.float32, (10, 224, 224, 16)),
           conv_defs=mobilenet_v2.V2_DEF,
           depth_multiplier=0.1)
       self.assertEqual(net.get_shape().as_list(), [10, 7, 7, 128])
 
   def testWithOutputStride16(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=16)
     self.assertEqual(out.get_shape().as_list()[1:3], [14, 14])
@@ -174,7 +168,7 @@ class MobilenetV2Test(tf.test.TestCase):
         multiplier_func=inverse_multiplier,
         num_outputs=16)
     _ = mobilenet_v2.mobilenet_base(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=new_def,
         depth_multiplier=0.1)
     s = [op.outputs[0].get_shape().as_list()[-1] for op in find_ops('Conv2D')]
@@ -183,9 +177,9 @@ class MobilenetV2Test(tf.test.TestCase):
     self.assertEqual([160, 8, 48, 8, 48], s[:5])
 
   def testWithOutputStride8AndExplicitPadding(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=8,
         use_explicit_padding=True,
@@ -193,9 +187,9 @@ class MobilenetV2Test(tf.test.TestCase):
     self.assertEqual(out.get_shape().as_list()[1:3], [28, 28])
 
   def testWithOutputStride16AndExplicitPadding(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     out, _ = mobilenet.mobilenet_base(
-        tf.compat.v1.placeholder(tf.float32, (10, 224, 224, 16)),
+        tf.placeholder(tf.float32, (10, 224, 224, 16)),
         conv_defs=mobilenet_v2.V2_DEF,
         output_stride=16,
         use_explicit_padding=True)
diff --git a/research/slim/nets/mobilenet/mobilenet_v3.py b/research/slim/nets/mobilenet/mobilenet_v3.py
index 36dbdaa9..fe8fd4af 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3.py
@@ -12,7 +12,341 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-"""Mobilenet V3 conv defs and helper functions."""
+"""Mobilenet V3 conv defs and helper functions.
+
+# pylint: disable=line-too-long
+
+Model definitions and layer breakdowns:
+==================
+==== V3 LARGE ====
+==================
+    Conv2D MobilenetV3/Conv/Conv2D                                                              351.2 k      1x224x224x3            432.0           5.42 M     1x112x112x16
+     Relu6 MobilenetV3/Conv/hard_swish/Relu6                                                          ?                -                ?                ?     1x112x112x16
+ DepthConv MobilenetV3/expanded_conv/depthwise/depthwise                                        401.4 k                -            144.0           1.81 M     1x112x112x16
+      Relu MobilenetV3/expanded_conv/depthwise/Relu                                                   ?                -                ?                ?     1x112x112x16
+    Conv2D MobilenetV3/expanded_conv/project/Conv2D                                             401.4 k     1x112x112x16            256.0           3.21 M     1x112x112x16
+    Conv2D MobilenetV3/expanded_conv_1/expand/Conv2D                                             1.00 M     1x112x112x16           1.02 k           12.8 M     1x112x112x64
+      Relu MobilenetV3/expanded_conv_1/expand/Relu                                                    ?                -                ?                ?     1x112x112x64
+ DepthConv MobilenetV3/expanded_conv_1/depthwise/depthwise                                       1.00 M                -            576.0           1.81 M       1x56x56x64
+      Relu MobilenetV3/expanded_conv_1/depthwise/Relu                                                 ?                -                ?                ?       1x56x56x64
+    Conv2D MobilenetV3/expanded_conv_1/project/Conv2D                                           276.0 k       1x56x56x64           1.54 k           4.82 M       1x56x56x24
+    Conv2D MobilenetV3/expanded_conv_2/expand/Conv2D                                            301.1 k       1x56x56x24           1.73 k           5.42 M       1x56x56x72
+      Relu MobilenetV3/expanded_conv_2/expand/Relu                                                    ?                -                ?                ?       1x56x56x72
+ DepthConv MobilenetV3/expanded_conv_2/depthwise/depthwise                                      451.6 k                -            648.0           2.03 M       1x56x56x72
+      Relu MobilenetV3/expanded_conv_2/depthwise/Relu                                                 ?                -                ?                ?       1x56x56x72
+    Conv2D MobilenetV3/expanded_conv_2/project/Conv2D                                           301.1 k       1x56x56x72           1.73 k           5.42 M       1x56x56x24
+    Conv2D MobilenetV3/expanded_conv_3/expand/Conv2D                                            301.1 k       1x56x56x24           1.73 k           5.42 M       1x56x56x72
+      Relu MobilenetV3/expanded_conv_3/expand/Relu                                                    ?                -                ?                ?       1x56x56x72
+ DepthConv MobilenetV3/expanded_conv_3/depthwise/depthwise                                      282.2 k                -           1.80 k           1.41 M       1x28x28x72
+      Relu MobilenetV3/expanded_conv_3/depthwise/Relu                                                 ?                -                ?                ?       1x28x28x72
+    Conv2D MobilenetV3/expanded_conv_3/squeeze_excite/Conv/Conv2D                                  96.0         1x1x1x72           1.73 k           1.73 k         1x1x1x24
+      Relu MobilenetV3/expanded_conv_3/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x24
+    Conv2D MobilenetV3/expanded_conv_3/squeeze_excite/Conv_1/Conv2D                                96.0         1x1x1x24           1.73 k           1.73 k         1x1x1x72
+     Relu6 MobilenetV3/expanded_conv_3/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?         1x1x1x72
+    Conv2D MobilenetV3/expanded_conv_3/project/Conv2D                                            87.8 k       1x28x28x72           2.88 k           2.26 M       1x28x28x40
+    Conv2D MobilenetV3/expanded_conv_4/expand/Conv2D                                            125.4 k       1x28x28x40           4.80 k           3.76 M      1x28x28x120
+      Relu MobilenetV3/expanded_conv_4/expand/Relu                                                    ?                -                ?                ?      1x28x28x120
+ DepthConv MobilenetV3/expanded_conv_4/depthwise/depthwise                                      188.2 k                -           3.00 k           2.35 M      1x28x28x120
+      Relu MobilenetV3/expanded_conv_4/depthwise/Relu                                                 ?                -                ?                ?      1x28x28x120
+    Conv2D MobilenetV3/expanded_conv_4/squeeze_excite/Conv/Conv2D                                 152.0        1x1x1x120           3.84 k           3.84 k         1x1x1x32
+      Relu MobilenetV3/expanded_conv_4/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x32
+    Conv2D MobilenetV3/expanded_conv_4/squeeze_excite/Conv_1/Conv2D                               152.0         1x1x1x32           3.84 k           3.84 k        1x1x1x120
+     Relu6 MobilenetV3/expanded_conv_4/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x120
+    Conv2D MobilenetV3/expanded_conv_4/project/Conv2D                                           125.4 k      1x28x28x120           4.80 k           3.76 M       1x28x28x40
+    Conv2D MobilenetV3/expanded_conv_5/expand/Conv2D                                            125.4 k       1x28x28x40           4.80 k           3.76 M      1x28x28x120
+      Relu MobilenetV3/expanded_conv_5/expand/Relu                                                    ?                -                ?                ?      1x28x28x120
+ DepthConv MobilenetV3/expanded_conv_5/depthwise/depthwise                                      188.2 k                -           3.00 k           2.35 M      1x28x28x120
+      Relu MobilenetV3/expanded_conv_5/depthwise/Relu                                                 ?                -                ?                ?      1x28x28x120
+    Conv2D MobilenetV3/expanded_conv_5/squeeze_excite/Conv/Conv2D                                 152.0        1x1x1x120           3.84 k           3.84 k         1x1x1x32
+      Relu MobilenetV3/expanded_conv_5/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x32
+    Conv2D MobilenetV3/expanded_conv_5/squeeze_excite/Conv_1/Conv2D                               152.0         1x1x1x32           3.84 k           3.84 k        1x1x1x120
+     Relu6 MobilenetV3/expanded_conv_5/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x120
+    Conv2D MobilenetV3/expanded_conv_5/project/Conv2D                                           125.4 k      1x28x28x120           4.80 k           3.76 M       1x28x28x40
+    Conv2D MobilenetV3/expanded_conv_6/expand/Conv2D                                            219.5 k       1x28x28x40           9.60 k           7.53 M      1x28x28x240
+     Relu6 MobilenetV3/expanded_conv_6/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x28x28x240
+ DepthConv MobilenetV3/expanded_conv_6/depthwise/depthwise                                      235.2 k                -           2.16 k          423.4 k      1x14x14x240
+     Relu6 MobilenetV3/expanded_conv_6/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x240
+    Conv2D MobilenetV3/expanded_conv_6/project/Conv2D                                            62.7 k      1x14x14x240           19.2 k           3.76 M       1x14x14x80
+    Conv2D MobilenetV3/expanded_conv_7/expand/Conv2D                                             54.9 k       1x14x14x80           16.0 k           3.14 M      1x14x14x200
+     Relu6 MobilenetV3/expanded_conv_7/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x200
+ DepthConv MobilenetV3/expanded_conv_7/depthwise/depthwise                                       78.4 k                -           1.80 k          352.8 k      1x14x14x200
+     Relu6 MobilenetV3/expanded_conv_7/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x200
+    Conv2D MobilenetV3/expanded_conv_7/project/Conv2D                                            54.9 k      1x14x14x200           16.0 k           3.14 M       1x14x14x80
+    Conv2D MobilenetV3/expanded_conv_8/expand/Conv2D                                             51.7 k       1x14x14x80           14.7 k           2.89 M      1x14x14x184
+     Relu6 MobilenetV3/expanded_conv_8/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x184
+ DepthConv MobilenetV3/expanded_conv_8/depthwise/depthwise                                       72.1 k                -           1.66 k          324.6 k      1x14x14x184
+     Relu6 MobilenetV3/expanded_conv_8/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x184
+    Conv2D MobilenetV3/expanded_conv_8/project/Conv2D                                            51.7 k      1x14x14x184           14.7 k           2.89 M       1x14x14x80
+    Conv2D MobilenetV3/expanded_conv_9/expand/Conv2D                                             51.7 k       1x14x14x80           14.7 k           2.89 M      1x14x14x184
+     Relu6 MobilenetV3/expanded_conv_9/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x184
+ DepthConv MobilenetV3/expanded_conv_9/depthwise/depthwise                                       72.1 k                -           1.66 k          324.6 k      1x14x14x184
+     Relu6 MobilenetV3/expanded_conv_9/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x184
+    Conv2D MobilenetV3/expanded_conv_9/project/Conv2D                                            51.7 k      1x14x14x184           14.7 k           2.89 M       1x14x14x80
+    Conv2D MobilenetV3/expanded_conv_10/expand/Conv2D                                           109.8 k       1x14x14x80           38.4 k           7.53 M      1x14x14x480
+     Relu6 MobilenetV3/expanded_conv_10/expand/hard_swish/Relu6                                       ?                -                ?                ?      1x14x14x480
+ DepthConv MobilenetV3/expanded_conv_10/depthwise/depthwise                                     188.2 k                -           4.32 k          846.7 k      1x14x14x480
+     Relu6 MobilenetV3/expanded_conv_10/depthwise/hard_swish/Relu6                                    ?                -                ?                ?      1x14x14x480
+    Conv2D MobilenetV3/expanded_conv_10/squeeze_excite/Conv/Conv2D                                600.0        1x1x1x480           57.6 k           57.6 k        1x1x1x120
+      Relu MobilenetV3/expanded_conv_10/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x120
+    Conv2D MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/Conv2D                              600.0        1x1x1x120           57.6 k           57.6 k        1x1x1x480
+     Relu6 MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x480
+    Conv2D MobilenetV3/expanded_conv_10/project/Conv2D                                          116.0 k      1x14x14x480           53.8 k           10.5 M      1x14x14x112
+    Conv2D MobilenetV3/expanded_conv_11/expand/Conv2D                                           153.7 k      1x14x14x112           75.3 k           14.8 M      1x14x14x672
+     Relu6 MobilenetV3/expanded_conv_11/expand/hard_swish/Relu6                                       ?                -                ?                ?      1x14x14x672
+ DepthConv MobilenetV3/expanded_conv_11/depthwise/depthwise                                     263.4 k                -           6.05 k           1.19 M      1x14x14x672
+     Relu6 MobilenetV3/expanded_conv_11/depthwise/hard_swish/Relu6                                    ?                -                ?                ?      1x14x14x672
+    Conv2D MobilenetV3/expanded_conv_11/squeeze_excite/Conv/Conv2D                                840.0        1x1x1x672          112.9 k          112.9 k        1x1x1x168
+      Relu MobilenetV3/expanded_conv_11/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x168
+    Conv2D MobilenetV3/expanded_conv_11/squeeze_excite/Conv_1/Conv2D                              840.0        1x1x1x168          112.9 k          112.9 k        1x1x1x672
+     Relu6 MobilenetV3/expanded_conv_11/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x672
+    Conv2D MobilenetV3/expanded_conv_11/project/Conv2D                                          153.7 k      1x14x14x672           75.3 k           14.8 M      1x14x14x112
+    Conv2D MobilenetV3/expanded_conv_12/expand/Conv2D                                           153.7 k      1x14x14x112           75.3 k           14.8 M      1x14x14x672
+     Relu6 MobilenetV3/expanded_conv_12/expand/hard_swish/Relu6                                       ?                -                ?                ?      1x14x14x672
+ DepthConv MobilenetV3/expanded_conv_12/depthwise/depthwise                                     164.6 k                -           16.8 k          823.2 k        1x7x7x672
+     Relu6 MobilenetV3/expanded_conv_12/depthwise/hard_swish/Relu6                                    ?                -                ?                ?        1x7x7x672
+    Conv2D MobilenetV3/expanded_conv_12/squeeze_excite/Conv/Conv2D                                840.0        1x1x1x672          112.9 k          112.9 k        1x1x1x168
+      Relu MobilenetV3/expanded_conv_12/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x168
+    Conv2D MobilenetV3/expanded_conv_12/squeeze_excite/Conv_1/Conv2D                              840.0        1x1x1x168          112.9 k          112.9 k        1x1x1x672
+     Relu6 MobilenetV3/expanded_conv_12/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x672
+    Conv2D MobilenetV3/expanded_conv_12/project/Conv2D                                           40.8 k        1x7x7x672          107.5 k           5.27 M        1x7x7x160
+    Conv2D MobilenetV3/expanded_conv_13/expand/Conv2D                                            54.9 k        1x7x7x160          153.6 k           7.53 M        1x7x7x960
+     Relu6 MobilenetV3/expanded_conv_13/expand/hard_swish/Relu6                                       ?                -                ?                ?        1x7x7x960
+ DepthConv MobilenetV3/expanded_conv_13/depthwise/depthwise                                      94.1 k                -           24.0 k           1.18 M        1x7x7x960
+     Relu6 MobilenetV3/expanded_conv_13/depthwise/hard_swish/Relu6                                    ?                -                ?                ?        1x7x7x960
+    Conv2D MobilenetV3/expanded_conv_13/squeeze_excite/Conv/Conv2D                               1.20 k        1x1x1x960          230.4 k          230.4 k        1x1x1x240
+      Relu MobilenetV3/expanded_conv_13/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x240
+    Conv2D MobilenetV3/expanded_conv_13/squeeze_excite/Conv_1/Conv2D                             1.20 k        1x1x1x240          230.4 k          230.4 k        1x1x1x960
+     Relu6 MobilenetV3/expanded_conv_13/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x960
+    Conv2D MobilenetV3/expanded_conv_13/project/Conv2D                                           54.9 k        1x7x7x960          153.6 k           7.53 M        1x7x7x160
+    Conv2D MobilenetV3/expanded_conv_14/expand/Conv2D                                            54.9 k        1x7x7x160          153.6 k           7.53 M        1x7x7x960
+     Relu6 MobilenetV3/expanded_conv_14/expand/hard_swish/Relu6                                       ?                -                ?                ?        1x7x7x960
+ DepthConv MobilenetV3/expanded_conv_14/depthwise/depthwise                                      94.1 k                -           24.0 k           1.18 M        1x7x7x960
+     Relu6 MobilenetV3/expanded_conv_14/depthwise/hard_swish/Relu6                                    ?                -                ?                ?        1x7x7x960
+    Conv2D MobilenetV3/expanded_conv_14/squeeze_excite/Conv/Conv2D                               1.20 k        1x1x1x960          230.4 k          230.4 k        1x1x1x240
+      Relu MobilenetV3/expanded_conv_14/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x240
+    Conv2D MobilenetV3/expanded_conv_14/squeeze_excite/Conv_1/Conv2D                             1.20 k        1x1x1x240          230.4 k          230.4 k        1x1x1x960
+     Relu6 MobilenetV3/expanded_conv_14/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x960
+    Conv2D MobilenetV3/expanded_conv_14/project/Conv2D                                           54.9 k        1x7x7x960          153.6 k           7.53 M        1x7x7x160
+    Conv2D MobilenetV3/Conv_1/Conv2D                                                             54.9 k        1x7x7x160          153.6 k           7.53 M        1x7x7x960
+     Relu6 MobilenetV3/Conv_1/hard_swish/Relu6                                                        ?                -                ?                ?        1x7x7x960
+   AvgPool MobilenetV3/AvgPool2D/AvgPool                                                              ?        1x7x7x960                ?           47.0 k        1x1x1x960
+    Conv2D MobilenetV3/Conv_2/Conv2D                                                             2.24 k        1x1x1x960           1.23 M           1.23 M       1x1x1x1280
+     Relu6 MobilenetV3/Conv_2/hard_swish/Relu6                                                        ?                -                ?                ?       1x1x1x1280
+    Conv2D MobilenetV3/Logits/Conv2d_1c_1x1/Conv2D                                               2.28 k       1x1x1x1280           1.28 M           1.28 M       1x1x1x1001
+-----
+
+
+==================
+==== V3 SMALL ====
+==================
+      op name                                                                                  ActMem        ConvInput   ConvParameters            Madds     OutputTensor
+    Conv2D MobilenetV3/Conv/Conv2D                                                              351.2 k      1x224x224x3            432.0           5.42 M     1x112x112x16
+     Relu6 MobilenetV3/Conv/hard_swish/Relu6                                                          ?                -                ?                ?     1x112x112x16
+ DepthConv MobilenetV3/expanded_conv/depthwise/depthwise                                        250.9 k                -            144.0          451.6 k       1x56x56x16
+      Relu MobilenetV3/expanded_conv/depthwise/Relu                                                   ?                -                ?                ?       1x56x56x16
+    Conv2D MobilenetV3/expanded_conv/squeeze_excite/Conv/Conv2D                                    24.0         1x1x1x16            128.0            128.0          1x1x1x8
+      Relu MobilenetV3/expanded_conv/squeeze_excite/Conv/Relu                                         ?                -                ?                ?          1x1x1x8
+    Conv2D MobilenetV3/expanded_conv/squeeze_excite/Conv_1/Conv2D                                  24.0          1x1x1x8            128.0            128.0         1x1x1x16
+     Relu6 MobilenetV3/expanded_conv/squeeze_excite/Conv_1/Relu6                                      ?                -                ?                ?         1x1x1x16
+    Conv2D MobilenetV3/expanded_conv/project/Conv2D                                             100.4 k       1x56x56x16            256.0          802.8 k       1x56x56x16
+    Conv2D MobilenetV3/expanded_conv_1/expand/Conv2D                                            276.0 k       1x56x56x16           1.15 k           3.61 M       1x56x56x72
+      Relu MobilenetV3/expanded_conv_1/expand/Relu                                                    ?                -                ?                ?       1x56x56x72
+ DepthConv MobilenetV3/expanded_conv_1/depthwise/depthwise                                      282.2 k                -            648.0          508.0 k       1x28x28x72
+      Relu MobilenetV3/expanded_conv_1/depthwise/Relu                                                 ?                -                ?                ?       1x28x28x72
+    Conv2D MobilenetV3/expanded_conv_1/project/Conv2D                                            75.3 k       1x28x28x72           1.73 k           1.35 M       1x28x28x24
+    Conv2D MobilenetV3/expanded_conv_2/expand/Conv2D                                             87.8 k       1x28x28x24           2.11 k           1.66 M       1x28x28x88
+      Relu MobilenetV3/expanded_conv_2/expand/Relu                                                    ?                -                ?                ?       1x28x28x88
+ DepthConv MobilenetV3/expanded_conv_2/depthwise/depthwise                                      138.0 k                -            792.0          620.9 k       1x28x28x88
+      Relu MobilenetV3/expanded_conv_2/depthwise/Relu                                                 ?                -                ?                ?       1x28x28x88
+    Conv2D MobilenetV3/expanded_conv_2/project/Conv2D                                            87.8 k       1x28x28x88           2.11 k           1.66 M       1x28x28x24
+    Conv2D MobilenetV3/expanded_conv_3/expand/Conv2D                                             94.1 k       1x28x28x24           2.30 k           1.81 M       1x28x28x96
+     Relu6 MobilenetV3/expanded_conv_3/expand/hard_swish/Relu6                                        ?                -                ?                ?       1x28x28x96
+ DepthConv MobilenetV3/expanded_conv_3/depthwise/depthwise                                       94.1 k                -           2.40 k          470.4 k       1x14x14x96
+     Relu6 MobilenetV3/expanded_conv_3/depthwise/hard_swish/Relu6                                     ?                -                ?                ?       1x14x14x96
+    Conv2D MobilenetV3/expanded_conv_3/squeeze_excite/Conv/Conv2D                                 120.0         1x1x1x96           2.30 k           2.30 k         1x1x1x24
+      Relu MobilenetV3/expanded_conv_3/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x24
+    Conv2D MobilenetV3/expanded_conv_3/squeeze_excite/Conv_1/Conv2D                               120.0         1x1x1x24           2.30 k           2.30 k         1x1x1x96
+     Relu6 MobilenetV3/expanded_conv_3/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?         1x1x1x96
+    Conv2D MobilenetV3/expanded_conv_3/project/Conv2D                                            26.7 k       1x14x14x96           3.84 k          752.6 k       1x14x14x40
+    Conv2D MobilenetV3/expanded_conv_4/expand/Conv2D                                             54.9 k       1x14x14x40           9.60 k           1.88 M      1x14x14x240
+     Relu6 MobilenetV3/expanded_conv_4/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x240
+ DepthConv MobilenetV3/expanded_conv_4/depthwise/depthwise                                       94.1 k                -           6.00 k           1.18 M      1x14x14x240
+     Relu6 MobilenetV3/expanded_conv_4/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x240
+    Conv2D MobilenetV3/expanded_conv_4/squeeze_excite/Conv/Conv2D                                 304.0        1x1x1x240           15.4 k           15.4 k         1x1x1x64
+      Relu MobilenetV3/expanded_conv_4/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x64
+    Conv2D MobilenetV3/expanded_conv_4/squeeze_excite/Conv_1/Conv2D                               304.0         1x1x1x64           15.4 k           15.4 k        1x1x1x240
+     Relu6 MobilenetV3/expanded_conv_4/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x240
+    Conv2D MobilenetV3/expanded_conv_4/project/Conv2D                                            54.9 k      1x14x14x240           9.60 k           1.88 M       1x14x14x40
+    Conv2D MobilenetV3/expanded_conv_5/expand/Conv2D                                             54.9 k       1x14x14x40           9.60 k           1.88 M      1x14x14x240
+     Relu6 MobilenetV3/expanded_conv_5/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x240
+ DepthConv MobilenetV3/expanded_conv_5/depthwise/depthwise                                       94.1 k                -           6.00 k           1.18 M      1x14x14x240
+     Relu6 MobilenetV3/expanded_conv_5/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x240
+    Conv2D MobilenetV3/expanded_conv_5/squeeze_excite/Conv/Conv2D                                 304.0        1x1x1x240           15.4 k           15.4 k         1x1x1x64
+      Relu MobilenetV3/expanded_conv_5/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x64
+    Conv2D MobilenetV3/expanded_conv_5/squeeze_excite/Conv_1/Conv2D                               304.0         1x1x1x64           15.4 k           15.4 k        1x1x1x240
+     Relu6 MobilenetV3/expanded_conv_5/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x240
+    Conv2D MobilenetV3/expanded_conv_5/project/Conv2D                                            54.9 k      1x14x14x240           9.60 k           1.88 M       1x14x14x40
+    Conv2D MobilenetV3/expanded_conv_6/expand/Conv2D                                             31.4 k       1x14x14x40           4.80 k          940.8 k      1x14x14x120
+     Relu6 MobilenetV3/expanded_conv_6/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x120
+ DepthConv MobilenetV3/expanded_conv_6/depthwise/depthwise                                       47.0 k                -           3.00 k          588.0 k      1x14x14x120
+     Relu6 MobilenetV3/expanded_conv_6/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x120
+    Conv2D MobilenetV3/expanded_conv_6/squeeze_excite/Conv/Conv2D                                 152.0        1x1x1x120           3.84 k           3.84 k         1x1x1x32
+      Relu MobilenetV3/expanded_conv_6/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x32
+    Conv2D MobilenetV3/expanded_conv_6/squeeze_excite/Conv_1/Conv2D                               152.0         1x1x1x32           3.84 k           3.84 k        1x1x1x120
+     Relu6 MobilenetV3/expanded_conv_6/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x120
+    Conv2D MobilenetV3/expanded_conv_6/project/Conv2D                                            32.9 k      1x14x14x120           5.76 k           1.13 M       1x14x14x48
+    Conv2D MobilenetV3/expanded_conv_7/expand/Conv2D                                             37.6 k       1x14x14x48           6.91 k           1.35 M      1x14x14x144
+     Relu6 MobilenetV3/expanded_conv_7/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x144
+ DepthConv MobilenetV3/expanded_conv_7/depthwise/depthwise                                       56.4 k                -           3.60 k          705.6 k      1x14x14x144
+     Relu6 MobilenetV3/expanded_conv_7/depthwise/hard_swish/Relu6                                     ?                -                ?                ?      1x14x14x144
+    Conv2D MobilenetV3/expanded_conv_7/squeeze_excite/Conv/Conv2D                                 184.0        1x1x1x144           5.76 k           5.76 k         1x1x1x40
+      Relu MobilenetV3/expanded_conv_7/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x40
+    Conv2D MobilenetV3/expanded_conv_7/squeeze_excite/Conv_1/Conv2D                               184.0         1x1x1x40           5.76 k           5.76 k        1x1x1x144
+     Relu6 MobilenetV3/expanded_conv_7/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x144
+    Conv2D MobilenetV3/expanded_conv_7/project/Conv2D                                            37.6 k      1x14x14x144           6.91 k           1.35 M       1x14x14x48
+    Conv2D MobilenetV3/expanded_conv_8/expand/Conv2D                                             65.9 k       1x14x14x48           13.8 k           2.71 M      1x14x14x288
+     Relu6 MobilenetV3/expanded_conv_8/expand/hard_swish/Relu6                                        ?                -                ?                ?      1x14x14x288
+ DepthConv MobilenetV3/expanded_conv_8/depthwise/depthwise                                       70.6 k                -           7.20 k          352.8 k        1x7x7x288
+     Relu6 MobilenetV3/expanded_conv_8/depthwise/hard_swish/Relu6                                     ?                -                ?                ?        1x7x7x288
+    Conv2D MobilenetV3/expanded_conv_8/squeeze_excite/Conv/Conv2D                                 360.0        1x1x1x288           20.7 k           20.7 k         1x1x1x72
+      Relu MobilenetV3/expanded_conv_8/squeeze_excite/Conv/Relu                                       ?                -                ?                ?         1x1x1x72
+    Conv2D MobilenetV3/expanded_conv_8/squeeze_excite/Conv_1/Conv2D                               360.0         1x1x1x72           20.7 k           20.7 k        1x1x1x288
+     Relu6 MobilenetV3/expanded_conv_8/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x288
+    Conv2D MobilenetV3/expanded_conv_8/project/Conv2D                                            18.8 k        1x7x7x288           27.6 k           1.35 M         1x7x7x96
+    Conv2D MobilenetV3/expanded_conv_9/expand/Conv2D                                             32.9 k         1x7x7x96           55.3 k           2.71 M        1x7x7x576
+     Relu6 MobilenetV3/expanded_conv_9/expand/hard_swish/Relu6                                        ?                -                ?                ?        1x7x7x576
+ DepthConv MobilenetV3/expanded_conv_9/depthwise/depthwise                                       56.4 k                -           14.4 k          705.6 k        1x7x7x576
+     Relu6 MobilenetV3/expanded_conv_9/depthwise/hard_swish/Relu6                                     ?                -                ?                ?        1x7x7x576
+    Conv2D MobilenetV3/expanded_conv_9/squeeze_excite/Conv/Conv2D                                 720.0        1x1x1x576           82.9 k           82.9 k        1x1x1x144
+      Relu MobilenetV3/expanded_conv_9/squeeze_excite/Conv/Relu                                       ?                -                ?                ?        1x1x1x144
+    Conv2D MobilenetV3/expanded_conv_9/squeeze_excite/Conv_1/Conv2D                               720.0        1x1x1x144           82.9 k           82.9 k        1x1x1x576
+     Relu6 MobilenetV3/expanded_conv_9/squeeze_excite/Conv_1/Relu6                                    ?                -                ?                ?        1x1x1x576
+    Conv2D MobilenetV3/expanded_conv_9/project/Conv2D                                            32.9 k        1x7x7x576           55.3 k           2.71 M         1x7x7x96
+    Conv2D MobilenetV3/expanded_conv_10/expand/Conv2D                                            32.9 k         1x7x7x96           55.3 k           2.71 M        1x7x7x576
+     Relu6 MobilenetV3/expanded_conv_10/expand/hard_swish/Relu6                                       ?                -                ?                ?        1x7x7x576
+ DepthConv MobilenetV3/expanded_conv_10/depthwise/depthwise                                      56.4 k                -           14.4 k          705.6 k        1x7x7x576
+     Relu6 MobilenetV3/expanded_conv_10/depthwise/hard_swish/Relu6                                    ?                -                ?                ?        1x7x7x576
+    Conv2D MobilenetV3/expanded_conv_10/squeeze_excite/Conv/Conv2D                                720.0        1x1x1x576           82.9 k           82.9 k        1x1x1x144
+      Relu MobilenetV3/expanded_conv_10/squeeze_excite/Conv/Relu                                      ?                -                ?                ?        1x1x1x144
+    Conv2D MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/Conv2D                              720.0        1x1x1x144           82.9 k           82.9 k        1x1x1x576
+     Relu6 MobilenetV3/expanded_conv_10/squeeze_excite/Conv_1/Relu6                                   ?                -                ?                ?        1x1x1x576
+    Conv2D MobilenetV3/expanded_conv_10/project/Conv2D                                           32.9 k        1x7x7x576           55.3 k           2.71 M         1x7x7x96
+    Conv2D MobilenetV3/Conv_1/Conv2D                                                             32.9 k         1x7x7x96           55.3 k           2.71 M        1x7x7x576
+     Relu6 MobilenetV3/Conv_1/hard_swish/Relu6                                                        ?                -                ?                ?        1x7x7x576
+   AvgPool MobilenetV3/AvgPool2D/AvgPool                                                              ?        1x7x7x576                ?           28.2 k        1x1x1x576
+    Conv2D MobilenetV3/Conv_2/Conv2D                                                             1.60 k        1x1x1x576          589.8 k          589.8 k       1x1x1x1024
+     Relu6 MobilenetV3/Conv_2/hard_swish/Relu6                                                        ?                -                ?                ?       1x1x1x1024
+    Conv2D MobilenetV3/Logits/Conv2d_1c_1x1/Conv2D                                               2.02 k       1x1x1x1024           1.03 M           1.03 M       1x1x1x1001
+-----
+     Total Total                                                                                 2.96 M                -           2.53 M           56.5 M                -
+
+
+====================
+==== V3 EDGETPU ====
+====================
+        op name                                                                                  ActMem        ConvInput   ConvParameters            Madds     OutputTensor
+    Conv2D MobilenetEdgeTPU/Conv/Conv2D                                                         551.9 k      1x224x224x3            864.0           10.8 M     1x112x112x32
+      Relu MobilenetEdgeTPU/Conv/Relu                                                                 ?                -                ?                ?     1x112x112x32
+    Conv2D MobilenetEdgeTPU/expanded_conv/project/Conv2D                                        602.1 k     1x112x112x32            512.0           6.42 M     1x112x112x16
+    Conv2D MobilenetEdgeTPU/expanded_conv_1/expand/Conv2D                                       602.1 k     1x112x112x16           18.4 k           57.8 M      1x56x56x128
+      Relu MobilenetEdgeTPU/expanded_conv_1/expand/Relu                                               ?                -                ?                ?      1x56x56x128
+    Conv2D MobilenetEdgeTPU/expanded_conv_1/project/Conv2D                                      501.8 k      1x56x56x128           4.10 k           12.8 M       1x56x56x32
+    Conv2D MobilenetEdgeTPU/expanded_conv_2/expand/Conv2D                                       501.8 k       1x56x56x32           36.9 k          115.6 M      1x56x56x128
+      Relu MobilenetEdgeTPU/expanded_conv_2/expand/Relu                                               ?                -                ?                ?      1x56x56x128
+    Conv2D MobilenetEdgeTPU/expanded_conv_2/project/Conv2D                                      501.8 k      1x56x56x128           4.10 k           12.8 M       1x56x56x32
+    Conv2D MobilenetEdgeTPU/expanded_conv_3/expand/Conv2D                                       501.8 k       1x56x56x32           36.9 k          115.6 M      1x56x56x128
+      Relu MobilenetEdgeTPU/expanded_conv_3/expand/Relu                                               ?                -                ?                ?      1x56x56x128
+    Conv2D MobilenetEdgeTPU/expanded_conv_3/project/Conv2D                                      501.8 k      1x56x56x128           4.10 k           12.8 M       1x56x56x32
+    Conv2D MobilenetEdgeTPU/expanded_conv_4/expand/Conv2D                                       501.8 k       1x56x56x32           36.9 k          115.6 M      1x56x56x128
+      Relu MobilenetEdgeTPU/expanded_conv_4/expand/Relu                                               ?                -                ?                ?      1x56x56x128
+    Conv2D MobilenetEdgeTPU/expanded_conv_4/project/Conv2D                                      501.8 k      1x56x56x128           4.10 k           12.8 M       1x56x56x32
+    Conv2D MobilenetEdgeTPU/expanded_conv_5/expand/Conv2D                                       301.1 k       1x56x56x32           73.7 k           57.8 M      1x28x28x256
+      Relu MobilenetEdgeTPU/expanded_conv_5/expand/Relu                                               ?                -                ?                ?      1x28x28x256
+    Conv2D MobilenetEdgeTPU/expanded_conv_5/project/Conv2D                                      238.3 k      1x28x28x256           12.3 k           9.63 M       1x28x28x48
+    Conv2D MobilenetEdgeTPU/expanded_conv_6/expand/Conv2D                                       188.2 k       1x28x28x48           82.9 k           65.0 M      1x28x28x192
+      Relu MobilenetEdgeTPU/expanded_conv_6/expand/Relu                                               ?                -                ?                ?      1x28x28x192
+    Conv2D MobilenetEdgeTPU/expanded_conv_6/project/Conv2D                                      188.2 k      1x28x28x192           9.22 k           7.23 M       1x28x28x48
+    Conv2D MobilenetEdgeTPU/expanded_conv_7/expand/Conv2D                                       188.2 k       1x28x28x48           82.9 k           65.0 M      1x28x28x192
+      Relu MobilenetEdgeTPU/expanded_conv_7/expand/Relu                                               ?                -                ?                ?      1x28x28x192
+    Conv2D MobilenetEdgeTPU/expanded_conv_7/project/Conv2D                                      188.2 k      1x28x28x192           9.22 k           7.23 M       1x28x28x48
+    Conv2D MobilenetEdgeTPU/expanded_conv_8/expand/Conv2D                                       188.2 k       1x28x28x48           82.9 k           65.0 M      1x28x28x192
+      Relu MobilenetEdgeTPU/expanded_conv_8/expand/Relu                                               ?                -                ?                ?      1x28x28x192
+    Conv2D MobilenetEdgeTPU/expanded_conv_8/project/Conv2D                                      188.2 k      1x28x28x192           9.22 k           7.23 M       1x28x28x48
+    Conv2D MobilenetEdgeTPU/expanded_conv_9/expand/Conv2D                                       338.7 k       1x28x28x48           18.4 k           14.5 M      1x28x28x384
+      Relu MobilenetEdgeTPU/expanded_conv_9/expand/Relu                                               ?                -                ?                ?      1x28x28x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_9/depthwise/depthwise                                 376.3 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu                                            ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_9/project/Conv2D                                       94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_10/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_10/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_10/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_10/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_11/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_11/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_11/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_11/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_12/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_12/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_12/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_12/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_13/expand/Conv2D                                      169.3 k       1x14x14x96           73.7 k           14.5 M      1x14x14x768
+      Relu MobilenetEdgeTPU/expanded_conv_13/expand/Relu                                              ?                -                ?                ?      1x14x14x768
+ DepthConv MobilenetEdgeTPU/expanded_conv_13/depthwise/depthwise                                301.1 k                -           6.91 k           1.35 M      1x14x14x768
+      Relu MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu                                           ?                -                ?                ?      1x14x14x768
+    Conv2D MobilenetEdgeTPU/expanded_conv_13/project/Conv2D                                     169.3 k      1x14x14x768           73.7 k           14.5 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_14/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_14/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_14/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_14/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_15/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_15/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_15/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_15/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_16/expand/Conv2D                                       94.1 k       1x14x14x96           36.9 k           7.23 M      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_16/expand/Relu                                              ?                -                ?                ?      1x14x14x384
+ DepthConv MobilenetEdgeTPU/expanded_conv_16/depthwise/depthwise                                150.5 k                -           3.46 k          677.4 k      1x14x14x384
+      Relu MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu                                           ?                -                ?                ?      1x14x14x384
+    Conv2D MobilenetEdgeTPU/expanded_conv_16/project/Conv2D                                      94.1 k      1x14x14x384           36.9 k           7.23 M       1x14x14x96
+    Conv2D MobilenetEdgeTPU/expanded_conv_17/expand/Conv2D                                      169.3 k       1x14x14x96           73.7 k           14.5 M      1x14x14x768
+      Relu MobilenetEdgeTPU/expanded_conv_17/expand/Relu                                              ?                -                ?                ?      1x14x14x768
+ DepthConv MobilenetEdgeTPU/expanded_conv_17/depthwise/depthwise                                188.2 k                -           19.2 k          940.8 k        1x7x7x768
+      Relu MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu                                           ?                -                ?                ?        1x7x7x768
+    Conv2D MobilenetEdgeTPU/expanded_conv_17/project/Conv2D                                      45.5 k        1x7x7x768          122.9 k           6.02 M        1x7x7x160
+    Conv2D MobilenetEdgeTPU/expanded_conv_18/expand/Conv2D                                       39.2 k        1x7x7x160          102.4 k           5.02 M        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_18/expand/Relu                                              ?                -                ?                ?        1x7x7x640
+ DepthConv MobilenetEdgeTPU/expanded_conv_18/depthwise/depthwise                                 62.7 k                -           16.0 k          784.0 k        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu                                           ?                -                ?                ?        1x7x7x640
+    Conv2D MobilenetEdgeTPU/expanded_conv_18/project/Conv2D                                      39.2 k        1x7x7x640          102.4 k           5.02 M        1x7x7x160
+    Conv2D MobilenetEdgeTPU/expanded_conv_19/expand/Conv2D                                       39.2 k        1x7x7x160          102.4 k           5.02 M        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_19/expand/Relu                                              ?                -                ?                ?        1x7x7x640
+ DepthConv MobilenetEdgeTPU/expanded_conv_19/depthwise/depthwise                                 62.7 k                -           16.0 k          784.0 k        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu                                           ?                -                ?                ?        1x7x7x640
+    Conv2D MobilenetEdgeTPU/expanded_conv_19/project/Conv2D                                      39.2 k        1x7x7x640          102.4 k           5.02 M        1x7x7x160
+    Conv2D MobilenetEdgeTPU/expanded_conv_20/expand/Conv2D                                       39.2 k        1x7x7x160          102.4 k           5.02 M        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_20/expand/Relu                                              ?                -                ?                ?        1x7x7x640
+ DepthConv MobilenetEdgeTPU/expanded_conv_20/depthwise/depthwise                                 62.7 k                -           16.0 k          784.0 k        1x7x7x640
+      Relu MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu                                           ?                -                ?                ?        1x7x7x640
+    Conv2D MobilenetEdgeTPU/expanded_conv_20/project/Conv2D                                      39.2 k        1x7x7x640          102.4 k           5.02 M        1x7x7x160
+    Conv2D MobilenetEdgeTPU/expanded_conv_21/expand/Conv2D                                       70.6 k        1x7x7x160          204.8 k           10.0 M       1x7x7x1280
+      Relu MobilenetEdgeTPU/expanded_conv_21/expand/Relu                                              ?                -                ?                ?       1x7x7x1280
+ DepthConv MobilenetEdgeTPU/expanded_conv_21/depthwise/depthwise                                125.4 k                -           11.5 k          564.5 k       1x7x7x1280
+      Relu MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu                                           ?                -                ?                ?       1x7x7x1280
+    Conv2D MobilenetEdgeTPU/expanded_conv_21/project/Conv2D                                      72.1 k       1x7x7x1280          245.8 k           12.0 M        1x7x7x192
+    Conv2D MobilenetEdgeTPU/Conv_1/Conv2D                                                        72.1 k        1x7x7x192          245.8 k           12.0 M       1x7x7x1280
+      Relu MobilenetEdgeTPU/Conv_1/Relu                                                               ?                -                ?                ?       1x7x7x1280
+   AvgPool MobilenetEdgeTPU/Logits/AvgPool2D                                                          ?       1x7x7x1280                ?           62.7 k       1x1x1x1280
+    Conv2D MobilenetEdgeTPU/Logits/Conv2d_1c_1x1/Conv2D                                          2.28 k       1x1x1x1280           1.28 M           1.28 M       1x1x1x1001
+-----
+     Total Total                                                                                 11.6 M                -           4.05 M          990.7 M                -
+
+
+# pylint: enable=line-too-long
+"""
 
 from __future__ import absolute_import
 from __future__ import division
@@ -22,13 +356,12 @@ import copy
 import functools
 import numpy as np
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets.mobilenet import conv_blocks as ops
 from nets.mobilenet import mobilenet as lib
 
-slim = contrib_slim
 op = lib.op
 expand_input = ops.expand_input_by_factor
 
@@ -45,7 +378,7 @@ _se4 = lambda expansion_tensor, input_tensor: squeeze_excite(expansion_tensor)
 
 
 def hard_swish(x):
-  with tf.compat.v1.name_scope('hard_swish'):
+  with tf.name_scope('hard_swish'):
     return x * tf.nn.relu6(x + np.float32(3)) * np.float32(1. / 6.)
 
 
@@ -126,6 +459,16 @@ DEFAULTS = {
     },
 }
 
+DEFAULTS_GROUP_NORM = {
+    (ops.expanded_conv,): dict(normalizer_fn=slim.group_norm, residual=True),
+    (slim.conv2d, slim.fully_connected, slim.separable_conv2d): {
+        'normalizer_fn': slim.group_norm,
+        'activation_fn': tf.nn.relu,
+    },
+    (slim.group_norm,): {
+        'groups': 8
+    },
+}
 # Compatible checkpoint: http://mldash/5511169891790690458#scalars
 V3_LARGE = dict(
     defaults=dict(DEFAULTS),
@@ -276,13 +619,14 @@ def mobilenet(input_tensor,
               scope='MobilenetV3',
               conv_defs=None,
               finegrain_classification_mode=False,
+              use_groupnorm=False,
               **kwargs):
   """Creates mobilenet V3 network.
 
   Inference mode is created by default. To create training use training_scope
   below.
 
-  with tf.contrib.slim.arg_scope(mobilenet_v3.training_scope()):
+  with slim.arg_scope(mobilenet_v3.training_scope()):
      logits, endpoints = mobilenet_v3.mobilenet(input_tensor)
 
   Args:
@@ -298,6 +642,8 @@ def mobilenet(input_tensor,
     https://arxiv.org/abs/1801.04381
     it improves performance for ImageNet-type of problems.
       *Note* ignored if final_endpoint makes the builder exit earlier.
+    use_groupnorm: When set to True, use group_norm as normalizer_fn.
+
     **kwargs: passed directly to mobilenet.mobilenet:
       prediction_fn- what prediction function to use.
       reuse-: whether to reuse variables (if reuse set to true, scope
@@ -313,6 +659,16 @@ def mobilenet(input_tensor,
   if 'multiplier' in kwargs:
     raise ValueError('mobilenetv2 doesn\'t support generic '
                      'multiplier parameter use "depth_multiplier" instead.')
+
+  if use_groupnorm:
+    conv_defs = copy.deepcopy(conv_defs)
+    conv_defs['defaults'] = dict(DEFAULTS_GROUP_NORM)
+    conv_defs['defaults'].update({
+        (slim.group_norm,): {
+            'groups': kwargs.pop('groups', 8)
+        }
+    })
+
   if finegrain_classification_mode:
     conv_defs = copy.deepcopy(conv_defs)
     conv_defs['spec'][-1] = conv_defs['spec'][-1]._replace(
diff --git a/research/slim/nets/mobilenet/mobilenet_v3_test.py b/research/slim/nets/mobilenet/mobilenet_v3_test.py
index 45f1b103..4fd0d7f5 100644
--- a/research/slim/nets/mobilenet/mobilenet_v3_test.py
+++ b/research/slim/nets/mobilenet/mobilenet_v3_test.py
@@ -18,54 +18,83 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from absl.testing import absltest
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from nets.mobilenet import mobilenet_v3
+from google3.testing.pybase import parameterized
 
 
-class MobilenetV3Test(absltest.TestCase):
+class MobilenetV3Test(tf.test.TestCase, parameterized.TestCase):
 
-  def setUp(self):
-    super(MobilenetV3Test, self).setUp()
-    tf.compat.v1.reset_default_graph()
+  # pylint: disable = g-unreachable-test-method
+  def assertVariablesHaveNormalizerFn(self, use_groupnorm):
+    global_variables = [v.name for v in tf.global_variables()]
+    has_batch_norm = False
+    has_group_norm = False
+    for global_variable in global_variables:
+      if 'BatchNorm' in global_variable:
+        has_batch_norm = True
+      if 'GroupNorm' in global_variable:
+        has_group_norm = True
+    if use_groupnorm:
+      self.assertFalse(has_batch_norm)
+      self.assertTrue(has_group_norm)
+    else:
+      self.assertTrue(has_batch_norm)
+      self.assertFalse(has_group_norm)
 
-  def testMobilenetV3Large(self):
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetV3Large(self, use_groupnorm):
     logits, endpoints = mobilenet_v3.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)))
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        use_groupnorm=use_groupnorm)
     self.assertEqual(endpoints['layer_19'].shape, [1, 1, 1, 1280])
     self.assertEqual(logits.shape, [1, 1001])
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
 
-  def testMobilenetV3Small(self):
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetV3Small(self, use_groupnorm):
     _, endpoints = mobilenet_v3.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)),
-        conv_defs=mobilenet_v3.V3_SMALL)
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        conv_defs=mobilenet_v3.V3_SMALL,
+        use_groupnorm=use_groupnorm)
     self.assertEqual(endpoints['layer_15'].shape, [1, 1, 1, 1024])
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
 
-  def testMobilenetEdgeTpu(self):
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetEdgeTpu(self, use_groupnorm):
     _, endpoints = mobilenet_v3.edge_tpu(
-        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)))
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        use_groupnorm=use_groupnorm)
     self.assertIn('Inference mode is created by default',
                   mobilenet_v3.edge_tpu.__doc__)
     self.assertEqual(endpoints['layer_24'].shape, [1, 7, 7, 1280])
     self.assertStartsWith(
         endpoints['layer_24'].name, 'MobilenetEdgeTPU')
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
 
   def testMobilenetEdgeTpuChangeScope(self):
     _, endpoints = mobilenet_v3.edge_tpu(
-        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)), scope='Scope')
+        tf.placeholder(tf.float32, (1, 224, 224, 3)), scope='Scope')
     self.assertStartsWith(
         endpoints['layer_24'].name, 'Scope')
 
-  def testMobilenetV3BaseOnly(self):
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetV3BaseOnly(self, use_groupnorm):
     result, endpoints = mobilenet_v3.mobilenet(
-        tf.compat.v1.placeholder(tf.float32, (1, 224, 224, 3)),
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
         conv_defs=mobilenet_v3.V3_LARGE,
+        use_groupnorm=use_groupnorm,
         base_only=True,
         final_endpoint='layer_17')
     # Get the latest layer before average pool.
     self.assertEqual(endpoints['layer_17'].shape, [1, 7, 7, 960])
     self.assertEqual(result, endpoints['layer_17'])
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
 
   def testMobilenetV3BaseOnly_VariableInput(self):
     result, endpoints = mobilenet_v3.mobilenet(
@@ -78,5 +107,34 @@ class MobilenetV3Test(absltest.TestCase):
                      [None, None, None, 960])
     self.assertEqual(result, endpoints['layer_17'])
 
+  # Use reduce mean for pooling and check for operation 'ReduceMean' in graph
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetV3WithReduceMean(self, use_groupnorm):
+    _, _ = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        conv_defs=mobilenet_v3.V3_SMALL,
+        use_groupnorm=use_groupnorm,
+        use_reduce_mean_for_pooling=True)
+    g = tf.get_default_graph()
+    reduce_mean = [v for v in g.get_operations() if 'ReduceMean' in v.name]
+    self.assertNotEmpty(reduce_mean)
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
+
+  @parameterized.named_parameters(('without_groupnorm', False),
+                                  ('with_groupnorm', True))
+  def testMobilenetV3WithOutReduceMean(self, use_groupnorm):
+    _, _ = mobilenet_v3.mobilenet(
+        tf.placeholder(tf.float32, (1, 224, 224, 3)),
+        conv_defs=mobilenet_v3.V3_SMALL,
+        use_groupnorm=use_groupnorm,
+        use_reduce_mean_for_pooling=False)
+    g = tf.get_default_graph()
+    reduce_mean = [v for v in g.get_operations() if 'ReduceMean' in v.name]
+    self.assertEmpty(reduce_mean)
+    self.assertVariablesHaveNormalizerFn(use_groupnorm)
+
+
 if __name__ == '__main__':
-  absltest.main()
+  # absltest.main()
+  tf.test.main()
diff --git a/research/slim/nets/mobilenet_v1.py b/research/slim/nets/mobilenet_v1.py
index 107c3474..f714d330 100644
--- a/research/slim/nets/mobilenet_v1.py
+++ b/research/slim/nets/mobilenet_v1.py
@@ -108,11 +108,8 @@ from __future__ import print_function
 from collections import namedtuple
 import functools
 
-import tensorflow as tf
-from tensorflow.contrib import layers as contrib_layers
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 # Conv and DepthSepConv namedtuple define layers of the MobileNet architecture
 # Conv defines 3x3 convolution layers
@@ -233,7 +230,7 @@ def mobilenet_v1_base(inputs,
   padding = 'SAME'
   if use_explicit_padding:
     padding = 'VALID'
-  with tf.compat.v1.variable_scope(scope, 'MobilenetV1', [inputs]):
+  with tf.variable_scope(scope, 'MobilenetV1', [inputs]):
     with slim.arg_scope([slim.conv2d, slim.separable_conv2d], padding=padding):
       # The current_stride variable keeps track of the output stride of the
       # activations, i.e., the running product of convolution strides up to the
@@ -311,7 +308,7 @@ def mobilenet_v1(inputs,
                  min_depth=8,
                  depth_multiplier=1.0,
                  conv_defs=None,
-                 prediction_fn=contrib_layers.softmax,
+                 prediction_fn=slim.softmax,
                  spatial_squeeze=True,
                  reuse=None,
                  scope='MobilenetV1',
@@ -359,7 +356,7 @@ def mobilenet_v1(inputs,
     raise ValueError('Invalid input tensor rank, expected 4, was: %d' %
                      len(input_shape))
 
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'MobilenetV1', [inputs], reuse=reuse) as scope:
     with slim.arg_scope([slim.batch_norm, slim.dropout],
                         is_training=is_training):
@@ -367,7 +364,7 @@ def mobilenet_v1(inputs,
                                           min_depth=min_depth,
                                           depth_multiplier=depth_multiplier,
                                           conv_defs=conv_defs)
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         if global_pool:
           # Global average pooling.
           net = tf.reduce_mean(
@@ -435,7 +432,7 @@ def mobilenet_v1_arg_scope(
     regularize_depthwise=False,
     batch_norm_decay=0.9997,
     batch_norm_epsilon=0.001,
-    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS,
+    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,
     normalizer_fn=slim.batch_norm):
   """Defines the default MobilenetV1 arg scope.
 
@@ -466,8 +463,8 @@ def mobilenet_v1_arg_scope(
     batch_norm_params['is_training'] = is_training
 
   # Set weight_decay for weights in Conv and DepthSepConv layers.
-  weights_init = tf.compat.v1.truncated_normal_initializer(stddev=stddev)
-  regularizer = contrib_layers.l2_regularizer(weight_decay)
+  weights_init = tf.truncated_normal_initializer(stddev=stddev)
+  regularizer = slim.l2_regularizer(weight_decay)
   if regularize_depthwise:
     depthwise_regularizer = regularizer
   else:
diff --git a/research/slim/nets/mobilenet_v1_eval.py b/research/slim/nets/mobilenet_v1_eval.py
index c7bd590e..e43b159b 100644
--- a/research/slim/nets/mobilenet_v1_eval.py
+++ b/research/slim/nets/mobilenet_v1_eval.py
@@ -19,17 +19,16 @@ from __future__ import division
 from __future__ import print_function
 
 import math
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from tensorflow.contrib import quantize as contrib_quantize
-from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import mobilenet_v1
 from preprocessing import preprocessing_factory
 
-slim = contrib_slim
-
-flags = tf.compat.v1.app.flags
+flags = tf.app.flags
 
 flags.DEFINE_string('master', '', 'Session master')
 flags.DEFINE_integer('batch_size', 250, 'Batch size')
@@ -74,7 +73,7 @@ def imagenet_input(is_training):
 
   image = image_preprocessing_fn(image, FLAGS.image_size, FLAGS.image_size)
 
-  images, labels = tf.compat.v1.train.batch(
+  images, labels = tf.train.batch(
       tensors=[image, label],
       batch_size=FLAGS.batch_size,
       num_threads=4,
@@ -95,10 +94,10 @@ def metrics(logits, labels):
   labels = tf.squeeze(labels)
   names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({
       'Accuracy':
-          tf.compat.v1.metrics.accuracy(
+          tf.metrics.accuracy(
               tf.argmax(input=logits, axis=1), labels),
       'Recall_5':
-          tf.compat.v1.metrics.recall_at_k(labels, logits, 5),
+          tf.metrics.recall_at_k(labels, logits, 5),
   })
   for name, value in names_to_values.iteritems():
     slim.summaries.add_scalar_summary(
@@ -154,4 +153,4 @@ def main(unused_arg):
 
 
 if __name__ == '__main__':
-  tf.compat.v1.app.run(main)
+  tf.app.run(main)
diff --git a/research/slim/nets/mobilenet_v1_test.py b/research/slim/nets/mobilenet_v1_test.py
index 3f8d9b2c..5e005a5b 100644
--- a/research/slim/nets/mobilenet_v1_test.py
+++ b/research/slim/nets/mobilenet_v1_test.py
@@ -19,13 +19,11 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import mobilenet_v1
 
-slim = contrib_slim
-
 
 class MobilenetV1Test(tf.test.TestCase):
 
@@ -105,7 +103,7 @@ class MobilenetV1Test(tf.test.TestCase):
             inputs, final_endpoint=endpoint)
         self.assertTrue(out_tensor.op.name.startswith(
             'MobilenetV1/' + endpoint))
-        self.assertItemsEqual(endpoints[:index+1], end_points.keys())
+        self.assertItemsEqual(endpoints[:index + 1], end_points.keys())
 
   def testBuildCustomNetworkUsingConvDefs(self):
     batch_size = 5
@@ -420,13 +418,13 @@ class MobilenetV1Test(tf.test.TestCase):
                          [batch_size, 4, 4, 1024])
 
   def testUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 2
     height, width = 224, 224
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes)
       self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
@@ -434,18 +432,18 @@ class MobilenetV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_13_pointwise']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 7, 7, 1024])
 
   def testGlobalPoolUnknownImageShape(self):
-    tf.compat.v1.reset_default_graph()
+    tf.reset_default_graph()
     batch_size = 1
     height, width = 250, 300
     num_classes = 1000
     input_np = np.random.uniform(0, 1, (batch_size, height, width, 3))
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(
+      inputs = tf.placeholder(
           tf.float32, shape=(batch_size, None, None, 3))
       logits, end_points = mobilenet_v1.mobilenet_v1(inputs, num_classes,
                                                      global_pool=True)
@@ -454,7 +452,7 @@ class MobilenetV1Test(tf.test.TestCase):
                            [batch_size, num_classes])
       pre_pool = end_points['Conv2d_13_pointwise']
       feed_dict = {inputs: input_np}
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       pre_pool_out = sess.run(pre_pool, feed_dict=feed_dict)
       self.assertListEqual(list(pre_pool_out.shape), [batch_size, 8, 10, 1024])
 
@@ -463,7 +461,7 @@ class MobilenetV1Test(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
 
-    inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+    inputs = tf.placeholder(tf.float32, (None, height, width, 3))
     logits, _ = mobilenet_v1.mobilenet_v1(inputs, num_classes)
     self.assertTrue(logits.op.name.startswith('MobilenetV1/Logits'))
     self.assertListEqual(logits.get_shape().as_list(),
@@ -471,7 +469,7 @@ class MobilenetV1Test(tf.test.TestCase):
     images = tf.random.uniform((batch_size, height, width, 3))
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -486,7 +484,7 @@ class MobilenetV1Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -504,7 +502,7 @@ class MobilenetV1Test(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (eval_batch_size,))
 
@@ -516,7 +514,7 @@ class MobilenetV1Test(tf.test.TestCase):
                                           spatial_squeeze=False)
 
     with self.test_session() as sess:
-      tf.compat.v1.global_variables_initializer().run()
+      tf.global_variables_initializer().run()
       logits_out = sess.run(logits)
       self.assertListEqual(list(logits_out.shape), [1, 1, 1, num_classes])
 
diff --git a/research/slim/nets/mobilenet_v1_train.py b/research/slim/nets/mobilenet_v1_train.py
index 1035ea31..ddfaf855 100644
--- a/research/slim/nets/mobilenet_v1_train.py
+++ b/research/slim/nets/mobilenet_v1_train.py
@@ -18,17 +18,16 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from tensorflow.contrib import quantize as contrib_quantize
-from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from nets import mobilenet_v1
 from preprocessing import preprocessing_factory
 
-slim = contrib_slim
-
-flags = tf.compat.v1.app.flags
+flags = tf.app.flags
 
 flags.DEFINE_string('master', '', 'Session master')
 flags.DEFINE_integer('task', 0, 'Task')
@@ -104,10 +103,10 @@ def imagenet_input(is_training):
 
   image = image_preprocessing_fn(image, FLAGS.image_size, FLAGS.image_size)
 
-  images, labels = tf.compat.v1.train.batch([image, label],
-                                            batch_size=FLAGS.batch_size,
-                                            num_threads=4,
-                                            capacity=5 * FLAGS.batch_size)
+  images, labels = tf.train.batch([image, label],
+                                  batch_size=FLAGS.batch_size,
+                                  num_threads=4,
+                                  capacity=5 * FLAGS.batch_size)
   labels = slim.one_hot_encoding(labels, FLAGS.num_classes)
   return images, labels
 
@@ -122,7 +121,7 @@ def build_model():
   """
   g = tf.Graph()
   with g.as_default(), tf.device(
-      tf.compat.v1.train.replica_device_setter(FLAGS.ps_tasks)):
+      tf.train.replica_device_setter(FLAGS.ps_tasks)):
     inputs, labels = imagenet_input(is_training=True)
     with slim.arg_scope(mobilenet_v1.mobilenet_v1_arg_scope(is_training=True)):
       logits, _ = mobilenet_v1.mobilenet_v1(
@@ -131,7 +130,7 @@ def build_model():
           depth_multiplier=FLAGS.depth_multiplier,
           num_classes=FLAGS.num_classes)
 
-    tf.compat.v1.losses.softmax_cross_entropy(labels, logits)
+    tf.losses.softmax_cross_entropy(labels, logits)
 
     # Call rewriter to produce graph with fake quant ops and folded batch norms
     # quant_delay delays start of quantization till quant_delay steps, allowing
@@ -139,19 +138,19 @@ def build_model():
     if FLAGS.quantize:
       contrib_quantize.create_training_graph(quant_delay=get_quant_delay())
 
-    total_loss = tf.compat.v1.losses.get_total_loss(name='total_loss')
+    total_loss = tf.losses.get_total_loss(name='total_loss')
     # Configure the learning rate using an exponential decay.
     num_epochs_per_decay = 2.5
     imagenet_size = 1271167
     decay_steps = int(imagenet_size / FLAGS.batch_size * num_epochs_per_decay)
 
-    learning_rate = tf.compat.v1.train.exponential_decay(
+    learning_rate = tf.train.exponential_decay(
         get_learning_rate(),
-        tf.compat.v1.train.get_or_create_global_step(),
+        tf.train.get_or_create_global_step(),
         decay_steps,
         _LEARNING_RATE_DECAY_FACTOR,
         staircase=True)
-    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate)
+    opt = tf.train.GradientDescentOptimizer(learning_rate)
 
     train_tensor = slim.learning.create_train_op(
         total_loss,
@@ -166,8 +165,8 @@ def get_checkpoint_init_fn():
   """Returns the checkpoint init_fn if the checkpoint is provided."""
   if FLAGS.fine_tune_checkpoint:
     variables_to_restore = slim.get_variables_to_restore()
-    global_step_reset = tf.compat.v1.assign(
-        tf.compat.v1.train.get_or_create_global_step(), 0)
+    global_step_reset = tf.assign(
+        tf.train.get_or_create_global_step(), 0)
     # When restoring from a floating point model, the min/max values for
     # quantized weights and activations are not present.
     # We instruct slim to ignore variables that are missing during restoration
@@ -203,7 +202,7 @@ def train_model():
         save_summaries_secs=FLAGS.save_summaries_secs,
         save_interval_secs=FLAGS.save_interval_secs,
         init_fn=get_checkpoint_init_fn(),
-        global_step=tf.compat.v1.train.get_global_step())
+        global_step=tf.train.get_global_step())
 
 
 def main(unused_arg):
@@ -211,4 +210,4 @@ def main(unused_arg):
 
 
 if __name__ == '__main__':
-  tf.compat.v1.app.run(main)
+  tf.app.run(main)
diff --git a/research/slim/nets/nasnet/nasnet.py b/research/slim/nets/nasnet/nasnet.py
index 664fa302..ecee8d5a 100644
--- a/research/slim/nets/nasnet/nasnet.py
+++ b/research/slim/nets/nasnet/nasnet.py
@@ -21,16 +21,13 @@ from __future__ import division
 from __future__ import print_function
 
 import copy
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
-from tensorflow.contrib import slim as contrib_slim
-from tensorflow.contrib import training as contrib_training
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
+from tensorflow.contrib import training as contrib_training
 from nets.nasnet import nasnet_utils
 
-arg_scope = contrib_framework.arg_scope
-slim = contrib_slim
+arg_scope = slim.arg_scope
 
 
 # Notes for training NASNet Cifar Model
@@ -142,9 +139,8 @@ def nasnet_cifar_arg_scope(weight_decay=5e-4,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
-  weights_initializer = contrib_layers.variance_scaling_initializer(
-      mode='FAN_OUT')
+  weights_regularizer = slim.l2_regularizer(weight_decay)
+  weights_initializer = slim.variance_scaling_initializer(mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
                  weights_initializer=weights_initializer):
@@ -178,9 +174,8 @@ def nasnet_mobile_arg_scope(weight_decay=4e-5,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
-  weights_initializer = contrib_layers.variance_scaling_initializer(
-      mode='FAN_OUT')
+  weights_regularizer = slim.l2_regularizer(weight_decay)
+  weights_initializer = slim.variance_scaling_initializer(mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
                  weights_initializer=weights_initializer):
@@ -214,9 +209,8 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
       'scale': True,
       'fused': True,
   }
-  weights_regularizer = contrib_layers.l2_regularizer(weight_decay)
-  weights_initializer = contrib_layers.variance_scaling_initializer(
-      mode='FAN_OUT')
+  weights_regularizer = slim.l2_regularizer(weight_decay)
+  weights_initializer = slim.variance_scaling_initializer(mode='FAN_OUT')
   with arg_scope([slim.fully_connected, slim.conv2d, slim.separable_conv2d],
                  weights_regularizer=weights_regularizer,
                  weights_initializer=weights_initializer):
@@ -231,9 +225,9 @@ def nasnet_large_arg_scope(weight_decay=5e-5,
 def _build_aux_head(net, end_points, num_classes, hparams, scope):
   """Auxiliary head used for all models across all datasets."""
   activation_fn = tf.nn.relu6 if hparams.use_bounded_activation else tf.nn.relu
-  with tf.compat.v1.variable_scope(scope):
+  with tf.variable_scope(scope):
     aux_logits = tf.identity(net)
-    with tf.compat.v1.variable_scope('aux_logits'):
+    with tf.variable_scope('aux_logits'):
       aux_logits = slim.avg_pool2d(
           aux_logits, [5, 5], stride=3, padding='VALID')
       aux_logits = slim.conv2d(aux_logits, 128, [1, 1], scope='proj')
@@ -248,7 +242,7 @@ def _build_aux_head(net, end_points, num_classes, hparams, scope):
       aux_logits = slim.conv2d(aux_logits, 768, shape, padding='VALID')
       aux_logits = slim.batch_norm(aux_logits, scope='aux_bn1')
       aux_logits = activation_fn(aux_logits)
-      aux_logits = contrib_layers.flatten(aux_logits)
+      aux_logits = slim.flatten(aux_logits)
       aux_logits = slim.fully_connected(aux_logits, num_classes)
       end_points['AuxLogits'] = aux_logits
 
@@ -302,7 +296,7 @@ def build_nasnet_cifar(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.compat.v1.logging.info(
+    tf.logging.info(
         'A GPU is available on the machine, consider using NCHW '
         'data format for increased speed on GPU.')
 
@@ -355,7 +349,7 @@ def build_nasnet_mobile(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.compat.v1.logging.info(
+    tf.logging.info(
         'A GPU is available on the machine, consider using NCHW '
         'data format for increased speed on GPU.')
 
@@ -411,7 +405,7 @@ def build_nasnet_large(images, num_classes,
   _update_hparams(hparams, is_training)
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.compat.v1.logging.info(
+    tf.logging.info(
         'A GPU is available on the machine, consider using NCHW '
         'data format for increased speed on GPU.')
 
@@ -537,7 +531,7 @@ def _build_nasnet_base(images,
     cell_outputs.append(net)
 
   # Final softmax layer
-  with tf.compat.v1.variable_scope('final_layer'):
+  with tf.variable_scope('final_layer'):
     net = activation_fn(net)
     net = nasnet_utils.global_avg_pool(net)
     if add_and_check_endpoint('global_pool', net) or not num_classes:
diff --git a/research/slim/nets/nasnet/nasnet_test.py b/research/slim/nets/nasnet/nasnet_test.py
index deb347d3..a98f6d10 100644
--- a/research/slim/nets/nasnet/nasnet_test.py
+++ b/research/slim/nets/nasnet/nasnet_test.py
@@ -17,13 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets.nasnet import nasnet
 
-slim = contrib_slim
-
 
 class NASNetTest(tf.test.TestCase):
 
@@ -32,7 +30,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       logits, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -49,7 +47,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       logits, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -66,7 +64,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       logits, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -83,7 +81,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = None
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       net, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -96,7 +94,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       net, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -109,7 +107,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = None
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       net, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -122,7 +120,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       _, end_points = nasnet.build_nasnet_cifar(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 32, 32, 96],
@@ -153,7 +151,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -164,9 +162,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     for use_aux_head in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
-      tf.compat.v1.train.create_global_step()
+      tf.train.create_global_step()
       config = nasnet.cifar_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
@@ -179,7 +177,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
       _, end_points = nasnet.build_nasnet_mobile(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 28, 28, 88],
@@ -204,7 +202,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -215,9 +213,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
-      tf.compat.v1.train.create_global_step()
+      tf.train.create_global_step()
       config = nasnet.mobile_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
@@ -230,7 +228,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
       _, end_points = nasnet.build_nasnet_large(inputs, num_classes)
     endpoints_shapes = {'Stem': [batch_size, 42, 42, 336],
@@ -261,7 +259,7 @@ class NASNetTest(tf.test.TestCase):
                         'Predictions': [batch_size, num_classes]}
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -272,9 +270,9 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
-      tf.compat.v1.train.create_global_step()
+      tf.train.create_global_step()
       config = nasnet.large_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
@@ -287,19 +285,19 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     # Force all Variables to reside on the device.
-    with tf.compat.v1.variable_scope('on_cpu'), tf.device('/cpu:0'):
+    with tf.variable_scope('on_cpu'), tf.device('/cpu:0'):
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         nasnet.build_nasnet_mobile(inputs, num_classes)
-    with tf.compat.v1.variable_scope('on_gpu'), tf.device('/gpu:0'):
+    with tf.variable_scope('on_gpu'), tf.device('/gpu:0'):
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         nasnet.build_nasnet_mobile(inputs, num_classes)
-    for v in tf.compat.v1.get_collection(
-        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
+    for v in tf.get_collection(
+        tf.GraphKeys.GLOBAL_VARIABLES, scope='on_cpu'):
       self.assertDeviceEqual(v.device, '/cpu:0')
-    for v in tf.compat.v1.get_collection(
-        tf.compat.v1.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
+    for v in tf.get_collection(
+        tf.GraphKeys.GLOBAL_VARIABLES, scope='on_gpu'):
       self.assertDeviceEqual(v.device, '/gpu:0')
 
   def testUnknownBatchSizeMobileModel(self):
@@ -307,13 +305,13 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     with self.test_session() as sess:
-      inputs = tf.compat.v1.placeholder(tf.float32, (None, height, width, 3))
+      inputs = tf.placeholder(tf.float32, (None, height, width, 3))
       with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
         logits, _ = nasnet.build_nasnet_mobile(inputs, num_classes)
       self.assertListEqual(logits.get_shape().as_list(),
                            [None, num_classes])
       images = tf.random.uniform((batch_size, height, width, 3))
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEquals(output.shape, (batch_size, num_classes))
 
@@ -328,7 +326,7 @@ class NASNetTest(tf.test.TestCase):
                                                num_classes,
                                                is_training=False)
       predictions = tf.argmax(input=logits, axis=1)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
@@ -337,7 +335,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     config = nasnet.cifar_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
@@ -351,7 +349,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     config = nasnet.mobile_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_mobile_arg_scope()):
@@ -365,7 +363,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     config = nasnet.large_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(nasnet.nasnet_large_arg_scope()):
@@ -379,7 +377,7 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    global_step = tf.compat.v1.train.create_global_step()
+    global_step = tf.train.create_global_step()
     with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
       logits, end_points = nasnet.build_nasnet_cifar(inputs,
                                                      num_classes,
@@ -398,14 +396,14 @@ class NASNetTest(tf.test.TestCase):
     height, width = 32, 32
     num_classes = 10
     for use_bounded_activation in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
       config = nasnet.cifar_config()
       config.set_hparam('use_bounded_activation', use_bounded_activation)
       with slim.arg_scope(nasnet.nasnet_cifar_arg_scope()):
         _, _ = nasnet.build_nasnet_cifar(
             inputs, num_classes, config=config)
-      for node in tf.compat.v1.get_default_graph().as_graph_def().node:
+      for node in tf.get_default_graph().as_graph_def().node:
         if node.op.startswith('Relu'):
           self.assertEqual(node.op == 'Relu6', use_bounded_activation)
 
diff --git a/research/slim/nets/nasnet/nasnet_utils.py b/research/slim/nets/nasnet/nasnet_utils.py
index 1d68854f..3fe1ceee 100644
--- a/research/slim/nets/nasnet/nasnet_utils.py
+++ b/research/slim/nets/nasnet/nasnet_utils.py
@@ -31,13 +31,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
-arg_scope = contrib_framework.arg_scope
-slim = contrib_slim
+import tf_slim as slim
 
+arg_scope = slim.arg_scope
 DATA_FORMAT_NCHW = 'NCHW'
 DATA_FORMAT_NHWC = 'NHWC'
 INVALID = 'null'
@@ -56,14 +54,14 @@ def calc_reduction_layers(num_cells, num_reduction_layers):
   return reduction_layers
 
 
-@contrib_framework.add_arg_scope
+@slim.add_arg_scope
 def get_channel_index(data_format=INVALID):
   assert data_format != INVALID
   axis = 3 if data_format == 'NHWC' else 1
   return axis
 
 
-@contrib_framework.add_arg_scope
+@slim.add_arg_scope
 def get_channel_dim(shape, data_format=INVALID):
   assert data_format != INVALID
   assert len(shape) == 4
@@ -75,7 +73,7 @@ def get_channel_dim(shape, data_format=INVALID):
     raise ValueError('Not a valid data_format', data_format)
 
 
-@contrib_framework.add_arg_scope
+@slim.add_arg_scope
 def global_avg_pool(x, data_format=INVALID):
   """Average pool away the height and width spatial dimensions of x."""
   assert data_format != INVALID
@@ -87,7 +85,7 @@ def global_avg_pool(x, data_format=INVALID):
     return tf.reduce_mean(input_tensor=x, axis=[2, 3])
 
 
-@contrib_framework.add_arg_scope
+@slim.add_arg_scope
 def factorized_reduction(net, output_filters, stride, data_format=INVALID):
   """Reduces the shape of net without information loss due to striding."""
   assert data_format != INVALID
@@ -101,8 +99,8 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
     stride_spec = [1, 1, stride, stride]
 
   # Skip path 1
-  path1 = tf.compat.v2.nn.avg_pool2d(
-      input=net,
+  path1 = tf.nn.avg_pool2d(
+      net,
       ksize=[1, 1, 1, 1],
       strides=stride_spec,
       padding='VALID',
@@ -120,9 +118,8 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
     pad_arr = [[0, 0], [0, 0], [0, 1], [0, 1]]
     path2 = tf.pad(tensor=net, paddings=pad_arr)[:, :, 1:, 1:]
     concat_axis = 1
-
-  path2 = tf.compat.v2.nn.avg_pool2d(
-      input=path2,
+  path2 = tf.nn.avg_pool2d(
+      path2,
       ksize=[1, 1, 1, 1],
       strides=stride_spec,
       padding='VALID',
@@ -138,7 +135,7 @@ def factorized_reduction(net, output_filters, stride, data_format=INVALID):
   return final_path
 
 
-@contrib_framework.add_arg_scope
+@slim.add_arg_scope
 def drop_path(net, keep_prob, is_training=True):
   """Drops out a whole example hiddenstate with the specified probability."""
   if is_training:
@@ -324,10 +321,10 @@ class NasNetABaseCell(object):
     self._filter_size = int(self._num_conv_filters * filter_scaling)
 
     i = 0
-    with tf.compat.v1.variable_scope(scope):
+    with tf.variable_scope(scope):
       net = self._cell_base(net, prev_layer)
       for iteration in range(5):
-        with tf.compat.v1.variable_scope('comb_iter_{}'.format(iteration)):
+        with tf.variable_scope('comb_iter_{}'.format(iteration)):
           left_hiddenstate_idx, right_hiddenstate_idx = (
               self._hiddenstate_indices[i],
               self._hiddenstate_indices[i + 1])
@@ -340,17 +337,17 @@ class NasNetABaseCell(object):
           operation_right = self._operations[i+1]
           i += 2
           # Apply conv operations
-          with tf.compat.v1.variable_scope('left'):
+          with tf.variable_scope('left'):
             h1 = self._apply_conv_operation(h1, operation_left,
                                             stride, original_input_left,
                                             current_step)
-          with tf.compat.v1.variable_scope('right'):
+          with tf.variable_scope('right'):
             h2 = self._apply_conv_operation(h2, operation_right,
                                             stride, original_input_right,
                                             current_step)
 
           # Combine hidden states using 'add'.
-          with tf.compat.v1.variable_scope('combine'):
+          with tf.variable_scope('combine'):
             h = h1 + h2
             if self._use_bounded_activation:
               h = tf.nn.relu6(h)
@@ -358,7 +355,7 @@ class NasNetABaseCell(object):
           # Add hiddenstate to the list of hiddenstates we can choose from
           net.append(h)
 
-      with tf.compat.v1.variable_scope('cell_output'):
+      with tf.variable_scope('cell_output'):
         net = self._combine_unused_states(net)
 
       return net
@@ -419,7 +416,7 @@ class NasNetABaseCell(object):
       should_reduce = should_reduce and not used_h
       if should_reduce:
         stride = 2 if final_height != curr_height else 1
-        with tf.compat.v1.variable_scope('reduction_{}'.format(idx)):
+        with tf.variable_scope('reduction_{}'.format(idx)):
           net[idx] = factorized_reduction(
               net[idx], final_num_filters, stride)
 
@@ -431,7 +428,7 @@ class NasNetABaseCell(object):
     net = tf.concat(values=states_to_combine, axis=concat_axis)
     return net
 
-  @contrib_framework.add_arg_scope  # No public API. For internal use only.
+  @slim.add_arg_scope  # No public API. For internal use only.
   def _apply_drop_path(self, net, current_step=None,
                        use_summaries=False, drop_connect_version='v3'):
     """Apply drop_path regularization.
@@ -460,24 +457,23 @@ class NasNetABaseCell(object):
         layer_ratio = (self._cell_num + 1)/float(num_cells)
         if use_summaries:
           with tf.device('/cpu:0'):
-            tf.compat.v1.summary.scalar('layer_ratio', layer_ratio)
+            tf.summary.scalar('layer_ratio', layer_ratio)
         drop_path_keep_prob = 1 - layer_ratio * (1 - drop_path_keep_prob)
       if drop_connect_version in ['v1', 'v3']:
         # Decrease the keep probability over time
         if current_step is None:
-          current_step = tf.compat.v1.train.get_or_create_global_step()
+          current_step = tf.train.get_or_create_global_step()
         current_step = tf.cast(current_step, tf.float32)
         drop_path_burn_in_steps = self._total_training_steps
         current_ratio = current_step / drop_path_burn_in_steps
         current_ratio = tf.minimum(1.0, current_ratio)
         if use_summaries:
           with tf.device('/cpu:0'):
-            tf.compat.v1.summary.scalar('current_ratio', current_ratio)
+            tf.summary.scalar('current_ratio', current_ratio)
         drop_path_keep_prob = (1 - current_ratio * (1 - drop_path_keep_prob))
       if use_summaries:
         with tf.device('/cpu:0'):
-          tf.compat.v1.summary.scalar('drop_path_keep_prob',
-                                      drop_path_keep_prob)
+          tf.summary.scalar('drop_path_keep_prob', drop_path_keep_prob)
       net = drop_path(net, drop_path_keep_prob)
     return net
 
diff --git a/research/slim/nets/nasnet/nasnet_utils_test.py b/research/slim/nets/nasnet/nasnet_utils_test.py
index d165418d..2269e27d 100644
--- a/research/slim/nets/nasnet/nasnet_utils_test.py
+++ b/research/slim/nets/nasnet/nasnet_utils_test.py
@@ -17,8 +17,7 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from nets.nasnet import nasnet_utils
 
@@ -51,12 +50,20 @@ class NasnetUtilsTest(tf.test.TestCase):
 
   def testGlobalAvgPool(self):
     data_formats = ['NHWC', 'NCHW']
-    inputs = tf.compat.v1.placeholder(tf.float32, (5, 10, 20, 10))
+    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))
     for data_format in data_formats:
       output = nasnet_utils.global_avg_pool(
           inputs, data_format)
       self.assertEqual(output.shape, [5, 10])
 
+  def test_factorized_reduction(self):
+    data_format = 'NHWC'
+    output_shape = (5, 10, 20, 16)
+    inputs = tf.placeholder(tf.float32, (5, 10, 20, 10))
+    output = nasnet_utils.factorized_reduction(
+        inputs, 16, stride=1, data_format=data_format)
+    self.assertSequenceEqual(output_shape, output.shape.as_list())
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/slim/nets/nasnet/pnasnet.py b/research/slim/nets/nasnet/pnasnet.py
index 5e612e33..f77946fd 100644
--- a/research/slim/nets/nasnet/pnasnet.py
+++ b/research/slim/nets/nasnet/pnasnet.py
@@ -22,16 +22,14 @@ from __future__ import division
 from __future__ import print_function
 
 import copy
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from tensorflow.contrib import training as contrib_training
 
 from nets.nasnet import nasnet
 from nets.nasnet import nasnet_utils
 
-arg_scope = contrib_framework.arg_scope
-slim = contrib_slim
+arg_scope = slim.arg_scope
 
 
 def large_imagenet_config():
@@ -147,7 +145,7 @@ def _build_pnasnet_base(images,
       # pylint: enable=protected-access
 
   # Final softmax layer
-  with tf.compat.v1.variable_scope('final_layer'):
+  with tf.variable_scope('final_layer'):
     net = activation_fn(net)
     net = nasnet_utils.global_avg_pool(net)
     if add_and_check_endpoint('global_pool', net) or not num_classes:
@@ -176,7 +174,7 @@ def build_pnasnet_large(images,
   # pylint: enable=protected-access
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.compat.v1.logging.info(
+    tf.logging.info(
         'A GPU is available on the machine, consider using NCHW '
         'data format for increased speed on GPU.')
 
@@ -225,7 +223,7 @@ def build_pnasnet_mobile(images,
   # pylint: enable=protected-access
 
   if tf.test.is_gpu_available() and hparams.data_format == 'NHWC':
-    tf.compat.v1.logging.info(
+    tf.logging.info(
         'A GPU is available on the machine, consider using NCHW '
         'data format for increased speed on GPU.')
 
diff --git a/research/slim/nets/nasnet/pnasnet_test.py b/research/slim/nets/nasnet/pnasnet_test.py
index 8e1df4db..622b344b 100644
--- a/research/slim/nets/nasnet/pnasnet_test.py
+++ b/research/slim/nets/nasnet/pnasnet_test.py
@@ -17,13 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets.nasnet import pnasnet
 
-slim = contrib_slim
-
 
 class PNASNetTest(tf.test.TestCase):
 
@@ -32,7 +30,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       logits, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -49,7 +47,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       logits, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
     auxlogits = end_points['AuxLogits']
@@ -64,20 +62,20 @@ class PNASNetTest(tf.test.TestCase):
   def testBuildNonExistingLayerLargeModel(self):
     """Tests that the model is built correctly without unnecessary layers."""
     inputs = tf.random.uniform((5, 331, 331, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       pnasnet.build_pnasnet_large(inputs, 1000)
-    vars_names = [x.op.name for x in tf.compat.v1.trainable_variables()]
+    vars_names = [x.op.name for x in tf.trainable_variables()]
     self.assertIn('cell_stem_0/1x1/weights', vars_names)
     self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names)
 
   def testBuildNonExistingLayerMobileModel(self):
     """Tests that the model is built correctly without unnecessary layers."""
     inputs = tf.random.uniform((5, 224, 224, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       pnasnet.build_pnasnet_mobile(inputs, 1000)
-    vars_names = [x.op.name for x in tf.compat.v1.trainable_variables()]
+    vars_names = [x.op.name for x in tf.trainable_variables()]
     self.assertIn('cell_stem_0/1x1/weights', vars_names)
     self.assertNotIn('cell_stem_1/comb_iter_0/right/1x1/weights', vars_names)
 
@@ -86,7 +84,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = None
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       net, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -99,7 +97,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = None
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       net, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
     self.assertFalse('AuxLogits' in end_points)
@@ -112,7 +110,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
       _, end_points = pnasnet.build_pnasnet_large(inputs, num_classes)
 
@@ -138,7 +136,7 @@ class PNASNetTest(tf.test.TestCase):
     self.assertEqual(len(end_points), 17)
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertIn(endpoint_name, end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -149,7 +147,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
       _, end_points = pnasnet.build_pnasnet_mobile(inputs, num_classes)
 
@@ -173,7 +171,7 @@ class PNASNetTest(tf.test.TestCase):
     self.assertEqual(len(end_points), 14)
     self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
     for endpoint_name in endpoints_shapes:
-      tf.compat.v1.logging.info('Endpoint name: {}'.format(endpoint_name))
+      tf.logging.info('Endpoint name: {}'.format(endpoint_name))
       expected_shape = endpoints_shapes[endpoint_name]
       self.assertIn(endpoint_name, end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
@@ -184,9 +182,9 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
-      tf.compat.v1.train.create_global_step()
+      tf.train.create_global_step()
       config = pnasnet.large_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
@@ -199,9 +197,9 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_aux_head in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
-      tf.compat.v1.train.create_global_step()
+      tf.train.create_global_step()
       config = pnasnet.mobile_imagenet_config()
       config.set_hparam('use_aux_head', int(use_aux_head))
       with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
@@ -214,7 +212,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 331, 331
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     config = pnasnet.large_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(pnasnet.pnasnet_large_arg_scope()):
@@ -228,7 +226,7 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     inputs = tf.random.uniform((batch_size, height, width, 3))
-    tf.compat.v1.train.create_global_step()
+    tf.train.create_global_step()
     config = pnasnet.mobile_imagenet_config()
     config.set_hparam('data_format', 'NCHW')
     with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
@@ -242,14 +240,14 @@ class PNASNetTest(tf.test.TestCase):
     height, width = 224, 224
     num_classes = 1000
     for use_bounded_activation in (True, False):
-      tf.compat.v1.reset_default_graph()
+      tf.reset_default_graph()
       inputs = tf.random.uniform((batch_size, height, width, 3))
       config = pnasnet.mobile_imagenet_config()
       config.set_hparam('use_bounded_activation', use_bounded_activation)
       with slim.arg_scope(pnasnet.pnasnet_mobile_arg_scope()):
         _, _ = pnasnet.build_pnasnet_mobile(
             inputs, num_classes, config=config)
-      for node in tf.compat.v1.get_default_graph().as_graph_def().node:
+      for node in tf.get_default_graph().as_graph_def().node:
         if node.op.startswith('Relu'):
           self.assertEqual(node.op == 'Relu6', use_bounded_activation)
 
diff --git a/research/slim/nets/nets_factory.py b/research/slim/nets/nets_factory.py
index 1c34f80e..52cd0229 100644
--- a/research/slim/nets/nets_factory.py
+++ b/research/slim/nets/nets_factory.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 import functools
-from tensorflow.contrib import slim as contrib_slim
+import tf_slim as slim
 
 from nets import alexnet
 from nets import cifarnet
@@ -37,8 +37,6 @@ from nets.nasnet import nasnet
 from nets.nasnet import pnasnet
 
 
-slim = contrib_slim
-
 networks_map = {
     'alexnet_v2': alexnet.alexnet_v2,
     'cifarnet': cifarnet.cifarnet,
diff --git a/research/slim/nets/nets_factory_test.py b/research/slim/nets/nets_factory_test.py
index 3e16fc12..176fbbaf 100644
--- a/research/slim/nets/nets_factory_test.py
+++ b/research/slim/nets/nets_factory_test.py
@@ -20,7 +20,7 @@ from __future__ import division
 from __future__ import print_function
 
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from nets import nets_factory
 
diff --git a/research/slim/nets/overfeat.py b/research/slim/nets/overfeat.py
index 8cd70960..24f940ca 100644
--- a/research/slim/nets/overfeat.py
+++ b/research/slim/nets/overfeat.py
@@ -31,13 +31,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 
 
@@ -45,7 +43,7 @@ def overfeat_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.compat.v1.zeros_initializer()):
+                      biases_initializer=tf.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME'):
       with slim.arg_scope([slim.max_pool2d], padding='VALID') as arg_sc:
         return arg_sc
@@ -91,7 +89,7 @@ def overfeat(inputs,
       None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.compat.v1.variable_scope(scope, 'overfeat', [inputs]) as sc:
+  with tf.variable_scope(scope, 'overfeat', [inputs]) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d
     with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d],
@@ -110,7 +108,7 @@ def overfeat(inputs,
       with slim.arg_scope(
           [slim.conv2d],
           weights_initializer=trunc_normal(0.005),
-          biases_initializer=tf.compat.v1.constant_initializer(0.1)):
+          biases_initializer=tf.constant_initializer(0.1)):
         net = slim.conv2d(net, 3072, [6, 6], padding='VALID', scope='fc6')
         net = slim.dropout(net, dropout_keep_prob, is_training=is_training,
                            scope='dropout6')
@@ -130,7 +128,7 @@ def overfeat(inputs,
               num_classes, [1, 1],
               activation_fn=None,
               normalizer_fn=None,
-              biases_initializer=tf.compat.v1.zeros_initializer(),
+              biases_initializer=tf.zeros_initializer(),
               scope='fc8')
           if spatial_squeeze:
             net = tf.squeeze(net, [1, 2], name='fc8/squeezed')
diff --git a/research/slim/nets/overfeat_test.py b/research/slim/nets/overfeat_test.py
index 894df8e2..6d8880d5 100644
--- a/research/slim/nets/overfeat_test.py
+++ b/research/slim/nets/overfeat_test.py
@@ -17,13 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import overfeat
 
-slim = contrib_slim
-
 
 class OverFeatTest(tf.test.TestCase):
 
@@ -154,7 +152,7 @@ class OverFeatTest(tf.test.TestCase):
       logits, _ = overfeat.overfeat(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.compat.v1.get_variable_scope().reuse_variables()
+      tf.get_variable_scope().reuse_variables()
       eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = overfeat.overfeat(eval_inputs, is_training=False,
@@ -171,7 +169,7 @@ class OverFeatTest(tf.test.TestCase):
     with self.test_session() as sess:
       inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = overfeat.overfeat(inputs)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/nets/pix2pix.py b/research/slim/nets/pix2pix.py
index b393d653..9e0e5708 100644
--- a/research/slim/nets/pix2pix.py
+++ b/research/slim/nets/pix2pix.py
@@ -32,11 +32,8 @@ from __future__ import print_function
 import collections
 import functools
 
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
-
-layers = contrib_layers
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def pix2pix_arg_scope():
@@ -54,12 +51,11 @@ def pix2pix_arg_scope():
       'epsilon': 0.00001,
   }
 
-  with contrib_framework.arg_scope(
-      [layers.conv2d, layers.conv2d_transpose],
-      normalizer_fn=layers.instance_norm,
+  with slim.arg_scope(
+      [slim.conv2d, slim.conv2d_transpose],
+      normalizer_fn=slim.instance_norm,
       normalizer_params=instance_norm_params,
-      weights_initializer=tf.compat.v1.random_normal_initializer(0,
-                                                                 0.02)) as sc:
+      weights_initializer=tf.random_normal_initializer(0, 0.02)) as sc:
     return sc
 
 
@@ -89,9 +85,9 @@ def upsample(net, num_outputs, kernel_size, method='nn_upsample_conv'):
     net = tf.image.resize(
         net, [kernel_size[0] * height, kernel_size[1] * width],
         method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
-    net = layers.conv2d(net, num_outputs, [4, 4], activation_fn=None)
+    net = slim.conv2d(net, num_outputs, [4, 4], activation_fn=None)
   elif method == 'conv2d_transpose':
-    net = layers.conv2d_transpose(
+    net = slim.conv2d_transpose(
         net, num_outputs, [4, 4], stride=kernel_size, activation_fn=None)
   else:
     raise ValueError('Unknown method: [%s]' % method)
@@ -168,23 +164,23 @@ def pix2pix_generator(net,
   ###########
   # Encoder #
   ###########
-  with tf.compat.v1.variable_scope('encoder'):
-    with contrib_framework.arg_scope([layers.conv2d],
-                                     kernel_size=[4, 4],
-                                     stride=2,
-                                     activation_fn=tf.nn.leaky_relu):
+  with tf.variable_scope('encoder'):
+    with slim.arg_scope([slim.conv2d],
+                        kernel_size=[4, 4],
+                        stride=2,
+                        activation_fn=tf.nn.leaky_relu):
 
       for block_id, block in enumerate(blocks):
         # No normalizer for the first encoder layers as per 'Image-to-Image',
         # Section 5.1.1
         if block_id == 0:
           # First layer doesn't use normalizer_fn
-          net = layers.conv2d(net, block.num_filters, normalizer_fn=None)
+          net = slim.conv2d(net, block.num_filters, normalizer_fn=None)
         elif block_id < len(blocks) - 1:
-          net = layers.conv2d(net, block.num_filters)
+          net = slim.conv2d(net, block.num_filters)
         else:
           # Last layer doesn't use activation_fn nor normalizer_fn
-          net = layers.conv2d(
+          net = slim.conv2d(
               net, block.num_filters, activation_fn=None, normalizer_fn=None)
 
         encoder_activations.append(net)
@@ -196,10 +192,10 @@ def pix2pix_generator(net,
   reversed_blocks = list(blocks)
   reversed_blocks.reverse()
 
-  with tf.compat.v1.variable_scope('decoder'):
+  with tf.variable_scope('decoder'):
     # Dropout is used at both train and test time as per 'Image-to-Image',
     # Section 2.1 (last paragraph).
-    with contrib_framework.arg_scope([layers.dropout], is_training=True):
+    with slim.arg_scope([slim.dropout], is_training=True):
 
       for block_id, block in enumerate(reversed_blocks):
         if block_id > 0:
@@ -209,13 +205,13 @@ def pix2pix_generator(net,
         net = tf.nn.relu(net)
         net = upsample_fn(net, block.num_filters, [2, 2])
         if block.decoder_keep_prob > 0:
-          net = layers.dropout(net, keep_prob=block.decoder_keep_prob)
+          net = slim.dropout(net, keep_prob=block.decoder_keep_prob)
         end_points['decoder%d' % block_id] = net
 
-  with tf.compat.v1.variable_scope('output'):
+  with tf.variable_scope('output'):
     # Explicitly set the normalizer_fn to None to override any default value
     # that may come from an arg_scope, such as pix2pix_arg_scope.
-    logits = layers.conv2d(
+    logits = slim.conv2d(
         net, num_outputs, [4, 4], activation_fn=None, normalizer_fn=None)
     logits = tf.reshape(logits, input_size)
 
@@ -236,7 +232,7 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
       list determines the number of layers in the discriminator.
     padding: Amount of reflection padding applied before each convolution.
     pad_mode: mode for tf.pad, one of "CONSTANT", "REFLECT", or "SYMMETRIC".
-    activation_fn: activation fn for layers.conv2d.
+    activation_fn: activation fn for slim.conv2d.
     is_training: Whether or not the model is training or testing.
 
   Returns:
@@ -251,7 +247,7 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
 
   def padded(net, scope):
     if padding:
-      with tf.compat.v1.variable_scope(scope):
+      with tf.variable_scope(scope):
         spatial_pad = tf.constant(
             [[0, 0], [padding, padding], [padding, padding], [0, 0]],
             dtype=tf.int32)
@@ -259,25 +255,25 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
     else:
       return net
 
-  with contrib_framework.arg_scope([layers.conv2d],
-                                   kernel_size=[4, 4],
-                                   stride=2,
-                                   padding='valid',
-                                   activation_fn=activation_fn):
+  with slim.arg_scope([slim.conv2d],
+                      kernel_size=[4, 4],
+                      stride=2,
+                      padding='valid',
+                      activation_fn=activation_fn):
 
     # No normalization on the input layer.
-    net = layers.conv2d(
+    net = slim.conv2d(
         padded(net, 'conv0'), num_filters[0], normalizer_fn=None, scope='conv0')
 
     end_points['conv0'] = net
 
     for i in range(1, num_layers - 1):
-      net = layers.conv2d(
+      net = slim.conv2d(
           padded(net, 'conv%d' % i), num_filters[i], scope='conv%d' % i)
       end_points['conv%d' % i] = net
 
     # Stride 1 on the last layer.
-    net = layers.conv2d(
+    net = slim.conv2d(
         padded(net, 'conv%d' % (num_layers - 1)),
         num_filters[-1],
         stride=1,
@@ -285,7 +281,7 @@ def pix2pix_discriminator(net, num_filters, padding=2, pad_mode='REFLECT',
     end_points['conv%d' % (num_layers - 1)] = net
 
     # 1-dim logits, stride 1, no activation, no normalization.
-    logits = layers.conv2d(
+    logits = slim.conv2d(
         padded(net, 'conv%d' % num_layers),
         1,
         stride=1,
diff --git a/research/slim/nets/pix2pix_test.py b/research/slim/nets/pix2pix_test.py
index d1bfa1b1..d24bb4c4 100644
--- a/research/slim/nets/pix2pix_test.py
+++ b/research/slim/nets/pix2pix_test.py
@@ -18,8 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 from nets import pix2pix
 
 
@@ -36,13 +36,13 @@ class GeneratorTest(tf.test.TestCase):
     num_outputs = 4
 
     images = tf.ones((batch_size, height, width, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, _ = pix2pix.pix2pix_generator(
           images, num_outputs, blocks=self._reduced_default_blocks(),
           upsample_method='nn_upsample_conv')
 
     with self.test_session() as session:
-      session.run(tf.compat.v1.global_variables_initializer())
+      session.run(tf.global_variables_initializer())
       np_outputs = session.run(logits)
       self.assertListEqual([batch_size, height, width, num_outputs],
                            list(np_outputs.shape))
@@ -53,13 +53,13 @@ class GeneratorTest(tf.test.TestCase):
     num_outputs = 4
 
     images = tf.ones((batch_size, height, width, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, _ = pix2pix.pix2pix_generator(
           images, num_outputs, blocks=self._reduced_default_blocks(),
           upsample_method='conv2d_transpose')
 
     with self.test_session() as session:
-      session.run(tf.compat.v1.global_variables_initializer())
+      session.run(tf.global_variables_initializer())
       np_outputs = session.run(logits)
       self.assertListEqual([batch_size, height, width, num_outputs],
                            list(np_outputs.shape))
@@ -74,7 +74,7 @@ class GeneratorTest(tf.test.TestCase):
         pix2pix.Block(64, 0.5),
         pix2pix.Block(128, 0),
     ]
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       _, end_points = pix2pix.pix2pix_generator(
           images, num_outputs, blocks)
 
@@ -106,7 +106,7 @@ class DiscriminatorTest(tf.test.TestCase):
     output_size = self._layer_output_size(output_size, stride=1)
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, end_points = pix2pix.pix2pix_discriminator(
           images, num_filters=[64, 128, 256, 512])
     self.assertListEqual([batch_size, output_size, output_size, 1],
@@ -125,7 +125,7 @@ class DiscriminatorTest(tf.test.TestCase):
     output_size = self._layer_output_size(output_size, stride=1, pad=0)
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       logits, end_points = pix2pix.pix2pix_discriminator(
           images, num_filters=[64, 128, 256, 512], padding=0)
     self.assertListEqual([batch_size, output_size, output_size, 1],
@@ -138,7 +138,7 @@ class DiscriminatorTest(tf.test.TestCase):
     input_size = 256
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       with self.assertRaises(TypeError):
         pix2pix.pix2pix_discriminator(
             images, num_filters=[64, 128, 256, 512], padding=1.5)
@@ -148,7 +148,7 @@ class DiscriminatorTest(tf.test.TestCase):
     input_size = 256
 
     images = tf.ones((batch_size, input_size, input_size, 3))
-    with contrib_framework.arg_scope(pix2pix.pix2pix_arg_scope()):
+    with slim.arg_scope(pix2pix.pix2pix_arg_scope()):
       with self.assertRaises(ValueError):
         pix2pix.pix2pix_discriminator(
             images, num_filters=[64, 128, 256, 512], padding=-1)
diff --git a/research/slim/nets/post_training_quantization.py b/research/slim/nets/post_training_quantization.py
index b2d6e3f6..3176b9a3 100644
--- a/research/slim/nets/post_training_quantization.py
+++ b/research/slim/nets/post_training_quantization.py
@@ -21,7 +21,7 @@ from __future__ import print_function
 import functools
 from absl import app
 from absl import flags
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 import tensorflow_datasets as tfds
 from nets import nets_factory
 from preprocessing import preprocessing_factory
@@ -127,7 +127,7 @@ def _representative_dataset_gen():
   dataset = tfds.builder(FLAGS.dataset_name, data_dir=FLAGS.dataset_dir)
   dataset.download_and_prepare()
   data = dataset.as_dataset()[FLAGS.dataset_split]
-  iterator = tf.compat.v1.data.make_one_shot_iterator(data)
+  iterator = tf.data.make_one_shot_iterator(data)
   if FLAGS.use_model_specific_preprocessing:
     preprocess_fn = functools.partial(
         preprocessing_factory.get_preprocessing(name=FLAGS.model_name),
diff --git a/research/slim/nets/resnet_utils.py b/research/slim/nets/resnet_utils.py
index 3230e280..1431bc71 100644
--- a/research/slim/nets/resnet_utils.py
+++ b/research/slim/nets/resnet_utils.py
@@ -38,10 +38,8 @@ from __future__ import division
 from __future__ import print_function
 
 import collections
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 class Block(collections.namedtuple('Block', ['scope', 'unit_fn', 'args'])):
@@ -181,7 +179,7 @@ def stack_blocks_dense(net, blocks, output_stride=None,
   rate = 1
 
   for block in blocks:
-    with tf.compat.v1.variable_scope(block.scope, 'block', [net]) as sc:
+    with tf.variable_scope(block.scope, 'block', [net]) as sc:
       block_stride = 1
       for i, unit in enumerate(block.args):
         if store_non_strided_activations and i == len(block.args) - 1:
@@ -189,7 +187,7 @@ def stack_blocks_dense(net, blocks, output_stride=None,
           block_stride = unit.get('stride', 1)
           unit = dict(unit, stride=1)
 
-        with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
+        with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
           # If we have reached the target output_stride, then we need to employ
           # atrous convolution with stride=1 and multiply the atrous rate by the
           # current unit's stride for use in subsequent layers.
@@ -228,7 +226,7 @@ def resnet_arg_scope(
     batch_norm_scale=True,
     activation_fn=tf.nn.relu,
     use_batch_norm=True,
-    batch_norm_updates_collections=tf.compat.v1.GraphKeys.UPDATE_OPS):
+    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS):
   """Defines the default ResNet arg scope.
 
   TODO(gpapan): The batch-normalization related default values above are
diff --git a/research/slim/nets/resnet_v1.py b/research/slim/nets/resnet_v1.py
index 8451f466..1ca55611 100644
--- a/research/slim/nets/resnet_v1.py
+++ b/research/slim/nets/resnet_v1.py
@@ -34,7 +34,7 @@ units.
 
 Typical use:
 
-   from tensorflow.contrib.slim.nets import resnet_v1
+   from tf_slim.nets import resnet_v1
 
 ResNet-101 for image classification into 1000 classes:
 
@@ -56,14 +56,13 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import resnet_utils
 
 
 resnet_arg_scope = resnet_utils.resnet_arg_scope
-slim = contrib_slim
 
 
 class NoOpScope(object):
@@ -109,7 +108,7 @@ def bottleneck(inputs,
   Returns:
     The ResNet unit's output.
   """
-  with tf.compat.v1.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:
+  with tf.variable_scope(scope, 'bottleneck_v1', [inputs]) as sc:
     depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
     if depth == depth_in:
       shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')
@@ -219,7 +218,7 @@ def resnet_v1(inputs,
   Raises:
     ValueError: If the target output_stride is not valid.
   """
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
diff --git a/research/slim/nets/resnet_v1_test.py b/research/slim/nets/resnet_v1_test.py
index fb70a5a3..25fb9d7f 100644
--- a/research/slim/nets/resnet_v1_test.py
+++ b/research/slim/nets/resnet_v1_test.py
@@ -19,15 +19,13 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import resnet_utils
 from nets import resnet_v1
 
-slim = contrib_slim
-
-tf.compat.v1.disable_resource_variables()
+tf.disable_resource_variables()
 
 
 def create_test_input(batch_size, height, width, channels):
@@ -45,8 +43,7 @@ def create_test_input(batch_size, height, width, channels):
     constant `Tensor` with the mesh grid values along the spatial dimensions.
   """
   if None in [batch_size, height, width, channels]:
-    return tf.compat.v1.placeholder(tf.float32,
-                                    (batch_size, height, width, channels))
+    return tf.placeholder(tf.float32, (batch_size, height, width, channels))
   else:
     return tf.cast(
         np.tile(
@@ -83,9 +80,9 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.compat.v1.get_variable('Conv/weights', initializer=w)
-    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.compat.v1.get_variable_scope().reuse_variables()
+    tf.get_variable('Conv/weights', initializer=w)
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
     y1_expected = tf.cast([[14, 28, 43, 26], [28, 48, 66, 37], [43, 66, 84, 46],
@@ -105,7 +102,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -121,9 +118,9 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.compat.v1.get_variable('Conv/weights', initializer=w)
-    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.compat.v1.get_variable_scope().reuse_variables()
+    tf.get_variable('Conv/weights', initializer=w)
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
     y1_expected = tf.cast(
@@ -144,7 +141,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = y2_expected
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -152,7 +149,7 @@ class ResnetUtilsTest(tf.test.TestCase):
 
   def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):
     """A plain ResNet without extra layers before or after the ResNet blocks."""
-    with tf.compat.v1.variable_scope(scope, values=[inputs]):
+    with tf.variable_scope(scope, values=[inputs]):
       with slim.arg_scope([slim.conv2d], outputs_collections='end_points'):
         net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)
         end_points = slim.utils.convert_collection_to_dict('end_points')
@@ -189,9 +186,9 @@ class ResnetUtilsTest(tf.test.TestCase):
   def _stack_blocks_nondense(self, net, blocks):
     """A simplified ResNet Block stacker without output stride control."""
     for block in blocks:
-      with tf.compat.v1.variable_scope(block.scope, 'block', [net]):
+      with tf.variable_scope(block.scope, 'block', [net]):
         for i, unit in enumerate(block.args):
-          with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
+          with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
             net = block.unit_fn(net, rate=1, **unit)
     return net
 
@@ -219,7 +216,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.compat.v1.set_random_seed(0)
+              tf.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
               # Dense feature extraction followed by subsampling.
               output = resnet_utils.stack_blocks_dense(inputs,
@@ -232,10 +229,10 @@ class ResnetUtilsTest(tf.test.TestCase):
 
               output = resnet_utils.subsample(output, factor)
               # Make the two networks use the same weights.
-              tf.compat.v1.get_variable_scope().reuse_variables()
+              tf.get_variable_scope().reuse_variables()
               # Feature extraction at the nominal network rate.
               expected = self._stack_blocks_nondense(inputs, blocks)
-              sess.run(tf.compat.v1.global_variables_initializer())
+              sess.run(tf.global_variables_initializer())
               output, expected = sess.run([output, expected])
               self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)
 
@@ -262,7 +259,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.compat.v1.set_random_seed(0)
+              tf.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
 
               # Subsampling at the last unit of the block.
@@ -274,7 +271,7 @@ class ResnetUtilsTest(tf.test.TestCase):
                   'output')
 
               # Make the two networks use the same weights.
-              tf.compat.v1.get_variable_scope().reuse_variables()
+              tf.get_variable_scope().reuse_variables()
 
               # Subsample activations at the end of the blocks.
               expected = resnet_utils.stack_blocks_dense(
@@ -284,7 +281,7 @@ class ResnetUtilsTest(tf.test.TestCase):
               expected_end_points = slim.utils.convert_collection_to_dict(
                   'expected')
 
-              sess.run(tf.compat.v1.global_variables_initializer())
+              sess.run(tf.global_variables_initializer())
 
               # Make sure that the final output is the same.
               output, expected = sess.run([output, expected])
@@ -475,7 +472,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
       with slim.arg_scope(resnet_utils.resnet_arg_scope()):
         with tf.Graph().as_default():
           with self.test_session() as sess:
-            tf.compat.v1.set_random_seed(0)
+            tf.set_random_seed(0)
             inputs = create_test_input(2, 81, 81, 3)
             # Dense feature extraction followed by subsampling.
             output, _ = self._resnet_small(inputs, None, is_training=False,
@@ -487,11 +484,11 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
               factor = nominal_stride // output_stride
             output = resnet_utils.subsample(output, factor)
             # Make the two networks use the same weights.
-            tf.compat.v1.get_variable_scope().reuse_variables()
+            tf.get_variable_scope().reuse_variables()
             # Feature extraction at the nominal network rate.
             expected, _ = self._resnet_small(inputs, None, is_training=False,
                                              global_pool=False)
-            sess.run(tf.compat.v1.global_variables_initializer())
+            sess.run(tf.global_variables_initializer())
             self.assertAllClose(output.eval(), expected.eval(),
                                 atol=1e-4, rtol=1e-4)
 
@@ -511,7 +508,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [None, 1, 1, num_classes])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 1, 1, num_classes))
 
@@ -526,7 +523,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 3, 3, 32))
 
@@ -545,7 +542,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 9, 9, 32))
 
diff --git a/research/slim/nets/resnet_v2.py b/research/slim/nets/resnet_v2.py
index b08af074..a30e8f1f 100644
--- a/research/slim/nets/resnet_v2.py
+++ b/research/slim/nets/resnet_v2.py
@@ -28,7 +28,7 @@ The key difference of the full preactivation 'v2' variant compared to the
 
 Typical use:
 
-   from tensorflow.contrib.slim.nets import resnet_v2
+   from tf_slim.nets import resnet_v2
 
 ResNet-101 for image classification into 1000 classes:
 
@@ -50,12 +50,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import resnet_utils
 
-slim = contrib_slim
 resnet_arg_scope = resnet_utils.resnet_arg_scope
 
 
@@ -84,7 +83,7 @@ def bottleneck(inputs, depth, depth_bottleneck, stride, rate=1,
   Returns:
     The ResNet unit's output.
   """
-  with tf.compat.v1.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:
+  with tf.variable_scope(scope, 'bottleneck_v2', [inputs]) as sc:
     depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
     preact = slim.batch_norm(inputs, activation_fn=tf.nn.relu, scope='preact')
     if depth == depth_in:
@@ -181,7 +180,7 @@ def resnet_v2(inputs,
   Raises:
     ValueError: If the target output_stride is not valid.
   """
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'resnet_v2', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
diff --git a/research/slim/nets/resnet_v2_test.py b/research/slim/nets/resnet_v2_test.py
index 5ca52274..d06cbde9 100644
--- a/research/slim/nets/resnet_v2_test.py
+++ b/research/slim/nets/resnet_v2_test.py
@@ -19,15 +19,13 @@ from __future__ import division
 from __future__ import print_function
 
 import numpy as np
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import resnet_utils
 from nets import resnet_v2
 
-slim = contrib_slim
-
-tf.compat.v1.disable_resource_variables()
+tf.disable_resource_variables()
 
 
 def create_test_input(batch_size, height, width, channels):
@@ -45,8 +43,7 @@ def create_test_input(batch_size, height, width, channels):
     constant `Tensor` with the mesh grid values along the spatial dimensions.
   """
   if None in [batch_size, height, width, channels]:
-    return tf.compat.v1.placeholder(tf.float32,
-                                    (batch_size, height, width, channels))
+    return tf.placeholder(tf.float32, (batch_size, height, width, channels))
   else:
     return tf.cast(
         np.tile(
@@ -83,9 +80,9 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.compat.v1.get_variable('Conv/weights', initializer=w)
-    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.compat.v1.get_variable_scope().reuse_variables()
+    tf.get_variable('Conv/weights', initializer=w)
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
     y1_expected = tf.cast([[14, 28, 43, 26], [28, 48, 66, 37], [43, 66, 84, 46],
@@ -105,7 +102,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = tf.reshape(y4_expected, [1, n2, n2, 1])
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -121,9 +118,9 @@ class ResnetUtilsTest(tf.test.TestCase):
     w = create_test_input(1, 3, 3, 1)
     w = tf.reshape(w, [3, 3, 1, 1])
 
-    tf.compat.v1.get_variable('Conv/weights', initializer=w)
-    tf.compat.v1.get_variable('Conv/biases', initializer=tf.zeros([1]))
-    tf.compat.v1.get_variable_scope().reuse_variables()
+    tf.get_variable('Conv/weights', initializer=w)
+    tf.get_variable('Conv/biases', initializer=tf.zeros([1]))
+    tf.get_variable_scope().reuse_variables()
 
     y1 = slim.conv2d(x, 1, [3, 3], stride=1, scope='Conv')
     y1_expected = tf.cast(
@@ -144,7 +141,7 @@ class ResnetUtilsTest(tf.test.TestCase):
     y4_expected = y2_expected
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       self.assertAllClose(y1.eval(), y1_expected.eval())
       self.assertAllClose(y2.eval(), y2_expected.eval())
       self.assertAllClose(y3.eval(), y3_expected.eval())
@@ -152,7 +149,7 @@ class ResnetUtilsTest(tf.test.TestCase):
 
   def _resnet_plain(self, inputs, blocks, output_stride=None, scope=None):
     """A plain ResNet without extra layers before or after the ResNet blocks."""
-    with tf.compat.v1.variable_scope(scope, values=[inputs]):
+    with tf.variable_scope(scope, values=[inputs]):
       with slim.arg_scope([slim.conv2d], outputs_collections='end_points'):
         net = resnet_utils.stack_blocks_dense(inputs, blocks, output_stride)
         end_points = slim.utils.convert_collection_to_dict('end_points')
@@ -189,9 +186,9 @@ class ResnetUtilsTest(tf.test.TestCase):
   def _stack_blocks_nondense(self, net, blocks):
     """A simplified ResNet Block stacker without output stride control."""
     for block in blocks:
-      with tf.compat.v1.variable_scope(block.scope, 'block', [net]):
+      with tf.variable_scope(block.scope, 'block', [net]):
         for i, unit in enumerate(block.args):
-          with tf.compat.v1.variable_scope('unit_%d' % (i + 1), values=[net]):
+          with tf.variable_scope('unit_%d' % (i + 1), values=[net]):
             net = block.unit_fn(net, rate=1, **unit)
     return net
 
@@ -219,7 +216,7 @@ class ResnetUtilsTest(tf.test.TestCase):
         for output_stride in [1, 2, 4, 8, None]:
           with tf.Graph().as_default():
             with self.test_session() as sess:
-              tf.compat.v1.set_random_seed(0)
+              tf.set_random_seed(0)
               inputs = create_test_input(1, height, width, 3)
               # Dense feature extraction followed by subsampling.
               output = resnet_utils.stack_blocks_dense(inputs,
@@ -232,10 +229,10 @@ class ResnetUtilsTest(tf.test.TestCase):
 
               output = resnet_utils.subsample(output, factor)
               # Make the two networks use the same weights.
-              tf.compat.v1.get_variable_scope().reuse_variables()
+              tf.get_variable_scope().reuse_variables()
               # Feature extraction at the nominal network rate.
               expected = self._stack_blocks_nondense(inputs, blocks)
-              sess.run(tf.compat.v1.global_variables_initializer())
+              sess.run(tf.global_variables_initializer())
               output, expected = sess.run([output, expected])
               self.assertAllClose(output, expected, atol=1e-4, rtol=1e-4)
 
@@ -392,7 +389,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
       with slim.arg_scope(resnet_utils.resnet_arg_scope()):
         with tf.Graph().as_default():
           with self.test_session() as sess:
-            tf.compat.v1.set_random_seed(0)
+            tf.set_random_seed(0)
             inputs = create_test_input(2, 81, 81, 3)
             # Dense feature extraction followed by subsampling.
             output, _ = self._resnet_small(inputs, None,
@@ -405,12 +402,12 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
               factor = nominal_stride // output_stride
             output = resnet_utils.subsample(output, factor)
             # Make the two networks use the same weights.
-            tf.compat.v1.get_variable_scope().reuse_variables()
+            tf.get_variable_scope().reuse_variables()
             # Feature extraction at the nominal network rate.
             expected, _ = self._resnet_small(inputs, None,
                                              is_training=False,
                                              global_pool=False)
-            sess.run(tf.compat.v1.global_variables_initializer())
+            sess.run(tf.global_variables_initializer())
             self.assertAllClose(output.eval(), expected.eval(),
                                 atol=1e-4, rtol=1e-4)
 
@@ -430,7 +427,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [None, 1, 1, num_classes])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 1, 1, num_classes))
 
@@ -446,7 +443,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 3, 3, 32))
 
@@ -465,7 +462,7 @@ class ResnetCompleteNetworkTest(tf.test.TestCase):
                          [batch, None, None, 32])
     images = create_test_input(batch, height, width, 3)
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(output, {inputs: images.eval()})
       self.assertEqual(output.shape, (batch, 9, 9, 32))
 
diff --git a/research/slim/nets/s3dg.py b/research/slim/nets/s3dg.py
index 3f443ad4..5197a81a 100644
--- a/research/slim/nets/s3dg.py
+++ b/research/slim/nets/s3dg.py
@@ -24,22 +24,19 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import framework as contrib_framework
-from tensorflow.contrib import layers as contrib_layers
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import i3d_utils
 
 # pylint: disable=g-long-lambda
-trunc_normal = lambda stddev: tf.compat.v1.truncated_normal_initializer(
+trunc_normal = lambda stddev: tf.truncated_normal_initializer(
     0.0, stddev)
 conv3d_spatiotemporal = i3d_utils.conv3d_spatiotemporal
 inception_block_v1_3d = i3d_utils.inception_block_v1_3d
 
-# Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more
-# update-to-date tf.contrib.* API.
-arg_scope = contrib_framework.arg_scope
-layers = contrib_layers
+
+arg_scope = slim.arg_scope
 
 
 def s3dg_arg_scope(weight_decay=1e-7,
@@ -72,12 +69,11 @@ def s3dg_arg_scope(weight_decay=1e-7,
       }
   }
 
-  with arg_scope(
-      [layers.conv3d, conv3d_spatiotemporal],
-      weights_regularizer=layers.l2_regularizer(weight_decay),
-      activation_fn=tf.nn.relu,
-      normalizer_fn=layers.batch_norm,
-      normalizer_params=batch_norm_params):
+  with arg_scope([slim.conv3d, conv3d_spatiotemporal],
+                 weights_regularizer=slim.l2_regularizer(weight_decay),
+                 activation_fn=tf.nn.relu,
+                 normalizer_fn=slim.batch_norm,
+                 normalizer_params=batch_norm_params):
     with arg_scope([conv3d_spatiotemporal], separable=True) as sc:
       return sc
 
@@ -115,13 +111,13 @@ def self_gating(input_tensor, scope, data_format='NDHWC'):
   h = input_shape[index_h]
   num_channels = input_shape[index_c]
 
-  spatiotemporal_average = layers.avg_pool3d(
+  spatiotemporal_average = slim.avg_pool3d(
       input_tensor, [t, w, h],
       stride=1,
       data_format=data_format,
       scope=scope + '/self_gating/avg_pool3d')
 
-  weights = layers.conv3d(
+  weights = slim.conv3d(
       spatiotemporal_average,
       num_channels, [1, 1, 1],
       activation_fn=None,
@@ -211,13 +207,12 @@ def s3dg_base(inputs,
     raise ValueError('depth_multiplier is not greater than zero.')
   depth = lambda d: max(int(d * depth_multiplier), min_depth)
 
-  with tf.compat.v1.variable_scope(scope, 'InceptionV1', [inputs]):
-    with arg_scope([layers.conv3d], weights_initializer=trunc_normal(0.01)):
-      with arg_scope(
-          [layers.conv3d, layers.max_pool3d, conv3d_spatiotemporal],
-          stride=1,
-          data_format=data_format,
-          padding='SAME'):
+  with tf.variable_scope(scope, 'InceptionV1', [inputs]):
+    with arg_scope([slim.conv3d], weights_initializer=trunc_normal(0.01)):
+      with arg_scope([slim.conv3d, slim.max_pool3d, conv3d_spatiotemporal],
+                     stride=1,
+                     data_format=data_format,
+                     padding='SAME'):
         # batch_size x 32 x 112 x 112 x 64
         end_point = 'Conv2d_1a_7x7'
         if first_temporal_kernel_size not in [1, 3, 5, 7]:
@@ -235,14 +230,13 @@ def s3dg_base(inputs,
           return net, end_points
         # batch_size x 32 x 56 x 56 x 64
         end_point = 'MaxPool_2a_3x3'
-        net = layers.max_pool3d(
-            net, [1, 3, 3], stride=[1, 2, 2], scope=end_point)
+        net = slim.max_pool3d(net, [1, 3, 3], stride=[1, 2, 2], scope=end_point)
         end_points[end_point] = net
         if final_endpoint == end_point:
           return net, end_points
         # batch_size x 32 x 56 x 56 x 64
         end_point = 'Conv2d_2b_1x1'
-        net = layers.conv3d(net, depth(64), [1, 1, 1], scope=end_point)
+        net = slim.conv3d(net, depth(64), [1, 1, 1], scope=end_point)
         end_points[end_point] = net
         if final_endpoint == end_point:
           return net, end_points
@@ -261,8 +255,7 @@ def s3dg_base(inputs,
           return net, end_points
         # batch_size x 32 x 28 x 28 x 192
         end_point = 'MaxPool_3a_3x3'
-        net = layers.max_pool3d(
-            net, [1, 3, 3], stride=[1, 2, 2], scope=end_point)
+        net = slim.max_pool3d(net, [1, 3, 3], stride=[1, 2, 2], scope=end_point)
         end_points[end_point] = net
         if final_endpoint == end_point:
           return net, end_points
@@ -313,8 +306,7 @@ def s3dg_base(inputs,
           return net, end_points
 
         end_point = 'MaxPool_4a_3x3'
-        net = layers.max_pool3d(
-            net, [3, 3, 3], stride=[2, 2, 2], scope=end_point)
+        net = slim.max_pool3d(net, [3, 3, 3], stride=[2, 2, 2], scope=end_point)
         end_points[end_point] = net
         if final_endpoint == end_point:
           return net, end_points
@@ -435,8 +427,7 @@ def s3dg_base(inputs,
           return net, end_points
 
         end_point = 'MaxPool_5a_2x2'
-        net = layers.max_pool3d(
-            net, [2, 2, 2], stride=[2, 2, 2], scope=end_point)
+        net = slim.max_pool3d(net, [2, 2, 2], stride=[2, 2, 2], scope=end_point)
         end_points[end_point] = net
         if final_endpoint == end_point:
           return net, end_points
@@ -499,7 +490,7 @@ def s3dg(inputs,
          depth_multiplier=1.0,
          dropout_keep_prob=0.8,
          is_training=True,
-         prediction_fn=layers.softmax,
+         prediction_fn=slim.softmax,
          spatial_squeeze=True,
          reuse=None,
          data_format='NDHWC',
@@ -558,10 +549,9 @@ def s3dg(inputs,
   """
   assert data_format in ['NDHWC', 'NCDHW']
   # Final pooling and prediction
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'InceptionV1', [inputs, num_classes], reuse=reuse) as scope:
-    with arg_scope(
-        [layers.batch_norm, layers.dropout], is_training=is_training):
+    with arg_scope([slim.batch_norm, slim.dropout], is_training=is_training):
       net, end_points = s3dg_base(
           inputs,
           first_temporal_kernel_size=first_temporal_kernel_size,
@@ -572,18 +562,18 @@ def s3dg(inputs,
           depth_multiplier=depth_multiplier,
           data_format=data_format,
           scope=scope)
-      with tf.compat.v1.variable_scope('Logits'):
+      with tf.variable_scope('Logits'):
         if data_format.startswith('NC'):
           net = tf.transpose(a=net, perm=[0, 2, 3, 4, 1])
         kernel_size = i3d_utils.reduced_kernel_size_3d(net, [2, 7, 7])
-        net = layers.avg_pool3d(
+        net = slim.avg_pool3d(
             net,
             kernel_size,
             stride=1,
             data_format='NDHWC',
             scope='AvgPool_0a_7x7')
-        net = layers.dropout(net, dropout_keep_prob, scope='Dropout_0b')
-        logits = layers.conv3d(
+        net = slim.dropout(net, dropout_keep_prob, scope='Dropout_0b')
+        logits = slim.conv3d(
             net,
             num_classes, [1, 1, 1],
             activation_fn=None,
diff --git a/research/slim/nets/s3dg_test.py b/research/slim/nets/s3dg_test.py
index c3dc57c1..9d0fb375 100644
--- a/research/slim/nets/s3dg_test.py
+++ b/research/slim/nets/s3dg_test.py
@@ -18,7 +18,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import six
+import tensorflow.compat.v1 as tf
 
 from nets import s3dg
 
@@ -55,7 +56,7 @@ class S3DGTest(tf.test.TestCase):
                           'Mixed_3c', 'MaxPool_4a_3x3', 'Mixed_4b', 'Mixed_4c',
                           'Mixed_4d', 'Mixed_4e', 'Mixed_4f', 'MaxPool_5a_2x2',
                           'Mixed_5b', 'Mixed_5c']
-    self.assertItemsEqual(end_points.keys(), expected_endpoints)
+    self.assertItemsEqual(list(end_points.keys()), expected_endpoints)
 
   def testBuildOnlyUptoFinalEndpointNoGating(self):
     batch_size = 5
@@ -101,8 +102,9 @@ class S3DGTest(tf.test.TestCase):
                         'Mixed_5b': [5, 8, 7, 7, 832],
                         'Mixed_5c': [5, 8, 7, 7, 1024]}
 
-    self.assertItemsEqual(endpoints_shapes.keys(), end_points.keys())
-    for endpoint_name, expected_shape in endpoints_shapes.iteritems():
+    self.assertItemsEqual(
+        list(endpoints_shapes.keys()), list(end_points.keys()))
+    for endpoint_name, expected_shape in six.iteritems(endpoints_shapes):
       self.assertTrue(endpoint_name in end_points)
       self.assertListEqual(end_points[endpoint_name].get_shape().as_list(),
                            expected_shape)
@@ -141,7 +143,7 @@ class S3DGTest(tf.test.TestCase):
     predictions = tf.argmax(input=logits, axis=1)
 
     with self.test_session() as sess:
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(predictions)
       self.assertEquals(output.shape, (batch_size,))
 
diff --git a/research/slim/nets/vgg.py b/research/slim/nets/vgg.py
index 2f30a744..c812f587 100644
--- a/research/slim/nets/vgg.py
+++ b/research/slim/nets/vgg.py
@@ -41,10 +41,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 
 def vgg_arg_scope(weight_decay=0.0005):
@@ -59,7 +57,7 @@ def vgg_arg_scope(weight_decay=0.0005):
   with slim.arg_scope([slim.conv2d, slim.fully_connected],
                       activation_fn=tf.nn.relu,
                       weights_regularizer=slim.l2_regularizer(weight_decay),
-                      biases_initializer=tf.compat.v1.zeros_initializer()):
+                      biases_initializer=tf.zeros_initializer()):
     with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:
       return arg_sc
 
@@ -105,7 +103,7 @@ def vgg_a(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.compat.v1.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
+  with tf.variable_scope(scope, 'vgg_a', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
     with slim.arg_scope([slim.conv2d, slim.max_pool2d],
@@ -187,7 +185,7 @@ def vgg_16(inputs,
       or the input to the logits layer (if num_classes is 0 or None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'vgg_16', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
@@ -271,7 +269,7 @@ def vgg_19(inputs,
       None).
     end_points: a dict of tensors with intermediate activations.
   """
-  with tf.compat.v1.variable_scope(
+  with tf.variable_scope(
       scope, 'vgg_19', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     # Collect outputs for conv2d, fully_connected and max_pool2d.
diff --git a/research/slim/nets/vgg_test.py b/research/slim/nets/vgg_test.py
index 988c3dbe..97983cbf 100644
--- a/research/slim/nets/vgg_test.py
+++ b/research/slim/nets/vgg_test.py
@@ -17,13 +17,11 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
 
 from nets import vgg
 
-slim = contrib_slim
-
 
 class VGGATest(tf.test.TestCase):
 
@@ -170,7 +168,7 @@ class VGGATest(tf.test.TestCase):
       logits, _ = vgg.vgg_a(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.compat.v1.get_variable_scope().reuse_variables()
+      tf.get_variable_scope().reuse_variables()
       eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_a(eval_inputs, is_training=False,
@@ -187,7 +185,7 @@ class VGGATest(tf.test.TestCase):
     with self.test_session() as sess:
       inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_a(inputs)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
@@ -357,7 +355,7 @@ class VGG16Test(tf.test.TestCase):
       logits, _ = vgg.vgg_16(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.compat.v1.get_variable_scope().reuse_variables()
+      tf.get_variable_scope().reuse_variables()
       eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_16(eval_inputs, is_training=False,
@@ -374,7 +372,7 @@ class VGG16Test(tf.test.TestCase):
     with self.test_session() as sess:
       inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_16(inputs)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
@@ -559,7 +557,7 @@ class VGG19Test(tf.test.TestCase):
       logits, _ = vgg.vgg_19(train_inputs)
       self.assertListEqual(logits.get_shape().as_list(),
                            [train_batch_size, num_classes])
-      tf.compat.v1.get_variable_scope().reuse_variables()
+      tf.get_variable_scope().reuse_variables()
       eval_inputs = tf.random.uniform(
           (eval_batch_size, eval_height, eval_width, 3))
       logits, _ = vgg.vgg_19(eval_inputs, is_training=False,
@@ -576,7 +574,7 @@ class VGG19Test(tf.test.TestCase):
     with self.test_session() as sess:
       inputs = tf.random.uniform((batch_size, height, width, 3))
       logits, _ = vgg.vgg_19(inputs)
-      sess.run(tf.compat.v1.global_variables_initializer())
+      sess.run(tf.global_variables_initializer())
       output = sess.run(logits)
       self.assertTrue(output.any())
 
diff --git a/research/slim/preprocessing/cifarnet_preprocessing.py b/research/slim/preprocessing/cifarnet_preprocessing.py
index 2f66e77b..3ed55a84 100644
--- a/research/slim/preprocessing/cifarnet_preprocessing.py
+++ b/research/slim/preprocessing/cifarnet_preprocessing.py
@@ -20,12 +20,10 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
-_PADDING = 4
 
-slim = contrib_slim
+_PADDING = 4
 
 
 def preprocess_for_train(image,
diff --git a/research/slim/preprocessing/inception_preprocessing.py b/research/slim/preprocessing/inception_preprocessing.py
index 1bac04d5..0de560e7 100644
--- a/research/slim/preprocessing/inception_preprocessing.py
+++ b/research/slim/preprocessing/inception_preprocessing.py
@@ -18,7 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
 
 from tensorflow.python.ops import control_flow_ops
 
diff --git a/research/slim/preprocessing/lenet_preprocessing.py b/research/slim/preprocessing/lenet_preprocessing.py
index d5cdec9b..28271135 100644
--- a/research/slim/preprocessing/lenet_preprocessing.py
+++ b/research/slim/preprocessing/lenet_preprocessing.py
@@ -18,10 +18,7 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
-
-slim = contrib_slim
+import tensorflow.compat.v1 as tf
 
 
 def preprocess_image(image,
diff --git a/research/slim/preprocessing/preprocessing_factory.py b/research/slim/preprocessing/preprocessing_factory.py
index d3f56d72..a6318b7c 100644
--- a/research/slim/preprocessing/preprocessing_factory.py
+++ b/research/slim/preprocessing/preprocessing_factory.py
@@ -17,15 +17,12 @@
 from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
-from tensorflow.contrib import slim as contrib_slim
 
 from preprocessing import cifarnet_preprocessing
 from preprocessing import inception_preprocessing
 from preprocessing import lenet_preprocessing
 from preprocessing import vgg_preprocessing
 
-slim = contrib_slim
-
 
 def get_preprocessing(name, is_training=False, use_grayscale=False):
   """Returns preprocessing_fn(image, height, width, **kwargs).
diff --git a/research/slim/preprocessing/vgg_preprocessing.py b/research/slim/preprocessing/vgg_preprocessing.py
index ae1db145..1fe26bf6 100644
--- a/research/slim/preprocessing/vgg_preprocessing.py
+++ b/research/slim/preprocessing/vgg_preprocessing.py
@@ -32,10 +32,8 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
-from tensorflow.contrib import slim as contrib_slim
+import tensorflow.compat.v1 as tf
 
-slim = contrib_slim
 
 _R_MEAN = 123.68
 _G_MEAN = 116.78
diff --git a/research/slim/setup.py b/research/slim/setup.py
index 3ec7ecd6..c258ae16 100644
--- a/research/slim/setup.py
+++ b/research/slim/setup.py
@@ -17,10 +17,15 @@
 from setuptools import find_packages
 from setuptools import setup
 
+install_requires = [
+    'six',
+    'tf-slim>=1.1',
+]
 
 setup(
     name='slim',
     version='0.1',
+    install_requires=install_requires,
     include_package_data=True,
     packages=find_packages(),
     description='tf-slim',
diff --git a/research/slim/slim_walkthrough.ipynb b/research/slim/slim_walkthrough.ipynb
index 262281a5..c5bca282 100644
--- a/research/slim/slim_walkthrough.ipynb
+++ b/research/slim/slim_walkthrough.ipynb
@@ -58,13 +58,13 @@
     "import matplotlib.pyplot as plt\n",
     "import math\n",
     "import numpy as np\n",
-    "import tensorflow as tf\n",
+    "import tensorflow.compat.v1 as tf\n",
     "import time\n",
     "\n",
     "from datasets import dataset_utils\n",
     "\n",
     "# Main slim library\n",
-    "from tensorflow.contrib import slim"
+    "import tf_slim as slim"
    ]
   },
   {
@@ -452,7 +452,7 @@
    },
    "outputs": [],
    "source": [
-    "import tensorflow as tf\n",
+    "import tensorflow.compat.v1 as tf\n",
     "from datasets import dataset_utils\n",
     "\n",
     "url = \"http://download.tensorflow.org/data/flowers.tar.gz\"\n",
@@ -480,7 +480,7 @@
    "outputs": [],
    "source": [
     "from datasets import flowers\n",
-    "import tensorflow as tf\n",
+    "import tensorflow.compat.v1 as tf\n",
     "\n",
     "from tensorflow.contrib import slim\n",
     "\n",
diff --git a/research/slim/train_image_classifier.py b/research/slim/train_image_classifier.py
index 4453cc23..4503e7e9 100644
--- a/research/slim/train_image_classifier.py
+++ b/research/slim/train_image_classifier.py
@@ -18,17 +18,16 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-import tensorflow as tf
+import tensorflow.compat.v1 as tf
+import tf_slim as slim
+
 from tensorflow.contrib import quantize as contrib_quantize
-from tensorflow.contrib import slim as contrib_slim
 
 from datasets import dataset_factory
 from deployment import model_deploy
 from nets import nets_factory
 from preprocessing import preprocessing_factory
 
-slim = contrib_slim
-
 tf.app.flags.DEFINE_string(
     'master', '', 'The address of the TensorFlow master to use.')
 
