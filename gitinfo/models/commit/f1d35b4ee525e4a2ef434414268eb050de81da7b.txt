commit f1d35b4ee525e4a2ef434414268eb050de81da7b
Author: Hongkun Yu <hongkuny@google.com>
Date:   Mon Nov 11 10:16:35 2019 -0800

    Release keras bert:
    - Update classifier example.
    - Add new converted checkpoints.
    - Update benchmark,
    
    PiperOrigin-RevId: 279762797

diff --git a/official/benchmark/bert_benchmark.py b/official/benchmark/bert_benchmark.py
index 46f2c971..f3047166 100644
--- a/official/benchmark/bert_benchmark.py
+++ b/official/benchmark/bert_benchmark.py
@@ -35,7 +35,7 @@ from official.nlp.bert import run_classifier
 from official.utils.misc import distribution_utils
 
 # pylint: disable=line-too-long
-PRETRAINED_CHECKPOINT_PATH = 'gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16/bert_model.ckpt'
+PRETRAINED_CHECKPOINT_PATH = 'placer/prod/home/tensorflow-performance-data/datasets/bert/keras_bert/bert_model.ckpt'
 CLASSIFIER_TRAIN_DATA_PATH = 'gs://tf-perfzero-data/bert/classification/mrpc_train.tf_record'
 CLASSIFIER_EVAL_DATA_PATH = 'gs://tf-perfzero-data/bert/classification/mrpc_eval.tf_record'
 CLASSIFIER_INPUT_META_DATA_PATH = 'gs://tf-perfzero-data/bert/classification/mrpc_meta_data'
diff --git a/official/benchmark/bert_squad_benchmark.py b/official/benchmark/bert_squad_benchmark.py
index 0b6ee139..4a2e4ab6 100644
--- a/official/benchmark/bert_squad_benchmark.py
+++ b/official/benchmark/bert_squad_benchmark.py
@@ -34,7 +34,7 @@ from official.nlp.bert import run_squad
 from official.utils.misc import distribution_utils
 
 # pylint: disable=line-too-long
-PRETRAINED_CHECKPOINT_PATH = 'gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16/bert_model.ckpt'
+PRETRAINED_CHECKPOINT_PATH = '/placer/prod/home/tensorflow-performance-data/datasets/bert/tf_20/uncased_L-24_H-1024_A-16/bert_model.ckpt'
 SQUAD_TRAIN_DATA_PATH = 'gs://tf-perfzero-data/bert/squad/squad_train.tf_record'
 SQUAD_PREDICT_FILE = 'gs://tf-perfzero-data/bert/squad/dev-v1.1.json'
 SQUAD_VOCAB_FILE = 'gs://tf-perfzero-data/bert/squad/vocab.txt'
diff --git a/official/nlp/bert/README.md b/official/nlp/bert/README.md
index e1162b7d..d13309ad 100644
--- a/official/nlp/bert/README.md
+++ b/official/nlp/bert/README.md
@@ -32,6 +32,11 @@ are going to release new pre-trained checkpoints soon.
 We provide checkpoints that are converted from [google-research/bert](https://github.com/google-research/bert),
 in order to keep consistent with BERT paper.
 
+The stable model checkpoints work with [v2.0 release](https://github.com/tensorflow/models/releases/tag/v2.0).
+
+**Note: these checkpoints are not compatible with the current master
+[run_classifier.py](run_classifier.py) example.**
+
 *   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/tf_20/wwm_uncased_L-24_H-1024_A-16.tar.gz)**:
     24-layer, 1024-hidden, 16-heads, 340M parameters
 *   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/tf_20/wwm_cased_L-24_H-1024_A-16.tar.gz)**:
@@ -45,12 +50,28 @@ in order to keep consistent with BERT paper.
 *   **[`BERT-Large, Cased`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/tf_20/cased_L-24_H-1024_A-16.tar.gz)**:
     24-layer, 1024-hidden, 16-heads, 340M parameters
 
-We recommend to host checkpoints on Google Cloud storage buckets when you use
-Cloud GPU/TPU. For example, in the following tutorial, we use:
+**Note: We are in the middle of a transition stage to switch BERT implementation
+to use Keras functional-style networks in [nlp/modeling](../modeling).
+The checkpoint above will be deleted once transition is done.**
 
-```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
-```
+The new checkpoints work with [run_classifier.py](run_classifier.py) example
+are:
+
+*   **[`BERT-Large, Uncased (Whole Word Masking)`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/wwm_uncased_L-24_H-1024_A-16.tar.gz)**:
+    24-layer, 1024-hidden, 16-heads, 340M parameters
+*   **[`BERT-Large, Cased (Whole Word Masking)`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/wwm_cased_L-24_H-1024_A-16.tar.gz)**:
+    24-layer, 1024-hidden, 16-heads, 340M parameters
+*   **[`BERT-Base, Uncased`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/uncased_L-12_H-768_A-12.tar.gz)**:
+    12-layer, 768-hidden, 12-heads, 110M parameters
+*   **[`BERT-Large, Uncased`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16.tar.gz)**:
+    24-layer, 1024-hidden, 16-heads, 340M parameters
+*   **[`BERT-Base, Cased`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/cased_L-12_H-768_A-12.tar.gz)**:
+    12-layer, 768-hidden, 12-heads , 110M parameters
+*   **[`BERT-Large, Cased`](https://storage.googleapis.com/cloud-tpu-checkpoints/bert/keras_bert/cased_L-24_H-1024_A-16.tar.gz)**:
+    24-layer, 1024-hidden, 16-heads, 340M parameters
+
+We recommend to host checkpoints on Google Cloud storage buckets when you use
+Cloud GPU/TPU.
 
 ### Restoring from Checkpoints
 
@@ -175,7 +196,7 @@ The unzipped pre-trained model files can also be found in the Google Cloud
 Storage folder `gs://cloud-tpu-checkpoints/bert/tf_20`. For example:
 
 ```shell
-export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/tf_20/uncased_L-24_H-1024_A-16
+export BERT_BASE_DIR=gs://cloud-tpu-checkpoints/bert/keras_bert/uncased_L-24_H-1024_A-16
 export MODEL_DIR=gs://some_bucket/my_output_dir
 ```
 
diff --git a/official/nlp/bert_models.py b/official/nlp/bert_models.py
index cab8556f..f9efa0c8 100644
--- a/official/nlp/bert_models.py
+++ b/official/nlp/bert_models.py
@@ -24,6 +24,8 @@ import tensorflow_hub as hub
 
 from official.modeling import tf_utils
 from official.nlp import bert_modeling as modeling
+from official.nlp.modeling import networks
+from official.nlp.modeling.networks import bert_classifier
 
 
 def gather_indexes(sequence_tensor, positions):
@@ -160,8 +162,8 @@ class BertPretrainLossAndMetricLayer(tf.keras.layers.Layer):
         lm_output, sentence_output, lm_label_ids, lm_label_weights,
         sentence_labels
     ])
-    return super(BertPretrainLossAndMetricLayer, self).__call__(
-        inputs, **kwargs)
+    return super(BertPretrainLossAndMetricLayer,
+                 self).__call__(inputs, **kwargs)
 
   def _add_metrics(self, lm_output, lm_labels, lm_label_weights,
                    lm_per_example_loss, sentence_output, sentence_labels,
@@ -354,9 +356,7 @@ def squad_model(bert_config,
       shape=(max_seq_length,), dtype=tf.int32, name='segment_ids')
 
   if hub_module_url:
-    core_model = hub.KerasLayer(
-        hub_module_url,
-        trainable=True)
+    core_model = hub.KerasLayer(hub_module_url, trainable=True)
     _, sequence_output = core_model(
         [input_word_ids, input_mask, input_type_ids])
     # Sets the shape manually due to a bug in TF shape inference.
@@ -417,32 +417,44 @@ def classifier_model(bert_config,
     Combined prediction model (words, mask, type) -> (one-hot labels)
     BERT sub-model (words, mask, type) -> (bert_outputs)
   """
-  input_word_ids = tf.keras.layers.Input(
-      shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')
-  input_mask = tf.keras.layers.Input(
-      shape=(max_seq_length,), dtype=tf.int32, name='input_mask')
-  input_type_ids = tf.keras.layers.Input(
-      shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')
-  if hub_module_url:
-    bert_model = hub.KerasLayer(hub_module_url, trainable=True)
-    pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])
-  else:
-    bert_model = modeling.get_bert_model(
-        input_word_ids,
-        input_mask,
-        input_type_ids,
-        config=bert_config,
-        float_type=float_type)
-    pooled_output = bert_model.outputs[0]
-
   if final_layer_initializer is not None:
     initializer = final_layer_initializer
   else:
     initializer = tf.keras.initializers.TruncatedNormal(
         stddev=bert_config.initializer_range)
 
+  if not hub_module_url:
+    bert_encoder = networks.TransformerEncoder(
+        vocab_size=bert_config.vocab_size,
+        hidden_size=bert_config.hidden_size,
+        num_layers=bert_config.num_hidden_layers,
+        num_attention_heads=bert_config.num_attention_heads,
+        intermediate_size=bert_config.intermediate_size,
+        activation=tf_utils.get_activation('gelu'),
+        dropout_rate=bert_config.hidden_dropout_prob,
+        attention_dropout_rate=bert_config.attention_probs_dropout_prob,
+        sequence_length=max_seq_length,
+        max_sequence_length=bert_config.max_position_embeddings,
+        type_vocab_size=bert_config.type_vocab_size,
+        initializer=tf.keras.initializers.TruncatedNormal(
+            stddev=bert_config.initializer_range))
+    return bert_classifier.BertClassifier(
+        bert_encoder,
+        num_classes=num_labels,
+        dropout_rate=bert_config.hidden_dropout_prob,
+        initializer=initializer), bert_encoder
+
+  input_word_ids = tf.keras.layers.Input(
+      shape=(max_seq_length,), dtype=tf.int32, name='input_word_ids')
+  input_mask = tf.keras.layers.Input(
+      shape=(max_seq_length,), dtype=tf.int32, name='input_mask')
+  input_type_ids = tf.keras.layers.Input(
+      shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')
+  bert_model = hub.KerasLayer(hub_module_url, trainable=True)
+  pooled_output, _ = bert_model([input_word_ids, input_mask, input_type_ids])
   output = tf.keras.layers.Dropout(rate=bert_config.hidden_dropout_prob)(
       pooled_output)
+
   output = tf.keras.layers.Dense(
       num_labels,
       kernel_initializer=initializer,
diff --git a/official/nlp/modeling/__init__.py b/official/nlp/modeling/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/official/nlp/modeling/__init__.py
@@ -0,0 +1 @@
+
diff --git a/official/nlp/modeling/layers/__init__.py b/official/nlp/modeling/layers/__init__.py
new file mode 100644
index 00000000..af408100
--- /dev/null
+++ b/official/nlp/modeling/layers/__init__.py
@@ -0,0 +1,21 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Layers package definition."""
+from official.nlp.modeling.layers.attention import Attention
+from official.nlp.modeling.layers.dense_einsum import DenseEinsum
+from official.nlp.modeling.layers.masked_softmax import MaskedSoftmax
+from official.nlp.modeling.layers.on_device_embedding import OnDeviceEmbedding
+from official.nlp.modeling.layers.position_embedding import PositionEmbedding
+from official.nlp.modeling.layers.transformer import Transformer
diff --git a/official/nlp/modeling/layers/attention.py b/official/nlp/modeling/layers/attention.py
new file mode 100644
index 00000000..840764f1
--- /dev/null
+++ b/official/nlp/modeling/layers/attention.py
@@ -0,0 +1,194 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based attention layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import math
+import tensorflow as tf
+
+from official.nlp.modeling.layers import dense_einsum
+from official.nlp.modeling.layers import masked_softmax
+
+
+@tf.keras.utils.register_keras_serializable(package="Text")
+class Attention(tf.keras.layers.Layer):
+  """Attention layer.
+
+  This is an implementation of multi-headed attention based on "Attention
+  is all you Need". If `from_tensor` and `to_tensor` are the same, then
+  this is self-attention. Each timestep in `from_tensor` attends to the
+  corresponding sequence in `to_tensor`, and returns a fixed-width vector.
+
+  This function first projects `from_tensor` into a "query" tensor and
+  `to_tensor` into "key" and "value" tensors. These are (effectively) a list
+  of tensors of length `num_attention_heads`, where each tensor is of shape
+  [batch_size, seq_length, size_per_head].
+
+  Then, the query and key tensors are dot-producted and scaled. These are
+  softmaxed to obtain attention probabilities. The value tensors are then
+  interpolated by these probabilities, then concatenated back to a single
+  tensor and returned.
+
+  Attributes:
+    num_heads: Number of attention heads.
+    head_size: Size of each attention head.
+    dropout: Dropout probability.
+    kernel_initializer: Initializer for dense layer kernels.
+    bias_initializer: Initializer for dense layer biases.
+    kernel_regularizer: Regularizer for dense layer kernels.
+    bias_regularizer: Regularizer for dense layer biases.
+    activity_regularizer: Regularizer for dense layer activity.
+    kernel_constraint: Constraint for dense layer kernels.
+    bias_constraint: Constraint for dense layer kernels.
+  """
+
+  def __init__(self,
+               num_heads,
+               head_size,
+               dropout_rate=0.0,
+               kernel_initializer="glorot_uniform",
+               bias_initializer="zeros",
+               kernel_regularizer=None,
+               bias_regularizer=None,
+               activity_regularizer=None,
+               kernel_constraint=None,
+               bias_constraint=None,
+               **kwargs):
+    super(Attention, self).__init__(**kwargs)
+    self._num_heads = num_heads
+    self._head_size = head_size
+    self._dropout_rate = dropout_rate
+    self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)
+    self._bias_initializer = tf.keras.initializers.get(bias_initializer)
+    self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)
+    self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)
+    self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)
+    self._bias_constraint = tf.keras.constraints.get(bias_constraint)
+
+    self._query_dense = dense_einsum.DenseEinsum(
+        output_shape=(self._num_heads, self._head_size),
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="query")
+
+    self._key_dense = dense_einsum.DenseEinsum(
+        output_shape=(self._num_heads, self._head_size),
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="key")
+
+    self._value_dense = dense_einsum.DenseEinsum(
+        output_shape=(self._num_heads, self._head_size),
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="value")
+
+    self._masked_softmax = masked_softmax.MaskedSoftmax(mask_expansion_axes=[1])
+
+    self._dropout = tf.keras.layers.Dropout(
+        rate=self._dropout_rate, dtype=self.dtype)
+
+  def compute_output_shape(self, input_shape):
+    # TODO(momernick): validate tensor dimensioos
+    from_tensor_shape = tf.TensorShape(input_shape[0])
+    batch = from_tensor_shape[0]
+    from_tensor_length = from_tensor_shape[1]
+    return tf.TensorShape(
+        (batch, from_tensor_length, self._num_heads, self._head_size))
+
+  def get_config(self):
+    config = {
+        "num_heads":
+            self._num_heads,
+        "head_size":
+            self._head_size,
+        "dropout_rate":
+            self._dropout_rate,
+        "kernel_initializer":
+            tf.keras.initializers.serialize(self._kernel_initializer),
+        "bias_initializer":
+            tf.keras.initializers.serialize(self._bias_initializer),
+        "kernel_regularizer":
+            tf.keras.regularizers.serialize(self._kernel_regularizer),
+        "bias_regularizer":
+            tf.keras.regularizers.serialize(self._bias_regularizer),
+        "activity_regularizer":
+            tf.keras.regularizers.serialize(self._activity_regularizer),
+        "kernel_constraint":
+            tf.keras.constraints.serialize(self._kernel_constraint),
+        "bias_constraint":
+            tf.keras.constraints.serialize(self._bias_constraint)
+    }
+    base_config = super(Attention, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def call(self, inputs):
+    from_tensor = inputs[0]
+    to_tensor = inputs[1]
+    attention_mask = inputs[2] if len(inputs) == 3 else None
+
+    # Scalar dimensions referenced here:
+    #   B = batch size (number of sequences)
+    #   F = `from_tensor` sequence length
+    #   T = `to_tensor` sequence length
+    #   N = `num_attention_heads`
+    #   H = `size_per_head`
+    # `query_tensor` = [B, F, N ,H]
+    query_tensor = self._query_dense(from_tensor)
+
+    # `key_tensor` = [B, T, N, H]
+    key_tensor = self._key_dense(to_tensor)
+
+    # `value_tensor` = [B, T, N, H]
+    value_tensor = self._value_dense(to_tensor)
+
+    # Take the dot product between "query" and "key" to get the raw
+    # attention scores.
+    attention_scores = tf.einsum("BTNH,BFNH->BNFT", key_tensor, query_tensor)
+    attention_scores = tf.multiply(attention_scores,
+                                   1.0 / math.sqrt(float(self._head_size)))
+
+    # Normalize the attention scores to probabilities.
+    # `attention_probs` = [B, N, F, T]
+    attention_probs = self._masked_softmax([attention_scores, attention_mask])
+
+    # This is actually dropping out entire tokens to attend to, which might
+    # seem a bit unusual, but is taken from the original Transformer paper.
+    attention_probs = self._dropout(attention_probs)
+
+    # `context_layer` = [B, F, N, H]
+    return tf.einsum("BNFT,BTNH->BFNH", attention_probs, value_tensor)
diff --git a/official/nlp/modeling/layers/attention_test.py b/official/nlp/modeling/layers/attention_test.py
new file mode 100644
index 00000000..f65acb01
--- /dev/null
+++ b/official/nlp/modeling/layers/attention_test.py
@@ -0,0 +1,92 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for the attention layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import attention
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class AttentionLayerTest(keras_parameterized.TestCase):
+
+  def test_non_masked_attention(self):
+    """Test that the attention layer can be created without a mask tensor."""
+    test_layer = attention.Attention(num_heads=12, head_size=64)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    from_tensor = tf.keras.Input(shape=(40, 80))
+    to_tensor = tf.keras.Input(shape=(20, 80))
+    output = test_layer([from_tensor, to_tensor])
+    self.assertEqual(output.shape.as_list(), [None, 40, 12, 64])
+
+  def test_non_masked_self_attention(self):
+    """Test with one input (self-attenntion) and no mask tensor."""
+    test_layer = attention.Attention(num_heads=12, head_size=64)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    from_tensor = tf.keras.Input(shape=(40, 80))
+    output = test_layer([from_tensor, from_tensor])
+    self.assertEqual(output.shape.as_list(), [None, 40, 12, 64])
+
+  def test_masked_attention(self):
+    """Test with a mask tensor."""
+    test_layer = attention.Attention(num_heads=2, head_size=2)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    from_tensor = tf.keras.Input(shape=(4, 8))
+    to_tensor = tf.keras.Input(shape=(2, 8))
+    mask_tensor = tf.keras.Input(shape=(4, 2))
+    output = test_layer([from_tensor, to_tensor, mask_tensor])
+
+    # Create a model containing the test layer.
+    model = tf.keras.Model([from_tensor, to_tensor, mask_tensor], output)
+
+    # Generate data for the input (non-mask) tensors.
+    from_data = 10 * np.random.random_sample((3, 4, 8))
+    to_data = 10 * np.random.random_sample((3, 2, 8))
+
+    # Invoke the data with a random set of mask data. This should mask at least
+    # one element.
+    mask_data = np.random.randint(2, size=(3, 4, 2))
+    masked_output_data = model.predict([from_data, to_data, mask_data])
+
+    # Invoke the same data, but with a null mask (where no elements are masked).
+    null_mask_data = np.ones((3, 4, 2))
+    unmasked_output_data = model.predict([from_data, to_data, null_mask_data])
+
+    # Because one data is masked and one is not, the outputs should not be the
+    # same.
+    self.assertNotAllClose(masked_output_data, unmasked_output_data)
+
+  def test_initializer(self):
+    """Test with a specified initializer."""
+    test_layer = attention.Attention(
+        num_heads=12,
+        head_size=64,
+        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))
+    # Create a 3-dimensional input (the first dimension is implicit).
+    from_tensor = tf.keras.Input(shape=(40, 80))
+    output = test_layer([from_tensor, from_tensor])
+    self.assertEqual(output.shape.as_list(), [None, 40, 12, 64])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/layers/dense_einsum.py b/official/nlp/modeling/layers/dense_einsum.py
new file mode 100644
index 00000000..9dadede8
--- /dev/null
+++ b/official/nlp/modeling/layers/dense_einsum.py
@@ -0,0 +1,190 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based einsum layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+_CHR_IDX = ["a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m"]
+
+
+@tf.keras.utils.register_keras_serializable(package="Text")
+class DenseEinsum(tf.keras.layers.Layer):
+  """A densely connected layer that uses tf.einsum as the backing computation.
+
+  This layer can perform einsum calculations of arbitrary dimensionality.
+
+  Attributes:
+    output_shape: Positive integer or tuple, dimensionality of the output space.
+    num_summed_dimensions: The number of dimensions to sum over. Standard 2D
+      matmul should use 1, 3D matmul should use 2, and so forth.
+    activation: Activation function to use. If you don't specify anything, no
+      activation is applied
+      (ie. "linear" activation: `a(x) = x`).
+    use_bias: Boolean, whether the layer uses a bias vector.
+    kernel_initializer: Initializer for the `kernel` weights matrix.
+    bias_initializer: Initializer for the bias vector.
+    kernel_regularizer: Regularizer function applied to the `kernel` weights
+      matrix.
+    bias_regularizer: Regularizer function applied to the bias vector.
+    activity_regularizer: Regularizer function applied to the output of the
+      layer (its "activation")..
+    kernel_constraint: Constraint function applied to the `kernel` weights
+      matrix.
+    bias_constraint: Constraint function applied to the bias vector.
+  Input shape:
+    N-D tensor with shape: `(batch_size, ..., input_dim)`. The most common
+      situation would be a 2D input with shape `(batch_size, input_dim)`.
+  Output shape:
+    N-D tensor with shape: `(batch_size, ..., units)`. For instance, for a 2D
+      input with shape `(batch_size, input_dim)`, the output would have shape
+      `(batch_size, units)`.
+  """
+
+  def __init__(self,
+               output_shape,
+               num_summed_dimensions=1,
+               activation=None,
+               use_bias=True,
+               kernel_initializer="glorot_uniform",
+               bias_initializer="zeros",
+               kernel_regularizer=None,
+               bias_regularizer=None,
+               activity_regularizer=None,
+               kernel_constraint=None,
+               bias_constraint=None,
+               **kwargs):
+    super(DenseEinsum, self).__init__(**kwargs)
+    self._output_shape = output_shape if isinstance(
+        output_shape, (list, tuple)) else (output_shape,)
+    self._activation = tf.keras.activations.get(activation)
+    self._use_bias = use_bias
+    self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)
+    self._bias_initializer = tf.keras.initializers.get(bias_initializer)
+    self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)
+    self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)
+    self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)
+    self._bias_constraint = tf.keras.constraints.get(bias_constraint)
+    self._num_summed_dimensions = num_summed_dimensions
+    self._einsum_string = None
+
+  def _build_einsum_string(self, free_input_dims, bound_dims, output_dims):
+    input_str = ""
+    kernel_str = ""
+    output_str = ""
+    letter_offset = 0
+    for i in range(free_input_dims):
+      char = _CHR_IDX[i + letter_offset]
+      input_str += char
+      output_str += char
+
+    letter_offset += free_input_dims
+    for i in range(bound_dims):
+      char = _CHR_IDX[i + letter_offset]
+      input_str += char
+      kernel_str += char
+
+    letter_offset += bound_dims
+    for i in range(output_dims):
+      char = _CHR_IDX[i + letter_offset]
+      kernel_str += char
+      output_str += char
+
+    return input_str + "," + kernel_str + "->" + output_str
+
+  def build(self, input_shape):
+    input_shape = tf.TensorShape(input_shape)
+    input_rank = input_shape.rank
+    free_input_dims = input_rank - self._num_summed_dimensions
+    output_dims = len(self._output_shape)
+
+    self._einsum_string = self._build_einsum_string(free_input_dims,
+                                                    self._num_summed_dimensions,
+                                                    output_dims)
+
+    # This is only saved for testing purposes.
+    self._kernel_shape = (
+        input_shape[free_input_dims:].concatenate(self._output_shape))
+
+    self._kernel = self.add_weight(
+        "kernel",
+        shape=self._kernel_shape,
+        initializer=self._kernel_initializer,
+        regularizer=self._kernel_regularizer,
+        constraint=self._kernel_constraint,
+        dtype=self.dtype,
+        trainable=True)
+    if self._use_bias:
+      self._bias = self.add_weight(
+          "bias",
+          shape=self._output_shape,
+          initializer=self._bias_initializer,
+          regularizer=self._bias_regularizer,
+          constraint=self._bias_constraint,
+          dtype=self.dtype,
+          trainable=True)
+    else:
+      self._bias = None
+    super(DenseEinsum, self).build(input_shape)
+
+  def compute_output_shape(self, input_shape):
+    input_shape = tf.TensorShape(input_shape)
+    input_shape = input_shape.with_rank_at_least(self._num_summed_dimensions +
+                                                 1)
+    for i in range(self._num_summed_dimensions):
+      if tf.dimension_value(input_shape[-1 * i]) is None:
+        raise ValueError(
+            "The %s dimension of input_shape must be defined, but saw: %s" %
+            (-1 * i, input_shape))
+    return input_shape[:-1 * self._num_summed_dimensions].concatenate(
+        self._units)
+
+  def get_config(self):
+    config = {
+        "output_shape":
+            self._output_shape,
+        "activation":
+            tf.keras.activations.serialize(self._activation),
+        "use_bias":
+            self._use_bias,
+        "kernel_initializer":
+            tf.keras.initializers.serialize(self._kernel_initializer),
+        "bias_initializer":
+            tf.keras.initializers.serialize(self._bias_initializer),
+        "kernel_regularizer":
+            tf.keras.regularizers.serialize(self._kernel_regularizer),
+        "bias_regularizer":
+            tf.keras.regularizers.serialize(self._bias_regularizer),
+        "activity_regularizer":
+            tf.keras.regularizers.serialize(self._activity_regularizer),
+        "kernel_constraint":
+            tf.keras.constraints.serialize(self._kernel_constraint),
+        "bias_constraint":
+            tf.keras.constraints.serialize(self._bias_constraint)
+    }
+    base_config = super(DenseEinsum, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def call(self, inputs):
+    ret = tf.einsum(self._einsum_string, inputs, self._kernel)
+    if self._use_bias:
+      ret += self._bias
+    if self._activation is not None:
+      ret = self._activation(ret)
+    return ret
diff --git a/official/nlp/modeling/layers/dense_einsum_test.py b/official/nlp/modeling/layers/dense_einsum_test.py
new file mode 100644
index 00000000..57a60fe5
--- /dev/null
+++ b/official/nlp/modeling/layers/dense_einsum_test.py
@@ -0,0 +1,123 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for Keras-based einsum layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import dense_einsum
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class DenseEinsumLayer(keras_parameterized.TestCase):
+
+  def test_3D_einsum_with_two_bound_dimensions(self):
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=(64,), num_summed_dimensions=2)
+    # Create a 4-dimensional input (the first dimension is implicit).
+    input_tensor = tf.keras.Input(shape=(None, 40, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(test_layer._einsum_string, "abcd,cde->abe")
+    self.assertEqual(test_layer._kernel_shape, (40, 80, 64))
+
+  def test_3D_einsum_with_one_bound_dimensions(self):
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=(64, 32), num_summed_dimensions=1)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(test_layer._einsum_string, "abc,cde->abde")
+    self.assertEqual(test_layer._kernel_shape, (80, 64, 32))
+
+  def test_2D_einsum_with_one_bound_dimensions(self):
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=(64,), num_summed_dimensions=1)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(test_layer._einsum_string, "abc,cd->abd")
+    self.assertEqual(test_layer._kernel_shape, (80, 64))
+
+  def test_bias_term_can_be_disabled(self):
+    # A layer created using the bias should have two weights.
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=64, num_summed_dimensions=1, use_bias=True)
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(2, len(test_layer.get_weights()))
+
+    # A layer created without the bias should have only one weight.
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=64, num_summed_dimensions=1, use_bias=False)
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(1, len(test_layer.get_weights()))
+
+  def test_activation(self):
+    # Create a model that does not use an activation.
+    no_activation_layer = dense_einsum.DenseEinsum(
+        output_shape=64, num_summed_dimensions=1, activation=None)
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    output_tensor = no_activation_layer(input_tensor)
+    no_activation_model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Create a model that uses a softmax activation.
+    activation_layer = dense_einsum.DenseEinsum(
+        output_shape=64, num_summed_dimensions=1, activation="softmax")
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    output_tensor = activation_layer(input_tensor)
+    activation_model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Make sure the models' weights are identical.
+    activation_model.set_weights(no_activation_model.get_weights())
+
+    # Predict using each model on the same input data. The output should be
+    # different, since one is using a softmax - even though the models' weights
+    # are the same.
+    input_values = 10 * np.random.random_sample((10, 4, 80))
+    non_activated_data = no_activation_model.predict(input_values)
+    activated_data = activation_model.predict(input_values)
+    self.assertNotAllClose(activated_data, non_activated_data)
+
+  def test_non_iterable_output_shape(self):
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=64, num_summed_dimensions=1)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    input_tensor = tf.keras.Input(shape=(None, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(test_layer._einsum_string, "abc,cd->abd")
+    self.assertEqual(test_layer._kernel_shape, (80, 64))
+
+  def test_with_explicit_initializer(self):
+    test_layer = dense_einsum.DenseEinsum(
+        output_shape=(64,),
+        num_summed_dimensions=2,
+        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))
+    # Create a 4-dimensional input (the first dimension is implicit).
+    input_tensor = tf.keras.Input(shape=(None, 40, 80))
+    _ = test_layer(input_tensor)
+    self.assertEqual(test_layer._einsum_string, "abcd,cde->abe")
+    self.assertEqual(test_layer._kernel_shape, (40, 80, 64))
+
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/official/nlp/modeling/layers/masked_softmax.py b/official/nlp/modeling/layers/masked_softmax.py
new file mode 100644
index 00000000..257b2ae9
--- /dev/null
+++ b/official/nlp/modeling/layers/masked_softmax.py
@@ -0,0 +1,61 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based softmax layer with optional masking."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class MaskedSoftmax(tf.keras.layers.Layer):
+  """Performs a softmax with optional masking on a tensor.
+
+  Attributes:
+    mask_expansion_axes: Any axes that should be padded on the mask tensor.
+  """
+
+  def __init__(self, mask_expansion_axes=None, **kwargs):
+    self._mask_expansion_axes = mask_expansion_axes
+    super(MaskedSoftmax, self).__init__(**kwargs)
+
+  def call(self, inputs):
+    if isinstance(inputs, list) and len(inputs) == 2:
+      scores, mask = inputs
+    else:
+      scores, mask = (inputs, None)
+
+    if mask is not None:
+      if self._mask_expansion_axes is not None:
+        mask = tf.expand_dims(mask, axis=self._mask_expansion_axes)
+
+      # Since attention_mask is 1.0 for positions we want to attend and 0.0 for
+      # masked positions, this operation will create a tensor which is 0.0 for
+      # positions we want to attend and -10000.0 for masked positions.
+      adder = (1.0 - tf.cast(mask, scores.dtype)) * -10000.0
+
+      # Since we are adding it to the raw scores before the softmax, this is
+      # effectively the same as removing these entirely.
+      scores += adder
+
+    return tf.nn.softmax(scores)
+
+  def get_config(self):
+    config = {'mask_expansion_axes': self._mask_expansion_axes}
+    base_config = super(MaskedSoftmax, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
diff --git a/official/nlp/modeling/layers/masked_softmax_test.py b/official/nlp/modeling/layers/masked_softmax_test.py
new file mode 100644
index 00000000..148f60cb
--- /dev/null
+++ b/official/nlp/modeling/layers/masked_softmax_test.py
@@ -0,0 +1,88 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for Keras-based masked softmax layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import masked_softmax
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class MaskedSoftmaxLayerTest(keras_parameterized.TestCase):
+
+  def test_non_masked_softmax(self):
+    test_layer = masked_softmax.MaskedSoftmax()
+    input_tensor = tf.keras.Input(shape=(4, 8))
+    output = test_layer(input_tensor)
+    model = tf.keras.Model(input_tensor, output)
+
+    input_data = 10 * np.random.random_sample((3, 4, 8))
+    output_data = model.predict(input_data)
+    expected_data = tf.nn.softmax(input_data)
+    self.assertAllClose(expected_data, output_data)
+
+  def test_masked_softmax(self):
+    test_layer = masked_softmax.MaskedSoftmax()
+    input_tensor = tf.keras.Input(shape=(4, 8))
+    mask_tensor = tf.keras.Input(shape=(4, 8))
+    output = test_layer([input_tensor, mask_tensor])
+    model = tf.keras.Model([input_tensor, mask_tensor], output)
+
+    input_data = 10 * np.random.random_sample((3, 4, 8))
+    mask_data = np.random.randint(2, size=(3, 4, 8))
+
+    output_data = model.predict([input_data, mask_data])
+    expected_zeros = np.greater(mask_data, 0)
+    is_zeros = np.greater(output_data, 0)
+    self.assertAllEqual(expected_zeros, is_zeros)
+
+  def test_masked_softmax_with_none_mask(self):
+    test_layer = masked_softmax.MaskedSoftmax()
+    input_tensor = tf.keras.Input(shape=(4, 8))
+    output = test_layer([input_tensor, None])
+    model = tf.keras.Model(input_tensor, output)
+
+    input_data = 10 * np.random.random_sample((3, 4, 8))
+    output_data = model.predict(input_data)
+    expected_data = tf.nn.softmax(input_data)
+    self.assertAllClose(expected_data, output_data)
+
+  def test_softmax_with_axes_expansion(self):
+    test_layer = masked_softmax.MaskedSoftmax(mask_expansion_axes=[1])
+    input_tensor = tf.keras.Input(shape=(4, 8))
+    mask_tensor = tf.keras.Input(shape=(8))
+    output = test_layer([input_tensor, mask_tensor])
+    model = tf.keras.Model([input_tensor, mask_tensor], output)
+
+    input_data = 10 * np.random.random_sample((3, 4, 8))
+    mask_data = np.random.randint(2, size=(3, 8))
+
+    output_data = model.predict([input_data, mask_data])
+    expanded_mask = np.expand_dims(mask_data, axis=1) * np.ones_like(input_data)
+    expected_zeros = np.greater(expanded_mask, 0)
+    is_zeros = np.greater(output_data, 0)
+    self.assertAllEqual(expected_zeros, is_zeros)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/layers/on_device_embedding.py b/official/nlp/modeling/layers/on_device_embedding.py
new file mode 100644
index 00000000..f2cb8667
--- /dev/null
+++ b/official/nlp/modeling/layers/on_device_embedding.py
@@ -0,0 +1,92 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based one-hot embedding layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from official.modeling import tf_utils
+
+
+@tf.keras.utils.register_keras_serializable(package="Text")
+class OnDeviceEmbedding(tf.keras.layers.Layer):
+  """Performs an embedding lookup suitable for accelerator devices.
+
+  This layer uses either tf.gather or tf.one_hot to translate integer indices to
+  float embeddings.
+
+  Attributes:
+    vocab_size: Number of elements in the vocabulary.
+    embedding_width: Output size of the embedding layer.
+    initializer: The initializer to use for the embedding weights. Defaults to
+      "glorot_uniform".
+    use_one_hot: Whether to use tf.one_hot over tf.gather for the embedding
+      lookup. Defaults to False (that is, using tf.gather). Setting this option
+      to True may improve performance, especially on small vocabulary sizes,
+      but will generally require more memory.
+  """
+
+  def __init__(self,
+               vocab_size,
+               embedding_width,
+               initializer="glorot_uniform",
+               use_one_hot=False,
+               **kwargs):
+    # We need to have a default dtype of float32, since the inputs (which Keras
+    # usually uses to infer the dtype) will always be int32.
+    if "dtype" not in kwargs:
+      kwargs["dtype"] = "float32"
+
+    super(OnDeviceEmbedding, self).__init__(**kwargs)
+    self._vocab_size = vocab_size
+    self._embedding_width = embedding_width
+    self._initializer = initializer
+    self._use_one_hot = use_one_hot
+
+  def get_config(self):
+    config = {
+        "vocab_size": self._vocab_size,
+        "embedding_width": self._embedding_width,
+        "initializer": self._initializer,
+        "use_one_hot": self._use_one_hot,
+    }
+    base_config = super(OnDeviceEmbedding, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def build(self, input_shape):
+    self.embeddings = self.add_weight(
+        "embeddings",
+        shape=[self._vocab_size, self._embedding_width],
+        initializer=self._initializer)
+
+    super(OnDeviceEmbedding, self).build(input_shape)
+
+  def call(self, inputs):
+    input_shape = tf_utils.get_shape_list(inputs, expected_rank=2)
+    input_shape.append(self._embedding_width)
+    flat_inputs = tf.reshape(inputs, [-1])
+    if self._use_one_hot:
+      one_hot_data = tf.one_hot(
+          flat_inputs, depth=self._vocab_size, dtype=self._dtype)
+      embeddings = tf.matmul(one_hot_data, self.embeddings)
+    else:
+      embeddings = tf.gather(self.embeddings, flat_inputs)
+    embeddings = tf.reshape(embeddings, input_shape)
+
+    return embeddings
diff --git a/official/nlp/modeling/layers/on_device_embedding_test.py b/official/nlp/modeling/layers/on_device_embedding_test.py
new file mode 100644
index 00000000..e2346f9a
--- /dev/null
+++ b/official/nlp/modeling/layers/on_device_embedding_test.py
@@ -0,0 +1,193 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for Keras-based one-hot embedding layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import on_device_embedding
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class OnDeviceEmbeddingTest(keras_parameterized.TestCase):
+
+  def test_layer_creation(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size, embedding_width=embedding_width)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # The output should be the same as the input, save that it has an extra
+    # embedding_width dimension on the end.
+    expected_output_shape = [None, sequence_length, embedding_width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    self.assertEqual(output_tensor.dtype, tf.float32)
+
+  def test_layer_creation_with_float16_dtype(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size, embedding_width=embedding_width, dtype="float16")
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # The output should be the same as the input, save that it has an extra
+    # embedding_width dimension on the end.
+    expected_output_shape = [None, sequence_length, embedding_width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    self.assertEqual(output_tensor.dtype, tf.float16)
+
+  def test_layer_invocation(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size, embedding_width=embedding_width)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # Create a model from the test layer.
+    model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 3
+    input_data = np.random.randint(
+        vocab_size, size=(batch_size, sequence_length))
+    output = model.predict(input_data)
+    self.assertEqual(tf.float32, output.dtype)
+
+  def test_layer_invocation_with_float16_dtype(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size, embedding_width=embedding_width, dtype="float16")
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # Create a model from the test layer.
+    model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 3
+    input_data = np.random.randint(
+        vocab_size, size=(batch_size, sequence_length))
+    output = model.predict(input_data)
+    self.assertEqual(tf.float16, output.dtype)
+
+  def test_one_hot_layer_creation(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size,
+        embedding_width=embedding_width,
+        use_one_hot=True)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # The output should be the same as the input, save that it has an extra
+    # embedding_width dimension on the end.
+    expected_output_shape = [None, sequence_length, embedding_width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    self.assertEqual(output_tensor.dtype, tf.float32)
+
+  def test_one_hot_layer_creation_with_float16_dtype(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size,
+        embedding_width=embedding_width,
+        dtype="float16",
+        use_one_hot=True)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # The output should be the same as the input, save that it has an extra
+    # embedding_width dimension on the end.
+    expected_output_shape = [None, sequence_length, embedding_width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    self.assertEqual(output_tensor.dtype, tf.float16)
+
+  def test_one_hot_layer_invocation(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size,
+        embedding_width=embedding_width,
+        use_one_hot=True)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # Create a model from the test layer.
+    model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 3
+    input_data = np.random.randint(
+        vocab_size, size=(batch_size, sequence_length))
+    output = model.predict(input_data)
+    self.assertEqual(tf.float32, output.dtype)
+
+  def test_one_hot_layer_invocation_with_float16_dtype(self):
+    vocab_size = 31
+    embedding_width = 27
+    test_layer = on_device_embedding.OnDeviceEmbedding(
+        vocab_size=vocab_size,
+        embedding_width=embedding_width,
+        dtype="float16",
+        use_one_hot=True)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    sequence_length = 23
+    input_tensor = tf.keras.Input(shape=(sequence_length), dtype=tf.int32)
+    output_tensor = test_layer(input_tensor)
+
+    # Create a model from the test layer.
+    model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 3
+    input_data = np.random.randint(
+        vocab_size, size=(batch_size, sequence_length))
+    output = model.predict(input_data)
+    self.assertEqual(tf.float16, output.dtype)
+
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/official/nlp/modeling/layers/position_embedding.py b/official/nlp/modeling/layers/position_embedding.py
new file mode 100644
index 00000000..f00e3d93
--- /dev/null
+++ b/official/nlp/modeling/layers/position_embedding.py
@@ -0,0 +1,125 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based positional embedding layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from official.modeling import tf_utils
+
+
+@tf.keras.utils.register_keras_serializable(package="Text")
+class PositionEmbedding(tf.keras.layers.Layer):
+  """Creates a positional embedding.
+
+  This layer creates a positional embedding as described in "BERT: Pre-training
+  of Deep Bidirectional Transformers for Language Understanding"
+  (https://arxiv.org/abs/1810.04805).
+
+  This layer can be set up to either create a statically shaped slice or a
+  dynamically shaped slice. If `use_dynamic_slicing` is True, the input tensor
+  can have a dynamic 1st dimension, while if `use_dynamic_slicing` is False the
+  input size must be fixed.
+
+  Attributes:
+    use_dynamic_slicing: Whether to use the dynamic slicing path.
+    max_sequence_length: The maximum size of the dynamic sequence. Only
+      applicable if `use_dynamic_slicing` is True.
+    initializer: The initializer to use for the embedding weights. Defaults to
+      "glorot_uniform".
+  """
+
+  def __init__(self,
+               initializer="glorot_uniform",
+               use_dynamic_slicing=False,
+               max_sequence_length=None,
+               **kwargs):
+    # We need to have a default dtype of float32, since the inputs (which Keras
+    # usually uses to infer the dtype) will always be int32.
+    if "dtype" not in kwargs:
+      kwargs["dtype"] = "float32"
+
+    super(PositionEmbedding, self).__init__(**kwargs)
+    if use_dynamic_slicing and max_sequence_length is None:
+      raise ValueError(
+          "If `use_dynamic_slicing` is True, `max_sequence_length` must be set."
+      )
+    self._max_sequence_length = max_sequence_length
+    self._initializer = tf.keras.initializers.get(initializer)
+    self._use_dynamic_slicing = use_dynamic_slicing
+
+  def get_config(self):
+    config = {
+        "max_sequence_length": self._max_sequence_length,
+        "initializer": tf.keras.initializers.serialize(self._initializer),
+        "use_dynamic_slicing": self._use_dynamic_slicing,
+    }
+    base_config = super(PositionEmbedding, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def build(self, input_shape):
+    """Implements build() for the layer."""
+    dimension_list = input_shape.as_list()
+
+    if len(dimension_list) != 3:
+      raise ValueError("PositionEmbedding expects a 3-dimensional input tensor "
+                       "of shape [batch, sequence, width]")
+    seq_length = dimension_list[1]
+    width = dimension_list[2]
+
+    # If we are not using dynamic slicing, we must assume that the sequence
+    # length is fixed and max_sequence_length should not be specified.
+    if not self._use_dynamic_slicing:
+      if seq_length is None:
+        raise ValueError(
+            "PositionEmbedding must have `use_dynamic_slicing` set "
+            "to True (and max_sequence_length set) when the "
+            "sequence (1st) dimension of the input is None.")
+      if self._max_sequence_length is not None:
+        raise ValueError(
+            "When `use_dynamic_slicing` is False, max_sequence_length should "
+            "not be specified and we ought to use seq_length to get the "
+            "variable shape.")
+
+    if self._max_sequence_length is not None:
+      weight_sequence_length = self._max_sequence_length
+    else:
+      weight_sequence_length = seq_length
+
+    self._position_embeddings = self.add_weight(
+        "embeddings",
+        shape=[weight_sequence_length, width],
+        initializer=self._initializer)
+
+    super(PositionEmbedding, self).build(input_shape)
+
+  def call(self, inputs):
+    """Implements call() for the layer."""
+    if self._use_dynamic_slicing:
+      input_shape = tf_utils.get_shape_list(inputs, expected_rank=3)
+      seq_length = input_shape[1]
+      width = input_shape[2]
+
+      position_embeddings = tf.expand_dims(
+          tf.slice(self._position_embeddings, [0, 0], [seq_length, width]),
+          axis=0)
+    else:
+      position_embeddings = tf.expand_dims(self._position_embeddings, axis=0)
+
+    return position_embeddings
diff --git a/official/nlp/modeling/layers/position_embedding_test.py b/official/nlp/modeling/layers/position_embedding_test.py
new file mode 100644
index 00000000..c8e7c746
--- /dev/null
+++ b/official/nlp/modeling/layers/position_embedding_test.py
@@ -0,0 +1,103 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for Keras-based positional embedding layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import position_embedding
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class PositionEmbeddingLayerTest(keras_parameterized.TestCase):
+
+  def test_static_layer_output_shape(self):
+    test_layer = position_embedding.PositionEmbedding()
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_length = 21
+    width = 30
+    input_tensor = tf.keras.Input(shape=(sequence_length, width))
+    output_tensor = test_layer(input_tensor)
+
+    # When using static positional embedding shapes, the output is expected
+    # to be the same as the input shape in all dimensions save batch.
+    expected_output_shape = [1, sequence_length, width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    # The default output dtype for this layer should be tf.float32.
+    self.assertEqual(tf.float32, output_tensor.dtype)
+
+  def test_float16_dtype(self):
+    test_layer = position_embedding.PositionEmbedding(dtype="float16")
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_length = 21
+    width = 30
+    input_tensor = tf.keras.Input(shape=(sequence_length, width))
+    output_tensor = test_layer(input_tensor)
+
+    # When using static positional embedding shapes, the output is expected
+    # to be the same as the input shape in all dimensions save batch.
+    expected_output_shape = [1, sequence_length, width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+    # The default output dtype for this layer should be tf.float32.
+    self.assertEqual(tf.float16, output_tensor.dtype)
+
+  def test_dynamic_layer_output_shape(self):
+    max_sequence_length = 40
+    test_layer = position_embedding.PositionEmbedding(
+        use_dynamic_slicing=True, max_sequence_length=max_sequence_length)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    width = 30
+    input_tensor = tf.keras.Input(shape=(None, width))
+    output_tensor = test_layer(input_tensor)
+
+    # When using dynamic positional embedding shapes, the output is expected
+    # to be the same as the input shape in all dimensions - but may be None if
+    # the input shape is None there.
+    expected_output_shape = [1, None, width]
+    self.assertEqual(expected_output_shape, output_tensor.shape.as_list())
+
+  def test_dynamic_layer_slicing(self):
+    max_sequence_length = 40
+    test_layer = position_embedding.PositionEmbedding(
+        use_dynamic_slicing=True, max_sequence_length=max_sequence_length)
+    # Create a 3-dimensional input (the first dimension is implicit).
+    width = 30
+    input_tensor = tf.keras.Input(shape=(None, width))
+    output_tensor = test_layer(input_tensor)
+
+    model = tf.keras.Model(input_tensor, output_tensor)
+
+    # Create input data that is shorter than max_sequence_length, which should
+    # trigger a down-slice.
+    input_length = 17
+    # Note: This test explicitly uses a batch size of 1. This is to get around
+    # Keras' restriction on Model invocations: inputs are expected to have the
+    # same batch cardinality as outputs. In practice, this layer should be used
+    # inside a model, where it can be projected when added to another tensor.
+    input_data = np.ones((1, input_length, width))
+    output_data = model.predict(input_data)
+
+    self.assertAllEqual([1, input_length, width], output_data.shape)
+
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/official/nlp/modeling/layers/transformer.py b/official/nlp/modeling/layers/transformer.py
new file mode 100644
index 00000000..d7dfdf34
--- /dev/null
+++ b/official/nlp/modeling/layers/transformer.py
@@ -0,0 +1,230 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Keras-based transformer block layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from official.nlp.modeling.layers import attention
+from official.nlp.modeling.layers import dense_einsum
+
+
+@tf.keras.utils.register_keras_serializable(package="Text")
+class Transformer(tf.keras.layers.Layer):
+  """Transformer layer.
+
+  This layer implements the Transformer from "Attention Is All You Need".
+  (https://arxiv.org/abs/1706.03762).
+
+  Attributes:
+    num_attention_heads: Number of attention heads.
+    intermediate_size: Size of the intermediate layer.
+    intermediate_activation: Activation for the intermediate layer.
+    dropout_rate: Dropout probability for the post-attention and output dropout.
+    attention_dropout_rate: Dropout probability for within the attention layer.
+    kernel_initializer: Initializer for dense layer kernels.
+    bias_initializer: Initializer for dense layer biases.
+    kernel_regularizer: Regularizer for dense layer kernels.
+    bias_regularizer: Regularizer for dense layer biases.
+    activity_regularizer: Regularizer for dense layer activity.
+    kernel_constraint: Constraint for dense layer kernels.
+    bias_constraint: Constraint for dense layer kernels.
+  """
+
+  def __init__(self,
+               num_attention_heads,
+               intermediate_size,
+               intermediate_activation,
+               dropout_rate=0.0,
+               attention_dropout_rate=0.0,
+               kernel_initializer="glorot_uniform",
+               bias_initializer="zeros",
+               kernel_regularizer=None,
+               bias_regularizer=None,
+               activity_regularizer=None,
+               kernel_constraint=None,
+               bias_constraint=None,
+               **kwargs):
+    super(Transformer, self).__init__(**kwargs)
+
+    self._num_heads = num_attention_heads
+    self._intermediate_size = intermediate_size
+    self._intermediate_activation = intermediate_activation
+    self._attention_dropout_rate = attention_dropout_rate
+    self._dropout_rate = dropout_rate
+    self._kernel_initializer = tf.keras.initializers.get(kernel_initializer)
+    self._bias_initializer = tf.keras.initializers.get(bias_initializer)
+    self._kernel_regularizer = tf.keras.regularizers.get(kernel_regularizer)
+    self._bias_regularizer = tf.keras.regularizers.get(bias_regularizer)
+    self._kernel_constraint = tf.keras.constraints.get(kernel_constraint)
+    self._bias_constraint = tf.keras.constraints.get(bias_constraint)
+
+  def build(self, input_shape):
+    input_tensor = input_shape[0] if len(input_shape) == 2 else input_shape
+    input_tensor_shape = tf.TensorShape(input_tensor)
+    if len(input_tensor_shape) != 3:
+      raise ValueError("TransformerLayer expects a three-dimensional input of "
+                       "shape [batch, sequence, width].")
+    batch_size, sequence_length, hidden_size = input_tensor_shape
+
+    if len(input_shape) == 2:
+      mask_tensor_shape = tf.TensorShape(input_shape[1])
+      expected_mask_tensor_shape = tf.TensorShape(
+          [batch_size, sequence_length, sequence_length])
+      if not expected_mask_tensor_shape.is_compatible_with(mask_tensor_shape):
+        raise ValueError("When passing a mask tensor to TransformerLayer, the "
+                         "mask tensor must be of shape [batch, "
+                         "sequence_length, sequence_length] (here %s). Got a "
+                         "mask tensor of shape %s." %
+                         (expected_mask_tensor_shape, mask_tensor_shape))
+    if hidden_size % self._num_heads != 0:
+      raise ValueError(
+          "The input size (%d) is not a multiple of the number of attention "
+          "heads (%d)" % (hidden_size, self._num_heads))
+    self._attention_head_size = int(hidden_size // self._num_heads)
+
+    self._attention_layer = attention.Attention(
+        num_heads=self._num_heads,
+        head_size=self._attention_head_size,
+        dropout_rate=self._attention_dropout_rate,
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="self_attention")
+    self._attention_output_dense = dense_einsum.DenseEinsum(
+        output_shape=hidden_size,
+        num_summed_dimensions=2,
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="self_attention_output")
+    self._attention_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
+    self._attention_layer_norm = (
+        tf.keras.layers.LayerNormalization(
+            name="self_attention_layer_norm", axis=-1, epsilon=1e-12))
+    self._intermediate_dense = dense_einsum.DenseEinsum(
+        output_shape=self._intermediate_size,
+        activation=self._intermediate_activation,
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=tf.float32,  # This layer is always float32 for numeric stability.
+        name="intermediate")
+    self._output_dense = dense_einsum.DenseEinsum(
+        output_shape=hidden_size,
+        kernel_initializer=self._kernel_initializer,
+        bias_initializer=self._bias_initializer,
+        kernel_regularizer=self._kernel_regularizer,
+        bias_regularizer=self._bias_regularizer,
+        activity_regularizer=self._activity_regularizer,
+        kernel_constraint=self._kernel_constraint,
+        bias_constraint=self._bias_constraint,
+        dtype=self.dtype,
+        name="output")
+    self._output_dropout = tf.keras.layers.Dropout(rate=self._dropout_rate)
+    self._output_layer_norm = tf.keras.layers.LayerNormalization(
+        name="output_layer_norm", axis=-1, epsilon=1e-12)
+
+    super(Transformer, self).build(input_shape)
+
+  def compute_output_shape(self, input_shape):
+    data_tensor_shape = tf.TensorShape(input_shape[0])
+    batch = data_tensor_shape[0]
+    sequence_length = data_tensor_shape[1]
+
+    return tf.TensorShape((batch, sequence_length, self._output_einsum_shape))
+
+  def get_config(self):
+    config = {
+        "num_attention_heads":
+            self._num_heads,
+        "intermediate_size":
+            self._intermediate_size,
+        "intermediate_activation":
+            self._intermediate_activation,
+        "dropout_rate":
+            self._dropout_rate,
+        "attention_dropout_rate":
+            self._attention_dropout_rate,
+        "kernel_initializer":
+            tf.keras.initializers.serialize(self._kernel_initializer),
+        "bias_initializer":
+            tf.keras.initializers.serialize(self._bias_initializer),
+        "kernel_regularizer":
+            tf.keras.regularizers.serialize(self._kernel_regularizer),
+        "bias_regularizer":
+            tf.keras.regularizers.serialize(self._bias_regularizer),
+        "activity_regularizer":
+            tf.keras.regularizers.serialize(self._activity_regularizer),
+        "kernel_constraint":
+            tf.keras.constraints.serialize(self._kernel_constraint),
+        "bias_constraint":
+            tf.keras.constraints.serialize(self._bias_constraint)
+    }
+    base_config = super(Transformer, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def call(self, inputs):
+    if isinstance(inputs, (list, tuple)) and len(inputs) == 2:
+      input_tensor, attention_mask = inputs
+    else:
+      input_tensor, attention_mask = (inputs, None)
+
+    attention_inputs = [input_tensor, input_tensor]
+
+    if attention_mask is not None:
+      attention_inputs.append(attention_mask)
+
+    attention_output = self._attention_layer(attention_inputs)
+    attention_output = self._attention_output_dense(attention_output)
+    attention_output = self._attention_dropout(attention_output)
+    # Use float32 in keras layer norm and the gelu activation in the
+    # intermediate dense layer for numeric stability
+    if self.dtype == tf.float16:
+      input_tensor = tf.cast(input_tensor, tf.float32)
+      attention_output = tf.cast(attention_output, tf.float32)
+    attention_output = self._attention_layer_norm(input_tensor +
+                                                  attention_output)
+    intermediate_output = self._intermediate_dense(attention_output)
+    if self.dtype == tf.float16:
+      intermediate_output = tf.cast(intermediate_output, tf.float16)
+    layer_output = self._output_dense(intermediate_output)
+    layer_output = self._output_dropout(layer_output)
+    # Use float32 in keras layer norm for numeric stability
+    if self.dtype == tf.float16:
+      layer_output = tf.cast(layer_output, tf.float32)
+    layer_output = self._output_layer_norm(layer_output + attention_output)
+    if self.dtype == tf.float16:
+      layer_output = tf.cast(layer_output, tf.float16)
+
+    return layer_output
diff --git a/official/nlp/modeling/layers/transformer_test.py b/official/nlp/modeling/layers/transformer_test.py
new file mode 100644
index 00000000..e86462c6
--- /dev/null
+++ b/official/nlp/modeling/layers/transformer_test.py
@@ -0,0 +1,168 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for Keras-based transformer block layer."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.layers import transformer
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class TransformerLayerTest(keras_parameterized.TestCase):
+
+  def test_layer_creation(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    output_tensor = test_layer(data_tensor)
+    # The default output of a transformer layer should be the same as the input.
+    self.assertEqual(data_tensor.shape.as_list(), output_tensor.shape.as_list())
+
+  def test_layer_creation_with_mask(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    # Create a 2-dimensional input (the first dimension is implicit).
+    mask_tensor = tf.keras.Input(shape=(sequence_length, sequence_length))
+    output_tensor = test_layer([data_tensor, mask_tensor])
+    # The default output of a transformer layer should be the same as the input.
+    self.assertEqual(data_tensor.shape.as_list(), output_tensor.shape.as_list())
+
+  def test_layer_creation_with_incorrect_mask_fails(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    # Create a 2-dimensional input (the first dimension is implicit).
+    mask_tensor = tf.keras.Input(shape=(sequence_length, sequence_length - 3))
+    with self.assertRaisesRegex(ValueError, 'When passing a mask tensor.*'):
+      _ = test_layer([data_tensor, mask_tensor])
+
+  def test_layer_invocation(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    output_tensor = test_layer(data_tensor)
+
+    # Create a model from the test layer.
+    model = tf.keras.Model(data_tensor, output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 6
+    input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, width))
+    _ = model.predict(input_data)
+
+  def test_layer_invocation_with_mask(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    # Create a 2-dimensional input (the first dimension is implicit).
+    mask_tensor = tf.keras.Input(shape=(sequence_length, sequence_length))
+    output_tensor = test_layer([data_tensor, mask_tensor])
+
+    # Create a model from the test layer.
+    model = tf.keras.Model([data_tensor, mask_tensor], output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 6
+    input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, width))
+    # The attention mask should be of shape (batch, from_seq_len, to_seq_len),
+    # which here is (batch, sequence_length, sequence_length)
+    mask_data = np.random.randint(
+        2, size=(batch_size, sequence_length, sequence_length))
+    _ = model.predict([input_data, mask_data])
+
+  def test_layer_invocation_with_float16_dtype(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu',
+        dtype='float16')
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(
+        shape=(sequence_length, width), dtype=tf.float16)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    mask_tensor = tf.keras.Input(shape=(sequence_length, sequence_length))
+    output_tensor = test_layer([data_tensor, mask_tensor])
+
+    # Create a model from the test layer.
+    model = tf.keras.Model([data_tensor, mask_tensor], output_tensor)
+
+    # Invoke the model on test data. We can't validate the output data itself
+    # (the NN is too complex) but this will rule out structural runtime errors.
+    batch_size = 6
+    input_data = (10 * np.random.random_sample(
+        (batch_size, sequence_length, width))).astype(np.float16)
+    # The attention mask should be of shape (batch, from_seq_len, to_seq_len),
+    # which here is (batch, sequence_length, sequence_length)
+    mask_data = np.random.randint(
+        2, size=(batch_size, sequence_length, sequence_length))
+    _ = model.predict([input_data, mask_data])
+
+  def test_transform_with_initializer(self):
+    test_layer = transformer.Transformer(
+        num_attention_heads=10,
+        intermediate_size=2048,
+        intermediate_activation='relu',
+        kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02))
+    sequence_length = 21
+    width = 80
+    # Create a 3-dimensional input (the first dimension is implicit).
+    data_tensor = tf.keras.Input(shape=(sequence_length, width))
+    output = test_layer(data_tensor)
+    # The default output of a transformer layer should be the same as the input.
+    self.assertEqual(data_tensor.shape.as_list(), output.shape.as_list())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/losses/__init__.py b/official/nlp/modeling/losses/__init__.py
new file mode 100644
index 00000000..919bad30
--- /dev/null
+++ b/official/nlp/modeling/losses/__init__.py
@@ -0,0 +1,17 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Activations package definition. Subject to change."""
+from official.nlp.modeling.losses.weighted_sparse_categorical_crossentropy import loss as weighted_sparse_categorical_crossentropy_loss
+from official.nlp.modeling.losses.weighted_sparse_categorical_crossentropy import per_example_loss as weighted_sparse_categorical_crossentropy_per_example_loss
diff --git a/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy.py b/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy.py
new file mode 100644
index 00000000..fc02ec80
--- /dev/null
+++ b/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy.py
@@ -0,0 +1,106 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Sparse categorical cross-entropy losses."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+
+def _adjust_labels(labels, predictions):
+  """Adjust the 'labels' tensor by squeezing it if needed."""
+  labels = tf.cast(labels, tf.int32)
+  if len(predictions.shape) == len(labels.shape):
+    labels = tf.squeeze(labels, [-1])
+  return labels, predictions
+
+
+def _validate_rank(labels, predictions, weights):
+  if weights is not None and len(weights.shape) != len(labels.shape):
+    raise RuntimeError(
+        ("Weight and label tensors were not of the same rank. weights.shape "
+         "was %s, and labels.shape was %s.") %
+        (predictions.shape, labels.shape))
+  if (len(predictions.shape) - 1) != len(labels.shape):
+    raise RuntimeError(
+        ("Weighted sparse categorical crossentropy expects `labels` to have a "
+         "rank of one less than `predictions`. labels.shape was %s, and "
+         "predictions.shape was %s.") % (labels.shape, predictions.shape))
+
+
+def per_example_loss(labels, predictions, weights=None):
+  """Calculate a per-example sparse categorical crossentropy loss.
+
+  This loss function assumes that the predictions are post-softmax.
+  Args:
+    labels: The labels to evaluate against. Should be a set of integer indices
+      ranging from 0 to (vocab_size-1).
+    predictions: The network predictions. Should have softmax already applied.
+    weights: An optional weight array of the same shape as the 'labels' array.
+      If None, all examples will be used.
+
+  Returns:
+    A tensor of shape predictions.shape[:-1] containing the per-example
+      loss.
+  """
+  # When using these functions with the Keras core API, we will need to squeeze
+  # the labels tensor - Keras adds a spurious inner dimension.
+  labels, predictions = _adjust_labels(labels, predictions)
+  _validate_rank(labels, predictions, weights)
+
+  labels_one_hot = tf.keras.backend.one_hot(labels, predictions.shape[-1])
+  labels_one_hot = tf.keras.backend.cast(labels_one_hot, predictions.dtype)
+  per_example_loss_data = -tf.keras.backend.sum(
+      predictions * labels_one_hot, axis=[-1])
+  if weights is not None:
+    weights = tf.keras.backend.cast(weights, per_example_loss_data.dtype)
+    per_example_loss_data = weights * per_example_loss_data
+  return per_example_loss_data
+
+
+def loss(labels, predictions, weights=None):
+  """Calculate a per-batch sparse categorical crossentropy loss.
+
+  This loss function assumes that the predictions are post-softmax.
+  Args:
+    labels: The labels to evaluate against. Should be a set of integer indices
+      ranging from 0 to (vocab_size-1).
+    predictions: The network predictions. Should have softmax already applied.
+    weights: An optional weight array of the same shape as the 'labels' array.
+      If None, all examples will be used.
+
+  Returns:
+    A loss scalar.
+
+  Raises:
+    RuntimeError if the passed tensors do not have the same rank.
+  """
+  # When using these functions with the Keras core API, we will need to squeeze
+  # the labels tensor - Keras adds a spurious inner dimension.
+  labels, predictions = _adjust_labels(labels, predictions)
+  _validate_rank(labels, predictions, weights)
+
+  per_example_loss_data = per_example_loss(labels, predictions, weights)
+
+  if weights is None:
+    return tf.keras.backend.mean(per_example_loss_data)
+  else:
+    numerator = tf.keras.backend.sum(per_example_loss_data)
+    weights = tf.keras.backend.cast(weights, predictions.dtype)
+    denominator = tf.keras.backend.sum(weights) + 1e-5
+    return numerator / denominator
diff --git a/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy_test.py b/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy_test.py
new file mode 100644
index 00000000..deb4d120
--- /dev/null
+++ b/official/nlp/modeling/losses/weighted_sparse_categorical_crossentropy_test.py
@@ -0,0 +1,381 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for masked LM loss."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling import networks
+from official.nlp.modeling.losses import weighted_sparse_categorical_crossentropy
+
+
+@keras_parameterized.run_all_keras_modes
+class ClassificationLossTest(keras_parameterized.TestCase):
+
+  def create_lm_model(self,
+                      vocab_size,
+                      sequence_length,
+                      hidden_size,
+                      num_predictions,
+                      output="predictions"):
+    # First, create a transformer stack that we can use to get the LM's
+    # vocabulary weight.
+    xformer_stack = networks.TransformerEncoder(
+        vocab_size=vocab_size,
+        num_layers=1,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_attention_heads=4,
+    )
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    lm_outputs, _ = xformer_stack([word_ids, mask, type_ids])
+
+    # Create a maskedLM from the transformer stack.
+    test_network = networks.MaskedLM(
+        num_predictions=num_predictions,
+        input_width=lm_outputs.shape[-1],
+        source_network=xformer_stack,
+        output=output)
+
+    # Create a model from the masked LM layer.
+    lm_input_tensor = tf.keras.Input(shape=(sequence_length, hidden_size))
+    masked_lm_positions = tf.keras.Input(
+        shape=(num_predictions,), dtype=tf.int32)
+    output = test_network([lm_input_tensor, masked_lm_positions])
+    return tf.keras.Model([lm_input_tensor, masked_lm_positions], output)
+
+  def create_classification_model(self, input_width, num_classes):
+    test_object = networks.Classification(
+        input_width=input_width, num_classes=num_classes)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    pooled_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(pooled_data)
+    return tf.keras.Model(pooled_data, output)
+
+  def test_per_example_loss_3d_input(self):
+    """Test per-example loss with a 3-dimensional input, from a masked LM."""
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    model = self.create_lm_model(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Get the output of the masked LM.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    output_data = model.predict([lm_input_data, masked_position_data])
+
+    # Calculate per-example loss.
+    labels = np.random.randint(vocab_size, size=(batch_size, num_predictions))
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels)
+
+    # Per-example loss data should have one value per prediction, and those
+    # values shouldn't be zero in this case (as we're using random data).
+    expected_shape = [batch_size, num_predictions]
+    self.assertEqual(expected_shape, per_example_loss_data.shape.as_list())
+    self.assertNotAllClose(
+        tf.zeros_like(per_example_loss_data), per_example_loss_data)
+
+  def test_per_example_loss_2d_input(self):
+    """Test per-example loss with a 2-d input, from a classifier."""
+    input_width = 512
+    num_classes = 10
+    model = self.create_classification_model(input_width, num_classes)
+
+    # Invoke the network as part of a Model.
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    output_data = model.predict(input_data)
+
+    # Calculate per example loss.
+    labels = np.random.randint(num_classes, size=(batch_size))
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels)
+
+    # Per-example loss data should have one value per batch item, and those
+    # values shouldn't be zero in this case (as we're using random data).
+    self.assertEqual([batch_size], per_example_loss_data.shape.as_list())
+    self.assertNotAllClose(
+        tf.zeros_like(per_example_loss_data), per_example_loss_data)
+
+  def test_per_example_loss_weights_3d_input(self):
+    """Test weighted per-example loss with a 3-d input, from a masked LM."""
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    model = self.create_lm_model(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Get the output of the masked LM.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    output_data = model.predict([lm_input_data, masked_position_data])
+
+    # Calculate per-example loss with weights.
+    labels = np.random.randint(vocab_size, size=(batch_size, num_predictions))
+    weights = np.random.randint(2, size=(batch_size, num_predictions))
+
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels, weights=weights)
+
+    # Weighted per-example loss data should be equivalent to multiplying the
+    # loss tensor by the weights tensor.
+    expected_weighted_loss = per_example_loss_data * weights
+    self.assertAllClose(expected_weighted_loss, per_example_loss_data)
+
+  def test_per_example_loss_weights_2d_input(self):
+    """Test weighted per-example loss with a 2-d input, from a classifier."""
+    input_width = 512
+    num_classes = 10
+    model = self.create_classification_model(input_width, num_classes)
+
+    # Invoke the network as part of a Model.
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    output_data = model.predict(input_data)
+
+    # Calculate per-example loss with weights.
+    labels = np.random.randint(num_classes, size=(batch_size))
+    weights = np.random.randint(2, size=(batch_size))
+
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels, weights=weights)
+
+    # Weighted per-example loss data should be equivalent to multiplying the
+    # loss tensor by the weights tensor.
+    expected_weighted_loss = per_example_loss_data * weights
+    self.assertAllClose(expected_weighted_loss, per_example_loss_data)
+
+  def test_loss_3d_input(self):
+    """Test overall loss with a 3-dimensional input, from a masked LM."""
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    model = self.create_lm_model(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Get the output of the masked LM.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    output_data = model.predict([lm_input_data, masked_position_data])
+
+    # Calculate loss.
+    labels = np.random.randint(vocab_size, size=(batch_size, num_predictions))
+    weights = np.random.randint(2, size=(batch_size, num_predictions))
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=weights)
+
+    # Total loss data should have one value, and that value shouldn't be zero
+    # in this case (as we're using random data).
+    expected_shape = []  # Scalar
+    self.assertEqual(expected_shape, per_example_loss_data.shape.as_list())
+    self.assertNotAllClose(
+        tf.zeros_like(per_example_loss_data), per_example_loss_data)
+
+  def test_loss_2d_input(self):
+    """Test overall loss with a 2-d input, from a classifier."""
+    input_width = 512
+    num_classes = 10
+    model = self.create_classification_model(input_width, num_classes)
+
+    # Invoke the network as part of a Model.
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    output_data = model.predict(input_data)
+
+    # Calculate per example loss.
+    labels = np.random.randint(num_classes, size=(batch_size))
+    loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels)
+
+    # Loss data should have one value only, and that value shouldn't be zero in
+    # this case (as we're using random data).
+    self.assertNotAllClose(0, loss_data)
+
+  def test_loss_weights_3d_input(self):
+    """Test masked loss with a 3-dimensional input, from a masked LM."""
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    model = self.create_lm_model(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Get the output of the masked LM.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    output_data = model.predict([lm_input_data, masked_position_data])
+
+    # Calculate a fully masked weight tensor. This should give a loss of zero.
+    labels = np.random.randint(vocab_size, size=(batch_size, num_predictions))
+    null_weights = np.zeros((batch_size, num_predictions))
+    weighted_loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=null_weights)
+
+    # Because the tensor is fully masked, the loss should be 0.
+    self.assertAllClose(0, weighted_loss_data)
+
+  def test_loss_weights_2d_input(self):
+    """Test masked loss with a 2-d input, from a classifier."""
+    input_width = 512
+    num_classes = 10
+    model = self.create_classification_model(input_width, num_classes)
+
+    # Invoke the network as part of a Model.
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    output_data = model.predict(input_data)
+
+    # Calculate a fully masked weight tensor. This should give a loss of zero.
+    labels = np.random.randint(num_classes, size=(batch_size))
+    null_weights = np.zeros((batch_size))
+    weighted_loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=null_weights)
+
+    # Because the tensor is fully masked, the loss should be 0.
+    self.assertAllClose(0, weighted_loss_data)
+
+  def test_mismatched_predictions_and_labels_ranks_squeezes(self):
+    """Test that the loss asserts when rank(predictions)-1 != rank(labels)."""
+    batch_size = 3
+    output_data = np.random.random_sample((batch_size, 10))
+    labels = np.random.randint(10, size=(batch_size, 1))
+
+    # All that this test tests is that the squeeze is successful.
+    _ = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels)
+
+  def test_mismatched_weights_and_labels_ranks_fail(self):
+    """Test that the loss asserts when rank(predictions) != rank(labels)."""
+    batch_size = 3
+    output_data = np.random.random_sample((batch_size, 10, 15))
+    labels = np.random.randint(10, size=(batch_size, 10))
+    weights = np.random.randint(2, size=(batch_size))
+
+    with self.assertRaisesRegex(RuntimeError, ".*of the same rank.*"):
+      _ = weighted_sparse_categorical_crossentropy.per_example_loss(
+          predictions=output_data, labels=labels, weights=weights)
+    with self.assertRaisesRegex(RuntimeError, ".*of the same rank.*"):
+      _ = weighted_sparse_categorical_crossentropy.loss(
+          predictions=output_data, labels=labels, weights=weights)
+
+  def test_tf_tensor_inputs(self):
+    """Test that tf.Tensors can be used as inputs to the loss function."""
+    batch_size = 3
+    output_data = tf.convert_to_tensor(
+        np.random.random_sample((batch_size, 10, 15)))
+    labels = tf.convert_to_tensor(np.random.randint(10, size=(batch_size, 10)))
+    weights = tf.convert_to_tensor(np.random.randint(2, size=(batch_size, 10)))
+
+    # We're not trying to validate numerical correctness, just ensure that
+    # we can in fact pass tensors to these functions without causing runtime
+    # errors from the shape checking code.
+    _ = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels, weights=weights)
+    _ = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=weights)
+
+  def test_legacy_lm_loss_compatibility(self):
+    """Test to validate computational correctness during refactors."""
+    # This is the empirical output of a masked LM with the following parameters:
+    #   batch_size = 3
+    #   vocab_size = 5
+    #   sequence_length = 4
+    #   num_predictions = 2
+    output_data = np.array(
+        [[[-2.5286622, -1.0963473, -1.4925185, -2.4451098, -1.2923571],
+          [-2.7117882, -1.1205841, -4.02187, -0.9966936, -1.5119683]],
+         [[-2.5379114, -0.82479054, -2.287932, -1.3747153, -2.053741],
+          [-2.5379114, -0.82479054, -2.287932, -1.3747153, -2.053741]],
+         [[-2.7760355, -1.8219438, -3.0924666, -1.0779881, -0.9407509],
+          [-2.7760355, -1.8219438, -3.0924666, -1.0779881, -0.9407509]]])
+    labels = np.array([[4, 0], [2, 2], [2, 1]])
+
+    # Validate that per_example loss calculations are the same.
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels)
+    expected_per_example_loss_data = [[1.2923571, 2.7117882],
+                                      [2.287932, 2.287932],
+                                      [3.0924666, 1.8219438]]
+    self.assertAllClose(expected_per_example_loss_data, per_example_loss_data)
+
+    # Validate that overall loss calculations are the same.
+    weights = np.array([[1, 0], [0, 0], [0, 0]])
+    loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=weights)
+    expected_loss_data = 1.2923441
+    self.assertAllClose(expected_loss_data, loss_data)
+
+  def test_legacy_classification_loss_compatibility(self):
+    """Test to validate computational correctness during refactors."""
+    # This is the empirical output of a classifier with the following params:
+    #   batch_size = 2
+    #   num_classes = 3
+    output_data = np.array([[-1.6094601e-03, -1.0966038e+01, -6.4434357e+00],
+                            [-1.6975292e-03, -6.4009643e+00, -1.0226612e+01]])
+    labels = np.array([2, 1])
+
+    # Validate that per_example loss calculations are the same.
+    per_example_loss_data = weighted_sparse_categorical_crossentropy.per_example_loss(
+        predictions=output_data, labels=labels)
+    expected_per_example_loss_data = [6.4434357, 6.4009643]
+    self.assertAllClose(expected_per_example_loss_data, per_example_loss_data)
+
+    # Validate that overall loss calculations are the same.
+    weights = None
+    loss_data = weighted_sparse_categorical_crossentropy.loss(
+        predictions=output_data, labels=labels, weights=weights)
+    expected_loss_data = 6.4222
+    self.assertAllClose(expected_loss_data, loss_data)
+
+
+if __name__ == "__main__":
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/__init__.py b/official/nlp/modeling/networks/__init__.py
new file mode 100644
index 00000000..2f0368b2
--- /dev/null
+++ b/official/nlp/modeling/networks/__init__.py
@@ -0,0 +1,19 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Networks package definition."""
+from official.nlp.modeling.networks.classification import Classification
+from official.nlp.modeling.networks.masked_lm import MaskedLM
+from official.nlp.modeling.networks.span_labeling import SpanLabeling
+from official.nlp.modeling.networks.transformer_encoder import TransformerEncoder
diff --git a/official/nlp/modeling/networks/bert_classifier.py b/official/nlp/modeling/networks/bert_classifier.py
new file mode 100644
index 00000000..8b181549
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_classifier.py
@@ -0,0 +1,91 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Trainer network for BERT-style models."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from official.nlp.modeling import networks
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class BertClassifier(tf.keras.Model):
+  """Classifier model based on a BERT-style transformer-based encoder.
+
+  This is an implementation of the network structure surrounding a transformer
+  encoder as described in "BERT: Pre-training of Deep Bidirectional Transformers
+  for Language Understanding" (https://arxiv.org/abs/1810.04805).
+
+  The BertClassifier allows a user to pass in a transformer stack, and
+  instantiates a classification network based on the passed `num_classes`
+  argument.
+
+  Attributes:
+    network: A transformer network. This network should output a sequence output
+      and a classification output. Furthermore, it should expose its embedding
+      table via a "get_embedding_table" method.
+    num_classes: Number of classes to predict from the classification network.
+    initializer: The initializer (if any) to use in the classification networks.
+      Defaults to a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               network,
+               num_classes,
+               initializer='glorot_uniform',
+               output='logits',
+               dropout_rate=0.1,
+               **kwargs):
+    self._self_setattr_tracking = False
+    self._config = {
+        'network': network,
+        'num_classes': num_classes,
+        'initializer': initializer,
+        'output': output,
+    }
+
+    # We want to use the inputs of the passed network as the inputs to this
+    # Model. To do this, we need to keep a handle to the network inputs for use
+    # when we construct the Model object at the end of init.
+    inputs = network.inputs
+
+    # Because we have a copy of inputs to create this Model object, we can
+    # invoke the Network object with its own input tensors to start the Model.
+    _, cls_output = network(inputs)
+    cls_output = tf.keras.layers.Dropout(rate=dropout_rate)(cls_output)
+
+    self.classifier = networks.Classification(
+        input_width=cls_output.shape[-1],
+        num_classes=num_classes,
+        initializer=initializer,
+        output=output,
+        name='classification')
+    predictions = self.classifier(cls_output)
+
+    super(BertClassifier, self).__init__(
+        inputs=inputs, outputs=predictions, **kwargs)
+
+  def get_config(self):
+    return self._config
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
diff --git a/official/nlp/modeling/networks/bert_classifier_test.py b/official/nlp/modeling/networks/bert_classifier_test.py
new file mode 100644
index 00000000..f3a880a8
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_classifier_test.py
@@ -0,0 +1,105 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for BERT trainer network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling import networks
+from official.nlp.modeling.networks import bert_classifier
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class BertClassifierTest(keras_parameterized.TestCase):
+
+  def test_bert_trainer(self):
+    """Validate that the Keras object can be created."""
+    # Build a transformer network to use within the BERT trainer.
+    vocab_size = 100
+    sequence_length = 512
+    test_network = networks.TransformerEncoder(
+        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)
+
+    # Create a BERT trainer with the created network.
+    num_classes = 3
+    bert_trainer_model = bert_classifier.BertClassifier(
+        test_network,
+        num_classes=num_classes)
+
+    # Create a set of 2-dimensional inputs (the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+
+    # Invoke the trainer model on the inputs. This causes the layer to be built.
+    cls_outs = bert_trainer_model([word_ids, mask, type_ids])
+
+    # Validate that the outputs are of the expected shape.
+    expected_classification_shape = [None, num_classes]
+    self.assertAllEqual(expected_classification_shape, cls_outs.shape.as_list())
+
+  def test_bert_trainer_tensor_call(self):
+    """Validate that the Keras object can be invoked."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=2)
+
+    # Create a BERT trainer with the created network.
+    bert_trainer_model = bert_classifier.BertClassifier(
+        test_network, num_classes=2)
+
+    # Create a set of 2-dimensional data tensors to feed into the model.
+    word_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+    mask = tf.constant([[1, 1], [1, 0]], dtype=tf.int32)
+    type_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+
+    # Invoke the trainer model on the tensors. In Eager mode, this does the
+    # actual calculation. (We can't validate the outputs, since the network is
+    # too complex: this simply ensures we're not hitting runtime errors.)
+    _ = bert_trainer_model([word_ids, mask, type_ids])
+
+  def test_serialize_deserialize(self):
+    """Validate that the BERT trainer can be serialized and deserialized."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=5)
+
+    # Create a BERT trainer with the created network. (Note that all the args
+    # are different, so we can catch any serialization mismatches.)
+    bert_trainer_model = bert_classifier.BertClassifier(
+        test_network, num_classes=4, initializer='zeros', output='predictions')
+
+    # Create another BERT trainer via serialization and deserialization.
+    config = bert_trainer_model.get_config()
+    new_bert_trainer_model = bert_classifier.BertClassifier.from_config(config)
+
+    # Validate that the config can be forced to JSON.
+    _ = new_bert_trainer_model.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(bert_trainer_model.get_config(),
+                        new_bert_trainer_model.get_config())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/bert_pretrainer.py b/official/nlp/modeling/networks/bert_pretrainer.py
new file mode 100644
index 00000000..f3439377
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_pretrainer.py
@@ -0,0 +1,127 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Trainer network for BERT-style models."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import copy
+import tensorflow as tf
+
+from official.nlp.modeling import networks
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class BertPretrainer(tf.keras.Model):
+  """BERT network training model.
+
+  This is an implementation of the network structure surrounding a transformer
+  encoder as described in "BERT: Pre-training of Deep Bidirectional Transformers
+  for Language Understanding" (https://arxiv.org/abs/1810.04805).
+
+  The BertTrainer allows a user to pass in a transformer stack, and instantiates
+  the masked language model and classification networks that are used to create
+  the training objectives.
+
+  Attributes:
+    network: A transformer network. This network should output a sequence output
+      and a classification output. Furthermore, it should expose its embedding
+      table via a "get_embedding_table" method.
+    num_classes: Number of classes to predict from the classification network.
+    num_token_predictions: Number of tokens to predict from the masked LM.
+    activation: The activation (if any) to use in the masked LM and
+      classification networks. If None, no activation will be used.
+    initializer: The initializer (if any) to use in the masked LM and
+      classification networks. Defaults to a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               network,
+               num_classes,
+               num_token_predictions,
+               activation=None,
+               output_activation=None,
+               initializer='glorot_uniform',
+               output='logits',
+               **kwargs):
+    self._self_setattr_tracking = False
+    self._config = {
+        'network': network,
+        'num_classes': num_classes,
+        'num_token_predictions': num_token_predictions,
+        'activation': activation,
+        'output_activation': output_activation,
+        'initializer': initializer,
+        'output': output,
+    }
+
+    # We want to use the inputs of the passed network as the inputs to this
+    # Model. To do this, we need to keep a copy of the network inputs for use
+    # when we construct the Model object at the end of init. (We keep a copy
+    # because we'll be adding another tensor to the copy later.)
+    network_inputs = network.inputs
+    inputs = copy.copy(network_inputs)
+
+    # Because we have a copy of inputs to create this Model object, we can
+    # invoke the Network object with its own input tensors to start the Model.
+    # Note that, because of how deferred construction happens, we can't use
+    # the copy of the list here - by the time the network is invoked, the list
+    # object contains the additional input added below.
+    sequence_output, cls_output = network(network_inputs)
+
+    sequence_output_length = sequence_output.shape.as_list()[1]
+    if sequence_output_length < num_token_predictions:
+      raise ValueError(
+          "The passed network's output length is %s, which is less than the "
+          'requested num_token_predictions %s.' %
+          (sequence_output_length, num_token_predictions))
+
+    masked_lm_positions = tf.keras.layers.Input(
+        shape=(num_token_predictions,),
+        name='masked_lm_positions',
+        dtype=tf.int32)
+    inputs.append(masked_lm_positions)
+
+    self.masked_lm = networks.MaskedLM(
+        num_predictions=num_token_predictions,
+        input_width=sequence_output.shape[-1],
+        source_network=network,
+        activation=activation,
+        initializer=initializer,
+        output=output,
+        name='masked_lm')
+    lm_outputs = self.masked_lm([sequence_output, masked_lm_positions])
+
+    self.classification = networks.Classification(
+        input_width=cls_output.shape[-1],
+        num_classes=num_classes,
+        initializer=initializer,
+        output=output,
+        name='classification')
+    sentence_outputs = self.classification(cls_output)
+
+    super(BertPretrainer, self).__init__(
+        inputs=inputs, outputs=[lm_outputs, sentence_outputs], **kwargs)
+
+  def get_config(self):
+    return self._config
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
diff --git a/official/nlp/modeling/networks/bert_pretrainer_test.py b/official/nlp/modeling/networks/bert_pretrainer_test.py
new file mode 100644
index 00000000..f4f91f0d
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_pretrainer_test.py
@@ -0,0 +1,111 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for BERT trainer network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling import networks
+from official.nlp.modeling.networks import bert_pretrainer
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class BertPretrainerTest(keras_parameterized.TestCase):
+
+  def test_bert_trainer(self):
+    """Validate that the Keras object can be created."""
+    # Build a transformer network to use within the BERT trainer.
+    vocab_size = 100
+    sequence_length = 512
+    test_network = networks.TransformerEncoder(
+        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)
+
+    # Create a BERT trainer with the created network.
+    num_classes = 3
+    num_token_predictions = 2
+    bert_trainer_model = bert_pretrainer.BertPretrainer(
+        test_network,
+        num_classes=num_classes,
+        num_token_predictions=num_token_predictions)
+
+    # Create a set of 2-dimensional inputs (the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    lm_mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+
+    # Invoke the trainer model on the inputs. This causes the layer to be built.
+    lm_outs, cls_outs = bert_trainer_model([word_ids, mask, type_ids, lm_mask])
+
+    # Validate that the outputs are of the expected shape.
+    expected_lm_shape = [None, num_token_predictions, vocab_size]
+    expected_classification_shape = [None, num_classes]
+    self.assertAllEqual(expected_lm_shape, lm_outs.shape.as_list())
+    self.assertAllEqual(expected_classification_shape, cls_outs.shape.as_list())
+
+  def test_bert_trainer_tensor_call(self):
+    """Validate that the Keras object can be invoked."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=2)
+
+    # Create a BERT trainer with the created network.
+    bert_trainer_model = bert_pretrainer.BertPretrainer(
+        test_network, num_classes=2, num_token_predictions=2)
+
+    # Create a set of 2-dimensional data tensors to feed into the model.
+    word_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+    mask = tf.constant([[1, 1], [1, 0]], dtype=tf.int32)
+    type_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+    lm_mask = tf.constant([[1, 1], [1, 0]], dtype=tf.int32)
+
+    # Invoke the trainer model on the tensors. In Eager mode, this does the
+    # actual calculation. (We can't validate the outputs, since the network is
+    # too complex: this simply ensures we're not hitting runtime errors.)
+    _, _ = bert_trainer_model([word_ids, mask, type_ids, lm_mask])
+
+  def test_serialize_deserialize(self):
+    """Validate that the BERT trainer can be serialized and deserialized."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=5)
+
+    # Create a BERT trainer with the created network. (Note that all the args
+    # are different, so we can catch any serialization mismatches.)
+    bert_trainer_model = bert_pretrainer.BertPretrainer(
+        test_network, num_classes=4, num_token_predictions=3)
+
+    # Create another BERT trainer via serialization and deserialization.
+    config = bert_trainer_model.get_config()
+    new_bert_trainer_model = bert_pretrainer.BertPretrainer.from_config(config)
+
+    # Validate that the config can be forced to JSON.
+    _ = new_bert_trainer_model.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(bert_trainer_model.get_config(),
+                        new_bert_trainer_model.get_config())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/bert_span_labeler.py b/official/nlp/modeling/networks/bert_span_labeler.py
new file mode 100644
index 00000000..71a261b9
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_span_labeler.py
@@ -0,0 +1,97 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Trainer network for BERT-style models."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from official.nlp.modeling import networks
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class BertSpanLabeler(tf.keras.Model):
+  """Span labeler model based on a BERT-style transformer-based encoder.
+
+  This is an implementation of the network structure surrounding a transformer
+  encoder as described in "BERT: Pre-training of Deep Bidirectional Transformers
+  for Language Understanding" (https://arxiv.org/abs/1810.04805).
+
+  The BertSpanLabeler allows a user to pass in a transformer stack, and
+  instantiates a span labeling network based on a single dense layer.
+
+  Attributes:
+    network: A transformer network. This network should output a sequence output
+      and a classification output. Furthermore, it should expose its embedding
+      table via a "get_embedding_table" method.
+    initializer: The initializer (if any) to use in the span labeling network.
+      Defaults to a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               network,
+               initializer='glorot_uniform',
+               output='logits',
+               **kwargs):
+    self._self_setattr_tracking = False
+    self._config = {
+        'network': network,
+        'initializer': initializer,
+        'output': output,
+    }
+    # We want to use the inputs of the passed network as the inputs to this
+    # Model. To do this, we need to keep a handle to the network inputs for use
+    # when we construct the Model object at the end of init.
+    inputs = network.inputs
+
+    # Because we have a copy of inputs to create this Model object, we can
+    # invoke the Network object with its own input tensors to start the Model.
+    sequence_output, _ = network(inputs)
+
+    # This is an instance variable for ease of access to the underlying task
+    # network.
+    self.span_labeling = networks.SpanLabeling(
+        input_width=sequence_output.shape[-1],
+        initializer=initializer,
+        output=output,
+        name='span_labeling')
+    start_logits, end_logits = self.span_labeling(sequence_output)
+
+    # Use identity layers wrapped in lambdas to explicitly name the output
+    # tensors. This allows us to use string-keyed dicts in Keras fit/predict/
+    # evaluate calls.
+    start_logits = tf.keras.layers.Lambda(
+        tf.identity, name='start_positions')(
+            start_logits)
+    end_logits = tf.keras.layers.Lambda(
+        tf.identity, name='end_positions')(
+            end_logits)
+
+    logits = [start_logits, end_logits]
+
+    super(BertSpanLabeler, self).__init__(
+        inputs=inputs, outputs=logits, **kwargs)
+
+  def get_config(self):
+    return self._config
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
diff --git a/official/nlp/modeling/networks/bert_span_labeler_test.py b/official/nlp/modeling/networks/bert_span_labeler_test.py
new file mode 100644
index 00000000..c761a655
--- /dev/null
+++ b/official/nlp/modeling/networks/bert_span_labeler_test.py
@@ -0,0 +1,124 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for BERT trainer network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling import networks
+from official.nlp.modeling.networks import bert_span_labeler
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class BertSpanLabelerTest(keras_parameterized.TestCase):
+
+  def test_bert_trainer(self):
+    """Validate that the Keras object can be created."""
+    # Build a transformer network to use within the BERT trainer.
+    vocab_size = 100
+    sequence_length = 512
+    test_network = networks.TransformerEncoder(
+        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)
+
+    # Create a BERT trainer with the created network.
+    bert_trainer_model = bert_span_labeler.BertSpanLabeler(test_network)
+
+    # Create a set of 2-dimensional inputs (the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+
+    # Invoke the trainer model on the inputs. This causes the layer to be built.
+    cls_outs = bert_trainer_model([word_ids, mask, type_ids])
+
+    # Validate that there are 2 outputs are of the expected shape.
+    self.assertEqual(2, len(cls_outs))
+    expected_shape = [None, sequence_length]
+    for out in cls_outs:
+      self.assertAllEqual(expected_shape, out.shape.as_list())
+
+  def test_bert_trainer_named_compilation(self):
+    """Validate compilation using explicit output names."""
+    # Build a transformer network to use within the BERT trainer.
+    vocab_size = 100
+    sequence_length = 512
+    test_network = networks.TransformerEncoder(
+        vocab_size=vocab_size, num_layers=2, sequence_length=sequence_length)
+
+    # Create a BERT trainer with the created network.
+    bert_trainer_model = bert_span_labeler.BertSpanLabeler(test_network)
+
+    # Attempt to compile the model using a string-keyed dict of output names to
+    # loss functions. This will validate that the outputs are named as we
+    # expect.
+    bert_trainer_model.compile(
+        optimizer='sgd',
+        loss={
+            'start_positions': 'mse',
+            'end_positions': 'mse'
+        })
+
+  def test_bert_trainer_tensor_call(self):
+    """Validate that the Keras object can be invoked."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=2)
+
+    # Create a BERT trainer with the created network.
+    bert_trainer_model = bert_span_labeler.BertSpanLabeler(test_network)
+
+    # Create a set of 2-dimensional data tensors to feed into the model.
+    word_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+    mask = tf.constant([[1, 1], [1, 0]], dtype=tf.int32)
+    type_ids = tf.constant([[1, 1], [2, 2]], dtype=tf.int32)
+
+    # Invoke the trainer model on the tensors. In Eager mode, this does the
+    # actual calculation. (We can't validate the outputs, since the network is
+    # too complex: this simply ensures we're not hitting runtime errors.)
+    _ = bert_trainer_model([word_ids, mask, type_ids])
+
+  def test_serialize_deserialize(self):
+    """Validate that the BERT trainer can be serialized and deserialized."""
+    # Build a transformer network to use within the BERT trainer. (Here, we use
+    # a short sequence_length for convenience.)
+    test_network = networks.TransformerEncoder(
+        vocab_size=100, num_layers=2, sequence_length=5)
+
+    # Create a BERT trainer with the created network. (Note that all the args
+    # are different, so we can catch any serialization mismatches.)
+    bert_trainer_model = bert_span_labeler.BertSpanLabeler(test_network)
+
+    # Create another BERT trainer via serialization and deserialization.
+    config = bert_trainer_model.get_config()
+    new_bert_trainer_model = bert_span_labeler.BertSpanLabeler.from_config(
+        config)
+
+    # Validate that the config can be forced to JSON.
+    _ = new_bert_trainer_model.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(bert_trainer_model.get_config(),
+                        new_bert_trainer_model.get_config())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/classification.py b/official/nlp/modeling/networks/classification.py
new file mode 100644
index 00000000..bf9bf985
--- /dev/null
+++ b/official/nlp/modeling/networks/classification.py
@@ -0,0 +1,86 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Classification network."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+# pylint: disable=g-direct-tensorflow-import
+from tensorflow.python.keras.engine import network
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class Classification(network.Network):
+  """Classification network head for BERT modeling.
+
+  This network implements a simple classifier head based on a dense layer.
+
+  Attributes:
+    input_width: The innermost dimension of the input tensor to this network.
+    num_classes: The number of classes that this network should classify to.
+    activation: The activation, if any, for the dense layer in this network.
+    initializer: The intializer for the dense layer in this network. Defaults to
+      a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               input_width,
+               num_classes,
+               initializer='glorot_uniform',
+               output='logits',
+               **kwargs):
+    self._self_setattr_tracking = False
+    self._config_dict = {
+        'input_width': input_width,
+        'num_classes': num_classes,
+        'initializer': initializer,
+        'output': output,
+    }
+
+    cls_output = tf.keras.layers.Input(
+        shape=(input_width,), name='cls_output', dtype=tf.float32)
+
+    self.logits = tf.keras.layers.Dense(
+        num_classes,
+        activation=None,
+        kernel_initializer=initializer,
+        name='predictions/transform/logits')(
+            cls_output)
+    predictions = tf.keras.layers.Activation(tf.nn.log_softmax)(self.logits)
+
+    if output == 'logits':
+      output_tensors = self.logits
+    elif output == 'predictions':
+      output_tensors = predictions
+    else:
+      raise ValueError(
+          ('Unknown `output` value "%s". `output` can be either "logits" or '
+           '"predictions"') % output)
+
+    super(Classification, self).__init__(
+        inputs=[cls_output], outputs=output_tensors, **kwargs)
+
+  def get_config(self):
+    return self._config_dict
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
diff --git a/official/nlp/modeling/networks/classification_test.py b/official/nlp/modeling/networks/classification_test.py
new file mode 100644
index 00000000..6f710747
--- /dev/null
+++ b/official/nlp/modeling/networks/classification_test.py
@@ -0,0 +1,179 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for classification network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.networks import classification
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class ClassificationTest(keras_parameterized.TestCase):
+
+  def test_network_creation(self):
+    """Validate that the Keras object can be created."""
+    input_width = 512
+    num_classes = 10
+    test_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes)
+    # Create a 2-dimensional input (the first dimension is implicit).
+    cls_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(cls_data)
+
+    # Validate that the outputs are of the expected shape.
+    expected_output_shape = [None, num_classes]
+    self.assertEqual(expected_output_shape, output.shape.as_list())
+
+  def test_network_invocation(self):
+    """Validate that the Keras object can be invoked."""
+    input_width = 512
+    num_classes = 10
+    test_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes, output='predictions')
+    # Create a 2-dimensional input (the first dimension is implicit).
+    cls_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(cls_data)
+
+    # Invoke the network as part of a Model.
+    model = tf.keras.Model(cls_data, output)
+    input_data = 10 * np.random.random_sample((3, input_width))
+    _ = model.predict(input_data)
+
+  def test_network_invocation_with_internal_logits(self):
+    """Validate that the logit outputs are correct."""
+    input_width = 512
+    num_classes = 10
+    test_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes, output='predictions')
+
+    # Create a 2-dimensional input (the first dimension is implicit).
+    cls_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(cls_data)
+    model = tf.keras.Model(cls_data, output)
+    logits_model = tf.keras.Model(test_object.inputs, test_object.logits)
+
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    outputs = model.predict(input_data)
+    logits = logits_model.predict(input_data)
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, num_classes)
+    self.assertEqual(expected_output_shape, outputs.shape)
+    self.assertEqual(expected_output_shape, logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    calculated_softmax = softmax_model.predict(logits)
+    self.assertAllClose(outputs, calculated_softmax)
+
+  def test_network_invocation_with_internal_and_external_logits(self):
+    """Validate that the logit outputs are correct."""
+    input_width = 512
+    num_classes = 10
+    test_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes, output='logits')
+
+    # Create a 2-dimensional input (the first dimension is implicit).
+    cls_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(cls_data)
+    model = tf.keras.Model(cls_data, output)
+    logits_model = tf.keras.Model(test_object.inputs, test_object.logits)
+
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    outputs = model.predict(input_data)
+    logits = logits_model.predict(input_data)
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, num_classes)
+    self.assertEqual(expected_output_shape, outputs.shape)
+    self.assertEqual(expected_output_shape, logits.shape)
+
+    self.assertAllClose(outputs, logits)
+
+  def test_network_invocation_with_logit_output(self):
+    """Validate that the logit outputs are correct."""
+    input_width = 512
+    num_classes = 10
+    test_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes, output='predictions')
+    logit_object = classification.Classification(
+        input_width=input_width, num_classes=num_classes, output='logits')
+    logit_object.set_weights(test_object.get_weights())
+
+    # Create a 2-dimensional input (the first dimension is implicit).
+    cls_data = tf.keras.Input(shape=(input_width,), dtype=tf.float32)
+    output = test_object(cls_data)
+    logit_output = logit_object(cls_data)
+
+    model = tf.keras.Model(cls_data, output)
+    logits_model = tf.keras.Model(cls_data, logit_output)
+
+    batch_size = 3
+    input_data = 10 * np.random.random_sample((batch_size, input_width))
+    outputs = model.predict(input_data)
+    logits = logits_model.predict(input_data)
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, num_classes)
+    self.assertEqual(expected_output_shape, outputs.shape)
+    self.assertEqual(expected_output_shape, logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    calculated_softmax = softmax_model.predict(logits)
+    self.assertAllClose(outputs, calculated_softmax)
+
+  def test_serialize_deserialize(self):
+    # Create a network object that sets all of its config options.
+    network = classification.Classification(
+        input_width=128,
+        num_classes=10,
+        initializer='zeros',
+        output='predictions')
+
+    # Create another network object from the first object's config.
+    new_network = classification.Classification.from_config(
+        network.get_config())
+
+    # Validate that the config can be forced to JSON.
+    _ = new_network.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(network.get_config(), new_network.get_config())
+
+  def test_unknown_output_type_fails(self):
+    with self.assertRaisesRegex(ValueError, 'Unknown `output` value "bad".*'):
+      _ = classification.Classification(
+          input_width=128, num_classes=10, output='bad')
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/masked_lm.py b/official/nlp/modeling/networks/masked_lm.py
new file mode 100644
index 00000000..a8eadff2
--- /dev/null
+++ b/official/nlp/modeling/networks/masked_lm.py
@@ -0,0 +1,189 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Masked language model network."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from tensorflow.python.keras.engine import network  # pylint: disable=g-direct-tensorflow-import
+from official.modeling import tf_utils
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class MaskedLM(network.Network):
+  """Masked language model network head for BERT modeling.
+
+  This network implements a masked language model based on the provided network.
+  It assumes that the network being passed has a "get_embedding_table()" method.
+
+  Attributes:
+    input_width: The innermost dimension of the input tensor to this network.
+    num_predictions: The number of predictions to make per sequence.
+    source_network: The network with the embedding layer to use for the
+      embedding layer.
+    activation: The activation, if any, for the dense layer in this network.
+    initializer: The intializer for the dense layer in this network. Defaults to
+      a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               input_width,
+               num_predictions,
+               source_network,
+               activation=None,
+               initializer='glorot_uniform',
+               output='logits',
+               **kwargs):
+
+    embedding_table = source_network.get_embedding_table()
+    vocab_size, hidden_size = embedding_table.shape
+
+    sequence_data = tf.keras.layers.Input(
+        shape=(None, input_width), name='sequence_data', dtype=tf.float32)
+    masked_lm_positions = tf.keras.layers.Input(
+        shape=(num_predictions,), name='masked_lm_positions', dtype=tf.int32)
+
+    masked_lm_input = tf.keras.layers.Lambda(
+        lambda x: self._gather_indexes(x[0], x[1]))(
+            [sequence_data, masked_lm_positions])
+    lm_data = (
+        tf.keras.layers.Dense(
+            hidden_size,
+            activation=activation,
+            kernel_initializer=initializer,
+            name='cls/predictions/transform/dense')(masked_lm_input))
+    lm_data = tf.keras.layers.LayerNormalization(
+        axis=-1, epsilon=1e-12, name='cls/predictions/transform/LayerNorm')(
+            lm_data)
+    lm_data = tf.keras.layers.Lambda(
+        lambda x: tf.matmul(x, embedding_table, transpose_b=True))(
+            lm_data)
+    logits = Bias(
+        initializer=tf.keras.initializers.Zeros(),
+        name='cls/predictions/output_bias')(
+            lm_data)
+
+    # We can't use the standard Keras reshape layer here, since it expects
+    # the input and output batch size to be the same.
+    reshape_layer = tf.keras.layers.Lambda(
+        lambda x: tf.reshape(x, [-1, num_predictions, vocab_size]))
+
+    self.logits = reshape_layer(logits)
+    predictions = tf.keras.layers.Activation(tf.nn.log_softmax)(self.logits)
+
+    if output == 'logits':
+      output_tensors = self.logits
+    elif output == 'predictions':
+      output_tensors = predictions
+    else:
+      raise ValueError(
+          ('Unknown `output` value "%s". `output` can be either "logits" or '
+           '"predictions"') % output)
+
+    super(MaskedLM, self).__init__(
+        inputs=[sequence_data, masked_lm_positions],
+        outputs=output_tensors,
+        **kwargs)
+
+  def get_config(self):
+    raise NotImplementedError('MaskedLM cannot be directly serialized at this '
+                              'time. Please use it only in Layers or '
+                              'functionally subclassed Models/Networks.')
+
+  def _gather_indexes(self, sequence_tensor, positions):
+    """Gathers the vectors at the specific positions.
+
+    Args:
+        sequence_tensor: Sequence output of `BertModel` layer of shape
+          (`batch_size`, `seq_length`, num_hidden) where num_hidden is number of
+          hidden units of `BertModel` layer.
+        positions: Positions ids of tokens in sequence to mask for pretraining
+          of with dimension (batch_size, num_predictions) where
+          `num_predictions` is maximum number of tokens to mask out and predict
+          per each sequence.
+
+    Returns:
+        Masked out sequence tensor of shape (batch_size * num_predictions,
+        num_hidden).
+    """
+    sequence_shape = tf_utils.get_shape_list(
+        sequence_tensor, name='sequence_output_tensor')
+    batch_size, seq_length, width = sequence_shape
+
+    flat_offsets = tf.keras.backend.reshape(
+        tf.range(0, batch_size, dtype=tf.int32) * seq_length, [-1, 1])
+    flat_positions = tf.keras.backend.reshape(positions + flat_offsets, [-1])
+    flat_sequence_tensor = tf.keras.backend.reshape(
+        sequence_tensor, [batch_size * seq_length, width])
+    output_tensor = tf.gather(flat_sequence_tensor, flat_positions)
+
+    return output_tensor
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+# Temporary until we can create a Dense layer that ties the embedding.
+class Bias(tf.keras.layers.Layer):
+  """Adds a bias term to an input."""
+
+  def __init__(self,
+               initializer='zeros',
+               regularizer=None,
+               constraint=None,
+               activation=None,
+               **kwargs):
+    super(Bias, self).__init__(**kwargs)
+    self._initializer = tf.keras.initializers.get(initializer)
+    self._regularizer = tf.keras.regularizers.get(regularizer)
+    self._constraint = tf.keras.constraints.get(constraint)
+    self._activation = tf.keras.activations.get(activation)
+
+  def build(self, input_shape):
+    input_shape = tf.TensorShape(input_shape)
+    self._bias = self.add_weight(
+        'bias',
+        shape=input_shape[1:],
+        initializer=self._initializer,
+        regularizer=self._regularizer,
+        constraint=self._constraint,
+        dtype=self._dtype,
+        trainable=True)
+
+    super(Bias, self).build(input_shape)
+
+  def compute_output_shape(self, input_shape):
+    return input_shape
+
+  def get_config(self):
+    config = {
+        'activation': tf.keras.activations.serialize(self._activation),
+        'initializer': tf.keras.initializers.serialize(self._initializer),
+        'regularizer': tf.keras.regularizers.serialize(self._regularizer),
+        'constraint': tf.keras.constraints.serialize(self._constraint)
+    }
+    base_config = super(Bias, self).get_config()
+    return dict(list(base_config.items()) + list(config.items()))
+
+  def call(self, inputs):
+    outputs = tf.nn.bias_add(inputs, self._bias)
+    if self._activation is not None:
+      return self._activation(outputs)  # pylint: disable=not-callable
+    else:
+      return outputs
diff --git a/official/nlp/modeling/networks/masked_lm_test.py b/official/nlp/modeling/networks/masked_lm_test.py
new file mode 100644
index 00000000..2b7b382c
--- /dev/null
+++ b/official/nlp/modeling/networks/masked_lm_test.py
@@ -0,0 +1,227 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for masked language model network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+
+from official.nlp.modeling.networks import masked_lm
+from official.nlp.modeling.networks import transformer_encoder
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class MaskedLMTest(keras_parameterized.TestCase):
+
+  def create_network(self,
+                     vocab_size,
+                     sequence_length,
+                     hidden_size,
+                     num_predictions,
+                     output='predictions',
+                     xformer_stack=None):
+    # First, create a transformer stack that we can use to get the LM's
+    # vocabulary weight.
+    if xformer_stack is None:
+      xformer_stack = transformer_encoder.TransformerEncoder(
+          vocab_size=vocab_size,
+          num_layers=1,
+          sequence_length=sequence_length,
+          hidden_size=hidden_size,
+          num_attention_heads=4,
+      )
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    lm_outputs, _ = xformer_stack([word_ids, mask, type_ids])
+
+    # Create a maskedLM from the transformer stack.
+    test_network = masked_lm.MaskedLM(
+        num_predictions=num_predictions,
+        input_width=lm_outputs.shape[-1],
+        source_network=xformer_stack,
+        output=output)
+    return test_network
+
+  def test_network_creation(self):
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    test_network = self.create_network(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Make sure that the output tensor of the masked LM is the right shape.
+    lm_input_tensor = tf.keras.Input(shape=(sequence_length, hidden_size))
+    masked_lm_positions = tf.keras.Input(
+        shape=(num_predictions,), dtype=tf.int32)
+    output = test_network([lm_input_tensor, masked_lm_positions])
+
+    expected_output_shape = [None, num_predictions, vocab_size]
+    self.assertEqual(expected_output_shape, output.shape.as_list())
+
+  def test_network_invocation_with_internal_logits(self):
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    test_network = self.create_network(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Create a model from the masked LM layer.
+    lm_input_tensor = tf.keras.Input(shape=(sequence_length, hidden_size))
+    masked_lm_positions = tf.keras.Input(
+        shape=(num_predictions,), dtype=tf.int32)
+    output = test_network([lm_input_tensor, masked_lm_positions])
+    model = tf.keras.Model([lm_input_tensor, masked_lm_positions], output)
+    logits_model = tf.keras.Model(test_network.inputs, test_network.logits)
+
+    # Invoke the masked LM on some fake data to make sure there are no runtime
+    # errors in the code.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    outputs = model.predict([lm_input_data, masked_position_data])
+    logits = logits_model.predict([lm_input_data, masked_position_data])
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, num_predictions, vocab_size)
+    self.assertEqual(expected_output_shape, outputs.shape)
+    self.assertEqual(expected_output_shape, logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    calculated_softmax = softmax_model.predict(logits)
+    self.assertAllClose(outputs, calculated_softmax)
+
+  def test_network_invocation_with_external_logits(self):
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    xformer_stack = transformer_encoder.TransformerEncoder(
+        vocab_size=vocab_size,
+        num_layers=1,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_attention_heads=4,
+    )
+    test_network = self.create_network(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions,
+        xformer_stack=xformer_stack,
+        output='predictions')
+    logit_network = self.create_network(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions,
+        xformer_stack=xformer_stack,
+        output='logits')
+    logit_network.set_weights(test_network.get_weights())
+
+    # Create a model from the masked LM layer.
+    lm_input_tensor = tf.keras.Input(shape=(sequence_length, hidden_size))
+    masked_lm_positions = tf.keras.Input(
+        shape=(num_predictions,), dtype=tf.int32)
+    output = test_network([lm_input_tensor, masked_lm_positions])
+    logit_output = logit_network([lm_input_tensor, masked_lm_positions])
+
+    model = tf.keras.Model([lm_input_tensor, masked_lm_positions], output)
+    logits_model = tf.keras.Model(([lm_input_tensor, masked_lm_positions]),
+                                  logit_output)
+
+    # Invoke the masked LM on some fake data to make sure there are no runtime
+    # errors in the code.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    outputs = model.predict([lm_input_data, masked_position_data])
+    logits = logits_model.predict([lm_input_data, masked_position_data])
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, num_predictions, vocab_size)
+    self.assertEqual(expected_output_shape, outputs.shape)
+    self.assertEqual(expected_output_shape, logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    calculated_softmax = softmax_model.predict(logits)
+    self.assertAllClose(outputs, calculated_softmax)
+
+  def test_network_invocation(self):
+    vocab_size = 100
+    sequence_length = 32
+    hidden_size = 64
+    num_predictions = 21
+    test_network = self.create_network(
+        vocab_size=vocab_size,
+        sequence_length=sequence_length,
+        hidden_size=hidden_size,
+        num_predictions=num_predictions)
+
+    # Create a model from the masked LM layer.
+    lm_input_tensor = tf.keras.Input(shape=(sequence_length, hidden_size))
+    masked_lm_positions = tf.keras.Input(
+        shape=(num_predictions,), dtype=tf.int32)
+    output = test_network([lm_input_tensor, masked_lm_positions])
+    model = tf.keras.Model([lm_input_tensor, masked_lm_positions], output)
+
+    # Invoke the masked LM on some fake data to make sure there are no runtime
+    # errors in the code.
+    batch_size = 3
+    lm_input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, hidden_size))
+    masked_position_data = np.random.randint(
+        2, size=(batch_size, num_predictions))
+    _ = model.predict([lm_input_data, masked_position_data])
+
+  def test_unknown_output_type_fails(self):
+    with self.assertRaisesRegex(ValueError, 'Unknown `output` value "bad".*'):
+      _ = self.create_network(
+          vocab_size=8,
+          sequence_length=8,
+          hidden_size=8,
+          num_predictions=8,
+          output='bad')
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/span_labeling.py b/official/nlp/modeling/networks/span_labeling.py
new file mode 100644
index 00000000..df421e9d
--- /dev/null
+++ b/official/nlp/modeling/networks/span_labeling.py
@@ -0,0 +1,97 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Span labeling network."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+# pylint: disable=g-direct-tensorflow-import
+from tensorflow.python.keras.engine import network
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class SpanLabeling(network.Network):
+  """Span labeling network head for BERT modeling.
+
+  This network implements a simple single-span labeler based on a dense layer.
+
+  Attributes:
+    input_width: The innermost dimension of the input tensor to this network.
+    activation: The activation, if any, for the dense layer in this network.
+    initializer: The intializer for the dense layer in this network. Defaults to
+      a Glorot uniform initializer.
+    output: The output style for this network. Can be either 'logits' or
+      'predictions'.
+  """
+
+  def __init__(self,
+               input_width,
+               activation=None,
+               initializer='glorot_uniform',
+               output='logits',
+               **kwargs):
+    self._self_setattr_tracking = False
+    self._config = {
+        'input_width': input_width,
+        'activation': activation,
+        'initializer': initializer,
+        'output': output,
+    }
+
+    sequence_data = tf.keras.layers.Input(
+        shape=(None, input_width), name='sequence_data', dtype=tf.float32)
+
+    time_distributed_dense = tf.keras.layers.TimeDistributed(
+        tf.keras.layers.Dense(
+            2,  # This layer predicts start location and end location.
+            activation=activation,
+            kernel_initializer=initializer,
+            name='predictions/transform/logits'))
+
+    intermediate_logits = time_distributed_dense(sequence_data)
+    self.start_logits, self.end_logits = (
+        tf.keras.layers.Lambda(self._split_output_tensor)(intermediate_logits))
+
+    start_predictions = tf.keras.layers.Activation(tf.nn.log_softmax)(
+        self.start_logits)
+    end_predictions = tf.keras.layers.Activation(tf.nn.log_softmax)(
+        self.end_logits)
+
+    if output == 'logits':
+      output_tensors = [self.start_logits, self.end_logits]
+    elif output == 'predictions':
+      output_tensors = [start_predictions, end_predictions]
+    else:
+      raise ValueError(
+          ('Unknown `output` value "%s". `output` can be either "logits" or '
+           '"predictions"') % output)
+
+    super(SpanLabeling, self).__init__(
+        inputs=[sequence_data], outputs=output_tensors, **kwargs)
+
+  def _split_output_tensor(self, tensor):
+    transposed_tensor = tf.transpose(tensor, [2, 0, 1])
+    return tf.unstack(transposed_tensor)
+
+  def get_config(self):
+    return self._config
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
diff --git a/official/nlp/modeling/networks/span_labeling_test.py b/official/nlp/modeling/networks/span_labeling_test.py
new file mode 100644
index 00000000..8533a77b
--- /dev/null
+++ b/official/nlp/modeling/networks/span_labeling_test.py
@@ -0,0 +1,174 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for span_labeling network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.networks import span_labeling
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class SpanLabelingTest(keras_parameterized.TestCase):
+
+  def test_network_creation(self):
+    """Validate that the Keras object can be created."""
+    sequence_length = 15
+    input_width = 512
+    test_network = span_labeling.SpanLabeling(
+        input_width=input_width, output='predictions')
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_data = tf.keras.Input(
+        shape=(sequence_length, input_width), dtype=tf.float32)
+    start_outputs, end_outputs = test_network(sequence_data)
+
+    # Validate that the outputs are of the expected shape.
+    expected_output_shape = [None, sequence_length]
+    self.assertEqual(expected_output_shape, start_outputs.shape.as_list())
+    self.assertEqual(expected_output_shape, end_outputs.shape.as_list())
+
+  def test_network_invocation(self):
+    """Validate that the Keras object can be invoked."""
+    sequence_length = 15
+    input_width = 512
+    test_network = span_labeling.SpanLabeling(input_width=input_width)
+
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_data = tf.keras.Input(
+        shape=(sequence_length, input_width), dtype=tf.float32)
+    outputs = test_network(sequence_data)
+    model = tf.keras.Model(sequence_data, outputs)
+
+    # Invoke the network as part of a Model.
+    batch_size = 3
+    input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, input_width))
+    start_outputs, end_outputs = model.predict(input_data)
+
+    # Validate that the outputs are of the expected shape.
+    expected_output_shape = (batch_size, sequence_length)
+    self.assertEqual(expected_output_shape, start_outputs.shape)
+    self.assertEqual(expected_output_shape, end_outputs.shape)
+
+  def test_network_invocation_with_internal_logit_output(self):
+    """Validate that the logit outputs are correct."""
+    sequence_length = 15
+    input_width = 512
+    test_network = span_labeling.SpanLabeling(
+        input_width=input_width, output='predictions')
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_data = tf.keras.Input(
+        shape=(sequence_length, input_width), dtype=tf.float32)
+    output = test_network(sequence_data)
+    model = tf.keras.Model(sequence_data, output)
+    logit_model = tf.keras.Model(
+        test_network.inputs,
+        [test_network.start_logits, test_network.end_logits])
+
+    batch_size = 3
+    input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, input_width))
+    start_outputs, end_outputs = model.predict(input_data)
+    start_logits, end_logits = logit_model.predict(input_data)
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, sequence_length)
+    self.assertEqual(expected_output_shape, start_outputs.shape)
+    self.assertEqual(expected_output_shape, end_outputs.shape)
+    self.assertEqual(expected_output_shape, start_logits.shape)
+    self.assertEqual(expected_output_shape, end_logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    start_softmax = softmax_model.predict(start_logits)
+    self.assertAllClose(start_outputs, start_softmax)
+    end_softmax = softmax_model.predict(end_logits)
+    self.assertAllClose(end_outputs, end_softmax)
+
+  def test_network_invocation_with_external_logit_output(self):
+    """Validate that the logit outputs are correct."""
+    sequence_length = 15
+    input_width = 512
+    test_network = span_labeling.SpanLabeling(
+        input_width=input_width, output='predictions')
+    logit_network = span_labeling.SpanLabeling(
+        input_width=input_width, output='logits')
+    logit_network.set_weights(test_network.get_weights())
+
+    # Create a 3-dimensional input (the first dimension is implicit).
+    sequence_data = tf.keras.Input(
+        shape=(sequence_length, input_width), dtype=tf.float32)
+    output = test_network(sequence_data)
+    logit_output = logit_network(sequence_data)
+    model = tf.keras.Model(sequence_data, output)
+    logit_model = tf.keras.Model(sequence_data, logit_output)
+
+    batch_size = 3
+    input_data = 10 * np.random.random_sample(
+        (batch_size, sequence_length, input_width))
+    start_outputs, end_outputs = model.predict(input_data)
+    start_logits, end_logits = logit_model.predict(input_data)
+
+    # Ensure that the tensor shapes are correct.
+    expected_output_shape = (batch_size, sequence_length)
+    self.assertEqual(expected_output_shape, start_outputs.shape)
+    self.assertEqual(expected_output_shape, end_outputs.shape)
+    self.assertEqual(expected_output_shape, start_logits.shape)
+    self.assertEqual(expected_output_shape, end_logits.shape)
+
+    # Ensure that the logits, when softmaxed, create the outputs.
+    input_tensor = tf.keras.Input(expected_output_shape[1:])
+    output_tensor = tf.keras.layers.Activation(tf.nn.log_softmax)(input_tensor)
+    softmax_model = tf.keras.Model(input_tensor, output_tensor)
+
+    start_softmax = softmax_model.predict(start_logits)
+    self.assertAllClose(start_outputs, start_softmax)
+    end_softmax = softmax_model.predict(end_logits)
+    self.assertAllClose(end_outputs, end_softmax)
+
+  def test_serialize_deserialize(self):
+    # Create a network object that sets all of its config options.
+    network = span_labeling.SpanLabeling(
+        input_width=128,
+        activation='relu',
+        initializer='zeros',
+        output='predictions')
+
+    # Create another network object from the first object's config.
+    new_network = span_labeling.SpanLabeling.from_config(network.get_config())
+
+    # Validate that the config can be forced to JSON.
+    _ = new_network.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(network.get_config(), new_network.get_config())
+
+  def test_unknown_output_type_fails(self):
+    with self.assertRaisesRegex(ValueError, 'Unknown `output` value "bad".*'):
+      _ = span_labeling.SpanLabeling(input_width=10, output='bad')
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/official/nlp/modeling/networks/transformer_encoder.py b/official/nlp/modeling/networks/transformer_encoder.py
new file mode 100644
index 00000000..8d1a1b83
--- /dev/null
+++ b/official/nlp/modeling/networks/transformer_encoder.py
@@ -0,0 +1,196 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Transformer-based text encoder network."""
+
+from __future__ import absolute_import
+from __future__ import division
+# from __future__ import google_type_annotations
+from __future__ import print_function
+
+import tensorflow as tf
+
+from tensorflow.python.keras.engine import network  # pylint: disable=g-direct-tensorflow-import
+from official.modeling import activations
+from official.nlp import bert_modeling
+from official.nlp.modeling import layers
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class TransformerEncoder(network.Network):
+  """Bi-directional Transformer-based encoder network.
+
+  This network implements a bi-directional Transformer-based encoder as
+  described in "BERT: Pre-training of Deep Bidirectional Transformers for
+  Language Understanding" (https://arxiv.org/abs/1810.04805). It includes the
+  embedding lookups and transformer layers, but not the masked language model
+  or classification task networks.
+
+  The default values for this object are taken from the BERT-Base implementation
+  in "BERT: Pre-training of Deep Bidirectional Transformers for Language
+  Understanding".
+
+  Attributes:
+    vocab_size: The size of the token vocabulary.
+    hidden_size: The size of the transformer hidden layers.
+    num_layers: The number of transformer layers.
+    num_attention_heads: The number of attention heads for each transformer. The
+      hidden size must be divisible by the number of attention heads.
+    sequence_length: The sequence length that this encoder expects. If None, the
+      sequence length is dynamic; if an integer, the encoder will require
+      sequences padded to this length.
+    max_sequence_length: The maximum sequence length that this encoder can
+      consume. If None, max_sequence_length uses the value from sequence length.
+      This determines the variable shape for positional embeddings.
+    type_vocab_size: The number of types that the 'type_ids' input can take.
+    intermediate_size: The intermediate size for the transformer layers.
+    activation: The activation to use for the transformer layers.
+    dropout_rate: The dropout rate to use for the transformer layers.
+    attention_dropout_rate: The dropout rate to use for the attention layers
+      within the transformer layers.
+    initializer: The initialzer to use for all weights in this encoder.
+    float_dtype: The dtype of this encoder. Can be 'float32' or 'float16'.
+  """
+
+  def __init__(self,
+               vocab_size,
+               hidden_size=768,
+               num_layers=12,
+               num_attention_heads=12,
+               sequence_length=512,
+               max_sequence_length=None,
+               type_vocab_size=16,
+               intermediate_size=3072,
+               activation=activations.gelu,
+               dropout_rate=0.1,
+               attention_dropout_rate=0.1,
+               initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),
+               float_dtype='float32',
+               **kwargs):
+    activation = tf.keras.activations.get(activation)
+    initializer = tf.keras.initializers.get(initializer)
+
+    if not max_sequence_length:
+      max_sequence_length = sequence_length
+    self._self_setattr_tracking = False
+    self._config_dict = {
+        'vocab_size': vocab_size,
+        'hidden_size': hidden_size,
+        'num_layers': num_layers,
+        'num_attention_heads': num_attention_heads,
+        'sequence_length': sequence_length,
+        'max_sequence_length': max_sequence_length,
+        'type_vocab_size': type_vocab_size,
+        'intermediate_size': intermediate_size,
+        'activation': tf.keras.activations.serialize(activation),
+        'dropout_rate': dropout_rate,
+        'attention_dropout_rate': attention_dropout_rate,
+        'initializer': tf.keras.initializers.serialize(initializer),
+        'float_dtype': float_dtype,
+    }
+
+    word_ids = tf.keras.layers.Input(
+        shape=(sequence_length,), dtype=tf.int32, name='input_word_ids')
+    mask = tf.keras.layers.Input(
+        shape=(sequence_length,), dtype=tf.int32, name='input_mask')
+    type_ids = tf.keras.layers.Input(
+        shape=(sequence_length,), dtype=tf.int32, name='input_type_ids')
+
+    self._embedding_layer = layers.OnDeviceEmbedding(
+        vocab_size=vocab_size,
+        embedding_width=hidden_size,
+        initializer=initializer,
+        dtype=float_dtype,
+        name='word_embeddings')
+    word_embeddings = self._embedding_layer(word_ids)
+
+    # Always uses dynamic slicing for simplicity.
+    self._position_embedding_layer = layers.PositionEmbedding(
+        initializer=initializer,
+        use_dynamic_slicing=True,
+        max_sequence_length=max_sequence_length,
+        dtype=float_dtype)
+    position_embeddings = self._position_embedding_layer(word_embeddings)
+
+    type_embeddings = (
+        layers.OnDeviceEmbedding(
+            vocab_size=type_vocab_size,
+            embedding_width=hidden_size,
+            initializer=initializer,
+            use_one_hot=True,
+            dtype=float_dtype,
+            name='type_embeddings')(type_ids))
+
+    embeddings = tf.keras.layers.Add()(
+        [word_embeddings, position_embeddings, type_embeddings])
+    embeddings = (
+        tf.keras.layers.LayerNormalization(
+            name='embeddings/layer_norm',
+            axis=-1,
+            epsilon=1e-12,
+            dtype=float_dtype)(embeddings))
+    embeddings = (
+        tf.keras.layers.Dropout(rate=dropout_rate,
+                                dtype=tf.float32)(embeddings))
+
+    if float_dtype == 'float16':
+      embeddings = tf.cast(embeddings, tf.float16)
+
+    data = embeddings
+    attention_mask = MakeAttentionMaskLayer()([data, mask])
+    for i in range(num_layers):
+      layer = layers.Transformer(
+          num_attention_heads=num_attention_heads,
+          intermediate_size=intermediate_size,
+          intermediate_activation=activation,
+          dropout_rate=dropout_rate,
+          attention_dropout_rate=attention_dropout_rate,
+          kernel_initializer=initializer,
+          dtype=float_dtype,
+          name='transformer/layer_%d' % i)
+      data = layer([data, attention_mask])
+
+    first_token_tensor = (
+        tf.keras.layers.Lambda(lambda x: tf.squeeze(x[:, 0:1, :], axis=1))(data)
+    )
+    cls_output = tf.keras.layers.Dense(
+        units=hidden_size,
+        activation='tanh',
+        kernel_initializer=initializer,
+        dtype=float_dtype,
+        name='pooler_transform')(
+            first_token_tensor)
+
+    super(TransformerEncoder, self).__init__(
+        inputs=[word_ids, mask, type_ids],
+        outputs=[data, cls_output],
+        **kwargs)
+
+  def get_embedding_table(self):
+    return self._embedding_layer.embeddings
+
+  def get_config(self):
+    return self._config_dict
+
+  @classmethod
+  def from_config(cls, config, custom_objects=None):
+    return cls(**config)
+
+
+@tf.keras.utils.register_keras_serializable(package='Text')
+class MakeAttentionMaskLayer(tf.keras.layers.Layer):
+
+  def call(self, inputs):
+    return bert_modeling.create_attention_mask_from_input_mask(
+        inputs[0], inputs[1])
diff --git a/official/nlp/modeling/networks/transformer_encoder_test.py b/official/nlp/modeling/networks/transformer_encoder_test.py
new file mode 100644
index 00000000..33706048
--- /dev/null
+++ b/official/nlp/modeling/networks/transformer_encoder_test.py
@@ -0,0 +1,169 @@
+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for transformer-based text encoder network."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+
+from tensorflow.python.keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import
+from official.nlp.modeling.networks import transformer_encoder
+
+
+# This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It
+# guarantees forward compatibility of this code for the V2 switchover.
+@keras_parameterized.run_all_keras_modes
+class TransformerEncoderTest(keras_parameterized.TestCase):
+
+  def test_network_creation(self):
+    hidden_size = 32
+    sequence_length = 21
+    # Create a small TransformerEncoder for testing.
+    test_network = transformer_encoder.TransformerEncoder(
+        vocab_size=100,
+        hidden_size=hidden_size,
+        sequence_length=sequence_length,
+        num_attention_heads=2,
+        num_layers=3)
+    # Create the inputs (note that the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    data, pooled = test_network([word_ids, mask, type_ids])
+
+    expected_data_shape = [None, sequence_length, hidden_size]
+    expected_pooled_shape = [None, hidden_size]
+    self.assertAllEqual(expected_data_shape, data.shape.as_list())
+    self.assertAllEqual(expected_pooled_shape, pooled.shape.as_list())
+
+    # The default output dtype is float32.
+    self.assertAllEqual(tf.float32, data.dtype)
+    self.assertAllEqual(tf.float32, pooled.dtype)
+
+  def test_network_creation_with_float16_dtype(self):
+    hidden_size = 32
+    sequence_length = 21
+    # Create a small TransformerEncoder for testing.
+    test_network = transformer_encoder.TransformerEncoder(
+        vocab_size=100,
+        hidden_size=hidden_size,
+        sequence_length=sequence_length,
+        num_attention_heads=2,
+        num_layers=3,
+        float_dtype="float16")
+    # Create the inputs (note that the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    data, pooled = test_network([word_ids, mask, type_ids])
+
+    expected_data_shape = [None, sequence_length, hidden_size]
+    expected_pooled_shape = [None, hidden_size]
+    self.assertAllEqual(expected_data_shape, data.shape.as_list())
+    self.assertAllEqual(expected_pooled_shape, pooled.shape.as_list())
+
+    # If float_dtype is set to float16, the output should always be float16.
+    self.assertAllEqual(tf.float16, data.dtype)
+    self.assertAllEqual(tf.float16, pooled.dtype)
+
+  def test_network_invocation(self):
+    hidden_size = 32
+    sequence_length = 21
+    vocab_size = 57
+    num_types = 7
+    # Create a small TransformerEncoder for testing.
+    test_network = transformer_encoder.TransformerEncoder(
+        vocab_size=vocab_size,
+        hidden_size=hidden_size,
+        sequence_length=sequence_length,
+        num_attention_heads=2,
+        num_layers=3,
+        type_vocab_size=num_types)
+    self.assertTrue(
+        test_network._position_embedding_layer._use_dynamic_slicing)
+    # Create the inputs (note that the first dimension is implicit).
+    word_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    mask = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    type_ids = tf.keras.Input(shape=(sequence_length,), dtype=tf.int32)
+    data, pooled = test_network([word_ids, mask, type_ids])
+
+    # Create a model based off of this network:
+    model = tf.keras.Model([word_ids, mask, type_ids], [data, pooled])
+
+    # Invoke the model. We can't validate the output data here (the model is too
+    # complex) but this will catch structural runtime errors.
+    batch_size = 3
+    word_id_data = np.random.randint(
+        vocab_size, size=(batch_size, sequence_length))
+    mask_data = np.random.randint(2, size=(batch_size, sequence_length))
+    type_id_data = np.random.randint(
+        num_types, size=(batch_size, sequence_length))
+    _ = model.predict([word_id_data, mask_data, type_id_data])
+
+    # Creates a TransformerEncoder with max_sequence_length != sequence_length
+    max_sequence_length = 128
+    test_network = transformer_encoder.TransformerEncoder(
+        vocab_size=vocab_size,
+        hidden_size=hidden_size,
+        sequence_length=sequence_length,
+        max_sequence_length=max_sequence_length,
+        num_attention_heads=2,
+        num_layers=3,
+        type_vocab_size=num_types)
+    self.assertTrue(test_network._position_embedding_layer._use_dynamic_slicing)
+    model = tf.keras.Model([word_ids, mask, type_ids], [data, pooled])
+    _ = model.predict([word_id_data, mask_data, type_id_data])
+
+  def test_serialize_deserialize(self):
+    # Create a network object that sets all of its config options.
+    kwargs = dict(
+        vocab_size=100,
+        hidden_size=32,
+        num_layers=3,
+        num_attention_heads=2,
+        sequence_length=21,
+        max_sequence_length=21,
+        type_vocab_size=12,
+        intermediate_size=1223,
+        activation="relu",
+        dropout_rate=0.05,
+        attention_dropout_rate=0.22,
+        initializer="glorot_uniform",
+        float_dtype="float16")
+    network = transformer_encoder.TransformerEncoder(**kwargs)
+
+    expected_config = dict(kwargs)
+    expected_config["activation"] = tf.keras.activations.serialize(
+        tf.keras.activations.get(expected_config["activation"]))
+    expected_config["initializer"] = tf.keras.initializers.serialize(
+        tf.keras.initializers.get(expected_config["initializer"]))
+    self.assertEqual(network.get_config(), expected_config)
+
+    # Create another network object from the first object's config.
+    new_network = transformer_encoder.TransformerEncoder.from_config(
+        network.get_config())
+
+    # Validate that the config can be forced to JSON.
+    _ = new_network.to_json()
+
+    # If the serialization was successful, the new config should match the old.
+    self.assertAllEqual(network.get_config(), new_network.get_config())
+
+
+if __name__ == "__main__":
+  tf.test.main()
