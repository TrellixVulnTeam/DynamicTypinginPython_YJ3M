commit 03038d9cde896e2f75d60c0c09f7a3f6b468dc26
Author: Ayush Dubey <ayushd@google.com>
Date:   Wed Jan 22 08:59:15 2020 -0800

    Update resnet README with multi-worker benchmark instructions.
    
    PiperOrigin-RevId: 290963657

diff --git a/official/vision/image_classification/README.md b/official/vision/image_classification/README.md
index b19d6b59..024c0932 100644
--- a/official/vision/image_classification/README.md
+++ b/official/vision/image_classification/README.md
@@ -113,6 +113,19 @@ distributed training across the GPUs.
 If you wish to run without `tf.distribute.Strategy`, you can do so by setting
 `--distribution_strategy=off`.
 
+### Running on multiple GPU hosts
+
+You can also train these models on multiple hosts, each with GPUs, using
+`tf.distribute.Strategy`.
+
+The easiest way to run multi-host benchmarks is to set the
+[`TF_CONFIG`](https://www.tensorflow.org/guide/distributed_training#TF_CONFIG)
+appropriately at each host.  e.g., to run using `MultiWorkerMirroredStrategy` on
+2 hosts, the `cluster` in `TF_CONFIG` should have 2 `host:port` entries, and
+host `i` should have the `task` in `TF_CONFIG` set to `{"type": "worker",
+"index": i}`.  `MultiWorkerMirroredStrategy` will automatically use all the
+available GPUs at each host.
+
 ### Running on Cloud TPUs
 
 Note: This model will **not** work with TPUs on Colab.
