commit 7546a9e3d1c51798bd20407ea1749e13b87b0368
Author: Haoyu Zhang <haoyuzhang@google.com>
Date:   Fri May 31 13:52:08 2019 -0700

    Fix internal lint errors (#6937)

diff --git a/official/recommendation/ncf_common.py b/official/recommendation/ncf_common.py
index f04e4a8b..42276631 100644
--- a/official/recommendation/ncf_common.py
+++ b/official/recommendation/ncf_common.py
@@ -335,6 +335,7 @@ def define_ncf_flags():
       help=flags_core.help_wrap(
           "If True, we use a custom training loop for keras."))
 
+
 def convert_to_softmax_logits(logits):
   '''Convert the logits returned by the base model to softmax logits.
 
diff --git a/official/recommendation/ncf_keras_main.py b/official/recommendation/ncf_keras_main.py
index e491aac5..a080e3fe 100644
--- a/official/recommendation/ncf_keras_main.py
+++ b/official/recommendation/ncf_keras_main.py
@@ -353,7 +353,7 @@ def run_ncf(_):
         train_loss += train_step()
         time_callback.on_batch_end(step+epoch*num_train_steps)
       logging.info("Done training epoch %s, epoch loss=%s.",
-          epoch+1, train_loss/num_train_steps)
+                   epoch+1, train_loss/num_train_steps)
       eval_input_iterator.initialize()
       hr_sum = 0
       hr_count = 0
diff --git a/official/resnet/keras/keras_imagenet_benchmark.py b/official/resnet/keras/keras_imagenet_benchmark.py
index 4b1c2f7d..3cb9edb9 100644
--- a/official/resnet/keras/keras_imagenet_benchmark.py
+++ b/official/resnet/keras/keras_imagenet_benchmark.py
@@ -603,9 +603,7 @@ class Resnet50KerasBenchmarkBase(keras_benchmark.KerasBenchmark):
     self._run_and_report_benchmark()
 
   def benchmark_xla_8_gpu_fp16_cloning_tweaked(self):
-    """Test Keras model with manual config tuning, XLA, 8 GPUs, fp16, and
-       cloning.
-    """
+    """Test with manual config tuning, XLA, 8 GPUs, fp16, and cloning."""
     self._setup()
 
     FLAGS.num_gpus = 8
@@ -623,8 +621,9 @@ class Resnet50KerasBenchmarkBase(keras_benchmark.KerasBenchmark):
     self._run_and_report_benchmark()
 
   def benchmark_xla_8_gpu_fp16_tweaked_delay_measure(self):
-    """Test Keras model with manual config tuning, XLA, 8 GPUs and fp16. Delay
-       performance measurement for stable performance on 96 vCPU platforms.
+    """Test with manual config tuning, XLA, 8 GPUs and fp16.
+
+    Delay performance measurement for stable performance on 96 vCPU platforms.
     """
     self._setup()
 
@@ -643,9 +642,9 @@ class Resnet50KerasBenchmarkBase(keras_benchmark.KerasBenchmark):
     self._run_and_report_benchmark()
 
   def benchmark_xla_8_gpu_fp16_cloning_tweaked_delay_measure(self):
-    """Test Keras model with manual config tuning, XLA, 8 GPUs, fp16, and
-       cloning. Delay performance measurement for stable performance on 96 vCPU
-       platforms.
+    """Test with manual config tuning, XLA, 8 GPUs, fp16, and cloning.
+
+    Delay performance measurement for stable performance on 96 vCPU platforms.
     """
     self._setup()
 
@@ -821,9 +820,9 @@ class Resnet50KerasBenchmarkBase(keras_benchmark.KerasBenchmark):
     self._run_and_report_benchmark()
 
   def benchmark_graph_xla_8_gpu_fp16_tweaked_delay_measure(self):
-    """Test Keras model in legacy graph mode with manual config tuning, XLA,
-       8 GPUs and fp16. Delay performance measurement for stable performance
-       on 96 vCPU platforms.
+    """Test in legacy graph mode with manual config tuning, XLA, 8 GPUs, fp16.
+
+    Delay performance measurement for stable performance on 96 vCPU platforms.
     """
     self._setup()
 
@@ -841,8 +840,7 @@ class Resnet50KerasBenchmarkBase(keras_benchmark.KerasBenchmark):
     self._run_and_report_benchmark()
 
   def benchmark_graph_xla_8_gpu_fp16_tweaked_optional_next(self):
-    """Test Keras model in legacy graph mode with manual config tuning, XLA,
-       8 GPUs and fp16.
+    """Test in legacy graph mode with manual config tuning, XLA, 8 GPUs, fp16.
 
     This test also enables get_next_as_optional.
     """
diff --git a/official/transformer/v2/data_pipeline.py b/official/transformer/v2/data_pipeline.py
index a83eca66..45f7cf1f 100644
--- a/official/transformer/v2/data_pipeline.py
+++ b/official/transformer/v2/data_pipeline.py
@@ -251,7 +251,8 @@ def _read_and_batch_from_files(
         ([max_length], [max_length]), drop_remainder=True)
   else:
     # Group and batch such that each batch has examples of similar length.
-    # TODO: _batch_examples might need to do something special for num_replicas.
+    # TODO(xunkai): _batch_examples might need to do something special for
+    # num_replicas.
     dataset = _batch_examples(dataset, batch_size, max_length)
 
   dataset = dataset.repeat(repeat)
diff --git a/official/transformer/v2/transformer_main.py b/official/transformer/v2/transformer_main.py
index 49b2742e..41f1010b 100644
--- a/official/transformer/v2/transformer_main.py
+++ b/official/transformer/v2/transformer_main.py
@@ -25,7 +25,7 @@ from __future__ import print_function
 import os
 import tempfile
 
-from absl import app as absl_app
+from absl import app as absl_app  # pylint: disable=unused-import
 from absl import flags
 import tensorflow as tf
 
