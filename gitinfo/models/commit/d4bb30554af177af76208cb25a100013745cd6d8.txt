commit d4bb30554af177af76208cb25a100013745cd6d8
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Tue Jun 9 16:31:11 2020 -0700

    Update nlp.modeling.layers.ReZeroTransformer to be consistent with nlp.modeling.layers.Transformer
    
    PiperOrigin-RevId: 315584374

diff --git a/official/nlp/modeling/layers/rezero_transformer.py b/official/nlp/modeling/layers/rezero_transformer.py
index 25287d5a..42bc1af0 100644
--- a/official/nlp/modeling/layers/rezero_transformer.py
+++ b/official/nlp/modeling/layers/rezero_transformer.py
@@ -143,8 +143,14 @@ class ReZeroTransformer(tf.keras.layers.Layer):
         kernel_constraint=self._kernel_constraint,
         bias_constraint=self._bias_constraint,
         name="intermediate")
+    policy = tf.keras.mixed_precision.experimental.global_policy()
+    if policy.name == "mixed_bfloat16":
+      # bfloat16 causes BERT with the LAMB optimizer to not converge
+      # as well, so we use float32.
+      # TODO(b/154538392): Investigate this.
+      policy = tf.float32
     self._intermediate_activation_layer = tf.keras.layers.Activation(
-        self._intermediate_activation)
+        self._intermediate_activation, dtype=policy)
     self._output_dense = dense_einsum.DenseEinsum(
         output_shape=hidden_size,
         kernel_initializer=self._kernel_initializer,
