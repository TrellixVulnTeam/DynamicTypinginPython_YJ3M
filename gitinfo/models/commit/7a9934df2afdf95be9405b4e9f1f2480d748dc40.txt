commit 7a9934df2afdf95be9405b4e9f1f2480d748dc40
Author: Zhichao Lu <lzc@google.com>
Date:   Wed Jan 31 14:35:04 2018 -0800

    Merged commit includes the following changes:
    184048729  by Zhichao Lu:
    
        Modify target_assigner so that it creates regression targets taking keypoints into account.
    
    --
    184027183  by Zhichao Lu:
    
        Resnet V1 FPN based feature extractors for SSD meta architecture in Object Detection V2 API.
    
    --
    184004730  by Zhichao Lu:
    
        Expose a lever to override the configured mask_type.
    
    --
    183933113  by Zhichao Lu:
    
        Weight shared convolutional box predictor as described in https://arxiv.org/abs/1708.02002
    
    --
    183929669  by Zhichao Lu:
    
        Expanding box list operations for future data augmentations.
    
    --
    183916792  by Zhichao Lu:
    
        Fix unrecognized assertion function in tests.
    
    --
    183906851  by Zhichao Lu:
    
        - Change ssd meta architecture to use regression weights to compute loss normalizer.
    
    --
    183871003  by Zhichao Lu:
    
        Fix config_util_test wrong dependency.
    
    --
    183782120  by Zhichao Lu:
    
        Add __init__ file to third_party directories.
    
    --
    183779109  by Zhichao Lu:
    
        Setup regular version sync.
    
    --
    183768772  by Zhichao Lu:
    
        Make test compatible with numpy 1.12 and higher
    
    --
    183767893  by Zhichao Lu:
    
        Make test compatible with numpy 1.12 and higher
    
    --
    183719318  by Zhichao Lu:
    
        Use the new test interface in ssd feature extractor.
    
    --
    183714671  by Zhichao Lu:
    
        Use the new test_case interface for all anchor generators.
    
    --
    183708155  by Zhichao Lu:
    
        Change variable scopes in ConvolutionalBoxPredictor such that previously trained checkpoints are still compatible after the change in BoxPredictor interface
    
    --
    183705798  by Zhichao Lu:
    
        Internal change.
    
    --
    183636023  by Zhichao Lu:
    
        Fixing argument name for np_box_list_ops.concatenate() function.
    
    --
    183490404  by Zhichao Lu:
    
        Make sure code that relies in SSD older code still works.
    
    --
    183426762  by Zhichao Lu:
    
        Internal change
    
    183412315  by Zhichao Lu:
    
        Internal change
    
    183337814  by Zhichao Lu:
    
        Internal change
    
    183303933  by Zhichao Lu:
    
        Internal change
    
    183257349  by Zhichao Lu:
    
        Internal change
    
    183254447  by Zhichao Lu:
    
        Internal change
    
    183251200  by Zhichao Lu:
    
        Internal change
    
    183135002  by Zhichao Lu:
    
        Internal change
    
    182851500  by Zhichao Lu:
    
        Internal change
    
    182839607  by Zhichao Lu:
    
        Internal change
    
    182830719  by Zhichao Lu:
    
        Internal change
    
    182533923  by Zhichao Lu:
    
        Internal change
    
    182391090  by Zhichao Lu:
    
        Internal change
    
    182262339  by Zhichao Lu:
    
        Internal change
    
    182244645  by Zhichao Lu:
    
        Internal change
    
    182241613  by Zhichao Lu:
    
        Internal change
    
    182133027  by Zhichao Lu:
    
        Internal change
    
    182058807  by Zhichao Lu:
    
        Internal change
    
    181812028  by Zhichao Lu:
    
        Internal change
    
    181788857  by Zhichao Lu:
    
        Internal change
    
    181656761  by Zhichao Lu:
    
        Internal change
    
    181541125  by Zhichao Lu:
    
        Internal change
    
    181538702  by Zhichao Lu:
    
        Internal change
    
    181125385  by Zhichao Lu:
    
        Internal change
    
    180957758  by Zhichao Lu:
    
        Internal change
    
    180941434  by Zhichao Lu:
    
        Internal change
    
    180852569  by Zhichao Lu:
    
        Internal change
    
    180846001  by Zhichao Lu:
    
        Internal change
    
    180832145  by Zhichao Lu:
    
        Internal change
    
    180740495  by Zhichao Lu:
    
        Internal change
    
    180729150  by Zhichao Lu:
    
        Internal change
    
    180589008  by Zhichao Lu:
    
        Internal change
    
    180585408  by Zhichao Lu:
    
        Internal change
    
    180581039  by Zhichao Lu:
    
        Internal change
    
    180286388  by Zhichao Lu:
    
        Internal change
    
    179934081  by Zhichao Lu:
    
        Internal change
    
    179841242  by Zhichao Lu:
    
        Internal change
    
    179831694  by Zhichao Lu:
    
        Internal change
    
    179761005  by Zhichao Lu:
    
        Internal change
    
    179610632  by Zhichao Lu:
    
        Internal change
    
    179605363  by Zhichao Lu:
    
        Internal change
    
    179603774  by Zhichao Lu:
    
        Internal change
    
    179598614  by Zhichao Lu:
    
        Internal change
    
    179597809  by Zhichao Lu:
    
        Internal change
    
    179494630  by Zhichao Lu:
    
        Internal change
    
    179367492  by Zhichao Lu:
    
        Internal change
    
    179250050  by Zhichao Lu:
    
        Internal change
    
    179247385  by Zhichao Lu:
    
        Internal change
    
    179207897  by Zhichao Lu:
    
        Internal change
    
    179076230  by Zhichao Lu:
    
        Internal change
    
    178862066  by Zhichao Lu:
    
        Internal change
    
    178854216  by Zhichao Lu:
    
        Internal change
    
    178853109  by Zhichao Lu:
    
        Internal change
    
    178709753  by Zhichao Lu:
    
        Internal change
    
    178640707  by Zhichao Lu:
    
        Internal change
    
    178421534  by Zhichao Lu:
    
        Internal change
    
    178287174  by Zhichao Lu:
    
        Internal change
    
    178257399  by Zhichao Lu:
    
        Internal change
    
    177681867  by Zhichao Lu:
    
        Internal change
    
    177654820  by Zhichao Lu:
    
        Internal change
    
    177654052  by Zhichao Lu:
    
        Internal change
    
    177638787  by Zhichao Lu:
    
        Internal change
    
    177598305  by Zhichao Lu:
    
        Internal change
    
    177538488  by Zhichao Lu:
    
        Internal change
    
    177474197  by Zhichao Lu:
    
        Internal change
    
    177271928  by Zhichao Lu:
    
        Internal change
    
    177250285  by Zhichao Lu:
    
        Internal change
    
    177210762  by Zhichao Lu:
    
        Internal change
    
    177197135  by Zhichao Lu:
    
        Internal change
    
    177037781  by Zhichao Lu:
    
        Internal change
    
    176917394  by Zhichao Lu:
    
        Internal change
    
    176683171  by Zhichao Lu:
    
        Internal change
    
    176450793  by Zhichao Lu:
    
        Internal change
    
    176388133  by Zhichao Lu:
    
        Internal change
    
    176197721  by Zhichao Lu:
    
        Internal change
    
    176195315  by Zhichao Lu:
    
        Internal change
    
    176128748  by Zhichao Lu:
    
        Internal change
    
    175743440  by Zhichao Lu:
    
        Use Toggle instead of bool to make the layout optimizer name and usage consistent with other optimizers.
    
    --
    175578178  by Zhichao Lu:
    
        Internal change
    
    175463518  by Zhichao Lu:
    
        Internal change
    
    175316616  by Zhichao Lu:
    
        Internal change
    
    175302470  by Zhichao Lu:
    
        Internal change
    
    175300323  by Zhichao Lu:
    
        Internal change
    
    175269680  by Zhichao Lu:
    
        Internal change
    
    175260574  by Zhichao Lu:
    
        Internal change
    
    175122281  by Zhichao Lu:
    
        Internal change
    
    175111708  by Zhichao Lu:
    
        Internal change
    
    175110183  by Zhichao Lu:
    
        Internal change
    
    174877166  by Zhichao Lu:
    
        Internal change
    
    174868399  by Zhichao Lu:
    
        Internal change
    
    174754200  by Zhichao Lu:
    
        Internal change
    
    174544534  by Zhichao Lu:
    
        Internal change
    
    174536143  by Zhichao Lu:
    
        Internal change
    
    174513795  by Zhichao Lu:
    
        Internal change
    
    174463713  by Zhichao Lu:
    
        Internal change
    
    174403525  by Zhichao Lu:
    
        Internal change
    
    174385170  by Zhichao Lu:
    
        Internal change
    
    174358498  by Zhichao Lu:
    
        Internal change
    
    174249903  by Zhichao Lu:
    
        Fix nasnet image classification and object detection by moving the option to turn ON or OFF batch norm training into it's own arg_scope used only by detection
    
    --
    174216508  by Zhichao Lu:
    
        Internal change
    
    174065370  by Zhichao Lu:
    
        Internal change
    
    174048035  by Zhichao Lu:
    
        Fix the pointer for downloading the NAS Faster-RCNN model.
    
    --
    174042677  by Zhichao Lu:
    
        Internal change
    
    173964116  by Zhichao Lu:
    
        Internal change
    
    173790182  by Zhichao Lu:
    
        Internal change
    
    173779919  by Zhichao Lu:
    
        Internal change
    
    173753775  by Zhichao Lu:
    
        Internal change
    
    173753160  by Zhichao Lu:
    
        Internal change
    
    173737519  by Zhichao Lu:
    
        Internal change
    
    173696066  by Zhichao Lu:
    
        Internal change
    
    173611554  by Zhichao Lu:
    
        Internal change
    
    173475124  by Zhichao Lu:
    
        Internal change
    
    173412497  by Zhichao Lu:
    
        Internal change
    
    173404010  by Zhichao Lu:
    
        Internal change
    
    173375014  by Zhichao Lu:
    
        Internal change
    
    173345107  by Zhichao Lu:
    
        Internal change
    
    173298413  by Zhichao Lu:
    
        Internal change
    
    173289754  by Zhichao Lu:
    
        Internal change
    
    173275544  by Zhichao Lu:
    
        Internal change
    
    173273275  by Zhichao Lu:
    
        Internal change
    
    173271885  by Zhichao Lu:
    
        Internal change
    
    173264856  by Zhichao Lu:
    
        Internal change
    
    173263791  by Zhichao Lu:
    
        Internal change
    
    173261215  by Zhichao Lu:
    
        Internal change
    
    173175740  by Zhichao Lu:
    
        Internal change
    
    173010193  by Zhichao Lu:
    
        Internal change
    
    172815204  by Zhichao Lu:
    
        Allow for label maps in tf.Example decoding.
    
    --
    172696028  by Zhichao Lu:
    
        Internal change
    
    172509113  by Zhichao Lu:
    
        Allow for label maps in tf.Example decoding.
    
    --
    172475999  by Zhichao Lu:
    
        Internal change
    
    172166621  by Zhichao Lu:
    
        Internal change
    
    172151758  by Zhichao Lu:
    
        Minor updates to some README files.
    
        As a result of these friendly issues:
        https://github.com/tensorflow/models/issues/2530
        https://github.com/tensorflow/models/issues/2534
    
    --
    172147420  by Zhichao Lu:
    
        Fix illegal summary name and move from slim's get_or_create_global_step deprecated use of tf.contrib.framework* to tf.train*.
    
    --
    172111377  by Zhichao Lu:
    
        Internal change
    
    172004247  by Zhichao Lu:
    
        Internal change
    
    171996881  by Zhichao Lu:
    
        Internal change
    
    171835204  by Zhichao Lu:
    
        Internal change
    
    171826090  by Zhichao Lu:
    
        Internal change
    
    171784016  by Zhichao Lu:
    
        Internal change
    
    171699876  by Zhichao Lu:
    
        Internal change
    
    171053425  by Zhichao Lu:
    
        Internal change
    
    170905734  by Zhichao Lu:
    
        Internal change
    
    170889179  by Zhichao Lu:
    
        Internal change
    
    170734389  by Zhichao Lu:
    
        Internal change
    
    170705852  by Zhichao Lu:
    
        Internal change
    
    170401574  by Zhichao Lu:
    
        Internal change
    
    170352571  by Zhichao Lu:
    
        Internal change
    
    170215443  by Zhichao Lu:
    
        Internal change
    
    170184288  by Zhichao Lu:
    
        Internal change
    
    169936898  by Zhichao Lu:
    
        Internal change
    
    169763373  by Zhichao Lu:
    
        Fix broken GitHub links in tensorflow and tensorflow_models resulting from The Great Models Move (a.k.a. the research subfolder)
    
    --
    169744825  by Zhichao Lu:
    
        Internal change
    
    169638135  by Zhichao Lu:
    
        Internal change
    
    169561814  by Zhichao Lu:
    
        Internal change
    
    169444091  by Zhichao Lu:
    
        Internal change
    
    169292330  by Zhichao Lu:
    
        Internal change
    
    169145185  by Zhichao Lu:
    
        Internal change
    
    168906035  by Zhichao Lu:
    
        Internal change
    
    168790411  by Zhichao Lu:
    
        Internal change
    
    168708911  by Zhichao Lu:
    
        Internal change
    
    168611969  by Zhichao Lu:
    
        Internal change
    
    168535975  by Zhichao Lu:
    
        Internal change
    
    168381815  by Zhichao Lu:
    
        Internal change
    
    168244740  by Zhichao Lu:
    
        Internal change
    
    168240024  by Zhichao Lu:
    
        Internal change
    
    168168016  by Zhichao Lu:
    
        Internal change
    
    168071571  by Zhichao Lu:
    
        Move display strings to below the bounding box if they would otherwise be outside the image.
    
    --
    168067771  by Zhichao Lu:
    
        Internal change
    
    167970950  by Zhichao Lu:
    
        Internal change
    
    167884533  by Zhichao Lu:
    
        Internal change
    
    167626173  by Zhichao Lu:
    
        Internal change
    
    167277422  by Zhichao Lu:
    
        Internal change
    
    167249393  by Zhichao Lu:
    
        Internal change
    
    167248954  by Zhichao Lu:
    
        Internal change
    
    167189395  by Zhichao Lu:
    
        Internal change
    
    167107797  by Zhichao Lu:
    
        Internal change
    
    167061250  by Zhichao Lu:
    
        Internal change
    
    166871147  by Zhichao Lu:
    
        Internal change
    
    166867617  by Zhichao Lu:
    
        Internal change
    
    166862112  by Zhichao Lu:
    
        Internal change
    
    166715648  by Zhichao Lu:
    
        Internal change
    
    166635615  by Zhichao Lu:
    
        Internal change
    
    166383182  by Zhichao Lu:
    
        Internal change
    
    166371326  by Zhichao Lu:
    
        Internal change
    
    166254711  by Zhichao Lu:
    
        Internal change
    
    166106294  by Zhichao Lu:
    
        Internal change
    
    166081204  by Zhichao Lu:
    
        Internal change
    
    165972262  by Zhichao Lu:
    
        Internal change
    
    165816702  by Zhichao Lu:
    
        Internal change
    
    165764471  by Zhichao Lu:
    
        Internal change
    
    165724134  by Zhichao Lu:
    
        Internal change
    
    165655829  by Zhichao Lu:
    
        Internal change
    
    165587904  by Zhichao Lu:
    
        Internal change
    
    165534540  by Zhichao Lu:
    
        Internal change
    
    165177692  by Zhichao Lu:
    
        Internal change
    
    165091822  by Zhichao Lu:
    
        Internal change
    
    165019730  by Zhichao Lu:
    
        Internal change
    
    165002942  by Zhichao Lu:
    
        Internal change
    
    164897728  by Zhichao Lu:
    
        Internal change
    
    164782618  by Zhichao Lu:
    
        Internal change
    
    164710379  by Zhichao Lu:
    
        Internal change
    
    164639237  by Zhichao Lu:
    
        Internal change
    
    164069251  by Zhichao Lu:
    
        Internal change
    
    164058169  by Zhichao Lu:
    
        Internal change
    
    163913796  by Zhichao Lu:
    
        Internal change
    
    163756696  by Zhichao Lu:
    
        Internal change
    
    163524665  by Zhichao Lu:
    
        Internal change
    
    163393399  by Zhichao Lu:
    
        Internal change
    
    163385733  by Zhichao Lu:
    
        Internal change
    
    162525065  by Zhichao Lu:
    
        Internal change
    
    162376984  by Zhichao Lu:
    
        Internal change
    
    162026661  by Zhichao Lu:
    
        Internal change
    
    161956004  by Zhichao Lu:
    
        Internal change
    
    161817520  by Zhichao Lu:
    
        Internal change
    
    161718688  by Zhichao Lu:
    
        Internal change
    
    161624398  by Zhichao Lu:
    
        Internal change
    
    161575120  by Zhichao Lu:
    
        Internal change
    
    161483997  by Zhichao Lu:
    
        Internal change
    
    161462189  by Zhichao Lu:
    
        Internal change
    
    161452968  by Zhichao Lu:
    
        Internal change
    
    161443992  by Zhichao Lu:
    
        Internal change
    
    161408607  by Zhichao Lu:
    
        Internal change
    
    161262084  by Zhichao Lu:
    
        Internal change
    
    161214023  by Zhichao Lu:
    
        Internal change
    
    161025667  by Zhichao Lu:
    
        Internal change
    
    160982216  by Zhichao Lu:
    
        Internal change
    
    160666760  by Zhichao Lu:
    
        Internal change
    
    160570489  by Zhichao Lu:
    
        Internal change
    
    160553112  by Zhichao Lu:
    
        Internal change
    
    160458261  by Zhichao Lu:
    
        Internal change
    
    160349302  by Zhichao Lu:
    
        Internal change
    
    160296092  by Zhichao Lu:
    
        Internal change
    
    160287348  by Zhichao Lu:
    
        Internal change
    
    160199279  by Zhichao Lu:
    
        Internal change
    
    160160156  by Zhichao Lu:
    
        Internal change
    
    160151954  by Zhichao Lu:
    
        Internal change
    
    160005404  by Zhichao Lu:
    
        Internal change
    
    159983265  by Zhichao Lu:
    
        Internal change
    
    159819896  by Zhichao Lu:
    
        Internal change
    
    159749419  by Zhichao Lu:
    
        Internal change
    
    159596448  by Zhichao Lu:
    
        Internal change
    
    159587801  by Zhichao Lu:
    
        Internal change
    
    159587342  by Zhichao Lu:
    
        Internal change
    
    159476256  by Zhichao Lu:
    
        Internal change
    
    159463992  by Zhichao Lu:
    
        Internal change
    
    159455585  by Zhichao Lu:
    
        Internal change
    
    159270798  by Zhichao Lu:
    
        Internal change
    
    159256633  by Zhichao Lu:
    
        Internal change
    
    159141989  by Zhichao Lu:
    
        Internal change
    
    159079098  by Zhichao Lu:
    
        Internal change
    
    159078559  by Zhichao Lu:
    
        Internal change
    
    159077055  by Zhichao Lu:
    
        Internal change
    
    159072046  by Zhichao Lu:
    
        Internal change
    
    159071092  by Zhichao Lu:
    
        Internal change
    
    159069262  by Zhichao Lu:
    
        Internal change
    
    159037430  by Zhichao Lu:
    
        Internal change
    
    159035747  by Zhichao Lu:
    
        Internal change
    
    159023868  by Zhichao Lu:
    
        Internal change
    
    158939092  by Zhichao Lu:
    
        Internal change
    
    158912561  by Zhichao Lu:
    
        Internal change
    
    158903825  by Zhichao Lu:
    
        Internal change
    
    158894348  by Zhichao Lu:
    
        Internal change
    
    158884934  by Zhichao Lu:
    
        Internal change
    
    158878010  by Zhichao Lu:
    
        Internal change
    
    158874620  by Zhichao Lu:
    
        Internal change
    
    158869501  by Zhichao Lu:
    
        Internal change
    
    158842623  by Zhichao Lu:
    
        Internal change
    
    158801298  by Zhichao Lu:
    
        Internal change
    
    158775487  by Zhichao Lu:
    
        Internal change
    
    158773668  by Zhichao Lu:
    
        Internal change
    
    158771394  by Zhichao Lu:
    
        Internal change
    
    158668928  by Zhichao Lu:
    
        Internal change
    
    158596865  by Zhichao Lu:
    
        Internal change
    
    158587317  by Zhichao Lu:
    
        Internal change
    
    158586348  by Zhichao Lu:
    
        Internal change
    
    158585707  by Zhichao Lu:
    
        Internal change
    
    158577134  by Zhichao Lu:
    
        Internal change
    
    158459749  by Zhichao Lu:
    
        Internal change
    
    158459678  by Zhichao Lu:
    
        Internal change
    
    158328972  by Zhichao Lu:
    
        Internal change
    
    158324255  by Zhichao Lu:
    
        Internal change
    
    158319576  by Zhichao Lu:
    
        Internal change
    
    158290802  by Zhichao Lu:
    
        Internal change
    
    158273041  by Zhichao Lu:
    
        Internal change
    
    158240477  by Zhichao Lu:
    
        Internal change
    
    158204316  by Zhichao Lu:
    
        Internal change
    
    158154161  by Zhichao Lu:
    
        Internal change
    
    158077203  by Zhichao Lu:
    
        Internal change
    
    158041397  by Zhichao Lu:
    
        Internal change
    
    158029233  by Zhichao Lu:
    
        Internal change
    
    157976306  by Zhichao Lu:
    
        Internal change
    
    157966896  by Zhichao Lu:
    
        Internal change
    
    157945642  by Zhichao Lu:
    
        Internal change
    
    157943135  by Zhichao Lu:
    
        Internal change
    
    157942158  by Zhichao Lu:
    
        Internal change
    
    157897866  by Zhichao Lu:
    
        Internal change
    
    157866667  by Zhichao Lu:
    
        Internal change
    
    157845915  by Zhichao Lu:
    
        Internal change
    
    157842592  by Zhichao Lu:
    
        Internal change
    
    157832761  by Zhichao Lu:
    
        Internal change
    
    157824451  by Zhichao Lu:
    
        Internal change
    
    157816531  by Zhichao Lu:
    
        Internal change
    
    157782130  by Zhichao Lu:
    
        Internal change
    
    157733752  by Zhichao Lu:
    
        Internal change
    
    157654577  by Zhichao Lu:
    
        Internal change
    
    157639285  by Zhichao Lu:
    
        Internal change
    
    157530694  by Zhichao Lu:
    
        Internal change
    
    157518469  by Zhichao Lu:
    
        Internal change
    
    157514626  by Zhichao Lu:
    
        Internal change
    
    157481413  by Zhichao Lu:
    
        Internal change
    
    157267863  by Zhichao Lu:
    
        Internal change
    
    157263616  by Zhichao Lu:
    
        Internal change
    
    157234554  by Zhichao Lu:
    
        Internal change
    
    157174595  by Zhichao Lu:
    
        Internal change
    
    157169681  by Zhichao Lu:
    
        Internal change
    
    157156425  by Zhichao Lu:
    
        Internal change
    
    157024436  by Zhichao Lu:
    
        Internal change
    
    157016195  by Zhichao Lu:
    
        Internal change
    
    156941658  by Zhichao Lu:
    
        Internal change
    
    156880859  by Zhichao Lu:
    
        Internal change
    
    156790636  by Zhichao Lu:
    
        Internal change
    
    156565969  by Zhichao Lu:
    
        Internal change
    
    156522345  by Zhichao Lu:
    
        Internal change
    
    156518570  by Zhichao Lu:
    
        Internal change
    
    156509878  by Zhichao Lu:
    
        Internal change
    
    156509134  by Zhichao Lu:
    
        Internal change
    
    156472497  by Zhichao Lu:
    
        Internal change
    
    156471429  by Zhichao Lu:
    
        Internal change
    
    156470865  by Zhichao Lu:
    
        Internal change
    
    156461563  by Zhichao Lu:
    
        Internal change
    
    156437521  by Zhichao Lu:
    
        Internal change
    
    156334994  by Zhichao Lu:
    
        Internal change
    
    156319604  by Zhichao Lu:
    
        Internal change
    
    156234305  by Zhichao Lu:
    
        Internal change
    
    156226207  by Zhichao Lu:
    
        Internal change
    
    156215347  by Zhichao Lu:
    
        Internal change
    
    156127227  by Zhichao Lu:
    
        Internal change
    
    156120405  by Zhichao Lu:
    
        Internal change
    
    156113752  by Zhichao Lu:
    
        Internal change
    
    156098936  by Zhichao Lu:
    
        Internal change
    
    155924066  by Zhichao Lu:
    
        Internal change
    
    155883241  by Zhichao Lu:
    
        Internal change
    
    155806887  by Zhichao Lu:
    
        Internal change
    
    155641849  by Zhichao Lu:
    
        Internal change
    
    155593034  by Zhichao Lu:
    
        Internal change
    
    155570702  by Zhichao Lu:
    
        Internal change
    
    155515306  by Zhichao Lu:
    
        Internal change
    
    155514787  by Zhichao Lu:
    
        Internal change
    
    155445237  by Zhichao Lu:
    
        Internal change
    
    155438672  by Zhichao Lu:
    
        Internal change
    
    155264448  by Zhichao Lu:
    
        Internal change
    
    155222148  by Zhichao Lu:
    
        Internal change
    
    155106590  by Zhichao Lu:
    
        Internal change
    
    155090562  by Zhichao Lu:
    
        Internal change
    
    154973775  by Zhichao Lu:
    
        Internal change
    
    154972880  by Zhichao Lu:
    
        Internal change
    
    154871596  by Zhichao Lu:
    
        Internal change
    
    154835007  by Zhichao Lu:
    
        Internal change
    
    154788175  by Zhichao Lu:
    
        Internal change
    
    154731169  by Zhichao Lu:
    
        Internal change
    
    154721261  by Zhichao Lu:
    
        Internal change
    
    154594626  by Zhichao Lu:
    
        Internal change
    
    154588305  by Zhichao Lu:
    
        Internal change
    
    154578994  by Zhichao Lu:
    
        Internal change
    
    154571515  by Zhichao Lu:
    
        Internal change
    
    154552873  by Zhichao Lu:
    
        Internal change
    
    154549672  by Zhichao Lu:
    
        Internal change
    
    154463631  by Zhichao Lu:
    
        Internal change
    
    154437690  by Zhichao Lu:
    
        Internal change
    
    154412359  by Zhichao Lu:
    
        Internal change
    
    154374026  by Zhichao Lu:
    
        Internal change
    
    154361648  by Zhichao Lu:
    
        Internal change
    
    154310164  by Zhichao Lu:
    
        Internal change
    
    154220862  by Zhichao Lu:
    
        Internal change
    
    154187281  by Zhichao Lu:
    
        Internal change
    
    154186651  by Zhichao Lu:
    
        Internal change
    
    154119783  by Zhichao Lu:
    
        Internal change
    
    154114285  by Zhichao Lu:
    
        Internal change
    
    154095717  by Zhichao Lu:
    
        Internal change
    
    154057972  by Zhichao Lu:
    
        Internal change
    
    154055285  by Zhichao Lu:
    
        Internal change
    
    153659288  by Zhichao Lu:
    
        Internal change
    
    153637797  by Zhichao Lu:
    
        Internal change
    
    153561771  by Zhichao Lu:
    
        Internal change
    
    153540765  by Zhichao Lu:
    
        Internal change
    
    153496128  by Zhichao Lu:
    
        Internal change
    
    153473323  by Zhichao Lu:
    
        Internal change
    
    153368812  by Zhichao Lu:
    
        Internal change
    
    153367292  by Zhichao Lu:
    
        Internal change
    
    153201890  by Zhichao Lu:
    
        Internal change
    
    153074177  by Zhichao Lu:
    
        Internal change
    
    152980017  by Zhichao Lu:
    
        Internal change
    
    152978434  by Zhichao Lu:
    
        Internal change
    
    152951821  by Zhichao Lu:
    
        Internal change
    
    152904076  by Zhichao Lu:
    
        Internal change
    
    152883703  by Zhichao Lu:
    
        Internal change
    
    152869747  by Zhichao Lu:
    
        Internal change
    
    152827463  by Zhichao Lu:
    
        Internal change
    
    152756886  by Zhichao Lu:
    
        Internal change
    
    152752840  by Zhichao Lu:
    
        Internal change
    
    152736347  by Zhichao Lu:
    
        Internal change
    
    152728184  by Zhichao Lu:
    
        Internal change
    
    152720120  by Zhichao Lu:
    
        Internal change
    
    152710964  by Zhichao Lu:
    
        Internal change
    
    152706735  by Zhichao Lu:
    
        Internal change
    
    152681133  by Zhichao Lu:
    
        Internal change
    
    152517758  by Zhichao Lu:
    
        Internal change
    
    152516381  by Zhichao Lu:
    
        Internal change
    
    152511258  by Zhichao Lu:
    
        Internal change
    
    152319164  by Zhichao Lu:
    
        Internal change
    
    152316404  by Zhichao Lu:
    
        Internal change
    
    152309261  by Zhichao Lu:
    
        Internal change
    
    152308007  by Zhichao Lu:
    
        Internal change
    
    152296551  by Zhichao Lu:
    
        Internal change
    
    152188069  by Zhichao Lu:
    
        Internal change
    
    152158644  by Zhichao Lu:
    
        Internal change
    
    152153578  by Zhichao Lu:
    
        Internal change
    
    152152285  by Zhichao Lu:
    
        Internal change
    
    152055035  by Zhichao Lu:
    
        Internal change
    
    152036778  by Zhichao Lu:
    
        Internal change
    
    152020728  by Zhichao Lu:
    
        Internal change
    
    152014842  by Zhichao Lu:
    
        Internal change
    
    151848225  by Zhichao Lu:
    
        Internal change
    
    151741308  by Zhichao Lu:
    
        Internal change
    
    151740499  by Zhichao Lu:
    
        Internal change
    
    151736189  by Zhichao Lu:
    
        Internal change
    
    151612892  by Zhichao Lu:
    
        Internal change
    
    151599502  by Zhichao Lu:
    
        Internal change
    
    151538547  by Zhichao Lu:
    
        Internal change
    
    151496530  by Zhichao Lu:
    
        Internal change
    
    151476070  by Zhichao Lu:
    
        Internal change
    
    151448662  by Zhichao Lu:
    
        Internal change
    
    151411627  by Zhichao Lu:
    
        Internal change
    
    151397737  by Zhichao Lu:
    
        Internal change
    
    151169523  by Zhichao Lu:
    
        Internal change
    
    151148956  by Zhichao Lu:
    
        Internal change
    
    150944227  by Zhichao Lu:
    
        Internal change
    
    150276683  by Zhichao Lu:
    
        Internal change
    
    149986687  by Zhichao Lu:
    
        Internal change
    
    149218749  by Zhichao Lu:
    
        Internal change
    
    PiperOrigin-RevId: 184048729

diff --git a/research/object_detection/BUILD b/research/object_detection/BUILD
index df835b74..6cda848d 100644
--- a/research/object_detection/BUILD
+++ b/research/object_detection/BUILD
@@ -7,6 +7,43 @@ package(
 licenses(["notice"])
 
 # Apache 2.0
+
+py_library(
+    name = "inputs",
+    srcs = [
+        "inputs.py",
+    ],
+    deps = [
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection:trainer",
+        "//tensorflow/models/research/object_detection/builders:dataset_builder",
+        "//tensorflow/models/research/object_detection/builders:preprocessor_builder",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:ops",
+    ],
+)
+
+py_test(
+    name = "inputs_test",
+    srcs = [
+        "inputs_test.py",
+    ],
+    data = [
+        "//tensorflow/models/research/object_detection/data:pet_label_map.pbtxt",
+        "//tensorflow/models/research/object_detection/samples/configs:faster_rcnn_resnet50_pets.config",
+        "//tensorflow/models/research/object_detection/samples/configs:ssd_inception_v2_pets.config",
+        "//tensorflow/models/research/object_detection/test_data:pets_examples.record",
+    ],
+    deps = [
+        ":inputs",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+    ],
+)
+
 py_binary(
     name = "train",
     srcs = [
@@ -15,9 +52,10 @@ py_binary(
     deps = [
         ":trainer",
         "//tensorflow",
-        "//tensorflow_models/object_detection/builders:input_reader_builder",
-        "//tensorflow_models/object_detection/builders:model_builder",
-        "//tensorflow_models/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/builders:dataset_builder",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
     ],
 )
 
@@ -26,14 +64,14 @@ py_library(
     srcs = ["trainer.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/builders:optimizer_builder",
-        "//tensorflow_models/object_detection/builders:preprocessor_builder",
-        "//tensorflow_models/object_detection/core:batcher",
-        "//tensorflow_models/object_detection/core:preprocessor",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/object_detection/utils:variables_helper",
-        "//tensorflow_models/slim:model_deploy",
+        "//tensorflow/models/research/object_detection/builders:optimizer_builder",
+        "//tensorflow/models/research/object_detection/builders:preprocessor_builder",
+        "//tensorflow/models/research/object_detection/core:batcher",
+        "//tensorflow/models/research/object_detection/core:preprocessor",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:variables_helper",
+        "//third_party/tensorflow_models/slim:model_deploy",
     ],
 )
 
@@ -43,10 +81,10 @@ py_test(
     deps = [
         ":trainer",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/core:model",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/protos:train_py_pb2",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/core:model",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
     ],
 )
 
@@ -57,13 +95,13 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:box_list_ops",
-        "//tensorflow_models/object_detection/core:keypoint_ops",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:label_map_util",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/object_detection/utils:visualization_utils",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
+        "//tensorflow/models/research/object_detection/core:keypoint_ops",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:visualization_utils",
     ],
 )
 
@@ -72,11 +110,11 @@ py_library(
     srcs = ["evaluator.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection:eval_util",
-        "//tensorflow_models/object_detection/core:prefetcher",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/protos:eval_py_pb2",
-        "//tensorflow_models/object_detection/utils:object_detection_evaluation",
+        "//tensorflow/models/research/object_detection:eval_util",
+        "//tensorflow/models/research/object_detection/core:prefetcher",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:object_detection_evaluation",
     ],
 )
 
@@ -88,10 +126,11 @@ py_binary(
     deps = [
         ":evaluator",
         "//tensorflow",
-        "//tensorflow_models/object_detection/builders:input_reader_builder",
-        "//tensorflow_models/object_detection/builders:model_builder",
-        "//tensorflow_models/object_detection/utils:config_util",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/builders:dataset_builder",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
 
@@ -103,9 +142,9 @@ py_library(
     deps = [
         "//tensorflow",
         "//tensorflow/python/tools:freeze_graph_lib",
-        "//tensorflow_models/object_detection/builders:model_builder",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/data_decoders:tf_example_decoder",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
     ],
 )
 
@@ -117,9 +156,9 @@ py_test(
     deps = [
         ":exporter",
         "//tensorflow",
-        "//tensorflow_models/object_detection/builders:model_builder",
-        "//tensorflow_models/object_detection/core:model",
-        "//tensorflow_models/object_detection/protos:pipeline_py_pb2",
+        "//tensorflow/models/research/object_detection/builders:model_builder",
+        "//tensorflow/models/research/object_detection/core:model",
+        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
     ],
 )
 
@@ -131,6 +170,6 @@ py_binary(
     deps = [
         ":exporter",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:pipeline_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
     ],
 )
diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index a0d8ddf2..d4a4adef 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -32,6 +32,7 @@ https://scholar.googleusercontent.com/scholar.bib?q=info:l291WsrB-hQJ:scholar.go
 * Derek Chow, github: [derekjchow](https://github.com/derekjchow)
 * Chen Sun, github: [jesu9](https://github.com/jesu9)
 * Menglong Zhu, github: [dreamdragon](https://github.com/dreamdragon)
+* Alireza Fathi, github: [afathi3](https://github.com/afathi3)
 
 
 ## Table of contents
diff --git a/research/object_detection/anchor_generators/BUILD b/research/object_detection/anchor_generators/BUILD
index cb421a0c..63662fb6 100644
--- a/research/object_detection/anchor_generators/BUILD
+++ b/research/object_detection/anchor_generators/BUILD
@@ -14,9 +14,9 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:anchor_generator",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/core:anchor_generator",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
@@ -28,6 +28,7 @@ py_test(
     deps = [
         ":grid_anchor_generator",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
 
@@ -39,8 +40,8 @@ py_library(
     deps = [
         ":grid_anchor_generator",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:anchor_generator",
-        "//tensorflow_models/object_detection/core:box_list_ops",
+        "//tensorflow/models/research/object_detection/core:anchor_generator",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
     ],
 )
 
@@ -51,6 +52,32 @@ py_test(
     ],
     deps = [
         ":multiple_grid_anchor_generator",
-        "//third_party/py/numpy",
+        "//numpy",
+        "//tensorflow/models/research/object_detection/utils:test_case",
+    ],
+)
+
+py_library(
+    name = "multiscale_grid_anchor_generator",
+    srcs = [
+        "multiscale_grid_anchor_generator.py",
+    ],
+    deps = [
+        ":grid_anchor_generator",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:anchor_generator",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
+    ],
+)
+
+py_test(
+    name = "multiscale_grid_anchor_generator_test",
+    srcs = [
+        "multiscale_grid_anchor_generator_test.py",
+    ],
+    deps = [
+        ":multiscale_grid_anchor_generator",
+        "//numpy",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator.py b/research/object_detection/anchor_generators/grid_anchor_generator.py
index d2ea2c07..b3a51791 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator.py
@@ -42,25 +42,27 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
       scales: a list of (float) scales, default=(0.5, 1.0, 2.0)
       aspect_ratios: a list of (float) aspect ratios, default=(0.5, 1.0, 2.0)
       base_anchor_size: base anchor size as height, width (
-                        (length-2 float32 list, default=[256, 256])
+                        (length-2 float32 list or tensor, default=[256, 256])
       anchor_stride: difference in centers between base anchors for adjacent
-                     grid positions (length-2 float32 list, default=[16, 16])
+                     grid positions (length-2 float32 list or tensor,
+                     default=[16, 16])
       anchor_offset: center of the anchor with scale and aspect ratio 1 for the
                      upper left element of the grid, this should be zero for
                      feature networks with only VALID padding and even receptive
                      field size, but may need additional calculation if other
-                     padding is used (length-2 float32 tensor, default=[0, 0])
+                     padding is used (length-2 float32 list or tensor,
+                     default=[0, 0])
     """
     # Handle argument defaults
     if base_anchor_size is None:
       base_anchor_size = [256, 256]
-    base_anchor_size = tf.constant(base_anchor_size, tf.float32)
+    base_anchor_size = tf.to_float(tf.convert_to_tensor(base_anchor_size))
     if anchor_stride is None:
       anchor_stride = [16, 16]
-    anchor_stride = tf.constant(anchor_stride, dtype=tf.float32)
+    anchor_stride = tf.to_float(tf.convert_to_tensor(anchor_stride))
     if anchor_offset is None:
       anchor_offset = [0, 0]
-    anchor_offset = tf.constant(anchor_offset, dtype=tf.float32)
+    anchor_offset = tf.to_float(tf.convert_to_tensor(anchor_offset))
 
     self._scales = scales
     self._aspect_ratios = aspect_ratios
@@ -91,7 +93,11 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
         allowed.
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
+        this BoxList also holds a `feature_map_index` field which is set to 0
+        for each anchor; this field exists for interchangeability reasons with
+        the MultipleGridAnchorGenerator (see the docstring for the corresponding
+        `_generate` function in multiple_grid_anchor_generator.py)
     Raises:
       ValueError: if feature_map_shape_list, box_specs_list do not have the same
         length.
@@ -109,13 +115,20 @@ class GridAnchorGenerator(anchor_generator.AnchorGenerator):
                                                    self._aspect_ratios)
     scales_grid = tf.reshape(scales_grid, [-1])
     aspect_ratios_grid = tf.reshape(aspect_ratios_grid, [-1])
-    return tile_anchors(grid_height,
-                        grid_width,
-                        scales_grid,
-                        aspect_ratios_grid,
-                        self._base_anchor_size,
-                        self._anchor_stride,
-                        self._anchor_offset)
+    anchors = tile_anchors(grid_height,
+                           grid_width,
+                           scales_grid,
+                           aspect_ratios_grid,
+                           self._base_anchor_size,
+                           self._anchor_stride,
+                           self._anchor_offset)
+
+    num_anchors = anchors.num_boxes_static()
+    if num_anchors is None:
+      num_anchors = anchors.num_boxes()
+    anchor_indices = tf.zeros([num_anchors])
+    anchors.add_field('feature_map_index', anchor_indices)
+    return anchors
 
 
 def tile_anchors(grid_height,
diff --git a/research/object_detection/anchor_generators/grid_anchor_generator_test.py b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
index 80a82a39..6c53e89d 100644
--- a/research/object_detection/anchor_generators/grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/grid_anchor_generator_test.py
@@ -14,62 +14,90 @@
 # ==============================================================================
 
 """Tests for object_detection.grid_anchor_generator."""
-
+import numpy as np
 import tensorflow as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
+from object_detection.utils import test_case
 
 
-class GridAnchorGeneratorTest(tf.test.TestCase):
+class GridAnchorGeneratorTest(test_case.TestCase):
 
   def test_construct_single_anchor(self):
     """Builds a 1x1 anchor grid to test the size of the output boxes."""
-    scales = [0.5, 1.0, 2.0]
-    aspect_ratios = [0.25, 1.0, 4.0]
-    anchor_offset = [7, -3]
+    def graph_fn():
+      scales = [0.5, 1.0, 2.0]
+      aspect_ratios = [0.25, 1.0, 4.0]
+      anchor_offset = [7, -3]
+      anchor_generator = grid_anchor_generator.GridAnchorGenerator(
+          scales, aspect_ratios, anchor_offset=anchor_offset)
+      anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
     exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],
                           [-505, -131, 519, 125], [-57, -67, 71, 61],
                           [-121, -131, 135, 125], [-249, -259, 263, 253],
                           [-25, -131, 39, 125], [-57, -259, 71, 253],
                           [-121, -515, 135, 509]]
-
-    anchor_generator = grid_anchor_generator.GridAnchorGenerator(
-        scales, aspect_ratios,
-        anchor_offset=anchor_offset)
-    anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-    anchor_corners = anchors.get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_anchor_grid(self):
-    base_anchor_size = [10, 10]
-    anchor_stride = [19, 19]
-    anchor_offset = [0, 0]
-    scales = [0.5, 1.0, 2.0]
-    aspect_ratios = [1.0]
+    def graph_fn():
+      base_anchor_size = [10, 10]
+      anchor_stride = [19, 19]
+      anchor_offset = [0, 0]
+      scales = [0.5, 1.0, 2.0]
+      aspect_ratios = [1.0]
 
+      anchor_generator = grid_anchor_generator.GridAnchorGenerator(
+          scales,
+          aspect_ratios,
+          base_anchor_size=base_anchor_size,
+          anchor_stride=anchor_stride,
+          anchor_offset=anchor_offset)
+
+      anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
     exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
                           [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],
                           [-5., 14., 5, 24], [-10., 9., 10, 29],
                           [16.5, -2.5, 21.5, 2.5], [14., -5., 24, 5],
                           [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],
                           [14., 14., 24, 24], [9., 9., 29, 29]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
-    anchor_generator = grid_anchor_generator.GridAnchorGenerator(
-        scales,
-        aspect_ratios,
-        base_anchor_size=base_anchor_size,
-        anchor_stride=anchor_stride,
-        anchor_offset=anchor_offset)
+  def test_construct_anchor_grid_with_dynamic_feature_map_shapes(self):
+    def graph_fn(feature_map_height, feature_map_width):
+      base_anchor_size = [10, 10]
+      anchor_stride = [19, 19]
+      anchor_offset = [0, 0]
+      scales = [0.5, 1.0, 2.0]
+      aspect_ratios = [1.0]
+      anchor_generator = grid_anchor_generator.GridAnchorGenerator(
+          scales,
+          aspect_ratios,
+          base_anchor_size=base_anchor_size,
+          anchor_stride=anchor_stride,
+          anchor_offset=anchor_offset)
 
-    anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-    anchor_corners = anchors.get()
+      anchors = anchor_generator.generate(
+          feature_map_shape_list=[(feature_map_height, feature_map_width)])
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
 
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
+                          [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],
+                          [-5., 14., 5, 24], [-10., 9., 10, 29],
+                          [16.5, -2.5, 21.5, 2.5], [14., -5., 24, 5],
+                          [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],
+                          [14., 14., 24, 24], [9., 9., 29, 29]]
+    anchor_corners_out = self.execute_cpu(graph_fn,
+                                          [np.array(2, dtype=np.int32),
+                                           np.array(2, dtype=np.int32)])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index b49f12dc..ead0ad64 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -165,7 +165,10 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         grid.
 
     Returns:
-      boxes: a BoxList holding a collection of N anchor boxes
+      boxes: a BoxList holding a collection of N anchor boxes.  Additionally
+        this BoxList also holds a `feature_map_index` field which, for each
+        anchor, stores the index of the corresponding feature map which was used
+        to generate it.
     Raises:
       ValueError: if feature_map_shape_list, box_specs_list do not have the same
         length.
@@ -208,6 +211,7 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         raise ValueError('%s must be a list of pairs.' % arg_name)
 
     anchor_grid_list = []
+    anchor_indices_list = []
     min_im_shape = tf.minimum(im_height, im_width)
     scale_height = min_im_shape / im_height
     scale_width = min_im_shape / im_width
@@ -215,32 +219,40 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         scale_height * self._base_anchor_size[0],
         scale_width * self._base_anchor_size[1]
     ]
-    for grid_size, scales, aspect_ratios, stride, offset in zip(
-        feature_map_shape_list, self._scales, self._aspect_ratios,
-        anchor_strides, anchor_offsets):
-      anchor_grid_list.append(
-          grid_anchor_generator.tile_anchors(
-              grid_height=grid_size[0],
-              grid_width=grid_size[1],
-              scales=scales,
-              aspect_ratios=aspect_ratios,
-              base_anchor_size=base_anchor_size,
-              anchor_stride=stride,
-              anchor_offset=offset))
+    for feature_map_index, (
+        grid_size, scales, aspect_ratios, stride, offset) in enumerate(
+            zip(feature_map_shape_list, self._scales, self._aspect_ratios,
+                anchor_strides, anchor_offsets)):
+      tiled_anchors = grid_anchor_generator.tile_anchors(
+          grid_height=grid_size[0],
+          grid_width=grid_size[1],
+          scales=scales,
+          aspect_ratios=aspect_ratios,
+          base_anchor_size=base_anchor_size,
+          anchor_stride=stride,
+          anchor_offset=offset)
+      anchor_grid_list.append(tiled_anchors)
+      num_anchors_in_layer = tiled_anchors.num_boxes_static()
+      if num_anchors_in_layer is None:
+        num_anchors_in_layer = tiled_anchors.num_boxes()
+      anchor_indices_list.append(
+          feature_map_index * tf.ones([num_anchors_in_layer]))
     concatenated_anchors = box_list_ops.concatenate(anchor_grid_list)
+    anchor_indices = tf.concat(anchor_indices_list, 0)
     num_anchors = concatenated_anchors.num_boxes_static()
     if num_anchors is None:
       num_anchors = concatenated_anchors.num_boxes()
     if self._clip_window is not None:
       concatenated_anchors = box_list_ops.clip_to_window(
           concatenated_anchors, self._clip_window, filter_nonoverlapping=False)
-      # TODO(jonathanhuang): make reshape an option for the clip_to_window op
+      # TODO: make reshape an option for the clip_to_window op
       concatenated_anchors.set(
           tf.reshape(concatenated_anchors.get(), [num_anchors, 4]))
 
     stddevs_tensor = 0.01 * tf.ones(
         [num_anchors, 4], dtype=tf.float32, name='stddevs')
     concatenated_anchors.add_field('stddev', stddevs_tensor)
+    concatenated_anchors.add_field('feature_map_index', anchor_indices)
 
     return concatenated_anchors
 
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
index 03ec970b..2afbf433 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator_test.py
@@ -20,35 +20,45 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.anchor_generators import multiple_grid_anchor_generator as ag
+from object_detection.utils import test_case
 
 
-class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
+class MultipleGridAnchorGeneratorTest(test_case.TestCase):
 
   def test_construct_single_anchor_grid(self):
     """Builds a 1x1 anchor grid to test the size of the output boxes."""
+    def graph_fn():
+
+      box_specs_list = [[(.5, .25), (1.0, .25), (2.0, .25),
+                         (.5, 1.0), (1.0, 1.0), (2.0, 1.0),
+                         (.5, 4.0), (1.0, 4.0), (2.0, 4.0)]]
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list,
+          base_anchor_size=tf.constant([256, 256], dtype=tf.float32),
+          anchor_strides=[(16, 16)],
+          anchor_offsets=[(7, -3)])
+      anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
+      return anchors.get()
     exp_anchor_corners = [[-121, -35, 135, 29], [-249, -67, 263, 61],
                           [-505, -131, 519, 125], [-57, -67, 71, 61],
                           [-121, -131, 135, 125], [-249, -259, 263, 253],
                           [-25, -131, 39, 125], [-57, -259, 71, 253],
                           [-121, -515, 135, 509]]
 
-    box_specs_list = [[(.5, .25), (1.0, .25), (2.0, .25),
-                       (.5, 1.0), (1.0, 1.0), (2.0, 1.0),
-                       (.5, 4.0), (1.0, 4.0), (2.0, 4.0)]]
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list,
-        base_anchor_size=tf.constant([256, 256], dtype=tf.float32),
-        anchor_strides=[(16, 16)],
-        anchor_offsets=[(7, -3)])
-    anchors = anchor_generator.generate(feature_map_shape_list=[(1, 1)])
-    anchor_corners = anchors.get()
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_anchor_grid(self):
-    box_specs_list = [[(0.5, 1.0), (1.0, 1.0), (2.0, 1.0)]]
+    def graph_fn():
+      box_specs_list = [[(0.5, 1.0), (1.0, 1.0), (2.0, 1.0)]]
 
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list,
+          base_anchor_size=tf.constant([10, 10], dtype=tf.float32),
+          anchor_strides=[(19, 19)],
+          anchor_offsets=[(0, 0)])
+      anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
+      return anchors.get()
     exp_anchor_corners = [[-2.5, -2.5, 2.5, 2.5], [-5., -5., 5., 5.],
                           [-10., -10., 10., 10.], [-2.5, 16.5, 2.5, 21.5],
                           [-5., 14., 5, 24], [-10., 9., 10, 29],
@@ -56,55 +66,74 @@ class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
                           [9., -10., 29, 10], [16.5, 16.5, 21.5, 21.5],
                           [14., 14., 24, 24], [9., 9., 29, 29]]
 
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list,
-        base_anchor_size=tf.constant([10, 10], dtype=tf.float32),
-        anchor_strides=[(19, 19)],
-        anchor_offsets=[(0, 0)])
-    anchors = anchor_generator.generate(feature_map_shape_list=[(2, 2)])
-    anchor_corners = anchors.get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_anchor_grid_non_square(self):
-    box_specs_list = [[(1.0, 1.0)]]
+
+    def graph_fn():
+      box_specs_list = [[(1.0, 1.0)]]
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list, base_anchor_size=tf.constant([1, 1],
+                                                       dtype=tf.float32))
+      anchors = anchor_generator.generate(feature_map_shape_list=[(tf.constant(
+          1, dtype=tf.int32), tf.constant(2, dtype=tf.int32))])
+      return anchors.get()
 
     exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list, base_anchor_size=tf.constant([1, 1], dtype=tf.float32))
-    anchors = anchor_generator.generate(feature_map_shape_list=[(tf.constant(
-        1, dtype=tf.int32), tf.constant(2, dtype=tf.int32))])
-    anchor_corners = anchors.get()
+  def test_construct_dynamic_size_anchor_grid(self):
 
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    def graph_fn(height, width):
+      box_specs_list = [[(1.0, 1.0)]]
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list, base_anchor_size=tf.constant([1, 1],
+                                                       dtype=tf.float32))
+      anchors = anchor_generator.generate(feature_map_shape_list=[(height,
+                                                                   width)])
+      return anchors.get()
 
-  def test_construct_anchor_grid_normalized(self):
-    box_specs_list = [[(1.0, 1.0)]]
+    exp_anchor_corners = [[0., -0.25, 1., 0.75], [0., 0.25, 1., 1.25]]
 
-    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
+    anchor_corners_out = self.execute_cpu(graph_fn,
+                                          [np.array(1, dtype=np.int32),
+                                           np.array(2, dtype=np.int32)])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list, base_anchor_size=tf.constant([1, 1], dtype=tf.float32))
-    anchors = anchor_generator.generate(
-        feature_map_shape_list=[(tf.constant(1, dtype=tf.int32), tf.constant(
-            2, dtype=tf.int32))],
-        im_height=320,
-        im_width=640)
-    anchor_corners = anchors.get()
+  def test_construct_anchor_grid_normalized(self):
+    def graph_fn():
+      box_specs_list = [[(1.0, 1.0)]]
+
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list, base_anchor_size=tf.constant([1, 1],
+                                                       dtype=tf.float32))
+      anchors = anchor_generator.generate(
+          feature_map_shape_list=[(tf.constant(1, dtype=tf.int32), tf.constant(
+              2, dtype=tf.int32))],
+          im_height=320,
+          im_width=640)
+      return anchors.get()
 
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+    exp_anchor_corners = [[0., 0., 1., 0.5], [0., 0.5, 1., 1.]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
   def test_construct_multiple_grids(self):
-    box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],
-                      [(1.0, 1.0), (1.0, 0.5)]]
 
+    def graph_fn():
+      box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],
+                        [(1.0, 1.0), (1.0, 0.5)]]
+
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list,
+          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
+          anchor_strides=[(.25, .25), (.5, .5)],
+          anchor_offsets=[(.125, .125), (.25, .25)])
+      anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4),
+                                                                  (2, 2)])
+      return anchors.get()
     # height and width of box with .5 aspect ratio
     h = np.sqrt(2)
     w = 1.0/np.sqrt(2)
@@ -121,26 +150,27 @@ class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
                             [.125-1.0, .125-1.0, .125+1.0, .125+1.0],
                             [.125-.5*h, .125-.5*w, .125+.5*h, .125+.5*w],]
 
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list,
-        base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
-        anchor_strides=[(.25, .25), (.5, .5)],
-        anchor_offsets=[(.125, .125), (.25, .25)])
-    anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2)])
-    anchor_corners = anchors.get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertEquals(anchor_corners_out.shape, (56, 4))
-      big_grid_corners = anchor_corners_out[0:3, :]
-      small_grid_corners = anchor_corners_out[48:, :]
-      self.assertAllClose(small_grid_corners, exp_small_grid_corners)
-      self.assertAllClose(big_grid_corners, exp_big_grid_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertEquals(anchor_corners_out.shape, (56, 4))
+    big_grid_corners = anchor_corners_out[0:3, :]
+    small_grid_corners = anchor_corners_out[48:, :]
+    self.assertAllClose(small_grid_corners, exp_small_grid_corners)
+    self.assertAllClose(big_grid_corners, exp_big_grid_corners)
 
   def test_construct_multiple_grids_with_clipping(self):
-    box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],
-                      [(1.0, 1.0), (1.0, 0.5)]]
 
+    def graph_fn():
+      box_specs_list = [[(1.0, 1.0), (2.0, 1.0), (1.0, 0.5)],
+                        [(1.0, 1.0), (1.0, 0.5)]]
+
+      clip_window = tf.constant([0, 0, 1, 1], dtype=tf.float32)
+      anchor_generator = ag.MultipleGridAnchorGenerator(
+          box_specs_list,
+          base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
+          clip_window=clip_window)
+      anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4),
+                                                                  (2, 2)])
+      return anchors.get()
     # height and width of box with .5 aspect ratio
     h = np.sqrt(2)
     w = 1.0/np.sqrt(2)
@@ -153,18 +183,9 @@ class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
                               [.25, .25, 1, 1],
                               [.75-.5*h, .75-.5*w, 1, 1]]
 
-    clip_window = tf.constant([0, 0, 1, 1], dtype=tf.float32)
-    anchor_generator = ag.MultipleGridAnchorGenerator(
-        box_specs_list,
-        base_anchor_size=tf.constant([1.0, 1.0], dtype=tf.float32),
-        clip_window=clip_window)
-    anchors = anchor_generator.generate(feature_map_shape_list=[(4, 4), (2, 2)])
-    anchor_corners = anchors.get()
-
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      small_grid_corners = anchor_corners_out[48:, :]
-      self.assertAllClose(small_grid_corners, exp_small_grid_corners)
+    anchor_corners_out = self.execute(graph_fn, [])
+    small_grid_corners = anchor_corners_out[48:, :]
+    self.assertAllClose(small_grid_corners, exp_small_grid_corners)
 
   def test_invalid_box_specs(self):
     # not all box specs are pairs
@@ -229,38 +250,39 @@ class MultipleGridAnchorGeneratorTest(tf.test.TestCase):
       anchor_generator.generate(feature_map_shape_list=[(4), (2, 2)])
 
 
-class CreateSSDAnchorsTest(tf.test.TestCase):
+class CreateSSDAnchorsTest(test_case.TestCase):
 
   def test_create_ssd_anchors_returns_correct_shape(self):
-    anchor_generator = ag.create_ssd_anchors(
-        num_layers=6,
-        min_scale=0.2,
-        max_scale=0.95,
-        aspect_ratios=(1.0, 2.0, 3.0, 1.0 / 2, 1.0 / 3),
-        reduce_boxes_in_lowest_layer=True)
-
-    feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
-                              (5, 5), (3, 3), (1, 1)]
-    anchors = anchor_generator.generate(
-        feature_map_shape_list=feature_map_shape_list)
-    anchor_corners = anchors.get()
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertEquals(anchor_corners_out.shape, (7308, 4))
-
-    anchor_generator = ag.create_ssd_anchors(
-        num_layers=6, min_scale=0.2, max_scale=0.95,
-        aspect_ratios=(1.0, 2.0, 3.0, 1.0/2, 1.0/3),
-        reduce_boxes_in_lowest_layer=False)
-
-    feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
-                              (5, 5), (3, 3), (1, 1)]
-    anchors = anchor_generator.generate(
-        feature_map_shape_list=feature_map_shape_list)
-    anchor_corners = anchors.get()
-    with self.test_session():
-      anchor_corners_out = anchor_corners.eval()
-      self.assertEquals(anchor_corners_out.shape, (11640, 4))
+
+    def graph_fn1():
+      anchor_generator = ag.create_ssd_anchors(
+          num_layers=6,
+          min_scale=0.2,
+          max_scale=0.95,
+          aspect_ratios=(1.0, 2.0, 3.0, 1.0 / 2, 1.0 / 3),
+          reduce_boxes_in_lowest_layer=True)
+
+      feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
+                                (5, 5), (3, 3), (1, 1)]
+      anchors = anchor_generator.generate(
+          feature_map_shape_list=feature_map_shape_list)
+      return anchors.get()
+    anchor_corners_out = self.execute(graph_fn1, [])
+    self.assertEquals(anchor_corners_out.shape, (7308, 4))
+
+    def graph_fn2():
+      anchor_generator = ag.create_ssd_anchors(
+          num_layers=6, min_scale=0.2, max_scale=0.95,
+          aspect_ratios=(1.0, 2.0, 3.0, 1.0/2, 1.0/3),
+          reduce_boxes_in_lowest_layer=False)
+
+      feature_map_shape_list = [(38, 38), (19, 19), (10, 10),
+                                (5, 5), (3, 3), (1, 1)]
+      anchors = anchor_generator.generate(
+          feature_map_shape_list=feature_map_shape_list)
+      return anchors.get()
+    anchor_corners_out = self.execute(graph_fn2, [])
+    self.assertEquals(anchor_corners_out.shape, (11640, 4))
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
new file mode 100644
index 00000000..a4e4eac8
--- /dev/null
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -0,0 +1,123 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Generates grid anchors on the fly corresponding to multiple CNN layers.
+
+Generates grid anchors on the fly corresponding to multiple CNN layers as
+described in:
+"Focal Loss for Dense Object Detection"
+T.-Y. Lin, P. Goyal, R. Girshick, K. He, P. Dollar
+"""
+
+from object_detection.anchor_generators import grid_anchor_generator
+from object_detection.core import box_list_ops
+
+
+class MultiscaleGridAnchorGenerator(object):
+  """Generate a grid of anchors for multiple CNN layers."""
+
+  def __init__(self, min_level, max_level, anchor_scale, aspect_ratios,
+               scales_per_octave):
+    """Constructs a MultiscaleGridAnchorGenerator.
+
+    To construct anchors, at multiple scale resolutions, one must provide a
+    the minimum level and maximum levels on a scale pyramid. To define the size
+    of anchor, the anchor scale is provided to decide the size relatively to the
+    stride of the corresponding feature map. The generator allows one pixel
+    location on feature map maps to multiple anchors, that have different aspect
+    ratios and intermediate scales.
+
+    Args:
+      min_level: minimum level in feature pyramid.
+      max_level: maximum level in feature pyramid.
+      anchor_scale: anchor scale and feature stride define the size of the base
+        anchor on an image. For example, given a feature pyramid with strides
+        [2^3, ..., 2^7] and anchor scale 4. The base anchor size is
+        4 * [2^3, ..., 2^7].
+      aspect_ratios: list or tuple of (float) aspect ratios to place on each
+        grid point.
+      scales_per_octave: integer number of intermediate scales per scale octave.
+    """
+    self._anchor_grid_info = []
+    self._aspect_ratios = aspect_ratios
+    self._scales_per_octave = scales_per_octave
+
+    for level in range(min_level, max_level + 1):
+      anchor_stride = [2**level, 2**level]
+      scales = []
+      aspects = []
+      for scale in range(scales_per_octave):
+        scales.append(2**(float(scale) / scales_per_octave))
+      for aspect_ratio in aspect_ratios:
+        aspects.append(aspect_ratio)
+      base_anchor_size = [2**level * anchor_scale, 2**level * anchor_scale]
+      self._anchor_grid_info.append({
+          'level': level,
+          'info': [scales, aspects, base_anchor_size, anchor_stride]
+      })
+
+  def name_scope(self):
+    return 'MultiscaleGridAnchorGenerator'
+
+  def num_anchors_per_location(self):
+    """Returns the number of anchors per spatial location.
+
+    Returns:
+      a list of integers, one for each expected feature map to be passed to
+      the Generate function.
+    """
+    return self._aspect_ratios * self._scales_per_octave
+
+  def generate(self, feature_map_shape_list, im_height, im_width):
+    """Generates a collection of bounding boxes to be used as anchors.
+
+    Args:
+      feature_map_shape_list: list of pairs of convnet layer resolutions in the
+        format [(height_0, width_0), (height_1, width_1), ...]. For example,
+        setting feature_map_shape_list=[(8, 8), (7, 7)] asks for anchors that
+        correspond to an 8x8 layer followed by a 7x7 layer.
+      im_height: the height of the image to generate the grid for.
+      im_width: the width of the image to generate the grid for.
+
+    Returns:
+      boxes: a BoxList holding a collection of N anchor boxes
+    """
+
+    anchor_grid_list = []
+    for feat_shape, grid_info in zip(feature_map_shape_list,
+                                     self._anchor_grid_info):
+      # TODO check the feature_map_shape_list is consistent with
+      # self._anchor_grid_info
+      level = grid_info['level']
+      stride = 2**level
+      scales, aspect_ratios, base_anchor_size, anchor_stride = grid_info['info']
+      feat_h = feat_shape[0]
+      feat_w = feat_shape[1]
+      anchor_offset = [0, 0]
+      if im_height % 2.0**level == 0:
+        anchor_offset[0] = stride / 2.0
+      if im_width % 2.0**level == 0:
+        anchor_offset[1] = stride / 2.0
+      ag = grid_anchor_generator.GridAnchorGenerator(
+          scales,
+          aspect_ratios,
+          base_anchor_size=base_anchor_size,
+          anchor_stride=anchor_stride,
+          anchor_offset=anchor_offset)
+      anchor_grid_list.append(
+          ag.generate(feature_map_shape_list=[(feat_h, feat_w)]))
+
+    concatenated_anchors = box_list_ops.concatenate(anchor_grid_list)
+
+    return concatenated_anchors
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
new file mode 100644
index 00000000..9bf31f30
--- /dev/null
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -0,0 +1,193 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for anchor_generators.multiscale_grid_anchor_generator_test.py."""
+import numpy as np
+import tensorflow as tf
+
+from object_detection.anchor_generators import multiscale_grid_anchor_generator as mg
+from object_detection.utils import test_case
+
+
+class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
+
+  def test_construct_single_anchor(self):
+    min_level = 5
+    max_level = 5
+    anchor_scale = 4.0
+    aspect_ratios = [1.0]
+    scales_per_octave = 1
+    im_height = 64
+    im_width = 64
+    feature_map_shape_list = [(2, 2)]
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112]]
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+    anchors = anchor_generator.generate(feature_map_shape_list,
+                                        im_height, im_width)
+    anchor_corners = anchors.get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_odd_input_dimension(self):
+
+    def graph_fn():
+      min_level = 5
+      max_level = 5
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 65
+      im_width = 65
+      feature_map_shape_list = [(3, 3)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
+                                          im_width)
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
+    anchor_corners_out = self.execute(graph_fn, [])
+    exp_anchor_corners = [[-64, -64, 64, 64],
+                          [-64, -32, 64, 96],
+                          [-64, 0, 64, 128],
+                          [-32, -64, 96, 64],
+                          [-32, -32, 96, 96],
+                          [-32, 0, 96, 128],
+                          [0, -64, 128, 64],
+                          [0, -32, 128, 96],
+                          [0, 0, 128, 128]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_on_two_feature_maps(self):
+
+    def graph_fn():
+      min_level = 5
+      max_level = 6
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(2, 2), (1, 1)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
+                                          im_width)
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
+
+    anchor_corners_out = self.execute(graph_fn, [])
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112],
+                          [-96, -96, 160, 160]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_two_scales_per_octave(self):
+
+    def graph_fn():
+      min_level = 6
+      max_level = 6
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 2
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(1, 1), (1, 1)]
+
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
+                                          im_width)
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
+    # There are 4 set of anchors in this configuration. The order is:
+    # [[2**0.0 intermediate scale + 1.0 aspect],
+    #  [2**0.5 intermediate scale + 1.0 aspect]]
+    exp_anchor_corners = [[-96., -96., 160., 160.],
+                          [-149.0193, -149.0193, 213.0193, 213.0193]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchor_with_two_scales_per_octave_and_aspect(self):
+    def graph_fn():
+      min_level = 6
+      max_level = 6
+      anchor_scale = 4.0
+      aspect_ratios = [1.0, 2.0]
+      scales_per_octave = 2
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(1, 1), (1, 1), (1, 1), (1, 1)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
+                                          im_width)
+      anchor_corners = anchors.get()
+      return anchor_corners
+    # There are 4 set of anchors in this configuration. The order is:
+    # [[2**0.0 intermediate scale + 1.0 aspect],
+    #  [2**0.5 intermediate scale + 1.0 aspect],
+    #  [2**0.0 intermediate scale + 2.0 aspect],
+    #  [2**0.5 intermediate scale + 2.0 aspect]]
+    exp_anchor_corners = [[-96., -96., 160., 160.],
+                          [-149.0193, -149.0193, 213.0193, 213.0193],
+                          [-58.50967, -149.0193, 122.50967, 213.0193],
+                          [-96., -224., 160., 288.]]
+    anchor_corners_out = self.execute(graph_fn, [])
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_single_anchors_on_feature_maps_with_dynamic_shape(self):
+
+    def graph_fn(feature_map1_height, feature_map1_width, feature_map2_height,
+                 feature_map2_width):
+      min_level = 5
+      max_level = 6
+      anchor_scale = 4.0
+      aspect_ratios = [1.0]
+      scales_per_octave = 1
+      im_height = 64
+      im_width = 64
+      feature_map_shape_list = [(feature_map1_height, feature_map1_width),
+                                (feature_map2_height, feature_map2_width)]
+      anchor_generator = mg.MultiscaleGridAnchorGenerator(
+          min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave)
+      anchors = anchor_generator.generate(feature_map_shape_list, im_height,
+                                          im_width)
+      anchor_corners = anchors.get()
+      return (anchor_corners,)
+
+    anchor_corners_out = self.execute_cpu(graph_fn, [
+        np.array(2, dtype=np.int32),
+        np.array(2, dtype=np.int32),
+        np.array(1, dtype=np.int32),
+        np.array(1, dtype=np.int32)
+    ])
+    exp_anchor_corners = [[-48, -48, 80, 80],
+                          [-48, -16, 80, 112],
+                          [-16, -48, 112, 80],
+                          [-16, -16, 112, 112],
+                          [-96, -96, 160, 160]]
+    self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/box_coders/BUILD b/research/object_detection/box_coders/BUILD
index ecb3cc7a..f74484b7 100644
--- a/research/object_detection/box_coders/BUILD
+++ b/research/object_detection/box_coders/BUILD
@@ -13,8 +13,8 @@ py_library(
         "faster_rcnn_box_coder.py",
     ],
     deps = [
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -26,7 +26,7 @@ py_test(
     deps = [
         ":faster_rcnn_box_coder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -36,9 +36,9 @@ py_library(
         "keypoint_box_coder.py",
     ],
     deps = [
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -50,8 +50,8 @@ py_test(
     deps = [
         ":keypoint_box_coder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -61,8 +61,8 @@ py_library(
         "mean_stddev_box_coder.py",
     ],
     deps = [
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -74,7 +74,7 @@ py_test(
     deps = [
         ":mean_stddev_box_coder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -84,8 +84,8 @@ py_library(
         "square_box_coder.py",
     ],
     deps = [
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -97,6 +97,6 @@ py_test(
     deps = [
         ":square_box_coder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
diff --git a/research/object_detection/builders/BUILD b/research/object_detection/builders/BUILD
index d1bb3f03..7f267281 100644
--- a/research/object_detection/builders/BUILD
+++ b/research/object_detection/builders/BUILD
@@ -20,18 +20,19 @@ py_library(
         ":matcher_builder",
         ":post_processing_builder",
         ":region_similarity_calculator_builder",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/object_detection/meta_architectures:rfcn_meta_arch",
-        "//tensorflow_models/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow_models/object_detection/models:embedded_ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow_models/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_inception_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_inception_v3_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow_models/object_detection/protos:model_py_pb2",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//tensorflow/models/research/object_detection/meta_architectures:rfcn_meta_arch",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/models:embedded_ssd_mobilenet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_nas_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
     ],
 )
 
@@ -41,15 +42,16 @@ py_test(
     deps = [
         ":model_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow_models/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_inception_v2_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_inception_v3_feature_extractor",
-        "//tensorflow_models/object_detection/models:ssd_mobilenet_v1_feature_extractor",
-        "//tensorflow_models/object_detection/protos:model_py_pb2",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_resnet_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_inception_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_nas_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:faster_rcnn_resnet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_inception_v2_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_inception_v3_feature_extractor",
+        "//tensorflow/models/research/object_detection/models:ssd_mobilenet_v1_feature_extractor",
+        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
     ],
 )
 
@@ -57,9 +59,9 @@ py_library(
     name = "matcher_builder",
     srcs = ["matcher_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/matchers:argmax_matcher",
-        "//tensorflow_models/object_detection/matchers:bipartite_matcher",
-        "//tensorflow_models/object_detection/protos:matcher_py_pb2",
+        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
+        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/protos:matcher_py_pb2",
     ],
 )
 
@@ -68,9 +70,9 @@ py_test(
     srcs = ["matcher_builder_test.py"],
     deps = [
         ":matcher_builder",
-        "//tensorflow_models/object_detection/matchers:argmax_matcher",
-        "//tensorflow_models/object_detection/matchers:bipartite_matcher",
-        "//tensorflow_models/object_detection/protos:matcher_py_pb2",
+        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
+        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/protos:matcher_py_pb2",
     ],
 )
 
@@ -78,11 +80,11 @@ py_library(
     name = "box_coder_builder",
     srcs = ["box_coder_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow_models/object_detection/box_coders:keypoint_box_coder",
-        "//tensorflow_models/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow_models/object_detection/box_coders:square_box_coder",
-        "//tensorflow_models/object_detection/protos:box_coder_py_pb2",
+        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:square_box_coder",
+        "//tensorflow/models/research/object_detection/protos:box_coder_py_pb2",
     ],
 )
 
@@ -92,11 +94,11 @@ py_test(
     deps = [
         ":box_coder_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow_models/object_detection/box_coders:keypoint_box_coder",
-        "//tensorflow_models/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow_models/object_detection/box_coders:square_box_coder",
-        "//tensorflow_models/object_detection/protos:box_coder_py_pb2",
+        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:square_box_coder",
+        "//tensorflow/models/research/object_detection/protos:box_coder_py_pb2",
     ],
 )
 
@@ -104,9 +106,10 @@ py_library(
     name = "anchor_generator_builder",
     srcs = ["anchor_generator_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow_models/object_detection/anchor_generators:multiple_grid_anchor_generator",
-        "//tensorflow_models/object_detection/protos:anchor_generator_py_pb2",
+        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/anchor_generators:multiple_grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/anchor_generators:multiscale_grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/protos:anchor_generator_py_pb2",
     ],
 )
 
@@ -116,9 +119,35 @@ py_test(
     deps = [
         ":anchor_generator_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow_models/object_detection/anchor_generators:multiple_grid_anchor_generator",
-        "//tensorflow_models/object_detection/protos:anchor_generator_py_pb2",
+        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/anchor_generators:multiple_grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/anchor_generators:multiscale_grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/protos:anchor_generator_py_pb2",
+    ],
+)
+
+py_library(
+    name = "dataset_builder",
+    srcs = ["dataset_builder.py"],
+    deps = [
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+    ],
+)
+
+py_test(
+    name = "dataset_builder_test",
+    srcs = [
+        "dataset_builder_test.py",
+    ],
+    deps = [
+        ":dataset_builder",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
     ],
 )
 
@@ -127,8 +156,8 @@ py_library(
     srcs = ["input_reader_builder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/data_decoders:tf_example_decoder",
-        "//tensorflow_models/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/data_decoders:tf_example_decoder",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
     ],
 )
 
@@ -140,8 +169,8 @@ py_test(
     deps = [
         ":input_reader_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
     ],
 )
 
@@ -149,8 +178,8 @@ py_library(
     name = "losses_builder",
     srcs = ["losses_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/protos:losses_py_pb2",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/protos:losses_py_pb2",
     ],
 )
 
@@ -159,8 +188,8 @@ py_test(
     srcs = ["losses_builder_test.py"],
     deps = [
         ":losses_builder",
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/protos:losses_py_pb2",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/protos:losses_py_pb2",
     ],
 )
 
@@ -169,7 +198,7 @@ py_library(
     srcs = ["optimizer_builder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:learning_schedules",
+        "//tensorflow/models/research/object_detection/utils:learning_schedules",
     ],
 )
 
@@ -179,7 +208,7 @@ py_test(
     deps = [
         ":optimizer_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:optimizer_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:optimizer_py_pb2",
     ],
 )
 
@@ -188,8 +217,8 @@ py_library(
     srcs = ["post_processing_builder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:post_processing",
-        "//tensorflow_models/object_detection/protos:post_processing_py_pb2",
+        "//tensorflow/models/research/object_detection/core:post_processing",
+        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
     ],
 )
 
@@ -199,7 +228,7 @@ py_test(
     deps = [
         ":post_processing_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:post_processing_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
     ],
 )
 
@@ -207,7 +236,7 @@ py_library(
     name = "hyperparams_builder",
     srcs = ["hyperparams_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
     ],
 )
 
@@ -217,7 +246,7 @@ py_test(
     deps = [
         ":hyperparams_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
     ],
 )
 
@@ -226,8 +255,8 @@ py_library(
     srcs = ["box_predictor_builder.py"],
     deps = [
         ":hyperparams_builder",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/protos:box_predictor_py_pb2",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
     ],
 )
 
@@ -238,8 +267,8 @@ py_test(
         ":box_predictor_builder",
         ":hyperparams_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:box_predictor_py_pb2",
-        "//tensorflow_models/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
     ],
 )
 
@@ -247,8 +276,8 @@ py_library(
     name = "region_similarity_calculator_builder",
     srcs = ["region_similarity_calculator_builder.py"],
     deps = [
-        "//tensorflow_models/object_detection/core:region_similarity_calculator",
-        "//tensorflow_models/object_detection/protos:region_similarity_calculator_py_pb2",
+        "//tensorflow/models/research/object_detection/core:region_similarity_calculator",
+        "//tensorflow/models/research/object_detection/protos:region_similarity_calculator_py_pb2",
     ],
 )
 
@@ -266,8 +295,8 @@ py_library(
     srcs = ["preprocessor_builder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:preprocessor",
-        "//tensorflow_models/object_detection/protos:preprocessor_py_pb2",
+        "//tensorflow/models/research/object_detection/core:preprocessor",
+        "//tensorflow/models/research/object_detection/protos:preprocessor_py_pb2",
     ],
 )
 
@@ -279,8 +308,8 @@ py_test(
     deps = [
         ":preprocessor_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:preprocessor",
-        "//tensorflow_models/object_detection/protos:preprocessor_py_pb2",
+        "//tensorflow/models/research/object_detection/core:preprocessor",
+        "//tensorflow/models/research/object_detection/protos:preprocessor_py_pb2",
     ],
 )
 
@@ -289,8 +318,8 @@ py_library(
     srcs = ["image_resizer_builder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:preprocessor",
-        "//tensorflow_models/object_detection/protos:image_resizer_py_pb2",
+        "//tensorflow/models/research/object_detection/core:preprocessor",
+        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
     ],
 )
 
@@ -300,6 +329,6 @@ py_test(
     deps = [
         ":image_resizer_builder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:image_resizer_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:image_resizer_py_pb2",
     ],
 )
diff --git a/research/object_detection/builders/anchor_generator_builder.py b/research/object_detection/builders/anchor_generator_builder.py
index 40a65c5c..072ca376 100644
--- a/research/object_detection/builders/anchor_generator_builder.py
+++ b/research/object_detection/builders/anchor_generator_builder.py
@@ -17,6 +17,7 @@
 
 from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.anchor_generators import multiple_grid_anchor_generator
+from object_detection.anchor_generators import multiscale_grid_anchor_generator
 from object_detection.protos import anchor_generator_pb2
 
 
@@ -78,5 +79,15 @@ def build(anchor_generator_config):
         anchor_offsets=anchor_offsets,
         reduce_boxes_in_lowest_layer=(
             ssd_anchor_generator_config.reduce_boxes_in_lowest_layer))
+  elif anchor_generator_config.WhichOneof(
+      'anchor_generator_oneof') == 'multiscale_anchor_generator':
+    cfg = anchor_generator_config.multiscale_anchor_generator
+    return multiscale_grid_anchor_generator.MultiscaleGridAnchorGenerator(
+        cfg.min_lvl,
+        cfg.max_lvl,
+        cfg.anchor_scale,
+        cfg.aspect_ratios,
+        cfg.scales_per_octave
+    )
   else:
     raise ValueError('Empty anchor generator.')
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 3e10b394..38eb1c23 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -68,6 +68,24 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes):
     )
     return box_predictor_object
 
+  if  box_predictor_oneof == 'weight_shared_convolutional_box_predictor':
+    conv_box_predictor = (box_predictor_config.
+                          weight_shared_convolutional_box_predictor)
+    conv_hyperparams = argscope_fn(conv_box_predictor.conv_hyperparams,
+                                   is_training)
+    box_predictor_object = box_predictor.WeightSharedConvolutionalBoxPredictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        conv_hyperparams=conv_hyperparams,
+        depth=conv_box_predictor.depth,
+        num_layers_before_predictor=(conv_box_predictor.
+                                     num_layers_before_predictor),
+        kernel_size=conv_box_predictor.kernel_size,
+        box_code_size=conv_box_predictor.box_code_size,
+        class_prediction_bias_init=conv_box_predictor.class_prediction_bias_init
+    )
+    return box_predictor_object
+
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
     mask_rcnn_box_predictor = box_predictor_config.mask_rcnn_box_predictor
     fc_hyperparams = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,
@@ -85,8 +103,12 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes):
         box_code_size=mask_rcnn_box_predictor.box_code_size,
         conv_hyperparams=conv_hyperparams,
         predict_instance_masks=mask_rcnn_box_predictor.predict_instance_masks,
-        mask_prediction_conv_depth=(mask_rcnn_box_predictor.
-                                    mask_prediction_conv_depth),
+        mask_height=mask_rcnn_box_predictor.mask_height,
+        mask_width=mask_rcnn_box_predictor.mask_width,
+        mask_prediction_num_conv_layers=(
+            mask_rcnn_box_predictor.mask_prediction_num_conv_layers),
+        mask_prediction_conv_depth=(
+            mask_rcnn_box_predictor.mask_prediction_conv_depth),
         predict_keypoints=mask_rcnn_box_predictor.predict_keypoints)
     return box_predictor_object
 
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 6bafd482..c754cb79 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -150,6 +150,120 @@ class ConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertTrue(box_predictor._is_training)
 
 
+class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
+
+  def test_box_predictor_calls_conv_argscope_fn(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l1_regularizer {
+          weight: 0.0003
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          mean: 0.0
+          stddev: 0.3
+        }
+      }
+      activation: RELU_6
+    """
+    hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)
+    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):
+      return (conv_hyperparams_arg, is_training)
+
+    box_predictor_proto = box_predictor_pb2.BoxPredictor()
+    (box_predictor_proto.weight_shared_convolutional_box_predictor
+     .conv_hyperparams.CopyFrom(hyperparams_proto))
+    box_predictor = box_predictor_builder.build(
+        argscope_fn=mock_conv_argscope_builder,
+        box_predictor_config=box_predictor_proto,
+        is_training=False,
+        num_classes=10)
+    (conv_hyperparams_actual, is_training) = box_predictor._conv_hyperparams
+    self.assertAlmostEqual((hyperparams_proto.regularizer.
+                            l1_regularizer.weight),
+                           (conv_hyperparams_actual.regularizer.l1_regularizer.
+                            weight))
+    self.assertAlmostEqual((hyperparams_proto.initializer.
+                            truncated_normal_initializer.stddev),
+                           (conv_hyperparams_actual.initializer.
+                            truncated_normal_initializer.stddev))
+    self.assertAlmostEqual((hyperparams_proto.initializer.
+                            truncated_normal_initializer.mean),
+                           (conv_hyperparams_actual.initializer.
+                            truncated_normal_initializer.mean))
+    self.assertEqual(hyperparams_proto.activation,
+                     conv_hyperparams_actual.activation)
+    self.assertFalse(is_training)
+
+  def test_construct_non_default_conv_box_predictor(self):
+    box_predictor_text_proto = """
+      weight_shared_convolutional_box_predictor {
+        depth: 2
+        num_layers_before_predictor: 2
+        kernel_size: 7
+        box_code_size: 3
+        class_prediction_bias_init: 4.0
+      }
+    """
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l1_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, hyperparams_proto)
+    def mock_conv_argscope_builder(conv_hyperparams_arg, is_training):
+      return (conv_hyperparams_arg, is_training)
+
+    box_predictor_proto = box_predictor_pb2.BoxPredictor()
+    text_format.Merge(box_predictor_text_proto, box_predictor_proto)
+    (box_predictor_proto.weight_shared_convolutional_box_predictor.
+     conv_hyperparams.CopyFrom(hyperparams_proto))
+    box_predictor = box_predictor_builder.build(
+        argscope_fn=mock_conv_argscope_builder,
+        box_predictor_config=box_predictor_proto,
+        is_training=False,
+        num_classes=10)
+    self.assertEqual(box_predictor._depth, 2)
+    self.assertEqual(box_predictor._num_layers_before_predictor, 2)
+    self.assertAlmostEqual(box_predictor._class_prediction_bias_init, 4.0)
+    self.assertEqual(box_predictor.num_classes, 10)
+    self.assertFalse(box_predictor._is_training)
+
+  def test_construct_default_conv_box_predictor(self):
+    box_predictor_text_proto = """
+      weight_shared_convolutional_box_predictor {
+        conv_hyperparams {
+          regularizer {
+            l1_regularizer {
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+            }
+          }
+        }
+      }"""
+    box_predictor_proto = box_predictor_pb2.BoxPredictor()
+    text_format.Merge(box_predictor_text_proto, box_predictor_proto)
+    box_predictor = box_predictor_builder.build(
+        argscope_fn=hyperparams_builder.build,
+        box_predictor_config=box_predictor_proto,
+        is_training=True,
+        num_classes=90)
+    self.assertEqual(box_predictor._depth, 0)
+    self.assertEqual(box_predictor._num_layers_before_predictor, 0)
+    self.assertEqual(box_predictor.num_classes, 90)
+    self.assertTrue(box_predictor._is_training)
+
+
 class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
 
   def test_box_predictor_builder_calls_fc_argscope_fn(self):
@@ -247,6 +361,8 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
         hyperparams_pb2.Hyperparams.CONV)
     box_predictor_proto.mask_rcnn_box_predictor.predict_instance_masks = True
     box_predictor_proto.mask_rcnn_box_predictor.mask_prediction_conv_depth = 512
+    box_predictor_proto.mask_rcnn_box_predictor.mask_height = 16
+    box_predictor_proto.mask_rcnn_box_predictor.mask_width = 16
     mock_argscope_fn = mock.Mock(return_value='arg_scope')
     box_predictor = box_predictor_builder.build(
         argscope_fn=mock_argscope_fn,
diff --git a/research/object_detection/builders/dataset_builder.py b/research/object_detection/builders/dataset_builder.py
new file mode 100644
index 00000000..869a070d
--- /dev/null
+++ b/research/object_detection/builders/dataset_builder.py
@@ -0,0 +1,69 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""tf.data.Dataset builder.
+
+Creates data sources for DetectionModels from an InputReader config. See
+input_reader.proto for options.
+
+Note: If users wishes to also use their own InputReaders with the Object
+Detection configuration framework, they should define their own builder function
+that wraps the build function.
+"""
+
+import tensorflow as tf
+
+from object_detection.data_decoders import tf_example_decoder
+from object_detection.protos import input_reader_pb2
+from object_detection.utils import dataset_util
+
+
+def build(input_reader_config, num_workers=1, worker_index=0):
+  """Builds a tf.data.Dataset based on the InputReader config.
+
+  Args:
+    input_reader_config: A input_reader_pb2.InputReader object.
+    num_workers: Number of workers / shards.
+    worker_index: Id for the current worker.
+
+  Returns:
+    A tf.data.Dataset based on the input_reader_config.
+
+  Raises:
+    ValueError: On invalid input reader proto.
+    ValueError: If no input paths are specified.
+  """
+  if not isinstance(input_reader_config, input_reader_pb2.InputReader):
+    raise ValueError('input_reader_config not of type '
+                     'input_reader_pb2.InputReader.')
+
+  if input_reader_config.WhichOneof('input_reader') == 'tf_record_input_reader':
+    config = input_reader_config.tf_record_input_reader
+    if not config.input_path:
+      raise ValueError('At least one input path must be specified in '
+                       '`input_reader_config`.')
+
+    label_map_proto_file = None
+    if input_reader_config.HasField('label_map_path'):
+      label_map_proto_file = input_reader_config.label_map_path
+    decoder = tf_example_decoder.TfExampleDecoder(
+        load_instance_masks=input_reader_config.load_instance_masks,
+        instance_mask_type=input_reader_config.mask_type,
+        label_map_proto_file=label_map_proto_file)
+
+    return dataset_util.read_dataset(
+        tf.data.TFRecordDataset, decoder.decode, config.input_path[:],
+        input_reader_config, num_workers, worker_index)
+
+  raise ValueError('Unsupported input_reader_config.')
diff --git a/research/object_detection/builders/dataset_builder_test.py b/research/object_detection/builders/dataset_builder_test.py
new file mode 100644
index 00000000..7dcd6caf
--- /dev/null
+++ b/research/object_detection/builders/dataset_builder_test.py
@@ -0,0 +1,160 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for dataset_builder."""
+
+import os
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+
+from tensorflow.core.example import example_pb2
+from tensorflow.core.example import feature_pb2
+from object_detection.builders import dataset_builder
+from object_detection.core import standard_fields as fields
+from object_detection.protos import input_reader_pb2
+from object_detection.utils import dataset_util
+
+
+class DatasetBuilderTest(tf.test.TestCase):
+
+  def create_tf_record(self):
+    path = os.path.join(self.get_temp_dir(), 'tfrecord')
+    writer = tf.python_io.TFRecordWriter(path)
+
+    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    flat_mask = (4 * 5) * [1.0]
+    with self.test_session():
+      encoded_jpeg = tf.image.encode_jpeg(tf.constant(image_tensor)).eval()
+    example = example_pb2.Example(
+        features=feature_pb2.Features(
+            feature={
+                'image/encoded':
+                    feature_pb2.Feature(
+                        bytes_list=feature_pb2.BytesList(value=[encoded_jpeg])),
+                'image/format':
+                    feature_pb2.Feature(
+                        bytes_list=feature_pb2.BytesList(
+                            value=['jpeg'.encode('utf-8')])),
+                'image/height':
+                    feature_pb2.Feature(
+                        int64_list=feature_pb2.Int64List(value=[4])),
+                'image/width':
+                    feature_pb2.Feature(
+                        int64_list=feature_pb2.Int64List(value=[5])),
+                'image/object/bbox/xmin':
+                    feature_pb2.Feature(
+                        float_list=feature_pb2.FloatList(value=[0.0])),
+                'image/object/bbox/xmax':
+                    feature_pb2.Feature(
+                        float_list=feature_pb2.FloatList(value=[1.0])),
+                'image/object/bbox/ymin':
+                    feature_pb2.Feature(
+                        float_list=feature_pb2.FloatList(value=[0.0])),
+                'image/object/bbox/ymax':
+                    feature_pb2.Feature(
+                        float_list=feature_pb2.FloatList(value=[1.0])),
+                'image/object/class/label':
+                    feature_pb2.Feature(
+                        int64_list=feature_pb2.Int64List(value=[2])),
+                'image/object/mask':
+                    feature_pb2.Feature(
+                        float_list=feature_pb2.FloatList(value=flat_mask)),
+            }))
+    writer.write(example.SerializeToString())
+    writer.close()
+
+    return path
+
+  def test_build_tf_record_input_reader(self):
+    tf_record_path = self.create_tf_record()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    tensor_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(input_reader_proto)).get_next()
+
+    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    with sv.prepare_or_wait_for_session() as sess:
+      sv.start_queue_runners(sess)
+      output_dict = sess.run(tensor_dict)
+
+    self.assertTrue(
+        fields.InputDataFields.groundtruth_instance_masks not in output_dict)
+    self.assertEquals((4, 5, 3),
+                      output_dict[fields.InputDataFields.image].shape)
+    self.assertEquals([2],
+                      output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEquals(
+        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllEqual(
+        [0.0, 0.0, 1.0, 1.0],
+        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+
+  def test_build_tf_record_input_reader_and_load_instance_masks(self):
+    tf_record_path = self.create_tf_record()
+
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      load_instance_masks: true
+      tf_record_input_reader {{
+        input_path: '{0}'
+      }}
+    """.format(tf_record_path)
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    tensor_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(input_reader_proto)).get_next()
+
+    sv = tf.train.Supervisor(logdir=self.get_temp_dir())
+    with sv.prepare_or_wait_for_session() as sess:
+      sv.start_queue_runners(sess)
+      output_dict = sess.run(tensor_dict)
+
+    self.assertEquals((4, 5, 3),
+                      output_dict[fields.InputDataFields.image].shape)
+    self.assertEquals([2],
+                      output_dict[fields.InputDataFields.groundtruth_classes])
+    self.assertEquals(
+        (1, 4), output_dict[fields.InputDataFields.groundtruth_boxes].shape)
+    self.assertAllEqual(
+        [0.0, 0.0, 1.0, 1.0],
+        output_dict[fields.InputDataFields.groundtruth_boxes][0])
+    self.assertAllEqual(
+        (1, 4, 5),
+        output_dict[fields.InputDataFields.groundtruth_instance_masks].shape)
+
+  def test_raises_error_with_no_input_paths(self):
+    input_reader_text_proto = """
+      shuffle: false
+      num_readers: 1
+      load_instance_masks: true
+    """
+    input_reader_proto = input_reader_pb2.InputReader()
+    text_format.Merge(input_reader_text_proto, input_reader_proto)
+    with self.assertRaises(ValueError):
+      dataset_builder.build(input_reader_proto)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/builders/image_resizer_builder.py b/research/object_detection/builders/image_resizer_builder.py
index 9d81c7d3..f9a600ca 100644
--- a/research/object_detection/builders/image_resizer_builder.py
+++ b/research/object_detection/builders/image_resizer_builder.py
@@ -83,7 +83,8 @@ def build(image_resizer_config):
         preprocessor.resize_to_range,
         min_dimension=keep_aspect_ratio_config.min_dimension,
         max_dimension=keep_aspect_ratio_config.max_dimension,
-        method=method)
+        method=method,
+        pad_to_max_dimension=keep_aspect_ratio_config.pad_to_max_dimension)
   if image_resizer_config.WhichOneof(
       'image_resizer_oneof') == 'fixed_shape_resizer':
     fixed_shape_resizer_config = image_resizer_config.fixed_shape_resizer
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index 4ef557a5..6744d95d 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -29,11 +29,11 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
     images = tf.to_float(
         tf.random_uniform(input_shape, minval=0, maxval=255, dtype=tf.int32))
-    resized_images = image_resizer_fn(images)
+    resized_images, _ = image_resizer_fn(images)
     with self.test_session() as sess:
       return sess.run(resized_images).shape
 
-  def test_built_keep_aspect_ratio_resizer_returns_expected_shape(self):
+  def test_build_keep_aspect_ratio_resizer_returns_expected_shape(self):
     image_resizer_text_proto = """
       keep_aspect_ratio_resizer {
         min_dimension: 10
@@ -46,6 +46,20 @@ class ImageResizerBuilderTest(tf.test.TestCase):
         input_shape, image_resizer_text_proto)
     self.assertEqual(output_shape, expected_output_shape)
 
+  def test_build_keep_aspect_ratio_resizer_with_padding(self):
+    image_resizer_text_proto = """
+      keep_aspect_ratio_resizer {
+        min_dimension: 10
+        max_dimension: 20
+        pad_to_max_dimension: true
+      }
+    """
+    input_shape = (50, 25, 3)
+    expected_output_shape = (20, 20, 3)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
   def test_built_fixed_shape_resizer_returns_expected_shape(self):
     image_resizer_text_proto = """
       fixed_shape_resizer {
@@ -69,7 +83,7 @@ class ImageResizerBuilderTest(tf.test.TestCase):
     text_format.Merge(text_proto, image_resizer_config)
     image_resizer_fn = image_resizer_builder.build(image_resizer_config)
     image_placeholder = tf.placeholder(tf.uint8, [1, None, None, 3])
-    resized_image = image_resizer_fn(image_placeholder)
+    resized_image, _ = image_resizer_fn(image_placeholder)
     with self.test_session() as sess:
       return sess.run(resized_image, feed_dict={image_placeholder: image})
 
diff --git a/research/object_detection/builders/input_reader_builder.py b/research/object_detection/builders/input_reader_builder.py
index 530e879c..8cb5e2f0 100644
--- a/research/object_detection/builders/input_reader_builder.py
+++ b/research/object_detection/builders/input_reader_builder.py
@@ -69,6 +69,7 @@ def build(input_reader_config):
       label_map_proto_file = input_reader_config.label_map_path
     decoder = tf_example_decoder.TfExampleDecoder(
         load_instance_masks=input_reader_config.load_instance_masks,
+        instance_mask_type=input_reader_config.mask_type,
         label_map_proto_file=label_map_proto_file)
     return decoder.decode(string_tensor)
 
diff --git a/research/object_detection/builders/losses_builder.py b/research/object_detection/builders/losses_builder.py
index c2b0a1f1..6f27da3d 100644
--- a/research/object_detection/builders/losses_builder.py
+++ b/research/object_detection/builders/losses_builder.py
@@ -116,18 +116,17 @@ def build_faster_rcnn_classification_loss(loss_config):
   loss_type = loss_config.WhichOneof('classification_loss')
 
   if loss_type == 'weighted_sigmoid':
-    config = loss_config.weighted_sigmoid
-    return losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSigmoidClassificationLoss()
   if loss_type == 'weighted_softmax':
     config = loss_config.weighted_softmax
     return losses.WeightedSoftmaxClassificationLoss(
-        anchorwise_output=config.anchorwise_output)
+        logit_scale=config.logit_scale)
 
   # By default, Faster RCNN second stage classifier uses Softmax loss
   # with anchor-wise outputs.
+  config = loss_config.weighted_softmax
   return losses.WeightedSoftmaxClassificationLoss(
-      anchorwise_output=True)
+      logit_scale=config.logit_scale)
 
 
 def _build_localization_loss(loss_config):
@@ -148,14 +147,10 @@ def _build_localization_loss(loss_config):
   loss_type = loss_config.WhichOneof('localization_loss')
 
   if loss_type == 'weighted_l2':
-    config = loss_config.weighted_l2
-    return losses.WeightedL2LocalizationLoss(
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedL2LocalizationLoss()
 
   if loss_type == 'weighted_smooth_l1':
-    config = loss_config.weighted_smooth_l1
-    return losses.WeightedSmoothL1LocalizationLoss(
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSmoothL1LocalizationLoss()
 
   if loss_type == 'weighted_iou':
     return losses.WeightedIOULocalizationLoss()
@@ -181,9 +176,7 @@ def _build_classification_loss(loss_config):
   loss_type = loss_config.WhichOneof('classification_loss')
 
   if loss_type == 'weighted_sigmoid':
-    config = loss_config.weighted_sigmoid
-    return losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=config.anchorwise_output)
+    return losses.WeightedSigmoidClassificationLoss()
 
   if loss_type == 'weighted_sigmoid_focal':
     config = loss_config.weighted_sigmoid_focal
@@ -191,21 +184,18 @@ def _build_classification_loss(loss_config):
     if config.HasField('alpha'):
       alpha = config.alpha
     return losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=config.anchorwise_output,
         gamma=config.gamma,
         alpha=alpha)
 
   if loss_type == 'weighted_softmax':
     config = loss_config.weighted_softmax
     return losses.WeightedSoftmaxClassificationLoss(
-        anchorwise_output=config.anchorwise_output,
         logit_scale=config.logit_scale)
 
   if loss_type == 'bootstrapped_sigmoid':
     config = loss_config.bootstrapped_sigmoid
     return losses.BootstrappedSigmoidClassificationLoss(
         alpha=config.alpha,
-        bootstrap_type=('hard' if config.hard_bootstrap else 'soft'),
-        anchorwise_output=config.anchorwise_output)
+        bootstrap_type=('hard' if config.hard_bootstrap else 'soft'))
 
   raise ValueError('Empty loss config.')
diff --git a/research/object_detection/builders/losses_builder_test.py b/research/object_detection/builders/losses_builder_test.py
index d4105203..5bef7192 100644
--- a/research/object_detection/builders/losses_builder_test.py
+++ b/research/object_detection/builders/losses_builder_test.py
@@ -80,7 +80,6 @@ class LocalizationLossBuilderTest(tf.test.TestCase):
     losses_text_proto = """
       localization_loss {
         weighted_smooth_l1 {
-          anchorwise_output: true
         }
       }
       classification_loss {
@@ -245,7 +244,7 @@ class ClassificationLossBuilderTest(tf.test.TestCase):
     targets = tf.constant([[[0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]])
     weights = tf.constant([[1.0, 1.0]])
     loss = classification_loss(predictions, targets, weights=weights)
-    self.assertEqual(loss.shape, [1, 2])
+    self.assertEqual(loss.shape, [1, 2, 3])
 
   def test_raise_error_on_empty_config(self):
     losses_text_proto = """
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 5467a91b..7e2019ca 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -106,6 +106,7 @@ def _build_ssd_feature_extractor(feature_extractor_config, is_training,
   min_depth = feature_extractor_config.min_depth
   pad_to_multiple = feature_extractor_config.pad_to_multiple
   batch_norm_trainable = feature_extractor_config.batch_norm_trainable
+  use_explicit_padding = feature_extractor_config.use_explicit_padding
   conv_hyperparams = hyperparams_builder.build(
       feature_extractor_config.conv_hyperparams, is_training)
 
@@ -115,7 +116,8 @@ def _build_ssd_feature_extractor(feature_extractor_config, is_training,
   feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]
   return feature_extractor_class(is_training, depth_multiplier, min_depth,
                                  pad_to_multiple, conv_hyperparams,
-                                 batch_norm_trainable, reuse_weights)
+                                 batch_norm_trainable, reuse_weights,
+                                 use_explicit_padding)
 
 
 def _build_ssd_model(ssd_config, is_training):
@@ -228,7 +230,7 @@ def _build_faster_rcnn_model(frcnn_config, is_training):
   feature_extractor = _build_faster_rcnn_feature_extractor(
       frcnn_config.feature_extractor, is_training)
 
-  first_stage_only = frcnn_config.first_stage_only
+  number_of_stages = frcnn_config.number_of_stages
   first_stage_anchor_generator = anchor_generator_builder.build(
       frcnn_config.first_stage_anchor_generator)
 
@@ -283,7 +285,7 @@ def _build_faster_rcnn_model(frcnn_config, is_training):
       'num_classes': num_classes,
       'image_resizer_fn': image_resizer_fn,
       'feature_extractor': feature_extractor,
-      'first_stage_only': first_stage_only,
+      'number_of_stages': number_of_stages,
       'first_stage_anchor_generator': first_stage_anchor_generator,
       'first_stage_atrous_rate': first_stage_atrous_rate,
       'first_stage_box_predictor_arg_scope':
diff --git a/research/object_detection/builders/optimizer_builder_test.py b/research/object_detection/builders/optimizer_builder_test.py
index e5bcbba1..dff5a6cd 100644
--- a/research/object_detection/builders/optimizer_builder_test.py
+++ b/research/object_detection/builders/optimizer_builder_test.py
@@ -196,7 +196,7 @@ class OptimizerBuilderTest(tf.test.TestCase):
     optimizer = optimizer_builder.build(optimizer_proto, global_summaries)
     self.assertTrue(
         isinstance(optimizer, tf.contrib.opt.MovingAverageOptimizer))
-    # TODO(rathodv): Find a way to not depend on the private members.
+    # TODO: Find a way to not depend on the private members.
     self.assertAlmostEqual(optimizer._ema._decay, 0.2)
 
   def testBuildEmptyOptimizer(self):
diff --git a/research/object_detection/core/BUILD b/research/object_detection/core/BUILD
index 5d8aaad7..af8e33ef 100644
--- a/research/object_detection/core/BUILD
+++ b/research/object_detection/core/BUILD
@@ -53,7 +53,7 @@ py_library(
     deps = [
         ":box_list",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -113,7 +113,7 @@ py_library(
         ":box_list",
         ":box_list_ops",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
@@ -162,6 +162,7 @@ py_library(
         ":keypoint_ops",
         ":standard_fields",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -211,6 +212,7 @@ py_library(
         ":box_list_ops",
         ":standard_fields",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -232,15 +234,16 @@ py_library(
     ],
     deps = [
         ":box_list",
-        ":box_list_ops",
         ":matcher",
         ":region_similarity_calculator",
+        ":standard_fields",
         "//tensorflow",
-        "//tensorflow_models/object_detection/box_coders:faster_rcnn_box_coder",
-        "//tensorflow_models/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/matchers:argmax_matcher",
-        "//tensorflow_models/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/box_coders:faster_rcnn_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/matchers:argmax_matcher",
+        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -254,8 +257,10 @@ py_test(
         ":region_similarity_calculator",
         ":target_assigner",
         "//tensorflow",
-        "//tensorflow_models/object_detection/box_coders:mean_stddev_box_coder",
-        "//tensorflow_models/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/box_coders:keypoint_box_coder",
+        "//tensorflow/models/research/object_detection/box_coders:mean_stddev_box_coder",
+        "//tensorflow/models/research/object_detection/matchers:bipartite_matcher",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
 
@@ -274,9 +279,9 @@ py_library(
     srcs = ["box_predictor.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/object_detection/utils:shape_utils",
-        "//tensorflow_models/object_detection/utils:static_shape",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/utils:static_shape",
     ],
 )
 
@@ -286,8 +291,9 @@ py_test(
     deps = [
         ":box_predictor",
         "//tensorflow",
-        "//tensorflow_models/object_detection/builders:hyperparams_builder",
-        "//tensorflow_models/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/builders:hyperparams_builder",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
 
@@ -298,7 +304,7 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list_ops",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
     ],
 )
 
@@ -309,7 +315,7 @@ py_test(
     ],
     deps = [
         ":region_similarity_calculator",
-        "//tensorflow_models/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list",
     ],
 )
 
@@ -330,7 +336,7 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
diff --git a/research/object_detection/core/anchor_generator.py b/research/object_detection/core/anchor_generator.py
index ed6a2bc5..05660067 100644
--- a/research/object_detection/core/anchor_generator.py
+++ b/research/object_detection/core/anchor_generator.py
@@ -77,8 +77,8 @@ class AnchorGenerator(object):
   def generate(self, feature_map_shape_list, **params):
     """Generates a collection of bounding boxes to be used as anchors.
 
-    TODO: remove **params from argument list and make stride and offsets (for
-        multiple_grid_anchor_generator) constructor arguments.
+    TODO: remove **params from argument list and make stride and
+      offsets (for multiple_grid_anchor_generator) constructor arguments.
 
     Args:
       feature_map_shape_list: list of (height, width) pairs in the format
@@ -140,3 +140,4 @@ class AnchorGenerator(object):
                                * feature_map_shape[0]
                                * feature_map_shape[1])
     return tf.assert_equal(expected_num_anchors, anchors.num_boxes())
+
diff --git a/research/object_detection/core/box_list_ops.py b/research/object_detection/core/box_list_ops.py
index eaacd0ea..5bc3b7d0 100644
--- a/research/object_detection/core/box_list_ops.py
+++ b/research/object_detection/core/box_list_ops.py
@@ -183,7 +183,8 @@ def prune_completely_outside_window(boxlist, window, scope=None):
     scope: name scope.
 
   Returns:
-    pruned_corners: a tensor with shape [M_out, 4] where M_out <= M_in
+    pruned_boxlist: a new BoxList with all bounding boxes partially or fully in
+      the window.
     valid_indices: a tensor with shape [M_out] indexing the valid bounding boxes
      in the input tensor.
   """
@@ -656,7 +657,7 @@ def filter_greater_than(boxlist, thresh, scope=None):
   This op keeps the collection of boxes whose corresponding scores are
   greater than the input threshold.
 
-  TODO: Change function name to filter_scores_greater_than
+  TODO: Change function name to FilterScoresGreaterThan
 
   Args:
     boxlist: BoxList holding N boxes.  Must contain a 'scores' field
@@ -982,3 +983,79 @@ def pad_or_clip_box_list(boxlist, num_boxes, scope=None):
           boxlist.get_field(field), num_boxes)
       subboxlist.add_field(field, subfield)
     return subboxlist
+
+
+def select_random_box(boxlist,
+                      default_box=None,
+                      seed=None,
+                      scope=None):
+  """Selects a random bounding box from a `BoxList`.
+
+  Args:
+    boxlist: A BoxList.
+    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,
+      this default box will be returned. If None, will use a default box of
+      [[-1., -1., -1., -1.]].
+    seed: Random seed.
+    scope: Name scope.
+
+  Returns:
+    bbox: A [1, 4] tensor with a random bounding box.
+    valid: A bool tensor indicating whether a valid bounding box is returned
+      (True) or whether the default box is returned (False).
+  """
+  with tf.name_scope(scope, 'SelectRandomBox'):
+    bboxes = boxlist.get()
+    combined_shape = shape_utils.combined_static_and_dynamic_shape(bboxes)
+    number_of_boxes = combined_shape[0]
+    default_box = default_box or tf.constant([[-1., -1., -1., -1.]])
+
+    def select_box():
+      random_index = tf.random_uniform([],
+                                       maxval=number_of_boxes,
+                                       dtype=tf.int32,
+                                       seed=seed)
+      return tf.expand_dims(bboxes[random_index], axis=0), tf.constant(True)
+
+  return tf.cond(
+      tf.greater_equal(number_of_boxes, 1),
+      true_fn=select_box,
+      false_fn=lambda: (default_box, tf.constant(False)))
+
+
+def get_minimal_coverage_box(boxlist,
+                             default_box=None,
+                             scope=None):
+  """Creates a single bounding box which covers all boxes in the boxlist.
+
+  Args:
+    boxlist: A Boxlist.
+    default_box: A [1, 4] float32 tensor. If no boxes are present in `boxlist`,
+      this default box will be returned. If None, will use a default box of
+      [[0., 0., 1., 1.]].
+    scope: Name scope.
+
+  Returns:
+    A [1, 4] float32 tensor with a bounding box that tightly covers all the
+    boxes in the box list. If the boxlist does not contain any boxes, the
+    default box is returned.
+  """
+  with tf.name_scope(scope, 'CreateCoverageBox'):
+    num_boxes = boxlist.num_boxes()
+
+    def coverage_box(bboxes):
+      y_min, x_min, y_max, x_max = tf.split(
+          value=bboxes, num_or_size_splits=4, axis=1)
+      y_min_coverage = tf.reduce_min(y_min, axis=0)
+      x_min_coverage = tf.reduce_min(x_min, axis=0)
+      y_max_coverage = tf.reduce_max(y_max, axis=0)
+      x_max_coverage = tf.reduce_max(x_max, axis=0)
+      return tf.stack(
+          [y_min_coverage, x_min_coverage, y_max_coverage, x_max_coverage],
+          axis=1)
+
+    default_box = default_box or tf.constant([[0., 0., 1., 1.]])
+    return tf.cond(
+        tf.greater_equal(num_boxes, 1),
+        true_fn=lambda: coverage_box(boxlist.get()),
+        false_fn=lambda: default_box)
diff --git a/research/object_detection/core/box_list_ops_test.py b/research/object_detection/core/box_list_ops_test.py
index 467bb3c6..fd251189 100644
--- a/research/object_detection/core/box_list_ops_test.py
+++ b/research/object_detection/core/box_list_ops_test.py
@@ -153,6 +153,25 @@ class BoxListOpsTest(tf.test.TestCase):
       extra_data_out = sess.run(pruned.get_field('extra_data'))
       self.assertAllEqual(extra_data_out, [[1], [2], [3], [4], [6]])
 
+  def test_prune_completely_outside_window_with_empty_boxlist(self):
+    window = tf.constant([0, 0, 9, 14], tf.float32)
+    corners = tf.zeros(shape=[0, 4], dtype=tf.float32)
+    boxes = box_list.BoxList(corners)
+    boxes.add_field('extra_data', tf.zeros(shape=[0], dtype=tf.int32))
+    pruned, keep_indices = box_list_ops.prune_completely_outside_window(boxes,
+                                                                        window)
+    pruned_boxes = pruned.get()
+    extra = pruned.get_field('extra_data')
+
+    exp_pruned_boxes = np.zeros(shape=[0, 4], dtype=np.float32)
+    exp_extra = np.zeros(shape=[0], dtype=np.int32)
+    with self.test_session() as sess:
+      pruned_boxes_out, keep_indices_out, extra_out = sess.run(
+          [pruned_boxes, keep_indices, extra])
+      self.assertAllClose(exp_pruned_boxes, pruned_boxes_out)
+      self.assertAllEqual([], keep_indices_out)
+      self.assertAllEqual(exp_extra, extra_out)
+
   def test_intersection(self):
     corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
     corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
@@ -593,6 +612,58 @@ class BoxListOpsTest(tf.test.TestCase):
       self.assertAllEqual(expected_classes, classes_out)
       self.assertAllClose(expected_scores, scores_out)
 
+  def test_select_random_box(self):
+    boxes = [[0., 0., 1., 1.],
+             [0., 1., 2., 3.],
+             [0., 2., 3., 4.]]
+
+    corners = tf.constant(boxes, dtype=tf.float32)
+    boxlist = box_list.BoxList(corners)
+    random_bbox, valid = box_list_ops.select_random_box(boxlist)
+    with self.test_session() as sess:
+      random_bbox_out, valid_out = sess.run([random_bbox, valid])
+
+    norm_small = any(
+        [np.linalg.norm(random_bbox_out - box) < 1e-6 for box in boxes])
+
+    self.assertTrue(norm_small)
+    self.assertTrue(valid_out)
+
+  def test_select_random_box_with_empty_boxlist(self):
+    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
+    boxlist = box_list.BoxList(corners)
+    random_bbox, valid = box_list_ops.select_random_box(boxlist)
+    with self.test_session() as sess:
+      random_bbox_out, valid_out = sess.run([random_bbox, valid])
+
+    expected_bbox_out = np.array([[-1., -1., -1., -1.]], dtype=np.float32)
+    self.assertAllEqual(expected_bbox_out, random_bbox_out)
+    self.assertFalse(valid_out)
+
+  def test_get_minimal_coverage_box(self):
+    boxes = [[0., 0., 1., 1.],
+             [-1., 1., 2., 3.],
+             [0., 2., 3., 4.]]
+
+    expected_coverage_box = [[-1., 0., 3., 4.]]
+
+    corners = tf.constant(boxes, dtype=tf.float32)
+    boxlist = box_list.BoxList(corners)
+    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
+    with self.test_session() as sess:
+      coverage_box_out = sess.run(coverage_box)
+
+    self.assertAllClose(expected_coverage_box, coverage_box_out)
+
+  def test_get_minimal_coverage_box_with_empty_boxlist(self):
+    corners = tf.constant([], shape=[0, 4], dtype=tf.float32)
+    boxlist = box_list.BoxList(corners)
+    coverage_box = box_list_ops.get_minimal_coverage_box(boxlist)
+    with self.test_session() as sess:
+      coverage_box_out = sess.run(coverage_box)
+
+    self.assertAllClose([[0.0, 0.0, 1.0, 1.0]], coverage_box_out)
+
 
 class ConcatenateTest(tf.test.TestCase):
 
@@ -958,5 +1029,6 @@ class BoxRefinementTest(tf.test.TestCase):
       self.assertAllClose(expected_scores, scores_out)
       self.assertAllEqual(extra_field_out, [0, 1, 1])
 
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 8378a8ea..2d73cdcf 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -27,6 +27,7 @@ These modules are separated from the main model since the same
 few box predictor architectures are shared across many models.
 """
 from abc import abstractmethod
+import math
 import tensorflow as tf
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
@@ -59,8 +60,8 @@ class BoxPredictor(object):
   def num_classes(self):
     return self._num_classes
 
-  def predict(self, image_features, num_predictions_per_location, scope,
-              **params):
+  def predict(self, image_features, num_predictions_per_location,
+              scope=None, **params):
     """Computes encoded object locations and corresponding confidences.
 
     Takes a high level image feature map as input and produce two predictions,
@@ -70,10 +71,10 @@ class BoxPredictor(object):
     and do not assume anything about their shapes.
 
     Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
-      num_predictions_per_location: an integer representing the number of box
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
+      width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
       scope: Variable and Op scope name.
       **params: Additional keyword arguments for specific implementations of
               BoxPredictor.
@@ -86,8 +87,21 @@ class BoxPredictor(object):
         class_predictions_with_background: A float tensor of shape
           [batch_size, num_anchors, num_classes + 1] representing the class
           predictions for the proposals.
+
+    Raises:
+      ValueError: If length of `image_features` is not equal to length of
+        `num_predictions_per_location`.
     """
-    with tf.variable_scope(scope):
+    if len(image_features) != len(num_predictions_per_location):
+      raise ValueError('image_feature and num_predictions_per_location must '
+                       'be of same length, found: {} vs {}'.
+                       format(len(image_features),
+                              len(num_predictions_per_location)))
+    if scope is not None:
+      with tf.variable_scope(scope):
+        return self._predict(image_features, num_predictions_per_location,
+                             **params)
+    else:
       return self._predict(image_features, num_predictions_per_location,
                            **params)
 
@@ -98,10 +112,10 @@ class BoxPredictor(object):
     """Implementations must override this method.
 
     Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
-      num_predictions_per_location: an integer representing the number of box
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
       **params: Additional keyword arguments for specific implementations of
               BoxPredictor.
 
@@ -169,28 +183,35 @@ class RfcnBoxPredictor(BoxPredictor):
     """Computes encoded object locations and corresponding confidences.
 
     Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
-      num_predictions_per_location: an integer representing the number of box
-        predictions to be made per spatial location in the feature map.
-        Currently, this must be set to 1, or an error will be raised.
+      image_features: A list of float tensors of shape [batch_size, height_i,
+      width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
+        Currently, this must be set to [1], or an error will be raised.
       proposal_boxes: A float tensor of shape [batch_size, num_proposals,
         box_code_size].
 
     Returns:
       box_encodings: A float tensor of shape
-        [batch_size, 1, num_classes, code_size] representing the
+        [batch_size, num_anchors, num_classes, code_size] representing the
         location of the objects.
       class_predictions_with_background: A float tensor of shape
-        [batch_size, 1, num_classes + 1] representing the class
+        [batch_size, num_anchors, num_classes + 1] representing the class
         predictions for the proposals.
+
     Raises:
-      ValueError: if num_predictions_per_location is not 1.
+      ValueError: if num_predictions_per_location is not 1 or if
+        len(image_features) is not 1.
     """
-    if num_predictions_per_location != 1:
+    if (len(num_predictions_per_location) != 1 or
+        num_predictions_per_location[0] != 1):
       raise ValueError('Currently RfcnBoxPredictor only supports '
                        'predicting a single box per class per location.')
-
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.
+                       format(len(image_features)))
+    image_feature = image_features[0]
+    num_predictions_per_location = num_predictions_per_location[0]
     batch_size = tf.shape(proposal_boxes)[0]
     num_boxes = tf.shape(proposal_boxes)[1]
     def get_box_indices(proposals):
@@ -202,7 +223,7 @@ class RfcnBoxPredictor(BoxPredictor):
           tf.range(start=0, limit=proposals_shape[0]), 1)
       return tf.reshape(ones_mat * multiplier, [-1])
 
-    net = image_features
+    net = image_feature
     with slim.arg_scope(self._conv_hyperparams):
       net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')
       # Location predictions.
@@ -280,6 +301,7 @@ class MaskRCNNBoxPredictor(BoxPredictor):
                predict_instance_masks=False,
                mask_height=14,
                mask_width=14,
+               mask_prediction_num_conv_layers=2,
                mask_prediction_conv_depth=256,
                predict_keypoints=False):
     """Constructor.
@@ -304,13 +326,21 @@ class MaskRCNNBoxPredictor(BoxPredictor):
         boxes.
       mask_height: Desired output mask height. The default value is 14.
       mask_width: Desired output mask width. The default value is 14.
+      mask_prediction_num_conv_layers: Number of convolution layers applied to
+        the image_features in mask prediction branch.
       mask_prediction_conv_depth: The depth for the first conv2d_transpose op
-        applied to the image_features in the mask prediciton branch.
+        applied to the image_features in the mask prediction branch. If set
+        to 0, the depth of the convolution layers will be automatically chosen
+        based on the number of object classes and the number of channels in the
+        image features.
       predict_keypoints: Whether to predict keypoints insde detection boxes.
 
 
     Raises:
-      ValueError: If predict_instance_masks or predict_keypoints is true.
+      ValueError: If predict_instance_masks is true but conv_hyperparams is not
+        set.
+      ValueError: If predict_keypoints is true since it is not implemented yet.
+      ValueError: If mask_prediction_num_conv_layers is smaller than two.
     """
     super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)
     self._fc_hyperparams = fc_hyperparams
@@ -321,6 +351,7 @@ class MaskRCNNBoxPredictor(BoxPredictor):
     self._predict_instance_masks = predict_instance_masks
     self._mask_height = mask_height
     self._mask_width = mask_width
+    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers
     self._mask_prediction_conv_depth = mask_prediction_conv_depth
     self._predict_keypoints = predict_keypoints
     if self._predict_keypoints:
@@ -329,52 +360,33 @@ class MaskRCNNBoxPredictor(BoxPredictor):
         self._conv_hyperparams is None):
       raise ValueError('`conv_hyperparams` must be provided when predicting '
                        'masks.')
+    if self._mask_prediction_num_conv_layers < 2:
+      raise ValueError(
+          'Mask prediction should consist of at least 2 conv layers')
 
   @property
   def num_classes(self):
     return self._num_classes
 
-  def _predict(self, image_features, num_predictions_per_location):
-    """Computes encoded object locations and corresponding confidences.
-
-    Flattens image_features and applies fully connected ops (with no
-    non-linearity) to predict box encodings and class predictions.  In this
-    setting, anchors are not spatially arranged in any way and are assumed to
-    have been folded into the batch dimension.  Thus we output 1 for the
-    anchors dimension.
+  @property
+  def predicts_instance_masks(self):
+    return self._predict_instance_masks
 
-    Also optionally predicts instance masks.
-    The mask prediction head is based on the Mask RCNN paper with the following
-    modifications: We replace the deconvolution layer with a bilinear resize
-    and a convolution.
+  def _predict_boxes_and_classes(self, image_features):
+    """Predicts boxes and class scores.
 
     Args:
       image_features: A float tensor of shape [batch_size, height, width,
         channels] containing features for a batch of images.
-      num_predictions_per_location: an integer representing the number of box
-        predictions to be made per spatial location in the feature map.
-        Currently, this must be set to 1, or an error will be raised.
 
     Returns:
-      A dictionary containing the following tensors.
-        box_encodings: A float tensor of shape
-          [batch_size, 1, num_classes, code_size] representing the
-          location of the objects.
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, 1, num_classes + 1] representing the class
-          predictions for the proposals.
-      If predict_masks is True the dictionary also contains:
-        instance_masks: A float tensor of shape
-          [batch_size, 1, num_classes, image_height, image_width]
-      If predict_keypoints is True the dictionary also contains:
-        keypoints: [batch_size, 1, num_keypoints, 2]
-
-    Raises:
-      ValueError: if num_predictions_per_location is not 1.
+      box_encodings: A float tensor of shape
+        [batch_size, 1, num_classes, code_size] representing the location of the
+        objects.
+      class_predictions_with_background: A float tensor of shape
+        [batch_size, 1, num_classes + 1] representing the class predictions for
+        the proposals.
     """
-    if num_predictions_per_location != 1:
-      raise ValueError('Currently FullyConnectedBoxPredictor only supports '
-                       'predicting a single box per class per location.')
     spatial_averaged_image_features = tf.reduce_mean(image_features, [1, 2],
                                                      keep_dims=True,
                                                      name='AvgPool')
@@ -398,34 +410,155 @@ class MaskRCNNBoxPredictor(BoxPredictor):
         box_encodings, [-1, 1, self._num_classes, self._box_code_size])
     class_predictions_with_background = tf.reshape(
         class_predictions_with_background, [-1, 1, self._num_classes + 1])
+    return box_encodings, class_predictions_with_background
+
+  def _get_mask_predictor_conv_depth(self, num_feature_channels, num_classes,
+                                     class_weight=3.0, feature_weight=2.0):
+    """Computes the depth of the mask predictor convolutions.
+
+    Computes the depth of the mask predictor convolutions given feature channels
+    and number of classes by performing a weighted average of the two in
+    log space to compute the number of convolution channels. The weights that
+    are used for computing the weighted average do not need to sum to 1.
+
+    Args:
+      num_feature_channels: An integer containing the number of feature
+        channels.
+      num_classes: An integer containing the number of classes.
+      class_weight: Class weight used in computing the weighted average.
+      feature_weight: Feature weight used in computing the weighted average.
 
-    predictions_dict = {
-        BOX_ENCODINGS: box_encodings,
-        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_with_background
-    }
-
-    if self._predict_instance_masks:
-      with slim.arg_scope(self._conv_hyperparams):
-        upsampled_features = tf.image.resize_bilinear(
-            image_features,
-            [self._mask_height, self._mask_width],
-            align_corners=True)
+    Returns:
+      An integer containing the number of convolution channels used by mask
+        predictor.
+    """
+    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)
+    num_classes_log = math.log(float(num_classes), 2.0)
+    weighted_num_feature_channels_log = (
+        num_feature_channels_log * feature_weight)
+    weighted_num_classes_log = num_classes_log * class_weight
+    total_weight = feature_weight + class_weight
+    num_conv_channels_log = round(
+        (weighted_num_feature_channels_log + weighted_num_classes_log) /
+        total_weight)
+    return int(math.pow(2.0, num_conv_channels_log))
+
+  def _predict_masks(self, image_features):
+    """Performs mask prediction.
+
+    Args:
+      image_features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      instance_masks: A float tensor of shape
+          [batch_size, 1, num_classes, image_height, image_width].
+    """
+    num_conv_channels = self._mask_prediction_conv_depth
+    if num_conv_channels == 0:
+      num_feature_channels = image_features.get_shape().as_list()[3]
+      num_conv_channels = self._get_mask_predictor_conv_depth(
+          num_feature_channels, self.num_classes)
+    with slim.arg_scope(self._conv_hyperparams):
+      upsampled_features = tf.image.resize_bilinear(
+          image_features,
+          [self._mask_height, self._mask_width],
+          align_corners=True)
+      for _ in range(self._mask_prediction_num_conv_layers - 1):
         upsampled_features = slim.conv2d(
             upsampled_features,
-            num_outputs=self._mask_prediction_conv_depth,
-            kernel_size=[2, 2])
-        mask_predictions = slim.conv2d(upsampled_features,
-                                       num_outputs=self.num_classes,
-                                       activation_fn=None,
-                                       kernel_size=[3, 3])
-        instance_masks = tf.expand_dims(tf.transpose(mask_predictions,
-                                                     perm=[0, 3, 1, 2]),
-                                        axis=1,
-                                        name='MaskPredictor')
-      predictions_dict[MASK_PREDICTIONS] = instance_masks
+            num_outputs=num_conv_channels,
+            kernel_size=[3, 3])
+      mask_predictions = slim.conv2d(upsampled_features,
+                                     num_outputs=self.num_classes,
+                                     activation_fn=None,
+                                     kernel_size=[3, 3])
+      return tf.expand_dims(
+          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),
+          axis=1,
+          name='MaskPredictor')
+
+  def _predict(self, image_features, num_predictions_per_location,
+               predict_boxes_and_classes=True, predict_auxiliary_outputs=False):
+    """Optionally computes encoded object locations, confidences, and masks.
+
+    Flattens image_features and applies fully connected ops (with no
+    non-linearity) to predict box encodings and class predictions.  In this
+    setting, anchors are not spatially arranged in any way and are assumed to
+    have been folded into the batch dimension.  Thus we output 1 for the
+    anchors dimension.
+
+    Also optionally predicts instance masks.
+    The mask prediction head is based on the Mask RCNN paper with the following
+    modifications: We replace the deconvolution layer with a bilinear resize
+    and a convolution.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
+        Currently, this must be set to [1], or an error will be raised.
+      predict_boxes_and_classes: If true, the function will perform box
+        refinement and classification.
+      predict_auxiliary_outputs: If true, the function will perform other
+        predictions such as mask, keypoint, boundaries, etc. if any.
+
+    Returns:
+      A dictionary containing the following tensors.
+        box_encodings: A float tensor of shape
+          [batch_size, 1, num_classes, code_size] representing the
+          location of the objects.
+        class_predictions_with_background: A float tensor of shape
+          [batch_size, 1, num_classes + 1] representing the class
+          predictions for the proposals.
+      If predict_masks is True the dictionary also contains:
+        instance_masks: A float tensor of shape
+          [batch_size, 1, num_classes, image_height, image_width]
+      If predict_keypoints is True the dictionary also contains:
+        keypoints: [batch_size, 1, num_keypoints, 2]
+
+    Raises:
+      ValueError: If num_predictions_per_location is not 1 or if both
+        predict_boxes_and_classes and predict_auxiliary_outputs are false or if
+        len(image_features) is not 1.
+    """
+    if (len(num_predictions_per_location) != 1 or
+        num_predictions_per_location[0] != 1):
+      raise ValueError('Currently FullyConnectedBoxPredictor only supports '
+                       'predicting a single box per class per location.')
+    if not predict_boxes_and_classes and not predict_auxiliary_outputs:
+      raise ValueError('Should perform at least one prediction.')
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.
+                       format(len(image_features)))
+    image_feature = image_features[0]
+    num_predictions_per_location = num_predictions_per_location[0]
+    predictions_dict = {}
+
+    if predict_boxes_and_classes:
+      (box_encodings, class_predictions_with_background
+      ) = self._predict_boxes_and_classes(image_feature)
+      predictions_dict[BOX_ENCODINGS] = box_encodings
+      predictions_dict[
+          CLASS_PREDICTIONS_WITH_BACKGROUND] = class_predictions_with_background
+
+    if self._predict_instance_masks and predict_auxiliary_outputs:
+      predictions_dict[MASK_PREDICTIONS] = self._predict_masks(image_feature)
+
     return predictions_dict
 
 
+class _NoopVariableScope(object):
+  """A dummy class that does not push any scope."""
+
+  def __enter__(self):
+    return None
+
+  def __exit__(self, exc_type, exc_value, traceback):
+    return False
+
+
 class ConvolutionalBoxPredictor(BoxPredictor):
   """Convolutional Box Predictor.
 
@@ -497,14 +630,15 @@ class ConvolutionalBoxPredictor(BoxPredictor):
     self._apply_sigmoid_to_scores = apply_sigmoid_to_scores
     self._class_prediction_bias_init = class_prediction_bias_init
 
-  def _predict(self, image_features, num_predictions_per_location):
+  def _predict(self, image_features, num_predictions_per_location_list):
     """Computes encoded object locations and corresponding confidences.
 
     Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
-      num_predictions_per_location: an integer representing the number of box
-        predictions to be made per spatial location in the feature map.
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location_list: A list of integers representing the
+        number of box predictions to be made per spatial location for each
+        feature map.
 
     Returns:
       A dictionary containing the following tensors.
@@ -514,53 +648,210 @@ class ConvolutionalBoxPredictor(BoxPredictor):
         class_predictions_with_background: A float tensor of shape
           [batch_size, num_anchors, num_classes + 1] representing the class
           predictions for the proposals.
+
     """
-    # Add a slot for the background class.
-    num_class_slots = self.num_classes + 1
-    net = image_features
-    with slim.arg_scope(self._conv_hyperparams), \
-         slim.arg_scope([slim.dropout], is_training=self._is_training):
-      # Add additional conv layers before the class predictor.
-      features_depth = static_shape.get_depth(image_features.get_shape())
-      depth = max(min(features_depth, self._max_depth), self._min_depth)
-      tf.logging.info('depth of additional conv before box predictor: {}'.
-                      format(depth))
-      if depth > 0 and self._num_layers_before_predictor > 0:
-        for i in range(self._num_layers_before_predictor):
-          net = slim.conv2d(
-              net, depth, [1, 1], scope='Conv2d_%d_1x1_%d' % (i, depth))
-      with slim.arg_scope([slim.conv2d], activation_fn=None,
-                          normalizer_fn=None, normalizer_params=None):
-        box_encodings = slim.conv2d(
-            net, num_predictions_per_location * self._box_code_size,
-            [self._kernel_size, self._kernel_size],
-            scope='BoxEncodingPredictor')
-        if self._use_dropout:
-          net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
-        class_predictions_with_background = slim.conv2d(
-            net, num_predictions_per_location * num_class_slots,
-            [self._kernel_size, self._kernel_size], scope='ClassPredictor',
-            biases_initializer=tf.constant_initializer(
-                self._class_prediction_bias_init))
-        if self._apply_sigmoid_to_scores:
-          class_predictions_with_background = tf.sigmoid(
-              class_predictions_with_background)
-
-    combined_feature_map_shape = shape_utils.combined_static_and_dynamic_shape(
-        image_features)
-    box_encodings = tf.reshape(
-        box_encodings, tf.stack([combined_feature_map_shape[0],
-                                 combined_feature_map_shape[1] *
-                                 combined_feature_map_shape[2] *
-                                 num_predictions_per_location,
-                                 1, self._box_code_size]))
-    class_predictions_with_background = tf.reshape(
-        class_predictions_with_background,
-        tf.stack([combined_feature_map_shape[0],
-                  combined_feature_map_shape[1] *
-                  combined_feature_map_shape[2] *
-                  num_predictions_per_location,
-                  num_class_slots]))
-    return {BOX_ENCODINGS: box_encodings,
+    box_encodings_list = []
+    class_predictions_list = []
+    # TODO: Come up with a better way to generate scope names
+    # in box predictor once we have time to retrain all models in the zoo.
+    # The following lines create scope names to be backwards compatible with the
+    # existing checkpoints.
+    box_predictor_scopes = [_NoopVariableScope()]
+    if len(image_features) > 1:
+      box_predictor_scopes = [
+          tf.variable_scope('BoxPredictor_{}'.format(i))
+          for i in range(len(image_features))
+      ]
+
+    for (image_feature,
+         num_predictions_per_location, box_predictor_scope) in zip(
+             image_features, num_predictions_per_location_list,
+             box_predictor_scopes):
+      with box_predictor_scope:
+        # Add a slot for the background class.
+        num_class_slots = self.num_classes + 1
+        net = image_feature
+        with slim.arg_scope(self._conv_hyperparams), \
+             slim.arg_scope([slim.dropout], is_training=self._is_training):
+          # Add additional conv layers before the class predictor.
+          features_depth = static_shape.get_depth(image_feature.get_shape())
+          depth = max(min(features_depth, self._max_depth), self._min_depth)
+          tf.logging.info('depth of additional conv before box predictor: {}'.
+                          format(depth))
+          if depth > 0 and self._num_layers_before_predictor > 0:
+            for i in range(self._num_layers_before_predictor):
+              net = slim.conv2d(
+                  net, depth, [1, 1], scope='Conv2d_%d_1x1_%d' % (i, depth))
+          with slim.arg_scope([slim.conv2d], activation_fn=None,
+                              normalizer_fn=None, normalizer_params=None):
+            box_encodings = slim.conv2d(
+                net, num_predictions_per_location * self._box_code_size,
+                [self._kernel_size, self._kernel_size],
+                scope='BoxEncodingPredictor')
+            if self._use_dropout:
+              net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
+            class_predictions_with_background = slim.conv2d(
+                net, num_predictions_per_location * num_class_slots,
+                [self._kernel_size, self._kernel_size], scope='ClassPredictor',
+                biases_initializer=tf.constant_initializer(
+                    self._class_prediction_bias_init))
+            if self._apply_sigmoid_to_scores:
+              class_predictions_with_background = tf.sigmoid(
+                  class_predictions_with_background)
+
+        combined_feature_map_shape = (shape_utils.
+                                      combined_static_and_dynamic_shape(
+                                          image_feature))
+        box_encodings = tf.reshape(
+            box_encodings, tf.stack([combined_feature_map_shape[0],
+                                     combined_feature_map_shape[1] *
+                                     combined_feature_map_shape[2] *
+                                     num_predictions_per_location,
+                                     1, self._box_code_size]))
+        box_encodings_list.append(box_encodings)
+        class_predictions_with_background = tf.reshape(
+            class_predictions_with_background,
+            tf.stack([combined_feature_map_shape[0],
+                      combined_feature_map_shape[1] *
+                      combined_feature_map_shape[2] *
+                      num_predictions_per_location,
+                      num_class_slots]))
+        class_predictions_list.append(class_predictions_with_background)
+    return {BOX_ENCODINGS: tf.concat(box_encodings_list, axis=1),
             CLASS_PREDICTIONS_WITH_BACKGROUND:
-            class_predictions_with_background}
+            tf.concat(class_predictions_list, axis=1)}
+
+
+# TODO: Merge the implementation with ConvolutionalBoxPredictor above
+# since they are very similar.
+class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
+  """Convolutional Box Predictor with weight sharing.
+
+  Defines the box predictor as defined in
+  https://arxiv.org/abs/1708.02002. This class differs from
+  ConvolutionalBoxPredictor in that it shares weights and biases while
+  predicting from different feature maps.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               conv_hyperparams,
+               depth,
+               num_layers_before_predictor,
+               box_code_size,
+               kernel_size=3,
+               class_prediction_bias_init=0.0):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams: Slim arg_scope with hyperparameters for convolution ops.
+      depth: depth of conv layers.
+      num_layers_before_predictor: Number of the additional conv layers before
+        the predictor.
+      box_code_size: Size of encoding for each box.
+      kernel_size: Size of final convolution kernel.
+      class_prediction_bias_init: constant value to initialize bias of the last
+        conv2d layer before class prediction.
+    """
+    super(WeightSharedConvolutionalBoxPredictor, self).__init__(is_training,
+                                                                num_classes)
+    self._conv_hyperparams = conv_hyperparams
+    self._depth = depth
+    self._num_layers_before_predictor = num_layers_before_predictor
+    self._box_code_size = box_code_size
+    self._kernel_size = kernel_size
+    self._class_prediction_bias_init = class_prediction_bias_init
+
+  def _predict(self, image_features, num_predictions_per_location_list):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels] containing features for a batch of images. Note that
+        all tensors in the list must have the same number of channels.
+      num_predictions_per_location_list: A list of integers representing the
+        number of box predictions to be made per spatial location for each
+        feature map. Note that all values must be the same since the weights are
+        shared.
+
+    Returns:
+      A dictionary containing the following tensors.
+        box_encodings: A float tensor of shape [batch_size, num_anchors, 1,
+          code_size] representing the location of the objects, where
+          num_anchors = feat_height * feat_width * num_predictions_per_location
+        class_predictions_with_background: A float tensor of shape
+          [batch_size, num_anchors, num_classes + 1] representing the class
+          predictions for the proposals.
+
+    Raises:
+      ValueError: If the image feature maps do not have the same number of
+        channels or if the num predictions per locations is differs between the
+        feature maps.
+    """
+    if len(set(num_predictions_per_location_list)) > 1:
+      raise ValueError('num predictions per location must be same for all'
+                       'feature maps, found: {}'.format(
+                           num_predictions_per_location_list))
+    feature_channels = [
+        image_feature.shape[3].value for image_feature in image_features
+    ]
+    if len(set(feature_channels)) > 1:
+      raise ValueError('all feature maps must have the same number of '
+                       'channels, found: {}'.format(feature_channels))
+    box_encodings_list = []
+    class_predictions_list = []
+    for (image_feature, num_predictions_per_location) in zip(
+        image_features, num_predictions_per_location_list):
+      # Add a slot for the background class.
+      with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
+                             reuse=tf.AUTO_REUSE):
+        num_class_slots = self.num_classes + 1
+        net = image_feature
+        with slim.arg_scope(self._conv_hyperparams):
+          for i in range(self._num_layers_before_predictor):
+            net = slim.conv2d(net,
+                              self._depth,
+                              [self._kernel_size, self._kernel_size],
+                              stride=1,
+                              padding='SAME',
+                              scope='conv2d_{}'.format(i))
+          box_encodings = slim.conv2d(
+              net, num_predictions_per_location * self._box_code_size,
+              [self._kernel_size, self._kernel_size],
+              activation_fn=None, stride=1, padding='SAME',
+              scope='BoxEncodingPredictor')
+          class_predictions_with_background = slim.conv2d(
+              net, num_predictions_per_location * num_class_slots,
+              [self._kernel_size, self._kernel_size],
+              activation_fn=None, stride=1, padding='SAME',
+              biases_initializer=tf.constant_initializer(
+                  self._class_prediction_bias_init),
+              scope='ClassPredictor')
+
+          combined_feature_map_shape = (shape_utils.
+                                        combined_static_and_dynamic_shape(
+                                            image_feature))
+          box_encodings = tf.reshape(
+              box_encodings, tf.stack([combined_feature_map_shape[0],
+                                       combined_feature_map_shape[1] *
+                                       combined_feature_map_shape[2] *
+                                       num_predictions_per_location,
+                                       1, self._box_code_size]))
+          box_encodings_list.append(box_encodings)
+          class_predictions_with_background = tf.reshape(
+              class_predictions_with_background,
+              tf.stack([combined_feature_map_shape[0],
+                        combined_feature_map_shape[1] *
+                        combined_feature_map_shape[2] *
+                        num_predictions_per_location,
+                        num_class_slots]))
+          class_predictions_list.append(class_predictions_with_background)
+    return {BOX_ENCODINGS: tf.concat(box_encodings_list, axis=1),
+            CLASS_PREDICTIONS_WITH_BACKGROUND:
+            tf.concat(class_predictions_list, axis=1)}
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/core/box_predictor_test.py
index e5e5a3c9..e076f441 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/core/box_predictor_test.py
@@ -14,7 +14,6 @@
 # ==============================================================================
 
 """Tests for object_detection.core.box_predictor."""
-
 import numpy as np
 import tensorflow as tf
 
@@ -22,6 +21,7 @@ from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
 from object_detection.core import box_predictor
 from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
 
 
 class MaskRCNNBoxPredictorTest(tf.test.TestCase):
@@ -55,7 +55,8 @@ class MaskRCNNBoxPredictorTest(tf.test.TestCase):
         box_code_size=4,
     )
     box_predictions = mask_box_predictor.predict(
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[1],
+        scope='BoxPredictor')
     box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
     class_predictions_with_background = box_predictions[
         box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
@@ -93,12 +94,16 @@ class MaskRCNNBoxPredictorTest(tf.test.TestCase):
             op_type=hyperparams_pb2.Hyperparams.CONV),
         predict_instance_masks=True)
     box_predictions = mask_box_predictor.predict(
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features],
+        num_predictions_per_location=[1],
+        scope='BoxPredictor',
+        predict_boxes_and_classes=True,
+        predict_auxiliary_outputs=True)
     mask_predictions = box_predictions[box_predictor.MASK_PREDICTIONS]
     self.assertListEqual([2, 1, 5, 14, 14],
                          mask_predictions.get_shape().as_list())
 
-  def test_do_not_return_instance_masks_and_keypoints_without_request(self):
+  def test_do_not_return_instance_masks_without_request(self):
     image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
     mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(
         is_training=False,
@@ -108,7 +113,8 @@ class MaskRCNNBoxPredictorTest(tf.test.TestCase):
         dropout_keep_prob=0.5,
         box_code_size=4)
     box_predictions = mask_box_predictor.predict(
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[1],
+        scope='BoxPredictor')
     self.assertEqual(len(box_predictions), 2)
     self.assertTrue(box_predictor.BOX_ENCODINGS in box_predictions)
     self.assertTrue(box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND
@@ -156,7 +162,8 @@ class RfcnBoxPredictorTest(tf.test.TestCase):
         box_code_size=4
     )
     box_predictions = rfcn_box_predictor.predict(
-        image_features, num_predictions_per_location=1, scope='BoxPredictor',
+        [image_features], num_predictions_per_location=[1],
+        scope='BoxPredictor',
         proposal_boxes=proposal_boxes)
     box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
     class_predictions_with_background = box_predictions[
@@ -173,7 +180,7 @@ class RfcnBoxPredictorTest(tf.test.TestCase):
       self.assertAllEqual(class_predictions_shape, [8, 1, 3])
 
 
-class ConvolutionalBoxPredictorTest(tf.test.TestCase):
+class ConvolutionalBoxPredictorTest(test_case.TestCase):
 
   def _build_arg_scope_with_conv_hyperparams(self):
     conv_hyperparams = hyperparams_pb2.Hyperparams()
@@ -192,36 +199,94 @@ class ConvolutionalBoxPredictorTest(tf.test.TestCase):
     return hyperparams_builder.build(conv_hyperparams, is_training=True)
 
   def test_get_boxes_for_five_aspect_ratios_per_location(self):
-    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)
-    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
-        is_training=False,
-        num_classes=0,
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
-        min_depth=0,
-        max_depth=32,
-        num_layers_before_predictor=1,
-        use_dropout=True,
-        dropout_keep_prob=0.8,
-        kernel_size=1,
-        box_code_size=4
-    )
-    box_predictions = conv_box_predictor.predict(
-        image_features, num_predictions_per_location=5, scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    objectness_predictions = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
-
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      (box_encodings_shape,
-       objectness_predictions_shape) = sess.run(
-           [tf.shape(box_encodings), tf.shape(objectness_predictions)])
-      self.assertAllEqual(box_encodings_shape, [4, 320, 1, 4])
-      self.assertAllEqual(objectness_predictions_shape, [4, 320, 1])
+    def graph_fn(image_features):
+      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=0,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          min_depth=0,
+          max_depth=32,
+          num_layers_before_predictor=1,
+          use_dropout=True,
+          dropout_keep_prob=0.8,
+          kernel_size=1,
+          box_code_size=4
+      )
+      box_predictions = conv_box_predictor.predict(
+          [image_features], num_predictions_per_location=[5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      objectness_predictions = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, objectness_predictions)
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, objectness_predictions) = self.execute(graph_fn,
+                                                           [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])
 
   def test_get_boxes_for_one_aspect_ratio_per_location(self):
-    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)
+    def graph_fn(image_features):
+      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=0,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          min_depth=0,
+          max_depth=32,
+          num_layers_before_predictor=1,
+          use_dropout=True,
+          dropout_keep_prob=0.8,
+          kernel_size=1,
+          box_code_size=4
+      )
+      box_predictions = conv_box_predictor.predict(
+          [image_features], num_predictions_per_location=[1],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      objectness_predictions = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, objectness_predictions)
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, objectness_predictions) = self.execute(graph_fn,
+                                                           [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 64, 1, 4])
+    self.assertAllEqual(objectness_predictions.shape, [4, 64, 1])
+
+  def test_get_multi_class_predictions_for_five_aspect_ratios_per_location(
+      self):
+    num_classes_without_background = 6
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    def graph_fn(image_features):
+      conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          min_depth=0,
+          max_depth=32,
+          num_layers_before_predictor=1,
+          use_dropout=True,
+          dropout_keep_prob=0.8,
+          kernel_size=1,
+          box_code_size=4
+      )
+      box_predictions = conv_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      class_predictions_with_background = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, class_predictions_with_background)
+    (box_encodings,
+     class_predictions_with_background) = self.execute(graph_fn,
+                                                       [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 320, num_classes_without_background+1])
+
+  def test_get_predictions_with_feature_maps_of_dynamic_shape(
+      self):
+    image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
     conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
         is_training=False,
         num_classes=0,
@@ -235,71 +300,177 @@ class ConvolutionalBoxPredictorTest(tf.test.TestCase):
         box_code_size=4
     )
     box_predictions = conv_box_predictor.predict(
-        image_features, num_predictions_per_location=1, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[5],
+        scope='BoxPredictor')
     box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
     objectness_predictions = box_predictions[
         box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
-
     init_op = tf.global_variables_initializer()
+
+    resolution = 32
+    expected_num_anchors = resolution*resolution*5
     with self.test_session() as sess:
       sess.run(init_op)
       (box_encodings_shape,
        objectness_predictions_shape) = sess.run(
-           [tf.shape(box_encodings), tf.shape(objectness_predictions)])
-      self.assertAllEqual(box_encodings_shape, [4, 64, 1, 4])
-      self.assertAllEqual(objectness_predictions_shape, [4, 64, 1])
+           [tf.shape(box_encodings), tf.shape(objectness_predictions)],
+           feed_dict={image_features:
+                      np.random.rand(4, resolution, resolution, 64)})
+      self.assertAllEqual(box_encodings_shape, [4, expected_num_anchors, 1, 4])
+      self.assertAllEqual(objectness_predictions_shape,
+                          [4, expected_num_anchors, 1])
+
+
+class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
+
+  def _build_arg_scope_with_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      activation: RELU_6
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.build(conv_hyperparams, is_training=True)
+
+  def test_get_boxes_for_five_aspect_ratios_per_location(self):
+
+    def graph_fn(image_features):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=0,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=1,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features], num_predictions_per_location=[5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      objectness_predictions = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, objectness_predictions)
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, objectness_predictions) = self.execute(
+        graph_fn, [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(objectness_predictions.shape, [4, 320, 1])
 
   def test_get_multi_class_predictions_for_five_aspect_ratios_per_location(
       self):
+
     num_classes_without_background = 6
-    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)
-    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
-        is_training=False,
-        num_classes=num_classes_without_background,
-        conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
-        min_depth=0,
-        max_depth=32,
-        num_layers_before_predictor=1,
-        use_dropout=True,
-        dropout_keep_prob=0.8,
-        kernel_size=1,
-        box_code_size=4
-    )
-    box_predictions = conv_box_predictor.predict(
-        image_features,
-        num_predictions_per_location=5,
-        scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    class_predictions_with_background = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+    def graph_fn(image_features):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=1,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      class_predictions_with_background = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, class_predictions_with_background)
 
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      (box_encodings_shape, class_predictions_with_background_shape
-      ) = sess.run([
-          tf.shape(box_encodings), tf.shape(class_predictions_with_background)])
-      self.assertAllEqual(box_encodings_shape, [4, 320, 1, 4])
-      self.assertAllEqual(class_predictions_with_background_shape,
-                          [4, 320, num_classes_without_background+1])
-
-  def test_get_boxes_for_five_aspect_ratios_per_location_fully_convolutional(
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features])
+    self.assertAllEqual(box_encodings.shape, [4, 320, 1, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 320, num_classes_without_background+1])
+
+  def test_get_multi_class_predictions_from_two_feature_maps(
+      self):
+
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=1,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      class_predictions_with_background = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, class_predictions_with_background)
+
+    image_features1 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    image_features2 = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features1, image_features2])
+    self.assertAllEqual(box_encodings.shape, [4, 640, 1, 4])
+    self.assertAllEqual(class_predictions_with_background.shape,
+                        [4, 640, num_classes_without_background+1])
+
+  def test_predictions_from_multiple_feature_maps_share_weights(self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=2,
+          box_code_size=4)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
+      class_predictions_with_background = box_predictions[
+          box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 32, 32, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_0/weights',
+        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_0/biases',
+        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_1/weights',
+        'BoxPredictor/WeightSharedConvolutionalBoxPredictor/conv2d_1/biases',
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxEncodingPredictor/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/biases')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
+  def test_get_predictions_with_feature_maps_of_dynamic_shape(
       self):
     image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
-    conv_box_predictor = box_predictor.ConvolutionalBoxPredictor(
+    conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
         is_training=False,
         num_classes=0,
         conv_hyperparams=self._build_arg_scope_with_conv_hyperparams(),
-        min_depth=0,
-        max_depth=32,
+        depth=32,
         num_layers_before_predictor=1,
-        use_dropout=True,
-        dropout_keep_prob=0.8,
-        kernel_size=1,
-        box_code_size=4
-    )
+        box_code_size=4)
     box_predictions = conv_box_predictor.predict(
-        image_features, num_predictions_per_location=5, scope='BoxPredictor')
+        [image_features], num_predictions_per_location=[5],
+        scope='BoxPredictor')
     box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
     objectness_predictions = box_predictions[
         box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
diff --git a/research/object_detection/core/losses.py b/research/object_detection/core/losses.py
index b8478c15..2fd2f547 100644
--- a/research/object_detection/core/losses.py
+++ b/research/object_detection/core/losses.py
@@ -50,8 +50,10 @@ class Loss(object):
     """Call the loss function.
 
     Args:
-      prediction_tensor: a tensor representing predicted quantities.
-      target_tensor: a tensor representing regression or classification targets.
+      prediction_tensor: an N-d tensor of shape [batch, anchors, ...]
+        representing predicted quantities.
+      target_tensor: an N-d tensor of shape [batch, anchors, ...] representing
+        regression or classification targets.
       ignore_nan_targets: whether to ignore nan targets in the loss computation.
         E.g. can be used if the target tensor is missing groundtruth data that
         shouldn't be factored into the loss.
@@ -81,7 +83,8 @@ class Loss(object):
               the Loss.
 
     Returns:
-      loss: a tensor representing the value of the loss function
+      loss: an N-d tensor of shape [batch, anchors, ...] containing the loss per
+        anchor
     """
     pass
 
@@ -92,15 +95,6 @@ class WeightedL2LocalizationLoss(Loss):
   Loss[b,a] = .5 * ||weights[b,a] * (prediction[b,a,:] - target[b,a,:])||^2
   """
 
-  def __init__(self, anchorwise_output=False):
-    """Constructor.
-
-    Args:
-      anchorwise_output: Outputs loss per anchor. (default False)
-
-    """
-    self._anchorwise_output = anchorwise_output
-
   def _compute_loss(self, prediction_tensor, target_tensor, weights):
     """Compute loss function.
 
@@ -112,15 +106,13 @@ class WeightedL2LocalizationLoss(Loss):
       weights: a float tensor of shape [batch_size, num_anchors]
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
+        representing the value of the loss function.
     """
     weighted_diff = (prediction_tensor - target_tensor) * tf.expand_dims(
         weights, 2)
     square_diff = 0.5 * tf.square(weighted_diff)
-    if self._anchorwise_output:
-      return tf.reduce_sum(square_diff, 2)
-    return tf.reduce_sum(square_diff)
+    return tf.reduce_sum(square_diff, 2)
 
 
 class WeightedSmoothL1LocalizationLoss(Loss):
@@ -132,15 +124,6 @@ class WeightedSmoothL1LocalizationLoss(Loss):
   See also Equation (3) in the Fast R-CNN paper by Ross Girshick (ICCV 2015)
   """
 
-  def __init__(self, anchorwise_output=False):
-    """Constructor.
-
-    Args:
-      anchorwise_output: Outputs loss per anchor. (default False)
-
-    """
-    self._anchorwise_output = anchorwise_output
-
   def _compute_loss(self, prediction_tensor, target_tensor, weights):
     """Compute loss function.
 
@@ -152,7 +135,8 @@ class WeightedSmoothL1LocalizationLoss(Loss):
       weights: a float tensor of shape [batch_size, num_anchors]
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
+        representing the value of the loss function.
     """
     diff = prediction_tensor - target_tensor
     abs_diff = tf.abs(diff)
@@ -160,9 +144,7 @@ class WeightedSmoothL1LocalizationLoss(Loss):
     anchorwise_smooth_l1norm = tf.reduce_sum(
         tf.where(abs_diff_lt_1, 0.5 * tf.square(abs_diff), abs_diff - 0.5),
         2) * weights
-    if self._anchorwise_output:
-      return anchorwise_smooth_l1norm
-    return tf.reduce_sum(anchorwise_smooth_l1norm)
+    return anchorwise_smooth_l1norm
 
 
 class WeightedIOULocalizationLoss(Loss):
@@ -184,27 +166,19 @@ class WeightedIOULocalizationLoss(Loss):
       weights: a float tensor of shape [batch_size, num_anchors]
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors] tensor
+        representing the value of the loss function.
     """
     predicted_boxes = box_list.BoxList(tf.reshape(prediction_tensor, [-1, 4]))
     target_boxes = box_list.BoxList(tf.reshape(target_tensor, [-1, 4]))
     per_anchor_iou_loss = 1.0 - box_list_ops.matched_iou(predicted_boxes,
                                                          target_boxes)
-    return tf.reduce_sum(tf.reshape(weights, [-1]) * per_anchor_iou_loss)
+    return tf.reshape(weights, [-1]) * per_anchor_iou_loss
 
 
 class WeightedSigmoidClassificationLoss(Loss):
   """Sigmoid cross entropy classification loss function."""
 
-  def __init__(self, anchorwise_output=False):
-    """Constructor.
-
-    Args:
-      anchorwise_output: Outputs loss per anchor. (default False)
-
-    """
-    self._anchorwise_output = anchorwise_output
-
   def _compute_loss(self,
                     prediction_tensor,
                     target_tensor,
@@ -222,8 +196,8 @@ class WeightedSigmoidClassificationLoss(Loss):
         If provided, computes loss only for the specified class indices.
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
+        representing the value of the loss function.
     """
     weights = tf.expand_dims(weights, 2)
     if class_indices is not None:
@@ -233,9 +207,7 @@ class WeightedSigmoidClassificationLoss(Loss):
           [1, 1, -1])
     per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(
         labels=target_tensor, logits=prediction_tensor))
-    if self._anchorwise_output:
-      return tf.reduce_sum(per_entry_cross_ent * weights, 2)
-    return tf.reduce_sum(per_entry_cross_ent * weights)
+    return per_entry_cross_ent * weights
 
 
 class SigmoidFocalClassificationLoss(Loss):
@@ -245,15 +217,13 @@ class SigmoidFocalClassificationLoss(Loss):
   examples. See https://arxiv.org/pdf/1708.02002.pdf for the loss definition.
   """
 
-  def __init__(self, anchorwise_output=False, gamma=2.0, alpha=0.25):
+  def __init__(self, gamma=2.0, alpha=0.25):
     """Constructor.
 
     Args:
-      anchorwise_output: Outputs loss per anchor. (default False)
       gamma: exponent of the modulating factor (1 - p_t) ^ gamma.
       alpha: optional alpha weighting factor to balance positives vs negatives.
     """
-    self._anchorwise_output = anchorwise_output
     self._alpha = alpha
     self._gamma = gamma
 
@@ -274,8 +244,8 @@ class SigmoidFocalClassificationLoss(Loss):
         If provided, computes loss only for the specified class indices.
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
+        representing the value of the loss function.
     """
     weights = tf.expand_dims(weights, 2)
     if class_indices is not None:
@@ -297,25 +267,21 @@ class SigmoidFocalClassificationLoss(Loss):
                              (1 - target_tensor) * (1 - self._alpha))
     focal_cross_entropy_loss = (modulating_factor * alpha_weight_factor *
                                 per_entry_cross_ent)
-    if self._anchorwise_output:
-      return tf.reduce_sum(focal_cross_entropy_loss * weights, 2)
-    return tf.reduce_sum(focal_cross_entropy_loss * weights)
+    return focal_cross_entropy_loss * weights
 
 
 class WeightedSoftmaxClassificationLoss(Loss):
   """Softmax loss function."""
 
-  def __init__(self, anchorwise_output=False, logit_scale=1.0):
+  def __init__(self, logit_scale=1.0):
     """Constructor.
 
     Args:
-      anchorwise_output: Whether to output loss per anchor (default False)
       logit_scale: When this value is high, the prediction is "diffused" and
                    when this value is low, the prediction is made peakier.
                    (default 1.0)
 
     """
-    self._anchorwise_output = anchorwise_output
     self._logit_scale = logit_scale
 
   def _compute_loss(self, prediction_tensor, target_tensor, weights):
@@ -329,7 +295,8 @@ class WeightedSoftmaxClassificationLoss(Loss):
       weights: a float tensor of shape [batch_size, num_anchors]
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
+      loss: a float tensor of shape [batch_size, num_anchors]
+        representing the value of the loss function.
     """
     num_classes = prediction_tensor.get_shape().as_list()[-1]
     prediction_tensor = tf.divide(
@@ -337,9 +304,7 @@ class WeightedSoftmaxClassificationLoss(Loss):
     per_row_cross_ent = (tf.nn.softmax_cross_entropy_with_logits(
         labels=tf.reshape(target_tensor, [-1, num_classes]),
         logits=tf.reshape(prediction_tensor, [-1, num_classes])))
-    if self._anchorwise_output:
-      return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights
-    return tf.reduce_sum(per_row_cross_ent * tf.reshape(weights, [-1]))
+    return tf.reshape(per_row_cross_ent, tf.shape(weights)) * weights
 
 
 class BootstrappedSigmoidClassificationLoss(Loss):
@@ -359,14 +324,13 @@ class BootstrappedSigmoidClassificationLoss(Loss):
   Reed et al. (ICLR 2015).
   """
 
-  def __init__(self, alpha, bootstrap_type='soft', anchorwise_output=False):
+  def __init__(self, alpha, bootstrap_type='soft'):
     """Constructor.
 
     Args:
       alpha: a float32 scalar tensor between 0 and 1 representing interpolation
         weight
       bootstrap_type: set to either 'hard' or 'soft' (default)
-      anchorwise_output: Outputs loss per anchor. (default False)
 
     Raises:
       ValueError: if bootstrap_type is not either 'hard' or 'soft'
@@ -376,7 +340,6 @@ class BootstrappedSigmoidClassificationLoss(Loss):
                        '\'hard\' or \'soft.\'')
     self._alpha = alpha
     self._bootstrap_type = bootstrap_type
-    self._anchorwise_output = anchorwise_output
 
   def _compute_loss(self, prediction_tensor, target_tensor, weights):
     """Compute loss function.
@@ -389,8 +352,8 @@ class BootstrappedSigmoidClassificationLoss(Loss):
       weights: a float tensor of shape [batch_size, num_anchors]
 
     Returns:
-      loss: a (scalar) tensor representing the value of the loss function
-            or a float tensor of shape [batch_size, num_anchors]
+      loss: a float tensor of shape [batch_size, num_anchors, num_classes]
+        representing the value of the loss function.
     """
     if self._bootstrap_type == 'soft':
       bootstrap_target_tensor = self._alpha * target_tensor + (
@@ -401,9 +364,7 @@ class BootstrappedSigmoidClassificationLoss(Loss):
               tf.sigmoid(prediction_tensor) > 0.5, tf.float32)
     per_entry_cross_ent = (tf.nn.sigmoid_cross_entropy_with_logits(
         labels=bootstrap_target_tensor, logits=prediction_tensor))
-    if self._anchorwise_output:
-      return tf.reduce_sum(per_entry_cross_ent * tf.expand_dims(weights, 2), 2)
-    return tf.reduce_sum(per_entry_cross_ent * tf.expand_dims(weights, 2))
+    return per_entry_cross_ent * tf.expand_dims(weights, 2)
 
 
 class HardExampleMiner(object):
diff --git a/research/object_detection/core/losses_test.py b/research/object_detection/core/losses_test.py
index b37ae3f7..46620c2c 100644
--- a/research/object_detection/core/losses_test.py
+++ b/research/object_detection/core/losses_test.py
@@ -26,7 +26,7 @@ from object_detection.core import matcher
 
 class WeightedL2LocalizationLossTest(tf.test.TestCase):
 
-  def testReturnsCorrectLoss(self):
+  def testReturnsCorrectWeightedLoss(self):
     batch_size = 3
     num_anchors = 10
     code_size = 4
@@ -36,7 +36,8 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
                            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],
                            [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], tf.float32)
     loss_op = losses.WeightedL2LocalizationLoss()
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss_op(prediction_tensor, target_tensor,
+                                 weights=weights))
 
     expected_loss = (3 * 5 * 4) / 2.0
     with self.test_session() as sess:
@@ -50,7 +51,7 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
     prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
     target_tensor = tf.zeros([batch_size, num_anchors, code_size])
     weights = tf.ones([batch_size, num_anchors])
-    loss_op = losses.WeightedL2LocalizationLoss(anchorwise_output=True)
+    loss_op = losses.WeightedL2LocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
 
     expected_loss = np.ones((batch_size, num_anchors)) * 2
@@ -58,22 +59,6 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
       loss_output = sess.run(loss)
       self.assertAllClose(loss_output, expected_loss)
 
-  def testReturnsCorrectLossSum(self):
-    batch_size = 3
-    num_anchors = 16
-    code_size = 4
-    prediction_tensor = tf.ones([batch_size, num_anchors, code_size])
-    target_tensor = tf.zeros([batch_size, num_anchors, code_size])
-    weights = tf.ones([batch_size, num_anchors])
-    loss_op = losses.WeightedL2LocalizationLoss(anchorwise_output=False)
-    loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
-    expected_loss = tf.nn.l2_loss(prediction_tensor - target_tensor)
-    with self.test_session() as sess:
-      loss_output = sess.run(loss)
-      expected_loss_output = sess.run(expected_loss)
-      self.assertAllClose(loss_output, expected_loss_output)
-
   def testReturnsCorrectNanLoss(self):
     batch_size = 3
     num_anchors = 10
@@ -87,6 +72,7 @@ class WeightedL2LocalizationLossTest(tf.test.TestCase):
     loss_op = losses.WeightedL2LocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights,
                    ignore_nan_targets=True)
+    loss = tf.reduce_sum(loss)
 
     expected_loss = (3 * 5 * 4) / 2.0
     with self.test_session() as sess:
@@ -111,6 +97,7 @@ class WeightedSmoothL1LocalizationLossTest(tf.test.TestCase):
                            [0, 3, 0]], tf.float32)
     loss_op = losses.WeightedSmoothL1LocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
 
     exp_loss = 7.695
     with self.test_session() as sess:
@@ -130,6 +117,7 @@ class WeightedIOULocalizationLossTest(tf.test.TestCase):
     weights = [[1.0, .5, 2.0]]
     loss_op = losses.WeightedIOULocalizationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
     exp_loss = 2.0
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -159,6 +147,7 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
 
     exp_loss = -2 * math.log(.5)
     with self.test_session() as sess:
@@ -184,8 +173,9 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 0]], tf.float32)
-    loss_op = losses.WeightedSigmoidClassificationLoss(True)
+    loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss, axis=2)
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
@@ -214,9 +204,10 @@ class WeightedSigmoidClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     # Ignores the last class.
     class_indices = tf.constant([0, 1, 2], tf.int32)
-    loss_op = losses.WeightedSigmoidClassificationLoss(True)
+    loss_op = losses.WeightedSigmoidClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights,
                    class_indices=class_indices)
+    loss = tf.reduce_sum(loss, axis=2)
 
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
@@ -245,14 +236,13 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [0],
                                   [0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights), axis=2)
+    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                 target_tensor,
+                                                 weights=weights), axis=2)
 
     with self.test_session() as sess:
       sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
@@ -272,14 +262,13 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [0],
                                   [0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights), axis=2)
+    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                 target_tensor,
+                                                 weights=weights), axis=2)
 
     with self.test_session() as sess:
       sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
@@ -299,14 +288,13 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [0],
                                   [0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=False, gamma=2.0, alpha=None)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=False)
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=None)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights))
+    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                 target_tensor,
+                                                 weights=weights))
 
     with self.test_session() as sess:
       sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
@@ -326,14 +314,13 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [0],
                                   [0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, gamma=2.0, alpha=1.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=1.0)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights), axis=2)
+    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                 target_tensor,
+                                                 weights=weights), axis=2)
 
     with self.test_session() as sess:
       sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
@@ -355,14 +342,13 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [0],
                                   [0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, gamma=2.0, alpha=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
-    sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
-                                   weights=weights)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(gamma=2.0, alpha=0.0)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights), axis=2)
+    sigmoid_loss = tf.reduce_sum(sigmoid_loss_op(prediction_tensor,
+                                                 target_tensor,
+                                                 weights=weights), axis=2)
 
     with self.test_session() as sess:
       sigmoid_loss, focal_loss = sess.run([sigmoid_loss, focal_loss])
@@ -391,10 +377,8 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 0]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, alpha=0.5, gamma=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.5, gamma=0.0)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
     focal_loss = focal_loss_op(prediction_tensor, target_tensor,
                                weights=weights)
     sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
@@ -423,10 +407,8 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 0]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=True, alpha=None, gamma=0.0)
-    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=None, gamma=0.0)
+    sigmoid_loss_op = losses.WeightedSigmoidClassificationLoss()
     focal_loss = focal_loss_op(prediction_tensor, target_tensor,
                                weights=weights)
     sigmoid_loss = sigmoid_loss_op(prediction_tensor, target_tensor,
@@ -456,11 +438,10 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=False, alpha=1.0, gamma=0.0)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=1.0, gamma=0.0)
 
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights))
     with self.test_session() as sess:
       focal_loss = sess.run(focal_loss)
       self.assertAllClose(
@@ -489,11 +470,10 @@ class SigmoidFocalClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 1]], tf.float32)
-    focal_loss_op = losses.SigmoidFocalClassificationLoss(
-        anchorwise_output=False, alpha=0.75, gamma=0.0)
+    focal_loss_op = losses.SigmoidFocalClassificationLoss(alpha=0.75, gamma=0.0)
 
-    focal_loss = focal_loss_op(prediction_tensor, target_tensor,
-                               weights=weights)
+    focal_loss = tf.reduce_sum(focal_loss_op(prediction_tensor, target_tensor,
+                                             weights=weights))
     with self.test_session() as sess:
       focal_loss = sess.run(focal_loss)
       self.assertAllClose(
@@ -528,6 +508,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     loss_op = losses.WeightedSoftmaxClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
 
     exp_loss = - 1.5 * math.log(.5)
     with self.test_session() as sess:
@@ -553,7 +534,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, .5, 1],
                            [1, 1, 1, 0]], tf.float32)
-    loss_op = losses.WeightedSoftmaxClassificationLoss(True)
+    loss_op = losses.WeightedSoftmaxClassificationLoss()
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
 
     exp_loss = np.matrix([[0, 0, - 0.5 * math.log(.5), 0],
@@ -564,7 +545,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
 
   def testReturnsCorrectAnchorWiseLossWithHighLogitScaleSetting(self):
     """At very high logit_scale, all predictions will be ~0.33."""
-    # TODO(yonib): Also test logit_scale with anchorwise=False.
+    # TODO: Also test logit_scale with anchorwise=False.
     logit_scale = 10e16
     prediction_tensor = tf.constant([[[-100, 100, -100],
                                       [100, -100, -100],
@@ -584,8 +565,7 @@ class WeightedSoftmaxClassificationLossTest(tf.test.TestCase):
                                   [1, 0, 0]]], tf.float32)
     weights = tf.constant([[1, 1, 1, 1],
                            [1, 1, 1, 1]], tf.float32)
-    loss_op = losses.WeightedSoftmaxClassificationLoss(
-        anchorwise_output=True, logit_scale=logit_scale)
+    loss_op = losses.WeightedSoftmaxClassificationLoss(logit_scale=logit_scale)
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
 
     uniform_distribution_loss = - math.log(.33333333333)
@@ -621,6 +601,7 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
         alpha, bootstrap_type='soft')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
     exp_loss = -math.log(.5)
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -649,6 +630,7 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
         alpha, bootstrap_type='hard')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
+    loss = tf.reduce_sum(loss)
     exp_loss = -math.log(.5)
     with self.test_session() as sess:
       loss_output = sess.run(loss)
@@ -675,9 +657,9 @@ class BootstrappedSigmoidClassificationLossTest(tf.test.TestCase):
                            [1, 1, 1, 0]], tf.float32)
     alpha = tf.constant(.5, tf.float32)
     loss_op = losses.BootstrappedSigmoidClassificationLoss(
-        alpha, bootstrap_type='hard', anchorwise_output=True)
+        alpha, bootstrap_type='hard')
     loss = loss_op(prediction_tensor, target_tensor, weights=weights)
-
+    loss = tf.reduce_sum(loss, axis=2)
     exp_loss = np.matrix([[0, 0, -math.log(.5), 0],
                           [-math.log(.5), 0, 0, 0]])
     with self.test_session() as sess:
diff --git a/research/object_detection/core/matcher.py b/research/object_detection/core/matcher.py
index 8ada42ee..ddded899 100644
--- a/research/object_detection/core/matcher.py
+++ b/research/object_detection/core/matcher.py
@@ -168,6 +168,34 @@ class Match(object):
   def _reshape_and_cast(self, t):
     return tf.cast(tf.reshape(t, [-1]), tf.int32)
 
+  def gather_based_on_match(self, input_tensor, unmatched_value,
+                            ignored_value):
+    """Gathers elements from `input_tensor` based on match results.
+
+    For columns that are matched to a row, gathered_tensor[col] is set to
+    input_tensor[match_results[col]]. For columns that are unmatched,
+    gathered_tensor[col] is set to unmatched_value. Finally, for columns that
+    are ignored gathered_tensor[col] is set to ignored_value.
+
+    Note that the input_tensor.shape[1:] must match with unmatched_value.shape
+    and ignored_value.shape
+
+    Args:
+      input_tensor: Tensor to gather values from.
+      unmatched_value: Constant tensor value for unmatched columns.
+      ignored_value: Constant tensor value for ignored columns.
+
+    Returns:
+      gathered_tensor: A tensor containing values gathered from input_tensor.
+        The shape of the gathered tensor is [match_results.shape[0]] +
+        input_tensor.shape[1:].
+    """
+    input_tensor = tf.concat([tf.stack([ignored_value, unmatched_value]),
+                              input_tensor], axis=0)
+    gather_indices = tf.maximum(self.match_results + 2, 0)
+    gathered_tensor = tf.gather(input_tensor, gather_indices)
+    return gathered_tensor
+
 
 class Matcher(object):
   """Abstract base class for matcher.
@@ -195,7 +223,7 @@ class Matcher(object):
 
   @abstractmethod
   def _match(self, similarity_matrix, **params):
-    """Method to be overriden by implementations.
+    """Method to be overridden by implementations.
 
     Args:
       similarity_matrix: Float tensor of shape [N, M] with pairwise similarity
diff --git a/research/object_detection/core/matcher_test.py b/research/object_detection/core/matcher_test.py
index 7054015f..db6158cd 100644
--- a/research/object_detection/core/matcher_test.py
+++ b/research/object_detection/core/matcher_test.py
@@ -20,7 +20,7 @@ import tensorflow as tf
 from object_detection.core import matcher
 
 
-class AnchorMatcherTest(tf.test.TestCase):
+class MatchTest(tf.test.TestCase):
 
   def test_get_correct_matched_columnIndices(self):
     match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
@@ -145,6 +145,32 @@ class AnchorMatcherTest(tf.test.TestCase):
       self.assertAllEqual(all_indices_sorted,
                           np.arange(num_matches, dtype=np.int32))
 
+  def test_scalar_gather_based_on_match(self):
+    match_results = tf.constant([3, 1, -1, 0, -1, 5, -2])
+    input_tensor = tf.constant([0, 1, 2, 3, 4, 5, 6, 7], dtype=tf.float32)
+    expected_gathered_tensor = [3, 1, 100, 0, 100, 5, 200]
+    match = matcher.Match(match_results)
+    gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                  unmatched_value=100.,
+                                                  ignored_value=200.)
+    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    with self.test_session():
+      gathered_tensor_out = gathered_tensor.eval()
+    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
+
+  def test_multidimensional_gather_based_on_match(self):
+    match_results = tf.constant([1, -1, -2])
+    input_tensor = tf.constant([[0, 0.5, 0, 0.5], [0, 0, 0.5, 0.5]],
+                               dtype=tf.float32)
+    expected_gathered_tensor = [[0, 0, 0.5, 0.5], [0, 0, 0, 0], [0, 0, 0, 0]]
+    match = matcher.Match(match_results)
+    gathered_tensor = match.gather_based_on_match(input_tensor,
+                                                  unmatched_value=tf.zeros(4),
+                                                  ignored_value=tf.zeros(4))
+    self.assertEquals(gathered_tensor.dtype, tf.float32)
+    with self.test_session():
+      gathered_tensor_out = gathered_tensor.eval()
+    self.assertAllEqual(expected_gathered_tensor, gathered_tensor_out)
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/model.py b/research/object_detection/core/model.py
index 08843944..7cbaaa46 100644
--- a/research/object_detection/core/model.py
+++ b/research/object_detection/core/model.py
@@ -39,6 +39,17 @@ resize/reshaping necessary (see docstring for the preprocess function).
 Output classes are always integers in the range [0, num_classes).  Any mapping
 of these integers to semantic labels is to be handled outside of this class.
 
+Images are resized in the `preprocess` method. All of `preprocess`, `predict`,
+and `postprocess` should be stateless.
+
+The `preprocess` method runs `image_resizer_fn` that returns resized_images and
+`true_image_shapes`. Since `image_resizer_fn` can pad the images with zeros,
+true_image_shapes indicate the slices that contain the image without padding.
+This is useful for padding images to be a fixed size for batching.
+
+The `postprocess` method uses the true image shapes to clip predictions that lie
+outside of images.
+
 By default, DetectionModels produce bounding box detections; However, we support
 a handful of auxiliary annotations associated with each bounding box, namely,
 instance masks and keypoints.
@@ -106,12 +117,12 @@ class DetectionModel(object):
 
     This function is responsible for any scaling/shifting of input values that
     is necessary prior to running the detector on an input image.
-    It is also responsible for any resizing that might be necessary as images
-    are assumed to arrive in arbitrary sizes.  While this function could
-    conceivably be part of the predict method (below), it is often convenient
-    to keep these separate --- for example, we may want to preprocess on one
-    device, place onto a queue, and let another device (e.g., the GPU) handle
-    prediction.
+    It is also responsible for any resizing, padding that might be necessary
+    as images are assumed to arrive in arbitrary sizes.  While this function
+    could conceivably be part of the predict method (below), it is often
+    convenient to keep these separate --- for example, we may want to preprocess
+    on one device, place onto a queue, and let another device (e.g., the GPU)
+    handle prediction.
 
     A few important notes about the preprocess function:
     + We assume that this operation does not have any trainable variables nor
@@ -134,11 +145,15 @@ class DetectionModel(object):
     Returns:
       preprocessed_inputs: a [batch, height_out, width_out, channels] float32
         tensor representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
     """
     pass
 
   @abstractmethod
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
     """Predict prediction tensors from inputs tensor.
 
     Outputs of this function can be passed to loss or postprocess functions.
@@ -146,6 +161,10 @@ class DetectionModel(object):
     Args:
       preprocessed_inputs: a [batch, height, width, channels] float32 tensor
         representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding prediction tensors to be
@@ -154,7 +173,7 @@ class DetectionModel(object):
     pass
 
   @abstractmethod
-  def postprocess(self, prediction_dict, **params):
+  def postprocess(self, prediction_dict, true_image_shapes, **params):
     """Convert predicted output tensors to final detections.
 
     Outputs adhere to the following conventions:
@@ -172,6 +191,10 @@ class DetectionModel(object):
 
     Args:
       prediction_dict: a dictionary holding prediction tensors.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
       **params: Additional keyword arguments for specific implementations of
         DetectionModel.
 
@@ -190,7 +213,7 @@ class DetectionModel(object):
     pass
 
   @abstractmethod
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
     """Compute scalar loss tensors with respect to provided groundtruth.
 
     Calling this function requires that groundtruth tensors have been
@@ -198,6 +221,10 @@ class DetectionModel(object):
 
     Args:
       prediction_dict: a dictionary holding predicted tensors
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       a dictionary mapping strings (loss names) to scalar tensors representing
diff --git a/research/object_detection/core/post_processing.py b/research/object_detection/core/post_processing.py
index d34f0683..c65fd48f 100644
--- a/research/object_detection/core/post_processing.py
+++ b/research/object_detection/core/post_processing.py
@@ -20,6 +20,7 @@ import tensorflow as tf
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import standard_fields as fields
+from object_detection.utils import shape_utils
 
 
 def multiclass_non_max_suppression(boxes,
@@ -31,6 +32,7 @@ def multiclass_non_max_suppression(boxes,
                                    clip_window=None,
                                    change_coordinate_frame=False,
                                    masks=None,
+                                   boundaries=None,
                                    additional_fields=None,
                                    scope=None):
   """Multi-class version of non maximum suppression.
@@ -66,6 +68,9 @@ def multiclass_non_max_suppression(boxes,
     masks: (optional) a [k, q, mask_height, mask_width] float32 tensor
       containing box masks. `q` can be either number of classes or 1 depending
       on whether a separate mask is predicted per class.
+    boundaries: (optional) a [k, q, boundary_height, boundary_width] float32
+      tensor containing box boundaries. `q` can be either number of classes or 1
+      depending on whether a separate boundary is predicted per class.
     additional_fields: (optional) If not None, a dictionary that maps keys to
       tensors whose first dimensions are all of size `k`. After non-maximum
       suppression, all tensors corresponding to the selected boxes will be
@@ -114,6 +119,8 @@ def multiclass_non_max_suppression(boxes,
     per_class_boxes_list = tf.unstack(boxes, axis=1)
     if masks is not None:
       per_class_masks_list = tf.unstack(masks, axis=1)
+    if boundaries is not None:
+      per_class_boundaries_list = tf.unstack(boundaries, axis=1)
     boxes_ids = (range(num_classes) if len(per_class_boxes_list) > 1
                  else [0] * num_classes)
     for class_idx, boxes_idx in zip(range(num_classes), boxes_ids):
@@ -128,6 +135,10 @@ def multiclass_non_max_suppression(boxes,
         per_class_masks = per_class_masks_list[boxes_idx]
         boxlist_and_class_scores.add_field(fields.BoxListFields.masks,
                                            per_class_masks)
+      if boundaries is not None:
+        per_class_boundaries = per_class_boundaries_list[boxes_idx]
+        boxlist_and_class_scores.add_field(fields.BoxListFields.boundaries,
+                                           per_class_boundaries)
       if additional_fields is not None:
         for key, tensor in additional_fields.items():
           boxlist_and_class_scores.add_field(key, tensor)
@@ -194,9 +205,12 @@ def batch_multiclass_non_max_suppression(boxes,
     max_size_per_class: maximum number of retained boxes per class.
     max_total_size: maximum number of boxes retained over all classes. By
       default returns all boxes retained after capping boxes per class.
-    clip_window: A float32 tensor of the form [y_min, x_min, y_max, x_max]
-      representing the window to clip boxes to before performing non-max
-      suppression.
+    clip_window: A float32 tensor of shape [batch_size, 4]  where each entry is
+      of the form [y_min, x_min, y_max, x_max] representing the window to clip
+      boxes to before performing non-max suppression. This argument can also be
+      a tensor of shape [4] in which case, the same clip window is applied to
+      all images in the batch. If clip_widow is None, all boxes are used to
+      perform non-max suppression.
     change_coordinate_frame: Whether to normalize coordinates after clipping
       relative to clip_window (this can only be set to True if a clip_window
       is provided)
@@ -242,7 +256,9 @@ def batch_multiclass_non_max_suppression(boxes,
   if q != 1 and q != num_classes:
     raise ValueError('third dimension of boxes must be either 1 or equal '
                      'to the third dimension of scores')
-
+  if change_coordinate_frame and clip_window is None:
+    raise ValueError('if change_coordinate_frame is True, then a clip_window'
+                     'must be specified.')
   original_masks = masks
   original_additional_fields = additional_fields
   with tf.name_scope(scope, 'BatchMultiClassNonMaxSuppression'):
@@ -266,6 +282,16 @@ def batch_multiclass_non_max_suppression(boxes,
       masks_shape = tf.stack([batch_size, num_anchors, 1, 0, 0])
       masks = tf.zeros(masks_shape)
 
+    if clip_window is None:
+      clip_window = tf.stack([
+          tf.reduce_min(boxes[:, :, :, 0]),
+          tf.reduce_min(boxes[:, :, :, 1]),
+          tf.reduce_max(boxes[:, :, :, 2]),
+          tf.reduce_max(boxes[:, :, :, 3])
+      ])
+    if clip_window.shape.ndims == 1:
+      clip_window = tf.tile(tf.expand_dims(clip_window, 0), [batch_size, 1])
+
     if additional_fields is None:
       additional_fields = {}
 
@@ -283,6 +309,9 @@ def batch_multiclass_non_max_suppression(boxes,
           per_image_masks - A [num_anchors, q, mask_height, mask_width] float32
             tensor containing box masks. `q` can be either number of classes
             or 1 depending on whether a separate mask is predicted per class.
+          per_image_clip_window - A 1D float32 tensor of the form
+            [ymin, xmin, ymax, xmax] representing the window to clip the boxes
+            to.
           per_image_additional_fields - (optional) A variable number of float32
             tensors each with size [num_anchors, ...].
           per_image_num_valid_boxes - A tensor of type `int32`. A 1-D tensor of
@@ -311,9 +340,10 @@ def batch_multiclass_non_max_suppression(boxes,
       per_image_boxes = args[0]
       per_image_scores = args[1]
       per_image_masks = args[2]
+      per_image_clip_window = args[3]
       per_image_additional_fields = {
           key: value
-          for key, value in zip(additional_fields, args[3:-1])
+          for key, value in zip(additional_fields, args[4:-1])
       }
       per_image_num_valid_boxes = args[-1]
       per_image_boxes = tf.reshape(
@@ -345,7 +375,7 @@ def batch_multiclass_non_max_suppression(boxes,
           iou_thresh,
           max_size_per_class,
           max_total_size,
-          clip_window=clip_window,
+          clip_window=per_image_clip_window,
           change_coordinate_frame=change_coordinate_frame,
           masks=per_image_masks,
           additional_fields=per_image_additional_fields)
@@ -367,10 +397,10 @@ def batch_multiclass_non_max_suppression(boxes,
       num_additional_fields = len(additional_fields)
     num_nmsed_outputs = 4 + num_additional_fields
 
-    batch_outputs = tf.map_fn(
+    batch_outputs = shape_utils.static_or_dynamic_map_fn(
         _single_image_nms_fn,
-        elems=([boxes, scores, masks] + list(additional_fields.values()) +
-               [num_valid_boxes]),
+        elems=([boxes, scores, masks, clip_window] +
+               list(additional_fields.values()) + [num_valid_boxes]),
         dtype=(num_nmsed_outputs * [tf.float32] + [tf.int32]),
         parallel_iterations=parallel_iterations)
 
diff --git a/research/object_detection/core/post_processing_test.py b/research/object_detection/core/post_processing_test.py
index 542f8e18..96741399 100644
--- a/research/object_detection/core/post_processing_test.py
+++ b/research/object_detection/core/post_processing_test.py
@@ -571,6 +571,125 @@ class MulticlassNonMaxSuppressionTest(tf.test.TestCase):
       self.assertAllClose(nmsed_classes, exp_nms_classes)
       self.assertAllClose(num_detections, [2, 3])
 
+  def test_batch_multiclass_nms_with_per_batch_clip_window(self):
+    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],
+                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
+                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
+                          [[0, 10, 1, 11], [0, 10, 1, 11]]],
+                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
+                          [[0, 100, 1, 101], [0, 100, 1, 101]],
+                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
+                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],
+                        tf.float32)
+    scores = tf.constant([[[.9, 0.01], [.75, 0.05],
+                           [.6, 0.01], [.95, 0]],
+                          [[.5, 0.01], [.3, 0.01],
+                           [.01, .85], [.01, .5]]])
+    clip_window = tf.constant([0., 0., 200., 200.])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = np.array([[[0, 10, 1, 11],
+                                 [0, 0, 1, 1],
+                                 [0, 0, 0, 0],
+                                 [0, 0, 0, 0]],
+                                [[0, 10.1, 1, 11.1],
+                                 [0, 100, 1, 101],
+                                 [0, 0, 0, 0],
+                                 [0, 0, 0, 0]]])
+    exp_nms_scores = np.array([[.95, .9, 0, 0],
+                               [.5, .3, 0, 0]])
+    exp_nms_classes = np.array([[0, 0, 0, 0],
+                                [0, 0, 0, 0]])
+
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+     nmsed_additional_fields, num_detections
+    ) = post_processing.batch_multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh,
+        max_size_per_class=max_output_size, max_total_size=max_output_size,
+        clip_window=clip_window)
+
+    self.assertIsNone(nmsed_masks)
+    self.assertIsNone(nmsed_additional_fields)
+    # Check static shapes
+    self.assertAllEqual(nmsed_boxes.shape.as_list(),
+                        exp_nms_corners.shape)
+    self.assertAllEqual(nmsed_scores.shape.as_list(),
+                        exp_nms_scores.shape)
+    self.assertAllEqual(nmsed_classes.shape.as_list(),
+                        exp_nms_classes.shape)
+    self.assertEqual(num_detections.shape.as_list(), [2])
+
+    with self.test_session() as sess:
+      (nmsed_boxes, nmsed_scores, nmsed_classes,
+       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,
+                                   num_detections])
+      self.assertAllClose(nmsed_boxes, exp_nms_corners)
+      self.assertAllClose(nmsed_scores, exp_nms_scores)
+      self.assertAllClose(nmsed_classes, exp_nms_classes)
+      self.assertAllClose(num_detections, [2, 2])
+
+  def test_batch_multiclass_nms_with_per_image_clip_window(self):
+    boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],
+                          [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
+                          [[0, -0.1, 1, 0.9], [0, -0.1, 1, 0.9]],
+                          [[0, 10, 1, 11], [0, 10, 1, 11]]],
+                         [[[0, 10.1, 1, 11.1], [0, 10.1, 1, 11.1]],
+                          [[0, 100, 1, 101], [0, 100, 1, 101]],
+                          [[0, 1000, 1, 1002], [0, 999, 2, 1004]],
+                          [[0, 1000, 1, 1002.1], [0, 999, 2, 1002.7]]]],
+                        tf.float32)
+    scores = tf.constant([[[.9, 0.01], [.75, 0.05],
+                           [.6, 0.01], [.95, 0]],
+                          [[.5, 0.01], [.3, 0.01],
+                           [.01, .85], [.01, .5]]])
+    clip_window = tf.constant([[0., 0., 5., 5.],
+                               [0., 0., 200., 200.]])
+    score_thresh = 0.1
+    iou_thresh = .5
+    max_output_size = 4
+
+    exp_nms_corners = np.array([[[0, 0, 1, 1],
+                                 [0, 0, 0, 0],
+                                 [0, 0, 0, 0],
+                                 [0, 0, 0, 0]],
+                                [[0, 10.1, 1, 11.1],
+                                 [0, 100, 1, 101],
+                                 [0, 0, 0, 0],
+                                 [0, 0, 0, 0]]])
+    exp_nms_scores = np.array([[.9, 0., 0., 0.],
+                               [.5, .3, 0, 0]])
+    exp_nms_classes = np.array([[0, 0, 0, 0],
+                                [0, 0, 0, 0]])
+
+    (nmsed_boxes, nmsed_scores, nmsed_classes, nmsed_masks,
+     nmsed_additional_fields, num_detections
+    ) = post_processing.batch_multiclass_non_max_suppression(
+        boxes, scores, score_thresh, iou_thresh,
+        max_size_per_class=max_output_size, max_total_size=max_output_size,
+        clip_window=clip_window)
+
+    self.assertIsNone(nmsed_masks)
+    self.assertIsNone(nmsed_additional_fields)
+    # Check static shapes
+    self.assertAllEqual(nmsed_boxes.shape.as_list(),
+                        exp_nms_corners.shape)
+    self.assertAllEqual(nmsed_scores.shape.as_list(),
+                        exp_nms_scores.shape)
+    self.assertAllEqual(nmsed_classes.shape.as_list(),
+                        exp_nms_classes.shape)
+    self.assertEqual(num_detections.shape.as_list(), [2])
+
+    with self.test_session() as sess:
+      (nmsed_boxes, nmsed_scores, nmsed_classes,
+       num_detections) = sess.run([nmsed_boxes, nmsed_scores, nmsed_classes,
+                                   num_detections])
+      self.assertAllClose(nmsed_boxes, exp_nms_corners)
+      self.assertAllClose(nmsed_scores, exp_nms_scores)
+      self.assertAllClose(nmsed_classes, exp_nms_classes)
+      self.assertAllClose(num_detections, [1, 2])
+
   def test_batch_multiclass_nms_with_masks(self):
     boxes = tf.constant([[[[0, 0, 1, 1], [0, 0, 4, 5]],
                           [[0, 0.1, 1, 1.1], [0, 0.1, 2, 1.1]],
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 33435f7b..0b671b3d 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -51,6 +51,7 @@ from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import keypoint_ops
 from object_detection.core import standard_fields as fields
+from object_detection.utils import shape_utils
 
 
 def _apply_with_random_selector(x, func, num_cases):
@@ -1647,6 +1648,7 @@ def _compute_new_static_size(image, min_dimension, max_dimension):
   image_shape = image.get_shape().as_list()
   orig_height = image_shape[0]
   orig_width = image_shape[1]
+  num_channels = image_shape[2]
   orig_min_dim = min(orig_height, orig_width)
   # Calculates the larger of the possible sizes
   large_scale_factor = min_dimension / float(orig_min_dim)
@@ -1674,7 +1676,7 @@ def _compute_new_static_size(image, min_dimension, max_dimension):
       new_size = small_size
   else:
     new_size = large_size
-  return tf.constant(new_size)
+  return tf.constant(new_size + [num_channels])
 
 
 def _compute_new_dynamic_size(image, min_dimension, max_dimension):
@@ -1682,6 +1684,7 @@ def _compute_new_dynamic_size(image, min_dimension, max_dimension):
   image_shape = tf.shape(image)
   orig_height = tf.to_float(image_shape[0])
   orig_width = tf.to_float(image_shape[1])
+  num_channels = image_shape[2]
   orig_min_dim = tf.minimum(orig_height, orig_width)
   # Calculates the larger of the possible sizes
   min_dimension = tf.constant(min_dimension, dtype=tf.float32)
@@ -1711,7 +1714,7 @@ def _compute_new_dynamic_size(image, min_dimension, max_dimension):
         lambda: small_size, lambda: large_size)
   else:
     new_size = large_size
-  return new_size
+  return tf.stack(tf.unstack(new_size) + [num_channels])
 
 
 def resize_to_range(image,
@@ -1719,7 +1722,8 @@ def resize_to_range(image,
                     min_dimension=None,
                     max_dimension=None,
                     method=tf.image.ResizeMethod.BILINEAR,
-                    align_corners=False):
+                    align_corners=False,
+                    pad_to_max_dimension=False):
   """Resizes an image so its dimensions are within the provided value.
 
   The output size can be described by two cases:
@@ -1740,15 +1744,22 @@ def resize_to_range(image,
             BILINEAR.
     align_corners: bool. If true, exactly align all 4 corners of the input
                    and output. Defaults to False.
+    pad_to_max_dimension: Whether to resize the image and pad it with zeros
+      so the resulting image is of the spatial size
+      [max_dimension, max_dimension]. If masks are included they are padded
+      similarly.
 
   Returns:
-    A 3D tensor of shape [new_height, new_width, channels],
-    where the image has been resized (with bilinear interpolation) so that
-    min(new_height, new_width) == min_dimension or
-    max(new_height, new_width) == max_dimension.
-
-    If masks is not None, also outputs masks:
-    A 3D tensor of shape [num_instances, new_height, new_width]
+    Note that the position of the resized_image_shape changes based on whether
+    masks are present.
+    resized_image: A 3D tensor of shape [new_height, new_width, channels],
+      where the image has been resized (with bilinear interpolation) so that
+      min(new_height, new_width) == min_dimension or
+      max(new_height, new_width) == max_dimension.
+    resized_masks: If masks is not None, also outputs masks. A 3D tensor of
+      shape [num_instances, new_height, new_width].
+    resized_image_shape: A 1D tensor of shape [3] containing shape of the
+      resized image.
 
   Raises:
     ValueError: if the image is not a 3D tensor.
@@ -1762,16 +1773,27 @@ def resize_to_range(image,
     else:
       new_size = _compute_new_dynamic_size(image, min_dimension, max_dimension)
     new_image = tf.image.resize_images(
-        image, new_size, method=method, align_corners=align_corners)
+        image, new_size[:-1], method=method, align_corners=align_corners)
+
+    if pad_to_max_dimension:
+      new_image = tf.image.pad_to_bounding_box(
+          new_image, 0, 0, max_dimension, max_dimension)
 
-    result = new_image
+    result = [new_image]
     if masks is not None:
       new_masks = tf.expand_dims(masks, 3)
-      new_masks = tf.image.resize_nearest_neighbor(
-          new_masks, new_size, align_corners=align_corners)
+      new_masks = tf.image.resize_images(
+          new_masks,
+          new_size[:-1],
+          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
+          align_corners=align_corners)
       new_masks = tf.squeeze(new_masks, 3)
-      result = [new_image, new_masks]
+      if pad_to_max_dimension:
+        new_masks = tf.image.pad_to_bounding_box(
+            new_masks, 0, 0, max_dimension, max_dimension)
+      result.append(new_masks)
 
+    result.append(new_size)
     return result
 
 
@@ -1789,10 +1811,13 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
     min_dimension: minimum image dimension.
 
   Returns:
-    a tuple containing the following:
-      Resized image. A tensor of size [new_height, new_width, channels].
-      (optional) Resized masks. A tensor of
-        size [num_instances, new_height, new_width].
+    Note that the position of the resized_image_shape changes based on whether
+    masks are present.
+    resized_image: A tensor of size [new_height, new_width, channels].
+    resized_masks: If masks is not None, also outputs masks. A 3D tensor of
+      shape [num_instances, new_height, new_width]
+    resized_image_shape: A 1D tensor of shape [3] containing the shape of the
+      resized image.
 
   Raises:
     ValueError: if the image is not a 3D tensor.
@@ -1803,6 +1828,7 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
   with tf.name_scope('ResizeGivenMinDimension', values=[image, min_dimension]):
     image_height = tf.shape(image)[0]
     image_width = tf.shape(image)[1]
+    num_channels = tf.shape(image)[2]
     min_image_dimension = tf.minimum(image_height, image_width)
     min_target_dimension = tf.maximum(min_image_dimension, min_dimension)
     target_ratio = tf.to_float(min_target_dimension) / tf.to_float(
@@ -1813,13 +1839,16 @@ def resize_to_min_dimension(image, masks=None, min_dimension=600):
         tf.expand_dims(image, axis=0),
         size=[target_height, target_width],
         align_corners=True)
-    result = tf.squeeze(image, axis=0)
+    result = [tf.squeeze(image, axis=0)]
+
     if masks is not None:
       masks = tf.image.resize_nearest_neighbor(
           tf.expand_dims(masks, axis=3),
           size=[target_height, target_width],
           align_corners=True)
-      result = (result, tf.squeeze(masks, axis=3))
+      result.append(tf.squeeze(masks, axis=3))
+
+    result.append(tf.stack([target_height, target_width, num_channels]))
     return result
 
 
@@ -1854,6 +1883,8 @@ def scale_boxes_to_pixel_coordinates(image, boxes, keypoints=None):
   return tuple(result)
 
 
+# TODO: Investigate if instead the function should return None if
+# masks is None.
 # pylint: disable=g-doc-return-or-yield
 def resize_image(image,
                  masks=None,
@@ -1861,7 +1892,28 @@ def resize_image(image,
                  new_width=1024,
                  method=tf.image.ResizeMethod.BILINEAR,
                  align_corners=False):
-  """See `tf.image.resize_images` for detailed doc."""
+  """Resizes images to the given height and width.
+
+  Args:
+    image: A 3D tensor of shape [height, width, channels]
+    masks: (optional) rank 3 float32 tensor with shape
+           [num_instances, height, width] containing instance masks.
+    new_height: (optional) (scalar) desired height of the image.
+    new_width: (optional) (scalar) desired width of the image.
+    method: (optional) interpolation method used in resizing. Defaults to
+            BILINEAR.
+    align_corners: bool. If true, exactly align all 4 corners of the input
+                   and output. Defaults to False.
+
+  Returns:
+    Note that the position of the resized_image_shape changes based on whether
+    masks are present.
+    resized_image: A tensor of size [new_height, new_width, channels].
+    resized_masks: If masks is not None, also outputs masks. A 3D tensor of
+      shape [num_instances, new_height, new_width]
+    resized_image_shape: A 1D tensor of shape [3] containing the shape of the
+      resized image.
+  """
   with tf.name_scope(
       'ResizeImage',
       values=[image, new_height, new_width, method, align_corners]):
@@ -1869,7 +1921,8 @@ def resize_image(image,
         image, [new_height, new_width],
         method=method,
         align_corners=align_corners)
-    result = new_image
+    image_shape = shape_utils.combined_static_and_dynamic_shape(image)
+    result = [new_image]
     if masks is not None:
       num_instances = tf.shape(masks)[0]
       new_size = tf.constant([new_height, new_width], dtype=tf.int32)
@@ -1886,8 +1939,9 @@ def resize_image(image,
 
       masks = tf.cond(num_instances > 0, resize_masks_branch,
                       reshape_masks_branch)
-      result = [new_image, masks]
+      result.append(masks)
 
+    result.append(tf.stack([new_height, new_width, image_shape[2]]))
     return result
 
 
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index a163bea0..08a01bb1 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -1853,7 +1853,7 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_masks_shape_list):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_image(
+      out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
@@ -1880,7 +1880,7 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_masks_shape_list):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_image(
+      out_image, out_masks, _ = preprocessor.resize_image(
           in_image, in_masks, new_height=height, new_width=width)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
@@ -1900,7 +1900,7 @@ class PreprocessorTest(tf.test.TestCase):
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
       in_image = tf.random_uniform(in_shape)
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       self.assertAllEqual(out_image.get_shape().as_list(), expected_shape)
 
@@ -1913,7 +1913,7 @@ class PreprocessorTest(tf.test.TestCase):
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
       in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
       with self.test_session() as sess:
@@ -1938,7 +1938,7 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_masks_shape_list):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
       self.assertAllEqual(out_masks.get_shape().as_list(), expected_mask_shape)
       self.assertAllEqual(out_image.get_shape().as_list(), expected_image_shape)
@@ -1960,7 +1960,7 @@ class PreprocessorTest(tf.test.TestCase):
       in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
       in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
@@ -1991,7 +1991,7 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_masks_shape_list):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_to_range(
+      out_image, out_masks, _ = preprocessor.resize_to_range(
           in_image, in_masks, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
@@ -2016,7 +2016,7 @@ class PreprocessorTest(tf.test.TestCase):
 
     for in_shape, expected_shape in zip(in_shape_list, expected_shape_list):
       in_image = tf.random_uniform(in_shape)
-      out_image = preprocessor.resize_to_range(
+      out_image, _ = preprocessor.resize_to_range(
           in_image, min_dimension=min_dim, max_dimension=max_dim)
       out_image_shape = tf.shape(out_image)
 
@@ -2039,7 +2039,7 @@ class PreprocessorTest(tf.test.TestCase):
       in_image = tf.placeholder(tf.float32, shape=(None, None, 3))
       in_masks = tf.placeholder(tf.float32, shape=(None, None, None))
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_to_min_dimension(
+      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
@@ -2069,7 +2069,7 @@ class PreprocessorTest(tf.test.TestCase):
                                      expected_masks_shape_list):
       in_image = tf.random_uniform(in_image_shape)
       in_masks = tf.random_uniform(in_masks_shape)
-      out_image, out_masks = preprocessor.resize_to_min_dimension(
+      out_image, out_masks, _ = preprocessor.resize_to_min_dimension(
           in_image, in_masks, min_dimension=min_dim)
       out_image_shape = tf.shape(out_image)
       out_masks_shape = tf.shape(out_masks)
diff --git a/research/object_detection/core/standard_fields.py b/research/object_detection/core/standard_fields.py
index 7cbf5ee8..e8418a2a 100644
--- a/research/object_detection/core/standard_fields.py
+++ b/research/object_detection/core/standard_fields.py
@@ -57,6 +57,7 @@ class InputDataFields(object):
     groundtruth_keypoints: ground truth keypoints.
     groundtruth_keypoint_visibilities: ground truth keypoint visibilities.
     groundtruth_label_scores: groundtruth label scores.
+    groundtruth_weights: groundtruth weight factor for bounding boxes.
   """
   image = 'image'
   original_image = 'original_image'
@@ -79,10 +80,11 @@ class InputDataFields(object):
   groundtruth_keypoints = 'groundtruth_keypoints'
   groundtruth_keypoint_visibilities = 'groundtruth_keypoint_visibilities'
   groundtruth_label_scores = 'groundtruth_label_scores'
+  groundtruth_weights = 'groundtruth_weights'
 
 
 class DetectionResultFields(object):
-  """Naming converntions for storing the output of the detector.
+  """Naming conventions for storing the output of the detector.
 
   Attributes:
     source_id: source of the original image.
@@ -162,6 +164,7 @@ class TfExampleFields(object):
     object_is_crowd: [DEPRECATED, use object_group_of instead]
       is the object a single object or a crowd
     object_segment_area: the area of the segment.
+    object_weight: a weight factor for the object's bounding box.
     instance_masks: instance segmentation masks.
     instance_boundaries: instance boundaries.
     instance_classes: Classes for each instance segmentation mask.
@@ -194,6 +197,7 @@ class TfExampleFields(object):
   object_depiction = 'image/object/depiction'
   object_is_crowd = 'image/object/is_crowd'
   object_segment_area = 'image/object/segment/area'
+  object_weight = 'image/object/weight'
   instance_masks = 'image/segmentation/object'
   instance_boundaries = 'image/boundaries/object'
   instance_classes = 'image/segmentation/object/class'
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index d028dd59..f519ceb7 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -37,19 +37,19 @@ from object_detection.box_coders import faster_rcnn_box_coder
 from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_coder as bcoder
 from object_detection.core import box_list
-from object_detection.core import box_list_ops
 from object_detection.core import matcher as mat
 from object_detection.core import region_similarity_calculator as sim_calc
+from object_detection.core import standard_fields as fields
 from object_detection.matchers import argmax_matcher
 from object_detection.matchers import bipartite_matcher
+from object_detection.utils import shape_utils
 
 
 class TargetAssigner(object):
   """Target assigner to compute classification and regression targets."""
 
   def __init__(self, similarity_calc, matcher, box_coder,
-               positive_class_weight=1.0, negative_class_weight=1.0,
-               unmatched_cls_target=None):
+               negative_class_weight=1.0, unmatched_cls_target=None):
     """Construct Object Detection Target Assigner.
 
     Args:
@@ -58,10 +58,8 @@ class TargetAssigner(object):
         anchors.
       box_coder: an object_detection.core.BoxCoder used to encode matching
         groundtruth boxes with respect to anchors.
-      positive_class_weight: classification weight to be associated to positive
-        anchors (default: 1.0)
       negative_class_weight: classification weight to be associated to negative
-        anchors (default: 1.0)
+        anchors (default: 1.0). The weight must be in [0., 1.].
       unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]
         which is consistent with the classification target for each
         anchor (and can be empty for scalar targets).  This shape must thus be
@@ -82,7 +80,6 @@ class TargetAssigner(object):
     self._similarity_calc = similarity_calc
     self._matcher = matcher
     self._box_coder = box_coder
-    self._positive_class_weight = positive_class_weight
     self._negative_class_weight = negative_class_weight
     if unmatched_cls_target is None:
       self._unmatched_cls_target = tf.constant([0], tf.float32)
@@ -94,7 +91,7 @@ class TargetAssigner(object):
     return self._box_coder
 
   def assign(self, anchors, groundtruth_boxes, groundtruth_labels=None,
-             **params):
+             groundtruth_weights=None, **params):
     """Assign classification and regression targets to each anchor.
 
     For a given set of anchors and groundtruth detections, match anchors
@@ -113,6 +110,9 @@ class TargetAssigner(object):
         [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set
         to None, groundtruth_labels assumes a binary problem where all
         ground_truth boxes get a positive label (of 1).
+      groundtruth_weights: a float tensor of shape [M] indicating the weight to
+        assign to all anchors match to a particular groundtruth box. The weights
+        must be in [0., 1.]. If None, all weights are set to 1.
       **params: Additional keyword arguments for specific implementations of
               the Matcher.
 
@@ -140,14 +140,21 @@ class TargetAssigner(object):
       groundtruth_labels = tf.ones(tf.expand_dims(groundtruth_boxes.num_boxes(),
                                                   0))
       groundtruth_labels = tf.expand_dims(groundtruth_labels, -1)
-    unmatched_shape_assert = tf.assert_equal(
-        tf.shape(groundtruth_labels)[1:], tf.shape(self._unmatched_cls_target),
-        message='Unmatched class target shape incompatible '
-        'with groundtruth labels shape!')
-    labels_and_box_shapes_assert = tf.assert_equal(
-        tf.shape(groundtruth_labels)[0], groundtruth_boxes.num_boxes(),
-        message='Groundtruth boxes and labels have incompatible shapes!')
-
+    unmatched_shape_assert = shape_utils.assert_shape_equal(
+        shape_utils.combined_static_and_dynamic_shape(groundtruth_labels)[1:],
+        shape_utils.combined_static_and_dynamic_shape(
+            self._unmatched_cls_target))
+    labels_and_box_shapes_assert = shape_utils.assert_shape_equal(
+        shape_utils.combined_static_and_dynamic_shape(
+            groundtruth_labels)[:1],
+        shape_utils.combined_static_and_dynamic_shape(
+            groundtruth_boxes.get())[:1])
+
+    if groundtruth_weights is None:
+      num_gt_boxes = groundtruth_boxes.num_boxes_static()
+      if not num_gt_boxes:
+        num_gt_boxes = groundtruth_boxes.num_boxes()
+      groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)
     with tf.control_dependencies(
         [unmatched_shape_assert, labels_and_box_shapes_assert]):
       match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes,
@@ -158,16 +165,16 @@ class TargetAssigner(object):
                                                     match)
       cls_targets = self._create_classification_targets(groundtruth_labels,
                                                         match)
-      reg_weights = self._create_regression_weights(match)
-      cls_weights = self._create_classification_weights(
-          match, self._positive_class_weight, self._negative_class_weight)
+      reg_weights = self._create_regression_weights(match, groundtruth_weights)
+      cls_weights = self._create_classification_weights(match,
+                                                        groundtruth_weights)
 
-      num_anchors = anchors.num_boxes_static()
-      if num_anchors is not None:
-        reg_targets = self._reset_target_shape(reg_targets, num_anchors)
-        cls_targets = self._reset_target_shape(cls_targets, num_anchors)
-        reg_weights = self._reset_target_shape(reg_weights, num_anchors)
-        cls_weights = self._reset_target_shape(cls_weights, num_anchors)
+    num_anchors = anchors.num_boxes_static()
+    if num_anchors is not None:
+      reg_targets = self._reset_target_shape(reg_targets, num_anchors)
+      cls_targets = self._reset_target_shape(cls_targets, num_anchors)
+      reg_weights = self._reset_target_shape(reg_weights, num_anchors)
+      cls_weights = self._reset_target_shape(cls_weights, num_anchors)
 
     return cls_targets, cls_weights, reg_targets, reg_weights, match
 
@@ -198,23 +205,31 @@ class TargetAssigner(object):
     Returns:
       reg_targets: a float32 tensor with shape [N, box_code_dimension]
     """
-    matched_anchor_indices = match.matched_column_indices()
-    unmatched_ignored_anchor_indices = (match.
-                                        unmatched_or_ignored_column_indices())
-    matched_gt_indices = match.matched_row_indices()
-    matched_anchors = box_list_ops.gather(anchors,
-                                          matched_anchor_indices)
-    matched_gt_boxes = box_list_ops.gather(groundtruth_boxes,
-                                           matched_gt_indices)
-    matched_reg_targets = self._box_coder.encode(matched_gt_boxes,
-                                                 matched_anchors)
+    matched_gt_boxes = match.gather_based_on_match(
+        groundtruth_boxes.get(),
+        unmatched_value=tf.zeros(4),
+        ignored_value=tf.zeros(4))
+    matched_gt_boxlist = box_list.BoxList(matched_gt_boxes)
+    if groundtruth_boxes.has_field(fields.BoxListFields.keypoints):
+      groundtruth_keypoints = groundtruth_boxes.get_field(
+          fields.BoxListFields.keypoints)
+      matched_keypoints = match.gather_based_on_match(
+          groundtruth_keypoints,
+          unmatched_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]),
+          ignored_value=tf.zeros(groundtruth_keypoints.get_shape()[1:]))
+      matched_gt_boxlist.add_field(fields.BoxListFields.keypoints,
+                                   matched_keypoints)
+    matched_reg_targets = self._box_coder.encode(matched_gt_boxlist, anchors)
+    match_results_shape = shape_utils.combined_static_and_dynamic_shape(
+        match.match_results)
+
+    # Zero out the unmatched and ignored regression targets.
     unmatched_ignored_reg_targets = tf.tile(
-        self._default_regression_target(),
-        tf.stack([tf.size(unmatched_ignored_anchor_indices), 1]))
-    reg_targets = tf.dynamic_stitch(
-        [matched_anchor_indices, unmatched_ignored_anchor_indices],
-        [matched_reg_targets, unmatched_ignored_reg_targets])
-    # TODO: summarize the number of matches on average.
+        self._default_regression_target(), [match_results_shape[0], 1])
+    matched_anchors_mask = match.matched_column_indicator()
+    reg_targets = tf.where(matched_anchors_mask,
+                           matched_reg_targets,
+                           unmatched_ignored_reg_targets)
     return reg_targets
 
   def _default_regression_target(self):
@@ -245,27 +260,16 @@ class TargetAssigner(object):
         and groundtruth boxes.
 
     Returns:
-      cls_targets: a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k],
-        where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels
-        which has shape [num_gt_boxes, d_1, d_2, ... d_k].
+      a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the
+      subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has
+      shape [num_gt_boxes, d_1, d_2, ... d_k].
     """
-    matched_anchor_indices = match.matched_column_indices()
-    unmatched_ignored_anchor_indices = (match.
-                                        unmatched_or_ignored_column_indices())
-    matched_gt_indices = match.matched_row_indices()
-    matched_cls_targets = tf.gather(groundtruth_labels, matched_gt_indices)
-
-    ones = self._unmatched_cls_target.shape.ndims * [1]
-    unmatched_ignored_cls_targets = tf.tile(
-        tf.expand_dims(self._unmatched_cls_target, 0),
-        tf.stack([tf.size(unmatched_ignored_anchor_indices)] + ones))
-
-    cls_targets = tf.dynamic_stitch(
-        [matched_anchor_indices, unmatched_ignored_anchor_indices],
-        [matched_cls_targets, unmatched_ignored_cls_targets])
-    return cls_targets
-
-  def _create_regression_weights(self, match):
+    return match.gather_based_on_match(
+        groundtruth_labels,
+        unmatched_value=self._unmatched_cls_target,
+        ignored_value=self._unmatched_cls_target)
+
+  def _create_regression_weights(self, match, groundtruth_weights):
     """Set regression weight for each anchor.
 
     Only positive anchors are set to contribute to the regression loss, so this
@@ -275,18 +279,18 @@ class TargetAssigner(object):
     Args:
       match: a matcher.Match object that provides a matching between anchors
         and groundtruth boxes.
+      groundtruth_weights: a float tensor of shape [M] indicating the weight to
+        assign to all anchors match to a particular groundtruth box.
 
     Returns:
-      reg_weights: a float32 tensor with shape [num_anchors] representing
-        regression weights
+      a float32 tensor with shape [num_anchors] representing regression weights.
     """
-    reg_weights = tf.cast(match.matched_column_indicator(), tf.float32)
-    return reg_weights
+    return match.gather_based_on_match(
+        groundtruth_weights, ignored_value=0., unmatched_value=0.)
 
   def _create_classification_weights(self,
                                      match,
-                                     positive_class_weight=1.0,
-                                     negative_class_weight=1.0):
+                                     groundtruth_weights):
     """Create classification weights for each anchor.
 
     Positive (matched) anchors are associated with a weight of
@@ -299,25 +303,23 @@ class TargetAssigner(object):
     Args:
       match: a matcher.Match object that provides a matching between anchors
         and groundtruth boxes.
-      positive_class_weight: weight to be associated to positive anchors
-      negative_class_weight: weight to be associated to negative anchors
+      groundtruth_weights: a float tensor of shape [M] indicating the weight to
+        assign to all anchors match to a particular groundtruth box.
 
     Returns:
-      cls_weights: a float32 tensor with shape [num_anchors] representing
-        classification weights.
+      a float32 tensor with shape [num_anchors] representing classification
+      weights.
     """
-    matched_indicator = tf.cast(match.matched_column_indicator(), tf.float32)
-    ignore_indicator = tf.cast(match.ignored_column_indicator(), tf.float32)
-    unmatched_indicator = 1.0 - matched_indicator - ignore_indicator
-    cls_weights = (positive_class_weight * matched_indicator
-                   + negative_class_weight * unmatched_indicator)
-    return cls_weights
+    return match.gather_based_on_match(
+        groundtruth_weights,
+        ignored_value=0.,
+        unmatched_value=self._negative_class_weight)
 
   def get_box_coder(self):
     """Get BoxCoder of this TargetAssigner.
 
     Returns:
-      BoxCoder: BoxCoder object.
+      BoxCoder object.
     """
     return self._box_coder
 
@@ -325,7 +327,6 @@ class TargetAssigner(object):
 # TODO: This method pulls in all the implementation dependencies into
 # core. Therefore its best to have this factory method outside of core.
 def create_target_assigner(reference, stage=None,
-                           positive_class_weight=1.0,
                            negative_class_weight=1.0,
                            unmatched_cls_target=None):
   """Factory function for creating standard target assigners.
@@ -333,8 +334,6 @@ def create_target_assigner(reference, stage=None,
   Args:
     reference: string referencing the type of TargetAssigner.
     stage: string denoting stage: {proposal, detection}.
-    positive_class_weight: classification weight to be associated to positive
-      anchors (default: 1.0)
     negative_class_weight: classification weight to be associated to negative
       anchors (default: 1.0)
     unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]
@@ -383,7 +382,6 @@ def create_target_assigner(reference, stage=None,
     raise ValueError('No valid combination of reference and stage.')
 
   return TargetAssigner(similarity_calc, matcher, box_coder,
-                        positive_class_weight=positive_class_weight,
                         negative_class_weight=negative_class_weight,
                         unmatched_cls_target=unmatched_cls_target)
 
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index 5055e170..6de12ea1 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -17,135 +17,238 @@
 import numpy as np
 import tensorflow as tf
 
+from object_detection.box_coders import keypoint_box_coder
 from object_detection.box_coders import mean_stddev_box_coder
 from object_detection.core import box_list
 from object_detection.core import region_similarity_calculator
+from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner as targetassigner
 from object_detection.matchers import argmax_matcher
 from object_detection.matchers import bipartite_matcher
+from object_detection.utils import test_case
 
 
-class TargetAssignerTest(tf.test.TestCase):
+class TargetAssignerTest(test_case.TestCase):
 
   def test_assign_agnostic(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder, unmatched_cls_target=None)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0, 0.5, .5, 1.0]])
-    prior_stddevs = tf.constant(3 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
-
-    box_corners = [[0.0, 0.0, 0.5, 0.5], [0.5, 0.5, 0.9, 0.9]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0, 0.5, .5, 1.0]], dtype=np.float32)
+    anchor_stddevs = np.array(3 * [4 * [.1]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.9, 0.9]],
+                                       dtype=np.float32)
     exp_cls_targets = [[1], [1], [0]]
     exp_cls_weights = [1, 1, 1]
     exp_reg_targets = [[0, 0, 0, 0],
                        [0, 0, -1, 1],
                        [0, 0, 0, 0]]
     exp_reg_weights = [1, 1, 0]
-    exp_matching_anchors = [0, 1]
-
-    result = target_assigner.assign(priors, boxes, num_valid_rows=2)
-    (cls_targets, cls_weights, reg_targets, reg_weights, match) = result
-
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out,
-       reg_targets_out, reg_weights_out, matching_anchors_out) = sess.run(
-           [cls_targets, cls_weights, reg_targets, reg_weights,
-            match.matched_column_indices()])
-
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(matching_anchors_out, exp_matching_anchors)
-      self.assertEquals(cls_targets_out.dtype, np.float32)
-      self.assertEquals(cls_weights_out.dtype, np.float32)
-      self.assertEquals(reg_targets_out.dtype, np.float32)
-      self.assertEquals(reg_weights_out.dtype, np.float32)
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
-
-  def test_assign_with_ignored_matches(self):
+
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
+
+  def test_assign_class_agnostic_with_ignored_matches(self):
     # Note: test is very similar to above. The third box matched with an IOU
     # of 0.35, which is between the matched and unmatched threshold. This means
     # That like above the expected classification targets are [1, 1, 0].
     # Unlike above, the third target is ignored and therefore expected
     # classification weights are [1, 1, 0].
-    similarity_calc = region_similarity_calculator.IouSimilarity()
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
-                                           unmatched_threshold=0.3)
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0.0, 0.5, .9, 1.0]])
-    prior_stddevs = tf.constant(3 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
-
-    box_corners = [[0.0, 0.0, 0.5, 0.5],
-                   [0.5, 0.5, 0.9, 0.9]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.3)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)
+    anchor_stddevs = np.array(3 * [4 * [.1]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.9, 0.9]], dtype=np.float32)
     exp_cls_targets = [[1], [1], [0]]
     exp_cls_weights = [1, 1, 0]
     exp_reg_targets = [[0, 0, 0, 0],
                        [0, 0, -1, 1],
                        [0, 0, 0, 0]]
     exp_reg_weights = [1, 1, 0]
-    exp_matching_anchors = [0, 1]
-
-    result = target_assigner.assign(priors, boxes)
-    (cls_targets, cls_weights, reg_targets, reg_weights, match) = result
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out,
-       reg_targets_out, reg_weights_out, matching_anchors_out) = sess.run(
-           [cls_targets, cls_weights, reg_targets, reg_weights,
-            match.matched_column_indices()])
-
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(matching_anchors_out, exp_matching_anchors)
-      self.assertEquals(cls_targets_out.dtype, np.float32)
-      self.assertEquals(cls_weights_out.dtype, np.float32)
-      self.assertEquals(reg_targets_out.dtype, np.float32)
-      self.assertEquals(reg_weights_out.dtype, np.float32)
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
+
+  def test_assign_agnostic_with_keypoints(self):
+    def graph_fn(anchor_means, groundtruth_box_corners,
+                 groundtruth_keypoints):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = keypoint_box_coder.KeypointBoxCoder(
+          num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,
+                                    groundtruth_keypoints)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 1.0],
+                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.45, 0.45, 0.95, 0.95]],
+                                       dtype=np.float32)
+    groundtruth_keypoints = np.array(
+        [[[0.1, 0.2], [0.1, 0.3], [0.2, 0.2], [0.2, 0.2], [0.1, 0.1], [0.9, 0]],
+         [[0, 0.3], [0.2, 0.4], [0.5, 0.6], [0, 0.6], [0.8, 0.2], [0.2, 0.4]]],
+        dtype=np.float32)
+    exp_cls_targets = [[1], [1], [0]]
+    exp_cls_weights = [1, 1, 1]
+    exp_reg_targets = [[0, 0, 0, 0, -3, -1, -3, 1, -1, -1, -1, -1, -3, -3, 13,
+                        -5],
+                       [-1, -1, 0, 0, -15, -9, -11, -7, -5, -3, -15, -3, 1, -11,
+                        -11, -7],
+                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
+    exp_reg_weights = [1, 1, 0]
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means,
+                                                groundtruth_box_corners,
+                                                groundtruth_keypoints])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
+
+  def test_assign_class_agnostic_with_keypoints_and_ignored_matches(self):
+    # Note: test is very similar to above. The third box matched with an IOU
+    # of 0.35, which is between the matched and unmatched threshold. This means
+    # That like above the expected classification targets are [1, 1, 0].
+    # Unlike above, the third target is ignored and therefore expected
+    # classification weights are [1, 1, 0].
+    def graph_fn(anchor_means, groundtruth_box_corners,
+                 groundtruth_keypoints):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = keypoint_box_coder.KeypointBoxCoder(
+          num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,
+                                    groundtruth_keypoints)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 1.0],
+                             [0.0, 0.5, .9, 1.0]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.45, 0.45, 0.95, 0.95]],
+                                       dtype=np.float32)
+    groundtruth_keypoints = np.array(
+        [[[0.1, 0.2], [0.1, 0.3], [0.2, 0.2], [0.2, 0.2], [0.1, 0.1], [0.9, 0]],
+         [[0, 0.3], [0.2, 0.4], [0.5, 0.6], [0, 0.6], [0.8, 0.2], [0.2, 0.4]]],
+        dtype=np.float32)
+    exp_cls_targets = [[1], [1], [0]]
+    exp_cls_weights = [1, 1, 1]
+    exp_reg_targets = [[0, 0, 0, 0, -3, -1, -3, 1, -1, -1, -1, -1, -3, -3, 13,
+                        -5],
+                       [-1, -1, 0, 0, -15, -9, -11, -7, -5, -3, -15, -3, 1, -11,
+                        -11, -7],
+                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
+    exp_reg_weights = [1, 1, 0]
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means,
+                                                groundtruth_box_corners,
+                                                groundtruth_keypoints])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
 
   def test_assign_multiclass(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0, 0.5, .5, 1.0],
-                               [.75, 0, 1.0, .25]])
-    prior_stddevs = tf.constant(4 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
-
-    box_corners = [[0.0, 0.0, 0.5, 0.5],
-                   [0.5, 0.5, 0.9, 0.9],
-                   [.75, 0, .95, .27]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
-
-    groundtruth_labels = tf.constant([[0, 1, 0, 0, 0, 0, 0],
-                                      [0, 0, 0, 0, 0, 1, 0],
-                                      [0, 0, 0, 1, 0, 0, 0]], tf.float32)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
+                 groundtruth_labels):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder,
+          unmatched_cls_target=unmatched_cls_target)
+
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
+                                      groundtruth_labels)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0, 0.5, .5, 1.0],
+                             [.75, 0, 1.0, .25]], dtype=np.float32)
+    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.9, 0.9],
+                                        [.75, 0, .95, .27]], dtype=np.float32)
+    groundtruth_labels = np.array([[0, 1, 0, 0, 0, 0, 0],
+                                   [0, 0, 0, 0, 0, 1, 0],
+                                   [0, 0, 0, 1, 0, 0, 0]], dtype=np.float32)
 
     exp_cls_targets = [[0, 1, 0, 0, 0, 0, 0],
                        [0, 0, 0, 0, 0, 1, 0],
@@ -157,88 +260,98 @@ class TargetAssignerTest(tf.test.TestCase):
                        [0, 0, 0, 0],
                        [0, 0, -.5, .2]]
     exp_reg_weights = [1, 1, 0, 1]
-    exp_matching_anchors = [0, 1, 3]
-
-    result = target_assigner.assign(priors, boxes, groundtruth_labels,
-                                    num_valid_rows=3)
-    (cls_targets, cls_weights, reg_targets, reg_weights, match) = result
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out,
-       reg_targets_out, reg_weights_out, matching_anchors_out) = sess.run(
-           [cls_targets, cls_weights, reg_targets, reg_weights,
-            match.matched_column_indices()])
-
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(matching_anchors_out, exp_matching_anchors)
-      self.assertEquals(cls_targets_out.dtype, np.float32)
-      self.assertEquals(cls_weights_out.dtype, np.float32)
-      self.assertEquals(reg_targets_out.dtype, np.float32)
-      self.assertEquals(reg_weights_out.dtype, np.float32)
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
-
-  def test_assign_multiclass_unequal_class_weights(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        positive_class_weight=1.0, negative_class_weight=0.5,
-        unmatched_cls_target=unmatched_cls_target)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0, 0.5, .5, 1.0],
-                               [.75, 0, 1.0, .25]])
-    prior_stddevs = tf.constant(4 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
 
-    box_corners = [[0.0, 0.0, 0.5, 0.5],
-                   [0.5, 0.5, 0.9, 0.9],
-                   [.75, 0, .95, .27]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
-
-    groundtruth_labels = tf.constant([[0, 1, 0, 0, 0, 0, 0],
-                                      [0, 0, 0, 0, 0, 1, 0],
-                                      [0, 0, 0, 1, 0, 0, 0]], tf.float32)
-
-    exp_cls_weights = [1, 1, .5, 1]
-    result = target_assigner.assign(priors, boxes, groundtruth_labels,
-                                    num_valid_rows=3)
-    (_, cls_weights, _, _, _) = result
-    with self.test_session() as sess:
-      cls_weights_out = sess.run(cls_weights)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners,
+                                                groundtruth_labels])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
+
+  def test_assign_multiclass_with_groundtruth_weights(self):
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
+                 groundtruth_labels, groundtruth_weights):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder,
+          unmatched_cls_target=unmatched_cls_target)
+
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
+                                      groundtruth_labels,
+                                      groundtruth_weights)
+      (_, cls_weights, _, reg_weights, _) = result
+      return (cls_weights, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0, 0.5, .5, 1.0],
+                             [.75, 0, 1.0, .25]], dtype=np.float32)
+    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.9, 0.9],
+                                        [.75, 0, .95, .27]], dtype=np.float32)
+    groundtruth_labels = np.array([[0, 1, 0, 0, 0, 0, 0],
+                                   [0, 0, 0, 0, 0, 1, 0],
+                                   [0, 0, 0, 1, 0, 0, 0]], dtype=np.float32)
+    groundtruth_weights = np.array([0.3, 0., 0.5], dtype=np.float32)
+
+    exp_cls_weights = [0.3, 0., 1, 0.5]   # background class gets weight of 1.
+    exp_reg_weights = [0.3, 0., 0., 0.5]  # background class gets weight of 0.
+
+    (cls_weights_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners,
+                                                groundtruth_labels,
+                                                groundtruth_weights])
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
 
   def test_assign_multidimensional_class_targets(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    unmatched_cls_target = tf.constant([[0, 0], [0, 0]], tf.float32)
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0, 0.5, .5, 1.0],
-                               [.75, 0, 1.0, .25]])
-    prior_stddevs = tf.constant(4 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
-
-    box_corners = [[0.0, 0.0, 0.5, 0.5],
-                   [0.5, 0.5, 0.9, 0.9],
-                   [.75, 0, .95, .27]]
-    boxes = box_list.BoxList(tf.constant(box_corners))
-
-    groundtruth_labels = tf.constant([[[0, 1], [1, 0]],
-                                      [[1, 0], [0, 1]],
-                                      [[0, 1], [1, .5]]], tf.float32)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
+                 groundtruth_labels):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+
+      unmatched_cls_target = tf.constant([[0, 0], [0, 0]], tf.float32)
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder,
+          unmatched_cls_target=unmatched_cls_target)
+
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
+                                      groundtruth_labels)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0, 0.5, .5, 1.0],
+                             [.75, 0, 1.0, .25]], dtype=np.float32)
+    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)
+    groundtruth_box_corners = np.array([[0.0, 0.0, 0.5, 0.5],
+                                        [0.5, 0.5, 0.9, 0.9],
+                                        [.75, 0, .95, .27]], dtype=np.float32)
+
+    groundtruth_labels = np.array([[[0, 1], [1, 0]],
+                                   [[1, 0], [0, 1]],
+                                   [[0, 1], [1, .5]]], np.float32)
 
     exp_cls_targets = [[[0, 1], [1, 0]],
                        [[1, 0], [0, 1]],
@@ -250,52 +363,46 @@ class TargetAssignerTest(tf.test.TestCase):
                        [0, 0, 0, 0],
                        [0, 0, -.5, .2]]
     exp_reg_weights = [1, 1, 0, 1]
-    exp_matching_anchors = [0, 1, 3]
-
-    result = target_assigner.assign(priors, boxes, groundtruth_labels,
-                                    num_valid_rows=3)
-    (cls_targets, cls_weights, reg_targets, reg_weights, match) = result
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out,
-       reg_targets_out, reg_weights_out, matching_anchors_out) = sess.run(
-           [cls_targets, cls_weights, reg_targets, reg_weights,
-            match.matched_column_indices()])
-
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(matching_anchors_out, exp_matching_anchors)
-      self.assertEquals(cls_targets_out.dtype, np.float32)
-      self.assertEquals(cls_weights_out.dtype, np.float32)
-      self.assertEquals(reg_targets_out.dtype, np.float32)
-      self.assertEquals(reg_weights_out.dtype, np.float32)
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners,
+                                                groundtruth_labels])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
 
   def test_assign_empty_groundtruth(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    unmatched_cls_target = tf.constant([0, 0, 0], tf.float32)
-    target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
-
-    prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
-                               [0.5, 0.5, 1.0, 0.8],
-                               [0, 0.5, .5, 1.0],
-                               [.75, 0, 1.0, .25]])
-    prior_stddevs = tf.constant(4 * [4 * [.1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
-
-    box_corners_expanded = tf.constant([[0.0, 0.0, 0.0, 0.0]])
-    box_corners = tf.slice(box_corners_expanded, [0, 0], [0, 4])
-    boxes = box_list.BoxList(box_corners)
-
-    groundtruth_labels_expanded = tf.constant([[0, 0, 0]], tf.float32)
-    groundtruth_labels = tf.slice(groundtruth_labels_expanded, [0, 0], [0, 3])
-
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
+                 groundtruth_labels):
+      similarity_calc = region_similarity_calculator.IouSimilarity()
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                             unmatched_threshold=0.5)
+      box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
+      unmatched_cls_target = tf.constant([0, 0, 0], tf.float32)
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      target_assigner = targetassigner.TargetAssigner(
+          similarity_calc, matcher, box_coder,
+          unmatched_cls_target=unmatched_cls_target)
+      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
+                                      groundtruth_labels)
+      (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_box_corners = np.zeros((0, 4), dtype=np.float32)
+    groundtruth_labels = np.zeros((0, 3), dtype=np.float32)
+    anchor_means = np.array([[0.0, 0.0, 0.5, 0.5],
+                             [0.5, 0.5, 1.0, 0.8],
+                             [0, 0.5, .5, 1.0],
+                             [.75, 0, 1.0, .25]],
+                            dtype=np.float32)
+    anchor_stddevs = np.array(4 * [4 * [.1]], dtype=np.float32)
     exp_cls_targets = [[0, 0, 0],
                        [0, 0, 0],
                        [0, 0, 0],
@@ -306,26 +413,18 @@ class TargetAssignerTest(tf.test.TestCase):
                        [0, 0, 0, 0],
                        [0, 0, 0, 0]]
     exp_reg_weights = [0, 0, 0, 0]
-    exp_matching_anchors = []
-
-    result = target_assigner.assign(priors, boxes, groundtruth_labels)
-    (cls_targets, cls_weights, reg_targets, reg_weights, match) = result
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out,
-       reg_targets_out, reg_weights_out, matching_anchors_out) = sess.run(
-           [cls_targets, cls_weights, reg_targets, reg_weights,
-            match.matched_column_indices()])
-
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(matching_anchors_out, exp_matching_anchors)
-      self.assertEquals(cls_targets_out.dtype, np.float32)
-      self.assertEquals(cls_weights_out.dtype, np.float32)
-      self.assertEquals(reg_targets_out.dtype, np.float32)
-      self.assertEquals(reg_weights_out.dtype, np.float32)
-      self.assertEquals(matching_anchors_out.dtype, np.int32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_box_corners,
+                                                groundtruth_labels])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
+    self.assertEquals(cls_targets_out.dtype, np.float32)
+    self.assertEquals(cls_weights_out.dtype, np.float32)
+    self.assertEquals(reg_targets_out.dtype, np.float32)
+    self.assertEquals(reg_weights_out.dtype, np.float32)
 
   def test_raises_error_on_incompatible_groundtruth_boxes_and_labels(self):
     similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
@@ -353,14 +452,9 @@ class TargetAssignerTest(tf.test.TestCase):
     groundtruth_labels = tf.constant([[0, 1, 0, 0, 0, 0, 0],
                                       [0, 0, 0, 0, 0, 1, 0],
                                       [0, 0, 0, 1, 0, 0, 0]], tf.float32)
-    result = target_assigner.assign(priors, boxes, groundtruth_labels,
-                                    num_valid_rows=3)
-    (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
-    with self.test_session() as sess:
-      with self.assertRaisesWithPredicateMatch(
-          tf.errors.InvalidArgumentError,
-          'Groundtruth boxes and labels have incompatible shapes!'):
-        sess.run([cls_targets, cls_weights, reg_targets, reg_weights])
+    with self.assertRaisesRegexp(ValueError, 'Unequal shapes'):
+      target_assigner.assign(priors, boxes, groundtruth_labels,
+                             num_valid_rows=3)
 
   def test_raises_error_on_invalid_groundtruth_labels(self):
     similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
@@ -380,7 +474,6 @@ class TargetAssignerTest(tf.test.TestCase):
                    [0.5, 0.5, 0.9, 0.9],
                    [.75, 0, .95, .27]]
     boxes = box_list.BoxList(tf.constant(box_corners))
-
     groundtruth_labels = tf.constant([[[0, 1], [1, 0]]], tf.float32)
 
     with self.assertRaises(ValueError):
@@ -388,61 +481,66 @@ class TargetAssignerTest(tf.test.TestCase):
                              num_valid_rows=3)
 
 
-class BatchTargetAssignerTest(tf.test.TestCase):
+class BatchTargetAssignerTest(test_case.TestCase):
 
   def _get_agnostic_target_assigner(self):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
+    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                           unmatched_threshold=0.5)
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
     return targetassigner.TargetAssigner(
         similarity_calc, matcher, box_coder,
-        positive_class_weight=1.0,
-        negative_class_weight=1.0,
         unmatched_cls_target=None)
 
   def _get_multi_class_target_assigner(self, num_classes):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
+    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                           unmatched_threshold=0.5)
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
     unmatched_cls_target = tf.constant([1] + num_classes * [0], tf.float32)
     return targetassigner.TargetAssigner(
         similarity_calc, matcher, box_coder,
-        positive_class_weight=1.0,
-        negative_class_weight=1.0,
         unmatched_cls_target=unmatched_cls_target)
 
   def _get_multi_dimensional_target_assigner(self, target_dimensions):
-    similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
-    matcher = bipartite_matcher.GreedyBipartiteMatcher()
+    similarity_calc = region_similarity_calculator.IouSimilarity()
+    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
+                                           unmatched_threshold=0.5)
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
     unmatched_cls_target = tf.constant(np.zeros(target_dimensions),
                                        tf.float32)
     return targetassigner.TargetAssigner(
         similarity_calc, matcher, box_coder,
-        positive_class_weight=1.0,
-        negative_class_weight=1.0,
         unmatched_cls_target=unmatched_cls_target)
 
   def test_batch_assign_targets(self):
-    box_list1 = box_list.BoxList(tf.constant([[0., 0., 0.2, 0.2]]))
-    box_list2 = box_list.BoxList(tf.constant(
-        [[0, 0.25123152, 1, 1],
-         [0.015789, 0.0985, 0.55789, 0.3842]]
-    ))
-
-    gt_box_batch = [box_list1, box_list2]
-    gt_class_targets = [None, None]
-
-    prior_means = tf.constant([[0, 0, .25, .25],
-                               [0, .25, 1, 1],
-                               [0, .1, .5, .5],
-                               [.75, .75, 1, 1]])
-    prior_stddevs = tf.constant([[.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
+                 groundtruth_boxlist2):
+      box_list1 = box_list.BoxList(groundtruth_boxlist1)
+      box_list2 = box_list.BoxList(groundtruth_boxlist2)
+      gt_box_batch = [box_list1, box_list2]
+      gt_class_targets = [None, None]
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      agnostic_target_assigner = self._get_agnostic_target_assigner()
+      (cls_targets, cls_weights, reg_targets, reg_weights,
+       _) = targetassigner.batch_assign_targets(
+           agnostic_target_assigner, anchors_boxlist, gt_box_batch,
+           gt_class_targets)
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)
+    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],
+                                     [0.015789, 0.0985, 0.55789, 0.3842]],
+                                    dtype=np.float32)
+    anchor_means = np.array([[0, 0, .25, .25],
+                             [0, .25, 1, 1],
+                             [0, .1, .5, .5],
+                             [.75, .75, 1, 1]], dtype=np.float32)
+    anchor_stddevs = np.array([[.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1]], dtype=np.float32)
 
     exp_reg_targets = [[[0, 0, -0.5, -0.5],
                         [0, 0, 0, 0],
@@ -458,58 +556,55 @@ class BatchTargetAssignerTest(tf.test.TestCase):
                        [[0], [1], [1], [0]]]
     exp_reg_weights = [[1, 0, 0, 0],
                        [0, 1, 1, 0]]
-    exp_match_0 = [0]
-    exp_match_1 = [1, 2]
-
-    agnostic_target_assigner = self._get_agnostic_target_assigner()
-    (cls_targets, cls_weights, reg_targets, reg_weights,
-     match_list) = targetassigner.batch_assign_targets(
-         agnostic_target_assigner, priors, gt_box_batch, gt_class_targets)
-    self.assertTrue(isinstance(match_list, list) and len(match_list) == 2)
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out, reg_targets_out, reg_weights_out,
-       match_out_0, match_out_1) = sess.run([
-           cls_targets, cls_weights, reg_targets, reg_weights] + [
-               match.matched_column_indices() for match in match_list])
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(match_out_0, exp_match_0)
-      self.assertAllClose(match_out_1, exp_match_1)
-
-  def test_batch_assign_multiclass_targets(self):
-    box_list1 = box_list.BoxList(tf.constant([[0., 0., 0.2, 0.2]]))
-
-    box_list2 = box_list.BoxList(tf.constant(
-        [[0, 0.25123152, 1, 1],
-         [0.015789, 0.0985, 0.55789, 0.3842]]
-    ))
-
-    gt_box_batch = [box_list1, box_list2]
 
-    class_targets1 = tf.constant([[0, 1, 0, 0]], tf.float32)
-    class_targets2 = tf.constant([[0, 0, 0, 1],
-                                  [0, 0, 1, 0]], tf.float32)
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_boxlist1,
+                                                groundtruth_boxlist2])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
 
-    gt_class_targets = [class_targets1, class_targets2]
-
-    prior_means = tf.constant([[0, 0, .25, .25],
-                               [0, .25, 1, 1],
-                               [0, .1, .5, .5],
-                               [.75, .75, 1, 1]])
-    prior_stddevs = tf.constant([[.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
+  def test_batch_assign_multiclass_targets(self):
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
+                 groundtruth_boxlist2, class_targets1, class_targets2):
+      box_list1 = box_list.BoxList(groundtruth_boxlist1)
+      box_list2 = box_list.BoxList(groundtruth_boxlist2)
+      gt_box_batch = [box_list1, box_list2]
+      gt_class_targets = [class_targets1, class_targets2]
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      multiclass_target_assigner = self._get_multi_class_target_assigner(
+          num_classes=3)
+      (cls_targets, cls_weights, reg_targets, reg_weights,
+       _) = targetassigner.batch_assign_targets(
+           multiclass_target_assigner, anchors_boxlist, gt_box_batch,
+           gt_class_targets)
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)
+    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],
+                                     [0.015789, 0.0985, 0.55789, 0.3842]],
+                                    dtype=np.float32)
+    class_targets1 = np.array([[0, 1, 0, 0]], dtype=np.float32)
+    class_targets2 = np.array([[0, 0, 0, 1],
+                               [0, 0, 1, 0]], dtype=np.float32)
+
+    anchor_means = np.array([[0, 0, .25, .25],
+                             [0, .25, 1, 1],
+                             [0, .1, .5, .5],
+                             [.75, .75, 1, 1]], dtype=np.float32)
+    anchor_stddevs = np.array([[.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1]], dtype=np.float32)
 
     exp_reg_targets = [[[0, 0, -0.5, -0.5],
                         [0, 0, 0, 0],
-                        [0, 0, 0, 0],
-                        [0, 0, 0, 0]],
-                       [[0, 0, 0, 0],
+                        [0, 0, 0, 0,],
+                        [0, 0, 0, 0,],],
+                       [[0, 0, 0, 0,],
                         [0, 0.01231521, 0, 0],
                         [0.15789001, -0.01500003, 0.57889998, -1.15799987],
                         [0, 0, 0, 0]]]
@@ -525,68 +620,68 @@ class BatchTargetAssignerTest(tf.test.TestCase):
                         [1, 0, 0, 0]]]
     exp_reg_weights = [[1, 0, 0, 0],
                        [0, 1, 1, 0]]
-    exp_match_0 = [0]
-    exp_match_1 = [1, 2]
-
-    multiclass_target_assigner = self._get_multi_class_target_assigner(
-        num_classes=3)
-
-    (cls_targets, cls_weights, reg_targets, reg_weights,
-     match_list) = targetassigner.batch_assign_targets(
-         multiclass_target_assigner, priors, gt_box_batch, gt_class_targets)
-    self.assertTrue(isinstance(match_list, list) and len(match_list) == 2)
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out, reg_targets_out, reg_weights_out,
-       match_out_0, match_out_1) = sess.run([
-           cls_targets, cls_weights, reg_targets, reg_weights] + [
-               match.matched_column_indices() for match in match_list])
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(match_out_0, exp_match_0)
-      self.assertAllClose(match_out_1, exp_match_1)
+
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_boxlist1,
+                                                groundtruth_boxlist2,
+                                                class_targets1,
+                                                class_targets2])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
 
   def test_batch_assign_multidimensional_targets(self):
-    box_list1 = box_list.BoxList(tf.constant([[0., 0., 0.2, 0.2]]))
-
-    box_list2 = box_list.BoxList(tf.constant(
-        [[0, 0.25123152, 1, 1],
-         [0.015789, 0.0985, 0.55789, 0.3842]]
-    ))
-
-    gt_box_batch = [box_list1, box_list2]
-    class_targets1 = tf.constant([[[0, 1, 1],
-                                   [1, 1, 0]]], tf.float32)
-    class_targets2 = tf.constant([[[0, 1, 1],
-                                   [1, 1, 0]],
-                                  [[0, 0, 1],
-                                   [0, 0, 1]]], tf.float32)
-
-    gt_class_targets = [class_targets1, class_targets2]
-
-    prior_means = tf.constant([[0, 0, .25, .25],
-                               [0, .25, 1, 1],
-                               [0, .1, .5, .5],
-                               [.75, .75, 1, 1]])
-    prior_stddevs = tf.constant([[.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1],
-                                 [.1, .1, .1, .1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_boxlist1,
+                 groundtruth_boxlist2, class_targets1, class_targets2):
+      box_list1 = box_list.BoxList(groundtruth_boxlist1)
+      box_list2 = box_list.BoxList(groundtruth_boxlist2)
+      gt_box_batch = [box_list1, box_list2]
+      gt_class_targets = [class_targets1, class_targets2]
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+      multiclass_target_assigner = self._get_multi_dimensional_target_assigner(
+          target_dimensions=(2, 3))
+      (cls_targets, cls_weights, reg_targets, reg_weights,
+       _) = targetassigner.batch_assign_targets(
+           multiclass_target_assigner, anchors_boxlist, gt_box_batch,
+           gt_class_targets)
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)
+    groundtruth_boxlist2 = np.array([[0, 0.25123152, 1, 1],
+                                     [0.015789, 0.0985, 0.55789, 0.3842]],
+                                    dtype=np.float32)
+    class_targets1 = np.array([[0, 1, 0, 0]], dtype=np.float32)
+    class_targets2 = np.array([[0, 0, 0, 1],
+                               [0, 0, 1, 0]], dtype=np.float32)
+    class_targets1 = np.array([[[0, 1, 1],
+                                [1, 1, 0]]], dtype=np.float32)
+    class_targets2 = np.array([[[0, 1, 1],
+                                [1, 1, 0]],
+                               [[0, 0, 1],
+                                [0, 0, 1]]], dtype=np.float32)
+
+    anchor_means = np.array([[0, 0, .25, .25],
+                             [0, .25, 1, 1],
+                             [0, .1, .5, .5],
+                             [.75, .75, 1, 1]], dtype=np.float32)
+    anchor_stddevs = np.array([[.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1],
+                               [.1, .1, .1, .1]], dtype=np.float32)
 
     exp_reg_targets = [[[0, 0, -0.5, -0.5],
                         [0, 0, 0, 0],
-                        [0, 0, 0, 0],
-                        [0, 0, 0, 0]],
-                       [[0, 0, 0, 0],
+                        [0, 0, 0, 0,],
+                        [0, 0, 0, 0,],],
+                       [[0, 0, 0, 0,],
                         [0, 0.01231521, 0, 0],
                         [0.15789001, -0.01500003, 0.57889998, -1.15799987],
                         [0, 0, 0, 0]]]
     exp_cls_weights = [[1, 1, 1, 1],
                        [1, 1, 1, 1]]
-
     exp_cls_targets = [[[[0., 1., 1.],
                          [1., 1., 0.]],
                         [[0., 0., 0.],
@@ -605,72 +700,60 @@ class BatchTargetAssignerTest(tf.test.TestCase):
                          [0., 0., 0.]]]]
     exp_reg_weights = [[1, 0, 0, 0],
                        [0, 1, 1, 0]]
-    exp_match_0 = [0]
-    exp_match_1 = [1, 2]
-
-    multiclass_target_assigner = self._get_multi_dimensional_target_assigner(
-        target_dimensions=(2, 3))
-
-    (cls_targets, cls_weights, reg_targets, reg_weights,
-     match_list) = targetassigner.batch_assign_targets(
-         multiclass_target_assigner, priors, gt_box_batch, gt_class_targets)
-    self.assertTrue(isinstance(match_list, list) and len(match_list) == 2)
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out, reg_targets_out, reg_weights_out,
-       match_out_0, match_out_1) = sess.run([
-           cls_targets, cls_weights, reg_targets, reg_weights] + [
-               match.matched_column_indices() for match in match_list])
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(match_out_0, exp_match_0)
-      self.assertAllClose(match_out_1, exp_match_1)
+
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(graph_fn, [anchor_means, anchor_stddevs,
+                                                groundtruth_boxlist1,
+                                                groundtruth_boxlist2,
+                                                class_targets1,
+                                                class_targets2])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
 
   def test_batch_assign_empty_groundtruth(self):
-    box_coords_expanded = tf.zeros((1, 4), tf.float32)
-    box_coords = tf.slice(box_coords_expanded, [0, 0], [0, 4])
-    box_list1 = box_list.BoxList(box_coords)
-    gt_box_batch = [box_list1]
-
-    prior_means = tf.constant([[0, 0, .25, .25],
-                               [0, .25, 1, 1]])
-    prior_stddevs = tf.constant([[.1, .1, .1, .1],
-                                 [.1, .1, .1, .1]])
-    priors = box_list.BoxList(prior_means)
-    priors.add_field('stddev', prior_stddevs)
 
+    def graph_fn(anchor_means, anchor_stddevs, groundtruth_box_corners,
+                 gt_class_targets):
+      groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
+      gt_box_batch = [groundtruth_boxlist]
+      gt_class_targets_batch = [gt_class_targets]
+      anchors_boxlist = box_list.BoxList(anchor_means)
+      anchors_boxlist.add_field('stddev', anchor_stddevs)
+
+      multiclass_target_assigner = self._get_multi_class_target_assigner(
+          num_classes=3)
+
+      (cls_targets, cls_weights, reg_targets, reg_weights,
+       _) = targetassigner.batch_assign_targets(
+           multiclass_target_assigner, anchors_boxlist,
+           gt_box_batch, gt_class_targets_batch)
+      return (cls_targets, cls_weights, reg_targets, reg_weights)
+
+    groundtruth_box_corners = np.zeros((0, 4), dtype=np.float32)
+    anchor_means = np.array([[0, 0, .25, .25],
+                             [0, .25, 1, 1]], dtype=np.float32)
+    anchor_stddevs = np.array([[.1, .1, .1, .1],
+                               [.1, .1, .1, .1]], dtype=np.float32)
     exp_reg_targets = [[[0, 0, 0, 0],
                         [0, 0, 0, 0]]]
     exp_cls_weights = [[1, 1]]
     exp_cls_targets = [[[1, 0, 0, 0],
                         [1, 0, 0, 0]]]
     exp_reg_weights = [[0, 0]]
-    exp_match_0 = []
-
     num_classes = 3
     pad = 1
-    gt_class_targets = tf.zeros((0, num_classes + pad))
-    gt_class_targets_batch = [gt_class_targets]
-
-    multiclass_target_assigner = self._get_multi_class_target_assigner(
-        num_classes=3)
-
-    (cls_targets, cls_weights, reg_targets, reg_weights,
-     match_list) = targetassigner.batch_assign_targets(
-         multiclass_target_assigner, priors,
-         gt_box_batch, gt_class_targets_batch)
-    self.assertTrue(isinstance(match_list, list) and len(match_list) == 1)
-    with self.test_session() as sess:
-      (cls_targets_out, cls_weights_out, reg_targets_out, reg_weights_out,
-       match_out_0) = sess.run([
-           cls_targets, cls_weights, reg_targets, reg_weights] + [
-               match.matched_column_indices() for match in match_list])
-      self.assertAllClose(cls_targets_out, exp_cls_targets)
-      self.assertAllClose(cls_weights_out, exp_cls_weights)
-      self.assertAllClose(reg_targets_out, exp_reg_targets)
-      self.assertAllClose(reg_weights_out, exp_reg_weights)
-      self.assertAllClose(match_out_0, exp_match_0)
+    gt_class_targets = np.zeros((0, num_classes + pad), dtype=np.float32)
+
+    (cls_targets_out, cls_weights_out, reg_targets_out,
+     reg_weights_out) = self.execute(
+         graph_fn, [anchor_means, anchor_stddevs, groundtruth_box_corners,
+                    gt_class_targets])
+    self.assertAllClose(cls_targets_out, exp_cls_targets)
+    self.assertAllClose(cls_weights_out, exp_cls_weights)
+    self.assertAllClose(reg_targets_out, exp_reg_targets)
+    self.assertAllClose(reg_weights_out, exp_reg_weights)
 
 
 class CreateTargetAssignerTest(tf.test.TestCase):
diff --git a/research/object_detection/data/BUILD b/research/object_detection/data/BUILD
new file mode 100644
index 00000000..6bf397ac
--- /dev/null
+++ b/research/object_detection/data/BUILD
@@ -0,0 +1,9 @@
+package(
+    default_visibility = ["//visibility:public"],
+)
+
+licenses(["notice"])
+
+exports_files([
+    "pet_label_map.pbtxt",
+])
diff --git a/research/object_detection/data_decoders/BUILD b/research/object_detection/data_decoders/BUILD
index d6b48ac0..a336e582 100644
--- a/research/object_detection/data_decoders/BUILD
+++ b/research/object_detection/data_decoders/BUILD
@@ -12,9 +12,10 @@ py_library(
     srcs = ["tf_example_decoder.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:data_decoder",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/core:data_decoder",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
 
@@ -24,6 +25,7 @@ py_test(
     deps = [
         ":tf_example_decoder",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
     ],
 )
diff --git a/research/object_detection/data_decoders/tf_example_decoder.py b/research/object_detection/data_decoders/tf_example_decoder.py
index 4dc3dc5c..a56fe86d 100644
--- a/research/object_detection/data_decoders/tf_example_decoder.py
+++ b/research/object_detection/data_decoders/tf_example_decoder.py
@@ -22,6 +22,7 @@ import tensorflow as tf
 
 from object_detection.core import data_decoder
 from object_detection.core import standard_fields as fields
+from object_detection.protos import input_reader_pb2
 from object_detection.utils import label_map_util
 
 slim_example_decoder = tf.contrib.slim.tfexample_decoder
@@ -32,12 +33,15 @@ class TfExampleDecoder(data_decoder.DataDecoder):
 
   def __init__(self,
                load_instance_masks=False,
+               instance_mask_type=input_reader_pb2.NUMERICAL_MASKS,
                label_map_proto_file=None,
                use_display_name=False):
     """Constructor sets keys_to_features and items_to_handlers.
 
     Args:
       load_instance_masks: whether or not to load and handle instance masks.
+      instance_mask_type: type of instance masks. Options are provided in
+        input_reader.proto. This is only used if `load_instance_masks` is True.
       label_map_proto_file: a file path to a
         object_detection.protos.StringIntLabelMap proto. If provided, then the
         mapped IDs of 'image/object/class/text' will take precedence over the
@@ -46,6 +50,11 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       use_display_name: whether or not to use the `display_name` for label
         mapping (instead of `name`).  Only used if label_map_proto_file is
         provided.
+
+    Raises:
+      ValueError: If `instance_mask_type` option is not one of
+        input_reader_pb2.DEFAULT, input_reader_pb2.NUMERICAL, or
+        input_reader_pb2.PNG_MASKS.
     """
     self.keys_to_features = {
         'image/encoded':
@@ -83,6 +92,8 @@ class TfExampleDecoder(data_decoder.DataDecoder):
             tf.VarLenFeature(tf.int64),
         'image/object/group_of':
             tf.VarLenFeature(tf.int64),
+        'image/object/weight':
+            tf.VarLenFeature(tf.float32),
     }
     self.items_to_handlers = {
         fields.InputDataFields.image: slim_example_decoder.Image(
@@ -104,19 +115,46 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         fields.InputDataFields.groundtruth_difficult: (
             slim_example_decoder.Tensor('image/object/difficult')),
         fields.InputDataFields.groundtruth_group_of: (
-            slim_example_decoder.Tensor('image/object/group_of'))
+            slim_example_decoder.Tensor('image/object/group_of')),
+        fields.InputDataFields.groundtruth_weights: (
+            slim_example_decoder.Tensor('image/object/weight')),
     }
     if load_instance_masks:
-      self.keys_to_features['image/object/mask'] = tf.VarLenFeature(tf.float32)
-      self.items_to_handlers[
-          fields.InputDataFields.groundtruth_instance_masks] = (
-              slim_example_decoder.ItemHandlerCallback(
-                  ['image/object/mask', 'image/height', 'image/width'],
-                  self._reshape_instance_masks))
-    # TODO: Add label_handler that decodes from 'image/object/class/text'
-    # primarily after the recent tf.contrib.slim changes make into a release
-    # supported by cloudml.
-    label_handler = slim_example_decoder.Tensor('image/object/class/label')
+      if instance_mask_type in (input_reader_pb2.DEFAULT,
+                                input_reader_pb2.NUMERICAL_MASKS):
+        self.keys_to_features['image/object/mask'] = (
+            tf.VarLenFeature(tf.float32))
+        self.items_to_handlers[
+            fields.InputDataFields.groundtruth_instance_masks] = (
+                slim_example_decoder.ItemHandlerCallback(
+                    ['image/object/mask', 'image/height', 'image/width'],
+                    self._reshape_instance_masks))
+      elif instance_mask_type == input_reader_pb2.PNG_MASKS:
+        self.keys_to_features['image/object/mask'] = tf.VarLenFeature(tf.string)
+        self.items_to_handlers[
+            fields.InputDataFields.groundtruth_instance_masks] = (
+                slim_example_decoder.ItemHandlerCallback(
+                    ['image/object/mask'], self._decode_png_instance_masks))
+      else:
+        raise ValueError('Did not recognize the `instance_mask_type` option.')
+    if label_map_proto_file:
+      label_map = label_map_util.get_label_map_dict(label_map_proto_file,
+                                                    use_display_name)
+      # We use a default_value of -1, but we expect all labels to be contained
+      # in the label map.
+      table = tf.contrib.lookup.HashTable(
+          initializer=tf.contrib.lookup.KeyValueTensorInitializer(
+              keys=tf.constant(list(label_map.keys())),
+              values=tf.constant(list(label_map.values()), dtype=tf.int64)),
+          default_value=-1)
+      # If the label_map_proto is provided, try to use it in conjunction with
+      # the class text, and fall back to a materialized ID.
+      label_handler = slim_example_decoder.BackupHandler(
+          slim_example_decoder.LookupTensor(
+              'image/object/class/text', table, default_value=''),
+          slim_example_decoder.Tensor('image/object/class/label'))
+    else:
+      label_handler = slim_example_decoder.Tensor('image/object/class/label')
     self.items_to_handlers[
         fields.InputDataFields.groundtruth_classes] = label_handler
 
@@ -149,8 +187,10 @@ class TfExampleDecoder(data_decoder.DataDecoder):
         [None] indicating if the boxes represent `difficult` instances.
       fields.InputDataFields.groundtruth_group_of - 1D bool tensor of shape
         [None] indicating if the boxes represent `group_of` instances.
-      fields.InputDataFields.groundtruth_instance_masks - 3D int64 tensor of
+      fields.InputDataFields.groundtruth_instance_masks - 3D float32 tensor of
         shape [None, None, None] containing instance masks.
+      fields.InputDataFields.groundtruth_weights - 1D float32 tensor of
+        shape [None] indicating the weights of groundtruth boxes.
     """
     serialized_example = tf.reshape(tf_example_string_tensor, shape=[])
     decoder = slim_example_decoder.TFExampleDecoder(self.keys_to_features,
@@ -167,7 +207,7 @@ class TfExampleDecoder(data_decoder.DataDecoder):
     """Reshape instance segmentation masks.
 
     The instance segmentation masks are reshaped to [num_instances, height,
-    width] and cast to boolean type to save memory.
+    width].
 
     Args:
       keys_to_tensors: a dictionary from keys to tensors.
@@ -184,3 +224,29 @@ class TfExampleDecoder(data_decoder.DataDecoder):
       masks = tf.sparse_tensor_to_dense(masks)
     masks = tf.reshape(tf.to_float(tf.greater(masks, 0.0)), to_shape)
     return tf.cast(masks, tf.float32)
+
+  def _decode_png_instance_masks(self, keys_to_tensors):
+    """Decode PNG instance segmentation masks and stack into dense tensor.
+
+    The instance segmentation masks are reshaped to [num_instances, height,
+    width].
+
+    Args:
+      keys_to_tensors: a dictionary from keys to tensors.
+
+    Returns:
+      A 3-D float tensor of shape [num_instances, height, width] with values
+        in {0, 1}.
+    """
+
+    def decode_png_mask(image_buffer):
+      image = tf.squeeze(
+          tf.image.decode_image(image_buffer, channels=1), axis=2)
+      image.set_shape([None, None])
+      image = tf.to_float(tf.greater(image, 0))
+      return image
+
+    png_masks = keys_to_tensors['image/object/mask']
+    if isinstance(png_masks, tf.SparseTensor):
+      png_masks = tf.sparse_tensor_to_dense(png_masks, default_value='')
+    return tf.map_fn(decode_png_mask, png_masks, dtype=tf.float32)
diff --git a/research/object_detection/data_decoders/tf_example_decoder_test.py b/research/object_detection/data_decoders/tf_example_decoder_test.py
index 04d00531..23af3acc 100644
--- a/research/object_detection/data_decoders/tf_example_decoder_test.py
+++ b/research/object_detection/data_decoders/tf_example_decoder_test.py
@@ -21,6 +21,7 @@ import tensorflow as tf
 
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
+from object_detection.protos import input_reader_pb2
 
 
 class TfExampleDecoderTest(tf.test.TestCase):
@@ -116,6 +117,36 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(decoded_png, tensor_dict[fields.InputDataFields.image])
     self.assertEqual('image_id', tensor_dict[fields.InputDataFields.source_id])
 
+  def testDecodePngInstanceMasks(self):
+    image_tensor = np.random.randint(256, size=(10, 10, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    mask_1 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)
+    mask_2 = np.random.randint(0, 2, size=(10, 10, 1)).astype(np.uint8)
+    encoded_png_1 = self._EncodeImage(mask_1, encoding_type='png')
+    decoded_png_1 = np.squeeze(mask_1.astype(np.float32))
+    encoded_png_2 = self._EncodeImage(mask_2, encoding_type='png')
+    decoded_png_2 = np.squeeze(mask_2.astype(np.float32))
+    encoded_masks = [encoded_png_1, encoded_png_2]
+    decoded_masks = np.stack([decoded_png_1, decoded_png_2])
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded': self._BytesFeature(encoded_jpeg),
+                'image/format': self._BytesFeature('jpeg'),
+                'image/object/mask': self._BytesFeature(encoded_masks)
+            })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        load_instance_masks=True, instance_mask_type=input_reader_pb2.PNG_MASKS)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(
+        decoded_masks,
+        tensor_dict[fields.InputDataFields.groundtruth_instance_masks])
+
   def testDecodeBoundingBox(self):
     image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -168,6 +199,48 @@ class TfExampleDecoderTest(tf.test.TestCase):
     self.assertAllEqual(bbox_classes,
                         tensor_dict[fields.InputDataFields.groundtruth_classes])
 
+  def testDecodeObjectLabelWithMapping(self):
+    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    bbox_classes_text = ['cat', 'dog']
+    example = tf.train.Example(
+        features=tf.train.Features(
+            feature={
+                'image/encoded':
+                    self._BytesFeature(encoded_jpeg),
+                'image/format':
+                    self._BytesFeature('jpeg'),
+                'image/object/class/text':
+                    self._BytesFeature(bbox_classes_text),
+            })).SerializeToString()
+
+    label_map_string = """
+      item {
+        id:3
+        name:'cat'
+      }
+      item {
+        id:1
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+    example_decoder = tf_example_decoder.TfExampleDecoder(
+        label_map_proto_file=label_map_path)
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[fields.InputDataFields.groundtruth_classes]
+                         .get_shape().as_list()), [None])
+
+    with self.test_session() as sess:
+      sess.run(tf.tables_initializer())
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual([3, 1],
+                        tensor_dict[fields.InputDataFields.groundtruth_classes])
+
   def testDecodeObjectArea(self):
     image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
     encoded_jpeg = self._EncodeImage(image_tensor)
@@ -259,6 +332,30 @@ class TfExampleDecoderTest(tf.test.TestCase):
         [bool(item) for item in object_group_of],
         tensor_dict[fields.InputDataFields.groundtruth_group_of])
 
+  def testDecodeObjectWeight(self):
+    image_tensor = np.random.randint(255, size=(4, 5, 3)).astype(np.uint8)
+    encoded_jpeg = self._EncodeImage(image_tensor)
+    object_weights = [0.75, 1.0]
+    example = tf.train.Example(features=tf.train.Features(
+        feature={
+            'image/encoded': self._BytesFeature(encoded_jpeg),
+            'image/format': self._BytesFeature('jpeg'),
+            'image/object/weight': self._FloatFeature(object_weights),
+        })).SerializeToString()
+
+    example_decoder = tf_example_decoder.TfExampleDecoder()
+    tensor_dict = example_decoder.decode(tf.convert_to_tensor(example))
+
+    self.assertAllEqual((tensor_dict[
+        fields.InputDataFields.groundtruth_weights].get_shape().as_list()),
+                        [None])
+    with self.test_session() as sess:
+      tensor_dict = sess.run(tensor_dict)
+
+    self.assertAllEqual(
+        object_weights,
+        tensor_dict[fields.InputDataFields.groundtruth_weights])
+
   def testDecodeInstanceSegmentation(self):
     num_instances = 4
     image_height = 5
diff --git a/research/object_detection/dataset_tools/BUILD b/research/object_detection/dataset_tools/BUILD
index bb5ce2e5..4d7a6ed1 100644
--- a/research/object_detection/dataset_tools/BUILD
+++ b/research/object_detection/dataset_tools/BUILD
@@ -1,4 +1,4 @@
-# Tensorflow Object Detection API: main runnables.
+# Tensorflow Object Detection API: dataset tools.
 
 package(
     default_visibility = ["//visibility:public"],
@@ -8,18 +8,43 @@ licenses(["notice"])
 
 # Apache 2.0
 
+py_binary(
+    name = "create_coco_tf_record",
+    srcs = [
+        "create_coco_tf_record.py",
+    ],
+    deps = [
+        "//PIL:pil",
+        "//pycocotools",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
+    ],
+)
+
+py_test(
+    name = "create_coco_tf_record_test",
+    srcs = [
+        "create_coco_tf_record_test.py",
+    ],
+    deps = [
+        ":create_coco_tf_record",
+        "//tensorflow",
+    ],
+)
+
 py_binary(
     name = "create_kitti_tf_record",
     srcs = [
         "create_kitti_tf_record.py",
     ],
     deps = [
-        "//third_party/py/PIL:pil",
-        "//third_party/py/lxml",
+        "//PIL:pil",
+        "//lxml",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:dataset_util",
-        "//tensorflow_models/object_detection/utils:label_map_util",
-        "//tensorflow_models/object_detection/utils:np_box_ops",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:np_box_ops",
     ],
 )
 
@@ -40,11 +65,11 @@ py_binary(
         "create_pascal_tf_record.py",
     ],
     deps = [
-        "//third_party/py/PIL:pil",
-        "//third_party/py/lxml",
+        "//PIL:pil",
+        "//lxml",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:dataset_util",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
 
@@ -65,11 +90,11 @@ py_binary(
         "create_pet_tf_record.py",
     ],
     deps = [
-        "//third_party/py/PIL:pil",
-        "//third_party/py/lxml",
+        "//PIL:pil",
+        "//lxml",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:dataset_util",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
 
@@ -78,8 +103,8 @@ py_library(
     srcs = ["oid_tfrecord_creation.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
     ],
 )
 
@@ -88,9 +113,9 @@ py_test(
     srcs = ["oid_tfrecord_creation_test.py"],
     deps = [
         ":oid_tfrecord_creation",
-        "//third_party/py/contextlib2",
-        "//third_party/py/pandas",
-        "//third_party/py/tensorflow",
+        "//contextlib2",
+        "//pandas",
+        "//tensorflow",
     ],
 )
 
@@ -99,9 +124,9 @@ py_binary(
     srcs = ["create_oid_tf_record.py"],
     deps = [
         ":oid_tfrecord_creation",
-        "//third_party/py/contextlib2",
-        "//third_party/py/pandas",
+        "//contextlib2",
+        "//pandas",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
diff --git a/research/object_detection/dataset_tools/__init__.py b/research/object_detection/dataset_tools/__init__.py
index 8b137891..e69de29b 100644
--- a/research/object_detection/dataset_tools/__init__.py
+++ b/research/object_detection/dataset_tools/__init__.py
@@ -1 +0,0 @@
-
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
new file mode 100644
index 00000000..066b8d05
--- /dev/null
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -0,0 +1,261 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Convert raw COCO dataset to TFRecord for object_detection.
+
+Example usage:
+    python create_coco_tf_record.py --logtostderr \
+      --train_image_dir="${TRAIN_IMAGE_DIR}" \
+      --val_image_dir="${VAL_IMAGE_DIR}" \
+      --test_image_dir="${TEST_IMAGE_DIR}" \
+      --train_annotations_file="${TRAIN_ANNOTATIONS_FILE}" \
+      --val_annotations_file="${VAL_ANNOTATIONS_FILE}" \
+      --testdev_annotations_file="${TESTDEV_ANNOTATIONS_FILE}" \
+      --output_dir="${OUTPUT_DIR}"
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import hashlib
+import io
+import json
+import os
+import numpy as np
+import PIL.Image
+
+from pycocotools import mask
+import tensorflow as tf
+
+from object_detection.utils import dataset_util
+from object_detection.utils import label_map_util
+
+
+flags = tf.app.flags
+tf.flags.DEFINE_boolean('include_masks', False,
+                        'Whether to include instance segmentations masks '
+                        '(PNG encoded) in the result. default: False.')
+tf.flags.DEFINE_string('train_image_dir', '',
+                       'Training image directory.')
+tf.flags.DEFINE_string('val_image_dir', '',
+                       'Validation image directory.')
+tf.flags.DEFINE_string('test_image_dir', '',
+                       'Test image directory.')
+tf.flags.DEFINE_string('train_annotations_file', '',
+                       'Training annotations JSON file.')
+tf.flags.DEFINE_string('val_annotations_file', '',
+                       'Validation annotations JSON file.')
+tf.flags.DEFINE_string('testdev_annotations_file', '',
+                       'Test-dev annotations JSON file.')
+tf.flags.DEFINE_string('output_dir', '/tmp/', 'Output data directory.')
+
+FLAGS = flags.FLAGS
+
+tf.logging.set_verbosity(tf.logging.INFO)
+
+
+def create_tf_example(image,
+                      annotations_list,
+                      image_dir,
+                      category_index,
+                      include_masks=False):
+  """Converts image and annotations to a tf.Example proto.
+
+  Args:
+    image: dict with keys:
+      [u'license', u'file_name', u'coco_url', u'height', u'width',
+      u'date_captured', u'flickr_url', u'id']
+    annotations_list:
+      list of dicts with keys:
+      [u'segmentation', u'area', u'iscrowd', u'image_id',
+      u'bbox', u'category_id', u'id']
+      Notice that bounding box coordinates in the official COCO dataset are
+      given as [x, y, width, height] tuples using absolute coordinates where
+      x, y represent the top-left (0-indexed) corner.  This function converts
+      to the format expected by the Tensorflow Object Detection API (which is
+      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative
+      to image size).
+    image_dir: Directory containing the image files.
+    category_index: a dict containing COCO category information keyed
+      by the 'id' field of each category.  See the
+      label_map_util.create_category_index function.
+    include_masks: Whether to include instance segmentations masks
+      (PNG encoded) in the result. default: False.
+
+  Returns:
+    example: The converted tf.Example
+    num_annotations_skipped: Number of (invalid) annotations that were ignored.
+
+  Raises:
+    ValueError: if the image pointed to by data['filename'] is not a valid JPEG
+  """
+  image_height = image['height']
+  image_width = image['width']
+  filename = image['file_name']
+
+  full_path = os.path.join(image_dir, filename)
+  with tf.gfile.GFile(full_path, 'rb') as fid:
+    encoded_jpg = fid.read()
+  encoded_jpg_io = io.BytesIO(encoded_jpg)
+  image = PIL.Image.open(encoded_jpg_io)
+  key = hashlib.sha256(encoded_jpg).hexdigest()
+
+  xmin = []
+  xmax = []
+  ymin = []
+  ymax = []
+  is_crowd = []
+  category_names = []
+  area = []
+  encoded_mask_png = []
+  num_annotations_skipped = 0
+  for object_annotations in annotations_list:
+    (x, y, width, height) = tuple(object_annotations['bbox'])
+    if width <= 0 or height <= 0:
+      num_annotations_skipped += 1
+      continue
+    if x + width > image_width or y + height > image_height:
+      num_annotations_skipped += 1
+      continue
+    xmin.append(float(x) / image_width)
+    xmax.append(float(x + width) / image_width)
+    ymin.append(float(y) / image_height)
+    ymax.append(float(y + height) / image_height)
+    is_crowd.append(object_annotations['iscrowd'])
+    category_id = int(object_annotations['category_id'])
+    category_names.append(category_index[category_id]['name'].encode('utf8'))
+    area.append(object_annotations['area'])
+
+    if include_masks:
+      run_len_encoding = mask.frPyObjects(
+          object_annotations['segmentation'], image_height, image_width)
+      binary_mask = mask.decode(run_len_encoding)
+      if not object_annotations['iscrowd']:
+        binary_mask = np.amax(binary_mask, axis=2)
+      pil_image = PIL.Image.fromarray(binary_mask)
+      output_io = io.BytesIO()
+      pil_image.save(output_io, format='PNG')
+      encoded_mask_png.append(output_io.getvalue())
+
+  feature_dict = {
+      'image/height': dataset_util.int64_feature(image_height),
+      'image/width': dataset_util.int64_feature(image_width),
+      'image/filename': dataset_util.bytes_feature(
+          filename.encode('utf8')),
+      'image/source_id': dataset_util.bytes_feature(
+          filename.encode('utf8')),
+      'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
+      'image/encoded': dataset_util.bytes_feature(encoded_jpg),
+      'image/format': dataset_util.bytes_feature('jpeg'.encode('utf8')),
+      'image/object/bbox/xmin': dataset_util.float_list_feature(xmin),
+      'image/object/bbox/xmax': dataset_util.float_list_feature(xmax),
+      'image/object/bbox/ymin': dataset_util.float_list_feature(ymin),
+      'image/object/bbox/ymax': dataset_util.float_list_feature(ymax),
+      'image/object/class/text': dataset_util.bytes_list_feature(
+          category_names),
+      'image/object/is_crowd': dataset_util.int64_list_feature(is_crowd),
+      'image/object/area': dataset_util.float_list_feature(area),
+  }
+  if include_masks:
+    feature_dict['image/object/mask'] = (
+        dataset_util.bytes_list_feature(encoded_mask_png))
+  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
+  return example, num_annotations_skipped
+
+
+def _create_tf_record_from_coco_annotations(
+    annotations_file, image_dir, output_path, include_masks):
+  """Loads COCO annotation json files and converts to tf.Record format.
+
+  Args:
+    annotations_file: JSON file containing bounding box annotations.
+    image_dir: Directory containing the image files.
+    output_path: Path to output tf.Record file.
+    include_masks: Whether to include instance segmentations masks
+      (PNG encoded) in the result. default: False.
+  """
+  with tf.gfile.GFile(annotations_file, 'r') as fid:
+    groundtruth_data = json.load(fid)
+    images = groundtruth_data['images']
+    category_index = label_map_util.create_category_index(
+        groundtruth_data['categories'])
+
+    annotations_index = {}
+    if 'annotations' in groundtruth_data:
+      tf.logging.info(
+          'Found groundtruth annotations. Building annotations index.')
+      for annotation in groundtruth_data['annotations']:
+        image_id = annotation['image_id']
+        if image_id not in annotations_index:
+          annotations_index[image_id] = []
+        annotations_index[image_id].append(annotation)
+    missing_annotation_count = 0
+    for image in images:
+      image_id = image['id']
+      if image_id not in annotations_index:
+        missing_annotation_count += 1
+        annotations_index[image_id] = []
+    tf.logging.info('%d images are missing annotations.',
+                    missing_annotation_count)
+
+    tf.logging.info('writing to output path: %s', output_path)
+    writer = tf.python_io.TFRecordWriter(output_path)
+    total_num_annotations_skipped = 0
+    for idx, image in enumerate(images):
+      if idx % 100 == 0:
+        tf.logging.info('On image %d of %d', idx, len(images))
+      annotations_list = annotations_index[image['id']]
+      tf_example, num_annotations_skipped = create_tf_example(
+          image, annotations_list, image_dir, category_index, include_masks)
+      total_num_annotations_skipped += num_annotations_skipped
+      writer.write(tf_example.SerializeToString())
+    writer.close()
+    tf.logging.info('Finished writing, skipped %d annotations.',
+                    total_num_annotations_skipped)
+
+
+def main(_):
+  assert FLAGS.train_image_dir, '`train_image_dir` missing.'
+  assert FLAGS.val_image_dir, '`val_image_dir` missing.'
+  assert FLAGS.test_image_dir, '`test_image_dir` missing.'
+  assert FLAGS.train_annotations_file, '`train_annotations_file` missing.'
+  assert FLAGS.val_annotations_file, '`val_annotations_file` missing.'
+  assert FLAGS.testdev_annotations_file, '`testdev_annotations_file` missing.'
+
+  if not tf.gfile.IsDirectory(FLAGS.output_dir):
+    tf.gfile.MakeDirs(FLAGS.output_dir)
+  train_output_path = os.path.join(FLAGS.output_dir, 'coco_train.record')
+  val_output_path = os.path.join(FLAGS.output_dir, 'coco_val.record')
+  testdev_output_path = os.path.join(FLAGS.output_dir, 'coco_testdev.record')
+
+  _create_tf_record_from_coco_annotations(
+      FLAGS.train_annotations_file,
+      FLAGS.train_image_dir,
+      train_output_path,
+      FLAGS.include_masks)
+  _create_tf_record_from_coco_annotations(
+      FLAGS.val_annotations_file,
+      FLAGS.val_image_dir,
+      val_output_path,
+      FLAGS.include_masks)
+  _create_tf_record_from_coco_annotations(
+      FLAGS.testdev_annotations_file,
+      FLAGS.test_image_dir,
+      testdev_output_path,
+      FLAGS.include_masks)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record_test.py b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
new file mode 100644
index 00000000..23bcb14d
--- /dev/null
+++ b/research/object_detection/dataset_tools/create_coco_tf_record_test.py
@@ -0,0 +1,187 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Test for create_coco_tf_record.py."""
+
+import io
+import os
+
+import numpy as np
+import PIL.Image
+import tensorflow as tf
+
+from object_detection.dataset_tools import create_coco_tf_record
+
+
+class CreateCocoTFRecordTest(tf.test.TestCase):
+
+  def _assertProtoEqual(self, proto_field, expectation):
+    """Helper function to assert if a proto field equals some value.
+
+    Args:
+      proto_field: The protobuf field to compare.
+      expectation: The expected value of the protobuf field.
+    """
+    proto_list = [p for p in proto_field]
+    self.assertListEqual(proto_list, expectation)
+
+  def test_create_tf_example(self):
+    image_file_name = 'tmp_image.jpg'
+    image_data = np.random.rand(256, 256, 3)
+    tmp_dir = self.get_temp_dir()
+    save_path = os.path.join(tmp_dir, image_file_name)
+    image = PIL.Image.fromarray(image_data, 'RGB')
+    image.save(save_path)
+
+    image = {
+        'file_name': image_file_name,
+        'height': 256,
+        'width': 256,
+        'id': 11,
+    }
+
+    annotations_list = [
+        {
+            'area': .5,
+            'iscrowd': False,
+            'image_id': 11,
+            'bbox': [64, 64, 128, 128],
+            'category_id': 2,
+            'id': 1000,
+        }
+    ]
+
+    image_dir = tmp_dir
+    category_index = {
+        1: {'name': 'dog', 'id': 1},
+        2: {'name': 'cat', 'id': 2},
+        3: {'name': 'human', 'id': 3}
+    }
+
+    example, num_annotations_skipped = create_coco_tf_record.create_tf_example(
+        image, annotations_list, image_dir, category_index)
+
+    self.assertEqual(num_annotations_skipped, 0)
+    self._assertProtoEqual(
+        example.features.feature['image/height'].int64_list.value, [256])
+    self._assertProtoEqual(
+        example.features.feature['image/width'].int64_list.value, [256])
+    self._assertProtoEqual(
+        example.features.feature['image/filename'].bytes_list.value,
+        [image_file_name])
+    self._assertProtoEqual(
+        example.features.feature['image/source_id'].bytes_list.value,
+        [image_file_name])
+    self._assertProtoEqual(
+        example.features.feature['image/format'].bytes_list.value, ['jpeg'])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmin'].float_list.value,
+        [0.25])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymin'].float_list.value,
+        [0.25])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmax'].float_list.value,
+        [0.75])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymax'].float_list.value,
+        [0.75])
+    self._assertProtoEqual(
+        example.features.feature['image/object/class/text'].bytes_list.value,
+        ['cat'])
+
+  def test_create_tf_example_with_instance_masks(self):
+    image_file_name = 'tmp_image.jpg'
+    image_data = np.random.rand(8, 8, 3)
+    tmp_dir = self.get_temp_dir()
+    save_path = os.path.join(tmp_dir, image_file_name)
+    image = PIL.Image.fromarray(image_data, 'RGB')
+    image.save(save_path)
+
+    image = {
+        'file_name': image_file_name,
+        'height': 8,
+        'width': 8,
+        'id': 11,
+    }
+
+    annotations_list = [
+        {
+            'area': .5,
+            'iscrowd': False,
+            'image_id': 11,
+            'bbox': [0, 0, 8, 8],
+            'segmentation': [[4, 0, 0, 0, 0, 4],
+                             [8, 4, 4, 8, 8, 8]],
+            'category_id': 1,
+            'id': 1000,
+        }
+    ]
+
+    image_dir = tmp_dir
+    category_index = {
+        1: {'name': 'dog', 'id': 1},
+    }
+
+    example, num_annotations_skipped = create_coco_tf_record.create_tf_example(
+        image, annotations_list, image_dir, category_index, include_masks=True)
+
+    self.assertEqual(num_annotations_skipped, 0)
+    self._assertProtoEqual(
+        example.features.feature['image/height'].int64_list.value, [8])
+    self._assertProtoEqual(
+        example.features.feature['image/width'].int64_list.value, [8])
+    self._assertProtoEqual(
+        example.features.feature['image/filename'].bytes_list.value,
+        [image_file_name])
+    self._assertProtoEqual(
+        example.features.feature['image/source_id'].bytes_list.value,
+        [image_file_name])
+    self._assertProtoEqual(
+        example.features.feature['image/format'].bytes_list.value, ['jpeg'])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmin'].float_list.value,
+        [0])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymin'].float_list.value,
+        [0])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/xmax'].float_list.value,
+        [1])
+    self._assertProtoEqual(
+        example.features.feature['image/object/bbox/ymax'].float_list.value,
+        [1])
+    self._assertProtoEqual(
+        example.features.feature['image/object/class/text'].bytes_list.value,
+        ['dog'])
+    encoded_mask_pngs = [io.BytesIO(encoded_masks)
+                         for encoded_masks in example.features.feature[
+                             'image/object/mask'].bytes_list.value]
+    pil_masks = [np.array(PIL.Image.open(encoded_mask_png))
+                 for encoded_mask_png in encoded_mask_pngs]
+    self.assertTrue(len(pil_masks) == 1)
+    self.assertAllEqual(pil_masks[0],
+                        [[1, 1, 1, 0, 0, 0, 0, 0],
+                         [1, 1, 0, 0, 0, 0, 0, 0],
+                         [1, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 1],
+                         [0, 0, 0, 0, 0, 0, 1, 1],
+                         [0, 0, 0, 0, 0, 1, 1, 1],
+                         [0, 0, 0, 0, 1, 1, 1, 1]])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record.py b/research/object_detection/dataset_tools/create_kitti_tf_record.py
index 2bf2ff34..25af7cc0 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record.py
@@ -120,7 +120,7 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
 
     # Filter all bounding boxes of this frame that are of a legal class, and
     # don't overlap with a dontcare region.
-    # TODO(talremez) filter out targets that are truncated or heavily occluded.
+    # TODO filter out targets that are truncated or heavily occluded.
     annotation_for_image = filter_annotations(img_anno, classes_to_use)
 
     example = prepare_example(image_path, annotation_for_image, label_map_dict)
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
index 22f27f1a..37ac4b8b 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record_test.py
@@ -24,7 +24,7 @@ import tensorflow as tf
 from object_detection.dataset_tools import create_kitti_tf_record
 
 
-class DictToTFExampleTest(tf.test.TestCase):
+class CreateKittiTFRecordTest(tf.test.TestCase):
 
   def _assertProtoEqual(self, proto_field, expectation):
     """Helper function to assert if a proto field equals some value.
diff --git a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
index a1c31fac..66929bd4 100644
--- a/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
+++ b/research/object_detection/dataset_tools/create_pascal_tf_record_test.py
@@ -24,7 +24,7 @@ import tensorflow as tf
 from object_detection.dataset_tools import create_pascal_tf_record
 
 
-class DictToTFExampleTest(tf.test.TestCase):
+class CreatePascalTFRecordTest(tf.test.TestCase):
 
   def _assertProtoEqual(self, proto_field, expectation):
     """Helper function to assert if a proto field equals some value.
diff --git a/research/object_detection/dataset_tools/create_pet_tf_record.py b/research/object_detection/dataset_tools/create_pet_tf_record.py
index a8663297..ed339e3a 100644
--- a/research/object_detection/dataset_tools/create_pet_tf_record.py
+++ b/research/object_detection/dataset_tools/create_pet_tf_record.py
@@ -50,6 +50,8 @@ flags.DEFINE_boolean('faces_only', True, 'If True, generates bounding boxes '
                      'for pet faces.  Otherwise generates bounding boxes (as '
                      'well as segmentations for full pet bodies).  Note that '
                      'in the latter case, the resulting files are much larger.')
+flags.DEFINE_string('mask_type', 'png', 'How to represent instance '
+                    'segmentation masks. Options are "png" or "numerical".')
 FLAGS = flags.FLAGS
 
 
@@ -72,7 +74,8 @@ def dict_to_tf_example(data,
                        label_map_dict,
                        image_subdirectory,
                        ignore_difficult_instances=False,
-                       faces_only=True):
+                       faces_only=True,
+                       mask_type='png'):
   """Convert XML derived dict to tf.Example proto.
 
   Notice that this function normalizes the bounding box coordinates provided
@@ -89,6 +92,8 @@ def dict_to_tf_example(data,
       dataset  (default: False).
     faces_only: If True, generates bounding boxes for pet faces.  Otherwise
       generates bounding boxes (as well as segmentations for full pet bodies).
+    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to
+      smaller file sizes.
 
   Returns:
     example: The converted tf.Example.
@@ -158,7 +163,7 @@ def dict_to_tf_example(data,
     truncated.append(int(obj['truncated']))
     poses.append(obj['pose'].encode('utf8'))
     if not faces_only:
-      mask_remapped = mask_np != 2
+      mask_remapped = (mask_np != 2).astype(np.uint8)
       masks.append(mask_remapped)
 
   feature_dict = {
@@ -182,10 +187,20 @@ def dict_to_tf_example(data,
       'image/object/view': dataset_util.bytes_list_feature(poses),
   }
   if not faces_only:
-    mask_stack = np.stack(masks).astype(np.float32)
-    masks_flattened = np.reshape(mask_stack, [-1])
-    feature_dict['image/object/mask'] = (
-        dataset_util.float_list_feature(masks_flattened.tolist()))
+    if mask_type == 'numerical':
+      mask_stack = np.stack(masks).astype(np.float32)
+      masks_flattened = np.reshape(mask_stack, [-1])
+      feature_dict['image/object/mask'] = (
+          dataset_util.float_list_feature(masks_flattened.tolist()))
+    elif mask_type == 'png':
+      encoded_mask_png_list = []
+      for mask in masks:
+        img = PIL.Image.fromarray(mask)
+        output = io.BytesIO()
+        img.save(output, format='PNG')
+        encoded_mask_png_list.append(output.getvalue())
+      feature_dict['image/object/mask'] = (
+          dataset_util.bytes_list_feature(encoded_mask_png_list))
 
   example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
   return example
@@ -196,7 +211,8 @@ def create_tf_record(output_filename,
                      annotations_dir,
                      image_dir,
                      examples,
-                     faces_only=True):
+                     faces_only=True,
+                     mask_type='png'):
   """Creates a TFRecord file from examples.
 
   Args:
@@ -207,6 +223,8 @@ def create_tf_record(output_filename,
     examples: Examples to parse and save to tf record.
     faces_only: If True, generates bounding boxes for pet faces.  Otherwise
       generates bounding boxes (as well as segmentations for full pet bodies).
+    mask_type: 'numerical' or 'png'. 'png' is recommended because it leads to
+      smaller file sizes.
   """
   writer = tf.python_io.TFRecordWriter(output_filename)
   for idx, example in enumerate(examples):
@@ -225,7 +243,12 @@ def create_tf_record(output_filename,
 
     try:
       tf_example = dict_to_tf_example(
-          data, mask_path, label_map_dict, image_dir, faces_only=faces_only)
+          data,
+          mask_path,
+          label_map_dict,
+          image_dir,
+          faces_only=faces_only,
+          mask_type=mask_type)
       writer.write(tf_example.SerializeToString())
     except ValueError:
       logging.warning('Invalid example: %s, ignoring.', xml_path)
@@ -233,7 +256,7 @@ def create_tf_record(output_filename,
   writer.close()
 
 
-# TODO(derekjchow): Add test for pet/PASCAL main files.
+# TODO: Add test for pet/PASCAL main files.
 def main(_):
   data_dir = FLAGS.data_dir
   label_map_dict = label_map_util.get_label_map_dict(FLAGS.label_map_path)
@@ -262,10 +285,22 @@ def main(_):
                                      'pet_train_with_masks.record')
     val_output_path = os.path.join(FLAGS.output_dir,
                                    'pet_val_with_masks.record')
-  create_tf_record(train_output_path, label_map_dict, annotations_dir,
-                   image_dir, train_examples, faces_only=FLAGS.faces_only)
-  create_tf_record(val_output_path, label_map_dict, annotations_dir,
-                   image_dir, val_examples, faces_only=FLAGS.faces_only)
+  create_tf_record(
+      train_output_path,
+      label_map_dict,
+      annotations_dir,
+      image_dir,
+      train_examples,
+      faces_only=FLAGS.faces_only,
+      mask_type=FLAGS.mask_type)
+  create_tf_record(
+      val_output_path,
+      label_map_dict,
+      annotations_dir,
+      image_dir,
+      val_examples,
+      faces_only=FLAGS.faces_only,
+      mask_type=FLAGS.mask_type)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/dataset_tools/download_and_preprocess_mscoco.sh b/research/object_detection/dataset_tools/download_and_preprocess_mscoco.sh
new file mode 100644
index 00000000..2b687736
--- /dev/null
+++ b/research/object_detection/dataset_tools/download_and_preprocess_mscoco.sh
@@ -0,0 +1,106 @@
+#!/bin/bash
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+# Script to download and preprocess the MSCOCO data set for detection.
+#
+# The outputs of this script are TFRecord files containing serialized
+# tf.Example protocol buffers. See create_coco_tf_record.py for details of how
+# the tf.Example protocol buffers are constructed and see
+# http://cocodataset.org/#overview for an overview of the dataset.
+#
+# usage:
+#  bash object_detection/dataset_tools/download_and_preprocess_mscoco.sh \
+#    /tmp/mscoco
+set -e
+
+if [ -z "$1" ]; then
+  echo "usage download_and_preprocess_mscoco.sh [data dir]"
+  exit
+fi
+
+if [ "$(uname)" == "Darwin" ]; then
+  UNZIP="tar -xf"
+else
+  UNZIP="unzip -nq"
+fi
+
+# Create the output directories.
+OUTPUT_DIR="${1%/}"
+SCRATCH_DIR="${OUTPUT_DIR}/raw-data"
+mkdir -p "${OUTPUT_DIR}"
+mkdir -p "${SCRATCH_DIR}"
+CURRENT_DIR=$(pwd)
+
+# Helper function to download and unpack a .zip file.
+function download_and_unzip() {
+  local BASE_URL=${1}
+  local FILENAME=${2}
+
+  if [ ! -f ${FILENAME} ]; then
+    echo "Downloading ${FILENAME} to $(pwd)"
+    wget -nd -c "${BASE_URL}/${FILENAME}"
+  else
+    echo "Skipping download of ${FILENAME}"
+  fi
+  echo "Unzipping ${FILENAME}"
+  ${UNZIP} ${FILENAME}
+}
+
+cd ${SCRATCH_DIR}
+
+# Download the images.
+BASE_IMAGE_URL="http://images.cocodataset.org/zips"
+
+# TRAIN_IMAGE_FILE="train2017.zip"
+download_and_unzip ${BASE_IMAGE_URL} ${TRAIN_IMAGE_FILE}
+TRAIN_IMAGE_DIR="${SCRATCH_DIR}/train2017"
+
+VAL_IMAGE_FILE="val2017.zip"
+download_and_unzip ${BASE_IMAGE_URL} ${VAL_IMAGE_FILE}
+VAL_IMAGE_DIR="${SCRATCH_DIR}/val2017"
+
+TEST_IMAGE_FILE="test2017.zip"
+download_and_unzip ${BASE_IMAGE_URL} ${TEST_IMAGE_FILE}
+TEST_IMAGE_DIR="${SCRATCH_DIR}/test2017"
+
+# Download the annotations.
+BASE_INSTANCES_URL="http://images.cocodataset.org/annotations"
+INSTANCES_FILE="annotations_trainval2017.zip"
+download_and_unzip ${BASE_INSTANCES_URL} ${INSTANCES_FILE}
+
+TRAIN_ANNOTATIONS_FILE="${SCRATCH_DIR}/annotations/instances_train2017.json"
+VAL_ANNOTATIONS_FILE="${SCRATCH_DIR}/annotations/instances_val2017.json"
+
+# Download the test image info.
+BASE_IMAGE_INFO_URL="http://images.cocodataset.org/annotations"
+IMAGE_INFO_FILE="image_info_test2017.zip"
+download_and_unzip ${BASE_IMAGE_INFO_URL} ${IMAGE_INFO_FILE}
+
+TESTDEV_ANNOTATIONS_FILE="${SCRATCH_DIR}/annotations/image_info_test-dev2017.json"
+
+# # Build TFRecords of the image data.
+cd "${CURRENT_DIR}"
+python object_detection/dataset_tools/create_coco_tf_record.py \
+  --logtostderr \
+  --include_masks \
+  --train_image_dir="${TRAIN_IMAGE_DIR}" \
+  --val_image_dir="${VAL_IMAGE_DIR}" \
+  --test_image_dir="${TEST_IMAGE_DIR}" \
+  --train_annotations_file="${TRAIN_ANNOTATIONS_FILE}" \
+  --val_annotations_file="${VAL_ANNOTATIONS_FILE}" \
+  --testdev_annotations_file="${TESTDEV_ANNOTATIONS_FILE}" \
+  --output_dir="${OUTPUT_DIR}"
+
diff --git a/research/object_detection/dataset_tools/oid_tfrecord_creation.py b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
index d2180890..1bc41c0b 100644
--- a/research/object_detection/dataset_tools/oid_tfrecord_creation.py
+++ b/research/object_detection/dataset_tools/oid_tfrecord_creation.py
@@ -18,7 +18,6 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
-from six.moves import xrange
 import tensorflow as tf
 
 from object_detection.core import standard_fields
diff --git a/research/object_detection/eval.py b/research/object_detection/eval.py
index 175ac1ee..66f3d561 100644
--- a/research/object_detection/eval.py
+++ b/research/object_detection/eval.py
@@ -48,9 +48,10 @@ import os
 import tensorflow as tf
 
 from object_detection import evaluator
-from object_detection.builders import input_reader_builder
+from object_detection.builders import dataset_builder
 from object_detection.builders import model_builder
 from object_detection.utils import config_util
+from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 
 
@@ -103,19 +104,20 @@ def main(unused_argv):
 
   model_config = configs['model']
   eval_config = configs['eval_config']
+  input_config = configs['eval_input_config']
   if FLAGS.eval_training_data:
     input_config = configs['train_input_config']
-  else:
-    input_config = configs['eval_input_config']
 
   model_fn = functools.partial(
       model_builder.build,
       model_config=model_config,
       is_training=False)
 
-  create_input_dict_fn = functools.partial(
-      input_reader_builder.build,
-      input_config)
+  def get_next(config):
+    return dataset_util.make_initializable_iterator(
+        dataset_builder.build(config)).get_next()
+
+  create_input_dict_fn = functools.partial(get_next, input_config)
 
   label_map = label_map_util.load_labelmap(input_config.label_map_path)
   max_num_classes = max([item.id for item in label_map.item])
diff --git a/research/object_detection/eval_util.py b/research/object_detection/eval_util.py
index 6a37be76..96e30870 100644
--- a/research/object_detection/eval_util.py
+++ b/research/object_detection/eval_util.py
@@ -40,14 +40,13 @@ def write_metrics(metrics, global_step, summary_dir):
     summary_dir: Directory to write tensorflow summaries to.
   """
   logging.info('Writing metrics to tf summary.')
-  summary_writer = tf.summary.FileWriter(summary_dir)
+  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
   for key in sorted(metrics):
     summary = tf.Summary(value=[
         tf.Summary.Value(tag=key, simple_value=metrics[key]),
     ])
     summary_writer.add_summary(summary, global_step)
     logging.info('%s: %f', key, metrics[key])
-  summary_writer.close()
   logging.info('Metrics written to tf summary.')
 
 
@@ -60,8 +59,12 @@ def visualize_detection_results(result_dict,
                                 export_dir='',
                                 agnostic_mode=False,
                                 show_groundtruth=False,
+                                groundtruth_box_visualization_color='black',
                                 min_score_thresh=.5,
-                                max_num_predictions=20):
+                                max_num_predictions=20,
+                                skip_scores=False,
+                                skip_labels=False,
+                                keep_image_id_for_visualization_export=False):
   """Visualizes detection results and writes visualizations to image summaries.
 
   This function visualizes an image with its detected bounding boxes and writes
@@ -99,44 +102,57 @@ def visualize_detection_results(result_dict,
       class-agnostic mode or not.
     show_groundtruth: boolean (default: False) controlling whether to show
       groundtruth boxes in addition to detected boxes
+    groundtruth_box_visualization_color: box color for visualizing groundtruth
+      boxes
     min_score_thresh: minimum score threshold for a box to be visualized
     max_num_predictions: maximum number of detections to visualize
+    skip_scores: whether to skip score when drawing a single detection
+    skip_labels: whether to skip label when drawing a single detection
+    keep_image_id_for_visualization_export: whether to keep image identifier in
+      filename when exported to export_dir
   Raises:
     ValueError: if result_dict does not contain the expected keys (i.e.,
       'original_image', 'detection_boxes', 'detection_scores',
       'detection_classes')
   """
+  detection_fields = fields.DetectionResultFields
+  input_fields = fields.InputDataFields
   if not set([
-      'original_image', 'detection_boxes', 'detection_scores',
-      'detection_classes'
+      input_fields.original_image,
+      detection_fields.detection_boxes,
+      detection_fields.detection_scores,
+      detection_fields.detection_classes,
   ]).issubset(set(result_dict.keys())):
     raise ValueError('result_dict does not contain all expected keys.')
-  if show_groundtruth and 'groundtruth_boxes' not in result_dict:
+  if show_groundtruth and input_fields.groundtruth_boxes not in result_dict:
     raise ValueError('If show_groundtruth is enabled, result_dict must contain '
                      'groundtruth_boxes.')
   logging.info('Creating detection visualizations.')
   category_index = label_map_util.create_category_index(categories)
 
-  image = np.squeeze(result_dict['original_image'], axis=0)
-  detection_boxes = result_dict['detection_boxes']
-  detection_scores = result_dict['detection_scores']
-  detection_classes = np.int32((result_dict['detection_classes']))
-  detection_keypoints = result_dict.get('detection_keypoints', None)
-  detection_masks = result_dict.get('detection_masks', None)
+  image = np.squeeze(result_dict[input_fields.original_image], axis=0)
+  detection_boxes = result_dict[detection_fields.detection_boxes]
+  detection_scores = result_dict[detection_fields.detection_scores]
+  detection_classes = np.int32((result_dict[
+      detection_fields.detection_classes]))
+  detection_keypoints = result_dict.get(detection_fields.detection_keypoints)
+  detection_masks = result_dict.get(detection_fields.detection_masks)
+  detection_boundaries = result_dict.get(detection_fields.detection_boundaries)
 
   # Plot groundtruth underneath detections
   if show_groundtruth:
-    groundtruth_boxes = result_dict['groundtruth_boxes']
-    groundtruth_keypoints = result_dict.get('groundtruth_keypoints', None)
+    groundtruth_boxes = result_dict[input_fields.groundtruth_boxes]
+    groundtruth_keypoints = result_dict.get(input_fields.groundtruth_keypoints)
     vis_utils.visualize_boxes_and_labels_on_image_array(
-        image,
-        groundtruth_boxes,
-        None,
-        None,
-        category_index,
+        image=image,
+        boxes=groundtruth_boxes,
+        classes=None,
+        scores=None,
+        category_index=category_index,
         keypoints=groundtruth_keypoints,
         use_normalized_coordinates=False,
-        max_boxes_to_draw=None)
+        max_boxes_to_draw=None,
+        groundtruth_box_visualization_color=groundtruth_box_visualization_color)
   vis_utils.visualize_boxes_and_labels_on_image_array(
       image,
       detection_boxes,
@@ -144,14 +160,23 @@ def visualize_detection_results(result_dict,
       detection_scores,
       category_index,
       instance_masks=detection_masks,
+      instance_boundaries=detection_boundaries,
       keypoints=detection_keypoints,
       use_normalized_coordinates=False,
       max_boxes_to_draw=max_num_predictions,
       min_score_thresh=min_score_thresh,
-      agnostic_mode=agnostic_mode)
+      agnostic_mode=agnostic_mode,
+      skip_scores=skip_scores,
+      skip_labels=skip_labels)
 
   if export_dir:
-    export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
+    if keep_image_id_for_visualization_export and result_dict[fields.
+                                                              InputDataFields()
+                                                              .key]:
+      export_path = os.path.join(export_dir, 'export-{}-{}.png'.format(
+          tag, result_dict[fields.InputDataFields().key]))
+    else:
+      export_path = os.path.join(export_dir, 'export-{}.png'.format(tag))
     vis_utils.save_image_array_as_png(image, export_path)
 
   summary = tf.Summary(value=[
@@ -161,9 +186,8 @@ def visualize_detection_results(result_dict,
               encoded_image_string=vis_utils.encode_image_array_as_png_str(
                   image)))
   ])
-  summary_writer = tf.summary.FileWriter(summary_dir)
+  summary_writer = tf.summary.FileWriterCache.get(summary_dir)
   summary_writer.add_summary(summary, global_step)
-  summary_writer.close()
 
   logging.info('Detection visualizations written to summary with tag %s.', tag)
 
@@ -260,8 +284,10 @@ def _run_checkpoint_once(tensor_dict,
             result_dict = {}
         else:
           result_dict = batch_processor(tensor_dict, sess, batch, counters)
+        if not result_dict:
+          continue
         for evaluator in evaluators:
-          # TODO: Use image_id tensor once we fix the input data
+          # TODO(b/65130867): Use image_id tensor once we fix the input data
           # decoders to return correct image_id.
           # TODO: result_dict contains batches of images, while
           # add_single_ground_truth_image_info expects a single image. Fix
@@ -422,9 +448,9 @@ def result_dict_for_single_example(image,
         (Optional).
     class_agnostic: Boolean indicating whether the detections are class-agnostic
       (i.e. binary). Default False.
-    scale_to_absolute: Boolean indicating whether boxes, masks, keypoints should
-      be scaled to absolute coordinates. Note that for IoU based evaluations,
-      it does not matter whether boxes are expressed in absolute or relative
+    scale_to_absolute: Boolean indicating whether boxes and keypoints should be
+      scaled to absolute coordinates. Note that for IoU based evaluations, it
+      does not matter whether boxes are expressed in absolute or relative
       coordinates. Default False.
 
   Returns:
@@ -436,8 +462,8 @@ def result_dict_for_single_example(image,
       `scale_to_absolute`.
     'detection_scores': [max_detections] float32 tensor of scores.
     'detection_classes': [max_detections] int64 tensor of 1-indexed classes.
-    'detection_masks': [max_detections, None, None] float32 tensor of binarized
-      masks. (Only present if available in `detections`)
+    'detection_masks': [max_detections, H, W] float32 tensor of binarized
+      masks, reframed to full image masks.
     'groundtruth_boxes': [num_boxes, 4] float32 tensor of boxes, in
       normalized or absolute coordinates, depending on the value of
       `scale_to_absolute`. (Optional)
@@ -481,15 +507,13 @@ def result_dict_for_single_example(image,
 
   if detection_fields.detection_masks in detections:
     detection_masks = detections[detection_fields.detection_masks][0]
-    output_dict[detection_fields.detection_masks] = detection_masks
-    if scale_to_absolute:
-      # TODO: This should be done in model's postprocess
-      # function ideally.
-      detection_masks_reframed = ops.reframe_box_masks_to_image_masks(
-          detection_masks, detection_boxes, image_shape[1], image_shape[2])
-      detection_masks_reframed = tf.to_float(
-          tf.greater(detection_masks_reframed, 0.5))
-      output_dict[detection_fields.detection_masks] = detection_masks_reframed
+    # TODO: This should be done in model's postprocess
+    # function ideally.
+    detection_masks_reframed = ops.reframe_box_masks_to_image_masks(
+        detection_masks, detection_boxes, image_shape[1], image_shape[2])
+    detection_masks_reframed = tf.cast(
+        tf.greater(detection_masks_reframed, 0.5), tf.uint8)
+    output_dict[detection_fields.detection_masks] = detection_masks_reframed
   if detection_fields.detection_keypoints in detections:
     detection_keypoints = detections[detection_fields.detection_keypoints][0]
     output_dict[detection_fields.detection_keypoints] = detection_keypoints
@@ -500,6 +524,9 @@ def result_dict_for_single_example(image,
           absolute_detection_keypoints)
 
   if groundtruth:
+    if input_data_fields.groundtruth_instance_masks in groundtruth:
+      groundtruth[input_data_fields.groundtruth_instance_masks] = tf.cast(
+          groundtruth[input_data_fields.groundtruth_instance_masks], tf.uint8)
     output_dict.update(groundtruth)
     if scale_to_absolute:
       groundtruth_boxes = groundtruth[input_data_fields.groundtruth_boxes]
diff --git a/research/object_detection/evaluator.py b/research/object_detection/evaluator.py
index 74722d00..7d55d9f5 100644
--- a/research/object_detection/evaluator.py
+++ b/research/object_detection/evaluator.py
@@ -30,14 +30,20 @@ from object_detection.utils import object_detection_evaluation
 # in the dictionary must implement
 # utils.object_detection_evaluation.DetectionEvaluator interface.
 EVAL_METRICS_CLASS_DICT = {
-    'pascal_voc_metrics':
+    'pascal_voc_detection_metrics':
         object_detection_evaluation.PascalDetectionEvaluator,
-    'weighted_pascal_voc_metrics':
+    'weighted_pascal_voc_detection_metrics':
         object_detection_evaluation.WeightedPascalDetectionEvaluator,
-    'open_images_metrics':
+    'pascal_voc_instance_segmentation_metrics':
+        object_detection_evaluation.PascalInstanceSegmentationEvaluator,
+    'weighted_pascal_voc_instance_segmentation_metrics':
+        object_detection_evaluation.WeightedPascalInstanceSegmentationEvaluator,
+    'open_images_detection_metrics':
         object_detection_evaluation.OpenImagesDetectionEvaluator
 }
 
+EVAL_DEFAULT_METRIC = 'pascal_voc_detection_metrics'
+
 
 def _extract_prediction_tensors(model,
                                 create_input_dict_fn,
@@ -56,9 +62,10 @@ def _extract_prediction_tensors(model,
   prefetch_queue = prefetcher.prefetch(input_dict, capacity=500)
   input_dict = prefetch_queue.dequeue()
   original_image = tf.expand_dims(input_dict[fields.InputDataFields.image], 0)
-  preprocessed_image = model.preprocess(tf.to_float(original_image))
-  prediction_dict = model.predict(preprocessed_image)
-  detections = model.postprocess(prediction_dict)
+  preprocessed_image, true_image_shapes = model.preprocess(
+      tf.to_float(original_image))
+  prediction_dict = model.predict(preprocessed_image, true_image_shapes)
+  detections = model.postprocess(prediction_dict, true_image_shapes)
 
   groundtruth = None
   if not ignore_groundtruth:
@@ -103,17 +110,21 @@ def get_evaluators(eval_config, categories):
   Raises:
     ValueError: if metric is not in the metric class dictionary.
   """
-  eval_metric_fn_key = eval_config.metrics_set
-  if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:
-    raise ValueError('Metric not found: {}'.format(eval_metric_fn_key))
-  return [
-      EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](
-          categories=categories)
-  ]
+  eval_metric_fn_keys = eval_config.metrics_set
+  if not eval_metric_fn_keys:
+    eval_metric_fn_keys = [EVAL_DEFAULT_METRIC]
+  evaluators_list = []
+  for eval_metric_fn_key in eval_metric_fn_keys:
+    if eval_metric_fn_key not in EVAL_METRICS_CLASS_DICT:
+      raise ValueError('Metric not found: {}'.format(eval_metric_fn_key))
+    else:
+      evaluators_list.append(
+          EVAL_METRICS_CLASS_DICT[eval_metric_fn_key](categories=categories))
+  return evaluators_list
 
 
 def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
-             checkpoint_dir, eval_dir):
+             checkpoint_dir, eval_dir, graph_hook_fn=None):
   """Evaluation function for detection models.
 
   Args:
@@ -124,6 +135,10 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
                 have an integer 'id' field and string 'name' field.
     checkpoint_dir: directory to load the checkpoints to evaluate from.
     eval_dir: directory to write evaluation metrics summary to.
+    graph_hook_fn: Optional function that is called after the training graph is
+      completely built. This is helpful to perform additional changes to the
+      training graph such as optimizing batchnorm. The function should modify
+      the default graph.
 
   Returns:
     metrics: A dictionary containing metric names and values from the latest
@@ -177,12 +192,23 @@ def evaluate(create_input_dict_fn, create_model_fn, eval_config, categories,
           categories=categories,
           summary_dir=eval_dir,
           export_dir=eval_config.visualization_export_dir,
-          show_groundtruth=eval_config.visualization_export_dir)
+          show_groundtruth=eval_config.visualize_groundtruth_boxes,
+          groundtruth_box_visualization_color=eval_config.
+          groundtruth_box_visualization_color,
+          min_score_thresh=eval_config.min_score_threshold,
+          max_num_predictions=eval_config.max_num_boxes_to_visualize,
+          skip_scores=eval_config.skip_scores,
+          skip_labels=eval_config.skip_labels,
+          keep_image_id_for_visualization_export=eval_config.
+          keep_image_id_for_visualization_export)
     return result_dict
 
   variables_to_restore = tf.global_variables()
   global_step = tf.train.get_or_create_global_step()
   variables_to_restore.append(global_step)
+
+  if graph_hook_fn: graph_hook_fn()
+
   if eval_config.use_moving_averages:
     variable_averages = tf.train.ExponentialMovingAverage(0.0)
     variables_to_restore = variable_averages.variables_to_restore()
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 279d1d16..5d0699f1 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -65,6 +65,31 @@ with contents:
  - model.ckpt.meta
  - frozen_inference_graph.pb
  + saved_model (a directory)
+
+Config overrides (see the `config_override` flag) are text protobufs
+(also of type pipeline_pb2.TrainEvalPipelineConfig) which are used to override
+certain fields in the provided pipeline_config_path.  These are useful for
+making small changes to the inference graph that differ from the training or
+eval config.
+
+Example Usage (in which we change the second stage post-processing score
+threshold to be 0.5):
+
+python export_inference_graph \
+    --input_type image_tensor \
+    --pipeline_config_path path/to/ssd_inception_v2.config \
+    --trained_checkpoint_prefix path/to/model.ckpt \
+    --output_directory path/to/exported_model_directory \
+    --config_override " \
+            model{ \
+              faster_rcnn { \
+                second_stage_post_processing { \
+                  batch_non_max_suppression { \
+                    score_threshold: 0.5 \
+                  } \
+                } \
+              } \
+            }"
 """
 import tensorflow as tf
 from google.protobuf import text_format
@@ -92,7 +117,9 @@ flags.DEFINE_string('trained_checkpoint_prefix', None,
                     'Path to trained checkpoint, typically of the form '
                     'path/to/model.ckpt')
 flags.DEFINE_string('output_directory', None, 'Path to write outputs.')
-
+flags.DEFINE_string('config_override', '',
+                    'pipeline_pb2.TrainEvalPipelineConfig '
+                    'text proto to override pipeline_config_path.')
 tf.app.flags.mark_flag_as_required('pipeline_config_path')
 tf.app.flags.mark_flag_as_required('trained_checkpoint_prefix')
 tf.app.flags.mark_flag_as_required('output_directory')
@@ -103,6 +130,7 @@ def main(_):
   pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
   with tf.gfile.GFile(FLAGS.pipeline_config_path, 'r') as f:
     text_format.Merge(f.read(), pipeline_config)
+  text_format.Merge(FLAGS.config_override, pipeline_config)
   if FLAGS.input_shape:
     input_shape = [
         int(dim) if dim != '-1' else None
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index ef8fe194..dac1f1d8 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -18,7 +18,7 @@ import logging
 import os
 import tempfile
 import tensorflow as tf
-from tensorflow.core.protobuf import rewriter_config_pb2
+from google.protobuf import text_format
 from tensorflow.python import pywrap_tensorflow
 from tensorflow.python.client import session
 from tensorflow.python.framework import graph_util
@@ -43,7 +43,6 @@ def freeze_graph_with_def_protos(
     filename_tensor_name,
     clear_devices,
     initializer_nodes,
-    optimize_graph=True,
     variable_names_blacklist=''):
   """Converts all variables in a graph and checkpoint into constants."""
   del restore_op_name, filename_tensor_name  # Unused by updated loading code.
@@ -65,20 +64,7 @@ def freeze_graph_with_def_protos(
 
   with tf.Graph().as_default():
     tf.import_graph_def(input_graph_def, name='')
-
-    if optimize_graph:
-      logging.info('Graph Rewriter optimizations enabled')
-      rewrite_options = rewriter_config_pb2.RewriterConfig(
-          layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)
-      rewrite_options.optimizers.append('pruning')
-      rewrite_options.optimizers.append('constfold')
-      rewrite_options.optimizers.append('layout')
-      graph_options = tf.GraphOptions(
-          rewrite_options=rewrite_options, infer_shapes=True)
-    else:
-      logging.info('Graph Rewriter optimizations disabled')
-      graph_options = tf.GraphOptions()
-    config = tf.ConfigProto(graph_options=graph_options)
+    config = tf.ConfigProto(graph_options=tf.GraphOptions())
     with session.Session(config=config) as sess:
       if input_saver_def:
         saver = saver_lib.Saver(saver_def=input_saver_def)
@@ -227,23 +213,31 @@ def _add_output_tensor_nodes(postprocessed_tensors,
   Returns:
     A tensor dict containing the added output tensor nodes.
   """
+  detection_fields = fields.DetectionResultFields
   label_id_offset = 1
-  boxes = postprocessed_tensors.get('detection_boxes')
-  scores = postprocessed_tensors.get('detection_scores')
-  classes = postprocessed_tensors.get('detection_classes') + label_id_offset
-  masks = postprocessed_tensors.get('detection_masks')
-  num_detections = postprocessed_tensors.get('num_detections')
+  boxes = postprocessed_tensors.get(detection_fields.detection_boxes)
+  scores = postprocessed_tensors.get(detection_fields.detection_scores)
+  classes = postprocessed_tensors.get(
+      detection_fields.detection_classes) + label_id_offset
+  masks = postprocessed_tensors.get(detection_fields.detection_masks)
+  num_detections = postprocessed_tensors.get(detection_fields.num_detections)
   outputs = {}
-  outputs['detection_boxes'] = tf.identity(boxes, name='detection_boxes')
-  outputs['detection_scores'] = tf.identity(scores, name='detection_scores')
-  outputs['detection_classes'] = tf.identity(classes, name='detection_classes')
-  outputs['num_detections'] = tf.identity(num_detections, name='num_detections')
+  outputs[detection_fields.detection_boxes] = tf.identity(
+      boxes, name=detection_fields.detection_boxes)
+  outputs[detection_fields.detection_scores] = tf.identity(
+      scores, name=detection_fields.detection_scores)
+  outputs[detection_fields.detection_classes] = tf.identity(
+      classes, name=detection_fields.detection_classes)
+  outputs[detection_fields.num_detections] = tf.identity(
+      num_detections, name=detection_fields.num_detections)
   if masks is not None:
-    outputs['detection_masks'] = tf.identity(masks, name='detection_masks')
+    outputs[detection_fields.detection_masks] = tf.identity(
+        masks, name=detection_fields.detection_masks)
   for output_key in outputs:
     tf.add_to_collection(output_collection_name, outputs[output_key])
   if masks is not None:
-    tf.add_to_collection(output_collection_name, outputs['detection_masks'])
+    tf.add_to_collection(output_collection_name,
+                         outputs[detection_fields.detection_masks])
   return outputs
 
 
@@ -328,8 +322,8 @@ def _export_inference_graph(input_type,
                             output_directory,
                             additional_output_tensor_names=None,
                             input_shape=None,
-                            optimize_graph=True,
-                            output_collection_name='inference_op'):
+                            output_collection_name='inference_op',
+                            graph_hook_fn=None):
   """Export helper."""
   tf.gfile.MakeDirs(output_directory)
   frozen_graph_path = os.path.join(output_directory,
@@ -348,14 +342,18 @@ def _export_inference_graph(input_type,
   placeholder_tensor, input_tensors = input_placeholder_fn_map[input_type](
       **placeholder_args)
   inputs = tf.to_float(input_tensors)
-  preprocessed_inputs = detection_model.preprocess(inputs)
-  output_tensors = detection_model.predict(preprocessed_inputs)
-  postprocessed_tensors = detection_model.postprocess(output_tensors)
+  preprocessed_inputs, true_image_shapes = detection_model.preprocess(inputs)
+  output_tensors = detection_model.predict(
+      preprocessed_inputs, true_image_shapes)
+  postprocessed_tensors = detection_model.postprocess(
+      output_tensors, true_image_shapes)
   outputs = _add_output_tensor_nodes(postprocessed_tensors,
                                      output_collection_name)
   # Add global step to the graph.
   slim.get_or_create_global_step()
 
+  if graph_hook_fn: graph_hook_fn()
+
   if use_moving_averages:
     temp_checkpoint_file = tempfile.NamedTemporaryFile()
     replace_variable_values_with_moving_averages(
@@ -387,7 +385,6 @@ def _export_inference_graph(input_type,
       restore_op_name='save/restore_all',
       filename_tensor_name='save/Const:0',
       clear_devices=True,
-      optimize_graph=optimize_graph,
       initializer_nodes='')
   _write_frozen_graph(frozen_graph_path, frozen_graph_def)
   _write_saved_model(saved_model_path, frozen_graph_def,
@@ -399,7 +396,6 @@ def export_inference_graph(input_type,
                            trained_checkpoint_prefix,
                            output_directory,
                            input_shape=None,
-                           optimize_graph=True,
                            output_collection_name='inference_op',
                            additional_output_tensor_names=None):
   """Exports inference graph for the model specified in the pipeline config.
@@ -412,11 +408,10 @@ def export_inference_graph(input_type,
     output_directory: Path to write outputs.
     input_shape: Sets a fixed shape for an `image_tensor` input. If not
       specified, will default to [None, None, None, 3].
-    optimize_graph: Whether to optimize graph using Grappler.
     output_collection_name: Name of collection to add output tensors to.
       If None, does not add output tensors to a collection.
     additional_output_tensor_names: list of additional output
-    tensors to include in the frozen graph.
+      tensors to include in the frozen graph.
   """
   detection_model = model_builder.build(pipeline_config.model,
                                         is_training=False)
@@ -424,4 +419,10 @@ def export_inference_graph(input_type,
                           pipeline_config.eval_config.use_moving_averages,
                           trained_checkpoint_prefix,
                           output_directory, additional_output_tensor_names,
-                          input_shape, optimize_graph, output_collection_name)
+                          input_shape, output_collection_name,
+                          graph_hook_fn=None)
+  pipeline_config.eval_config.use_moving_averages = False
+  config_text = text_format.MessageToString(pipeline_config)
+  with tf.gfile.Open(
+      os.path.join(output_directory, 'pipeline.config'), 'wb') as f:
+    f.write(config_text)
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index 0a999005..a1be0f64 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -18,6 +18,7 @@ import os
 import numpy as np
 import six
 import tensorflow as tf
+from google.protobuf import text_format
 from object_detection import exporter
 from object_detection.builders import model_builder
 from object_detection.core import model
@@ -37,12 +38,13 @@ class FakeModel(model.DetectionModel):
     self._add_detection_masks = add_detection_masks
 
   def preprocess(self, inputs):
-    return tf.identity(inputs)
+    true_image_shapes = []  # Doesn't matter for the fake model.
+    return tf.identity(inputs), true_image_shapes
 
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
     return {'image': tf.layers.conv2d(preprocessed_inputs, 3, 1)}
 
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
     with tf.control_dependencies(prediction_dict.values()):
       postprocessed_tensors = {
           'detection_boxes': tf.constant([[[0.0, 0.0, 0.5, 0.5],
@@ -63,7 +65,7 @@ class FakeModel(model.DetectionModel):
   def restore_map(self, checkpoint_path, from_detection_checkpoint):
     pass
 
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
     pass
 
 
@@ -74,10 +76,10 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     g = tf.Graph()
     with g.as_default():
       mock_model = FakeModel()
-      preprocessed_inputs = mock_model.preprocess(
+      preprocessed_inputs, true_image_shapes = mock_model.preprocess(
           tf.placeholder(tf.float32, shape=[None, None, None, 3]))
-      predictions = mock_model.predict(preprocessed_inputs)
-      mock_model.postprocess(predictions)
+      predictions = mock_model.predict(preprocessed_inputs, true_image_shapes)
+      mock_model.postprocess(predictions, true_image_shapes)
       if use_moving_averages:
         tf.train.ExponentialMovingAverage(0.0).apply()
       slim.get_or_create_global_step()
@@ -213,10 +215,10 @@ class ExportInferenceGraphTest(tf.test.TestCase):
     graph = tf.Graph()
     with graph.as_default():
       fake_model = FakeModel()
-      preprocessed_inputs = fake_model.preprocess(
+      preprocessed_inputs, true_image_shapes = fake_model.preprocess(
           tf.placeholder(dtype=tf.float32, shape=[None, None, None, 3]))
-      predictions = fake_model.predict(preprocessed_inputs)
-      fake_model.postprocess(predictions)
+      predictions = fake_model.predict(preprocessed_inputs, true_image_shapes)
+      fake_model.postprocess(predictions, true_image_shapes)
       exporter.replace_variable_values_with_moving_averages(
           graph, trained_checkpoint_prefix, new_checkpoint_prefix)
 
@@ -448,7 +450,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       masks = inference_graph.get_tensor_by_name('detection_masks:0')
       num_detections = inference_graph.get_tensor_by_name('num_detections:0')
       with self.assertRaisesRegexp(tf.errors.InvalidArgumentError,
-                                   '^TensorArray has inconsistent shapes.'):
+                                   'TensorArray.*shape'):
         sess.run([boxes, scores, classes, masks, num_detections],
                  feed_dict={image_str_tensor: image_str_batch_np})
 
@@ -495,6 +497,31 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       self.assertAllClose(masks_np, np.arange(64).reshape([2, 2, 4, 4]))
       self.assertAllClose(num_detections_np, [2, 1])
 
+  def test_export_graph_saves_pipeline_file(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(trained_checkpoint_prefix,
+                                          use_moving_averages=True)
+    output_directory = os.path.join(tmp_dir, 'output')
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel()
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      exporter.export_inference_graph(
+          input_type='image_tensor',
+          pipeline_config=pipeline_config,
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
+          output_directory=output_directory)
+      expected_pipeline_path = os.path.join(
+          output_directory, 'pipeline.config')
+      self.assertTrue(os.path.exists(expected_pipeline_path))
+
+      written_pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      with tf.gfile.GFile(expected_pipeline_path, 'r') as f:
+        proto_str = f.read()
+        text_format.Merge(proto_str, written_pipeline_config)
+        self.assertProtoEquals(pipeline_config, written_pipeline_config)
+
   def test_export_saved_model_and_run_inference(self):
     tmp_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
diff --git a/research/object_detection/g3doc/evaluation_protocols.md b/research/object_detection/g3doc/evaluation_protocols.md
index 033a1adf..849592ef 100644
--- a/research/object_detection/g3doc/evaluation_protocols.md
+++ b/research/object_detection/g3doc/evaluation_protocols.md
@@ -4,17 +4,16 @@ The Tensorflow Object Detection API currently supports three evaluation protocol
 that can be configured in `EvalConfig` by setting `metrics_set` to the
 corresponding value.
 
-## PASCAL VOC 2007 metric
+## PASCAL VOC 2007 detection metric
 
-`EvalConfig.metrics_set='pascal_voc_metrics'`
+`EvalConfig.metrics_set='pascal_voc_detection_metrics'`
 
 The commonly used mAP metric for evaluating the quality of object detectors, computed according to the protocol of the PASCAL VOC Challenge 2007.
 The protocol is available [here](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/devkit_doc_07-Jun-2007.pdf).
 
+## Weighted PASCAL VOC detection metric
 
-## Weighted PASCAL VOC metric
-
-`EvalConfig.metrics_set='weighted_pascal_voc_metrics'`
+`EvalConfig.metrics_set='weighted_pascal_voc_detection_metrics'`
 
 The weighted PASCAL metric computes the mean average precision as the average
 precision when treating all classes as a single class. In comparison,
@@ -25,7 +24,21 @@ For example, the test set consists of two classes, "cat" and "dog", and there ar
 According to PASCAL VOC 2007 metric, performance on each of the two classes would contribute equally towards the final mAP value,
 while for the Weighted PASCAL VOC metric the final mAP value will be influenced by frequency of each class.
 
-## Open Images metric {#open-images}
+## PASCAL VOC 2007 instance segmentation metric
+
+`EvalConfig.metrics_set='pascal_voc_instance_segmentation_metrics'`
+
+Similar to pascal voc 2007 detection metric, but computes the intersection over
+union based on the object masks instead of object boxes.
+
+## Weighted PASCAL VOC detection metric
+
+`EvalConfig.metrics_set='weighted_pascal_voc_instance_segmentation_metrics'`
+
+Similar to the weighted pascal voc 2007 detection metric, but computes the
+intersection over union based on the object masks instead of object boxes.
+
+## Open Images detection metric {#open-images}
 
 `EvalConfig.metrics_set='open_images_metrics'`
 
diff --git a/research/object_detection/g3doc/exporting_models.md b/research/object_detection/g3doc/exporting_models.md
index 2da97908..7b594f18 100644
--- a/research/object_detection/g3doc/exporting_models.md
+++ b/research/object_detection/g3doc/exporting_models.md
@@ -8,7 +8,7 @@ graph proto. A checkpoint will typically consist of three files:
 * model.ckpt-${CHECKPOINT_NUMBER}.meta
 
 After you've identified a candidate checkpoint to export, run the following
-command from tensorflow/models/research/:
+command from tensorflow/models/research:
 
 ``` bash
 # From tensorflow/models/research/
diff --git a/research/object_detection/g3doc/running_pets.md b/research/object_detection/g3doc/running_pets.md
index 74e0ef15..6ae91799 100644
--- a/research/object_detection/g3doc/running_pets.md
+++ b/research/object_detection/g3doc/running_pets.md
@@ -308,6 +308,18 @@ python object_detection/export_inference_graph.py \
 Afterwards, you should see a directory named `exported_graphs` containing the
 SavedModel and frozen graph.
 
+## Configuring the Instance Segmentation Pipeline
+
+Mask prediction can be turned on for an object detection config by adding
+`predict_instance_masks: true` within the `MaskRCNNBoxPredictor`. Other
+parameters such as mask size, number of convolutions in the mask layer, and the
+convolution hyper parameters can be defined. We will use
+`mask_rcnn_resnet101_pets.config` as a starting point for configuring the
+instance segmentation pipeline. Everything above that was mentioned about object
+detection holds true for instance segmentation. Instance segmentation consists
+of an object detection model with an additional head that predicts the object
+mask inside each predicted box once we remove the training and other details.
+
 ## What's Next
 
 Congratulations, you have now trained an object detector for various cats and
diff --git a/research/object_detection/g3doc/using_your_own_dataset.md b/research/object_detection/g3doc/using_your_own_dataset.md
index c403930e..a36698a2 100644
--- a/research/object_detection/g3doc/using_your_own_dataset.md
+++ b/research/object_detection/g3doc/using_your_own_dataset.md
@@ -103,7 +103,7 @@ FLAGS = flags.FLAGS
 
 
 def create_tf_example(example):
-  # TODO(user): Populate the following variables from your example.
+  # TODO: Populate the following variables from your example.
   height = None # Image height
   width = None # Image width
   filename = None # Filename of the image. Empty if image is not from file
@@ -139,7 +139,7 @@ def create_tf_example(example):
 def main(_):
   writer = tf.python_io.TFRecordWriter(FLAGS.output_path)
 
-  # TODO(user): Write code to read in your dataset to examples variable
+  # TODO: Write code to read in your dataset to examples variable
 
   for example in examples:
     tf_example = create_tf_example(example)
diff --git a/research/object_detection/inference/BUILD b/research/object_detection/inference/BUILD
index c36df0d0..c651818a 100644
--- a/research/object_detection/inference/BUILD
+++ b/research/object_detection/inference/BUILD
@@ -13,7 +13,7 @@ py_library(
     srcs = ["detection_inference.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -22,11 +22,11 @@ py_test(
     srcs = ["detection_inference_test.py"],
     deps = [
         ":detection_inference",
-        "//third_party/py/PIL:pil",
-        "//third_party/py/numpy",
+        "//PIL:pil",
+        "//numpy",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:dataset_util",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:dataset_util",
     ],
 )
 
diff --git a/research/object_detection/inference/__init__.py b/research/object_detection/inference/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/object_detection/inference/detection_inference_test.py b/research/object_detection/inference/detection_inference_test.py
index eabb6b47..94898970 100644
--- a/research/object_detection/inference/detection_inference_test.py
+++ b/research/object_detection/inference/detection_inference_test.py
@@ -17,6 +17,7 @@ r"""Tests for detection_inference.py."""
 import os
 import StringIO
 
+
 import numpy as np
 from PIL import Image
 import tensorflow as tf
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
new file mode 100644
index 00000000..b2a23744
--- /dev/null
+++ b/research/object_detection/inputs.py
@@ -0,0 +1,245 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Model input function for tf-learn object detection model."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import functools
+
+import tensorflow as tf
+from object_detection import trainer
+from object_detection.builders import dataset_builder
+from object_detection.builders import preprocessor_builder
+from object_detection.core import prefetcher
+from object_detection.core import standard_fields as fields
+from object_detection.data_decoders import tf_example_decoder
+from object_detection.protos import eval_pb2
+from object_detection.protos import input_reader_pb2
+from object_detection.protos import train_pb2
+from object_detection.utils import dataset_util
+from object_detection.utils import ops as util_ops
+
+FEATURES_IMAGE = 'images'
+FEATURES_KEY = 'key'
+SERVING_FED_EXAMPLE_KEY = 'serialized_example'
+
+
+def create_train_input_fn(num_classes, train_config, train_input_config):
+  """Creates a train `input` function for `Estimator`.
+
+  Args:
+    num_classes: Number of classes, which does not include a background
+      category.
+    train_config: A train_pb2.TrainConfig.
+    train_input_config: An input_reader_pb2.InputReader.
+
+  Returns:
+    `input_fn` for `Estimator` in TRAIN mode.
+  """
+
+  def _train_input_fn():
+    """Returns `features` and `labels` tensor dictionaries for training.
+
+    Returns:
+      features: Dictionary of feature tensors.
+        features['images'] is a list of N [1, H, W, C] float32 tensors,
+          where N is the number of images in a batch.
+        features['key'] is a list of N string tensors, each representing a
+          unique identifier for the image.
+      labels: Dictionary of groundtruth tensors.
+        labels['locations_list'] is a list of N [num_boxes, 4] float32 tensors
+          containing the corners of the groundtruth boxes.
+        labels['classes_list'] is a list of N [num_boxes, num_classes] float32
+          padded one-hot tensors of classes.
+        labels['masks_list'] is a list of N [num_boxes, H, W] float32 tensors
+          containing only binary values, which represent instance masks for
+          objects if present in the dataset. Else returns None.
+        labels[fields.InputDataFields.groundtruth_weights] is a list of N
+          [num_boxes] float32 tensors containing groundtruth weights for the
+          boxes.
+
+    Raises:
+      TypeError: if the `train_config` or `train_input_config` are not of the
+        correct type.
+    """
+    if not isinstance(train_config, train_pb2.TrainConfig):
+      raise TypeError('For training mode, the `train_config` must be a '
+                      'train_pb2.TrainConfig.')
+    if not isinstance(train_input_config, input_reader_pb2.InputReader):
+      raise TypeError('The `train_input_config` must be a '
+                      'input_reader_pb2.InputReader.')
+
+    def get_next(config):
+      return dataset_util.make_initializable_iterator(
+          dataset_builder.build(config)).get_next()
+
+    create_tensor_dict_fn = functools.partial(get_next, train_input_config)
+
+    data_augmentation_options = [
+        preprocessor_builder.build(step)
+        for step in train_config.data_augmentation_options
+    ]
+
+    input_queue = trainer.create_input_queue(
+        batch_size_per_clone=train_config.batch_size,
+        create_tensor_dict_fn=create_tensor_dict_fn,
+        batch_queue_capacity=train_config.batch_queue_capacity,
+        num_batch_queue_threads=train_config.num_batch_queue_threads,
+        prefetch_queue_capacity=train_config.prefetch_queue_capacity,
+        data_augmentation_options=data_augmentation_options)
+
+    (images_tuple, image_keys, locations_tuple, classes_tuple, masks_tuple,
+     keypoints_tuple, weights_tuple) = (trainer.get_inputs(
+         input_queue=input_queue, num_classes=num_classes))
+
+    features = {
+        FEATURES_IMAGE: list(images_tuple),
+        FEATURES_KEY: list(image_keys)
+    }
+    labels = {
+        'locations_list': list(locations_tuple),
+        'classes_list': list(classes_tuple)
+    }
+
+    # Make sure that there are no tuple elements with None.
+    if all(masks is not None for masks in masks_tuple):
+      labels['masks_list'] = list(masks_tuple)
+    if all(keypoints is not None for keypoints in keypoints_tuple):
+      labels['keypoints_list'] = list(keypoints_tuple)
+    if all((elem is not None for elem in weights_tuple)):
+      labels[fields.InputDataFields.groundtruth_weights] = list(weights_tuple)
+
+    return features, labels
+
+  return _train_input_fn
+
+
+def create_eval_input_fn(num_classes, eval_config, eval_input_config):
+  """Creates an eval `input` function for `Estimator`.
+
+  Args:
+    num_classes: Number of classes, which does not include a background
+      category.
+    eval_config: An eval_pb2.EvalConfig.
+    eval_input_config: An input_reader_pb2.InputReader.
+
+  Returns:
+    `input_fn` for `Estimator` in EVAL mode.
+  """
+
+  def _eval_input_fn():
+    """Returns `features` and `labels` tensor dictionaries for evaluation.
+
+    Returns:
+      features: Dictionary of feature tensors.
+        features['images'] is a [1, H, W, C] float32 tensor.
+        features['key'] is a string tensor representing a unique identifier for
+          the image.
+      labels: Dictionary of groundtruth tensors.
+        labels['locations_list'] is a list of 1 [num_boxes, 4] float32 tensors
+          containing the corners of the groundtruth boxes.
+        labels['classes_list'] is a list of 1 [num_boxes, num_classes] float32
+          padded one-hot tensors of classes.
+        labels['masks_list'] is an (optional) list of 1 [num_boxes, H, W]
+          float32 tensors containing only binary values, which represent
+          instance masks for objects if present in the dataset. Else returns
+          None.
+        labels['image_id_list'] is a list of 1 string tensors containing the
+          original image id.
+        labels['area_list'] is a list of 1 [num_boxes] float32 tensors
+          containing object mask area in pixels squared.
+        labels['is_crowd_list'] is a list of 1 [num_boxes] bool tensors
+          indicating if the boxes enclose a crowd.
+        labels['difficult_list'] is a list of 1 [num_boxes] bool tensors
+          indicating if the boxes represent `difficult` instances.
+
+    Raises:
+      TypeError: if the `eval_config` or `eval_input_config` are not of the
+        correct type.
+    """
+    if not isinstance(eval_config, eval_pb2.EvalConfig):
+      raise TypeError('For eval mode, the `eval_config` must be a '
+                      'eval_pb2.EvalConfig.')
+    if not isinstance(eval_input_config, input_reader_pb2.InputReader):
+      raise TypeError('The `eval_input_config` must be a '
+                      'input_reader_pb2.InputReader.')
+
+    input_dict = dataset_util.make_initializable_iterator(
+        dataset_builder.build(eval_input_config)).get_next()
+    prefetch_queue = prefetcher.prefetch(input_dict, capacity=500)
+    input_dict = prefetch_queue.dequeue()
+    original_image = tf.to_float(
+        tf.expand_dims(input_dict[fields.InputDataFields.image], 0))
+    features = {}
+    features[FEATURES_IMAGE] = original_image
+    features[FEATURES_KEY] = input_dict[fields.InputDataFields.source_id]
+
+    labels = {}
+    labels['locations_list'] = [
+        input_dict[fields.InputDataFields.groundtruth_boxes]
+    ]
+    classes_gt = tf.cast(input_dict[fields.InputDataFields.groundtruth_classes],
+                         tf.int32)
+    classes_gt -= 1  # Remove the label id offset.
+    labels['classes_list'] = [
+        util_ops.padded_one_hot_encoding(
+            indices=classes_gt, depth=num_classes, left_pad=0)
+    ]
+    labels['image_id_list'] = [input_dict[fields.InputDataFields.source_id]]
+    labels['area_list'] = [input_dict[fields.InputDataFields.groundtruth_area]]
+    labels['is_crowd_list'] = [
+        input_dict[fields.InputDataFields.groundtruth_is_crowd]
+    ]
+    labels['difficult_list'] = [
+        input_dict[fields.InputDataFields.groundtruth_difficult]
+    ]
+    if fields.InputDataFields.groundtruth_instance_masks in input_dict:
+      labels['masks_list'] = [
+          input_dict[fields.InputDataFields.groundtruth_instance_masks]
+      ]
+
+    return features, labels
+
+  return _eval_input_fn
+
+
+def create_predict_input_fn():
+  """Creates a predict `input` function for `Estimator`.
+
+  Returns:
+    `input_fn` for `Estimator` in PREDICT mode.
+  """
+
+  def _predict_input_fn():
+    """Decodes serialized tf.Examples and returns `ServingInputReceiver`.
+
+    Returns:
+      `ServingInputReceiver`.
+    """
+    example = tf.placeholder(dtype=tf.string, shape=[], name='input_feature')
+
+    decoder = tf_example_decoder.TfExampleDecoder(load_instance_masks=False)
+
+    input_dict = decoder.decode(example)
+    images = tf.to_float(input_dict[fields.InputDataFields.image])
+    images = tf.expand_dims(images, axis=0)
+
+    return tf.estimator.export.ServingInputReceiver(
+        features={FEATURES_IMAGE: images},
+        receiver_tensors={SERVING_FED_EXAMPLE_KEY: example})
+
+  return _predict_input_fn
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
new file mode 100644
index 00000000..bea0fad9
--- /dev/null
+++ b/research/object_detection/inputs_test.py
@@ -0,0 +1,199 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+"""Tests for object_detection.tflearn.inputs."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+
+import tensorflow as tf
+
+from object_detection import inputs
+from object_detection.core import standard_fields as fields
+from object_detection.utils import config_util
+
+FLAGS = tf.flags.FLAGS
+
+
+def _get_configs_for_model(model_name):
+  """Returns configurations for model."""
+  # TODO: Make sure these tests work fine outside google3.
+  fname = os.path.join(
+      FLAGS.test_srcdir,
+      ('google3/third_party/tensorflow_models/'
+       'object_detection/samples/configs/' + model_name + '.config'))
+  label_map_path = os.path.join(FLAGS.test_srcdir,
+                                ('google3/third_party/tensorflow_models/'
+                                 'object_detection/data/pet_label_map.pbtxt'))
+  data_path = os.path.join(FLAGS.test_srcdir,
+                           ('google3/third_party/tensorflow_models/'
+                            'object_detection/test_data/pets_examples.record'))
+  configs = config_util.get_configs_from_pipeline_file(fname)
+  return config_util.merge_external_params_with_configs(
+      configs,
+      train_input_path=data_path,
+      eval_input_path=data_path,
+      label_map_path=label_map_path)
+
+
+class InputsTest(tf.test.TestCase):
+
+  def _assert_training_inputs(self, features, labels, num_classes, batch_size):
+    self.assertEqual(batch_size, len(features['images']))
+    self.assertEqual(batch_size, len(features['key']))
+    self.assertEqual(batch_size, len(labels['locations_list']))
+    self.assertEqual(batch_size, len(labels['classes_list']))
+    for i in range(batch_size):
+      image = features['images'][i]
+      key = features['key'][i]
+      locations_list = labels['locations_list'][i]
+      classes_list = labels['classes_list'][i]
+      weights_list = labels[fields.InputDataFields.groundtruth_weights][i]
+      self.assertEqual([1, None, None, 3], image.shape.as_list())
+      self.assertEqual(tf.float32, image.dtype)
+      self.assertEqual(tf.string, key.dtype)
+      self.assertEqual([None, 4], locations_list.shape.as_list())
+      self.assertEqual(tf.float32, locations_list.dtype)
+      self.assertEqual([None, num_classes], classes_list.shape.as_list())
+      self.assertEqual(tf.float32, classes_list.dtype)
+      self.assertEqual([None], weights_list.shape.as_list())
+      self.assertEqual(tf.float32, weights_list.dtype)
+
+  def _assert_eval_inputs(self, features, labels, num_classes):
+    self.assertEqual(1, len(labels['locations_list']))
+    self.assertEqual(1, len(labels['classes_list']))
+    self.assertEqual(1, len(labels['image_id_list']))
+    self.assertEqual(1, len(labels['area_list']))
+    self.assertEqual(1, len(labels['is_crowd_list']))
+    self.assertEqual(1, len(labels['difficult_list']))
+    image = features['images']
+    key = features['key']
+    locations_list = labels['locations_list'][0]
+    classes_list = labels['classes_list'][0]
+    image_id_list = labels['image_id_list'][0]
+    area_list = labels['area_list'][0]
+    is_crowd_list = labels['is_crowd_list'][0]
+    difficult_list = labels['difficult_list'][0]
+    self.assertEqual([1, None, None, 3], image.shape.as_list())
+    self.assertEqual(tf.float32, image.dtype)
+    self.assertEqual(tf.string, key.dtype)
+    self.assertEqual([None, 4], locations_list.shape.as_list())
+    self.assertEqual(tf.float32, locations_list.dtype)
+    self.assertEqual([None, num_classes], classes_list.shape.as_list())
+    self.assertEqual(tf.float32, classes_list.dtype)
+    self.assertEqual(tf.string, image_id_list.dtype)
+    self.assertEqual(tf.float32, area_list.dtype)
+    self.assertEqual(tf.bool, is_crowd_list.dtype)
+    self.assertEqual(tf.int64, difficult_list.dtype)
+
+  def test_faster_rcnn_resnet50_train_input(self):
+    """Tests the training input function for FasterRcnnResnet50."""
+    configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
+    classes = 37
+    batch_size = configs['train_config'].batch_size
+    train_input_fn = inputs.create_train_input_fn(
+        classes, configs['train_config'], configs['train_input_config'])
+    features, labels = train_input_fn()
+    self._assert_training_inputs(features, labels, classes, batch_size)
+
+  def test_faster_rcnn_resnet50_eval_input(self):
+    """Tests the eval input function for FasterRcnnResnet50."""
+    configs = _get_configs_for_model('faster_rcnn_resnet50_pets')
+    classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(classes, configs['eval_config'],
+                                                configs['eval_input_config'])
+    features, labels = eval_input_fn()
+    self._assert_eval_inputs(features, labels, classes)
+
+  def test_ssd_inceptionV2_train_input(self):
+    """Tests the training input function for SSDInceptionV2."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    batch_size = configs['train_config'].batch_size
+    train_input_fn = inputs.create_train_input_fn(
+        classes, configs['train_config'], configs['train_input_config'])
+    features, labels = train_input_fn()
+    self._assert_training_inputs(features, labels, classes, batch_size)
+
+  def test_ssd_inceptionV2_eval_input(self):
+    """Tests the eval input function for SSDInceptionV2."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(classes, configs['eval_config'],
+                                                configs['eval_input_config'])
+    features, labels = eval_input_fn()
+    self._assert_eval_inputs(features, labels, classes)
+
+  def test_predict_input(self):
+    """Tests the predict input function."""
+    predict_input_fn = inputs.create_predict_input_fn()
+    serving_input_receiver = predict_input_fn()
+
+    image = serving_input_receiver.features['images']
+    receiver_tensors = serving_input_receiver.receiver_tensors[
+        'serialized_example']
+    self.assertEqual([1, None, None, 3], image.shape.as_list())
+    self.assertEqual(tf.float32, image.dtype)
+    self.assertEqual(tf.string, receiver_tensors.dtype)
+
+  def test_error_with_bad_train_config(self):
+    """Tests that a TypeError is raised with improper train config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    train_input_fn = inputs.create_train_input_fn(
+        num_classes=classes,
+        train_config=configs['eval_config'],  # Expecting `TrainConfig`.
+        train_input_config=configs['train_input_config'])
+    with self.assertRaises(TypeError):
+      train_input_fn()
+
+  def test_error_with_bad_train_input_config(self):
+    """Tests that a TypeError is raised with improper train input config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    train_input_fn = inputs.create_train_input_fn(
+        num_classes=classes,
+        train_config=configs['train_config'],
+        train_input_config=configs['model'])  # Expecting `InputReader`.
+    with self.assertRaises(TypeError):
+      train_input_fn()
+
+  def test_error_with_bad_eval_config(self):
+    """Tests that a TypeError is raised with improper eval config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(
+        num_classes=classes,
+        eval_config=configs['train_config'],  # Expecting `EvalConfig`.
+        eval_input_config=configs['eval_input_config'])
+    with self.assertRaises(TypeError):
+      eval_input_fn()
+
+  def test_error_with_bad_eval_input_config(self):
+    """Tests that a TypeError is raised with improper eval input config."""
+    configs = _get_configs_for_model('ssd_inception_v2_pets')
+    classes = 37
+    eval_input_fn = inputs.create_eval_input_fn(
+        num_classes=classes,
+        eval_config=configs['eval_config'],
+        eval_input_config=configs['model'])  # Expecting `InputReader`.
+    with self.assertRaises(TypeError):
+      eval_input_fn()
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/matchers/BUILD b/research/object_detection/matchers/BUILD
index 1bc5992f..8f2ed5f4 100644
--- a/research/object_detection/matchers/BUILD
+++ b/research/object_detection/matchers/BUILD
@@ -14,7 +14,8 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:matcher",
+        "//tensorflow/models/research/object_detection/core:matcher",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -24,6 +25,7 @@ py_test(
     deps = [
         ":argmax_matcher",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
 
@@ -35,7 +37,7 @@ py_library(
     deps = [
         "//tensorflow",
         "//tensorflow/contrib/image:image_py",
-        "//tensorflow_models/object_detection/core:matcher",
+        "//tensorflow/models/research/object_detection/core:matcher",
     ],
 )
 
diff --git a/research/object_detection/matchers/argmax_matcher.py b/research/object_detection/matchers/argmax_matcher.py
index 97d85185..2407293b 100644
--- a/research/object_detection/matchers/argmax_matcher.py
+++ b/research/object_detection/matchers/argmax_matcher.py
@@ -26,10 +26,10 @@ This matcher is used in Fast(er)-RCNN.
 Note: matchers are used in TargetAssigners. There is a create_target_assigner
 factory function for popular implementations.
 """
-
 import tensorflow as tf
 
 from object_detection.core import matcher
+from object_detection.utils import shape_utils
 
 
 class ArgMaxMatcher(matcher.Matcher):
@@ -119,7 +119,9 @@ class ArgMaxMatcher(matcher.Matcher):
       Returns:
         matches:  int32 tensor indicating the row each column matches to.
       """
-      return -1 * tf.ones([tf.shape(similarity_matrix)[1]], dtype=tf.int32)
+      similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
+          similarity_matrix)
+      return -1 * tf.ones([similarity_matrix_shape[1]], dtype=tf.int32)
 
     def _match_when_rows_are_non_empty():
       """Performs matching when the rows of similarity matrix are non empty.
@@ -128,7 +130,7 @@ class ArgMaxMatcher(matcher.Matcher):
         matches:  int32 tensor indicating the row each column matches to.
       """
       # Matches for each column
-      matches = tf.argmax(similarity_matrix, 0)
+      matches = tf.argmax(similarity_matrix, 0, output_type=tf.int32)
 
       # Deal with matched and unmatched threshold
       if self._matched_threshold is not None:
@@ -156,23 +158,31 @@ class ArgMaxMatcher(matcher.Matcher):
                                                      -1)
 
       if self._force_match_for_each_row:
-        forced_matches_ids = tf.cast(tf.argmax(similarity_matrix, 1), tf.int32)
-
-        # Set matches[forced_matches_ids] = [0, ..., R], R is number of rows.
-        row_range = tf.range(tf.shape(similarity_matrix)[0])
-        col_range = tf.range(tf.shape(similarity_matrix)[1])
-        forced_matches_values = tf.cast(row_range, matches.dtype)
-        keep_matches_ids, _ = tf.setdiff1d(col_range, forced_matches_ids)
-        keep_matches_values = tf.gather(matches, keep_matches_ids)
-        matches = tf.dynamic_stitch(
-            [forced_matches_ids,
-             keep_matches_ids], [forced_matches_values, keep_matches_values])
-
-      return tf.cast(matches, tf.int32)
-
-    return tf.cond(
-        tf.greater(tf.shape(similarity_matrix)[0], 0),
-        _match_when_rows_are_non_empty, _match_when_rows_are_empty)
+        similarity_matrix_shape = shape_utils.combined_static_and_dynamic_shape(
+            similarity_matrix)
+        force_match_column_ids = tf.argmax(similarity_matrix, 1,
+                                           output_type=tf.int32)
+        force_match_column_indicators = tf.one_hot(
+            force_match_column_ids, depth=similarity_matrix_shape[1])
+        force_match_row_ids = tf.argmax(force_match_column_indicators, 0,
+                                        output_type=tf.int32)
+        force_match_column_mask = tf.cast(
+            tf.reduce_max(force_match_column_indicators, 0), tf.bool)
+        final_matches = tf.where(force_match_column_mask,
+                                 force_match_row_ids, matches)
+        return final_matches
+      else:
+        return matches
+
+    if similarity_matrix.shape.is_fully_defined():
+      if similarity_matrix.shape[0].value == 0:
+        return _match_when_rows_are_empty()
+      else:
+        return _match_when_rows_are_non_empty()
+    else:
+      return tf.cond(
+          tf.greater(tf.shape(similarity_matrix)[0], 0),
+          _match_when_rows_are_non_empty, _match_when_rows_are_empty)
 
   def _set_values_using_indicator(self, x, indicator, val):
     """Set the indicated fields of x to val.
diff --git a/research/object_detection/matchers/argmax_matcher_test.py b/research/object_detection/matchers/argmax_matcher_test.py
index 36740f4b..694bebdc 100644
--- a/research/object_detection/matchers/argmax_matcher_test.py
+++ b/research/object_detection/matchers/argmax_matcher_test.py
@@ -19,177 +19,168 @@ import numpy as np
 import tensorflow as tf
 
 from object_detection.matchers import argmax_matcher
+from object_detection.utils import test_case
 
 
-class ArgMaxMatcherTest(tf.test.TestCase):
+class ArgMaxMatcherTest(test_case.TestCase):
 
   def test_return_correct_matches_with_default_thresholds(self):
+
+    def graph_fn(similarity_matrix):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
+      match = matcher.match(similarity_matrix)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
+
     similarity = np.array([[1., 1, 1, 3, 1],
                            [2, -1, 2, 0, 4],
-                           [3, 0, -1, 0, 0]])
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
     expected_matched_rows = np.array([2, 0, 1, 0, 1])
+    (res_matched_cols, res_unmatched_cols,
+     res_match_results) = self.execute(graph_fn, [similarity])
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
-
-    with self.test_session() as sess:
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
-
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, np.arange(similarity.shape[1]))
-    self.assertEmpty(res_unmatched_cols)
+    self.assertAllEqual(res_match_results[res_matched_cols],
+                        expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], [0, 1, 2, 3, 4])
+    self.assertFalse(np.all(res_unmatched_cols))
 
   def test_return_correct_matches_with_empty_rows(self):
 
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
-    sim = 0.2*tf.ones([0, 5])
-    match = matcher.match(sim)
-    unmatched_cols = match.unmatched_column_indices()
-
-    with self.test_session() as sess:
-      res_unmatched_cols = sess.run(unmatched_cols)
-      self.assertAllEqual(res_unmatched_cols, np.arange(5))
+    def graph_fn(similarity_matrix):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
+      match = matcher.match(similarity_matrix)
+      return match.unmatched_column_indicator()
+    similarity = 0.2 * np.ones([0, 5], dtype=np.float32)
+    res_unmatched_cols = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0], np.arange(5))
 
   def test_return_correct_matches_with_matched_threshold(self):
+
+    def graph_fn(similarity):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.)
+      match = matcher.match(similarity)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
+
     similarity = np.array([[1, 1, 1, 3, 1],
                            [2, -1, 2, 0, 4],
-                           [3, 0, -1, 0, 0]], dtype=np.int32)
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
     expected_matched_cols = np.array([0, 3, 4])
     expected_matched_rows = np.array([2, 0, 1])
     expected_unmatched_cols = np.array([1, 2])
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
-
-    init_op = tf.global_variables_initializer()
+    (res_matched_cols, res_unmatched_cols,
+     match_results) = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],
+                        expected_unmatched_cols)
 
-    with self.test_session() as sess:
-      sess.run(init_op)
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_with_matched_and_unmatched_threshold(self):
 
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, expected_matched_cols)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,
+                                             unmatched_threshold=2.)
+      match = matcher.match(similarity)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
 
-  def test_return_correct_matches_with_matched_and_unmatched_threshold(self):
     similarity = np.array([[1, 1, 1, 3, 1],
                            [2, -1, 2, 0, 4],
-                           [3, 0, -1, 0, 0]], dtype=np.int32)
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3,
-                                           unmatched_threshold=2)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
     expected_matched_cols = np.array([0, 3, 4])
     expected_matched_rows = np.array([2, 0, 1])
     expected_unmatched_cols = np.array([1])  # col 2 has too high maximum val
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
+    (res_matched_cols, res_unmatched_cols,
+     match_results) = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],
+                        expected_unmatched_cols)
 
-    with self.test_session() as sess:
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_negatives_lower_than_unmatched_false(self):
 
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, expected_matched_cols)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
+      matcher = argmax_matcher.ArgMaxMatcher(
+          matched_threshold=3.,
+          unmatched_threshold=2.,
+          negatives_lower_than_unmatched=False)
+      match = matcher.match(similarity)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
 
-  def test_return_correct_matches_negatives_lower_than_unmatched_false(self):
     similarity = np.array([[1, 1, 1, 3, 1],
                            [2, -1, 2, 0, 4],
-                           [3, 0, -1, 0, 0]], dtype=np.int32)
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3,
-                                           unmatched_threshold=2,
-                                           negatives_lower_than_unmatched=False)
+                           [3, 0, -1, 0, 0]], dtype=np.float32)
     expected_matched_cols = np.array([0, 3, 4])
     expected_matched_rows = np.array([2, 0, 1])
     expected_unmatched_cols = np.array([2])  # col 1 has too low maximum val
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
+    (res_matched_cols, res_unmatched_cols,
+     match_results) = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],
+                        expected_unmatched_cols)
 
-    with self.test_session() as sess:
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
+  def test_return_correct_matches_unmatched_row_not_using_force_match(self):
 
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, expected_matched_cols)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    def graph_fn(similarity):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,
+                                             unmatched_threshold=2.)
+      match = matcher.match(similarity)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
 
-  def test_return_correct_matches_unmatched_row_not_using_force_match(self):
     similarity = np.array([[1, 1, 1, 3, 1],
                            [-1, 0, -2, -2, -1],
-                           [3, 0, -1, 2, 0]], dtype=np.int32)
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3,
-                                           unmatched_threshold=2)
+                           [3, 0, -1, 2, 0]], dtype=np.float32)
     expected_matched_cols = np.array([0, 3])
     expected_matched_rows = np.array([2, 0])
     expected_unmatched_cols = np.array([1, 2, 4])
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
-
-    with self.test_session() as sess:
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
-
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, expected_matched_cols)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    (res_matched_cols, res_unmatched_cols,
+     match_results) = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],
+                        expected_unmatched_cols)
 
   def test_return_correct_matches_unmatched_row_while_using_force_match(self):
+    def graph_fn(similarity):
+      matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3.,
+                                             unmatched_threshold=2.,
+                                             force_match_for_each_row=True)
+      match = matcher.match(similarity)
+      matched_cols = match.matched_column_indicator()
+      unmatched_cols = match.unmatched_column_indicator()
+      match_results = match.match_results
+      return (matched_cols, unmatched_cols, match_results)
+
     similarity = np.array([[1, 1, 1, 3, 1],
                            [-1, 0, -2, -2, -1],
-                           [3, 0, -1, 2, 0]], dtype=np.int32)
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=3,
-                                           unmatched_threshold=2,
-                                           force_match_for_each_row=True)
+                           [3, 0, -1, 2, 0]], dtype=np.float32)
     expected_matched_cols = np.array([0, 1, 3])
     expected_matched_rows = np.array([2, 1, 0])
     expected_unmatched_cols = np.array([2, 4])  # col 2 has too high max val
 
-    sim = tf.constant(similarity)
-    match = matcher.match(sim)
-    matched_cols = match.matched_column_indices()
-    matched_rows = match.matched_row_indices()
-    unmatched_cols = match.unmatched_column_indices()
-
-    with self.test_session() as sess:
-      res_matched_cols = sess.run(matched_cols)
-      res_matched_rows = sess.run(matched_rows)
-      res_unmatched_cols = sess.run(unmatched_cols)
-
-    self.assertAllEqual(res_matched_rows, expected_matched_rows)
-    self.assertAllEqual(res_matched_cols, expected_matched_cols)
-    self.assertAllEqual(res_unmatched_cols, expected_unmatched_cols)
+    (res_matched_cols, res_unmatched_cols,
+     match_results) = self.execute(graph_fn, [similarity])
+    self.assertAllEqual(match_results[res_matched_cols], expected_matched_rows)
+    self.assertAllEqual(np.nonzero(res_matched_cols)[0], expected_matched_cols)
+    self.assertAllEqual(np.nonzero(res_unmatched_cols)[0],
+                        expected_unmatched_cols)
 
   def test_valid_arguments_corner_case(self):
     argmax_matcher.ArgMaxMatcher(matched_threshold=1,
@@ -211,27 +202,6 @@ class ArgMaxMatcherTest(tf.test.TestCase):
       argmax_matcher.ArgMaxMatcher(matched_threshold=1,
                                    unmatched_threshold=2)
 
-  def test_set_values_using_indicator(self):
-    input_a = np.array([3, 4, 5, 1, 4, 3, 2])
-    expected_b = np.array([3, 0, 0, 1, 0, 3, 2])  # Set a>3 to 0
-    expected_c = np.array(
-        [3., 4., 5., -1., 4., 3., -1.])  # Set a<3 to -1. Float32
-    idxb_ = input_a > 3
-    idxc_ = input_a < 3
-
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=None)
-
-    a = tf.constant(input_a)
-    idxb = tf.constant(idxb_)
-    idxc = tf.constant(idxc_)
-    b = matcher._set_values_using_indicator(a, idxb, 0)
-    c = matcher._set_values_using_indicator(tf.cast(a, tf.float32), idxc, -1)
-    with self.test_session() as sess:
-      res_b = sess.run(b)
-      res_c = sess.run(c)
-      self.assertAllEqual(res_b, expected_b)
-      self.assertAllEqual(res_c, expected_c)
-
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/matchers/bipartite_matcher.py b/research/object_detection/matchers/bipartite_matcher.py
index 3d717d12..640b21e7 100644
--- a/research/object_detection/matchers/bipartite_matcher.py
+++ b/research/object_detection/matchers/bipartite_matcher.py
@@ -27,8 +27,8 @@ class GreedyBipartiteMatcher(matcher.Matcher):
   def _match(self, similarity_matrix, num_valid_rows=-1):
     """Bipartite matches a collection rows and columns. A greedy bi-partite.
 
-    TODO: Add num_valid_columns options to match only that many columns with
-        all the rows.
+    TODO: Add num_valid_columns options to match only that many columns
+    with all the rows.
 
     Args:
       similarity_matrix: Float tensor of shape [N, M] with pairwise similarity
diff --git a/research/object_detection/meta_architectures/BUILD b/research/object_detection/meta_architectures/BUILD
index 0172a9c0..0306e1be 100644
--- a/research/object_detection/meta_architectures/BUILD
+++ b/research/object_detection/meta_architectures/BUILD
@@ -13,12 +13,14 @@ py_library(
     srcs = ["ssd_meta_arch.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/core:model",
-        "//tensorflow_models/object_detection/core:target_assigner",
-        "//tensorflow_models/object_detection/utils:shape_utils",
-        "//tensorflow_models/object_detection/utils:visualization_utils",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/core:model",
+        "//tensorflow/models/research/object_detection/core:target_assigner",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/utils:test_case",
+        "//tensorflow/models/research/object_detection/utils:visualization_utils",
     ],
 )
 
@@ -28,13 +30,12 @@ py_test(
     deps = [
         ":ssd_meta_arch",
         "//tensorflow",
-        "//tensorflow/python:training",
-        "//tensorflow_models/object_detection/core:anchor_generator",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/core:post_processing",
-        "//tensorflow_models/object_detection/core:region_similarity_calculator",
-        "//tensorflow_models/object_detection/utils:test_utils",
+        "//tensorflow/models/research/object_detection/core:anchor_generator",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/core:post_processing",
+        "//tensorflow/models/research/object_detection/core:region_similarity_calculator",
+        "//tensorflow/models/research/object_detection/utils:test_utils",
     ],
 )
 
@@ -45,18 +46,18 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow_models/object_detection/core:balanced_positive_negative_sampler",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:box_list_ops",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/core:model",
-        "//tensorflow_models/object_detection/core:post_processing",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/core:target_assigner",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/core:balanced_positive_negative_sampler",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/core:model",
+        "//tensorflow/models/research/object_detection/core:post_processing",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:target_assigner",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -68,14 +69,14 @@ py_library(
     deps = [
         ":faster_rcnn_meta_arch",
         "//tensorflow",
-        "//tensorflow_models/object_detection/anchor_generators:grid_anchor_generator",
-        "//tensorflow_models/object_detection/builders:box_predictor_builder",
-        "//tensorflow_models/object_detection/builders:hyperparams_builder",
-        "//tensorflow_models/object_detection/builders:post_processing_builder",
-        "//tensorflow_models/object_detection/core:losses",
-        "//tensorflow_models/object_detection/protos:box_predictor_py_pb2",
-        "//tensorflow_models/object_detection/protos:hyperparams_py_pb2",
-        "//tensorflow_models/object_detection/protos:post_processing_py_pb2",
+        "//tensorflow/models/research/object_detection/anchor_generators:grid_anchor_generator",
+        "//tensorflow/models/research/object_detection/builders:box_predictor_builder",
+        "//tensorflow/models/research/object_detection/builders:hyperparams_builder",
+        "//tensorflow/models/research/object_detection/builders:post_processing_builder",
+        "//tensorflow/models/research/object_detection/core:losses",
+        "//tensorflow/models/research/object_detection/protos:box_predictor_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:post_processing_py_pb2",
     ],
 )
 
@@ -93,8 +94,8 @@ py_library(
     deps = [
         ":faster_rcnn_meta_arch",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index ae878b93..5d4a099b 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -21,13 +21,17 @@ See Faster R-CNN: Ren, Shaoqing, et al.
 "Faster R-CNN: Towards real-time object detection with region proposal
 networks." Advances in neural information processing systems. 2015.
 
-We allow for two modes: first_stage_only=True and first_stage_only=False.  In
-the former setting, all of the user facing methods (e.g., predict, postprocess,
-loss) can be used as if the model consisted only of the RPN, returning class
-agnostic proposals (these can be thought of as approximate detections with no
-associated class information).  In the latter setting, proposals are computed,
-then passed through a second stage "box classifier" to yield (multi-class)
-detections.
+We allow for three modes: number_of_stages={1, 2, 3}. In case of 1 stage,
+all of the user facing methods (e.g., predict, postprocess, loss) can be used as
+if the model consisted only of the RPN, returning class agnostic proposals
+(these can be thought of as approximate detections with no associated class
+information).  In case of 2 stages, proposals are computed, then passed
+through a second stage "box classifier" to yield (multi-class) detections.
+Finally, in case of 3 stages which is only used during eval, proposals are
+computed, then passed through a second stage "box classifier" that will compute
+refined boxes and classes, and then features are pooled from the refined and
+non-maximum suppressed boxes and are passed through the box classifier again. If
+number of stages is 3 during training it will be reduced to two automatically.
 
 Implementations of Faster R-CNN models must define a new
 FasterRCNNFeatureExtractor and override three methods: `preprocess`,
@@ -62,6 +66,32 @@ Following the API (see model.DetectionModel definition), our outputs after
 postprocessing operations are always normalized boxes however, internally, we
 sometimes convert to absolute --- e.g. for loss computation.  In particular,
 anchors and proposal_boxes are both represented as absolute coordinates.
+
+Images are resized in the `preprocess` method.
+
+The Faster R-CNN meta architecture has two post-processing methods
+`_postprocess_rpn` which is applied after first stage and
+`_postprocess_box_classifier` which is applied after second stage. There are
+three different ways post-processing can happen depending on number_of_stages
+configured in the meta architecture:
+
+1. When number_of_stages is 1:
+  `_postprocess_rpn` is run as part of the `postprocess` method where
+  true_image_shapes is used to clip proposals, perform non-max suppression and
+  normalize them.
+2. When number of stages is 2:
+  `_postprocess_rpn` is run as part of the `_predict_second_stage` method where
+  `resized_image_shapes` is used to clip proposals, perform non-max suppression
+  and normalize them. In this case `postprocess` method skips `_postprocess_rpn`
+  and only runs `_postprocess_box_classifier` using `true_image_shapes` to clip
+  detections, perform non-max suppression and normalize them.
+3. When number of stages is 3:
+  `_postprocess_rpn` is run as part of the `_predict_second_stage` using
+  `resized_image_shapes` to clip proposals, perform non-max suppression and
+  normalize them. Subsequently, `_postprocess_box_classifier` is run as part of
+  `_predict_third_stage` using `true_image_shapes` to clip detections, peform
+  non-max suppression and normalize them. In this case, the `postprocess` method
+  skips both `_postprocess_rpn` and `_postprocess_box_classifier`.
 """
 from abc import abstractmethod
 from functools import partial
@@ -152,7 +182,8 @@ class FasterRCNNFeatureExtractor(object):
         [batch_size * self.max_num_proposals, height, width, depth]
         representing box classifier features for each proposal.
     """
-    with tf.variable_scope(scope, values=[proposal_feature_maps]):
+    with tf.variable_scope(
+        scope, values=[proposal_feature_maps], reuse=tf.AUTO_REUSE):
       return self._extract_box_classifier_features(proposal_feature_maps, scope)
 
   @abstractmethod
@@ -194,7 +225,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                num_classes,
                image_resizer_fn,
                feature_extractor,
-               first_stage_only,
+               number_of_stages,
                first_stage_anchor_generator,
                first_stage_atrous_rate,
                first_stage_box_predictor_arg_scope,
@@ -232,12 +263,22 @@ class FasterRCNNMetaArch(model.DetectionModel):
         assigned classification targets can range from {0,... K}).
       image_resizer_fn: A callable for image resizing.  This callable
         takes a rank-3 image tensor of shape [height, width, channels]
-        (corresponding to a single image) and returns a rank-3 image tensor,
-        possibly with new spatial dimensions. See
-        builders/image_resizer_builder.py.
+        (corresponding to a single image), an optional rank-3 instance mask
+        tensor of shape [num_masks, height, width] and returns a resized rank-3
+        image tensor, a resized mask tensor if one was provided in the input. In
+        addition this callable must also return a 1-D tensor of the form
+        [height, width, channels] containing the size of the true image, as the
+        image resizer can perform zero padding. See protos/image_resizer.proto.
       feature_extractor: A FasterRCNNFeatureExtractor object.
-      first_stage_only:  Whether to construct only the Region Proposal Network
-        (RPN) part of the model.
+      number_of_stages:  An integer values taking values in {1, 2, 3}. If
+        1, the function will construct only the Region Proposal Network (RPN)
+        part of the model. If 2, the function will perform box refinement and
+        other auxiliary predictions all in the second stage. If 3, it will
+        extract features from refined boxes and perform the auxiliary
+        predictions on the non-maximum suppressed refined boxes.
+        If is_training is true and the value of number_of_stages is 3, it is
+        reduced to 2 since all the model heads are trained in parallel in second
+        stage during training.
       first_stage_anchor_generator: An anchor_generator.AnchorGenerator object
         (note that currently we only support
         grid_anchor_generator.GridAnchorGenerator objects)
@@ -333,7 +374,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._is_training = is_training
     self._image_resizer_fn = image_resizer_fn
     self._feature_extractor = feature_extractor
-    self._first_stage_only = first_stage_only
+    self._number_of_stages = number_of_stages
 
     # The first class is reserved as background.
     unmatched_cls_target = tf.constant(
@@ -368,9 +409,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._first_stage_max_proposals = first_stage_max_proposals
 
     self._first_stage_localization_loss = (
-        losses.WeightedSmoothL1LocalizationLoss(anchorwise_output=True))
+        losses.WeightedSmoothL1LocalizationLoss())
     self._first_stage_objectness_loss = (
-        losses.WeightedSoftmaxClassificationLoss(anchorwise_output=True))
+        losses.WeightedSoftmaxClassificationLoss())
     self._first_stage_loc_loss_weight = first_stage_localization_loss_weight
     self._first_stage_obj_loss_weight = first_stage_objectness_loss_weight
 
@@ -389,10 +430,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._second_stage_score_conversion_fn = second_stage_score_conversion_fn
 
     self._second_stage_localization_loss = (
-        losses.WeightedSmoothL1LocalizationLoss(anchorwise_output=True))
+        losses.WeightedSmoothL1LocalizationLoss())
     self._second_stage_classification_loss = second_stage_classification_loss
     self._second_stage_mask_loss = (
-        losses.WeightedSigmoidClassificationLoss(anchorwise_output=True))
+        losses.WeightedSigmoidClassificationLoss())
     self._second_stage_loc_loss_weight = second_stage_localization_loss_weight
     self._second_stage_cls_loss_weight = second_stage_classification_loss_weight
     self._second_stage_mask_loss_weight = (
@@ -400,6 +441,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._hard_example_miner = hard_example_miner
     self._parallel_iterations = parallel_iterations
 
+    if self._number_of_stages <= 0 or self._number_of_stages > 3:
+      raise ValueError('Number of stages should be a value in {1, 2, 3}.')
+    if self._is_training and self._number_of_stages == 3:
+      self._number_of_stages = 2
+
   @property
   def first_stage_feature_extractor_scope(self):
     return 'FirstStageFeatureExtractor'
@@ -432,6 +478,14 @@ class FasterRCNNMetaArch(model.DetectionModel):
       return self._second_stage_batch_size
     return self._first_stage_max_proposals
 
+  @property
+  def anchors(self):
+    if not self._anchors:
+      raise RuntimeError('anchors have not been constructed yet!')
+    if not isinstance(self._anchors, box_list.BoxList):
+      raise RuntimeError('anchors should be a BoxList object, but is not.')
+    return self._anchors
+
   def preprocess(self, inputs):
     """Feature-extractor specific preprocessing.
 
@@ -448,24 +502,53 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Returns:
       preprocessed_inputs: a [batch, height_out, width_out, channels] float
         tensor representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
     Raises:
       ValueError: if inputs tensor does not have type tf.float32
     """
     if inputs.dtype is not tf.float32:
       raise ValueError('`preprocess` expects a tf.float32 tensor')
     with tf.name_scope('Preprocessor'):
-      resized_inputs = tf.map_fn(self._image_resizer_fn,
-                                 elems=inputs,
-                                 dtype=tf.float32,
-                                 parallel_iterations=self._parallel_iterations)
-      return self._feature_extractor.preprocess(resized_inputs)
+      outputs = shape_utils.static_or_dynamic_map_fn(
+          self._image_resizer_fn,
+          elems=inputs,
+          dtype=[tf.float32, tf.int32],
+          parallel_iterations=self._parallel_iterations)
+      resized_inputs = outputs[0]
+      true_image_shapes = outputs[1]
+      return (self._feature_extractor.preprocess(resized_inputs),
+              true_image_shapes)
+
+  def _compute_clip_window(self, image_shapes):
+    """Computes clip window for non max suppression based on image shapes.
 
-  def predict(self, preprocessed_inputs):
+    This function assumes that the clip window's left top corner is at (0, 0).
+
+    Args:
+      image_shapes: A 2-D int32 tensor of shape [batch_size, 3] containing
+      shapes of images in the batch. Each row represents [height, width,
+      channels] of an image.
+
+    Returns:
+      A 2-D float32 tensor of shape [batch_size, 4] containing the clip window
+      for each image in the form [ymin, xmin, ymax, xmax].
+    """
+    clip_heights = image_shapes[:, 0]
+    clip_widths = image_shapes[:, 1]
+    clip_window = tf.to_float(tf.stack([tf.zeros_like(clip_heights),
+                                        tf.zeros_like(clip_heights),
+                                        clip_heights, clip_widths], axis=1))
+    return clip_window
+
+  def predict(self, preprocessed_inputs, true_image_shapes):
     """Predicts unpostprocessed tensors from input tensor.
 
     This function takes an input batch of images and runs it through the
     forward pass of the network to yield "raw" un-postprocessed predictions.
-    If `first_stage_only` is True, this function only returns first stage
+    If `number_of_stages` is 1, this function only returns first stage
     RPN predictions (un-postprocessed).  Otherwise it returns both
     first stage RPN predictions as well as second stage box classifier
     predictions.
@@ -481,6 +564,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Args:
       preprocessed_inputs: a [batch, height, width, channels] float tensor
         representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -504,7 +591,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
           `num_anchors` can differ depending on whether the model is created in
           training or inference mode.
 
-        (and if first_stage_only=False):
+        (and if number_of_stages=1):
         7) refined_box_encodings: a 3-D tensor with shape
           [total_num_proposals, num_classes, 4] representing predicted
           (final) refined box encodings, where
@@ -526,6 +613,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
         11) mask_predictions: (optional) a 4-D tensor with shape
           [total_num_padded_proposals, num_classes, mask_height, mask_width]
           containing instance mask predictions.
+
+    Raises:
+      ValueError: If `predict` is called before `preprocess`.
     """
     (rpn_box_predictor_features, rpn_features_to_crop, anchors_boxlist,
      image_shape) = self._extract_rpn_feature_maps(preprocessed_inputs)
@@ -544,7 +634,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       anchors_boxlist = box_list_ops.clip_to_window(
           anchors_boxlist, clip_window)
 
-    anchors = anchors_boxlist.get()
+    self._anchors = anchors_boxlist
     prediction_dict = {
         'rpn_box_predictor_features': rpn_box_predictor_features,
         'rpn_features_to_crop': rpn_features_to_crop,
@@ -552,22 +642,46 @@ class FasterRCNNMetaArch(model.DetectionModel):
         'rpn_box_encodings': rpn_box_encodings,
         'rpn_objectness_predictions_with_background':
         rpn_objectness_predictions_with_background,
-        'anchors': anchors
+        'anchors': self._anchors.get()
     }
 
-    if not self._first_stage_only:
+    if self._number_of_stages >= 2:
       prediction_dict.update(self._predict_second_stage(
           rpn_box_encodings,
           rpn_objectness_predictions_with_background,
           rpn_features_to_crop,
-          anchors, image_shape))
+          self._anchors.get(), image_shape, true_image_shapes))
+
+    if self._number_of_stages == 3:
+      prediction_dict = self._predict_third_stage(
+          prediction_dict, true_image_shapes)
+
     return prediction_dict
 
+  def _image_batch_shape_2d(self, image_batch_shape_1d):
+    """Takes a 1-D image batch shape tensor and converts it to a 2-D tensor.
+
+    Example:
+    If 1-D image batch shape tensor is [2, 300, 300, 3]. The corresponding 2-D
+    image batch tensor would be [[300, 300, 3], [300, 300, 3]]
+
+    Args:
+      image_batch_shape_1d: 1-D tensor of the form [batch_size, height,
+        width, channels].
+
+    Returns:
+      image_batch_shape_2d: 2-D tensor of shape [batch_size, 3] were each row is
+        of the form [height, width, channels].
+    """
+    return tf.tile(tf.expand_dims(image_batch_shape_1d[1:], 0),
+                   [image_batch_shape_1d[0], 1])
+
   def _predict_second_stage(self, rpn_box_encodings,
                             rpn_objectness_predictions_with_background,
                             rpn_features_to_crop,
                             anchors,
-                            image_shape):
+                            image_shape,
+                            true_image_shapes):
     """Predicts the output tensors from second stage of Faster R-CNN.
 
     Args:
@@ -584,6 +698,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
       anchors: 2-D float tensor of shape
         [num_anchors, self._box_coder.code_size].
       image_shape: A 1D int32 tensors of size [4] containing the image shape.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -617,9 +735,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
           [total_num_padded_proposals, num_classes, mask_height, mask_width]
           containing instance mask predictions.
     """
+    image_shape_2d = self._image_batch_shape_2d(image_shape)
     proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(
         rpn_box_encodings, rpn_objectness_predictions_with_background,
-        anchors, image_shape)
+        anchors, image_shape_2d, true_image_shapes)
 
     flattened_proposal_feature_maps = (
         self._compute_second_stage_input_feature_maps(
@@ -630,10 +749,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
             flattened_proposal_feature_maps,
             scope=self.second_stage_feature_extractor_scope))
 
+    predict_auxiliary_outputs = False
+    if self._number_of_stages == 2:
+      predict_auxiliary_outputs = True
     box_predictions = self._mask_rcnn_box_predictor.predict(
-        box_classifier_features,
-        num_predictions_per_location=1,
-        scope=self.second_stage_box_predictor_scope)
+        [box_classifier_features],
+        num_predictions_per_location=[1],
+        scope=self.second_stage_box_predictor_scope,
+        predict_boxes_and_classes=True,
+        predict_auxiliary_outputs=predict_auxiliary_outputs)
+
     refined_box_encodings = tf.squeeze(
         box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
     class_predictions_with_background = tf.squeeze(box_predictions[
@@ -658,6 +783,100 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
     return prediction_dict
 
+  def _predict_third_stage(self, prediction_dict, image_shapes):
+    """Predicts non-box, non-class outputs using refined detections.
+
+    Args:
+     prediction_dict: a dictionary holding "raw" prediction tensors:
+        1) refined_box_encodings: a 3-D tensor with shape
+          [total_num_proposals, num_classes, 4] representing predicted
+          (final) refined box encodings, where
+          total_num_proposals=batch_size*self._max_num_proposals
+        2) class_predictions_with_background: a 3-D tensor with shape
+          [total_num_proposals, num_classes + 1] containing class
+          predictions (logits) for each of the anchors, where
+          total_num_proposals=batch_size*self._max_num_proposals.
+          Note that this tensor *includes* background class predictions
+          (at class index 0).
+        3) num_proposals: An int32 tensor of shape [batch_size] representing the
+          number of proposals generated by the RPN.  `num_proposals` allows us
+          to keep track of which entries are to be treated as zero paddings and
+          which are not since we always pad the number of proposals to be
+          `self.max_num_proposals` for each image.
+        4) proposal_boxes: A float32 tensor of shape
+          [batch_size, self.max_num_proposals, 4] representing
+          decoded proposal bounding boxes in absolute coordinates.
+      image_shapes: A 2-D int32 tensors of shape [batch_size, 3] containing
+        shapes of images in the batch.
+
+    Returns:
+      prediction_dict: a dictionary that in addition to the input predictions
+      does hold the following predictions as well:
+        1) mask_predictions: (optional) a 4-D tensor with shape
+          [batch_size, max_detection, mask_height, mask_width] containing
+          instance mask predictions.
+    """
+    detections_dict = self._postprocess_box_classifier(
+        prediction_dict['refined_box_encodings'],
+        prediction_dict['class_predictions_with_background'],
+        prediction_dict['proposal_boxes'],
+        prediction_dict['num_proposals'],
+        image_shapes)
+    prediction_dict.update(detections_dict)
+    detection_boxes = detections_dict[
+        fields.DetectionResultFields.detection_boxes]
+    detection_classes = detections_dict[
+        fields.DetectionResultFields.detection_classes]
+    rpn_features_to_crop = prediction_dict['rpn_features_to_crop']
+    batch_size = tf.shape(detection_boxes)[0]
+    max_detection = tf.shape(detection_boxes)[1]
+    flattened_detected_feature_maps = (
+        self._compute_second_stage_input_feature_maps(
+            rpn_features_to_crop, detection_boxes))
+    detected_box_classifier_features = (
+        self._feature_extractor.extract_box_classifier_features(
+            flattened_detected_feature_maps,
+            scope=self.second_stage_feature_extractor_scope))
+    box_predictions = self._mask_rcnn_box_predictor.predict(
+        [detected_box_classifier_features],
+        num_predictions_per_location=[1],
+        scope=self.second_stage_box_predictor_scope,
+        predict_boxes_and_classes=False,
+        predict_auxiliary_outputs=True)
+    if box_predictor.MASK_PREDICTIONS in box_predictions:
+      detection_masks = tf.squeeze(box_predictions[
+          box_predictor.MASK_PREDICTIONS], axis=1)
+      detection_masks = self._gather_instance_masks(detection_masks,
+                                                    detection_classes)
+      mask_height = tf.shape(detection_masks)[1]
+      mask_width = tf.shape(detection_masks)[2]
+      prediction_dict[fields.DetectionResultFields.detection_masks] = (
+          tf.reshape(detection_masks,
+                     [batch_size, max_detection, mask_height, mask_width]))
+    return prediction_dict
+
+  def _gather_instance_masks(self, instance_masks, classes):
+    """Gathers the masks that correspond to classes.
+
+    Args:
+      instance_masks: A 4-D float32 tensor with shape
+        [K, num_classes, mask_height, mask_width].
+      classes: A 2-D int32 tensor with shape [batch_size, max_detection].
+
+    Returns:
+      masks: a 3-D float32 tensor with shape [K, mask_height, mask_width].
+    """
+    k = tf.shape(instance_masks)[0]
+    num_mask_classes = tf.shape(instance_masks)[1]
+    instance_mask_height = tf.shape(instance_masks)[2]
+    instance_mask_width = tf.shape(instance_masks)[3]
+    classes = tf.reshape(classes, [-1])
+    instance_masks = tf.reshape(instance_masks, [
+        -1, instance_mask_height, instance_mask_width
+    ])
+    return tf.gather(instance_masks,
+                     tf.range(k) * num_mask_classes + tf.to_int32(classes))
+
   def _extract_rpn_feature_maps(self, preprocessed_inputs):
     """Extracts RPN features.
 
@@ -728,8 +947,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       raise RuntimeError('anchor_generator is expected to generate anchors '
                          'corresponding to a single feature map.')
     box_predictions = self._first_stage_box_predictor.predict(
-        rpn_box_predictor_features,
-        num_anchors_per_location[0],
+        [rpn_box_predictor_features],
+        num_anchors_per_location,
         scope=self.first_stage_box_predictor_scope)
 
     box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
@@ -776,7 +995,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     pruned_anchors_boxlist, keep_indices = box_list_ops.prune_outside_window(
         anchors_boxlist, clip_window)
     def _batch_gather_kept_indices(predictions_tensor):
-      return tf.map_fn(
+      return shape_utils.static_or_dynamic_map_fn(
           partial(tf.gather, indices=keep_indices),
           elems=predictions_tensor,
           dtype=tf.float32,
@@ -804,7 +1023,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                                combined_shape[2:])
     return tf.reshape(inputs, flattened_shape)
 
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
     """Convert prediction tensors to final detections.
 
     This function converts raw predictions tensors to final detection results.
@@ -812,20 +1031,24 @@ class FasterRCNNMetaArch(model.DetectionModel):
     scores are to be interpreted as logits, but if a score_converter is used,
     then scores are remapped (and may thus have a different interpretation).
 
-    If first_stage_only=True, the returned results represent proposals from the
+    If number_of_stages=1, the returned results represent proposals from the
     first stage RPN and are padded to have self.max_num_proposals for each
     image; otherwise, the results can be interpreted as multiclass detections
     from the full two-stage model and are padded to self._max_detections.
 
     Args:
       prediction_dict: a dictionary holding prediction tensors (see the
-        documentation for the predict method.  If first_stage_only=True, we
+        documentation for the predict method.  If number_of_stages=1, we
         expect prediction_dict to contain `rpn_box_encodings`,
         `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,
-        `image_shape`, and `anchors` fields.  Otherwise we expect
-        prediction_dict to additionally contain `refined_box_encodings`,
+        and `anchors` fields.  Otherwise we expect prediction_dict to
+        additionally contain `refined_box_encodings`,
         `class_predictions_with_background`, `num_proposals`,
         `proposal_boxes` and, optionally, `mask_predictions` fields.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       detections: a dictionary containing the following fields
@@ -834,36 +1057,55 @@ class FasterRCNNMetaArch(model.DetectionModel):
         detection_classes: [batch, max_detections]
           (this entry is only created if rpn_mode=False)
         num_detections: [batch]
+
+    Raises:
+      ValueError: If `predict` is called before `preprocess`.
     """
+
     with tf.name_scope('FirstStagePostprocessor'):
-      image_shape = prediction_dict['image_shape']
-      if self._first_stage_only:
+      if self._number_of_stages == 1:
         proposal_boxes, proposal_scores, num_proposals = self._postprocess_rpn(
             prediction_dict['rpn_box_encodings'],
             prediction_dict['rpn_objectness_predictions_with_background'],
             prediction_dict['anchors'],
-            image_shape)
+            true_image_shapes,
+            true_image_shapes)
         return {
-            'detection_boxes': proposal_boxes,
-            'detection_scores': proposal_scores,
-            'num_detections': tf.to_float(num_proposals)
+            fields.DetectionResultFields.detection_boxes: proposal_boxes,
+            fields.DetectionResultFields.detection_scores: proposal_scores,
+            fields.DetectionResultFields.num_detections:
+                tf.to_float(num_proposals),
         }
+
     with tf.name_scope('SecondStagePostprocessor'):
-      mask_predictions = prediction_dict.get(box_predictor.MASK_PREDICTIONS)
-      detections_dict = self._postprocess_box_classifier(
-          prediction_dict['refined_box_encodings'],
-          prediction_dict['class_predictions_with_background'],
-          prediction_dict['proposal_boxes'],
-          prediction_dict['num_proposals'],
-          image_shape,
-          mask_predictions=mask_predictions)
+      if self._number_of_stages == 2:
+        mask_predictions = prediction_dict.get(box_predictor.MASK_PREDICTIONS)
+        detections_dict = self._postprocess_box_classifier(
+            prediction_dict['refined_box_encodings'],
+            prediction_dict['class_predictions_with_background'],
+            prediction_dict['proposal_boxes'],
+            prediction_dict['num_proposals'],
+            true_image_shapes,
+            mask_predictions=mask_predictions)
+        return detections_dict
+
+    if self._number_of_stages == 3:
+      # Post processing is already performed in 3rd stage. We need to transfer
+      # postprocessed tensors from `prediction_dict` to `detections_dict`.
+      detections_dict = {}
+      for key in prediction_dict:
+        if key == fields.DetectionResultFields.detection_masks:
+          detections_dict[key] = tf.sigmoid(prediction_dict[key])
+        elif 'detection' in key:
+          detections_dict[key] = prediction_dict[key]
       return detections_dict
 
   def _postprocess_rpn(self,
                        rpn_box_encodings_batch,
                        rpn_objectness_predictions_with_background_batch,
                        anchors,
-                       image_shape):
+                       image_shapes,
+                       true_image_shapes):
     """Converts first stage prediction tensors from the RPN to proposals.
 
     This function decodes the raw RPN predictions, runs non-max suppression
@@ -885,7 +1127,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
       anchors: A 2-D tensor of shape [num_anchors, 4] representing anchors
         for the first stage RPN.  Note that `num_anchors` can differ depending
         on whether the model is created in training or inference mode.
-      image_shape: A 1-D tensor representing the input image shape.
+      image_shapes: A 2-D tensor of shape [batch, 3] containing the shapes of
+        images in the batch.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       proposal_boxes: A float tensor with shape
@@ -909,7 +1156,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     proposal_boxes = tf.squeeze(proposal_boxes, axis=2)
     rpn_objectness_softmax_without_background = tf.nn.softmax(
         rpn_objectness_predictions_with_background_batch)[:, :, 1]
-    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
+    clip_window = self._compute_clip_window(image_shapes)
     (proposal_boxes, proposal_scores, _, _, _,
      num_proposals) = post_processing.batch_multiclass_non_max_suppression(
          tf.expand_dims(proposal_boxes, axis=2),
@@ -924,19 +1171,22 @@ class FasterRCNNMetaArch(model.DetectionModel):
       proposal_boxes = tf.stop_gradient(proposal_boxes)
       if not self._hard_example_miner:
         (groundtruth_boxlists, groundtruth_classes_with_background_list,
-         _) = self._format_groundtruth_data(image_shape)
+         _) = self._format_groundtruth_data(true_image_shapes)
         (proposal_boxes, proposal_scores,
          num_proposals) = self._unpad_proposals_and_sample_box_classifier_batch(
              proposal_boxes, proposal_scores, num_proposals,
              groundtruth_boxlists, groundtruth_classes_with_background_list)
     # normalize proposal boxes
-    proposal_boxes_reshaped = tf.reshape(proposal_boxes, [-1, 4])
-    normalized_proposal_boxes_reshaped = box_list_ops.to_normalized_coordinates(
-        box_list.BoxList(proposal_boxes_reshaped),
-        image_shape[1], image_shape[2], check_range=False).get()
-    proposal_boxes = tf.reshape(normalized_proposal_boxes_reshaped,
-                                [-1, proposal_boxes.shape[1].value, 4])
-    return proposal_boxes, proposal_scores, num_proposals
+    def normalize_boxes(args):
+      proposal_boxes_per_image = args[0]
+      image_shape = args[1]
+      normalized_boxes_per_image = box_list_ops.to_normalized_coordinates(
+          box_list.BoxList(proposal_boxes_per_image), image_shape[0],
+          image_shape[1], check_range=False).get()
+      return normalized_boxes_per_image
+    normalized_proposal_boxes = shape_utils.static_or_dynamic_map_fn(
+        normalize_boxes, elems=[proposal_boxes, image_shapes], dtype=tf.float32)
+    return normalized_proposal_boxes, proposal_scores, num_proposals
 
   def _unpad_proposals_and_sample_box_classifier_batch(
       self,
@@ -951,7 +1201,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       proposal_boxes: A float tensor with shape
         [batch_size, num_proposals, 4] representing the (potentially zero
         padded) proposal boxes for all images in the batch.  These boxes are
-        represented as normalized coordinates.
+        represented in absolute coordinates.
       proposal_scores:  A float tensor with shape
         [batch_size, num_proposals] representing the (potentially zero
         padded) proposal objectness scores for all images in the batch.
@@ -968,7 +1218,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
       proposal_boxes: A float tensor with shape
         [batch_size, second_stage_batch_size, 4] representing the (potentially
         zero padded) proposal boxes for all images in the batch.  These boxes
-        are represented as normalized coordinates.
+        are represented in absolute coordinates.
       proposal_scores:  A float tensor with shape
         [batch_size, second_stage_batch_size] representing the (potentially zero
         padded) proposal objectness scores for all images in the batch.
@@ -1022,7 +1272,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
             tf.stack(single_image_proposal_score_sample),
             tf.stack(single_image_num_proposals_sample))
 
-  def _format_groundtruth_data(self, image_shape):
+  def _format_groundtruth_data(self, true_image_shapes):
     """Helper function for preparing groundtruth data for target assignment.
 
     In order to be consistent with the model.DetectionModel interface,
@@ -1035,8 +1285,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
        image_shape.
 
     Args:
-      image_shape: A 1-D int32 tensor of shape [4] representing the shape of the
-        input image batch.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       groundtruth_boxlists: A list of BoxLists containing (absolute) coordinates
@@ -1050,8 +1302,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     groundtruth_boxlists = [
         box_list_ops.to_absolute_coordinates(
-            box_list.BoxList(boxes), image_shape[1], image_shape[2])
-        for boxes in self.groundtruth_lists(fields.BoxListFields.boxes)]
+            box_list.BoxList(boxes), true_image_shapes[i, 0],
+            true_image_shapes[i, 1])
+        for i, boxes in enumerate(
+            self.groundtruth_lists(fields.BoxListFields.boxes))
+    ]
     groundtruth_classes_with_background_list = [
         tf.to_float(
             tf.pad(one_hot_encoding, [[0, 0], [1, 0]], mode='CONSTANT'))
@@ -1063,12 +1318,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if groundtruth_masks_list is not None:
       resized_masks_list = []
       for mask in groundtruth_masks_list:
-        resized_4d_mask = tf.image.resize_images(
-            tf.expand_dims(mask, axis=3),
-            image_shape[1:3],
-            method=tf.image.ResizeMethod.NEAREST_NEIGHBOR,
-            align_corners=True)
-        resized_masks_list.append(tf.squeeze(resized_4d_mask, axis=3))
+        _, resized_mask, _ = self._image_resizer_fn(
+            # Reuse the given `image_resizer_fn` to resize groundtruth masks.
+            # `mask` tensor for an image is of the shape [num_masks,
+            # image_height, image_width]. Below we create a dummy image of the
+            # the shape [image_height, image_width, 1] to use with
+            # `image_resizer_fn`.
+            image=tf.zeros(tf.stack([tf.shape(mask)[1], tf.shape(mask)[2], 1])),
+            masks=mask)
+        resized_masks_list.append(resized_mask)
+
       groundtruth_masks_list = resized_masks_list
 
     return (groundtruth_boxlists, groundtruth_classes_with_background_list,
@@ -1152,7 +1411,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
                                   class_predictions_with_background,
                                   proposal_boxes,
                                   num_proposals,
-                                  image_shape,
+                                  image_shapes,
                                   mask_predictions=None):
     """Converts predictions from the second stage box classifier to detections.
 
@@ -1169,7 +1428,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
         bounding boxes in absolute coordinates.
       num_proposals: a 1-D int32 tensor of shape [batch] representing the number
         of proposals predicted for each image in the batch.
-      image_shape: a 1-D int32 tensor representing the input image shape.
+      image_shapes: a 2-D int32 tensor containing shapes of input image in the
+        batch.
       mask_predictions: (optional) a 4-D float tensor with shape
         [total_num_padded_proposals, num_classes, mask_height, mask_width]
         containing instance mask prediction logits.
@@ -1202,8 +1462,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         tf.slice(class_predictions_with_background_batch,
                  [0, 0, 1], [-1, -1, -1]),
         [-1, self.max_num_proposals, self.num_classes])
-    clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
-
+    clip_window = self._compute_clip_window(image_shapes)
     mask_predictions_batch = None
     if mask_predictions is not None:
       mask_height = mask_predictions.shape[2].value
@@ -1220,12 +1479,14 @@ class FasterRCNNMetaArch(model.DetectionModel):
          change_coordinate_frame=True,
          num_valid_boxes=num_proposals,
          masks=mask_predictions_batch)
-    detections = {'detection_boxes': nmsed_boxes,
-                  'detection_scores': nmsed_scores,
-                  'detection_classes': nmsed_classes,
-                  'num_detections': tf.to_float(num_detections)}
+    detections = {
+        fields.DetectionResultFields.detection_boxes: nmsed_boxes,
+        fields.DetectionResultFields.detection_scores: nmsed_scores,
+        fields.DetectionResultFields.detection_classes: nmsed_classes,
+        fields.DetectionResultFields.num_detections: tf.to_float(num_detections)
+    }
     if nmsed_masks is not None:
-      detections['detection_masks'] = nmsed_masks
+      detections[fields.DetectionResultFields.detection_masks] = nmsed_masks
     return detections
 
   def _batch_decode_boxes(self, box_encodings, anchor_boxes):
@@ -1257,22 +1518,26 @@ class FasterRCNNMetaArch(model.DetectionModel):
                       tf.stack([combined_shape[0], combined_shape[1],
                                 num_classes, 4]))
 
-  def loss(self, prediction_dict, scope=None):
+  def loss(self, prediction_dict, true_image_shapes, scope=None):
     """Compute scalar loss tensors given prediction tensors.
 
-    If first_stage_only=True, only RPN related losses are computed (i.e.,
+    If number_of_stages=1, only RPN related losses are computed (i.e.,
     `rpn_localization_loss` and `rpn_objectness_loss`).  Otherwise all
     losses are computed.
 
     Args:
       prediction_dict: a dictionary holding prediction tensors (see the
-        documentation for the predict method.  If first_stage_only=True, we
+        documentation for the predict method.  If number_of_stages=1, we
         expect prediction_dict to contain `rpn_box_encodings`,
         `rpn_objectness_predictions_with_background`, `rpn_features_to_crop`,
         `image_shape`, and `anchors` fields.  Otherwise we expect
         prediction_dict to additionally contain `refined_box_encodings`,
         `class_predictions_with_background`, `num_proposals`, and
         `proposal_boxes` fields.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
       scope: Optional scope name.
 
     Returns:
@@ -1283,15 +1548,15 @@ class FasterRCNNMetaArch(model.DetectionModel):
     """
     with tf.name_scope(scope, 'Loss', prediction_dict.values()):
       (groundtruth_boxlists, groundtruth_classes_with_background_list,
-       groundtruth_masks_list
-      ) = self._format_groundtruth_data(prediction_dict['image_shape'])
+       groundtruth_masks_list) = self._format_groundtruth_data(
+           true_image_shapes)
       loss_dict = self._loss_rpn(
           prediction_dict['rpn_box_encodings'],
           prediction_dict['rpn_objectness_predictions_with_background'],
           prediction_dict['anchors'],
           groundtruth_boxlists,
           groundtruth_classes_with_background_list)
-      if not self._first_stage_only:
+      if self._number_of_stages > 1:
         loss_dict.update(
             self._loss_box_classifier(
                 prediction_dict['refined_box_encodings'],
@@ -1352,7 +1617,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         return self._first_stage_sampler.subsample(
             tf.cast(cls_weights, tf.bool),
             self._first_stage_minibatch_size, tf.cast(cls_targets, tf.bool))
-      batch_sampled_indices = tf.to_float(tf.map_fn(
+      batch_sampled_indices = tf.to_float(shape_utils.static_or_dynamic_map_fn(
           _minibatch_subsample_fn,
           [batch_cls_targets, batch_cls_weights],
           dtype=tf.bool,
@@ -1491,10 +1756,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
       second_stage_loc_losses = self._second_stage_localization_loss(
           reshaped_refined_box_encodings,
           batch_reg_targets, weights=batch_reg_weights) / normalizer
-      second_stage_cls_losses = self._second_stage_classification_loss(
-          class_predictions_with_background,
-          batch_cls_targets_with_background,
-          weights=batch_cls_weights) / normalizer
+      second_stage_cls_losses = ops.reduce_sum_trailing_dimensions(
+          self._second_stage_classification_loss(
+              class_predictions_with_background,
+              batch_cls_targets_with_background,
+              weights=batch_cls_weights),
+          ndims=2) / normalizer
+
       second_stage_loc_loss = tf.reduce_sum(
           tf.boolean_mask(second_stage_loc_losses, paddings_indicator))
       second_stage_cls_loss = tf.reduce_sum(
@@ -1522,9 +1790,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
         # Create a new target assigner that matches the proposals to groundtruth
         # and returns the mask targets.
-        # TODO: Move `unmatched_cls_target` from constructor to assign function.
-        # This will enable reuse of a single target assigner for both class
-        # targets and mask targets.
+        # TODO: Move `unmatched_cls_target` from constructor to assign
+        # function. This will enable reuse of a single target assigner for both
+        # class targets and mask targets.
         mask_target_assigner = target_assigner.create_target_assigner(
             'FasterRCNN', 'detection',
             unmatched_cls_target=tf.zeros(image_shape[1:3], dtype=tf.float32))
@@ -1566,14 +1834,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
             flat_cropped_gt_mask,
             [batch_size, -1, mask_height * mask_width])
 
-        second_stage_mask_losses = self._second_stage_mask_loss(
-            reshaped_prediction_masks,
-            batch_cropped_gt_mask,
-            weights=batch_mask_target_weights) / (
-                mask_height * mask_width *
-                tf.maximum(tf.reduce_sum(batch_mask_target_weights, axis=1,
-                                         keep_dims=True),
-                           tf.ones((batch_size, 1))))
+        second_stage_mask_losses = ops.reduce_sum_trailing_dimensions(
+            self._second_stage_mask_loss(
+                reshaped_prediction_masks,
+                batch_cropped_gt_mask,
+                weights=batch_mask_target_weights),
+            ndims=2) / (
+                mask_height * mask_width * tf.maximum(
+                    tf.reduce_sum(
+                        batch_mask_target_weights, axis=1, keep_dims=True
+                    ), tf.ones((batch_size, 1))))
         second_stage_mask_loss = tf.reduce_sum(
             tf.boolean_mask(second_stage_mask_losses, paddings_indicator))
 
@@ -1647,7 +1917,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
           cls_losses=tf.expand_dims(single_image_cls_loss, 0),
           decoded_boxlist_list=[proposal_boxlist])
 
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self,
+                  from_detection_checkpoint=True,
+                  load_all_detection_checkpoint_vars=False):
     """Returns a map of variables to load from a foreign checkpoint.
 
     See parent class for details.
@@ -1655,7 +1927,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
     Args:
       from_detection_checkpoint: whether to restore from a full detection
         checkpoint (with compatible variable names) or to restore from a
-        classification checkpoint for initialization prior to training.
+        classification checkpoint for initialization prior to training. Default
+        True.
+       load_all_detection_checkpoint_vars: whether to load all variables (when
+         `from_detection_checkpoint` is True). If False, only variables within
+         the feature extractor scopes are included. Default False.
 
     Returns:
       A dict mapping variable names (to load from a checkpoint) to variables in
@@ -1670,8 +1946,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
     variables_to_restore.append(slim.get_or_create_global_step())
     # Only load feature extractor variables to be consistent with loading from
     # a classification checkpoint.
+    include_patterns = None
+    if not load_all_detection_checkpoint_vars:
+      include_patterns = [
+          self.first_stage_feature_extractor_scope,
+          self.second_stage_feature_extractor_scope
+      ]
     feature_extractor_variables = tf.contrib.framework.filter_variables(
-        variables_to_restore,
-        include_patterns=[self.first_stage_feature_extractor_scope,
-                          self.second_stage_feature_extractor_scope])
+        variables_to_restore, include_patterns=include_patterns)
     return {var.op.name: var for var in feature_extractor_variables}
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
index b31a22db..17ede33e 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test.py
@@ -26,7 +26,7 @@ class FasterRCNNMetaArchTest(
 
   def test_postprocess_second_stage_only_inference_mode_with_masks(self):
     model = self._build_model(
-        is_training=False, first_stage_only=False, second_stage_batch_size=6)
+        is_training=False, number_of_stages=2, second_stage_batch_size=6)
 
     batch_size = 2
     total_num_padded_proposals = batch_size * model.max_num_proposals
@@ -61,6 +61,7 @@ class FasterRCNNMetaArchTest(
                                      [[1, 1], [1, 1]],
                                      [[0, 0], [0, 0]]]])
 
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     detections = model.postprocess({
         'refined_box_encodings': refined_box_encodings,
         'class_predictions_with_background': class_predictions_with_background,
@@ -68,7 +69,7 @@ class FasterRCNNMetaArchTest(
         'proposal_boxes': proposal_boxes,
         'image_shape': image_shape,
         'mask_predictions': mask_predictions
-    })
+    }, true_image_shapes)
     with self.test_session() as sess:
       detections_out = sess.run(detections)
       self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
@@ -79,6 +80,227 @@ class FasterRCNNMetaArchTest(
       self.assertAllClose(detections_out['num_detections'], [5, 4])
       self.assertAllClose(detections_out['detection_masks'],
                           exp_detection_masks)
+      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
+      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
+
+  def test_predict_correct_shapes_in_inference_mode_three_stages_with_masks(
+      self):
+    batch_size = 2
+    image_size = 10
+    max_num_proposals = 8
+    initial_crop_size = 3
+    maxpool_stride = 1
+
+    input_shapes = [(batch_size, image_size, image_size, 3),
+                    (None, image_size, image_size, 3),
+                    (batch_size, None, None, 3),
+                    (None, None, None, 3)]
+    expected_num_anchors = image_size * image_size * 3 * 3
+    expected_shapes = {
+        'rpn_box_predictor_features':
+        (2, image_size, image_size, 512),
+        'rpn_features_to_crop': (2, image_size, image_size, 3),
+        'image_shape': (4,),
+        'rpn_box_encodings': (2, expected_num_anchors, 4),
+        'rpn_objectness_predictions_with_background':
+        (2, expected_num_anchors, 2),
+        'anchors': (expected_num_anchors, 4),
+        'refined_box_encodings': (2 * max_num_proposals, 2, 4),
+        'class_predictions_with_background': (2 * max_num_proposals, 2 + 1),
+        'num_proposals': (2,),
+        'proposal_boxes': (2, max_num_proposals, 4),
+        'proposal_boxes_normalized': (2, max_num_proposals, 4),
+        'box_classifier_features':
+        self._get_box_classifier_features_shape(image_size,
+                                                batch_size,
+                                                max_num_proposals,
+                                                initial_crop_size,
+                                                maxpool_stride,
+                                                3)
+    }
+
+    for input_shape in input_shapes:
+      test_graph = tf.Graph()
+      with test_graph.as_default():
+        model = self._build_model(
+            is_training=False,
+            number_of_stages=3,
+            second_stage_batch_size=2,
+            predict_masks=True)
+        preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
+        _, true_image_shapes = model.preprocess(preprocessed_inputs)
+        result_tensor_dict = model.predict(preprocessed_inputs,
+                                           true_image_shapes)
+        init_op = tf.global_variables_initializer()
+      with self.test_session(graph=test_graph) as sess:
+        sess.run(init_op)
+        tensor_dict_out = sess.run(result_tensor_dict, feed_dict={
+            preprocessed_inputs:
+            np.zeros((batch_size, image_size, image_size, 3))})
+      self.assertEqual(
+          set(tensor_dict_out.keys()),
+          set(expected_shapes.keys()).union(
+              set([
+                  'detection_boxes', 'detection_scores', 'detection_classes',
+                  'detection_masks', 'num_detections'
+              ])))
+      for key in expected_shapes:
+        self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
+      self.assertAllEqual(tensor_dict_out['detection_boxes'].shape, [2, 5, 4])
+      self.assertAllEqual(tensor_dict_out['detection_masks'].shape,
+                          [2, 5, 14, 14])
+      self.assertAllEqual(tensor_dict_out['detection_classes'].shape, [2, 5])
+      self.assertAllEqual(tensor_dict_out['detection_scores'].shape, [2, 5])
+      self.assertAllEqual(tensor_dict_out['num_detections'].shape, [2])
+
+  def test_predict_gives_correct_shapes_in_train_mode_both_stages_with_masks(
+      self):
+    test_graph = tf.Graph()
+    with test_graph.as_default():
+      model = self._build_model(
+          is_training=True,
+          number_of_stages=2,
+          second_stage_batch_size=7,
+          predict_masks=True)
+
+      batch_size = 2
+      image_size = 10
+      max_num_proposals = 7
+      initial_crop_size = 3
+      maxpool_stride = 1
+
+      image_shape = (batch_size, image_size, image_size, 3)
+      preprocessed_inputs = tf.zeros(image_shape, dtype=tf.float32)
+      groundtruth_boxes_list = [
+          tf.constant([[0, 0, .5, .5], [.5, .5, 1, 1]], dtype=tf.float32),
+          tf.constant([[0, .5, .5, 1], [.5, 0, 1, .5]], dtype=tf.float32)
+      ]
+      groundtruth_classes_list = [
+          tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
+          tf.constant([[1, 0], [1, 0]], dtype=tf.float32)
+      ]
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+
+      result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      expected_shapes = {
+          'rpn_box_predictor_features': (2, image_size, image_size, 512),
+          'rpn_features_to_crop': (2, image_size, image_size, 3),
+          'image_shape': (4,),
+          'refined_box_encodings': (2 * max_num_proposals, 2, 4),
+          'class_predictions_with_background': (2 * max_num_proposals, 2 + 1),
+          'num_proposals': (2,),
+          'proposal_boxes': (2, max_num_proposals, 4),
+          'proposal_boxes_normalized': (2, max_num_proposals, 4),
+          'box_classifier_features':
+              self._get_box_classifier_features_shape(
+                  image_size, batch_size, max_num_proposals, initial_crop_size,
+                  maxpool_stride, 3),
+          'mask_predictions': (2 * max_num_proposals, 2, 14, 14)
+      }
+
+      init_op = tf.global_variables_initializer()
+      with self.test_session(graph=test_graph) as sess:
+        sess.run(init_op)
+        tensor_dict_out = sess.run(result_tensor_dict)
+        self.assertEqual(
+            set(tensor_dict_out.keys()),
+            set(expected_shapes.keys()).union(
+                set([
+                    'rpn_box_encodings',
+                    'rpn_objectness_predictions_with_background',
+                    'anchors',
+                ])))
+        for key in expected_shapes:
+          self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
+
+        anchors_shape_out = tensor_dict_out['anchors'].shape
+        self.assertEqual(2, len(anchors_shape_out))
+        self.assertEqual(4, anchors_shape_out[1])
+        num_anchors_out = anchors_shape_out[0]
+        self.assertAllEqual(tensor_dict_out['rpn_box_encodings'].shape,
+                            (2, num_anchors_out, 4))
+        self.assertAllEqual(
+            tensor_dict_out['rpn_objectness_predictions_with_background'].shape,
+            (2, num_anchors_out, 2))
+
+  def test_postprocess_third_stage_only_inference_mode(self):
+    num_proposals_shapes = [(2), (None)]
+    refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]
+    class_predictions_with_background_shapes = [(16, 3), (None, 3)]
+    proposal_boxes_shapes = [(2, 8, 4), (None, 8, 4)]
+    batch_size = 2
+    image_shape = np.array((2, 36, 48, 3), dtype=np.int32)
+    for (num_proposals_shape, refined_box_encoding_shape,
+         class_predictions_with_background_shape,
+         proposal_boxes_shape) in zip(num_proposals_shapes,
+                                      refined_box_encodings_shapes,
+                                      class_predictions_with_background_shapes,
+                                      proposal_boxes_shapes):
+      tf_graph = tf.Graph()
+      with tf_graph.as_default():
+        model = self._build_model(
+            is_training=False, number_of_stages=3,
+            second_stage_batch_size=6, predict_masks=True)
+        total_num_padded_proposals = batch_size * model.max_num_proposals
+        proposal_boxes = np.array(
+            [[[1, 1, 2, 3],
+              [0, 0, 1, 1],
+              [.5, .5, .6, .6],
+              4*[0], 4*[0], 4*[0], 4*[0], 4*[0]],
+             [[2, 3, 6, 8],
+              [1, 2, 5, 3],
+              4*[0], 4*[0], 4*[0], 4*[0], 4*[0], 4*[0]]])
+        num_proposals = np.array([3, 2], dtype=np.int32)
+        refined_box_encodings = np.zeros(
+            [total_num_padded_proposals, model.num_classes, 4])
+        class_predictions_with_background = np.ones(
+            [total_num_padded_proposals, model.num_classes+1])
+
+        num_proposals_placeholder = tf.placeholder(tf.int32,
+                                                   shape=num_proposals_shape)
+        refined_box_encodings_placeholder = tf.placeholder(
+            tf.float32, shape=refined_box_encoding_shape)
+        class_predictions_with_background_placeholder = tf.placeholder(
+            tf.float32, shape=class_predictions_with_background_shape)
+        proposal_boxes_placeholder = tf.placeholder(
+            tf.float32, shape=proposal_boxes_shape)
+        image_shape_placeholder = tf.placeholder(tf.int32, shape=(4))
+        _, true_image_shapes = model.preprocess(
+            tf.zeros(image_shape_placeholder))
+        detections = model.postprocess({
+            'refined_box_encodings': refined_box_encodings_placeholder,
+            'class_predictions_with_background':
+            class_predictions_with_background_placeholder,
+            'num_proposals': num_proposals_placeholder,
+            'proposal_boxes': proposal_boxes_placeholder,
+            'image_shape': image_shape_placeholder,
+            'detection_boxes': tf.zeros([2, 5, 4]),
+            'detection_masks': tf.zeros([2, 5, 14, 14]),
+            'detection_scores': tf.zeros([2, 5]),
+            'detection_classes': tf.zeros([2, 5]),
+            'num_detections': tf.zeros([2]),
+        }, true_image_shapes)
+      with self.test_session(graph=tf_graph) as sess:
+        detections_out = sess.run(
+            detections,
+            feed_dict={
+                refined_box_encodings_placeholder: refined_box_encodings,
+                class_predictions_with_background_placeholder:
+                class_predictions_with_background,
+                num_proposals_placeholder: num_proposals,
+                proposal_boxes_placeholder: proposal_boxes,
+                image_shape_placeholder: image_shape
+            })
+      self.assertAllEqual(detections_out['detection_boxes'].shape, [2, 5, 4])
+      self.assertAllEqual(detections_out['detection_masks'].shape,
+                          [2, 5, 14, 14])
+      self.assertAllClose(detections_out['detection_scores'].shape, [2, 5])
+      self.assertAllClose(detections_out['detection_classes'].shape, [2, 5])
+      self.assertAllClose(detections_out['num_detections'].shape, [2])
+      self.assertTrue(np.amax(detections_out['detection_masks'] <= 1.0))
+      self.assertTrue(np.amin(detections_out['detection_masks'] >= 0.0))
 
   def _get_box_classifier_features_shape(self,
                                          image_size,
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index 1e84dad3..00c522f6 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -89,10 +89,39 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     """
     return box_predictor_text_proto
 
-  def _get_second_stage_box_predictor(self, num_classes, is_training):
+  def _add_mask_to_second_stage_box_predictor_text_proto(self):
+    box_predictor_text_proto = """
+      mask_rcnn_box_predictor {
+        predict_instance_masks: true
+        mask_height: 14
+        mask_width: 14
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+      }
+    """
+    return box_predictor_text_proto
+
+  def _get_second_stage_box_predictor(self, num_classes, is_training,
+                                      predict_masks):
     box_predictor_proto = box_predictor_pb2.BoxPredictor()
     text_format.Merge(self._get_second_stage_box_predictor_text_proto(),
                       box_predictor_proto)
+    if predict_masks:
+      text_format.Merge(
+          self._add_mask_to_second_stage_box_predictor_text_proto(),
+          box_predictor_proto)
+
     return box_predictor_builder.build(
         hyperparams_builder.build,
         box_predictor_proto,
@@ -109,15 +138,36 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
   def _build_model(self,
                    is_training,
-                   first_stage_only,
+                   number_of_stages,
                    second_stage_batch_size,
                    first_stage_max_proposals=8,
                    num_classes=2,
                    hard_mining=False,
-                   softmax_second_stage_classification_loss=True):
-
-    def image_resizer_fn(image):
-      return tf.identity(image)
+                   softmax_second_stage_classification_loss=True,
+                   predict_masks=False,
+                   pad_to_max_dimension=None):
+
+    def image_resizer_fn(image, masks=None):
+      """Fake image resizer function."""
+      resized_inputs = []
+      resized_image = tf.identity(image)
+      if pad_to_max_dimension is not None:
+        resized_image = tf.image.pad_to_bounding_box(image, 0, 0,
+                                                     pad_to_max_dimension,
+                                                     pad_to_max_dimension)
+      resized_inputs.append(resized_image)
+      if masks is not None:
+        resized_masks = tf.identity(masks)
+        if pad_to_max_dimension is not None:
+          resized_masks = tf.image.pad_to_bounding_box(tf.transpose(masks,
+                                                                    [1, 2, 0]),
+                                                       0, 0,
+                                                       pad_to_max_dimension,
+                                                       pad_to_max_dimension)
+          resized_masks = tf.transpose(resized_masks, [2, 0, 1])
+        resized_inputs.append(resized_masks)
+      resized_inputs.append(tf.shape(image))
+      return resized_inputs
 
     # anchors in this test are designed so that a subset of anchors are inside
     # the image and a subset of anchors are outside.
@@ -181,10 +231,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     second_stage_classification_loss_weight = 1.0
     if softmax_second_stage_classification_loss:
       second_stage_classification_loss = (
-          losses.WeightedSoftmaxClassificationLoss(anchorwise_output=True))
+          losses.WeightedSoftmaxClassificationLoss())
     else:
       second_stage_classification_loss = (
-          losses.WeightedSigmoidClassificationLoss(anchorwise_output=True))
+          losses.WeightedSigmoidClassificationLoss())
 
     hard_example_miner = None
     if hard_mining:
@@ -201,7 +251,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'num_classes': num_classes,
         'image_resizer_fn': image_resizer_fn,
         'feature_extractor': fake_feature_extractor,
-        'first_stage_only': first_stage_only,
+        'number_of_stages': number_of_stages,
         'first_stage_anchor_generator': first_stage_anchor_generator,
         'first_stage_atrous_rate': first_stage_atrous_rate,
         'first_stage_box_predictor_arg_scope':
@@ -232,23 +282,27 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         second_stage_classification_loss,
         'hard_example_miner': hard_example_miner}
 
-    return self._get_model(self._get_second_stage_box_predictor(
-        num_classes=num_classes, is_training=is_training), **common_kwargs)
+    return self._get_model(
+        self._get_second_stage_box_predictor(
+            num_classes=num_classes,
+            is_training=is_training,
+            predict_masks=predict_masks), **common_kwargs)
 
   def test_predict_gives_correct_shapes_in_inference_mode_first_stage_only(
       self):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
-          is_training=False, first_stage_only=True, second_stage_batch_size=2)
+          is_training=False, number_of_stages=1, second_stage_batch_size=2)
       batch_size = 2
       height = 10
       width = 12
       input_image_shape = (batch_size, height, width, 3)
 
-      preprocessed_inputs = tf.placeholder(dtype=tf.float32,
-                                           shape=(batch_size, None, None, 3))
-      prediction_dict = model.predict(preprocessed_inputs)
+      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
+      preprocessed_inputs = tf.placeholder(
+          dtype=tf.float32, shape=(batch_size, None, None, 3))
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
 
       # In inference mode, anchors are clipped to the image window, but not
       # pruned.  Since MockFasterRCNN.extract_proposal_features returns a
@@ -269,7 +323,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       }
 
       init_op = tf.global_variables_initializer()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         prediction_out = sess.run(prediction_dict,
                                   feed_dict={
@@ -295,14 +349,15 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
-          is_training=True, first_stage_only=True, second_stage_batch_size=2)
+          is_training=True, number_of_stages=1, second_stage_batch_size=2)
       batch_size = 2
       height = 10
       width = 12
       input_image_shape = (batch_size, height, width, 3)
-      preprocessed_inputs = tf.placeholder(dtype=tf.float32,
-                                           shape=(batch_size, None, None, 3))
-      prediction_dict = model.predict(preprocessed_inputs)
+      _, true_image_shapes = model.preprocess(tf.zeros(input_image_shape))
+      preprocessed_inputs = tf.placeholder(
+          dtype=tf.float32, shape=(batch_size, None, None, 3))
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
 
       expected_output_keys = set([
           'rpn_box_predictor_features', 'rpn_features_to_crop', 'image_shape',
@@ -314,7 +369,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       num_anchors_strict_upper_bound = height * width * 3 * 3
 
       init_op = tf.global_variables_initializer()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         prediction_out = sess.run(prediction_dict,
                                   feed_dict={
@@ -344,8 +399,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
             prediction_out['rpn_objectness_predictions_with_background'].shape,
             (batch_size, num_anchors_out, 2))
 
-  def test_predict_correct_shapes_in_inference_mode_both_stages(
-      self):
+  def test_predict_correct_shapes_in_inference_mode_two_stages(self):
     batch_size = 2
     image_size = 10
     max_num_proposals = 8
@@ -384,10 +438,14 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       test_graph = tf.Graph()
       with test_graph.as_default():
         model = self._build_model(
-            is_training=False, first_stage_only=False,
-            second_stage_batch_size=2)
+            is_training=False,
+            number_of_stages=2,
+            second_stage_batch_size=2,
+            predict_masks=False)
         preprocessed_inputs = tf.placeholder(tf.float32, shape=input_shape)
-        result_tensor_dict = model.predict(preprocessed_inputs)
+        _, true_image_shapes = model.preprocess(preprocessed_inputs)
+        result_tensor_dict = model.predict(
+            preprocessed_inputs, true_image_shapes)
         init_op = tf.global_variables_initializer()
       with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
@@ -403,7 +461,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
-          is_training=True, first_stage_only=False, second_stage_batch_size=7)
+          is_training=True,
+          number_of_stages=2,
+          second_stage_batch_size=7,
+          predict_masks=False)
 
       batch_size = 2
       image_size = 10
@@ -420,10 +481,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
           tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
           tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]
 
+      _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
 
-      result_tensor_dict = model.predict(preprocessed_inputs)
+      result_tensor_dict = model.predict(preprocessed_inputs, true_image_shapes)
       expected_shapes = {
           'rpn_box_predictor_features':
           (2, image_size, image_size, 512),
@@ -444,7 +506,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       }
 
       init_op = tf.global_variables_initializer()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph) as sess:
         sess.run(init_op)
         tensor_dict_out = sess.run(result_tensor_dict)
         self.assertEqual(set(tensor_dict_out.keys()),
@@ -465,9 +527,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
             tensor_dict_out['rpn_objectness_predictions_with_background'].shape,
             (2, num_anchors_out, 2))
 
-  def test_postprocess_first_stage_only_inference_mode(self):
+  def _test_postprocess_first_stage_only_inference_mode(
+      self, pad_to_max_dimension=None):
     model = self._build_model(
-        is_training=False, first_stage_only=True, second_stage_batch_size=6)
+        is_training=False, number_of_stages=1, second_stage_batch_size=6,
+        pad_to_max_dimension=pad_to_max_dimension)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -490,13 +554,13 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
          [10, -11]]], dtype=tf.float32)
     rpn_features_to_crop = tf.ones((batch_size, 8, 8, 10), dtype=tf.float32)
     image_shape = tf.constant([batch_size, 32, 32, 3], dtype=tf.int32)
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     proposals = model.postprocess({
         'rpn_box_encodings': rpn_box_encodings,
         'rpn_objectness_predictions_with_background':
         rpn_objectness_predictions_with_background,
         'rpn_features_to_crop': rpn_features_to_crop,
-        'anchors': anchors,
-        'image_shape': image_shape})
+        'anchors': anchors}, true_image_shapes)
     expected_proposal_boxes = [
         [[0, 0, .5, .5], [.5, .5, 1, 1], [0, .5, .5, 1], [.5, 0, 1.0, .5]]
         + 4 * [4 * [0]],
@@ -518,9 +582,18 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       self.assertAllEqual(proposals_out['num_detections'],
                           expected_num_proposals)
 
-  def test_postprocess_first_stage_only_train_mode(self):
+  def test_postprocess_first_stage_only_inference_mode(self):
+    self._test_postprocess_first_stage_only_inference_mode()
+
+  def test_postprocess_first_stage_only_inference_mode_padded_image(self):
+    self._test_postprocess_first_stage_only_inference_mode(
+        pad_to_max_dimension=56)
+
+  def _test_postprocess_first_stage_only_train_mode(self,
+                                                    pad_to_max_dimension=None):
     model = self._build_model(
-        is_training=True, first_stage_only=True, second_stage_batch_size=2)
+        is_training=True, number_of_stages=1, second_stage_batch_size=2,
+        pad_to_max_dimension=pad_to_max_dimension)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -549,6 +622,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     groundtruth_classes_list = [tf.constant([[1, 0], [0, 1]], dtype=tf.float32),
                                 tf.constant([[1, 0], [1, 0]], dtype=tf.float32)]
 
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list)
     proposals = model.postprocess({
@@ -556,8 +630,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'rpn_objectness_predictions_with_background':
         rpn_objectness_predictions_with_background,
         'rpn_features_to_crop': rpn_features_to_crop,
-        'anchors': anchors,
-        'image_shape': image_shape})
+        'anchors': anchors}, true_image_shapes)
     expected_proposal_boxes = [
         [[0, 0, .5, .5], [.5, .5, 1, 1]], [[0, .5, .5, 1], [.5, 0, 1, .5]]]
     expected_proposal_scores = [[1, 1],
@@ -577,8 +650,15 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       self.assertAllEqual(proposals_out['num_detections'],
                           expected_num_proposals)
 
-  def test_postprocess_second_stage_only_inference_mode(self):
-    num_proposals_shapes = [(2), (None)]
+  def test_postprocess_first_stage_only_train_mode(self):
+    self._test_postprocess_first_stage_only_train_mode()
+
+  def test_postprocess_first_stage_only_train_mode_padded_image(self):
+    self._test_postprocess_first_stage_only_train_mode(pad_to_max_dimension=56)
+
+  def _test_postprocess_second_stage_only_inference_mode(
+      self, pad_to_max_dimension=None):
+    num_proposals_shapes = [(2), (None,)]
     refined_box_encodings_shapes = [(16, 2, 4), (None, 2, 4)]
     class_predictions_with_background_shapes = [(16, 3), (None, 3)]
     proposal_boxes_shapes = [(2, 8, 4), (None, 8, 4)]
@@ -593,8 +673,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       tf_graph = tf.Graph()
       with tf_graph.as_default():
         model = self._build_model(
-            is_training=False, first_stage_only=False,
-            second_stage_batch_size=6)
+            is_training=False, number_of_stages=2,
+            second_stage_batch_size=6,
+            pad_to_max_dimension=pad_to_max_dimension)
+        _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
         total_num_padded_proposals = batch_size * model.max_num_proposals
         proposal_boxes = np.array(
             [[[1, 1, 2, 3],
@@ -626,8 +708,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
             class_predictions_with_background_placeholder,
             'num_proposals': num_proposals_placeholder,
             'proposal_boxes': proposal_boxes_placeholder,
-            'image_shape': image_shape_placeholder,
-        })
+        }, true_image_shapes)
       with self.test_session(graph=tf_graph) as sess:
         detections_out = sess.run(
             detections,
@@ -646,21 +727,28 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
                           [[0, 0, 0, 1, 1], [0, 0, 1, 1, 0]])
       self.assertAllClose(detections_out['num_detections'], [5, 4])
 
+  def test_postprocess_second_stage_only_inference_mode(self):
+    self._test_postprocess_second_stage_only_inference_mode()
+
+  def test_postprocess_second_stage_only_inference_mode_padded_image(self):
+    self._test_postprocess_second_stage_only_inference_mode(
+        pad_to_max_dimension=56)
+
   def test_preprocess_preserves_input_shapes(self):
     image_shapes = [(3, None, None, 3),
                     (None, 10, 10, 3),
                     (None, None, None, 3)]
     for image_shape in image_shapes:
       model = self._build_model(
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
       image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
-      preprocessed_inputs = model.preprocess(image_placeholder)
+      preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
   # TODO: Split test into two - with and without masks.
   def test_loss_first_stage_only_mode(self):
     model = self._build_model(
-        is_training=True, first_stage_only=True, second_stage_batch_size=6)
+        is_training=True, number_of_stages=1, second_stage_batch_size=6)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -698,9 +786,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'image_shape': image_shape,
         'anchors': anchors
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
       self.assertAllClose(loss_dict_out['first_stage_localization_loss'], 0)
@@ -711,7 +800,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
   # TODO: Split test into two - with and without masks.
   def test_loss_full(self):
     model = self._build_model(
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -793,10 +882,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'num_proposals': num_proposals,
         'mask_predictions': mask_predictions_logits
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list,
                               groundtruth_masks_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
@@ -808,7 +898,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
   def test_loss_full_zero_padded_proposals(self):
     model = self._build_model(
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
     batch_size = 1
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -880,10 +970,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'num_proposals': num_proposals,
         'mask_predictions': mask_predictions_logits
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list,
                               groundtruth_masks_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
@@ -895,7 +986,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
   def test_loss_full_multiple_label_groundtruth(self):
     model = self._build_model(
-        is_training=True, first_stage_only=False, second_stage_batch_size=6,
+        is_training=True, number_of_stages=2, second_stage_batch_size=6,
         softmax_second_stage_classification_loss=False)
     batch_size = 1
     anchors = tf.constant(
@@ -975,10 +1066,11 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'num_proposals': num_proposals,
         'mask_predictions': mask_predictions_logits
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list,
                               groundtruth_masks_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
@@ -990,7 +1082,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
   def test_loss_full_zero_padded_proposals_nonzero_loss_with_two_images(self):
     model = self._build_model(
-        is_training=True, first_stage_only=False, second_stage_batch_size=6)
+        is_training=True, number_of_stages=2, second_stage_batch_size=6)
     batch_size = 2
     anchors = tf.constant(
         [[0, 0, 16, 16],
@@ -1074,9 +1166,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'proposal_boxes': proposal_boxes,
         'num_proposals': num_proposals
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
@@ -1089,7 +1182,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
 
   def test_loss_with_hard_mining(self):
     model = self._build_model(is_training=True,
-                              first_stage_only=False,
+                              number_of_stages=2,
                               second_stage_batch_size=None,
                               first_stage_max_proposals=6,
                               hard_mining=True)
@@ -1163,9 +1256,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'proposal_boxes': proposal_boxes,
         'num_proposals': num_proposals
     }
+    _, true_image_shapes = model.preprocess(tf.zeros(image_shape))
     model.provide_groundtruth(groundtruth_boxes_list,
                               groundtruth_classes_list)
-    loss_dict = model.loss(prediction_dict)
+    loss_dict = model.loss(prediction_dict, true_image_shapes)
 
     with self.test_session() as sess:
       loss_dict_out = sess.run(loss_dict)
@@ -1185,7 +1279,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
       save_path = self.get_temp_dir()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
         sess.run(init_op)
         saved_model_path = saver.save(sess, save_path)
 
@@ -1194,64 +1288,89 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
       model = self._build_model(
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
 
       inputs_shape = (2, 20, 20, 3)
       inputs = tf.to_float(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs = model.preprocess(inputs)
-      prediction_dict = model.predict(preprocessed_inputs)
-      model.postprocess(prediction_dict)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      model.postprocess(prediction_dict, true_image_shapes)
       var_map = model.restore_map(from_detection_checkpoint=False)
       self.assertIsInstance(var_map, dict)
       saver = tf.train.Saver(var_map)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
         saver.restore(sess, saved_model_path)
         for var in sess.run(tf.report_uninitialized_variables()):
-          self.assertNotIn(model.first_stage_feature_extractor_scope, var.name)
-          self.assertNotIn(model.second_stage_feature_extractor_scope,
-                           var.name)
+          self.assertNotIn(model.first_stage_feature_extractor_scope, var)
+          self.assertNotIn(model.second_stage_feature_extractor_scope, var)
 
   def test_restore_map_for_detection_ckpt(self):
     # Define first detection graph and save variables.
     test_graph_detection1 = tf.Graph()
     with test_graph_detection1.as_default():
       model = self._build_model(
-          is_training=False, first_stage_only=False, second_stage_batch_size=6)
+          is_training=False, number_of_stages=2, second_stage_batch_size=6)
       inputs_shape = (2, 20, 20, 3)
       inputs = tf.to_float(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs = model.preprocess(inputs)
-      prediction_dict = model.predict(preprocessed_inputs)
-      model.postprocess(prediction_dict)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      model.postprocess(prediction_dict, true_image_shapes)
+      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
       save_path = self.get_temp_dir()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection1) as sess:
         sess.run(init_op)
         saved_model_path = saver.save(sess, save_path)
 
     # Define second detection graph and restore variables.
     test_graph_detection2 = tf.Graph()
     with test_graph_detection2.as_default():
-      model2 = self._build_model(is_training=False, first_stage_only=False,
+      model2 = self._build_model(is_training=False, number_of_stages=2,
                                  second_stage_batch_size=6, num_classes=42)
 
       inputs_shape2 = (2, 20, 20, 3)
       inputs2 = tf.to_float(tf.random_uniform(
           inputs_shape2, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs2 = model2.preprocess(inputs2)
-      prediction_dict2 = model2.predict(preprocessed_inputs2)
-      model2.postprocess(prediction_dict2)
+      preprocessed_inputs2, true_image_shapes = model2.preprocess(inputs2)
+      prediction_dict2 = model2.predict(preprocessed_inputs2, true_image_shapes)
+      model2.postprocess(prediction_dict2, true_image_shapes)
+      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
       var_map = model2.restore_map(from_detection_checkpoint=True)
       self.assertIsInstance(var_map, dict)
       saver = tf.train.Saver(var_map)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection2) as sess:
         saver.restore(sess, saved_model_path)
-        for var in sess.run(tf.report_uninitialized_variables()):
-          self.assertNotIn(model2.first_stage_feature_extractor_scope, var.name)
-          self.assertNotIn(model2.second_stage_feature_extractor_scope,
-                           var.name)
+        uninitialized_vars_list = sess.run(tf.report_uninitialized_variables())
+        self.assertIn('another_variable', uninitialized_vars_list)
+        for var in uninitialized_vars_list:
+          self.assertNotIn(model2.first_stage_feature_extractor_scope, var)
+          self.assertNotIn(model2.second_stage_feature_extractor_scope, var)
+
+  def test_load_all_det_checkpoint_vars(self):
+    test_graph_detection = tf.Graph()
+    with test_graph_detection.as_default():
+      model = self._build_model(
+          is_training=False,
+          number_of_stages=2,
+          second_stage_batch_size=6,
+          num_classes=42)
+
+      inputs_shape = (2, 20, 20, 3)
+      inputs = tf.to_float(
+          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      model.postprocess(prediction_dict, true_image_shapes)
+      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
+      var_map = model.restore_map(
+          from_detection_checkpoint=True,
+          load_all_detection_checkpoint_vars=True)
+      self.assertIsInstance(var_map, dict)
+      self.assertIn('another_variable', var_map)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index a1154555..411410b1 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -21,8 +21,8 @@ The R-FCN meta architecture is similar to Faster R-CNN and only differs in the
 second stage. Hence this class inherits FasterRCNNMetaArch and overrides only
 the `_predict_second_stage` method.
 
-Similar to Faster R-CNN we allow for two modes: first_stage_only=True and
-first_stage_only=False.  In the former setting, all of the user facing methods
+Similar to Faster R-CNN we allow for two modes: number_of_stages=1 and
+number_of_stages=2.  In the former setting, all of the user facing methods
 (e.g., predict, postprocess, loss) can be used as if the model consisted
 only of the RPN, returning class agnostic proposals (these can be thought of as
 approximate detections with no associated class information).  In the latter
@@ -53,7 +53,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                num_classes,
                image_resizer_fn,
                feature_extractor,
-               first_stage_only,
+               number_of_stages,
                first_stage_anchor_generator,
                first_stage_atrous_rate,
                first_stage_box_predictor_arg_scope,
@@ -90,8 +90,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         returns a rank-3 image tensor, possibly with new spatial dimensions.
         See builders/image_resizer_builder.py.
       feature_extractor: A FasterRCNNFeatureExtractor object.
-      first_stage_only:  Whether to construct only the Region Proposal Network
-        (RPN) part of the model.
+      number_of_stages:  Valid values are {1, 2}. If 1 will only construct the
+        Region Proposal Network (RPN) part of the model.
       first_stage_anchor_generator: An anchor_generator.AnchorGenerator object
         (note that currently we only support
         grid_anchor_generator.GridAnchorGenerator objects)
@@ -165,7 +165,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         num_classes,
         image_resizer_fn,
         feature_extractor,
-        first_stage_only,
+        number_of_stages,
         first_stage_anchor_generator,
         first_stage_atrous_rate,
         first_stage_box_predictor_arg_scope,
@@ -199,14 +199,15 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                             rpn_objectness_predictions_with_background,
                             rpn_features,
                             anchors,
-                            image_shape):
-    """Predicts the output tensors from 2nd stage of FasterRCNN.
+                            image_shape,
+                            true_image_shapes):
+    """Predicts the output tensors from 2nd stage of R-FCN.
 
     Args:
-      rpn_box_encodings: 4-D float tensor of shape
+      rpn_box_encodings: 3-D float tensor of shape
         [batch_size, num_valid_anchors, self._box_coder.code_size] containing
         predicted boxes.
-      rpn_objectness_predictions_with_background: 2-D float tensor of shape
+      rpn_objectness_predictions_with_background: 3-D float tensor of shape
         [batch_size, num_valid_anchors, 2] containing class
         predictions (logits) for each of the anchors.  Note that this
         tensor *includes* background class predictions (at class index 0).
@@ -216,6 +217,10 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       anchors: 2-D float tensor of shape
         [num_anchors, self._box_coder.code_size].
       image_shape: A 1D int32 tensors of size [4] containing the image shape.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
@@ -223,7 +228,7 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
           [total_num_proposals, num_classes, 4] representing predicted
           (final) refined box encodings, where
           total_num_proposals=batch_size*self._max_num_proposals
-        2) class_predictions_with_background: a 3-D tensor with shape
+        2) class_predictions_with_background: a 2-D tensor with shape
           [total_num_proposals, num_classes + 1] containing class
           predictions (logits) for each of the anchors, where
           total_num_proposals=batch_size*self._max_num_proposals.
@@ -247,9 +252,11 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
           [batch_size, feature_map_height, feature_map_width, depth],
           representing the box classifier features.
     """
+    image_shape_2d = tf.tile(tf.expand_dims(image_shape[1:], 0),
+                             [image_shape[0], 1])
     proposal_boxes_normalized, _, num_proposals = self._postprocess_rpn(
         rpn_box_encodings, rpn_objectness_predictions_with_background,
-        anchors, image_shape)
+        anchors, image_shape_2d, true_image_shapes)
 
     box_classifier_features = (
         self._feature_extractor.extract_box_classifier_features(
@@ -257,8 +264,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
             scope=self.second_stage_feature_extractor_scope))
 
     box_predictions = self._rfcn_box_predictor.predict(
-        box_classifier_features,
-        num_predictions_per_location=1,
+        [box_classifier_features],
+        num_predictions_per_location=[1],
         scope=self.second_stage_box_predictor_scope,
         proposal_boxes=proposal_boxes_normalized)
     refined_box_encodings = tf.squeeze(
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index f15cc4af..7b5123c1 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -23,10 +23,10 @@ import re
 import tensorflow as tf
 
 from object_detection.core import box_list
-from object_detection.core import box_predictor as bpredictor
 from object_detection.core import model
 from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner
+from object_detection.utils import ops
 from object_detection.utils import shape_utils
 from object_detection.utils import visualization_utils
 
@@ -43,7 +43,8 @@ class SSDFeatureExtractor(object):
                pad_to_multiple,
                conv_hyperparams,
                batch_norm_trainable=True,
-               reuse_weights=None):
+               reuse_weights=None,
+               use_explicit_padding=False):
     """Constructor.
 
     Args:
@@ -58,6 +59,8 @@ class SSDFeatureExtractor(object):
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
     """
     self._is_training = is_training
     self._depth_multiplier = depth_multiplier
@@ -66,6 +69,7 @@ class SSDFeatureExtractor(object):
     self._conv_hyperparams = conv_hyperparams
     self._batch_norm_trainable = batch_norm_trainable
     self._reuse_weights = reuse_weights
+    self._use_explicit_padding = use_explicit_padding
 
   @abstractmethod
   def preprocess(self, resized_inputs):
@@ -78,6 +82,10 @@ class SSDFeatureExtractor(object):
     Returns:
       preprocessed_inputs: a [batch, height, width, channels] float tensor
         representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
     """
     pass
 
@@ -122,9 +130,9 @@ class SSDMetaArch(model.DetectionModel):
                add_summaries=True):
     """SSDMetaArch Constructor.
 
-    TODO: group NMS parameters + score converter into a class and loss
-    parameters into a class and write config protos for postprocessing
-    and losses.
+    TODO(rathodv,jonathanhuang): group NMS parameters + score converter into
+    a class and loss parameters into a class and write config protos for
+    postprocessing and losses.
 
     Args:
       is_training: A boolean indicating whether the training version of the
@@ -138,7 +146,9 @@ class SSDMetaArch(model.DetectionModel):
         region_similarity_calculator.RegionSimilarityCalculator object.
       image_resizer_fn: a callable for image resizing.  This callable always
         takes a rank-3 image tensor (corresponding to a single image) and
-        returns a rank-3 image tensor, possibly with new spatial dimensions.
+        returns a rank-3 image tensor, possibly with new spatial dimensions and
+        a 1-D tensor of shape [3] indicating shape of true image within
+        the resized image tensor as the resized image tensor could be padded.
         See builders/image_resizer_builder.py.
       non_max_suppression_fn: batch_multiclass_non_max_suppression
         callable that takes `boxes`, `scores` and optional `clip_window`
@@ -174,14 +184,14 @@ class SSDMetaArch(model.DetectionModel):
     self._matcher = matcher
     self._region_similarity_calculator = region_similarity_calculator
 
-    # TODO: handle agnostic mode and positive/negative class weights
+    # TODO: handle agnostic mode and positive/negative class
+    # weights
     unmatched_cls_target = None
     unmatched_cls_target = tf.constant([1] + self.num_classes * [0], tf.float32)
     self._target_assigner = target_assigner.TargetAssigner(
         self._region_similarity_calculator,
         self._matcher,
         self._box_coder,
-        positive_class_weight=1.0,
         negative_class_weight=1.0,
         unmatched_cls_target=unmatched_cls_target)
 
@@ -210,7 +220,9 @@ class SSDMetaArch(model.DetectionModel):
   def preprocess(self, inputs):
     """Feature-extractor specific preprocessing.
 
-    See base class.
+    SSD meta architecture uses a default clip_window of [0, 0, 1, 1] during
+    post-processing. On calling `preprocess` method, clip_window gets updated
+    based on `true_image_shapes` returned by `image_resizer_fn`.
 
     Args:
       inputs: a [batch, height_in, width_in, channels] float tensor representing
@@ -219,20 +231,69 @@ class SSDMetaArch(model.DetectionModel):
     Returns:
       preprocessed_inputs: a [batch, height_out, width_out, channels] float
         tensor representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
+
     Raises:
       ValueError: if inputs tensor does not have type tf.float32
     """
     if inputs.dtype is not tf.float32:
       raise ValueError('`preprocess` expects a tf.float32 tensor')
     with tf.name_scope('Preprocessor'):
-      # TODO: revisit whether to always use batch size as the number of parallel
-      # iterations vs allow for dynamic batching.
-      resized_inputs = tf.map_fn(self._image_resizer_fn,
-                                 elems=inputs,
-                                 dtype=tf.float32)
-      return self._feature_extractor.preprocess(resized_inputs)
-
-  def predict(self, preprocessed_inputs):
+      # TODO: revisit whether to always use batch size as
+      # the number of parallel iterations vs allow for dynamic batching.
+      outputs = shape_utils.static_or_dynamic_map_fn(
+          self._image_resizer_fn,
+          elems=inputs,
+          dtype=[tf.float32, tf.int32])
+      resized_inputs = outputs[0]
+      true_image_shapes = outputs[1]
+
+      return (self._feature_extractor.preprocess(resized_inputs),
+              true_image_shapes)
+
+  def _compute_clip_window(self, preprocessed_images, true_image_shapes):
+    """Computes clip window to use during post_processing.
+
+    Computes a new clip window to use during post-processing based on
+    `resized_image_shapes` and `true_image_shapes` only if `preprocess` method
+    has been called. Otherwise returns a default clip window of [0, 0, 1, 1].
+
+    Args:
+      preprocessed_images: the [batch, height, width, channels] image
+          tensor.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros. Or None if the clip window should cover the full image.
+
+    Returns:
+      a 2-D float32 tensor of the form [batch_size, 4] containing the clip
+      window for each image in the batch in normalized coordinates (relative to
+      the resized dimensions) where each clip window is of the form [ymin, xmin,
+      ymax, xmax] or a default clip window of [0, 0, 1, 1].
+
+    """
+    if true_image_shapes is None:
+      return tf.constant([0, 0, 1, 1], dtype=tf.float32)
+
+    resized_inputs_shape = shape_utils.combined_static_and_dynamic_shape(
+        preprocessed_images)
+    true_heights, true_widths, _ = tf.unstack(
+        tf.to_float(true_image_shapes), axis=1)
+    padded_height = tf.to_float(resized_inputs_shape[1])
+    padded_width = tf.to_float(resized_inputs_shape[2])
+    return tf.stack(
+        [
+            tf.zeros_like(true_heights),
+            tf.zeros_like(true_widths), true_heights / padded_height,
+            true_widths / padded_width
+        ],
+        axis=1)
+
+  def predict(self, preprocessed_inputs, true_image_shapes):
     """Predicts unpostprocessed tensors from input tensor.
 
     This function takes an input batch of images and runs it through the forward
@@ -244,18 +305,24 @@ class SSDMetaArch(model.DetectionModel):
 
     Args:
       preprocessed_inputs: a [batch, height, width, channels] image tensor.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding "raw" prediction tensors:
-        1) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
+        1) preprocessed_inputs: the [batch, height, width, channels] image
+          tensor.
+        2) box_encodings: 4-D float tensor of shape [batch_size, num_anchors,
           box_code_dimension] containing predicted boxes.
-        2) class_predictions_with_background: 3-D float tensor of shape
+        3) class_predictions_with_background: 3-D float tensor of shape
           [batch_size, num_anchors, num_classes+1] containing class predictions
           (logits) for each of the anchors.  Note that this tensor *includes*
           background class predictions (at class index 0).
-        3) feature_maps: a list of tensors where the ith tensor has shape
+        4) feature_maps: a list of tensors where the ith tensor has shape
           [batch, height_i, width_i, depth_i].
-        4) anchors: 2-D float tensor of shape [num_anchors, 4] containing
+        5) anchors: 2-D float tensor of shape [num_anchors, 4] containing
           the generated anchors in normalized coordinates.
     """
     with tf.variable_scope(None, self._extract_features_scope,
@@ -268,9 +335,13 @@ class SSDMetaArch(model.DetectionModel):
         feature_map_spatial_dims,
         im_height=image_shape[1],
         im_width=image_shape[2])
-    (box_encodings, class_predictions_with_background
-    ) = self._add_box_predictions_to_feature_maps(feature_maps)
+    prediction_dict = self._box_predictor.predict(
+        feature_maps, self._anchor_generator.num_anchors_per_location())
+    box_encodings = tf.squeeze(prediction_dict['box_encodings'], axis=2)
+    class_predictions_with_background = prediction_dict[
+        'class_predictions_with_background']
     predictions_dict = {
+        'preprocessed_inputs': preprocessed_inputs,
         'box_encodings': box_encodings,
         'class_predictions_with_background': class_predictions_with_background,
         'feature_maps': feature_maps,
@@ -278,68 +349,6 @@ class SSDMetaArch(model.DetectionModel):
     }
     return predictions_dict
 
-  def _add_box_predictions_to_feature_maps(self, feature_maps):
-    """Adds box predictors to each feature map and returns concatenated results.
-
-    Args:
-      feature_maps: a list of tensors where the ith tensor has shape
-        [batch, height_i, width_i, depth_i]
-
-    Returns:
-      box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
-          box_code_dimension] containing predicted boxes.
-      class_predictions_with_background: 3-D float tensor of shape
-          [batch_size, num_anchors, num_classes+1] containing class predictions
-          (logits) for each of the anchors.  Note that this tensor *includes*
-          background class predictions (at class index 0).
-
-    Raises:
-      RuntimeError: if the number of feature maps extracted via the
-        extract_features method does not match the length of the
-        num_anchors_per_locations list that was passed to the constructor.
-      RuntimeError: if box_encodings from the box_predictor does not have
-        shape of the form  [batch_size, num_anchors, 1, code_size].
-    """
-    num_anchors_per_location_list = (
-        self._anchor_generator.num_anchors_per_location())
-    if len(feature_maps) != len(num_anchors_per_location_list):
-      raise RuntimeError('the number of feature maps must match the '
-                         'length of self.anchors.NumAnchorsPerLocation().')
-    box_encodings_list = []
-    cls_predictions_with_background_list = []
-    for idx, (feature_map, num_anchors_per_location
-             ) in enumerate(zip(feature_maps, num_anchors_per_location_list)):
-      box_predictor_scope = 'BoxPredictor_{}'.format(idx)
-      box_predictions = self._box_predictor.predict(feature_map,
-                                                    num_anchors_per_location,
-                                                    box_predictor_scope)
-      box_encodings = box_predictions[bpredictor.BOX_ENCODINGS]
-      cls_predictions_with_background = box_predictions[
-          bpredictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
-
-      box_encodings_shape = box_encodings.get_shape().as_list()
-      if len(box_encodings_shape) != 4 or box_encodings_shape[2] != 1:
-        raise RuntimeError('box_encodings from the box_predictor must be of '
-                           'shape `[batch_size, num_anchors, 1, code_size]`; '
-                           'actual shape', box_encodings_shape)
-      box_encodings = tf.squeeze(box_encodings, axis=2)
-      box_encodings_list.append(box_encodings)
-      cls_predictions_with_background_list.append(
-          cls_predictions_with_background)
-
-    num_predictions = sum(
-        [tf.shape(box_encodings)[1] for box_encodings in box_encodings_list])
-    num_anchors = self.anchors.num_boxes()
-    anchors_assert = tf.assert_equal(num_anchors, num_predictions, [
-        'Mismatch: number of anchors vs number of predictions', num_anchors,
-        num_predictions
-    ])
-    with tf.control_dependencies([anchors_assert]):
-      box_encodings = tf.concat(box_encodings_list, 1)
-      class_predictions_with_background = tf.concat(
-          cls_predictions_with_background_list, 1)
-    return box_encodings, class_predictions_with_background
-
   def _get_feature_map_spatial_dims(self, feature_maps):
     """Return list of spatial dimensions for each feature map in a list.
 
@@ -356,7 +365,7 @@ class SSDMetaArch(model.DetectionModel):
     ]
     return [(shape[1], shape[2]) for shape in feature_map_shapes]
 
-  def postprocess(self, prediction_dict):
+  def postprocess(self, prediction_dict, true_image_shapes):
     """Converts prediction tensors to final detections.
 
     This function converts raw predictions tensors to final detection results by
@@ -370,12 +379,18 @@ class SSDMetaArch(model.DetectionModel):
 
     Args:
       prediction_dict: a dictionary holding prediction tensors with
-        1) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
+        1) preprocessed_inputs: a [batch, height, width, channels] image
+          tensor.
+        2) box_encodings: 3-D float tensor of shape [batch_size, num_anchors,
           box_code_dimension] containing predicted boxes.
-        2) class_predictions_with_background: 3-D float tensor of shape
+        3) class_predictions_with_background: 3-D float tensor of shape
           [batch_size, num_anchors, num_classes+1] containing class predictions
           (logits) for each of the anchors.  Note that this tensor *includes*
           background class predictions.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros. Or None, if the clip window should cover the full image.
 
     Returns:
       detections: a dictionary containing the following fields
@@ -393,18 +408,18 @@ class SSDMetaArch(model.DetectionModel):
         'class_predictions_with_background' not in prediction_dict):
       raise ValueError('prediction_dict does not contain expected entries.')
     with tf.name_scope('Postprocessor'):
+      preprocessed_images = prediction_dict['preprocessed_inputs']
       box_encodings = prediction_dict['box_encodings']
       class_predictions = prediction_dict['class_predictions_with_background']
       detection_boxes, detection_keypoints = self._batch_decode(box_encodings)
       detection_boxes = tf.expand_dims(detection_boxes, axis=2)
 
-      class_predictions_without_background = tf.slice(class_predictions,
-                                                      [0, 0, 1],
-                                                      [-1, -1, -1])
-      detection_scores = self._score_conversion_fn(
-          class_predictions_without_background)
-      clip_window = tf.constant([0, 0, 1, 1], tf.float32)
+      detection_scores_with_background = self._score_conversion_fn(
+          class_predictions)
+      detection_scores = tf.slice(detection_scores_with_background, [0, 0, 1],
+                                  [-1, -1, -1])
       additional_fields = None
+
       if detection_keypoints is not None:
         additional_fields = {
             fields.BoxListFields.keypoints: detection_keypoints}
@@ -412,19 +427,23 @@ class SSDMetaArch(model.DetectionModel):
        num_detections) = self._non_max_suppression_fn(
            detection_boxes,
            detection_scores,
-           clip_window=clip_window,
+           clip_window=self._compute_clip_window(
+               preprocessed_images, true_image_shapes),
            additional_fields=additional_fields)
-      detection_dict = {'detection_boxes': nmsed_boxes,
-                        'detection_scores': nmsed_scores,
-                        'detection_classes': nmsed_classes,
-                        'num_detections': tf.to_float(num_detections)}
+      detection_dict = {
+          fields.DetectionResultFields.detection_boxes: nmsed_boxes,
+          fields.DetectionResultFields.detection_scores: nmsed_scores,
+          fields.DetectionResultFields.detection_classes: nmsed_classes,
+          fields.DetectionResultFields.num_detections:
+              tf.to_float(num_detections)
+      }
       if (nmsed_additional_fields is not None and
           fields.BoxListFields.keypoints in nmsed_additional_fields):
-        detection_dict['detection_keypoints'] = nmsed_additional_fields[
-            fields.BoxListFields.keypoints]
+        detection_dict[fields.DetectionResultFields.detection_keypoints] = (
+            nmsed_additional_fields[fields.BoxListFields.keypoints])
       return detection_dict
 
-  def loss(self, prediction_dict, scope=None):
+  def loss(self, prediction_dict, true_image_shapes, scope=None):
     """Compute scalar loss tensors with respect to provided groundtruth.
 
     Calling this function requires that groundtruth tensors have been
@@ -438,6 +457,10 @@ class SSDMetaArch(model.DetectionModel):
           [batch_size, num_anchors, num_classes+1] containing class predictions
           (logits) for each of the anchors. Note that this tensor *includes*
           background class predictions.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
       scope: Optional scope name.
 
     Returns:
@@ -457,17 +480,17 @@ class SSDMetaArch(model.DetectionModel):
       if self._add_summaries:
         self._summarize_input(
             self.groundtruth_lists(fields.BoxListFields.boxes), match_list)
-      num_matches = tf.stack(
-          [match.num_matched_columns() for match in match_list])
       location_losses = self._localization_loss(
           prediction_dict['box_encodings'],
           batch_reg_targets,
           ignore_nan_targets=True,
           weights=batch_reg_weights)
-      cls_losses = self._classification_loss(
-          prediction_dict['class_predictions_with_background'],
-          batch_cls_targets,
-          weights=batch_cls_weights)
+      cls_losses = ops.reduce_sum_trailing_dimensions(
+          self._classification_loss(
+              prediction_dict['class_predictions_with_background'],
+              batch_cls_targets,
+              weights=batch_cls_weights),
+          ndims=2)
 
       if self._hard_example_miner:
         (localization_loss, classification_loss) = self._apply_hard_mining(
@@ -487,7 +510,8 @@ class SSDMetaArch(model.DetectionModel):
       # Optionally normalize by number of positive matches
       normalizer = tf.constant(1.0, dtype=tf.float32)
       if self._normalize_loss_by_num_matches:
-        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(num_matches)), 1.0)
+        normalizer = tf.maximum(tf.to_float(tf.reduce_sum(batch_reg_weights)),
+                                1.0)
 
       with tf.name_scope('localization_loss'):
         localization_loss = ((self._localization_loss_weight / normalizer) *
@@ -675,7 +699,9 @@ class SSDMetaArch(model.DetectionModel):
         [combined_shape[0], combined_shape[1], 4]))
     return decoded_boxes, decoded_keypoints
 
-  def restore_map(self, from_detection_checkpoint=True):
+  def restore_map(self,
+                  from_detection_checkpoint=True,
+                  load_all_detection_checkpoint_vars=False):
     """Returns a map of variables to load from a foreign checkpoint.
 
     See parent class for details.
@@ -684,6 +710,9 @@ class SSDMetaArch(model.DetectionModel):
       from_detection_checkpoint: whether to restore from a full detection
         checkpoint (with compatible variable names) or to restore from a
         classification checkpoint for initialization prior to training.
+      load_all_detection_checkpoint_vars: whether to load all variables (when
+         `from_detection_checkpoint` is True). If False, only variables within
+         the appropriate scopes are included. Default False.
 
     Returns:
       A dict mapping variable names (to load from a checkpoint) to variables in
@@ -691,10 +720,15 @@ class SSDMetaArch(model.DetectionModel):
     """
     variables_to_restore = {}
     for variable in tf.global_variables():
-      if variable.op.name.startswith(self._extract_features_scope):
-        var_name = variable.op.name
-        if not from_detection_checkpoint:
-          var_name = (re.split('^' + self._extract_features_scope + '/',
-                               var_name)[-1])
+      var_name = variable.op.name
+      if from_detection_checkpoint and load_all_detection_checkpoint_vars:
         variables_to_restore[var_name] = variable
+      else:
+        if var_name.startswith(self._extract_features_scope):
+          if not from_detection_checkpoint:
+            var_name = (
+                re.split('^' + self._extract_features_scope + '/',
+                         var_name)[-1])
+          variables_to_restore[var_name] = variable
+
     return variables_to_restore
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index 9112ed09..db9616b4 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -24,6 +24,7 @@ from object_detection.core import losses
 from object_detection.core import post_processing
 from object_detection.core import region_similarity_calculator as sim_calc
 from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
 slim = tf.contrib.slim
@@ -46,7 +47,7 @@ class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   def extract_features(self, preprocessed_inputs):
     with tf.variable_scope('mock_model'):
       features = slim.conv2d(inputs=preprocessed_inputs, num_outputs=32,
-                             kernel_size=[1, 1], scope='layer1')
+                             kernel_size=1, scope='layer1')
       return [features]
 
 
@@ -64,37 +65,31 @@ class MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):
         tf.constant([[0, 0, .5, .5],
                      [0, .5, .5, 1],
                      [.5, 0, 1, .5],
-                     [.5, .5, 1, 1]], tf.float32))
+                     [1., 1., 1.5, 1.5]  # Anchor that is outside clip_window.
+                    ], tf.float32))
 
+  def num_anchors(self):
+    return 4
 
-class SsdMetaArchTest(tf.test.TestCase):
 
-  def setUp(self):
-    """Set up mock SSD model.
+class SsdMetaArchTest(test_case.TestCase):
 
-    Here we set up a simple mock SSD model that will always predict 4
-    detections that happen to always be exactly the anchors that are set up
-    in the above MockAnchorGenerator.  Because we let max_detections=5,
-    we will also always end up with an extra padded row in the detection
-    results.
-    """
+  def _create_model(self, apply_hard_mining=True):
     is_training = False
-    self._num_classes = 1
+    num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
     mock_box_predictor = test_utils.MockBoxPredictor(
-        is_training, self._num_classes)
+        is_training, num_classes)
     mock_box_coder = test_utils.MockBoxCoder()
     fake_feature_extractor = FakeSSDFeatureExtractor()
     mock_matcher = test_utils.MockMatcher()
     region_similarity_calculator = sim_calc.IouSimilarity()
 
     def image_resizer_fn(image):
-      return tf.identity(image)
+      return [tf.identity(image), tf.shape(image)]
 
-    classification_loss = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    localization_loss = losses.WeightedSmoothL1LocalizationLoss(
-        anchorwise_output=True)
+    classification_loss = losses.WeightedSigmoidClassificationLoss()
+    localization_loss = losses.WeightedSmoothL1LocalizationLoss()
     non_max_suppression_fn = functools.partial(
         post_processing.batch_multiclass_non_max_suppression,
         score_thresh=-20.0,
@@ -105,48 +100,56 @@ class SsdMetaArchTest(tf.test.TestCase):
     localization_loss_weight = 1.0
     normalize_loss_by_num_matches = False
 
-    # This hard example miner is expected to be a no-op.
-    hard_example_miner = losses.HardExampleMiner(
-        num_hard_examples=None,
-        iou_threshold=1.0)
+    hard_example_miner = None
+    if apply_hard_mining:
+      # This hard example miner is expected to be a no-op.
+      hard_example_miner = losses.HardExampleMiner(
+          num_hard_examples=None,
+          iou_threshold=1.0)
 
-    self._num_anchors = 4
-    self._code_size = 4
-    self._model = ssd_meta_arch.SSDMetaArch(
+    code_size = 4
+    model = ssd_meta_arch.SSDMetaArch(
         is_training, mock_anchor_generator, mock_box_predictor, mock_box_coder,
         fake_feature_extractor, mock_matcher, region_similarity_calculator,
         image_resizer_fn, non_max_suppression_fn, tf.identity,
         classification_loss, localization_loss, classification_loss_weight,
         localization_loss_weight, normalize_loss_by_num_matches,
-        hard_example_miner)
+        hard_example_miner, add_summaries=False)
+    return model, num_classes, mock_anchor_generator.num_anchors(), code_size
 
-  def test_preprocess_preserves_input_shapes(self):
+  def test_preprocess_preserves_shapes_with_dynamic_input_image(self):
     image_shapes = [(3, None, None, 3),
                     (None, 10, 10, 3),
                     (None, None, None, 3)]
+    model, _, _, _ = self._create_model()
     for image_shape in image_shapes:
       image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
-      preprocessed_inputs = self._model.preprocess(image_placeholder)
+      preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
-  def test_predict_results_have_correct_keys_and_shapes(self):
+  def test_preprocess_preserves_shape_with_static_input_image(self):
+    def graph_fn(input_image):
+      model, _, _, _ = self._create_model()
+      return model.preprocess(input_image)
+    input_image = np.random.rand(2, 3, 3, 3).astype(np.float32)
+    preprocessed_inputs, _ = self.execute(graph_fn, [input_image])
+    self.assertAllEqual(preprocessed_inputs.shape, [2, 3, 3, 3])
+
+  def test_predict_result_shapes_on_image_with_dynamic_shape(self):
     batch_size = 3
     image_size = 2
-    input_shapes = [(batch_size, image_size, image_size, 3),
-                    (None, image_size, image_size, 3),
+    input_shapes = [(None, image_size, image_size, 3),
                     (batch_size, None, None, 3),
                     (None, None, None, 3)]
-    expected_box_encodings_shape_out = (
-        batch_size, self._num_anchors, self._code_size)
-    expected_class_predictions_with_background_shape_out = (
-        batch_size, self._num_anchors, self._num_classes+1)
 
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
       with tf_graph.as_default():
+        model, num_classes, num_anchors, code_size = self._create_model()
         preprocessed_input_placeholder = tf.placeholder(tf.float32,
                                                         shape=input_shape)
-        prediction_dict = self._model.predict(preprocessed_input_placeholder)
+        prediction_dict = model.predict(
+            preprocessed_input_placeholder, true_image_shapes=None)
 
         self.assertTrue('box_encodings' in prediction_dict)
         self.assertTrue('class_predictions_with_background' in prediction_dict)
@@ -161,12 +164,42 @@ class SsdMetaArchTest(tf.test.TestCase):
                                       preprocessed_input_placeholder:
                                       np.random.uniform(
                                           size=(batch_size, 2, 2, 3))})
+      expected_box_encodings_shape_out = (batch_size, num_anchors, code_size)
+      expected_class_predictions_with_background_shape_out = (batch_size,
+                                                              num_anchors,
+                                                              num_classes + 1)
+
       self.assertAllEqual(prediction_out['box_encodings'].shape,
                           expected_box_encodings_shape_out)
       self.assertAllEqual(
           prediction_out['class_predictions_with_background'].shape,
           expected_class_predictions_with_background_shape_out)
 
+  def test_predict_result_shapes_on_image_with_static_shape(self):
+
+    with tf.Graph().as_default():
+      _, num_classes, num_anchors, code_size = self._create_model()
+
+    def graph_fn(input_image):
+      model, _, _, _ = self._create_model()
+      predictions = model.predict(input_image, true_image_shapes=None)
+      return (predictions['box_encodings'],
+              predictions['class_predictions_with_background'],
+              predictions['feature_maps'],
+              predictions['anchors'])
+    batch_size = 3
+    image_size = 2
+    channels = 3
+    input_image = np.random.rand(batch_size, image_size, image_size,
+                                 channels).astype(np.float32)
+    expected_box_encodings_shape = (batch_size, num_anchors, code_size)
+    expected_class_predictions_shape = (batch_size, num_anchors, num_classes+1)
+    (box_encodings, class_predictions, _, _) = self.execute(graph_fn,
+                                                            [input_image])
+    self.assertAllEqual(box_encodings.shape, expected_box_encodings_shape)
+    self.assertAllEqual(class_predictions.shape,
+                        expected_class_predictions_shape)
+
   def test_postprocess_results_are_correct(self):
     batch_size = 2
     image_size = 2
@@ -178,26 +211,30 @@ class SsdMetaArchTest(tf.test.TestCase):
     expected_boxes = np.array([[[0, 0, .5, .5],
                                 [0, .5, .5, 1],
                                 [.5, 0, 1, .5],
-                                [.5, .5, 1, 1],
-                                [0, 0, 0, 0]],
+                                [0, 0, 0, 0],   # pruned prediction
+                                [0, 0, 0, 0]],  # padding
                                [[0, 0, .5, .5],
                                 [0, .5, .5, 1],
                                 [.5, 0, 1, .5],
-                                [.5, .5, 1, 1],
-                                [0, 0, 0, 0]]])
+                                [0, 0, 0, 0],  # pruned prediction
+                                [0, 0, 0, 0]]  # padding
+                              ])
     expected_scores = np.array([[0, 0, 0, 0, 0],
                                 [0, 0, 0, 0, 0]])
     expected_classes = np.array([[0, 0, 0, 0, 0],
                                  [0, 0, 0, 0, 0]])
-    expected_num_detections = np.array([4, 4])
+    expected_num_detections = np.array([3, 3])
 
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
       with tf_graph.as_default():
-        preprocessed_input_placeholder = tf.placeholder(tf.float32,
-                                                        shape=input_shape)
-        prediction_dict = self._model.predict(preprocessed_input_placeholder)
-        detections = self._model.postprocess(prediction_dict)
+        model, _, _, _ = self._create_model()
+        input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
+        preprocessed_inputs, true_image_shapes = model.preprocess(
+            input_placeholder)
+        prediction_dict = model.predict(preprocessed_inputs,
+                                        true_image_shapes)
+        detections = model.postprocess(prediction_dict, true_image_shapes)
         self.assertTrue('detection_boxes' in detections)
         self.assertTrue('detection_scores' in detections)
         self.assertTrue('detection_classes' in detections)
@@ -207,7 +244,7 @@ class SsdMetaArchTest(tf.test.TestCase):
         sess.run(init_op)
         detections_out = sess.run(detections,
                                   feed_dict={
-                                      preprocessed_input_placeholder:
+                                      input_placeholder:
                                       np.random.uniform(
                                           size=(batch_size, 2, 2, 3))})
       self.assertAllClose(detections_out['detection_boxes'], expected_boxes)
@@ -217,47 +254,91 @@ class SsdMetaArchTest(tf.test.TestCase):
                           expected_num_detections)
 
   def test_loss_results_are_correct(self):
-    batch_size = 2
-    preprocessed_input = tf.random_uniform((batch_size, 2, 2, 3),
-                                           dtype=tf.float32)
-    groundtruth_boxes_list = [tf.constant([[0, 0, .5, .5]], dtype=tf.float32),
-                              tf.constant([[0, 0, .5, .5]], dtype=tf.float32)]
-    groundtruth_classes_list = [tf.constant([[1]], dtype=tf.float32),
-                                tf.constant([[1]], dtype=tf.float32)]
-    self._model.provide_groundtruth(groundtruth_boxes_list,
-                                    groundtruth_classes_list)
-    prediction_dict = self._model.predict(preprocessed_input)
-    loss_dict = self._model.loss(prediction_dict)
-
-    self.assertTrue('localization_loss' in loss_dict)
-    self.assertTrue('classification_loss' in loss_dict)
 
+    with tf.Graph().as_default():
+      _, num_classes, num_anchors, _ = self._create_model()
+    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
+                 groundtruth_classes1, groundtruth_classes2):
+      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
+      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
+      model, _, _, _ = self._create_model(apply_hard_mining=False)
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+      prediction_dict = model.predict(preprocessed_tensor,
+                                      true_image_shapes=None)
+      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
+      return (loss_dict['localization_loss'], loss_dict['classification_loss'])
+
+    batch_size = 2
+    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
+    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
+    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
+    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
+    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
     expected_localization_loss = 0.0
-    expected_classification_loss = (batch_size * self._num_anchors
-                                    * (self._num_classes+1) * np.log(2.0))
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      losses_out = sess.run(loss_dict)
+    expected_classification_loss = (batch_size * num_anchors
+                                    * (num_classes+1) * np.log(2.0))
+    (localization_loss,
+     classification_loss) = self.execute(graph_fn, [preprocessed_input,
+                                                    groundtruth_boxes1,
+                                                    groundtruth_boxes2,
+                                                    groundtruth_classes1,
+                                                    groundtruth_classes2])
+    self.assertAllClose(localization_loss, expected_localization_loss)
+    self.assertAllClose(classification_loss, expected_classification_loss)
+
+  def test_loss_results_are_correct_with_hard_example_mining(self):
+
+    with tf.Graph().as_default():
+      _, num_classes, num_anchors, _ = self._create_model()
+    def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
+                 groundtruth_classes1, groundtruth_classes2):
+      groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
+      groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
+      model, _, _, _ = self._create_model()
+      model.provide_groundtruth(groundtruth_boxes_list,
+                                groundtruth_classes_list)
+      prediction_dict = model.predict(preprocessed_tensor,
+                                      true_image_shapes=None)
+      loss_dict = model.loss(prediction_dict, true_image_shapes=None)
+      return (loss_dict['localization_loss'], loss_dict['classification_loss'])
 
-      self.assertAllClose(losses_out['localization_loss'],
-                          expected_localization_loss)
-      self.assertAllClose(losses_out['classification_loss'],
-                          expected_classification_loss)
+    batch_size = 2
+    preprocessed_input = np.random.rand(batch_size, 2, 2, 3).astype(np.float32)
+    groundtruth_boxes1 = np.array([[0, 0, .5, .5]], dtype=np.float32)
+    groundtruth_boxes2 = np.array([[0, 0, .5, .5]], dtype=np.float32)
+    groundtruth_classes1 = np.array([[1]], dtype=np.float32)
+    groundtruth_classes2 = np.array([[1]], dtype=np.float32)
+    expected_localization_loss = 0.0
+    expected_classification_loss = (batch_size * num_anchors
+                                    * (num_classes+1) * np.log(2.0))
+    (localization_loss, classification_loss) = self.execute_cpu(
+        graph_fn, [
+            preprocessed_input, groundtruth_boxes1, groundtruth_boxes2,
+            groundtruth_classes1, groundtruth_classes2
+        ])
+    self.assertAllClose(localization_loss, expected_localization_loss)
+    self.assertAllClose(classification_loss, expected_classification_loss)
 
   def test_restore_map_for_detection_ckpt(self):
+    model, _, _, _ = self._create_model()
+    model.predict(tf.constant(np.array([[[0, 0], [1, 1]], [[1, 0], [0, 1]]],
+                                       dtype=np.float32)),
+                  true_image_shapes=None)
     init_op = tf.global_variables_initializer()
     saver = tf.train.Saver()
     save_path = self.get_temp_dir()
     with self.test_session() as sess:
       sess.run(init_op)
       saved_model_path = saver.save(sess, save_path)
-      var_map = self._model.restore_map(from_detection_checkpoint=True)
+      var_map = model.restore_map(
+          from_detection_checkpoint=True,
+          load_all_detection_checkpoint_vars=False)
       self.assertIsInstance(var_map, dict)
       saver = tf.train.Saver(var_map)
       saver.restore(sess, saved_model_path)
       for var in sess.run(tf.report_uninitialized_variables()):
-        self.assertNotIn('FeatureExtractor', var.name)
+        self.assertNotIn('FeatureExtractor', var)
 
   def test_restore_map_for_classification_ckpt(self):
     # Define mock tensorflow classification graph and save variables.
@@ -271,7 +352,7 @@ class SsdMetaArchTest(tf.test.TestCase):
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
       save_path = self.get_temp_dir()
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_classification) as sess:
         sess.run(init_op)
         saved_model_path = saver.save(sess, save_path)
 
@@ -279,19 +360,39 @@ class SsdMetaArchTest(tf.test.TestCase):
     # classification checkpoint.
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
+      model, _, _, _ = self._create_model()
       inputs_shape = [2, 2, 2, 3]
       inputs = tf.to_float(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32))
-      preprocessed_inputs = self._model.preprocess(inputs)
-      prediction_dict = self._model.predict(preprocessed_inputs)
-      self._model.postprocess(prediction_dict)
-      var_map = self._model.restore_map(from_detection_checkpoint=False)
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      model.postprocess(prediction_dict, true_image_shapes)
+      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
+      var_map = model.restore_map(from_detection_checkpoint=False)
+      self.assertNotIn('another_variable', var_map)
       self.assertIsInstance(var_map, dict)
       saver = tf.train.Saver(var_map)
-      with self.test_session() as sess:
+      with self.test_session(graph=test_graph_detection) as sess:
         saver.restore(sess, saved_model_path)
         for var in sess.run(tf.report_uninitialized_variables()):
-          self.assertNotIn('FeatureExtractor', var.name)
+          self.assertNotIn('FeatureExtractor', var)
+
+  def test_load_all_det_checkpoint_vars(self):
+    test_graph_detection = tf.Graph()
+    with test_graph_detection.as_default():
+      model, _, _, _ = self._create_model()
+      inputs_shape = [2, 2, 2, 3]
+      inputs = tf.to_float(
+          tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
+      preprocessed_inputs, true_image_shapes = model.preprocess(inputs)
+      prediction_dict = model.predict(preprocessed_inputs, true_image_shapes)
+      model.postprocess(prediction_dict, true_image_shapes)
+      another_variable = tf.Variable([17.0], name='another_variable')  # pylint: disable=unused-variable
+      var_map = model.restore_map(
+          from_detection_checkpoint=True,
+          load_all_detection_checkpoint_vars=True)
+      self.assertIsInstance(var_map, dict)
+      self.assertIn('another_variable', var_map)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/metrics/BUILD b/research/object_detection/metrics/BUILD
index 878f16a6..1bb2b05a 100644
--- a/research/object_detection/metrics/BUILD
+++ b/research/object_detection/metrics/BUILD
@@ -8,6 +8,57 @@ licenses(["notice"])
 
 # Apache 2.0
 
+py_library(
+    name = "coco_tools",
+    srcs = [
+        "coco_tools.py",
+    ],
+    deps = [
+        "//file/localfile",
+        "//file/placer",
+        "//pycocotools",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:json_utils",
+    ],
+)
+
+py_test(
+    name = "coco_tools_test",
+    srcs = [
+        "coco_tools_test.py",
+    ],
+    deps = [
+        ":coco_tools",
+        "//testing/pybase",
+        "//numpy",
+    ],
+)
+
+py_library(
+    name = "coco_evaluation",
+    srcs = [
+        "coco_evaluation.py",
+    ],
+    deps = [
+        ":coco_tools",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:object_detection_evaluation",
+    ],
+)
+
+py_test(
+    name = "coco_evaluation_test",
+    srcs = [
+        "coco_evaluation_test.py",
+    ],
+    deps = [
+        ":coco_evaluation",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+    ],
+)
+
 py_binary(
     name = "offline_eval_map_corloc",
     srcs = [
@@ -15,11 +66,11 @@ py_binary(
     ],
     deps = [
         ":tf_example_parser",
-        "//tensorflow_models/object_detection:evaluator",
-        "//tensorflow_models/object_detection/builders:input_reader_builder",
-        "//tensorflow_models/object_detection/core:standard_fields",
-        "//tensorflow_models/object_detection/utils:config_util",
-        "//tensorflow_models/object_detection/utils:label_map_util",
+        "//tensorflow/models/research/object_detection:evaluator",
+        "//tensorflow/models/research/object_detection/builders:input_reader_builder",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/utils:config_util",
+        "//tensorflow/models/research/object_detection/utils:label_map_util",
     ],
 )
 
@@ -39,8 +90,8 @@ py_library(
     srcs = ["tf_example_parser.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:data_parser",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:data_parser",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -50,6 +101,6 @@ py_test(
     deps = [
         ":tf_example_parser",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
diff --git a/research/object_detection/metrics/__init__.py b/research/object_detection/metrics/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/object_detection/metrics/coco_evaluation.py b/research/object_detection/metrics/coco_evaluation.py
new file mode 100644
index 00000000..c599b5bd
--- /dev/null
+++ b/research/object_detection/metrics/coco_evaluation.py
@@ -0,0 +1,451 @@
+"""Class for evaluating object detections with COCO metrics."""
+import numpy as np
+import tensorflow as tf
+
+from object_detection.core import standard_fields
+from object_detection.metrics import coco_tools
+from object_detection.utils import object_detection_evaluation
+
+
+class CocoDetectionEvaluator(object_detection_evaluation.DetectionEvaluator):
+  """Class to evaluate COCO detection metrics."""
+
+  def __init__(self, categories, all_metrics_per_category=False):
+    """Constructor.
+
+    Args:
+      categories: A list of dicts, each of which has the following keys -
+        'id': (required) an integer id uniquely identifying this category.
+        'name': (required) string representing category name e.g., 'cat', 'dog'.
+      all_metrics_per_category: Whether to include all the summary metrics for
+        each category in per_category_ap. Be careful with setting it to true if
+        you have more than handful of categories, because it will pollute
+        your mldash.
+    """
+    super(CocoDetectionEvaluator, self).__init__(categories)
+    # _image_ids is a dictionary that maps unique image ids to Booleans which
+    # indicate whether a corresponding detection has been added.
+    self._image_ids = {}
+    self._groundtruth_list = []
+    self._detection_boxes_list = []
+    self._category_id_set = set([cat['id'] for cat in self._categories])
+    self._annotation_id = 1
+    self._metrics = None
+    self._all_metrics_per_category = all_metrics_per_category
+
+  def clear(self):
+    """Clears the state to prepare for a fresh evaluation."""
+    self._image_ids.clear()
+    self._groundtruth_list = []
+    self._detection_boxes_list = []
+
+  def add_single_ground_truth_image_info(self,
+                                         image_id,
+                                         groundtruth_dict):
+    """Adds groundtruth for a single image to be used for evaluation.
+
+    If the image has already been added, a warning is logged, and groundtruth is
+    ignored.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      groundtruth_dict: A dictionary containing -
+        InputDataFields.groundtruth_boxes: float32 numpy array of shape
+          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format
+          [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        InputDataFields.groundtruth_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed groundtruth classes for the boxes.
+    """
+    if image_id in self._image_ids:
+      tf.logging.warning('Ignoring ground truth with image id %s since it was '
+                         'previously added', image_id)
+      return
+
+    self._groundtruth_list.extend(
+        coco_tools.
+        ExportSingleImageGroundtruthToCoco(
+            image_id=image_id,
+            next_annotation_id=self._annotation_id,
+            category_id_set=self._category_id_set,
+            groundtruth_boxes=groundtruth_dict[standard_fields.InputDataFields.
+                                               groundtruth_boxes],
+            groundtruth_classes=groundtruth_dict[standard_fields.
+                                                 InputDataFields.
+                                                 groundtruth_classes]))
+    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.
+                                            groundtruth_boxes].shape[0]
+    self._image_ids[image_id] = False
+
+  def add_single_detected_image_info(self,
+                                     image_id,
+                                     detections_dict):
+    """Adds detections for a single image to be used for evaluation.
+
+    If a detection has already been added for this image id, a warning is
+    logged, and the detection is skipped.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      detections_dict: A dictionary containing -
+        DetectionResultFields.detection_boxes: float32 numpy array of shape
+          [num_boxes, 4] containing `num_boxes` detection boxes of the format
+          [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        DetectionResultFields.detection_scores: float32 numpy array of shape
+          [num_boxes] containing detection scores for the boxes.
+        DetectionResultFields.detection_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed detection classes for the boxes.
+        DetectionResultFields.detection_masks: optional uint8 numpy array of
+          shape [num_boxes, image_height, image_width] containing instance
+          masks for the boxes.
+
+    Raises:
+      ValueError: If groundtruth for the image_id is not available.
+    """
+    if image_id not in self._image_ids:
+      raise ValueError('Missing groundtruth for image id: {}'.format(image_id))
+
+    if self._image_ids[image_id]:
+      tf.logging.warning('Ignoring detection with image id %s since it was '
+                         'previously added', image_id)
+      return
+
+    self._detection_boxes_list.extend(
+        coco_tools.ExportSingleImageDetectionBoxesToCoco(
+            image_id=image_id,
+            category_id_set=self._category_id_set,
+            detection_boxes=detections_dict[standard_fields.
+                                            DetectionResultFields
+                                            .detection_boxes],
+            detection_scores=detections_dict[standard_fields.
+                                             DetectionResultFields.
+                                             detection_scores],
+            detection_classes=detections_dict[standard_fields.
+                                              DetectionResultFields.
+                                              detection_classes]))
+    self._image_ids[image_id] = True
+
+  def evaluate(self):
+    """Evaluates the detection boxes and returns a dictionary of coco metrics.
+
+    Returns:
+      A dictionary holding -
+
+      1. summary_metrics:
+      'DetectionBoxes_Precision/mAP': mean average precision over classes
+        averaged over IOU thresholds ranging from .5 to .95 with .05
+        increments.
+      'DetectionBoxes_Precision/mAP@.50IOU': mean average precision at 50% IOU
+      'DetectionBoxes_Precision/mAP@.75IOU': mean average precision at 75% IOU
+      'DetectionBoxes_Precision/mAP (small)': mean average precision for small
+        objects (area < 32^2 pixels).
+      'DetectionBoxes_Precision/mAP (medium)': mean average precision for
+        medium sized objects (32^2 pixels < area < 96^2 pixels).
+      'DetectionBoxes_Precision/mAP (large)': mean average precision for large
+        objects (96^2 pixels < area < 10000^2 pixels).
+      'DetectionBoxes_Recall/AR@1': average recall with 1 detection.
+      'DetectionBoxes_Recall/AR@10': average recall with 10 detections.
+      'DetectionBoxes_Recall/AR@100': average recall with 100 detections.
+      'DetectionBoxes_Recall/AR@100 (small)': average recall for small objects
+        with 100.
+      'DetectionBoxes_Recall/AR@100 (medium)': average recall for medium objects
+        with 100.
+      'DetectionBoxes_Recall/AR@100 (large)': average recall for large objects
+        with 100 detections.
+
+      2. per_category_ap: category specific results with keys of the form:
+      'Precision mAP ByCategory/category' (without the supercategory part if
+      no supercategories exist). For backward compatibility
+      'PerformanceByCategory' is included in the output regardless of
+      all_metrics_per_category.
+    """
+    groundtruth_dict = {
+        'annotations': self._groundtruth_list,
+        'images': [{'id': image_id} for image_id in self._image_ids],
+        'categories': self._categories
+    }
+    coco_wrapped_groundtruth = coco_tools.COCOWrapper(groundtruth_dict)
+    coco_wrapped_detections = coco_wrapped_groundtruth.LoadAnnotations(
+        self._detection_boxes_list)
+    box_evaluator = coco_tools.COCOEvalWrapper(
+        coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)
+    box_metrics, box_per_category_ap = box_evaluator.ComputeMetrics(
+        all_metrics_per_category=self._all_metrics_per_category)
+    box_metrics.update(box_per_category_ap)
+    box_metrics = {'DetectionBoxes_'+ key: value
+                   for key, value in box_metrics.iteritems()}
+    return box_metrics
+
+  def get_estimator_eval_metric_ops(self, image_id, groundtruth_boxes,
+                                    groundtruth_classes, detection_boxes,
+                                    detection_scores, detection_classes):
+    """Returns a dictionary of eval metric ops to use with `tf.EstimatorSpec`.
+
+    Note that once value_op is called, the detections and groundtruth added via
+    update_op are cleared.
+
+    Args:
+      image_id: Unique string/integer identifier for the image.
+      groundtruth_boxes: float32 tensor of shape [num_boxes, 4] containing
+        `num_boxes` groundtruth boxes of the format
+        [ymin, xmin, ymax, xmax] in absolute image coordinates.
+      groundtruth_classes: int32 tensor of shape [num_boxes] containing
+        1-indexed groundtruth classes for the boxes.
+      detection_boxes: float32 tensor of shape [num_boxes, 4] containing
+        `num_boxes` detection boxes of the format [ymin, xmin, ymax, xmax]
+        in absolute image coordinates.
+      detection_scores: float32 tensor of shape [num_boxes] containing
+        detection scores for the boxes.
+      detection_classes: int32 tensor of shape [num_boxes] containing
+        1-indexed detection classes for the boxes.
+
+    Returns:
+      a dictionary of metric names to tuple of value_op and update_op that can
+      be used as eval metric ops in tf.EstimatorSpec. Note that all update ops
+      must be run together and similarly all value ops must be run together to
+      guarantee correct behaviour.
+    """
+    def update_op(
+        image_id,
+        groundtruth_boxes,
+        groundtruth_classes,
+        detection_boxes,
+        detection_scores,
+        detection_classes):
+      self.add_single_ground_truth_image_info(
+          image_id,
+          {'groundtruth_boxes': groundtruth_boxes,
+           'groundtruth_classes': groundtruth_classes})
+      self.add_single_detected_image_info(
+          image_id,
+          {'detection_boxes': detection_boxes,
+           'detection_scores': detection_scores,
+           'detection_classes': detection_classes})
+
+    update_op = tf.py_func(update_op, [image_id,
+                                       groundtruth_boxes,
+                                       groundtruth_classes,
+                                       detection_boxes,
+                                       detection_scores,
+                                       detection_classes], [])
+    metric_names = ['DetectionBoxes_Precision/mAP',
+                    'DetectionBoxes_Precision/mAP@.50IOU',
+                    'DetectionBoxes_Precision/mAP@.75IOU',
+                    'DetectionBoxes_Precision/mAP (large)',
+                    'DetectionBoxes_Precision/mAP (medium)',
+                    'DetectionBoxes_Precision/mAP (small)',
+                    'DetectionBoxes_Recall/AR@1',
+                    'DetectionBoxes_Recall/AR@10',
+                    'DetectionBoxes_Recall/AR@100',
+                    'DetectionBoxes_Recall/AR@100 (large)',
+                    'DetectionBoxes_Recall/AR@100 (medium)',
+                    'DetectionBoxes_Recall/AR@100 (small)']
+    for category_dict in self._categories:
+      metric_names.append('DetectionBoxes_PerformanceByCategory/mAP/' +
+                          category_dict['name'])
+
+    def first_value_func():
+      self._metrics = self.evaluate()
+      self.clear()
+      return np.float32(self._metrics[metric_names[0]])
+
+    def value_func_factory(metric_name):
+      def value_func():
+        return np.float32(self._metrics[metric_name])
+      return value_func
+
+    first_value_op = tf.py_func(first_value_func, [], tf.float32)
+    eval_metric_ops = {metric_names[0]: (first_value_op, update_op)}
+    with tf.control_dependencies([first_value_op]):
+      for metric_name in metric_names[1:]:
+        eval_metric_ops[metric_name] = (tf.py_func(
+            value_func_factory(metric_name), [], np.float32), update_op)
+    return eval_metric_ops
+
+
+def _check_mask_type_and_value(array_name, masks):
+  """Checks whether mask dtype is uint8 anf the values are either 0 or 1."""
+  if masks.dtype != np.uint8:
+    raise ValueError('{} must be of type np.uint8. Found {}.'.format(
+        array_name, masks.dtype))
+  if np.any(np.logical_and(masks != 0, masks != 1)):
+    raise ValueError('{} elements can only be either 0 or 1.'.format(
+        array_name))
+
+
+class CocoMaskEvaluator(object_detection_evaluation.DetectionEvaluator):
+  """Class to evaluate COCO detection metrics."""
+
+  def __init__(self, categories):
+    """Constructor.
+
+    Args:
+      categories: A list of dicts, each of which has the following keys -
+        'id': (required) an integer id uniquely identifying this category.
+        'name': (required) string representing category name e.g., 'cat', 'dog'.
+    """
+    super(CocoMaskEvaluator, self).__init__(categories)
+    self._image_id_to_mask_shape_map = {}
+    self._image_ids_with_detections = set([])
+    self._groundtruth_list = []
+    self._detection_masks_list = []
+    self._category_id_set = set([cat['id'] for cat in self._categories])
+    self._annotation_id = 1
+
+  def clear(self):
+    """Clears the state to prepare for a fresh evaluation."""
+    self._image_id_to_mask_shape_map.clear()
+    self._image_ids_with_detections.clear()
+    self._groundtruth_list = []
+    self._detection_masks_list = []
+
+  def add_single_ground_truth_image_info(self,
+                                         image_id,
+                                         groundtruth_dict):
+    """Adds groundtruth for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      groundtruth_dict: A dictionary containing -
+        InputDataFields.groundtruth_boxes: float32 numpy array of shape
+          [num_boxes, 4] containing `num_boxes` groundtruth boxes of the format
+          [ymin, xmin, ymax, xmax] in absolute image coordinates.
+        InputDataFields.groundtruth_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed groundtruth classes for the boxes.
+        InputDataFields.groundtruth_instance_masks: uint8 numpy array of shape
+          [num_boxes, image_height, image_width] containing groundtruth masks
+          corresponding to the boxes. The elements of the array must be in
+          {0, 1}.
+    """
+    if image_id in self._image_id_to_mask_shape_map:
+      tf.logging.warning('Ignoring ground truth with image id %s since it was '
+                         'previously added', image_id)
+      return
+
+    groundtruth_instance_masks = groundtruth_dict[
+        standard_fields.InputDataFields.groundtruth_instance_masks]
+    _check_mask_type_and_value(standard_fields.InputDataFields.
+                               groundtruth_instance_masks,
+                               groundtruth_instance_masks)
+    self._groundtruth_list.extend(
+        coco_tools.
+        ExportSingleImageGroundtruthToCoco(
+            image_id=image_id,
+            next_annotation_id=self._annotation_id,
+            category_id_set=self._category_id_set,
+            groundtruth_boxes=groundtruth_dict[standard_fields.InputDataFields.
+                                               groundtruth_boxes],
+            groundtruth_classes=groundtruth_dict[standard_fields.
+                                                 InputDataFields.
+                                                 groundtruth_classes],
+            groundtruth_masks=groundtruth_instance_masks))
+    self._annotation_id += groundtruth_dict[standard_fields.InputDataFields.
+                                            groundtruth_boxes].shape[0]
+    self._image_id_to_mask_shape_map[image_id] = groundtruth_dict[
+        standard_fields.InputDataFields.groundtruth_instance_masks].shape
+
+  def add_single_detected_image_info(self,
+                                     image_id,
+                                     detections_dict):
+    """Adds detections for a single image to be used for evaluation.
+
+    Args:
+      image_id: A unique string/integer identifier for the image.
+      detections_dict: A dictionary containing -
+        DetectionResultFields.detection_scores: float32 numpy array of shape
+          [num_boxes] containing detection scores for the boxes.
+        DetectionResultFields.detection_classes: integer numpy array of shape
+          [num_boxes] containing 1-indexed detection classes for the boxes.
+        DetectionResultFields.detection_masks: optional uint8 numpy array of
+          shape [num_boxes, image_height, image_width] containing instance
+          masks corresponding to the boxes. The elements of the array must be
+          in {0, 1}.
+
+    Raises:
+      ValueError: If groundtruth for the image_id is not available or if
+        spatial shapes of groundtruth_instance_masks and detection_masks are
+        incompatible.
+    """
+    if image_id not in self._image_id_to_mask_shape_map:
+      raise ValueError('Missing groundtruth for image id: {}'.format(image_id))
+
+    if image_id in self._image_ids_with_detections:
+      tf.logging.warning('Ignoring detection with image id %s since it was '
+                         'previously added', image_id)
+      return
+
+    groundtruth_masks_shape = self._image_id_to_mask_shape_map[image_id]
+    detection_masks = detections_dict[standard_fields.DetectionResultFields.
+                                      detection_masks]
+    if groundtruth_masks_shape[1:] != detection_masks.shape[1:]:
+      raise ValueError('Spatial shape of groundtruth masks and detection masks '
+                       'are incompatible: {} vs {}'.format(
+                           groundtruth_masks_shape,
+                           detection_masks.shape))
+    _check_mask_type_and_value(standard_fields.DetectionResultFields.
+                               detection_masks,
+                               detection_masks)
+    self._detection_masks_list.extend(
+        coco_tools.ExportSingleImageDetectionMasksToCoco(
+            image_id=image_id,
+            category_id_set=self._category_id_set,
+            detection_masks=detection_masks,
+            detection_scores=detections_dict[standard_fields.
+                                             DetectionResultFields.
+                                             detection_scores],
+            detection_classes=detections_dict[standard_fields.
+                                              DetectionResultFields.
+                                              detection_classes]))
+    self._image_ids_with_detections.update([image_id])
+
+  def evaluate(self):
+    """Evaluates the detection masks and returns a dictionary of coco metrics.
+
+    Returns:
+      A dictionary holding -
+
+      1. summary_metrics:
+      'Precision/mAP': mean average precision over classes averaged over IOU
+        thresholds ranging from .5 to .95 with .05 increments
+      'Precision/mAP@.50IOU': mean average precision at 50% IOU
+      'Precision/mAP@.75IOU': mean average precision at 75% IOU
+      'Precision/mAP (small)': mean average precision for small objects
+                      (area < 32^2 pixels)
+      'Precision/mAP (medium)': mean average precision for medium sized
+                      objects (32^2 pixels < area < 96^2 pixels)
+      'Precision/mAP (large)': mean average precision for large objects
+                      (96^2 pixels < area < 10000^2 pixels)
+      'Recall/AR@1': average recall with 1 detection
+      'Recall/AR@10': average recall with 10 detections
+      'Recall/AR@100': average recall with 100 detections
+      'Recall/AR@100 (small)': average recall for small objects with 100
+        detections
+      'Recall/AR@100 (medium)': average recall for medium objects with 100
+        detections
+      'Recall/AR@100 (large)': average recall for large objects with 100
+        detections
+
+      2. per_category_ap: category specific results with keys of the form:
+      'Precision mAP ByCategory/category' (without the supercategory part if
+      no supercategories exist). For backward compatibility
+      'PerformanceByCategory' is included in the output regardless of
+      all_metrics_per_category.
+    """
+    groundtruth_dict = {
+        'annotations': self._groundtruth_list,
+        'images': [{'id': image_id, 'height': shape[1], 'width': shape[2]}
+                   for image_id, shape in self._image_id_to_mask_shape_map.
+                   iteritems()],
+        'categories': self._categories
+    }
+    coco_wrapped_groundtruth = coco_tools.COCOWrapper(
+        groundtruth_dict, detection_type='segmentation')
+    coco_wrapped_detection_masks = coco_wrapped_groundtruth.LoadAnnotations(
+        self._detection_masks_list)
+    mask_evaluator = coco_tools.COCOEvalWrapper(
+        coco_wrapped_groundtruth, coco_wrapped_detection_masks,
+        agnostic_mode=False, iou_type='segm')
+    mask_metrics, mask_per_category_ap = mask_evaluator.ComputeMetrics()
+    mask_metrics.update(mask_per_category_ap)
+    mask_metrics = {'DetectionMasks_'+ key: value
+                    for key, value in mask_metrics.iteritems()}
+    return mask_metrics
diff --git a/research/object_detection/metrics/coco_evaluation_test.py b/research/object_detection/metrics/coco_evaluation_test.py
new file mode 100644
index 00000000..30c61549
--- /dev/null
+++ b/research/object_detection/metrics/coco_evaluation_test.py
@@ -0,0 +1,365 @@
+"""Tests for image.understanding.object_detection.metrics.coco_evaluation."""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import math
+import numpy as np
+import tensorflow as tf
+from object_detection.core import standard_fields
+from object_detection.metrics import coco_evaluation
+
+
+class CocoDetectionEvaluationTest(tf.test.TestCase):
+
+  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
+    """Tests that mAP is calculated correctly on GT and Detections."""
+    category_list = [{'id': 0, 'name': 'person'},
+                     {'id': 1, 'name': 'cat'},
+                     {'id': 2, 'name': 'dog'}]
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image2',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[50., 50., 100., 100.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image2',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[50., 50., 100., 100.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image3',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[25., 25., 50., 50.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image3',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[25., 25., 50., 50.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
+
+  def testReturnAllMetricsPerCategory(self):
+    """Tests that mAP is calculated correctly on GT and Detections."""
+    category_list = [{'id': 0, 'name': 'person'}]
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(
+        category_list, all_metrics_per_category=True)
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    metrics = coco_evaluator.evaluate()
+    expected_metrics = [
+        'DetectionBoxes_Recall AR@10 ByCategory/person',
+        'DetectionBoxes_Precision mAP (medium) ByCategory/person',
+        'DetectionBoxes_Precision mAP ByCategory/person',
+        'DetectionBoxes_Precision mAP@.50IOU ByCategory/person',
+        'DetectionBoxes_Precision mAP (small) ByCategory/person',
+        'DetectionBoxes_Precision mAP (large) ByCategory/person',
+        'DetectionBoxes_Recall AR@1 ByCategory/person',
+        'DetectionBoxes_Precision mAP@.75IOU ByCategory/person',
+        'DetectionBoxes_Recall AR@100 ByCategory/person',
+        'DetectionBoxes_Recall AR@100 (medium) ByCategory/person',
+        'DetectionBoxes_Recall AR@100 (large) ByCategory/person']
+    self.assertTrue(set(expected_metrics).issubset(set(metrics)))
+
+  def testRejectionOnDuplicateGroundtruth(self):
+    """Tests that groundtruth cannot be added more than once for an image."""
+    categories = [{'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'},
+                  {'id': 3, 'name': 'elephant'}]
+    #  Add groundtruth
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)
+    image_key1 = 'img1'
+    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
+                                  dtype=float)
+    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)
+    coco_evaluator.add_single_ground_truth_image_info(image_key1, {
+        standard_fields.InputDataFields.groundtruth_boxes:
+            groundtruth_boxes1,
+        standard_fields.InputDataFields.groundtruth_classes:
+            groundtruth_class_labels1
+    })
+    groundtruth_lists_len = len(coco_evaluator._groundtruth_list)
+
+    # Add groundtruth with the same image id.
+    coco_evaluator.add_single_ground_truth_image_info(image_key1, {
+        standard_fields.InputDataFields.groundtruth_boxes:
+            groundtruth_boxes1,
+        standard_fields.InputDataFields.groundtruth_classes:
+            groundtruth_class_labels1
+    })
+    self.assertEqual(groundtruth_lists_len,
+                     len(coco_evaluator._groundtruth_list))
+
+  def testRejectionOnDuplicateDetections(self):
+    """Tests that detections cannot be added more than once for an image."""
+    categories = [{'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'},
+                  {'id': 3, 'name': 'elephant'}]
+    #  Add groundtruth
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[99., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1])
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    detections_lists_len = len(coco_evaluator._detection_boxes_list)
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',  # Note that this image id was previously added.
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1])
+        })
+    self.assertEqual(detections_lists_len,
+                     len(coco_evaluator._detection_boxes_list))
+
+  def testExceptionRaisedWithMissingGroundtruth(self):
+    """Tests that exception is raised for detection with missing groundtruth."""
+    categories = [{'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'},
+                  {'id': 3, 'name': 'elephant'}]
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(categories)
+    with self.assertRaises(ValueError):
+      coco_evaluator.add_single_detected_image_info(
+          image_id='image1',
+          detections_dict={
+              standard_fields.DetectionResultFields.detection_boxes:
+                  np.array([[100., 100., 200., 200.]]),
+              standard_fields.DetectionResultFields.detection_scores:
+                  np.array([.8]),
+              standard_fields.DetectionResultFields.detection_classes:
+                  np.array([1])
+          })
+
+
+class CocoEvaluationPyFuncTest(tf.test.TestCase):
+
+  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
+    category_list = [{'id': 0, 'name': 'person'},
+                     {'id': 1, 'name': 'cat'},
+                     {'id': 2, 'name': 'dog'}]
+    coco_evaluator = coco_evaluation.CocoDetectionEvaluator(category_list)
+    image_id = tf.placeholder(tf.string, shape=())
+    groundtruth_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    groundtruth_classes = tf.placeholder(tf.float32, shape=(None))
+    detection_boxes = tf.placeholder(tf.float32, shape=(None, 4))
+    detection_scores = tf.placeholder(tf.float32, shape=(None))
+    detection_classes = tf.placeholder(tf.float32, shape=(None))
+
+    eval_metric_ops = coco_evaluator.get_estimator_eval_metric_ops(
+        image_id, groundtruth_boxes,
+        groundtruth_classes,
+        detection_boxes,
+        detection_scores,
+        detection_classes)
+
+    _, update_op = eval_metric_ops['DetectionBoxes_Precision/mAP']
+
+    with self.test_session() as sess:
+      sess.run(update_op,
+               feed_dict={
+                   image_id: 'image1',
+                   groundtruth_boxes: np.array([[100., 100., 200., 200.]]),
+                   groundtruth_classes: np.array([1]),
+                   detection_boxes: np.array([[100., 100., 200., 200.]]),
+                   detection_scores: np.array([.8]),
+                   detection_classes: np.array([1])
+               })
+      sess.run(update_op,
+               feed_dict={
+                   image_id: 'image2',
+                   groundtruth_boxes: np.array([[50., 50., 100., 100.]]),
+                   groundtruth_classes: np.array([3]),
+                   detection_boxes: np.array([[50., 50., 100., 100.]]),
+                   detection_scores: np.array([.7]),
+                   detection_classes: np.array([3])
+               })
+      sess.run(update_op,
+               feed_dict={
+                   image_id: 'image3',
+                   groundtruth_boxes: np.array([[25., 25., 50., 50.]]),
+                   groundtruth_classes: np.array([2]),
+                   detection_boxes: np.array([[25., 25., 50., 50.]]),
+                   detection_scores: np.array([.9]),
+                   detection_classes: np.array([2])
+               })
+    metrics = {}
+    for key, (value_op, _) in eval_metric_ops.iteritems():
+      metrics[key] = value_op
+    metrics = sess.run(metrics)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP@.50IOU'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP@.75IOU'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP (large)'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP (medium)'],
+                           -1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Precision/mAP (small)'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@1'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@10'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100 (large)'], 1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100 (medium)'],
+                           -1.0)
+    self.assertAlmostEqual(metrics['DetectionBoxes_Recall/AR@100 (small)'], 1.0)
+    self.assertAlmostEqual(metrics[
+        'DetectionBoxes_PerformanceByCategory/mAP/dog'], 1.0)
+    self.assertAlmostEqual(metrics[
+        'DetectionBoxes_PerformanceByCategory/mAP/cat'], 1.0)
+    self.assertTrue(math.isnan(metrics[
+        'DetectionBoxes_PerformanceByCategory/mAP/person']))
+    self.assertFalse(coco_evaluator._groundtruth_list)
+    self.assertFalse(coco_evaluator._detection_boxes_list)
+    self.assertFalse(coco_evaluator._image_ids)
+
+
+class CocoMaskEvaluationTest(tf.test.TestCase):
+
+  def testGetOneMAPWithMatchingGroundtruthAndDetections(self):
+    category_list = [{'id': 0, 'name': 'person'},
+                     {'id': 1, 'name': 'cat'},
+                     {'id': 2, 'name': 'dog'}]
+    coco_evaluator = coco_evaluation.CocoMaskEvaluator(category_list)
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image1',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+            np.pad(np.ones([1, 100, 100], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image1',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[100., 100., 200., 200.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1]),
+            standard_fields.DetectionResultFields.detection_masks:
+            np.pad(np.ones([1, 100, 100], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image2',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[50., 50., 100., 100.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+            np.pad(np.ones([1, 50, 50], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image2',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[50., 50., 100., 100.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1]),
+            standard_fields.DetectionResultFields.detection_masks:
+            np.pad(np.ones([1, 50, 50], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    coco_evaluator.add_single_ground_truth_image_info(
+        image_id='image3',
+        groundtruth_dict={
+            standard_fields.InputDataFields.groundtruth_boxes:
+            np.array([[25., 25., 50., 50.]]),
+            standard_fields.InputDataFields.groundtruth_classes: np.array([1]),
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+            np.pad(np.ones([1, 25, 25], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    coco_evaluator.add_single_detected_image_info(
+        image_id='image3',
+        detections_dict={
+            standard_fields.DetectionResultFields.detection_boxes:
+            np.array([[25., 25., 50., 50.]]),
+            standard_fields.DetectionResultFields.detection_scores:
+            np.array([.8]),
+            standard_fields.DetectionResultFields.detection_classes:
+            np.array([1]),
+            standard_fields.DetectionResultFields.detection_masks:
+            np.pad(np.ones([1, 25, 25], dtype=np.uint8),
+                   ((0, 0), (10, 10), (10, 10)), mode='constant')
+        })
+    metrics = coco_evaluator.evaluate()
+    self.assertAlmostEqual(metrics['DetectionMasks_Precision/mAP'], 1.0)
+    coco_evaluator.clear()
+    self.assertFalse(coco_evaluator._image_id_to_mask_shape_map)
+    self.assertFalse(coco_evaluator._image_ids_with_detections)
+    self.assertFalse(coco_evaluator._groundtruth_list)
+    self.assertFalse(coco_evaluator._detection_masks_list)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/coco_tools.py b/research/object_detection/metrics/coco_tools.py
new file mode 100644
index 00000000..4195b146
--- /dev/null
+++ b/research/object_detection/metrics/coco_tools.py
@@ -0,0 +1,813 @@
+"""Wrappers for third party pycocotools to be used within i/u/object_detection.
+
+Note that nothing in this file is tensorflow related and thus cannot
+be called directly as a slim metric, for example.
+
+TODO: wrap as a slim metric in metrics.py
+
+
+Usage example: given a set of images with ids in the list image_ids
+and corresponding lists of numpy arrays encoding groundtruth (boxes and classes)
+and detections (boxes, scores and classes), where elements of each list
+correspond to detections/annotations of a single image,
+then evaluation (in multi-class mode) can be invoked as follows:
+
+  groundtruth_dict = coco_tools.ExportGroundtruthToCOCO(
+      image_ids, groundtruth_boxes_list, groundtruth_classes_list,
+      max_num_classes, output_path=None)
+  detections_list = coco_tools.ExportDetectionsToCOCO(
+      image_ids, detection_boxes_list, detection_scores_list,
+      detection_classes_list, output_path=None)
+  groundtruth = coco_tools.COCOWrapper(groundtruth_dict)
+  detections = groundtruth.LoadAnnotations(detections_list)
+  evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,
+                                         agnostic_mode=False)
+  metrics = evaluator.ComputeMetrics()
+
+"""
+from collections import OrderedDict
+import copy
+import time
+import numpy as np
+
+from pycocotools import coco
+from pycocotools import cocoeval
+from pycocotools import mask
+
+import tensorflow as tf
+
+from object_detection.utils import json_utils
+
+
+class COCOWrapper(coco.COCO):
+  """Wrapper for the pycocotools COCO class."""
+
+  def __init__(self, dataset, detection_type='bbox'):
+    """COCOWrapper constructor.
+
+    See http://mscoco.org/dataset/#format for a description of the format.
+    By default, the coco.COCO class constructor reads from a JSON file.
+    This function duplicates the same behavior but loads from a dictionary,
+    allowing us to perform evaluation without writing to external storage.
+
+    Args:
+      dataset: a dictionary holding bounding box annotations in the COCO format.
+      detection_type: type of detections being wrapped. Can be one of ['bbox',
+        'segmentation']
+
+    Raises:
+      ValueError: if detection_type is unsupported.
+    """
+    supported_detection_types = ['bbox', 'segmentation']
+    if detection_type not in supported_detection_types:
+      raise ValueError('Unsupported detection type: {}. '
+                       'Supported values are: {}'.format(
+                           detection_type, supported_detection_types))
+    self._detection_type = detection_type
+    coco.COCO.__init__(self)
+    self.dataset = dataset
+    self.createIndex()
+
+  def LoadAnnotations(self, annotations):
+    """Load annotations dictionary into COCO datastructure.
+
+    See http://mscoco.org/dataset/#format for a description of the annotations
+    format.  As above, this function replicates the default behavior of the API
+    but does not require writing to external storage.
+
+    Args:
+      annotations: python list holding object detection results where each
+        detection is encoded as a dict with required keys ['image_id',
+        'category_id', 'score'] and one of ['bbox', 'segmentation'] based on
+        `detection_type`.
+
+    Returns:
+      a coco.COCO datastructure holding object detection annotations results
+
+    Raises:
+      ValueError: if annotations is not a list
+      ValueError: if annotations do not correspond to the images contained
+        in self.
+    """
+    results = coco.COCO()
+    results.dataset['images'] = [img for img in self.dataset['images']]
+
+    tf.logging.info('Loading and preparing annotation results...')
+    tic = time.time()
+
+    if not isinstance(annotations, list):
+      raise ValueError('annotations is not a list of objects')
+    annotation_img_ids = [ann['image_id'] for ann in annotations]
+    if (set(annotation_img_ids) != (set(annotation_img_ids)
+                                    & set(self.getImgIds()))):
+      raise ValueError('Results do not correspond to current coco set')
+    results.dataset['categories'] = copy.deepcopy(self.dataset['categories'])
+    if self._detection_type == 'bbox':
+      for idx, ann in enumerate(annotations):
+        bb = ann['bbox']
+        ann['area'] = bb[2] * bb[3]
+        ann['id'] = idx + 1
+        ann['iscrowd'] = 0
+    elif self._detection_type == 'segmentation':
+      for idx, ann in enumerate(annotations):
+        ann['area'] = mask.area(ann['segmentation'])
+        ann['bbox'] = mask.toBbox(ann['segmentation'])
+        ann['id'] = idx + 1
+        ann['iscrowd'] = 0
+    tf.logging.info('DONE (t=%0.2fs)', (time.time() - tic))
+
+    results.dataset['annotations'] = annotations
+    results.createIndex()
+    return results
+
+
+class COCOEvalWrapper(cocoeval.COCOeval):
+  """Wrapper for the pycocotools COCOeval class.
+
+  To evaluate, create two objects (groundtruth_dict and detections_list)
+  using the conventions listed at http://mscoco.org/dataset/#format.
+  Then call evaluation as follows:
+
+    groundtruth = coco_tools.COCOWrapper(groundtruth_dict)
+    detections = groundtruth.LoadAnnotations(detections_list)
+    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections,
+                                           agnostic_mode=False)
+
+    metrics = evaluator.ComputeMetrics()
+  """
+
+  def __init__(self, groundtruth=None, detections=None, agnostic_mode=False,
+               iou_type='bbox'):
+    """COCOEvalWrapper constructor.
+
+    Note that for the area-based metrics to be meaningful, detection and
+    groundtruth boxes must be in image coordinates measured in pixels.
+
+    Args:
+      groundtruth: a coco.COCO (or coco_tools.COCOWrapper) object holding
+        groundtruth annotations
+      detections: a coco.COCO (or coco_tools.COCOWrapper) object holding
+        detections
+      agnostic_mode: boolean (default: False).  If True, evaluation ignores
+        class labels, treating all detections as proposals.
+      iou_type: IOU type to use for evaluation. Supports `bbox` or `segm`.
+    """
+    cocoeval.COCOeval.__init__(self, groundtruth, detections,
+                               iouType=iou_type)
+    if agnostic_mode:
+      self.params.useCats = 0
+
+  def GetCategory(self, category_id):
+    """Fetches dictionary holding category information given category id.
+
+    Args:
+      category_id: integer id
+    Returns:
+      dictionary holding 'id', 'name'.
+    """
+    return self.cocoGt.cats[category_id]
+
+  def GetAgnosticMode(self):
+    """Returns true if COCO Eval is configured to evaluate in agnostic mode."""
+    return self.params.useCats == 0
+
+  def GetCategoryIdList(self):
+    """Returns list of valid category ids."""
+    return self.params.catIds
+
+  def ComputeMetrics(self, all_metrics_per_category=False):
+    """Computes detection metrics.
+
+    Args:
+      all_metrics_per_category: If true, include all the summery metrics for
+        each category in per_category_ap. Be careful with setting it to true if
+        you have more than handful of categories, because it will pollute
+        your mldash.
+    Returns:
+      1. summary_metrics: a dictionary holding:
+        'Precision/mAP': mean average precision over classes averaged over IOU
+          thresholds ranging from .5 to .95 with .05 increments
+        'Precision/mAP@.50IOU': mean average precision at 50% IOU
+        'Precision/mAP@.75IOU': mean average precision at 75% IOU
+        'Precision/mAP (small)': mean average precision for small objects
+                        (area < 32^2 pixels)
+        'Precision/mAP (medium)': mean average precision for medium sized
+                        objects (32^2 pixels < area < 96^2 pixels)
+        'Precision/mAP (large)': mean average precision for large objects
+                        (96^2 pixels < area < 10000^2 pixels)
+        'Recall/AR@1': average recall with 1 detection
+        'Recall/AR@10': average recall with 10 detections
+        'Recall/AR@100': average recall with 100 detections
+        'Recall/AR@100 (small)': average recall for small objects with 100
+          detections
+        'Recall/AR@100 (medium)': average recall for medium objects with 100
+          detections
+        'Recall/AR@100 (large)': average recall for large objects with 100
+          detections
+      2. per_category_ap: a dictionary holding category specific results with
+        keys of the form: 'Precision mAP ByCategory/category'
+        (without the supercategory part if no supercategories exist).
+        For backward compatibility 'PerformanceByCategory' is included in the
+        output regardless of all_metrics_per_category.
+        If evaluating class-agnostic mode, per_category_ap is an empty
+        dictionary.
+    """
+    self.evaluate()
+    self.accumulate()
+    self.summarize()
+
+    summary_metrics = OrderedDict([
+        ('Precision/mAP', self.stats[0]),
+        ('Precision/mAP@.50IOU', self.stats[1]),
+        ('Precision/mAP@.75IOU', self.stats[2]),
+        ('Precision/mAP (small)', self.stats[3]),
+        ('Precision/mAP (medium)', self.stats[4]),
+        ('Precision/mAP (large)', self.stats[5]),
+        ('Recall/AR@1', self.stats[6]),
+        ('Recall/AR@10', self.stats[7]),
+        ('Recall/AR@100', self.stats[8]),
+        ('Recall/AR@100 (small)', self.stats[9]),
+        ('Recall/AR@100 (medium)', self.stats[10]),
+        ('Recall/AR@100 (large)', self.stats[11])
+    ])
+    per_category_ap = OrderedDict([])
+    if self.GetAgnosticMode():
+      return summary_metrics, per_category_ap
+    for category_index, category_id in enumerate(self.GetCategoryIdList()):
+      category = self.GetCategory(category_id)['name']
+      # Kept for backward compatilbility
+      per_category_ap['PerformanceByCategory/mAP/{}'.format(
+          category)] = self.category_stats[0][category_index]
+      if all_metrics_per_category:
+        per_category_ap['Precision mAP ByCategory/{}'.format(
+            category)] = self.category_stats[0][category_index]
+        per_category_ap['Precision mAP@.50IOU ByCategory/{}'.format(
+            category)] = self.category_stats[1][category_index]
+        per_category_ap['Precision mAP@.75IOU ByCategory/{}'.format(
+            category)] = self.category_stats[2][category_index]
+        per_category_ap['Precision mAP (small) ByCategory/{}'.format(
+            category)] = self.category_stats[3][category_index]
+        per_category_ap['Precision mAP (medium) ByCategory/{}'.format(
+            category)] = self.category_stats[4][category_index]
+        per_category_ap['Precision mAP (large) ByCategory/{}'.format(
+            category)] = self.category_stats[5][category_index]
+        per_category_ap['Recall AR@1 ByCategory/{}'.format(
+            category)] = self.category_stats[6][category_index]
+        per_category_ap['Recall AR@10 ByCategory/{}'.format(
+            category)] = self.category_stats[7][category_index]
+        per_category_ap['Recall AR@100 ByCategory/{}'.format(
+            category)] = self.category_stats[8][category_index]
+        per_category_ap['Recall AR@100 (small) ByCategory/{}'.format(
+            category)] = self.category_stats[9][category_index]
+        per_category_ap['Recall AR@100 (medium) ByCategory/{}'.format(
+            category)] = self.category_stats[10][category_index]
+        per_category_ap['Recall AR@100 (large) ByCategory/{}'.format(
+            category)] = self.category_stats[11][category_index]
+
+    return summary_metrics, per_category_ap
+
+
+def _ConvertBoxToCOCOFormat(box):
+  """Converts a box in [ymin, xmin, ymax, xmax] format to COCO format.
+
+  This is a utility function for converting from our internal
+  [ymin, xmin, ymax, xmax] convention to the convention used by the COCO API
+  i.e., [xmin, ymin, width, height].
+
+  Args:
+    box: a [ymin, xmin, ymax, xmax] numpy array
+
+  Returns:
+    a list of floats representing [xmin, ymin, width, height]
+  """
+  return [float(box[1]), float(box[0]), float(box[3] - box[1]),
+          float(box[2] - box[0])]
+
+
+def _RleCompress(masks):
+  """Compresses mask using Run-length encoding provided by pycocotools.
+
+  Args:
+    masks: uint8 numpy array of shape [mask_height, mask_width] with values in
+    {0, 1}.
+
+  Returns:
+    A pycocotools Run-length encoding of the mask.
+  """
+  return mask.encode(np.asfortranarray(masks))
+
+
+def ExportSingleImageGroundtruthToCoco(image_id,
+                                       next_annotation_id,
+                                       category_id_set,
+                                       groundtruth_boxes,
+                                       groundtruth_classes,
+                                       groundtruth_masks=None):
+  """Export groundtruth of a single image to COCO format.
+
+  This function converts groundtruth detection annotations represented as numpy
+  arrays to dictionaries that can be ingested by the COCO evaluation API. Note
+  that the image_ids provided here must match the ones given to
+  ExportSingleImageDetectionsToCoco. We assume that boxes and classes are in
+  correspondence - that is: groundtruth_boxes[i, :], and
+  groundtruth_classes[i] are associated with the same groundtruth annotation.
+
+  In the exported result, "area" fields are always set to the area of the
+  groundtruth bounding box and "iscrowd" fields are always set to 0.
+  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+
+  Args:
+    image_id: a unique image identifier either of type integer or string.
+    next_annotation_id: integer specifying the first id to use for the
+      groundtruth annotations. All annotations are assigned a continuous integer
+      id starting from this value.
+    category_id_set: A set of valid class ids. Groundtruth with classes not in
+      category_id_set are dropped.
+    groundtruth_boxes: numpy array (float32) with shape [num_gt_boxes, 4]
+    groundtruth_classes: numpy array (int) with shape [num_gt_boxes]
+    groundtruth_masks: optional uint8 numpy array of shape [num_detections,
+      image_height, image_width] containing detection_masks.
+
+  Returns:
+    a list of groundtruth annotations for a single image in the COCO format.
+
+  Raises:
+    ValueError: if (1) groundtruth_boxes and groundtruth_classes do not have the
+      right lengths or (2) if each of the elements inside these lists do not
+      have the correct shapes or (3) if image_ids are not integers
+  """
+
+  if len(groundtruth_classes.shape) != 1:
+    raise ValueError('groundtruth_classes is '
+                     'expected to be of rank 1.')
+  if len(groundtruth_boxes.shape) != 2:
+    raise ValueError('groundtruth_boxes is expected to be of '
+                     'rank 2.')
+  if groundtruth_boxes.shape[1] != 4:
+    raise ValueError('groundtruth_boxes should have '
+                     'shape[1] == 4.')
+  num_boxes = groundtruth_classes.shape[0]
+  if num_boxes != groundtruth_boxes.shape[0]:
+    raise ValueError('Corresponding entries in groundtruth_classes, '
+                     'and groundtruth_boxes should have '
+                     'compatible shapes (i.e., agree on the 0th dimension).'
+                     'Classes shape: %d. Boxes shape: %d. Image ID: %s' % (
+                         groundtruth_classes.shape[0],
+                         groundtruth_boxes.shape[0], image_id))
+  groundtruth_list = []
+  for i in range(num_boxes):
+    if groundtruth_classes[i] in category_id_set:
+      export_dict = {
+          'id': next_annotation_id + i,
+          'image_id': image_id,
+          'category_id': int(groundtruth_classes[i]),
+          'bbox': list(_ConvertBoxToCOCOFormat(groundtruth_boxes[i, :])),
+          'area': float((groundtruth_boxes[i, 2] - groundtruth_boxes[i, 0]) *
+                        (groundtruth_boxes[i, 3] - groundtruth_boxes[i, 1])),
+          'iscrowd': 0
+      }
+      if groundtruth_masks is not None:
+        export_dict['segmentation'] = _RleCompress(groundtruth_masks[i])
+      groundtruth_list.append(export_dict)
+  return groundtruth_list
+
+
+def ExportGroundtruthToCOCO(image_ids,
+                            groundtruth_boxes,
+                            groundtruth_classes,
+                            categories,
+                            output_path=None):
+  """Export groundtruth detection annotations in numpy arrays to COCO API.
+
+  This function converts a set of groundtruth detection annotations represented
+  as numpy arrays to dictionaries that can be ingested by the COCO API.
+  Inputs to this function are three lists: image ids for each groundtruth image,
+  groundtruth boxes for each image and groundtruth classes respectively.
+  Note that the image_ids provided here must match the ones given to the
+  ExportDetectionsToCOCO function in order for evaluation to work properly.
+  We assume that for each image, boxes, scores and classes are in
+  correspondence --- that is: image_id[i], groundtruth_boxes[i, :] and
+  groundtruth_classes[i] are associated with the same groundtruth annotation.
+
+  In the exported result, "area" fields are always set to the area of the
+  groundtruth bounding box and "iscrowd" fields are always set to 0.
+  TODO: pass in "iscrowd" array for evaluating on COCO dataset.
+
+  Args:
+    image_ids: a list of unique image identifier either of type integer or
+      string.
+    groundtruth_boxes: list of numpy arrays with shape [num_gt_boxes, 4]
+      (note that num_gt_boxes can be different for each entry in the list)
+    groundtruth_classes: list of numpy arrays (int) with shape [num_gt_boxes]
+      (note that num_gt_boxes can be different for each entry in the list)
+    categories: a list of dictionaries representing all possible categories.
+        Each dict in this list has the following keys:
+          'id': (required) an integer id uniquely identifying this category
+          'name': (required) string representing category name
+            e.g., 'cat', 'dog', 'pizza'
+          'supercategory': (optional) string representing the supercategory
+            e.g., 'animal', 'vehicle', 'food', etc
+    output_path: (optional) path for exporting result to JSON
+  Returns:
+    dictionary that can be read by COCO API
+  Raises:
+    ValueError: if (1) groundtruth_boxes and groundtruth_classes do not have the
+      right lengths or (2) if each of the elements inside these lists do not
+      have the correct shapes or (3) if image_ids are not integers
+  """
+  category_id_set = set([cat['id'] for cat in categories])
+  groundtruth_export_list = []
+  image_export_list = []
+  if not len(image_ids) == len(groundtruth_boxes) == len(groundtruth_classes):
+    raise ValueError('Input lists must have the same length')
+
+  # For reasons internal to the COCO API, it is important that annotation ids
+  # are not equal to zero; we thus start counting from 1.
+  annotation_id = 1
+  for image_id, boxes, classes in zip(image_ids, groundtruth_boxes,
+                                      groundtruth_classes):
+    image_export_list.append({'id': image_id})
+    groundtruth_export_list.extend(ExportSingleImageGroundtruthToCoco(
+        image_id,
+        annotation_id,
+        category_id_set,
+        boxes,
+        classes))
+    num_boxes = classes.shape[0]
+    annotation_id += num_boxes
+
+  groundtruth_dict = {
+      'annotations': groundtruth_export_list,
+      'images': image_export_list,
+      'categories': categories
+  }
+  if output_path:
+    with tf.gfile.GFile(output_path, 'w') as fid:
+      json_utils.Dump(groundtruth_dict, fid, float_digits=4, indent=2)
+  return groundtruth_dict
+
+
+def ExportSingleImageDetectionBoxesToCoco(image_id,
+                                          category_id_set,
+                                          detection_boxes,
+                                          detection_scores,
+                                          detection_classes):
+  """Export detections of a single image to COCO format.
+
+  This function converts detections represented as numpy arrays to dictionaries
+  that can be ingested by the COCO evaluation API. Note that the image_ids
+  provided here must match the ones given to the
+  ExporSingleImageDetectionBoxesToCoco. We assume that boxes, and classes are in
+  correspondence - that is: boxes[i, :], and classes[i]
+  are associated with the same groundtruth annotation.
+
+  Args:
+    image_id: unique image identifier either of type integer or string.
+    category_id_set: A set of valid class ids. Detections with classes not in
+      category_id_set are dropped.
+    detection_boxes: float numpy array of shape [num_detections, 4] containing
+      detection boxes.
+    detection_scores: float numpy array of shape [num_detections] containing
+      scored for the detection boxes.
+    detection_classes: integer numpy array of shape [num_detections] containing
+      the classes for detection boxes.
+
+  Returns:
+    a list of detection annotations for a single image in the COCO format.
+
+  Raises:
+    ValueError: if (1) detection_boxes, detection_scores and detection_classes
+      do not have the right lengths or (2) if each of the elements inside these
+      lists do not have the correct shapes or (3) if image_ids are not integers.
+  """
+
+  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:
+    raise ValueError('All entries in detection_classes and detection_scores'
+                     'expected to be of rank 1.')
+  if len(detection_boxes.shape) != 2:
+    raise ValueError('All entries in detection_boxes expected to be of '
+                     'rank 2.')
+  if detection_boxes.shape[1] != 4:
+    raise ValueError('All entries in detection_boxes should have '
+                     'shape[1] == 4.')
+  num_boxes = detection_classes.shape[0]
+  if not num_boxes == detection_boxes.shape[0] == detection_scores.shape[0]:
+    raise ValueError('Corresponding entries in detection_classes, '
+                     'detection_scores and detection_boxes should have '
+                     'compatible shapes (i.e., agree on the 0th dimension). '
+                     'Classes shape: %d. Boxes shape: %d. '
+                     'Scores shape: %d' % (
+                         detection_classes.shape[0], detection_boxes.shape[0],
+                         detection_scores.shape[0]
+                     ))
+  detections_list = []
+  for i in range(num_boxes):
+    if detection_classes[i] in category_id_set:
+      detections_list.append({
+          'image_id': image_id,
+          'category_id': int(detection_classes[i]),
+          'bbox': list(_ConvertBoxToCOCOFormat(detection_boxes[i, :])),
+          'score': float(detection_scores[i])
+      })
+  return detections_list
+
+
+def ExportSingleImageDetectionMasksToCoco(image_id,
+                                          category_id_set,
+                                          detection_masks,
+                                          detection_scores,
+                                          detection_classes):
+  """Export detection masks of a single image to COCO format.
+
+  This function converts detections represented as numpy arrays to dictionaries
+  that can be ingested by the COCO evaluation API. We assume that
+  detection_masks, detection_scores, and detection_classes are in correspondence
+  - that is: detection_masks[i, :], detection_classes[i] and detection_scores[i]
+    are associated with the same annotation.
+
+  Args:
+    image_id: unique image identifier either of type integer or string.
+    category_id_set: A set of valid class ids. Detections with classes not in
+      category_id_set are dropped.
+    detection_masks: uint8 numpy array of shape [num_detections, image_height,
+      image_width] containing detection_masks.
+    detection_scores: float numpy array of shape [num_detections] containing
+      scores for detection masks.
+    detection_classes: integer numpy array of shape [num_detections] containing
+      the classes for detection masks.
+
+  Returns:
+    a list of detection mask annotations for a single image in the COCO format.
+
+  Raises:
+    ValueError: if (1) detection_masks, detection_scores and detection_classes
+      do not have the right lengths or (2) if each of the elements inside these
+      lists do not have the correct shapes or (3) if image_ids are not integers.
+  """
+
+  if len(detection_classes.shape) != 1 or len(detection_scores.shape) != 1:
+    raise ValueError('All entries in detection_classes and detection_scores'
+                     'expected to be of rank 1.')
+  num_boxes = detection_classes.shape[0]
+  if not num_boxes == len(detection_masks) == detection_scores.shape[0]:
+    raise ValueError('Corresponding entries in detection_classes, '
+                     'detection_scores and detection_masks should have '
+                     'compatible lengths and shapes '
+                     'Classes length: %d.  Masks length: %d. '
+                     'Scores length: %d' % (
+                         detection_classes.shape[0], len(detection_masks),
+                         detection_scores.shape[0]
+                     ))
+  detections_list = []
+  for i in range(num_boxes):
+    if detection_classes[i] in category_id_set:
+      detections_list.append({
+          'image_id': image_id,
+          'category_id': int(detection_classes[i]),
+          'segmentation': _RleCompress(detection_masks[i]),
+          'score': float(detection_scores[i])
+      })
+  return detections_list
+
+
+def ExportDetectionsToCOCO(image_ids,
+                           detection_boxes,
+                           detection_scores,
+                           detection_classes,
+                           categories,
+                           output_path=None):
+  """Export detection annotations in numpy arrays to COCO API.
+
+  This function converts a set of predicted detections represented
+  as numpy arrays to dictionaries that can be ingested by the COCO API.
+  Inputs to this function are lists, consisting of boxes, scores and
+  classes, respectively, corresponding to each image for which detections
+  have been produced.  Note that the image_ids provided here must
+  match the ones given to the ExportGroundtruthToCOCO function in order
+  for evaluation to work properly.
+
+  We assume that for each image, boxes, scores and classes are in
+  correspondence --- that is: detection_boxes[i, :], detection_scores[i] and
+  detection_classes[i] are associated with the same detection.
+
+  Args:
+    image_ids: a list of unique image identifier either of type integer or
+      string.
+    detection_boxes: list of numpy arrays with shape [num_detection_boxes, 4]
+    detection_scores: list of numpy arrays (float) with shape
+      [num_detection_boxes]. Note that num_detection_boxes can be different
+      for each entry in the list.
+    detection_classes: list of numpy arrays (int) with shape
+      [num_detection_boxes]. Note that num_detection_boxes can be different
+      for each entry in the list.
+    categories: a list of dictionaries representing all possible categories.
+      Each dict in this list must have an integer 'id' key uniquely identifying
+      this category.
+    output_path: (optional) path for exporting result to JSON
+
+  Returns:
+    list of dictionaries that can be read by COCO API, where each entry
+    corresponds to a single detection and has keys from:
+    ['image_id', 'category_id', 'bbox', 'score'].
+  Raises:
+    ValueError: if (1) detection_boxes and detection_classes do not have the
+      right lengths or (2) if each of the elements inside these lists do not
+      have the correct shapes or (3) if image_ids are not integers.
+  """
+  category_id_set = set([cat['id'] for cat in categories])
+  detections_export_list = []
+  if not (len(image_ids) == len(detection_boxes) == len(detection_scores) ==
+          len(detection_classes)):
+    raise ValueError('Input lists must have the same length')
+  for image_id, boxes, scores, classes in zip(image_ids, detection_boxes,
+                                              detection_scores,
+                                              detection_classes):
+    detections_export_list.extend(ExportSingleImageDetectionBoxesToCoco(
+        image_id,
+        category_id_set,
+        boxes,
+        scores,
+        classes))
+  if output_path:
+    with tf.gfile.GFile(output_path, 'w') as fid:
+      json_utils.Dump(detections_export_list, fid, float_digits=4, indent=2)
+  return detections_export_list
+
+
+def ExportSegmentsToCOCO(image_ids,
+                         detection_masks,
+                         detection_scores,
+                         detection_classes,
+                         categories,
+                         output_path=None):
+  """Export segmentation masks in numpy arrays to COCO API.
+
+  This function converts a set of predicted instance masks represented
+  as numpy arrays to dictionaries that can be ingested by the COCO API.
+  Inputs to this function are lists, consisting of segments, scores and
+  classes, respectively, corresponding to each image for which detections
+  have been produced.
+
+  Note this function is recommended to use for small dataset.
+  For large dataset, it should be used with a merge function
+  (e.g. in map reduce), otherwise the memory consumption is large.
+
+  We assume that for each image, masks, scores and classes are in
+  correspondence --- that is: detection_masks[i, :, :, :], detection_scores[i]
+  and detection_classes[i] are associated with the same detection.
+
+  Args:
+    image_ids: list of image ids (typically ints or strings)
+    detection_masks: list of numpy arrays with shape [num_detection, h, w, 1]
+      and type uint8. The height and width should match the shape of
+      corresponding image.
+    detection_scores: list of numpy arrays (float) with shape
+      [num_detection]. Note that num_detection can be different
+      for each entry in the list.
+    detection_classes: list of numpy arrays (int) with shape
+      [num_detection]. Note that num_detection can be different
+      for each entry in the list.
+    categories: a list of dictionaries representing all possible categories.
+      Each dict in this list must have an integer 'id' key uniquely identifying
+      this category.
+    output_path: (optional) path for exporting result to JSON
+
+  Returns:
+    list of dictionaries that can be read by COCO API, where each entry
+    corresponds to a single detection and has keys from:
+    ['image_id', 'category_id', 'segmentation', 'score'].
+
+  Raises:
+    ValueError: if detection_masks and detection_classes do not have the
+      right lengths or if each of the elements inside these lists do not
+      have the correct shapes.
+  """
+  if not (len(image_ids) == len(detection_masks) == len(detection_scores) ==
+          len(detection_classes)):
+    raise ValueError('Input lists must have the same length')
+
+  segment_export_list = []
+  for image_id, masks, scores, classes in zip(image_ids, detection_masks,
+                                              detection_scores,
+                                              detection_classes):
+
+    if len(classes.shape) != 1 or len(scores.shape) != 1:
+      raise ValueError('All entries in detection_classes and detection_scores'
+                       'expected to be of rank 1.')
+    if len(masks.shape) != 4:
+      raise ValueError('All entries in masks expected to be of '
+                       'rank 4. Given {}'.format(masks.shape))
+
+    num_boxes = classes.shape[0]
+    if not num_boxes == masks.shape[0] == scores.shape[0]:
+      raise ValueError('Corresponding entries in segment_classes, '
+                       'detection_scores and detection_boxes should have '
+                       'compatible shapes (i.e., agree on the 0th dimension).')
+
+    category_id_set = set([cat['id'] for cat in categories])
+    segment_export_list.extend(ExportSingleImageDetectionMasksToCoco(
+        image_id, category_id_set, np.squeeze(masks, axis=3), scores, classes))
+
+  if output_path:
+    with tf.gfile.GFile(output_path, 'w') as fid:
+      json_utils.Dump(segment_export_list, fid, float_digits=4, indent=2)
+  return segment_export_list
+
+
+def ExportKeypointsToCOCO(image_ids,
+                          detection_keypoints,
+                          detection_scores,
+                          detection_classes,
+                          categories,
+                          output_path=None):
+  """Exports keypoints in numpy arrays to COCO API.
+
+  This function converts a set of predicted keypoints represented
+  as numpy arrays to dictionaries that can be ingested by the COCO API.
+  Inputs to this function are lists, consisting of keypoints, scores and
+  classes, respectively, corresponding to each image for which detections
+  have been produced.
+
+  We assume that for each image, keypoints, scores and classes are in
+  correspondence --- that is: detection_keypoints[i, :, :, :],
+  detection_scores[i] and detection_classes[i] are associated with the same
+  detection.
+
+  Args:
+    image_ids: list of image ids (typically ints or strings)
+    detection_keypoints: list of numpy arrays with shape
+      [num_detection, num_keypoints, 2] and type float32 in absolute
+      x-y coordinates.
+    detection_scores: list of numpy arrays (float) with shape
+      [num_detection]. Note that num_detection can be different
+      for each entry in the list.
+    detection_classes: list of numpy arrays (int) with shape
+      [num_detection]. Note that num_detection can be different
+      for each entry in the list.
+    categories: a list of dictionaries representing all possible categories.
+      Each dict in this list must have an integer 'id' key uniquely identifying
+      this category and an integer 'num_keypoints' key specifying the number of
+      keypoints the category has.
+    output_path: (optional) path for exporting result to JSON
+
+  Returns:
+    list of dictionaries that can be read by COCO API, where each entry
+    corresponds to a single detection and has keys from:
+    ['image_id', 'category_id', 'keypoints', 'score'].
+
+  Raises:
+    ValueError: if detection_keypoints and detection_classes do not have the
+      right lengths or if each of the elements inside these lists do not
+      have the correct shapes.
+  """
+  if not (len(image_ids) == len(detection_keypoints) ==
+          len(detection_scores) == len(detection_classes)):
+    raise ValueError('Input lists must have the same length')
+
+  keypoints_export_list = []
+  for image_id, keypoints, scores, classes in zip(
+      image_ids, detection_keypoints, detection_scores, detection_classes):
+
+    if len(classes.shape) != 1 or len(scores.shape) != 1:
+      raise ValueError('All entries in detection_classes and detection_scores'
+                       'expected to be of rank 1.')
+    if len(keypoints.shape) != 3:
+      raise ValueError('All entries in keypoints expected to be of '
+                       'rank 3. Given {}'.format(keypoints.shape))
+
+    num_boxes = classes.shape[0]
+    if not num_boxes == keypoints.shape[0] == scores.shape[0]:
+      raise ValueError('Corresponding entries in detection_classes, '
+                       'detection_keypoints, and detection_scores should have '
+                       'compatible shapes (i.e., agree on the 0th dimension).')
+
+    category_id_set = set([cat['id'] for cat in categories])
+    category_id_to_num_keypoints_map = {
+        cat['id']: cat['num_keypoints'] for cat in categories
+        if 'num_keypoints' in cat}
+
+    for i in range(num_boxes):
+      if classes[i] not in category_id_set:
+        raise ValueError('class id should be in category_id_set\n')
+
+      if classes[i] in category_id_to_num_keypoints_map:
+        num_keypoints = category_id_to_num_keypoints_map[classes[i]]
+        # Adds extra ones to indicate the visibility for each keypoint as is
+        # recommended by MSCOCO.
+        instance_keypoints = np.concatenate(
+            [keypoints[i, 0:num_keypoints, :],
+             np.expand_dims(np.ones(num_keypoints), axis=1)],
+            axis=1).astype(int)
+
+        instance_keypoints = instance_keypoints.flatten().tolist()
+        keypoints_export_list.append({
+            'image_id': image_id,
+            'category_id': int(classes[i]),
+            'keypoints': instance_keypoints,
+            'score': float(scores[i])
+        })
+
+  if output_path:
+    with tf.gfile.GFile(output_path, 'w') as fid:
+      json_utils.Dump(keypoints_export_list, fid, float_digits=4, indent=2)
+  return keypoints_export_list
diff --git a/research/object_detection/metrics/coco_tools_test.py b/research/object_detection/metrics/coco_tools_test.py
new file mode 100644
index 00000000..115b146d
--- /dev/null
+++ b/research/object_detection/metrics/coco_tools_test.py
@@ -0,0 +1,258 @@
+"""Tests for google3.image.understanding.object_detection.metrics.coco_tools."""
+import json
+import os
+import re
+import numpy as np
+
+from pycocotools import mask
+
+import tensorflow as tf
+
+from object_detection.metrics import coco_tools
+
+
+class CocoToolsTest(tf.test.TestCase):
+
+  def setUp(self):
+    groundtruth_annotations_list = [
+        {
+            'id': 1,
+            'image_id': 'first',
+            'category_id': 1,
+            'bbox': [100., 100., 100., 100.],
+            'area': 100.**2,
+            'iscrowd': 0
+        },
+        {
+            'id': 2,
+            'image_id': 'second',
+            'category_id': 1,
+            'bbox': [50., 50., 50., 50.],
+            'area': 50.**2,
+            'iscrowd': 0
+        },
+    ]
+    image_list = [{'id': 'first'}, {'id': 'second'}]
+    category_list = [{'id': 0, 'name': 'person'},
+                     {'id': 1, 'name': 'cat'},
+                     {'id': 2, 'name': 'dog'}]
+    self._groundtruth_dict = {
+        'annotations': groundtruth_annotations_list,
+        'images': image_list,
+        'categories': category_list
+    }
+
+    self._detections_list = [
+        {
+            'image_id': 'first',
+            'category_id': 1,
+            'bbox': [100., 100., 100., 100.],
+            'score': .8
+        },
+        {
+            'image_id': 'second',
+            'category_id': 1,
+            'bbox': [50., 50., 50., 50.],
+            'score': .7
+        },
+    ]
+
+  def testCocoWrappers(self):
+    groundtruth = coco_tools.COCOWrapper(self._groundtruth_dict)
+    detections = groundtruth.LoadAnnotations(self._detections_list)
+    evaluator = coco_tools.COCOEvalWrapper(groundtruth, detections)
+    summary_metrics, _ = evaluator.ComputeMetrics()
+    self.assertAlmostEqual(1.0, summary_metrics['Precision/mAP'])
+
+  def testExportGroundtruthToCOCO(self):
+    image_ids = ['first', 'second']
+    groundtruth_boxes = [np.array([[100, 100, 200, 200]], np.float),
+                         np.array([[50, 50, 100, 100]], np.float)]
+    groundtruth_classes = [np.array([1], np.int32), np.array([1], np.int32)]
+    categories = [{'id': 0, 'name': 'person'},
+                  {'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'}]
+    output_path = os.path.join(tf.test.get_temp_dir(), 'groundtruth.json')
+    result = coco_tools.ExportGroundtruthToCOCO(
+        image_ids,
+        groundtruth_boxes,
+        groundtruth_classes,
+        categories,
+        output_path=output_path)
+    self.assertDictEqual(result, self._groundtruth_dict)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      written_result = f.read()
+      # The json output should have floats written to 4 digits of precision.
+      matcher = re.compile(r'"bbox":\s+\[\n\s+\d+.\d\d\d\d,', re.MULTILINE)
+      self.assertTrue(matcher.findall(written_result))
+      written_result = json.loads(written_result)
+      self.assertAlmostEqual(result, written_result)
+
+  def testExportDetectionsToCOCO(self):
+    image_ids = ['first', 'second']
+    detections_boxes = [np.array([[100, 100, 200, 200]], np.float),
+                        np.array([[50, 50, 100, 100]], np.float)]
+    detections_scores = [np.array([.8], np.float), np.array([.7], np.float)]
+    detections_classes = [np.array([1], np.int32), np.array([1], np.int32)]
+    categories = [{'id': 0, 'name': 'person'},
+                  {'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'}]
+    output_path = os.path.join(tf.test.get_temp_dir(), 'detections.json')
+    result = coco_tools.ExportDetectionsToCOCO(
+        image_ids,
+        detections_boxes,
+        detections_scores,
+        detections_classes,
+        categories,
+        output_path=output_path)
+    self.assertListEqual(result, self._detections_list)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      written_result = f.read()
+      # The json output should have floats written to 4 digits of precision.
+      matcher = re.compile(r'"bbox":\s+\[\n\s+\d+.\d\d\d\d,', re.MULTILINE)
+      self.assertTrue(matcher.findall(written_result))
+      written_result = json.loads(written_result)
+      self.assertAlmostEqual(result, written_result)
+
+  def testExportSegmentsToCOCO(self):
+    image_ids = ['first', 'second']
+    detection_masks = [np.array(
+        [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],
+        dtype=np.uint8), np.array(
+            [[[0, 1, 0, 1], [0, 1, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1]]],
+            dtype=np.uint8)]
+
+    for i, detection_mask in enumerate(detection_masks):
+      detection_masks[i] = detection_mask[:, :, :, None]
+
+    detection_scores = [np.array([.8], np.float), np.array([.7], np.float)]
+    detection_classes = [np.array([1], np.int32), np.array([1], np.int32)]
+
+    categories = [{'id': 0, 'name': 'person'},
+                  {'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'}]
+    output_path = os.path.join(tf.test.get_temp_dir(), 'segments.json')
+    result = coco_tools.ExportSegmentsToCOCO(
+        image_ids,
+        detection_masks,
+        detection_scores,
+        detection_classes,
+        categories,
+        output_path=output_path)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      written_result = f.read()
+      written_result = json.loads(written_result)
+      mask_load = mask.decode([written_result[0]['segmentation']])
+      self.assertTrue(np.allclose(mask_load, detection_masks[0]))
+      self.assertAlmostEqual(result, written_result)
+
+  def testExportKeypointsToCOCO(self):
+    image_ids = ['first', 'second']
+    detection_keypoints = [
+        np.array(
+            [[[100, 200], [300, 400], [500, 600]],
+             [[50, 150], [250, 350], [450, 550]]], dtype=np.int32),
+        np.array(
+            [[[110, 210], [310, 410], [510, 610]],
+             [[60, 160], [260, 360], [460, 560]]], dtype=np.int32)]
+
+    detection_scores = [np.array([.8, 0.2], np.float),
+                        np.array([.7, 0.3], np.float)]
+    detection_classes = [np.array([1, 1], np.int32), np.array([1, 1], np.int32)]
+
+    categories = [{'id': 1, 'name': 'person', 'num_keypoints': 3},
+                  {'id': 2, 'name': 'cat'},
+                  {'id': 3, 'name': 'dog'}]
+
+    output_path = os.path.join(tf.test.get_temp_dir(), 'keypoints.json')
+    result = coco_tools.ExportKeypointsToCOCO(
+        image_ids,
+        detection_keypoints,
+        detection_scores,
+        detection_classes,
+        categories,
+        output_path=output_path)
+
+    with tf.gfile.GFile(output_path, 'r') as f:
+      written_result = f.read()
+      written_result = json.loads(written_result)
+      self.assertAlmostEqual(result, written_result)
+
+  def testSingleImageDetectionBoxesExport(self):
+    boxes = np.array([[0, 0, 1, 1],
+                      [0, 0, .5, .5],
+                      [.5, .5, 1, 1]], dtype=np.float32)
+    classes = np.array([1, 2, 3], dtype=np.int32)
+    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)
+    coco_boxes = np.array([[0, 0, 1, 1],
+                           [0, 0, .5, .5],
+                           [.5, .5, .5, .5]], dtype=np.float32)
+    coco_annotations = coco_tools.ExportSingleImageDetectionBoxesToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        detection_boxes=boxes,
+        detection_classes=classes,
+        detection_scores=scores)
+    for i, annotation in enumerate(coco_annotations):
+      self.assertEqual(annotation['image_id'], 'first_image')
+      self.assertEqual(annotation['category_id'], classes[i])
+      self.assertAlmostEqual(annotation['score'], scores[i])
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+
+  def testSingleImageDetectionMaskExport(self):
+    masks = np.array(
+        [[[1, 1,], [1, 1]],
+         [[0, 0], [0, 1]],
+         [[0, 0], [0, 0]]], dtype=np.uint8)
+    classes = np.array([1, 2, 3], dtype=np.int32)
+    scores = np.array([0.8, 0.2, 0.7], dtype=np.float32)
+    coco_annotations = coco_tools.ExportSingleImageDetectionMasksToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        detection_classes=classes,
+        detection_scores=scores,
+        detection_masks=masks)
+    expected_counts = ['04', '31', '4']
+    for i, mask_annotation in enumerate(coco_annotations):
+      self.assertEqual(mask_annotation['segmentation']['counts'],
+                       expected_counts[i])
+      self.assertTrue(np.all(np.equal(mask.decode(
+          mask_annotation['segmentation']), masks[i])))
+      self.assertEqual(mask_annotation['image_id'], 'first_image')
+      self.assertEqual(mask_annotation['category_id'], classes[i])
+      self.assertAlmostEqual(mask_annotation['score'], scores[i])
+
+  def testSingleImageGroundtruthExport(self):
+    masks = np.array(
+        [[[1, 1,], [1, 1]],
+         [[0, 0], [0, 1]],
+         [[0, 0], [0, 0]]], dtype=np.uint8)
+    boxes = np.array([[0, 0, 1, 1],
+                      [0, 0, .5, .5],
+                      [.5, .5, 1, 1]], dtype=np.float32)
+    coco_boxes = np.array([[0, 0, 1, 1],
+                           [0, 0, .5, .5],
+                           [.5, .5, .5, .5]], dtype=np.float32)
+    classes = np.array([1, 2, 3], dtype=np.int32)
+    next_annotation_id = 1
+    coco_annotations = coco_tools.ExportSingleImageGroundtruthToCoco(
+        image_id='first_image',
+        category_id_set=set([1, 2, 3]),
+        next_annotation_id=next_annotation_id,
+        groundtruth_boxes=boxes,
+        groundtruth_classes=classes,
+        groundtruth_masks=masks)
+    expected_counts = ['04', '31', '4']
+    for i, annotation in enumerate(coco_annotations):
+      self.assertEqual(annotation['segmentation']['counts'],
+                       expected_counts[i])
+      self.assertTrue(np.all(np.equal(mask.decode(
+          annotation['segmentation']), masks[i])))
+      self.assertTrue(np.all(np.isclose(annotation['bbox'], coco_boxes[i])))
+      self.assertEqual(annotation['image_id'], 'first_image')
+      self.assertEqual(annotation['category_id'], classes[i])
+      self.assertEqual(annotation['id'], i + next_annotation_id)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/metrics/offline_eval_map_corloc.py b/research/object_detection/metrics/offline_eval_map_corloc.py
index 421b4d1f..b7b1eb69 100644
--- a/research/object_detection/metrics/offline_eval_map_corloc.py
+++ b/research/object_detection/metrics/offline_eval_map_corloc.py
@@ -22,7 +22,7 @@ The evaluation metrics set is supplied in object_detection.protos.EvalConfig
 in metrics_set field.
 Currently two set of metrics are supported:
 - pascal_voc_metrics: standard PASCAL VOC 2007 metric
-- open_images_metrics: Open Image V2 metric
+- open_images_detection_metrics: Open Image V2 metric
 All other field of object_detection.protos.EvalConfig are ignored.
 
 Example usage:
diff --git a/research/object_detection/models/BUILD b/research/object_detection/models/BUILD
index 36efaba5..ef00cab6 100644
--- a/research/object_detection/models/BUILD
+++ b/research/object_detection/models/BUILD
@@ -15,6 +15,7 @@ py_library(
     ],
     deps = [
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:ops",
     ],
 )
 
@@ -36,6 +37,7 @@ py_library(
     ],
     deps = [
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/utils:test_case",
     ],
 )
 
@@ -47,9 +49,10 @@ py_library(
     deps = [
         ":feature_map_generators",
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/slim:inception_v2",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//third_party/tensorflow_models/slim:inception_v2",
     ],
 )
 
@@ -61,9 +64,10 @@ py_library(
     deps = [
         ":feature_map_generators",
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/slim:inception_v3",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//third_party/tensorflow_models/slim:inception_v3",
     ],
 )
 
@@ -73,9 +77,10 @@ py_library(
     deps = [
         ":feature_map_generators",
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:ssd_meta_arch",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/slim:mobilenet_v1",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//third_party/tensorflow_models/slim:mobilenet_v1",
     ],
 )
 
@@ -86,8 +91,39 @@ py_library(
         ":feature_map_generators",
         ":ssd_mobilenet_v1_feature_extractor",
         "//tensorflow",
-        "//tensorflow_models/object_detection/utils:ops",
-        "//tensorflow_models/slim:mobilenet_v1",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//third_party/tensorflow_models/slim:mobilenet_v1",
+    ],
+)
+
+py_library(
+    name = "ssd_resnet_v1_fpn_feature_extractor",
+    srcs = ["ssd_resnet_v1_fpn_feature_extractor.py"],
+    deps = [
+        ":feature_map_generators",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/meta_architectures:ssd_meta_arch",
+        "//tensorflow/models/research/object_detection/utils:ops",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
+        "//third_party/tensorflow_models/slim:resnet_v1",
+    ],
+)
+
+py_library(
+    name = "ssd_resnet_v1_fpn_feature_extractor_testbase",
+    srcs = ["ssd_resnet_v1_fpn_feature_extractor_testbase.py"],
+    deps = [
+        "//tensorflow/models/research/object_detection/models:ssd_feature_extractor_test",
+    ],
+)
+
+py_test(
+    name = "ssd_resnet_v1_fpn_feature_extractor_test",
+    srcs = ["ssd_resnet_v1_fpn_feature_extractor_test.py"],
+    deps = [
+        ":ssd_resnet_v1_fpn_feature_extractor",
+        ":ssd_resnet_v1_fpn_feature_extractor_testbase",
+        "//tensorflow",
     ],
 )
 
@@ -153,8 +189,8 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/slim:nasnet",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//third_party/tensorflow_models/slim:nasnet",
     ],
 )
 
@@ -165,8 +201,8 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/slim:inception_resnet_v2",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//third_party/tensorflow_models/slim:inception_resnet_v2",
     ],
 )
 
@@ -188,8 +224,8 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/slim:inception_v2",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//third_party/tensorflow_models/slim:inception_v2",
     ],
 )
 
@@ -211,9 +247,9 @@ py_library(
     ],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/meta_architectures:faster_rcnn_meta_arch",
-        "//tensorflow_models/slim:resnet_utils",
-        "//tensorflow_models/slim:resnet_v1",
+        "//tensorflow/models/research/object_detection/meta_architectures:faster_rcnn_meta_arch",
+        "//third_party/tensorflow_models/slim:resnet_utils",
+        "//third_party/tensorflow_models/slim:resnet_v1",
     ],
 )
 
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
index a29cb84f..b2ccbceb 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor.py
@@ -51,7 +51,8 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
                pad_to_multiple,
                conv_hyperparams,
                batch_norm_trainable=True,
-               reuse_weights=None):
+               reuse_weights=None,
+               use_explicit_padding=False):
     """MobileNetV1 Feature Extractor for Embedded-friendly SSD Models.
 
     Args:
@@ -66,6 +67,8 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
 
     Raises:
       ValueError: upon invalid `pad_to_multiple` values.
@@ -76,7 +79,8 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
 
     super(EmbeddedSSDMobileNetV1FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
+        use_explicit_padding)
 
   def extract_features(self, preprocessed_inputs):
     """Extract features from preprocessed inputs.
@@ -88,13 +92,25 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
     Returns:
       feature_maps: a list of tensors where the ith tensor has shape
         [batch, height_i, width_i, depth_i]
+
+    Raises:
+      ValueError: if image height or width are not 256 pixels.
     """
-    preprocessed_inputs.get_shape().assert_has_rank(4)
-    shape_assert = tf.Assert(
-        tf.logical_and(
-            tf.equal(tf.shape(preprocessed_inputs)[1], 256),
-            tf.equal(tf.shape(preprocessed_inputs)[2], 256)),
-        ['image size must be 256 in both height and width.'])
+    image_shape = preprocessed_inputs.get_shape()
+    image_shape.assert_has_rank(4)
+    image_height = image_shape[1].value
+    image_width = image_shape[2].value
+
+    if image_height is None or image_width is None:
+      shape_assert = tf.Assert(
+          tf.logical_and(tf.equal(tf.shape(preprocessed_inputs)[1], 256),
+                         tf.equal(tf.shape(preprocessed_inputs)[2], 256)),
+          ['image size must be 256 in both height and width.'])
+      with tf.control_dependencies([shape_assert]):
+        preprocessed_inputs = tf.identity(preprocessed_inputs)
+    elif image_height != 256 or image_width != 256:
+      raise ValueError('image size must be = 256 in both height and width;'
+                       ' image dim = %d,%d' % (image_height, image_width))
 
     feature_map_layout = {
         'from_layer': [
@@ -102,10 +118,11 @@ class EmbeddedSSDMobileNetV1FeatureExtractor(
         ],
         'layer_depth': [-1, -1, 512, 256, 256],
         'conv_kernel_size': [-1, -1, 3, 3, 2],
+        'use_explicit_padding': self._use_explicit_padding,
     }
 
-    with tf.control_dependencies([shape_assert]):
-      with slim.arg_scope(self._conv_hyperparams):
+    with slim.arg_scope(self._conv_hyperparams):
+      with slim.arg_scope([slim.batch_norm], fused=False):
         with tf.variable_scope('MobilenetV1',
                                reuse=self._reuse_weights) as scope:
           _, image_features = mobilenet_v1.mobilenet_v1_base(
diff --git a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
index cef5de51..f15efdce 100644
--- a/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/embedded_ssd_mobilenet_v1_feature_extractor_test.py
@@ -22,7 +22,7 @@ from object_detection.models import ssd_feature_extractor_test
 
 
 class EmbeddedSSDMobileNetV1FeatureExtractorTest(
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
                                 is_training=True, batch_norm_trainable=True):
@@ -51,11 +51,23 @@ class EmbeddedSSDMobileNetV1FeatureExtractorTest(
     image_width = 256
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 16, 16, 512), (4, 8, 8, 1024),
-                                  (4, 4, 4, 512), (4, 2, 2, 256),
-                                  (4, 1, 1, 256)]
+    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),
+                                  (2, 4, 4, 512), (2, 2, 2, 256),
+                                  (2, 1, 1, 256)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),
+                                  (2, 4, 4, 512), (2, 2, 2, 256),
+                                  (2, 1, 1, 256)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
@@ -63,10 +75,10 @@ class EmbeddedSSDMobileNetV1FeatureExtractorTest(
     image_width = 256
     depth_multiplier = 0.5**12
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 16, 16, 32), (4, 8, 8, 32), (4, 4, 4, 32),
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 16, 16, 32), (2, 8, 8, 32), (2, 4, 4, 32),
+                                  (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple_of_1(
@@ -75,11 +87,11 @@ class EmbeddedSSDMobileNetV1FeatureExtractorTest(
     image_width = 256
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 16, 16, 512), (4, 8, 8, 1024),
-                                  (4, 4, 4, 512), (4, 2, 2, 256),
-                                  (4, 1, 1, 256)]
+    expected_feature_map_shape = [(2, 16, 16, 512), (2, 8, 8, 1024),
+                                  (2, 4, 4, 512), (2, 2, 2, 256),
+                                  (2, 1, 1, 256)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_raises_error_with_pad_to_multiple_not_1(self):
diff --git a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
index 29430d86..8b959ee1 100644
--- a/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_inception_resnet_v2_feature_extractor.py
@@ -180,7 +180,7 @@ class FasterRCNNInceptionResnetV2FeatureExtractor(
     faster_rcnn_meta_arch.FasterRCNNFeatureExtractor which does not work for
     InceptionResnetV2 checkpoints.
 
-    TODO: revisit whether it's possible to force the
+    TODO(jonathanhuang,rathodv): revisit whether it's possible to force the
     `Repeat` namescope as created in `_extract_box_classifier_features` to
     start counting at 2 (e.g. `Repeat_2`) so that the default restore_fn can
     be used.
diff --git a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
index 8575bf33..2779026a 100644
--- a/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
+++ b/research/object_detection/models/faster_rcnn_resnet_v1_feature_extractor.py
@@ -111,7 +111,8 @@ class FasterRCNNResnetV1FeatureExtractor(
 
     with tf.control_dependencies([shape_assert]):
       # Disables batchnorm for fine-tuning with smaller batch sizes.
-      # TODO: Figure out if it is needed when image batch size is bigger.
+      # TODO: Figure out if it is needed when image
+      # batch size is bigger.
       with slim.arg_scope(
           resnet_utils.resnet_arg_scope(
               batch_norm_epsilon=1e-5,
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 8eb7e621..37672cf5 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -25,6 +25,7 @@ of final feature maps.
 """
 import collections
 import tensorflow as tf
+from object_detection.utils import ops
 slim = tf.contrib.slim
 
 
@@ -115,6 +116,9 @@ def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
   feature_map_keys = []
   feature_maps = []
   base_from_layer = ''
+  use_explicit_padding = False
+  if 'use_explicit_padding' in feature_map_layout:
+    use_explicit_padding = feature_map_layout['use_explicit_padding']
   use_depthwise = False
   if 'use_depthwise' in feature_map_layout:
     use_depthwise = feature_map_layout['use_depthwise']
@@ -139,16 +143,21 @@ def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
             padding='SAME',
             stride=1,
             scope=layer_name)
-      stride = 2
       layer_name = '{}_2_Conv2d_{}_{}x{}_s2_{}'.format(
           base_from_layer, index, conv_kernel_size, conv_kernel_size,
           depth_fn(layer_depth))
+      stride = 2
+      padding = 'SAME'
+      if use_explicit_padding:
+        padding = 'VALID'
+        intermediate_layer = ops.fixed_padding(
+            intermediate_layer, conv_kernel_size)
       if use_depthwise:
         feature_map = slim.separable_conv2d(
             intermediate_layer,
             None, [conv_kernel_size, conv_kernel_size],
             depth_multiplier=1,
-            padding='SAME',
+            padding=padding,
             stride=stride,
             scope=layer_name + '_depthwise')
         feature_map = slim.conv2d(
@@ -161,10 +170,56 @@ def multi_resolution_feature_maps(feature_map_layout, depth_multiplier,
         feature_map = slim.conv2d(
             intermediate_layer,
             depth_fn(layer_depth), [conv_kernel_size, conv_kernel_size],
-            padding='SAME',
+            padding=padding,
             stride=stride,
             scope=layer_name)
       feature_map_keys.append(layer_name)
     feature_maps.append(feature_map)
   return collections.OrderedDict(
       [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])
+
+
+def fpn_top_down_feature_maps(image_features, depth, scope=None):
+  """Generates `top-down` feature maps for Feature Pyramid Networks.
+
+  See https://arxiv.org/abs/1612.03144 for details.
+
+  Args:
+    image_features: list of image feature tensors. Spatial resolutions of
+      succesive tensors must reduce exactly by a factor of 2.
+    depth: depth of output feature maps.
+    scope: A scope name to wrap this op under.
+
+  Returns:
+    feature_maps: an OrderedDict mapping keys (feature map names) to
+      tensors where each tensor has shape [batch, height_i, width_i, depth_i].
+  """
+  with tf.variable_scope(
+      scope, 'top_down', image_features):
+    num_levels = len(image_features)
+    output_feature_maps_list = []
+    output_feature_map_keys = []
+    with slim.arg_scope(
+        [slim.conv2d],
+        activation_fn=None, normalizer_fn=None, padding='SAME', stride=1):
+      top_down = slim.conv2d(
+          image_features[-1],
+          depth, [1, 1], scope='projection_%d' % num_levels)
+      output_feature_maps_list.append(top_down)
+      output_feature_map_keys.append(
+          'top_down_feature_map_%d' % (num_levels - 1))
+
+      for level in reversed(range(num_levels - 1)):
+        top_down = ops.nearest_neighbor_upsampling(top_down, 2)
+        residual = slim.conv2d(
+            image_features[level], depth, [1, 1],
+            scope='projection_%d' % (level + 1))
+        top_down = 0.5 * top_down + 0.5 * residual
+        output_feature_maps_list.append(slim.conv2d(
+            top_down,
+            depth, [3, 3],
+            activation_fn=None,
+            scope='smoothing_%d' % (level + 1)))
+        output_feature_map_keys.append('top_down_feature_map_%d' % level)
+      return collections.OrderedDict(
+          reversed(zip(output_feature_map_keys, output_feature_maps_list)))
diff --git a/research/object_detection/models/feature_map_generators_test.py b/research/object_detection/models/feature_map_generators_test.py
index cb69f0e4..cbbf6cf1 100644
--- a/research/object_detection/models/feature_map_generators_test.py
+++ b/research/object_detection/models/feature_map_generators_test.py
@@ -40,7 +40,7 @@ EMBEDDED_SSD_MOBILENET_V1_LAYOUT = {
 }
 
 
-# TODO(rathodv): add tests with different anchor strides.
+# TODO: add tests with different anchor strides.
 class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
 
   def test_get_expected_feature_map_shapes_with_inception_v2(self):
@@ -134,6 +134,34 @@ class MultiResolutionFeatureMapGeneratorTest(tf.test.TestCase):
       self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
 
 
+class FPNFeatureMapGeneratorTest(tf.test.TestCase):
+
+  def test_get_expected_feature_map_shapes(self):
+    image_features = [
+        tf.random_uniform([4, 8, 8, 256], dtype=tf.float32),
+        tf.random_uniform([4, 4, 4, 256], dtype=tf.float32),
+        tf.random_uniform([4, 2, 2, 256], dtype=tf.float32),
+        tf.random_uniform([4, 1, 1, 256], dtype=tf.float32),
+    ]
+    feature_maps = feature_map_generators.fpn_top_down_feature_maps(
+        image_features=image_features, depth=128)
+
+    expected_feature_map_shapes = {
+        'top_down_feature_map_0': (4, 8, 8, 128),
+        'top_down_feature_map_1': (4, 4, 4, 128),
+        'top_down_feature_map_2': (4, 2, 2, 128),
+        'top_down_feature_map_3': (4, 1, 1, 128)
+    }
+
+    init_op = tf.global_variables_initializer()
+    with self.test_session() as sess:
+      sess.run(init_op)
+      out_feature_maps = sess.run(feature_maps)
+      out_feature_map_shapes = {key: value.shape
+                                for key, value in out_feature_maps.items()}
+      self.assertDictEqual(out_feature_map_shapes, expected_feature_map_shapes)
+
+
 class GetDepthFunctionTest(tf.test.TestCase):
 
   def test_return_min_depth_when_multiplier_is_small(self):
diff --git a/research/object_detection/models/ssd_feature_extractor_test.py b/research/object_detection/models/ssd_feature_extractor_test.py
index 0b3da468..7f400156 100644
--- a/research/object_detection/models/ssd_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_feature_extractor_test.py
@@ -17,33 +17,14 @@
 
 from abc import abstractmethod
 
+import itertools
 import numpy as np
 import tensorflow as tf
 
+from object_detection.utils import test_case
 
-class SsdFeatureExtractorTestBase(object):
 
-  def _validate_features_shape(self,
-                               feature_extractor,
-                               preprocessed_inputs,
-                               expected_feature_map_shapes):
-    """Checks the extracted features are of correct shape.
-
-    Args:
-      feature_extractor: The feature extractor to test.
-      preprocessed_inputs: A [batch, height, width, 3] tensor to extract
-                           features with.
-      expected_feature_map_shapes: The expected shape of the extracted features.
-    """
-    feature_maps = feature_extractor.extract_features(preprocessed_inputs)
-    feature_map_shapes = [tf.shape(feature_map) for feature_map in feature_maps]
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      feature_map_shapes_out = sess.run(feature_map_shapes)
-      for shape_out, exp_shape_out in zip(
-          feature_map_shapes_out, expected_feature_map_shapes):
-        self.assertAllEqual(shape_out, exp_shape_out)
+class SsdFeatureExtractorTestBase(test_case.TestCase):
 
   @abstractmethod
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
@@ -59,14 +40,39 @@ class SsdFeatureExtractorTestBase(object):
     pass
 
   def check_extract_features_returns_correct_shape(
-      self, image_height, image_width, depth_multiplier, pad_to_multiple,
-      expected_feature_map_shapes_out):
-    feature_extractor = self._create_feature_extractor(depth_multiplier,
-                                                       pad_to_multiple)
-    preprocessed_inputs = tf.random_uniform(
-        [4, image_height, image_width, 3], dtype=tf.float32)
-    self._validate_features_shape(
-        feature_extractor, preprocessed_inputs, expected_feature_map_shapes_out)
+      self, batch_size, image_height, image_width, depth_multiplier,
+      pad_to_multiple, expected_feature_map_shapes):
+    def graph_fn(image_tensor):
+      feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                         pad_to_multiple)
+      feature_maps = feature_extractor.extract_features(image_tensor)
+      return feature_maps
+
+    image_tensor = np.random.rand(batch_size, image_height, image_width,
+                                  3).astype(np.float32)
+    feature_maps = self.execute(graph_fn, [image_tensor])
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shapes):
+      self.assertAllEqual(feature_map.shape, expected_shape)
+
+  def check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+      self, batch_size, image_height, image_width, depth_multiplier,
+      pad_to_multiple, expected_feature_map_shapes):
+    def graph_fn(image_height, image_width):
+      feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                         pad_to_multiple)
+      image_tensor = tf.random_uniform([batch_size, image_height, image_width,
+                                        3], dtype=tf.float32)
+      feature_maps = feature_extractor.extract_features(image_tensor)
+      return feature_maps
+
+    feature_maps = self.execute_cpu(graph_fn, [
+        np.array(image_height, dtype=np.int32),
+        np.array(image_width, dtype=np.int32)
+    ])
+    for feature_map, expected_shape in itertools.izip(
+        feature_maps, expected_feature_map_shapes):
+      self.assertAllEqual(feature_map.shape, expected_shape)
 
   def check_extract_features_raises_error_with_invalid_image_size(
       self, image_height, image_width, depth_multiplier, pad_to_multiple):
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index d1685d7f..5cf0bdce 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -19,6 +19,7 @@ import tensorflow as tf
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
 from object_detection.utils import ops
+from object_detection.utils import shape_utils
 from nets import inception_v2
 
 slim = tf.contrib.slim
@@ -34,7 +35,8 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                pad_to_multiple,
                conv_hyperparams,
                batch_norm_trainable=True,
-               reuse_weights=None):
+               reuse_weights=None,
+               use_explicit_padding=False):
     """InceptionV2 Feature Extractor for SSD Models.
 
     Args:
@@ -49,10 +51,13 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
     """
     super(SSDInceptionV2FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
+        use_explicit_padding)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -80,32 +85,29 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       feature_maps: a list of tensors where the ith tensor has shape
         [batch, height_i, width_i, depth_i]
     """
-    preprocessed_inputs.get_shape().assert_has_rank(4)
-    shape_assert = tf.Assert(
-        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
-                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
 
     feature_map_layout = {
         'from_layer': ['Mixed_4c', 'Mixed_5c', '', '', '', ''],
         'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_explicit_padding': self._use_explicit_padding,
     }
 
-    with tf.control_dependencies([shape_assert]):
-      with slim.arg_scope(self._conv_hyperparams):
-        with tf.variable_scope('InceptionV2',
-                               reuse=self._reuse_weights) as scope:
-          _, image_features = inception_v2.inception_v2_base(
-              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-              final_endpoint='Mixed_5c',
-              min_depth=self._min_depth,
-              depth_multiplier=self._depth_multiplier,
-              scope=scope)
-          feature_maps = feature_map_generators.multi_resolution_feature_maps(
-              feature_map_layout=feature_map_layout,
-              depth_multiplier=self._depth_multiplier,
-              min_depth=self._min_depth,
-              insert_1x1_conv=True,
-              image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
+      with tf.variable_scope('InceptionV2',
+                             reuse=self._reuse_weights) as scope:
+        _, image_features = inception_v2.inception_v2_base(
+            ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+            final_endpoint='Mixed_5c',
+            min_depth=self._min_depth,
+            depth_multiplier=self._depth_multiplier,
+            scope=scope)
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py b/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
index b265ccb0..b4ba65a2 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor_test.py
@@ -22,7 +22,7 @@ from object_detection.models import ssd_inception_v2_feature_extractor
 
 
 class SsdInceptionV2FeatureExtractorTest(
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
                                 is_training=True, batch_norm_trainable=True):
@@ -49,11 +49,23 @@ class SsdInceptionV2FeatureExtractorTest(
     image_width = 128
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 8, 8, 576), (4, 4, 4, 1024),
-                                  (4, 2, 2, 512), (4, 1, 1, 256),
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1024),
+                                  (2, 2, 2, 512), (2, 1, 1, 256),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 8, 8, 576), (2, 4, 4, 1024),
+                                  (2, 2, 2, 512), (2, 1, 1, 256),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_299(self):
@@ -61,11 +73,11 @@ class SsdInceptionV2FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 19, 19, 576), (4, 10, 10, 1024),
-                                  (4, 5, 5, 512), (4, 3, 3, 256),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 19, 19, 576), (2, 10, 10, 1024),
+                                  (2, 5, 5, 512), (2, 3, 3, 256),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
@@ -73,11 +85,11 @@ class SsdInceptionV2FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 0.5**12
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 19, 19, 128), (4, 10, 10, 128),
-                                  (4, 5, 5, 32), (4, 3, 3, 32),
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 19, 19, 128), (2, 10, 10, 128),
+                                  (2, 5, 5, 32), (2, 3, 3, 32),
+                                  (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
@@ -85,11 +97,11 @@ class SsdInceptionV2FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 32
-    expected_feature_map_shape = [(4, 20, 20, 576), (4, 10, 10, 1024),
-                                  (4, 5, 5, 512), (4, 3, 3, 256),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 20, 20, 576), (2, 10, 10, 1024),
+                                  (2, 5, 5, 512), (2, 3, 3, 256),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_raises_error_with_invalid_image_size(self):
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index 3a782eb2..987c020c 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -19,6 +19,7 @@ import tensorflow as tf
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
 from object_detection.utils import ops
+from object_detection.utils import shape_utils
 from nets import inception_v3
 
 slim = tf.contrib.slim
@@ -34,7 +35,8 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                pad_to_multiple,
                conv_hyperparams,
                batch_norm_trainable=True,
-               reuse_weights=None):
+               reuse_weights=None,
+               use_explicit_padding=False):
     """InceptionV3 Feature Extractor for SSD Models.
 
     Args:
@@ -49,10 +51,13 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
     """
     super(SSDInceptionV3FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
+        use_explicit_padding)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -80,32 +85,28 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       feature_maps: a list of tensors where the ith tensor has shape
         [batch, height_i, width_i, depth_i]
     """
-    preprocessed_inputs.get_shape().assert_has_rank(4)
-    shape_assert = tf.Assert(
-        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
-                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
 
     feature_map_layout = {
         'from_layer': ['Mixed_5d', 'Mixed_6e', 'Mixed_7c', '', '', ''],
         'layer_depth': [-1, -1, -1, 512, 256, 128],
+        'use_explicit_padding': self._use_explicit_padding,
     }
 
-    with tf.control_dependencies([shape_assert]):
-      with slim.arg_scope(self._conv_hyperparams):
-        with tf.variable_scope('InceptionV3',
-                               reuse=self._reuse_weights) as scope:
-          _, image_features = inception_v3.inception_v3_base(
-              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-              final_endpoint='Mixed_7c',
-              min_depth=self._min_depth,
-              depth_multiplier=self._depth_multiplier,
-              scope=scope)
-          feature_maps = feature_map_generators.multi_resolution_feature_maps(
-              feature_map_layout=feature_map_layout,
-              depth_multiplier=self._depth_multiplier,
-              min_depth=self._min_depth,
-              insert_1x1_conv=True,
-              image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
+      with tf.variable_scope('InceptionV3', reuse=self._reuse_weights) as scope:
+        _, image_features = inception_v3.inception_v3_base(
+            ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+            final_endpoint='Mixed_7c',
+            min_depth=self._min_depth,
+            depth_multiplier=self._depth_multiplier,
+            scope=scope)
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py b/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
index 89c1a288..4e1698db 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor_test.py
@@ -22,7 +22,7 @@ from object_detection.models import ssd_inception_v3_feature_extractor
 
 
 class SsdInceptionV3FeatureExtractorTest(
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
                                 is_training=True, batch_norm_trainable=True):
@@ -49,11 +49,23 @@ class SsdInceptionV3FeatureExtractorTest(
     image_width = 128
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 13, 13, 288), (4, 6, 6, 768),
-                                  (4, 2, 2, 2048), (4, 1, 1, 512),
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 13, 13, 288), (2, 6, 6, 768),
+                                  (2, 2, 2, 2048), (2, 1, 1, 512),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 13, 13, 288), (2, 6, 6, 768),
+                                  (2, 2, 2, 2048), (2, 1, 1, 512),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_299(self):
@@ -61,11 +73,11 @@ class SsdInceptionV3FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 35, 35, 288), (4, 17, 17, 768),
-                                  (4, 8, 8, 2048), (4, 4, 4, 512),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 35, 35, 288), (2, 17, 17, 768),
+                                  (2, 8, 8, 2048), (2, 4, 4, 512),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
@@ -73,11 +85,11 @@ class SsdInceptionV3FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 0.5**12
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 35, 35, 128), (4, 17, 17, 128),
-                                  (4, 8, 8, 192), (4, 4, 4, 32),
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 35, 35, 128), (2, 17, 17, 128),
+                                  (2, 8, 8, 192), (2, 4, 4, 32),
+                                  (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
@@ -85,11 +97,11 @@ class SsdInceptionV3FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 32
-    expected_feature_map_shape = [(4, 37, 37, 288), (4, 18, 18, 768),
-                                  (4, 8, 8, 2048), (4, 4, 4, 512),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 37, 37, 288), (2, 18, 18, 768),
+                                  (2, 8, 8, 2048), (2, 4, 4, 512),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_raises_error_with_invalid_image_size(self):
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index 456e2d1d..1785d04c 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -20,6 +20,7 @@ import tensorflow as tf
 from object_detection.meta_architectures import ssd_meta_arch
 from object_detection.models import feature_map_generators
 from object_detection.utils import ops
+from object_detection.utils import shape_utils
 from nets import mobilenet_v1
 
 slim = tf.contrib.slim
@@ -35,7 +36,8 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                pad_to_multiple,
                conv_hyperparams,
                batch_norm_trainable=True,
-               reuse_weights=None):
+               reuse_weights=None,
+               use_explicit_padding=False):
     """MobileNetV1 Feature Extractor for SSD Models.
 
     Args:
@@ -50,10 +52,13 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         (e.g. 1), it is desirable to disable batch norm update and use
         pretrained batch norm params.
       reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
     """
     super(SSDMobileNetV1FeatureExtractor, self).__init__(
         is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams, batch_norm_trainable, reuse_weights)
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
+        use_explicit_padding)
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -81,34 +86,32 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       feature_maps: a list of tensors where the ith tensor has shape
         [batch, height_i, width_i, depth_i]
     """
-    preprocessed_inputs.get_shape().assert_has_rank(4)
-    shape_assert = tf.Assert(
-        tf.logical_and(tf.greater_equal(tf.shape(preprocessed_inputs)[1], 33),
-                       tf.greater_equal(tf.shape(preprocessed_inputs)[2], 33)),
-        ['image size must at least be 33 in both height and width.'])
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
 
     feature_map_layout = {
         'from_layer': ['Conv2d_11_pointwise', 'Conv2d_13_pointwise', '', '',
                        '', ''],
         'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_explicit_padding': self._use_explicit_padding,
     }
 
-    with tf.control_dependencies([shape_assert]):
-      with slim.arg_scope(self._conv_hyperparams):
-        with slim.arg_scope([slim.batch_norm], fused=False):
-          with tf.variable_scope('MobilenetV1',
-                                 reuse=self._reuse_weights) as scope:
-            _, image_features = mobilenet_v1.mobilenet_v1_base(
-                ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
-                final_endpoint='Conv2d_13_pointwise',
-                min_depth=self._min_depth,
-                depth_multiplier=self._depth_multiplier,
-                scope=scope)
-            feature_maps = feature_map_generators.multi_resolution_feature_maps(
-                feature_map_layout=feature_map_layout,
-                depth_multiplier=self._depth_multiplier,
-                min_depth=self._min_depth,
-                insert_1x1_conv=True,
-                image_features=image_features)
+    with slim.arg_scope(self._conv_hyperparams):
+      # TODO: Enable fused batch norm once quantization supports it.
+      with slim.arg_scope([slim.batch_norm], fused=False):
+        with tf.variable_scope('MobilenetV1',
+                               reuse=self._reuse_weights) as scope:
+          _, image_features = mobilenet_v1.mobilenet_v1_base(
+              ops.pad_to_multiple(preprocessed_inputs, self._pad_to_multiple),
+              final_endpoint='Conv2d_13_pointwise',
+              min_depth=self._min_depth,
+              depth_multiplier=self._depth_multiplier,
+              scope=scope)
+          feature_maps = feature_map_generators.multi_resolution_feature_maps(
+              feature_map_layout=feature_map_layout,
+              depth_multiplier=self._depth_multiplier,
+              min_depth=self._min_depth,
+              insert_1x1_conv=True,
+              image_features=image_features)
 
     return feature_maps.values()
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
index 9159ceb1..84c73128 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor_test.py
@@ -24,7 +24,7 @@ slim = tf.contrib.slim
 
 
 class SsdMobilenetV1FeatureExtractorTest(
-    ssd_feature_extractor_test.SsdFeatureExtractorTestBase, tf.test.TestCase):
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
 
   def _create_feature_extractor(self, depth_multiplier, pad_to_multiple,
                                 is_training=True, batch_norm_trainable=True):
@@ -52,11 +52,11 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 128
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 8, 8, 512), (4, 4, 4, 1024),
-                                  (4, 2, 2, 512), (4, 1, 1, 256),
-                                  (4, 1, 1, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 8, 8, 512), (2, 4, 4, 1024),
+                                  (2, 2, 2, 512), (2, 1, 1, 256),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_299(self):
@@ -64,11 +64,23 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 19, 19, 512), (4, 10, 10, 1024),
-                                  (4, 5, 5, 512), (4, 3, 3, 256),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 19, 19, 512), (2, 10, 10, 1024),
+                                  (2, 5, 5, 512), (2, 3, 3, 256),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_with_dynamic_image_shape(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 8, 8, 512), (2, 4, 4, 1024),
+                                  (2, 2, 2, 512), (2, 1, 1, 256),
+                                  (2, 1, 1, 256), (2, 1, 1, 128)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_enforcing_min_depth(self):
@@ -76,11 +88,11 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 0.5**12
     pad_to_multiple = 1
-    expected_feature_map_shape = [(4, 19, 19, 32), (4, 10, 10, 32),
-                                  (4, 5, 5, 32), (4, 3, 3, 32),
-                                  (4, 2, 2, 32), (4, 1, 1, 32)]
+    expected_feature_map_shape = [(2, 19, 19, 32), (2, 10, 10, 32),
+                                  (2, 5, 5, 32), (2, 3, 3, 32),
+                                  (2, 2, 2, 32), (2, 1, 1, 32)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
@@ -88,11 +100,11 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 299
     depth_multiplier = 1.0
     pad_to_multiple = 32
-    expected_feature_map_shape = [(4, 20, 20, 512), (4, 10, 10, 1024),
-                                  (4, 5, 5, 512), (4, 3, 3, 256),
-                                  (4, 2, 2, 256), (4, 1, 1, 128)]
+    expected_feature_map_shape = [(2, 20, 20, 512), (2, 10, 10, 1024),
+                                  (2, 5, 5, 512), (2, 3, 3, 256),
+                                  (2, 2, 2, 256), (2, 1, 1, 128)]
     self.check_extract_features_returns_correct_shape(
-        image_height, image_width, depth_multiplier, pad_to_multiple,
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
         expected_feature_map_shape)
 
   def test_extract_features_raises_error_with_invalid_image_size(self):
@@ -108,7 +120,7 @@ class SsdMobilenetV1FeatureExtractorTest(
     image_width = 128
     depth_multiplier = 1
     pad_to_multiple = 1
-    test_image = np.random.rand(4, image_height, image_width, 3)
+    test_image = np.random.rand(2, image_height, image_width, 3)
     feature_extractor = self._create_feature_extractor(depth_multiplier,
                                                        pad_to_multiple)
     preprocessed_image = feature_extractor.preprocess(test_image)
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
new file mode 100644
index 00000000..b043c2c2
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -0,0 +1,246 @@
+"""SSD Feature Pyramid Network (FPN) feature extractors based on Resnet v1.
+
+See https://arxiv.org/abs/1708.02002 for details.
+"""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets import resnet_v1
+
+slim = tf.contrib.slim
+
+
+class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD FPN feature extractor based on Resnet v1 architecture."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               resnet_base_fn,
+               resnet_scope_name,
+               batch_norm_trainable=True,
+               reuse_weights=None,
+               use_explicit_padding=False):
+    """SSD FPN feature extractor based on Resnet v1 architecture.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+        UNUSED currently.
+      min_depth: minimum feature extractor depth. UNUSED Currently.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name to construct resnet
+      batch_norm_trainable: Whether to update batch norm parameters during
+        training or not. When training with a small batch size
+        (e.g. 1), it is desirable to disable batch norm update and use
+        pretrained batch norm params.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False. UNUSED currently.
+
+    Raises:
+      ValueError: On supplying invalid arguments for unused arguments.
+    """
+    super(_SSDResnetV1FpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams, batch_norm_trainable, reuse_weights,
+        use_explicit_padding)
+    if self._depth_multiplier != 1.0:
+      raise ValueError('Only depth 1.0 is supported, found: {}'.
+                       format(self._depth_multiplier))
+    if self._use_explicit_padding is True:
+      raise ValueError('Explicit padding is not a valid option.')
+    self._resnet_base_fn = resnet_base_fn
+    self._resnet_scope_name = resnet_scope_name
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    VGG style channel mean subtraction as described here:
+    https://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-mdnge.
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    channel_means = [123.68, 116.779, 103.939]
+    return resized_inputs - [[channel_means]]
+
+  def _filter_features(self, image_features):
+    # TODO: Change resnet endpoint to strip scope prefixes instead
+    # of munging the scope here.
+    filtered_image_features = dict({})
+    for key, feature in image_features.items():
+      feature_name = key.split('/')[-1]
+      if feature_name in ['block2', 'block3', 'block4']:
+        filtered_image_features[feature_name] = feature
+    return filtered_image_features
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+
+    Raises:
+      ValueError: depth multiplier is not supported.
+    """
+    if self._depth_multiplier != 1.0:
+      raise ValueError('Depth multiplier not supported.')
+
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        129, preprocessed_inputs)
+
+    with tf.variable_scope(
+        self._resnet_scope_name, reuse=self._reuse_weights) as scope:
+      with slim.arg_scope(resnet_v1.resnet_arg_scope()):
+        _, image_features = self._resnet_base_fn(
+            inputs=ops.pad_to_multiple(preprocessed_inputs,
+                                       self._pad_to_multiple),
+            num_classes=None,
+            is_training=self._is_training and self._batch_norm_trainable,
+            global_pool=False,
+            output_stride=None,
+            store_non_strided_activations=True,
+            scope=scope)
+      image_features = self._filter_features(image_features)
+      last_feature_map = image_features['block4']
+      with slim.arg_scope(self._conv_hyperparams):
+        for i in range(5, 7):
+          last_feature_map = slim.conv2d(
+              last_feature_map,
+              num_outputs=256,
+              kernel_size=[3, 3],
+              stride=2,
+              padding='SAME',
+              scope='block{}'.format(i))
+          image_features['bottomup_{}'.format(i)] = last_feature_map
+        feature_maps = feature_map_generators.fpn_top_down_feature_maps(
+            [
+                image_features[key] for key in
+                ['block2', 'block3', 'block4', 'bottomup_5', 'bottomup_6']
+            ],
+            depth=256,
+            scope='top_down_features')
+    return feature_maps.values()
+
+
+class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               batch_norm_trainable=True,
+               reuse_weights=None,
+               use_explicit_padding=False):
+    """Resnet50 v1 FPN Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      batch_norm_trainable: Whether to update batch norm parameters during
+        training or not. When training with a small batch size
+        (e.g. 1), it is desirable to disable batch norm update and use
+        pretrained batch norm params.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+    """
+    super(SSDResnet50V1FpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams, resnet_v1.resnet_v1_50, 'resnet_v1_50_fpn',
+        batch_norm_trainable, reuse_weights, use_explicit_padding)
+
+
+class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               batch_norm_trainable=True,
+               reuse_weights=None,
+               use_explicit_padding=False):
+    """Resnet101 v1 FPN Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      batch_norm_trainable: Whether to update batch norm parameters during
+        training or not. When training with a small batch size
+        (e.g. 1), it is desirable to disable batch norm update and use
+        pretrained batch norm params.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+    """
+    super(SSDResnet101V1FpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams, resnet_v1.resnet_v1_101, 'resnet_v1_101_fpn',
+        batch_norm_trainable, reuse_weights, use_explicit_padding)
+
+
+class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams,
+               batch_norm_trainable=True,
+               reuse_weights=None,
+               use_explicit_padding=False):
+    """Resnet152 v1 FPN Feature Extractor for SSD Models.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams: tf slim arg_scope for conv2d and separable_conv2d ops.
+      batch_norm_trainable: Whether to update batch norm parameters during
+        training or not. When training with a small batch size
+        (e.g. 1), it is desirable to disable batch norm update and use
+        pretrained batch norm params.
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+    """
+    super(SSDResnet152V1FpnFeatureExtractor, self).__init__(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams, resnet_v1.resnet_v1_152, 'resnet_v1_152_fpn',
+        batch_norm_trainable, reuse_weights, use_explicit_padding)
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
new file mode 100644
index 00000000..4b917283
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_test.py
@@ -0,0 +1,65 @@
+"""Tests for ssd resnet v1 FPN feature extractors."""
+import tensorflow as tf
+
+from object_detection.models import ssd_resnet_v1_fpn_feature_extractor
+from object_detection.models import ssd_resnet_v1_fpn_feature_extractor_testbase
+
+
+class SSDResnet50V1FeatureExtractorTest(
+    ssd_resnet_v1_fpn_feature_extractor_testbase.
+    SSDResnetFeatureExtractorTestBase):
+  """SSDResnet50v1Fpn feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+    min_depth = 32
+    conv_hyperparams = {}
+    batch_norm_trainable = True
+    is_training = True
+    return ssd_resnet_v1_fpn_feature_extractor.SSDResnet50V1FpnFeatureExtractor(
+        is_training, depth_multiplier, min_depth, pad_to_multiple,
+        conv_hyperparams, batch_norm_trainable)
+
+  def _scope_name(self):
+    return 'resnet_v1_50_fpn'
+
+
+class SSDResnet101V1FeatureExtractorTest(
+    ssd_resnet_v1_fpn_feature_extractor_testbase.
+    SSDResnetFeatureExtractorTestBase):
+  """SSDResnet101v1Fpn feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+    min_depth = 32
+    conv_hyperparams = {}
+    batch_norm_trainable = True
+    is_training = True
+    return (
+        ssd_resnet_v1_fpn_feature_extractor.SSDResnet101V1FpnFeatureExtractor(
+            is_training, depth_multiplier, min_depth, pad_to_multiple,
+            conv_hyperparams, batch_norm_trainable))
+
+  def _scope_name(self):
+    return 'resnet_v1_101_fpn'
+
+
+class SSDResnet152V1FeatureExtractorTest(
+    ssd_resnet_v1_fpn_feature_extractor_testbase.
+    SSDResnetFeatureExtractorTestBase):
+  """SSDResnet152v1Fpn feature extractor test."""
+
+  def _create_feature_extractor(self, depth_multiplier, pad_to_multiple):
+    min_depth = 32
+    conv_hyperparams = {}
+    batch_norm_trainable = True
+    is_training = True
+    return (
+        ssd_resnet_v1_fpn_feature_extractor.SSDResnet152V1FpnFeatureExtractor(
+            is_training, depth_multiplier, min_depth, pad_to_multiple,
+            conv_hyperparams, batch_norm_trainable))
+
+  def _scope_name(self):
+    return 'resnet_v1_152_fpn'
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
new file mode 100644
index 00000000..c640c742
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor_testbase.py
@@ -0,0 +1,77 @@
+"""Tests for ssd resnet v1 FPN feature extractors."""
+import abc
+import numpy as np
+
+from object_detection.models import ssd_feature_extractor_test
+
+
+class SSDResnetFeatureExtractorTestBase(
+    ssd_feature_extractor_test.SsdFeatureExtractorTestBase):
+  """Helper test class for SSD Resnet v1 FPN feature extractors."""
+
+  @abc.abstractmethod
+  def _scope_name(self):
+    pass
+
+  def test_extract_features_returns_correct_shapes_256(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),
+                                  (2, 8, 8, 256), (2, 4, 4, 256),
+                                  (2, 2, 2, 256)]
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_dynamic_inputs(self):
+    image_height = 256
+    image_width = 256
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),
+                                  (2, 8, 8, 256), (2, 4, 4, 256),
+                                  (2, 2, 2, 256)]
+    self.check_extract_features_returns_correct_shapes_with_dynamic_inputs(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_returns_correct_shapes_with_pad_to_multiple(self):
+    image_height = 254
+    image_width = 254
+    depth_multiplier = 1.0
+    pad_to_multiple = 32
+    expected_feature_map_shape = [(2, 32, 32, 256), (2, 16, 16, 256),
+                                  (2, 8, 8, 256), (2, 4, 4, 256),
+                                  (2, 2, 2, 256)]
+
+    self.check_extract_features_returns_correct_shape(
+        2, image_height, image_width, depth_multiplier, pad_to_multiple,
+        expected_feature_map_shape)
+
+  def test_extract_features_raises_error_with_invalid_image_size(self):
+    image_height = 32
+    image_width = 32
+    depth_multiplier = 1.0
+    pad_to_multiple = 1
+    self.check_extract_features_raises_error_with_invalid_image_size(
+        image_height, image_width, depth_multiplier, pad_to_multiple)
+
+  def test_preprocess_returns_correct_value_range(self):
+    image_height = 128
+    image_width = 128
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    test_image = np.random.rand(4, image_height, image_width, 3)
+    feature_extractor = self._create_feature_extractor(depth_multiplier,
+                                                       pad_to_multiple)
+    preprocessed_image = feature_extractor.preprocess(test_image)
+    self.assertAllClose(preprocessed_image,
+                        test_image - [[123.68, 116.779, 103.939]])
+
+  def test_variables_only_created_in_scope(self):
+    depth_multiplier = 1
+    pad_to_multiple = 1
+    self.check_feature_extractor_variables_under_scope(
+        depth_multiplier, pad_to_multiple, self._scope_name())
diff --git a/research/object_detection/protos/BUILD b/research/object_detection/protos/BUILD
index 1b7eb148..439d43f0 100644
--- a/research/object_detection/protos/BUILD
+++ b/research/object_detection/protos/BUILD
@@ -9,6 +9,7 @@ licenses(["notice"])
 proto_library(
     name = "argmax_matcher_proto",
     srcs = ["argmax_matcher.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -20,6 +21,7 @@ py_proto_library(
 proto_library(
     name = "bipartite_matcher_proto",
     srcs = ["bipartite_matcher.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -31,6 +33,7 @@ py_proto_library(
 proto_library(
     name = "matcher_proto",
     srcs = ["matcher.proto"],
+    cc_api_version = 2,
     deps = [
         ":argmax_matcher_proto",
         ":bipartite_matcher_proto",
@@ -46,6 +49,7 @@ py_proto_library(
 proto_library(
     name = "faster_rcnn_box_coder_proto",
     srcs = ["faster_rcnn_box_coder.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -57,6 +61,7 @@ py_proto_library(
 proto_library(
     name = "keypoint_box_coder_proto",
     srcs = ["keypoint_box_coder.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -68,6 +73,7 @@ py_proto_library(
 proto_library(
     name = "mean_stddev_box_coder_proto",
     srcs = ["mean_stddev_box_coder.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -79,6 +85,7 @@ py_proto_library(
 proto_library(
     name = "square_box_coder_proto",
     srcs = ["square_box_coder.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -90,6 +97,7 @@ py_proto_library(
 proto_library(
     name = "box_coder_proto",
     srcs = ["box_coder.proto"],
+    cc_api_version = 2,
     deps = [
         ":faster_rcnn_box_coder_proto",
         ":keypoint_box_coder_proto",
@@ -107,6 +115,7 @@ py_proto_library(
 proto_library(
     name = "grid_anchor_generator_proto",
     srcs = ["grid_anchor_generator.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -118,6 +127,7 @@ py_proto_library(
 proto_library(
     name = "ssd_anchor_generator_proto",
     srcs = ["ssd_anchor_generator.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -126,11 +136,25 @@ py_proto_library(
     deps = [":ssd_anchor_generator_proto"],
 )
 
+proto_library(
+    name = "multiscale_anchor_generator_proto",
+    srcs = ["multiscale_anchor_generator.proto"],
+    cc_api_version = 2,
+)
+
+py_proto_library(
+    name = "multiscale_anchor_generator_py_pb2",
+    api_version = 2,
+    deps = [":multiscale_anchor_generator_proto"],
+)
+
 proto_library(
     name = "anchor_generator_proto",
     srcs = ["anchor_generator.proto"],
+    cc_api_version = 2,
     deps = [
         ":grid_anchor_generator_proto",
+        ":multiscale_anchor_generator_proto",
         ":ssd_anchor_generator_proto",
     ],
 )
@@ -144,6 +168,7 @@ py_proto_library(
 proto_library(
     name = "input_reader_proto",
     srcs = ["input_reader.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -155,6 +180,7 @@ py_proto_library(
 proto_library(
     name = "losses_proto",
     srcs = ["losses.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -166,6 +192,7 @@ py_proto_library(
 proto_library(
     name = "optimizer_proto",
     srcs = ["optimizer.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -177,6 +204,7 @@ py_proto_library(
 proto_library(
     name = "post_processing_proto",
     srcs = ["post_processing.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -188,6 +216,7 @@ py_proto_library(
 proto_library(
     name = "hyperparams_proto",
     srcs = ["hyperparams.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -199,6 +228,7 @@ py_proto_library(
 proto_library(
     name = "box_predictor_proto",
     srcs = ["box_predictor.proto"],
+    cc_api_version = 2,
     deps = [":hyperparams_proto"],
 )
 
@@ -211,6 +241,7 @@ py_proto_library(
 proto_library(
     name = "region_similarity_calculator_proto",
     srcs = ["region_similarity_calculator.proto"],
+    cc_api_version = 2,
     deps = [],
 )
 
@@ -223,6 +254,7 @@ py_proto_library(
 proto_library(
     name = "preprocessor_proto",
     srcs = ["preprocessor.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -234,6 +266,7 @@ py_proto_library(
 proto_library(
     name = "train_proto",
     srcs = ["train.proto"],
+    cc_api_version = 2,
     deps = [
         ":optimizer_proto",
         ":preprocessor_proto",
@@ -249,6 +282,7 @@ py_proto_library(
 proto_library(
     name = "eval_proto",
     srcs = ["eval.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -260,6 +294,7 @@ py_proto_library(
 proto_library(
     name = "image_resizer_proto",
     srcs = ["image_resizer.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
@@ -271,19 +306,21 @@ py_proto_library(
 proto_library(
     name = "faster_rcnn_proto",
     srcs = ["faster_rcnn.proto"],
+    cc_api_version = 2,
     deps = [
         ":box_predictor_proto",
-        "//object_detection/protos:anchor_generator_proto",
-        "//object_detection/protos:hyperparams_proto",
-        "//object_detection/protos:image_resizer_proto",
-        "//object_detection/protos:losses_proto",
-        "//object_detection/protos:post_processing_proto",
+        "//tensorflow/models/research/object_detection/protos:anchor_generator_proto",
+        "//tensorflow/models/research/object_detection/protos:hyperparams_proto",
+        "//tensorflow/models/research/object_detection/protos:image_resizer_proto",
+        "//tensorflow/models/research/object_detection/protos:losses_proto",
+        "//tensorflow/models/research/object_detection/protos:post_processing_proto",
     ],
 )
 
 proto_library(
     name = "ssd_proto",
     srcs = ["ssd.proto"],
+    cc_api_version = 2,
     deps = [
         ":anchor_generator_proto",
         ":box_coder_proto",
@@ -300,6 +337,7 @@ proto_library(
 proto_library(
     name = "model_proto",
     srcs = ["model.proto"],
+    cc_api_version = 2,
     deps = [
         ":faster_rcnn_proto",
         ":ssd_proto",
@@ -315,6 +353,7 @@ py_proto_library(
 proto_library(
     name = "pipeline_proto",
     srcs = ["pipeline.proto"],
+    cc_api_version = 2,
     deps = [
         ":eval_proto",
         ":input_reader_proto",
@@ -332,6 +371,7 @@ py_proto_library(
 proto_library(
     name = "string_int_label_map_proto",
     srcs = ["string_int_label_map.proto"],
+    cc_api_version = 2,
 )
 
 py_proto_library(
diff --git a/research/object_detection/protos/anchor_generator.proto b/research/object_detection/protos/anchor_generator.proto
index 4b7b1d62..c47b558f 100644
--- a/research/object_detection/protos/anchor_generator.proto
+++ b/research/object_detection/protos/anchor_generator.proto
@@ -4,6 +4,7 @@ package object_detection.protos;
 
 import "object_detection/protos/grid_anchor_generator.proto";
 import "object_detection/protos/ssd_anchor_generator.proto";
+import "object_detection/protos/multiscale_anchor_generator.proto";
 
 // Configuration proto for the anchor generator to use in the object detection
 // pipeline. See core/anchor_generator.py for details.
@@ -11,5 +12,6 @@ message AnchorGenerator {
   oneof anchor_generator_oneof {
     GridAnchorGenerator grid_anchor_generator = 1;
     SsdAnchorGenerator ssd_anchor_generator = 2;
+    MultiscaleAnchorGenerator multiscale_anchor_generator = 3;
   }
 }
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index 4aa445cc..f9310fcd 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -11,6 +11,7 @@ message BoxPredictor {
     ConvolutionalBoxPredictor convolutional_box_predictor = 1;
     MaskRCNNBoxPredictor mask_rcnn_box_predictor = 2;
     RfcnBoxPredictor rfcn_box_predictor = 3;
+    WeightSharedConvolutionalBoxPredictor weight_shared_convolutional_box_predictor = 4;
   }
 }
 
@@ -46,12 +47,39 @@ message ConvolutionalBoxPredictor {
   optional int32 box_code_size = 8 [default = 4];
 
   // Whether to apply sigmoid to the output of class predictions.
-  // TODO: Do we need this since we have a post processing module.?
+  // TODO(jonathanhuang): Do we need this since we have a post processing
+  // module.?
   optional bool apply_sigmoid_to_scores = 9 [default = false];
 
   optional float class_prediction_bias_init = 10 [default = 0.0];
 }
 
+// Configuration proto for weight shared convolutional box predictor.
+message WeightSharedConvolutionalBoxPredictor {
+  // Hyperparameters for convolution ops used in the box predictor.
+  optional Hyperparams conv_hyperparams = 1;
+
+  // Number of the additional conv layers before the predictor.
+  optional int32 num_layers_before_predictor = 4 [default = 0];
+
+  // Output depth for the convolution ops prior to predicting box encodings
+  // and class predictions.
+  optional int32 depth = 2 [default = 0];
+
+  // Size of final convolution kernel. If the spatial resolution of the feature
+  // map is smaller than the kernel size, then the kernel size is set to
+  // min(feature_width, feature_height).
+  optional int32 kernel_size = 7 [default = 3];
+
+  // Size of the encoding for boxes.
+  optional int32 box_code_size = 8 [default = 4];
+
+  // Bias initialization for class prediction. It has been show to stabilize
+  // training where there are large number of negative boxes. See
+  // https://arxiv.org/abs/1708.02002 for details.
+  optional float class_prediction_bias_init = 10 [default = 0.0];
+}
+
 message MaskRCNNBoxPredictor {
   // Hyperparameters for fully connected ops used in the box predictor.
   optional Hyperparams fc_hyperparams = 1;
@@ -71,12 +99,22 @@ message MaskRCNNBoxPredictor {
   // Whether to predict instance masks inside detection boxes.
   optional bool predict_instance_masks = 6 [default = false];
 
-  // The depth for the first conv2d_transpose op  applied to the
-  // image_features in the mask prediciton branch
+  // The depth for the first conv2d_transpose op applied to the
+  // image_features in the mask prediction branch. If set to 0, the value
+  // will be set automatically based on the number of channels in the image
+  // features and the number of classes.
   optional int32 mask_prediction_conv_depth = 7 [default = 256];
 
   // Whether to predict keypoints inside detection boxes.
   optional bool predict_keypoints = 8 [default = false];
+
+  // The height and the width of the predicted mask.
+  optional int32 mask_height = 9 [default = 15];
+  optional int32 mask_width = 10 [default = 15];
+
+  // The number of convolutions applied to image_features in the mask prediction
+  // branch.
+  optional int32 mask_prediction_num_conv_layers = 11 [default = 2];
 }
 
 message RfcnBoxPredictor {
diff --git a/research/object_detection/protos/eval.proto b/research/object_detection/protos/eval.proto
index c5a30ec6..9410b6f9 100644
--- a/research/object_detection/protos/eval.proto
+++ b/research/object_detection/protos/eval.proto
@@ -26,9 +26,8 @@ message EvalConfig {
   // BNS name of the TensorFlow master.
   optional string eval_master = 7 [default=""];
 
-  // Type of metrics to use for evaluation. Currently supports only Pascal VOC
-  // detection metrics.
-  optional string metrics_set = 8 [default="pascal_voc_metrics"];
+  // Type of metrics to use for evaluation.
+  repeated string metrics_set = 8;
 
   // Path to export detections to COCO compatible JSON format.
   optional string export_path = 9 [default=''];
@@ -38,10 +37,35 @@ message EvalConfig {
   optional bool ignore_groundtruth = 10 [default=false];
 
   // Use exponential moving averages of variables for evaluation.
+  // TODO(rathodv): When this is false make sure the model is constructed
+  // without moving averages in restore_fn.
   optional bool use_moving_averages = 11 [default=false];
 
   // Whether to evaluate instance masks.
   // Note that since there is no evaluation code currently for instance
   // segmenation this option is unused.
   optional bool eval_instance_masks = 12 [default=false];
+
+  // Minimum score threshold for a detected object box to be visualized
+  optional float min_score_threshold = 13 [default=0.5];
+
+  // Maximum number of detections to visualize
+  optional int32 max_num_boxes_to_visualize = 14 [default=20];
+
+  // When drawing a single detection, each label is by default visualized as
+  // <label name> : <label score>. One can skip the name or/and score using the
+  // following fields:
+  optional bool skip_scores = 15 [default=false];
+  optional bool skip_labels = 16 [default=false];
+
+  // Whether to show groundtruth boxes in addition to detected boxes in
+  // visualizations.
+  optional bool visualize_groundtruth_boxes = 17 [default=false];
+
+  // Box color for visualizing groundtruth boxes.
+  optional string groundtruth_box_visualization_color = 18 [default="black"];
+
+  // Whether to keep image identifier in filename when exported to
+  // visualization_export_dir.
+  optional bool keep_image_id_for_visualization_export = 19 [default=false];
 }
diff --git a/research/object_detection/protos/faster_rcnn.proto b/research/object_detection/protos/faster_rcnn.proto
index 20c859e2..1d02c2eb 100644
--- a/research/object_detection/protos/faster_rcnn.proto
+++ b/research/object_detection/protos/faster_rcnn.proto
@@ -20,7 +20,7 @@ import "object_detection/protos/post_processing.proto";
 message FasterRcnn {
 
   // Whether to construct only the Region Proposal Network (RPN).
-  optional bool first_stage_only = 1 [default=false];
+  optional int32 number_of_stages = 1 [default=2];
 
   // Number of classes to predict.
   optional int32 num_classes = 3;
diff --git a/research/object_detection/protos/image_resizer.proto b/research/object_detection/protos/image_resizer.proto
index 67f6cacd..8e8eec7f 100644
--- a/research/object_detection/protos/image_resizer.proto
+++ b/research/object_detection/protos/image_resizer.proto
@@ -29,6 +29,11 @@ message KeepAspectRatioResizer {
 
   // Desired method when resizing image.
   optional ResizeType resize_method = 3 [default = BILINEAR];
+
+  // Whether to pad the image with zeros so the output spatial size is
+  // [max_dimension, max_dimension]. Note that the zeros are padded to the
+  // bottom and the right of the resized image.
+  optional bool pad_to_max_dimension = 4 [default = false];
 }
 
 // Configuration proto for image resizer that resizes to a fixed shape.
diff --git a/research/object_detection/protos/input_reader.proto b/research/object_detection/protos/input_reader.proto
index ed460dad..e379dda7 100644
--- a/research/object_detection/protos/input_reader.proto
+++ b/research/object_detection/protos/input_reader.proto
@@ -15,6 +15,13 @@ package object_detection.protos;
 // 'groundtruth_instance_masks': (Optional), a [num_boxes, image_height,
 //    image_width] float tensor storing binary mask of the objects in boxes.
 
+// Instance mask format. Note that PNG masks are much more space efficient.
+enum InstanceMaskType {
+  DEFAULT = 0;          // Default implementation, currently NUMERICAL_MASKS
+  NUMERICAL_MASKS = 1;  // [num_masks, H, W] float32 binary masks.
+  PNG_MASKS = 2;        // Encoded PNG masks.
+}
+
 message InputReader {
   // Path to StringIntLabelMap pbtxt file specifying the mapping from string
   // labels to integer ids.
@@ -24,6 +31,12 @@ message InputReader {
   // shuffled randomly.
   optional bool shuffle = 2 [default=true];
 
+  // Buffer size to be used when shuffling.
+  optional uint32 shuffle_buffer_size = 11 [default = 100];
+
+  // Buffer size to be used when shuffling file names.
+  optional uint32 filenames_shuffle_buffer_size = 12 [default = 100];
+
   // Maximum number of records to keep in reader queue.
   optional uint32 queue_capacity = 3 [default=2000];
 
@@ -38,9 +51,15 @@ message InputReader {
   // Number of reader instances to create.
   optional uint32 num_readers = 6 [default=8];
 
+  // Size of the buffer for prefetching (in batches).
+  optional uint32 prefetch_buffer_size = 13 [default = 2];
+
   // Whether to load groundtruth instance masks.
   optional bool load_instance_masks = 7 [default = false];
 
+  // Type of instance mask.
+  optional InstanceMaskType mask_type = 10 [default = NUMERICAL_MASKS];
+
   oneof input_reader {
     TFRecordInputReader tf_record_input_reader = 8;
     ExternalInputReader external_input_reader = 9;
diff --git a/research/object_detection/protos/losses.proto b/research/object_detection/protos/losses.proto
index e2d189b5..3a2ae661 100644
--- a/research/object_detection/protos/losses.proto
+++ b/research/object_detection/protos/losses.proto
@@ -33,12 +33,14 @@ message LocalizationLoss {
 
 // L2 location loss: 0.5 * ||weight * (a - b)|| ^ 2
 message WeightedL2LocalizationLoss {
+  // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 1 [default=false];
 }
 
 // SmoothL1 (Huber) location loss: .5 * x ^ 2 if |x| < 1 else |x| - .5
 message WeightedSmoothL1LocalizationLoss {
+  // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 1 [default=false];
 }
@@ -59,6 +61,7 @@ message ClassificationLoss {
 
 // Classification loss using a sigmoid function over class predictions.
 message WeightedSigmoidClassificationLoss {
+  // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 1 [default=false];
 }
@@ -66,6 +69,7 @@ message WeightedSigmoidClassificationLoss {
 // Sigmoid Focal cross entropy loss as described in
 // https://arxiv.org/abs/1708.02002
 message SigmoidFocalClassificationLoss {
+  // DEPRECATED, do not use.
   optional bool anchorwise_output = 1 [default = false];
   // modulating factor for the loss.
   optional float gamma = 2 [default = 2.0];
@@ -75,6 +79,7 @@ message SigmoidFocalClassificationLoss {
 
 // Classification loss using a softmax function over class predictions.
 message WeightedSoftmaxClassificationLoss {
+  // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 1 [default=false];
   // Scale logit (input) value before calculating softmax classification loss.
@@ -93,6 +98,7 @@ message BootstrappedSigmoidClassificationLoss {
   // probabilities.
   optional bool hard_bootstrap = 2 [default=false];
 
+  // DEPRECATED, do not use.
   // Output loss per anchor.
   optional bool anchorwise_output = 3 [default=false];
 }
diff --git a/research/object_detection/protos/multiscale_anchor_generator.proto b/research/object_detection/protos/multiscale_anchor_generator.proto
new file mode 100644
index 00000000..50869793
--- /dev/null
+++ b/research/object_detection/protos/multiscale_anchor_generator.proto
@@ -0,0 +1,22 @@
+  syntax = "proto2";
+
+package object_detection.protos;
+
+// Configuration proto for RetinaNet anchor generator described in
+// https://arxiv.org/abs/1708.02002. See
+// anchor_generators/multiscale_grid_anchor_generator.py for details.
+message MultiscaleAnchorGenerator {
+  // minimum level in feature pyramid
+  optional int32 min_level = 1 [default = 3];
+
+  // maximum level in feature pyramid
+  optional int32 max_level = 2 [default = 7];
+
+  // Scale of anchor to feature stride
+  optional float anchor_scale = 3 [default = 4.0];
+
+  // Aspect ratios for anchors at each grid point.
+  repeated float aspect_ratios = 4;
+
+  // Number of intermediate scale each scale octave
+}
diff --git a/research/object_detection/protos/pipeline.proto b/research/object_detection/protos/pipeline.proto
index 67f4e544..b1013027 100644
--- a/research/object_detection/protos/pipeline.proto
+++ b/research/object_detection/protos/pipeline.proto
@@ -15,4 +15,5 @@ message TrainEvalPipelineConfig {
   optional InputReader train_input_reader = 3;
   optional EvalConfig eval_config = 4;
   optional InputReader eval_input_reader = 5;
+  extensions 1000 to max;
 }
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index fcfb450a..9b3f25f1 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -3,7 +3,7 @@ syntax = "proto2";
 package object_detection.protos;
 
 // Message for defining a preprocessing operation on input data.
-// See: //object_detection/core/preprocessor.py
+// See: //third_party/tensorflow_models/object_detection/core/preprocessor.py
 message PreprocessingStep {
   oneof preprocessing_step {
     NormalizeImage normalize_image = 1;
@@ -202,7 +202,7 @@ message RandomCropPadImage {
   repeated float max_padded_size_ratio = 9;
 
   // Color of the padding. If unset, will pad using average color of the input
-  // image.
+  // image. This field should be of length 3.
   repeated float pad_color = 10;
 }
 
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 067c2fff..d88a4fd2 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -82,4 +82,8 @@ message SsdFeatureExtractor {
   // will apply only to the additional layers that are added and are outside the
   // canned arg_scope.
   optional bool batch_norm_trainable = 6 [default=true];
+
+  // Whether to use explicit padding when extracting SSD multiresolution
+  // features. Note that this does not apply to the base feature extractor.
+  optional bool use_explicit_padding = 7 [default=false];
 }
diff --git a/research/object_detection/protos/train.proto b/research/object_detection/protos/train.proto
index ae905c78..02f4cec3 100644
--- a/research/object_detection/protos/train.proto
+++ b/research/object_detection/protos/train.proto
@@ -35,6 +35,11 @@ message TrainConfig {
   // If false, it assumes the checkpoint was a object classification model.
   optional bool from_detection_checkpoint = 8 [default=false];
 
+  // Whether to load all checkpoint vars that match model variable names and
+  // sizes. This option is only available if `from_detection_checkpoint` is
+  // True.
+  optional bool load_all_detection_checkpoint_vars = 19 [default = false];
+
   // Number of steps to train the DetectionModel for. If 0, will train the model
   // indefinitely.
   optional uint32 num_steps = 9 [default=0];
@@ -66,4 +71,11 @@ message TrainConfig {
   // This is useful when each box can have multiple labels.
   // Note that only Sigmoid classification losses should be used.
   optional bool merge_multiple_label_boxes = 17 [default=false];
+
+  // Whether to add regularization loss to `total_loss`. This is true by
+  // default and adds all regularization losses defined in the model to
+  // `total_loss`.
+  // Setting this option to false is very useful while debugging the model and
+  // losses.
+  optional bool add_regularization_loss = 18 [default=true];
 }
diff --git a/research/object_detection/samples/configs/BUILD b/research/object_detection/samples/configs/BUILD
new file mode 100644
index 00000000..652a929f
--- /dev/null
+++ b/research/object_detection/samples/configs/BUILD
@@ -0,0 +1,10 @@
+package(
+    default_visibility = ["//visibility:public"],
+)
+
+licenses(["notice"])
+
+exports_files([
+    "faster_rcnn_resnet50_pets.config",
+    "ssd_inception_v2_pets.config",
+])
diff --git a/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config b/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config
new file mode 100644
index 00000000..1b66d0d5
--- /dev/null
+++ b/research/object_detection/samples/configs/embedded_ssd_mobilenet_v1_coco.config
@@ -0,0 +1,186 @@
+# Embedded SSD with Mobilenet v1 configuration for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    num_classes: 90
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 5
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 256
+        width: 256
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 1
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+          batch_norm {
+            train: true,
+            scale: true,
+            center: true,
+            decay: 0.9997,
+            epsilon: 0.001,
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'embedded_ssd_mobilenet_v1'
+      min_depth: 16
+      depth_multiplier: 0.125
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.03
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.001,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid {
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      hard_example_miner {
+        num_hard_examples: 3000
+        iou_threshold: 0.99
+        loss_type: CLASSIFICATION
+        max_negatives_per_positive: 3
+        min_negatives_per_image: 0
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 32
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "/PATH_TO_BE_CONFIGURED/model.ckpt"
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  use_moving_averages: true
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
index 8cee5b1a..063113af 100644
--- a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_coco.config
@@ -143,5 +143,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config
new file mode 100644
index 00000000..ef83c9c4
--- /dev/null
+++ b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_cosine_lr_coco.config
@@ -0,0 +1,132 @@
+# Faster R-CNN with Inception Resnet v2, Atrous version, with Cosine
+# Learning Rate schedule.
+# Trained on COCO, initialized from Imagenet classification checkpoint
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 600
+        max_dimension: 1024
+      }
+    }
+    feature_extractor {
+      type: 'faster_rcnn_inception_resnet_v2'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 17
+    maxpool_kernel_size: 1
+    maxpool_stride: 1
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        cosine_decay_learning_rate {
+          learning_rate_base: 0.0006
+          total_steps: 1200000
+          warmup_learning_rate: 0.00006
+          warmup_steps: 20000
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid.config b/research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config
similarity index 100%
rename from research/object_detection/faster_rcnn_inception_resnet_v2_atrous_oid.config
rename to research/object_detection/samples/configs/faster_rcnn_inception_resnet_v2_atrous_oid.config
diff --git a/research/object_detection/samples/configs/faster_rcnn_nas_coco.config b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
index a32cb033..fffaacf2 100644
--- a/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_nas_coco.config
@@ -8,7 +8,7 @@ model {
   faster_rcnn {
     num_classes: 90
     image_resizer {
-      # TODO: Only fixed_shape_resizer is currently supported for NASNet
+      # TODO(shlens): Only fixed_shape_resizer is currently supported for NASNet
       # featurization. The reason for this is that nasnet.py only supports
       # inputs with fully known shapes. We need to update nasnet.py to handle
       # shapes not known at compile time.
@@ -43,7 +43,7 @@ model {
     }
     first_stage_nms_score_threshold: 0.0
     first_stage_nms_iou_threshold: 0.7
-    first_stage_max_proposals: 50
+    first_stage_max_proposals: 500
     first_stage_localization_loss_weight: 2.0
     first_stage_objectness_loss_weight: 1.0
     initial_crop_size: 17
@@ -131,7 +131,6 @@ train_input_reader: {
 }
 
 eval_config: {
-  metrics_set: "pascal_voc_metrics"
   num_examples: 8000
   # Note: The below line limits the evaluation process to 10 evaluations.
   # Remove the below line to evaluate indefinitely.
@@ -144,5 +143,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config
new file mode 100644
index 00000000..806b9833
--- /dev/null
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_atrous_coco.config
@@ -0,0 +1,140 @@
+# Faster R-CNN with Resnet-101 (v1), Atrous version
+# Trained on COCO, initialized from Imagenet classification checkpoint
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 600
+        max_dimension: 1024
+      }
+    }
+    feature_extractor {
+      type: 'faster_rcnn_resnet101'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0003
+          schedule {
+            step: 0
+            learning_rate: .0003
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00003
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000003
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
index ed11bb94..e9e271b0 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet101_coco.config
@@ -1,8 +1,5 @@
-# Faster R-CNN with Resnet-101 (v1) configuration for MSCOCO Dataset.
-# Users should configure the fine_tune_checkpoint field in the train config as
-# well as the label_map_path and input_path fields in the train_input_reader and
-# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
-# should be configured.
+# Faster R-CNN with Resnet-101 (v1)
+# Trained on COCO, initialized from Imagenet classification checkpoint
 
 model {
   faster_rcnn {
@@ -107,13 +104,7 @@ train_config: {
     use_moving_average: false
   }
   gradient_clipping_by_norm: 10.0
-  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
-  from_detection_checkpoint: true
-  # Note: The below line limits the training process to 200K steps, which we
-  # empirically found to be sufficient enough to train the pets dataset. This
-  # effectively bypasses the learning rate schedule (the learning rate will
-  # never decay). Remove the below line to train indefinitely.
-  num_steps: 200000
+  fine_tune_checkpoint: "/namespace/vale-project/models/classification/imagenet/resnet_v1_101/msra_model.ckpt"
   data_augmentation_options {
     random_horizontal_flip {
     }
@@ -121,25 +112,32 @@ train_config: {
 }
 
 train_input_reader: {
-  tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  label_map_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/raw/mscoco_label_map.pbtxt"
+  external_input_reader {
+    [object_detection.protos.GoogleInputReader.google_input_reader] {
+      ss_table_input_reader: {
+        input_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/example_sstables/mscoco_alltasks_trainvalminusminival2014-?????-of-00101"
+        data_type: TF_EXAMPLE
+      }
+    }
   }
-  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
 }
 
 eval_config: {
-  num_examples: 8000
-  # Note: The below line limits the evaluation process to 10 evaluations.
-  # Remove the below line to evaluate indefinitely.
-  max_evals: 10
+  metrics_set: "coco_detection_metrics"
+  use_moving_averages: false
 }
 
 eval_input_reader: {
-  tf_record_input_reader {
-    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
-  }
-  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  label_map_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/raw/mscoco_label_map.pbtxt"
   shuffle: false
-  num_readers: 1
   num_epochs: 1
+  external_input_reader {
+    [object_detection.protos.GoogleInputReader.google_input_reader] {
+      ss_table_input_reader: {
+        input_path: "/placer/prod/home/vale-project-placer/datasets/mscoco/example_sstables/mscoco_alltasks_minival2014-?????-of-00020"
+        data_type: TF_EXAMPLE
+      }
+    }
+  }
 }
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
index d537b08f..456727b8 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet152_coco.config
@@ -141,5 +141,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config b/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
index e3257860..eaff8dc6 100644
--- a/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
+++ b/research/object_detection/samples/configs/faster_rcnn_resnet50_coco.config
@@ -141,5 +141,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config
new file mode 100644
index 00000000..40928eed
--- /dev/null
+++ b/research/object_detection/samples/configs/mask_rcnn_inception_resnet_v2_atrous_coco.config
@@ -0,0 +1,166 @@
+# Mask R-CNN with Inception Resnet v2, Atrous version
+# Configured for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 800
+        max_dimension: 1365
+      }
+    }
+    number_of_stages: 3
+    feature_extractor {
+      type: 'faster_rcnn_inception_resnet_v2'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 17
+    maxpool_kernel_size: 1
+    maxpool_stride: 1
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        predict_instance_masks: true
+        mask_height: 33
+        mask_width: 33
+        mask_prediction_conv_depth: 0
+        mask_prediction_num_conv_layers: 4
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+    second_stage_mask_prediction_loss_weight: 4.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0003
+          schedule {
+            step: 0
+            learning_rate: .0003
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00003
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000003
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config b/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config
new file mode 100644
index 00000000..06304daa
--- /dev/null
+++ b/research/object_detection/samples/configs/mask_rcnn_inception_v2_coco.config
@@ -0,0 +1,165 @@
+# Mask R-CNN with Inception V2
+# Configured for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 800
+        max_dimension: 1365
+      }
+    }
+    number_of_stages: 3
+    feature_extractor {
+      type: 'faster_rcnn_inception_v2'
+      first_stage_features_stride: 16
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 16
+        width_stride: 16
+      }
+    }
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        predict_instance_masks: true
+        mask_height: 15
+        mask_width: 15
+        mask_prediction_conv_depth: 0
+        mask_prediction_num_conv_layers: 2
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+    second_stage_mask_prediction_loss_weight: 4.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0002
+          schedule {
+            step: 0
+            learning_rate: .0002
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00002
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000002
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config
new file mode 100644
index 00000000..8203a3e6
--- /dev/null
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet101_atrous_coco.config
@@ -0,0 +1,166 @@
+# Mask R-CNN with Resnet-101 (v1), Atrous version
+# Configured for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 800
+        max_dimension: 1365
+      }
+    }
+    number_of_stages: 3
+    feature_extractor {
+      type: 'faster_rcnn_resnet101'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        predict_instance_masks: true
+        mask_height: 33
+        mask_width: 33
+        mask_prediction_conv_depth: 0
+        mask_prediction_num_conv_layers: 4
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+    second_stage_mask_prediction_loss_weight: 4.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0003
+          schedule {
+            step: 0
+            learning_rate: .0003
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00003
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000003
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
new file mode 100644
index 00000000..d1ae0e92
--- /dev/null
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet101_pets.config
@@ -0,0 +1,159 @@
+# Mask R-CNN with Resnet-101 (v1) configured for the Oxford-IIIT Pet Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 37
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 600
+        max_dimension: 1024
+      }
+    }
+    number_of_stages: 3
+    feature_extractor {
+      type: 'faster_rcnn_resnet101'
+      first_stage_features_stride: 16
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 16
+        width_stride: 16
+      }
+    }
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        predict_instance_masks: true
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0007
+          schedule {
+            step: 0
+            learning_rate: 0.0007
+          }
+          schedule {
+            step: 15000
+            learning_rate: 0.00007
+          }
+          schedule {
+            step: 30000
+            learning_rate: 0.000007
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 2000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config b/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config
new file mode 100644
index 00000000..ce20d547
--- /dev/null
+++ b/research/object_detection/samples/configs/mask_rcnn_resnet50_atrous_coco.config
@@ -0,0 +1,166 @@
+# Mask R-CNN with Resnet-50 (v1), Atrous version
+# Configured for MSCOCO Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  faster_rcnn {
+    num_classes: 90
+    image_resizer {
+      keep_aspect_ratio_resizer {
+        min_dimension: 800
+        max_dimension: 1365
+      }
+    }
+    number_of_stages: 3
+    feature_extractor {
+      type: 'faster_rcnn_resnet50'
+      first_stage_features_stride: 8
+    }
+    first_stage_anchor_generator {
+      grid_anchor_generator {
+        scales: [0.25, 0.5, 1.0, 2.0]
+        aspect_ratios: [0.5, 1.0, 2.0]
+        height_stride: 8
+        width_stride: 8
+      }
+    }
+    first_stage_atrous_rate: 2
+    first_stage_box_predictor_conv_hyperparams {
+      op: CONV
+      regularizer {
+        l2_regularizer {
+          weight: 0.0
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          stddev: 0.01
+        }
+      }
+    }
+    first_stage_nms_score_threshold: 0.0
+    first_stage_nms_iou_threshold: 0.7
+    first_stage_max_proposals: 300
+    first_stage_localization_loss_weight: 2.0
+    first_stage_objectness_loss_weight: 1.0
+    initial_crop_size: 14
+    maxpool_kernel_size: 2
+    maxpool_stride: 2
+    second_stage_box_predictor {
+      mask_rcnn_box_predictor {
+        use_dropout: false
+        dropout_keep_probability: 1.0
+        predict_instance_masks: true
+        mask_height: 33
+        mask_width: 33
+        mask_prediction_conv_depth: 0
+        mask_prediction_num_conv_layers: 4
+        fc_hyperparams {
+          op: FC
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            variance_scaling_initializer {
+              factor: 1.0
+              uniform: true
+              mode: FAN_AVG
+            }
+          }
+        }
+        conv_hyperparams {
+          op: CONV
+          regularizer {
+            l2_regularizer {
+              weight: 0.0
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.01
+            }
+          }
+        }
+      }
+    }
+    second_stage_post_processing {
+      batch_non_max_suppression {
+        score_threshold: 0.0
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 300
+      }
+      score_converter: SOFTMAX
+    }
+    second_stage_localization_loss_weight: 2.0
+    second_stage_classification_loss_weight: 1.0
+    second_stage_mask_prediction_loss_weight: 4.0
+  }
+}
+
+train_config: {
+  batch_size: 1
+  optimizer {
+    momentum_optimizer: {
+      learning_rate: {
+        manual_step_learning_rate {
+          initial_learning_rate: 0.0003
+          schedule {
+            step: 0
+            learning_rate: .0003
+          }
+          schedule {
+            step: 900000
+            learning_rate: .00003
+          }
+          schedule {
+            step: 1200000
+            learning_rate: .000003
+          }
+        }
+      }
+      momentum_optimizer_value: 0.9
+    }
+    use_moving_average: false
+  }
+  gradient_clipping_by_norm: 10.0
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 8000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/mscoco_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/rfcn_resnet101_coco.config b/research/object_detection/samples/configs/rfcn_resnet101_coco.config
index 6c383fa7..9ce9e930 100644
--- a/research/object_detection/samples/configs/rfcn_resnet101_coco.config
+++ b/research/object_detection/samples/configs/rfcn_resnet101_coco.config
@@ -138,5 +138,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/ssd_inception_v2_coco.config b/research/object_detection/samples/configs/ssd_inception_v2_coco.config
index 62e7e4f2..301a6269 100644
--- a/research/object_detection/samples/configs/ssd_inception_v2_coco.config
+++ b/research/object_detection/samples/configs/ssd_inception_v2_coco.config
@@ -102,12 +102,10 @@ model {
     loss {
       classification_loss {
         weighted_sigmoid {
-          anchorwise_output: true
         }
       }
       localization_loss {
         weighted_smooth_l1 {
-          anchorwise_output: true
         }
       }
       hard_example_miner {
@@ -187,5 +185,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/ssd_inception_v2_pets.config b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
index 41b15880..24a8540b 100644
--- a/research/object_detection/samples/configs/ssd_inception_v2_pets.config
+++ b/research/object_detection/samples/configs/ssd_inception_v2_pets.config
@@ -102,12 +102,10 @@ model {
     loss {
       classification_loss {
         weighted_sigmoid {
-          anchorwise_output: true
         }
       }
       localization_loss {
         weighted_smooth_l1 {
-          anchorwise_output: true
         }
       }
       hard_example_miner {
diff --git a/research/object_detection/samples/configs/ssd_inception_v3_pets.config b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
new file mode 100644
index 00000000..58051162
--- /dev/null
+++ b/research/object_detection/samples/configs/ssd_inception_v3_pets.config
@@ -0,0 +1,188 @@
+# SSD with Inception v2 configured for Oxford-IIIT Pets Dataset.
+# Users should configure the fine_tune_checkpoint field in the train config as
+# well as the label_map_path and input_path fields in the train_input_reader and
+# eval_input_reader. Search for "PATH_TO_BE_CONFIGURED" to find the fields that
+# should be configured.
+
+model {
+  ssd {
+    num_classes: 37
+    box_coder {
+      faster_rcnn_box_coder {
+        y_scale: 10.0
+        x_scale: 10.0
+        height_scale: 5.0
+        width_scale: 5.0
+      }
+    }
+    matcher {
+      argmax_matcher {
+        matched_threshold: 0.5
+        unmatched_threshold: 0.5
+        ignore_thresholds: false
+        negatives_lower_than_unmatched: true
+        force_match_for_each_row: true
+      }
+    }
+    similarity_calculator {
+      iou_similarity {
+      }
+    }
+    anchor_generator {
+      ssd_anchor_generator {
+        num_layers: 6
+        min_scale: 0.2
+        max_scale: 0.95
+        aspect_ratios: 1.0
+        aspect_ratios: 2.0
+        aspect_ratios: 0.5
+        aspect_ratios: 3.0
+        aspect_ratios: 0.3333
+        reduce_boxes_in_lowest_layer: true
+      }
+    }
+    image_resizer {
+      fixed_shape_resizer {
+        height: 300
+        width: 300
+      }
+    }
+    box_predictor {
+      convolutional_box_predictor {
+        min_depth: 0
+        max_depth: 0
+        num_layers_before_predictor: 0
+        use_dropout: false
+        dropout_keep_probability: 0.8
+        kernel_size: 3
+        box_code_size: 4
+        apply_sigmoid_to_scores: false
+        conv_hyperparams {
+          activation: RELU_6,
+          regularizer {
+            l2_regularizer {
+              weight: 0.00004
+            }
+          }
+          initializer {
+            truncated_normal_initializer {
+              stddev: 0.03
+              mean: 0.0
+            }
+          }
+        }
+      }
+    }
+    feature_extractor {
+      type: 'ssd_inception_v3'
+      min_depth: 16
+      depth_multiplier: 1.0
+      conv_hyperparams {
+        activation: RELU_6,
+        regularizer {
+          l2_regularizer {
+            weight: 0.00004
+          }
+        }
+        initializer {
+          truncated_normal_initializer {
+            stddev: 0.1
+            mean: 0.0
+          }
+        }
+        batch_norm {
+          train: true,
+          scale: true,
+          center: true,
+          decay: 0.9997,
+          epsilon: 0.01,
+        }
+      }
+    }
+    loss {
+      classification_loss {
+        weighted_sigmoid {
+        }
+      }
+      localization_loss {
+        weighted_smooth_l1 {
+        }
+      }
+      hard_example_miner {
+        num_hard_examples: 3000
+        iou_threshold: 0.99
+        loss_type: CLASSIFICATION
+        max_negatives_per_positive: 3
+        min_negatives_per_image: 0
+      }
+      classification_weight: 1.0
+      localization_weight: 1.0
+    }
+    normalize_loss_by_num_matches: true
+    post_processing {
+      batch_non_max_suppression {
+        score_threshold: 1e-8
+        iou_threshold: 0.6
+        max_detections_per_class: 100
+        max_total_detections: 100
+      }
+      score_converter: SIGMOID
+    }
+  }
+}
+
+train_config: {
+  batch_size: 24
+  optimizer {
+    rms_prop_optimizer: {
+      learning_rate: {
+        exponential_decay_learning_rate {
+          initial_learning_rate: 0.004
+          decay_steps: 800720
+          decay_factor: 0.95
+        }
+      }
+      momentum_optimizer_value: 0.9
+      decay: 0.9
+      epsilon: 1.0
+    }
+  }
+  fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
+  from_detection_checkpoint: true
+  # Note: The below line limits the training process to 200K steps, which we
+  # empirically found to be sufficient enough to train the pets dataset. This
+  # effectively bypasses the learning rate schedule (the learning rate will
+  # never decay). Remove the below line to train indefinitely.
+  num_steps: 200000
+  data_augmentation_options {
+    random_horizontal_flip {
+    }
+  }
+  data_augmentation_options {
+    ssd_random_crop {
+    }
+  }
+}
+
+train_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_train.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+}
+
+eval_config: {
+  num_examples: 2000
+  # Note: The below line limits the evaluation process to 10 evaluations.
+  # Remove the below line to evaluate indefinitely.
+  max_evals: 10
+}
+
+eval_input_reader: {
+  tf_record_input_reader {
+    input_path: "PATH_TO_BE_CONFIGURED/pet_val.record"
+  }
+  label_map_path: "PATH_TO_BE_CONFIGURED/pet_label_map.pbtxt"
+  shuffle: false
+  num_readers: 1
+}
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
index d46a5432..73da9861 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_coco.config
@@ -108,12 +108,10 @@ model {
     loss {
       classification_loss {
         weighted_sigmoid {
-          anchorwise_output: true
         }
       }
       localization_loss {
         weighted_smooth_l1 {
-          anchorwise_output: true
         }
       }
       hard_example_miner {
@@ -193,5 +191,4 @@ eval_input_reader: {
   label_map_path: "PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt"
   shuffle: false
   num_readers: 1
-  num_epochs: 1
 }
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
index a6741357..72a0b5d9 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_pets.config
@@ -108,12 +108,10 @@ model {
     loss {
       classification_loss {
         weighted_sigmoid {
-          anchorwise_output: true
         }
       }
       localization_loss {
         weighted_smooth_l1 {
-          anchorwise_output: true
         }
       }
       hard_example_miner {
diff --git a/research/object_detection/test_ckpt/ssd_inception_v2.pb b/research/object_detection/test_ckpt/ssd_inception_v2.pb
new file mode 100644
index 00000000..090d15ce
Binary files /dev/null and b/research/object_detection/test_ckpt/ssd_inception_v2.pb differ
diff --git a/research/object_detection/test_data/BUILD b/research/object_detection/test_data/BUILD
new file mode 100644
index 00000000..a198ca5e
--- /dev/null
+++ b/research/object_detection/test_data/BUILD
@@ -0,0 +1,9 @@
+package(
+    default_visibility = ["//visibility:public"],
+)
+
+licenses(["notice"])
+
+exports_files([
+    "pets_examples.record",
+])
diff --git a/research/object_detection/test_data/pets_examples.record b/research/object_detection/test_data/pets_examples.record
new file mode 100644
index 00000000..54099c7f
Binary files /dev/null and b/research/object_detection/test_data/pets_examples.record differ
diff --git a/research/object_detection/test_images/BUILD b/research/object_detection/test_images/BUILD
new file mode 100644
index 00000000..3ef7bd68
--- /dev/null
+++ b/research/object_detection/test_images/BUILD
@@ -0,0 +1,10 @@
+package(
+    default_visibility = ["//visibility:public"],
+)
+
+licenses(["notice"])
+
+exports_files([
+    "image1.jpg",
+    "image2.jpg",
+])
diff --git a/research/object_detection/train.py b/research/object_detection/train.py
index faab1acc..30568fcd 100644
--- a/research/object_detection/train.py
+++ b/research/object_detection/train.py
@@ -47,9 +47,10 @@ import os
 import tensorflow as tf
 
 from object_detection import trainer
-from object_detection.builders import input_reader_builder
+from object_detection.builders import dataset_builder
 from object_detection.builders import model_builder
 from object_detection.utils import config_util
+from object_detection.utils import dataset_util
 
 tf.logging.set_verbosity(tf.logging.INFO)
 
@@ -114,8 +115,13 @@ def main(_):
       model_config=model_config,
       is_training=True)
 
-  create_input_dict_fn = functools.partial(
-      input_reader_builder.build, input_config)
+  def get_next(config):
+    return dataset_util.make_initializable_iterator(
+        dataset_builder.build(
+            config, num_workers=FLAGS.worker_replicas,
+            worker_index=FLAGS.task)).get_next()
+
+  create_input_dict_fn = functools.partial(get_next, input_config)
 
   env = json.loads(os.environ.get('TF_CONFIG', '{}'))
   cluster_data = env.get('cluster', None)
diff --git a/research/object_detection/trainer.py b/research/object_detection/trainer.py
index ea91777b..227748df 100644
--- a/research/object_detection/trainer.py
+++ b/research/object_detection/trainer.py
@@ -108,6 +108,8 @@ def get_inputs(input_queue, num_classes, merge_multiple_label_boxes=False):
     keypoints_list: a list of 3-D float tensors of shape [num_boxes,
       num_keypoints, 2] containing keypoints for objects if present in the
       input queue. Else returns None.
+    weights_lists: a list of 1-D float32 tensors of shape [num_boxes]
+      containing groundtruth weight for each box.
   """
   read_data_list = input_queue.dequeue()
   label_id_offset = 1
@@ -132,7 +134,10 @@ def get_inputs(input_queue, num_classes, merge_multiple_label_boxes=False):
     if (merge_multiple_label_boxes and (
         masks_gt is not None or keypoints_gt is not None)):
       raise NotImplementedError('Multi-label support is only for boxes.')
-    return image, key, location_gt, classes_gt, masks_gt, keypoints_gt
+    weights_gt = read_data.get(
+        fields.InputDataFields.groundtruth_weights)
+    return (image, key, location_gt, classes_gt, masks_gt, keypoints_gt,
+            weights_gt)
 
   return zip(*map(extract_images_and_targets, read_data_list))
 
@@ -147,12 +152,21 @@ def _create_losses(input_queue, create_model_fn, train_config):
   """
   detection_model = create_model_fn()
   (images, _, groundtruth_boxes_list, groundtruth_classes_list,
-   groundtruth_masks_list, groundtruth_keypoints_list) = get_inputs(
+   groundtruth_masks_list, groundtruth_keypoints_list, _) = get_inputs(
        input_queue,
        detection_model.num_classes,
        train_config.merge_multiple_label_boxes)
-  images = [detection_model.preprocess(image) for image in images]
-  images = tf.concat(images, 0)
+
+  preprocessed_images = []
+  true_image_shapes = []
+  for image in images:
+    resized_image, true_image_shape = detection_model.preprocess(image)
+    preprocessed_images.append(resized_image)
+    true_image_shapes.append(true_image_shape)
+
+  images = tf.concat(preprocessed_images, 0)
+  true_image_shapes = tf.concat(true_image_shapes, 0)
+
   if any(mask is None for mask in groundtruth_masks_list):
     groundtruth_masks_list = None
   if any(keypoints is None for keypoints in groundtruth_keypoints_list):
@@ -162,16 +176,16 @@ def _create_losses(input_queue, create_model_fn, train_config):
                                       groundtruth_classes_list,
                                       groundtruth_masks_list,
                                       groundtruth_keypoints_list)
-  prediction_dict = detection_model.predict(images)
+  prediction_dict = detection_model.predict(images, true_image_shapes)
 
-  losses_dict = detection_model.loss(prediction_dict)
+  losses_dict = detection_model.loss(prediction_dict, true_image_shapes)
   for loss_tensor in losses_dict.values():
     tf.losses.add_loss(loss_tensor)
 
 
 def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
           num_clones, worker_replicas, clone_on_cpu, ps_tasks, worker_job_name,
-          is_chief, train_dir):
+          is_chief, train_dir, graph_hook_fn=None):
   """Training function for detection models.
 
   Args:
@@ -188,6 +202,10 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
     worker_job_name: Name of the worker job.
     is_chief: Whether this replica is the chief replica.
     train_dir: Directory to write checkpoints and training summaries to.
+    graph_hook_fn: Optional function that is called after the training graph is
+      completely built. This is helpful to perform additional changes to the
+      training graph such as optimizing batchnorm. The function should modify
+      the default graph.
   """
 
   detection_model = create_model_fn()
@@ -217,7 +235,7 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
           train_config.prefetch_queue_capacity, data_augmentation_options)
 
     # Gather initial summaries.
-    # TODO(rathodv): See if summaries can be added/extracted from global tf
+    # TODO: See if summaries can be added/extracted from global tf
     # collections so that they don't have to be passed around.
     summaries = set(tf.get_collection(tf.GraphKeys.SUMMARIES))
     global_summaries = set([])
@@ -258,8 +276,11 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
       init_fn = initializer_fn
 
     with tf.device(deploy_config.optimizer_device()):
+      regularization_losses = (None if train_config.add_regularization_loss
+                               else [])
       total_loss, grads_and_vars = model_deploy.optimize_clones(
-          clones, training_optimizer, regularization_losses=None)
+          clones, training_optimizer,
+          regularization_losses=regularization_losses)
       total_loss = tf.check_numerics(total_loss, 'LossTensor is inf or nan.')
 
       # Optionally multiply bias gradients by train_config.bias_grad_multiplier.
@@ -285,11 +306,14 @@ def train(create_tensor_dict_fn, create_model_fn, train_config, master, task,
       grad_updates = training_optimizer.apply_gradients(grads_and_vars,
                                                         global_step=global_step)
       update_ops.append(grad_updates)
-
-      update_op = tf.group(*update_ops)
+      update_op = tf.group(*update_ops, name='update_barrier')
       with tf.control_dependencies([update_op]):
         train_tensor = tf.identity(total_loss, name='train_op')
 
+    if graph_hook_fn:
+      with tf.device(deploy_config.variables_device()):
+        graph_hook_fn()
+
     # Add summaries.
     for model_var in slim.get_model_variables():
       global_summaries.add(tf.summary.histogram(model_var.op.name, model_var))
diff --git a/research/object_detection/trainer_test.py b/research/object_detection/trainer_test.py
index caa8c1eb..044e94ef 100644
--- a/research/object_detection/trainer_test.py
+++ b/research/object_detection/trainer_test.py
@@ -51,10 +51,8 @@ class FakeDetectionModel(model.DetectionModel):
 
   def __init__(self):
     super(FakeDetectionModel, self).__init__(num_classes=NUMBER_OF_CLASSES)
-    self._classification_loss = losses.WeightedSigmoidClassificationLoss(
-        anchorwise_output=True)
-    self._localization_loss = losses.WeightedSmoothL1LocalizationLoss(
-        anchorwise_output=True)
+    self._classification_loss = losses.WeightedSigmoidClassificationLoss()
+    self._localization_loss = losses.WeightedSmoothL1LocalizationLoss()
 
   def preprocess(self, inputs):
     """Input preprocessing, resizes images to 28x28.
@@ -65,14 +63,24 @@ class FakeDetectionModel(model.DetectionModel):
 
     Returns:
       preprocessed_inputs: a [batch, 28, 28, channels] float32 tensor.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
     """
-    return tf.image.resize_images(inputs, [28, 28])
+    true_image_shapes = [inputs.shape[:-1].as_list()
+                         for _ in range(inputs.shape[-1])]
+    return tf.image.resize_images(inputs, [28, 28]), true_image_shapes
 
-  def predict(self, preprocessed_inputs):
+  def predict(self, preprocessed_inputs, true_image_shapes):
     """Prediction tensors from inputs tensor.
 
     Args:
       preprocessed_inputs: a [batch, 28, 28, channels] float32 tensor.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       prediction_dict: a dictionary holding prediction tensors to be
@@ -89,11 +97,15 @@ class FakeDetectionModel(model.DetectionModel):
         'box_encodings': tf.reshape(box_prediction, [-1, 1, 4])
     }
 
-  def postprocess(self, prediction_dict, **params):
+  def postprocess(self, prediction_dict, true_image_shapes, **params):
     """Convert predicted output tensors to final detections. Unused.
 
     Args:
       prediction_dict: a dictionary holding prediction tensors.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
       **params: Additional keyword arguments for specific implementations of
         DetectionModel.
 
@@ -107,7 +119,7 @@ class FakeDetectionModel(model.DetectionModel):
         'num_detections': None
     }
 
-  def loss(self, prediction_dict):
+  def loss(self, prediction_dict, true_image_shapes):
     """Compute scalar loss tensors with respect to provided groundtruth.
 
     Calling this function requires that groundtruth tensors have been
@@ -115,6 +127,10 @@ class FakeDetectionModel(model.DetectionModel):
 
     Args:
       prediction_dict: a dictionary holding predicted tensors
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
 
     Returns:
       a dictionary mapping strings (loss names) to scalar tensors representing
diff --git a/research/object_detection/utils/BUILD b/research/object_detection/utils/BUILD
index 7e511c95..332a9a86 100644
--- a/research/object_detection/utils/BUILD
+++ b/research/object_detection/utils/BUILD
@@ -8,6 +8,12 @@ licenses(["notice"])
 
 # Apache 2.0
 
+py_library(
+    name = "test_case",
+    srcs = ["test_case.py"],
+    deps = ["//tensorflow"],
+)
+
 py_library(
     name = "category_util",
     srcs = ["category_util.py"],
@@ -18,12 +24,13 @@ py_library(
     name = "config_util",
     srcs = ["config_util.py"],
     deps = [
+        "//pyglib/logging",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:eval_py_pb2",
-        "//tensorflow_models/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow_models/object_detection/protos:model_py_pb2",
-        "//tensorflow_models/object_detection/protos:pipeline_py_pb2",
-        "//tensorflow_models/object_detection/protos:train_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:eval_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
     ],
 )
 
@@ -35,13 +42,28 @@ py_library(
     ],
 )
 
+py_library(
+    name = "json_utils",
+    srcs = ["json_utils.py"],
+    deps = [],
+)
+
+py_test(
+    name = "json_utils_test",
+    srcs = ["json_utils_test.py"],
+    deps = [
+        ":json_utils",
+        "//tensorflow",
+    ],
+)
+
 py_library(
     name = "label_map_util",
     srcs = ["label_map_util.py"],
     deps = [
-        "//third_party/py/google/protobuf",
+        "//google/protobuf",
         "//tensorflow",
-        "//tensorflow_models/object_detection/protos:string_int_label_map_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:string_int_label_map_py_pb2",
     ],
 )
 
@@ -56,13 +78,22 @@ py_library(
 py_library(
     name = "metrics",
     srcs = ["metrics.py"],
-    deps = ["//third_party/py/numpy"],
+    deps = ["//numpy"],
 )
 
 py_library(
     name = "np_box_list",
     srcs = ["np_box_list.py"],
-    deps = ["//tensorflow"],
+    deps = ["//numpy"],
+)
+
+py_library(
+    name = "np_box_mask_list",
+    srcs = ["np_box_mask_list.py"],
+    deps = [
+        ":np_box_list",
+        "//numpy",
+    ],
 )
 
 py_library(
@@ -71,7 +102,18 @@ py_library(
     deps = [
         ":np_box_list",
         ":np_box_ops",
-        "//tensorflow",
+        "//numpy",
+    ],
+)
+
+py_library(
+    name = "np_box_mask_list_ops",
+    srcs = ["np_box_mask_list_ops.py"],
+    deps = [
+        ":np_box_list_ops",
+        ":np_box_mask_list",
+        ":np_mask_ops",
+        "//numpy",
     ],
 )
 
@@ -81,6 +123,12 @@ py_library(
     deps = ["//tensorflow"],
 )
 
+py_library(
+    name = "np_mask_ops",
+    srcs = ["np_mask_ops.py"],
+    deps = ["//numpy"],
+)
+
 py_library(
     name = "object_detection_evaluation",
     srcs = ["object_detection_evaluation.py"],
@@ -89,7 +137,7 @@ py_library(
         ":metrics",
         ":per_image_evaluation",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -97,11 +145,12 @@ py_library(
     name = "ops",
     srcs = ["ops.py"],
     deps = [
+        ":shape_utils",
         ":static_shape",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:box_list_ops",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_list_ops",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -111,6 +160,8 @@ py_library(
     deps = [
         ":np_box_list",
         ":np_box_list_ops",
+        ":np_box_mask_list",
+        ":np_box_mask_list_ops",
         "//tensorflow",
     ],
 )
@@ -118,7 +169,10 @@ py_library(
 py_library(
     name = "shape_utils",
     srcs = ["shape_utils.py"],
-    deps = ["//tensorflow"],
+    deps = [
+        ":static_shape",
+        "//tensorflow",
+    ],
 )
 
 py_library(
@@ -132,12 +186,12 @@ py_library(
     srcs = ["test_utils.py"],
     deps = [
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:anchor_generator",
-        "//tensorflow_models/object_detection/core:box_coder",
-        "//tensorflow_models/object_detection/core:box_list",
-        "//tensorflow_models/object_detection/core:box_predictor",
-        "//tensorflow_models/object_detection/core:matcher",
-        "//tensorflow_models/object_detection/utils:shape_utils",
+        "//tensorflow/models/research/object_detection/core:anchor_generator",
+        "//tensorflow/models/research/object_detection/core:box_coder",
+        "//tensorflow/models/research/object_detection/core:box_list",
+        "//tensorflow/models/research/object_detection/core:box_predictor",
+        "//tensorflow/models/research/object_detection/core:matcher",
+        "//tensorflow/models/research/object_detection/utils:shape_utils",
     ],
 )
 
@@ -153,10 +207,12 @@ py_library(
     name = "visualization_utils",
     srcs = ["visualization_utils.py"],
     deps = [
-        "//third_party/py/PIL:pil",
-        "//third_party/py/matplotlib",
-        "//third_party/py/six",
+        "//PIL:pil",
+        "//Tkinter",  # buildcleaner: keep
+        "//matplotlib",
+        "//six",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -174,11 +230,11 @@ py_test(
     srcs = ["config_util_test.py"],
     deps = [
         ":config_util",
-        "//tensorflow:tensorflow_google",
-        "//tensorflow_models/object_detection/protos:input_reader_py_pb2",
-        "//tensorflow_models/object_detection/protos:model_py_pb2",
-        "//tensorflow_models/object_detection/protos:pipeline_py_pb2",
-        "//tensorflow_models/object_detection/protos:train_py_pb2",
+        "//tensorflow",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:model_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:pipeline_py_pb2",
+        "//tensorflow/models/research/object_detection/protos:train_py_pb2",
     ],
 )
 
@@ -188,6 +244,7 @@ py_test(
     deps = [
         ":dataset_util",
         "//tensorflow",
+        "//tensorflow/models/research/object_detection/protos:input_reader_py_pb2",
     ],
 )
 
@@ -223,6 +280,17 @@ py_test(
     srcs = ["np_box_list_test.py"],
     deps = [
         ":np_box_list",
+        "//numpy",
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "np_box_mask_list_test",
+    srcs = ["np_box_mask_list_test.py"],
+    deps = [
+        ":np_box_mask_list",
+        "//numpy",
         "//tensorflow",
     ],
 )
@@ -233,6 +301,18 @@ py_test(
     deps = [
         ":np_box_list",
         ":np_box_list_ops",
+        "//numpy",
+        "//tensorflow",
+    ],
+)
+
+py_test(
+    name = "np_box_mask_list_ops_test",
+    srcs = ["np_box_mask_list_ops_test.py"],
+    deps = [
+        ":np_box_mask_list",
+        ":np_box_mask_list_ops",
+        "//numpy",
         "//tensorflow",
     ],
 )
@@ -246,13 +326,22 @@ py_test(
     ],
 )
 
+py_test(
+    name = "np_mask_ops_test",
+    srcs = ["np_mask_ops_test.py"],
+    deps = [
+        ":np_mask_ops",
+        "//tensorflow",
+    ],
+)
+
 py_test(
     name = "object_detection_evaluation_test",
     srcs = ["object_detection_evaluation_test.py"],
     deps = [
         ":object_detection_evaluation",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -261,8 +350,9 @@ py_test(
     srcs = ["ops_test.py"],
     deps = [
         ":ops",
+        ":test_case",
         "//tensorflow",
-        "//tensorflow_models/object_detection/core:standard_fields",
+        "//tensorflow/models/research/object_detection/core:standard_fields",
     ],
 )
 
@@ -280,6 +370,7 @@ py_test(
     srcs = ["shape_utils_test.py"],
     deps = [
         ":shape_utils",
+        "//numpy",
         "//tensorflow",
     ],
 )
@@ -315,10 +406,11 @@ py_test(
     name = "visualization_utils_test",
     srcs = ["visualization_utils_test.py"],
     data = [
-        "//tensorflow_models/object_detection/test_images:image1.jpg",
+        "//tensorflow/models/research/object_detection/test_images:image1.jpg",
     ],
     deps = [
         ":visualization_utils",
-        "//third_party/py/PIL:pil",
+        "//pyglib/flags",
+        "//PIL:pil",
     ],
 )
diff --git a/research/object_detection/utils/config_util.py b/research/object_detection/utils/config_util.py
index 1bf30089..3470d212 100644
--- a/research/object_detection/utils/config_util.py
+++ b/research/object_detection/utils/config_util.py
@@ -228,6 +228,9 @@ def merge_external_params_with_configs(configs, hparams=None, **kwargs):
       if value:
         _update_label_map_path(configs, value)
         tf.logging.info("Overwriting label map path: %s", value)
+    if key == "mask_type":
+      _update_mask_type(configs, value)
+      tf.logging.info("Overwritten mask type: %s", value)
   return configs
 
 
@@ -450,3 +453,18 @@ def _update_label_map_path(configs, label_map_path):
   """
   configs["train_input_config"].label_map_path = label_map_path
   configs["eval_input_config"].label_map_path = label_map_path
+
+
+def _update_mask_type(configs, mask_type):
+  """Updates the mask type for both train and eval input readers.
+
+  The configs dictionary is updated in place, and hence not returned.
+
+  Args:
+    configs: Dictionary of configuration objects. See outputs from
+      get_configs_from_pipeline_file() or get_configs_from_multiple_files().
+    mask_type: A string name representing a value of
+      input_reader_pb2.InstanceMaskType
+  """
+  configs["train_input_config"].mask_type = mask_type
+  configs["eval_input_config"].mask_type = mask_type
diff --git a/research/object_detection/utils/config_util_test.py b/research/object_detection/utils/config_util_test.py
index 075509e8..e93e8281 100644
--- a/research/object_detection/utils/config_util_test.py
+++ b/research/object_detection/utils/config_util_test.py
@@ -16,8 +16,7 @@
 
 import os
 
-import google3
-import tensorflow.google as tf
+import tensorflow as tf
 
 from google.protobuf import text_format
 
@@ -154,7 +153,7 @@ class ConfigUtilTest(tf.test.TestCase):
     """Asserts successful updating of all learning rate schemes."""
     original_learning_rate = 0.7
     learning_rate_scaling = 0.1
-    hparams = tf.HParams(learning_rate=0.15)
+    hparams = tf.contrib.training.HParams(learning_rate=0.15)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     # Constant learning rate.
@@ -216,7 +215,7 @@ class ConfigUtilTest(tf.test.TestCase):
   def testNewBatchSize(self):
     """Tests that batch size is updated appropriately."""
     original_batch_size = 2
-    hparams = tf.HParams(batch_size=16)
+    hparams = tf.contrib.training.HParams(batch_size=16)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -231,7 +230,7 @@ class ConfigUtilTest(tf.test.TestCase):
   def testNewBatchSizeWithClipping(self):
     """Tests that batch size is clipped to 1 from below."""
     original_batch_size = 2
-    hparams = tf.HParams(batch_size=0.5)
+    hparams = tf.contrib.training.HParams(batch_size=0.5)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -246,7 +245,7 @@ class ConfigUtilTest(tf.test.TestCase):
   def testNewMomentumOptimizerValue(self):
     """Tests that new momentum value is updated appropriately."""
     original_momentum_value = 0.4
-    hparams = tf.HParams(momentum_optimizer_value=1.1)
+    hparams = tf.contrib.training.HParams(momentum_optimizer_value=1.1)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -265,7 +264,7 @@ class ConfigUtilTest(tf.test.TestCase):
     original_localization_weight = 0.1
     original_classification_weight = 0.2
     new_weight_ratio = 5.0
-    hparams = tf.HParams(
+    hparams = tf.contrib.training.HParams(
         classification_localization_weight_ratio=new_weight_ratio)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
@@ -288,7 +287,8 @@ class ConfigUtilTest(tf.test.TestCase):
     original_gamma = 1.0
     new_alpha = 0.3
     new_gamma = 2.0
-    hparams = tf.HParams(focal_loss_alpha=new_alpha, focal_loss_gamma=new_gamma)
+    hparams = tf.contrib.training.HParams(
+        focal_loss_alpha=new_alpha, focal_loss_gamma=new_gamma)
     pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
 
     pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
@@ -396,6 +396,25 @@ class ConfigUtilTest(tf.test.TestCase):
     self.assertEqual(new_label_map_path,
                      configs["eval_input_config"].label_map_path)
 
+  def testNewMaskType(self):
+    """Tests that mask type can be overwritten in input readers."""
+    original_mask_type = input_reader_pb2.NUMERICAL_MASKS
+    new_mask_type = input_reader_pb2.PNG_MASKS
+    pipeline_config_path = os.path.join(self.get_temp_dir(), "pipeline.config")
+
+    pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+    train_input_reader = pipeline_config.train_input_reader
+    train_input_reader.mask_type = original_mask_type
+    eval_input_reader = pipeline_config.eval_input_reader
+    eval_input_reader.mask_type = original_mask_type
+    _write_config(pipeline_config, pipeline_config_path)
+
+    configs = config_util.get_configs_from_pipeline_file(pipeline_config_path)
+    configs = config_util.merge_external_params_with_configs(
+        configs, mask_type=new_mask_type)
+    self.assertEqual(new_mask_type, configs["train_input_config"].mask_type)
+    self.assertEqual(new_mask_type, configs["eval_input_config"].mask_type)
+
 
 if __name__ == "__main__":
   tf.test.main()
diff --git a/research/object_detection/utils/dataset_util.py b/research/object_detection/utils/dataset_util.py
index 014a9118..d9c52867 100644
--- a/research/object_detection/utils/dataset_util.py
+++ b/research/object_detection/utils/dataset_util.py
@@ -84,3 +84,64 @@ def recursive_parse_xml_to_dict(xml):
         result[child.tag] = []
       result[child.tag].append(child_result[child.tag])
   return {xml.tag: result}
+
+
+def make_initializable_iterator(dataset):
+  """Creates an iterator, and initializes tables.
+
+  This is useful in cases where make_one_shot_iterator wouldn't work because
+  the graph contains a hash table that needs to be initialized.
+
+  Args:
+    dataset: A `tf.data.Dataset` object.
+
+  Returns:
+    A `tf.data.Iterator`.
+  """
+  iterator = dataset.make_initializable_iterator()
+  tf.add_to_collection(tf.GraphKeys.TABLE_INITIALIZERS, iterator.initializer)
+  return iterator
+
+
+def read_dataset(
+    file_read_func, decode_func, input_files, config, num_workers=1,
+    worker_index=0):
+  """Reads a dataset, and handles repetition and shuffling.
+
+  Args:
+    file_read_func: Function to use in tf.data.Dataset.interleave, to read
+      every individual file into a tf.data.Dataset.
+    decode_func: Function to apply to all records.
+    input_files: A list of file paths to read.
+    config: A input_reader_builder.InputReader object.
+    num_workers: Number of workers / shards.
+    worker_index: Id for the current worker.
+
+  Returns:
+    A tf.data.Dataset based on config.
+  """
+  # Shard, shuffle, and read files.
+  filenames = tf.concat([tf.matching_files(pattern) for pattern in input_files],
+                        0)
+  dataset = tf.data.Dataset.from_tensor_slices(filenames)
+  dataset = dataset.shard(num_workers, worker_index)
+  dataset = dataset.repeat(config.num_epochs or None)
+  if config.shuffle:
+    dataset = dataset.shuffle(config.filenames_shuffle_buffer_size,
+                              reshuffle_each_iteration=True)
+
+  # Read file records and shuffle them.
+  # If cycle_length is larger than the number of files, more than one reader
+  # will be assigned to the same file, leading to repetition.
+  cycle_length = tf.cast(
+      tf.minimum(config.num_readers, tf.size(filenames)), tf.int64)
+  # TODO: find the optimal block_length.
+  dataset = dataset.interleave(
+      file_read_func, cycle_length=cycle_length, block_length=1)
+
+  if config.shuffle:
+    dataset = dataset.shuffle(config.shuffle_buffer_size,
+                              reshuffle_each_iteration=True)
+
+  dataset = dataset.map(decode_func, num_parallel_calls=config.num_readers)
+  return dataset.prefetch(config.prefetch_buffer_size)
diff --git a/research/object_detection/utils/dataset_util_test.py b/research/object_detection/utils/dataset_util_test.py
index 99cfb2cd..b0cd7a03 100644
--- a/research/object_detection/utils/dataset_util_test.py
+++ b/research/object_detection/utils/dataset_util_test.py
@@ -18,11 +18,29 @@
 import os
 import tensorflow as tf
 
+from object_detection.protos import input_reader_pb2
 from object_detection.utils import dataset_util
 
 
 class DatasetUtilTest(tf.test.TestCase):
 
+  def setUp(self):
+    self._path_template = os.path.join(self.get_temp_dir(), 'examples_%s.txt')
+
+    for i in range(5):
+      path = self._path_template % i
+      with tf.gfile.Open(path, 'wb') as f:
+        f.write('\n'.join([str(i + 1), str((i + 1) * 10)]))
+
+  def _get_dataset_next(self, files, config, batch_size):
+    def decode_func(value):
+      return [tf.string_to_number(value, out_type=tf.int32)]
+
+    dataset = dataset_util.read_dataset(
+        tf.data.TextLineDataset, decode_func, files, config)
+    dataset = dataset.batch(batch_size)
+    return dataset.make_one_shot_iterator().get_next()
+
   def test_read_examples_list(self):
     example_list_data = """example1 1\nexample2 2"""
     example_list_path = os.path.join(self.get_temp_dir(), 'examples.txt')
@@ -32,6 +50,47 @@ class DatasetUtilTest(tf.test.TestCase):
     examples = dataset_util.read_examples_list(example_list_path)
     self.assertListEqual(['example1', 'example2'], examples)
 
+  def test_make_initializable_iterator_with_hashTable(self):
+    keys = [1, 0, -1]
+    dataset = tf.data.Dataset.from_tensor_slices([[1, 2, -1, 5]])
+    table = tf.contrib.lookup.HashTable(
+        initializer=tf.contrib.lookup.KeyValueTensorInitializer(
+            keys=keys,
+            values=list(reversed(keys))),
+        default_value=100)
+    dataset = dataset.map(table.lookup)
+    data = dataset_util.make_initializable_iterator(dataset).get_next()
+    init = tf.tables_initializer()
+
+    with self.test_session() as sess:
+      sess.run(init)
+      self.assertAllEqual(sess.run(data), [-1, 100, 1, 100])
+
+  def test_read_dataset(self):
+    config = input_reader_pb2.InputReader()
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '*'], config,
+                                  batch_size=20)
+    with self.test_session() as sess:
+      self.assertAllEqual(sess.run(data),
+                          [[1, 10, 2, 20, 3, 30, 4, 40, 5, 50, 1, 10, 2, 20, 3,
+                            30, 4, 40, 5, 50]])
+
+  def test_read_dataset_single_epoch(self):
+    config = input_reader_pb2.InputReader()
+    config.num_epochs = 1
+    config.num_readers = 1
+    config.shuffle = False
+
+    data = self._get_dataset_next([self._path_template % '0'], config,
+                                  batch_size=30)
+    with self.test_session() as sess:
+      # First batch will retrieve as much as it can, second batch will fail.
+      self.assertAllEqual(sess.run(data), [[1, 10]])
+      self.assertRaises(tf.errors.OutOfRangeError, sess.run, data)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/json_utils.py b/research/object_detection/utils/json_utils.py
new file mode 100644
index 00000000..7507e078
--- /dev/null
+++ b/research/object_detection/utils/json_utils.py
@@ -0,0 +1,73 @@
+"""Utilities for dealing with writing json strings.
+
+json_utils wraps json.dump and json.dumps so that they can be used to safely
+control the precision of floats when writing to json strings or files.
+"""
+import json
+from json import encoder
+
+
+def Dump(obj, fid, float_digits=-1, **params):
+  """Wrapper of json.dump that allows specifying the float precision used.
+
+  Args:
+    obj: The object to dump.
+    fid: The file id to write to.
+    float_digits: The number of digits of precision when writing floats out.
+    **params: Additional parameters to pass to json.dumps.
+  """
+  original_encoder = encoder.FLOAT_REPR
+  if float_digits >= 0:
+    encoder.FLOAT_REPR = lambda o: format(o, '.%df' % float_digits)
+  try:
+    json.dump(obj, fid, **params)
+  finally:
+    encoder.FLOAT_REPR = original_encoder
+
+
+def Dumps(obj, float_digits=-1, **params):
+  """Wrapper of json.dumps that allows specifying the float precision used.
+
+  Args:
+    obj: The object to dump.
+    float_digits: The number of digits of precision when writing floats out.
+    **params: Additional parameters to pass to json.dumps.
+
+  Returns:
+    output: JSON string representation of obj.
+  """
+  original_encoder = encoder.FLOAT_REPR
+  original_c_make_encoder = encoder.c_make_encoder
+  if float_digits >= 0:
+    encoder.FLOAT_REPR = lambda o: format(o, '.%df' % float_digits)
+    encoder.c_make_encoder = None
+  try:
+    output = json.dumps(obj, **params)
+  finally:
+    encoder.FLOAT_REPR = original_encoder
+    encoder.c_make_encoder = original_c_make_encoder
+
+  return output
+
+
+def PrettyParams(**params):
+  """Returns parameters for use with Dump and Dumps to output pretty json.
+
+  Example usage:
+    ```json_str = json_utils.Dumps(obj, **json_utils.PrettyParams())```
+    ```json_str = json_utils.Dumps(
+                      obj, **json_utils.PrettyParams(allow_nans=False))```
+
+  Args:
+    **params: Additional params to pass to json.dump or json.dumps.
+
+  Returns:
+    params: Parameters that are compatible with json_utils.Dump and
+      json_utils.Dumps.
+  """
+  params['float_digits'] = 4
+  params['sort_keys'] = True
+  params['indent'] = 2
+  params['separators'] = (',', ': ')
+  return params
+
diff --git a/research/object_detection/utils/json_utils_test.py b/research/object_detection/utils/json_utils_test.py
new file mode 100644
index 00000000..9499a76c
--- /dev/null
+++ b/research/object_detection/utils/json_utils_test.py
@@ -0,0 +1,83 @@
+"""Tests for google3.image.understanding.object_detection.utils.json_utils."""
+import os
+
+import tensorflow as tf
+
+from object_detection.utils import json_utils
+
+
+class JsonUtilsTest(tf.test.TestCase):
+
+  def testDumpReasonablePrecision(self):
+    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
+    with tf.gfile.GFile(output_path, 'w') as f:
+      json_utils.Dump(1.0, f, float_digits=2)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      self.assertEqual(f.read(), '1.00')
+
+  def testDumpPassExtraParams(self):
+    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
+    with tf.gfile.GFile(output_path, 'w') as f:
+      json_utils.Dump([1.0], f, float_digits=2, indent=3)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      self.assertEqual(f.read(), '[\n   1.00\n]')
+
+  def testDumpZeroPrecision(self):
+    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
+    with tf.gfile.GFile(output_path, 'w') as f:
+      json_utils.Dump(1.0, f, float_digits=0, indent=3)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      self.assertEqual(f.read(), '1')
+
+  def testDumpUnspecifiedPrecision(self):
+    output_path = os.path.join(tf.test.get_temp_dir(), 'test.json')
+    with tf.gfile.GFile(output_path, 'w') as f:
+      json_utils.Dump(1.012345, f)
+    with tf.gfile.GFile(output_path, 'r') as f:
+      self.assertEqual(f.read(), '1.012345')
+
+  def testDumpsReasonablePrecision(self):
+    s = json_utils.Dumps(1.0, float_digits=2)
+    self.assertEqual(s, '1.00')
+
+  def testDumpsPassExtraParams(self):
+    s = json_utils.Dumps([1.0], float_digits=2, indent=3)
+    self.assertEqual(s, '[\n   1.00\n]')
+
+  def testDumpsZeroPrecision(self):
+    s = json_utils.Dumps(1.0, float_digits=0)
+    self.assertEqual(s, '1')
+
+  def testDumpsUnspecifiedPrecision(self):
+    s = json_utils.Dumps(1.012345)
+    self.assertEqual(s, '1.012345')
+
+  def testPrettyParams(self):
+    s = json_utils.Dumps({'v': 1.012345, 'n': 2}, **json_utils.PrettyParams())
+    self.assertEqual(s, '{\n  "n": 2,\n  "v": 1.0123\n}')
+
+  def testPrettyParamsExtraParamsInside(self):
+    s = json_utils.Dumps(
+        {'v': 1.012345,
+         'n': float('nan')}, **json_utils.PrettyParams(allow_nan=True))
+    self.assertEqual(s, '{\n  "n": NaN,\n  "v": 1.0123\n}')
+
+    with self.assertRaises(ValueError):
+      s = json_utils.Dumps(
+          {'v': 1.012345,
+           'n': float('nan')}, **json_utils.PrettyParams(allow_nan=False))
+
+  def testPrettyParamsExtraParamsOutside(self):
+    s = json_utils.Dumps(
+        {'v': 1.012345,
+         'n': float('nan')}, allow_nan=True, **json_utils.PrettyParams())
+    self.assertEqual(s, '{\n  "n": NaN,\n  "v": 1.0123\n}')
+
+    with self.assertRaises(ValueError):
+      s = json_utils.Dumps(
+          {'v': 1.012345,
+           'n': float('nan')}, allow_nan=False, **json_utils.PrettyParams())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index bf7bae63..c602178e 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -55,6 +55,18 @@ def create_category_index(categories):
   return category_index
 
 
+def get_max_label_map_index(label_map):
+  """Get maximum index in label map.
+
+  Args:
+    label_map: a StringIntLabelMapProto
+
+  Returns:
+    an integer
+  """
+  return max([item.id for item in label_map.item])
+
+
 def convert_label_map_to_categories(label_map,
                                     max_num_classes,
                                     use_display_name=True):
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index 8671754c..ad66b60c 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -170,6 +170,12 @@ class LabelMapUtilTest(tf.test.TestCase):
     }]
     self.assertListEqual(expected_categories_list, cat_no_offset)
 
+  def test_get_max_label_map_index(self):
+    num_classes = 4
+    label_map_proto = self._generate_label_map(num_classes=num_classes)
+    max_index = label_map_util.get_max_label_map_index(label_map_proto)
+    self.assertEqual(num_classes, max_index)
+
   def test_create_category_index(self):
     categories = [{'name': u'1', 'id': 1}, {'name': u'2', 'id': 2}]
     category_index = label_map_util.create_category_index(categories)
diff --git a/research/object_detection/utils/np_box_list_ops.py b/research/object_detection/utils/np_box_list_ops.py
index 9763b841..0f451671 100644
--- a/research/object_detection/utils/np_box_list_ops.py
+++ b/research/object_detection/utils/np_box_list_ops.py
@@ -21,7 +21,6 @@ Example box operations that are supported:
 """
 
 import numpy as np
-from six.moves import xrange
 
 from object_detection.utils import np_box_list
 from object_detection.utils import np_box_ops
@@ -97,7 +96,7 @@ def ioa(boxlist1, boxlist2):
 def gather(boxlist, indices, fields=None):
   """Gather boxes from BoxList according to indices and return new BoxList.
 
-  By default, Gather returns boxes corresponding to the input index list, as
+  By default, gather returns boxes corresponding to the input index list, as
   well as all additional fields stored in the boxlist (indexing into the
   first dimension).  However one can optionally only gather from a
   subset of fields.
diff --git a/research/object_detection/utils/np_box_list_test.py b/research/object_detection/utils/np_box_list_test.py
index bb0ee5d2..2b98d1f9 100644
--- a/research/object_detection/utils/np_box_list_test.py
+++ b/research/object_detection/utils/np_box_list_test.py
@@ -100,16 +100,16 @@ class AddExtraFieldTest(tf.test.TestCase):
 
   def test_get_extra_fields(self):
     boxlist = self.boxlist
-    self.assertSameElements(boxlist.get_extra_fields(), [])
+    self.assertItemsEqual(boxlist.get_extra_fields(), [])
 
     scores = np.array([0.5, 0.7, 0.9], dtype=float)
     boxlist.add_field('scores', scores)
-    self.assertSameElements(boxlist.get_extra_fields(), ['scores'])
+    self.assertItemsEqual(boxlist.get_extra_fields(), ['scores'])
 
     labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],
                       dtype=int)
     boxlist.add_field('labels', labels)
-    self.assertSameElements(boxlist.get_extra_fields(), ['scores', 'labels'])
+    self.assertItemsEqual(boxlist.get_extra_fields(), ['scores', 'labels'])
 
   def test_get_coordinates(self):
     y_min, x_min, y_max, x_max = self.boxlist.get_coordinates()
diff --git a/research/object_detection/utils/np_box_mask_list.py b/research/object_detection/utils/np_box_mask_list.py
new file mode 100644
index 00000000..28cf50c2
--- /dev/null
+++ b/research/object_detection/utils/np_box_mask_list.py
@@ -0,0 +1,63 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Numpy BoxMaskList classes and functions."""
+
+import numpy as np
+from object_detection.utils import np_box_list
+
+
+class BoxMaskList(np_box_list.BoxList):
+  """Convenience wrapper for BoxList with masks.
+
+  BoxMaskList extends the np_box_list.BoxList to contain masks as well.
+  In particular, its constructor receives both boxes and masks. Note that the
+  masks correspond to the full image.
+  """
+
+  def __init__(self, box_data, mask_data):
+    """Constructs box collection.
+
+    Args:
+      box_data: a numpy array of shape [N, 4] representing box coordinates
+      mask_data: a numpy array of shape [N, height, width] representing masks
+        with values are in {0,1}. The masks correspond to the full
+        image. The height and the width will be equal to image height and width.
+
+    Raises:
+      ValueError: if bbox data is not a numpy array
+      ValueError: if invalid dimensions for bbox data
+      ValueError: if mask data is not a numpy array
+      ValueError: if invalid dimension for mask data
+    """
+    super(BoxMaskList, self).__init__(box_data)
+    if not isinstance(mask_data, np.ndarray):
+      raise ValueError('Mask data must be a numpy array.')
+    if len(mask_data.shape) != 3:
+      raise ValueError('Invalid dimensions for mask data.')
+    if mask_data.dtype != np.uint8:
+      raise ValueError('Invalid data type for mask data: uint8 is required.')
+    if mask_data.shape[0] != box_data.shape[0]:
+      raise ValueError('There should be the same number of boxes and masks.')
+    self.data['masks'] = mask_data
+
+  def get_masks(self):
+    """Convenience function for accessing masks.
+
+    Returns:
+      a numpy array of shape [N, height, width] representing masks
+    """
+    return self.get_field('masks')
+
diff --git a/research/object_detection/utils/np_box_mask_list_ops.py b/research/object_detection/utils/np_box_mask_list_ops.py
new file mode 100644
index 00000000..a50a1194
--- /dev/null
+++ b/research/object_detection/utils/np_box_mask_list_ops.py
@@ -0,0 +1,400 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Operations for np_box_mask_list.BoxMaskList.
+
+Example box operations that are supported:
+  * Areas: compute bounding box areas
+  * IOU: pairwise intersection-over-union scores
+"""
+
+import numpy as np
+
+from object_detection.utils import np_box_list_ops
+from object_detection.utils import np_box_mask_list
+from object_detection.utils import np_mask_ops
+
+
+def box_list_to_box_mask_list(boxlist):
+  """Converts a BoxList containing 'masks' into a BoxMaskList.
+
+  Args:
+    boxlist: An np_box_list.BoxList object.
+
+  Returns:
+    An np_box_mask_list.BoxMaskList object.
+
+  Raises:
+    ValueError: If boxlist does not contain `masks` as a field.
+  """
+  if not boxlist.has_field('masks'):
+    raise ValueError('boxlist does not contain mask field.')
+  box_mask_list = np_box_mask_list.BoxMaskList(
+      box_data=boxlist.get(),
+      mask_data=boxlist.get_field('masks'))
+  extra_fields = boxlist.get_extra_fields()
+  for key in extra_fields:
+    if key != 'masks':
+      box_mask_list.data[key] = boxlist.get_field(key)
+  return box_mask_list
+
+
+def area(box_mask_list):
+  """Computes area of masks.
+
+  Args:
+    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes and masks
+
+  Returns:
+    a numpy array with shape [N*1] representing mask areas
+  """
+  return np_mask_ops.area(box_mask_list.get_masks())
+
+
+def intersection(box_mask_list1, box_mask_list2):
+  """Compute pairwise intersection areas between masks.
+
+  Args:
+    box_mask_list1: BoxMaskList holding N boxes and masks
+    box_mask_list2: BoxMaskList holding M boxes and masks
+
+  Returns:
+    a numpy array with shape [N*M] representing pairwise intersection area
+  """
+  return np_mask_ops.intersection(box_mask_list1.get_masks(),
+                                  box_mask_list2.get_masks())
+
+
+def iou(box_mask_list1, box_mask_list2):
+  """Computes pairwise intersection-over-union between box and mask collections.
+
+  Args:
+    box_mask_list1: BoxMaskList holding N boxes and masks
+    box_mask_list2: BoxMaskList holding M boxes and masks
+
+  Returns:
+    a numpy array with shape [N, M] representing pairwise iou scores.
+  """
+  return np_mask_ops.iou(box_mask_list1.get_masks(),
+                         box_mask_list2.get_masks())
+
+
+def ioa(box_mask_list1, box_mask_list2):
+  """Computes pairwise intersection-over-area between box and mask collections.
+
+  Intersection-over-area (ioa) between two masks mask1 and mask2 is defined as
+  their intersection area over mask2's area. Note that ioa is not symmetric,
+  that is, IOA(mask1, mask2) != IOA(mask2, mask1).
+
+  Args:
+    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks
+    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks
+
+  Returns:
+    a numpy array with shape [N, M] representing pairwise ioa scores.
+  """
+  return np_mask_ops.ioa(box_mask_list1.get_masks(), box_mask_list2.get_masks())
+
+
+def gather(box_mask_list, indices, fields=None):
+  """Gather boxes from np_box_mask_list.BoxMaskList according to indices.
+
+  By default, gather returns boxes corresponding to the input index list, as
+  well as all additional fields stored in the box_mask_list (indexing into the
+  first dimension).  However one can optionally only gather from a
+  subset of fields.
+
+  Args:
+    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes
+    indices: a 1-d numpy array of type int_
+    fields: (optional) list of fields to also gather from.  If None (default),
+        all fields are gathered from.  Pass an empty fields list to only gather
+        the box coordinates.
+
+  Returns:
+    subbox_mask_list: a np_box_mask_list.BoxMaskList corresponding to the subset
+        of the input box_mask_list specified by indices
+
+  Raises:
+    ValueError: if specified field is not contained in box_mask_list or if the
+        indices are not of type int_
+  """
+  if fields is not None:
+    if 'masks' not in fields:
+      fields.append('masks')
+  return box_list_to_box_mask_list(
+      np_box_list_ops.gather(
+          boxlist=box_mask_list, indices=indices, fields=fields))
+
+
+def sort_by_field(box_mask_list, field,
+                  order=np_box_list_ops.SortOrder.DESCEND):
+  """Sort boxes and associated fields according to a scalar field.
+
+  A common use case is reordering the boxes according to descending scores.
+
+  Args:
+    box_mask_list: BoxMaskList holding N boxes.
+    field: A BoxMaskList field for sorting and reordering the BoxMaskList.
+    order: (Optional) 'descend' or 'ascend'. Default is descend.
+
+  Returns:
+    sorted_box_mask_list: A sorted BoxMaskList with the field in the specified
+      order.
+  """
+  return box_list_to_box_mask_list(
+      np_box_list_ops.sort_by_field(
+          boxlist=box_mask_list, field=field, order=order))
+
+
+def non_max_suppression(box_mask_list,
+                        max_output_size=10000,
+                        iou_threshold=1.0,
+                        score_threshold=-10.0):
+  """Non maximum suppression.
+
+  This op greedily selects a subset of detection bounding boxes, pruning
+  away boxes that have high IOU (intersection over union) overlap (> thresh)
+  with already selected boxes. In each iteration, the detected bounding box with
+  highest score in the available pool is selected.
+
+  Args:
+    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain
+      a 'scores' field representing detection scores. All scores belong to the
+      same class.
+    max_output_size: maximum number of retained boxes
+    iou_threshold: intersection over union threshold.
+    score_threshold: minimum score threshold. Remove the boxes with scores
+                     less than this value. Default value is set to -10. A very
+                     low threshold to pass pretty much all the boxes, unless
+                     the user sets a different score threshold.
+
+  Returns:
+    an np_box_mask_list.BoxMaskList holding M boxes where M <= max_output_size
+
+  Raises:
+    ValueError: if 'scores' field does not exist
+    ValueError: if threshold is not in [0, 1]
+    ValueError: if max_output_size < 0
+  """
+  if not box_mask_list.has_field('scores'):
+    raise ValueError('Field scores does not exist')
+  if iou_threshold < 0. or iou_threshold > 1.0:
+    raise ValueError('IOU threshold must be in [0, 1]')
+  if max_output_size < 0:
+    raise ValueError('max_output_size must be bigger than 0.')
+
+  box_mask_list = filter_scores_greater_than(box_mask_list, score_threshold)
+  if box_mask_list.num_boxes() == 0:
+    return box_mask_list
+
+  box_mask_list = sort_by_field(box_mask_list, 'scores')
+
+  # Prevent further computation if NMS is disabled.
+  if iou_threshold == 1.0:
+    if box_mask_list.num_boxes() > max_output_size:
+      selected_indices = np.arange(max_output_size)
+      return gather(box_mask_list, selected_indices)
+    else:
+      return box_mask_list
+
+  masks = box_mask_list.get_masks()
+  num_masks = box_mask_list.num_boxes()
+
+  # is_index_valid is True only for all remaining valid boxes,
+  is_index_valid = np.full(num_masks, 1, dtype=bool)
+  selected_indices = []
+  num_output = 0
+  for i in xrange(num_masks):
+    if num_output < max_output_size:
+      if is_index_valid[i]:
+        num_output += 1
+        selected_indices.append(i)
+        is_index_valid[i] = False
+        valid_indices = np.where(is_index_valid)[0]
+        if valid_indices.size == 0:
+          break
+
+        intersect_over_union = np_mask_ops.iou(
+            np.expand_dims(masks[i], axis=0), masks[valid_indices])
+        intersect_over_union = np.squeeze(intersect_over_union, axis=0)
+        is_index_valid[valid_indices] = np.logical_and(
+            is_index_valid[valid_indices],
+            intersect_over_union <= iou_threshold)
+  return gather(box_mask_list, np.array(selected_indices))
+
+
+def multi_class_non_max_suppression(box_mask_list, score_thresh, iou_thresh,
+                                    max_output_size):
+  """Multi-class version of non maximum suppression.
+
+  This op greedily selects a subset of detection bounding boxes, pruning
+  away boxes that have high IOU (intersection over union) overlap (> thresh)
+  with already selected boxes.  It operates independently for each class for
+  which scores are provided (via the scores field of the input box_list),
+  pruning boxes with score less than a provided threshold prior to
+  applying NMS.
+
+  Args:
+    box_mask_list: np_box_mask_list.BoxMaskList holding N boxes.  Must contain a
+      'scores' field representing detection scores.  This scores field is a
+      tensor that can be 1 dimensional (in the case of a single class) or
+      2-dimensional, in which case we assume that it takes the
+      shape [num_boxes, num_classes]. We further assume that this rank is known
+      statically and that scores.shape[1] is also known (i.e., the number of
+      classes is fixed and known at graph construction time).
+    score_thresh: scalar threshold for score (low scoring boxes are removed).
+    iou_thresh: scalar threshold for IOU (boxes that that high IOU overlap
+      with previously selected boxes are removed).
+    max_output_size: maximum number of retained boxes per class.
+
+  Returns:
+    a box_mask_list holding M boxes with a rank-1 scores field representing
+      corresponding scores for each box with scores sorted in decreasing order
+      and a rank-1 classes field representing a class label for each box.
+  Raises:
+    ValueError: if iou_thresh is not in [0, 1] or if input box_mask_list does
+      not have a valid scores field.
+  """
+  if not 0 <= iou_thresh <= 1.0:
+    raise ValueError('thresh must be between 0 and 1')
+  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):
+    raise ValueError('box_mask_list must be a box_mask_list')
+  if not box_mask_list.has_field('scores'):
+    raise ValueError('input box_mask_list must have \'scores\' field')
+  scores = box_mask_list.get_field('scores')
+  if len(scores.shape) == 1:
+    scores = np.reshape(scores, [-1, 1])
+  elif len(scores.shape) == 2:
+    if scores.shape[1] is None:
+      raise ValueError('scores field must have statically defined second '
+                       'dimension')
+  else:
+    raise ValueError('scores field must be of rank 1 or 2')
+
+  num_boxes = box_mask_list.num_boxes()
+  num_scores = scores.shape[0]
+  num_classes = scores.shape[1]
+
+  if num_boxes != num_scores:
+    raise ValueError('Incorrect scores field length: actual vs expected.')
+
+  selected_boxes_list = []
+  for class_idx in range(num_classes):
+    box_mask_list_and_class_scores = np_box_mask_list.BoxMaskList(
+        box_data=box_mask_list.get(),
+        mask_data=box_mask_list.get_masks())
+    class_scores = np.reshape(scores[0:num_scores, class_idx], [-1])
+    box_mask_list_and_class_scores.add_field('scores', class_scores)
+    box_mask_list_filt = filter_scores_greater_than(
+        box_mask_list_and_class_scores, score_thresh)
+    nms_result = non_max_suppression(
+        box_mask_list_filt,
+        max_output_size=max_output_size,
+        iou_threshold=iou_thresh,
+        score_threshold=score_thresh)
+    nms_result.add_field(
+        'classes',
+        np.zeros_like(nms_result.get_field('scores')) + class_idx)
+    selected_boxes_list.append(nms_result)
+  selected_boxes = np_box_list_ops.concatenate(selected_boxes_list)
+  sorted_boxes = np_box_list_ops.sort_by_field(selected_boxes, 'scores')
+  return box_list_to_box_mask_list(boxlist=sorted_boxes)
+
+
+def prune_non_overlapping_masks(box_mask_list1, box_mask_list2, minoverlap=0.0):
+  """Prunes the boxes in list1 that overlap less than thresh with list2.
+
+  For each mask in box_mask_list1, we want its IOA to be more than minoverlap
+  with at least one of the masks in box_mask_list2. If it does not, we remove
+  it. If the masks are not full size image, we do the pruning based on boxes.
+
+  Args:
+    box_mask_list1: np_box_mask_list.BoxMaskList holding N boxes and masks.
+    box_mask_list2: np_box_mask_list.BoxMaskList holding M boxes and masks.
+    minoverlap: Minimum required overlap between boxes, to count them as
+                overlapping.
+
+  Returns:
+    A pruned box_mask_list with size [N', 4].
+  """
+  intersection_over_area = ioa(box_mask_list2, box_mask_list1)  # [M, N] tensor
+  intersection_over_area = np.amax(intersection_over_area, axis=0)  # [N] tensor
+  keep_bool = np.greater_equal(intersection_over_area, np.array(minoverlap))
+  keep_inds = np.nonzero(keep_bool)[0]
+  new_box_mask_list1 = gather(box_mask_list1, keep_inds)
+  return new_box_mask_list1
+
+
+def concatenate(box_mask_lists, fields=None):
+  """Concatenate list of box_mask_lists.
+
+  This op concatenates a list of input box_mask_lists into a larger
+  box_mask_list.  It also
+  handles concatenation of box_mask_list fields as long as the field tensor
+  shapes are equal except for the first dimension.
+
+  Args:
+    box_mask_lists: list of np_box_mask_list.BoxMaskList objects
+    fields: optional list of fields to also concatenate.  By default, all
+      fields from the first BoxMaskList in the list are included in the
+      concatenation.
+
+  Returns:
+    a box_mask_list with number of boxes equal to
+      sum([box_mask_list.num_boxes() for box_mask_list in box_mask_list])
+  Raises:
+    ValueError: if box_mask_lists is invalid (i.e., is not a list, is empty, or
+      contains non box_mask_list objects), or if requested fields are not
+      contained in all box_mask_lists
+  """
+  if fields is not None:
+    if 'masks' not in fields:
+      fields.append('masks')
+  return box_list_to_box_mask_list(
+      np_box_list_ops.concatenate(boxlists=box_mask_lists, fields=fields))
+
+
+def filter_scores_greater_than(box_mask_list, thresh):
+  """Filter to keep only boxes and masks with score exceeding a given threshold.
+
+  This op keeps the collection of boxes and masks whose corresponding scores are
+  greater than the input threshold.
+
+  Args:
+    box_mask_list: BoxMaskList holding N boxes and masks.  Must contain a
+      'scores' field representing detection scores.
+    thresh: scalar threshold
+
+  Returns:
+    a BoxMaskList holding M boxes and masks where M <= N
+
+  Raises:
+    ValueError: if box_mask_list not a np_box_mask_list.BoxMaskList object or
+      if it does not have a scores field
+  """
+  if not isinstance(box_mask_list, np_box_mask_list.BoxMaskList):
+    raise ValueError('box_mask_list must be a BoxMaskList')
+  if not box_mask_list.has_field('scores'):
+    raise ValueError('input box_mask_list must have \'scores\' field')
+  scores = box_mask_list.get_field('scores')
+  if len(scores.shape) > 2:
+    raise ValueError('Scores should have rank 1 or 2')
+  if len(scores.shape) == 2 and scores.shape[1] != 1:
+    raise ValueError('Scores should have rank 1 or have shape '
+                     'consistent with [None, 1]')
+  high_score_indices = np.reshape(np.where(np.greater(scores, thresh)),
+                                  [-1]).astype(np.int32)
+  return gather(box_mask_list, high_score_indices)
diff --git a/research/object_detection/utils/np_box_mask_list_ops_test.py b/research/object_detection/utils/np_box_mask_list_ops_test.py
new file mode 100644
index 00000000..e2b99fee
--- /dev/null
+++ b/research/object_detection/utils/np_box_mask_list_ops_test.py
@@ -0,0 +1,191 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.utils.np_box_mask_list_ops."""
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import np_box_mask_list
+from object_detection.utils import np_box_mask_list_ops
+
+
+class AreaRelatedTest(tf.test.TestCase):
+
+  def setUp(self):
+    boxes1 = np.array([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]],
+                      dtype=float)
+    masks1_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks1_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 1],
+                         [1, 1, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks1 = np.stack([masks1_0, masks1_1])
+    boxes2 = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                       [0.0, 0.0, 20.0, 20.0]],
+                      dtype=float)
+    masks2_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2_2 = np.array([[1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2 = np.stack([masks2_0, masks2_1, masks2_2])
+    self.box_mask_list1 = np_box_mask_list.BoxMaskList(
+        box_data=boxes1, mask_data=masks1)
+    self.box_mask_list2 = np_box_mask_list.BoxMaskList(
+        box_data=boxes2, mask_data=masks2)
+
+  def test_area(self):
+    areas = np_box_mask_list_ops.area(self.box_mask_list1)
+    expected_areas = np.array([8.0, 10.0], dtype=float)
+    self.assertAllClose(expected_areas, areas)
+
+  def test_intersection(self):
+    intersection = np_box_mask_list_ops.intersection(self.box_mask_list1,
+                                                     self.box_mask_list2)
+    expected_intersection = np.array([[8.0, 0.0, 8.0], [0.0, 9.0, 7.0]],
+                                     dtype=float)
+    self.assertAllClose(intersection, expected_intersection)
+
+  def test_iou(self):
+    iou = np_box_mask_list_ops.iou(self.box_mask_list1, self.box_mask_list2)
+    expected_iou = np.array(
+        [[1.0, 0.0, 8.0 / 25.0], [0.0, 9.0 / 16.0, 7.0 / 28.0]], dtype=float)
+    self.assertAllClose(iou, expected_iou)
+
+  def test_ioa(self):
+    ioa21 = np_box_mask_list_ops.ioa(self.box_mask_list1, self.box_mask_list2)
+    expected_ioa21 = np.array([[1.0, 0.0, 8.0/25.0],
+                               [0.0, 9.0/15.0, 7.0/25.0]],
+                              dtype=np.float32)
+    self.assertAllClose(ioa21, expected_ioa21)
+
+
+class NonMaximumSuppressionTest(tf.test.TestCase):
+
+  def setUp(self):
+    boxes1 = np.array(
+        [[4.0, 3.0, 7.0, 6.0], [5.0, 6.0, 10.0, 10.0]], dtype=float)
+    boxes2 = np.array(
+        [[3.0, 4.0, 6.0, 8.0], [5.0, 6.0, 10.0, 10.0], [1.0, 1.0, 10.0, 10.0]],
+        dtype=float)
+    masks1 = np.array(
+        [[[0, 1, 0], [1, 1, 0], [0, 0, 0]], [[0, 1, 1], [0, 1, 1], [0, 1, 1]]],
+        dtype=np.uint8)
+    masks2 = np.array(
+        [[[0, 1, 0], [1, 1, 1], [0, 0, 0]], [[0, 1, 0], [0, 0, 1], [0, 1, 1]],
+         [[0, 1, 1], [0, 1, 1], [0, 1, 1]]],
+        dtype=np.uint8)
+    self.boxes1 = boxes1
+    self.boxes2 = boxes2
+    self.masks1 = masks1
+    self.masks2 = masks2
+
+  def test_with_no_scores_field(self):
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=self.boxes1, mask_data=self.masks1)
+    max_output_size = 3
+    iou_threshold = 0.5
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list_ops.non_max_suppression(
+          box_mask_list, max_output_size, iou_threshold)
+
+  def test_nms_disabled_max_output_size_equals_one(self):
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=self.boxes2, mask_data=self.masks2)
+    box_mask_list.add_field('scores',
+                            np.array([.9, .75, .6], dtype=float))
+    max_output_size = 1
+    iou_threshold = 1.  # No NMS
+    expected_boxes = np.array([[3.0, 4.0, 6.0, 8.0]], dtype=float)
+    expected_masks = np.array(
+        [[[0, 1, 0], [1, 1, 1], [0, 0, 0]]], dtype=np.uint8)
+    nms_box_mask_list = np_box_mask_list_ops.non_max_suppression(
+        box_mask_list, max_output_size, iou_threshold)
+    self.assertAllClose(nms_box_mask_list.get(), expected_boxes)
+    self.assertAllClose(nms_box_mask_list.get_masks(), expected_masks)
+
+  def test_multiclass_nms(self):
+    boxes = np.array(
+        [[0.2, 0.4, 0.8, 0.8], [0.4, 0.2, 0.8, 0.8], [0.6, 0.0, 1.0, 1.0]],
+        dtype=np.float32)
+    mask0 = np.array([[0, 0, 0, 0, 0],
+                      [0, 0, 1, 1, 0],
+                      [0, 0, 1, 1, 0],
+                      [0, 0, 1, 1, 0],
+                      [0, 0, 0, 0, 0]],
+                     dtype=np.uint8)
+    mask1 = np.array([[0, 0, 0, 0, 0],
+                      [0, 0, 0, 0, 0],
+                      [0, 1, 1, 1, 0],
+                      [0, 1, 1, 1, 0],
+                      [0, 0, 0, 0, 0]],
+                     dtype=np.uint8)
+    mask2 = np.array([[0, 0, 0, 0, 0],
+                      [0, 0, 0, 0, 0],
+                      [0, 0, 0, 0, 0],
+                      [1, 1, 1, 1, 1],
+                      [1, 1, 1, 1, 1]],
+                     dtype=np.uint8)
+    masks = np.stack([mask0, mask1, mask2])
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=masks)
+    scores = np.array([[-0.2, 0.1, 0.5, -0.4, 0.3],
+                       [0.7, -0.7, 0.6, 0.2, -0.9],
+                       [0.4, 0.34, -0.9, 0.2, 0.31]],
+                      dtype=np.float32)
+    box_mask_list.add_field('scores', scores)
+    box_mask_list_clean = np_box_mask_list_ops.multi_class_non_max_suppression(
+        box_mask_list, score_thresh=0.25, iou_thresh=0.1, max_output_size=3)
+
+    scores_clean = box_mask_list_clean.get_field('scores')
+    classes_clean = box_mask_list_clean.get_field('classes')
+    boxes = box_mask_list_clean.get()
+    masks = box_mask_list_clean.get_masks()
+    expected_scores = np.array([0.7, 0.6, 0.34, 0.31])
+    expected_classes = np.array([0, 2, 1, 4])
+    expected_boxes = np.array([[0.4, 0.2, 0.8, 0.8],
+                               [0.4, 0.2, 0.8, 0.8],
+                               [0.6, 0.0, 1.0, 1.0],
+                               [0.6, 0.0, 1.0, 1.0]],
+                              dtype=np.float32)
+    self.assertAllClose(scores_clean, expected_scores)
+    self.assertAllClose(classes_clean, expected_classes)
+    self.assertAllClose(boxes, expected_boxes)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/np_box_mask_list_test.py b/research/object_detection/utils/np_box_mask_list_test.py
new file mode 100644
index 00000000..033825b6
--- /dev/null
+++ b/research/object_detection/utils/np_box_mask_list_test.py
@@ -0,0 +1,182 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.utils.np_box_mask_list_test."""
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import np_box_mask_list
+
+
+class BoxMaskListTest(tf.test.TestCase):
+
+  def test_invalid_box_mask_data(self):
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=[0, 0, 1, 1],
+          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 0, 1, 1]], dtype=int),
+          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([0, 1, 1, 3, 4], dtype=float),
+          mask_data=np.zeros([1, 3, 3], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 1, 1, 3], [3, 1, 1, 5]], dtype=float),
+          mask_data=np.zeros([2, 3, 3], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),
+          mask_data=np.zeros([3, 5, 5], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),
+          mask_data=np.zeros([2, 5], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),
+          mask_data=np.zeros([2, 5, 5, 5], dtype=np.uint8))
+
+    with self.assertRaises(ValueError):
+      np_box_mask_list.BoxMaskList(
+          box_data=np.array([[0, 1, 1, 3], [1, 1, 1, 5]], dtype=float),
+          mask_data=np.zeros([2, 5, 5], dtype=np.int32))
+
+  def test_has_field_with_existed_field(self):
+    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                      [0.0, 0.0, 20.0, 20.0]],
+                     dtype=float)
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=np.zeros([3, 5, 5], dtype=np.uint8))
+    self.assertTrue(box_mask_list.has_field('boxes'))
+    self.assertTrue(box_mask_list.has_field('masks'))
+
+  def test_has_field_with_nonexisted_field(self):
+    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                      [0.0, 0.0, 20.0, 20.0]],
+                     dtype=float)
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=np.zeros([3, 3, 3], dtype=np.uint8))
+    self.assertFalse(box_mask_list.has_field('scores'))
+
+  def test_get_field_with_existed_field(self):
+    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                      [0.0, 0.0, 20.0, 20.0]],
+                     dtype=float)
+    masks = np.zeros([3, 3, 3], dtype=np.uint8)
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=masks)
+    self.assertTrue(np.allclose(box_mask_list.get_field('boxes'), boxes))
+    self.assertTrue(np.allclose(box_mask_list.get_field('masks'), masks))
+
+  def test_get_field_with_nonexited_field(self):
+    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                      [0.0, 0.0, 20.0, 20.0]],
+                     dtype=float)
+    masks = np.zeros([3, 3, 3], dtype=np.uint8)
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=masks)
+    with self.assertRaises(ValueError):
+      box_mask_list.get_field('scores')
+
+
+class AddExtraFieldTest(tf.test.TestCase):
+
+  def setUp(self):
+    boxes = np.array([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                      [0.0, 0.0, 20.0, 20.0]],
+                     dtype=float)
+    masks = np.zeros([3, 3, 3], dtype=np.uint8)
+    self.box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=masks)
+
+  def test_add_already_existed_field_bbox(self):
+    with self.assertRaises(ValueError):
+      self.box_mask_list.add_field('boxes',
+                                   np.array([[0, 0, 0, 1, 0]], dtype=float))
+
+  def test_add_already_existed_field_mask(self):
+    with self.assertRaises(ValueError):
+      self.box_mask_list.add_field('masks',
+                                   np.zeros([3, 3, 3], dtype=np.uint8))
+
+  def test_add_invalid_field_data(self):
+    with self.assertRaises(ValueError):
+      self.box_mask_list.add_field('scores', np.array([0.5, 0.7], dtype=float))
+    with self.assertRaises(ValueError):
+      self.box_mask_list.add_field('scores',
+                                   np.array([0.5, 0.7, 0.9, 0.1], dtype=float))
+
+  def test_add_single_dimensional_field_data(self):
+    box_mask_list = self.box_mask_list
+    scores = np.array([0.5, 0.7, 0.9], dtype=float)
+    box_mask_list.add_field('scores', scores)
+    self.assertTrue(np.allclose(scores, self.box_mask_list.get_field('scores')))
+
+  def test_add_multi_dimensional_field_data(self):
+    box_mask_list = self.box_mask_list
+    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],
+                      dtype=int)
+    box_mask_list.add_field('labels', labels)
+    self.assertTrue(np.allclose(labels, self.box_mask_list.get_field('labels')))
+
+  def test_get_extra_fields(self):
+    box_mask_list = self.box_mask_list
+    self.assertItemsEqual(box_mask_list.get_extra_fields(), ['masks'])
+
+    scores = np.array([0.5, 0.7, 0.9], dtype=float)
+    box_mask_list.add_field('scores', scores)
+    self.assertItemsEqual(box_mask_list.get_extra_fields(), ['masks', 'scores'])
+
+    labels = np.array([[0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1]],
+                      dtype=int)
+    box_mask_list.add_field('labels', labels)
+    self.assertItemsEqual(box_mask_list.get_extra_fields(),
+                          ['masks', 'scores', 'labels'])
+
+  def test_get_coordinates(self):
+    y_min, x_min, y_max, x_max = self.box_mask_list.get_coordinates()
+
+    expected_y_min = np.array([3.0, 14.0, 0.0], dtype=float)
+    expected_x_min = np.array([4.0, 14.0, 0.0], dtype=float)
+    expected_y_max = np.array([6.0, 15.0, 20.0], dtype=float)
+    expected_x_max = np.array([8.0, 15.0, 20.0], dtype=float)
+
+    self.assertTrue(np.allclose(y_min, expected_y_min))
+    self.assertTrue(np.allclose(x_min, expected_x_min))
+    self.assertTrue(np.allclose(y_max, expected_y_max))
+    self.assertTrue(np.allclose(x_max, expected_x_max))
+
+  def test_num_boxes(self):
+    boxes = np.array([[0., 0., 100., 100.], [10., 30., 50., 70.]], dtype=float)
+    masks = np.zeros([2, 5, 5], dtype=np.uint8)
+    box_mask_list = np_box_mask_list.BoxMaskList(
+        box_data=boxes, mask_data=masks)
+    expected_num_boxes = 2
+    self.assertEquals(box_mask_list.num_boxes(), expected_num_boxes)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/np_mask_ops.py b/research/object_detection/utils/np_mask_ops.py
new file mode 100644
index 00000000..b7918b4e
--- /dev/null
+++ b/research/object_detection/utils/np_mask_ops.py
@@ -0,0 +1,119 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Operations for [N, height, width] numpy arrays representing masks.
+
+Example mask operations that are supported:
+  * Areas: compute mask areas
+  * IOU: pairwise intersection-over-union scores
+"""
+import numpy as np
+
+EPSILON = 1e-7
+
+
+def area(masks):
+  """Computes area of masks.
+
+  Args:
+    masks: Numpy array with shape [N, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+
+  Returns:
+    a numpy array with shape [N*1] representing mask areas.
+
+  Raises:
+    ValueError: If masks.dtype is not np.uint8
+  """
+  if masks.dtype != np.uint8:
+    raise ValueError('Masks type should be np.uint8')
+  return np.sum(masks, axis=(1, 2), dtype=np.float32)
+
+
+def intersection(masks1, masks2):
+  """Compute pairwise intersection areas between masks.
+
+  Args:
+    masks1: a numpy array with shape [N, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+    masks2: a numpy array with shape [M, height, width] holding M masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+
+  Returns:
+    a numpy array with shape [N*M] representing pairwise intersection area.
+
+  Raises:
+    ValueError: If masks1 and masks2 are not of type np.uint8.
+  """
+  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:
+    raise ValueError('masks1 and masks2 should be of type np.uint8')
+  n = masks1.shape[0]
+  m = masks2.shape[0]
+  answer = np.zeros([n, m], dtype=np.float32)
+  for i in np.arange(n):
+    for j in np.arange(m):
+      answer[i, j] = np.sum(np.minimum(masks1[i], masks2[j]), dtype=np.float32)
+  return answer
+
+
+def iou(masks1, masks2):
+  """Computes pairwise intersection-over-union between mask collections.
+
+  Args:
+    masks1: a numpy array with shape [N, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+    masks2: a numpy array with shape [M, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+
+  Returns:
+    a numpy array with shape [N, M] representing pairwise iou scores.
+
+  Raises:
+    ValueError: If masks1 and masks2 are not of type np.uint8.
+  """
+  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:
+    raise ValueError('masks1 and masks2 should be of type np.uint8')
+  intersect = intersection(masks1, masks2)
+  area1 = area(masks1)
+  area2 = area(masks2)
+  union = np.expand_dims(area1, axis=1) + np.expand_dims(
+      area2, axis=0) - intersect
+  return intersect / np.maximum(union, EPSILON)
+
+
+def ioa(masks1, masks2):
+  """Computes pairwise intersection-over-area between box collections.
+
+  Intersection-over-area (ioa) between two masks, mask1 and mask2 is defined as
+  their intersection area over mask2's area. Note that ioa is not symmetric,
+  that is, IOA(mask1, mask2) != IOA(mask2, mask1).
+
+  Args:
+    masks1: a numpy array with shape [N, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+    masks2: a numpy array with shape [M, height, width] holding N masks. Masks
+      values are of type np.uint8 and values are in {0,1}.
+
+  Returns:
+    a numpy array with shape [N, M] representing pairwise ioa scores.
+
+  Raises:
+    ValueError: If masks1 and masks2 are not of type np.uint8.
+  """
+  if masks1.dtype != np.uint8 or masks2.dtype != np.uint8:
+    raise ValueError('masks1 and masks2 should be of type np.uint8')
+  intersect = intersection(masks1, masks2)
+  areas = np.expand_dims(area(masks2), axis=0)
+  return intersect / (areas + EPSILON)
diff --git a/research/object_detection/utils/np_mask_ops_test.py b/research/object_detection/utils/np_mask_ops_test.py
new file mode 100644
index 00000000..b65e5830
--- /dev/null
+++ b/research/object_detection/utils/np_mask_ops_test.py
@@ -0,0 +1,88 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.np_mask_ops."""
+
+import numpy as np
+import tensorflow as tf
+
+from object_detection.utils import np_mask_ops
+
+
+class MaskOpsTests(tf.test.TestCase):
+
+  def setUp(self):
+    masks1_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks1_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 1],
+                         [1, 1, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks1 = np.stack([masks1_0, masks1_1])
+    masks2_0 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0],
+                         [1, 1, 1, 1, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2_1 = np.array([[1, 1, 1, 1, 1, 1, 1, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0],
+                         [0, 0, 0, 0, 0, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2_2 = np.array([[1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0],
+                         [1, 1, 1, 1, 1, 0, 0, 0]],
+                        dtype=np.uint8)
+    masks2 = np.stack([masks2_0, masks2_1, masks2_2])
+    self.masks1 = masks1
+    self.masks2 = masks2
+
+  def testArea(self):
+    areas = np_mask_ops.area(self.masks1)
+    expected_areas = np.array([8.0, 10.0], dtype=np.float32)
+    self.assertAllClose(expected_areas, areas)
+
+  def testIntersection(self):
+    intersection = np_mask_ops.intersection(self.masks1, self.masks2)
+    expected_intersection = np.array(
+        [[8.0, 0.0, 8.0], [0.0, 9.0, 7.0]], dtype=np.float32)
+    self.assertAllClose(intersection, expected_intersection)
+
+  def testIOU(self):
+    iou = np_mask_ops.iou(self.masks1, self.masks2)
+    expected_iou = np.array(
+        [[1.0, 0.0, 8.0/25.0], [0.0, 9.0 / 16.0, 7.0 / 28.0]], dtype=np.float32)
+    self.assertAllClose(iou, expected_iou)
+
+  def testIOA(self):
+    ioa21 = np_mask_ops.ioa(self.masks1, self.masks2)
+    expected_ioa21 = np.array([[1.0, 0.0, 8.0/25.0],
+                               [0.0, 9.0/15.0, 7.0/25.0]],
+                              dtype=np.float32)
+    self.assertAllClose(ioa21, expected_ioa21)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 5db1557d..76ba5ff1 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -109,7 +109,8 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
                matching_iou_threshold=0.5,
                evaluate_corlocs=False,
                metric_prefix=None,
-               use_weighted_mean_ap=False):
+               use_weighted_mean_ap=False,
+               evaluate_masks=False):
     """Constructor.
 
     Args:
@@ -125,20 +126,28 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
       use_weighted_mean_ap: (optional) boolean which determines if the mean
         average precision is computed directly from the scores and tp_fp_labels
         of all classes.
+      evaluate_masks: If False, evaluation will be performed based on boxes.
+        If True, mask evaluation will be performed instead.
+
+    Raises:
+      ValueError: If the category ids are not 1-indexed.
     """
     super(ObjectDetectionEvaluator, self).__init__(categories)
     self._num_classes = max([cat['id'] for cat in categories])
+    if min(cat['id'] for cat in categories) < 1:
+      raise ValueError('Classes should be 1-indexed.')
     self._matching_iou_threshold = matching_iou_threshold
     self._use_weighted_mean_ap = use_weighted_mean_ap
     self._label_id_offset = 1
+    self._evaluate_masks = evaluate_masks
     self._evaluation = ObjectDetectionEvaluation(
-        self._num_classes,
+        num_groundtruth_classes=self._num_classes,
         matching_iou_threshold=self._matching_iou_threshold,
         use_weighted_mean_ap=self._use_weighted_mean_ap,
         label_id_offset=self._label_id_offset)
     self._image_ids = set([])
     self._evaluate_corlocs = evaluate_corlocs
-    self._metric_prefix = (metric_prefix + '/') if metric_prefix else ''
+    self._metric_prefix = (metric_prefix + '_') if metric_prefix else ''
 
   def add_single_ground_truth_image_info(self, image_id, groundtruth_dict):
     """Adds groundtruth for a single image to be used for evaluation.
@@ -156,16 +165,19 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
           M numpy boolean array denoting whether a ground truth box is a
           difficult instance or not. This field is optional to support the case
           that no boxes are difficult.
+        standard_fields.InputDataFields.groundtruth_instance_masks: Optional
+          numpy array of shape [num_boxes, height, width] with values in {0, 1}.
 
     Raises:
-      ValueError: On adding groundtruth for an image more than once.
+      ValueError: On adding groundtruth for an image more than once. Will also
+        raise error if instance masks are not in groundtruth dictionary.
     """
     if image_id in self._image_ids:
       raise ValueError('Image with id {} already added.'.format(image_id))
 
-    groundtruth_classes = groundtruth_dict[
-        standard_fields.InputDataFields.groundtruth_classes]
-    groundtruth_classes -= self._label_id_offset
+    groundtruth_classes = (
+        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -
+        self._label_id_offset)
     # If the key is not present in the groundtruth_dict or the array is empty
     # (unless there are no annotations for the groundtruth on this image)
     # use values from the dictionary or insert None otherwise.
@@ -181,11 +193,20 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
         logging.warn(
             'image %s does not have groundtruth difficult flag specified',
             image_id)
+    groundtruth_masks = None
+    if self._evaluate_masks:
+      if (standard_fields.InputDataFields.groundtruth_instance_masks not in
+          groundtruth_dict):
+        raise ValueError('Instance masks not in groundtruth dictionary.')
+      groundtruth_masks = groundtruth_dict[
+          standard_fields.InputDataFields.groundtruth_instance_masks]
     self._evaluation.add_single_ground_truth_image_info(
-        image_id,
-        groundtruth_dict[standard_fields.InputDataFields.groundtruth_boxes],
-        groundtruth_classes,
-        groundtruth_is_difficult_list=groundtruth_difficult)
+        image_key=image_id,
+        groundtruth_boxes=groundtruth_dict[
+            standard_fields.InputDataFields.groundtruth_boxes],
+        groundtruth_class_labels=groundtruth_classes,
+        groundtruth_is_difficult_list=groundtruth_difficult,
+        groundtruth_masks=groundtruth_masks)
     self._image_ids.update([image_id])
 
   def add_single_detected_image_info(self, image_id, detections_dict):
@@ -202,15 +223,31 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
         standard_fields.DetectionResultFields.detection_classes: integer numpy
           array of shape [num_boxes] containing 1-indexed detection classes for
           the boxes.
+        standard_fields.DetectionResultFields.detection_masks: uint8 numpy
+          array of shape [num_boxes, height, width] containing `num_boxes` masks
+          of values ranging between 0 and 1.
+
+    Raises:
+      ValueError: If detection masks are not in detections dictionary.
     """
-    detection_classes = detections_dict[
-        standard_fields.DetectionResultFields.detection_classes]
-    detection_classes -= self._label_id_offset
+    detection_classes = (
+        detections_dict[standard_fields.DetectionResultFields.detection_classes]
+        - self._label_id_offset)
+    detection_masks = None
+    if self._evaluate_masks:
+      if (standard_fields.DetectionResultFields.detection_masks not in
+          detections_dict):
+        raise ValueError('Detection masks not in detections dictionary.')
+      detection_masks = detections_dict[
+          standard_fields.DetectionResultFields.detection_masks]
     self._evaluation.add_single_detected_image_info(
-        image_id,
-        detections_dict[standard_fields.DetectionResultFields.detection_boxes],
-        detections_dict[standard_fields.DetectionResultFields.detection_scores],
-        detection_classes)
+        image_key=image_id,
+        detected_boxes=detections_dict[
+            standard_fields.DetectionResultFields.detection_boxes],
+        detected_scores=detections_dict[
+            standard_fields.DetectionResultFields.detection_scores],
+        detected_class_labels=detection_classes,
+        detected_masks=detection_masks)
 
   def evaluate(self):
     """Compute evaluation result.
@@ -257,7 +294,7 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
   def clear(self):
     """Clears the state to prepare for a fresh evaluation."""
     self._evaluation = ObjectDetectionEvaluation(
-        self._num_classes,
+        num_groundtruth_classes=self._num_classes,
         matching_iou_threshold=self._matching_iou_threshold,
         use_weighted_mean_ap=self._use_weighted_mean_ap,
         label_id_offset=self._label_id_offset)
@@ -272,7 +309,7 @@ class PascalDetectionEvaluator(ObjectDetectionEvaluator):
         categories,
         matching_iou_threshold=matching_iou_threshold,
         evaluate_corlocs=False,
-        metric_prefix='PASCAL',
+        metric_prefix='PascalBoxes',
         use_weighted_mean_ap=False)
 
 
@@ -295,10 +332,47 @@ class WeightedPascalDetectionEvaluator(ObjectDetectionEvaluator):
         categories,
         matching_iou_threshold=matching_iou_threshold,
         evaluate_corlocs=False,
-        metric_prefix='WeightedPASCAL',
+        metric_prefix='WeightedPascalBoxes',
         use_weighted_mean_ap=True)
 
 
+class PascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):
+  """A class to evaluate instance masks using PASCAL metrics."""
+
+  def __init__(self, categories, matching_iou_threshold=0.5):
+    super(PascalInstanceSegmentationEvaluator, self).__init__(
+        categories,
+        matching_iou_threshold=matching_iou_threshold,
+        evaluate_corlocs=False,
+        metric_prefix='PascalMasks',
+        use_weighted_mean_ap=False,
+        evaluate_masks=True)
+
+
+class WeightedPascalInstanceSegmentationEvaluator(ObjectDetectionEvaluator):
+  """A class to evaluate instance masks using weighted PASCAL metrics.
+
+  Weighted PASCAL metrics computes the mean average precision as the average
+  precision given the scores and tp_fp_labels of all classes. In comparison,
+  PASCAL metrics computes the mean average precision as the mean of the
+  per-class average precisions.
+
+  This definition is very similar to the mean of the per-class average
+  precisions weighted by class frequency. However, they are typically not the
+  same as the average precision is not a linear function of the scores and
+  tp_fp_labels.
+  """
+
+  def __init__(self, categories, matching_iou_threshold=0.5):
+    super(WeightedPascalInstanceSegmentationEvaluator, self).__init__(
+        categories,
+        matching_iou_threshold=matching_iou_threshold,
+        evaluate_corlocs=False,
+        metric_prefix='WeightedPascalMasks',
+        use_weighted_mean_ap=True,
+        evaluate_masks=True)
+
+
 class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
   """A class to evaluate detections using Open Images V2 metrics.
 
@@ -348,9 +422,9 @@ class OpenImagesDetectionEvaluator(ObjectDetectionEvaluator):
     if image_id in self._image_ids:
       raise ValueError('Image with id {} already added.'.format(image_id))
 
-    groundtruth_classes = groundtruth_dict[
-        standard_fields.InputDataFields.groundtruth_classes]
-    groundtruth_classes -= self._label_id_offset
+    groundtruth_classes = (
+        groundtruth_dict[standard_fields.InputDataFields.groundtruth_classes] -
+        self._label_id_offset)
     # If the key is not present in the groundtruth_dict or the array is empty
     # (unless there are no annotations for the groundtruth on this image)
     # use values from the dictionary or insert None otherwise.
@@ -392,14 +466,20 @@ class ObjectDetectionEvaluation(object):
                nms_max_output_boxes=10000,
                use_weighted_mean_ap=False,
                label_id_offset=0):
+    if num_groundtruth_classes < 1:
+      raise ValueError('Need at least 1 groundtruth class for evaluation.')
+
     self.per_image_eval = per_image_evaluation.PerImageEvaluation(
-        num_groundtruth_classes, matching_iou_threshold, nms_iou_threshold,
-        nms_max_output_boxes)
+        num_groundtruth_classes=num_groundtruth_classes,
+        matching_iou_threshold=matching_iou_threshold,
+        nms_iou_threshold=nms_iou_threshold,
+        nms_max_output_boxes=nms_max_output_boxes)
     self.num_class = num_groundtruth_classes
     self.label_id_offset = label_id_offset
 
     self.groundtruth_boxes = {}
     self.groundtruth_class_labels = {}
+    self.groundtruth_masks = {}
     self.groundtruth_is_difficult_list = {}
     self.groundtruth_is_group_of_list = {}
     self.num_gt_instances_per_class = np.zeros(self.num_class, dtype=int)
@@ -432,7 +512,8 @@ class ObjectDetectionEvaluation(object):
                                          groundtruth_boxes,
                                          groundtruth_class_labels,
                                          groundtruth_is_difficult_list=None,
-                                         groundtruth_is_group_of_list=None):
+                                         groundtruth_is_group_of_list=None,
+                                         groundtruth_masks=None):
     """Adds groundtruth for a single image to be used for evaluation.
 
     Args:
@@ -448,6 +529,9 @@ class ObjectDetectionEvaluation(object):
       groundtruth_is_group_of_list: A length M numpy boolean array denoting
           whether a ground truth box is a group-of box or not. To support
           the case that no boxes are groups-of, it is by default set as None.
+      groundtruth_masks: uint8 numpy array of shape
+        [num_boxes, height, width] containing `num_boxes` groundtruth masks.
+        The mask values range from 0 to 1.
     """
     if image_key in self.groundtruth_boxes:
       logging.warn(
@@ -457,6 +541,7 @@ class ObjectDetectionEvaluation(object):
 
     self.groundtruth_boxes[image_key] = groundtruth_boxes
     self.groundtruth_class_labels[image_key] = groundtruth_class_labels
+    self.groundtruth_masks[image_key] = groundtruth_masks
     if groundtruth_is_difficult_list is None:
       num_boxes = groundtruth_boxes.shape[0]
       groundtruth_is_difficult_list = np.zeros(num_boxes, dtype=bool)
@@ -474,7 +559,8 @@ class ObjectDetectionEvaluation(object):
         groundtruth_is_group_of_list.astype(dtype=bool))
 
   def add_single_detected_image_info(self, image_key, detected_boxes,
-                                     detected_scores, detected_class_labels):
+                                     detected_scores, detected_class_labels,
+                                     detected_masks=None):
     """Adds detections for a single image to be used for evaluation.
 
     Args:
@@ -486,6 +572,9 @@ class ObjectDetectionEvaluation(object):
         detection scores for the boxes.
       detected_class_labels: integer numpy array of shape [num_boxes] containing
         0-indexed detection classes for the boxes.
+      detected_masks: np.uint8 numpy array of shape [num_boxes, height, width]
+        containing `num_boxes` detection masks with values ranging
+        between 0 and 1.
 
     Raises:
       ValueError: if the number of boxes, scores and class labels differ in
@@ -508,6 +597,10 @@ class ObjectDetectionEvaluation(object):
     if image_key in self.groundtruth_boxes:
       groundtruth_boxes = self.groundtruth_boxes[image_key]
       groundtruth_class_labels = self.groundtruth_class_labels[image_key]
+      # Masks are popped instead of look up. The reason is that we do not want
+      # to keep all masks in memory which can cause memory overflow.
+      groundtruth_masks = self.groundtruth_masks.pop(
+          image_key)
       groundtruth_is_difficult_list = self.groundtruth_is_difficult_list[
           image_key]
       groundtruth_is_group_of_list = self.groundtruth_is_group_of_list[
@@ -515,13 +608,23 @@ class ObjectDetectionEvaluation(object):
     else:
       groundtruth_boxes = np.empty(shape=[0, 4], dtype=float)
       groundtruth_class_labels = np.array([], dtype=int)
+      if detected_masks is None:
+        groundtruth_masks = None
+      else:
+        groundtruth_masks = np.empty(shape=[0, 1, 1], dtype=float)
       groundtruth_is_difficult_list = np.array([], dtype=bool)
       groundtruth_is_group_of_list = np.array([], dtype=bool)
     scores, tp_fp_labels, is_class_correctly_detected_in_image = (
         self.per_image_eval.compute_object_detection_metrics(
-            detected_boxes, detected_scores, detected_class_labels,
-            groundtruth_boxes, groundtruth_class_labels,
-            groundtruth_is_difficult_list, groundtruth_is_group_of_list))
+            detected_boxes=detected_boxes,
+            detected_scores=detected_scores,
+            detected_class_labels=detected_class_labels,
+            groundtruth_boxes=groundtruth_boxes,
+            groundtruth_class_labels=groundtruth_class_labels,
+            groundtruth_is_difficult_list=groundtruth_is_difficult_list,
+            groundtruth_is_group_of_list=groundtruth_is_group_of_list,
+            detected_masks=detected_masks,
+            groundtruth_masks=groundtruth_masks))
 
     for i in range(self.num_class):
       if scores[i].shape[0] > 0:
diff --git a/research/object_detection/utils/object_detection_evaluation_test.py b/research/object_detection/utils/object_detection_evaluation_test.py
index fcaf80a2..3bb52b42 100644
--- a/research/object_detection/utils/object_detection_evaluation_test.py
+++ b/research/object_detection/utils/object_detection_evaluation_test.py
@@ -89,12 +89,12 @@ class OpenImagesV2EvaluationTest(tf.test.TestCase):
     })
     metrics = oiv2_evaluator.evaluate()
     self.assertAlmostEqual(
-        metrics['OpenImagesV2/PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
     self.assertAlmostEqual(
-        metrics['OpenImagesV2/PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
     self.assertAlmostEqual(
-        metrics['OpenImagesV2/PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
-    self.assertAlmostEqual(metrics['OpenImagesV2/Precision/mAP@0.5IOU'],
+        metrics['OpenImagesV2_PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
+    self.assertAlmostEqual(metrics['OpenImagesV2_Precision/mAP@0.5IOU'],
                            0.05555555)
     oiv2_evaluator.clear()
     self.assertFalse(oiv2_evaluator._image_ids)
@@ -102,7 +102,7 @@ class OpenImagesV2EvaluationTest(tf.test.TestCase):
 
 class PascalEvaluationTest(tf.test.TestCase):
 
-  def test_returns_correct_metric_values(self):
+  def test_returns_correct_metric_values_on_boxes(self):
     categories = [{'id': 1, 'name': 'cat'},
                   {'id': 2, 'name': 'dog'},
                   {'id': 3, 'name': 'elephant'}]
@@ -158,12 +158,138 @@ class PascalEvaluationTest(tf.test.TestCase):
 
     metrics = pascal_evaluator.evaluate()
     self.assertAlmostEqual(
-        metrics['PASCAL/PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+        metrics['PascalBoxes_PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
+    self.assertAlmostEqual(
+        metrics['PascalBoxes_PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+    self.assertAlmostEqual(
+        metrics['PascalBoxes_PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
+    self.assertAlmostEqual(metrics['PascalBoxes_Precision/mAP@0.5IOU'],
+                           0.05555555)
+    pascal_evaluator.clear()
+    self.assertFalse(pascal_evaluator._image_ids)
+
+  def test_returns_correct_metric_values_on_masks(self):
+    categories = [{'id': 1, 'name': 'cat'},
+                  {'id': 2, 'name': 'dog'},
+                  {'id': 3, 'name': 'elephant'}]
+    #  Add groundtruth
+    pascal_evaluator = (
+        object_detection_evaluation.PascalInstanceSegmentationEvaluator(
+            categories))
+    image_key1 = 'img1'
+    groundtruth_boxes1 = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
+                                  dtype=float)
+    groundtruth_class_labels1 = np.array([1, 3, 1], dtype=int)
+    groundtruth_masks_1_0 = np.array([[1, 0, 0, 0],
+                                      [1, 0, 0, 0],
+                                      [1, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks_1_1 = np.array([[0, 0, 1, 0],
+                                      [0, 0, 1, 0],
+                                      [0, 0, 1, 0]], dtype=np.uint8)
+    groundtruth_masks_1_2 = np.array([[0, 1, 0, 0],
+                                      [0, 1, 0, 0],
+                                      [0, 1, 0, 0]], dtype=np.uint8)
+    groundtruth_masks1 = np.stack(
+        [groundtruth_masks_1_0, groundtruth_masks_1_1, groundtruth_masks_1_2],
+        axis=0)
+
+    pascal_evaluator.add_single_ground_truth_image_info(
+        image_key1, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes1,
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+                groundtruth_masks1,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels1,
+            standard_fields.InputDataFields.groundtruth_difficult:
+                np.array([], dtype=bool)
+        })
+    image_key2 = 'img2'
+    groundtruth_boxes2 = np.array([[10, 10, 11, 11], [500, 500, 510, 510],
+                                   [10, 10, 12, 12]], dtype=float)
+    groundtruth_class_labels2 = np.array([1, 1, 3], dtype=int)
+    groundtruth_is_difficult_list2 = np.array([False, True, False], dtype=bool)
+    groundtruth_masks_2_0 = np.array([[1, 1, 1, 1],
+                                      [0, 0, 0, 0],
+                                      [0, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks_2_1 = np.array([[0, 0, 0, 0],
+                                      [1, 1, 1, 1],
+                                      [0, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks_2_2 = np.array([[0, 0, 0, 0],
+                                      [0, 0, 0, 0],
+                                      [1, 1, 1, 1]], dtype=np.uint8)
+    groundtruth_masks2 = np.stack(
+        [groundtruth_masks_2_0, groundtruth_masks_2_1, groundtruth_masks_2_2],
+        axis=0)
+    pascal_evaluator.add_single_ground_truth_image_info(
+        image_key2, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes2,
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+                groundtruth_masks2,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels2,
+            standard_fields.InputDataFields.groundtruth_difficult:
+                groundtruth_is_difficult_list2
+        })
+    image_key3 = 'img3'
+    groundtruth_boxes3 = np.array([[0, 0, 1, 1]], dtype=float)
+    groundtruth_class_labels3 = np.array([2], dtype=int)
+    groundtruth_masks_3_0 = np.array([[1, 1, 1, 1],
+                                      [1, 1, 1, 1],
+                                      [1, 1, 1, 1]], dtype=np.uint8)
+    groundtruth_masks3 = np.stack([groundtruth_masks_3_0], axis=0)
+    pascal_evaluator.add_single_ground_truth_image_info(
+        image_key3, {
+            standard_fields.InputDataFields.groundtruth_boxes:
+                groundtruth_boxes3,
+            standard_fields.InputDataFields.groundtruth_instance_masks:
+                groundtruth_masks3,
+            standard_fields.InputDataFields.groundtruth_classes:
+                groundtruth_class_labels3
+        })
+
+    # Add detections
+    image_key = 'img2'
+    detected_boxes = np.array(
+        [[10, 10, 11, 11], [100, 100, 120, 120], [100, 100, 220, 220]],
+        dtype=float)
+    detected_class_labels = np.array([1, 1, 3], dtype=int)
+    detected_scores = np.array([0.7, 0.8, 0.9], dtype=float)
+    detected_masks_0 = np.array([[1, 1, 1, 1],
+                                 [0, 0, 1, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0],
+                                 [1, 1, 0, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_2 = np.array([[0, 1, 0, 0],
+                                 [0, 1, 1, 0],
+                                 [0, 1, 0, 0]], dtype=np.uint8)
+    detected_masks = np.stack(
+        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)
+
+    pascal_evaluator.add_single_detected_image_info(
+        image_key, {
+            standard_fields.DetectionResultFields.detection_boxes:
+                detected_boxes,
+            standard_fields.DetectionResultFields.detection_masks:
+                detected_masks,
+            standard_fields.DetectionResultFields.detection_scores:
+                detected_scores,
+            standard_fields.DetectionResultFields.detection_classes:
+                detected_class_labels
+        })
+
+    metrics = pascal_evaluator.evaluate()
+
+    self.assertAlmostEqual(
+        metrics['PascalMasks_PerformanceByCategory/AP@0.5IOU/dog'], 0.0)
     self.assertAlmostEqual(
-        metrics['PASCAL/PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
+        metrics['PascalMasks_PerformanceByCategory/AP@0.5IOU/elephant'], 0.0)
     self.assertAlmostEqual(
-        metrics['PASCAL/PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
-    self.assertAlmostEqual(metrics['PASCAL/Precision/mAP@0.5IOU'], 0.05555555)
+        metrics['PascalMasks_PerformanceByCategory/AP@0.5IOU/cat'], 0.16666666)
+    self.assertAlmostEqual(metrics['PascalMasks_Precision/mAP@0.5IOU'],
+                           0.05555555)
     pascal_evaluator.clear()
     self.assertFalse(pascal_evaluator._image_ids)
 
@@ -363,6 +489,11 @@ class ObjectDetectionEvaluationTest(tf.test.TestCase):
     self.od_eval.add_single_detected_image_info(
         image_key, detected_boxes, detected_scores, detected_class_labels)
 
+  def test_value_error_on_zero_classes(self):
+    with self.assertRaises(ValueError):
+      object_detection_evaluation.ObjectDetectionEvaluation(
+          num_groundtruth_classes=0)
+
   def test_add_single_ground_truth_image_info(self):
     expected_num_gt_instances_per_class = np.array([3, 1, 1], dtype=int)
     expected_num_gt_imgs_per_class = np.array([2, 1, 2], dtype=int)
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index daa3dec5..374c691b 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -23,6 +23,7 @@ import tensorflow as tf
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import standard_fields as fields
+from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
 
 
@@ -67,7 +68,7 @@ def normalized_to_image_coordinates(normalized_boxes, image_shape,
         box_list.BoxList(normalized_boxes),
         image_shape[1], image_shape[2], check_range=False).get()
 
-  absolute_boxes = tf.map_fn(
+  absolute_boxes = shape_utils.static_or_dynamic_map_fn(
       _to_absolute_coordinates,
       elems=(normalized_boxes),
       dtype=tf.float32,
@@ -115,6 +116,28 @@ def meshgrid(x, y):
     return xgrid, ygrid
 
 
+def fixed_padding(inputs, kernel_size, rate=1):
+  """Pads the input along the spatial dimensions independently of input size.
+
+  Args:
+    inputs: A tensor of size [batch, height_in, width_in, channels].
+    kernel_size: The kernel to be used in the conv2d or max_pool2d operation.
+                 Should be a positive integer.
+    rate: An integer, rate for atrous convolution.
+
+  Returns:
+    output: A tensor of size [batch, height_out, width_out, channels] with the
+      input, either intact (if kernel_size == 1) or padded (if kernel_size > 1).
+  """
+  kernel_size_effective = kernel_size + (kernel_size - 1) * (rate - 1)
+  pad_total = kernel_size_effective - 1
+  pad_beg = pad_total // 2
+  pad_end = pad_total - pad_beg
+  padded_inputs = tf.pad(inputs, [[0, 0], [pad_beg, pad_end],
+                                  [pad_beg, pad_end], [0, 0]])
+  return padded_inputs
+
+
 def pad_to_multiple(tensor, multiple):
   """Returns the tensor zero padded to the specified multiple.
 
@@ -284,6 +307,11 @@ def indices_to_dense_vector(indices,
                            [zeros, values])
 
 
+def reduce_sum_trailing_dimensions(tensor, ndims):
+  """Computes sum across all dimensions following first `ndims` dimensions."""
+  return tf.reduce_sum(tensor, axis=tuple(range(ndims, tensor.shape.ndims)))
+
+
 def retain_groundtruth(tensor_dict, valid_indices):
   """Retains groundtruth by valid indices.
 
@@ -627,7 +655,7 @@ def position_sensitive_crop_regions(image,
     position_sensitive_features = tf.add_n(image_crops) / len(image_crops)
     # Then average over spatial positions within the bins.
     position_sensitive_features = tf.reduce_mean(
-        position_sensitive_features, [1, 2], keep_dims=True)
+        position_sensitive_features, [1, 2], keepdims=True)
   else:
     # Reorder height/width to depth channel.
     block_size = bin_crop_size[0]
@@ -739,3 +767,28 @@ def merge_boxes_with_multiple_labels(boxes, classes, num_classes):
   class_encodings = tf.reshape(class_encodings, [-1, num_classes])
   merged_box_indices = tf.reshape(merged_box_indices, [-1])
   return merged_boxes, class_encodings, merged_box_indices
+
+
+def nearest_neighbor_upsampling(input_tensor, scale):
+  """Nearest neighbor upsampling implementation.
+
+  Nearest neighbor upsampling function that maps input tensor with shape
+  [batch_size, height, width, channels] to [batch_size, height * scale
+  , width * scale, channels]. This implementation only uses reshape and tile to
+  make it compatible with certain hardware.
+
+  Args:
+    input_tensor: A float32 tensor of size [batch, height_in, width_in,
+      channels].
+    scale: An integer multiple to scale resolution of input data.
+  Returns:
+    data_up: A float32 tensor of size
+      [batch, height_in*scale, width_in*scale, channels].
+  """
+  shape = shape_utils.combined_static_and_dynamic_shape(input_tensor)
+  shape_before_tile = [shape[0], shape[1], 1, shape[2], 1, shape[3]]
+  shape_after_tile = [shape[0], shape[1] * scale, shape[2] * scale, shape[3]]
+  data_reshaped = tf.reshape(input_tensor, shape_before_tile)
+  resized_tensor = tf.tile(data_reshaped, [1, 1, scale, 1, scale, 1])
+  resized_tensor = tf.reshape(resized_tensor, shape_after_tile)
+  return resized_tensor
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index 1bdd174b..97ec48e9 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -19,6 +19,7 @@ import tensorflow as tf
 
 from object_detection.core import standard_fields as fields
 from object_detection.utils import ops
+from object_detection.utils import test_case
 
 
 class NormalizedToImageCoordinatesTest(tf.test.TestCase):
@@ -42,6 +43,18 @@ class NormalizedToImageCoordinatesTest(tf.test.TestCase):
     self.assertAllEqual(absolute_boxes, expected_boxes)
 
 
+class ReduceSumTrailingDimensions(tf.test.TestCase):
+
+  def test_reduce_sum_trailing_dimensions(self):
+    input_tensor = tf.placeholder(tf.float32, shape=[None, None, None])
+    reduced_tensor = ops.reduce_sum_trailing_dimensions(input_tensor, ndims=2)
+    with self.test_session() as sess:
+      reduced_np = sess.run(reduced_tensor,
+                            feed_dict={input_tensor: np.ones((2, 2, 2),
+                                                             np.float32)})
+    self.assertAllClose(reduced_np, 2 * np.ones((2, 2), np.float32))
+
+
 class MeshgridTest(tf.test.TestCase):
 
   def test_meshgrid_numpy_comparison(self):
@@ -83,6 +96,30 @@ class MeshgridTest(tf.test.TestCase):
       self.assertEqual(ygrid_output[yind + xind], y[yind])
 
 
+class OpsTestFixedPadding(tf.test.TestCase):
+
+  def test_3x3_kernel(self):
+    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+    padded_tensor = ops.fixed_padding(tensor, 3)
+    with self.test_session() as sess:
+      padded_tensor_out = sess.run(padded_tensor)
+    self.assertEqual((1, 4, 4, 1), padded_tensor_out.shape)
+
+  def test_5x5_kernel(self):
+    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+    padded_tensor = ops.fixed_padding(tensor, 5)
+    with self.test_session() as sess:
+      padded_tensor_out = sess.run(padded_tensor)
+    self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)
+
+  def test_3x3_atrous_kernel(self):
+    tensor = tf.constant([[[[0.], [0.]], [[0.], [0.]]]])
+    padded_tensor = ops.fixed_padding(tensor, 3, 2)
+    with self.test_session() as sess:
+      padded_tensor_out = sess.run(padded_tensor)
+    self.assertEqual((1, 6, 6, 1), padded_tensor_out.shape)
+
+
 class OpsTestPadToMultiple(tf.test.TestCase):
 
   def test_zero_padding(self):
@@ -1128,5 +1165,19 @@ class MergeBoxesWithMultipleLabelsTest(tf.test.TestCase):
       self.assertAllEqual(np_merged_box_indices.shape, [0])
 
 
+class NearestNeighborUpsamplingTest(test_case.TestCase):
+
+  def test_upsampling(self):
+
+    def graph_fn(inputs):
+      custom_op_output = ops.nearest_neighbor_upsampling(inputs, scale=2)
+      tf_op_output = tf.image.resize_images(
+          inputs, [4, 4], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
+      return (custom_op_output, tf_op_output)
+    inputs = np.reshape(np.arange(2**4), [2, 2, 2, 2])
+    (custom_op_output, tf_op_output) = self.execute(graph_fn, [inputs])
+    self.assertAllClose(custom_op_output, tf_op_output)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/per_image_evaluation.py b/research/object_detection/utils/per_image_evaluation.py
index eb7001fc..baa3491d 100644
--- a/research/object_detection/utils/per_image_evaluation.py
+++ b/research/object_detection/utils/per_image_evaluation.py
@@ -17,11 +17,15 @@
 Annotate each detected result as true positives or false positive according to
 a predefined IOU ratio. Non Maximum Supression is used by default. Multi class
 detection is supported by default.
+Based on the settings, per image evaluation is either performed on boxes or
+on object masks.
 """
 import numpy as np
 
 from object_detection.utils import np_box_list
 from object_detection.utils import np_box_list_ops
+from object_detection.utils import np_box_mask_list
+from object_detection.utils import np_box_mask_list_ops
 
 
 class PerImageEvaluation(object):
@@ -49,7 +53,8 @@ class PerImageEvaluation(object):
   def compute_object_detection_metrics(
       self, detected_boxes, detected_scores, detected_class_labels,
       groundtruth_boxes, groundtruth_class_labels,
-      groundtruth_is_difficult_lists, groundtruth_is_group_of_list):
+      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
+      detected_masks=None, groundtruth_masks=None):
     """Evaluates detections as being tp, fp or ignored from a single image.
 
     The evaluation is done in two stages:
@@ -70,10 +75,15 @@ class PerImageEvaluation(object):
           regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
           representing M class labels of object instances in ground truth
-      groundtruth_is_difficult_lists: A boolean numpy array of length M denoting
+      groundtruth_is_difficult_list: A boolean numpy array of length M denoting
           whether a ground truth box is a difficult instance or not
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
           whether a ground truth box has group-of tag
+      detected_masks: (optional) A uint8 numpy array of shape
+        [N, height, width]. If not None, the metrics will be computed based
+        on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape
+        [M, height, width].
 
     Returns:
       scores: A list of C float numpy arrays. Each numpy array is of
@@ -86,22 +96,35 @@ class PerImageEvaluation(object):
           shape [C, 1], indicating whether the correponding class has a least
           one instance being correctly detected in the image
     """
-    detected_boxes, detected_scores, detected_class_labels = (
+    detected_boxes, detected_scores, detected_class_labels, detected_masks = (
         self._remove_invalid_boxes(detected_boxes, detected_scores,
-                                   detected_class_labels))
+                                   detected_class_labels, detected_masks))
     scores, tp_fp_labels = self._compute_tp_fp(
-        detected_boxes, detected_scores, detected_class_labels,
-        groundtruth_boxes, groundtruth_class_labels,
-        groundtruth_is_difficult_lists, groundtruth_is_group_of_list)
+        detected_boxes=detected_boxes,
+        detected_scores=detected_scores,
+        detected_class_labels=detected_class_labels,
+        groundtruth_boxes=groundtruth_boxes,
+        groundtruth_class_labels=groundtruth_class_labels,
+        groundtruth_is_difficult_list=groundtruth_is_difficult_list,
+        groundtruth_is_group_of_list=groundtruth_is_group_of_list,
+        detected_masks=detected_masks,
+        groundtruth_masks=groundtruth_masks)
 
     is_class_correctly_detected_in_image = self._compute_cor_loc(
-        detected_boxes, detected_scores, detected_class_labels,
-        groundtruth_boxes, groundtruth_class_labels)
+        detected_boxes=detected_boxes,
+        detected_scores=detected_scores,
+        detected_class_labels=detected_class_labels,
+        groundtruth_boxes=groundtruth_boxes,
+        groundtruth_class_labels=groundtruth_class_labels,
+        detected_masks=detected_masks,
+        groundtruth_masks=groundtruth_masks)
+
     return scores, tp_fp_labels, is_class_correctly_detected_in_image
 
   def _compute_cor_loc(self, detected_boxes, detected_scores,
                        detected_class_labels, groundtruth_boxes,
-                       groundtruth_class_labels):
+                       groundtruth_class_labels, detected_masks=None,
+                       groundtruth_masks=None):
     """Compute CorLoc score for object detection result.
 
     Args:
@@ -116,28 +139,51 @@ class PerImageEvaluation(object):
           regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
           representing M class labels of object instances in ground truth
+      detected_masks: (optional) A uint8 numpy array of shape
+        [N, height, width]. If not None, the scores will be computed based
+        on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape
+        [M, height, width].
+
     Returns:
       is_class_correctly_detected_in_image: a numpy integer array of
           shape [C, 1], indicating whether the correponding class has a least
           one instance being correctly detected in the image
+
+    Raises:
+      ValueError: If detected masks is not None but groundtruth masks are None,
+        or the other way around.
     """
+    if (detected_masks is not None and
+        groundtruth_masks is None) or (detected_masks is None and
+                                       groundtruth_masks is not None):
+      raise ValueError(
+          'If `detected_masks` is provided, then `groundtruth_masks` should '
+          'also be provided.'
+      )
+
     is_class_correctly_detected_in_image = np.zeros(
         self.num_groundtruth_classes, dtype=int)
     for i in range(self.num_groundtruth_classes):
-      gt_boxes_at_ith_class = groundtruth_boxes[groundtruth_class_labels ==
-                                                i, :]
-      detected_boxes_at_ith_class = detected_boxes[detected_class_labels ==
-                                                   i, :]
-      detected_scores_at_ith_class = detected_scores[detected_class_labels == i]
+      (gt_boxes_at_ith_class, gt_masks_at_ith_class,
+       detected_boxes_at_ith_class, detected_scores_at_ith_class,
+       detected_masks_at_ith_class) = self._get_ith_class_arrays(
+           detected_boxes, detected_scores, detected_masks,
+           detected_class_labels, groundtruth_boxes, groundtruth_masks,
+           groundtruth_class_labels, i)
       is_class_correctly_detected_in_image[i] = (
-          self._compute_is_aclass_correctly_detected_in_image(
-              detected_boxes_at_ith_class, detected_scores_at_ith_class,
-              gt_boxes_at_ith_class))
+          self._compute_is_class_correctly_detected_in_image(
+              detected_boxes=detected_boxes_at_ith_class,
+              detected_scores=detected_scores_at_ith_class,
+              groundtruth_boxes=gt_boxes_at_ith_class,
+              detected_masks=detected_masks_at_ith_class,
+              groundtruth_masks=gt_masks_at_ith_class))
 
     return is_class_correctly_detected_in_image
 
-  def _compute_is_aclass_correctly_detected_in_image(
-      self, detected_boxes, detected_scores, groundtruth_boxes):
+  def _compute_is_class_correctly_detected_in_image(
+      self, detected_boxes, detected_scores, groundtruth_boxes,
+      detected_masks=None, groundtruth_masks=None):
     """Compute CorLoc score for a single class.
 
     Args:
@@ -147,6 +193,11 @@ class PerImageEvaluation(object):
           score
       groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
           box coordinates
+      detected_masks: (optional) A np.uint8 numpy array of shape
+        [N, height, width]. If not None, the scores will be computed based
+        on masks.
+      groundtruth_masks: (optional) A np.uint8 numpy array of shape
+        [M, height, width].
 
     Returns:
       is_class_correctly_detected_in_image: An integer 1 or 0 denoting whether a
@@ -155,18 +206,30 @@ class PerImageEvaluation(object):
     if detected_boxes.size > 0:
       if groundtruth_boxes.size > 0:
         max_score_id = np.argmax(detected_scores)
-        detected_boxlist = np_box_list.BoxList(
-            np.expand_dims(detected_boxes[max_score_id, :], axis=0))
-        gt_boxlist = np_box_list.BoxList(groundtruth_boxes)
-        iou = np_box_list_ops.iou(detected_boxlist, gt_boxlist)
+        mask_mode = False
+        if detected_masks is not None and groundtruth_masks is not None:
+          mask_mode = True
+        if mask_mode:
+          detected_boxlist = np_box_mask_list.BoxMaskList(
+              box_data=np.expand_dims(detected_boxes[max_score_id], axis=0),
+              mask_data=np.expand_dims(detected_masks[max_score_id], axis=0))
+          gt_boxlist = np_box_mask_list.BoxMaskList(
+              box_data=groundtruth_boxes, mask_data=groundtruth_masks)
+          iou = np_box_mask_list_ops.iou(detected_boxlist, gt_boxlist)
+        else:
+          detected_boxlist = np_box_list.BoxList(
+              np.expand_dims(detected_boxes[max_score_id, :], axis=0))
+          gt_boxlist = np_box_list.BoxList(groundtruth_boxes)
+          iou = np_box_list_ops.iou(detected_boxlist, gt_boxlist)
         if np.max(iou) >= self.matching_iou_threshold:
           return 1
     return 0
 
   def _compute_tp_fp(self, detected_boxes, detected_scores,
                      detected_class_labels, groundtruth_boxes,
-                     groundtruth_class_labels, groundtruth_is_difficult_lists,
-                     groundtruth_is_group_of_list):
+                     groundtruth_class_labels, groundtruth_is_difficult_list,
+                     groundtruth_is_group_of_list,
+                     detected_masks=None, groundtruth_masks=None):
     """Labels true/false positives of detections of an image across all classes.
 
     Args:
@@ -181,10 +244,15 @@ class PerImageEvaluation(object):
           regions of object instances in ground truth
       groundtruth_class_labels: An integer numpy array of shape [M, 1],
           representing M class labels of object instances in ground truth
-      groundtruth_is_difficult_lists: A boolean numpy array of length M denoting
+      groundtruth_is_difficult_list: A boolean numpy array of length M denoting
           whether a ground truth box is a difficult instance or not
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
           whether a ground truth box has group-of tag
+      detected_masks: (optional) A np.uint8 numpy array of shape
+        [N, height, width]. If not None, the scores will be computed based
+        on masks.
+      groundtruth_masks: (optional) A np.uint8 numpy array of shape
+        [M, height, width].
 
     Returns:
       result_scores: A list of float numpy arrays. Each numpy array is of
@@ -193,37 +261,134 @@ class PerImageEvaluation(object):
       result_tp_fp_labels: A list of boolean numpy array. Each numpy array is of
           shape [K, 1], representing K True/False positive label of object
           instances detected with class label c
+
+    Raises:
+      ValueError: If detected masks is not None but groundtruth masks are None,
+        or the other way around.
     """
+    if detected_masks is not None and groundtruth_masks is None:
+      raise ValueError(
+          'Detected masks is available but groundtruth masks is not.')
+    if detected_masks is None and groundtruth_masks is not None:
+      raise ValueError(
+          'Groundtruth masks is available but detected masks is not.')
+
     result_scores = []
     result_tp_fp_labels = []
     for i in range(self.num_groundtruth_classes):
-      gt_boxes_at_ith_class = groundtruth_boxes[(groundtruth_class_labels == i
-                                                ), :]
       groundtruth_is_difficult_list_at_ith_class = (
-          groundtruth_is_difficult_lists[groundtruth_class_labels == i])
+          groundtruth_is_difficult_list[groundtruth_class_labels == i])
       groundtruth_is_group_of_list_at_ith_class = (
           groundtruth_is_group_of_list[groundtruth_class_labels == i])
-      detected_boxes_at_ith_class = detected_boxes[(detected_class_labels == i
-                                                   ), :]
-      detected_scores_at_ith_class = detected_scores[detected_class_labels == i]
+      (gt_boxes_at_ith_class, gt_masks_at_ith_class,
+       detected_boxes_at_ith_class, detected_scores_at_ith_class,
+       detected_masks_at_ith_class) = self._get_ith_class_arrays(
+           detected_boxes, detected_scores, detected_masks,
+           detected_class_labels, groundtruth_boxes, groundtruth_masks,
+           groundtruth_class_labels, i)
       scores, tp_fp_labels = self._compute_tp_fp_for_single_class(
-          detected_boxes_at_ith_class, detected_scores_at_ith_class,
-          gt_boxes_at_ith_class, groundtruth_is_difficult_list_at_ith_class,
-          groundtruth_is_group_of_list_at_ith_class)
+          detected_boxes=detected_boxes_at_ith_class,
+          detected_scores=detected_scores_at_ith_class,
+          groundtruth_boxes=gt_boxes_at_ith_class,
+          groundtruth_is_difficult_list=
+          groundtruth_is_difficult_list_at_ith_class,
+          groundtruth_is_group_of_list=
+          groundtruth_is_group_of_list_at_ith_class,
+          detected_masks=detected_masks_at_ith_class,
+          groundtruth_masks=gt_masks_at_ith_class)
       result_scores.append(scores)
       result_tp_fp_labels.append(tp_fp_labels)
     return result_scores, result_tp_fp_labels
 
-  def _remove_invalid_boxes(self, detected_boxes, detected_scores,
-                            detected_class_labels):
-    valid_indices = np.logical_and(detected_boxes[:, 0] < detected_boxes[:, 2],
-                                   detected_boxes[:, 1] < detected_boxes[:, 3])
-    return (detected_boxes[valid_indices, :], detected_scores[valid_indices],
-            detected_class_labels[valid_indices])
+  def _get_overlaps_and_scores_mask_mode(
+      self, detected_boxes, detected_scores, detected_masks, groundtruth_boxes,
+      groundtruth_masks, groundtruth_is_group_of_list):
+    """Computes overlaps and scores between detected and groudntruth masks.
+
+    Args:
+      detected_boxes: A numpy array of shape [N, 4] representing detected box
+          coordinates
+      detected_scores: A 1-d numpy array of length N representing classification
+          score
+      detected_masks: A uint8 numpy array of shape [N, height, width]. If not
+          None, the scores will be computed based on masks.
+      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
+          box coordinates
+      groundtruth_masks: A uint8 numpy array of shape [M, height, width].
+      groundtruth_is_group_of_list: A boolean numpy array of length M denoting
+          whether a ground truth box has group-of tag. If a groundtruth box
+          is group-of box, every detection matching this box is ignored.
+
+    Returns:
+      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
+          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.
+      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
+          gt_group_of_boxlist.num_boxes() == 0 it will be None.
+      scores: The score of the detected boxlist.
+      num_boxes: Number of non-maximum suppressed detected boxes.
+    """
+    detected_boxlist = np_box_mask_list.BoxMaskList(
+        box_data=detected_boxes, mask_data=detected_masks)
+    detected_boxlist.add_field('scores', detected_scores)
+    detected_boxlist = np_box_mask_list_ops.non_max_suppression(
+        detected_boxlist, self.nms_max_output_boxes, self.nms_iou_threshold)
+    gt_non_group_of_boxlist = np_box_mask_list.BoxMaskList(
+        box_data=groundtruth_boxes[~groundtruth_is_group_of_list],
+        mask_data=groundtruth_masks[~groundtruth_is_group_of_list])
+    gt_group_of_boxlist = np_box_mask_list.BoxMaskList(
+        box_data=groundtruth_boxes[groundtruth_is_group_of_list],
+        mask_data=groundtruth_masks[groundtruth_is_group_of_list])
+    iou = np_box_mask_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)
+    ioa = np_box_mask_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    scores = detected_boxlist.get_field('scores')
+    num_boxes = detected_boxlist.num_boxes()
+    return iou, ioa, scores, num_boxes
+
+  def _get_overlaps_and_scores_box_mode(
+      self,
+      detected_boxes,
+      detected_scores,
+      groundtruth_boxes,
+      groundtruth_is_group_of_list):
+    """Computes overlaps and scores between detected and groudntruth boxes.
+
+    Args:
+      detected_boxes: A numpy array of shape [N, 4] representing detected box
+          coordinates
+      detected_scores: A 1-d numpy array of length N representing classification
+          score
+      groundtruth_boxes: A numpy array of shape [M, 4] representing ground truth
+          box coordinates
+      groundtruth_is_group_of_list: A boolean numpy array of length M denoting
+          whether a ground truth box has group-of tag. If a groundtruth box
+          is group-of box, every detection matching this box is ignored.
+
+    Returns:
+      iou: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
+          gt_non_group_of_boxlist.num_boxes() == 0 it will be None.
+      ioa: A float numpy array of size [num_detected_boxes, num_gt_boxes]. If
+          gt_group_of_boxlist.num_boxes() == 0 it will be None.
+      scores: The score of the detected boxlist.
+      num_boxes: Number of non-maximum suppressed detected boxes.
+    """
+    detected_boxlist = np_box_list.BoxList(detected_boxes)
+    detected_boxlist.add_field('scores', detected_scores)
+    detected_boxlist = np_box_list_ops.non_max_suppression(
+        detected_boxlist, self.nms_max_output_boxes, self.nms_iou_threshold)
+    gt_non_group_of_boxlist = np_box_list.BoxList(
+        groundtruth_boxes[~groundtruth_is_group_of_list])
+    gt_group_of_boxlist = np_box_list.BoxList(
+        groundtruth_boxes[groundtruth_is_group_of_list])
+    iou = np_box_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)
+    ioa = np_box_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    scores = detected_boxlist.get_field('scores')
+    num_boxes = detected_boxlist.num_boxes()
+    return iou, ioa, scores, num_boxes
 
   def _compute_tp_fp_for_single_class(
       self, detected_boxes, detected_scores, groundtruth_boxes,
-      groundtruth_is_difficult_list, groundtruth_is_group_of_list):
+      groundtruth_is_difficult_list, groundtruth_is_group_of_list,
+      detected_masks=None, groundtruth_masks=None):
     """Labels boxes detected with the same class from the same image as tp/fp.
 
     Args:
@@ -240,6 +405,11 @@ class PerImageEvaluation(object):
       groundtruth_is_group_of_list: A boolean numpy array of length M denoting
           whether a ground truth box has group-of tag. If a groundtruth box
           is group-of box, every detection matching this box is ignored.
+      detected_masks: (optional) A uint8 numpy array of shape
+        [N, height, width]. If not None, the scores will be computed based
+        on masks.
+      groundtruth_masks: (optional) A uint8 numpy array of shape
+        [M, height, width].
 
     Returns:
       Two arrays of the same size, containing all boxes that were evaluated as
@@ -249,25 +419,37 @@ class PerImageEvaluation(object):
       scores: A numpy array representing the detection scores.
       tp_fp_labels: a boolean numpy array indicating whether a detection is a
           true positive.
-
     """
     if detected_boxes.size == 0:
       return np.array([], dtype=float), np.array([], dtype=bool)
-    detected_boxlist = np_box_list.BoxList(detected_boxes)
-    detected_boxlist.add_field('scores', detected_scores)
-    detected_boxlist = np_box_list_ops.non_max_suppression(
-        detected_boxlist, self.nms_max_output_boxes, self.nms_iou_threshold)
 
-    scores = detected_boxlist.get_field('scores')
+    mask_mode = False
+    if detected_masks is not None and groundtruth_masks is not None:
+      mask_mode = True
+
+    if mask_mode:
+      (iou, ioa, scores,
+       num_detected_boxes) = self._get_overlaps_and_scores_mask_mode(
+           detected_boxes=detected_boxes,
+           detected_scores=detected_scores,
+           detected_masks=detected_masks,
+           groundtruth_boxes=groundtruth_boxes,
+           groundtruth_masks=groundtruth_masks,
+           groundtruth_is_group_of_list=groundtruth_is_group_of_list)
+    else:
+      (iou, ioa, scores,
+       num_detected_boxes) = self._get_overlaps_and_scores_box_mode(
+           detected_boxes=detected_boxes,
+           detected_scores=detected_scores,
+           groundtruth_boxes=groundtruth_boxes,
+           groundtruth_is_group_of_list=groundtruth_is_group_of_list)
 
     if groundtruth_boxes.size == 0:
-      return scores, np.zeros(detected_boxlist.num_boxes(), dtype=bool)
+      return scores, np.zeros(num_detected_boxes, dtype=bool)
 
-    tp_fp_labels = np.zeros(detected_boxlist.num_boxes(), dtype=bool)
-    is_matched_to_difficult_box = np.zeros(
-        detected_boxlist.num_boxes(), dtype=bool)
-    is_matched_to_group_of_box = np.zeros(
-        detected_boxlist.num_boxes(), dtype=bool)
+    tp_fp_labels = np.zeros(num_detected_boxes, dtype=bool)
+    is_matched_to_difficult_box = np.zeros(num_detected_boxes, dtype=bool)
+    is_matched_to_group_of_box = np.zeros(num_detected_boxes, dtype=bool)
 
     # The evaluation is done in two stages:
     # 1. All detections are matched to non group-of boxes; true positives are
@@ -276,16 +458,12 @@ class PerImageEvaluation(object):
     #    group-of boxes and ignored if matched.
 
     # Tp-fp evaluation for non-group of boxes (if any).
-    gt_non_group_of_boxlist = np_box_list.BoxList(
-        groundtruth_boxes[~groundtruth_is_group_of_list, :])
-    if gt_non_group_of_boxlist.num_boxes() > 0:
+    if iou.shape[1] > 0:
       groundtruth_nongroup_of_is_difficult_list = groundtruth_is_difficult_list[
           ~groundtruth_is_group_of_list]
-      iou = np_box_list_ops.iou(detected_boxlist, gt_non_group_of_boxlist)
       max_overlap_gt_ids = np.argmax(iou, axis=1)
-      is_gt_box_detected = np.zeros(
-          gt_non_group_of_boxlist.num_boxes(), dtype=bool)
-      for i in range(detected_boxlist.num_boxes()):
+      is_gt_box_detected = np.zeros(iou.shape[1], dtype=bool)
+      for i in range(num_detected_boxes):
         gt_id = max_overlap_gt_ids[i]
         if iou[i, gt_id] >= self.matching_iou_threshold:
           if not groundtruth_nongroup_of_is_difficult_list[gt_id]:
@@ -296,12 +474,9 @@ class PerImageEvaluation(object):
             is_matched_to_difficult_box[i] = True
 
     # Tp-fp evaluation for group of boxes.
-    gt_group_of_boxlist = np_box_list.BoxList(
-        groundtruth_boxes[groundtruth_is_group_of_list, :])
-    if gt_group_of_boxlist.num_boxes() > 0:
-      ioa = np_box_list_ops.ioa(gt_group_of_boxlist, detected_boxlist)
+    if ioa.shape[0] > 0:
       max_overlap_group_of_gt = np.max(ioa, axis=0)
-      for i in range(detected_boxlist.num_boxes()):
+      for i in range(num_detected_boxes):
         if (not tp_fp_labels[i] and not is_matched_to_difficult_box[i] and
             max_overlap_group_of_gt[i] >= self.matching_iou_threshold):
           is_matched_to_group_of_box[i] = True
@@ -310,3 +485,83 @@ class PerImageEvaluation(object):
                   & ~is_matched_to_group_of_box], tp_fp_labels[
                       ~is_matched_to_difficult_box
                       & ~is_matched_to_group_of_box]
+
+  def _get_ith_class_arrays(self, detected_boxes, detected_scores,
+                            detected_masks, detected_class_labels,
+                            groundtruth_boxes, groundtruth_masks,
+                            groundtruth_class_labels, class_index):
+    """Returns numpy arrays belonging to class with index `class_index`.
+
+    Args:
+      detected_boxes: A numpy array containing detected boxes.
+      detected_scores: A numpy array containing detected scores.
+      detected_masks: A numpy array containing detected masks.
+      detected_class_labels: A numpy array containing detected class labels.
+      groundtruth_boxes: A numpy array containing groundtruth boxes.
+      groundtruth_masks: A numpy array containing groundtruth masks.
+      groundtruth_class_labels: A numpy array containing groundtruth class
+        labels.
+      class_index: An integer index.
+
+    Returns:
+      gt_boxes_at_ith_class: A numpy array containing groundtruth boxes labeled
+        as ith class.
+      gt_masks_at_ith_class: A numpy array containing groundtruth masks labeled
+        as ith class.
+      detected_boxes_at_ith_class: A numpy array containing detected boxes
+        corresponding to the ith class.
+      detected_scores_at_ith_class: A numpy array containing detected scores
+        corresponding to the ith class.
+      detected_masks_at_ith_class: A numpy array containing detected masks
+        corresponding to the ith class.
+    """
+    selected_groundtruth = (groundtruth_class_labels == class_index)
+    gt_boxes_at_ith_class = groundtruth_boxes[selected_groundtruth]
+    if groundtruth_masks is not None:
+      gt_masks_at_ith_class = groundtruth_masks[selected_groundtruth]
+    else:
+      gt_masks_at_ith_class = None
+    selected_detections = (detected_class_labels == class_index)
+    detected_boxes_at_ith_class = detected_boxes[selected_detections]
+    detected_scores_at_ith_class = detected_scores[selected_detections]
+    if detected_masks is not None:
+      detected_masks_at_ith_class = detected_masks[selected_detections]
+    else:
+      detected_masks_at_ith_class = None
+    return (gt_boxes_at_ith_class, gt_masks_at_ith_class,
+            detected_boxes_at_ith_class, detected_scores_at_ith_class,
+            detected_masks_at_ith_class)
+
+  def _remove_invalid_boxes(self, detected_boxes, detected_scores,
+                            detected_class_labels, detected_masks=None):
+    """Removes entries with invalid boxes.
+
+    A box is invalid if either its xmax is smaller than its xmin, or its ymax
+    is smaller than its ymin.
+
+    Args:
+      detected_boxes: A float numpy array of size [num_boxes, 4] containing box
+        coordinates in [ymin, xmin, ymax, xmax] format.
+      detected_scores: A float numpy array of size [num_boxes].
+      detected_class_labels: A int32 numpy array of size [num_boxes].
+      detected_masks: A uint8 numpy array of size [num_boxes, height, width].
+
+    Returns:
+      valid_detected_boxes: A float numpy array of size [num_valid_boxes, 4]
+        containing box coordinates in [ymin, xmin, ymax, xmax] format.
+      valid_detected_scores: A float numpy array of size [num_valid_boxes].
+      valid_detected_class_labels: A int32 numpy array of size
+        [num_valid_boxes].
+      valid_detected_masks: A uint8 numpy array of size
+        [num_valid_boxes, height, width].
+    """
+    valid_indices = np.logical_and(detected_boxes[:, 0] < detected_boxes[:, 2],
+                                   detected_boxes[:, 1] < detected_boxes[:, 3])
+    detected_boxes = detected_boxes[valid_indices]
+    detected_scores = detected_scores[valid_indices]
+    detected_class_labels = detected_class_labels[valid_indices]
+    if detected_masks is not None:
+      detected_masks = detected_masks[valid_indices]
+    return [
+        detected_boxes, detected_scores, detected_class_labels, detected_masks
+    ]
diff --git a/research/object_detection/utils/per_image_evaluation_test.py b/research/object_detection/utils/per_image_evaluation_test.py
index ffd089bf..2aa9931d 100644
--- a/research/object_detection/utils/per_image_evaluation_test.py
+++ b/research/object_detection/utils/per_image_evaluation_test.py
@@ -35,10 +35,29 @@ class SingleClassTpFpWithDifficultBoxesTest(tf.test.TestCase):
     self.detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
                                    dtype=float)
     self.detected_scores = np.array([0.6, 0.8, 0.5], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0],
+                                 [0, 0, 1, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0],
+                                 [1, 1, 0, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_2 = np.array([[0, 0, 0, 0],
+                                 [0, 1, 1, 0],
+                                 [0, 1, 0, 0]], dtype=np.uint8)
+    self.detected_masks = np.stack(
+        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)
     self.groundtruth_boxes = np.array([[0, 0, 1, 1], [0, 0, 10, 10]],
                                       dtype=float)
-
-  def test_match_to_not_difficult_box(self):
+    groundtruth_masks_0 = np.array([[1, 1, 0, 0],
+                                    [1, 1, 0, 0],
+                                    [0, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks_1 = np.array([[0, 0, 0, 1],
+                                    [0, 0, 0, 1],
+                                    [0, 0, 0, 1]], dtype=np.uint8)
+    self.groundtruth_masks = np.stack(
+        [groundtruth_masks_0, groundtruth_masks_1], axis=0)
+
+  def test_match_to_gt_box_0(self):
     groundtruth_groundtruth_is_difficult_list = np.array([False, True],
                                                          dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array(
@@ -52,7 +71,25 @@ class SingleClassTpFpWithDifficultBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
-  def test_match_to_difficult_box(self):
+  def test_mask_match_to_gt_mask_0(self):
+    groundtruth_groundtruth_is_difficult_list = np.array([False, True],
+                                                         dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array(
+        [False, False], dtype=bool)
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        self.groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=self.groundtruth_masks)
+    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)
+    expected_tp_fp_labels = np.array([True, False, False], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
+  def test_match_to_gt_box_1(self):
     groundtruth_groundtruth_is_difficult_list = np.array([True, False],
                                                          dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array(
@@ -66,6 +103,24 @@ class SingleClassTpFpWithDifficultBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
+  def test_mask_match_to_gt_mask_1(self):
+    groundtruth_groundtruth_is_difficult_list = np.array([True, False],
+                                                         dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array(
+        [False, False], dtype=bool)
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        self.groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=self.groundtruth_masks)
+    expected_scores = np.array([0.6, 0.5], dtype=float)
+    expected_tp_fp_labels = np.array([False, False], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
 
 class SingleClassTpFpWithGroupOfBoxesTest(tf.test.TestCase):
 
@@ -81,8 +136,31 @@ class SingleClassTpFpWithGroupOfBoxesTest(tf.test.TestCase):
     self.detected_boxes = np.array(
         [[0, 0, 1, 1], [0, 0, 2, 1], [0, 0, 3, 1]], dtype=float)
     self.detected_scores = np.array([0.8, 0.6, 0.5], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0],
+                                 [0, 0, 1, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0],
+                                 [1, 1, 0, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_2 = np.array([[0, 0, 0, 0],
+                                 [0, 1, 1, 0],
+                                 [0, 1, 0, 0]], dtype=np.uint8)
+    self.detected_masks = np.stack(
+        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)
+
     self.groundtruth_boxes = np.array(
         [[0, 0, 1, 1], [0, 0, 5, 5], [10, 10, 20, 20]], dtype=float)
+    groundtruth_masks_0 = np.array([[1, 0, 0, 0],
+                                    [1, 0, 0, 0],
+                                    [1, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks_1 = np.array([[0, 0, 1, 0],
+                                    [0, 0, 1, 0],
+                                    [0, 0, 1, 0]], dtype=np.uint8)
+    groundtruth_masks_2 = np.array([[0, 1, 0, 0],
+                                    [0, 1, 0, 0],
+                                    [0, 1, 0, 0]], dtype=np.uint8)
+    self.groundtruth_masks = np.stack(
+        [groundtruth_masks_0, groundtruth_masks_1, groundtruth_masks_2], axis=0)
 
   def test_match_to_non_group_of_and_group_of_box(self):
     groundtruth_groundtruth_is_difficult_list = np.array(
@@ -98,6 +176,24 @@ class SingleClassTpFpWithGroupOfBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
+  def test_mask_match_to_non_group_of_and_group_of_box(self):
+    groundtruth_groundtruth_is_difficult_list = np.array(
+        [False, False, False], dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array(
+        [False, True, True], dtype=bool)
+    expected_scores = np.array([0.6], dtype=float)
+    expected_tp_fp_labels = np.array([True], dtype=bool)
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        self.groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=self.groundtruth_masks)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
   def test_match_two_to_group_of_box(self):
     groundtruth_groundtruth_is_difficult_list = np.array(
         [False, False, False], dtype=bool)
@@ -112,32 +208,61 @@ class SingleClassTpFpWithGroupOfBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
+  def test_mask_match_two_to_group_of_box(self):
+    groundtruth_groundtruth_is_difficult_list = np.array(
+        [False, False, False], dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array(
+        [True, False, True], dtype=bool)
+    expected_scores = np.array([0.8], dtype=float)
+    expected_tp_fp_labels = np.array([True], dtype=bool)
+    scores, tp_fp_labels = self.eval._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        self.groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=self.groundtruth_masks)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
 
 class SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):
 
   def setUp(self):
     num_groundtruth_classes = 1
-    matching_iou_threshold1 = 0.5
-    matching_iou_threshold2 = 0.1
+    matching_iou_threshold_high_iou = 0.5
+    matching_iou_threshold_low_iou = 0.1
     nms_iou_threshold = 1.0
     nms_max_output_boxes = 10000
-    self.eval1 = per_image_evaluation.PerImageEvaluation(
-        num_groundtruth_classes, matching_iou_threshold1, nms_iou_threshold,
-        nms_max_output_boxes)
+    self.eval_high_iou = per_image_evaluation.PerImageEvaluation(
+        num_groundtruth_classes, matching_iou_threshold_high_iou,
+        nms_iou_threshold, nms_max_output_boxes)
 
-    self.eval2 = per_image_evaluation.PerImageEvaluation(
-        num_groundtruth_classes, matching_iou_threshold2, nms_iou_threshold,
-        nms_max_output_boxes)
+    self.eval_low_iou = per_image_evaluation.PerImageEvaluation(
+        num_groundtruth_classes, matching_iou_threshold_low_iou,
+        nms_iou_threshold, nms_max_output_boxes)
 
     self.detected_boxes = np.array([[0, 0, 1, 1], [0, 0, 2, 2], [0, 0, 3, 3]],
                                    dtype=float)
     self.detected_scores = np.array([0.6, 0.8, 0.5], dtype=float)
+    detected_masks_0 = np.array([[0, 1, 1, 0],
+                                 [0, 0, 1, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_1 = np.array([[1, 0, 0, 0],
+                                 [1, 1, 0, 0],
+                                 [0, 0, 0, 0]], dtype=np.uint8)
+    detected_masks_2 = np.array([[0, 0, 0, 0],
+                                 [0, 1, 1, 0],
+                                 [0, 1, 0, 0]], dtype=np.uint8)
+    self.detected_masks = np.stack(
+        [detected_masks_0, detected_masks_1, detected_masks_2], axis=0)
 
   def test_no_true_positives(self):
     groundtruth_boxes = np.array([[100, 100, 105, 105]], dtype=float)
     groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
         self.detected_boxes, self.detected_scores, groundtruth_boxes,
         groundtruth_groundtruth_is_difficult_list,
         groundtruth_groundtruth_is_group_of_list)
@@ -146,11 +271,32 @@ class SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
+  def test_mask_no_true_positives(self):
+    groundtruth_boxes = np.array([[100, 100, 105, 105]], dtype=float)
+    groundtruth_masks_0 = np.array([[1, 1, 1, 1],
+                                    [1, 1, 1, 1],
+                                    [1, 1, 1, 1]], dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_masks_0], axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=groundtruth_masks)
+    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)
+    expected_tp_fp_labels = np.array([False, False, False], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
   def test_one_true_positives_with_large_iou_threshold(self):
     groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)
     groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
         self.detected_boxes, self.detected_scores, groundtruth_boxes,
         groundtruth_groundtruth_is_difficult_list,
         groundtruth_groundtruth_is_group_of_list)
@@ -159,11 +305,32 @@ class SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):
     self.assertTrue(np.allclose(expected_scores, scores))
     self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
 
+  def test_mask_one_true_positives_with_large_iou_threshold(self):
+    groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)
+    groundtruth_masks_0 = np.array([[1, 0, 0, 0],
+                                    [1, 1, 0, 0],
+                                    [0, 0, 0, 0]], dtype=np.uint8)
+    groundtruth_masks = np.stack([groundtruth_masks_0], axis=0)
+    groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)
+    groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
+        self.detected_boxes,
+        self.detected_scores,
+        groundtruth_boxes,
+        groundtruth_groundtruth_is_difficult_list,
+        groundtruth_groundtruth_is_group_of_list,
+        detected_masks=self.detected_masks,
+        groundtruth_masks=groundtruth_masks)
+    expected_scores = np.array([0.8, 0.6, 0.5], dtype=float)
+    expected_tp_fp_labels = np.array([True, False, False], dtype=bool)
+    self.assertTrue(np.allclose(expected_scores, scores))
+    self.assertTrue(np.allclose(expected_tp_fp_labels, tp_fp_labels))
+
   def test_one_true_positives_with_very_small_iou_threshold(self):
     groundtruth_boxes = np.array([[0, 0, 1, 1]], dtype=float)
     groundtruth_groundtruth_is_difficult_list = np.zeros(1, dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array([False], dtype=bool)
-    scores, tp_fp_labels = self.eval2._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_low_iou._compute_tp_fp_for_single_class(
         self.detected_boxes, self.detected_scores, groundtruth_boxes,
         groundtruth_groundtruth_is_difficult_list,
         groundtruth_groundtruth_is_group_of_list)
@@ -177,7 +344,7 @@ class SingleClassTpFpNoDifficultBoxesTest(tf.test.TestCase):
     groundtruth_groundtruth_is_difficult_list = np.zeros(2, dtype=bool)
     groundtruth_groundtruth_is_group_of_list = np.array(
         [False, False], dtype=bool)
-    scores, tp_fp_labels = self.eval1._compute_tp_fp_for_single_class(
+    scores, tp_fp_labels = self.eval_high_iou._compute_tp_fp_for_single_class(
         self.detected_boxes, self.detected_scores, groundtruth_boxes,
         groundtruth_groundtruth_is_difficult_list,
         groundtruth_groundtruth_is_group_of_list)
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index 880d367e..06bbae92 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -17,6 +17,8 @@
 
 import tensorflow as tf
 
+from object_detection.utils import static_shape
+
 
 def _is_tensor(t):
   """Returns a boolean indicating whether the input is a tensor.
@@ -125,12 +127,183 @@ def combined_static_and_dynamic_shape(tensor):
   Returns:
     A list of size tensor.shape.ndims containing integers or a scalar tensor.
   """
-  static_shape = tensor.shape.as_list()
-  dynamic_shape = tf.shape(tensor)
+  static_tensor_shape = tensor.shape.as_list()
+  dynamic_tensor_shape = tf.shape(tensor)
   combined_shape = []
-  for index, dim in enumerate(static_shape):
+  for index, dim in enumerate(static_tensor_shape):
     if dim is not None:
       combined_shape.append(dim)
     else:
-      combined_shape.append(dynamic_shape[index])
+      combined_shape.append(dynamic_tensor_shape[index])
   return combined_shape
+
+
+def static_or_dynamic_map_fn(fn, elems, dtype=None,
+                             parallel_iterations=32, back_prop=True):
+  """Runs map_fn as a (static) for loop when possible.
+
+  This function rewrites the map_fn as an explicit unstack input -> for loop
+  over function calls -> stack result combination.  This allows our graphs to
+  be acyclic when the batch size is static.
+  For comparison, see https://www.tensorflow.org/api_docs/python/tf/map_fn.
+
+  Note that `static_or_dynamic_map_fn` currently is not *fully* interchangeable
+  with the default tf.map_fn function as it does not accept nested inputs (only
+  Tensors or lists of Tensors).  Likewise, the output of `fn` can only be a
+  Tensor or list of Tensors.
+
+  TODO: make this function fully interchangeable with tf.map_fn.
+
+  Args:
+    fn: The callable to be performed. It accepts one argument, which will have
+      the same structure as elems. Its output must have the
+      same structure as elems.
+    elems: A tensor or list of tensors, each of which will
+      be unpacked along their first dimension. The sequence of the
+      resulting slices will be applied to fn.
+    dtype:  (optional) The output type(s) of fn. If fn returns a structure of
+      Tensors differing from the structure of elems, then dtype is not optional
+      and must have the same structure as the output of fn.
+    parallel_iterations: (optional) number of batch items to process in
+      parallel.  This flag is only used if the native tf.map_fn is used
+      and defaults to 32 instead of 10 (unlike the standard tf.map_fn default).
+    back_prop: (optional) True enables support for back propagation.
+      This flag is only used if the native tf.map_fn is used.
+
+  Returns:
+    A tensor or sequence of tensors. Each tensor packs the
+    results of applying fn to tensors unpacked from elems along the first
+    dimension, from first to last.
+  Raises:
+    ValueError: if `elems` a Tensor or a list of Tensors.
+    ValueError: if `fn` does not return a Tensor or list of Tensors
+  """
+  if isinstance(elems, list):
+    for elem in elems:
+      if not isinstance(elem, tf.Tensor):
+        raise ValueError('`elems` must be a Tensor or list of Tensors.')
+
+    elem_shapes = [elem.shape.as_list() for elem in elems]
+    # Fall back on tf.map_fn if shapes of each entry of `elems` are None or fail
+    # to all be the same size along the batch dimension.
+    for elem_shape in elem_shapes:
+      if (not elem_shape or not elem_shape[0]
+          or elem_shape[0] != elem_shapes[0][0]):
+        return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)
+    arg_tuples = zip(*[tf.unstack(elem) for elem in elems])
+    outputs = [fn(arg_tuple) for arg_tuple in arg_tuples]
+  else:
+    if not isinstance(elems, tf.Tensor):
+      raise ValueError('`elems` must be a Tensor or list of Tensors.')
+    elems_shape = elems.shape.as_list()
+    if not elems_shape or not elems_shape[0]:
+      return tf.map_fn(fn, elems, dtype, parallel_iterations, back_prop)
+    outputs = [fn(arg) for arg in tf.unstack(elems)]
+  # Stack `outputs`, which is a list of Tensors or list of lists of Tensors
+  if all([isinstance(output, tf.Tensor) for output in outputs]):
+    return tf.stack(outputs)
+  else:
+    if all([isinstance(output, list) for output in outputs]):
+      if all([all(
+          [isinstance(entry, tf.Tensor) for entry in output_list])
+              for output_list in outputs]):
+        return [tf.stack(output_tuple) for output_tuple in zip(*outputs)]
+  raise ValueError('`fn` should return a Tensor or a list of Tensors.')
+
+
+def check_min_image_dim(min_dim, image_tensor):
+  """Checks that the image width/height are greater than some number.
+
+  This function is used to check that the width and height of an image are above
+  a certain value. If the image shape is static, this function will perform the
+  check at graph construction time. Otherwise, if the image shape varies, an
+  Assertion control dependency will be added to the graph.
+
+  Args:
+    min_dim: The minimum number of pixels along the width and height of the
+             image.
+    image_tensor: The image tensor to check size for.
+
+  Returns:
+    If `image_tensor` has dynamic size, return `image_tensor` with a Assert
+    control dependency. Otherwise returns image_tensor.
+
+  Raises:
+    ValueError: if `image_tensor`'s' width or height is smaller than `min_dim`.
+  """
+  image_shape = image_tensor.get_shape()
+  image_height = static_shape.get_height(image_shape)
+  image_width = static_shape.get_width(image_shape)
+  if image_height is None or image_width is None:
+    shape_assert = tf.Assert(
+        tf.logical_and(tf.greater_equal(tf.shape(image_tensor)[1], min_dim),
+                       tf.greater_equal(tf.shape(image_tensor)[2], min_dim)),
+        ['image size must be >= {} in both height and width.'.format(min_dim)])
+    with tf.control_dependencies([shape_assert]):
+      return tf.identity(image_tensor)
+
+  if image_height < min_dim or image_width < min_dim:
+    raise ValueError(
+        'image size must be >= %d in both height and width; image dim = %d,%d' %
+        (min_dim, image_height, image_width))
+
+  return image_tensor
+
+
+def assert_shape_equal(shape_a, shape_b):
+  """Asserts that shape_a and shape_b are equal.
+
+  If the shapes are static, raises a ValueError when the shapes
+  mismatch.
+
+  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes
+  mismatch.
+
+  Args:
+    shape_a: a list containing shape of the first tensor.
+    shape_b: a list containing shape of the second tensor.
+
+  Returns:
+    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op
+    when the shapes are dynamic.
+
+  Raises:
+    ValueError: When shapes are both static and unequal.
+  """
+  if (all(isinstance(dim, int) for dim in shape_a) and
+      all(isinstance(dim, int) for dim in shape_b)):
+    if shape_a != shape_b:
+      raise ValueError('Unequal shapes {}, {}'.format(shape_a, shape_b))
+    else: return tf.no_op()
+  else:
+    return tf.assert_equal(shape_a, shape_b)
+
+
+def assert_shape_equal_along_first_dimension(shape_a, shape_b):
+  """Asserts that shape_a and shape_b are the same along the 0th-dimension.
+
+  If the shapes are static, raises a ValueError when the shapes
+  mismatch.
+
+  If the shapes are dynamic, raises a tf InvalidArgumentError when the shapes
+  mismatch.
+
+  Args:
+    shape_a: a list containing shape of the first tensor.
+    shape_b: a list containing shape of the second tensor.
+
+  Returns:
+    Either a tf.no_op() when shapes are all static and a tf.assert_equal() op
+    when the shapes are dynamic.
+
+  Raises:
+    ValueError: When shapes are both static and unequal.
+  """
+  if isinstance(shape_a[0], int) and isinstance(shape_b[0], int):
+    if shape_a[0] != shape_b[0]:
+      raise ValueError('Unequal first dimension {}, {}'.format(
+          shape_a[0], shape_b[0]))
+    else: return tf.no_op()
+  else:
+    return tf.assert_equal(shape_a[0], shape_b[0])
+
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index abeacac8..d4548f04 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -15,6 +15,7 @@
 
 """Tests for object_detection.utils.shape_utils."""
 
+import numpy as np
 import tensorflow as tf
 
 from object_detection.utils import shape_utils
@@ -123,5 +124,198 @@ class UtilTest(tf.test.TestCase):
     self.assertListEqual(combined_shape[1:], [2, 3])
 
 
+class StaticOrDynamicMapFnTest(tf.test.TestCase):
+
+  def test_with_dynamic_shape(self):
+    def fn(input_tensor):
+      return tf.reduce_sum(input_tensor)
+    input_tensor = tf.placeholder(tf.float32, shape=(None, 2))
+    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
+
+    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
+
+    with self.test_session() as sess:
+      result1 = sess.run(
+          map_fn_output, feed_dict={
+              input_tensor: [[1, 2], [3, 1], [0, 4]]})
+      result2 = sess.run(
+          map_fn_output, feed_dict={
+              input_tensor: [[-1, 1], [0, 9]]})
+      self.assertAllEqual(result1, [3, 4, 4])
+      self.assertAllEqual(result2, [0, 9])
+
+  def test_with_static_shape(self):
+    def fn(input_tensor):
+      return tf.reduce_sum(input_tensor)
+    input_tensor = tf.constant([[1, 2], [3, 1], [0, 4]], dtype=tf.float32)
+    map_fn_output = shape_utils.static_or_dynamic_map_fn(fn, input_tensor)
+
+    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
+
+    with self.test_session() as sess:
+      result = sess.run(map_fn_output)
+      self.assertAllEqual(result, [3, 4, 4])
+
+  def test_with_multiple_dynamic_shapes(self):
+    def fn(elems):
+      input_tensor, scalar_index_tensor = elems
+      return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])
+
+    input_tensor = tf.placeholder(tf.float32, shape=(None, 3))
+    scalar_index_tensor = tf.placeholder(tf.int32, shape=(None, 1))
+    map_fn_output = shape_utils.static_or_dynamic_map_fn(
+        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
+
+    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    self.assertTrue(any(['map' == op_name[:3] for op_name in op_names]))
+
+    with self.test_session() as sess:
+      result1 = sess.run(
+          map_fn_output, feed_dict={
+              input_tensor: [[1, 2, 3], [4, 5, -1], [0, 6, 9]],
+              scalar_index_tensor: [[0], [2], [1]],
+          })
+      result2 = sess.run(
+          map_fn_output, feed_dict={
+              input_tensor: [[-1, 1, 0], [3, 9, 30]],
+              scalar_index_tensor: [[1], [0]]
+          })
+      self.assertAllEqual(result1, [1, -1, 6])
+      self.assertAllEqual(result2, [1, 3])
+
+  def test_with_multiple_static_shapes(self):
+    def fn(elems):
+      input_tensor, scalar_index_tensor = elems
+      return tf.reshape(tf.slice(input_tensor, scalar_index_tensor, [1]), [])
+
+    input_tensor = tf.constant([[1, 2, 3], [4, 5, -1], [0, 6, 9]],
+                               dtype=tf.float32)
+    scalar_index_tensor = tf.constant([[0], [2], [1]], dtype=tf.int32)
+    map_fn_output = shape_utils.static_or_dynamic_map_fn(
+        fn, [input_tensor, scalar_index_tensor], dtype=tf.float32)
+
+    op_names = [op.name for op in tf.get_default_graph().get_operations()]
+    self.assertTrue(all(['map' != op_name[:3] for op_name in op_names]))
+
+    with self.test_session() as sess:
+      result = sess.run(map_fn_output)
+      self.assertAllEqual(result, [1, -1, 6])
+
+  def test_fails_with_nested_input(self):
+    def fn(input_tensor):
+      return input_tensor
+    input_tensor1 = tf.constant([1])
+    input_tensor2 = tf.constant([2])
+    with self.assertRaisesRegexp(
+        ValueError, '`elems` must be a Tensor or list of Tensors.'):
+      shape_utils.static_or_dynamic_map_fn(
+          fn, [input_tensor1, [input_tensor2]], dtype=tf.float32)
+
+
+class CheckMinImageShapeTest(tf.test.TestCase):
+
+  def test_check_min_image_dim_static_shape(self):
+    input_tensor = tf.constant(np.zeros([1, 42, 42, 3]))
+    _ = shape_utils.check_min_image_dim(33, input_tensor)
+
+    with self.assertRaisesRegexp(
+        ValueError, 'image size must be >= 64 in both height and width.'):
+      _ = shape_utils.check_min_image_dim(64, input_tensor)
+
+  def test_check_min_image_dim_dynamic_shape(self):
+    input_placeholder = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    image_tensor = shape_utils.check_min_image_dim(33, input_placeholder)
+
+    with self.test_session() as sess:
+      sess.run(image_tensor,
+               feed_dict={input_placeholder: np.zeros([1, 42, 42, 3])})
+      with self.assertRaises(tf.errors.InvalidArgumentError):
+        sess.run(image_tensor,
+                 feed_dict={input_placeholder: np.zeros([1, 32, 32, 3])})
+
+
+class AssertShapeEqualTest(tf.test.TestCase):
+
+  def test_unequal_static_shape_raises_exception(self):
+    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+    shape_b = tf.constant(np.zeros([4, 2, 3, 1]))
+    with self.assertRaisesRegexp(
+        ValueError, 'Unequal shapes'):
+      shape_utils.assert_shape_equal(
+          shape_utils.combined_static_and_dynamic_shape(shape_a),
+          shape_utils.combined_static_and_dynamic_shape(shape_b))
+
+  def test_equal_static_shape_succeeds(self):
+    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+    shape_b = tf.constant(np.zeros([4, 2, 2, 1]))
+    with self.test_session() as sess:
+      op = shape_utils.assert_shape_equal(
+          shape_utils.combined_static_and_dynamic_shape(shape_a),
+          shape_utils.combined_static_and_dynamic_shape(shape_b))
+      sess.run(op)
+
+  def test_unequal_dynamic_shape_raises_tf_assert(self):
+    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    op = shape_utils.assert_shape_equal(
+        shape_utils.combined_static_and_dynamic_shape(tensor_a),
+        shape_utils.combined_static_and_dynamic_shape(tensor_b))
+    with self.test_session() as sess:
+      with self.assertRaises(tf.errors.InvalidArgumentError):
+        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
+                                tensor_b: np.zeros([1, 4, 4, 3])})
+
+  def test_equal_dynamic_shape_succeeds(self):
+    tensor_a = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    tensor_b = tf.placeholder(tf.float32, shape=[1, None, None, 3])
+    op = shape_utils.assert_shape_equal(
+        shape_utils.combined_static_and_dynamic_shape(tensor_a),
+        shape_utils.combined_static_and_dynamic_shape(tensor_b))
+    with self.test_session() as sess:
+      sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
+                              tensor_b: np.zeros([1, 2, 2, 3])})
+
+  def test_unequal_static_shape_along_first_dim_raises_exception(self):
+    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+    shape_b = tf.constant(np.zeros([6, 2, 3, 1]))
+    with self.assertRaisesRegexp(
+        ValueError, 'Unequal first dimension'):
+      shape_utils.assert_shape_equal_along_first_dimension(
+          shape_utils.combined_static_and_dynamic_shape(shape_a),
+          shape_utils.combined_static_and_dynamic_shape(shape_b))
+
+  def test_equal_static_shape_along_first_dim_succeeds(self):
+    shape_a = tf.constant(np.zeros([4, 2, 2, 1]))
+    shape_b = tf.constant(np.zeros([4, 7, 2]))
+    with self.test_session() as sess:
+      op = shape_utils.assert_shape_equal_along_first_dimension(
+          shape_utils.combined_static_and_dynamic_shape(shape_a),
+          shape_utils.combined_static_and_dynamic_shape(shape_b))
+      sess.run(op)
+
+  def test_unequal_dynamic_shape_along_first_dim_raises_tf_assert(self):
+    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
+    tensor_b = tf.placeholder(tf.float32, shape=[None, None, 3])
+    op = shape_utils.assert_shape_equal_along_first_dimension(
+        shape_utils.combined_static_and_dynamic_shape(tensor_a),
+        shape_utils.combined_static_and_dynamic_shape(tensor_b))
+    with self.test_session() as sess:
+      with self.assertRaises(tf.errors.InvalidArgumentError):
+        sess.run(op, feed_dict={tensor_a: np.zeros([1, 2, 2, 3]),
+                                tensor_b: np.zeros([2, 4, 3])})
+
+  def test_equal_dynamic_shape_along_first_dim_succeeds(self):
+    tensor_a = tf.placeholder(tf.float32, shape=[None, None, None, 3])
+    tensor_b = tf.placeholder(tf.float32, shape=[None])
+    op = shape_utils.assert_shape_equal_along_first_dimension(
+        shape_utils.combined_static_and_dynamic_shape(tensor_a),
+        shape_utils.combined_static_and_dynamic_shape(tensor_b))
+    with self.test_session() as sess:
+      sess.run(op, feed_dict={tensor_a: np.zeros([5, 2, 2, 3]),
+                              tensor_b: np.zeros([5])})
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
new file mode 100644
index 00000000..b829a5e4
--- /dev/null
+++ b/research/object_detection/utils/test_case.py
@@ -0,0 +1,80 @@
+"""A convenience wrapper around tf.test.TestCase to enable TPU tests."""
+
+import tensorflow as tf
+from tensorflow.contrib import tpu
+
+flags = tf.app.flags
+
+flags.DEFINE_bool('tpu_test', False, 'Whether to configure test for TPU.')
+FLAGS = flags.FLAGS
+
+
+class TestCase(tf.test.TestCase):
+  """Extends tf.test.TestCase to optionally allow running tests on TPU."""
+
+  def execute_tpu(self, graph_fn, inputs):
+    """Constructs the graph, executes it on TPU and returns the result.
+
+    Args:
+      graph_fn: a callable that constructs the tensorflow graph to test. The
+        arguments of this function should correspond to `inputs`.
+      inputs: a list of numpy arrays to feed input to the computation graph.
+
+    Returns:
+      A list of numpy arrays or a scalar returned from executing the tensorflow
+      graph.
+    """
+    with self.test_session(graph=tf.Graph()) as sess:
+      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
+      tpu_computation = tpu.rewrite(graph_fn, placeholders)
+      sess.run(tpu.initialize_system())
+      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
+                tf.local_variables_initializer()])
+      materialized_results = sess.run(tpu_computation,
+                                      feed_dict=dict(zip(placeholders, inputs)))
+      sess.run(tpu.shutdown_system())
+      if len(materialized_results) == 1:
+        materialized_results = materialized_results[0]
+    return materialized_results
+
+  def execute_cpu(self, graph_fn, inputs):
+    """Constructs the graph, executes it on CPU and returns the result.
+
+    Args:
+      graph_fn: a callable that constructs the tensorflow graph to test. The
+        arguments of this function should correspond to `inputs`.
+      inputs: a list of numpy arrays to feed input to the computation graph.
+
+    Returns:
+      A list of numpy arrays or a scalar returned from executing the tensorflow
+      graph.
+    """
+    with self.test_session(graph=tf.Graph()) as sess:
+      placeholders = [tf.placeholder_with_default(v, v.shape) for v in inputs]
+      results = graph_fn(*placeholders)
+      sess.run([tf.global_variables_initializer(), tf.tables_initializer(),
+                tf.local_variables_initializer()])
+      materialized_results = sess.run(results, feed_dict=dict(zip(placeholders,
+                                                                  inputs)))
+      if len(materialized_results) == 1:
+        materialized_results = materialized_results[0]
+    return materialized_results
+
+  def execute(self, graph_fn, inputs):
+    """Constructs the graph, creates a test session and returns the results.
+
+    The graph is executed either on TPU or CPU based on the `tpu_test` flag.
+
+    Args:
+      graph_fn: a callable that constructs the tensorflow graph to test. The
+        arguments of this function should correspond to `inputs`.
+      inputs: a list of numpy arrays to feed input to the computation graph.
+
+    Returns:
+      A list of numpy arrays or a scalar returned from executing the tensorflow
+      graph.
+    """
+    if FLAGS.tpu_test:
+      return self.execute_tpu(graph_fn, inputs)
+    else:
+      return self.execute_cpu(graph_fn, inputs)
diff --git a/research/object_detection/utils/test_utils.py b/research/object_detection/utils/test_utils.py
index e6277ea5..919bde96 100644
--- a/research/object_detection/utils/test_utils.py
+++ b/research/object_detection/utils/test_utils.py
@@ -46,12 +46,13 @@ class MockBoxPredictor(box_predictor.BoxPredictor):
     super(MockBoxPredictor, self).__init__(is_training, num_classes)
 
   def _predict(self, image_features, num_predictions_per_location):
+    image_feature = image_features[0]
     combined_feature_shape = shape_utils.combined_static_and_dynamic_shape(
-        image_features)
+        image_feature)
     batch_size = combined_feature_shape[0]
     num_anchors = (combined_feature_shape[1] * combined_feature_shape[2])
     code_size = 4
-    zero = tf.reduce_sum(0 * image_features)
+    zero = tf.reduce_sum(0 * image_feature)
     box_encodings = zero + tf.zeros(
         (batch_size, num_anchors, 1, code_size), dtype=tf.float32)
     class_predictions_with_background = zero + tf.zeros(
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index b27f814f..14aa3f32 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -96,7 +96,9 @@ def freeze_gradients_matching_regex(grads_and_vars, regex_list):
   return kept_grads_and_vars
 
 
-def get_variables_available_in_checkpoint(variables, checkpoint_path):
+def get_variables_available_in_checkpoint(variables,
+                                          checkpoint_path,
+                                          include_global_step=True):
   """Returns the subset of variables available in the checkpoint.
 
   Inspects given checkpoint and returns the subset of variables that are
@@ -107,6 +109,8 @@ def get_variables_available_in_checkpoint(variables, checkpoint_path):
   Args:
     variables: a list or dictionary of variables to find in checkpoint.
     checkpoint_path: path to the checkpoint to restore variables from.
+    include_global_step: whether to include `global_step` variable, if it
+      exists. Default True.
 
   Returns:
     A list or dictionary of variables.
@@ -120,13 +124,20 @@ def get_variables_available_in_checkpoint(variables, checkpoint_path):
   else:
     raise ValueError('`variables` is expected to be a list or dict.')
   ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)
-  ckpt_vars = ckpt_reader.get_variable_to_shape_map().keys()
+  ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
+  if not include_global_step:
+    ckpt_vars_to_shape_map.pop(tf.GraphKeys.GLOBAL_STEP, None)
   vars_in_ckpt = {}
   for variable_name, variable in sorted(variable_names_map.items()):
-    if variable_name in ckpt_vars:
-      vars_in_ckpt[variable_name] = variable
+    if variable_name in ckpt_vars_to_shape_map:
+      if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():
+        vars_in_ckpt[variable_name] = variable
+      else:
+        logging.warning('Variable [%s] is available in checkpoint, but has an '
+                        'incompatible shape with model variable.',
+                        variable_name)
     else:
-      logging.warning('Variable [%s] not available in checkpoint',
+      logging.warning('Variable [%s] is not available in checkpoint',
                       variable_name)
   if isinstance(variables, list):
     return vars_in_ckpt.values()
diff --git a/research/object_detection/utils/variables_helper_test.py b/research/object_detection/utils/variables_helper_test.py
index c04b1191..b6d58bb6 100644
--- a/research/object_detection/utils/variables_helper_test.py
+++ b/research/object_detection/utils/variables_helper_test.py
@@ -145,8 +145,11 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
 
   def test_return_variables_available_in_checkpoint(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
+    weight_variable = tf.Variable(1.0, name='weights')
+    global_step = tf.train.get_or_create_global_step()
     graph1_variables = [
-        tf.Variable(1.0, name='weights'),
+        weight_variable,
+        global_step
     ]
     init_op = tf.global_variables_initializer()
     saver = tf.train.Saver(graph1_variables)
@@ -156,8 +159,8 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
 
     graph2_variables = graph1_variables + [tf.Variable(1.0, name='biases')]
     out_variables = variables_helper.get_variables_available_in_checkpoint(
-        graph2_variables, checkpoint_path)
-    self.assertItemsEqual(out_variables, graph1_variables)
+        graph2_variables, checkpoint_path, include_global_step=False)
+    self.assertItemsEqual(out_variables, [weight_variable])
 
   def test_return_variables_available_an_checkpoint_with_dict_inputs(self):
     checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
@@ -180,6 +183,31 @@ class GetVariablesAvailableInCheckpointTest(tf.test.TestCase):
     self.assertItemsEqual(out_variables.keys(), ['ckpt_weights'])
     self.assertTrue(out_variables['ckpt_weights'].op.name == 'weights')
 
+  def test_return_variables_with_correct_sizes(self):
+    checkpoint_path = os.path.join(self.get_temp_dir(), 'graph.pb')
+    bias_variable = tf.Variable(3.0, name='biases')
+    global_step = tf.train.get_or_create_global_step()
+    graph1_variables = [
+        tf.Variable([[1.0, 2.0], [3.0, 4.0]], name='weights'),
+        bias_variable,
+        global_step
+    ]
+    init_op = tf.global_variables_initializer()
+    saver = tf.train.Saver(graph1_variables)
+    with self.test_session() as sess:
+      sess.run(init_op)
+      saver.save(sess, checkpoint_path)
+
+    graph2_variables = [
+        tf.Variable([1.0, 2.0], name='weights'),  # Note the new variable shape.
+        bias_variable,
+        global_step
+    ]
+
+    out_variables = variables_helper.get_variables_available_in_checkpoint(
+        graph2_variables, checkpoint_path, include_global_step=True)
+    self.assertItemsEqual(out_variables, [bias_variable, global_step])
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/visualization_utils.py b/research/object_detection/utils/visualization_utils.py
index 1bce2ca4..6cfa63ae 100644
--- a/research/object_detection/utils/visualization_utils.py
+++ b/research/object_detection/utils/visualization_utils.py
@@ -21,7 +21,9 @@ The functions do not return a value, instead they modify the image itself.
 """
 import collections
 import functools
-import matplotlib.pyplot as plt
+# Set headless-friendly backend.
+import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements
+import matplotlib.pyplot as plt  # pylint: disable=g-import-not-at-top
 import numpy as np
 import PIL.Image as Image
 import PIL.ImageColor as ImageColor
@@ -30,6 +32,8 @@ import PIL.ImageFont as ImageFont
 import six
 import tensorflow as tf
 
+from object_detection.core import standard_fields as fields
+
 
 _TITLE_LEFT_MARGIN = 10
 _TITLE_TOP_MARGIN = 10
@@ -100,9 +104,12 @@ def draw_bounding_box_on_image_array(image,
                                      use_normalized_coordinates=True):
   """Adds a bounding box to an image (numpy array).
 
+  Bounding box coordinates can be specified in either absolute (pixel) or
+  normalized coordinates by setting the use_normalized_coordinates argument.
+
   Args:
     image: a numpy array with shape [height, width, 3].
-    ymin: ymin of bounding box in normalized coordinates (same below).
+    ymin: ymin of bounding box.
     xmin: xmin of bounding box.
     ymax: ymax of bounding box.
     xmax: xmax of bounding box.
@@ -132,6 +139,9 @@ def draw_bounding_box_on_image(image,
                                use_normalized_coordinates=True):
   """Adds a bounding box to an image.
 
+  Bounding box coordinates can be specified in either absolute (pixel) or
+  normalized coordinates by setting the use_normalized_coordinates argument.
+
   Each string in display_str_list is displayed on a separate line above the
   bounding box in black text on a rectangle filled with the input 'color'.
   If the top of the bounding box extends to the edge of the image, the strings
@@ -255,14 +265,58 @@ def draw_bounding_boxes_on_image(image,
                                boxes[i, 3], color, thickness, display_str_list)
 
 
+def _visualize_boxes(image, boxes, classes, scores, category_index, **kwargs):
+  return visualize_boxes_and_labels_on_image_array(
+      image, boxes, classes, scores, category_index=category_index, **kwargs)
+
+
+def _visualize_boxes_and_masks(image, boxes, classes, scores, masks,
+                               category_index, **kwargs):
+  return visualize_boxes_and_labels_on_image_array(
+      image,
+      boxes,
+      classes,
+      scores,
+      category_index=category_index,
+      instance_masks=masks,
+      **kwargs)
+
+
+def _visualize_boxes_and_keypoints(image, boxes, classes, scores, keypoints,
+                                   category_index, **kwargs):
+  return visualize_boxes_and_labels_on_image_array(
+      image,
+      boxes,
+      classes,
+      scores,
+      category_index=category_index,
+      keypoints=keypoints,
+      **kwargs)
+
+
+def _visualize_boxes_and_masks_and_keypoints(
+    image, boxes, classes, scores, masks, keypoints, category_index, **kwargs):
+  return visualize_boxes_and_labels_on_image_array(
+      image,
+      boxes,
+      classes,
+      scores,
+      category_index=category_index,
+      instance_masks=masks,
+      keypoints=keypoints,
+      **kwargs)
+
+
 def draw_bounding_boxes_on_image_tensors(images,
                                          boxes,
                                          classes,
                                          scores,
                                          category_index,
+                                         instance_masks=None,
+                                         keypoints=None,
                                          max_boxes_to_draw=20,
                                          min_score_thresh=0.2):
-  """Draws bounding boxes on batch of image tensors.
+  """Draws bounding boxes, masks, and keypoints on batch of image tensors.
 
   Args:
     images: A 4D uint8 image tensor of shape [N, H, W, C].
@@ -272,37 +326,123 @@ def draw_bounding_boxes_on_image_tensors(images,
     scores: [N, max_detections] float32 tensor of detection scores.
     category_index: a dict that maps integer ids to category dicts. e.g.
       {1: {1: 'dog'}, 2: {2: 'cat'}, ...}
+    instance_masks: A 4D uint8 tensor of shape [N, max_detection, H, W] with
+      instance masks.
+    keypoints: A 4D float32 tensor of shape [N, max_detection, num_keypoints, 2]
+      with keypoints.
     max_boxes_to_draw: Maximum number of boxes to draw on an image. Default 20.
     min_score_thresh: Minimum score threshold for visualization. Default 0.2.
 
   Returns:
     4D image tensor of type uint8, with boxes drawn on top.
   """
-  visualize_boxes_fn = functools.partial(
-      visualize_boxes_and_labels_on_image_array,
-      category_index=category_index,
-      instance_masks=None,
-      keypoints=None,
-      use_normalized_coordinates=True,
-      max_boxes_to_draw=max_boxes_to_draw,
-      min_score_thresh=min_score_thresh,
-      agnostic_mode=False,
-      line_thickness=4)
+  visualization_keyword_args = {
+      'use_normalized_coordinates': True,
+      'max_boxes_to_draw': max_boxes_to_draw,
+      'min_score_thresh': min_score_thresh,
+      'agnostic_mode': False,
+      'line_thickness': 4
+  }
+
+  if instance_masks is not None and keypoints is None:
+    visualize_boxes_fn = functools.partial(
+        _visualize_boxes_and_masks,
+        category_index=category_index,
+        **visualization_keyword_args)
+    elems = [images, boxes, classes, scores, instance_masks]
+  elif instance_masks is None and keypoints is not None:
+    visualize_boxes_fn = functools.partial(
+        _visualize_boxes_and_keypoints,
+        category_index=category_index,
+        **visualization_keyword_args)
+    elems = [images, boxes, classes, scores, keypoints]
+  elif instance_masks is not None and keypoints is not None:
+    visualize_boxes_fn = functools.partial(
+        _visualize_boxes_and_masks_and_keypoints,
+        category_index=category_index,
+        **visualization_keyword_args)
+    elems = [images, boxes, classes, scores, instance_masks, keypoints]
+  else:
+    visualize_boxes_fn = functools.partial(
+        _visualize_boxes,
+        category_index=category_index,
+        **visualization_keyword_args)
+    elems = [images, boxes, classes, scores]
 
-  def draw_boxes(image_boxes_classes_scores):
+  def draw_boxes(image_and_detections):
     """Draws boxes on image."""
-    (image, boxes, classes, scores) = image_boxes_classes_scores
-    image_with_boxes = tf.py_func(visualize_boxes_fn,
-                                  [image, boxes, classes, scores], tf.uint8)
+    image_with_boxes = tf.py_func(visualize_boxes_fn, image_and_detections,
+                                  tf.uint8)
     return image_with_boxes
 
-  images = tf.map_fn(
-      draw_boxes, (images, boxes, classes, scores),
-      dtype=tf.uint8,
-      back_prop=False)
+  images = tf.map_fn(draw_boxes, elems, dtype=tf.uint8, back_prop=False)
   return images
 
 
+def draw_side_by_side_evaluation_image(eval_dict,
+                                       category_index,
+                                       max_boxes_to_draw=20,
+                                       min_score_thresh=0.2):
+  """Creates a side-by-side image with detections and groundtruth.
+
+  Bounding boxes (and instance masks, if available) are visualized on both
+  subimages.
+
+  Args:
+    eval_dict: The evaluation dictionary returned by
+      eval_util.result_dict_for_single_example().
+    category_index: A category index (dictionary) produced from a labelmap.
+    max_boxes_to_draw: The maximum number of boxes to draw for detections.
+    min_score_thresh: The minimum score threshold for showing detections.
+
+  Returns:
+    A [1, H, 2 * W, C] uint8 tensor. The subimage on the left corresponds to
+      detections, while the subimage on the right corresponds to groundtruth.
+  """
+  detection_fields = fields.DetectionResultFields()
+  input_data_fields = fields.InputDataFields()
+  instance_masks = None
+  if detection_fields.detection_masks in eval_dict:
+    instance_masks = tf.cast(
+        tf.expand_dims(eval_dict[detection_fields.detection_masks], axis=0),
+        tf.uint8)
+  keypoints = None
+  if detection_fields.detection_keypoints in eval_dict:
+    keypoints = tf.expand_dims(
+        eval_dict[detection_fields.detection_keypoints], axis=0)
+  groundtruth_instance_masks = None
+  if input_data_fields.groundtruth_instance_masks in eval_dict:
+    groundtruth_instance_masks = tf.cast(
+        tf.expand_dims(
+            eval_dict[input_data_fields.groundtruth_instance_masks], axis=0),
+        tf.uint8)
+  images_with_detections = draw_bounding_boxes_on_image_tensors(
+      eval_dict[input_data_fields.original_image],
+      tf.expand_dims(eval_dict[detection_fields.detection_boxes], axis=0),
+      tf.expand_dims(eval_dict[detection_fields.detection_classes], axis=0),
+      tf.expand_dims(eval_dict[detection_fields.detection_scores], axis=0),
+      category_index,
+      instance_masks=instance_masks,
+      keypoints=keypoints,
+      max_boxes_to_draw=max_boxes_to_draw,
+      min_score_thresh=min_score_thresh)
+  images_with_groundtruth = draw_bounding_boxes_on_image_tensors(
+      eval_dict[input_data_fields.original_image],
+      tf.expand_dims(eval_dict[input_data_fields.groundtruth_boxes], axis=0),
+      tf.expand_dims(eval_dict[input_data_fields.groundtruth_classes], axis=0),
+      tf.expand_dims(
+          tf.ones_like(
+              eval_dict[input_data_fields.groundtruth_classes],
+              dtype=tf.float32),
+          axis=0),
+      category_index,
+      instance_masks=groundtruth_instance_masks,
+      keypoints=None,
+      max_boxes_to_draw=None,
+      min_score_thresh=0.0)
+  return tf.concat([images_with_detections, images_with_groundtruth], axis=2)
+
+
 def draw_keypoints_on_image_array(image,
                                   keypoints,
                                   color='red',
@@ -352,7 +492,7 @@ def draw_keypoints_on_image(image,
                  outline=color, fill=color)
 
 
-def draw_mask_on_image_array(image, mask, color='red', alpha=0.7):
+def draw_mask_on_image_array(image, mask, color='red', alpha=0.4):
   """Draws mask on an image.
 
   Args:
@@ -360,7 +500,7 @@ def draw_mask_on_image_array(image, mask, color='red', alpha=0.7):
     mask: a uint8 numpy array of shape (img_height, img_height) with
       values between either 0 or 1.
     color: color to draw the keypoints with. Default is red.
-    alpha: transparency value between 0 and 1. (default: 0.7)
+    alpha: transparency value between 0 and 1. (default: 0.4)
 
   Raises:
     ValueError: On incorrect data type for image or masks.
@@ -371,6 +511,9 @@ def draw_mask_on_image_array(image, mask, color='red', alpha=0.7):
     raise ValueError('`mask` not of type np.uint8')
   if np.any(np.logical_and(mask != 1, mask != 0)):
     raise ValueError('`mask` elements should be in [0, 1]')
+  if image.shape[:2] != mask.shape:
+    raise ValueError('The image has spatial dimensions %s but the mask has '
+                     'dimensions %s' % (image.shape[:2], mask.shape))
   rgb = ImageColor.getrgb(color)
   pil_image = Image.fromarray(image)
 
@@ -382,18 +525,23 @@ def draw_mask_on_image_array(image, mask, color='red', alpha=0.7):
   np.copyto(image, np.array(pil_image.convert('RGB')))
 
 
-def visualize_boxes_and_labels_on_image_array(image,
-                                              boxes,
-                                              classes,
-                                              scores,
-                                              category_index,
-                                              instance_masks=None,
-                                              keypoints=None,
-                                              use_normalized_coordinates=False,
-                                              max_boxes_to_draw=20,
-                                              min_score_thresh=.5,
-                                              agnostic_mode=False,
-                                              line_thickness=4):
+def visualize_boxes_and_labels_on_image_array(
+    image,
+    boxes,
+    classes,
+    scores,
+    category_index,
+    instance_masks=None,
+    instance_boundaries=None,
+    keypoints=None,
+    use_normalized_coordinates=False,
+    max_boxes_to_draw=20,
+    min_score_thresh=.5,
+    agnostic_mode=False,
+    line_thickness=4,
+    groundtruth_box_visualization_color='black',
+    skip_scores=False,
+    skip_labels=False):
   """Overlay labeled boxes on an image with formatted scores and label names.
 
   This function groups boxes that correspond to the same location
@@ -411,8 +559,10 @@ def visualize_boxes_and_labels_on_image_array(image,
       boxes and plot all boxes as black with no classes or scores.
     category_index: a dict containing category dictionaries (each holding
       category index `id` and category name `name`) keyed by category indices.
-    instance_masks: a numpy array of shape [N, image_height, image_width], can
-      be None
+    instance_masks: a numpy array of shape [N, image_height, image_width] with
+      values ranging between 0 and 1, can be None.
+    instance_boundaries: a numpy array of shape [N, image_height, image_width]
+      with values ranging between 0 and 1, can be None.
     keypoints: a numpy array of shape [N, num_keypoints, 2], can
       be None
     use_normalized_coordinates: whether boxes is to be interpreted as
@@ -424,6 +574,10 @@ def visualize_boxes_and_labels_on_image_array(image,
       class-agnostic mode or not.  This mode will display scores but ignore
       classes.
     line_thickness: integer (default: 4) controlling line width of the boxes.
+    groundtruth_box_visualization_color: box color for visualizing groundtruth
+      boxes
+    skip_scores: whether to skip score when drawing a single detection
+    skip_labels: whether to skip label when drawing a single detection
 
   Returns:
     uint8 numpy array with shape (img_height, img_width, 3) with overlaid boxes.
@@ -433,6 +587,7 @@ def visualize_boxes_and_labels_on_image_array(image,
   box_to_display_str_map = collections.defaultdict(list)
   box_to_color_map = collections.defaultdict(str)
   box_to_instance_masks_map = {}
+  box_to_instance_boundaries_map = {}
   box_to_keypoints_map = collections.defaultdict(list)
   if not max_boxes_to_draw:
     max_boxes_to_draw = boxes.shape[0]
@@ -441,21 +596,26 @@ def visualize_boxes_and_labels_on_image_array(image,
       box = tuple(boxes[i].tolist())
       if instance_masks is not None:
         box_to_instance_masks_map[box] = instance_masks[i]
+      if instance_boundaries is not None:
+        box_to_instance_boundaries_map[box] = instance_boundaries[i]
       if keypoints is not None:
         box_to_keypoints_map[box].extend(keypoints[i])
       if scores is None:
-        box_to_color_map[box] = 'black'
+        box_to_color_map[box] = groundtruth_box_visualization_color
       else:
-        if not agnostic_mode:
-          if classes[i] in category_index.keys():
-            class_name = category_index[classes[i]]['name']
+        display_str = ''
+        if not skip_labels:
+          if not agnostic_mode:
+            if classes[i] in category_index.keys():
+              class_name = category_index[classes[i]]['name']
+            else:
+              class_name = 'N/A'
+            display_str = str(class_name)
+        if not skip_scores:
+          if not display_str:
+            display_str = '{}%'.format(int(100*scores[i]))
           else:
-            class_name = 'N/A'
-          display_str = '{}: {}%'.format(
-              class_name,
-              int(100*scores[i]))
-        else:
-          display_str = 'score: {}%'.format(int(100 * scores[i]))
+            display_str = '{}: {}%'.format(display_str, int(100*scores[i]))
         box_to_display_str_map[box].append(display_str)
         if agnostic_mode:
           box_to_color_map[box] = 'DarkOrange'
@@ -472,6 +632,13 @@ def visualize_boxes_and_labels_on_image_array(image,
           box_to_instance_masks_map[box],
           color=color
       )
+    if instance_boundaries is not None:
+      draw_mask_on_image_array(
+          image,
+          box_to_instance_boundaries_map[box],
+          color='red',
+          alpha=1.0
+      )
     draw_bounding_box_on_image_array(
         image,
         ymin,
@@ -518,7 +685,7 @@ def add_cdf_image_summary(values, name):
     fig.canvas.draw()
     width, height = fig.get_size_inches() * fig.get_dpi()
     image = np.fromstring(fig.canvas.tostring_rgb(), dtype='uint8').reshape(
-        1, height, width, 3)
+        1, int(height), int(width), 3)
     return image
   cdf_plot = tf.py_func(cdf_plot, [values], tf.uint8)
   tf.summary.image(name, cdf_plot)
diff --git a/research/object_detection/utils/visualization_utils_test.py b/research/object_detection/utils/visualization_utils_test.py
index 8a1041fc..dffe1cd6 100644
--- a/research/object_detection/utils/visualization_utils_test.py
+++ b/research/object_detection/utils/visualization_utils_test.py
@@ -145,7 +145,7 @@ class VisualizationUtilsTest(tf.test.TestCase):
         for i in range(images_with_boxes_np.shape[0]):
           img_name = 'image_' + str(i) + '.png'
           output_file = os.path.join(self.get_temp_dir(), img_name)
-          print('Writing output image %d to %s' % (i, output_file))
+          print 'Writing output image %d to %s' % (i, output_file)
           image_pil = Image.fromarray(images_with_boxes_np[i, ...])
           image_pil.save(output_file)
 
