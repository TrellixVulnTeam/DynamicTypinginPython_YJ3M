commit 02a9969e94feb51966f9bacddc1836d811f8ce69
Author: pkulzc <lzc@google.com>
Date:   Wed Aug 1 11:13:43 2018 -0700

    Refactor object detection box predictors and fix some issues with model_main. (#4965)
    
    * Merged commit includes the following changes:
    206852642  by Zhichao Lu:
    
        Build the balanced_positive_negative_sampler in the model builder for FasterRCNN. Also adds an option to use the static implementation of the sampler.
    
    --
    206803260  by Zhichao Lu:
    
        Fixes a misplaced argument in resnet fpn feature extractor.
    
    --
    206682736  by Zhichao Lu:
    
        This CL modifies the SSD meta architecture to support both Slim-based and Keras-based box predictors, and begins preparation for Keras box predictor support in the other meta architectures.
    
        Concretely, this CL adds a new `KerasBoxPredictor` base class and makes the meta architectures appropriately call whichever box predictors they are using.
    
        We can switch the non-ssd meta architectures to fully support Keras box predictors once the Keras Convolutional Box Predictor CL is submitted.
    
    --
    206669634  by Zhichao Lu:
    
        Adds an alternate method for balanced positive negative sampler using static shapes.
    
    --
    206643278  by Zhichao Lu:
    
        This CL adds a Keras layer hyperparameter configuration object to the hyperparams_builder.
    
        It automatically converts from Slim layer hyperparameter configs to Keras layer hyperparameters. Namely, it:
        - Builds Keras initializers/regularizers instead of Slim ones
        - sets weights_regularizer/initializer to kernel_regularizer/initializer
        - converts batchnorm decay to momentum
        - converts Slim l2 regularizer weights to the equivalent Keras l2 weights
    
        This will be used in the conversion of object detection feature extractors & box predictors to newer Tensorflow APIs.
    
    --
    206611681  by Zhichao Lu:
    
        Internal changes.
    
    --
    206591619  by Zhichao Lu:
    
        Clip the to shape when the input tensors are larger than the expected padded static shape
    
    --
    206517644  by Zhichao Lu:
    
        Make MultiscaleGridAnchorGenerator more consistent with MultipleGridAnchorGenerator.
    
    --
    206415624  by Zhichao Lu:
    
        Make the hardcoded feature pyramid network (FPN) levels configurable for both SSD
        Resnet and SSD Mobilenet.
    
    --
    206398204  by Zhichao Lu:
    
        This CL modifies the SSD meta architecture to support both Slim-based and Keras-based feature extractors.
    
        This allows us to begin the conversion of object detection to newer Tensorflow APIs.
    
    --
    206213448  by Zhichao Lu:
    
        Adding a method to compute the expected classification loss by background/foreground weighting.
    
    --
    206204232  by Zhichao Lu:
    
        Adding the keypoint head to the Mask RCNN pipeline.
    
    --
    206200352  by Zhichao Lu:
    
        - Create Faster R-CNN target assigner in the model builder. This allows configuring matchers in Target assigner to use TPU compatible ops (tf.gather in this case) without any change in meta architecture.
        - As a +ve side effect of the refactoring, we can now re-use a single target assigner for all of second stage heads in Faster R-CNN.
    
    --
    206178206  by Zhichao Lu:
    
        Force ssd feature extractor builder to use keyword arguments so values won't be passed to wrong arguments.
    
    --
    206168297  by Zhichao Lu:
    
        Updating exporter to use freeze_graph.freeze_graph_with_def_protos rather than a homegrown version.
    
    --
    206080748  by Zhichao Lu:
    
        Merge external contributions.
    
    --
    206074460  by Zhichao Lu:
    
        Update to preprocessor to apply temperature and softmax to the multiclass scores on read.
    
    --
    205960802  by Zhichao Lu:
    
        Fixing a bug in hierarchical label expansion script.
    
    --
    205944686  by Zhichao Lu:
    
        Update exporter to support exporting quantized model.
    
    --
    205912529  by Zhichao Lu:
    
        Add a two stage matcher to allow for thresholding by one criteria and then argmaxing on the other.
    
    --
    205909017  by Zhichao Lu:
    
        Add test for grayscale image_resizer
    
    --
    205892801  by Zhichao Lu:
    
        Add flag to decide whether to apply batch norm to conv layers of weight shared box predictor.
    
    --
    205824449  by Zhichao Lu:
    
        make sure that by default mask rcnn box predictor predicts 2 stages.
    
    --
    205730139  by Zhichao Lu:
    
        Updating warning message to be more explicit about variable size mismatch.
    
    --
    205696992  by Zhichao Lu:
    
        Remove utils/ops.py's dependency on core/box_list_ops.py. This will allow re-using TPU compatible ops from utils/ops.py in core/box_list_ops.py.
    
    --
    205696867  by Zhichao Lu:
    
        Refactoring mask rcnn predictor so have each head in a separate file.
        This CL lets us to add new heads more easily in the future to mask rcnn.
    
    --
    205492073  by Zhichao Lu:
    
        Refactor R-FCN box predictor to be TPU compliant.
    
        - Change utils/ops.py:position_sensitive_crop_regions to operate on single image and set of boxes without `box_ind`
        - Add a batch version that operations on batches of images and batches of boxes.
        - Refactor R-FCN box predictor to use the batched version of position sensitive crop regions.
    
    --
    205453567  by Zhichao Lu:
    
        Fix bug that cannot export inference graph when write_inference_graph flag is True.
    
    --
    205316039  by Zhichao Lu:
    
        Changing input tensor name.
    
    --
    205256307  by Zhichao Lu:
    
        Fix model zoo links for quantized model.
    
    --
    205164432  by Zhichao Lu:
    
        Fixes eval error when label map contains non-ascii characters.
    
    --
    205129842  by Zhichao Lu:
    
        Adds a option to clip the anchors to the window size without filtering the overlapped boxes in Faster-RCNN
    
    --
    205094863  by Zhichao Lu:
    
        Update to label map util to allow the option of adding a background class and fill in gaps in the label map. Useful for using multiclass scores which require a complete label map with explicit background label.
    
    --
    204989032  by Zhichao Lu:
    
        Add tf.prof support to exporter.
    
    --
    204825267  by Zhichao Lu:
    
        Modify mask rcnn box predictor tests for TPU compatibility.
    
    --
    204778749  by Zhichao Lu:
    
        Remove score filtering from postprocessing.py and rely on filtering logic in tf.image.non_max_suppression
    
    --
    204775818  by Zhichao Lu:
    
        Python3 fixes for object_detection.
    
    --
    204745920  by Zhichao Lu:
    
        Object Detection Dataset visualization tool (documentation).
    
    --
    204686993  by Zhichao Lu:
    
        Internal changes.
    
    --
    204559667  by Zhichao Lu:
    
        Refactor box_predictor.py into multiple files.
        The abstract base class remains in the object_detection/core, The other classes have moved to a separate file each in object_detection/predictors
    
    --
    204552847  by Zhichao Lu:
    
        Update blog post link.
    
    --
    204508028  by Zhichao Lu:
    
        Bump down the batch size to 1024 to be a bit more tolerant to OOM and double the number of iterations. This job still converges to 20.5 mAP in 3 hours.
    
    --
    
    PiperOrigin-RevId: 206852642
    
    * Add original post-processing back.

diff --git a/research/object_detection/README.md b/research/object_detection/README.md
index b5a93d53..68eb1b1f 100644
--- a/research/object_detection/README.md
+++ b/research/object_detection/README.md
@@ -79,7 +79,7 @@ Extras:
       Run the evaluation for the Open Images Challenge 2018</a><br>
   * <a href='g3doc/tpu_compatibility.md'>
       TPU compatible detection pipelines</a><br>
-  *  <a href='g3doc/running_on_mobile_tensorflowlite.md'>
+  * <a href='g3doc/running_on_mobile_tensorflowlite.md'>
       Running object detection on mobile devices with TensorFlow Lite</a><br>
 
 ## Getting Help
diff --git a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
index bd785c17..b870adce 100644
--- a/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiple_grid_anchor_generator.py
@@ -157,12 +157,10 @@ class MultipleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         correspond to an 8x8 layer followed by a 7x7 layer.
       im_height: the height of the image to generate the grid for. If both
         im_height and im_width are 1, the generated anchors default to
-        normalized coordinates, otherwise absolute coordinates are used for the
-        grid.
+        absolute coordinates, otherwise normalized coordinates are produced.
       im_width: the width of the image to generate the grid for. If both
         im_height and im_width are 1, the generated anchors default to
-        normalized coordinates, otherwise absolute coordinates are used for the
-        grid.
+        absolute coordinates, otherwise normalized coordinates are produced.
 
     Returns:
       boxes_list: a list of BoxLists each holding anchor boxes corresponding to
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
index a8d227c7..807a2ae1 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator.py
@@ -57,14 +57,12 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
     self._scales_per_octave = scales_per_octave
     self._normalize_coordinates = normalize_coordinates
 
+    scales = [2**(float(scale) / scales_per_octave)
+              for scale in xrange(scales_per_octave)]
+    aspects = list(aspect_ratios)
+
     for level in range(min_level, max_level + 1):
       anchor_stride = [2**level, 2**level]
-      scales = []
-      aspects = []
-      for scale in range(scales_per_octave):
-        scales.append(2**(float(scale) / scales_per_octave))
-      for aspect_ratio in aspect_ratios:
-        aspects.append(aspect_ratio)
       base_anchor_size = [2**level * anchor_scale, 2**level * anchor_scale]
       self._anchor_grid_info.append({
           'level': level,
@@ -84,7 +82,7 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
     return len(self._anchor_grid_info) * [
         len(self._aspect_ratios) * self._scales_per_octave]
 
-  def _generate(self, feature_map_shape_list, im_height, im_width):
+  def _generate(self, feature_map_shape_list, im_height=1, im_width=1):
     """Generates a collection of bounding boxes to be used as anchors.
 
     Currently we require the input image shape to be statically defined.  That
@@ -95,14 +93,20 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
         format [(height_0, width_0), (height_1, width_1), ...]. For example,
         setting feature_map_shape_list=[(8, 8), (7, 7)] asks for anchors that
         correspond to an 8x8 layer followed by a 7x7 layer.
-      im_height: the height of the image to generate the grid for.
-      im_width: the width of the image to generate the grid for.
+      im_height: the height of the image to generate the grid for. If both
+        im_height and im_width are 1, anchors can only be generated in
+        absolute coordinates.
+      im_width: the width of the image to generate the grid for. If both
+        im_height and im_width are 1, anchors can only be generated in
+        absolute coordinates.
 
     Returns:
       boxes_list: a list of BoxLists each holding anchor boxes corresponding to
         the input feature map shapes.
     Raises:
       ValueError: if im_height and im_width are not integers.
+      ValueError: if im_height and im_width are 1, but normalized coordinates
+        were requested.
     """
     if not isinstance(im_height, int) or not isinstance(im_width, int):
       raise ValueError('MultiscaleGridAnchorGenerator currently requires '
@@ -118,9 +122,9 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
       feat_h = feat_shape[0]
       feat_w = feat_shape[1]
       anchor_offset = [0, 0]
-      if im_height % 2.0**level == 0:
+      if im_height % 2.0**level == 0 or im_height == 1:
         anchor_offset[0] = stride / 2.0
-      if im_width % 2.0**level == 0:
+      if im_width % 2.0**level == 0 or im_width == 1:
         anchor_offset[1] = stride / 2.0
       ag = grid_anchor_generator.GridAnchorGenerator(
           scales,
@@ -131,6 +135,11 @@ class MultiscaleGridAnchorGenerator(anchor_generator.AnchorGenerator):
       (anchor_grid,) = ag.generate(feature_map_shape_list=[(feat_h, feat_w)])
 
       if self._normalize_coordinates:
+        if im_height == 1 or im_width == 1:
+          raise ValueError(
+              'Normalized coordinates were requested upon construction of the '
+              'MultiscaleGridAnchorGenerator, but a subsequent call to '
+              'generate did not supply dimension information.')
         anchor_grid = box_list_ops.to_normalized_coordinates(
             anchor_grid, im_height, im_width, check_range=False)
       anchor_grid_list.append(anchor_grid)
diff --git a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
index c96bdae7..ed5c90ce 100644
--- a/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
+++ b/research/object_detection/anchor_generators/multiscale_grid_anchor_generator_test.py
@@ -47,6 +47,40 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
       anchor_corners_out = anchor_corners.eval()
       self.assertAllClose(anchor_corners_out, exp_anchor_corners)
 
+  def test_construct_single_anchor_unit_dimensions(self):
+    min_level = 5
+    max_level = 5
+    anchor_scale = 1.0
+    aspect_ratios = [1.0]
+    scales_per_octave = 1
+    im_height = 1
+    im_width = 1
+    feature_map_shape_list = [(2, 2)]
+    # Positive offsets are produced.
+    exp_anchor_corners = [[0, 0, 32, 32],
+                          [0, 32, 32, 64],
+                          [32, 0, 64, 32],
+                          [32, 32, 64, 64]]
+
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
+        normalize_coordinates=False)
+    anchors_list = anchor_generator.generate(
+        feature_map_shape_list, im_height=im_height, im_width=im_width)
+    anchor_corners = anchors_list[0].get()
+
+    with self.test_session():
+      anchor_corners_out = anchor_corners.eval()
+      self.assertAllClose(anchor_corners_out, exp_anchor_corners)
+
+  def test_construct_normalized_anchors_fails_with_unit_dimensions(self):
+    anchor_generator = mg.MultiscaleGridAnchorGenerator(
+        min_level=5, max_level=5, anchor_scale=1.0, aspect_ratios=[1.0],
+        scales_per_octave=1, normalize_coordinates=True)
+    with self.assertRaisesRegexp(ValueError, 'Normalized coordinates'):
+      anchor_generator.generate(
+          feature_map_shape_list=[(2, 2)], im_height=1, im_width=1)
+
   def test_construct_single_anchor_in_normalized_coordinates(self):
     min_level = 5
     max_level = 5
@@ -94,7 +128,7 @@ class MultiscaleGridAnchorGeneratorTest(test_case.TestCase):
     anchor_generator = mg.MultiscaleGridAnchorGenerator(
         min_level, max_level, anchor_scale, aspect_ratios, scales_per_octave,
         normalize_coordinates=False)
-    with self.assertRaises(ValueError):
+    with self.assertRaisesRegexp(ValueError, 'statically defined'):
       anchor_generator.generate(
           feature_map_shape_list, im_height=im_height, im_width=im_width)
 
diff --git a/research/object_detection/builders/box_predictor_builder.py b/research/object_detection/builders/box_predictor_builder.py
index 9ac14487..ec5b843a 100644
--- a/research/object_detection/builders/box_predictor_builder.py
+++ b/research/object_detection/builders/box_predictor_builder.py
@@ -15,7 +15,12 @@
 
 """Function to build box predictor from configuration."""
 
-from object_detection.core import box_predictor
+from object_detection.predictors import convolutional_box_predictor
+from object_detection.predictors import mask_rcnn_box_predictor
+from object_detection.predictors import rfcn_box_predictor
+from object_detection.predictors.mask_rcnn_heads import box_head
+from object_detection.predictors.mask_rcnn_heads import class_head
+from object_detection.predictors.mask_rcnn_heads import mask_head
 from object_detection.protos import box_predictor_pb2
 
 
@@ -48,92 +53,112 @@ def build(argscope_fn, box_predictor_config, is_training, num_classes):
   box_predictor_oneof = box_predictor_config.WhichOneof('box_predictor_oneof')
 
   if  box_predictor_oneof == 'convolutional_box_predictor':
-    conv_box_predictor = box_predictor_config.convolutional_box_predictor
-    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,
+    config_box_predictor = box_predictor_config.convolutional_box_predictor
+    conv_hyperparams_fn = argscope_fn(config_box_predictor.conv_hyperparams,
                                       is_training)
-    box_predictor_object = box_predictor.ConvolutionalBoxPredictor(
-        is_training=is_training,
-        num_classes=num_classes,
-        conv_hyperparams_fn=conv_hyperparams_fn,
-        min_depth=conv_box_predictor.min_depth,
-        max_depth=conv_box_predictor.max_depth,
-        num_layers_before_predictor=(conv_box_predictor.
-                                     num_layers_before_predictor),
-        use_dropout=conv_box_predictor.use_dropout,
-        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,
-        kernel_size=conv_box_predictor.kernel_size,
-        box_code_size=conv_box_predictor.box_code_size,
-        apply_sigmoid_to_scores=conv_box_predictor.apply_sigmoid_to_scores,
-        class_prediction_bias_init=(conv_box_predictor.
-                                    class_prediction_bias_init),
-        use_depthwise=conv_box_predictor.use_depthwise
-    )
+    box_predictor_object = (
+        convolutional_box_predictor.ConvolutionalBoxPredictor(
+            is_training=is_training,
+            num_classes=num_classes,
+            conv_hyperparams_fn=conv_hyperparams_fn,
+            min_depth=config_box_predictor.min_depth,
+            max_depth=config_box_predictor.max_depth,
+            num_layers_before_predictor=(
+                config_box_predictor.num_layers_before_predictor),
+            use_dropout=config_box_predictor.use_dropout,
+            dropout_keep_prob=config_box_predictor.dropout_keep_probability,
+            kernel_size=config_box_predictor.kernel_size,
+            box_code_size=config_box_predictor.box_code_size,
+            apply_sigmoid_to_scores=config_box_predictor.
+            apply_sigmoid_to_scores,
+            class_prediction_bias_init=(
+                config_box_predictor.class_prediction_bias_init),
+            use_depthwise=config_box_predictor.use_depthwise))
     return box_predictor_object
 
   if  box_predictor_oneof == 'weight_shared_convolutional_box_predictor':
-    conv_box_predictor = (box_predictor_config.
-                          weight_shared_convolutional_box_predictor)
-    conv_hyperparams_fn = argscope_fn(conv_box_predictor.conv_hyperparams,
+    config_box_predictor = (
+        box_predictor_config.weight_shared_convolutional_box_predictor)
+    conv_hyperparams_fn = argscope_fn(config_box_predictor.conv_hyperparams,
                                       is_training)
-    box_predictor_object = box_predictor.WeightSharedConvolutionalBoxPredictor(
-        is_training=is_training,
-        num_classes=num_classes,
-        conv_hyperparams_fn=conv_hyperparams_fn,
-        depth=conv_box_predictor.depth,
-        num_layers_before_predictor=(
-            conv_box_predictor.num_layers_before_predictor),
-        kernel_size=conv_box_predictor.kernel_size,
-        box_code_size=conv_box_predictor.box_code_size,
-        class_prediction_bias_init=conv_box_predictor.
-        class_prediction_bias_init,
-        use_dropout=conv_box_predictor.use_dropout,
-        dropout_keep_prob=conv_box_predictor.dropout_keep_probability,
-        share_prediction_tower=conv_box_predictor.share_prediction_tower)
+    apply_batch_norm = config_box_predictor.conv_hyperparams.HasField(
+        'batch_norm')
+    box_predictor_object = (
+        convolutional_box_predictor.WeightSharedConvolutionalBoxPredictor(
+            is_training=is_training,
+            num_classes=num_classes,
+            conv_hyperparams_fn=conv_hyperparams_fn,
+            depth=config_box_predictor.depth,
+            num_layers_before_predictor=(
+                config_box_predictor.num_layers_before_predictor),
+            kernel_size=config_box_predictor.kernel_size,
+            box_code_size=config_box_predictor.box_code_size,
+            class_prediction_bias_init=config_box_predictor.
+            class_prediction_bias_init,
+            use_dropout=config_box_predictor.use_dropout,
+            dropout_keep_prob=config_box_predictor.dropout_keep_probability,
+            share_prediction_tower=config_box_predictor.share_prediction_tower,
+            apply_batch_norm=apply_batch_norm))
     return box_predictor_object
 
   if box_predictor_oneof == 'mask_rcnn_box_predictor':
-    mask_rcnn_box_predictor = box_predictor_config.mask_rcnn_box_predictor
-    fc_hyperparams_fn = argscope_fn(mask_rcnn_box_predictor.fc_hyperparams,
+    config_box_predictor = box_predictor_config.mask_rcnn_box_predictor
+    fc_hyperparams_fn = argscope_fn(config_box_predictor.fc_hyperparams,
                                     is_training)
     conv_hyperparams_fn = None
-    if mask_rcnn_box_predictor.HasField('conv_hyperparams'):
+    if config_box_predictor.HasField('conv_hyperparams'):
       conv_hyperparams_fn = argscope_fn(
-          mask_rcnn_box_predictor.conv_hyperparams, is_training)
-    box_predictor_object = box_predictor.MaskRCNNBoxPredictor(
+          config_box_predictor.conv_hyperparams, is_training)
+    box_prediction_head = box_head.BoxHead(
         is_training=is_training,
         num_classes=num_classes,
         fc_hyperparams_fn=fc_hyperparams_fn,
-        use_dropout=mask_rcnn_box_predictor.use_dropout,
-        dropout_keep_prob=mask_rcnn_box_predictor.dropout_keep_probability,
-        box_code_size=mask_rcnn_box_predictor.box_code_size,
-        conv_hyperparams_fn=conv_hyperparams_fn,
-        predict_instance_masks=mask_rcnn_box_predictor.predict_instance_masks,
-        mask_height=mask_rcnn_box_predictor.mask_height,
-        mask_width=mask_rcnn_box_predictor.mask_width,
-        mask_prediction_num_conv_layers=(
-            mask_rcnn_box_predictor.mask_prediction_num_conv_layers),
-        mask_prediction_conv_depth=(
-            mask_rcnn_box_predictor.mask_prediction_conv_depth),
-        masks_are_class_agnostic=(
-            mask_rcnn_box_predictor.masks_are_class_agnostic),
-        predict_keypoints=mask_rcnn_box_predictor.predict_keypoints,
+        use_dropout=config_box_predictor.use_dropout,
+        dropout_keep_prob=config_box_predictor.dropout_keep_probability,
+        box_code_size=config_box_predictor.box_code_size,
         share_box_across_classes=(
-            mask_rcnn_box_predictor.share_box_across_classes))
+            config_box_predictor.share_box_across_classes))
+    class_prediction_head = class_head.ClassHead(
+        is_training=is_training,
+        num_classes=num_classes,
+        fc_hyperparams_fn=fc_hyperparams_fn,
+        use_dropout=config_box_predictor.use_dropout,
+        dropout_keep_prob=config_box_predictor.dropout_keep_probability)
+    third_stage_heads = {}
+    if config_box_predictor.predict_instance_masks:
+      third_stage_heads[
+          mask_rcnn_box_predictor.MASK_PREDICTIONS] = mask_head.MaskHead(
+              num_classes=num_classes,
+              conv_hyperparams_fn=conv_hyperparams_fn,
+              mask_height=config_box_predictor.mask_height,
+              mask_width=config_box_predictor.mask_width,
+              mask_prediction_num_conv_layers=(
+                  config_box_predictor.mask_prediction_num_conv_layers),
+              mask_prediction_conv_depth=(
+                  config_box_predictor.mask_prediction_conv_depth),
+              masks_are_class_agnostic=(
+                  config_box_predictor.masks_are_class_agnostic))
+    box_predictor_object = mask_rcnn_box_predictor.MaskRCNNBoxPredictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        box_prediction_head=box_prediction_head,
+        class_prediction_head=class_prediction_head,
+        third_stage_heads=third_stage_heads)
     return box_predictor_object
 
   if box_predictor_oneof == 'rfcn_box_predictor':
-    rfcn_box_predictor = box_predictor_config.rfcn_box_predictor
-    conv_hyperparams_fn = argscope_fn(rfcn_box_predictor.conv_hyperparams,
+    config_box_predictor = box_predictor_config.rfcn_box_predictor
+    conv_hyperparams_fn = argscope_fn(config_box_predictor.conv_hyperparams,
                                       is_training)
-    box_predictor_object = box_predictor.RfcnBoxPredictor(
+    box_predictor_object = rfcn_box_predictor.RfcnBoxPredictor(
         is_training=is_training,
         num_classes=num_classes,
         conv_hyperparams_fn=conv_hyperparams_fn,
-        crop_size=[rfcn_box_predictor.crop_height,
-                   rfcn_box_predictor.crop_width],
-        num_spatial_bins=[rfcn_box_predictor.num_spatial_bins_height,
-                          rfcn_box_predictor.num_spatial_bins_width],
-        depth=rfcn_box_predictor.depth,
-        box_code_size=rfcn_box_predictor.box_code_size)
+        crop_size=[config_box_predictor.crop_height,
+                   config_box_predictor.crop_width],
+        num_spatial_bins=[config_box_predictor.num_spatial_bins_height,
+                          config_box_predictor.num_spatial_bins_width],
+        depth=config_box_predictor.depth,
+        box_code_size=config_box_predictor.box_code_size)
     return box_predictor_object
   raise ValueError('Unknown box predictor: {}'.format(box_predictor_oneof))
diff --git a/research/object_detection/builders/box_predictor_builder_test.py b/research/object_detection/builders/box_predictor_builder_test.py
index 35ad57be..1ef00357 100644
--- a/research/object_detection/builders/box_predictor_builder_test.py
+++ b/research/object_detection/builders/box_predictor_builder_test.py
@@ -20,6 +20,7 @@ import tensorflow as tf
 from google.protobuf import text_format
 from object_detection.builders import box_predictor_builder
 from object_detection.builders import hyperparams_builder
+from object_detection.predictors import mask_rcnn_box_predictor
 from object_detection.protos import box_predictor_pb2
 from object_detection.protos import hyperparams_pb2
 
@@ -239,6 +240,7 @@ class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertAlmostEqual(box_predictor._class_prediction_bias_init, 4.0)
     self.assertEqual(box_predictor.num_classes, 10)
     self.assertFalse(box_predictor._is_training)
+    self.assertEqual(box_predictor._apply_batch_norm, False)
 
   def test_construct_default_conv_box_predictor(self):
     box_predictor_text_proto = """
@@ -265,6 +267,37 @@ class WeightSharedConvolutionalBoxPredictorBuilderTest(tf.test.TestCase):
     self.assertEqual(box_predictor._num_layers_before_predictor, 0)
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
+    self.assertEqual(box_predictor._apply_batch_norm, False)
+
+  def test_construct_default_conv_box_predictor_with_batch_norm(self):
+    box_predictor_text_proto = """
+      weight_shared_convolutional_box_predictor {
+        conv_hyperparams {
+          regularizer {
+            l1_regularizer {
+            }
+          }
+          batch_norm {
+            train: true
+          }
+          initializer {
+            truncated_normal_initializer {
+            }
+          }
+        }
+      }"""
+    box_predictor_proto = box_predictor_pb2.BoxPredictor()
+    text_format.Merge(box_predictor_text_proto, box_predictor_proto)
+    box_predictor = box_predictor_builder.build(
+        argscope_fn=hyperparams_builder.build,
+        box_predictor_config=box_predictor_proto,
+        is_training=True,
+        num_classes=90)
+    self.assertEqual(box_predictor._depth, 0)
+    self.assertEqual(box_predictor._num_layers_before_predictor, 0)
+    self.assertEqual(box_predictor.num_classes, 90)
+    self.assertTrue(box_predictor._is_training)
+    self.assertEqual(box_predictor._apply_batch_norm, True)
 
 
 class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
@@ -297,7 +330,10 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
         is_training=False,
         num_classes=10)
     mock_argscope_fn.assert_called_with(hyperparams_proto, False)
-    self.assertEqual(box_predictor._fc_hyperparams_fn, 'arg_scope')
+    self.assertEqual(box_predictor._box_prediction_head._fc_hyperparams_fn,
+                     'arg_scope')
+    self.assertEqual(box_predictor._class_prediction_head._fc_hyperparams_fn,
+                     'arg_scope')
 
   def test_non_default_mask_rcnn_box_predictor(self):
     fc_hyperparams_text_proto = """
@@ -334,12 +370,16 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
         box_predictor_config=box_predictor_proto,
         is_training=True,
         num_classes=90)
-    self.assertTrue(box_predictor._use_dropout)
-    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.8)
+    box_head = box_predictor._box_prediction_head
+    class_head = box_predictor._class_prediction_head
+    self.assertTrue(box_head._use_dropout)
+    self.assertTrue(class_head._use_dropout)
+    self.assertAlmostEqual(box_head._dropout_keep_prob, 0.8)
+    self.assertAlmostEqual(class_head._dropout_keep_prob, 0.8)
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
-    self.assertEqual(box_predictor._box_code_size, 3)
-    self.assertEqual(box_predictor._share_box_across_classes, True)
+    self.assertEqual(box_head._box_code_size, 3)
+    self.assertEqual(box_head._share_box_across_classes, True)
 
   def test_build_default_mask_rcnn_box_predictor(self):
     box_predictor_proto = box_predictor_pb2.BoxPredictor()
@@ -350,13 +390,15 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
         box_predictor_config=box_predictor_proto,
         is_training=True,
         num_classes=90)
-    self.assertFalse(box_predictor._use_dropout)
-    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.5)
+    box_head = box_predictor._box_prediction_head
+    class_head = box_predictor._class_prediction_head
+    self.assertFalse(box_head._use_dropout)
+    self.assertFalse(class_head._use_dropout)
+    self.assertAlmostEqual(box_head._dropout_keep_prob, 0.5)
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
-    self.assertEqual(box_predictor._box_code_size, 4)
-    self.assertFalse(box_predictor._predict_instance_masks)
-    self.assertFalse(box_predictor._predict_keypoints)
+    self.assertEqual(box_head._box_code_size, 4)
+    self.assertEqual(len(box_predictor._third_stage_heads.keys()), 0)
 
   def test_build_box_predictor_with_mask_branch(self):
     box_predictor_proto = box_predictor_pb2.BoxPredictor()
@@ -379,14 +421,21 @@ class MaskRCNNBoxPredictorBuilderTest(tf.test.TestCase):
                    True),
          mock.call(box_predictor_proto.mask_rcnn_box_predictor.conv_hyperparams,
                    True)], any_order=True)
-    self.assertFalse(box_predictor._use_dropout)
-    self.assertAlmostEqual(box_predictor._dropout_keep_prob, 0.5)
+    box_head = box_predictor._box_prediction_head
+    class_head = box_predictor._class_prediction_head
+    third_stage_heads = box_predictor._third_stage_heads
+    self.assertFalse(box_head._use_dropout)
+    self.assertFalse(class_head._use_dropout)
+    self.assertAlmostEqual(box_head._dropout_keep_prob, 0.5)
+    self.assertAlmostEqual(class_head._dropout_keep_prob, 0.5)
     self.assertEqual(box_predictor.num_classes, 90)
     self.assertTrue(box_predictor._is_training)
-    self.assertEqual(box_predictor._box_code_size, 4)
-    self.assertTrue(box_predictor._predict_instance_masks)
-    self.assertEqual(box_predictor._mask_prediction_conv_depth, 512)
-    self.assertFalse(box_predictor._predict_keypoints)
+    self.assertEqual(box_head._box_code_size, 4)
+    self.assertTrue(
+        mask_rcnn_box_predictor.MASK_PREDICTIONS in third_stage_heads)
+    self.assertEqual(
+        third_stage_heads[mask_rcnn_box_predictor.MASK_PREDICTIONS]
+        ._mask_prediction_conv_depth, 512)
 
 
 class RfcnBoxPredictorBuilderTest(tf.test.TestCase):
diff --git a/research/object_detection/builders/hyperparams_builder.py b/research/object_detection/builders/hyperparams_builder.py
index 05adddda..6fe8036f 100644
--- a/research/object_detection/builders/hyperparams_builder.py
+++ b/research/object_detection/builders/hyperparams_builder.py
@@ -22,6 +22,95 @@ from object_detection.utils import context_manager
 slim = tf.contrib.slim
 
 
+class KerasLayerHyperparams(object):
+  """
+  A hyperparameter configuration object for Keras layers used in
+  Object Detection models.
+  """
+
+  def __init__(self, hyperparams_config):
+    """Builds keras hyperparameter config for layers based on the proto config.
+
+    It automatically converts from Slim layer hyperparameter configs to
+    Keras layer hyperparameters. Namely, it:
+    - Builds Keras initializers/regularizers instead of Slim ones
+    - sets weights_regularizer/initializer to kernel_regularizer/initializer
+    - converts batchnorm decay to momentum
+    - converts Slim l2 regularizer weights to the equivalent Keras l2 weights
+
+    Contains a hyperparameter configuration for ops that specifies kernel
+    initializer, kernel regularizer, activation. Also contains parameters for
+    batch norm operators based on the configuration.
+
+    Note that if the batch_norm parameters are not specified in the config
+    (i.e. left to default) then batch norm is excluded from the config.
+
+    Args:
+      hyperparams_config: hyperparams.proto object containing
+        hyperparameters.
+
+    Raises:
+      ValueError: if hyperparams_config is not of type hyperparams.Hyperparams.
+    """
+    if not isinstance(hyperparams_config,
+                      hyperparams_pb2.Hyperparams):
+      raise ValueError('hyperparams_config not of type '
+                       'hyperparams_pb.Hyperparams.')
+
+    self._batch_norm_params = None
+    if hyperparams_config.HasField('batch_norm'):
+      self._batch_norm_params = _build_keras_batch_norm_params(
+          hyperparams_config.batch_norm)
+
+    self._op_params = {
+        'kernel_regularizer': _build_keras_regularizer(
+            hyperparams_config.regularizer),
+        'kernel_initializer': _build_initializer(
+            hyperparams_config.initializer, build_for_keras=True),
+        'activation': _build_activation_fn(hyperparams_config.activation)
+    }
+
+  def use_batch_norm(self):
+    return self._batch_norm_params is not None
+
+  def batch_norm_params(self, **overrides):
+    """Returns a dict containing batchnorm layer construction hyperparameters.
+
+    Optionally overrides values in the batchnorm hyperparam dict. Overrides
+    only apply to individual calls of this method, and do not affect
+    future calls.
+
+    Args:
+      **overrides: keyword arguments to override in the hyperparams dictionary
+
+    Returns: dict containing the layer construction keyword arguments, with
+      values overridden by the `overrides` keyword arguments.
+    """
+    if self._batch_norm_params is None:
+      new_batch_norm_params = dict()
+    else:
+      new_batch_norm_params = self._batch_norm_params.copy()
+    new_batch_norm_params.update(overrides)
+    return new_batch_norm_params
+
+  def params(self, **overrides):
+    """Returns a dict containing the layer construction hyperparameters to use.
+
+    Optionally overrides values in the returned dict. Overrides
+    only apply to individual calls of this method, and do not affect
+    future calls.
+
+    Args:
+      **overrides: keyword arguments to override in the hyperparams dictionary.
+
+    Returns: dict containing the layer construction keyword arguments, with
+      values overridden by the `overrides` keyword arguments.
+    """
+    new_params = self._op_params.copy()
+    new_params.update(**overrides)
+    return new_params
+
+
 def build(hyperparams_config, is_training):
   """Builds tf-slim arg_scope for convolution ops based on the config.
 
@@ -72,7 +161,7 @@ def build(hyperparams_config, is_training):
           context_manager.IdentityContextManager()):
       with slim.arg_scope(
           affected_ops,
-          weights_regularizer=_build_regularizer(
+          weights_regularizer=_build_slim_regularizer(
               hyperparams_config.regularizer),
           weights_initializer=_build_initializer(
               hyperparams_config.initializer),
@@ -104,7 +193,7 @@ def _build_activation_fn(activation_fn):
   raise ValueError('Unknown activation function: {}'.format(activation_fn))
 
 
-def _build_regularizer(regularizer):
+def _build_slim_regularizer(regularizer):
   """Builds a tf-slim regularizer from config.
 
   Args:
@@ -124,11 +213,36 @@ def _build_regularizer(regularizer):
   raise ValueError('Unknown regularizer function: {}'.format(regularizer_oneof))
 
 
-def _build_initializer(initializer):
+def _build_keras_regularizer(regularizer):
+  """Builds a keras regularizer from config.
+
+  Args:
+    regularizer: hyperparams_pb2.Hyperparams.regularizer proto.
+
+  Returns:
+    Keras regularizer.
+
+  Raises:
+    ValueError: On unknown regularizer.
+  """
+  regularizer_oneof = regularizer.WhichOneof('regularizer_oneof')
+  if  regularizer_oneof == 'l1_regularizer':
+    return tf.keras.regularizers.l1(float(regularizer.l1_regularizer.weight))
+  if regularizer_oneof == 'l2_regularizer':
+    # The Keras L2 regularizer weight differs from the Slim L2 regularizer
+    # weight by a factor of 2
+    return tf.keras.regularizers.l2(
+        float(regularizer.l2_regularizer.weight * 0.5))
+  raise ValueError('Unknown regularizer function: {}'.format(regularizer_oneof))
+
+
+def _build_initializer(initializer, build_for_keras=False):
   """Build a tf initializer from config.
 
   Args:
     initializer: hyperparams_pb2.Hyperparams.regularizer proto.
+    build_for_keras: Whether the initializers should be built for Keras
+      operators. If false builds for Slim.
 
   Returns:
     tf initializer.
@@ -151,10 +265,42 @@ def _build_initializer(initializer):
     mode = enum_descriptor.values_by_number[initializer.
                                             variance_scaling_initializer.
                                             mode].name
-    return slim.variance_scaling_initializer(
-        factor=initializer.variance_scaling_initializer.factor,
-        mode=mode,
-        uniform=initializer.variance_scaling_initializer.uniform)
+    if build_for_keras:
+      if initializer.variance_scaling_initializer.uniform:
+        return tf.variance_scaling_initializer(
+            scale=initializer.variance_scaling_initializer.factor,
+            mode=mode.lower(),
+            distribution='uniform')
+      else:
+        # In TF 1.9 release and earlier, the truncated_normal distribution was
+        # not supported correctly. So, in these earlier versions of tensorflow,
+        # the ValueError will be raised, and we manually truncate the
+        # distribution scale.
+        #
+        # It is insufficient to just set distribution to `normal` from the
+        # start, because the `normal` distribution in newer Tensorflow versions
+        # creates a truncated distribution, whereas it created untruncated
+        # distributions in older versions.
+        try:
+          return tf.variance_scaling_initializer(
+              scale=initializer.variance_scaling_initializer.factor,
+              mode=mode.lower(),
+              distribution='truncated_normal')
+        except ValueError:
+          truncate_constant = 0.87962566103423978
+          truncated_scale = initializer.variance_scaling_initializer.factor / (
+              truncate_constant * truncate_constant
+          )
+          return tf.variance_scaling_initializer(
+              scale=truncated_scale,
+              mode=mode.lower(),
+              distribution='normal')
+
+    else:
+      return slim.variance_scaling_initializer(
+          factor=initializer.variance_scaling_initializer.factor,
+          mode=mode,
+          uniform=initializer.variance_scaling_initializer.uniform)
   raise ValueError('Unknown initializer function: {}'.format(
       initializer_oneof))
 
@@ -180,3 +326,25 @@ def _build_batch_norm_params(batch_norm, is_training):
       'is_training': is_training and batch_norm.train,
   }
   return batch_norm_params
+
+
+def _build_keras_batch_norm_params(batch_norm):
+  """Build a dictionary of Keras BatchNormalization params from config.
+
+  Args:
+    batch_norm: hyperparams_pb2.ConvHyperparams.batch_norm proto.
+
+  Returns:
+    A dictionary containing Keras BatchNormalization parameters.
+  """
+  # Note: Although decay is defined to be 1 - momentum in batch_norm,
+  # decay in the slim batch_norm layers was erroneously defined and is
+  # actually the same as momentum in the Keras batch_norm layers.
+  # For context, see: github.com/keras-team/keras/issues/6839
+  batch_norm_params = {
+      'momentum': batch_norm.decay,
+      'center': batch_norm.center,
+      'scale': batch_norm.scale,
+      'epsilon': batch_norm.epsilon,
+  }
+  return batch_norm_params
diff --git a/research/object_detection/builders/hyperparams_builder_test.py b/research/object_detection/builders/hyperparams_builder_test.py
index 943532fb..48e63fee 100644
--- a/research/object_detection/builders/hyperparams_builder_test.py
+++ b/research/object_detection/builders/hyperparams_builder_test.py
@@ -149,6 +149,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
       result = sess.run(regularizer(tf.constant(weights)))
     self.assertAllClose(np.abs(weights).sum() * 0.5, result)
 
+  def test_return_l1_regularized_weights_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l1_regularizer {
+          weight: 0.5
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+
+    regularizer = keras_config.params()['kernel_regularizer']
+    weights = np.array([1., -1, 4., 2.])
+    with self.test_session() as sess:
+      result = sess.run(regularizer(tf.constant(weights)))
+    self.assertAllClose(np.abs(weights).sum() * 0.5, result)
+
   def test_return_l2_regularizer_weights(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -174,6 +197,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
       result = sess.run(regularizer(tf.constant(weights)))
     self.assertAllClose(np.power(weights, 2).sum() / 2.0 * 0.42, result)
 
+  def test_return_l2_regularizer_weights_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+          weight: 0.42
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+
+    regularizer = keras_config.params()['kernel_regularizer']
+    weights = np.array([1., -1, 4., 2.])
+    with self.test_session() as sess:
+      result = sess.run(regularizer(tf.constant(weights)))
+    self.assertAllClose(np.power(weights, 2).sum() / 2.0 * 0.42, result)
+
   def test_return_non_default_batch_norm_params_with_train_during_train(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -206,6 +252,66 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self.assertTrue(batch_norm_params['scale'])
     self.assertTrue(batch_norm_params['is_training'])
 
+  def test_return_non_default_batch_norm_params_keras(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      batch_norm {
+        decay: 0.7
+        center: false
+        scale: true
+        epsilon: 0.03
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+
+    self.assertTrue(keras_config.use_batch_norm())
+    batch_norm_params = keras_config.batch_norm_params()
+    self.assertAlmostEqual(batch_norm_params['momentum'], 0.7)
+    self.assertAlmostEqual(batch_norm_params['epsilon'], 0.03)
+    self.assertFalse(batch_norm_params['center'])
+    self.assertTrue(batch_norm_params['scale'])
+
+  def test_return_non_default_batch_norm_params_keras_override(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      batch_norm {
+        decay: 0.7
+        center: false
+        scale: true
+        epsilon: 0.03
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+
+    self.assertTrue(keras_config.use_batch_norm())
+    batch_norm_params = keras_config.batch_norm_params(momentum=0.4)
+    self.assertAlmostEqual(batch_norm_params['momentum'], 0.4)
+    self.assertAlmostEqual(batch_norm_params['epsilon'], 0.03)
+    self.assertFalse(batch_norm_params['center'])
+    self.assertTrue(batch_norm_params['scale'])
+
   def test_return_batch_norm_params_with_notrain_during_eval(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -289,6 +395,24 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
     self.assertEqual(conv_scope_arguments['normalizer_fn'], None)
 
+  def test_do_not_use_batch_norm_if_default_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    self.assertFalse(keras_config.use_batch_norm())
+    self.assertEqual(keras_config.batch_norm_params(), {})
+
   def test_use_none_activation(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -309,6 +433,24 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
     self.assertEqual(conv_scope_arguments['activation_fn'], None)
 
+  def test_use_none_activation_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: NONE
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    self.assertEqual(keras_config.params()['activation'], None)
+
   def test_use_relu_activation(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -329,6 +471,24 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
     self.assertEqual(conv_scope_arguments['activation_fn'], tf.nn.relu)
 
+  def test_use_relu_activation_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: RELU
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    self.assertEqual(keras_config.params()['activation'], tf.nn.relu)
+
   def test_use_relu_6_activation(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -349,6 +509,43 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     conv_scope_arguments = scope[_get_scope_key(slim.conv2d)]
     self.assertEqual(conv_scope_arguments['activation_fn'], tf.nn.relu6)
 
+  def test_use_relu_6_activation_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: RELU_6
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    self.assertEqual(keras_config.params()['activation'], tf.nn.relu6)
+
+  def test_override_activation_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+      activation: RELU_6
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    new_params = keras_config.params(activation=tf.nn.relu)
+    self.assertEqual(new_params['activation'], tf.nn.relu)
+
   def _assert_variance_in_range(self, initializer, shape, variance,
                                 tol=1e-2):
     with tf.Graph().as_default() as g:
@@ -386,6 +583,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=2. / 100.)
 
+  def test_variance_in_range_with_variance_scaling_initializer_fan_in_keras(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        variance_scaling_initializer {
+          factor: 2.0
+          mode: FAN_IN
+          uniform: false
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=2. / 100.)
+
   def test_variance_in_range_with_variance_scaling_initializer_fan_out(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -410,6 +630,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=2. / 40.)
 
+  def test_variance_in_range_with_variance_scaling_initializer_fan_out_keras(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        variance_scaling_initializer {
+          factor: 2.0
+          mode: FAN_OUT
+          uniform: false
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=2. / 40.)
+
   def test_variance_in_range_with_variance_scaling_initializer_fan_avg(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -434,6 +677,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=4. / (100. + 40.))
 
+  def test_variance_in_range_with_variance_scaling_initializer_fan_avg_keras(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        variance_scaling_initializer {
+          factor: 2.0
+          mode: FAN_AVG
+          uniform: false
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=4. / (100. + 40.))
+
   def test_variance_in_range_with_variance_scaling_initializer_uniform(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -458,6 +724,29 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=2. / 100.)
 
+  def test_variance_in_range_with_variance_scaling_initializer_uniform_keras(
+      self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        variance_scaling_initializer {
+          factor: 2.0
+          mode: FAN_IN
+          uniform: true
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=2. / 100.)
+
   def test_variance_in_range_with_truncated_normal_initializer(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -481,6 +770,27 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=0.49, tol=1e-1)
 
+  def test_variance_in_range_with_truncated_normal_initializer_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+          mean: 0.0
+          stddev: 0.8
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=0.49, tol=1e-1)
+
   def test_variance_in_range_with_random_normal_initializer(self):
     conv_hyperparams_text_proto = """
       regularizer {
@@ -504,6 +814,27 @@ class HyperparamsBuilderTest(tf.test.TestCase):
     self._assert_variance_in_range(initializer, shape=[100, 40],
                                    variance=0.64, tol=1e-1)
 
+  def test_variance_in_range_with_random_normal_initializer_keras(self):
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        random_normal_initializer {
+          mean: 0.0
+          stddev: 0.8
+        }
+      }
+    """
+    conv_hyperparams_proto = hyperparams_pb2.Hyperparams()
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams_proto)
+    keras_config = hyperparams_builder.KerasLayerHyperparams(
+        conv_hyperparams_proto)
+    initializer = keras_config.params()['kernel_initializer']
+    self._assert_variance_in_range(initializer, shape=[100, 40],
+                                   variance=0.64, tol=1e-1)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/image_resizer_builder_test.py b/research/object_detection/builders/image_resizer_builder_test.py
index 38f620e0..f7da1912 100644
--- a/research/object_detection/builders/image_resizer_builder_test.py
+++ b/research/object_detection/builders/image_resizer_builder_test.py
@@ -46,6 +46,20 @@ class ImageResizerBuilderTest(tf.test.TestCase):
         input_shape, image_resizer_text_proto)
     self.assertEqual(output_shape, expected_output_shape)
 
+  def test_build_keep_aspect_ratio_resizer_grayscale(self):
+    image_resizer_text_proto = """
+      keep_aspect_ratio_resizer {
+        min_dimension: 10
+        max_dimension: 20
+        convert_to_grayscale: true
+      }
+    """
+    input_shape = (50, 25, 3)
+    expected_output_shape = (20, 10, 1)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
   def test_build_keep_aspect_ratio_resizer_with_padding(self):
     image_resizer_text_proto = """
       keep_aspect_ratio_resizer {
@@ -76,6 +90,20 @@ class ImageResizerBuilderTest(tf.test.TestCase):
         input_shape, image_resizer_text_proto)
     self.assertEqual(output_shape, expected_output_shape)
 
+  def test_built_fixed_shape_resizer_grayscale(self):
+    image_resizer_text_proto = """
+      fixed_shape_resizer {
+        height: 10
+        width: 20
+        convert_to_grayscale: true
+      }
+    """
+    input_shape = (50, 25, 3)
+    expected_output_shape = (10, 20, 1)
+    output_shape = self._shape_of_resized_random_image_given_text_proto(
+        input_shape, image_resizer_text_proto)
+    self.assertEqual(output_shape, expected_output_shape)
+
   def test_raises_error_on_invalid_input(self):
     invalid_input = 'invalid_input'
     with self.assertRaises(ValueError):
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index 6c586909..b68c6968 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -23,7 +23,8 @@ from object_detection.builders import losses_builder
 from object_detection.builders import matcher_builder
 from object_detection.builders import post_processing_builder
 from object_detection.builders import region_similarity_calculator_builder as sim_calc
-from object_detection.core import box_predictor
+from object_detection.core import balanced_positive_negative_sampler as sampler
+from object_detection.core import target_assigner
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.meta_architectures import rfcn_meta_arch
 from object_detection.meta_architectures import ssd_meta_arch
@@ -41,6 +42,7 @@ from object_detection.models.ssd_mobilenet_v1_feature_extractor import SSDMobile
 from object_detection.models.ssd_mobilenet_v1_fpn_feature_extractor import SSDMobileNetV1FpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v1_ppn_feature_extractor import SSDMobileNetV1PpnFeatureExtractor
 from object_detection.models.ssd_mobilenet_v2_feature_extractor import SSDMobileNetV2FeatureExtractor
+from object_detection.predictors import rfcn_box_predictor
 from object_detection.protos import model_pb2
 
 # A map of names to SSD feature extractors.
@@ -142,10 +144,34 @@ def _build_ssd_feature_extractor(feature_extractor_config, is_training,
     raise ValueError('Unknown ssd feature_extractor: {}'.format(feature_type))
 
   feature_extractor_class = SSD_FEATURE_EXTRACTOR_CLASS_MAP[feature_type]
-  return feature_extractor_class(
-      is_training, depth_multiplier, min_depth, pad_to_multiple,
-      conv_hyperparams, reuse_weights, use_explicit_padding, use_depthwise,
-      override_base_feature_extractor_hyperparams)
+  kwargs = {
+      'is_training':
+          is_training,
+      'depth_multiplier':
+          depth_multiplier,
+      'min_depth':
+          min_depth,
+      'pad_to_multiple':
+          pad_to_multiple,
+      'conv_hyperparams_fn':
+          conv_hyperparams,
+      'reuse_weights':
+          reuse_weights,
+      'use_explicit_padding':
+          use_explicit_padding,
+      'use_depthwise':
+          use_depthwise,
+      'override_base_feature_extractor_hyperparams':
+          override_base_feature_extractor_hyperparams
+  }
+
+  if feature_extractor_config.HasField('fpn'):
+    kwargs.update({
+        'fpn_min_level': feature_extractor_config.fpn.min_level,
+        'fpn_max_level': feature_extractor_config.fpn.max_level,
+    })
+
+  return feature_extractor_class(**kwargs)
 
 
 def _build_ssd_model(ssd_config, is_training, add_summaries,
@@ -291,6 +317,10 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
   first_stage_anchor_generator = anchor_generator_builder.build(
       frcnn_config.first_stage_anchor_generator)
 
+  first_stage_target_assigner = target_assigner.create_target_assigner(
+      'FasterRCNN',
+      'proposal',
+      use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)
   first_stage_atrous_rate = frcnn_config.first_stage_atrous_rate
   first_stage_box_predictor_arg_scope_fn = hyperparams_builder.build(
       frcnn_config.first_stage_box_predictor_conv_hyperparams, is_training)
@@ -298,8 +328,9 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       frcnn_config.first_stage_box_predictor_kernel_size)
   first_stage_box_predictor_depth = frcnn_config.first_stage_box_predictor_depth
   first_stage_minibatch_size = frcnn_config.first_stage_minibatch_size
-  first_stage_positive_balance_fraction = (
-      frcnn_config.first_stage_positive_balance_fraction)
+  first_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+      positive_fraction=frcnn_config.first_stage_positive_balance_fraction,
+      is_static=frcnn_config.use_static_balanced_label_sampler)
   first_stage_nms_score_threshold = frcnn_config.first_stage_nms_score_threshold
   first_stage_nms_iou_threshold = frcnn_config.first_stage_nms_iou_threshold
   first_stage_max_proposals = frcnn_config.first_stage_max_proposals
@@ -311,13 +342,19 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
   maxpool_kernel_size = frcnn_config.maxpool_kernel_size
   maxpool_stride = frcnn_config.maxpool_stride
 
+  second_stage_target_assigner = target_assigner.create_target_assigner(
+      'FasterRCNN',
+      'detection',
+      use_matmul_gather=frcnn_config.use_matmul_gather_in_matcher)
   second_stage_box_predictor = box_predictor_builder.build(
       hyperparams_builder.build,
       frcnn_config.second_stage_box_predictor,
       is_training=is_training,
       num_classes=num_classes)
   second_stage_batch_size = frcnn_config.second_stage_batch_size
-  second_stage_balance_fraction = frcnn_config.second_stage_balance_fraction
+  second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+      positive_fraction=frcnn_config.second_stage_balance_fraction,
+      is_static=frcnn_config.use_static_balanced_label_sampler)
   (second_stage_non_max_suppression_fn, second_stage_score_conversion_fn
   ) = post_processing_builder.build(frcnn_config.second_stage_post_processing)
   second_stage_localization_loss_weight = (
@@ -338,6 +375,8 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
         second_stage_localization_loss_weight)
 
   use_matmul_crop_and_resize = (frcnn_config.use_matmul_crop_and_resize)
+  clip_anchors_to_image = (
+      frcnn_config.clip_anchors_to_image)
 
   common_kwargs = {
       'is_training': is_training,
@@ -346,6 +385,7 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       'feature_extractor': feature_extractor,
       'number_of_stages': number_of_stages,
       'first_stage_anchor_generator': first_stage_anchor_generator,
+      'first_stage_target_assigner': first_stage_target_assigner,
       'first_stage_atrous_rate': first_stage_atrous_rate,
       'first_stage_box_predictor_arg_scope_fn':
       first_stage_box_predictor_arg_scope_fn,
@@ -353,15 +393,15 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       first_stage_box_predictor_kernel_size,
       'first_stage_box_predictor_depth': first_stage_box_predictor_depth,
       'first_stage_minibatch_size': first_stage_minibatch_size,
-      'first_stage_positive_balance_fraction':
-      first_stage_positive_balance_fraction,
+      'first_stage_sampler': first_stage_sampler,
       'first_stage_nms_score_threshold': first_stage_nms_score_threshold,
       'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,
       'first_stage_max_proposals': first_stage_max_proposals,
       'first_stage_localization_loss_weight': first_stage_loc_loss_weight,
       'first_stage_objectness_loss_weight': first_stage_obj_loss_weight,
+      'second_stage_target_assigner': second_stage_target_assigner,
       'second_stage_batch_size': second_stage_batch_size,
-      'second_stage_balance_fraction': second_stage_balance_fraction,
+      'second_stage_sampler': second_stage_sampler,
       'second_stage_non_max_suppression_fn':
       second_stage_non_max_suppression_fn,
       'second_stage_score_conversion_fn': second_stage_score_conversion_fn,
@@ -373,10 +413,12 @@ def _build_faster_rcnn_model(frcnn_config, is_training, add_summaries):
       second_stage_classification_loss_weight,
       'hard_example_miner': hard_example_miner,
       'add_summaries': add_summaries,
-      'use_matmul_crop_and_resize': use_matmul_crop_and_resize
+      'use_matmul_crop_and_resize': use_matmul_crop_and_resize,
+      'clip_anchors_to_image': clip_anchors_to_image
   }
 
-  if isinstance(second_stage_box_predictor, box_predictor.RfcnBoxPredictor):
+  if isinstance(second_stage_box_predictor,
+                rfcn_box_predictor.RfcnBoxPredictor):
     return rfcn_meta_arch.RFCNMetaArch(
         second_stage_rfcn_box_predictor=second_stage_box_predictor,
         **common_kwargs)
diff --git a/research/object_detection/builders/model_builder_test.py b/research/object_detection/builders/model_builder_test.py
index cb744f25..2b5a1160 100644
--- a/research/object_detection/builders/model_builder_test.py
+++ b/research/object_detection/builders/model_builder_test.py
@@ -54,12 +54,6 @@ SSD_RESNET_V1_FPN_FEAT_MAPS = {
     ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
     'ssd_resnet152_v1_fpn':
     ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
-    'ssd_resnet50_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet50V1PpnFeatureExtractor,
-    'ssd_resnet101_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet101V1PpnFeatureExtractor,
-    'ssd_resnet152_v1_ppn':
-    ssd_resnet_v1_ppn.SSDResnet152V1PpnFeatureExtractor
 }
 
 SSD_RESNET_V1_PPN_FEAT_MAPS = {
@@ -235,6 +229,10 @@ class ModelBuilderTest(tf.test.TestCase):
       ssd {
         feature_extractor {
           type: 'ssd_resnet50_v1_fpn'
+          fpn {
+            min_level: 3
+            max_level: 7
+          }
           conv_hyperparams {
             regularizer {
                 l2_regularizer {
@@ -479,6 +477,10 @@ class ModelBuilderTest(tf.test.TestCase):
         inplace_batchnorm_update: true
         feature_extractor {
           type: 'ssd_mobilenet_v1_fpn'
+          fpn {
+            min_level: 3
+            max_level: 7
+          }
           conv_hyperparams {
             regularizer {
                 l2_regularizer {
diff --git a/research/object_detection/builders/preprocessor_builder.py b/research/object_detection/builders/preprocessor_builder.py
index 10b92532..07e3378d 100644
--- a/research/object_detection/builders/preprocessor_builder.py
+++ b/research/object_detection/builders/preprocessor_builder.py
@@ -71,22 +71,38 @@ def _get_dict_from_proto(config):
 # function that should be used. The PreprocessingStep proto should be parsable
 # with _get_dict_from_proto.
 PREPROCESSING_FUNCTION_MAP = {
-    'normalize_image': preprocessor.normalize_image,
-    'random_pixel_value_scale': preprocessor.random_pixel_value_scale,
-    'random_image_scale': preprocessor.random_image_scale,
-    'random_rgb_to_gray': preprocessor.random_rgb_to_gray,
-    'random_adjust_brightness': preprocessor.random_adjust_brightness,
-    'random_adjust_contrast': preprocessor.random_adjust_contrast,
-    'random_adjust_hue': preprocessor.random_adjust_hue,
-    'random_adjust_saturation': preprocessor.random_adjust_saturation,
-    'random_distort_color': preprocessor.random_distort_color,
-    'random_jitter_boxes': preprocessor.random_jitter_boxes,
-    'random_crop_to_aspect_ratio': preprocessor.random_crop_to_aspect_ratio,
-    'random_black_patches': preprocessor.random_black_patches,
-    'rgb_to_gray': preprocessor.rgb_to_gray,
+    'normalize_image':
+        preprocessor.normalize_image,
+    'random_pixel_value_scale':
+        preprocessor.random_pixel_value_scale,
+    'random_image_scale':
+        preprocessor.random_image_scale,
+    'random_rgb_to_gray':
+        preprocessor.random_rgb_to_gray,
+    'random_adjust_brightness':
+        preprocessor.random_adjust_brightness,
+    'random_adjust_contrast':
+        preprocessor.random_adjust_contrast,
+    'random_adjust_hue':
+        preprocessor.random_adjust_hue,
+    'random_adjust_saturation':
+        preprocessor.random_adjust_saturation,
+    'random_distort_color':
+        preprocessor.random_distort_color,
+    'random_jitter_boxes':
+        preprocessor.random_jitter_boxes,
+    'random_crop_to_aspect_ratio':
+        preprocessor.random_crop_to_aspect_ratio,
+    'random_black_patches':
+        preprocessor.random_black_patches,
+    'rgb_to_gray':
+        preprocessor.rgb_to_gray,
     'scale_boxes_to_pixel_coordinates': (
         preprocessor.scale_boxes_to_pixel_coordinates),
-    'subtract_channel_mean': preprocessor.subtract_channel_mean,
+    'subtract_channel_mean':
+        preprocessor.subtract_channel_mean,
+    'convert_class_logits_to_softmax':
+        preprocessor.convert_class_logits_to_softmax,
 }
 
 
diff --git a/research/object_detection/builders/preprocessor_builder_test.py b/research/object_detection/builders/preprocessor_builder_test.py
index 9e5d8de8..8a72aa40 100644
--- a/research/object_detection/builders/preprocessor_builder_test.py
+++ b/research/object_detection/builders/preprocessor_builder_test.py
@@ -561,6 +561,18 @@ class PreprocessorBuilderTest(tf.test.TestCase):
                             'min_padded_size_ratio': (1.0, 1.0),
                             'max_padded_size_ratio': (2.0, 2.0)})
 
+  def test_build_normalize_image_convert_class_logits_to_softmax(self):
+    preprocessor_text_proto = """
+    convert_class_logits_to_softmax {
+        temperature: 2
+    }
+    """
+    preprocessor_proto = preprocessor_pb2.PreprocessingStep()
+    text_format.Merge(preprocessor_text_proto, preprocessor_proto)
+    function, args = preprocessor_builder.build(preprocessor_proto)
+    self.assertEqual(function, preprocessor.convert_class_logits_to_softmax)
+    self.assertEqual(args, {'temperature': 2})
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/builders/region_similarity_calculator_builder.py b/research/object_detection/builders/region_similarity_calculator_builder.py
index fa1d6717..157c94fb 100644
--- a/research/object_detection/builders/region_similarity_calculator_builder.py
+++ b/research/object_detection/builders/region_similarity_calculator_builder.py
@@ -51,6 +51,9 @@ def build(region_similarity_calculator_config):
     return region_similarity_calculator.IoaSimilarity()
   if similarity_calculator == 'neg_sq_dist_similarity':
     return region_similarity_calculator.NegSqDistSimilarity()
+  if similarity_calculator == 'thresholded_iou_similarity':
+    return region_similarity_calculator.ThresholdedIouSimilarity(
+        region_similarity_calculator_config.thresholded_iou_similarity.threshold
+    )
 
   raise ValueError('Unknown region similarity calculator.')
-
diff --git a/research/object_detection/core/balanced_positive_negative_sampler.py b/research/object_detection/core/balanced_positive_negative_sampler.py
index 7042c40f..90b121c0 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler.py
@@ -29,17 +29,19 @@ the minibatch_sampler base class.
 import tensorflow as tf
 
 from object_detection.core import minibatch_sampler
+from object_detection.utils import ops
 
 
 class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
   """Subsamples minibatches to a desired balance of positives and negatives."""
 
-  def __init__(self, positive_fraction=0.5):
+  def __init__(self, positive_fraction=0.5, is_static=False):
     """Constructs a minibatch sampler.
 
     Args:
       positive_fraction: desired fraction of positive examples (scalar in [0,1])
         in the batch.
+      is_static: If True, uses an implementation with static shape guarantees.
 
     Raises:
       ValueError: if positive_fraction < 0, or positive_fraction > 1
@@ -48,21 +50,159 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
       raise ValueError('positive_fraction should be in range [0,1]. '
                        'Received: %s.' % positive_fraction)
     self._positive_fraction = positive_fraction
+    self._is_static = is_static
 
-  def subsample(self, indicator, batch_size, labels):
+  def _get_num_pos_neg_samples(self, sorted_indices_tensor, sample_size):
+    """Counts the number of positives and negatives numbers to be sampled.
+
+    Args:
+      sorted_indices_tensor: A sorted int32 tensor of shape [N] which contains
+        the signed indices of the examples where the sign is based on the label
+        value. The examples that cannot be sampled are set to 0. It samples
+        atmost sample_size*positive_fraction positive examples and remaining
+        from negative examples.
+      sample_size: Size of subsamples.
+
+    Returns:
+      A tuple containing the number of positive and negative labels in the
+      subsample.
+    """
+    input_length = tf.shape(sorted_indices_tensor)[0]
+    valid_positive_index = tf.greater(sorted_indices_tensor,
+                                      tf.zeros(input_length, tf.int32))
+    num_sampled_pos = tf.reduce_sum(tf.cast(valid_positive_index, tf.int32))
+    max_num_positive_samples = tf.constant(
+        int(sample_size * self._positive_fraction), tf.int32)
+    num_positive_samples = tf.minimum(max_num_positive_samples, num_sampled_pos)
+    num_negative_samples = tf.constant(sample_size,
+                                       tf.int32) - num_positive_samples
+
+    return num_positive_samples, num_negative_samples
+
+  def _get_values_from_start_and_end(self, input_tensor, num_start_samples,
+                                     num_end_samples, total_num_samples):
+    """slices num_start_samples and last num_end_samples from input_tensor.
+
+    Args:
+      input_tensor: An int32 tensor of shape [N] to be sliced.
+      num_start_samples: Number of examples to be sliced from the beginning
+        of the input tensor.
+      num_end_samples: Number of examples to be sliced from the end of the
+        input tensor.
+      total_num_samples: Sum of is num_start_samples and num_end_samples. This
+        should be a scalar.
+
+    Returns:
+      A tensor containing the first num_start_samples and last num_end_samples
+      from input_tensor.
+
+    """
+    input_length = tf.shape(input_tensor)[0]
+    start_positions = tf.less(tf.range(input_length), num_start_samples)
+    end_positions = tf.greater_equal(
+        tf.range(input_length), input_length - num_end_samples)
+    selected_positions = tf.logical_or(start_positions, end_positions)
+    selected_positions = tf.cast(selected_positions, tf.int32)
+    indexed_positions = tf.multiply(tf.cumsum(selected_positions),
+                                    selected_positions)
+    one_hot_selector = tf.one_hot(indexed_positions - 1,
+                                  total_num_samples,
+                                  dtype=tf.int32)
+    return tf.tensordot(input_tensor, one_hot_selector, axes=[0, 0])
+
+  def _static_subsample(self, indicator, batch_size, labels):
+    """Returns subsampled minibatch.
+
+    Args:
+      indicator: boolean tensor of shape [N] whose True entries can be sampled.
+        N should be a complie time constant.
+      batch_size: desired batch size. This scalar cannot be None.
+      labels: boolean tensor of shape [N] denoting positive(=True) and negative
+        (=False) examples. N should be a complie time constant.
+
+    Returns:
+      sampled_idx_indicator: boolean tensor of shape [N], True for entries which
+        are sampled.
+
+    Raises:
+      ValueError: if labels and indicator are not 1D boolean tensors.
+    """
+    # Check if indicator and labels have a static size.
+    if not indicator.shape.is_fully_defined():
+      raise ValueError('indicator must be static in shape when is_static is'
+                       'True')
+    if not labels.shape.is_fully_defined():
+      raise ValueError('labels must be static in shape when is_static is'
+                       'True')
+    if not isinstance(batch_size, int):
+      raise ValueError('batch_size has to be an integer when is_static is'
+                       'True.')
+
+    input_length = tf.shape(indicator)[0]
+
+    # Shuffle indicator and label. Need to store the permutation to restore the
+    # order post sampling.
+    permutation = tf.random_shuffle(tf.range(input_length))
+    indicator = ops.matmul_gather_on_zeroth_axis(
+        tf.cast(indicator, tf.float32), permutation)
+    labels = ops.matmul_gather_on_zeroth_axis(
+        tf.cast(labels, tf.float32), permutation)
+
+    # index (starting from 1) when cls_weight is True, 0 when False
+    indicator_idx = tf.where(
+        tf.cast(indicator, tf.bool), tf.range(1, input_length + 1),
+        tf.zeros(input_length, tf.int32))
+
+    # Replace -1 for negative, +1 for positive labels
+    signed_label = tf.where(
+        tf.cast(labels, tf.bool), tf.ones(input_length, tf.int32),
+        tf.scalar_mul(-1, tf.ones(input_length, tf.int32)))
+    # negative of index for negative label, positive index for positive label,
+    # 0 when indicator is False.
+    signed_indicator_idx = tf.multiply(indicator_idx, signed_label)
+    sorted_signed_indicator_idx = tf.nn.top_k(
+        signed_indicator_idx, input_length, sorted=True).values
+
+    [num_positive_samples,
+     num_negative_samples] = self._get_num_pos_neg_samples(
+         sorted_signed_indicator_idx, batch_size)
+
+    sampled_idx = self._get_values_from_start_and_end(
+        sorted_signed_indicator_idx, num_positive_samples,
+        num_negative_samples, batch_size)
+
+    # Shift the indices to start from 0 and remove any samples that are set as
+    # False.
+    sampled_idx = tf.abs(sampled_idx) - tf.ones(batch_size, tf.int32)
+    sampled_idx = tf.multiply(
+        tf.cast(tf.greater_equal(sampled_idx, tf.constant(0)), tf.int32),
+        sampled_idx)
+
+    sampled_idx_indicator = tf.cast(tf.reduce_sum(
+        tf.one_hot(sampled_idx, depth=input_length),
+        axis=0), tf.bool)
+
+    # project back the order based on stored permutations
+    reprojections = tf.one_hot(permutation, depth=input_length, dtype=tf.int32)
+    return tf.cast(tf.tensordot(
+        tf.cast(sampled_idx_indicator, tf.int32),
+        reprojections, axes=[0, 0]), tf.bool)
+
+  def subsample(self, indicator, batch_size, labels, scope=None):
     """Returns subsampled minibatch.
 
     Args:
       indicator: boolean tensor of shape [N] whose True entries can be sampled.
       batch_size: desired batch size. If None, keeps all positive samples and
         randomly selects negative samples so that the positive sample fraction
-        matches self._positive_fraction.
+        matches self._positive_fraction. It cannot be None is is_static is True.
       labels: boolean tensor of shape [N] denoting positive(=True) and negative
           (=False) examples.
+      scope: name scope.
 
     Returns:
-      is_sampled: boolean tensor of shape [N], True for entries which are
-          sampled.
+      sampled_idx_indicator: boolean tensor of shape [N], True for entries which
+        are sampled.
 
     Raises:
       ValueError: if labels and indicator are not 1D boolean tensors.
@@ -79,27 +219,30 @@ class BalancedPositiveNegativeSampler(minibatch_sampler.MinibatchSampler):
     if indicator.dtype != tf.bool:
       raise ValueError('indicator should be of type bool. Received: %s' %
                        indicator.dtype)
-
-    # Only sample from indicated samples
-    negative_idx = tf.logical_not(labels)
-    positive_idx = tf.logical_and(labels, indicator)
-    negative_idx = tf.logical_and(negative_idx, indicator)
-
-    # Sample positive and negative samples separately
-    if batch_size is None:
-      max_num_pos = tf.reduce_sum(tf.to_int32(positive_idx))
-    else:
-      max_num_pos = int(self._positive_fraction * batch_size)
-    sampled_pos_idx = self.subsample_indicator(positive_idx, max_num_pos)
-    num_sampled_pos = tf.reduce_sum(tf.cast(sampled_pos_idx, tf.int32))
-    if batch_size is None:
-      negative_positive_ratio = (
-          1 - self._positive_fraction) / self._positive_fraction
-      max_num_neg = tf.to_int32(
-          negative_positive_ratio * tf.to_float(num_sampled_pos))
-    else:
-      max_num_neg = batch_size - num_sampled_pos
-    sampled_neg_idx = self.subsample_indicator(negative_idx, max_num_neg)
-
-    sampled_idx = tf.logical_or(sampled_pos_idx, sampled_neg_idx)
-    return sampled_idx
+    with tf.name_scope(scope, 'BalancedPositiveNegativeSampler'):
+      if self._is_static:
+        return self._static_subsample(indicator, batch_size, labels)
+
+      else:
+        # Only sample from indicated samples
+        negative_idx = tf.logical_not(labels)
+        positive_idx = tf.logical_and(labels, indicator)
+        negative_idx = tf.logical_and(negative_idx, indicator)
+
+        # Sample positive and negative samples separately
+        if batch_size is None:
+          max_num_pos = tf.reduce_sum(tf.to_int32(positive_idx))
+        else:
+          max_num_pos = int(self._positive_fraction * batch_size)
+        sampled_pos_idx = self.subsample_indicator(positive_idx, max_num_pos)
+        num_sampled_pos = tf.reduce_sum(tf.cast(sampled_pos_idx, tf.int32))
+        if batch_size is None:
+          negative_positive_ratio = (
+              1 - self._positive_fraction) / self._positive_fraction
+          max_num_neg = tf.to_int32(
+              negative_positive_ratio * tf.to_float(num_sampled_pos))
+        else:
+          max_num_neg = batch_size - num_sampled_pos
+        sampled_neg_idx = self.subsample_indicator(negative_idx, max_num_neg)
+
+        return tf.logical_or(sampled_pos_idx, sampled_neg_idx)
diff --git a/research/object_detection/core/balanced_positive_negative_sampler_test.py b/research/object_detection/core/balanced_positive_negative_sampler_test.py
index e39de534..c4669def 100644
--- a/research/object_detection/core/balanced_positive_negative_sampler_test.py
+++ b/research/object_detection/core/balanced_positive_negative_sampler_test.py
@@ -24,15 +24,16 @@ from object_detection.utils import test_case
 
 class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
 
-  def test_subsample_all_examples(self):
+  def _test_subsample_all_examples(self, is_static=False):
     numpy_labels = np.random.permutation(300)
     indicator = tf.constant(np.ones(300) == 1)
     numpy_labels = (numpy_labels - 200) > 0
 
     labels = tf.constant(numpy_labels)
 
-    sampler = (balanced_positive_negative_sampler.
-               BalancedPositiveNegativeSampler())
+    sampler = (
+        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler(
+            is_static=is_static))
     is_sampled = sampler.subsample(indicator, 64, labels)
     with self.test_session() as sess:
       is_sampled = sess.run(is_sampled)
@@ -41,7 +42,13 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
       self.assertTrue(sum(np.logical_and(
           np.logical_not(numpy_labels), is_sampled)) == 32)
 
-  def test_subsample_selection(self):
+  def test_subsample_all_examples_dynamic(self):
+    self._test_subsample_all_examples()
+
+  def test_subsample_all_examples_static(self):
+    self._test_subsample_all_examples(is_static=True)
+
+  def _test_subsample_selection(self, is_static=False):
     # Test random sampling when only some examples can be sampled:
     # 100 samples, 20 positives, 10 positives cannot be sampled
     numpy_labels = np.arange(100)
@@ -51,8 +58,9 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
 
     labels = tf.constant(numpy_labels)
 
-    sampler = (balanced_positive_negative_sampler.
-               BalancedPositiveNegativeSampler())
+    sampler = (
+        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler(
+            is_static=is_static))
     is_sampled = sampler.subsample(indicator, 64, labels)
     with self.test_session() as sess:
       is_sampled = sess.run(is_sampled)
@@ -63,6 +71,42 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
       self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
                                                      numpy_indicator))
 
+  def test_subsample_selection_dynamic(self):
+    self._test_subsample_selection()
+
+  def test_subsample_selection_static(self):
+    self._test_subsample_selection(is_static=True)
+
+  def _test_subsample_selection_larger_batch_size(self, is_static=False):
+    # Test random sampling when total number of examples that can be sampled are
+    # less than batch size:
+    # 100 samples, 50 positives, 40 positives cannot be sampled, batch size 64.
+    numpy_labels = np.arange(100)
+    numpy_indicator = numpy_labels < 60
+    indicator = tf.constant(numpy_indicator)
+    numpy_labels = (numpy_labels - 50) >= 0
+
+    labels = tf.constant(numpy_labels)
+
+    sampler = (
+        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler(
+            is_static=is_static))
+    is_sampled = sampler.subsample(indicator, 64, labels)
+    with self.test_session() as sess:
+      is_sampled = sess.run(is_sampled)
+      self.assertTrue(sum(is_sampled) == 60)
+      self.assertTrue(sum(np.logical_and(numpy_labels, is_sampled)) == 10)
+      self.assertTrue(
+          sum(np.logical_and(np.logical_not(numpy_labels), is_sampled)) == 50)
+      self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
+                                                     numpy_indicator))
+
+  def test_subsample_selection_larger_batch_size_dynamic(self):
+    self._test_subsample_selection_larger_batch_size()
+
+  def test_subsample_selection_larger_batch_size_static(self):
+    self._test_subsample_selection_larger_batch_size(is_static=True)
+
   def test_subsample_selection_no_batch_size(self):
     # Test random sampling when only some examples can be sampled:
     # 1000 samples, 6 positives (5 can be sampled).
@@ -85,6 +129,14 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
       self.assertAllEqual(is_sampled, np.logical_and(is_sampled,
                                                      numpy_indicator))
 
+  def test_subsample_selection_no_batch_size_static(self):
+    labels = tf.constant([[True, False, False]])
+    indicator = tf.constant([True, False, True])
+    sampler = (
+        balanced_positive_negative_sampler.BalancedPositiveNegativeSampler())
+    with self.assertRaises(ValueError):
+      sampler.subsample(indicator, None, labels)
+
   def test_raises_error_with_incorrect_label_shape(self):
     labels = tf.constant([[True, False, False]])
     indicator = tf.constant([True, False, True])
@@ -101,6 +153,5 @@ class BalancedPositiveNegativeSamplerTest(test_case.TestCase):
     with self.assertRaises(ValueError):
       sampler.subsample(indicator, 64, labels)
 
-
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/box_predictor.py b/research/object_detection/core/box_predictor.py
index 6b389d72..b98bb1f1 100644
--- a/research/object_detection/core/box_predictor.py
+++ b/research/object_detection/core/box_predictor.py
@@ -27,13 +27,7 @@ These modules are separated from the main model since the same
 few box predictor architectures are shared across many models.
 """
 from abc import abstractmethod
-import math
 import tensorflow as tf
-from object_detection.utils import ops
-from object_detection.utils import shape_utils
-from object_detection.utils import static_shape
-
-slim = tf.contrib.slim
 
 BOX_ENCODINGS = 'box_encodings'
 CLASS_PREDICTIONS_WITH_BACKGROUND = 'class_predictions_with_background'
@@ -56,6 +50,10 @@ class BoxPredictor(object):
     self._is_training = is_training
     self._num_classes = num_classes
 
+  @property
+  def is_keras_model(self):
+    return False
+
   @property
   def num_classes(self):
     return self._num_classes
@@ -136,26 +134,11 @@ class BoxPredictor(object):
     pass
 
 
-class RfcnBoxPredictor(BoxPredictor):
-  """RFCN Box Predictor.
+class KerasBoxPredictor(tf.keras.Model):
+  """Keras-based BoxPredictor."""
 
-  Applies a position sensitive ROI pooling on position sensitive feature maps to
-  predict classes and refined locations. See https://arxiv.org/abs/1605.06409
-  for details.
-
-  This is used for the second stage of the RFCN meta architecture. Notice that
-  locations are *not* shared across classes, thus for each anchor, a separate
-  prediction is made for each class.
-  """
-
-  def __init__(self,
-               is_training,
-               num_classes,
-               conv_hyperparams_fn,
-               num_spatial_bins,
-               depth,
-               crop_size,
-               box_code_size):
+  def __init__(self, is_training, num_classes, freeze_batchnorm,
+               inplace_batchnorm_update):
     """Constructor.
 
     Args:
@@ -164,835 +147,80 @@ class RfcnBoxPredictor(BoxPredictor):
         include the background category, so if groundtruth labels take values
         in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
         assigned classification targets can range from {0,... K}).
-      conv_hyperparams_fn: A function to construct tf-slim arg_scope with
-        hyperparameters for convolutional layers.
-      num_spatial_bins: A list of two integers `[spatial_bins_y,
-        spatial_bins_x]`.
-      depth: Target depth to reduce the input feature maps to.
-      crop_size: A list of two integers `[crop_height, crop_width]`.
-      box_code_size: Size of encoding for each box.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: Whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
     """
-    super(RfcnBoxPredictor, self).__init__(is_training, num_classes)
-    self._conv_hyperparams_fn = conv_hyperparams_fn
-    self._num_spatial_bins = num_spatial_bins
-    self._depth = depth
-    self._crop_size = crop_size
-    self._box_code_size = box_code_size
-
-  @property
-  def num_classes(self):
-    return self._num_classes
-
-  def _predict(self, image_features, num_predictions_per_location,
-               proposal_boxes):
-    """Computes encoded object locations and corresponding confidences.
+    super(KerasBoxPredictor, self).__init__()
 
-    Args:
-      image_features: A list of float tensors of shape [batch_size, height_i,
-      width_i, channels_i] containing features for a batch of images.
-      num_predictions_per_location: A list of integers representing the number
-        of box predictions to be made per spatial location for each feature map.
-        Currently, this must be set to [1], or an error will be raised.
-      proposal_boxes: A float tensor of shape [batch_size, num_proposals,
-        box_code_size].
-
-    Returns:
-      box_encodings: A list of float tensors of shape
-        [batch_size, num_anchors_i, q, code_size] representing the location of
-        the objects, where q is 1 or the number of classes. Each entry in the
-        list corresponds to a feature map in the input `image_features` list.
-      class_predictions_with_background: A list of float tensors of shape
-        [batch_size, num_anchors_i, num_classes + 1] representing the class
-        predictions for the proposals. Each entry in the list corresponds to a
-        feature map in the input `image_features` list.
-
-    Raises:
-      ValueError: if num_predictions_per_location is not 1 or if
-        len(image_features) is not 1.
-    """
-    if (len(num_predictions_per_location) != 1 or
-        num_predictions_per_location[0] != 1):
-      raise ValueError('Currently RfcnBoxPredictor only supports '
-                       'predicting a single box per class per location.')
-    if len(image_features) != 1:
-      raise ValueError('length of `image_features` must be 1. Found {}'.
-                       format(len(image_features)))
-    image_feature = image_features[0]
-    num_predictions_per_location = num_predictions_per_location[0]
-    batch_size = tf.shape(proposal_boxes)[0]
-    num_boxes = tf.shape(proposal_boxes)[1]
-    def get_box_indices(proposals):
-      proposals_shape = proposals.get_shape().as_list()
-      if any(dim is None for dim in proposals_shape):
-        proposals_shape = tf.shape(proposals)
-      ones_mat = tf.ones(proposals_shape[:2], dtype=tf.int32)
-      multiplier = tf.expand_dims(
-          tf.range(start=0, limit=proposals_shape[0]), 1)
-      return tf.reshape(ones_mat * multiplier, [-1])
-
-    net = image_feature
-    with slim.arg_scope(self._conv_hyperparams_fn()):
-      net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')
-      # Location predictions.
-      location_feature_map_depth = (self._num_spatial_bins[0] *
-                                    self._num_spatial_bins[1] *
-                                    self.num_classes *
-                                    self._box_code_size)
-      location_feature_map = slim.conv2d(net, location_feature_map_depth,
-                                         [1, 1], activation_fn=None,
-                                         scope='refined_locations')
-      box_encodings = ops.position_sensitive_crop_regions(
-          location_feature_map,
-          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),
-          box_ind=get_box_indices(proposal_boxes),
-          crop_size=self._crop_size,
-          num_spatial_bins=self._num_spatial_bins,
-          global_pool=True)
-      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[1, 2])
-      box_encodings = tf.reshape(box_encodings,
-                                 [batch_size * num_boxes, 1, self.num_classes,
-                                  self._box_code_size])
-
-      # Class predictions.
-      total_classes = self.num_classes + 1  # Account for background class.
-      class_feature_map_depth = (self._num_spatial_bins[0] *
-                                 self._num_spatial_bins[1] *
-                                 total_classes)
-      class_feature_map = slim.conv2d(net, class_feature_map_depth, [1, 1],
-                                      activation_fn=None,
-                                      scope='class_predictions')
-      class_predictions_with_background = ops.position_sensitive_crop_regions(
-          class_feature_map,
-          boxes=tf.reshape(proposal_boxes, [-1, self._box_code_size]),
-          box_ind=get_box_indices(proposal_boxes),
-          crop_size=self._crop_size,
-          num_spatial_bins=self._num_spatial_bins,
-          global_pool=True)
-      class_predictions_with_background = tf.squeeze(
-          class_predictions_with_background, squeeze_dims=[1, 2])
-      class_predictions_with_background = tf.reshape(
-          class_predictions_with_background,
-          [batch_size * num_boxes, 1, total_classes])
-
-    return {BOX_ENCODINGS: [box_encodings],
-            CLASS_PREDICTIONS_WITH_BACKGROUND:
-            [class_predictions_with_background]}
-
-
-# TODO(rathodv): Change the implementation to return lists of predictions.
-class MaskRCNNBoxPredictor(BoxPredictor):
-  """Mask R-CNN Box Predictor.
-
-  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).
-  Mask R-CNN. arXiv preprint arXiv:1703.06870.
-
-  This is used for the second stage of the Mask R-CNN detector where proposals
-  cropped from an image are arranged along the batch dimension of the input
-  image_features tensor. Notice that locations are *not* shared across classes,
-  thus for each anchor, a separate prediction is made for each class.
-
-  In addition to predicting boxes and classes, optionally this class allows
-  predicting masks and/or keypoints inside detection boxes.
-
-  Currently this box predictor makes per-class predictions; that is, each
-  anchor makes a separate box prediction for each class.
-  """
-
-  def __init__(self,
-               is_training,
-               num_classes,
-               fc_hyperparams_fn,
-               use_dropout,
-               dropout_keep_prob,
-               box_code_size,
-               conv_hyperparams_fn=None,
-               predict_instance_masks=False,
-               mask_height=14,
-               mask_width=14,
-               mask_prediction_num_conv_layers=2,
-               mask_prediction_conv_depth=256,
-               masks_are_class_agnostic=False,
-               predict_keypoints=False,
-               share_box_across_classes=False):
-    """Constructor.
-
-    Args:
-      is_training: Indicates whether the BoxPredictor is in training mode.
-      num_classes: number of classes.  Note that num_classes *does not*
-        include the background category, so if groundtruth labels take values
-        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
-        assigned classification targets can range from {0,... K}).
-      fc_hyperparams_fn: A function to generate tf-slim arg_scope with
-        hyperparameters for fully connected ops.
-      use_dropout: Option to use dropout or not.  Note that a single dropout
-        op is applied here prior to both box and class predictions, which stands
-        in contrast to the ConvolutionalBoxPredictor below.
-      dropout_keep_prob: Keep probability for dropout.
-        This is only used if use_dropout is True.
-      box_code_size: Size of encoding for each box.
-      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-        hyperparameters for convolution ops.
-      predict_instance_masks: Whether to predict object masks inside detection
-        boxes.
-      mask_height: Desired output mask height. The default value is 14.
-      mask_width: Desired output mask width. The default value is 14.
-      mask_prediction_num_conv_layers: Number of convolution layers applied to
-        the image_features in mask prediction branch.
-      mask_prediction_conv_depth: The depth for the first conv2d_transpose op
-        applied to the image_features in the mask prediction branch. If set
-        to 0, the depth of the convolution layers will be automatically chosen
-        based on the number of object classes and the number of channels in the
-        image features.
-      masks_are_class_agnostic: Boolean determining if the mask-head is
-        class-agnostic or not.
-      predict_keypoints: Whether to predict keypoints insde detection boxes.
-      share_box_across_classes: Whether to share boxes across classes rather
-        than use a different box for each class.
+    self._is_training = is_training
+    self._num_classes = num_classes
+    self._freeze_batchnorm = freeze_batchnorm
+    self._inplace_batchnorm_update = inplace_batchnorm_update
 
-    Raises:
-      ValueError: If predict_instance_masks is true but conv_hyperparams is not
-        set.
-      ValueError: If predict_keypoints is true since it is not implemented yet.
-      ValueError: If mask_prediction_num_conv_layers is smaller than two.
-    """
-    super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)
-    self._fc_hyperparams_fn = fc_hyperparams_fn
-    self._use_dropout = use_dropout
-    self._box_code_size = box_code_size
-    self._dropout_keep_prob = dropout_keep_prob
-    self._conv_hyperparams_fn = conv_hyperparams_fn
-    self._predict_instance_masks = predict_instance_masks
-    self._mask_height = mask_height
-    self._mask_width = mask_width
-    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers
-    self._mask_prediction_conv_depth = mask_prediction_conv_depth
-    self._masks_are_class_agnostic = masks_are_class_agnostic
-    self._predict_keypoints = predict_keypoints
-    self._share_box_across_classes = share_box_across_classes
-    if self._predict_keypoints:
-      raise ValueError('Keypoint prediction is unimplemented.')
-    if ((self._predict_instance_masks or self._predict_keypoints) and
-        self._conv_hyperparams_fn is None):
-      raise ValueError('`conv_hyperparams` must be provided when predicting '
-                       'masks.')
-    if self._mask_prediction_num_conv_layers < 2:
-      raise ValueError(
-          'Mask prediction should consist of at least 2 conv layers')
+  @property
+  def is_keras_model(self):
+    return True
 
   @property
   def num_classes(self):
     return self._num_classes
 
-  @property
-  def predicts_instance_masks(self):
-    return self._predict_instance_masks
-
-  def _predict_boxes_and_classes(self, image_features):
-    """Predicts boxes and class scores.
-
-    Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
-
-    Returns:
-      box_encodings: A float tensor of shape
-        [batch_size, 1, num_classes, code_size] representing the location of the
-        objects.
-      class_predictions_with_background: A float tensor of shape
-        [batch_size, 1, num_classes + 1] representing the class predictions for
-        the proposals.
-    """
-    spatial_averaged_image_features = tf.reduce_mean(image_features, [1, 2],
-                                                     keep_dims=True,
-                                                     name='AvgPool')
-    flattened_image_features = slim.flatten(spatial_averaged_image_features)
-    if self._use_dropout:
-      flattened_image_features = slim.dropout(flattened_image_features,
-                                              keep_prob=self._dropout_keep_prob,
-                                              is_training=self._is_training)
-    number_of_boxes = 1
-    if not self._share_box_across_classes:
-      number_of_boxes = self._num_classes
-
-    with slim.arg_scope(self._fc_hyperparams_fn()):
-      box_encodings = slim.fully_connected(
-          flattened_image_features,
-          number_of_boxes * self._box_code_size,
-          activation_fn=None,
-          scope='BoxEncodingPredictor')
-      class_predictions_with_background = slim.fully_connected(
-          flattened_image_features,
-          self._num_classes + 1,
-          activation_fn=None,
-          scope='ClassPredictor')
-    box_encodings = tf.reshape(
-        box_encodings, [-1, 1, number_of_boxes, self._box_code_size])
-    class_predictions_with_background = tf.reshape(
-        class_predictions_with_background, [-1, 1, self._num_classes + 1])
-    return box_encodings, class_predictions_with_background
-
-  def _get_mask_predictor_conv_depth(self, num_feature_channels, num_classes,
-                                     class_weight=3.0, feature_weight=2.0):
-    """Computes the depth of the mask predictor convolutions.
-
-    Computes the depth of the mask predictor convolutions given feature channels
-    and number of classes by performing a weighted average of the two in
-    log space to compute the number of convolution channels. The weights that
-    are used for computing the weighted average do not need to sum to 1.
-
-    Args:
-      num_feature_channels: An integer containing the number of feature
-        channels.
-      num_classes: An integer containing the number of classes.
-      class_weight: Class weight used in computing the weighted average.
-      feature_weight: Feature weight used in computing the weighted average.
-
-    Returns:
-      An integer containing the number of convolution channels used by mask
-        predictor.
-    """
-    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)
-    num_classes_log = math.log(float(num_classes), 2.0)
-    weighted_num_feature_channels_log = (
-        num_feature_channels_log * feature_weight)
-    weighted_num_classes_log = num_classes_log * class_weight
-    total_weight = feature_weight + class_weight
-    num_conv_channels_log = round(
-        (weighted_num_feature_channels_log + weighted_num_classes_log) /
-        total_weight)
-    return int(math.pow(2.0, num_conv_channels_log))
-
-  def _predict_masks(self, image_features):
-    """Performs mask prediction.
-
-    Args:
-      image_features: A float tensor of shape [batch_size, height, width,
-        channels] containing features for a batch of images.
+  def call(self, image_features, scope=None, **kwargs):
+    """Computes encoded object locations and corresponding confidences.
 
-    Returns:
-      instance_masks: A float tensor of shape
-          [batch_size, 1, num_classes, image_height, image_width].
-    """
-    num_conv_channels = self._mask_prediction_conv_depth
-    if num_conv_channels == 0:
-      num_feature_channels = image_features.get_shape().as_list()[3]
-      num_conv_channels = self._get_mask_predictor_conv_depth(
-          num_feature_channels, self.num_classes)
-    with slim.arg_scope(self._conv_hyperparams_fn()):
-      upsampled_features = tf.image.resize_bilinear(
-          image_features,
-          [self._mask_height, self._mask_width],
-          align_corners=True)
-      for _ in range(self._mask_prediction_num_conv_layers - 1):
-        upsampled_features = slim.conv2d(
-            upsampled_features,
-            num_outputs=num_conv_channels,
-            kernel_size=[3, 3])
-      num_masks = 1 if self._masks_are_class_agnostic else self.num_classes
-      mask_predictions = slim.conv2d(upsampled_features,
-                                     num_outputs=num_masks,
-                                     activation_fn=None,
-                                     kernel_size=[3, 3])
-      return tf.expand_dims(
-          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),
-          axis=1,
-          name='MaskPredictor')
-
-  def _predict(self, image_features, num_predictions_per_location,
-               predict_boxes_and_classes=True, predict_auxiliary_outputs=False):
-    """Optionally computes encoded object locations, confidences, and masks.
-
-    Flattens image_features and applies fully connected ops (with no
-    non-linearity) to predict box encodings and class predictions.  In this
-    setting, anchors are not spatially arranged in any way and are assumed to
-    have been folded into the batch dimension.  Thus we output 1 for the
-    anchors dimension.
-
-    Also optionally predicts instance masks.
-    The mask prediction head is based on the Mask RCNN paper with the following
-    modifications: We replace the deconvolution layer with a bilinear resize
-    and a convolution.
+    Takes a list of high level image feature maps as input and produces a list
+    of box encodings and a list of class scores where each element in the output
+    lists correspond to the feature maps in the input list.
 
     Args:
       image_features: A list of float tensors of shape [batch_size, height_i,
-        width_i, channels_i] containing features for a batch of images.
-      num_predictions_per_location: A list of integers representing the number
-        of box predictions to be made per spatial location for each feature map.
-        Currently, this must be set to [1], or an error will be raised.
-      predict_boxes_and_classes: If true, the function will perform box
-        refinement and classification.
-      predict_auxiliary_outputs: If true, the function will perform other
-        predictions such as mask, keypoint, boundaries, etc. if any.
+      width_i, channels_i] containing features for a batch of images.
+      scope: Variable and Op scope name.
+      **kwargs: Additional keyword arguments for specific implementations of
+              BoxPredictor.
 
     Returns:
-      A dictionary containing the following tensors.
-        box_encodings: A float tensor of shape
-          [batch_size, 1, num_classes, code_size] representing the
-          location of the objects.
-        class_predictions_with_background: A float tensor of shape
-          [batch_size, 1, num_classes + 1] representing the class
-          predictions for the proposals.
-      If predict_masks is True the dictionary also contains:
-        instance_masks: A float tensor of shape
-          [batch_size, 1, num_classes, image_height, image_width]
-      If predict_keypoints is True the dictionary also contains:
-        keypoints: [batch_size, 1, num_keypoints, 2]
-
-    Raises:
-      ValueError: If num_predictions_per_location is not 1 or if both
-        predict_boxes_and_classes and predict_auxiliary_outputs are false or if
-        len(image_features) is not 1.
+      A dictionary containing at least the following tensors.
+        box_encodings: A list of float tensors. Each entry in the list
+          corresponds to a feature map in the input `image_features` list. All
+          tensors in the list have one of the two following shapes:
+          a. [batch_size, num_anchors_i, q, code_size] representing the location
+            of the objects, where q is 1 or the number of classes.
+          b. [batch_size, num_anchors_i, code_size].
+        class_predictions_with_background: A list of float tensors of shape
+          [batch_size, num_anchors_i, num_classes + 1] representing the class
+          predictions for the proposals. Each entry in the list corresponds to a
+          feature map in the input `image_features` list.
     """
-    if (len(num_predictions_per_location) != 1 or
-        num_predictions_per_location[0] != 1):
-      raise ValueError('Currently FullyConnectedBoxPredictor only supports '
-                       'predicting a single box per class per location.')
-    if not predict_boxes_and_classes and not predict_auxiliary_outputs:
-      raise ValueError('Should perform at least one prediction.')
-    if len(image_features) != 1:
-      raise ValueError('length of `image_features` must be 1. Found {}'.
-                       format(len(image_features)))
-    image_feature = image_features[0]
-    num_predictions_per_location = num_predictions_per_location[0]
-    predictions_dict = {}
-
-    if predict_boxes_and_classes:
-      (box_encodings, class_predictions_with_background
-      ) = self._predict_boxes_and_classes(image_feature)
-      predictions_dict[BOX_ENCODINGS] = box_encodings
-      predictions_dict[
-          CLASS_PREDICTIONS_WITH_BACKGROUND] = class_predictions_with_background
-
-    if self._predict_instance_masks and predict_auxiliary_outputs:
-      predictions_dict[MASK_PREDICTIONS] = self._predict_masks(image_feature)
-
-    return predictions_dict
-
-
-class _NoopVariableScope(object):
-  """A dummy class that does not push any scope."""
-
-  def __enter__(self):
-    return None
-
-  def __exit__(self, exc_type, exc_value, traceback):
-    return False
-
+    return self._predict(image_features, **kwargs)
 
-class ConvolutionalBoxPredictor(BoxPredictor):
-  """Convolutional Box Predictor.
-
-  Optionally add an intermediate 1x1 convolutional layer after features and
-  predict in parallel branches box_encodings and
-  class_predictions_with_background.
-
-  Currently this box predictor assumes that predictions are "shared" across
-  classes --- that is each anchor makes box predictions which do not depend
-  on class.
-  """
-
-  def __init__(self,
-               is_training,
-               num_classes,
-               conv_hyperparams_fn,
-               min_depth,
-               max_depth,
-               num_layers_before_predictor,
-               use_dropout,
-               dropout_keep_prob,
-               kernel_size,
-               box_code_size,
-               apply_sigmoid_to_scores=False,
-               class_prediction_bias_init=0.0,
-               use_depthwise=False):
-    """Constructor.
-
-    Args:
-      is_training: Indicates whether the BoxPredictor is in training mode.
-      num_classes: number of classes.  Note that num_classes *does not*
-        include the background category, so if groundtruth labels take values
-        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
-        assigned classification targets can range from {0,... K}).
-      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-        hyperparameters for convolution ops.
-      min_depth: Minimum feature depth prior to predicting box encodings
-        and class predictions.
-      max_depth: Maximum feature depth prior to predicting box encodings
-        and class predictions. If max_depth is set to 0, no additional
-        feature map will be inserted before location and class predictions.
-      num_layers_before_predictor: Number of the additional conv layers before
-        the predictor.
-      use_dropout: Option to use dropout for class prediction or not.
-      dropout_keep_prob: Keep probability for dropout.
-        This is only used if use_dropout is True.
-      kernel_size: Size of final convolution kernel.  If the
-        spatial resolution of the feature map is smaller than the kernel size,
-        then the kernel size is automatically set to be
-        min(feature_width, feature_height).
-      box_code_size: Size of encoding for each box.
-      apply_sigmoid_to_scores: if True, apply the sigmoid on the output
-        class_predictions.
-      class_prediction_bias_init: constant value to initialize bias of the last
-        conv2d layer before class prediction.
-      use_depthwise: Whether to use depthwise convolutions for prediction
-        steps. Default is False.
-
-    Raises:
-      ValueError: if min_depth > max_depth.
-    """
-    super(ConvolutionalBoxPredictor, self).__init__(is_training, num_classes)
-    if min_depth > max_depth:
-      raise ValueError('min_depth should be less than or equal to max_depth')
-    self._conv_hyperparams_fn = conv_hyperparams_fn
-    self._min_depth = min_depth
-    self._max_depth = max_depth
-    self._num_layers_before_predictor = num_layers_before_predictor
-    self._use_dropout = use_dropout
-    self._kernel_size = kernel_size
-    self._box_code_size = box_code_size
-    self._dropout_keep_prob = dropout_keep_prob
-    self._apply_sigmoid_to_scores = apply_sigmoid_to_scores
-    self._class_prediction_bias_init = class_prediction_bias_init
-    self._use_depthwise = use_depthwise
-
-  def _predict(self, image_features, num_predictions_per_location_list):
-    """Computes encoded object locations and corresponding confidences.
+  @abstractmethod
+  def _predict(self, image_features, **kwargs):
+    """Implementations must override this method.
 
     Args:
       image_features: A list of float tensors of shape [batch_size, height_i,
         width_i, channels_i] containing features for a batch of images.
-      num_predictions_per_location_list: A list of integers representing the
-        number of box predictions to be made per spatial location for each
-        feature map.
-
-    Returns:
-      box_encodings: A list of float tensors of shape
-        [batch_size, num_anchors_i, q, code_size] representing the location of
-        the objects, where q is 1 or the number of classes. Each entry in the
-        list corresponds to a feature map in the input `image_features` list.
-      class_predictions_with_background: A list of float tensors of shape
-        [batch_size, num_anchors_i, num_classes + 1] representing the class
-        predictions for the proposals. Each entry in the list corresponds to a
-        feature map in the input `image_features` list.
-    """
-    box_encodings_list = []
-    class_predictions_list = []
-    # TODO(rathodv): Come up with a better way to generate scope names
-    # in box predictor once we have time to retrain all models in the zoo.
-    # The following lines create scope names to be backwards compatible with the
-    # existing checkpoints.
-    box_predictor_scopes = [_NoopVariableScope()]
-    if len(image_features) > 1:
-      box_predictor_scopes = [
-          tf.variable_scope('BoxPredictor_{}'.format(i))
-          for i in range(len(image_features))
-      ]
-
-    for (image_feature,
-         num_predictions_per_location, box_predictor_scope) in zip(
-             image_features, num_predictions_per_location_list,
-             box_predictor_scopes):
-      with box_predictor_scope:
-        # Add a slot for the background class.
-        num_class_slots = self.num_classes + 1
-        net = image_feature
-        with slim.arg_scope(self._conv_hyperparams_fn()), \
-             slim.arg_scope([slim.dropout], is_training=self._is_training):
-          # Add additional conv layers before the class predictor.
-          features_depth = static_shape.get_depth(image_feature.get_shape())
-          depth = max(min(features_depth, self._max_depth), self._min_depth)
-          tf.logging.info('depth of additional conv before box predictor: {}'.
-                          format(depth))
-          if depth > 0 and self._num_layers_before_predictor > 0:
-            for i in range(self._num_layers_before_predictor):
-              net = slim.conv2d(
-                  net, depth, [1, 1], scope='Conv2d_%d_1x1_%d' % (i, depth))
-          with slim.arg_scope([slim.conv2d], activation_fn=None,
-                              normalizer_fn=None, normalizer_params=None):
-            if self._use_depthwise:
-              box_encodings = slim.separable_conv2d(
-                  net, None, [self._kernel_size, self._kernel_size],
-                  padding='SAME', depth_multiplier=1, stride=1,
-                  rate=1, scope='BoxEncodingPredictor_depthwise')
-              box_encodings = slim.conv2d(
-                  box_encodings,
-                  num_predictions_per_location * self._box_code_size, [1, 1],
-                  scope='BoxEncodingPredictor')
-            else:
-              box_encodings = slim.conv2d(
-                  net, num_predictions_per_location * self._box_code_size,
-                  [self._kernel_size, self._kernel_size],
-                  scope='BoxEncodingPredictor')
-            if self._use_dropout:
-              net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
-            if self._use_depthwise:
-              class_predictions_with_background = slim.separable_conv2d(
-                  net, None, [self._kernel_size, self._kernel_size],
-                  padding='SAME', depth_multiplier=1, stride=1,
-                  rate=1, scope='ClassPredictor_depthwise')
-              class_predictions_with_background = slim.conv2d(
-                  class_predictions_with_background,
-                  num_predictions_per_location * num_class_slots,
-                  [1, 1], scope='ClassPredictor')
-            else:
-              class_predictions_with_background = slim.conv2d(
-                  net, num_predictions_per_location * num_class_slots,
-                  [self._kernel_size, self._kernel_size],
-                  scope='ClassPredictor',
-                  biases_initializer=tf.constant_initializer(
-                      self._class_prediction_bias_init))
-            if self._apply_sigmoid_to_scores:
-              class_predictions_with_background = tf.sigmoid(
-                  class_predictions_with_background)
-
-        combined_feature_map_shape = (shape_utils.
-                                      combined_static_and_dynamic_shape(
-                                          image_feature))
-        box_encodings = tf.reshape(
-            box_encodings, tf.stack([combined_feature_map_shape[0],
-                                     combined_feature_map_shape[1] *
-                                     combined_feature_map_shape[2] *
-                                     num_predictions_per_location,
-                                     1, self._box_code_size]))
-        box_encodings_list.append(box_encodings)
-        class_predictions_with_background = tf.reshape(
-            class_predictions_with_background,
-            tf.stack([combined_feature_map_shape[0],
-                      combined_feature_map_shape[1] *
-                      combined_feature_map_shape[2] *
-                      num_predictions_per_location,
-                      num_class_slots]))
-        class_predictions_list.append(class_predictions_with_background)
-    return {
-        BOX_ENCODINGS: box_encodings_list,
-        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
-    }
-
-
-# TODO(rathodv): Replace with slim.arg_scope_func_key once its available
-# externally.
-def _arg_scope_func_key(op):
-  """Returns a key that can be used to index arg_scope dictionary."""
-  return getattr(op, '_key_op', str(op))
-
-
-# TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above
-# since they are very similar.
-class WeightSharedConvolutionalBoxPredictor(BoxPredictor):
-  """Convolutional Box Predictor with weight sharing.
-
-  Defines the box predictor as defined in
-  https://arxiv.org/abs/1708.02002. This class differs from
-  ConvolutionalBoxPredictor in that it shares weights and biases while
-  predicting from different feature maps. However, batch_norm parameters are not
-  shared because the statistics of the activations vary among the different
-  feature maps.
-
-  Also note that separate multi-layer towers are constructed for the box
-  encoding and class predictors respectively.
-  """
-
-  def __init__(self,
-               is_training,
-               num_classes,
-               conv_hyperparams_fn,
-               depth,
-               num_layers_before_predictor,
-               box_code_size,
-               kernel_size=3,
-               class_prediction_bias_init=0.0,
-               use_dropout=False,
-               dropout_keep_prob=0.8,
-               share_prediction_tower=False):
-    """Constructor.
-
-    Args:
-      is_training: Indicates whether the BoxPredictor is in training mode.
-      num_classes: number of classes.  Note that num_classes *does not*
-        include the background category, so if groundtruth labels take values
-        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
-        assigned classification targets can range from {0,... K}).
-      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
-        hyperparameters for convolution ops.
-      depth: depth of conv layers.
-      num_layers_before_predictor: Number of the additional conv layers before
-        the predictor.
-      box_code_size: Size of encoding for each box.
-      kernel_size: Size of final convolution kernel.
-      class_prediction_bias_init: constant value to initialize bias of the last
-        conv2d layer before class prediction.
-      use_dropout: Whether to apply dropout to class prediction head.
-      dropout_keep_prob: Probability of keeping activiations.
-      share_prediction_tower: Whether to share the multi-layer tower between box
-        prediction and class prediction heads.
-    """
-    super(WeightSharedConvolutionalBoxPredictor, self).__init__(is_training,
-                                                                num_classes)
-    self._conv_hyperparams_fn = conv_hyperparams_fn
-    self._depth = depth
-    self._num_layers_before_predictor = num_layers_before_predictor
-    self._box_code_size = box_code_size
-    self._kernel_size = kernel_size
-    self._class_prediction_bias_init = class_prediction_bias_init
-    self._use_dropout = use_dropout
-    self._dropout_keep_prob = dropout_keep_prob
-    self._share_prediction_tower = share_prediction_tower
-
-  def _predict(self, image_features, num_predictions_per_location_list):
-    """Computes encoded object locations and corresponding confidences.
-
-    Args:
-      image_features: A list of float tensors of shape [batch_size, height_i,
-        width_i, channels] containing features for a batch of images. Note that
-        when not all tensors in the list have the same number of channels, an
-        additional projection layer will be added on top the tensor to generate
-        feature map with number of channels consitent with the majority.
-      num_predictions_per_location_list: A list of integers representing the
-        number of box predictions to be made per spatial location for each
-        feature map. Note that all values must be the same since the weights are
-        shared.
+      **kwargs: Additional keyword arguments for specific implementations of
+              BoxPredictor.
 
     Returns:
-      box_encodings: A list of float tensors of shape
-        [batch_size, num_anchors_i, code_size] representing the location of
-        the objects. Each entry in the list corresponds to a feature map in the
-        input `image_features` list.
-      class_predictions_with_background: A list of float tensors of shape
-        [batch_size, num_anchors_i, num_classes + 1] representing the class
-        predictions for the proposals. Each entry in the list corresponds to a
-        feature map in the input `image_features` list.
-
-
-    Raises:
-      ValueError: If the image feature maps do not have the same number of
-        channels or if the num predictions per locations is differs between the
-        feature maps.
+      A dictionary containing at least the following tensors.
+        box_encodings: A list of float tensors. Each entry in the list
+          corresponds to a feature map in the input `image_features` list. All
+          tensors in the list have one of the two following shapes:
+          a. [batch_size, num_anchors_i, q, code_size] representing the location
+            of the objects, where q is 1 or the number of classes.
+          b. [batch_size, num_anchors_i, code_size].
+        class_predictions_with_background: A list of float tensors of shape
+          [batch_size, num_anchors_i, num_classes + 1] representing the class
+          predictions for the proposals. Each entry in the list corresponds to a
+          feature map in the input `image_features` list.
     """
-    if len(set(num_predictions_per_location_list)) > 1:
-      raise ValueError('num predictions per location must be same for all'
-                       'feature maps, found: {}'.format(
-                           num_predictions_per_location_list))
-    feature_channels = [
-        image_feature.shape[3].value for image_feature in image_features
-    ]
-    has_different_feature_channels = len(set(feature_channels)) > 1
-    if has_different_feature_channels:
-      inserted_layer_counter = 0
-      target_channel = max(set(feature_channels), key=feature_channels.count)
-      tf.logging.info('Not all feature maps have the same number of '
-                      'channels, found: {}, addition project layers '
-                      'to bring all feature maps to uniform channels '
-                      'of {}'.format(feature_channels, target_channel))
-    box_encodings_list = []
-    class_predictions_list = []
-    num_class_slots = self.num_classes + 1
-    for feature_index, (image_feature,
-                        num_predictions_per_location) in enumerate(
-                            zip(image_features,
-                                num_predictions_per_location_list)):
-      # Add a slot for the background class.
-      with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
-                             reuse=tf.AUTO_REUSE):
-        with slim.arg_scope(self._conv_hyperparams_fn()) as sc:
-          apply_batch_norm = _arg_scope_func_key(slim.batch_norm) in sc
-          # Insert an additional projection layer if necessary.
-          if (has_different_feature_channels and
-              image_feature.shape[3].value != target_channel):
-            image_feature = slim.conv2d(
-                image_feature,
-                target_channel, [1, 1],
-                stride=1,
-                padding='SAME',
-                activation_fn=None,
-                normalizer_fn=(tf.identity if apply_batch_norm else None),
-                scope='ProjectionLayer/conv2d_{}'.format(
-                    inserted_layer_counter))
-            if apply_batch_norm:
-              image_feature = slim.batch_norm(
-                  image_feature,
-                  scope='ProjectionLayer/conv2d_{}/BatchNorm'.format(
-                      inserted_layer_counter))
-            inserted_layer_counter += 1
-          box_encodings_net = image_feature
-          class_predictions_net = image_feature
-          for i in range(self._num_layers_before_predictor):
-            box_prediction_tower_prefix = (
-                'PredictionTower' if self._share_prediction_tower
-                else 'BoxPredictionTower')
-            box_encodings_net = slim.conv2d(
-                box_encodings_net,
-                self._depth,
-                [self._kernel_size, self._kernel_size],
-                stride=1,
-                padding='SAME',
-                activation_fn=None,
-                normalizer_fn=(tf.identity if apply_batch_norm else None),
-                scope='{}/conv2d_{}'.format(box_prediction_tower_prefix, i))
-            if apply_batch_norm:
-              box_encodings_net = slim.batch_norm(
-                  box_encodings_net,
-                  scope='{}/conv2d_{}/BatchNorm/feature_{}'.
-                  format(box_prediction_tower_prefix, i, feature_index))
-            box_encodings_net = tf.nn.relu6(box_encodings_net)
-          box_encodings = slim.conv2d(
-              box_encodings_net,
-              num_predictions_per_location * self._box_code_size,
-              [self._kernel_size, self._kernel_size],
-              activation_fn=None, stride=1, padding='SAME',
-              normalizer_fn=None,
-              scope='BoxPredictor')
-
-          if self._share_prediction_tower:
-            class_predictions_net = box_encodings_net
-          else:
-            for i in range(self._num_layers_before_predictor):
-              class_predictions_net = slim.conv2d(
-                  class_predictions_net,
-                  self._depth,
-                  [self._kernel_size, self._kernel_size],
-                  stride=1,
-                  padding='SAME',
-                  activation_fn=None,
-                  normalizer_fn=(tf.identity if apply_batch_norm else None),
-                  scope='ClassPredictionTower/conv2d_{}'.format(i))
-              if apply_batch_norm:
-                class_predictions_net = slim.batch_norm(
-                    class_predictions_net,
-                    scope='ClassPredictionTower/conv2d_{}/BatchNorm/feature_{}'
-                    .format(i, feature_index))
-              class_predictions_net = tf.nn.relu6(class_predictions_net)
-          if self._use_dropout:
-            class_predictions_net = slim.dropout(
-                class_predictions_net, keep_prob=self._dropout_keep_prob)
-          class_predictions_with_background = slim.conv2d(
-              class_predictions_net,
-              num_predictions_per_location * num_class_slots,
-              [self._kernel_size, self._kernel_size],
-              activation_fn=None, stride=1, padding='SAME',
-              normalizer_fn=None,
-              biases_initializer=tf.constant_initializer(
-                  self._class_prediction_bias_init),
-              scope='ClassPredictor')
-
-          combined_feature_map_shape = (shape_utils.
-                                        combined_static_and_dynamic_shape(
-                                            image_feature))
-          box_encodings = tf.reshape(
-              box_encodings, tf.stack([combined_feature_map_shape[0],
-                                       combined_feature_map_shape[1] *
-                                       combined_feature_map_shape[2] *
-                                       num_predictions_per_location,
-                                       self._box_code_size]))
-          box_encodings_list.append(box_encodings)
-          class_predictions_with_background = tf.reshape(
-              class_predictions_with_background,
-              tf.stack([combined_feature_map_shape[0],
-                        combined_feature_map_shape[1] *
-                        combined_feature_map_shape[2] *
-                        num_predictions_per_location,
-                        num_class_slots]))
-          class_predictions_list.append(class_predictions_with_background)
-    return {
-        BOX_ENCODINGS: box_encodings_list,
-        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
-    }
+    raise NotImplementedError
diff --git a/research/object_detection/core/preprocessor.py b/research/object_detection/core/preprocessor.py
index 85187ec7..3e525989 100644
--- a/research/object_detection/core/preprocessor.py
+++ b/research/object_detection/core/preprocessor.py
@@ -2925,6 +2925,29 @@ def ssd_random_crop_pad_fixed_aspect_ratio(
   return result
 
 
+def convert_class_logits_to_softmax(multiclass_scores, temperature=1.0):
+  """Converts multiclass logits to softmax scores after applying temperature.
+
+  Args:
+    multiclass_scores: float32 tensor of shape
+      [num_instances, num_classes] representing the score for each box for each
+      class.
+    temperature: Scale factor to use prior to applying softmax. Larger
+      temperatures give more uniform distruibutions after softmax.
+
+  Returns:
+    multiclass_scores: float32 tensor of shape
+      [num_instances, num_classes] with scaling and softmax applied.
+  """
+
+  # Multiclass scores must be stored as logits. Apply temp and softmax.
+  multiclass_scores_scaled = tf.divide(
+      multiclass_scores, temperature, name='scale_logits')
+  multiclass_scores = tf.nn.softmax(multiclass_scores_scaled, name='softmax')
+
+  return multiclass_scores
+
+
 def get_default_func_arg_map(include_label_scores=False,
                              include_multiclass_scores=False,
                              include_instance_masks=False,
@@ -3003,8 +3026,7 @@ def get_default_func_arg_map(include_label_scores=False,
       random_crop_pad_image: (fields.InputDataFields.image,
                               fields.InputDataFields.groundtruth_boxes,
                               fields.InputDataFields.groundtruth_classes,
-                              groundtruth_label_scores,
-                              multiclass_scores),
+                              groundtruth_label_scores, multiclass_scores),
       random_crop_to_aspect_ratio: (
           fields.InputDataFields.image,
           fields.InputDataFields.groundtruth_boxes,
@@ -3051,20 +3073,15 @@ def get_default_func_arg_map(include_label_scores=False,
       subtract_channel_mean: (fields.InputDataFields.image,),
       one_hot_encoding: (fields.InputDataFields.groundtruth_image_classes,),
       rgb_to_gray: (fields.InputDataFields.image,),
-      ssd_random_crop: (
-          fields.InputDataFields.image,
-          fields.InputDataFields.groundtruth_boxes,
-          fields.InputDataFields.groundtruth_classes,
-          groundtruth_label_scores,
-          multiclass_scores,
-          groundtruth_instance_masks,
-          groundtruth_keypoints
-      ),
+      ssd_random_crop: (fields.InputDataFields.image,
+                        fields.InputDataFields.groundtruth_boxes,
+                        fields.InputDataFields.groundtruth_classes,
+                        groundtruth_label_scores, multiclass_scores,
+                        groundtruth_instance_masks, groundtruth_keypoints),
       ssd_random_crop_pad: (fields.InputDataFields.image,
                             fields.InputDataFields.groundtruth_boxes,
                             fields.InputDataFields.groundtruth_classes,
-                            groundtruth_label_scores,
-                            multiclass_scores),
+                            groundtruth_label_scores, multiclass_scores),
       ssd_random_crop_fixed_aspect_ratio: (
           fields.InputDataFields.image,
           fields.InputDataFields.groundtruth_boxes,
@@ -3079,6 +3096,7 @@ def get_default_func_arg_map(include_label_scores=False,
           groundtruth_instance_masks,
           groundtruth_keypoints,
       ),
+      convert_class_logits_to_softmax: (multiclass_scores,),
   }
 
   return prep_func_arg_map
diff --git a/research/object_detection/core/preprocessor_test.py b/research/object_detection/core/preprocessor_test.py
index 9e2c5056..47a8562f 100644
--- a/research/object_detection/core/preprocessor_test.py
+++ b/research/object_detection/core/preprocessor_test.py
@@ -2844,5 +2844,24 @@ class PreprocessorTest(tf.test.TestCase):
                                             include_instance_masks=True,
                                             include_keypoints=True)
 
+  def testConvertClassLogitsToSoftmax(self):
+    multiclass_scores = tf.constant(
+        [[1.0, 0.0], [0.5, 0.5], [1000, 1]], dtype=tf.float32)
+    temperature = 2.0
+
+    converted_multiclass_scores = (
+        preprocessor.convert_class_logits_to_softmax(
+            multiclass_scores=multiclass_scores, temperature=temperature))
+
+    expected_converted_multiclass_scores = [[[0.62245935, 0.37754068],
+                                             [0.5, 0.5], [1, 0]]]
+
+    with self.test_session() as sess:
+      (converted_multiclass_scores_) = sess.run([converted_multiclass_scores])
+
+      self.assertAllClose(converted_multiclass_scores_,
+                          expected_converted_multiclass_scores)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/region_similarity_calculator.py b/research/object_detection/core/region_similarity_calculator.py
index f344006a..793c7d38 100644
--- a/research/object_detection/core/region_similarity_calculator.py
+++ b/research/object_detection/core/region_similarity_calculator.py
@@ -24,6 +24,7 @@ from abc import abstractmethod
 import tensorflow as tf
 
 from object_detection.core import box_list_ops
+from object_detection.core import standard_fields as fields
 
 
 class RegionSimilarityCalculator(object):
@@ -33,7 +34,7 @@ class RegionSimilarityCalculator(object):
   def compare(self, boxlist1, boxlist2, scope=None):
     """Computes matrix of pairwise similarity between BoxLists.
 
-    This op (to be overriden) computes a measure of pairwise similarity between
+    This op (to be overridden) computes a measure of pairwise similarity between
     the boxes in the given BoxLists. Higher values indicate more similarity.
 
     Note that this method simply measures similarity and does not explicitly
@@ -112,3 +113,42 @@ class IoaSimilarity(RegionSimilarityCalculator):
       A tensor with shape [N, M] representing pairwise IOA scores.
     """
     return box_list_ops.ioa(boxlist1, boxlist2)
+
+
+class ThresholdedIouSimilarity(RegionSimilarityCalculator):
+  """Class to compute similarity based on thresholded IOU and score.
+
+  This class computes pairwise similarity between two BoxLists based on IOU and
+  a 'score' present in boxlist1. If IOU > threshold, then the entry in the
+  output pairwise tensor will contain `score`, otherwise 0.
+  """
+
+  def __init__(self, iou_threshold=0):
+    """Initialize the ThresholdedIouSimilarity.
+
+    Args:
+      iou_threshold: For a given pair of boxes, if the IOU is > iou_threshold,
+        then the comparison result will be the foreground probability of
+        the first box, otherwise it will be zero.
+    """
+    self._iou_threshold = iou_threshold
+
+  def _compare(self, boxlist1, boxlist2):
+    """Compute pairwise IOU similarity between the two BoxLists and score.
+
+    Args:
+      boxlist1: BoxList holding N boxes. Must have a score field.
+      boxlist2: BoxList holding M boxes.
+
+    Returns:
+      A tensor with shape [N, M] representing scores threholded by pairwise
+      iou scores.
+    """
+    ious = box_list_ops.iou(boxlist1, boxlist2)
+    scores = boxlist1.get_field(fields.BoxListFields.scores)
+    scores = tf.expand_dims(scores, axis=1)
+    row_replicated_scores = tf.tile(scores, [1, tf.shape(ious)[-1]])
+    thresholded_ious = tf.where(ious > self._iou_threshold,
+                                row_replicated_scores, tf.zeros_like(ious))
+
+    return thresholded_ious
diff --git a/research/object_detection/core/region_similarity_calculator_test.py b/research/object_detection/core/region_similarity_calculator_test.py
index 162151a3..1d0c26bb 100644
--- a/research/object_detection/core/region_similarity_calculator_test.py
+++ b/research/object_detection/core/region_similarity_calculator_test.py
@@ -18,6 +18,7 @@ import tensorflow as tf
 
 from object_detection.core import box_list
 from object_detection.core import region_similarity_calculator
+from object_detection.core import standard_fields as fields
 
 
 class RegionSimilarityCalculatorTest(tf.test.TestCase):
@@ -70,6 +71,25 @@ class RegionSimilarityCalculatorTest(tf.test.TestCase):
       self.assertAllClose(iou_output_1, exp_output_1)
       self.assertAllClose(iou_output_2, exp_output_2)
 
+  def test_get_correct_pairwise_similarity_based_on_thresholded_iou(self):
+    corners1 = tf.constant([[4.0, 3.0, 7.0, 5.0], [5.0, 6.0, 10.0, 7.0]])
+    corners2 = tf.constant([[3.0, 4.0, 6.0, 8.0], [14.0, 14.0, 15.0, 15.0],
+                            [0.0, 0.0, 20.0, 20.0]])
+    scores = tf.constant([.3, .6])
+    iou_threshold = .013
+
+    exp_output = tf.constant([[0.3, 0., 0.3], [0.6, 0., 0.]])
+    boxes1 = box_list.BoxList(corners1)
+    boxes1.add_field(fields.BoxListFields.scores, scores)
+    boxes2 = box_list.BoxList(corners2)
+    iou_similarity_calculator = (
+        region_similarity_calculator.ThresholdedIouSimilarity(
+            iou_threshold=iou_threshold))
+    iou_similarity = iou_similarity_calculator.compare(boxes1, boxes2)
+    with self.test_session() as sess:
+      iou_output = sess.run(iou_similarity)
+      self.assertAllClose(iou_output, exp_output)
+
 
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/core/target_assigner.py b/research/object_detection/core/target_assigner.py
index 14e66def..ac00a71a 100644
--- a/research/object_detection/core/target_assigner.py
+++ b/research/object_detection/core/target_assigner.py
@@ -49,7 +49,7 @@ class TargetAssigner(object):
   """Target assigner to compute classification and regression targets."""
 
   def __init__(self, similarity_calc, matcher, box_coder,
-               negative_class_weight=1.0, unmatched_cls_target=None):
+               negative_class_weight=1.0):
     """Construct Object Detection Target Assigner.
 
     Args:
@@ -60,12 +60,6 @@ class TargetAssigner(object):
         groundtruth boxes with respect to anchors.
       negative_class_weight: classification weight to be associated to negative
         anchors (default: 1.0). The weight must be in [0., 1.].
-      unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]
-        which is consistent with the classification target for each
-        anchor (and can be empty for scalar targets).  This shape must thus be
-        compatible with the groundtruth labels that are passed to the "assign"
-        function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).
-        If set to None, unmatched_cls_target is set to be [0] for each anchor.
 
     Raises:
       ValueError: if similarity_calc is not a RegionSimilarityCalculator or
@@ -81,17 +75,14 @@ class TargetAssigner(object):
     self._matcher = matcher
     self._box_coder = box_coder
     self._negative_class_weight = negative_class_weight
-    if unmatched_cls_target is None:
-      self._unmatched_cls_target = tf.constant([0], tf.float32)
-    else:
-      self._unmatched_cls_target = unmatched_cls_target
 
   @property
   def box_coder(self):
     return self._box_coder
 
+  # TODO(rathodv): move labels, scores, and weights to groundtruth_boxes fields.
   def assign(self, anchors, groundtruth_boxes, groundtruth_labels=None,
-             groundtruth_weights=None, **params):
+             unmatched_class_label=None, groundtruth_weights=None, **params):
     """Assign classification and regression targets to each anchor.
 
     For a given set of anchors and groundtruth detections, match anchors
@@ -110,6 +101,12 @@ class TargetAssigner(object):
         [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set
         to None, groundtruth_labels assumes a binary problem where all
         ground_truth boxes get a positive label (of 1).
+      unmatched_class_label: a float32 tensor with shape [d_1, d_2, ..., d_k]
+        which is consistent with the classification target for each
+        anchor (and can be empty for scalar targets).  This shape must thus be
+        compatible with the groundtruth labels that are passed to the "assign"
+        function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).
+        If set to None, unmatched_cls_target is set to be [0] for each anchor.
       groundtruth_weights: a float tensor of shape [M] indicating the weight to
         assign to all anchors match to a particular groundtruth box. The weights
         must be in [0., 1.]. If None, all weights are set to 1.
@@ -136,14 +133,17 @@ class TargetAssigner(object):
     if not isinstance(groundtruth_boxes, box_list.BoxList):
       raise ValueError('groundtruth_boxes must be an BoxList')
 
+    if unmatched_class_label is None:
+      unmatched_class_label = tf.constant([0], tf.float32)
+
     if groundtruth_labels is None:
       groundtruth_labels = tf.ones(tf.expand_dims(groundtruth_boxes.num_boxes(),
                                                   0))
       groundtruth_labels = tf.expand_dims(groundtruth_labels, -1)
+
     unmatched_shape_assert = shape_utils.assert_shape_equal(
         shape_utils.combined_static_and_dynamic_shape(groundtruth_labels)[1:],
-        shape_utils.combined_static_and_dynamic_shape(
-            self._unmatched_cls_target))
+        shape_utils.combined_static_and_dynamic_shape(unmatched_class_label))
     labels_and_box_shapes_assert = shape_utils.assert_shape_equal(
         shape_utils.combined_static_and_dynamic_shape(
             groundtruth_labels)[:1],
@@ -155,6 +155,12 @@ class TargetAssigner(object):
       if not num_gt_boxes:
         num_gt_boxes = groundtruth_boxes.num_boxes()
       groundtruth_weights = tf.ones([num_gt_boxes], dtype=tf.float32)
+
+    # set scores on the gt boxes
+    scores = 1 - groundtruth_labels[:, 0]
+
+    groundtruth_boxes.add_field(fields.BoxListFields.scores, scores)
+
     with tf.control_dependencies(
         [unmatched_shape_assert, labels_and_box_shapes_assert]):
       match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes,
@@ -164,6 +170,7 @@ class TargetAssigner(object):
                                                     groundtruth_boxes,
                                                     match)
       cls_targets = self._create_classification_targets(groundtruth_labels,
+                                                        unmatched_class_label,
                                                         match)
       reg_weights = self._create_regression_weights(match, groundtruth_weights)
       cls_weights = self._create_classification_weights(match,
@@ -245,7 +252,8 @@ class TargetAssigner(object):
     """
     return tf.constant([self._box_coder.code_size*[0]], tf.float32)
 
-  def _create_classification_targets(self, groundtruth_labels, match):
+  def _create_classification_targets(self, groundtruth_labels,
+                                     unmatched_class_label, match):
     """Create classification targets for each anchor.
 
     Assign a classification target of for each anchor to the matching
@@ -256,6 +264,11 @@ class TargetAssigner(object):
       groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]
         with labels for each of the ground_truth boxes. The subshape
         [d_1, ... d_k] can be empty (corresponding to scalar labels).
+      unmatched_class_label: a float32 tensor with shape [d_1, d_2, ..., d_k]
+        which is consistent with the classification target for each
+        anchor (and can be empty for scalar targets).  This shape must thus be
+        compatible with the groundtruth labels that are passed to the "assign"
+        function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).
       match: a matcher.Match object that provides a matching between anchors
         and groundtruth boxes.
 
@@ -266,8 +279,8 @@ class TargetAssigner(object):
     """
     return match.gather_based_on_match(
         groundtruth_labels,
-        unmatched_value=self._unmatched_cls_target,
-        ignored_value=self._unmatched_cls_target)
+        unmatched_value=unmatched_class_label,
+        ignored_value=unmatched_class_label)
 
   def _create_regression_weights(self, match, groundtruth_weights):
     """Set regression weight for each anchor.
@@ -327,8 +340,7 @@ class TargetAssigner(object):
 # TODO(rathodv): This method pulls in all the implementation dependencies into
 # core. Therefore its best to have this factory method outside of core.
 def create_target_assigner(reference, stage=None,
-                           negative_class_weight=1.0,
-                           unmatched_cls_target=None):
+                           negative_class_weight=1.0, use_matmul_gather=False):
   """Factory function for creating standard target assigners.
 
   Args:
@@ -336,12 +348,8 @@ def create_target_assigner(reference, stage=None,
     stage: string denoting stage: {proposal, detection}.
     negative_class_weight: classification weight to be associated to negative
       anchors (default: 1.0)
-    unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]
-      which is consistent with the classification target for each
-      anchor (and can be empty for scalar targets).  This shape must thus be
-      compatible with the groundtruth labels that are passed to the Assign
-      function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).
-      If set to None, unmatched_cls_target is set to be 0 for each anchor.
+    use_matmul_gather: whether to use matrix multiplication based gather which
+      are better suited for TPUs.
 
   Returns:
     TargetAssigner: desired target assigner.
@@ -358,7 +366,8 @@ def create_target_assigner(reference, stage=None,
     similarity_calc = sim_calc.IouSimilarity()
     matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.7,
                                            unmatched_threshold=0.3,
-                                           force_match_for_each_row=True)
+                                           force_match_for_each_row=True,
+                                           use_matmul_gather=use_matmul_gather)
     box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
         scale_factors=[10.0, 10.0, 5.0, 5.0])
 
@@ -366,7 +375,8 @@ def create_target_assigner(reference, stage=None,
     similarity_calc = sim_calc.IouSimilarity()
     # Uses all proposals with IOU < 0.5 as candidate negatives.
     matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
-                                           negatives_lower_than_unmatched=True)
+                                           negatives_lower_than_unmatched=True,
+                                           use_matmul_gather=use_matmul_gather)
     box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder(
         scale_factors=[10.0, 10.0, 5.0, 5.0])
 
@@ -375,21 +385,22 @@ def create_target_assigner(reference, stage=None,
     matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                            unmatched_threshold=0.1,
                                            force_match_for_each_row=False,
-                                           negatives_lower_than_unmatched=False)
+                                           negatives_lower_than_unmatched=False,
+                                           use_matmul_gather=use_matmul_gather)
     box_coder = faster_rcnn_box_coder.FasterRcnnBoxCoder()
 
   else:
     raise ValueError('No valid combination of reference and stage.')
 
   return TargetAssigner(similarity_calc, matcher, box_coder,
-                        negative_class_weight=negative_class_weight,
-                        unmatched_cls_target=unmatched_cls_target)
+                        negative_class_weight=negative_class_weight)
 
 
 def batch_assign_targets(target_assigner,
                          anchors_batch,
                          gt_box_batch,
                          gt_class_targets_batch,
+                         unmatched_class_label=None,
                          gt_weights_batch=None):
   """Batched assignment of classification and regression targets.
 
@@ -403,6 +414,11 @@ def batch_assign_targets(target_assigner,
       each tensor has shape [num_gt_boxes_i, classification_target_size] and
       num_gt_boxes_i is the number of boxes in the ith boxlist of
       gt_box_batch.
+    unmatched_class_label: a float32 tensor with shape [d_1, d_2, ..., d_k]
+      which is consistent with the classification target for each
+      anchor (and can be empty for scalar targets).  This shape must thus be
+      compatible with the groundtruth labels that are passed to the "assign"
+      function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).
     gt_weights_batch: A list of 1-D tf.float32 tensors of shape
       [num_boxes] containing weights for groundtruth boxes.
 
@@ -442,9 +458,9 @@ def batch_assign_targets(target_assigner,
     gt_weights_batch = [None] * len(gt_class_targets_batch)
   for anchors, gt_boxes, gt_class_targets, gt_weights in zip(
       anchors_batch, gt_box_batch, gt_class_targets_batch, gt_weights_batch):
-    (cls_targets, cls_weights, reg_targets,
-     reg_weights, match) = target_assigner.assign(
-         anchors, gt_boxes, gt_class_targets, gt_weights)
+    (cls_targets, cls_weights, reg_targets, reg_weights,
+     match) = target_assigner.assign(anchors, gt_boxes, gt_class_targets,
+                                     unmatched_class_label, gt_weights)
     cls_targets_list.append(cls_targets)
     cls_weights_list.append(cls_weights)
     reg_targets_list.append(reg_targets)
diff --git a/research/object_detection/core/target_assigner_test.py b/research/object_detection/core/target_assigner_test.py
index 34a35b64..5b0bb2ef 100644
--- a/research/object_detection/core/target_assigner_test.py
+++ b/research/object_detection/core/target_assigner_test.py
@@ -37,10 +37,11 @@ class TargetAssignerTest(test_case.TestCase):
                                              unmatched_threshold=0.5)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+          similarity_calc, matcher, box_coder)
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      result = target_assigner.assign(
+          anchors_boxlist, groundtruth_boxlist, unmatched_class_label=None)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -81,10 +82,11 @@ class TargetAssignerTest(test_case.TestCase):
                                              unmatched_threshold=0.3)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+          similarity_calc, matcher, box_coder)
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      result = target_assigner.assign(
+          anchors_boxlist, groundtruth_boxlist, unmatched_class_label=None)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -120,12 +122,13 @@ class TargetAssignerTest(test_case.TestCase):
       box_coder = keypoint_box_coder.KeypointBoxCoder(
           num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+          similarity_calc, matcher, box_coder)
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
       groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,
                                     groundtruth_keypoints)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      result = target_assigner.assign(
+          anchors_boxlist, groundtruth_boxlist, unmatched_class_label=None)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -174,12 +177,13 @@ class TargetAssignerTest(test_case.TestCase):
       box_coder = keypoint_box_coder.KeypointBoxCoder(
           num_keypoints=6, scale_factors=[10.0, 10.0, 5.0, 5.0])
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder, unmatched_cls_target=None)
+          similarity_calc, matcher, box_coder)
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
       groundtruth_boxlist.add_field(fields.BoxListFields.keypoints,
                                     groundtruth_keypoints)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist)
+      result = target_assigner.assign(
+          anchors_boxlist, groundtruth_boxlist, unmatched_class_label=None)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -221,15 +225,17 @@ class TargetAssignerTest(test_case.TestCase):
       matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                              unmatched_threshold=0.5)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
+      unmatched_class_label = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder,
-          unmatched_cls_target=unmatched_cls_target)
+          similarity_calc, matcher, box_coder)
 
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
-                                      groundtruth_labels)
+      result = target_assigner.assign(
+          anchors_boxlist,
+          groundtruth_boxlist,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -275,16 +281,18 @@ class TargetAssignerTest(test_case.TestCase):
       matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                              unmatched_threshold=0.5)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-      unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
+      unmatched_class_label = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder,
-          unmatched_cls_target=unmatched_cls_target)
+          similarity_calc, matcher, box_coder)
 
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
-                                      groundtruth_labels,
-                                      groundtruth_weights)
+      result = target_assigner.assign(
+          anchors_boxlist,
+          groundtruth_boxlist,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label,
+          groundtruth_weights=groundtruth_weights)
       (_, cls_weights, _, reg_weights, _) = result
       return (cls_weights, reg_weights)
 
@@ -318,15 +326,17 @@ class TargetAssignerTest(test_case.TestCase):
                                              unmatched_threshold=0.5)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
 
-      unmatched_cls_target = tf.constant([[0, 0], [0, 0]], tf.float32)
+      unmatched_class_label = tf.constant([[0, 0], [0, 0]], tf.float32)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder,
-          unmatched_cls_target=unmatched_cls_target)
+          similarity_calc, matcher, box_coder)
 
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
-                                      groundtruth_labels)
+      result = target_assigner.assign(
+          anchors_boxlist,
+          groundtruth_boxlist,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -371,14 +381,16 @@ class TargetAssignerTest(test_case.TestCase):
       matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                              unmatched_threshold=0.5)
       box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-      unmatched_cls_target = tf.constant([0, 0, 0], tf.float32)
+      unmatched_class_label = tf.constant([0, 0, 0], tf.float32)
       anchors_boxlist = box_list.BoxList(anchor_means)
       groundtruth_boxlist = box_list.BoxList(groundtruth_box_corners)
       target_assigner = targetassigner.TargetAssigner(
-          similarity_calc, matcher, box_coder,
-          unmatched_cls_target=unmatched_cls_target)
-      result = target_assigner.assign(anchors_boxlist, groundtruth_boxlist,
-                                      groundtruth_labels)
+          similarity_calc, matcher, box_coder)
+      result = target_assigner.assign(
+          anchors_boxlist,
+          groundtruth_boxlist,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label)
       (cls_targets, cls_weights, reg_targets, reg_weights, _) = result
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
@@ -415,10 +427,9 @@ class TargetAssignerTest(test_case.TestCase):
     similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
     matcher = bipartite_matcher.GreedyBipartiteMatcher()
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder()
-    unmatched_cls_target = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
+    unmatched_class_label = tf.constant([1, 0, 0, 0, 0, 0, 0], tf.float32)
     target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
+        similarity_calc, matcher, box_coder)
 
     prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5],
                                [0.5, 0.5, 1.0, 0.8],
@@ -436,17 +447,20 @@ class TargetAssignerTest(test_case.TestCase):
                                       [0, 0, 0, 0, 0, 1, 0],
                                       [0, 0, 0, 1, 0, 0, 0]], tf.float32)
     with self.assertRaisesRegexp(ValueError, 'Unequal shapes'):
-      target_assigner.assign(priors, boxes, groundtruth_labels,
-                             num_valid_rows=3)
+      target_assigner.assign(
+          priors,
+          boxes,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label,
+          num_valid_rows=3)
 
   def test_raises_error_on_invalid_groundtruth_labels(self):
     similarity_calc = region_similarity_calculator.NegSqDistSimilarity()
     matcher = bipartite_matcher.GreedyBipartiteMatcher()
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=1.0)
-    unmatched_cls_target = tf.constant([[0, 0], [0, 0], [0, 0]], tf.float32)
+    unmatched_class_label = tf.constant([[0, 0], [0, 0], [0, 0]], tf.float32)
     target_assigner = targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
+        similarity_calc, matcher, box_coder)
 
     prior_means = tf.constant([[0.0, 0.0, 0.5, 0.5]])
     priors = box_list.BoxList(prior_means)
@@ -458,41 +472,22 @@ class TargetAssignerTest(test_case.TestCase):
     groundtruth_labels = tf.constant([[[0, 1], [1, 0]]], tf.float32)
 
     with self.assertRaises(ValueError):
-      target_assigner.assign(priors, boxes, groundtruth_labels,
-                             num_valid_rows=3)
+      target_assigner.assign(
+          priors,
+          boxes,
+          groundtruth_labels,
+          unmatched_class_label=unmatched_class_label,
+          num_valid_rows=3)
 
 
 class BatchTargetAssignerTest(test_case.TestCase):
 
-  def _get_agnostic_target_assigner(self):
+  def _get_target_assigner(self):
     similarity_calc = region_similarity_calculator.IouSimilarity()
     matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
                                            unmatched_threshold=0.5)
     box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-    return targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=None)
-
-  def _get_multi_class_target_assigner(self, num_classes):
-    similarity_calc = region_similarity_calculator.IouSimilarity()
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
-                                           unmatched_threshold=0.5)
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-    unmatched_cls_target = tf.constant([1] + num_classes * [0], tf.float32)
-    return targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
-
-  def _get_multi_dimensional_target_assigner(self, target_dimensions):
-    similarity_calc = region_similarity_calculator.IouSimilarity()
-    matcher = argmax_matcher.ArgMaxMatcher(matched_threshold=0.5,
-                                           unmatched_threshold=0.5)
-    box_coder = mean_stddev_box_coder.MeanStddevBoxCoder(stddev=0.1)
-    unmatched_cls_target = tf.constant(np.zeros(target_dimensions),
-                                       tf.float32)
-    return targetassigner.TargetAssigner(
-        similarity_calc, matcher, box_coder,
-        unmatched_cls_target=unmatched_cls_target)
+    return targetassigner.TargetAssigner(similarity_calc, matcher, box_coder)
 
   def test_batch_assign_targets(self):
 
@@ -502,7 +497,7 @@ class BatchTargetAssignerTest(test_case.TestCase):
       gt_box_batch = [box_list1, box_list2]
       gt_class_targets = [None, None]
       anchors_boxlist = box_list.BoxList(anchor_means)
-      agnostic_target_assigner = self._get_agnostic_target_assigner()
+      agnostic_target_assigner = self._get_target_assigner()
       (cls_targets, cls_weights, reg_targets, reg_weights,
        _) = targetassigner.batch_assign_targets(
            agnostic_target_assigner, anchors_boxlist, gt_box_batch,
@@ -550,12 +545,13 @@ class BatchTargetAssignerTest(test_case.TestCase):
       gt_box_batch = [box_list1, box_list2]
       gt_class_targets = [class_targets1, class_targets2]
       anchors_boxlist = box_list.BoxList(anchor_means)
-      multiclass_target_assigner = self._get_multi_class_target_assigner(
-          num_classes=3)
+      multiclass_target_assigner = self._get_target_assigner()
+      num_classes = 3
+      unmatched_class_label = tf.constant([1] + num_classes * [0], tf.float32)
       (cls_targets, cls_weights, reg_targets, reg_weights,
        _) = targetassigner.batch_assign_targets(
            multiclass_target_assigner, anchors_boxlist, gt_box_batch,
-           gt_class_targets)
+           gt_class_targets, unmatched_class_label)
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
     groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)
@@ -613,12 +609,13 @@ class BatchTargetAssignerTest(test_case.TestCase):
       gt_class_targets = [class_targets1, class_targets2]
       gt_weights = [groundtruth_weights1, groundtruth_weights2]
       anchors_boxlist = box_list.BoxList(anchor_means)
-      multiclass_target_assigner = self._get_multi_class_target_assigner(
-          num_classes=3)
+      multiclass_target_assigner = self._get_target_assigner()
+      num_classes = 3
+      unmatched_class_label = tf.constant([1] + num_classes * [0], tf.float32)
       (cls_targets, cls_weights, reg_targets, reg_weights,
        _) = targetassigner.batch_assign_targets(
            multiclass_target_assigner, anchors_boxlist, gt_box_batch,
-           gt_class_targets, gt_weights)
+           gt_class_targets, unmatched_class_label, gt_weights)
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
     groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2],
@@ -680,12 +677,14 @@ class BatchTargetAssignerTest(test_case.TestCase):
       gt_box_batch = [box_list1, box_list2]
       gt_class_targets = [class_targets1, class_targets2]
       anchors_boxlist = box_list.BoxList(anchor_means)
-      multiclass_target_assigner = self._get_multi_dimensional_target_assigner(
-          target_dimensions=(2, 3))
+      multiclass_target_assigner = self._get_target_assigner()
+      target_dimensions = (2, 3)
+      unmatched_class_label = tf.constant(np.zeros(target_dimensions),
+                                          tf.float32)
       (cls_targets, cls_weights, reg_targets, reg_weights,
        _) = targetassigner.batch_assign_targets(
            multiclass_target_assigner, anchors_boxlist, gt_box_batch,
-           gt_class_targets)
+           gt_class_targets, unmatched_class_label)
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
     groundtruth_boxlist1 = np.array([[0., 0., 0.2, 0.2]], dtype=np.float32)
@@ -754,13 +753,13 @@ class BatchTargetAssignerTest(test_case.TestCase):
       gt_class_targets_batch = [gt_class_targets]
       anchors_boxlist = box_list.BoxList(anchor_means)
 
-      multiclass_target_assigner = self._get_multi_class_target_assigner(
-          num_classes=3)
-
+      multiclass_target_assigner = self._get_target_assigner()
+      num_classes = 3
+      unmatched_class_label = tf.constant([1] + num_classes * [0], tf.float32)
       (cls_targets, cls_weights, reg_targets, reg_weights,
        _) = targetassigner.batch_assign_targets(
            multiclass_target_assigner, anchors_boxlist,
-           gt_box_batch, gt_class_targets_batch)
+           gt_box_batch, gt_class_targets_batch, unmatched_class_label)
       return (cls_targets, cls_weights, reg_targets, reg_weights)
 
     groundtruth_box_corners = np.zeros((0, 4), dtype=np.float32)
diff --git a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
index 013a0330..64f2550b 100644
--- a/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
+++ b/research/object_detection/dataset_tools/oid_hierarchical_labels_expansion.py
@@ -162,12 +162,12 @@ def main(parsed_args):
       for line in source:
         if not header:
           header = line
+          target.writelines(header)
           continue
         if labels_file:
           expanded_lines = expansion_generator.expand_labels_from_csv(line)
         else:
           expanded_lines = expansion_generator.expand_boxes_from_csv(line)
-        expanded_lines = [header] + expanded_lines
         target.writelines(expanded_lines)
 
 
diff --git a/research/object_detection/export_inference_graph.py b/research/object_detection/export_inference_graph.py
index 1e9fcbda..6b5257be 100644
--- a/research/object_detection/export_inference_graph.py
+++ b/research/object_detection/export_inference_graph.py
@@ -140,10 +140,10 @@ def main(_):
     ]
   else:
     input_shape = None
-  exporter.export_inference_graph(FLAGS.input_type, pipeline_config,
-                                  FLAGS.trained_checkpoint_prefix,
-                                  FLAGS.output_directory, input_shape,
-                                  FLAGS.write_inference_graph)
+  exporter.export_inference_graph(
+      FLAGS.input_type, pipeline_config, FLAGS.trained_checkpoint_prefix,
+      FLAGS.output_directory, input_shape=input_shape,
+      write_inference_graph=FLAGS.write_inference_graph)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/export_tflite_ssd_graph_lib.py b/research/object_detection/export_tflite_ssd_graph_lib.py
index f29451cd..e60f93e8 100644
--- a/research/object_detection/export_tflite_ssd_graph_lib.py
+++ b/research/object_detection/export_tflite_ssd_graph_lib.py
@@ -258,6 +258,7 @@ def export_tflite_graph(pipeline_config, trained_checkpoint_prefix, output_dir,
       restore_op_name='save/restore_all',
       filename_tensor_name='save/Const:0',
       clear_devices=True,
+      output_graph='',
       initializer_nodes='')
 
   # Add new operation to do post processing in a custom op (TF Lite only)
diff --git a/research/object_detection/exporter.py b/research/object_detection/exporter.py
index 16ff79a3..ed62fac2 100644
--- a/research/object_detection/exporter.py
+++ b/research/object_detection/exporter.py
@@ -14,17 +14,16 @@
 # ==============================================================================
 
 """Functions to export object detection inference graph."""
-import logging
 import os
 import tempfile
 import tensorflow as tf
 from tensorflow.core.protobuf import saver_pb2
-from tensorflow.python import pywrap_tensorflow
 from tensorflow.python.client import session
-from tensorflow.python.framework import graph_util
 from tensorflow.python.platform import gfile
 from tensorflow.python.saved_model import signature_constants
+from tensorflow.python.tools import freeze_graph
 from tensorflow.python.training import saver as saver_lib
+from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.core import standard_fields as fields
 from object_detection.data_decoders import tf_example_decoder
@@ -32,70 +31,7 @@ from object_detection.utils import config_util
 
 slim = tf.contrib.slim
 
-
-# TODO(derekjchow): Replace with freeze_graph.freeze_graph_with_def_protos when
-# newer version of Tensorflow becomes more common.
-def freeze_graph_with_def_protos(
-    input_graph_def,
-    input_saver_def,
-    input_checkpoint,
-    output_node_names,
-    restore_op_name,
-    filename_tensor_name,
-    clear_devices,
-    initializer_nodes,
-    variable_names_blacklist=''):
-  """Converts all variables in a graph and checkpoint into constants."""
-  del restore_op_name, filename_tensor_name  # Unused by updated loading code.
-
-  # 'input_checkpoint' may be a prefix if we're using Saver V2 format
-  if not saver_lib.checkpoint_exists(input_checkpoint):
-    raise ValueError(
-        'Input checkpoint "' + input_checkpoint + '" does not exist!')
-
-  if not output_node_names:
-    raise ValueError(
-        'You must supply the name of a node to --output_node_names.')
-
-  # Remove all the explicit device specifications for this node. This helps to
-  # make the graph more portable.
-  if clear_devices:
-    for node in input_graph_def.node:
-      node.device = ''
-
-  with tf.Graph().as_default():
-    tf.import_graph_def(input_graph_def, name='')
-    config = tf.ConfigProto(graph_options=tf.GraphOptions())
-    with session.Session(config=config) as sess:
-      if input_saver_def:
-        saver = saver_lib.Saver(saver_def=input_saver_def)
-        saver.restore(sess, input_checkpoint)
-      else:
-        var_list = {}
-        reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)
-        var_to_shape_map = reader.get_variable_to_shape_map()
-        for key in var_to_shape_map:
-          try:
-            tensor = sess.graph.get_tensor_by_name(key + ':0')
-          except KeyError:
-            # This tensor doesn't exist in the graph (for example it's
-            # 'global_step' or a similar housekeeping element) so skip it.
-            continue
-          var_list[key] = tensor
-        saver = saver_lib.Saver(var_list=var_list)
-        saver.restore(sess, input_checkpoint)
-        if initializer_nodes:
-          sess.run(initializer_nodes)
-
-      variable_names_blacklist = (variable_names_blacklist.split(',') if
-                                  variable_names_blacklist else None)
-      output_graph_def = graph_util.convert_variables_to_constants(
-          sess,
-          input_graph_def,
-          output_node_names.split(','),
-          variable_names_blacklist=variable_names_blacklist)
-
-  return output_graph_def
+freeze_graph_with_def_protos = freeze_graph.freeze_graph_with_def_protos
 
 
 def replace_variable_values_with_moving_averages(graph,
@@ -247,18 +183,6 @@ def _add_output_tensor_nodes(postprocessed_tensors,
   return outputs
 
 
-def write_frozen_graph(frozen_graph_path, frozen_graph_def):
-  """Writes frozen graph to disk.
-
-  Args:
-    frozen_graph_path: Path to write inference graph.
-    frozen_graph_def: tf.GraphDef holding frozen graph.
-  """
-  with gfile.GFile(frozen_graph_path, 'wb') as f:
-    f.write(frozen_graph_def.SerializeToString())
-  logging.info('%d ops in the final graph.', len(frozen_graph_def.node))
-
-
 def write_saved_model(saved_model_path,
                       frozen_graph_def,
                       inputs,
@@ -384,6 +308,7 @@ def _export_inference_graph(input_type,
       output_collection_name=output_collection_name,
       graph_hook_fn=graph_hook_fn)
 
+  profile_inference_graph(tf.get_default_graph())
   saver_kwargs = {}
   if use_moving_averages:
     # This check is to be compatible with both version of SaverDef.
@@ -421,16 +346,17 @@ def _export_inference_graph(input_type,
   else:
     output_node_names = ','.join(outputs.keys())
 
-  frozen_graph_def = freeze_graph_with_def_protos(
+  frozen_graph_def = freeze_graph.freeze_graph_with_def_protos(
       input_graph_def=tf.get_default_graph().as_graph_def(),
       input_saver_def=input_saver_def,
       input_checkpoint=checkpoint_to_use,
       output_node_names=output_node_names,
       restore_op_name='save/restore_all',
       filename_tensor_name='save/Const:0',
+      output_graph=frozen_graph_path,
       clear_devices=True,
       initializer_nodes='')
-  write_frozen_graph(frozen_graph_path, frozen_graph_def)
+
   write_saved_model(saved_model_path, frozen_graph_def,
                     placeholder_tensor, outputs)
 
@@ -461,6 +387,11 @@ def export_inference_graph(input_type,
   """
   detection_model = model_builder.build(pipeline_config.model,
                                         is_training=False)
+  graph_rewriter_fn = None
+  if pipeline_config.HasField('graph_rewriter'):
+    graph_rewriter_config = pipeline_config.graph_rewriter
+    graph_rewriter_fn = graph_rewriter_builder.build(graph_rewriter_config,
+                                                     is_training=False)
   _export_inference_graph(
       input_type,
       detection_model,
@@ -470,7 +401,39 @@ def export_inference_graph(input_type,
       additional_output_tensor_names,
       input_shape,
       output_collection_name,
-      graph_hook_fn=None,
+      graph_hook_fn=graph_rewriter_fn,
       write_inference_graph=write_inference_graph)
   pipeline_config.eval_config.use_moving_averages = False
   config_util.save_pipeline_config(pipeline_config, output_directory)
+
+
+def profile_inference_graph(graph):
+  """Profiles the inference graph.
+
+  Prints model parameters and computation FLOPs given an inference graph.
+  BatchNorms are excluded from the parameter count due to the fact that
+  BatchNorms are usually folded. BatchNorm, Initializer, Regularizer
+  and BiasAdd are not considered in FLOP count.
+
+  Args:
+    graph: the inference graph.
+  """
+  tfprof_vars_option = (
+      tf.contrib.tfprof.model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
+  tfprof_flops_option = tf.contrib.tfprof.model_analyzer.FLOAT_OPS_OPTIONS
+
+  # Batchnorm is usually folded during inference.
+  tfprof_vars_option['trim_name_regexes'] = ['.*BatchNorm.*']
+  # Initializer and Regularizer are only used in training.
+  tfprof_flops_option['trim_name_regexes'] = [
+      '.*BatchNorm.*', '.*Initializer.*', '.*Regularizer.*', '.*BiasAdd.*'
+  ]
+
+  tf.contrib.tfprof.model_analyzer.print_model_analysis(
+      graph,
+      tfprof_options=tfprof_vars_option)
+
+  tf.contrib.tfprof.model_analyzer.print_model_analysis(
+      graph,
+      tfprof_options=tfprof_flops_option)
+
diff --git a/research/object_detection/exporter_test.py b/research/object_detection/exporter_test.py
index fb766545..d872b561 100644
--- a/research/object_detection/exporter_test.py
+++ b/research/object_detection/exporter_test.py
@@ -20,8 +20,10 @@ import six
 import tensorflow as tf
 from google.protobuf import text_format
 from object_detection import exporter
+from object_detection.builders import graph_rewriter_builder
 from object_detection.builders import model_builder
 from object_detection.core import model
+from object_detection.protos import graph_rewriter_pb2
 from object_detection.protos import pipeline_pb2
 
 if six.PY2:
@@ -75,8 +77,10 @@ class FakeModel(model.DetectionModel):
 
 class ExportInferenceGraphTest(tf.test.TestCase):
 
-  def _save_checkpoint_from_mock_model(self, checkpoint_path,
-                                       use_moving_averages):
+  def _save_checkpoint_from_mock_model(self,
+                                       checkpoint_path,
+                                       use_moving_averages,
+                                       enable_quantization=False):
     g = tf.Graph()
     with g.as_default():
       mock_model = FakeModel()
@@ -86,20 +90,28 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       mock_model.postprocess(predictions, true_image_shapes)
       if use_moving_averages:
         tf.train.ExponentialMovingAverage(0.0).apply()
-      slim.get_or_create_global_step()
+      tf.train.get_or_create_global_step()
+      if enable_quantization:
+        graph_rewriter_config = graph_rewriter_pb2.GraphRewriter()
+        graph_rewriter_config.quantization.delay = 500000
+        graph_rewriter_fn = graph_rewriter_builder.build(
+            graph_rewriter_config, is_training=False)
+        graph_rewriter_fn()
       saver = tf.train.Saver()
       init = tf.global_variables_initializer()
       with self.test_session() as sess:
         sess.run(init)
         saver.save(sess, checkpoint_path)
 
-  def _load_inference_graph(self, inference_graph_path):
+  def _load_inference_graph(self, inference_graph_path, is_binary=True):
     od_graph = tf.Graph()
     with od_graph.as_default():
       od_graph_def = tf.GraphDef()
       with tf.gfile.GFile(inference_graph_path) as fid:
-        serialized_graph = fid.read()
-        od_graph_def.ParseFromString(serialized_graph)
+        if is_binary:
+          od_graph_def.ParseFromString(fid.read())
+        else:
+          text_format.Parse(fid.read(), od_graph_def)
         tf.import_graph_def(od_graph_def, name='')
     return od_graph
 
@@ -284,6 +296,42 @@ class ExportInferenceGraphTest(tf.test.TestCase):
         [var_name for var_name, _ in tf.train.list_variables(output_directory)])
     self.assertTrue(expected_variables.issubset(actual_variables))
 
+  def test_export_model_with_quantization_nodes(self):
+    tmp_dir = self.get_temp_dir()
+    trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
+    self._save_checkpoint_from_mock_model(
+        trained_checkpoint_prefix,
+        use_moving_averages=False,
+        enable_quantization=True)
+    output_directory = os.path.join(tmp_dir, 'output')
+    inference_graph_path = os.path.join(output_directory,
+                                        'inference_graph.pbtxt')
+    with mock.patch.object(
+        model_builder, 'build', autospec=True) as mock_builder:
+      mock_builder.return_value = FakeModel()
+      pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()
+      text_format.Merge(
+          """graph_rewriter {
+               quantization {
+                 delay: 50000
+                 activation_bits: 8
+                 weight_bits: 8
+               }
+             }""", pipeline_config)
+      exporter.export_inference_graph(
+          input_type='image_tensor',
+          pipeline_config=pipeline_config,
+          trained_checkpoint_prefix=trained_checkpoint_prefix,
+          output_directory=output_directory,
+          write_inference_graph=True)
+    self._load_inference_graph(inference_graph_path, is_binary=False)
+    has_quant_nodes = False
+    for v in tf.global_variables():
+      if v.op.name.endswith('act_quant/min'):
+        has_quant_nodes = True
+        break
+    self.assertTrue(has_quant_nodes)
+
   def test_export_model_with_all_output_nodes(self):
     tmp_dir = self.get_temp_dir()
     trained_checkpoint_prefix = os.path.join(tmp_dir, 'model.ckpt')
@@ -564,16 +612,16 @@ class ExportInferenceGraphTest(tf.test.TestCase):
       output_node_names = ','.join(outputs.keys())
       saver = tf.train.Saver()
       input_saver_def = saver.as_saver_def()
-      frozen_graph_def = exporter.freeze_graph_with_def_protos(
+      exporter.freeze_graph_with_def_protos(
           input_graph_def=tf.get_default_graph().as_graph_def(),
           input_saver_def=input_saver_def,
           input_checkpoint=trained_checkpoint_prefix,
           output_node_names=output_node_names,
           restore_op_name='save/restore_all',
           filename_tensor_name='save/Const:0',
+          output_graph=inference_graph_path,
           clear_devices=True,
           initializer_nodes='')
-      exporter.write_frozen_graph(inference_graph_path, frozen_graph_def)
 
     inference_graph = self._load_inference_graph(inference_graph_path)
     tf_example_np = np.expand_dims(self._create_tf_example(
@@ -719,6 +767,7 @@ class ExportInferenceGraphTest(tf.test.TestCase):
           output_node_names=output_node_names,
           restore_op_name='save/restore_all',
           filename_tensor_name='save/Const:0',
+          output_graph='',
           clear_devices=True,
           initializer_nodes='')
       exporter.write_saved_model(
diff --git a/research/object_detection/g3doc/installation.md b/research/object_detection/g3doc/installation.md
index 2c3c6cd0..daa22a35 100644
--- a/research/object_detection/g3doc/installation.md
+++ b/research/object_detection/g3doc/installation.md
@@ -48,6 +48,7 @@ pip install --user jupyter
 pip install --user matplotlib
 ```
 
+<!-- common_typos_disable -->
 **Note**: sometimes "sudo apt-get install protobuf-compiler" will install
 Protobuf 3+ versions for you and some users have issues when using 3.5.
 If that is your case, try the [manual](#Manual-protobuf-compiler-installation-and-usage) installation.
@@ -88,6 +89,7 @@ protoc object_detection/protos/*.proto --python_out=.
 
 ## Manual protobuf-compiler installation and usage
 Download and install the 3.0 release of protoc, then unzip the file.
+
 ```bash
 # From tensorflow/models/research/
 wget -O protobuf.zip https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip
diff --git a/research/object_detection/inputs.py b/research/object_detection/inputs.py
index 91de994f..8f4aed97 100644
--- a/research/object_detection/inputs.py
+++ b/research/object_detection/inputs.py
@@ -219,20 +219,8 @@ def pad_input_data_to_static_shapes(tensor_dict, max_num_boxes, num_classes,
 
   padded_tensor_dict = {}
   for tensor_name in tensor_dict:
-    expected_shape = padding_shapes[tensor_name]
-    current_shape = shape_utils.combined_static_and_dynamic_shape(
-        tensor_dict[tensor_name])
-    trailing_paddings = [
-        expected_shape_dim - current_shape_dim if expected_shape_dim else 0
-        for expected_shape_dim, current_shape_dim in zip(
-            expected_shape, current_shape)
-    ]
-    paddings = tf.stack([tf.zeros(len(trailing_paddings), dtype=tf.int32),
-                         trailing_paddings],
-                        axis=1)
-    padded_tensor_dict[tensor_name] = tf.pad(
-        tensor_dict[tensor_name], paddings=paddings)
-    padded_tensor_dict[tensor_name].set_shape(expected_shape)
+    padded_tensor_dict[tensor_name] = shape_utils.pad_or_clip_nd(
+        tensor_dict[tensor_name], padding_shapes[tensor_name])
   return padded_tensor_dict
 
 
@@ -529,7 +517,7 @@ def create_predict_input_fn(model_config, predict_input_config):
       `ServingInputReceiver`.
     """
     del params
-    example = tf.placeholder(dtype=tf.string, shape=[], name='input_feature')
+    example = tf.placeholder(dtype=tf.string, shape=[], name='tf_example')
 
     num_classes = config_util.get_number_of_classes(model_config)
     model = model_builder.build(model_config, is_training=False)
diff --git a/research/object_detection/inputs_test.py b/research/object_detection/inputs_test.py
index 009c44c7..e389565c 100644
--- a/research/object_detection/inputs_test.py
+++ b/research/object_detection/inputs_test.py
@@ -657,6 +657,42 @@ class PadInputDataToStaticShapesFnTest(tf.test.TestCase):
         padded_tensor_dict[fields.InputDataFields.groundtruth_classes]
         .shape.as_list(), [3, 3])
 
+  def test_clip_boxes_and_classes(self):
+    input_tensor_dict = {
+        fields.InputDataFields.groundtruth_boxes:
+            tf.placeholder(tf.float32, [None, 4]),
+        fields.InputDataFields.groundtruth_classes:
+            tf.placeholder(tf.int32, [None, 3]),
+    }
+    padded_tensor_dict = inputs.pad_input_data_to_static_shapes(
+        tensor_dict=input_tensor_dict,
+        max_num_boxes=3,
+        num_classes=3,
+        spatial_image_shape=[5, 6])
+
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.groundtruth_boxes]
+        .shape.as_list(), [3, 4])
+    self.assertAllEqual(
+        padded_tensor_dict[fields.InputDataFields.groundtruth_classes]
+        .shape.as_list(), [3, 3])
+
+    with self.test_session() as sess:
+      out_tensor_dict = sess.run(
+          padded_tensor_dict,
+          feed_dict={
+              input_tensor_dict[fields.InputDataFields.groundtruth_boxes]:
+                  np.random.rand(5, 4),
+              input_tensor_dict[fields.InputDataFields.groundtruth_classes]:
+                  np.random.rand(2, 3),
+          })
+
+    self.assertAllEqual(
+        out_tensor_dict[fields.InputDataFields.groundtruth_boxes].shape, [3, 4])
+    self.assertAllEqual(
+        out_tensor_dict[fields.InputDataFields.groundtruth_classes].shape,
+        [3, 3])
+
   def test_do_not_pad_dynamic_images(self):
     input_tensor_dict = {
         fields.InputDataFields.image:
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
index 57af6faf..76f53e81 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch.py
@@ -12,7 +12,6 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # ==============================================================================
-
 """Faster R-CNN meta-architecture definition.
 
 General tensorflow implementation of Faster R-CNN detection models.
@@ -98,7 +97,6 @@ from functools import partial
 import tensorflow as tf
 
 from object_detection.anchor_generators import grid_anchor_generator
-from object_detection.core import balanced_positive_negative_sampler as sampler
 from object_detection.core import box_list
 from object_detection.core import box_list_ops
 from object_detection.core import box_predictor
@@ -107,6 +105,7 @@ from object_detection.core import model
 from object_detection.core import post_processing
 from object_detection.core import standard_fields as fields
 from object_detection.core import target_assigner
+from object_detection.predictors import convolutional_box_predictor
 from object_detection.utils import ops
 from object_detection.utils import shape_utils
 
@@ -228,12 +227,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
                feature_extractor,
                number_of_stages,
                first_stage_anchor_generator,
+               first_stage_target_assigner,
                first_stage_atrous_rate,
                first_stage_box_predictor_arg_scope_fn,
                first_stage_box_predictor_kernel_size,
                first_stage_box_predictor_depth,
                first_stage_minibatch_size,
-               first_stage_positive_balance_fraction,
+               first_stage_sampler,
                first_stage_nms_score_threshold,
                first_stage_nms_iou_threshold,
                first_stage_max_proposals,
@@ -242,9 +242,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
                initial_crop_size,
                maxpool_kernel_size,
                maxpool_stride,
+               second_stage_target_assigner,
                second_stage_mask_rcnn_box_predictor,
                second_stage_batch_size,
-               second_stage_balance_fraction,
+               second_stage_sampler,
                second_stage_non_max_suppression_fn,
                second_stage_score_conversion_fn,
                second_stage_localization_loss_weight,
@@ -254,7 +255,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
                hard_example_miner=None,
                parallel_iterations=16,
                add_summaries=True,
-               use_matmul_crop_and_resize=False):
+               use_matmul_crop_and_resize=False,
+               clip_anchors_to_image=False):
     """FasterRCNNMetaArch Constructor.
 
     Args:
@@ -285,6 +287,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
       first_stage_anchor_generator: An anchor_generator.AnchorGenerator object
         (note that currently we only support
         grid_anchor_generator.GridAnchorGenerator objects)
+      first_stage_target_assigner: Target assigner to use for first stage of
+        Faster R-CNN (RPN).
       first_stage_atrous_rate: A single integer indicating the atrous rate for
         the single convolution op which is applied to the `rpn_features_to_crop`
         tensor to obtain a tensor to be used for box prediction. Some feature
@@ -304,8 +308,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
         "batch size" refers to the number of anchors selected as contributing
         to the loss function for any given image within the image batch and is
         only called "batch_size" due to terminology from the Faster R-CNN paper.
-      first_stage_positive_balance_fraction: Fraction of positive examples
-        per image for the RPN. The recommended value for Faster RCNN is 0.5.
+      first_stage_sampler: Sampler to use for first stage loss (RPN loss).
       first_stage_nms_score_threshold: Score threshold for non max suppression
         for the Region Proposal Network (RPN).  This value is expected to be in
         [0, 1] as it is applied directly after a softmax transformation.  The
@@ -325,6 +328,10 @@ class FasterRCNNMetaArch(model.DetectionModel):
         max pool op on the cropped feature map during ROI pooling.
       maxpool_stride: A single integer indicating the stride of the max pool
         op on the cropped feature map during ROI pooling.
+      second_stage_target_assigner: Target assigner to use for second stage of
+        Faster R-CNN. If the model is configured with multiple prediction heads,
+        this target assigner is used to generate targets for all heads (with the
+        correct `unmatched_class_label`).
       second_stage_mask_rcnn_box_predictor: Mask R-CNN box predictor to use for
         the second stage.
       second_stage_batch_size: The batch size used for computing the
@@ -332,9 +339,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
         "batch size" refers to the number of proposals selected as contributing
         to the loss function for any given image within the image batch and is
         only called "batch_size" due to terminology from the Faster R-CNN paper.
-      second_stage_balance_fraction: Fraction of positive examples to use
-        per image for the box classifier. The recommended value for Faster RCNN
-        is 0.25.
+      second_stage_sampler:  Sampler to use for second stage loss (box
+        classifier loss).
       second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression
         callable that takes `boxes`, `scores`, optional `clip_window` and
         optional (kwarg) `mask` inputs (with all other inputs already set)
@@ -364,6 +370,9 @@ class FasterRCNNMetaArch(model.DetectionModel):
       use_matmul_crop_and_resize: Force the use of matrix multiplication based
         crop and resize instead of standard tf.image.crop_and_resize while
         computing second stage input feature maps.
+      clip_anchors_to_image: Normally, anchors generated for a given image size
+      are pruned during training if they lie outside the image window. This
+      option clips the anchors to be within the image instead of pruning.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals` at
@@ -388,13 +397,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._feature_extractor = feature_extractor
     self._number_of_stages = number_of_stages
 
-    # The first class is reserved as background.
-    unmatched_cls_target = tf.constant(
-        [1] + self._num_classes * [0], dtype=tf.float32)
-    self._proposal_target_assigner = target_assigner.create_target_assigner(
-        'FasterRCNN', 'proposal')
-    self._detector_target_assigner = target_assigner.create_target_assigner(
-        'FasterRCNN', 'detection', unmatched_cls_target=unmatched_cls_target)
+    self._proposal_target_assigner = first_stage_target_assigner
+    self._detector_target_assigner = second_stage_target_assigner
     # Both proposal and detector target assigners use the same box coder
     self._box_coder = self._proposal_target_assigner.box_coder
 
@@ -407,14 +411,19 @@ class FasterRCNNMetaArch(model.DetectionModel):
         first_stage_box_predictor_kernel_size)
     self._first_stage_box_predictor_depth = first_stage_box_predictor_depth
     self._first_stage_minibatch_size = first_stage_minibatch_size
-    self._first_stage_sampler = sampler.BalancedPositiveNegativeSampler(
-        positive_fraction=first_stage_positive_balance_fraction)
-    self._first_stage_box_predictor = box_predictor.ConvolutionalBoxPredictor(
-        self._is_training, num_classes=1,
-        conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,
-        min_depth=0, max_depth=0, num_layers_before_predictor=0,
-        use_dropout=False, dropout_keep_prob=1.0, kernel_size=1,
-        box_code_size=self._box_coder.code_size)
+    self._first_stage_sampler = first_stage_sampler
+    self._first_stage_box_predictor = (
+        convolutional_box_predictor.ConvolutionalBoxPredictor(
+            self._is_training,
+            num_classes=1,
+            conv_hyperparams_fn=self._first_stage_box_predictor_arg_scope_fn,
+            min_depth=0,
+            max_depth=0,
+            num_layers_before_predictor=0,
+            use_dropout=False,
+            dropout_keep_prob=1.0,
+            kernel_size=1,
+            box_code_size=self._box_coder.code_size))
 
     self._first_stage_nms_score_threshold = first_stage_nms_score_threshold
     self._first_stage_nms_iou_threshold = first_stage_nms_iou_threshold
@@ -435,8 +444,7 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._mask_rcnn_box_predictor = second_stage_mask_rcnn_box_predictor
 
     self._second_stage_batch_size = second_stage_batch_size
-    self._second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
-        positive_fraction=second_stage_balance_fraction)
+    self._second_stage_sampler = second_stage_sampler
 
     self._second_stage_nms_fn = second_stage_non_max_suppression_fn
     self._second_stage_score_conversion_fn = second_stage_score_conversion_fn
@@ -454,6 +462,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
     self._hard_example_miner = hard_example_miner
     self._parallel_iterations = parallel_iterations
 
+    self.clip_anchors_to_image = clip_anchors_to_image
+
     if self._number_of_stages <= 0 or self._number_of_stages > 3:
       raise ValueError('Number of stages should be a value in {1, 2, 3}.')
 
@@ -639,10 +649,14 @@ class FasterRCNNMetaArch(model.DetectionModel):
     # the image window at training time and clipping at inference time.
     clip_window = tf.to_float(tf.stack([0, 0, image_shape[1], image_shape[2]]))
     if self._is_training:
-      (rpn_box_encodings, rpn_objectness_predictions_with_background,
-       anchors_boxlist) = self._remove_invalid_anchors_and_predictions(
-           rpn_box_encodings, rpn_objectness_predictions_with_background,
-           anchors_boxlist, clip_window)
+      if self.clip_anchors_to_image:
+        anchors_boxlist = box_list_ops.clip_to_window(
+            anchors_boxlist, clip_window, filter_nonoverlapping=False)
+      else:
+        (rpn_box_encodings, rpn_objectness_predictions_with_background,
+         anchors_boxlist) = self._remove_invalid_anchors_and_predictions(
+             rpn_box_encodings, rpn_objectness_predictions_with_background,
+             anchors_boxlist, clip_window)
     else:
       anchors_boxlist = box_list_ops.clip_to_window(
           anchors_boxlist, clip_window)
@@ -761,11 +775,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
             flattened_proposal_feature_maps,
             scope=self.second_stage_feature_extractor_scope))
 
-    box_predictions = self._mask_rcnn_box_predictor.predict(
-        [box_classifier_features],
-        num_predictions_per_location=[1],
-        scope=self.second_stage_box_predictor_scope,
-        predict_boxes_and_classes=True)
+    if self._mask_rcnn_box_predictor.is_keras_model:
+      box_predictions = self._mask_rcnn_box_predictor(
+          [box_classifier_features],
+          prediction_stage=2)
+    else:
+      box_predictions = self._mask_rcnn_box_predictor.predict(
+          [box_classifier_features],
+          num_predictions_per_location=[1],
+          scope=self.second_stage_box_predictor_scope,
+          prediction_stage=2)
 
     refined_box_encodings = tf.squeeze(
         box_predictions[box_predictor.BOX_ENCODINGS],
@@ -834,12 +853,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if self._is_training:
       curr_box_classifier_features = prediction_dict['box_classifier_features']
       detection_classes = prediction_dict['class_predictions_with_background']
-      mask_predictions = self._mask_rcnn_box_predictor.predict(
-          [curr_box_classifier_features],
-          num_predictions_per_location=[1],
-          scope=self.second_stage_box_predictor_scope,
-          predict_boxes_and_classes=False,
-          predict_auxiliary_outputs=True)
+      if self._mask_rcnn_box_predictor.is_keras_model:
+        mask_predictions = self._mask_rcnn_box_predictor(
+            [curr_box_classifier_features],
+            prediction_stage=3)
+      else:
+        mask_predictions = self._mask_rcnn_box_predictor.predict(
+            [curr_box_classifier_features],
+            num_predictions_per_location=[1],
+            scope=self.second_stage_box_predictor_scope,
+            prediction_stage=3)
       prediction_dict['mask_predictions'] = tf.squeeze(mask_predictions[
           box_predictor.MASK_PREDICTIONS], axis=1)
     else:
@@ -865,12 +888,16 @@ class FasterRCNNMetaArch(model.DetectionModel):
               flattened_detected_feature_maps,
               scope=self.second_stage_feature_extractor_scope))
 
-      mask_predictions = self._mask_rcnn_box_predictor.predict(
-          [curr_box_classifier_features],
-          num_predictions_per_location=[1],
-          scope=self.second_stage_box_predictor_scope,
-          predict_boxes_and_classes=False,
-          predict_auxiliary_outputs=True)
+      if self._mask_rcnn_box_predictor.is_keras_model:
+        mask_predictions = self._mask_rcnn_box_predictor(
+            [curr_box_classifier_features],
+            prediction_stage=3)
+      else:
+        mask_predictions = self._mask_rcnn_box_predictor.predict(
+            [curr_box_classifier_features],
+            num_predictions_per_location=[1],
+            scope=self.second_stage_box_predictor_scope,
+            prediction_stage=3)
 
       detection_masks = tf.squeeze(mask_predictions[
           box_predictor.MASK_PREDICTIONS], axis=1)
@@ -976,10 +1003,14 @@ class FasterRCNNMetaArch(model.DetectionModel):
     if len(num_anchors_per_location) != 1:
       raise RuntimeError('anchor_generator is expected to generate anchors '
                          'corresponding to a single feature map.')
-    box_predictions = self._first_stage_box_predictor.predict(
-        [rpn_box_predictor_features],
-        num_anchors_per_location,
-        scope=self.first_stage_box_predictor_scope)
+    if self._first_stage_box_predictor.is_keras_model:
+      box_predictions = self._first_stage_box_predictor(
+          [rpn_box_predictor_features])
+    else:
+      box_predictions = self._first_stage_box_predictor.predict(
+          [rpn_box_predictor_features],
+          num_anchors_per_location,
+          scope=self.first_stage_box_predictor_scope)
 
     box_encodings = tf.concat(
         box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
@@ -1393,8 +1424,11 @@ class FasterRCNNMetaArch(model.DetectionModel):
       a BoxList contained sampled proposals.
     """
     (cls_targets, cls_weights, _, _, _) = self._detector_target_assigner.assign(
-        proposal_boxlist, groundtruth_boxlist,
-        groundtruth_classes_with_background)
+        proposal_boxlist,
+        groundtruth_boxlist,
+        groundtruth_classes_with_background,
+        unmatched_class_label=tf.constant(
+            [1] + self._num_classes * [0], dtype=tf.float32))
     # Selects all boxes as candidates if none of them is selected according
     # to cls_weights. This could happen as boxes within certain IOU ranges
     # are ignored. If triggered, the selected boxes will still be ignored
@@ -1672,7 +1706,8 @@ class FasterRCNNMetaArch(model.DetectionModel):
        batch_reg_weights, _) = target_assigner.batch_assign_targets(
            self._proposal_target_assigner, box_list.BoxList(anchors),
            groundtruth_boxlists,
-           len(groundtruth_boxlists) * [None], groundtruth_weights_list)
+           len(groundtruth_boxlists) * [None],
+           gt_weights_batch=groundtruth_weights_list)
       batch_cls_targets = tf.squeeze(batch_cls_targets, axis=2)
 
       def _minibatch_subsample_fn(inputs):
@@ -1792,9 +1827,13 @@ class FasterRCNNMetaArch(model.DetectionModel):
 
       (batch_cls_targets_with_background, batch_cls_weights, batch_reg_targets,
        batch_reg_weights, _) = target_assigner.batch_assign_targets(
-           self._detector_target_assigner, proposal_boxlists,
-           groundtruth_boxlists, groundtruth_classes_with_background_list,
-           groundtruth_weights_list)
+           self._detector_target_assigner,
+           proposal_boxlists,
+           groundtruth_boxlists,
+           groundtruth_classes_with_background_list,
+           unmatched_class_label=tf.constant(
+               [1] + self._num_classes * [0], dtype=tf.float32),
+           gt_weights_batch=groundtruth_weights_list)
 
       class_predictions_with_background = tf.reshape(
           class_predictions_with_background,
@@ -1866,18 +1905,12 @@ class FasterRCNNMetaArch(model.DetectionModel):
           raise ValueError('Groundtruth instance masks not provided. '
                            'Please configure input reader.')
 
-        # Create a new target assigner that matches the proposals to groundtruth
-        # and returns the mask targets.
-        # TODO(rathodv): Move `unmatched_cls_target` from constructor to assign
-        # function. This will enable reuse of a single target assigner for both
-        # class targets and mask targets.
-        mask_target_assigner = target_assigner.create_target_assigner(
-            'FasterRCNN', 'detection',
-            unmatched_cls_target=tf.zeros(image_shape[1:3], dtype=tf.float32))
-        (batch_mask_targets, _, _,
-         batch_mask_target_weights, _) = target_assigner.batch_assign_targets(
-             mask_target_assigner, proposal_boxlists, groundtruth_boxlists,
-             groundtruth_masks_list, groundtruth_weights_list)
+        unmatched_mask_label = tf.zeros(image_shape[1:3], dtype=tf.float32)
+        (batch_mask_targets, _, _, batch_mask_target_weights,
+         _) = target_assigner.batch_assign_targets(
+             self._detector_target_assigner, proposal_boxlists,
+             groundtruth_boxlists, groundtruth_masks_list, unmatched_mask_label,
+             groundtruth_weights_list)
 
         # Pad the prediction_masks with to add zeros for background class to be
         # consistent with class predictions.
diff --git a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
index a1927b0d..a804a19e 100644
--- a/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
+++ b/research/object_detection/meta_architectures/faster_rcnn_meta_arch_test_lib.py
@@ -21,7 +21,9 @@ from object_detection.anchor_generators import grid_anchor_generator
 from object_detection.builders import box_predictor_builder
 from object_detection.builders import hyperparams_builder
 from object_detection.builders import post_processing_builder
+from object_detection.core import balanced_positive_negative_sampler as sampler
 from object_detection.core import losses
+from object_detection.core import target_assigner
 from object_detection.meta_architectures import faster_rcnn_meta_arch
 from object_detection.protos import box_predictor_pb2
 from object_detection.protos import hyperparams_pb2
@@ -153,7 +155,9 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
                    predict_masks=False,
                    pad_to_max_dimension=None,
                    masks_are_class_agnostic=False,
-                   use_matmul_crop_and_resize=False):
+                   use_matmul_crop_and_resize=False,
+                   clip_anchors_to_image=False,
+                   use_matmul_gather_in_matcher=False):
 
     def image_resizer_fn(image, masks=None):
       """Fake image resizer function."""
@@ -186,6 +190,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         first_stage_anchor_scales,
         first_stage_anchor_aspect_ratios,
         anchor_stride=first_stage_anchor_strides)
+    first_stage_target_assigner = target_assigner.create_target_assigner(
+        'FasterRCNN',
+        'proposal',
+        use_matmul_gather=use_matmul_gather_in_matcher)
 
     fake_feature_extractor = FakeFasterRCNNFeatureExtractor()
 
@@ -211,7 +219,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     first_stage_atrous_rate = 1
     first_stage_box_predictor_depth = 512
     first_stage_minibatch_size = 3
-    first_stage_positive_balance_fraction = .5
+    first_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+        positive_fraction=0.5, is_static=False)
 
     first_stage_nms_score_threshold = -1.0
     first_stage_nms_iou_threshold = 1.0
@@ -230,9 +239,14 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     """
     post_processing_config = post_processing_pb2.PostProcessing()
     text_format.Merge(post_processing_text_proto, post_processing_config)
+
+    second_stage_target_assigner = target_assigner.create_target_assigner(
+        'FasterRCNN', 'detection',
+        use_matmul_gather=use_matmul_gather_in_matcher)
     second_stage_non_max_suppression_fn, _ = post_processing_builder.build(
         post_processing_config)
-    second_stage_balance_fraction = 1.0
+    second_stage_sampler = sampler.BalancedPositiveNegativeSampler(
+        positive_fraction=1.0, is_static=False)
 
     second_stage_score_conversion_fn = tf.identity
     second_stage_localization_loss_weight = 1.0
@@ -261,6 +275,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'feature_extractor': fake_feature_extractor,
         'number_of_stages': number_of_stages,
         'first_stage_anchor_generator': first_stage_anchor_generator,
+        'first_stage_target_assigner': first_stage_target_assigner,
         'first_stage_atrous_rate': first_stage_atrous_rate,
         'first_stage_box_predictor_arg_scope_fn':
         first_stage_box_predictor_arg_scope_fn,
@@ -268,8 +283,7 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         first_stage_box_predictor_kernel_size,
         'first_stage_box_predictor_depth': first_stage_box_predictor_depth,
         'first_stage_minibatch_size': first_stage_minibatch_size,
-        'first_stage_positive_balance_fraction':
-        first_stage_positive_balance_fraction,
+        'first_stage_sampler': first_stage_sampler,
         'first_stage_nms_score_threshold': first_stage_nms_score_threshold,
         'first_stage_nms_iou_threshold': first_stage_nms_iou_threshold,
         'first_stage_max_proposals': first_stage_max_proposals,
@@ -277,8 +291,9 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         first_stage_localization_loss_weight,
         'first_stage_objectness_loss_weight':
         first_stage_objectness_loss_weight,
+        'second_stage_target_assigner': second_stage_target_assigner,
         'second_stage_batch_size': second_stage_batch_size,
-        'second_stage_balance_fraction': second_stage_balance_fraction,
+        'second_stage_sampler': second_stage_sampler,
         'second_stage_non_max_suppression_fn':
         second_stage_non_max_suppression_fn,
         'second_stage_score_conversion_fn': second_stage_score_conversion_fn,
@@ -289,7 +304,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         'second_stage_classification_loss':
         second_stage_classification_loss,
         'hard_example_miner': hard_example_miner,
-        'use_matmul_crop_and_resize': use_matmul_crop_and_resize
+        'use_matmul_crop_and_resize': use_matmul_crop_and_resize,
+        'clip_anchors_to_image': clip_anchors_to_image
     }
 
     return self._get_model(
@@ -469,7 +485,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
         self.assertAllEqual(tensor_dict_out[key].shape, expected_shapes[key])
 
   def _test_predict_gives_correct_shapes_in_train_mode_both_stages(
-      self, use_matmul_crop_and_resize=False):
+      self, use_matmul_crop_and_resize=False,
+      clip_anchors_to_image=False):
     test_graph = tf.Graph()
     with test_graph.as_default():
       model = self._build_model(
@@ -477,7 +494,8 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
           number_of_stages=2,
           second_stage_batch_size=7,
           predict_masks=False,
-          use_matmul_crop_and_resize=use_matmul_crop_and_resize)
+          use_matmul_crop_and_resize=use_matmul_crop_and_resize,
+          clip_anchors_to_image=clip_anchors_to_image)
 
       batch_size = 2
       image_size = 10
@@ -547,6 +565,10 @@ class FasterRCNNMetaArchTestBase(tf.test.TestCase):
     self._test_predict_gives_correct_shapes_in_train_mode_both_stages(
         use_matmul_crop_and_resize=True)
 
+  def test_predict_gives_correct_shapes_in_train_mode_clip_anchors(self):
+    self._test_predict_gives_correct_shapes_in_train_mode_both_stages(
+        clip_anchors_to_image=True)
+
   def _test_postprocess_first_stage_only_inference_mode(
       self, pad_to_max_dimension=None):
     model = self._build_model(
diff --git a/research/object_detection/meta_architectures/rfcn_meta_arch.py b/research/object_detection/meta_architectures/rfcn_meta_arch.py
index 4e39070d..796ae14f 100644
--- a/research/object_detection/meta_architectures/rfcn_meta_arch.py
+++ b/research/object_detection/meta_architectures/rfcn_meta_arch.py
@@ -55,20 +55,22 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                feature_extractor,
                number_of_stages,
                first_stage_anchor_generator,
+               first_stage_target_assigner,
                first_stage_atrous_rate,
                first_stage_box_predictor_arg_scope_fn,
                first_stage_box_predictor_kernel_size,
                first_stage_box_predictor_depth,
                first_stage_minibatch_size,
-               first_stage_positive_balance_fraction,
+               first_stage_sampler,
                first_stage_nms_score_threshold,
                first_stage_nms_iou_threshold,
                first_stage_max_proposals,
                first_stage_localization_loss_weight,
                first_stage_objectness_loss_weight,
+               second_stage_target_assigner,
                second_stage_rfcn_box_predictor,
                second_stage_batch_size,
-               second_stage_balance_fraction,
+               second_stage_sampler,
                second_stage_non_max_suppression_fn,
                second_stage_score_conversion_fn,
                second_stage_localization_loss_weight,
@@ -77,7 +79,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
                hard_example_miner,
                parallel_iterations=16,
                add_summaries=True,
-               use_matmul_crop_and_resize=False):
+               use_matmul_crop_and_resize=False,
+               clip_anchors_to_image=False):
     """RFCNMetaArch Constructor.
 
     Args:
@@ -97,6 +100,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       first_stage_anchor_generator: An anchor_generator.AnchorGenerator object
         (note that currently we only support
         grid_anchor_generator.GridAnchorGenerator objects)
+      first_stage_target_assigner: Target assigner to use for first stage of
+        R-FCN (RPN).
       first_stage_atrous_rate: A single integer indicating the atrous rate for
         the single convolution op which is applied to the `rpn_features_to_crop`
         tensor to obtain a tensor to be used for box prediction. Some feature
@@ -116,8 +121,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         "batch size" refers to the number of anchors selected as contributing
         to the loss function for any given image within the image batch and is
         only called "batch_size" due to terminology from the Faster R-CNN paper.
-      first_stage_positive_balance_fraction: Fraction of positive examples
-        per image for the RPN. The recommended value for Faster RCNN is 0.5.
+      first_stage_sampler: The sampler for the boxes used to calculate the RPN
+        loss after the first stage.
       first_stage_nms_score_threshold: Score threshold for non max suppression
         for the Region Proposal Network (RPN).  This value is expected to be in
         [0, 1] as it is applied directly after a softmax transformation.  The
@@ -130,6 +135,10 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         Region Proposal Network (RPN).
       first_stage_localization_loss_weight: A float
       first_stage_objectness_loss_weight: A float
+      second_stage_target_assigner: Target assigner to use for second stage of
+        R-FCN. If the model is configured with multiple prediction heads, this
+        target assigner is used to generate targets for all heads (with the
+        correct `unmatched_class_label`).
       second_stage_rfcn_box_predictor: RFCN box predictor to use for
         second stage.
       second_stage_batch_size: The batch size used for computing the
@@ -137,9 +146,8 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         "batch size" refers to the number of proposals selected as contributing
         to the loss function for any given image within the image batch and is
         only called "batch_size" due to terminology from the Faster R-CNN paper.
-      second_stage_balance_fraction: Fraction of positive examples to use
-        per image for the box classifier. The recommended value for Faster RCNN
-        is 0.25.
+      second_stage_sampler: The sampler for the boxes used for second stage
+        box classifier.
       second_stage_non_max_suppression_fn: batch_multiclass_non_max_suppression
         callable that takes `boxes`, `scores`, optional `clip_window` and
         optional (kwarg) `mask` inputs (with all other inputs already set)
@@ -163,6 +171,9 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
       use_matmul_crop_and_resize: Force the use of matrix multiplication based
         crop and resize instead of standard tf.image.crop_and_resize while
         computing second stage input feature maps.
+      clip_anchors_to_image: The anchors generated are clip to the
+        window size without filtering the nonoverlapping anchors. This generates
+        a static number of anchors. This argument is unused.
 
     Raises:
       ValueError: If `second_stage_batch_size` > `first_stage_max_proposals`
@@ -178,12 +189,13 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         feature_extractor,
         number_of_stages,
         first_stage_anchor_generator,
+        first_stage_target_assigner,
         first_stage_atrous_rate,
         first_stage_box_predictor_arg_scope_fn,
         first_stage_box_predictor_kernel_size,
         first_stage_box_predictor_depth,
         first_stage_minibatch_size,
-        first_stage_positive_balance_fraction,
+        first_stage_sampler,
         first_stage_nms_score_threshold,
         first_stage_nms_iou_threshold,
         first_stage_max_proposals,
@@ -192,9 +204,10 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
         None,  # initial_crop_size is not used in R-FCN
         None,  # maxpool_kernel_size is not use in R-FCN
         None,  # maxpool_stride is not use in R-FCN
+        second_stage_target_assigner,
         None,  # fully_connected_box_predictor is not used in R-FCN.
         second_stage_batch_size,
-        second_stage_balance_fraction,
+        second_stage_sampler,
         second_stage_non_max_suppression_fn,
         second_stage_score_conversion_fn,
         second_stage_localization_loss_weight,
@@ -274,11 +287,16 @@ class RFCNMetaArch(faster_rcnn_meta_arch.FasterRCNNMetaArch):
             rpn_features,
             scope=self.second_stage_feature_extractor_scope))
 
-    box_predictions = self._rfcn_box_predictor.predict(
-        [box_classifier_features],
-        num_predictions_per_location=[1],
-        scope=self.second_stage_box_predictor_scope,
-        proposal_boxes=proposal_boxes_normalized)
+    if self._rfcn_box_predictor.is_keras_model:
+      box_predictions = self._rfcn_box_predictor(
+          [box_classifier_features],
+          proposal_boxes=proposal_boxes_normalized)
+    else:
+      box_predictions = self._rfcn_box_predictor.predict(
+          [box_classifier_features],
+          num_predictions_per_location=[1],
+          scope=self.second_stage_box_predictor_scope,
+          proposal_boxes=proposal_boxes_normalized)
     refined_box_encodings = tf.squeeze(
         tf.concat(box_predictions[box_predictor.BOX_ENCODINGS], axis=1), axis=1)
     class_predictions_with_background = tf.squeeze(
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch.py b/research/object_detection/meta_architectures/ssd_meta_arch.py
index 2be5f6ce..35057813 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch.py
@@ -35,7 +35,7 @@ slim = tf.contrib.slim
 
 
 class SSDFeatureExtractor(object):
-  """SSD Feature Extractor definition."""
+  """SSD Slim Feature Extractor definition."""
 
   def __init__(self,
                is_training,
@@ -77,6 +77,10 @@ class SSDFeatureExtractor(object):
     self._override_base_feature_extractor_hyperparams = (
         override_base_feature_extractor_hyperparams)
 
+  @property
+  def is_keras_model(self):
+    return False
+
   @abstractmethod
   def preprocess(self, resized_inputs):
     """Preprocesses images for feature extraction (minus image resizing).
@@ -113,6 +117,105 @@ class SSDFeatureExtractor(object):
     raise NotImplementedError
 
 
+class SSDKerasFeatureExtractor(tf.keras.Model):
+  """SSD Feature Extractor definition."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_config,
+               freeze_batchnorm,
+               inplace_batchnorm_update,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Constructor.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_config: A hyperparams.proto object containing
+        convolution hyperparameters for the layers added on top of the
+        base feature extractor.
+      freeze_batchnorm: Whether to freeze batch norm parameters during
+        training or not. When training with a small batch size (e.g. 1), it is
+        desirable to freeze batch norm update and use pretrained batch norm
+        params.
+      inplace_batchnorm_update: Whether to update batch norm moving average
+        values inplace. When this is false train op must add a control
+        dependency on tf.graphkeys.UPDATE_OPS collection in order to update
+        batch norm statistics.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_config`.
+    """
+    super(SSDKerasFeatureExtractor, self).__init__()
+
+    self._is_training = is_training
+    self._depth_multiplier = depth_multiplier
+    self._min_depth = min_depth
+    self._pad_to_multiple = pad_to_multiple
+    self._conv_hyperparams_config = conv_hyperparams_config
+    self._freeze_batchnorm = freeze_batchnorm
+    self._inplace_batchnorm_update = inplace_batchnorm_update
+    self._use_explicit_padding = use_explicit_padding
+    self._use_depthwise = use_depthwise
+    self._override_base_feature_extractor_hyperparams = (
+        override_base_feature_extractor_hyperparams)
+
+  @property
+  def is_keras_model(self):
+    return True
+
+  @abstractmethod
+  def preprocess(self, resized_inputs):
+    """Preprocesses images for feature extraction (minus image resizing).
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+      true_image_shapes: int32 tensor of shape [batch, 3] where each row is
+        of the form [height, width, channels] indicating the shapes
+        of true images in the resized images, as resized images can be padded
+        with zeros.
+    """
+    raise NotImplementedError
+
+  @abstractmethod
+  def _extract_features(self, preprocessed_inputs):
+    """Extracts features from preprocessed inputs.
+
+    This function is responsible for extracting feature maps from preprocessed
+    images.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    raise NotImplementedError
+
+  # This overrides the keras.Model `call` method with the _extract_features
+  # method.
+  def call(self, inputs, **kwargs):
+    return self._extract_features(inputs)
+
+
 class SSDMetaArch(model.DetectionModel):
   """SSD Meta-architecture definition."""
 
@@ -211,10 +314,6 @@ class SSDMetaArch(model.DetectionModel):
     self._freeze_batchnorm = freeze_batchnorm
     self._inplace_batchnorm_update = inplace_batchnorm_update
 
-    # Needed for fine-tuning from classification checkpoints whose
-    # variables do not have the feature extractor scope.
-    self._extract_features_scope = 'FeatureExtractor'
-
     self._anchor_generator = anchor_generator
     self._box_predictor = box_predictor
 
@@ -224,21 +323,30 @@ class SSDMetaArch(model.DetectionModel):
     self._region_similarity_calculator = region_similarity_calculator
     self._add_background_class = add_background_class
 
+    # Needed for fine-tuning from classification checkpoints whose
+    # variables do not have the feature extractor scope.
+    if self._feature_extractor.is_keras_model:
+      # Keras feature extractors will have a name they implicitly use to scope.
+      # So, all contained variables are prefixed by this name.
+      # To load from classification checkpoints, need to filter out this name.
+      self._extract_features_scope = feature_extractor.name
+    else:
+      # Slim feature extractors get an explicit naming scope
+      self._extract_features_scope = 'FeatureExtractor'
+
     # TODO(jonathanhuang): handle agnostic mode
     # weights
-    unmatched_cls_target = None
-    unmatched_cls_target = tf.constant([1] + self.num_classes * [0],
-                                       tf.float32)
+    self._unmatched_class_label = tf.constant([1] + self.num_classes * [0],
+                                              tf.float32)
     if encode_background_as_zeros:
-      unmatched_cls_target = tf.constant((self.num_classes + 1) * [0],
-                                         tf.float32)
+      self._unmatched_class_label = tf.constant((self.num_classes + 1) * [0],
+                                                tf.float32)
 
     self._target_assigner = target_assigner.TargetAssigner(
         self._region_similarity_calculator,
         self._matcher,
         self._box_coder,
-        negative_class_weight=negative_class_weight,
-        unmatched_cls_target=unmatched_cls_target)
+        negative_class_weight=negative_class_weight)
 
     self._classification_loss = classification_loss
     self._localization_loss = localization_loss
@@ -383,41 +491,53 @@ class SSDMetaArch(model.DetectionModel):
     """
     batchnorm_updates_collections = (None if self._inplace_batchnorm_update
                                      else tf.GraphKeys.UPDATE_OPS)
-    with slim.arg_scope([slim.batch_norm],
-                        is_training=(self._is_training and
-                                     not self._freeze_batchnorm),
-                        updates_collections=batchnorm_updates_collections):
-      with tf.variable_scope(None, self._extract_features_scope,
-                             [preprocessed_inputs]):
-        feature_maps = self._feature_extractor.extract_features(
-            preprocessed_inputs)
-      feature_map_spatial_dims = self._get_feature_map_spatial_dims(
-          feature_maps)
-      image_shape = shape_utils.combined_static_and_dynamic_shape(
-          preprocessed_inputs)
-      self._anchors = box_list_ops.concatenate(
-          self._anchor_generator.generate(
-              feature_map_spatial_dims,
-              im_height=image_shape[1],
-              im_width=image_shape[2]))
-      prediction_dict = self._box_predictor.predict(
-          feature_maps, self._anchor_generator.num_anchors_per_location())
-      box_encodings = tf.concat(prediction_dict['box_encodings'], axis=1)
-      if box_encodings.shape.ndims == 4 and box_encodings.shape[2] == 1:
-        box_encodings = tf.squeeze(box_encodings, axis=2)
-      class_predictions_with_background = tf.concat(
-          prediction_dict['class_predictions_with_background'], axis=1)
-      predictions_dict = {
-          'preprocessed_inputs': preprocessed_inputs,
-          'box_encodings': box_encodings,
-          'class_predictions_with_background':
-          class_predictions_with_background,
-          'feature_maps': feature_maps,
-          'anchors': self._anchors.get()
-      }
-      self._batched_prediction_tensor_names = [x for x in predictions_dict
-                                               if x != 'anchors']
-      return predictions_dict
+    if self._feature_extractor.is_keras_model:
+      feature_maps = self._feature_extractor(preprocessed_inputs)
+    else:
+      with slim.arg_scope([slim.batch_norm],
+                          is_training=(self._is_training and
+                                       not self._freeze_batchnorm),
+                          updates_collections=batchnorm_updates_collections):
+        with tf.variable_scope(None, self._extract_features_scope,
+                               [preprocessed_inputs]):
+          feature_maps = self._feature_extractor.extract_features(
+              preprocessed_inputs)
+
+    feature_map_spatial_dims = self._get_feature_map_spatial_dims(
+        feature_maps)
+    image_shape = shape_utils.combined_static_and_dynamic_shape(
+        preprocessed_inputs)
+    self._anchors = box_list_ops.concatenate(
+        self._anchor_generator.generate(
+            feature_map_spatial_dims,
+            im_height=image_shape[1],
+            im_width=image_shape[2]))
+    if self._box_predictor.is_keras_model:
+      prediction_dict = self._box_predictor(feature_maps)
+    else:
+      with slim.arg_scope([slim.batch_norm],
+                          is_training=(self._is_training and
+                                       not self._freeze_batchnorm),
+                          updates_collections=batchnorm_updates_collections):
+        prediction_dict = self._box_predictor.predict(
+            feature_maps, self._anchor_generator.num_anchors_per_location())
+
+    box_encodings = tf.concat(prediction_dict['box_encodings'], axis=1)
+    if box_encodings.shape.ndims == 4 and box_encodings.shape[2] == 1:
+      box_encodings = tf.squeeze(box_encodings, axis=2)
+    class_predictions_with_background = tf.concat(
+        prediction_dict['class_predictions_with_background'], axis=1)
+    predictions_dict = {
+        'preprocessed_inputs': preprocessed_inputs,
+        'box_encodings': box_encodings,
+        'class_predictions_with_background':
+        class_predictions_with_background,
+        'feature_maps': feature_maps,
+        'anchors': self._anchors.get()
+    }
+    self._batched_prediction_tensor_names = [x for x in predictions_dict
+                                             if x != 'anchors']
+    return predictions_dict
 
   def _get_feature_map_spatial_dims(self, feature_maps):
     """Return list of spatial dimensions for each feature map in a list.
@@ -710,7 +830,8 @@ class SSDMetaArch(model.DetectionModel):
         boxlist.add_field(fields.BoxListFields.keypoints, keypoints)
     return target_assigner.batch_assign_targets(
         self._target_assigner, self.anchors, groundtruth_boxlists,
-        groundtruth_classes_with_background_list, groundtruth_weights_list)
+        groundtruth_classes_with_background_list, self._unmatched_class_label,
+        groundtruth_weights_list)
 
   def _summarize_target_assignment(self, groundtruth_boxes_list, match_list):
     """Creates tensorflow summaries for the input boxes and anchors.
@@ -872,3 +993,4 @@ class SSDMetaArch(model.DetectionModel):
           variables_to_restore[var_name] = variable
 
     return variables_to_restore
+
diff --git a/research/object_detection/meta_architectures/ssd_meta_arch_test.py b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
index b1b62a3c..c5fd124b 100644
--- a/research/object_detection/meta_architectures/ssd_meta_arch_test.py
+++ b/research/object_detection/meta_architectures/ssd_meta_arch_test.py
@@ -15,6 +15,8 @@
 
 """Tests for object_detection.meta_architectures.ssd_meta_arch."""
 import functools
+from absl.testing import parameterized
+
 import numpy as np
 import tensorflow as tf
 
@@ -29,6 +31,7 @@ from object_detection.utils import test_case
 from object_detection.utils import test_utils
 
 slim = tf.contrib.slim
+keras = tf.keras.layers
 
 
 class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
@@ -51,6 +54,30 @@ class FakeSSDFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       return [features]
 
 
+class FakeSSDKerasFeatureExtractor(ssd_meta_arch.SSDKerasFeatureExtractor):
+
+  def __init__(self):
+    with tf.name_scope('mock_model'):
+      super(FakeSSDKerasFeatureExtractor, self).__init__(
+          is_training=True,
+          depth_multiplier=0,
+          min_depth=0,
+          pad_to_multiple=1,
+          conv_hyperparams_config=None,
+          freeze_batchnorm=False,
+          inplace_batchnorm_update=False,
+      )
+
+      self._conv = keras.Conv2D(filters=32, kernel_size=1, name='layer1')
+
+  def preprocess(self, resized_inputs):
+    return tf.identity(resized_inputs)
+
+  def _extract_features(self, preprocessed_inputs, **kwargs):
+    with tf.name_scope('mock_model'):
+      return [self._conv(preprocessed_inputs)]
+
+
 class MockAnchorGenerator2x2(anchor_generator.AnchorGenerator):
   """Sets up a simple 2x2 anchor grid on the unit square."""
 
@@ -79,20 +106,32 @@ def _get_value_for_matching_key(dictionary, suffix):
   raise ValueError('key not found {}'.format(suffix))
 
 
-class SsdMetaArchTest(test_case.TestCase):
+@parameterized.parameters(
+    {'use_keras': False},
+    {'use_keras': True},
+)
+class SsdMetaArchTest(test_case.TestCase, parameterized.TestCase):
 
   def _create_model(self,
                     apply_hard_mining=True,
                     normalize_loc_loss_by_codesize=False,
                     add_background_class=True,
-                    random_example_sampling=False):
+                    random_example_sampling=False,
+                    use_keras=False):
     is_training = False
     num_classes = 1
     mock_anchor_generator = MockAnchorGenerator2x2()
-    mock_box_predictor = test_utils.MockBoxPredictor(
-        is_training, num_classes)
+    if use_keras:
+      mock_box_predictor = test_utils.MockKerasBoxPredictor(
+          is_training, num_classes)
+    else:
+      mock_box_predictor = test_utils.MockBoxPredictor(
+          is_training, num_classes)
     mock_box_coder = test_utils.MockBoxCoder()
-    fake_feature_extractor = FakeSSDFeatureExtractor()
+    if use_keras:
+      fake_feature_extractor = FakeSSDKerasFeatureExtractor()
+    else:
+      fake_feature_extractor = FakeSSDFeatureExtractor()
     mock_matcher = test_utils.MockMatcher()
     region_similarity_calculator = sim_calc.IouSimilarity()
     encode_background_as_zeros = False
@@ -152,25 +191,26 @@ class SsdMetaArchTest(test_case.TestCase):
         random_example_sampler=random_example_sampler)
     return model, num_classes, mock_anchor_generator.num_anchors(), code_size
 
-  def test_preprocess_preserves_shapes_with_dynamic_input_image(self):
+  def test_preprocess_preserves_shapes_with_dynamic_input_image(
+      self, use_keras):
     image_shapes = [(3, None, None, 3),
                     (None, 10, 10, 3),
                     (None, None, None, 3)]
-    model, _, _, _ = self._create_model()
+    model, _, _, _ = self._create_model(use_keras=use_keras)
     for image_shape in image_shapes:
       image_placeholder = tf.placeholder(tf.float32, shape=image_shape)
       preprocessed_inputs, _ = model.preprocess(image_placeholder)
       self.assertAllEqual(preprocessed_inputs.shape.as_list(), image_shape)
 
-  def test_preprocess_preserves_shape_with_static_input_image(self):
+  def test_preprocess_preserves_shape_with_static_input_image(self, use_keras):
     def graph_fn(input_image):
-      model, _, _, _ = self._create_model()
+      model, _, _, _ = self._create_model(use_keras=use_keras)
       return model.preprocess(input_image)
     input_image = np.random.rand(2, 3, 3, 3).astype(np.float32)
     preprocessed_inputs, _ = self.execute(graph_fn, [input_image])
     self.assertAllEqual(preprocessed_inputs.shape, [2, 3, 3, 3])
 
-  def test_predict_result_shapes_on_image_with_dynamic_shape(self):
+  def test_predict_result_shapes_on_image_with_dynamic_shape(self, use_keras):
     batch_size = 3
     image_size = 2
     input_shapes = [(None, image_size, image_size, 3),
@@ -180,16 +220,17 @@ class SsdMetaArchTest(test_case.TestCase):
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
       with tf_graph.as_default():
-        model, num_classes, num_anchors, code_size = self._create_model()
+        model, num_classes, num_anchors, code_size = self._create_model(
+            use_keras=use_keras)
         preprocessed_input_placeholder = tf.placeholder(tf.float32,
                                                         shape=input_shape)
         prediction_dict = model.predict(
             preprocessed_input_placeholder, true_image_shapes=None)
 
-        self.assertTrue('box_encodings' in prediction_dict)
-        self.assertTrue('class_predictions_with_background' in prediction_dict)
-        self.assertTrue('feature_maps' in prediction_dict)
-        self.assertTrue('anchors' in prediction_dict)
+        self.assertIn('box_encodings', prediction_dict)
+        self.assertIn('class_predictions_with_background', prediction_dict)
+        self.assertIn('feature_maps', prediction_dict)
+        self.assertIn('anchors', prediction_dict)
 
         init_op = tf.global_variables_initializer()
       with self.test_session(graph=tf_graph) as sess:
@@ -210,10 +251,11 @@ class SsdMetaArchTest(test_case.TestCase):
           prediction_out['class_predictions_with_background'].shape,
           expected_class_predictions_with_background_shape_out)
 
-  def test_predict_result_shapes_on_image_with_static_shape(self):
+  def test_predict_result_shapes_on_image_with_static_shape(self, use_keras):
 
     with tf.Graph().as_default():
-      _, num_classes, num_anchors, code_size = self._create_model()
+      _, num_classes, num_anchors, code_size = self._create_model(
+          use_keras=use_keras)
 
     def graph_fn(input_image):
       model, _, _, _ = self._create_model()
@@ -235,7 +277,7 @@ class SsdMetaArchTest(test_case.TestCase):
     self.assertAllEqual(class_predictions.shape,
                         expected_class_predictions_shape)
 
-  def test_postprocess_results_are_correct(self):
+  def test_postprocess_results_are_correct(self, use_keras):
     batch_size = 2
     image_size = 2
     input_shapes = [(batch_size, image_size, image_size, 3),
@@ -266,17 +308,17 @@ class SsdMetaArchTest(test_case.TestCase):
     for input_shape in input_shapes:
       tf_graph = tf.Graph()
       with tf_graph.as_default():
-        model, _, _, _ = self._create_model()
+        model, _, _, _ = self._create_model(use_keras=use_keras)
         input_placeholder = tf.placeholder(tf.float32, shape=input_shape)
         preprocessed_inputs, true_image_shapes = model.preprocess(
             input_placeholder)
         prediction_dict = model.predict(preprocessed_inputs,
                                         true_image_shapes)
         detections = model.postprocess(prediction_dict, true_image_shapes)
-        self.assertTrue('detection_boxes' in detections)
-        self.assertTrue('detection_scores' in detections)
-        self.assertTrue('detection_classes' in detections)
-        self.assertTrue('num_detections' in detections)
+        self.assertIn('detection_boxes', detections)
+        self.assertIn('detection_scores', detections)
+        self.assertIn('detection_classes', detections)
+        self.assertIn('num_detections', detections)
         init_op = tf.global_variables_initializer()
       with self.test_session(graph=tf_graph) as sess:
         sess.run(init_op)
@@ -295,10 +337,10 @@ class SsdMetaArchTest(test_case.TestCase):
       self.assertAllClose(detections_out['num_detections'],
                           expected_num_detections)
 
-  def test_loss_results_are_correct(self):
+  def test_loss_results_are_correct(self, use_keras):
 
     with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model()
+      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
@@ -331,16 +373,18 @@ class SsdMetaArchTest(test_case.TestCase):
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_loss_results_are_correct_with_normalize_by_codesize_true(self):
+  def test_loss_results_are_correct_with_normalize_by_codesize_true(
+      self, use_keras):
 
     with tf.Graph().as_default():
-      _, _, _, _ = self._create_model()
+      _, _, _, _ = self._create_model(use_keras=use_keras)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
       model, _, _, _ = self._create_model(apply_hard_mining=False,
-                                          normalize_loc_loss_by_codesize=True)
+                                          normalize_loc_loss_by_codesize=True,
+                                          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(preprocessed_tensor,
@@ -362,10 +406,10 @@ class SsdMetaArchTest(test_case.TestCase):
                                                 groundtruth_classes2])
     self.assertAllClose(localization_loss, expected_localization_loss)
 
-  def test_loss_results_are_correct_with_hard_example_mining(self):
+  def test_loss_results_are_correct_with_hard_example_mining(self, use_keras):
 
     with tf.Graph().as_default():
-      _, num_classes, num_anchors, _ = self._create_model()
+      _, num_classes, num_anchors, _ = self._create_model(use_keras=use_keras)
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
@@ -397,18 +441,20 @@ class SsdMetaArchTest(test_case.TestCase):
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_loss_results_are_correct_without_add_background_class(self):
+  def test_loss_results_are_correct_without_add_background_class(
+      self, use_keras):
 
     with tf.Graph().as_default():
       _, num_classes, num_anchors, _ = self._create_model(
-          add_background_class=False)
+          add_background_class=False, use_keras=use_keras)
 
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
       model, _, _, _ = self._create_model(
-          apply_hard_mining=False, add_background_class=False)
+          apply_hard_mining=False, add_background_class=False,
+          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(
@@ -434,8 +480,8 @@ class SsdMetaArchTest(test_case.TestCase):
     self.assertAllClose(localization_loss, expected_localization_loss)
     self.assertAllClose(classification_loss, expected_classification_loss)
 
-  def test_restore_map_for_detection_ckpt(self):
-    model, _, _, _ = self._create_model()
+  def test_restore_map_for_detection_ckpt(self, use_keras):
+    model, _, _, _ = self._create_model(use_keras=use_keras)
     model.predict(tf.constant(np.array([[[[0, 0], [1, 1]], [[1, 0], [0, 1]]]],
                                        dtype=np.float32)),
                   true_image_shapes=None)
@@ -454,14 +500,22 @@ class SsdMetaArchTest(test_case.TestCase):
       for var in sess.run(tf.report_uninitialized_variables()):
         self.assertNotIn('FeatureExtractor', var)
 
-  def test_restore_map_for_classification_ckpt(self):
+  def test_restore_map_for_classification_ckpt(self, use_keras):
     # Define mock tensorflow classification graph and save variables.
     test_graph_classification = tf.Graph()
     with test_graph_classification.as_default():
       image = tf.placeholder(dtype=tf.float32, shape=[1, 20, 20, 3])
-      with tf.variable_scope('mock_model'):
-        net = slim.conv2d(image, num_outputs=32, kernel_size=1, scope='layer1')
-        slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
+      if use_keras:
+        with tf.name_scope('mock_model'):
+          layer_one = keras.Conv2D(32, kernel_size=1, name='layer1')
+          net = layer_one(image)
+          layer_two = keras.Conv2D(3, kernel_size=1, name='layer2')
+          layer_two(net)
+      else:
+        with tf.variable_scope('mock_model'):
+          net = slim.conv2d(image, num_outputs=32, kernel_size=1,
+                            scope='layer1')
+          slim.conv2d(net, num_outputs=3, kernel_size=1, scope='layer2')
 
       init_op = tf.global_variables_initializer()
       saver = tf.train.Saver()
@@ -474,7 +528,7 @@ class SsdMetaArchTest(test_case.TestCase):
     # classification checkpoint.
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model()
+      model, _, _, _ = self._create_model(use_keras=use_keras)
       inputs_shape = [2, 2, 2, 3]
       inputs = tf.to_float(tf.random_uniform(
           inputs_shape, minval=0, maxval=255, dtype=tf.int32))
@@ -491,10 +545,10 @@ class SsdMetaArchTest(test_case.TestCase):
         for var in sess.run(tf.report_uninitialized_variables()):
           self.assertNotIn('FeatureExtractor', var)
 
-  def test_load_all_det_checkpoint_vars(self):
+  def test_load_all_det_checkpoint_vars(self, use_keras):
     test_graph_detection = tf.Graph()
     with test_graph_detection.as_default():
-      model, _, _, _ = self._create_model()
+      model, _, _, _ = self._create_model(use_keras=use_keras)
       inputs_shape = [2, 2, 2, 3]
       inputs = tf.to_float(
           tf.random_uniform(inputs_shape, minval=0, maxval=255, dtype=tf.int32))
@@ -508,18 +562,22 @@ class SsdMetaArchTest(test_case.TestCase):
       self.assertIsInstance(var_map, dict)
       self.assertIn('another_variable', var_map)
 
-  def test_loss_results_are_correct_with_random_example_sampling(self):
+  def test_loss_results_are_correct_with_random_example_sampling(
+      self,
+      use_keras):
 
     with tf.Graph().as_default():
       _, num_classes, num_anchors, _ = self._create_model(
-          random_example_sampling=True)
+          random_example_sampling=True,
+          use_keras=use_keras)
     print num_classes, num_anchors
 
     def graph_fn(preprocessed_tensor, groundtruth_boxes1, groundtruth_boxes2,
                  groundtruth_classes1, groundtruth_classes2):
       groundtruth_boxes_list = [groundtruth_boxes1, groundtruth_boxes2]
       groundtruth_classes_list = [groundtruth_classes1, groundtruth_classes2]
-      model, _, _, _ = self._create_model(random_example_sampling=True)
+      model, _, _, _ = self._create_model(random_example_sampling=True,
+                                          use_keras=use_keras)
       model.provide_groundtruth(groundtruth_boxes_list,
                                 groundtruth_classes_list)
       prediction_dict = model.predict(
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 23bcd0df..90e0a4f7 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -202,6 +202,10 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
     params = params or {}
     total_loss, train_op, detections, export_outputs = None, None, None, None
     is_training = mode == tf.estimator.ModeKeys.TRAIN
+
+    # Make sure to set the Keras learning phase. True during training,
+    # False for inference.
+    tf.keras.backend.set_learning_phase(is_training)
     detection_model = detection_model_fn(is_training=is_training,
                                          add_summaries=(not use_tpu))
     scaffold_fn = None
@@ -279,7 +283,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
     if mode in (tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL):
       losses_dict = detection_model.loss(
           prediction_dict, features[fields.InputDataFields.true_image_shape])
-      losses = [loss_tensor for loss_tensor in losses_dict.itervalues()]
+      losses = [loss_tensor for loss_tensor in losses_dict.values()]
       if train_config.add_regularization_loss:
         regularization_losses = tf.get_collection(
             tf.GraphKeys.REGULARIZATION_LOSSES)
diff --git a/research/object_detection/models/feature_map_generators.py b/research/object_detection/models/feature_map_generators.py
index 899a4280..42ae19e1 100644
--- a/research/object_detection/models/feature_map_generators.py
+++ b/research/object_detection/models/feature_map_generators.py
@@ -221,8 +221,8 @@ def fpn_top_down_feature_maps(image_features, depth, scope=None):
             depth, [3, 3],
             scope='smoothing_%d' % (level + 1)))
         output_feature_map_keys.append('top_down_%s' % image_features[level][0])
-      return collections.OrderedDict(
-          reversed(zip(output_feature_map_keys, output_feature_maps_list)))
+      return collections.OrderedDict(reversed(
+          list(zip(output_feature_map_keys, output_feature_maps_list))))
 
 
 def pooling_pyramid_feature_maps(base_feature_map_depth, num_layers,
@@ -288,4 +288,3 @@ def pooling_pyramid_feature_maps(base_feature_map_depth, num_layers,
       feature_maps.append(feature_map)
   return collections.OrderedDict(
       [(x, y) for (x, y) in zip(feature_map_keys, feature_maps)])
-
diff --git a/research/object_detection/models/ssd_inception_v2_feature_extractor.py b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
index b97b0f2b..f7e97527 100644
--- a/research/object_detection/models/ssd_inception_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v2_feature_extractor.py
@@ -61,8 +61,15 @@ class SSDInceptionV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       ValueError: If `override_base_feature_extractor_hyperparams` is False.
     """
     super(SSDInceptionV2FeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
     if not self._override_base_feature_extractor_hyperparams:
       raise ValueError('SSD Inception V2 feature extractor always uses'
diff --git a/research/object_detection/models/ssd_inception_v3_feature_extractor.py b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
index 5d97e7b5..b0ff9ab6 100644
--- a/research/object_detection/models/ssd_inception_v3_feature_extractor.py
+++ b/research/object_detection/models/ssd_inception_v3_feature_extractor.py
@@ -61,8 +61,15 @@ class SSDInceptionV3FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       ValueError: If `override_base_feature_extractor_hyperparams` is False.
     """
     super(SSDInceptionV3FeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
     if not self._override_base_feature_extractor_hyperparams:
diff --git a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
index aada1111..3b859ab0 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py
@@ -61,8 +61,15 @@ class SSDMobileNetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         `conv_hyperparams_fn`.
     """
     super(SSDMobileNetV1FeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
   def preprocess(self, resized_inputs):
diff --git a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
index a52b7572..bb5bf73c 100644
--- a/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v1_fpn_feature_extractor.py
@@ -30,6 +30,61 @@ slim = tf.contrib.slim
 class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
   """SSD Feature Extractor using MobilenetV1 FPN features."""
 
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               fpn_min_level=3,
+               fpn_max_level=7,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """SSD FPN feature extractor based on Mobilenet v1 architecture.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the base
+        feature extractor.
+      fpn_min_level: the highest resolution feature map to use in FPN. The valid
+        values are {2, 3, 4, 5} which map to MobileNet v1 layers
+        {Conv2d_3_pointwise, Conv2d_5_pointwise, Conv2d_11_pointwise,
+        Conv2d_13_pointwise}, respectively.
+      fpn_max_level: the smallest resolution feature map to construct or use in
+        FPN. FPN constructions uses features maps starting from fpn_min_level
+        upto the fpn_max_level. In the case that there are not enough feature
+        maps in the backbone network, additional feature maps are created by
+        applying stride 2 convolutions until we get the desired number of fpn
+        levels.
+      reuse_weights: whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDMobileNetV1FpnFeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
+    self._fpn_min_level = fpn_min_level
+    self._fpn_max_level = fpn_max_level
+
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
 
@@ -78,24 +133,31 @@ class SSDMobileNetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       depth_fn = lambda d: max(int(d * self._depth_multiplier), self._min_depth)
       with slim.arg_scope(self._conv_hyperparams_fn()):
         with tf.variable_scope('fpn', reuse=self._reuse_weights):
+          feature_blocks = [
+              'Conv2d_3_pointwise', 'Conv2d_5_pointwise', 'Conv2d_11_pointwise',
+              'Conv2d_13_pointwise'
+          ]
+          base_fpn_max_level = min(self._fpn_max_level, 5)
+          feature_block_list = []
+          for level in range(self._fpn_min_level, base_fpn_max_level + 1):
+            feature_block_list.append(feature_blocks[level - 2])
           fpn_features = feature_map_generators.fpn_top_down_feature_maps(
-              [(key, image_features[key])
-               for key in ['Conv2d_5_pointwise', 'Conv2d_11_pointwise',
-                           'Conv2d_13_pointwise']],
+              [(key, image_features[key]) for key in feature_block_list],
               depth=depth_fn(256))
-          last_feature_map = fpn_features['top_down_Conv2d_13_pointwise']
-          coarse_features = {}
-          for i in range(14, 16):
+          feature_maps = []
+          for level in range(self._fpn_min_level, base_fpn_max_level + 1):
+            feature_maps.append(fpn_features['top_down_{}'.format(
+                feature_blocks[level - 2])])
+          last_feature_map = fpn_features['top_down_{}'.format(
+              feature_blocks[base_fpn_max_level - 2])]
+          # Construct coarse features
+          for i in range(base_fpn_max_level + 1, self._fpn_max_level + 1):
             last_feature_map = slim.conv2d(
                 last_feature_map,
                 num_outputs=depth_fn(256),
                 kernel_size=[3, 3],
                 stride=2,
                 padding='SAME',
-                scope='bottom_up_Conv2d_{}'.format(i))
-            coarse_features['bottom_up_Conv2d_{}'.format(i)] = last_feature_map
-    return [fpn_features['top_down_Conv2d_5_pointwise'],
-            fpn_features['top_down_Conv2d_11_pointwise'],
-            fpn_features['top_down_Conv2d_13_pointwise'],
-            coarse_features['bottom_up_Conv2d_14'],
-            coarse_features['bottom_up_Conv2d_15']]
+                scope='bottom_up_Conv2d_{}'.format(i - base_fpn_max_level + 13))
+            feature_maps.append(last_feature_map)
+    return feature_maps
diff --git a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
index 014b93a8..d8e5ccc9 100644
--- a/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
+++ b/research/object_detection/models/ssd_mobilenet_v2_feature_extractor.py
@@ -64,8 +64,15 @@ class SSDMobileNetV2FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
         `conv_hyperparams_fn`.
     """
     super(SSDMobileNetV2FeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding, use_depthwise,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
   def preprocess(self, resized_inputs):
diff --git a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
index 65bda3f4..5d0c78c5 100644
--- a/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
+++ b/research/object_detection/models/ssd_resnet_v1_fpn_feature_extractor.py
@@ -41,6 +41,8 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                resnet_base_fn,
                resnet_scope_name,
                fpn_scope_name,
+               fpn_min_level=3,
+               fpn_max_level=7,
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
@@ -61,6 +63,15 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       resnet_scope_name: scope name under which to construct resnet
       fpn_scope_name: scope name under which to construct the feature pyramid
         network.
+      fpn_min_level: the highest resolution feature map to use in FPN. The valid
+        values are {2, 3, 4, 5} which map to Resnet blocks {1, 2, 3, 4}
+        respectively.
+      fpn_max_level: the smallest resolution feature map to construct or use in
+        FPN. FPN constructions uses features maps starting from fpn_min_level
+        upto the fpn_max_level. In the case that there are not enough feature
+        maps in the backbone network, additional feature maps are created by
+        applying stride 2 convolutions until we get the desired number of fpn
+        levels.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
@@ -73,8 +84,15 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       ValueError: On supplying invalid arguments for unused arguments.
     """
     super(_SSDResnetV1FpnFeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, reuse_weights, use_explicit_padding,
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
     if self._depth_multiplier != 1.0:
       raise ValueError('Only depth 1.0 is supported, found: {}'.
@@ -84,6 +102,8 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     self._resnet_base_fn = resnet_base_fn
     self._resnet_scope_name = resnet_scope_name
     self._fpn_scope_name = fpn_scope_name
+    self._fpn_min_level = fpn_min_level
+    self._fpn_max_level = fpn_max_level
 
   def preprocess(self, resized_inputs):
     """SSD preprocessing.
@@ -108,7 +128,7 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
     filtered_image_features = dict({})
     for key, feature in image_features.items():
       feature_name = key.split('/')[-1]
-      if feature_name in ['block2', 'block3', 'block4']:
+      if feature_name in ['block1', 'block2', 'block3', 'block4']:
         filtered_image_features[feature_name] = feature
     return filtered_image_features
 
@@ -151,13 +171,21 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
       with slim.arg_scope(self._conv_hyperparams_fn()):
         with tf.variable_scope(self._fpn_scope_name,
                                reuse=self._reuse_weights):
+          base_fpn_max_level = min(self._fpn_max_level, 5)
+          feature_block_list = []
+          for level in range(self._fpn_min_level, base_fpn_max_level + 1):
+            feature_block_list.append('block{}'.format(level - 1))
           fpn_features = feature_map_generators.fpn_top_down_feature_maps(
-              [(key, image_features[key])
-               for key in ['block2', 'block3', 'block4']],
+              [(key, image_features[key]) for key in feature_block_list],
               depth=256)
-          last_feature_map = fpn_features['top_down_block4']
-          coarse_features = {}
-          for i in range(5, 7):
+          feature_maps = []
+          for level in range(self._fpn_min_level, base_fpn_max_level + 1):
+            feature_maps.append(
+                fpn_features['top_down_block{}'.format(level - 1)])
+          last_feature_map = fpn_features['top_down_block{}'.format(
+              base_fpn_max_level - 1)]
+          # Construct coarse features
+          for i in range(base_fpn_max_level, self._fpn_max_level):
             last_feature_map = slim.conv2d(
                 last_feature_map,
                 num_outputs=256,
@@ -165,15 +193,12 @@ class _SSDResnetV1FpnFeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
                 stride=2,
                 padding='SAME',
                 scope='bottom_up_block{}'.format(i))
-            coarse_features['bottom_up_block{}'.format(i)] = last_feature_map
-    return [fpn_features['top_down_block2'],
-            fpn_features['top_down_block3'],
-            fpn_features['top_down_block4'],
-            coarse_features['bottom_up_block5'],
-            coarse_features['bottom_up_block6']]
+            feature_maps.append(last_feature_map)
+    return feature_maps
 
 
 class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+  """SSD Resnet50 V1 FPN feature extractor."""
 
   def __init__(self,
                is_training,
@@ -181,6 +206,8 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                min_depth,
                pad_to_multiple,
                conv_hyperparams_fn,
+               fpn_min_level=3,
+               fpn_max_level=7,
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
@@ -197,6 +224,8 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
         and separable_conv2d ops in the layers that are added on top of the
         base feature extractor.
+      fpn_min_level: the minimum level in feature pyramid networks.
+      fpn_max_level: the maximum level in feature pyramid networks.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
@@ -206,13 +235,25 @@ class SSDResnet50V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         `conv_hyperparams_fn`.
     """
     super(SSDResnet50V1FpnFeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, resnet_v1.resnet_v1_50, 'resnet_v1_50', 'fpn',
-        reuse_weights, use_explicit_padding,
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        conv_hyperparams_fn,
+        resnet_v1.resnet_v1_50,
+        'resnet_v1_50',
+        'fpn',
+        fpn_min_level,
+        fpn_max_level,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
 
 class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+  """SSD Resnet101 V1 FPN feature extractor."""
 
   def __init__(self,
                is_training,
@@ -220,6 +261,8 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                min_depth,
                pad_to_multiple,
                conv_hyperparams_fn,
+               fpn_min_level=3,
+               fpn_max_level=7,
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
@@ -236,6 +279,8 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
         and separable_conv2d ops in the layers that are added on top of the
         base feature extractor.
+      fpn_min_level: the minimum level in feature pyramid networks.
+      fpn_max_level: the maximum level in feature pyramid networks.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
@@ -245,13 +290,25 @@ class SSDResnet101V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         `conv_hyperparams_fn`.
     """
     super(SSDResnet101V1FpnFeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, resnet_v1.resnet_v1_101, 'resnet_v1_101', 'fpn',
-        reuse_weights, use_explicit_padding,
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        conv_hyperparams_fn,
+        resnet_v1.resnet_v1_101,
+        'resnet_v1_101',
+        'fpn',
+        fpn_min_level,
+        fpn_max_level,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
 
 
 class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
+  """SSD Resnet152 V1 FPN feature extractor."""
 
   def __init__(self,
                is_training,
@@ -259,6 +316,8 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
                min_depth,
                pad_to_multiple,
                conv_hyperparams_fn,
+               fpn_min_level=3,
+               fpn_max_level=7,
                reuse_weights=None,
                use_explicit_padding=False,
                use_depthwise=False,
@@ -275,6 +334,8 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
       conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
         and separable_conv2d ops in the layers that are added on top of the
         base feature extractor.
+      fpn_min_level: the minimum level in feature pyramid networks.
+      fpn_max_level: the maximum level in feature pyramid networks.
       reuse_weights: Whether to reuse variables. Default is None.
       use_explicit_padding: Whether to use explicit padding when extracting
         features. Default is False. UNUSED currently.
@@ -284,7 +345,18 @@ class SSDResnet152V1FpnFeatureExtractor(_SSDResnetV1FpnFeatureExtractor):
         `conv_hyperparams_fn`.
     """
     super(SSDResnet152V1FpnFeatureExtractor, self).__init__(
-        is_training, depth_multiplier, min_depth, pad_to_multiple,
-        conv_hyperparams_fn, resnet_v1.resnet_v1_152, 'resnet_v1_152', 'fpn',
-        reuse_weights, use_explicit_padding,
+        is_training,
+        depth_multiplier,
+        min_depth,
+        pad_to_multiple,
+        conv_hyperparams_fn,
+        resnet_v1.resnet_v1_152,
+        'resnet_v1_152',
+        'fpn',
+        fpn_min_level,
+        fpn_max_level,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
         override_base_feature_extractor_hyperparams)
diff --git a/research/object_detection/object_detection_tutorial.ipynb b/research/object_detection/object_detection_tutorial.ipynb
index b5acce97..0bc4b92e 100644
--- a/research/object_detection/object_detection_tutorial.ipynb
+++ b/research/object_detection/object_detection_tutorial.ipynb
@@ -1,343 +1,474 @@
 {
- "cells": [
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Object Detection Demo\n",
-    "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
-   ]
+  "cells": [
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "V8-yl-s-WKMG"
+      },
+      "source": [
+        "# Object Detection Demo\n",
+        "Welcome to the object detection inference walkthrough!  This notebook will walk you step by step through the process of using a pre-trained model to detect objects in an image. Make sure to follow the [installation instructions](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md) before you start."
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "kFSqkTCdWKMI"
+      },
+      "source": [
+        "# Imports"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "hV4P5gyTWKMI"
+      },
+      "outputs": [],
+      "source": [
+        "import numpy as np\n",
+        "import os\n",
+        "import six.moves.urllib as urllib\n",
+        "import sys\n",
+        "import tarfile\n",
+        "import tensorflow as tf\n",
+        "import zipfile\n",
+        "\n",
+        "from collections import defaultdict\n",
+        "from io import StringIO\n",
+        "from matplotlib import pyplot as plt\n",
+        "from PIL import Image\n",
+        "\n",
+        "# This is needed since the notebook is stored in the object_detection folder.\n",
+        "sys.path.append(\"..\")\n",
+        "from object_detection.utils import ops as utils_ops\n",
+        "\n",
+        "if tf.__version__ \u003c '1.4.0':\n",
+        "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "Wy72mWwAWKMK"
+      },
+      "source": [
+        "## Env setup"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "v7m_NY_aWKMK"
+      },
+      "outputs": [],
+      "source": [
+        "# This is needed to display the images.\n",
+        "%matplotlib inline"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "r5FNuiRPWKMN"
+      },
+      "source": [
+        "## Object detection imports\n",
+        "Here are the imports from the object detection module."
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "bm0_uNRnWKMN"
+      },
+      "outputs": [],
+      "source": [
+        "from utils import label_map_util\n",
+        "\n",
+        "from utils import visualization_utils as vis_util"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "cfn_tRFOWKMO"
+      },
+      "source": [
+        "# Model preparation "
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "X_sEBLpVWKMQ"
+      },
+      "source": [
+        "## Variables\n",
+        "\n",
+        "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.  \n",
+        "\n",
+        "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "VyPz_t8WWKMQ"
+      },
+      "outputs": [],
+      "source": [
+        "# What model to download.\n",
+        "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
+        "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
+        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
+        "\n",
+        "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
+        "PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'\n",
+        "\n",
+        "# List of the strings that is used to add correct label for each box.\n",
+        "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
+        "\n",
+        "NUM_CLASSES = 90"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "7ai8pLZZWKMS"
+      },
+      "source": [
+        "## Download Model"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "KILYnwR5WKMS"
+      },
+      "outputs": [],
+      "source": [
+        "opener = urllib.request.URLopener()\n",
+        "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
+        "tar_file = tarfile.open(MODEL_FILE)\n",
+        "for file in tar_file.getmembers():\n",
+        "  file_name = os.path.basename(file.name)\n",
+        "  if 'frozen_inference_graph.pb' in file_name:\n",
+        "    tar_file.extract(file, os.getcwd())"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "YBcB9QHLWKMU"
+      },
+      "source": [
+        "## Load a (frozen) Tensorflow model into memory."
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "KezjCRVvWKMV"
+      },
+      "outputs": [],
+      "source": [
+        "detection_graph = tf.Graph()\n",
+        "with detection_graph.as_default():\n",
+        "  od_graph_def = tf.GraphDef()\n",
+        "  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
+        "    serialized_graph = fid.read()\n",
+        "    od_graph_def.ParseFromString(serialized_graph)\n",
+        "    tf.import_graph_def(od_graph_def, name='')"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "_1MVVTcLWKMW"
+      },
+      "source": [
+        "## Loading label map\n",
+        "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "hDbpHkiWWKMX"
+      },
+      "outputs": [],
+      "source": [
+        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
+        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
+        "category_index = label_map_util.create_category_index(categories)"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "EFsoUHvbWKMZ"
+      },
+      "source": [
+        "## Helper code"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "aSlYc3JkWKMa"
+      },
+      "outputs": [],
+      "source": [
+        "def load_image_into_numpy_array(image):\n",
+        "  (im_width, im_height) = image.size\n",
+        "  return np.array(image.getdata()).reshape(\n",
+        "      (im_height, im_width, 3)).astype(np.uint8)"
+      ]
+    },
+    {
+      "cell_type": "markdown",
+      "metadata": {
+        "colab_type": "text",
+        "id": "H0_1AGhrWKMc"
+      },
+      "source": [
+        "# Detection"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "jG-zn5ykWKMd"
+      },
+      "outputs": [],
+      "source": [
+        "# For the sake of simplicity we will use only 2 images:\n",
+        "# image1.jpg\n",
+        "# image2.jpg\n",
+        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
+        "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
+        "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
+        "\n",
+        "# Size, in inches, of the output images.\n",
+        "IMAGE_SIZE = (12, 8)"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "92BHxzcNWKMf"
+      },
+      "outputs": [],
+      "source": [
+        "def run_inference_for_single_image(image, graph):\n",
+        "  with graph.as_default():\n",
+        "    with tf.Session() as sess:\n",
+        "      # Get handles to input and output tensors\n",
+        "      ops = tf.get_default_graph().get_operations()\n",
+        "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
+        "      tensor_dict = {}\n",
+        "      for key in [\n",
+        "          'num_detections', 'detection_boxes', 'detection_scores',\n",
+        "          'detection_classes', 'detection_masks'\n",
+        "      ]:\n",
+        "        tensor_name = key + ':0'\n",
+        "        if tensor_name in all_tensor_names:\n",
+        "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
+        "              tensor_name)\n",
+        "      if 'detection_masks' in tensor_dict:\n",
+        "        # The following processing is only for single image\n",
+        "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
+        "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
+        "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
+        "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
+        "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
+        "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
+        "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
+        "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
+        "        detection_masks_reframed = tf.cast(\n",
+        "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
+        "        # Follow the convention by adding back the batch dimension\n",
+        "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
+        "            detection_masks_reframed, 0)\n",
+        "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
+        "\n",
+        "      # Run inference\n",
+        "      output_dict = sess.run(tensor_dict,\n",
+        "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
+        "\n",
+        "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
+        "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
+        "      output_dict['detection_classes'] = output_dict[\n",
+        "          'detection_classes'][0].astype(np.uint8)\n",
+        "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
+        "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
+        "      if 'detection_masks' in output_dict:\n",
+        "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
+        "  return output_dict"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "3a5wMHN8WKMh"
+      },
+      "outputs": [],
+      "source": [
+        "for image_path in TEST_IMAGE_PATHS:\n",
+        "  image = Image.open(image_path)\n",
+        "  # the array based representation of the image will be used later in order to prepare the\n",
+        "  # result image with boxes and labels on it.\n",
+        "  image_np = load_image_into_numpy_array(image)\n",
+        "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
+        "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
+        "  # Actual detection.\n",
+        "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
+        "  # Visualization of the results of a detection.\n",
+        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
+        "      image_np,\n",
+        "      output_dict['detection_boxes'],\n",
+        "      output_dict['detection_classes'],\n",
+        "      output_dict['detection_scores'],\n",
+        "      category_index,\n",
+        "      instance_masks=output_dict.get('detection_masks'),\n",
+        "      use_normalized_coordinates=True,\n",
+        "      line_thickness=8)\n",
+        "  plt.figure(figsize=IMAGE_SIZE)\n",
+        "  plt.imshow(image_np)"
+      ]
+    },
+    {
+      "cell_type": "code",
+      "execution_count": 0,
+      "metadata": {
+        "colab": {
+          "autoexec": {
+            "startup": false,
+            "wait_interval": 0
+          }
+        },
+        "colab_type": "code",
+        "id": "LQSEnEsPWKMj"
+      },
+      "outputs": [],
+      "source": [
+        ""
+      ]
+    }
+  ],
+  "metadata": {
+    "colab": {
+      "default_view": {},
+      "name": "object_detection_tutorial.ipynb?workspaceId=ronnyvotel:python_inference::citc",
+      "provenance": [],
+      "version": "0.3.2",
+      "views": {}
+    },
+    "kernelspec": {
+      "display_name": "Python 2",
+      "language": "python",
+      "name": "python2"
+    }
   },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Imports"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "scrolled": true
-   },
-   "outputs": [],
-   "source": [
-    "import numpy as np\n",
-    "import os\n",
-    "import six.moves.urllib as urllib\n",
-    "import sys\n",
-    "import tarfile\n",
-    "import tensorflow as tf\n",
-    "import zipfile\n",
-    "\n",
-    "from collections import defaultdict\n",
-    "from io import StringIO\n",
-    "from matplotlib import pyplot as plt\n",
-    "from PIL import Image\n",
-    "\n",
-    "# This is needed since the notebook is stored in the object_detection folder.\n",
-    "sys.path.append(\"..\")\n",
-    "from object_detection.utils import ops as utils_ops\n",
-    "\n",
-    "if tf.__version__ < '1.4.0':\n",
-    "  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\n"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Env setup"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# This is needed to display the images.\n",
-    "%matplotlib inline"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Object detection imports\n",
-    "Here are the imports from the object detection module."
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "from utils import label_map_util\n",
-    "\n",
-    "from utils import visualization_utils as vis_util"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Model preparation "
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Variables\n",
-    "\n",
-    "Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_CKPT` to point to a new .pb file.  \n",
-    "\n",
-    "By default we use an \"SSD with Mobilenet\" model here. See the [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md) for a list of other models that can be run out-of-the-box with varying speeds and accuracies."
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# What model to download.\n",
-    "MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'\n",
-    "MODEL_FILE = MODEL_NAME + '.tar.gz'\n",
-    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
-    "\n",
-    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
-    "PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n",
-    "\n",
-    "# List of the strings that is used to add correct label for each box.\n",
-    "PATH_TO_LABELS = os.path.join('data', 'mscoco_label_map.pbtxt')\n",
-    "\n",
-    "NUM_CLASSES = 90"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Download Model"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "opener = urllib.request.URLopener()\n",
-    "opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
-    "tar_file = tarfile.open(MODEL_FILE)\n",
-    "for file in tar_file.getmembers():\n",
-    "  file_name = os.path.basename(file.name)\n",
-    "  if 'frozen_inference_graph.pb' in file_name:\n",
-    "    tar_file.extract(file, os.getcwd())"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Load a (frozen) Tensorflow model into memory."
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "detection_graph = tf.Graph()\n",
-    "with detection_graph.as_default():\n",
-    "  od_graph_def = tf.GraphDef()\n",
-    "  with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
-    "    serialized_graph = fid.read()\n",
-    "    od_graph_def.ParseFromString(serialized_graph)\n",
-    "    tf.import_graph_def(od_graph_def, name='')"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Loading label map\n",
-    "Label maps map indices to category names, so that when our convolution network predicts `5`, we know that this corresponds to `airplane`.  Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
-    "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
-    "category_index = label_map_util.create_category_index(categories)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "## Helper code"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def load_image_into_numpy_array(image):\n",
-    "  (im_width, im_height) = image.size\n",
-    "  return np.array(image.getdata()).reshape(\n",
-    "      (im_height, im_width, 3)).astype(np.uint8)"
-   ]
-  },
-  {
-   "cell_type": "markdown",
-   "metadata": {},
-   "source": [
-    "# Detection"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "# For the sake of simplicity we will use only 2 images:\n",
-    "# image1.jpg\n",
-    "# image2.jpg\n",
-    "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
-    "PATH_TO_TEST_IMAGES_DIR = 'test_images'\n",
-    "TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]\n",
-    "\n",
-    "# Size, in inches, of the output images.\n",
-    "IMAGE_SIZE = (12, 8)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": [
-    "def run_inference_for_single_image(image, graph):\n",
-    "  with graph.as_default():\n",
-    "    with tf.Session() as sess:\n",
-    "      # Get handles to input and output tensors\n",
-    "      ops = tf.get_default_graph().get_operations()\n",
-    "      all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
-    "      tensor_dict = {}\n",
-    "      for key in [\n",
-    "          'num_detections', 'detection_boxes', 'detection_scores',\n",
-    "          'detection_classes', 'detection_masks'\n",
-    "      ]:\n",
-    "        tensor_name = key + ':0'\n",
-    "        if tensor_name in all_tensor_names:\n",
-    "          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
-    "              tensor_name)\n",
-    "      if 'detection_masks' in tensor_dict:\n",
-    "        # The following processing is only for single image\n",
-    "        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
-    "        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
-    "        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
-    "        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
-    "        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
-    "        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
-    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
-    "            detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
-    "        detection_masks_reframed = tf.cast(\n",
-    "            tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
-    "        # Follow the convention by adding back the batch dimension\n",
-    "        tensor_dict['detection_masks'] = tf.expand_dims(\n",
-    "            detection_masks_reframed, 0)\n",
-    "      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
-    "\n",
-    "      # Run inference\n",
-    "      output_dict = sess.run(tensor_dict,\n",
-    "                             feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
-    "\n",
-    "      # all outputs are float32 numpy arrays, so convert types as appropriate\n",
-    "      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
-    "      output_dict['detection_classes'] = output_dict[\n",
-    "          'detection_classes'][0].astype(np.uint8)\n",
-    "      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
-    "      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
-    "      if 'detection_masks' in output_dict:\n",
-    "        output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
-    "  return output_dict"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {
-    "scrolled": true
-   },
-   "outputs": [],
-   "source": [
-    "for image_path in TEST_IMAGE_PATHS:\n",
-    "  image = Image.open(image_path)\n",
-    "  # the array based representation of the image will be used later in order to prepare the\n",
-    "  # result image with boxes and labels on it.\n",
-    "  image_np = load_image_into_numpy_array(image)\n",
-    "  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
-    "  image_np_expanded = np.expand_dims(image_np, axis=0)\n",
-    "  # Actual detection.\n",
-    "  output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
-    "  # Visualization of the results of a detection.\n",
-    "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
-    "      image_np,\n",
-    "      output_dict['detection_boxes'],\n",
-    "      output_dict['detection_classes'],\n",
-    "      output_dict['detection_scores'],\n",
-    "      category_index,\n",
-    "      instance_masks=output_dict.get('detection_masks'),\n",
-    "      use_normalized_coordinates=True,\n",
-    "      line_thickness=8)\n",
-    "  plt.figure(figsize=IMAGE_SIZE)\n",
-    "  plt.imshow(image_np)"
-   ]
-  },
-  {
-   "cell_type": "code",
-   "execution_count": null,
-   "metadata": {},
-   "outputs": [],
-   "source": []
-  }
- ],
- "metadata": {
-  "colab": {
-   "version": "0.3.2"
-  },
-  "kernelspec": {
-   "display_name": "Python 2",
-   "language": "python",
-   "name": "python2"
-  },
-  "language_info": {
-   "codemirror_mode": {
-    "name": "ipython",
-    "version": 2
-   },
-   "file_extension": ".py",
-   "mimetype": "text/x-python",
-   "name": "python",
-   "nbconvert_exporter": "python",
-   "pygments_lexer": "ipython2",
-   "version": "2.7.10"
-  }
- },
- "nbformat": 4,
- "nbformat_minor": 2
+  "nbformat": 4,
+  "nbformat_minor": 0
 }
diff --git a/research/object_detection/predictors/__init__.py b/research/object_detection/predictors/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/object_detection/predictors/convolutional_box_predictor.py b/research/object_detection/predictors/convolutional_box_predictor.py
new file mode 100644
index 00000000..d16d5584
--- /dev/null
+++ b/research/object_detection/predictors/convolutional_box_predictor.py
@@ -0,0 +1,458 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Convolutional Box Predictors with and without weight sharing."""
+import tensorflow as tf
+from object_detection.core import box_predictor
+from object_detection.utils import shape_utils
+from object_detection.utils import static_shape
+
+slim = tf.contrib.slim
+
+BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
+CLASS_PREDICTIONS_WITH_BACKGROUND = (
+    box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
+MASK_PREDICTIONS = box_predictor.MASK_PREDICTIONS
+
+
+class _NoopVariableScope(object):
+  """A dummy class that does not push any scope."""
+
+  def __enter__(self):
+    return None
+
+  def __exit__(self, exc_type, exc_value, traceback):
+    return False
+
+
+class ConvolutionalBoxPredictor(box_predictor.BoxPredictor):
+  """Convolutional Box Predictor.
+
+  Optionally add an intermediate 1x1 convolutional layer after features and
+  predict in parallel branches box_encodings and
+  class_predictions_with_background.
+
+  Currently this box predictor assumes that predictions are "shared" across
+  classes --- that is each anchor makes box predictions which do not depend
+  on class.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               conv_hyperparams_fn,
+               min_depth,
+               max_depth,
+               num_layers_before_predictor,
+               use_dropout,
+               dropout_keep_prob,
+               kernel_size,
+               box_code_size,
+               apply_sigmoid_to_scores=False,
+               class_prediction_bias_init=0.0,
+               use_depthwise=False):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for convolution ops.
+      min_depth: Minimum feature depth prior to predicting box encodings
+        and class predictions.
+      max_depth: Maximum feature depth prior to predicting box encodings
+        and class predictions. If max_depth is set to 0, no additional
+        feature map will be inserted before location and class predictions.
+      num_layers_before_predictor: Number of the additional conv layers before
+        the predictor.
+      use_dropout: Option to use dropout for class prediction or not.
+      dropout_keep_prob: Keep probability for dropout.
+        This is only used if use_dropout is True.
+      kernel_size: Size of final convolution kernel.  If the
+        spatial resolution of the feature map is smaller than the kernel size,
+        then the kernel size is automatically set to be
+        min(feature_width, feature_height).
+      box_code_size: Size of encoding for each box.
+      apply_sigmoid_to_scores: if True, apply the sigmoid on the output
+        class_predictions.
+      class_prediction_bias_init: constant value to initialize bias of the last
+        conv2d layer before class prediction.
+      use_depthwise: Whether to use depthwise convolutions for prediction
+        steps. Default is False.
+
+    Raises:
+      ValueError: if min_depth > max_depth.
+    """
+    super(ConvolutionalBoxPredictor, self).__init__(is_training, num_classes)
+    if min_depth > max_depth:
+      raise ValueError('min_depth should be less than or equal to max_depth')
+    self._conv_hyperparams_fn = conv_hyperparams_fn
+    self._min_depth = min_depth
+    self._max_depth = max_depth
+    self._num_layers_before_predictor = num_layers_before_predictor
+    self._use_dropout = use_dropout
+    self._kernel_size = kernel_size
+    self._box_code_size = box_code_size
+    self._dropout_keep_prob = dropout_keep_prob
+    self._apply_sigmoid_to_scores = apply_sigmoid_to_scores
+    self._class_prediction_bias_init = class_prediction_bias_init
+    self._use_depthwise = use_depthwise
+
+  def _predict(self, image_features, num_predictions_per_location_list):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location_list: A list of integers representing the
+        number of box predictions to be made per spatial location for each
+        feature map.
+
+    Returns:
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
+    """
+    box_encodings_list = []
+    class_predictions_list = []
+    # TODO(rathodv): Come up with a better way to generate scope names
+    # in box predictor once we have time to retrain all models in the zoo.
+    # The following lines create scope names to be backwards compatible with the
+    # existing checkpoints.
+    box_predictor_scopes = [_NoopVariableScope()]
+    if len(image_features) > 1:
+      box_predictor_scopes = [
+          tf.variable_scope('BoxPredictor_{}'.format(i))
+          for i in range(len(image_features))
+      ]
+
+    for (image_feature,
+         num_predictions_per_location, box_predictor_scope) in zip(
+             image_features, num_predictions_per_location_list,
+             box_predictor_scopes):
+      with box_predictor_scope:
+        # Add a slot for the background class.
+        num_class_slots = self.num_classes + 1
+        net = image_feature
+        with slim.arg_scope(self._conv_hyperparams_fn()), \
+             slim.arg_scope([slim.dropout], is_training=self._is_training):
+          # Add additional conv layers before the class predictor.
+          features_depth = static_shape.get_depth(image_feature.get_shape())
+          depth = max(min(features_depth, self._max_depth), self._min_depth)
+          tf.logging.info('depth of additional conv before box predictor: {}'.
+                          format(depth))
+          if depth > 0 and self._num_layers_before_predictor > 0:
+            for i in range(self._num_layers_before_predictor):
+              net = slim.conv2d(
+                  net, depth, [1, 1], scope='Conv2d_%d_1x1_%d' % (i, depth))
+          with slim.arg_scope([slim.conv2d], activation_fn=None,
+                              normalizer_fn=None, normalizer_params=None):
+            if self._use_depthwise:
+              box_encodings = slim.separable_conv2d(
+                  net, None, [self._kernel_size, self._kernel_size],
+                  padding='SAME', depth_multiplier=1, stride=1,
+                  rate=1, scope='BoxEncodingPredictor_depthwise')
+              box_encodings = slim.conv2d(
+                  box_encodings,
+                  num_predictions_per_location * self._box_code_size, [1, 1],
+                  scope='BoxEncodingPredictor')
+            else:
+              box_encodings = slim.conv2d(
+                  net, num_predictions_per_location * self._box_code_size,
+                  [self._kernel_size, self._kernel_size],
+                  scope='BoxEncodingPredictor')
+            if self._use_dropout:
+              net = slim.dropout(net, keep_prob=self._dropout_keep_prob)
+            if self._use_depthwise:
+              class_predictions_with_background = slim.separable_conv2d(
+                  net, None, [self._kernel_size, self._kernel_size],
+                  padding='SAME', depth_multiplier=1, stride=1,
+                  rate=1, scope='ClassPredictor_depthwise')
+              class_predictions_with_background = slim.conv2d(
+                  class_predictions_with_background,
+                  num_predictions_per_location * num_class_slots,
+                  [1, 1], scope='ClassPredictor')
+            else:
+              class_predictions_with_background = slim.conv2d(
+                  net, num_predictions_per_location * num_class_slots,
+                  [self._kernel_size, self._kernel_size],
+                  scope='ClassPredictor',
+                  biases_initializer=tf.constant_initializer(
+                      self._class_prediction_bias_init))
+            if self._apply_sigmoid_to_scores:
+              class_predictions_with_background = tf.sigmoid(
+                  class_predictions_with_background)
+
+        combined_feature_map_shape = (shape_utils.
+                                      combined_static_and_dynamic_shape(
+                                          image_feature))
+        box_encodings = tf.reshape(
+            box_encodings, tf.stack([combined_feature_map_shape[0],
+                                     combined_feature_map_shape[1] *
+                                     combined_feature_map_shape[2] *
+                                     num_predictions_per_location,
+                                     1, self._box_code_size]))
+        box_encodings_list.append(box_encodings)
+        class_predictions_with_background = tf.reshape(
+            class_predictions_with_background,
+            tf.stack([combined_feature_map_shape[0],
+                      combined_feature_map_shape[1] *
+                      combined_feature_map_shape[2] *
+                      num_predictions_per_location,
+                      num_class_slots]))
+        class_predictions_list.append(class_predictions_with_background)
+    return {
+        BOX_ENCODINGS: box_encodings_list,
+        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
+    }
+
+
+# TODO(rathodv): Replace with slim.arg_scope_func_key once its available
+# externally.
+def _arg_scope_func_key(op):
+  """Returns a key that can be used to index arg_scope dictionary."""
+  return getattr(op, '_key_op', str(op))
+
+
+# TODO(rathodv): Merge the implementation with ConvolutionalBoxPredictor above
+# since they are very similar.
+class WeightSharedConvolutionalBoxPredictor(box_predictor.BoxPredictor):
+  """Convolutional Box Predictor with weight sharing.
+
+  Defines the box predictor as defined in
+  https://arxiv.org/abs/1708.02002. This class differs from
+  ConvolutionalBoxPredictor in that it shares weights and biases while
+  predicting from different feature maps. However, batch_norm parameters are not
+  shared because the statistics of the activations vary among the different
+  feature maps.
+
+  Also note that separate multi-layer towers are constructed for the box
+  encoding and class predictors respectively.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               conv_hyperparams_fn,
+               depth,
+               num_layers_before_predictor,
+               box_code_size,
+               kernel_size=3,
+               class_prediction_bias_init=0.0,
+               use_dropout=False,
+               dropout_keep_prob=0.8,
+               share_prediction_tower=False,
+               apply_batch_norm=True):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for convolution ops.
+      depth: depth of conv layers.
+      num_layers_before_predictor: Number of the additional conv layers before
+        the predictor.
+      box_code_size: Size of encoding for each box.
+      kernel_size: Size of final convolution kernel.
+      class_prediction_bias_init: constant value to initialize bias of the last
+        conv2d layer before class prediction.
+      use_dropout: Whether to apply dropout to class prediction head.
+      dropout_keep_prob: Probability of keeping activiations.
+      share_prediction_tower: Whether to share the multi-layer tower between box
+        prediction and class prediction heads.
+      apply_batch_norm: Whether to apply batch normalization to conv layers in
+        this predictor.
+    """
+    super(WeightSharedConvolutionalBoxPredictor, self).__init__(is_training,
+                                                                num_classes)
+    self._conv_hyperparams_fn = conv_hyperparams_fn
+    self._depth = depth
+    self._num_layers_before_predictor = num_layers_before_predictor
+    self._box_code_size = box_code_size
+    self._kernel_size = kernel_size
+    self._class_prediction_bias_init = class_prediction_bias_init
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+    self._share_prediction_tower = share_prediction_tower
+    self._apply_batch_norm = apply_batch_norm
+
+  def _predict(self, image_features, num_predictions_per_location_list):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+        width_i, channels] containing features for a batch of images. Note that
+        when not all tensors in the list have the same number of channels, an
+        additional projection layer will be added on top the tensor to generate
+        feature map with number of channels consitent with the majority.
+      num_predictions_per_location_list: A list of integers representing the
+        number of box predictions to be made per spatial location for each
+        feature map. Note that all values must be the same since the weights are
+        shared.
+
+    Returns:
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, code_size] representing the location of
+        the objects. Each entry in the list corresponds to a feature map in the
+        input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
+
+
+    Raises:
+      ValueError: If the image feature maps do not have the same number of
+        channels or if the num predictions per locations is differs between the
+        feature maps.
+    """
+    if len(set(num_predictions_per_location_list)) > 1:
+      raise ValueError('num predictions per location must be same for all'
+                       'feature maps, found: {}'.format(
+                           num_predictions_per_location_list))
+    feature_channels = [
+        image_feature.shape[3].value for image_feature in image_features
+    ]
+    has_different_feature_channels = len(set(feature_channels)) > 1
+    if has_different_feature_channels:
+      inserted_layer_counter = 0
+      target_channel = max(set(feature_channels), key=feature_channels.count)
+      tf.logging.info('Not all feature maps have the same number of '
+                      'channels, found: {}, addition project layers '
+                      'to bring all feature maps to uniform channels '
+                      'of {}'.format(feature_channels, target_channel))
+    box_encodings_list = []
+    class_predictions_list = []
+    num_class_slots = self.num_classes + 1
+    for feature_index, (image_feature,
+                        num_predictions_per_location) in enumerate(
+                            zip(image_features,
+                                num_predictions_per_location_list)):
+      # Add a slot for the background class.
+      with tf.variable_scope('WeightSharedConvolutionalBoxPredictor',
+                             reuse=tf.AUTO_REUSE):
+        with slim.arg_scope(self._conv_hyperparams_fn()):
+          # Insert an additional projection layer if necessary.
+          if (has_different_feature_channels and
+              image_feature.shape[3].value != target_channel):
+            image_feature = slim.conv2d(
+                image_feature,
+                target_channel, [1, 1],
+                stride=1,
+                padding='SAME',
+                activation_fn=None,
+                normalizer_fn=(tf.identity if self._apply_batch_norm else None),
+                scope='ProjectionLayer/conv2d_{}'.format(
+                    inserted_layer_counter))
+            if self._apply_batch_norm:
+              image_feature = slim.batch_norm(
+                  image_feature,
+                  scope='ProjectionLayer/conv2d_{}/BatchNorm'.format(
+                      inserted_layer_counter))
+            inserted_layer_counter += 1
+          box_encodings_net = image_feature
+          class_predictions_net = image_feature
+          for i in range(self._num_layers_before_predictor):
+            box_prediction_tower_prefix = (
+                'PredictionTower' if self._share_prediction_tower
+                else 'BoxPredictionTower')
+            box_encodings_net = slim.conv2d(
+                box_encodings_net,
+                self._depth, [self._kernel_size, self._kernel_size],
+                stride=1,
+                padding='SAME',
+                activation_fn=None,
+                normalizer_fn=(tf.identity if self._apply_batch_norm else None),
+                scope='{}/conv2d_{}'.format(box_prediction_tower_prefix, i))
+            if self._apply_batch_norm:
+              box_encodings_net = slim.batch_norm(
+                  box_encodings_net,
+                  scope='{}/conv2d_{}/BatchNorm/feature_{}'.
+                  format(box_prediction_tower_prefix, i, feature_index))
+            box_encodings_net = tf.nn.relu6(box_encodings_net)
+          box_encodings = slim.conv2d(
+              box_encodings_net,
+              num_predictions_per_location * self._box_code_size,
+              [self._kernel_size, self._kernel_size],
+              activation_fn=None, stride=1, padding='SAME',
+              normalizer_fn=None,
+              scope='BoxPredictor')
+
+          if self._share_prediction_tower:
+            class_predictions_net = box_encodings_net
+          else:
+            for i in range(self._num_layers_before_predictor):
+              class_predictions_net = slim.conv2d(
+                  class_predictions_net,
+                  self._depth, [self._kernel_size, self._kernel_size],
+                  stride=1,
+                  padding='SAME',
+                  activation_fn=None,
+                  normalizer_fn=(tf.identity
+                                 if self._apply_batch_norm else None),
+                  scope='ClassPredictionTower/conv2d_{}'.format(i))
+              if self._apply_batch_norm:
+                class_predictions_net = slim.batch_norm(
+                    class_predictions_net,
+                    scope='ClassPredictionTower/conv2d_{}/BatchNorm/feature_{}'
+                    .format(i, feature_index))
+              class_predictions_net = tf.nn.relu6(class_predictions_net)
+          if self._use_dropout:
+            class_predictions_net = slim.dropout(
+                class_predictions_net, keep_prob=self._dropout_keep_prob)
+          class_predictions_with_background = slim.conv2d(
+              class_predictions_net,
+              num_predictions_per_location * num_class_slots,
+              [self._kernel_size, self._kernel_size],
+              activation_fn=None, stride=1, padding='SAME',
+              normalizer_fn=None,
+              biases_initializer=tf.constant_initializer(
+                  self._class_prediction_bias_init),
+              scope='ClassPredictor')
+
+          combined_feature_map_shape = (shape_utils.
+                                        combined_static_and_dynamic_shape(
+                                            image_feature))
+          box_encodings = tf.reshape(
+              box_encodings, tf.stack([combined_feature_map_shape[0],
+                                       combined_feature_map_shape[1] *
+                                       combined_feature_map_shape[2] *
+                                       num_predictions_per_location,
+                                       self._box_code_size]))
+          box_encodings_list.append(box_encodings)
+          class_predictions_with_background = tf.reshape(
+              class_predictions_with_background,
+              tf.stack([combined_feature_map_shape[0],
+                        combined_feature_map_shape[1] *
+                        combined_feature_map_shape[2] *
+                        num_predictions_per_location,
+                        num_class_slots]))
+          class_predictions_list.append(class_predictions_with_background)
+    return {
+        BOX_ENCODINGS: box_encodings_list,
+        CLASS_PREDICTIONS_WITH_BACKGROUND: class_predictions_list
+    }
diff --git a/research/object_detection/core/box_predictor_test.py b/research/object_detection/predictors/convolutional_box_predictor_test.py
similarity index 78%
rename from research/object_detection/core/box_predictor_test.py
rename to research/object_detection/predictors/convolutional_box_predictor_test.py
index a5565620..16939938 100644
--- a/research/object_detection/core/box_predictor_test.py
+++ b/research/object_detection/predictors/convolutional_box_predictor_test.py
@@ -13,202 +13,17 @@
 # limitations under the License.
 # ==============================================================================
 
-"""Tests for object_detection.core.box_predictor."""
+"""Tests for object_detection.predictors.convolutional_box_predictor."""
 import numpy as np
 import tensorflow as tf
 
 from google.protobuf import text_format
 from object_detection.builders import hyperparams_builder
-from object_detection.core import box_predictor
+from object_detection.predictors import convolutional_box_predictor as box_predictor
 from object_detection.protos import hyperparams_pb2
 from object_detection.utils import test_case
 
 
-class MaskRCNNBoxPredictorTest(tf.test.TestCase):
-
-  def _build_arg_scope_with_hyperparams(self,
-                                        op_type=hyperparams_pb2.Hyperparams.FC):
-    hyperparams = hyperparams_pb2.Hyperparams()
-    hyperparams_text_proto = """
-      activation: NONE
-      regularizer {
-        l2_regularizer {
-        }
-      }
-      initializer {
-        truncated_normal_initializer {
-        }
-      }
-    """
-    text_format.Merge(hyperparams_text_proto, hyperparams)
-    hyperparams.op = op_type
-    return hyperparams_builder.build(hyperparams, is_training=True)
-
-  def test_get_boxes_with_five_classes(self):
-    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
-    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(
-        is_training=False,
-        num_classes=5,
-        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        use_dropout=False,
-        dropout_keep_prob=0.5,
-        box_code_size=4,
-    )
-    box_predictions = mask_box_predictor.predict(
-        [image_features], num_predictions_per_location=[1],
-        scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    class_predictions_with_background = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      (box_encodings_shape,
-       class_predictions_with_background_shape) = sess.run(
-           [tf.shape(box_encodings),
-            tf.shape(class_predictions_with_background)])
-      self.assertAllEqual(box_encodings_shape, [2, 1, 5, 4])
-      self.assertAllEqual(class_predictions_with_background_shape, [2, 1, 6])
-
-  def test_get_boxes_with_five_classes_share_box_across_classes(self):
-    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
-    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(
-        is_training=False,
-        num_classes=5,
-        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        use_dropout=False,
-        dropout_keep_prob=0.5,
-        box_code_size=4,
-        share_box_across_classes=True
-    )
-    box_predictions = mask_box_predictor.predict(
-        [image_features], num_predictions_per_location=[1],
-        scope='BoxPredictor')
-    box_encodings = box_predictions[box_predictor.BOX_ENCODINGS]
-    class_predictions_with_background = box_predictions[
-        box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND]
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      (box_encodings_shape,
-       class_predictions_with_background_shape) = sess.run(
-           [tf.shape(box_encodings),
-            tf.shape(class_predictions_with_background)])
-      self.assertAllEqual(box_encodings_shape, [2, 1, 1, 4])
-      self.assertAllEqual(class_predictions_with_background_shape, [2, 1, 6])
-
-  def test_value_error_on_predict_instance_masks_with_no_conv_hyperparms(self):
-    with self.assertRaises(ValueError):
-      box_predictor.MaskRCNNBoxPredictor(
-          is_training=False,
-          num_classes=5,
-          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-          use_dropout=False,
-          dropout_keep_prob=0.5,
-          box_code_size=4,
-          predict_instance_masks=True)
-
-  def test_get_instance_masks(self):
-    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
-    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(
-        is_training=False,
-        num_classes=5,
-        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        use_dropout=False,
-        dropout_keep_prob=0.5,
-        box_code_size=4,
-        conv_hyperparams_fn=self._build_arg_scope_with_hyperparams(
-            op_type=hyperparams_pb2.Hyperparams.CONV),
-        predict_instance_masks=True)
-    box_predictions = mask_box_predictor.predict(
-        [image_features],
-        num_predictions_per_location=[1],
-        scope='BoxPredictor',
-        predict_boxes_and_classes=True,
-        predict_auxiliary_outputs=True)
-    mask_predictions = box_predictions[box_predictor.MASK_PREDICTIONS]
-    self.assertListEqual([2, 1, 5, 14, 14],
-                         mask_predictions.get_shape().as_list())
-
-  def test_do_not_return_instance_masks_without_request(self):
-    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
-    mask_box_predictor = box_predictor.MaskRCNNBoxPredictor(
-        is_training=False,
-        num_classes=5,
-        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-        use_dropout=False,
-        dropout_keep_prob=0.5,
-        box_code_size=4)
-    box_predictions = mask_box_predictor.predict(
-        [image_features], num_predictions_per_location=[1],
-        scope='BoxPredictor')
-    self.assertEqual(len(box_predictions), 2)
-    self.assertTrue(box_predictor.BOX_ENCODINGS in box_predictions)
-    self.assertTrue(box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND
-                    in box_predictions)
-
-  def test_value_error_on_predict_keypoints(self):
-    with self.assertRaises(ValueError):
-      box_predictor.MaskRCNNBoxPredictor(
-          is_training=False,
-          num_classes=5,
-          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
-          use_dropout=False,
-          dropout_keep_prob=0.5,
-          box_code_size=4,
-          predict_keypoints=True)
-
-
-class RfcnBoxPredictorTest(tf.test.TestCase):
-
-  def _build_arg_scope_with_conv_hyperparams(self):
-    conv_hyperparams = hyperparams_pb2.Hyperparams()
-    conv_hyperparams_text_proto = """
-      regularizer {
-        l2_regularizer {
-        }
-      }
-      initializer {
-        truncated_normal_initializer {
-        }
-      }
-    """
-    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
-    return hyperparams_builder.build(conv_hyperparams, is_training=True)
-
-  def test_get_correct_box_encoding_and_class_prediction_shapes(self):
-    image_features = tf.random_uniform([4, 8, 8, 64], dtype=tf.float32)
-    proposal_boxes = tf.random_normal([4, 2, 4], dtype=tf.float32)
-    rfcn_box_predictor = box_predictor.RfcnBoxPredictor(
-        is_training=False,
-        num_classes=2,
-        conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
-        num_spatial_bins=[3, 3],
-        depth=4,
-        crop_size=[12, 12],
-        box_code_size=4
-    )
-    box_predictions = rfcn_box_predictor.predict(
-        [image_features], num_predictions_per_location=[1],
-        scope='BoxPredictor',
-        proposal_boxes=proposal_boxes)
-    box_encodings = tf.concat(
-        box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
-    class_predictions_with_background = tf.concat(
-        box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
-        axis=1)
-
-    init_op = tf.global_variables_initializer()
-    with self.test_session() as sess:
-      sess.run(init_op)
-      (box_encodings_shape,
-       class_predictions_shape) = sess.run(
-           [tf.shape(box_encodings),
-            tf.shape(class_predictions_with_background)])
-      self.assertAllEqual(box_encodings_shape, [8, 1, 2, 4])
-      self.assertAllEqual(class_predictions_shape, [8, 1, 3])
-
-
 class ConvolutionalBoxPredictorTest(test_case.TestCase):
 
   def _build_arg_scope_with_conv_hyperparams(self):
@@ -597,7 +412,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
     self.assertAllEqual(class_predictions_with_background.shape,
                         [4, 960, num_classes_without_background+1])
 
-  def test_predictions_from_multiple_feature_maps_share_weights_not_batchnorm(
+  def test_predictions_multiple_feature_maps_share_weights_separate_batchnorm(
       self):
     num_classes_without_background = 6
     def graph_fn(image_features1, image_features2):
@@ -663,6 +478,65 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
          'ClassPredictor/biases')])
     self.assertEqual(expected_variable_set, actual_variable_set)
 
+  def test_predictions_multiple_feature_maps_share_weights_without_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=2,
+          box_code_size=4,
+          apply_batch_norm=False)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Box prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictionTower/conv2d_1/biases'),
+        # Box prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/biases'),
+        # Class prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictionTower/conv2d_1/biases'),
+        # Class prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/biases')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
   def test_no_batchnorm_params_when_batchnorm_is_not_configured(self):
     num_classes_without_background = 6
     def graph_fn(image_features1, image_features2):
@@ -672,7 +546,8 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
           conv_hyperparams_fn=self._build_conv_arg_scope_no_batch_norm(),
           depth=32,
           num_layers_before_predictor=2,
-          box_code_size=4)
+          box_code_size=4,
+          apply_batch_norm=False)
       box_predictions = conv_box_predictor.predict(
           [image_features1, image_features2],
           num_predictions_per_location=[5, 5],
@@ -720,7 +595,7 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
          'ClassPredictor/biases')])
     self.assertEqual(expected_variable_set, actual_variable_set)
 
-  def test_predictions_share_weights_share_tower_not_batchnorm(
+  def test_predictions_share_weights_share_tower_separate_batchnorm(
       self):
     num_classes_without_background = 6
     def graph_fn(image_features1, image_features2):
@@ -774,6 +649,57 @@ class WeightSharedConvolutionalBoxPredictorTest(test_case.TestCase):
          'ClassPredictor/biases')])
     self.assertEqual(expected_variable_set, actual_variable_set)
 
+  def test_predictions_share_weights_share_tower_without_batchnorm(
+      self):
+    num_classes_without_background = 6
+    def graph_fn(image_features1, image_features2):
+      conv_box_predictor = box_predictor.WeightSharedConvolutionalBoxPredictor(
+          is_training=False,
+          num_classes=num_classes_without_background,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          depth=32,
+          num_layers_before_predictor=2,
+          box_code_size=4,
+          share_prediction_tower=True,
+          apply_batch_norm=False)
+      box_predictions = conv_box_predictor.predict(
+          [image_features1, image_features2],
+          num_predictions_per_location=[5, 5],
+          scope='BoxPredictor')
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    with self.test_session(graph=tf.Graph()):
+      graph_fn(tf.random_uniform([4, 32, 32, 3], dtype=tf.float32),
+               tf.random_uniform([4, 16, 16, 3], dtype=tf.float32))
+      actual_variable_set = set(
+          [var.op.name for var in tf.trainable_variables()])
+    expected_variable_set = set([
+        # Shared prediction tower
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_0/biases'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'PredictionTower/conv2d_1/biases'),
+        # Box prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'BoxPredictor/biases'),
+        # Class prediction head
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/weights'),
+        ('BoxPredictor/WeightSharedConvolutionalBoxPredictor/'
+         'ClassPredictor/biases')])
+    self.assertEqual(expected_variable_set, actual_variable_set)
+
   def test_get_predictions_with_feature_maps_of_dynamic_shape(
       self):
     image_features = tf.placeholder(dtype=tf.float32, shape=[4, None, None, 64])
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor.py b/research/object_detection/predictors/mask_rcnn_box_predictor.py
new file mode 100644
index 00000000..e89ac62f
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor.py
@@ -0,0 +1,141 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Box Predictor."""
+import tensorflow as tf
+
+from object_detection.core import box_predictor
+
+slim = tf.contrib.slim
+
+BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
+CLASS_PREDICTIONS_WITH_BACKGROUND = (
+    box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
+MASK_PREDICTIONS = box_predictor.MASK_PREDICTIONS
+
+
+class MaskRCNNBoxPredictor(box_predictor.BoxPredictor):
+  """Mask R-CNN Box Predictor.
+
+  See Mask R-CNN: He, K., Gkioxari, G., Dollar, P., & Girshick, R. (2017).
+  Mask R-CNN. arXiv preprint arXiv:1703.06870.
+
+  This is used for the second stage of the Mask R-CNN detector where proposals
+  cropped from an image are arranged along the batch dimension of the input
+  image_features tensor. Notice that locations are *not* shared across classes,
+  thus for each anchor, a separate prediction is made for each class.
+
+  In addition to predicting boxes and classes, optionally this class allows
+  predicting masks and/or keypoints inside detection boxes.
+
+  Currently this box predictor makes per-class predictions; that is, each
+  anchor makes a separate box prediction for each class.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               box_prediction_head,
+               class_prediction_head,
+               third_stage_heads):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      box_prediction_head: The head that predicts the boxes in second stage.
+      class_prediction_head: The head that predicts the classes in second stage.
+      third_stage_heads: A dictionary mapping head names to mask rcnn head
+        classes.
+    """
+    super(MaskRCNNBoxPredictor, self).__init__(is_training, num_classes)
+    self._box_prediction_head = box_prediction_head
+    self._class_prediction_head = class_prediction_head
+    self._third_stage_heads = third_stage_heads
+
+  @property
+  def num_classes(self):
+    return self._num_classes
+
+  def get_second_stage_prediction_heads(self):
+    return BOX_ENCODINGS, CLASS_PREDICTIONS_WITH_BACKGROUND
+
+  def get_third_stage_prediction_heads(self):
+    return sorted(self._third_stage_heads.keys())
+
+  def _predict(self,
+               image_features,
+               num_predictions_per_location,
+               prediction_stage=2):
+    """Optionally computes encoded object locations, confidences, and masks.
+
+    Predicts the heads belonging to the given prediction stage.
+
+    Args:
+      image_features: A list of float tensors of shape
+        [batch_size, height_i, width_i, channels_i] containing roi pooled
+        features for each image. The length of the list should be 1 otherwise
+        a ValueError will be raised.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
+        Currently, this must be set to [1], or an error will be raised.
+      prediction_stage: Prediction stage. Acceptable values are 2 and 3.
+
+    Returns:
+      A dictionary containing the predicted tensors that are listed in
+      self._prediction_heads. A subset of the following keys will exist in the
+      dictionary:
+        BOX_ENCODINGS: A float tensor of shape
+          [batch_size, 1, num_classes, code_size] representing the
+          location of the objects.
+        CLASS_PREDICTIONS_WITH_BACKGROUND: A float tensor of shape
+          [batch_size, 1, num_classes + 1] representing the class
+          predictions for the proposals.
+        MASK_PREDICTIONS: A float tensor of shape
+          [batch_size, 1, num_classes, image_height, image_width]
+
+    Raises:
+      ValueError: If num_predictions_per_location is not 1 or if
+        len(image_features) is not 1.
+      ValueError: if prediction_stage is not 2 or 3.
+    """
+    if (len(num_predictions_per_location) != 1 or
+        num_predictions_per_location[0] != 1):
+      raise ValueError('Currently FullyConnectedBoxPredictor only supports '
+                       'predicting a single box per class per location.')
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.format(
+          len(image_features)))
+    image_feature = image_features[0]
+    predictions_dict = {}
+
+    if prediction_stage == 2:
+      predictions_dict[BOX_ENCODINGS] = self._box_prediction_head.predict(
+          roi_pooled_features=image_feature)
+      predictions_dict[CLASS_PREDICTIONS_WITH_BACKGROUND] = (
+          self._class_prediction_head.predict(roi_pooled_features=image_feature)
+      )
+    elif prediction_stage == 3:
+      for prediction_head in self.get_third_stage_prediction_heads():
+        head_object = self._third_stage_heads[prediction_head]
+        predictions_dict[prediction_head] = head_object.predict(
+            roi_pooled_features=image_feature)
+    else:
+      raise ValueError('prediction_stage should be either 2 or 3.')
+
+    return predictions_dict
diff --git a/research/object_detection/predictors/mask_rcnn_box_predictor_test.py b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
new file mode 100644
index 00000000..965e94aa
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_box_predictor_test.py
@@ -0,0 +1,189 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_box_predictor."""
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors import mask_rcnn_box_predictor as box_predictor
+from object_detection.predictors.mask_rcnn_heads import box_head
+from object_detection.predictors.mask_rcnn_heads import class_head
+from object_detection.predictors.mask_rcnn_heads import mask_head
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class MaskRCNNBoxPredictorTest(test_case.TestCase):
+
+  def _build_arg_scope_with_hyperparams(self,
+                                        op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.build(hyperparams, is_training=True)
+
+  def _box_predictor_builder(self,
+                             is_training,
+                             num_classes,
+                             fc_hyperparams_fn,
+                             use_dropout,
+                             dropout_keep_prob,
+                             box_code_size,
+                             share_box_across_classes=False,
+                             conv_hyperparams_fn=None,
+                             predict_instance_masks=False):
+    box_prediction_head = box_head.BoxHead(
+        is_training=is_training,
+        num_classes=num_classes,
+        fc_hyperparams_fn=fc_hyperparams_fn,
+        use_dropout=use_dropout,
+        dropout_keep_prob=dropout_keep_prob,
+        box_code_size=box_code_size,
+        share_box_across_classes=share_box_across_classes)
+    class_prediction_head = class_head.ClassHead(
+        is_training=is_training,
+        num_classes=num_classes,
+        fc_hyperparams_fn=fc_hyperparams_fn,
+        use_dropout=use_dropout,
+        dropout_keep_prob=dropout_keep_prob)
+    third_stage_heads = {}
+    if predict_instance_masks:
+      third_stage_heads[box_predictor.MASK_PREDICTIONS] = mask_head.MaskHead(
+          num_classes=num_classes,
+          conv_hyperparams_fn=conv_hyperparams_fn)
+    return box_predictor.MaskRCNNBoxPredictor(
+        is_training=is_training,
+        num_classes=num_classes,
+        box_prediction_head=box_prediction_head,
+        class_prediction_head=class_prediction_head,
+        third_stage_heads=third_stage_heads)
+
+  def test_get_boxes_with_five_classes(self):
+    def graph_fn(image_features):
+      mask_box_predictor = self._box_predictor_builder(
+          is_training=False,
+          num_classes=5,
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+          use_dropout=False,
+          dropout_keep_prob=0.5,
+          box_code_size=4,
+      )
+      box_predictions = mask_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[1],
+          scope='BoxPredictor',
+          prediction_stage=2)
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND])
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    (box_encodings,
+     class_predictions_with_background) = self.execute(graph_fn,
+                                                       [image_features])
+    self.assertAllEqual(box_encodings.shape, [2, 1, 5, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [2, 1, 6])
+
+  def test_get_boxes_with_five_classes_share_box_across_classes(self):
+    def graph_fn(image_features):
+      mask_box_predictor = self._box_predictor_builder(
+          is_training=False,
+          num_classes=5,
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+          use_dropout=False,
+          dropout_keep_prob=0.5,
+          box_code_size=4,
+          share_box_across_classes=True
+      )
+      box_predictions = mask_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[1],
+          scope='BoxPredictor',
+          prediction_stage=2)
+      return (box_predictions[box_predictor.BOX_ENCODINGS],
+              box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND])
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    (box_encodings,
+     class_predictions_with_background) = self.execute(graph_fn,
+                                                       [image_features])
+    self.assertAllEqual(box_encodings.shape, [2, 1, 1, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [2, 1, 6])
+
+  def test_value_error_on_predict_instance_masks_with_no_conv_hyperparms(self):
+    with self.assertRaises(ValueError):
+      self._box_predictor_builder(
+          is_training=False,
+          num_classes=5,
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+          use_dropout=False,
+          dropout_keep_prob=0.5,
+          box_code_size=4,
+          predict_instance_masks=True)
+
+  def test_get_instance_masks(self):
+    def graph_fn(image_features):
+      mask_box_predictor = self._box_predictor_builder(
+          is_training=False,
+          num_classes=5,
+          fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+          use_dropout=False,
+          dropout_keep_prob=0.5,
+          box_code_size=4,
+          conv_hyperparams_fn=self._build_arg_scope_with_hyperparams(
+              op_type=hyperparams_pb2.Hyperparams.CONV),
+          predict_instance_masks=True)
+      box_predictions = mask_box_predictor.predict(
+          [image_features],
+          num_predictions_per_location=[1],
+          scope='BoxPredictor',
+          prediction_stage=3)
+      return (box_predictions[box_predictor.MASK_PREDICTIONS],)
+    image_features = np.random.rand(2, 7, 7, 3).astype(np.float32)
+    mask_predictions = self.execute(graph_fn, [image_features])
+    self.assertAllEqual(mask_predictions.shape, [2, 1, 5, 14, 14])
+
+  def test_do_not_return_instance_masks_without_request(self):
+    image_features = tf.random_uniform([2, 7, 7, 3], dtype=tf.float32)
+    mask_box_predictor = self._box_predictor_builder(
+        is_training=False,
+        num_classes=5,
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+        use_dropout=False,
+        dropout_keep_prob=0.5,
+        box_code_size=4)
+    box_predictions = mask_box_predictor.predict(
+        [image_features],
+        num_predictions_per_location=[1],
+        scope='BoxPredictor',
+        prediction_stage=2)
+    self.assertEqual(len(box_predictions), 2)
+    self.assertTrue(box_predictor.BOX_ENCODINGS in box_predictions)
+    self.assertTrue(box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND
+                    in box_predictions)
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_heads/__init__.py b/research/object_detection/predictors/mask_rcnn_heads/__init__.py
new file mode 100644
index 00000000..e69de29b
diff --git a/research/object_detection/predictors/mask_rcnn_heads/box_head.py b/research/object_detection/predictors/mask_rcnn_heads/box_head.py
new file mode 100644
index 00000000..48eeb51a
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/box_head.py
@@ -0,0 +1,96 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Box Head."""
+import tensorflow as tf
+
+from object_detection.predictors.mask_rcnn_heads import mask_rcnn_head
+
+slim = tf.contrib.slim
+
+
+class BoxHead(mask_rcnn_head.MaskRCNNHead):
+  """Mask RCNN box prediction head."""
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               fc_hyperparams_fn,
+               use_dropout,
+               dropout_keep_prob,
+               box_code_size,
+               share_box_across_classes=False):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      fc_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for fully connected ops.
+      use_dropout: Option to use dropout or not.  Note that a single dropout
+        op is applied here prior to both box and class predictions, which stands
+        in contrast to the ConvolutionalBoxPredictor below.
+      dropout_keep_prob: Keep probability for dropout.
+        This is only used if use_dropout is True.
+      box_code_size: Size of encoding for each box.
+      share_box_across_classes: Whether to share boxes across classes rather
+        than use a different box for each class.
+    """
+    super(BoxHead, self).__init__()
+    self._is_training = is_training
+    self._num_classes = num_classes
+    self._fc_hyperparams_fn = fc_hyperparams_fn
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+    self._box_code_size = box_code_size
+    self._share_box_across_classes = share_box_across_classes
+
+  def _predict(self, roi_pooled_features):
+    """Predicts boxes.
+
+    Args:
+      roi_pooled_features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      box_encodings: A float tensor of shape
+        [batch_size, 1, num_classes, code_size] representing the location of the
+        objects.
+    """
+    spatial_averaged_roi_pooled_features = tf.reduce_mean(
+        roi_pooled_features, [1, 2], keep_dims=True, name='AvgPool')
+    flattened_roi_pooled_features = slim.flatten(
+        spatial_averaged_roi_pooled_features)
+    if self._use_dropout:
+      flattened_roi_pooled_features = slim.dropout(
+          flattened_roi_pooled_features,
+          keep_prob=self._dropout_keep_prob,
+          is_training=self._is_training)
+    number_of_boxes = 1
+    if not self._share_box_across_classes:
+      number_of_boxes = self._num_classes
+
+    with slim.arg_scope(self._fc_hyperparams_fn()):
+      box_encodings = slim.fully_connected(
+          flattened_roi_pooled_features,
+          number_of_boxes * self._box_code_size,
+          activation_fn=None,
+          scope='BoxEncodingPredictor')
+    box_encodings = tf.reshape(box_encodings,
+                               [-1, 1, number_of_boxes, self._box_code_size])
+    return box_encodings
diff --git a/research/object_detection/predictors/mask_rcnn_heads/box_head_test.py b/research/object_detection/predictors/mask_rcnn_heads/box_head_test.py
new file mode 100644
index 00000000..01f6a3f8
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/box_head_test.py
@@ -0,0 +1,64 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_heads.box_head."""
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors.mask_rcnn_heads import box_head
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class BoxHeadTest(test_case.TestCase):
+
+  def _build_arg_scope_with_hyperparams(self,
+                                        op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.build(hyperparams, is_training=True)
+
+  def test_prediction_size(self):
+    box_prediction_head = box_head.BoxHead(
+        is_training=False,
+        num_classes=20,
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+        use_dropout=True,
+        dropout_keep_prob=0.5,
+        box_code_size=4,
+        share_box_across_classes=False)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = box_prediction_head.predict(
+        roi_pooled_features=roi_pooled_features)
+    tf.logging.info(prediction.shape)
+    self.assertAllEqual([64, 1, 20, 4], prediction.get_shape().as_list())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_heads/class_head.py b/research/object_detection/predictors/mask_rcnn_heads/class_head.py
new file mode 100644
index 00000000..64759bab
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/class_head.py
@@ -0,0 +1,82 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Class Head."""
+import tensorflow as tf
+
+from object_detection.predictors.mask_rcnn_heads import mask_rcnn_head
+
+slim = tf.contrib.slim
+
+
+class ClassHead(mask_rcnn_head.MaskRCNNHead):
+  """Mask RCNN class prediction head."""
+
+  def __init__(self, is_training, num_classes, fc_hyperparams_fn,
+               use_dropout, dropout_keep_prob):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      fc_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for fully connected ops.
+      use_dropout: Option to use dropout or not.  Note that a single dropout
+        op is applied here prior to both box and class predictions, which stands
+        in contrast to the ConvolutionalBoxPredictor below.
+      dropout_keep_prob: Keep probability for dropout.
+        This is only used if use_dropout is True.
+    """
+    super(ClassHead, self).__init__()
+    self._is_training = is_training
+    self._num_classes = num_classes
+    self._fc_hyperparams_fn = fc_hyperparams_fn
+    self._use_dropout = use_dropout
+    self._dropout_keep_prob = dropout_keep_prob
+
+  def _predict(self, roi_pooled_features):
+    """Predicts boxes and class scores.
+
+    Args:
+      roi_pooled_features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      class_predictions_with_background: A float tensor of shape
+        [batch_size, 1, num_classes + 1] representing the class predictions for
+        the proposals.
+    """
+    spatial_averaged_roi_pooled_features = tf.reduce_mean(
+        roi_pooled_features, [1, 2], keep_dims=True, name='AvgPool')
+    flattened_roi_pooled_features = slim.flatten(
+        spatial_averaged_roi_pooled_features)
+    if self._use_dropout:
+      flattened_roi_pooled_features = slim.dropout(
+          flattened_roi_pooled_features,
+          keep_prob=self._dropout_keep_prob,
+          is_training=self._is_training)
+
+    with slim.arg_scope(self._fc_hyperparams_fn()):
+      class_predictions_with_background = slim.fully_connected(
+          flattened_roi_pooled_features,
+          self._num_classes + 1,
+          activation_fn=None,
+          scope='ClassPredictor')
+    class_predictions_with_background = tf.reshape(
+        class_predictions_with_background, [-1, 1, self._num_classes + 1])
+    return class_predictions_with_background
diff --git a/research/object_detection/predictors/mask_rcnn_heads/class_head_test.py b/research/object_detection/predictors/mask_rcnn_heads/class_head_test.py
new file mode 100644
index 00000000..240b0a9a
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/class_head_test.py
@@ -0,0 +1,62 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_heads.class_head."""
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors.mask_rcnn_heads import class_head
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class ClassHeadTest(test_case.TestCase):
+
+  def _build_arg_scope_with_hyperparams(self,
+                                        op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.build(hyperparams, is_training=True)
+
+  def test_prediction_size(self):
+    class_prediction_head = class_head.ClassHead(
+        is_training=False,
+        num_classes=20,
+        fc_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+        use_dropout=True,
+        dropout_keep_prob=0.5)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = class_prediction_head.predict(
+        roi_pooled_features=roi_pooled_features)
+    tf.logging.info(prediction.shape)
+    self.assertAllEqual([64, 1, 21], prediction.get_shape().as_list())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_heads/keypoint_head.py b/research/object_detection/predictors/mask_rcnn_heads/keypoint_head.py
new file mode 100644
index 00000000..0a899d0f
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/keypoint_head.py
@@ -0,0 +1,90 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Keypoint Head."""
+import tensorflow as tf
+
+from object_detection.predictors.mask_rcnn_heads import mask_rcnn_head
+slim = tf.contrib.slim
+
+
+class KeypointHead(mask_rcnn_head.MaskRCNNHead):
+  """Mask RCNN keypoint prediction head."""
+
+  def __init__(self,
+               num_keypoints=17,
+               conv_hyperparams_fn=None,
+               keypoint_heatmap_height=56,
+               keypoint_heatmap_width=56,
+               keypoint_prediction_num_conv_layers=8,
+               keypoint_prediction_conv_depth=512):
+    """Constructor.
+
+    Args:
+      num_keypoints: (int scalar) number of keypoints.
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for convolution ops.
+      keypoint_heatmap_height: Desired output mask height. The default value
+        is 14.
+      keypoint_heatmap_width: Desired output mask width. The default value
+        is 14.
+      keypoint_prediction_num_conv_layers: Number of convolution layers applied
+        to the image_features in mask prediction branch.
+      keypoint_prediction_conv_depth: The depth for the first conv2d_transpose
+        op applied to the image_features in the mask prediction branch. If set
+        to 0, the depth of the convolution layers will be automatically chosen
+        based on the number of object classes and the number of channels in the
+        image features.
+    """
+    super(KeypointHead, self).__init__()
+    self._num_keypoints = num_keypoints
+    self._conv_hyperparams_fn = conv_hyperparams_fn
+    self._keypoint_heatmap_height = keypoint_heatmap_height
+    self._keypoint_heatmap_width = keypoint_heatmap_width
+    self._keypoint_prediction_num_conv_layers = (
+        keypoint_prediction_num_conv_layers)
+    self._keypoint_prediction_conv_depth = keypoint_prediction_conv_depth
+
+  def _predict(self, roi_pooled_features):
+    """Performs keypoint prediction.
+
+    Args:
+      roi_pooled_features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      instance_masks: A float tensor of shape
+          [batch_size, 1, num_keypoints, heatmap_height, heatmap_width].
+    """
+    with slim.arg_scope(self._conv_hyperparams_fn()):
+      net = slim.conv2d(
+          roi_pooled_features,
+          self._keypoint_prediction_conv_depth, [3, 3],
+          scope='conv_1')
+      for i in range(1, self._keypoint_prediction_num_conv_layers):
+        net = slim.conv2d(
+            net,
+            self._keypoint_prediction_conv_depth, [3, 3],
+            scope='conv_%d' % (i + 1))
+      net = slim.conv2d_transpose(
+          net, self._num_keypoints, [2, 2], scope='deconv1')
+      heatmaps_mask = tf.image.resize_bilinear(
+          net, [self._keypoint_heatmap_height, self._keypoint_heatmap_width],
+          align_corners=True,
+          name='upsample')
+      return tf.expand_dims(
+          tf.transpose(heatmaps_mask, perm=[0, 3, 1, 2]),
+          axis=1,
+          name='KeypointPredictor')
diff --git a/research/object_detection/predictors/mask_rcnn_heads/keypoint_head_test.py b/research/object_detection/predictors/mask_rcnn_heads/keypoint_head_test.py
new file mode 100644
index 00000000..1a0c9d6b
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/keypoint_head_test.py
@@ -0,0 +1,58 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_heads.keypoint_head."""
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors.mask_rcnn_heads import keypoint_head
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class KeypointHeadTest(test_case.TestCase):
+
+  def _build_arg_scope_with_hyperparams(self,
+                                        op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.build(hyperparams, is_training=True)
+
+  def test_prediction_size(self):
+    keypoint_prediction_head = keypoint_head.KeypointHead(
+        conv_hyperparams_fn=self._build_arg_scope_with_hyperparams())
+    roi_pooled_features = tf.random_uniform(
+        [64, 14, 14, 1024], minval=-2.0, maxval=2.0, dtype=tf.float32)
+    prediction = keypoint_prediction_head.predict(
+        roi_pooled_features=roi_pooled_features)
+    tf.logging.info(prediction.shape)
+    self.assertAllEqual([64, 1, 17, 56, 56], prediction.get_shape().as_list())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_heads/mask_head.py b/research/object_detection/predictors/mask_rcnn_heads/mask_head.py
new file mode 100644
index 00000000..2c4a4c0f
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/mask_head.py
@@ -0,0 +1,139 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Mask R-CNN Mask Head."""
+import math
+import tensorflow as tf
+
+from object_detection.predictors.mask_rcnn_heads import mask_rcnn_head
+
+slim = tf.contrib.slim
+
+
+class MaskHead(mask_rcnn_head.MaskRCNNHead):
+  """Mask RCNN mask prediction head."""
+
+  def __init__(self,
+               num_classes,
+               conv_hyperparams_fn=None,
+               mask_height=14,
+               mask_width=14,
+               mask_prediction_num_conv_layers=2,
+               mask_prediction_conv_depth=256,
+               masks_are_class_agnostic=False):
+    """Constructor.
+
+    Args:
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams_fn: A function to generate tf-slim arg_scope with
+        hyperparameters for convolution ops.
+      mask_height: Desired output mask height. The default value is 14.
+      mask_width: Desired output mask width. The default value is 14.
+      mask_prediction_num_conv_layers: Number of convolution layers applied to
+        the image_features in mask prediction branch.
+      mask_prediction_conv_depth: The depth for the first conv2d_transpose op
+        applied to the image_features in the mask prediction branch. If set
+        to 0, the depth of the convolution layers will be automatically chosen
+        based on the number of object classes and the number of channels in the
+        image features.
+      masks_are_class_agnostic: Boolean determining if the mask-head is
+        class-agnostic or not.
+
+    Raises:
+      ValueError: conv_hyperparams_fn is None.
+    """
+    super(MaskHead, self).__init__()
+    self._num_classes = num_classes
+    self._conv_hyperparams_fn = conv_hyperparams_fn
+    self._mask_height = mask_height
+    self._mask_width = mask_width
+    self._mask_prediction_num_conv_layers = mask_prediction_num_conv_layers
+    self._mask_prediction_conv_depth = mask_prediction_conv_depth
+    self._masks_are_class_agnostic = masks_are_class_agnostic
+    if conv_hyperparams_fn is None:
+      raise ValueError('conv_hyperparams_fn is None.')
+
+  def _get_mask_predictor_conv_depth(self,
+                                     num_feature_channels,
+                                     num_classes,
+                                     class_weight=3.0,
+                                     feature_weight=2.0):
+    """Computes the depth of the mask predictor convolutions.
+
+    Computes the depth of the mask predictor convolutions given feature channels
+    and number of classes by performing a weighted average of the two in
+    log space to compute the number of convolution channels. The weights that
+    are used for computing the weighted average do not need to sum to 1.
+
+    Args:
+      num_feature_channels: An integer containing the number of feature
+        channels.
+      num_classes: An integer containing the number of classes.
+      class_weight: Class weight used in computing the weighted average.
+      feature_weight: Feature weight used in computing the weighted average.
+
+    Returns:
+      An integer containing the number of convolution channels used by mask
+        predictor.
+    """
+    num_feature_channels_log = math.log(float(num_feature_channels), 2.0)
+    num_classes_log = math.log(float(num_classes), 2.0)
+    weighted_num_feature_channels_log = (
+        num_feature_channels_log * feature_weight)
+    weighted_num_classes_log = num_classes_log * class_weight
+    total_weight = feature_weight + class_weight
+    num_conv_channels_log = round(
+        (weighted_num_feature_channels_log + weighted_num_classes_log) /
+        total_weight)
+    return int(math.pow(2.0, num_conv_channels_log))
+
+  def _predict(self, roi_pooled_features):
+    """Performs mask prediction.
+
+    Args:
+      roi_pooled_features: A float tensor of shape [batch_size, height, width,
+        channels] containing features for a batch of images.
+
+    Returns:
+      instance_masks: A float tensor of shape
+          [batch_size, 1, num_classes, mask_height, mask_width].
+    """
+    num_conv_channels = self._mask_prediction_conv_depth
+    if num_conv_channels == 0:
+      num_feature_channels = roi_pooled_features.get_shape().as_list()[3]
+      num_conv_channels = self._get_mask_predictor_conv_depth(
+          num_feature_channels, self._num_classes)
+    with slim.arg_scope(self._conv_hyperparams_fn()):
+      upsampled_features = tf.image.resize_bilinear(
+          roi_pooled_features, [self._mask_height, self._mask_width],
+          align_corners=True)
+      for _ in range(self._mask_prediction_num_conv_layers - 1):
+        upsampled_features = slim.conv2d(
+            upsampled_features,
+            num_outputs=num_conv_channels,
+            kernel_size=[3, 3])
+      num_masks = 1 if self._masks_are_class_agnostic else self._num_classes
+      mask_predictions = slim.conv2d(
+          upsampled_features,
+          num_outputs=num_masks,
+          activation_fn=None,
+          kernel_size=[3, 3])
+      return tf.expand_dims(
+          tf.transpose(mask_predictions, perm=[0, 3, 1, 2]),
+          axis=1,
+          name='MaskPredictor')
diff --git a/research/object_detection/predictors/mask_rcnn_heads/mask_head_test.py b/research/object_detection/predictors/mask_rcnn_heads/mask_head_test.py
new file mode 100644
index 00000000..5b54ed31
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/mask_head_test.py
@@ -0,0 +1,64 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.mask_rcnn_heads.mask_head."""
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors.mask_rcnn_heads import mask_head
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class MaskHeadTest(test_case.TestCase):
+
+  def _build_arg_scope_with_hyperparams(self,
+                                        op_type=hyperparams_pb2.Hyperparams.FC):
+    hyperparams = hyperparams_pb2.Hyperparams()
+    hyperparams_text_proto = """
+      activation: NONE
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(hyperparams_text_proto, hyperparams)
+    hyperparams.op = op_type
+    return hyperparams_builder.build(hyperparams, is_training=True)
+
+  def test_prediction_size(self):
+    mask_prediction_head = mask_head.MaskHead(
+        num_classes=20,
+        conv_hyperparams_fn=self._build_arg_scope_with_hyperparams(),
+        mask_height=14,
+        mask_width=14,
+        mask_prediction_num_conv_layers=2,
+        mask_prediction_conv_depth=256,
+        masks_are_class_agnostic=False)
+    roi_pooled_features = tf.random_uniform(
+        [64, 7, 7, 1024], minval=-10.0, maxval=10.0, dtype=tf.float32)
+    prediction = mask_prediction_head.predict(
+        roi_pooled_features=roi_pooled_features)
+    tf.logging.info(prediction.shape)
+    self.assertAllEqual([64, 1, 20, 14, 14], prediction.get_shape().as_list())
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/predictors/mask_rcnn_heads/mask_rcnn_head.py b/research/object_detection/predictors/mask_rcnn_heads/mask_rcnn_head.py
new file mode 100644
index 00000000..b6f9785f
--- /dev/null
+++ b/research/object_detection/predictors/mask_rcnn_heads/mask_rcnn_head.py
@@ -0,0 +1,44 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Base Mask RCNN head class."""
+from abc import abstractmethod
+
+
+class MaskRCNNHead(object):
+  """Mask RCNN head base class."""
+
+  def __init__(self):
+    """Constructor."""
+
+  def predict(self, roi_pooled_features):
+    """Returns the head's predictions.
+
+    Args:
+      roi_pooled_features: A float tensor of shape
+        [batch_size, height, width, channels] containing ROI pooled features
+        from a batch of boxes.
+    """
+    return self._predict(roi_pooled_features)
+
+  @abstractmethod
+  def _predict(self, roi_pooled_features):
+    """The abstract internal prediction function that needs to be overloaded.
+
+    Args:
+      roi_pooled_features: A float tensor of shape
+        [batch_size, height, width, channels] containing ROI pooled features
+        from a batch of boxes.
+    """
diff --git a/research/object_detection/predictors/rfcn_box_predictor.py b/research/object_detection/predictors/rfcn_box_predictor.py
new file mode 100644
index 00000000..d16de044
--- /dev/null
+++ b/research/object_detection/predictors/rfcn_box_predictor.py
@@ -0,0 +1,159 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""RFCN Box Predictor."""
+import tensorflow as tf
+from object_detection.core import box_predictor
+from object_detection.utils import ops
+
+slim = tf.contrib.slim
+
+BOX_ENCODINGS = box_predictor.BOX_ENCODINGS
+CLASS_PREDICTIONS_WITH_BACKGROUND = (
+    box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND)
+MASK_PREDICTIONS = box_predictor.MASK_PREDICTIONS
+
+
+class RfcnBoxPredictor(box_predictor.BoxPredictor):
+  """RFCN Box Predictor.
+
+  Applies a position sensitive ROI pooling on position sensitive feature maps to
+  predict classes and refined locations. See https://arxiv.org/abs/1605.06409
+  for details.
+
+  This is used for the second stage of the RFCN meta architecture. Notice that
+  locations are *not* shared across classes, thus for each anchor, a separate
+  prediction is made for each class.
+  """
+
+  def __init__(self,
+               is_training,
+               num_classes,
+               conv_hyperparams_fn,
+               num_spatial_bins,
+               depth,
+               crop_size,
+               box_code_size):
+    """Constructor.
+
+    Args:
+      is_training: Indicates whether the BoxPredictor is in training mode.
+      num_classes: number of classes.  Note that num_classes *does not*
+        include the background category, so if groundtruth labels take values
+        in {0, 1, .., K-1}, num_classes=K (and not K+1, even though the
+        assigned classification targets can range from {0,... K}).
+      conv_hyperparams_fn: A function to construct tf-slim arg_scope with
+        hyperparameters for convolutional layers.
+      num_spatial_bins: A list of two integers `[spatial_bins_y,
+        spatial_bins_x]`.
+      depth: Target depth to reduce the input feature maps to.
+      crop_size: A list of two integers `[crop_height, crop_width]`.
+      box_code_size: Size of encoding for each box.
+    """
+    super(RfcnBoxPredictor, self).__init__(is_training, num_classes)
+    self._conv_hyperparams_fn = conv_hyperparams_fn
+    self._num_spatial_bins = num_spatial_bins
+    self._depth = depth
+    self._crop_size = crop_size
+    self._box_code_size = box_code_size
+
+  @property
+  def num_classes(self):
+    return self._num_classes
+
+  def _predict(self, image_features, num_predictions_per_location,
+               proposal_boxes):
+    """Computes encoded object locations and corresponding confidences.
+
+    Args:
+      image_features: A list of float tensors of shape [batch_size, height_i,
+      width_i, channels_i] containing features for a batch of images.
+      num_predictions_per_location: A list of integers representing the number
+        of box predictions to be made per spatial location for each feature map.
+        Currently, this must be set to [1], or an error will be raised.
+      proposal_boxes: A float tensor of shape [batch_size, num_proposals,
+        box_code_size].
+
+    Returns:
+      box_encodings: A list of float tensors of shape
+        [batch_size, num_anchors_i, q, code_size] representing the location of
+        the objects, where q is 1 or the number of classes. Each entry in the
+        list corresponds to a feature map in the input `image_features` list.
+      class_predictions_with_background: A list of float tensors of shape
+        [batch_size, num_anchors_i, num_classes + 1] representing the class
+        predictions for the proposals. Each entry in the list corresponds to a
+        feature map in the input `image_features` list.
+
+    Raises:
+      ValueError: if num_predictions_per_location is not 1 or if
+        len(image_features) is not 1.
+    """
+    if (len(num_predictions_per_location) != 1 or
+        num_predictions_per_location[0] != 1):
+      raise ValueError('Currently RfcnBoxPredictor only supports '
+                       'predicting a single box per class per location.')
+    if len(image_features) != 1:
+      raise ValueError('length of `image_features` must be 1. Found {}'.
+                       format(len(image_features)))
+    image_feature = image_features[0]
+    num_predictions_per_location = num_predictions_per_location[0]
+    batch_size = tf.shape(proposal_boxes)[0]
+    num_boxes = tf.shape(proposal_boxes)[1]
+    net = image_feature
+    with slim.arg_scope(self._conv_hyperparams_fn()):
+      net = slim.conv2d(net, self._depth, [1, 1], scope='reduce_depth')
+      # Location predictions.
+      location_feature_map_depth = (self._num_spatial_bins[0] *
+                                    self._num_spatial_bins[1] *
+                                    self.num_classes *
+                                    self._box_code_size)
+      location_feature_map = slim.conv2d(net, location_feature_map_depth,
+                                         [1, 1], activation_fn=None,
+                                         scope='refined_locations')
+      box_encodings = ops.batch_position_sensitive_crop_regions(
+          location_feature_map,
+          boxes=proposal_boxes,
+          crop_size=self._crop_size,
+          num_spatial_bins=self._num_spatial_bins,
+          global_pool=True)
+      box_encodings = tf.squeeze(box_encodings, squeeze_dims=[2, 3])
+      box_encodings = tf.reshape(box_encodings,
+                                 [batch_size * num_boxes, 1, self.num_classes,
+                                  self._box_code_size])
+
+      # Class predictions.
+      total_classes = self.num_classes + 1  # Account for background class.
+      class_feature_map_depth = (self._num_spatial_bins[0] *
+                                 self._num_spatial_bins[1] *
+                                 total_classes)
+      class_feature_map = slim.conv2d(net, class_feature_map_depth, [1, 1],
+                                      activation_fn=None,
+                                      scope='class_predictions')
+      class_predictions_with_background = (
+          ops.batch_position_sensitive_crop_regions(
+              class_feature_map,
+              boxes=proposal_boxes,
+              crop_size=self._crop_size,
+              num_spatial_bins=self._num_spatial_bins,
+              global_pool=True))
+      class_predictions_with_background = tf.squeeze(
+          class_predictions_with_background, squeeze_dims=[2, 3])
+      class_predictions_with_background = tf.reshape(
+          class_predictions_with_background,
+          [batch_size * num_boxes, 1, total_classes])
+
+    return {BOX_ENCODINGS: [box_encodings],
+            CLASS_PREDICTIONS_WITH_BACKGROUND:
+            [class_predictions_with_background]}
diff --git a/research/object_detection/predictors/rfcn_box_predictor_test.py b/research/object_detection/predictors/rfcn_box_predictor_test.py
new file mode 100644
index 00000000..104246d0
--- /dev/null
+++ b/research/object_detection/predictors/rfcn_box_predictor_test.py
@@ -0,0 +1,77 @@
+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Tests for object_detection.predictors.rfcn_box_predictor."""
+import numpy as np
+import tensorflow as tf
+
+from google.protobuf import text_format
+from object_detection.builders import hyperparams_builder
+from object_detection.predictors import rfcn_box_predictor as box_predictor
+from object_detection.protos import hyperparams_pb2
+from object_detection.utils import test_case
+
+
+class RfcnBoxPredictorTest(test_case.TestCase):
+
+  def _build_arg_scope_with_conv_hyperparams(self):
+    conv_hyperparams = hyperparams_pb2.Hyperparams()
+    conv_hyperparams_text_proto = """
+      regularizer {
+        l2_regularizer {
+        }
+      }
+      initializer {
+        truncated_normal_initializer {
+        }
+      }
+    """
+    text_format.Merge(conv_hyperparams_text_proto, conv_hyperparams)
+    return hyperparams_builder.build(conv_hyperparams, is_training=True)
+
+  def test_get_correct_box_encoding_and_class_prediction_shapes(self):
+
+    def graph_fn(image_features, proposal_boxes):
+      rfcn_box_predictor = box_predictor.RfcnBoxPredictor(
+          is_training=False,
+          num_classes=2,
+          conv_hyperparams_fn=self._build_arg_scope_with_conv_hyperparams(),
+          num_spatial_bins=[3, 3],
+          depth=4,
+          crop_size=[12, 12],
+          box_code_size=4
+      )
+      box_predictions = rfcn_box_predictor.predict(
+          [image_features], num_predictions_per_location=[1],
+          scope='BoxPredictor',
+          proposal_boxes=proposal_boxes)
+      box_encodings = tf.concat(
+          box_predictions[box_predictor.BOX_ENCODINGS], axis=1)
+      class_predictions_with_background = tf.concat(
+          box_predictions[box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND],
+          axis=1)
+      return (box_encodings, class_predictions_with_background)
+
+    image_features = np.random.rand(4, 8, 8, 64).astype(np.float32)
+    proposal_boxes = np.random.rand(4, 2, 4).astype(np.float32)
+    (box_encodings, class_predictions_with_background) = self.execute(
+        graph_fn, [image_features, proposal_boxes])
+
+    self.assertAllEqual(box_encodings.shape, [8, 1, 2, 4])
+    self.assertAllEqual(class_predictions_with_background.shape, [8, 1, 3])
+
+
+if __name__ == '__main__':
+  tf.test.main()
diff --git a/research/object_detection/protos/box_predictor.proto b/research/object_detection/protos/box_predictor.proto
index 5dbf47f4..5f58e597 100644
--- a/research/object_detection/protos/box_predictor.proto
+++ b/research/object_detection/protos/box_predictor.proto
@@ -93,6 +93,8 @@ message WeightSharedConvolutionalBoxPredictor {
   optional bool share_prediction_tower = 13 [default = false];
 }
 
+// TODO(alirezafathi): Refactor the proto file to be able to configure mask rcnn
+// head easily.
 message MaskRCNNBoxPredictor {
   // Hyperparameters for fully connected ops used in the box predictor.
   optional Hyperparams fc_hyperparams = 1;
diff --git a/research/object_detection/protos/faster_rcnn.proto b/research/object_detection/protos/faster_rcnn.proto
index dcc4d387..2b042210 100644
--- a/research/object_detection/protos/faster_rcnn.proto
+++ b/research/object_detection/protos/faster_rcnn.proto
@@ -142,6 +142,21 @@ message FasterRcnn {
   // standard tf.image.crop_and_resize while computing second stage input
   // feature maps.
   optional bool use_matmul_crop_and_resize = 31 [default = false];
+
+  // Normally, anchors generated for a given image size are pruned during
+  // training if they lie outside the image window. Setting this option to true,
+  // clips the anchors to be within the image instead of pruning.
+  optional bool clip_anchors_to_image = 32 [default = false];
+
+  // After peforming matching between anchors and targets, in order to pull out
+  // targets for training Faster R-CNN meta architecture we perform a gather
+  // operation. This options specifies whether to use an alternate
+  // implementation of tf.gather that is faster on TPUs.
+  optional bool use_matmul_gather_in_matcher = 33 [default = false];
+
+  // Whether to use the balanced positive negative sampler implementation with
+  // static shape guarantees.
+  optional bool use_static_balanced_label_sampler = 34 [default = false];
 }
 
 
diff --git a/research/object_detection/protos/preprocessor.proto b/research/object_detection/protos/preprocessor.proto
index eb16d982..795d6568 100644
--- a/research/object_detection/protos/preprocessor.proto
+++ b/research/object_detection/protos/preprocessor.proto
@@ -33,6 +33,7 @@ message PreprocessingStep {
     RandomVerticalFlip random_vertical_flip = 25;
     RandomRotation90 random_rotation90 = 26;
     RGBtoGray rgb_to_gray = 27;
+    ConvertClassLogitsToSoftmax convert_class_logits_to_softmax = 28;
   }
 }
 
@@ -409,3 +410,11 @@ message SSDRandomCropPadFixedAspectRatio {
   // width. Two entries per operation.
   repeated float max_padded_size_ratio = 4;
 }
+
+// Converts class logits to softmax optionally scaling the values by temperature
+// first.
+message ConvertClassLogitsToSoftmax {
+
+  // Scale to use on logits before applying softmax.
+  optional float temperature = 1 [default=1.0];
+}
diff --git a/research/object_detection/protos/region_similarity_calculator.proto b/research/object_detection/protos/region_similarity_calculator.proto
index e82424e2..2fc70c01 100644
--- a/research/object_detection/protos/region_similarity_calculator.proto
+++ b/research/object_detection/protos/region_similarity_calculator.proto
@@ -9,6 +9,7 @@ message RegionSimilarityCalculator {
     NegSqDistSimilarity neg_sq_dist_similarity = 1;
     IouSimilarity iou_similarity = 2;
     IoaSimilarity ioa_similarity = 3;
+    ThresholdedIouSimilarity thresholded_iou_similarity = 4;
   }
 }
 
@@ -23,3 +24,10 @@ message IouSimilarity {
 // Configuration for intersection-over-area (IOA) similarity calculator.
 message IoaSimilarity {
 }
+
+// Configuration for thresholded-intersection-over-union similarity calculator.
+message ThresholdedIouSimilarity {
+
+  // IOU threshold used for filtering scores.
+  optional float iou_threshold = 1 [default = 0.5];
+}
diff --git a/research/object_detection/protos/ssd.proto b/research/object_detection/protos/ssd.proto
index 6401bb36..8d3bf8cf 100644
--- a/research/object_detection/protos/ssd.proto
+++ b/research/object_detection/protos/ssd.proto
@@ -120,4 +120,30 @@ message SsdFeatureExtractor {
   // Whether to use depthwise separable convolutions for to extract additional
   // feature maps added by SSD.
   optional bool use_depthwise = 8 [default=false];
+
+  // Feature Pyramid Networks config.
+  optional FeaturePyramidNetworks fpn = 10;
+}
+
+// Configuration for Feature Pyramid Networks.
+message FeaturePyramidNetworks {
+  // We recommend to use multi_resolution_feature_map_generator with FPN, and
+  // the levels there must match the levels defined below for better
+  // performance.
+  // Correspondence from FPN levels to Resnet/Mobilenet V1 feature maps:
+  // FPN Level        Resnet Feature Map      Mobilenet-V1 Feature Map
+  //     2               Block 1                Conv2d_3_pointwise
+  //     3               Block 2                Conv2d_5_pointwise
+  //     4               Block 3                Conv2d_11_pointwise
+  //     5               Block 4                Conv2d_13_pointwise
+  //     6               Bottomup_5             bottom_up_Conv2d_14
+  //     7               Bottomup_6             bottom_up_Conv2d_15
+  //     8               Bottomup_7             bottom_up_Conv2d_16
+  //     9               Bottomup_8             bottom_up_Conv2d_17
+
+  // minimum level in feature pyramid
+  optional int32 min_level = 1 [default = 3];
+
+  // maximum level in feature pyramid
+  optional int32 max_level = 2 [default = 7];
 }
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
index 7ce26953..c21a1fab 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_300x300_coco14_sync.config
@@ -1,8 +1,7 @@
 # SSD with Mobilenet v1 feature extractor and focal loss.
 # Trained on COCO14, initialized from Imagenet classification checkpoint
 
-# Achieves 19.3 mAP on COCO14 minival dataset. Doubling the number of training
-# steps gets to 20.6 mAP.
+# Achieves 20.5 mAP on COCO14 minival dataset.
 
 # This config is TPU compatible
 
@@ -144,11 +143,11 @@ model {
 
 train_config: {
   fine_tune_checkpoint: "PATH_TO_BE_CONFIGURED/model.ckpt"
-  batch_size: 2048
+  batch_size: 1024
   sync_replicas: true
   startup_delay_steps: 0
   replicas_to_aggregate: 8
-  num_steps: 10000
+  num_steps: 20000
   data_augmentation_options {
     random_horizontal_flip {
     }
@@ -162,9 +161,9 @@ train_config: {
       learning_rate: {
         cosine_decay_learning_rate {
           learning_rate_base: 0.9
-          total_steps: 10000
+          total_steps: 20000
           warmup_learning_rate: 0.3
-          warmup_steps: 300
+          warmup_steps: 1000
         }
       }
       momentum_optimizer_value: 0.9
diff --git a/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config b/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
index d464e2fe..a2abb6b1 100644
--- a/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
+++ b/research/object_detection/samples/configs/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
@@ -79,6 +79,10 @@ model {
     }
     feature_extractor {
       type: 'ssd_mobilenet_v1_fpn'
+      fpn {
+        min_level: 3
+        max_level: 7
+      }
       min_depth: 16
       depth_multiplier: 1.0
       conv_hyperparams {
diff --git a/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config b/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
index 748714c7..503f8ecd 100644
--- a/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
+++ b/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
@@ -80,6 +80,10 @@ model {
     }
     feature_extractor {
       type: 'ssd_resnet50_v1_fpn'
+      fpn {
+        min_level: 3
+        max_level: 7
+      }
       min_depth: 16
       depth_multiplier: 1.0
       conv_hyperparams {
diff --git a/research/object_detection/utils/label_map_util.py b/research/object_detection/utils/label_map_util.py
index aef46c1d..06a1b345 100644
--- a/research/object_detection/utils/label_map_util.py
+++ b/research/object_detection/utils/label_map_util.py
@@ -139,15 +139,26 @@ def load_labelmap(path):
   return label_map
 
 
-def get_label_map_dict(label_map_path, use_display_name=False):
+def get_label_map_dict(label_map_path,
+                       use_display_name=False,
+                       fill_in_gaps_and_background=False):
   """Reads a label map and returns a dictionary of label names to id.
 
   Args:
-    label_map_path: path to label_map.
+    label_map_path: path to StringIntLabelMap proto text file.
     use_display_name: whether to use the label map items' display names as keys.
+    fill_in_gaps_and_background: whether to fill in gaps and background with
+    respect to the id field in the proto. The id: 0 is reserved for the
+    'background' class and will be added if it is missing. All other missing
+    ids in range(1, max(id)) will be added with a dummy class name
+    ("class_<id>") if they are missing.
 
   Returns:
     A dictionary mapping label names to id.
+
+  Raises:
+    ValueError: if fill_in_gaps_and_background and label_map has non-integer or
+    negative values.
   """
   label_map = load_labelmap(label_map_path)
   label_map_dict = {}
@@ -156,6 +167,24 @@ def get_label_map_dict(label_map_path, use_display_name=False):
       label_map_dict[item.display_name] = item.id
     else:
       label_map_dict[item.name] = item.id
+
+  if fill_in_gaps_and_background:
+    values = set(label_map_dict.values())
+
+    if 0 not in values:
+      label_map_dict['background'] = 0
+    if not all(isinstance(value, int) for value in values):
+      raise ValueError('The values in label map must be integers in order to'
+                       'fill_in_gaps_and_background.')
+    if not all(value >= 0 for value in values):
+      raise ValueError('The values in the label map must be positive.')
+
+    if len(values) != max(values) + 1:
+      # there are gaps in the labels, fill in gaps.
+      for value in range(1, max(values)):
+        if value not in values:
+          label_map_dict['class_' + str(value)] = value
+
   return label_map_dict
 
 
diff --git a/research/object_detection/utils/label_map_util_test.py b/research/object_detection/utils/label_map_util_test.py
index c2afbe65..d528b804 100644
--- a/research/object_detection/utils/label_map_util_test.py
+++ b/research/object_detection/utils/label_map_util_test.py
@@ -119,6 +119,30 @@ class LabelMapUtilTest(tf.test.TestCase):
     self.assertEqual(label_map_dict['dog'], 1)
     self.assertEqual(label_map_dict['cat'], 2)
 
+  def test_get_label_map_dict_with_fill_in_gaps_and_background(self):
+    label_map_string = """
+      item {
+        id:3
+        name:'cat'
+      }
+      item {
+        id:1
+        name:'dog'
+      }
+    """
+    label_map_path = os.path.join(self.get_temp_dir(), 'label_map.pbtxt')
+    with tf.gfile.Open(label_map_path, 'wb') as f:
+      f.write(label_map_string)
+
+    label_map_dict = label_map_util.get_label_map_dict(
+        label_map_path, fill_in_gaps_and_background=True)
+
+    self.assertEqual(label_map_dict['background'], 0)
+    self.assertEqual(label_map_dict['dog'], 1)
+    self.assertEqual(label_map_dict['class_2'], 2)
+    self.assertEqual(label_map_dict['cat'], 3)
+    self.assertEqual(len(label_map_dict), max(label_map_dict.values()) + 1)
+
   def test_keep_categories_with_unique_id(self):
     label_map_proto = string_int_label_map_pb2.StringIntLabelMap()
     label_map_string = """
diff --git a/research/object_detection/utils/object_detection_evaluation.py b/research/object_detection/utils/object_detection_evaluation.py
index 8a38d8c2..60c4abbc 100644
--- a/research/object_detection/utils/object_detection_evaluation.py
+++ b/research/object_detection/utils/object_detection_evaluation.py
@@ -31,6 +31,7 @@ from abc import ABCMeta
 from abc import abstractmethod
 import collections
 import logging
+import unicodedata
 import numpy as np
 
 from object_detection.core import standard_fields
@@ -284,18 +285,23 @@ class ObjectDetectionEvaluator(DetectionEvaluator):
     category_index = label_map_util.create_category_index(self._categories)
     for idx in range(per_class_ap.size):
       if idx + self._label_id_offset in category_index:
+        category_name = category_index[idx + self._label_id_offset]['name']
+        try:
+          category_name = unicode(category_name, 'utf-8')
+        except TypeError:
+          pass
+        category_name = unicodedata.normalize(
+            'NFKD', category_name).encode('ascii', 'ignore')
         display_name = (
             self._metric_prefix + 'PerformanceByCategory/AP@{}IOU/{}'.format(
-                self._matching_iou_threshold,
-                category_index[idx + self._label_id_offset]['name']))
+                self._matching_iou_threshold, category_name))
         pascal_metrics[display_name] = per_class_ap[idx]
 
         # Optionally add CorLoc metrics.classes
         if self._evaluate_corlocs:
           display_name = (
               self._metric_prefix + 'PerformanceByCategory/CorLoc@{}IOU/{}'
-              .format(self._matching_iou_threshold,
-                      category_index[idx + self._label_id_offset]['name']))
+              .format(self._matching_iou_threshold, category_name))
           pascal_metrics[display_name] = per_class_corloc[idx]
 
     return pascal_metrics
@@ -839,9 +845,9 @@ class ObjectDetectionEvaluation(object):
       if self.use_weighted_mean_ap:
         all_scores = np.append(all_scores, scores)
         all_tp_fp_labels = np.append(all_tp_fp_labels, tp_fp_labels)
-      print 'Scores and tpfp per class label: {}'.format(class_index)
-      print tp_fp_labels
-      print scores
+      logging.info('Scores and tpfp per class label: %d', class_index)
+      logging.info(tp_fp_labels)
+      logging.info(scores)
       precision, recall = metrics.compute_precision_recall(
           scores, tp_fp_labels, self.num_gt_instances_per_class[class_index])
       self.precisions_per_class.append(precision)
diff --git a/research/object_detection/utils/ops.py b/research/object_detection/utils/ops.py
index 662f7e0d..2dc314cd 100644
--- a/research/object_detection/utils/ops.py
+++ b/research/object_detection/utils/ops.py
@@ -20,8 +20,6 @@ import six
 
 import tensorflow as tf
 
-from object_detection.core import box_list
-from object_detection.core import box_list_ops
 from object_detection.core import standard_fields as fields
 from object_detection.utils import shape_utils
 from object_detection.utils import static_shape
@@ -60,13 +58,20 @@ def normalized_to_image_coordinates(normalized_boxes, image_shape,
     parallel_iterations: parallelism for the map_fn op.
 
   Returns:
-    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containg the
-      boxes in image coordinates.
+    absolute_boxes: a float32 tensor of shape [None, num_boxes, 4] containing
+      the boxes in image coordinates.
   """
+  x_scale = tf.cast(image_shape[2], tf.float32)
+  y_scale = tf.cast(image_shape[1], tf.float32)
   def _to_absolute_coordinates(normalized_boxes):
-    return box_list_ops.to_absolute_coordinates(
-        box_list.BoxList(normalized_boxes),
-        image_shape[1], image_shape[2], check_range=False).get()
+    y_min, x_min, y_max, x_max = tf.split(
+        value=normalized_boxes, num_or_size_splits=4, axis=1)
+    y_min = y_scale * y_min
+    y_max = y_scale * y_max
+    x_min = x_scale * x_min
+    x_max = x_scale * x_max
+    scaled_boxes = tf.concat([y_min, x_min, y_max, x_max], 1)
+    return scaled_boxes
 
   absolute_boxes = shape_utils.static_or_dynamic_map_fn(
       _to_absolute_coordinates,
@@ -538,13 +543,59 @@ def normalize_to_target(inputs,
     return tf.reshape(target_norm, mult_shape) * tf.truediv(inputs, lengths)
 
 
+def batch_position_sensitive_crop_regions(images,
+                                          boxes,
+                                          crop_size,
+                                          num_spatial_bins,
+                                          global_pool,
+                                          parallel_iterations=64):
+  """Position sensitive crop with batches of images and boxes.
+
+  This op is exactly like `position_sensitive_crop_regions` below but operates
+  on batches of images and boxes. See `position_sensitive_crop_regions` function
+  below for the operation applied per batch element.
+
+  Args:
+    images: A `Tensor`. Must be one of the following types: `uint8`, `int8`,
+      `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
+      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
+      Both `image_height` and `image_width` need to be positive.
+    boxes: A `Tensor` of type `float32`.
+      A 3-D tensor of shape `[batch, num_boxes, 4]`. Each box is specified in
+      normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value
+      of `y` is mapped to the image coordinate at `y * (image_height - 1)`, so
+      as the `[0, 1]` interval of normalized image height is mapped to
+      `[0, image_height - 1] in image height coordinates. We do allow y1 > y2,
+      in which case the sampled crop is an up-down flipped version of the
+      original image. The width dimension is treated similarly.
+    crop_size: See `position_sensitive_crop_regions` below.
+    num_spatial_bins: See `position_sensitive_crop_regions` below.
+    global_pool: See `position_sensitive_crop_regions` below.
+    parallel_iterations: Number of batch items to process in parallel.
+
+  Returns:
+  """
+  def _position_sensitive_crop_fn(inputs):
+    images, boxes = inputs
+    return position_sensitive_crop_regions(
+        images,
+        boxes,
+        crop_size=crop_size,
+        num_spatial_bins=num_spatial_bins,
+        global_pool=global_pool)
+
+  return shape_utils.static_or_dynamic_map_fn(
+      _position_sensitive_crop_fn,
+      elems=[images, boxes],
+      dtype=tf.float32,
+      parallel_iterations=parallel_iterations)
+
+
 def position_sensitive_crop_regions(image,
                                     boxes,
-                                    box_ind,
                                     crop_size,
                                     num_spatial_bins,
-                                    global_pool,
-                                    extrapolation_value=None):
+                                    global_pool):
   """Position-sensitive crop and pool rectangular regions from a feature grid.
 
   The output crops are split into `spatial_bins_y` vertical bins
@@ -565,23 +616,16 @@ def position_sensitive_crop_regions(image,
   Args:
     image: A `Tensor`. Must be one of the following types: `uint8`, `int8`,
       `int16`, `int32`, `int64`, `half`, `float32`, `float64`.
-      A 4-D tensor of shape `[batch, image_height, image_width, depth]`.
+      A 3-D tensor of shape `[image_height, image_width, depth]`.
       Both `image_height` and `image_width` need to be positive.
     boxes: A `Tensor` of type `float32`.
-      A 2-D tensor of shape `[num_boxes, 4]`. The `i`-th row of the tensor
-      specifies the coordinates of a box in the `box_ind[i]` image and is
-      specified in normalized coordinates `[y1, x1, y2, x2]`. A normalized
-      coordinate value of `y` is mapped to the image coordinate at
-      `y * (image_height - 1)`, so as the `[0, 1]` interval of normalized image
-      height is mapped to `[0, image_height - 1] in image height coordinates.
-      We do allow y1 > y2, in which case the sampled crop is an up-down flipped
-      version of the original image. The width dimension is treated similarly.
-      Normalized coordinates outside the `[0, 1]` range are allowed, in which
-      case we use `extrapolation_value` to extrapolate the input image values.
-    box_ind:  A `Tensor` of type `int32`.
-      A 1-D tensor of shape `[num_boxes]` with int32 values in `[0, batch)`.
-      The value of `box_ind[i]` specifies the image that the `i`-th box refers
-      to.
+      A 2-D tensor of shape `[num_boxes, 4]`. Each box is specified in
+      normalized coordinates `[y1, x1, y2, x2]`. A normalized coordinate value
+      of `y` is mapped to the image coordinate at `y * (image_height - 1)`, so
+      as the `[0, 1]` interval of normalized image height is mapped to
+      `[0, image_height - 1] in image height coordinates. We do allow y1 > y2,
+      in which case the sampled crop is an up-down flipped version of the
+      original image. The width dimension is treated similarly.
     crop_size: A list of two integers `[crop_height, crop_width]`. All
       cropped image patches are resized to this size. The aspect ratio of the
       image content is not preserved. Both `crop_height` and `crop_width` need
@@ -601,8 +645,7 @@ def position_sensitive_crop_regions(image,
       Note that using global_pool=True is equivalent to but more efficient than
         running the function with global_pool=False and then performing global
         average pooling.
-    extrapolation_value: An optional `float`. Defaults to `0`.
-      Value used for extrapolation, when applicable.
+
   Returns:
     position_sensitive_features: A 4-D tensor of shape
       `[num_boxes, K, K, crop_channels]`,
@@ -649,12 +692,17 @@ def position_sensitive_crop_regions(image,
                         ]
       position_sensitive_boxes.append(tf.stack(box_coordinates, axis=1))
 
-  image_splits = tf.split(value=image, num_or_size_splits=total_bins, axis=3)
+  image_splits = tf.split(value=image, num_or_size_splits=total_bins, axis=2)
 
   image_crops = []
   for (split, box) in zip(image_splits, position_sensitive_boxes):
-    crop = tf.image.crop_and_resize(split, box, box_ind, bin_crop_size,
-                                    extrapolation_value=extrapolation_value)
+    if split.shape.is_fully_defined() and box.shape.is_fully_defined():
+      crop = matmul_crop_and_resize(
+          tf.expand_dims(split, 0), box, bin_crop_size)
+    else:
+      crop = tf.image.crop_and_resize(
+          tf.expand_dims(split, 0), box,
+          tf.zeros(tf.shape(boxes)[0], dtype=tf.int32), bin_crop_size)
     image_crops.append(crop)
 
   if global_pool:
@@ -957,3 +1005,68 @@ def matmul_crop_and_resize(image, boxes, crop_size, scope=None):
               tf.matmul(kernel_h, tf.tile(channel, [num_crops, 1, 1])),
               kernel_w, transpose_b=True))
     return tf.stack(result_channels, axis=3)
+
+
+def expected_classification_loss_under_sampling(batch_cls_targets, cls_losses,
+                                                negative_to_positive_ratio,
+                                                minimum_negative_sampling):
+  """Computes classification loss by background/foreground weighting.
+
+  The weighting is such that the effective background/foreground weight ratio
+  is the negative_to_positive_ratio. if p_i is the foreground probability of
+  anchor a_i, L(a_i) is the anchors loss, N is the number of anchors, and M is
+  the sum of foreground probabilities across anchors, then the total loss L is
+  calculated as:
+
+  beta = K*M/(N-M)
+  L = sum_{i=1}^N [p_i + beta * (1 - p_i)] * (L(a_i))
+
+  Args:
+    batch_cls_targets: A tensor with shape [batch_size, num_anchors,
+        num_classes + 1], where 0'th index is the background class, containing
+        the class distrubution for the target assigned to a given anchor.
+    cls_losses: Float tensor of shape [batch_size, num_anchors]
+        representing anchorwise classification losses.
+    negative_to_positive_ratio: The desired background/foreground weight ratio.
+    minimum_negative_sampling: Minimum number of effective negative samples.
+      Used only when there are no positive examples.
+
+  Returns:
+    The classification loss.
+  """
+
+  num_anchors = tf.cast(tf.shape(batch_cls_targets)[1], tf.float32)
+
+  # find the p_i
+  foreground_probabilities = (
+      foreground_probabilities_from_targets(batch_cls_targets))
+  foreground_sum = tf.reduce_sum(foreground_probabilities, axis=-1)
+
+  k = negative_to_positive_ratio
+
+  # compute beta
+  denominators = (num_anchors - foreground_sum)
+  beta = tf.where(
+      tf.equal(denominators, 0), tf.zeros_like(foreground_sum),
+      k * foreground_sum / denominators)
+
+  # where the foreground sum is zero, use a minimum negative weight.
+  min_negative_weight = 1.0 * minimum_negative_sampling / num_anchors
+  beta = tf.where(
+      tf.equal(beta, 0), min_negative_weight * tf.ones_like(beta), beta)
+  beta = tf.reshape(beta, [-1, 1])
+
+  cls_loss_weights = foreground_probabilities + (
+      1 - foreground_probabilities) * beta
+
+  weighted_losses = cls_loss_weights * cls_losses
+
+  cls_losses = tf.reduce_sum(weighted_losses, axis=-1)
+
+  return cls_losses
+
+
+def foreground_probabilities_from_targets(batch_cls_targets):
+  foreground_probabilities = 1 - batch_cls_targets[:, :, 0]
+
+  return foreground_probabilities
diff --git a/research/object_detection/utils/ops_test.py b/research/object_detection/utils/ops_test.py
index e7865e27..9fbdb944 100644
--- a/research/object_detection/utils/ops_test.py
+++ b/research/object_detection/utils/ops_test.py
@@ -812,13 +812,12 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
 
   def test_position_sensitive(self):
     num_spatial_bins = [3, 2]
-    image_shape = [1, 3, 2, 6]
+    image_shape = [3, 2, 6]
 
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
     boxes = tf.random_uniform((2, 4))
-    box_ind = tf.constant([0, 0], dtype=tf.int32)
 
     # The result for both boxes should be [[1, 2], [3, 4], [5, 6]]
     # before averaging.
@@ -827,7 +826,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     for crop_size_mult in range(1, 3):
       crop_size = [3 * crop_size_mult, 2 * crop_size_mult]
       ps_crop_and_pool = ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
       with self.test_session() as sess:
         output = sess.run(ps_crop_and_pool)
@@ -835,24 +834,24 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
 
   def test_position_sensitive_with_equal_channels(self):
     num_spatial_bins = [2, 2]
-    image_shape = [1, 3, 3, 4]
+    image_shape = [3, 3, 4]
     crop_size = [2, 2]
 
     image = tf.constant(range(1, 3 * 3 + 1), dtype=tf.float32,
-                        shape=[1, 3, 3, 1])
-    tiled_image = tf.tile(image, [1, 1, 1, image_shape[3]])
+                        shape=[3, 3, 1])
+    tiled_image = tf.tile(image, [1, 1, image_shape[2]])
     boxes = tf.random_uniform((3, 4))
     box_ind = tf.constant([0, 0, 0], dtype=tf.int32)
 
     # All channels are equal so position-sensitive crop and resize should
     # work as the usual crop and resize for just one channel.
-    crop = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)
+    crop = tf.image.crop_and_resize(tf.expand_dims(image, axis=0), boxes,
+                                    box_ind, crop_size)
     crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
 
     ps_crop_and_pool = ops.position_sensitive_crop_regions(
         tiled_image,
         boxes,
-        box_ind,
         crop_size,
         num_spatial_bins,
         global_pool=True)
@@ -861,78 +860,53 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
       expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))
       self.assertAllClose(output, expected_output)
 
-  def test_position_sensitive_with_single_bin(self):
-    num_spatial_bins = [1, 1]
-    image_shape = [2, 3, 3, 4]
-    crop_size = [2, 2]
-
-    image = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((6, 4))
-    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
-
-    # When a single bin is used, position-sensitive crop and pool should be
-    # the same as non-position sensitive crop and pool.
-    crop = tf.image.crop_and_resize(image, boxes, box_ind, crop_size)
-    crop_and_pool = tf.reduce_mean(crop, [1, 2], keep_dims=True)
-
-    ps_crop_and_pool = ops.position_sensitive_crop_regions(
-        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
-
-    with self.test_session() as sess:
-      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))
-      self.assertAllClose(output, expected_output)
-
   def test_raise_value_error_on_num_bins_less_than_one(self):
     num_spatial_bins = [1, -1]
-    image_shape = [1, 1, 1, 2]
+    image_shape = [1, 1, 2]
     crop_size = [2, 2]
 
     image = tf.constant(1, dtype=tf.float32, shape=image_shape)
     boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
-    box_ind = tf.constant([0], dtype=tf.int32)
 
     with self.assertRaisesRegexp(ValueError, 'num_spatial_bins should be >= 1'):
       ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
   def test_raise_value_error_on_non_divisible_crop_size(self):
     num_spatial_bins = [2, 3]
-    image_shape = [1, 1, 1, 6]
+    image_shape = [1, 1, 6]
     crop_size = [3, 2]
 
     image = tf.constant(1, dtype=tf.float32, shape=image_shape)
     boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
-    box_ind = tf.constant([0], dtype=tf.int32)
 
     with self.assertRaisesRegexp(
         ValueError, 'crop_size should be divisible by num_spatial_bins'):
       ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
   def test_raise_value_error_on_non_divisible_num_channels(self):
     num_spatial_bins = [2, 2]
-    image_shape = [1, 1, 1, 5]
+    image_shape = [1, 1, 5]
     crop_size = [2, 2]
 
     image = tf.constant(1, dtype=tf.float32, shape=image_shape)
     boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
-    box_ind = tf.constant([0], dtype=tf.int32)
 
     with self.assertRaisesRegexp(
         ValueError, 'Dimension size must be evenly divisible by 4 but is 5'):
       ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
+          image, boxes, crop_size, num_spatial_bins, global_pool=True)
 
   def test_position_sensitive_with_global_pool_false(self):
     num_spatial_bins = [3, 2]
-    image_shape = [1, 3, 2, 6]
+    image_shape = [3, 2, 6]
     num_boxes = 2
 
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
     boxes = tf.random_uniform((num_boxes, 4))
-    box_ind = tf.constant([0, 0], dtype=tf.int32)
 
     expected_output = []
 
@@ -956,79 +930,21 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
     for crop_size_mult in range(1, 3):
       crop_size = [3 * crop_size_mult, 2 * crop_size_mult]
       ps_crop = ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)
+          image, boxes, crop_size, num_spatial_bins, global_pool=False)
       with self.test_session() as sess:
         output = sess.run(ps_crop)
 
       self.assertAllEqual(output, expected_output[crop_size_mult - 1])
 
-  def test_position_sensitive_with_global_pool_false_and_known_boxes(self):
-    num_spatial_bins = [2, 2]
-    image_shape = [2, 2, 2, 4]
-    crop_size = [2, 2]
-
-    image = tf.constant(range(1, 2 * 2 * 4  + 1) * 2, dtype=tf.float32,
-                        shape=image_shape)
-
-    # First box contains whole image, and second box contains only first row.
-    boxes = tf.constant(np.array([[0., 0., 1., 1.],
-                                  [0., 0., 0.5, 1.]]), dtype=tf.float32)
-    box_ind = tf.constant([0, 1], dtype=tf.int32)
-
-    expected_output = []
-
-    # Expected output, when the box containing whole image.
-    expected_output.append(
-        np.reshape(np.array([[4, 7],
-                             [10, 13]]),
-                   (1, 2, 2, 1))
-    )
-
-    # Expected output, when the box containing only first row.
-    expected_output.append(
-        np.reshape(np.array([[3, 6],
-                             [7, 10]]),
-                   (1, 2, 2, 1))
-    )
-    expected_output = np.concatenate(expected_output, axis=0)
-
-    ps_crop = ops.position_sensitive_crop_regions(
-        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)
-
-    with self.test_session() as sess:
-      output = sess.run(ps_crop)
-      self.assertAllEqual(output, expected_output)
-
-  def test_position_sensitive_with_global_pool_false_and_single_bin(self):
-    num_spatial_bins = [1, 1]
-    image_shape = [2, 3, 3, 4]
-    crop_size = [1, 1]
-
-    image = tf.random_uniform(image_shape)
-    boxes = tf.random_uniform((6, 4))
-    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
-
-    # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),
-    # the outputs are the same whatever the global_pool value is.
-    ps_crop_and_pool = ops.position_sensitive_crop_regions(
-        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=True)
-    ps_crop = ops.position_sensitive_crop_regions(
-        image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)
-
-    with self.test_session() as sess:
-      pooled_output, unpooled_output = sess.run((ps_crop_and_pool, ps_crop))
-      self.assertAllClose(pooled_output, unpooled_output)
-
   def test_position_sensitive_with_global_pool_false_and_do_global_pool(self):
     num_spatial_bins = [3, 2]
-    image_shape = [1, 3, 2, 6]
+    image_shape = [3, 2, 6]
     num_boxes = 2
 
     # First channel is 1's, second channel is 2's, etc.
     image = tf.constant(range(1, 3 * 2 + 1) * 6, dtype=tf.float32,
                         shape=image_shape)
     boxes = tf.random_uniform((num_boxes, 4))
-    box_ind = tf.constant([0, 0], dtype=tf.int32)
 
     expected_output = []
 
@@ -1059,7 +975,7 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
       # Perform global_pooling after running the function with
       # global_pool=False.
       ps_crop = ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)
+          image, boxes, crop_size, num_spatial_bins, global_pool=False)
       ps_crop_and_pool = tf.reduce_mean(
           ps_crop, reduction_indices=(1, 2), keep_dims=True)
 
@@ -1070,17 +986,99 @@ class OpsTestPositionSensitiveCropRegions(tf.test.TestCase):
 
   def test_raise_value_error_on_non_square_block_size(self):
     num_spatial_bins = [3, 2]
-    image_shape = [1, 3, 2, 6]
+    image_shape = [3, 2, 6]
     crop_size = [6, 2]
 
     image = tf.constant(1, dtype=tf.float32, shape=image_shape)
     boxes = tf.constant([[0, 0, 1, 1]], dtype=tf.float32)
-    box_ind = tf.constant([0], dtype=tf.int32)
 
     with self.assertRaisesRegexp(
         ValueError, 'Only support square bin crop size for now.'):
       ops.position_sensitive_crop_regions(
-          image, boxes, box_ind, crop_size, num_spatial_bins, global_pool=False)
+          image, boxes, crop_size, num_spatial_bins, global_pool=False)
+
+
+class OpsTestBatchPositionSensitiveCropRegions(tf.test.TestCase):
+
+  def test_position_sensitive_with_single_bin(self):
+    num_spatial_bins = [1, 1]
+    image_shape = [2, 3, 3, 4]
+    crop_size = [2, 2]
+
+    image = tf.random_uniform(image_shape)
+    boxes = tf.random_uniform((2, 3, 4))
+    box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
+
+    # When a single bin is used, position-sensitive crop and pool should be
+    # the same as non-position sensitive crop and pool.
+    crop = tf.image.crop_and_resize(image, tf.reshape(boxes, [-1, 4]), box_ind,
+                                    crop_size)
+    crop_and_pool = tf.reduce_mean(crop, [1, 2], keepdims=True)
+    crop_and_pool = tf.reshape(crop_and_pool, [2, 3, 1, 1, 4])
+
+    ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
+        image, boxes, crop_size, num_spatial_bins, global_pool=True)
+
+    with self.test_session() as sess:
+      expected_output, output = sess.run((crop_and_pool, ps_crop_and_pool))
+      self.assertAllClose(output, expected_output)
+
+  def test_position_sensitive_with_global_pool_false_and_known_boxes(self):
+    num_spatial_bins = [2, 2]
+    image_shape = [2, 2, 2, 4]
+    crop_size = [2, 2]
+
+    images = tf.constant(range(1, 2 * 2 * 4  + 1) * 2, dtype=tf.float32,
+                         shape=image_shape)
+
+    # First box contains whole image, and second box contains only first row.
+    boxes = tf.constant(np.array([[[0., 0., 1., 1.]],
+                                  [[0., 0., 0.5, 1.]]]), dtype=tf.float32)
+    # box_ind = tf.constant([0, 1], dtype=tf.int32)
+
+    expected_output = []
+
+    # Expected output, when the box containing whole image.
+    expected_output.append(
+        np.reshape(np.array([[4, 7],
+                             [10, 13]]),
+                   (1, 2, 2, 1))
+    )
+
+    # Expected output, when the box containing only first row.
+    expected_output.append(
+        np.reshape(np.array([[3, 6],
+                             [7, 10]]),
+                   (1, 2, 2, 1))
+    )
+    expected_output = np.stack(expected_output, axis=0)
+
+    ps_crop = ops.batch_position_sensitive_crop_regions(
+        images, boxes, crop_size, num_spatial_bins, global_pool=False)
+
+    with self.test_session() as sess:
+      output = sess.run(ps_crop)
+      self.assertAllEqual(output, expected_output)
+
+  def test_position_sensitive_with_global_pool_false_and_single_bin(self):
+    num_spatial_bins = [1, 1]
+    image_shape = [2, 3, 3, 4]
+    crop_size = [1, 1]
+
+    images = tf.random_uniform(image_shape)
+    boxes = tf.random_uniform((2, 3, 4))
+    # box_ind = tf.constant([0, 0, 0, 1, 1, 1], dtype=tf.int32)
+
+    # Since single_bin is used and crop_size = [1, 1] (i.e., no crop resize),
+    # the outputs are the same whatever the global_pool value is.
+    ps_crop_and_pool = ops.batch_position_sensitive_crop_regions(
+        images, boxes, crop_size, num_spatial_bins, global_pool=True)
+    ps_crop = ops.batch_position_sensitive_crop_regions(
+        images, boxes, crop_size, num_spatial_bins, global_pool=False)
+
+    with self.test_session() as sess:
+      pooled_output, unpooled_output = sess.run((ps_crop_and_pool, ps_crop))
+      self.assertAllClose(pooled_output, unpooled_output)
 
 
 class ReframeBoxMasksToImageMasksTest(tf.test.TestCase):
@@ -1365,5 +1363,86 @@ class OpsTestMatMulCropAndResize(test_case.TestCase):
       _ = ops.matmul_crop_and_resize(image, boxes, crop_size)
 
 
+class OpsTestExpectedClassificationLoss(test_case.TestCase):
+
+  def testExpectedClassificationLossUnderSamplingWithHardLabels(self):
+
+    def graph_fn(batch_cls_targets, cls_losses, negative_to_positive_ratio,
+                 minimum_negative_sampling):
+      return ops.expected_classification_loss_under_sampling(
+          batch_cls_targets, cls_losses, negative_to_positive_ratio,
+          minimum_negative_sampling)
+
+    batch_cls_targets = np.array(
+        [[[1., 0, 0], [0, 1., 0]], [[1., 0, 0], [0, 1., 0]]], dtype=np.float32)
+    cls_losses = np.array([[1, 2], [3, 4]], dtype=np.float32)
+    negative_to_positive_ratio = np.array([2], dtype=np.float32)
+    minimum_negative_sampling = np.array([1], dtype=np.float32)
+
+    classification_loss = self.execute(graph_fn, [
+        batch_cls_targets, cls_losses, negative_to_positive_ratio,
+        minimum_negative_sampling
+    ])
+
+    # expected_foregorund_sum = [1,1]
+    # expected_beta = [2,2]
+    # expected_cls_loss_weights = [2,1],[2,1]
+    # expected_classification_loss_under_sampling = [2*1+1*2, 2*3+1*4]
+    expected_classification_loss_under_sampling = [2 + 2, 6 + 4]
+
+    self.assertAllClose(expected_classification_loss_under_sampling,
+                        classification_loss)
+
+  def testExpectedClassificationLossUnderSamplingWithAllNegative(self):
+
+    def graph_fn(batch_cls_targets, cls_losses):
+      return ops.expected_classification_loss_under_sampling(
+          batch_cls_targets, cls_losses, negative_to_positive_ratio,
+          minimum_negative_sampling)
+
+    batch_cls_targets = np.array(
+        [[[1, 0, 0], [1, 0, 0]], [[1, 0, 0], [1, 0, 0]]], dtype=np.float32)
+    cls_losses = np.array([[1, 2], [3, 4]], dtype=np.float32)
+    negative_to_positive_ratio = np.array([2], dtype=np.float32)
+    minimum_negative_sampling = np.array([1], dtype=np.float32)
+
+    classification_loss = self.execute(graph_fn,
+                                       [batch_cls_targets, cls_losses])
+
+    # expected_foregorund_sum = [0,0]
+    # expected_beta = [0.5,0.5]
+    # expected_cls_loss_weights = [0.5,0.5],[0.5,0.5]
+    # expected_classification_loss_under_sampling = [.5*1+.5*2, .5*3+.5*4]
+    expected_classification_loss_under_sampling = [1.5, 3.5]
+
+    self.assertAllClose(expected_classification_loss_under_sampling,
+                        classification_loss)
+
+  def testExpectedClassificationLossUnderSamplingWithAllPositive(self):
+
+    def graph_fn(batch_cls_targets, cls_losses):
+      return ops.expected_classification_loss_under_sampling(
+          batch_cls_targets, cls_losses, negative_to_positive_ratio,
+          minimum_negative_sampling)
+
+    batch_cls_targets = np.array(
+        [[[0, 1., 0], [0, 1., 0]], [[0, 1, 0], [0, 0, 1]]], dtype=np.float32)
+    cls_losses = np.array([[1, 2], [3, 4]], dtype=np.float32)
+    negative_to_positive_ratio = np.array([2], dtype=np.float32)
+    minimum_negative_sampling = np.array([1], dtype=np.float32)
+
+    classification_loss = self.execute(graph_fn,
+                                       [batch_cls_targets, cls_losses])
+
+    # expected_foregorund_sum = [2,2]
+    # expected_beta = [0,0]
+    # expected_cls_loss_weights = [1,1],[1,1]
+    # expected_classification_loss_under_sampling = [1*1+1*2, 1*3+1*4]
+    expected_classification_loss_under_sampling = [1 + 2, 3 + 4]
+
+    self.assertAllClose(expected_classification_loss_under_sampling,
+                        classification_loss)
+
+
 if __name__ == '__main__':
   tf.test.main()
diff --git a/research/object_detection/utils/shape_utils.py b/research/object_detection/utils/shape_utils.py
index 06f389a8..dfa96e79 100644
--- a/research/object_detection/utils/shape_utils.py
+++ b/research/object_detection/utils/shape_utils.py
@@ -106,13 +106,49 @@ def pad_or_clip_tensor(t, length):
       length is an integer, the first dimension of the processed tensor is set
       to length statically.
   """
-  processed_t = tf.cond(
-      tf.greater(tf.shape(t)[0], length),
-      lambda: clip_tensor(t, length),
-      lambda: pad_tensor(t, length))
-  if not _is_tensor(length):
-    processed_t = _set_dim_0(processed_t, length)
-  return processed_t
+  return pad_or_clip_nd(t, [length] + t.shape.as_list()[1:])
+
+
+def pad_or_clip_nd(tensor, output_shape):
+  """Pad or Clip given tensor to the output shape.
+
+  Args:
+    tensor: Input tensor to pad or clip.
+    output_shape: A list of integers / scalar tensors (or None for dynamic dim)
+      representing the size to pad or clip each dimension of the input tensor.
+
+  Returns:
+    Input tensor padded and clipped to the output shape.
+  """
+  tensor_shape = tf.shape(tensor)
+  clip_size = [
+      tf.where(tensor_shape[i] - shape > 0, shape, -1)
+      if shape is not None else -1 for i, shape in enumerate(output_shape)
+  ]
+  clipped_tensor = tf.slice(
+      tensor,
+      begin=tf.zeros(len(clip_size), dtype=tf.int32),
+      size=clip_size)
+
+  # Pad tensor if the shape of clipped tensor is smaller than the expected
+  # shape.
+  clipped_tensor_shape = tf.shape(clipped_tensor)
+  trailing_paddings = [
+      shape - clipped_tensor_shape[i] if shape is not None else 0
+      for i, shape in enumerate(output_shape)
+  ]
+  paddings = tf.stack(
+      [
+          tf.zeros(len(trailing_paddings), dtype=tf.int32),
+          trailing_paddings
+      ],
+      axis=1)
+  padded_tensor = tf.pad(clipped_tensor, paddings=paddings)
+  output_static_shape = [
+      dim if not isinstance(dim, tf.Tensor) else None for dim in output_shape
+  ]
+  padded_tensor.set_shape(output_static_shape)
+  return padded_tensor
 
 
 def combined_static_and_dynamic_shape(tensor):
@@ -306,4 +342,3 @@ def assert_shape_equal_along_first_dimension(shape_a, shape_b):
     else: return tf.no_op()
   else:
     return tf.assert_equal(shape_a[0], shape_b[0])
-
diff --git a/research/object_detection/utils/shape_utils_test.py b/research/object_detection/utils/shape_utils_test.py
index d4548f04..b2b33456 100644
--- a/research/object_detection/utils/shape_utils_test.py
+++ b/research/object_detection/utils/shape_utils_test.py
@@ -123,6 +123,22 @@ class UtilTest(tf.test.TestCase):
     self.assertTrue(tf.contrib.framework.is_tensor(combined_shape[0]))
     self.assertListEqual(combined_shape[1:], [2, 3])
 
+  def test_pad_or_clip_nd_tensor(self):
+    tensor_placeholder = tf.placeholder(tf.float32, [None, 5, 4, 7])
+    output_tensor = shape_utils.pad_or_clip_nd(
+        tensor_placeholder, [None, 3, 5, tf.constant(6)])
+
+    self.assertAllEqual(output_tensor.shape.as_list(), [None, 3, 5, None])
+
+    with self.test_session() as sess:
+      output_tensor_np = sess.run(
+          output_tensor,
+          feed_dict={
+              tensor_placeholder: np.random.rand(2, 5, 4, 7),
+          })
+
+    self.assertAllEqual(output_tensor_np.shape, [2, 3, 5, 6])
+
 
 class StaticOrDynamicMapFnTest(tf.test.TestCase):
 
diff --git a/research/object_detection/utils/test_case.py b/research/object_detection/utils/test_case.py
index 7f8656cd..5d05a845 100644
--- a/research/object_detection/utils/test_case.py
+++ b/research/object_detection/utils/test_case.py
@@ -47,9 +47,10 @@ class TestCase(tf.test.TestCase):
       materialized_results = sess.run(tpu_computation,
                                       feed_dict=dict(zip(placeholders, inputs)))
       sess.run(tpu.shutdown_system())
-      if (len(materialized_results) == 1
-          and (isinstance(materialized_results, list)
-               or isinstance(materialized_results, tuple))):
+      if (hasattr(materialized_results, '__len__') and
+          len(materialized_results) == 1 and
+          (isinstance(materialized_results, list) or
+           isinstance(materialized_results, tuple))):
         materialized_results = materialized_results[0]
     return materialized_results
 
@@ -72,9 +73,11 @@ class TestCase(tf.test.TestCase):
                 tf.local_variables_initializer()])
       materialized_results = sess.run(results, feed_dict=dict(zip(placeholders,
                                                                   inputs)))
-      if (len(materialized_results) == 1
-          and (isinstance(materialized_results, list)
-               or isinstance(materialized_results, tuple))):
+
+      if (hasattr(materialized_results, '__len__') and
+          len(materialized_results) == 1 and
+          (isinstance(materialized_results, list) or
+           isinstance(materialized_results, tuple))):
         materialized_results = materialized_results[0]
     return materialized_results
 
diff --git a/research/object_detection/utils/test_utils.py b/research/object_detection/utils/test_utils.py
index 2e50419c..fa74c970 100644
--- a/research/object_detection/utils/test_utils.py
+++ b/research/object_detection/utils/test_utils.py
@@ -62,6 +62,30 @@ class MockBoxPredictor(box_predictor.BoxPredictor):
             class_predictions_with_background}
 
 
+class MockKerasBoxPredictor(box_predictor.KerasBoxPredictor):
+  """Simple box predictor that ignores inputs and outputs all zeros."""
+
+  def __init__(self, is_training, num_classes):
+    super(MockKerasBoxPredictor, self).__init__(
+        is_training, num_classes, False, False)
+
+  def _predict(self, image_features, **kwargs):
+    image_feature = image_features[0]
+    combined_feature_shape = shape_utils.combined_static_and_dynamic_shape(
+        image_feature)
+    batch_size = combined_feature_shape[0]
+    num_anchors = (combined_feature_shape[1] * combined_feature_shape[2])
+    code_size = 4
+    zero = tf.reduce_sum(0 * image_feature)
+    box_encodings = zero + tf.zeros(
+        (batch_size, num_anchors, 1, code_size), dtype=tf.float32)
+    class_predictions_with_background = zero + tf.zeros(
+        (batch_size, num_anchors, self.num_classes + 1), dtype=tf.float32)
+    return {box_predictor.BOX_ENCODINGS: box_encodings,
+            box_predictor.CLASS_PREDICTIONS_WITH_BACKGROUND:
+                class_predictions_with_background}
+
+
 class MockAnchorGenerator(anchor_generator.AnchorGenerator):
   """Mock anchor generator."""
 
diff --git a/research/object_detection/utils/variables_helper.py b/research/object_detection/utils/variables_helper.py
index 4f05a31d..141228cd 100644
--- a/research/object_detection/utils/variables_helper.py
+++ b/research/object_detection/utils/variables_helper.py
@@ -134,8 +134,11 @@ def get_variables_available_in_checkpoint(variables,
         vars_in_ckpt[variable_name] = variable
       else:
         logging.warning('Variable [%s] is available in checkpoint, but has an '
-                        'incompatible shape with model variable.',
-                        variable_name)
+                        'incompatible shape with model variable. Checkpoint '
+                        'shape: [%s], model variable shape: [%s]. This '
+                        'variable will not be initialized from the checkpoint.',
+                        variable_name, ckpt_vars_to_shape_map[variable_name],
+                        variable.shape.as_list())
     else:
       logging.warning('Variable [%s] is not available in checkpoint',
                       variable_name)
