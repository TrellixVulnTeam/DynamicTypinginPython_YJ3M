commit 052361ded78c0de8028e4d1242c6fe615509c165
Author: ofirnachum <ofirnachum@gmail.com>
Date:   Wed Dec 5 06:10:39 2018 -0800

    add training code

diff --git a/research/efficient-hrl/README.md b/research/efficient-hrl/README.md
index 101d6552..6ea4918e 100755
--- a/research/efficient-hrl/README.md
+++ b/research/efficient-hrl/README.md
@@ -1,28 +1,62 @@
-Code for performing Hierarchical RL based on
+Code for performing Hierarchical RL based on the following publications:
+
 "Data-Efficient Hierarchical Reinforcement Learning" by
 Ofir Nachum, Shixiang (Shane) Gu, Honglak Lee, and Sergey Levine
 (https://arxiv.org/abs/1805.08296).
 
-
-This library currently includes three of the environments used:
-Ant Maze, Ant Push, and Ant Fall.
-
-The training code is planned to be open-sourced at a later time.
+"Near-Optimal Representation Learning for Hierarchical Reinforcement Learning"
+by Ofir Nachum, Shixiang (Shane) Gu, Honglak Lee, and Sergey Levine
+(https://arxiv.org/abs/1810.01257).
 
 
 Requirements:
 * TensorFlow (see http://www.tensorflow.org for how to install/upgrade)
+* Gin Config (see https://github.com/google/gin-config)
+* Tensorflow Agents (see https://github.com/tensorflow/agents)
 * OpenAI Gym (see http://gym.openai.com/docs, be sure to install MuJoCo as well)
 * NumPy (see http://www.numpy.org/)
 
 
 Quick Start:
 
-Run a random policy on AntMaze (or AntPush, AntFall):
+Run a training job based on the original HIRO paper on Ant Maze:
+
+```
+python scripts/local_train.py test1 hiro_orig ant_maze base_uvf suite
+```
+
+Run a continuous evaluation job for that experiment:
 
 ```
-python environments/__init__.py --env=AntMaze
+python scripts/local_eval.py test1 hiro_orig ant_maze base_uvf suite
 ```
 
+To run the same experiment with online representation learning (the
+"Near-Optimal" paper), change `hiro_orig` to `hiro_repr`.
+You can also run with `hiro_xy` to run the same experiment with HIRO on only the
+xy coordinates of the agent.
+
+To run on other environments, change `ant_maze` to something else; e.g.,
+`ant_push_multi`, `ant_fall_multi`, etc.  See `context/configs/*` for other options.
+
+
+Basic Code Guide:
+
+The code for training resides in train.py.  The code trains a lower-level policy
+(a UVF agent in the code) and a higher-level policy (a MetaAgent in the code)
+concurrently.  The higher-level policy communicates goals to the lower-level
+policy.  In the code, this is called a context.  Not only does the lower-level
+policy act with respect to a context (a higher-level specified goal), but the
+higher-level policy also acts with respect to an environment-specified context
+(corresponding to the navigation target location associated with the task).
+Therefore, in `context/configs/*` you will find both specifications for task setup
+as well as goal configurations.  Most remaining hyperparameters used for
+training/evaluation may be found in `configs/*`.
+
+NOTE: Not all the code corresponding to the "Near-Optimal" paper is included.
+Namely, changes to low-level policy training proposed in the paper (discounting
+and auxiliary rewards) are not implemented here.  Performance should not change
+significantly.
+
 
 Maintained by Ofir Nachum (ofirnachum).
diff --git a/research/efficient-hrl/agent.py b/research/efficient-hrl/agent.py
new file mode 100644
index 00000000..cb02b51f
--- /dev/null
+++ b/research/efficient-hrl/agent.py
@@ -0,0 +1,774 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A UVF agent.
+"""
+
+import tensorflow as tf
+import gin.tf
+from agents import ddpg_agent
+# pylint: disable=unused-import
+import cond_fn
+from utils import utils as uvf_utils
+from context import gin_imports
+# pylint: enable=unused-import
+slim = tf.contrib.slim
+
+
+@gin.configurable
+class UvfAgentCore(object):
+  """Defines basic functions for UVF agent. Must be inherited with an RL agent.
+
+  Used as lower-level agent.
+  """
+
+  def __init__(self,
+               observation_spec,
+               action_spec,
+               tf_env,
+               tf_context,
+               step_cond_fn=cond_fn.env_transition,
+               reset_episode_cond_fn=cond_fn.env_restart,
+               reset_env_cond_fn=cond_fn.false_fn,
+               metrics=None,
+               **base_agent_kwargs):
+    """Constructs a UVF agent.
+
+    Args:
+      observation_spec: A TensorSpec defining the observations.
+      action_spec: A BoundedTensorSpec defining the actions.
+      tf_env: A Tensorflow environment object.
+      tf_context: A Context class.
+      step_cond_fn: A function indicating whether to increment the num of steps.
+      reset_episode_cond_fn: A function indicating whether to restart the
+      episode, resampling the context.
+      reset_env_cond_fn: A function indicating whether to perform a manual reset
+      of the environment.
+      metrics: A list of functions that evaluate metrics of the agent.
+      **base_agent_kwargs: A dictionary of parameters for base RL Agent.
+    Raises:
+      ValueError: If 'dqda_clipping' is < 0.
+    """
+    self._step_cond_fn = step_cond_fn
+    self._reset_episode_cond_fn = reset_episode_cond_fn
+    self._reset_env_cond_fn = reset_env_cond_fn
+    self.metrics = metrics
+
+    # expose tf_context methods
+    self.tf_context = tf_context(tf_env=tf_env)
+    self.set_replay = self.tf_context.set_replay
+    self.sample_contexts = self.tf_context.sample_contexts
+    self.compute_rewards = self.tf_context.compute_rewards
+    self.gamma_index = self.tf_context.gamma_index
+    self.context_specs = self.tf_context.context_specs
+    self.context_as_action_specs = self.tf_context.context_as_action_specs
+    self.init_context_vars = self.tf_context.create_vars
+
+    self.env_observation_spec = observation_spec[0]
+    merged_observation_spec = (uvf_utils.merge_specs(
+        (self.env_observation_spec,) + self.context_specs),)
+    self._context_vars = dict()
+    self._action_vars = dict()
+
+    self.BASE_AGENT_CLASS.__init__(
+        self,
+        observation_spec=merged_observation_spec,
+        action_spec=action_spec,
+        **base_agent_kwargs
+    )
+
+  def set_meta_agent(self, agent=None):
+    self._meta_agent = agent
+
+  @property
+  def meta_agent(self):
+    return self._meta_agent
+
+  def actor_loss(self, states, actions, rewards, discounts,
+                 next_states):
+    """Returns the next action for the state.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+      context: A list of [num_context_dims] tensor representing a context.
+    Returns:
+      A [num_action_dims] tensor representing the action.
+    """
+    return self.BASE_AGENT_CLASS.actor_loss(self, states)
+
+  def action(self, state, context=None):
+    """Returns the next action for the state.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+      context: A list of [num_context_dims] tensor representing a context.
+    Returns:
+      A [num_action_dims] tensor representing the action.
+    """
+    merged_state = self.merged_state(state, context)
+    return self.BASE_AGENT_CLASS.action(self, merged_state)
+
+  def actions(self, state, context=None):
+    """Returns the next action for the state.
+
+    Args:
+      state: A [-1, num_state_dims] tensor representing a state.
+      context: A list of [-1, num_context_dims] tensor representing a context.
+    Returns:
+      A [-1, num_action_dims] tensor representing the action.
+    """
+    merged_states = self.merged_states(state, context)
+    return self.BASE_AGENT_CLASS.actor_net(self, merged_states)
+
+  def log_probs(self, states, actions, state_reprs, contexts=None):
+    assert contexts is not None
+    batch_dims = [tf.shape(states)[0], tf.shape(states)[1]]
+    contexts = self.tf_context.context_multi_transition_fn(
+        contexts, states=tf.to_float(state_reprs))
+
+    flat_states = tf.reshape(states,
+                             [batch_dims[0] * batch_dims[1], states.shape[-1]])
+    flat_contexts = [tf.reshape(tf.cast(context, states.dtype),
+                                [batch_dims[0] * batch_dims[1], context.shape[-1]])
+                     for context in contexts]
+    flat_pred_actions = self.actions(flat_states, flat_contexts)
+    pred_actions = tf.reshape(flat_pred_actions,
+                              batch_dims + [flat_pred_actions.shape[-1]])
+
+    error = tf.square(actions - pred_actions)
+    spec_range = (self._action_spec.maximum - self._action_spec.minimum) / 2
+    normalized_error = error / tf.constant(spec_range) ** 2
+    return -normalized_error
+
+  @gin.configurable('uvf_add_noise_fn')
+  def add_noise_fn(self, action_fn, stddev=1.0, debug=False,
+                   clip=True, global_step=None):
+    """Returns the action_fn with additive Gaussian noise.
+
+    Args:
+      action_fn: A callable(`state`, `context`) which returns a
+        [num_action_dims] tensor representing a action.
+      stddev: stddev for the Ornstein-Uhlenbeck noise.
+      debug: Print debug messages.
+    Returns:
+      A [num_action_dims] action tensor.
+    """
+    if global_step is not None:
+      stddev *= tf.maximum(  # Decay exploration during training.
+          tf.train.exponential_decay(1.0, global_step, 1e6, 0.8), 0.5)
+    def noisy_action_fn(state, context=None):
+      """Noisy action fn."""
+      action = action_fn(state, context)
+      if debug:
+        action = uvf_utils.tf_print(
+            action, [action],
+            message='[add_noise_fn] pre-noise action',
+            first_n=100)
+      noise_dist = tf.distributions.Normal(tf.zeros_like(action),
+                                           tf.ones_like(action) * stddev)
+      noise = noise_dist.sample()
+      action += noise
+      if debug:
+        action = uvf_utils.tf_print(
+            action, [action],
+            message='[add_noise_fn] post-noise action',
+            first_n=100)
+      if clip:
+        action = uvf_utils.clip_to_spec(action, self._action_spec)
+      return action
+    return noisy_action_fn
+
+  def merged_state(self, state, context=None):
+    """Returns the merged state from the environment state and contexts.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+      context: A list of [num_context_dims] tensor representing a context.
+        If None, use the internal context.
+    Returns:
+      A [num_merged_state_dims] tensor representing the merged state.
+    """
+    if context is None:
+      context = list(self.context_vars)
+    state = tf.concat([state,] + context, axis=-1)
+    self._validate_states(self._batch_state(state))
+    return state
+
+  def merged_states(self, states, contexts=None):
+    """Returns the batch merged state from the batch env state and contexts.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      contexts: A list of [batch_size, num_context_dims] tensor
+        representing a batch of contexts. If None,
+        use the internal context.
+    Returns:
+      A [batch_size, num_merged_state_dims] tensor representing the batch
+        of merged states.
+    """
+    if contexts is None:
+      contexts = [tf.tile(tf.expand_dims(context, axis=0),
+                          (tf.shape(states)[0], 1)) for
+                  context in self.context_vars]
+    states = tf.concat([states,] + contexts, axis=-1)
+    self._validate_states(states)
+    return states
+
+  def unmerged_states(self, merged_states):
+    """Returns the batch state and contexts from the batch merged state.
+
+    Args:
+      merged_states: A [batch_size, num_merged_state_dims] tensor
+        representing a batch of merged states.
+    Returns:
+      A [batch_size, num_state_dims] tensor and a list of
+        [batch_size, num_context_dims] tensors representing the batch state
+        and contexts respectively.
+    """
+    self._validate_states(merged_states)
+    num_state_dims = self.env_observation_spec.shape.as_list()[0]
+    num_context_dims_list = [c.shape.as_list()[0] for c in self.context_specs]
+    states = merged_states[:, :num_state_dims]
+    contexts = []
+    i = num_state_dims
+    for num_context_dims in num_context_dims_list:
+      contexts.append(merged_states[:, i: i+num_context_dims])
+      i += num_context_dims
+    return states, contexts
+
+  def sample_random_actions(self, batch_size=1):
+    """Return random actions.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      A [batch_size, num_action_dims] tensor representing the batch of actions.
+    """
+    actions = tf.concat(
+        [
+            tf.random_uniform(
+                shape=(batch_size, 1),
+                minval=self._action_spec.minimum[i],
+                maxval=self._action_spec.maximum[i])
+            for i in range(self._action_spec.shape[0].value)
+        ],
+        axis=1)
+    return actions
+
+  def clip_actions(self, actions):
+    """Clip actions to spec.
+
+    Args:
+      actions: A [batch_size, num_action_dims] tensor representing
+      the batch of actions.
+    Returns:
+      A [batch_size, num_action_dims] tensor representing the batch
+      of clipped actions.
+    """
+    actions = tf.concat(
+        [
+            tf.clip_by_value(
+                actions[:, i:i+1],
+                self._action_spec.minimum[i],
+                self._action_spec.maximum[i])
+            for i in range(self._action_spec.shape[0].value)
+        ],
+        axis=1)
+    return actions
+
+  def mix_contexts(self, contexts, insert_contexts, indices):
+    """Mix two contexts based on indices.
+
+    Args:
+      contexts: A list of [batch_size, num_context_dims] tensor representing
+      the batch of contexts.
+      insert_contexts: A list of [batch_size, num_context_dims] tensor
+      representing the batch of contexts to be inserted.
+      indices: A list of a list of integers denoting indices to replace.
+    Returns:
+      A list of resulting contexts.
+    """
+    if indices is None: indices = [[]] * len(contexts)
+    assert len(contexts) == len(indices)
+    assert all([spec.shape.ndims == 1 for spec in self.context_specs])
+    mix_contexts = []
+    for contexts_, insert_contexts_, indices_, spec in zip(
+        contexts, insert_contexts, indices, self.context_specs):
+      mix_contexts.append(
+          tf.concat(
+              [
+                  insert_contexts_[:, i:i + 1] if i in indices_ else
+                  contexts_[:, i:i + 1] for i in range(spec.shape.as_list()[0])
+              ],
+              axis=1))
+    return mix_contexts
+
+  def begin_episode_ops(self, mode, action_fn=None, state=None):
+    """Returns ops that reset agent at beginning of episodes.
+
+    Args:
+      mode: a string representing the mode=[train, explore, eval].
+    Returns:
+      A list of ops.
+    """
+    all_ops = []
+    for _, action_var in sorted(self._action_vars.items()):
+      sample_action = self.sample_random_actions(1)[0]
+      all_ops.append(tf.assign(action_var, sample_action))
+    all_ops += self.tf_context.reset(mode=mode, agent=self._meta_agent,
+                                     action_fn=action_fn, state=state)
+    return all_ops
+
+  def cond_begin_episode_op(self, cond, input_vars, mode, meta_action_fn):
+    """Returns op that resets agent at beginning of episodes.
+
+    A new episode is begun if the cond op evalues to `False`.
+
+    Args:
+      cond: a Boolean tensor variable.
+      input_vars: A list of tensor variables.
+      mode: a string representing the mode=[train, explore, eval].
+    Returns:
+      Conditional begin op.
+    """
+    (state, action, reward, next_state,
+     state_repr, next_state_repr) = input_vars
+    def continue_fn():
+      """Continue op fn."""
+      items = [state, action, reward, next_state,
+               state_repr, next_state_repr] + list(self.context_vars)
+      batch_items = [tf.expand_dims(item, 0) for item in items]
+      (states, actions, rewards, next_states,
+       state_reprs, next_state_reprs) = batch_items[:6]
+      context_reward = self.compute_rewards(
+          mode, state_reprs, actions, rewards, next_state_reprs,
+          batch_items[6:])[0][0]
+      context_reward = tf.cast(context_reward, dtype=reward.dtype)
+      if self.meta_agent is not None:
+        meta_action = tf.concat(self.context_vars, -1)
+        items = [state, meta_action, reward, next_state,
+                 state_repr, next_state_repr] + list(self.meta_agent.context_vars)
+        batch_items = [tf.expand_dims(item, 0) for item in items]
+        (states, meta_actions, rewards, next_states,
+         state_reprs, next_state_reprs) = batch_items[:6]
+        meta_reward = self.meta_agent.compute_rewards(
+            mode, states, meta_actions, rewards,
+            next_states, batch_items[6:])[0][0]
+        meta_reward = tf.cast(meta_reward, dtype=reward.dtype)
+      else:
+        meta_reward = tf.constant(0, dtype=reward.dtype)
+
+      with tf.control_dependencies([context_reward, meta_reward]):
+        step_ops = self.tf_context.step(mode=mode, agent=self._meta_agent,
+                                        state=state,
+                                        next_state=next_state,
+                                        state_repr=state_repr,
+                                        next_state_repr=next_state_repr,
+                                        action_fn=meta_action_fn)
+      with tf.control_dependencies(step_ops):
+        context_reward, meta_reward = map(tf.identity, [context_reward, meta_reward])
+      return context_reward, meta_reward
+    def begin_episode_fn():
+      """Begin op fn."""
+      begin_ops = self.begin_episode_ops(mode=mode, action_fn=meta_action_fn, state=state)
+      with tf.control_dependencies(begin_ops):
+        return tf.zeros_like(reward), tf.zeros_like(reward)
+    with tf.control_dependencies(input_vars):
+      cond_begin_episode_op = tf.cond(cond, continue_fn, begin_episode_fn)
+    return cond_begin_episode_op
+
+  def get_env_base_wrapper(self, env_base, **begin_kwargs):
+    """Create a wrapper around env_base, with agent-specific begin/end_episode.
+
+    Args:
+      env_base: A python environment base.
+      **begin_kwargs: Keyword args for begin_episode_ops.
+    Returns:
+      An object with begin_episode() and end_episode().
+    """
+    begin_ops = self.begin_episode_ops(**begin_kwargs)
+    return uvf_utils.get_contextual_env_base(env_base, begin_ops)
+
+  def init_action_vars(self, name, i=None):
+    """Create and return a tensorflow Variable holding an action.
+
+    Args:
+      name: Name of the variables.
+      i: Integer id.
+    Returns:
+      A [num_action_dims] tensor.
+    """
+    if i is not None:
+      name += '_%d' % i
+    assert name not in self._action_vars, ('Conflict! %s is already '
+                                           'initialized.') % name
+    self._action_vars[name] = tf.Variable(
+        self.sample_random_actions(1)[0], name='%s_action' % (name))
+    self._validate_actions(tf.expand_dims(self._action_vars[name], 0))
+    return self._action_vars[name]
+
+  @gin.configurable('uvf_critic_function')
+  def critic_function(self, critic_vals, states, critic_fn=None):
+    """Computes q values based on outputs from the critic net.
+
+    Args:
+      critic_vals: A tf.float32 [batch_size, ...] tensor representing outputs
+        from the critic net.
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      critic_fn: A callable that process outputs from critic_net and
+        outputs a [batch_size] tensor representing q values.
+    Returns:
+      A tf.float32 [batch_size] tensor representing q values.
+    """
+    if critic_fn is not None:
+      env_states, contexts = self.unmerged_states(states)
+      critic_vals = critic_fn(critic_vals, env_states, contexts)
+    critic_vals.shape.assert_has_rank(1)
+    return critic_vals
+
+  def get_action_vars(self, key):
+    return self._action_vars[key]
+
+  def get_context_vars(self, key):
+    return self.tf_context.context_vars[key]
+
+  def step_cond_fn(self, *args):
+    return self._step_cond_fn(self, *args)
+
+  def reset_episode_cond_fn(self, *args):
+    return self._reset_episode_cond_fn(self, *args)
+
+  def reset_env_cond_fn(self, *args):
+    return self._reset_env_cond_fn(self, *args)
+
+  @property
+  def context_vars(self):
+    return self.tf_context.vars
+
+
+@gin.configurable
+class MetaAgentCore(UvfAgentCore):
+  """Defines basic functions for UVF Meta-agent. Must be inherited with an RL agent.
+
+  Used as higher-level agent.
+  """
+
+  def __init__(self,
+               observation_spec,
+               action_spec,
+               tf_env,
+               tf_context,
+               sub_context,
+               step_cond_fn=cond_fn.env_transition,
+               reset_episode_cond_fn=cond_fn.env_restart,
+               reset_env_cond_fn=cond_fn.false_fn,
+               metrics=None,
+               actions_reg=0.,
+               k=2,
+               **base_agent_kwargs):
+    """Constructs a Meta agent.
+
+    Args:
+      observation_spec: A TensorSpec defining the observations.
+      action_spec: A BoundedTensorSpec defining the actions.
+      tf_env: A Tensorflow environment object.
+      tf_context: A Context class.
+      step_cond_fn: A function indicating whether to increment the num of steps.
+      reset_episode_cond_fn: A function indicating whether to restart the
+      episode, resampling the context.
+      reset_env_cond_fn: A function indicating whether to perform a manual reset
+      of the environment.
+      metrics: A list of functions that evaluate metrics of the agent.
+      **base_agent_kwargs: A dictionary of parameters for base RL Agent.
+    Raises:
+      ValueError: If 'dqda_clipping' is < 0.
+    """
+    self._step_cond_fn = step_cond_fn
+    self._reset_episode_cond_fn = reset_episode_cond_fn
+    self._reset_env_cond_fn = reset_env_cond_fn
+    self.metrics = metrics
+    self._actions_reg = actions_reg
+    self._k = k
+
+    # expose tf_context methods
+    self.tf_context = tf_context(tf_env=tf_env)
+    self.sub_context = sub_context(tf_env=tf_env)
+    self.set_replay = self.tf_context.set_replay
+    self.sample_contexts = self.tf_context.sample_contexts
+    self.compute_rewards = self.tf_context.compute_rewards
+    self.gamma_index = self.tf_context.gamma_index
+    self.context_specs = self.tf_context.context_specs
+    self.context_as_action_specs = self.tf_context.context_as_action_specs
+    self.sub_context_as_action_specs = self.sub_context.context_as_action_specs
+    self.init_context_vars = self.tf_context.create_vars
+
+    self.env_observation_spec = observation_spec[0]
+    merged_observation_spec = (uvf_utils.merge_specs(
+        (self.env_observation_spec,) + self.context_specs),)
+    self._context_vars = dict()
+    self._action_vars = dict()
+
+    assert len(self.context_as_action_specs) == 1
+    self.BASE_AGENT_CLASS.__init__(
+        self,
+        observation_spec=merged_observation_spec,
+        action_spec=self.sub_context_as_action_specs,
+        **base_agent_kwargs
+    )
+
+  @gin.configurable('meta_add_noise_fn')
+  def add_noise_fn(self, action_fn, stddev=1.0, debug=False,
+                   global_step=None):
+    noisy_action_fn = super(MetaAgentCore, self).add_noise_fn(
+        action_fn, stddev,
+        clip=True, global_step=global_step)
+    return noisy_action_fn
+
+  def actor_loss(self, states, actions, rewards, discounts,
+                 next_states):
+    """Returns the next action for the state.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+      context: A list of [num_context_dims] tensor representing a context.
+    Returns:
+      A [num_action_dims] tensor representing the action.
+    """
+    actions = self.actor_net(states, stop_gradients=False)
+    regularizer = self._actions_reg * tf.reduce_mean(
+        tf.reduce_sum(tf.abs(actions[:, self._k:]), -1), 0)
+    loss = self.BASE_AGENT_CLASS.actor_loss(self, states)
+    return regularizer + loss
+
+
+@gin.configurable
+class UvfAgent(UvfAgentCore, ddpg_agent.TD3Agent):
+  """A DDPG agent with UVF.
+  """
+  BASE_AGENT_CLASS = ddpg_agent.TD3Agent
+  ACTION_TYPE = 'continuous'
+
+  def __init__(self, *args, **kwargs):
+    UvfAgentCore.__init__(self, *args, **kwargs)
+
+
+@gin.configurable
+class MetaAgent(MetaAgentCore, ddpg_agent.TD3Agent):
+  """A DDPG meta-agent.
+  """
+  BASE_AGENT_CLASS = ddpg_agent.TD3Agent
+  ACTION_TYPE = 'continuous'
+
+  def __init__(self, *args, **kwargs):
+    MetaAgentCore.__init__(self, *args, **kwargs)
+
+
+@gin.configurable()
+def state_preprocess_net(
+    states,
+    num_output_dims=2,
+    states_hidden_layers=(100,),
+    normalizer_fn=None,
+    activation_fn=tf.nn.relu,
+    zero_time=True,
+    images=False):
+  """Creates a simple feed forward net for embedding states.
+  """
+  with slim.arg_scope(
+      [slim.fully_connected],
+      activation_fn=activation_fn,
+      normalizer_fn=normalizer_fn,
+      weights_initializer=slim.variance_scaling_initializer(
+          factor=1.0/3.0, mode='FAN_IN', uniform=True)):
+
+    states_shape = tf.shape(states)
+    states_dtype = states.dtype
+    states = tf.to_float(states)
+    if images:  # Zero-out x-y
+      states *= tf.constant([0.] * 2 + [1.] * (states.shape[-1] - 2), dtype=states.dtype)
+    if zero_time:
+      states *= tf.constant([1.] * (states.shape[-1] - 1) + [0.], dtype=states.dtype)
+    orig_states = states
+    embed = states
+    if states_hidden_layers:
+      embed = slim.stack(embed, slim.fully_connected, states_hidden_layers,
+                         scope='states')
+
+    with slim.arg_scope([slim.fully_connected],
+                        weights_regularizer=None,
+                        weights_initializer=tf.random_uniform_initializer(
+                            minval=-0.003, maxval=0.003)):
+      embed = slim.fully_connected(embed, num_output_dims,
+                                   activation_fn=None,
+                                   normalizer_fn=None,
+                                   scope='value')
+
+    output = embed
+    output = tf.cast(output, states_dtype)
+    return output
+
+
+@gin.configurable()
+def action_embed_net(
+    actions,
+    states=None,
+    num_output_dims=2,
+    hidden_layers=(400, 300),
+    normalizer_fn=None,
+    activation_fn=tf.nn.relu,
+    zero_time=True,
+    images=False):
+  """Creates a simple feed forward net for embedding actions.
+  """
+  with slim.arg_scope(
+      [slim.fully_connected],
+      activation_fn=activation_fn,
+      normalizer_fn=normalizer_fn,
+      weights_initializer=slim.variance_scaling_initializer(
+          factor=1.0/3.0, mode='FAN_IN', uniform=True)):
+
+    actions = tf.to_float(actions)
+    if states is not None:
+      if images:  # Zero-out x-y
+        states *= tf.constant([0.] * 2 + [1.] * (states.shape[-1] - 2), dtype=states.dtype)
+      if zero_time:
+        states *= tf.constant([1.] * (states.shape[-1] - 1) + [0.], dtype=states.dtype)
+      actions = tf.concat([actions, tf.to_float(states)], -1)
+
+    embed = actions
+    if hidden_layers:
+      embed = slim.stack(embed, slim.fully_connected, hidden_layers,
+                         scope='hidden')
+
+    with slim.arg_scope([slim.fully_connected],
+                        weights_regularizer=None,
+                        weights_initializer=tf.random_uniform_initializer(
+                            minval=-0.003, maxval=0.003)):
+      embed = slim.fully_connected(embed, num_output_dims,
+                                   activation_fn=None,
+                                   normalizer_fn=None,
+                                   scope='value')
+      if num_output_dims == 1:
+        return embed[:, 0, ...]
+      else:
+        return embed
+
+
+def huber(x, kappa=0.1):
+  return (0.5 * tf.square(x) * tf.to_float(tf.abs(x) <= kappa) +
+          kappa * (tf.abs(x) - 0.5 * kappa) * tf.to_float(tf.abs(x) > kappa)
+          ) / kappa
+
+
+@gin.configurable()
+class StatePreprocess(object):
+  STATE_PREPROCESS_NET_SCOPE = 'state_process_net'
+  ACTION_EMBED_NET_SCOPE = 'action_embed_net'
+
+  def __init__(self, trainable=False,
+               state_preprocess_net=lambda states: states,
+               action_embed_net=lambda actions, *args, **kwargs: actions,
+               ndims=None):
+    self.trainable = trainable
+    self._scope = tf.get_variable_scope().name
+    self._ndims = ndims
+    self._state_preprocess_net = tf.make_template(
+        self.STATE_PREPROCESS_NET_SCOPE, state_preprocess_net,
+        create_scope_now_=True)
+    self._action_embed_net = tf.make_template(
+        self.ACTION_EMBED_NET_SCOPE, action_embed_net,
+        create_scope_now_=True)
+
+  def __call__(self, states):
+    batched = states.get_shape().ndims != 1
+    if not batched:
+      states = tf.expand_dims(states, 0)
+    embedded = self._state_preprocess_net(states)
+    if self._ndims is not None:
+      embedded = embedded[..., :self._ndims]
+    if not batched:
+      return embedded[0]
+    return embedded
+
+  def loss(self, states, next_states, low_actions, low_states):
+    batch_size = tf.shape(states)[0]
+    d = int(low_states.shape[1])
+    # Sample indices into meta-transition to train on.
+    probs = 0.99 ** tf.range(d, dtype=tf.float32)
+    probs *= tf.constant([1.0] * (d - 1) + [1.0 / (1 - 0.99)],
+                         dtype=tf.float32)
+    probs /= tf.reduce_sum(probs)
+    index_dist = tf.distributions.Categorical(probs=probs, dtype=tf.int64)
+    indices = index_dist.sample(batch_size)
+    batch_size = tf.cast(batch_size, tf.int64)
+    next_indices = tf.concat(
+        [tf.range(batch_size, dtype=tf.int64)[:, None],
+         (1 + indices[:, None]) % d], -1)
+    new_next_states = tf.where(indices < d - 1,
+                               tf.gather_nd(low_states, next_indices),
+                               next_states)
+    next_states = new_next_states
+
+    embed1 = tf.to_float(self._state_preprocess_net(states))
+    embed2 = tf.to_float(self._state_preprocess_net(next_states))
+    action_embed = self._action_embed_net(
+        tf.layers.flatten(low_actions), states=states)
+
+    tau = 2.0
+    fn = lambda z: tau * tf.reduce_sum(huber(z), -1)
+    all_embed = tf.get_variable('all_embed', [1024, int(embed1.shape[-1])],
+                                initializer=tf.zeros_initializer())
+    upd = all_embed.assign(tf.concat([all_embed[batch_size:], embed2], 0))
+    with tf.control_dependencies([upd]):
+      close = 1 * tf.reduce_mean(fn(embed1 + action_embed - embed2))
+      prior_log_probs = tf.reduce_logsumexp(
+          -fn((embed1 + action_embed)[:, None, :] - all_embed[None, :, :]),
+          axis=-1) - tf.log(tf.to_float(all_embed.shape[0]))
+      far = tf.reduce_mean(tf.exp(-fn((embed1 + action_embed)[1:] - embed2[:-1])
+                                  - tf.stop_gradient(prior_log_probs[1:])))
+      repr_log_probs = tf.stop_gradient(
+          -fn(embed1 + action_embed - embed2) - prior_log_probs) / tau
+    return close + far, repr_log_probs, indices
+
+  def get_trainable_vars(self):
+    return (
+        slim.get_trainable_variables(
+            uvf_utils.join_scope(self._scope, self.STATE_PREPROCESS_NET_SCOPE)) +
+        slim.get_trainable_variables(
+            uvf_utils.join_scope(self._scope, self.ACTION_EMBED_NET_SCOPE)))
+
+
+@gin.configurable()
+class InverseDynamics(object):
+  INVERSE_DYNAMICS_NET_SCOPE = 'inverse_dynamics'
+
+  def __init__(self, spec):
+    self._spec = spec
+
+  def sample(self, states, next_states, num_samples, orig_goals, sc=0.5):
+    goal_dim = orig_goals.shape[-1]
+    spec_range = (self._spec.maximum - self._spec.minimum) / 2 * tf.ones([goal_dim])
+    loc = tf.cast(next_states - states, tf.float32)[:, :goal_dim]
+    scale = sc * tf.tile(tf.reshape(spec_range, [1, goal_dim]),
+                         [tf.shape(states)[0], 1])
+    dist = tf.distributions.Normal(loc, scale)
+    if num_samples == 1:
+      return dist.sample()
+    samples = tf.concat([dist.sample(num_samples - 2),
+                         tf.expand_dims(loc, 0),
+                         tf.expand_dims(orig_goals, 0)], 0)
+    return uvf_utils.clip_to_spec(samples, self._spec)
diff --git a/research/efficient-hrl/agents/__init__.py b/research/efficient-hrl/agents/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/research/efficient-hrl/agents/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/efficient-hrl/agents/circular_buffer.py b/research/efficient-hrl/agents/circular_buffer.py
new file mode 100644
index 00000000..17b6304b
--- /dev/null
+++ b/research/efficient-hrl/agents/circular_buffer.py
@@ -0,0 +1,289 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A circular buffer where each element is a list of tensors.
+
+Each element of the buffer is a list of tensors. An example use case is a replay
+buffer in reinforcement learning, where each element is a list of tensors
+representing the state, action, reward etc.
+
+New elements are added sequentially, and once the buffer is full, we
+start overwriting them in a circular fashion. Reading does not remove any
+elements, only adding new elements does.
+"""
+
+import collections
+import numpy as np
+import tensorflow as tf
+
+import gin.tf
+
+
+@gin.configurable
+class CircularBuffer(object):
+  """A circular buffer where each element is a list of tensors."""
+
+  def __init__(self, buffer_size=1000, scope='replay_buffer'):
+    """Circular buffer of list of tensors.
+
+    Args:
+      buffer_size: (integer) maximum number of tensor lists the buffer can hold.
+      scope: (string) variable scope for creating the variables.
+    """
+    self._buffer_size = np.int64(buffer_size)
+    self._scope = scope
+    self._tensors = collections.OrderedDict()
+    with tf.variable_scope(self._scope):
+      self._num_adds = tf.Variable(0, dtype=tf.int64, name='num_adds')
+    self._num_adds_cs = tf.contrib.framework.CriticalSection(name='num_adds')
+
+  @property
+  def buffer_size(self):
+    return self._buffer_size
+
+  @property
+  def scope(self):
+    return self._scope
+
+  @property
+  def num_adds(self):
+    return self._num_adds
+
+  def _create_variables(self, tensors):
+    with tf.variable_scope(self._scope):
+      for name in tensors.keys():
+        tensor = tensors[name]
+        self._tensors[name] = tf.get_variable(
+            name='BufferVariable_' + name,
+            shape=[self._buffer_size] + tensor.get_shape().as_list(),
+            dtype=tensor.dtype,
+            trainable=False)
+
+  def _validate(self, tensors):
+    """Validate shapes of tensors."""
+    if len(tensors) != len(self._tensors):
+      raise ValueError('Expected tensors to have %d elements. Received %d '
+                       'instead.' % (len(self._tensors), len(tensors)))
+    if self._tensors.keys() != tensors.keys():
+      raise ValueError('The keys of tensors should be the always the same.'
+                       'Received %s instead %s.' %
+                       (tensors.keys(), self._tensors.keys()))
+    for name, tensor in tensors.items():
+      if tensor.get_shape().as_list() != self._tensors[
+          name].get_shape().as_list()[1:]:
+        raise ValueError('Tensor %s has incorrect shape.' % name)
+      if not tensor.dtype.is_compatible_with(self._tensors[name].dtype):
+        raise ValueError(
+            'Tensor %s has incorrect data type. Expected %s, received %s' %
+            (name, self._tensors[name].read_value().dtype, tensor.dtype))
+
+  def add(self, tensors):
+    """Adds an element (list/tuple/dict of tensors) to the buffer.
+
+    Args:
+      tensors: (list/tuple/dict of tensors) to be added to the buffer.
+    Returns:
+      An add operation that adds the input `tensors` to the buffer. Similar to
+        an enqueue_op.
+    Raises:
+      ValueError: If the shapes and data types of input `tensors' are not the
+        same across calls to the add function.
+    """
+    return self.maybe_add(tensors, True)
+
+  def maybe_add(self, tensors, condition):
+    """Adds an element (tensors) to the buffer based on the condition..
+
+    Args:
+      tensors: (list/tuple of tensors) to be added to the buffer.
+      condition: A boolean Tensor controlling whether the tensors would be added
+        to the buffer or not.
+    Returns:
+      An add operation that adds the input `tensors` to the buffer. Similar to
+        an maybe_enqueue_op.
+    Raises:
+      ValueError: If the shapes and data types of input `tensors' are not the
+        same across calls to the add function.
+    """
+    if not isinstance(tensors, dict):
+      names = [str(i) for i in range(len(tensors))]
+      tensors = collections.OrderedDict(zip(names, tensors))
+    if not isinstance(tensors, collections.OrderedDict):
+      tensors = collections.OrderedDict(
+          sorted(tensors.items(), key=lambda t: t[0]))
+    if not self._tensors:
+      self._create_variables(tensors)
+    else:
+      self._validate(tensors)
+
+    #@tf.critical_section(self._position_mutex)
+    def _increment_num_adds():
+      # Adding 0 to the num_adds variable is a trick to read the value of the
+      # variable and return a read-only tensor. Doing this in a critical
+      # section allows us to capture a snapshot of the variable that will
+      # not be affected by other threads updating num_adds.
+      return self._num_adds.assign_add(1) + 0
+    def _add():
+      num_adds_inc = self._num_adds_cs.execute(_increment_num_adds)
+      current_pos = tf.mod(num_adds_inc - 1, self._buffer_size)
+      update_ops = []
+      for name in self._tensors.keys():
+        update_ops.append(
+            tf.scatter_update(self._tensors[name], current_pos, tensors[name]))
+      return tf.group(*update_ops)
+
+    return tf.contrib.framework.smart_cond(condition, _add, tf.no_op)
+
+  def get_random_batch(self, batch_size, keys=None, num_steps=1):
+    """Samples a batch of tensors from the buffer with replacement.
+
+    Args:
+      batch_size: (integer) number of elements to sample.
+      keys: List of keys of tensors to retrieve. If None retrieve all.
+      num_steps: (integer) length of trajectories to return. If > 1 will return
+        a list of lists, where each internal list represents a trajectory of
+        length num_steps.
+    Returns:
+      A list of tensors, where each element in the list is a batch sampled from
+        one of the tensors in the buffer.
+    Raises:
+      ValueError: If get_random_batch is called before calling the add function.
+      tf.errors.InvalidArgumentError: If this operation is executed before any
+        items are added to the buffer.
+    """
+    if not self._tensors:
+      raise ValueError('The add function must be called before get_random_batch.')
+    if keys is None:
+      keys = self._tensors.keys()
+
+    latest_start_index = self.get_num_adds() - num_steps + 1
+    empty_buffer_assert = tf.Assert(
+        tf.greater(latest_start_index, 0),
+        ['Not enough elements have been added to the buffer.'])
+    with tf.control_dependencies([empty_buffer_assert]):
+      max_index = tf.minimum(self._buffer_size, latest_start_index)
+      indices = tf.random_uniform(
+          [batch_size],
+          minval=0,
+          maxval=max_index,
+          dtype=tf.int64)
+      if num_steps == 1:
+        return self.gather(indices, keys)
+      else:
+        return self.gather_nstep(num_steps, indices, keys)
+
+  def gather(self, indices, keys=None):
+    """Returns elements at the specified indices from the buffer.
+
+    Args:
+      indices: (list of integers or rank 1 int Tensor) indices in the buffer to
+        retrieve elements from.
+      keys: List of keys of tensors to retrieve. If None retrieve all.
+    Returns:
+      A list of tensors, where each element in the list is obtained by indexing
+        one of the tensors in the buffer.
+    Raises:
+      ValueError: If gather is called before calling the add function.
+      tf.errors.InvalidArgumentError: If indices are bigger than the number of
+        items in the buffer.
+    """
+    if not self._tensors:
+      raise ValueError('The add function must be called before calling gather.')
+    if keys is None:
+      keys = self._tensors.keys()
+    with tf.name_scope('Gather'):
+      index_bound_assert = tf.Assert(
+          tf.less(
+              tf.to_int64(tf.reduce_max(indices)),
+              tf.minimum(self.get_num_adds(), self._buffer_size)),
+          ['Index out of bounds.'])
+      with tf.control_dependencies([index_bound_assert]):
+        indices = tf.convert_to_tensor(indices)
+
+      batch = []
+      for key in keys:
+        batch.append(tf.gather(self._tensors[key], indices, name=key))
+      return batch
+
+  def gather_nstep(self, num_steps, indices, keys=None):
+    """Returns elements at the specified indices from the buffer.
+
+    Args:
+      num_steps: (integer) length of trajectories to return.
+      indices: (list of rank num_steps int Tensor) indices in the buffer to
+        retrieve elements from for multiple trajectories. Each Tensor in the
+        list represents the indices for a trajectory.
+      keys: List of keys of tensors to retrieve. If None retrieve all.
+    Returns:
+      A list of list-of-tensors, where each element in the list is obtained by
+        indexing one of the tensors in the buffer.
+    Raises:
+      ValueError: If gather is called before calling the add function.
+      tf.errors.InvalidArgumentError: If indices are bigger than the number of
+        items in the buffer.
+    """
+    if not self._tensors:
+      raise ValueError('The add function must be called before calling gather.')
+    if keys is None:
+      keys = self._tensors.keys()
+    with tf.name_scope('Gather'):
+      index_bound_assert = tf.Assert(
+          tf.less_equal(
+              tf.to_int64(tf.reduce_max(indices) + num_steps),
+              self.get_num_adds()),
+          ['Trajectory indices go out of bounds.'])
+      with tf.control_dependencies([index_bound_assert]):
+        indices = tf.map_fn(
+            lambda x: tf.mod(tf.range(x, x + num_steps), self._buffer_size),
+            indices,
+            dtype=tf.int64)
+
+      batch = []
+      for key in keys:
+
+        def SampleTrajectories(trajectory_indices, key=key,
+                               num_steps=num_steps):
+          trajectory_indices.set_shape([num_steps])
+          return tf.gather(self._tensors[key], trajectory_indices, name=key)
+
+        batch.append(tf.map_fn(SampleTrajectories, indices,
+                               dtype=self._tensors[key].dtype))
+      return batch
+
+  def get_position(self):
+    """Returns the position at which the last element was added.
+
+    Returns:
+      An int tensor representing the index at which the last element was added
+        to the buffer or -1 if no elements were added.
+    """
+    return tf.cond(self.get_num_adds() < 1,
+                   lambda: self.get_num_adds() - 1,
+                   lambda: tf.mod(self.get_num_adds() - 1, self._buffer_size))
+
+  def get_num_adds(self):
+    """Returns the number of additions to the buffer.
+
+    Returns:
+      An int tensor representing the number of elements that were added.
+    """
+    def num_adds():
+      return self._num_adds.value()
+
+    return self._num_adds_cs.execute(num_adds)
+
+  def get_num_tensors(self):
+    """Returns the number of tensors (slots) in the buffer."""
+    return len(self._tensors)
diff --git a/research/efficient-hrl/agents/ddpg_agent.py b/research/efficient-hrl/agents/ddpg_agent.py
new file mode 100644
index 00000000..904eb650
--- /dev/null
+++ b/research/efficient-hrl/agents/ddpg_agent.py
@@ -0,0 +1,739 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""A DDPG/NAF agent.
+
+Implements the Deep Deterministic Policy Gradient (DDPG) algorithm from
+"Continuous control with deep reinforcement learning" - Lilicrap et al.
+https://arxiv.org/abs/1509.02971, and the Normalized Advantage Functions (NAF)
+algorithm "Continuous Deep Q-Learning with Model-based Acceleration" - Gu et al.
+https://arxiv.org/pdf/1603.00748.
+"""
+
+import tensorflow as tf
+slim = tf.contrib.slim
+import gin.tf
+from utils import utils
+from agents import ddpg_networks as networks
+
+
+@gin.configurable
+class DdpgAgent(object):
+  """An RL agent that learns using the DDPG algorithm.
+
+  Example usage:
+
+  def critic_net(states, actions):
+    ...
+  def actor_net(states, num_action_dims):
+    ...
+
+  Given a tensorflow environment tf_env,
+  (of type learning.deepmind.rl.environments.tensorflow.python.tfpyenvironment)
+
+  obs_spec = tf_env.observation_spec()
+  action_spec = tf_env.action_spec()
+
+  ddpg_agent = agent.DdpgAgent(obs_spec,
+                               action_spec,
+                               actor_net=actor_net,
+                               critic_net=critic_net)
+
+  we can perform actions on the environment as follows:
+
+  state = tf_env.observations()[0]
+  action = ddpg_agent.actor_net(tf.expand_dims(state, 0))[0, :]
+  transition_type, reward, discount = tf_env.step([action])
+
+  Train:
+
+  critic_loss = ddpg_agent.critic_loss(states, actions, rewards, discounts,
+                                       next_states)
+  actor_loss = ddpg_agent.actor_loss(states)
+
+  critic_train_op = slim.learning.create_train_op(
+      critic_loss,
+      critic_optimizer,
+      variables_to_train=ddpg_agent.get_trainable_critic_vars(),
+  )
+
+  actor_train_op = slim.learning.create_train_op(
+      actor_loss,
+      actor_optimizer,
+      variables_to_train=ddpg_agent.get_trainable_actor_vars(),
+  )
+  """
+
+  ACTOR_NET_SCOPE = 'actor_net'
+  CRITIC_NET_SCOPE = 'critic_net'
+  TARGET_ACTOR_NET_SCOPE = 'target_actor_net'
+  TARGET_CRITIC_NET_SCOPE = 'target_critic_net'
+
+  def __init__(self,
+               observation_spec,
+               action_spec,
+               actor_net=networks.actor_net,
+               critic_net=networks.critic_net,
+               td_errors_loss=tf.losses.huber_loss,
+               dqda_clipping=0.,
+               actions_regularizer=0.,
+               target_q_clipping=None,
+               residual_phi=0.0,
+               debug_summaries=False):
+    """Constructs a DDPG agent.
+
+    Args:
+      observation_spec: A TensorSpec defining the observations.
+      action_spec: A BoundedTensorSpec defining the actions.
+      actor_net: A callable that creates the actor network. Must take the
+        following arguments: states, num_actions. Please see networks.actor_net
+        for an example.
+      critic_net: A callable that creates the critic network. Must take the
+        following arguments: states, actions. Please see networks.critic_net
+        for an example.
+      td_errors_loss: A callable defining the loss function for the critic
+        td error.
+      dqda_clipping: (float) clips the gradient dqda element-wise between
+        [-dqda_clipping, dqda_clipping]. Does not perform clipping if
+        dqda_clipping == 0.
+      actions_regularizer: A scalar, when positive penalizes the norm of the
+        actions. This can prevent saturation of actions for the actor_loss.
+      target_q_clipping: (tuple of floats) clips target q values within
+        (low, high) values when computing the critic loss.
+      residual_phi: (float) [0.0, 1.0] Residual algorithm parameter that
+        interpolates between Q-learning and residual gradient algorithm.
+        http://www.leemon.com/papers/1995b.pdf
+      debug_summaries: If True, add summaries to help debug behavior.
+    Raises:
+      ValueError: If 'dqda_clipping' is < 0.
+    """
+    self._observation_spec = observation_spec[0]
+    self._action_spec = action_spec[0]
+    self._state_shape = tf.TensorShape([None]).concatenate(
+        self._observation_spec.shape)
+    self._action_shape = tf.TensorShape([None]).concatenate(
+        self._action_spec.shape)
+    self._num_action_dims = self._action_spec.shape.num_elements()
+
+    self._scope = tf.get_variable_scope().name
+    self._actor_net = tf.make_template(
+        self.ACTOR_NET_SCOPE, actor_net, create_scope_now_=True)
+    self._critic_net = tf.make_template(
+        self.CRITIC_NET_SCOPE, critic_net, create_scope_now_=True)
+    self._target_actor_net = tf.make_template(
+        self.TARGET_ACTOR_NET_SCOPE, actor_net, create_scope_now_=True)
+    self._target_critic_net = tf.make_template(
+        self.TARGET_CRITIC_NET_SCOPE, critic_net, create_scope_now_=True)
+    self._td_errors_loss = td_errors_loss
+    if dqda_clipping < 0:
+      raise ValueError('dqda_clipping must be >= 0.')
+    self._dqda_clipping = dqda_clipping
+    self._actions_regularizer = actions_regularizer
+    self._target_q_clipping = target_q_clipping
+    self._residual_phi = residual_phi
+    self._debug_summaries = debug_summaries
+
+  def _batch_state(self, state):
+    """Convert state to a batched state.
+
+    Args:
+      state: Either a list/tuple with an state tensor [num_state_dims].
+    Returns:
+      A tensor [1, num_state_dims]
+    """
+    if isinstance(state, (tuple, list)):
+      state = state[0]
+    if state.get_shape().ndims == 1:
+      state = tf.expand_dims(state, 0)
+    return state
+
+  def action(self, state):
+    """Returns the next action for the state.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+    Returns:
+      A [num_action_dims] tensor representing the action.
+    """
+    return self.actor_net(self._batch_state(state), stop_gradients=True)[0, :]
+
+  @gin.configurable('ddpg_sample_action')
+  def sample_action(self, state, stddev=1.0):
+    """Returns the action for the state with additive noise.
+
+    Args:
+      state: A [num_state_dims] tensor representing a state.
+      stddev: stddev for the Ornstein-Uhlenbeck noise.
+    Returns:
+      A [num_action_dims] action tensor.
+    """
+    agent_action = self.action(state)
+    agent_action += tf.random_normal(tf.shape(agent_action)) * stddev
+    return utils.clip_to_spec(agent_action, self._action_spec)
+
+  def actor_net(self, states, stop_gradients=False):
+    """Returns the output of the actor network.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      stop_gradients: (boolean) if true, gradients cannot be propogated through
+        this operation.
+    Returns:
+      A [batch_size, num_action_dims] tensor of actions.
+    Raises:
+      ValueError: If `states` does not have the expected dimensions.
+    """
+    self._validate_states(states)
+    actions = self._actor_net(states, self._action_spec)
+    if stop_gradients:
+      actions = tf.stop_gradient(actions)
+    return actions
+
+  def critic_net(self, states, actions, for_critic_loss=False):
+    """Returns the output of the critic network.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      actions: A [batch_size, num_action_dims] tensor representing a batch
+        of actions.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    Raises:
+      ValueError: If `states` or `actions' do not have the expected dimensions.
+    """
+    self._validate_states(states)
+    self._validate_actions(actions)
+    return self._critic_net(states, actions,
+                            for_critic_loss=for_critic_loss)
+
+  def target_actor_net(self, states):
+    """Returns the output of the target actor network.
+
+    The target network is used to compute stable targets for training.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      A [batch_size, num_action_dims] tensor of actions.
+    Raises:
+      ValueError: If `states` does not have the expected dimensions.
+    """
+    self._validate_states(states)
+    actions = self._target_actor_net(states, self._action_spec)
+    return tf.stop_gradient(actions)
+
+  def target_critic_net(self, states, actions, for_critic_loss=False):
+    """Returns the output of the target critic network.
+
+    The target network is used to compute stable targets for training.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      actions: A [batch_size, num_action_dims] tensor representing a batch
+        of actions.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    Raises:
+      ValueError: If `states` or `actions' do not have the expected dimensions.
+    """
+    self._validate_states(states)
+    self._validate_actions(actions)
+    return tf.stop_gradient(
+        self._target_critic_net(states, actions,
+                                for_critic_loss=for_critic_loss))
+
+  def value_net(self, states, for_critic_loss=False):
+    """Returns the output of the critic evaluated with the actor.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    """
+    actions = self.actor_net(states)
+    return self.critic_net(states, actions,
+                           for_critic_loss=for_critic_loss)
+
+  def target_value_net(self, states, for_critic_loss=False):
+    """Returns the output of the target critic evaluated with the target actor.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    """
+    target_actions = self.target_actor_net(states)
+    return self.target_critic_net(states, target_actions,
+                                  for_critic_loss=for_critic_loss)
+
+  def critic_loss(self, states, actions, rewards, discounts,
+                  next_states):
+    """Computes a loss for training the critic network.
+
+    The loss is the mean squared error between the Q value predictions of the
+    critic and Q values estimated using TD-lambda.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      actions: A [batch_size, num_action_dims] tensor representing a batch
+        of actions.
+      rewards: A [batch_size, ...] tensor representing a batch of rewards,
+        broadcastable to the critic net output.
+      discounts: A [batch_size, ...] tensor representing a batch of discounts,
+        broadcastable to the critic net output.
+      next_states: A [batch_size, num_state_dims] tensor representing a batch
+        of next states.
+    Returns:
+      A rank-0 tensor representing the critic loss.
+    Raises:
+      ValueError: If any of the inputs do not have the expected dimensions, or
+        if their batch_sizes do not match.
+    """
+    self._validate_states(states)
+    self._validate_actions(actions)
+    self._validate_states(next_states)
+
+    target_q_values = self.target_value_net(next_states, for_critic_loss=True)
+    td_targets = target_q_values * discounts + rewards
+    if self._target_q_clipping is not None:
+      td_targets = tf.clip_by_value(td_targets, self._target_q_clipping[0],
+                                    self._target_q_clipping[1])
+    q_values = self.critic_net(states, actions, for_critic_loss=True)
+    td_errors = td_targets - q_values
+    if self._debug_summaries:
+      gen_debug_td_error_summaries(
+          target_q_values, q_values, td_targets, td_errors)
+
+    loss = self._td_errors_loss(td_targets, q_values)
+
+    if self._residual_phi > 0.0:  # compute residual gradient loss
+      residual_q_values = self.value_net(next_states, for_critic_loss=True)
+      residual_td_targets = residual_q_values * discounts + rewards
+      if self._target_q_clipping is not None:
+        residual_td_targets = tf.clip_by_value(residual_td_targets,
+                                               self._target_q_clipping[0],
+                                               self._target_q_clipping[1])
+      residual_td_errors = residual_td_targets - q_values
+      residual_loss = self._td_errors_loss(
+          residual_td_targets, residual_q_values)
+      loss = (loss * (1.0 - self._residual_phi) +
+              residual_loss * self._residual_phi)
+    return loss
+
+  def actor_loss(self, states):
+    """Computes a loss for training the actor network.
+
+    Note that output does not represent an actual loss. It is called a loss only
+    in the sense that its gradient w.r.t. the actor network weights is the
+    correct gradient for training the actor network,
+    i.e. dloss/dweights = (dq/da)*(da/dweights)
+    which is the gradient used in Algorithm 1 of Lilicrap et al.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      A rank-0 tensor representing the actor loss.
+    Raises:
+      ValueError: If `states` does not have the expected dimensions.
+    """
+    self._validate_states(states)
+    actions = self.actor_net(states, stop_gradients=False)
+    critic_values = self.critic_net(states, actions)
+    q_values = self.critic_function(critic_values, states)
+    dqda = tf.gradients([q_values], [actions])[0]
+    dqda_unclipped = dqda
+    if self._dqda_clipping > 0:
+      dqda = tf.clip_by_value(dqda, -self._dqda_clipping, self._dqda_clipping)
+
+    actions_norm = tf.norm(actions)
+    if self._debug_summaries:
+      with tf.name_scope('dqda'):
+        tf.summary.scalar('actions_norm', actions_norm)
+        tf.summary.histogram('dqda', dqda)
+        tf.summary.histogram('dqda_unclipped', dqda_unclipped)
+        tf.summary.histogram('actions', actions)
+        for a in range(self._num_action_dims):
+          tf.summary.histogram('dqda_unclipped_%d' % a, dqda_unclipped[:, a])
+          tf.summary.histogram('dqda_%d' % a, dqda[:, a])
+
+    actions_norm *= self._actions_regularizer
+    return slim.losses.mean_squared_error(tf.stop_gradient(dqda + actions),
+                                          actions,
+                                          scope='actor_loss') + actions_norm
+
+  @gin.configurable('ddpg_critic_function')
+  def critic_function(self, critic_values, states, weights=None):
+    """Computes q values based on critic_net outputs, states, and weights.
+
+    Args:
+      critic_values: A tf.float32 [batch_size, ...] tensor representing outputs
+        from the critic net.
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      weights: A list or Numpy array or tensor with a shape broadcastable to
+        `critic_values`.
+    Returns:
+      A tf.float32 [batch_size] tensor representing q values.
+    """
+    del states  # unused args
+    if weights is not None:
+      weights = tf.convert_to_tensor(weights, dtype=critic_values.dtype)
+      critic_values *= weights
+    if critic_values.shape.ndims > 1:
+      critic_values = tf.reduce_sum(critic_values,
+                                    range(1, critic_values.shape.ndims))
+    critic_values.shape.assert_has_rank(1)
+    return critic_values
+
+  @gin.configurable('ddpg_update_targets')
+  def update_targets(self, tau=1.0):
+    """Performs a soft update of the target network parameters.
+
+    For each weight w_s in the actor/critic networks, and its corresponding
+    weight w_t in the target actor/critic networks, a soft update is:
+    w_t = (1- tau) x w_t + tau x ws
+
+    Args:
+      tau: A float scalar in [0, 1]
+    Returns:
+      An operation that performs a soft update of the target network parameters.
+    Raises:
+      ValueError: If `tau` is not in [0, 1].
+    """
+    if tau < 0 or tau > 1:
+      raise ValueError('Input `tau` should be in [0, 1].')
+    update_actor = utils.soft_variables_update(
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.ACTOR_NET_SCOPE)),
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.TARGET_ACTOR_NET_SCOPE)),
+        tau)
+    update_critic = utils.soft_variables_update(
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.CRITIC_NET_SCOPE)),
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.TARGET_CRITIC_NET_SCOPE)),
+        tau)
+    return tf.group(update_actor, update_critic, name='update_targets')
+
+  def get_trainable_critic_vars(self):
+    """Returns a list of trainable variables in the critic network.
+
+    Returns:
+      A list of trainable variables in the critic network.
+    """
+    return slim.get_trainable_variables(
+        utils.join_scope(self._scope, self.CRITIC_NET_SCOPE))
+
+  def get_trainable_actor_vars(self):
+    """Returns a list of trainable variables in the actor network.
+
+    Returns:
+      A list of trainable variables in the actor network.
+    """
+    return slim.get_trainable_variables(
+        utils.join_scope(self._scope, self.ACTOR_NET_SCOPE))
+
+  def get_critic_vars(self):
+    """Returns a list of all variables in the critic network.
+
+    Returns:
+      A list of trainable variables in the critic network.
+    """
+    return slim.get_model_variables(
+        utils.join_scope(self._scope, self.CRITIC_NET_SCOPE))
+
+  def get_actor_vars(self):
+    """Returns a list of all variables in the actor network.
+
+    Returns:
+      A list of trainable variables in the actor network.
+    """
+    return slim.get_model_variables(
+        utils.join_scope(self._scope, self.ACTOR_NET_SCOPE))
+
+  def _validate_states(self, states):
+    """Raises a value error if `states` does not have the expected shape.
+
+    Args:
+      states: A tensor.
+    Raises:
+      ValueError: If states.shape or states.dtype are not compatible with
+        observation_spec.
+    """
+    states.shape.assert_is_compatible_with(self._state_shape)
+    if not states.dtype.is_compatible_with(self._observation_spec.dtype):
+      raise ValueError('states.dtype={} is not compatible with'
+                       ' observation_spec.dtype={}'.format(
+                           states.dtype, self._observation_spec.dtype))
+
+  def _validate_actions(self, actions):
+    """Raises a value error if `actions` does not have the expected shape.
+
+    Args:
+      actions: A tensor.
+    Raises:
+      ValueError: If actions.shape or actions.dtype are not compatible with
+        action_spec.
+    """
+    actions.shape.assert_is_compatible_with(self._action_shape)
+    if not actions.dtype.is_compatible_with(self._action_spec.dtype):
+      raise ValueError('actions.dtype={} is not compatible with'
+                       ' action_spec.dtype={}'.format(
+                           actions.dtype, self._action_spec.dtype))
+
+
+@gin.configurable
+class TD3Agent(DdpgAgent):
+  """An RL agent that learns using the TD3 algorithm."""
+
+  ACTOR_NET_SCOPE = 'actor_net'
+  CRITIC_NET_SCOPE = 'critic_net'
+  CRITIC_NET2_SCOPE = 'critic_net2'
+  TARGET_ACTOR_NET_SCOPE = 'target_actor_net'
+  TARGET_CRITIC_NET_SCOPE = 'target_critic_net'
+  TARGET_CRITIC_NET2_SCOPE = 'target_critic_net2'
+
+  def __init__(self,
+               observation_spec,
+               action_spec,
+               actor_net=networks.actor_net,
+               critic_net=networks.critic_net,
+               td_errors_loss=tf.losses.huber_loss,
+               dqda_clipping=0.,
+               actions_regularizer=0.,
+               target_q_clipping=None,
+               residual_phi=0.0,
+               debug_summaries=False):
+    """Constructs a TD3 agent.
+
+    Args:
+      observation_spec: A TensorSpec defining the observations.
+      action_spec: A BoundedTensorSpec defining the actions.
+      actor_net: A callable that creates the actor network. Must take the
+        following arguments: states, num_actions. Please see networks.actor_net
+        for an example.
+      critic_net: A callable that creates the critic network. Must take the
+        following arguments: states, actions. Please see networks.critic_net
+        for an example.
+      td_errors_loss: A callable defining the loss function for the critic
+        td error.
+      dqda_clipping: (float) clips the gradient dqda element-wise between
+        [-dqda_clipping, dqda_clipping]. Does not perform clipping if
+        dqda_clipping == 0.
+      actions_regularizer: A scalar, when positive penalizes the norm of the
+        actions. This can prevent saturation of actions for the actor_loss.
+      target_q_clipping: (tuple of floats) clips target q values within
+        (low, high) values when computing the critic loss.
+      residual_phi: (float) [0.0, 1.0] Residual algorithm parameter that
+        interpolates between Q-learning and residual gradient algorithm.
+        http://www.leemon.com/papers/1995b.pdf
+      debug_summaries: If True, add summaries to help debug behavior.
+    Raises:
+      ValueError: If 'dqda_clipping' is < 0.
+    """
+    self._observation_spec = observation_spec[0]
+    self._action_spec = action_spec[0]
+    self._state_shape = tf.TensorShape([None]).concatenate(
+        self._observation_spec.shape)
+    self._action_shape = tf.TensorShape([None]).concatenate(
+        self._action_spec.shape)
+    self._num_action_dims = self._action_spec.shape.num_elements()
+
+    self._scope = tf.get_variable_scope().name
+    self._actor_net = tf.make_template(
+        self.ACTOR_NET_SCOPE, actor_net, create_scope_now_=True)
+    self._critic_net = tf.make_template(
+        self.CRITIC_NET_SCOPE, critic_net, create_scope_now_=True)
+    self._critic_net2 = tf.make_template(
+        self.CRITIC_NET2_SCOPE, critic_net, create_scope_now_=True)
+    self._target_actor_net = tf.make_template(
+        self.TARGET_ACTOR_NET_SCOPE, actor_net, create_scope_now_=True)
+    self._target_critic_net = tf.make_template(
+        self.TARGET_CRITIC_NET_SCOPE, critic_net, create_scope_now_=True)
+    self._target_critic_net2 = tf.make_template(
+        self.TARGET_CRITIC_NET2_SCOPE, critic_net, create_scope_now_=True)
+    self._td_errors_loss = td_errors_loss
+    if dqda_clipping < 0:
+      raise ValueError('dqda_clipping must be >= 0.')
+    self._dqda_clipping = dqda_clipping
+    self._actions_regularizer = actions_regularizer
+    self._target_q_clipping = target_q_clipping
+    self._residual_phi = residual_phi
+    self._debug_summaries = debug_summaries
+
+  def get_trainable_critic_vars(self):
+    """Returns a list of trainable variables in the critic network.
+    NOTE: This gets the vars of both critic networks.
+
+    Returns:
+      A list of trainable variables in the critic network.
+    """
+    return (
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.CRITIC_NET_SCOPE)))
+
+  def critic_net(self, states, actions, for_critic_loss=False):
+    """Returns the output of the critic network.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      actions: A [batch_size, num_action_dims] tensor representing a batch
+        of actions.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    Raises:
+      ValueError: If `states` or `actions' do not have the expected dimensions.
+    """
+    values1 = self._critic_net(states, actions,
+                               for_critic_loss=for_critic_loss)
+    values2 = self._critic_net2(states, actions,
+                                for_critic_loss=for_critic_loss)
+    if for_critic_loss:
+      return values1, values2
+    return values1
+
+  def target_critic_net(self, states, actions, for_critic_loss=False):
+    """Returns the output of the target critic network.
+
+    The target network is used to compute stable targets for training.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+      actions: A [batch_size, num_action_dims] tensor representing a batch
+        of actions.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    Raises:
+      ValueError: If `states` or `actions' do not have the expected dimensions.
+    """
+    self._validate_states(states)
+    self._validate_actions(actions)
+    values1 = tf.stop_gradient(
+        self._target_critic_net(states, actions,
+                                for_critic_loss=for_critic_loss))
+    values2 = tf.stop_gradient(
+        self._target_critic_net2(states, actions,
+                                 for_critic_loss=for_critic_loss))
+    if for_critic_loss:
+      return values1, values2
+    return values1
+
+  def value_net(self, states, for_critic_loss=False):
+    """Returns the output of the critic evaluated with the actor.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    """
+    actions = self.actor_net(states)
+    return self.critic_net(states, actions,
+                           for_critic_loss=for_critic_loss)
+
+  def target_value_net(self, states, for_critic_loss=False):
+    """Returns the output of the target critic evaluated with the target actor.
+
+    Args:
+      states: A [batch_size, num_state_dims] tensor representing a batch
+        of states.
+    Returns:
+      q values: A [batch_size] tensor of q values.
+    """
+    target_actions = self.target_actor_net(states)
+    noise = tf.clip_by_value(
+        tf.random_normal(tf.shape(target_actions), stddev=0.2), -0.5, 0.5)
+    values1, values2 = self.target_critic_net(
+        states, target_actions + noise,
+        for_critic_loss=for_critic_loss)
+    values = tf.minimum(values1, values2)
+    return values, values
+
+  @gin.configurable('td3_update_targets')
+  def update_targets(self, tau=1.0):
+    """Performs a soft update of the target network parameters.
+
+    For each weight w_s in the actor/critic networks, and its corresponding
+    weight w_t in the target actor/critic networks, a soft update is:
+    w_t = (1- tau) x w_t + tau x ws
+
+    Args:
+      tau: A float scalar in [0, 1]
+    Returns:
+      An operation that performs a soft update of the target network parameters.
+    Raises:
+      ValueError: If `tau` is not in [0, 1].
+    """
+    if tau < 0 or tau > 1:
+      raise ValueError('Input `tau` should be in [0, 1].')
+    update_actor = utils.soft_variables_update(
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.ACTOR_NET_SCOPE)),
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.TARGET_ACTOR_NET_SCOPE)),
+        tau)
+    # NOTE: This updates both critic networks.
+    update_critic = utils.soft_variables_update(
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.CRITIC_NET_SCOPE)),
+        slim.get_trainable_variables(
+            utils.join_scope(self._scope, self.TARGET_CRITIC_NET_SCOPE)),
+        tau)
+    return tf.group(update_actor, update_critic, name='update_targets')
+
+
+def gen_debug_td_error_summaries(
+    target_q_values, q_values, td_targets, td_errors):
+  """Generates debug summaries for critic given a set of batch samples.
+
+  Args:
+    target_q_values: set of predicted next stage values.
+    q_values: current predicted value for the critic network.
+    td_targets: discounted target_q_values with added next stage reward.
+    td_errors: the different between td_targets and q_values.
+  """
+  with tf.name_scope('td_errors'):
+    tf.summary.histogram('td_targets', td_targets)
+    tf.summary.histogram('q_values', q_values)
+    tf.summary.histogram('target_q_values', target_q_values)
+    tf.summary.histogram('td_errors', td_errors)
+    with tf.name_scope('td_targets'):
+      tf.summary.scalar('mean', tf.reduce_mean(td_targets))
+      tf.summary.scalar('max', tf.reduce_max(td_targets))
+      tf.summary.scalar('min', tf.reduce_min(td_targets))
+    with tf.name_scope('q_values'):
+      tf.summary.scalar('mean', tf.reduce_mean(q_values))
+      tf.summary.scalar('max', tf.reduce_max(q_values))
+      tf.summary.scalar('min', tf.reduce_min(q_values))
+    with tf.name_scope('target_q_values'):
+      tf.summary.scalar('mean', tf.reduce_mean(target_q_values))
+      tf.summary.scalar('max', tf.reduce_max(target_q_values))
+      tf.summary.scalar('min', tf.reduce_min(target_q_values))
+    with tf.name_scope('td_errors'):
+      tf.summary.scalar('mean', tf.reduce_mean(td_errors))
+      tf.summary.scalar('max', tf.reduce_max(td_errors))
+      tf.summary.scalar('min', tf.reduce_min(td_errors))
+      tf.summary.scalar('mean_abs', tf.reduce_mean(tf.abs(td_errors)))
diff --git a/research/efficient-hrl/agents/ddpg_networks.py b/research/efficient-hrl/agents/ddpg_networks.py
new file mode 100644
index 00000000..63074dfb
--- /dev/null
+++ b/research/efficient-hrl/agents/ddpg_networks.py
@@ -0,0 +1,150 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Sample actor(policy) and critic(q) networks to use with DDPG/NAF agents.
+
+The DDPG networks are defined in "Section 7: Experiment Details" of
+"Continuous control with deep reinforcement learning" - Lilicrap et al.
+https://arxiv.org/abs/1509.02971
+
+The NAF critic network is based on "Section 4" of "Continuous deep Q-learning
+with model-based acceleration" - Gu et al. https://arxiv.org/pdf/1603.00748.
+"""
+
+import tensorflow as tf
+slim = tf.contrib.slim
+import gin.tf
+
+
+@gin.configurable('ddpg_critic_net')
+def critic_net(states, actions,
+               for_critic_loss=False,
+               num_reward_dims=1,
+               states_hidden_layers=(400,),
+               actions_hidden_layers=None,
+               joint_hidden_layers=(300,),
+               weight_decay=0.0001,
+               normalizer_fn=None,
+               activation_fn=tf.nn.relu,
+               zero_obs=False,
+               images=False):
+  """Creates a critic that returns q values for the given states and actions.
+
+  Args:
+    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor
+      representing a batch of states.
+    actions: (castable to tf.float32) a [batch_size, num_action_dims] tensor
+      representing a batch of actions.
+    num_reward_dims: Number of reward dimensions.
+    states_hidden_layers: tuple of hidden layers units for states.
+    actions_hidden_layers: tuple of hidden layers units for actions.
+    joint_hidden_layers: tuple of hidden layers units after joining states
+      and actions using tf.concat().
+    weight_decay: Weight decay for l2 weights regularizer.
+    normalizer_fn: Normalizer function, i.e. slim.layer_norm,
+    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...
+  Returns:
+    A tf.float32 [batch_size] tensor of q values, or a tf.float32
+      [batch_size, num_reward_dims] tensor of vector q values if
+      num_reward_dims > 1.
+  """
+  with slim.arg_scope(
+      [slim.fully_connected],
+      activation_fn=activation_fn,
+      normalizer_fn=normalizer_fn,
+      weights_regularizer=slim.l2_regularizer(weight_decay),
+      weights_initializer=slim.variance_scaling_initializer(
+          factor=1.0/3.0, mode='FAN_IN', uniform=True)):
+
+    orig_states = tf.to_float(states)
+    #states = tf.to_float(states)
+    states = tf.concat([tf.to_float(states), tf.to_float(actions)], -1)  #TD3
+    if images or zero_obs:
+      states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))  #LALA
+    actions = tf.to_float(actions)
+    if states_hidden_layers:
+      states = slim.stack(states, slim.fully_connected, states_hidden_layers,
+                          scope='states')
+    if actions_hidden_layers:
+      actions = slim.stack(actions, slim.fully_connected, actions_hidden_layers,
+                           scope='actions')
+    joint = tf.concat([states, actions], 1)
+    if joint_hidden_layers:
+      joint = slim.stack(joint, slim.fully_connected, joint_hidden_layers,
+                         scope='joint')
+    with slim.arg_scope([slim.fully_connected],
+                        weights_regularizer=None,
+                        weights_initializer=tf.random_uniform_initializer(
+                            minval=-0.003, maxval=0.003)):
+      value = slim.fully_connected(joint, num_reward_dims,
+                                   activation_fn=None,
+                                   normalizer_fn=None,
+                                   scope='q_value')
+    if num_reward_dims == 1:
+      value = tf.reshape(value, [-1])
+    if not for_critic_loss and num_reward_dims > 1:
+      value = tf.reduce_sum(
+          value * tf.abs(orig_states[:, -num_reward_dims:]), -1)
+  return value
+
+
+@gin.configurable('ddpg_actor_net')
+def actor_net(states, action_spec,
+              hidden_layers=(400, 300),
+              normalizer_fn=None,
+              activation_fn=tf.nn.relu,
+              zero_obs=False,
+              images=False):
+  """Creates an actor that returns actions for the given states.
+
+  Args:
+    states: (castable to tf.float32) a [batch_size, num_state_dims] tensor
+      representing a batch of states.
+    action_spec: (BoundedTensorSpec) A tensor spec indicating the shape
+      and range of actions.
+    hidden_layers: tuple of hidden layers units.
+    normalizer_fn: Normalizer function, i.e. slim.layer_norm,
+    activation_fn: Activation function, i.e. tf.nn.relu, slim.leaky_relu, ...
+  Returns:
+    A tf.float32 [batch_size, num_action_dims] tensor of actions.
+  """
+
+  with slim.arg_scope(
+      [slim.fully_connected],
+      activation_fn=activation_fn,
+      normalizer_fn=normalizer_fn,
+      weights_initializer=slim.variance_scaling_initializer(
+          factor=1.0/3.0, mode='FAN_IN', uniform=True)):
+
+    states = tf.to_float(states)
+    orig_states = states
+    if images or zero_obs:  # Zero-out x, y position. Hacky.
+      states *= tf.constant([0.0] * 2 + [1.0] * (states.shape[1] - 2))
+    if hidden_layers:
+      states = slim.stack(states, slim.fully_connected, hidden_layers,
+                          scope='states')
+    with slim.arg_scope([slim.fully_connected],
+                        weights_initializer=tf.random_uniform_initializer(
+                            minval=-0.003, maxval=0.003)):
+      actions = slim.fully_connected(states,
+                                     action_spec.shape.num_elements(),
+                                     scope='actions',
+                                     normalizer_fn=None,
+                                     activation_fn=tf.nn.tanh)
+      action_means = (action_spec.maximum + action_spec.minimum) / 2.0
+      action_magnitudes = (action_spec.maximum - action_spec.minimum) / 2.0
+      actions = action_means + action_magnitudes * actions
+
+  return actions
diff --git a/research/efficient-hrl/cond_fn.py b/research/efficient-hrl/cond_fn.py
new file mode 100644
index 00000000..cd1a276e
--- /dev/null
+++ b/research/efficient-hrl/cond_fn.py
@@ -0,0 +1,244 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Defines many boolean functions indicating when to step and reset.
+"""
+
+import tensorflow as tf
+import gin.tf
+
+
+@gin.configurable
+def env_transition(agent, state, action, transition_type, environment_steps,
+                   num_episodes):
+  """True if the transition_type is TRANSITION or FINAL_TRANSITION.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+  Returns:
+    cond: Returns an op that evaluates to true if the transition type is
+    not RESTARTING
+  """
+  del agent, state, action, num_episodes, environment_steps
+  cond = tf.logical_not(transition_type)
+  return cond
+
+
+@gin.configurable
+def env_restart(agent, state, action, transition_type, environment_steps,
+                num_episodes):
+  """True if the transition_type is RESTARTING.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+  Returns:
+    cond: Returns an op that evaluates to true if the transition type equals
+    RESTARTING.
+  """
+  del agent, state, action, num_episodes, environment_steps
+  cond = tf.identity(transition_type)
+  return cond
+
+
+@gin.configurable
+def every_n_steps(agent,
+                  state,
+                  action,
+                  transition_type,
+                  environment_steps,
+                  num_episodes,
+                  n=150):
+  """True once every n steps.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+    n: Return true once every n steps.
+  Returns:
+    cond: Returns an op that evaluates to true if environment_steps
+    equals 0 mod n. We increment the step before checking this condition, so
+    we do not need to add one to environment_steps.
+  """
+  del agent, state, action, transition_type, num_episodes
+  cond = tf.equal(tf.mod(environment_steps, n), 0)
+  return cond
+
+
+@gin.configurable
+def every_n_episodes(agent,
+                     state,
+                     action,
+                     transition_type,
+                     environment_steps,
+                     num_episodes,
+                     n=2,
+                     steps_per_episode=None):
+  """True once every n episodes.
+
+  Specifically, evaluates to True on the 0th step of every nth episode.
+  Unlike environment_steps, num_episodes starts at 0, so we do want to add
+  one to ensure it does not reset on the first call.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+    n: Return true once every n episodes.
+    steps_per_episode: How many steps per episode. Needed to determine when a
+    new episode starts.
+  Returns:
+    cond: Returns an op that evaluates to true on the last step of the episode
+      (i.e. if num_episodes equals 0 mod n).
+  """
+  assert steps_per_episode is not None
+  del agent, action, transition_type
+  ant_fell = tf.logical_or(state[2] < 0.2, state[2] > 1.0)
+  cond = tf.logical_and(
+      tf.logical_or(
+          ant_fell,
+          tf.equal(tf.mod(num_episodes + 1, n), 0)),
+      tf.equal(tf.mod(environment_steps, steps_per_episode), 0))
+  return cond
+
+
+@gin.configurable
+def failed_reset_after_n_episodes(agent,
+                                  state,
+                                  action,
+                                  transition_type,
+                                  environment_steps,
+                                  num_episodes,
+                                  steps_per_episode=None,
+                                  reset_state=None,
+                                  max_dist=1.0,
+                                  epsilon=1e-10):
+  """Every n episodes, returns True if the reset agent fails to return.
+
+  Specifically, evaluates to True if the distance between the state and the
+  reset state is greater than max_dist at the end of the episode.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+    steps_per_episode: How many steps per episode. Needed to determine when a
+    new episode starts.
+    reset_state: State to which the reset controller should return.
+    max_dist: Agent is considered to have successfully reset if its distance
+    from the reset_state is less than max_dist.
+    epsilon: small offset to ensure non-negative/zero distance.
+  Returns:
+    cond: Returns an op that evaluates to true if num_episodes+1 equals 0
+    mod n. We add one to the num_episodes so the environment is not reset after
+    the 0th step.
+  """
+  assert steps_per_episode is not None
+  assert reset_state is not None
+  del agent, state, action, transition_type, num_episodes
+  dist = tf.sqrt(
+      tf.reduce_sum(tf.squared_difference(state, reset_state)) + epsilon)
+  cond = tf.logical_and(
+      tf.greater(dist, tf.constant(max_dist)),
+      tf.equal(tf.mod(environment_steps, steps_per_episode), 0))
+  return cond
+
+
+@gin.configurable
+def q_too_small(agent,
+                state,
+                action,
+                transition_type,
+                environment_steps,
+                num_episodes,
+                q_min=0.5):
+  """True of q is too small.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+    q_min: Returns true if the qval is less than q_min
+  Returns:
+    cond: Returns an op that evaluates to true if qval is less than q_min.
+  """
+  del transition_type, environment_steps, num_episodes
+  state_for_reset_agent = tf.stack(state[:-1], tf.constant([0], dtype=tf.float))
+  qval = agent.BASE_AGENT_CLASS.critic_net(
+      tf.expand_dims(state_for_reset_agent, 0), tf.expand_dims(action, 0))[0, :]
+  cond = tf.greater(tf.constant(q_min), qval)
+  return cond
+
+
+@gin.configurable
+def true_fn(agent, state, action, transition_type, environment_steps,
+            num_episodes):
+  """Returns an op that evaluates to true.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+  Returns:
+    cond: op that always evaluates to True.
+  """
+  del agent, state, action, transition_type, environment_steps, num_episodes
+  cond = tf.constant(True, dtype=tf.bool)
+  return cond
+
+
+@gin.configurable
+def false_fn(agent, state, action, transition_type, environment_steps,
+             num_episodes):
+  """Returns an op that evaluates to false.
+
+  Args:
+    agent: RL agent.
+    state: A [num_state_dims] tensor representing a state.
+    action: Action performed.
+    transition_type: Type of transition after action
+    environment_steps: Number of steps performed by environment.
+    num_episodes: Number of episodes.
+  Returns:
+    cond: op that always evaluates to False.
+  """
+  del agent, state, action, transition_type, environment_steps, num_episodes
+  cond = tf.constant(False, dtype=tf.bool)
+  return cond
diff --git a/research/efficient-hrl/configs/base_uvf.gin b/research/efficient-hrl/configs/base_uvf.gin
new file mode 100644
index 00000000..2f3f47b6
--- /dev/null
+++ b/research/efficient-hrl/configs/base_uvf.gin
@@ -0,0 +1,68 @@
+#-*-Python-*-
+import gin.tf.external_configurables
+
+create_maze_env.top_down_view = %IMAGES
+## Create the agent
+AGENT_CLASS = @UvfAgent
+UvfAgent.tf_context = %CONTEXT
+UvfAgent.actor_net = @agent/ddpg_actor_net
+UvfAgent.critic_net = @agent/ddpg_critic_net
+UvfAgent.dqda_clipping = 0.0
+UvfAgent.td_errors_loss = @tf.losses.huber_loss
+UvfAgent.target_q_clipping = %TARGET_Q_CLIPPING
+
+# Create meta agent
+META_CLASS = @MetaAgent
+MetaAgent.tf_context = %META_CONTEXT
+MetaAgent.sub_context = %CONTEXT
+MetaAgent.actor_net = @meta/ddpg_actor_net
+MetaAgent.critic_net = @meta/ddpg_critic_net
+MetaAgent.dqda_clipping = 0.0
+MetaAgent.td_errors_loss = @tf.losses.huber_loss
+MetaAgent.target_q_clipping = %TARGET_Q_CLIPPING
+
+# Create state preprocess
+STATE_PREPROCESS_CLASS = @StatePreprocess
+StatePreprocess.ndims = %SUBGOAL_DIM
+state_preprocess_net.states_hidden_layers = (100, 100)
+state_preprocess_net.num_output_dims = %SUBGOAL_DIM
+state_preprocess_net.images = %IMAGES
+action_embed_net.num_output_dims = %SUBGOAL_DIM
+INVERSE_DYNAMICS_CLASS = @InverseDynamics
+
+# actor_net
+ACTOR_HIDDEN_SIZE_1 = 300
+ACTOR_HIDDEN_SIZE_2 = 300
+agent/ddpg_actor_net.hidden_layers = (%ACTOR_HIDDEN_SIZE_1, %ACTOR_HIDDEN_SIZE_2)
+agent/ddpg_actor_net.activation_fn = @tf.nn.relu
+agent/ddpg_actor_net.zero_obs = %ZERO_OBS
+agent/ddpg_actor_net.images = %IMAGES
+meta/ddpg_actor_net.hidden_layers = (%ACTOR_HIDDEN_SIZE_1, %ACTOR_HIDDEN_SIZE_2)
+meta/ddpg_actor_net.activation_fn = @tf.nn.relu
+meta/ddpg_actor_net.zero_obs = False
+meta/ddpg_actor_net.images = %IMAGES
+# critic_net
+CRITIC_HIDDEN_SIZE_1 = 300
+CRITIC_HIDDEN_SIZE_2 = 300
+agent/ddpg_critic_net.states_hidden_layers = (%CRITIC_HIDDEN_SIZE_1,)
+agent/ddpg_critic_net.actions_hidden_layers = None
+agent/ddpg_critic_net.joint_hidden_layers = (%CRITIC_HIDDEN_SIZE_2,)
+agent/ddpg_critic_net.weight_decay = 0.0
+agent/ddpg_critic_net.activation_fn = @tf.nn.relu
+agent/ddpg_critic_net.zero_obs = %ZERO_OBS
+agent/ddpg_critic_net.images = %IMAGES
+meta/ddpg_critic_net.states_hidden_layers = (%CRITIC_HIDDEN_SIZE_1,)
+meta/ddpg_critic_net.actions_hidden_layers = None
+meta/ddpg_critic_net.joint_hidden_layers = (%CRITIC_HIDDEN_SIZE_2,)
+meta/ddpg_critic_net.weight_decay = 0.0
+meta/ddpg_critic_net.activation_fn = @tf.nn.relu
+meta/ddpg_critic_net.zero_obs = False
+meta/ddpg_critic_net.images = %IMAGES
+
+tf.losses.huber_loss.delta = 1.0
+# Sample action
+uvf_add_noise_fn.stddev = 1.0
+meta_add_noise_fn.stddev = %META_EXPLORE_NOISE
+# Update targets
+ddpg_update_targets.tau = 0.001
+td3_update_targets.tau = 0.005
diff --git a/research/efficient-hrl/configs/eval_uvf.gin b/research/efficient-hrl/configs/eval_uvf.gin
new file mode 100644
index 00000000..7a58241e
--- /dev/null
+++ b/research/efficient-hrl/configs/eval_uvf.gin
@@ -0,0 +1,14 @@
+#-*-Python-*-
+# Config eval
+evaluate.environment = @create_maze_env()
+evaluate.agent_class = %AGENT_CLASS
+evaluate.meta_agent_class = %META_CLASS
+evaluate.state_preprocess_class = %STATE_PREPROCESS_CLASS
+evaluate.num_episodes_eval = 50
+evaluate.num_episodes_videos = 1
+evaluate.gamma = 1.0
+evaluate.eval_interval_secs = 1
+evaluate.generate_videos = False
+evaluate.generate_summaries = True
+evaluate.eval_modes = %EVAL_MODES
+evaluate.max_steps_per_episode = %RESET_EPISODE_PERIOD
diff --git a/research/efficient-hrl/configs/train_uvf.gin b/research/efficient-hrl/configs/train_uvf.gin
new file mode 100644
index 00000000..8b02d7a6
--- /dev/null
+++ b/research/efficient-hrl/configs/train_uvf.gin
@@ -0,0 +1,52 @@
+#-*-Python-*-
+# Create replay_buffer
+agent/CircularBuffer.buffer_size = 200000
+meta/CircularBuffer.buffer_size = 200000
+agent/CircularBuffer.scope = "agent"
+meta/CircularBuffer.scope = "meta"
+
+# Config train
+train_uvf.environment = @create_maze_env()
+train_uvf.agent_class = %AGENT_CLASS
+train_uvf.meta_agent_class = %META_CLASS
+train_uvf.state_preprocess_class = %STATE_PREPROCESS_CLASS
+train_uvf.inverse_dynamics_class = %INVERSE_DYNAMICS_CLASS
+train_uvf.replay_buffer = @agent/CircularBuffer()
+train_uvf.meta_replay_buffer = @meta/CircularBuffer()
+train_uvf.critic_optimizer = @critic/AdamOptimizer()
+train_uvf.actor_optimizer = @actor/AdamOptimizer()
+train_uvf.meta_critic_optimizer = @meta_critic/AdamOptimizer()
+train_uvf.meta_actor_optimizer = @meta_actor/AdamOptimizer()
+train_uvf.repr_optimizer = @repr/AdamOptimizer()
+train_uvf.num_episodes_train = 25000
+train_uvf.batch_size = 100
+train_uvf.initial_episodes = 5
+train_uvf.gamma = 0.99
+train_uvf.meta_gamma = 0.99
+train_uvf.reward_scale_factor = 1.0
+train_uvf.target_update_period = 2
+train_uvf.num_updates_per_observation = 1
+train_uvf.num_collect_per_update = 1
+train_uvf.num_collect_per_meta_update = 10
+train_uvf.debug_summaries = False
+train_uvf.log_every_n_steps = 1000
+train_uvf.save_policy_every_n_steps =100000
+
+# Config Optimizers
+critic/AdamOptimizer.learning_rate = 0.001
+critic/AdamOptimizer.beta1 = 0.9
+critic/AdamOptimizer.beta2 = 0.999
+actor/AdamOptimizer.learning_rate = 0.0001
+actor/AdamOptimizer.beta1 = 0.9
+actor/AdamOptimizer.beta2 = 0.999
+
+meta_critic/AdamOptimizer.learning_rate = 0.001
+meta_critic/AdamOptimizer.beta1 = 0.9
+meta_critic/AdamOptimizer.beta2 = 0.999
+meta_actor/AdamOptimizer.learning_rate = 0.0001
+meta_actor/AdamOptimizer.beta1 = 0.9
+meta_actor/AdamOptimizer.beta2 = 0.999
+
+repr/AdamOptimizer.learning_rate = 0.0001
+repr/AdamOptimizer.beta1 = 0.9
+repr/AdamOptimizer.beta2 = 0.999
diff --git a/research/efficient-hrl/context/__init__.py b/research/efficient-hrl/context/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/research/efficient-hrl/context/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/efficient-hrl/context/configs/ant_block.gin b/research/efficient-hrl/context/configs/ant_block.gin
new file mode 100644
index 00000000..d5bd4f01
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_block.gin
@@ -0,0 +1,67 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntBlock"
+ZERO_OBS = False
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4), (20, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1", "eval2", "eval3"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+    "eval3": [@eval3/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [3, 4]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [16, 0]
+eval2/ConstantSampler.value = [16, 16]
+eval3/ConstantSampler.value = [0, 16]
diff --git a/research/efficient-hrl/context/configs/ant_block_maze.gin b/research/efficient-hrl/context/configs/ant_block_maze.gin
new file mode 100644
index 00000000..cebf775b
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_block_maze.gin
@@ -0,0 +1,67 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntBlockMaze"
+ZERO_OBS = False
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4), (12, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1", "eval2", "eval3"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+    "eval3": [@eval3/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [3, 4]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [8, 0]
+eval2/ConstantSampler.value = [8, 16]
+eval3/ConstantSampler.value = [0, 16]
diff --git a/research/efficient-hrl/context/configs/ant_fall_multi.gin b/research/efficient-hrl/context/configs/ant_fall_multi.gin
new file mode 100644
index 00000000..eb89ad0c
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_fall_multi.gin
@@ -0,0 +1,62 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntFall"
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4, 0), (12, 28, 5))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [3]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1, 2]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [0, 27, 4.5]
diff --git a/research/efficient-hrl/context/configs/ant_fall_multi_img.gin b/research/efficient-hrl/context/configs/ant_fall_multi_img.gin
new file mode 100644
index 00000000..b54fb7c9
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_fall_multi_img.gin
@@ -0,0 +1,68 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntFall"
+IMAGES = True
+
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4, 0), (12, 28, 5))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [3]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+}
+meta/Context.context_transition_fn = @task/relative_context_transition_fn
+meta/Context.context_multi_transition_fn = @task/relative_context_multi_transition_fn
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1, 2]
+task/negative_distance.relative_context = True
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+task/relative_context_transition_fn.k = 3
+task/relative_context_multi_transition_fn.k = 3
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [0, 27, 0]
diff --git a/research/efficient-hrl/context/configs/ant_fall_single.gin b/research/efficient-hrl/context/configs/ant_fall_single.gin
new file mode 100644
index 00000000..56bbc070
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_fall_single.gin
@@ -0,0 +1,62 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntFall"
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4, 0), (12, 28, 5))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [3]
+meta/Context.samplers = {
+    "train": [@eval1/ConstantSampler],
+    "explore": [@eval1/ConstantSampler],
+    "eval1": [@eval1/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1, 2]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [0, 27, 4.5]
diff --git a/research/efficient-hrl/context/configs/ant_maze.gin b/research/efficient-hrl/context/configs/ant_maze.gin
new file mode 100644
index 00000000..3a0b73e3
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_maze.gin
@@ -0,0 +1,66 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntMaze"
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4), (20, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1", "eval2", "eval3"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+    "eval3": [@eval3/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [16, 0]
+eval2/ConstantSampler.value = [16, 16]
+eval3/ConstantSampler.value = [0, 16]
diff --git a/research/efficient-hrl/context/configs/ant_maze_img.gin b/research/efficient-hrl/context/configs/ant_maze_img.gin
new file mode 100644
index 00000000..ceed65a0
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_maze_img.gin
@@ -0,0 +1,72 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntMaze"
+IMAGES = True
+
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-4, -4), (20, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1", "eval2", "eval3"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+    "eval3": [@eval3/ConstantSampler],
+}
+meta/Context.context_transition_fn = @task/relative_context_transition_fn
+meta/Context.context_multi_transition_fn = @task/relative_context_multi_transition_fn
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = True
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+task/relative_context_transition_fn.k = 2
+task/relative_context_multi_transition_fn.k = 2
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [16, 0]
+eval2/ConstantSampler.value = [16, 16]
+eval3/ConstantSampler.value = [0, 16]
diff --git a/research/efficient-hrl/context/configs/ant_push_multi.gin b/research/efficient-hrl/context/configs/ant_push_multi.gin
new file mode 100644
index 00000000..db9b4ed7
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_push_multi.gin
@@ -0,0 +1,62 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntPush"
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-16, -4), (16, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval2"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval2": [@eval2/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval2/ConstantSampler.value = [0, 19]
diff --git a/research/efficient-hrl/context/configs/ant_push_multi_img.gin b/research/efficient-hrl/context/configs/ant_push_multi_img.gin
new file mode 100644
index 00000000..abdc4340
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_push_multi_img.gin
@@ -0,0 +1,68 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntPush"
+IMAGES = True
+
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-16, -4), (16, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval2"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval2": [@eval2/ConstantSampler],
+}
+meta/Context.context_transition_fn = @task/relative_context_transition_fn
+meta/Context.context_multi_transition_fn = @task/relative_context_multi_transition_fn
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = True
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+task/relative_context_transition_fn.k = 2
+task/relative_context_multi_transition_fn.k = 2
+MetaAgent.k = %SUBGOAL_DIM
+
+eval2/ConstantSampler.value = [0, 19]
diff --git a/research/efficient-hrl/context/configs/ant_push_single.gin b/research/efficient-hrl/context/configs/ant_push_single.gin
new file mode 100644
index 00000000..e85c5dfb
--- /dev/null
+++ b/research/efficient-hrl/context/configs/ant_push_single.gin
@@ -0,0 +1,62 @@
+#-*-Python-*-
+create_maze_env.env_name = "AntPush"
+context_range = (%CONTEXT_RANGE_MIN, %CONTEXT_RANGE_MAX)
+meta_context_range = ((-16, -4), (16, 20))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval2"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@eval2/ConstantSampler],
+    "explore": [@eval2/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval2/ConstantSampler.value = [0, 19]
diff --git a/research/efficient-hrl/context/configs/default.gin b/research/efficient-hrl/context/configs/default.gin
new file mode 100644
index 00000000..65f91e52
--- /dev/null
+++ b/research/efficient-hrl/context/configs/default.gin
@@ -0,0 +1,12 @@
+#-*-Python-*-
+ENV_CONTEXT = None
+EVAL_MODES = ["eval"]
+TARGET_Q_CLIPPING = None
+RESET_EPISODE_PERIOD = None
+ZERO_OBS = False
+CONTEXT_RANGE_MIN = -10
+CONTEXT_RANGE_MAX = 10
+SUBGOAL_DIM = 2
+
+uvf/negative_distance.summarize = False
+uvf/negative_distance.relative_context = True
diff --git a/research/efficient-hrl/context/configs/hiro_orig.gin b/research/efficient-hrl/context/configs/hiro_orig.gin
new file mode 100644
index 00000000..e39ba96b
--- /dev/null
+++ b/research/efficient-hrl/context/configs/hiro_orig.gin
@@ -0,0 +1,14 @@
+#-*-Python-*-
+ENV_CONTEXT = None
+EVAL_MODES = ["eval"]
+TARGET_Q_CLIPPING = None
+RESET_EPISODE_PERIOD = None
+ZERO_OBS = True
+IMAGES = False
+CONTEXT_RANGE_MIN = (-10, -10, -0.5, -1, -1, -1, -1, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3, -0.5, -0.3)
+CONTEXT_RANGE_MAX = ( 10,  10,  0.5,  1,  1,  1,  1,  0.5,  0.3,  0.5,  0.3,  0.5,  0.3,  0.5,  0.3)
+SUBGOAL_DIM = 15
+META_EXPLORE_NOISE = 1.0
+
+uvf/negative_distance.summarize = False
+uvf/negative_distance.relative_context = True
diff --git a/research/efficient-hrl/context/configs/hiro_repr.gin b/research/efficient-hrl/context/configs/hiro_repr.gin
new file mode 100644
index 00000000..a0a8057b
--- /dev/null
+++ b/research/efficient-hrl/context/configs/hiro_repr.gin
@@ -0,0 +1,18 @@
+#-*-Python-*-
+ENV_CONTEXT = None
+EVAL_MODES = ["eval"]
+TARGET_Q_CLIPPING = None
+RESET_EPISODE_PERIOD = None
+ZERO_OBS = False
+IMAGES = False
+CONTEXT_RANGE_MIN = -10
+CONTEXT_RANGE_MAX = 10
+SUBGOAL_DIM = 2
+META_EXPLORE_NOISE = 5.0
+
+StatePreprocess.trainable = True
+StatePreprocess.state_preprocess_net = @state_preprocess_net
+StatePreprocess.action_embed_net = @action_embed_net
+
+uvf/negative_distance.summarize = False
+uvf/negative_distance.relative_context = True
diff --git a/research/efficient-hrl/context/configs/hiro_xy.gin b/research/efficient-hrl/context/configs/hiro_xy.gin
new file mode 100644
index 00000000..f35026c9
--- /dev/null
+++ b/research/efficient-hrl/context/configs/hiro_xy.gin
@@ -0,0 +1,14 @@
+#-*-Python-*-
+ENV_CONTEXT = None
+EVAL_MODES = ["eval"]
+TARGET_Q_CLIPPING = None
+RESET_EPISODE_PERIOD = None
+ZERO_OBS = False
+IMAGES = False
+CONTEXT_RANGE_MIN = -10
+CONTEXT_RANGE_MAX = 10
+SUBGOAL_DIM = 2
+META_EXPLORE_NOISE = 1.0
+
+uvf/negative_distance.summarize = False
+uvf/negative_distance.relative_context = True
diff --git a/research/efficient-hrl/context/configs/point_maze.gin b/research/efficient-hrl/context/configs/point_maze.gin
new file mode 100644
index 00000000..0ea67d2d
--- /dev/null
+++ b/research/efficient-hrl/context/configs/point_maze.gin
@@ -0,0 +1,73 @@
+#-*-Python-*-
+# NOTE: For best training, low-level exploration (uvf_add_noise_fn.stddev)
+# should be reduced to around 0.1.
+create_maze_env.env_name = "PointMaze"
+context_range_min = -10
+context_range_max = 10
+context_range = (%context_range_min, %context_range_max)
+meta_context_range = ((-2, -2), (10, 10))
+
+RESET_EPISODE_PERIOD = 500
+RESET_ENV_PERIOD = 1
+# End episode every N steps
+UvfAgent.reset_episode_cond_fn = @every_n_steps
+every_n_steps.n = %RESET_EPISODE_PERIOD
+train_uvf.max_steps_per_episode = %RESET_EPISODE_PERIOD
+# Do a manual reset every N episodes
+UvfAgent.reset_env_cond_fn = @every_n_episodes
+every_n_episodes.n = %RESET_ENV_PERIOD
+every_n_episodes.steps_per_episode = %RESET_EPISODE_PERIOD
+
+## Config defaults
+EVAL_MODES = ["eval1", "eval2", "eval3"]
+
+## Config agent
+CONTEXT = @agent/Context
+META_CONTEXT = @meta/Context
+
+## Config agent context
+agent/Context.context_ranges = [%context_range]
+agent/Context.context_shapes = [%SUBGOAL_DIM]
+agent/Context.meta_action_every_n = 10
+agent/Context.samplers = {
+    "train": [@train/DirectionSampler],
+    "explore": [@train/DirectionSampler],
+    "eval1": [@uvf_eval1/ConstantSampler],
+    "eval2": [@uvf_eval2/ConstantSampler],
+    "eval3": [@uvf_eval3/ConstantSampler],
+}
+
+agent/Context.context_transition_fn = @relative_context_transition_fn
+agent/Context.context_multi_transition_fn = @relative_context_multi_transition_fn
+
+agent/Context.reward_fn = @uvf/negative_distance
+
+## Config meta context
+meta/Context.context_ranges = [%meta_context_range]
+meta/Context.context_shapes = [2]
+meta/Context.samplers = {
+    "train": [@train/RandomSampler],
+    "explore": [@train/RandomSampler],
+    "eval1": [@eval1/ConstantSampler],
+    "eval2": [@eval2/ConstantSampler],
+    "eval3": [@eval3/ConstantSampler],
+}
+meta/Context.reward_fn = @task/negative_distance
+
+## Config rewards
+task/negative_distance.state_indices = [0, 1]
+task/negative_distance.relative_context = False
+task/negative_distance.diff = False
+task/negative_distance.offset = 0.0
+
+## Config samplers
+train/RandomSampler.context_range = %meta_context_range
+train/DirectionSampler.context_range = %context_range
+train/DirectionSampler.k = %SUBGOAL_DIM
+relative_context_transition_fn.k = %SUBGOAL_DIM
+relative_context_multi_transition_fn.k = %SUBGOAL_DIM
+MetaAgent.k = %SUBGOAL_DIM
+
+eval1/ConstantSampler.value = [8, 0]
+eval2/ConstantSampler.value = [8, 8]
+eval3/ConstantSampler.value = [0, 8]
diff --git a/research/efficient-hrl/context/context.py b/research/efficient-hrl/context/context.py
new file mode 100644
index 00000000..76be00b4
--- /dev/null
+++ b/research/efficient-hrl/context/context.py
@@ -0,0 +1,467 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Context for Universal Value Function agents.
+
+A context specifies a list of contextual variables, each with
+  own sampling and reward computation methods.
+
+Examples of contextual variables include
+  goal states, reward combination vectors, etc.
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+import numpy as np
+import tensorflow as tf
+from tf_agents import specs
+import gin.tf
+from utils import utils as uvf_utils
+
+
+@gin.configurable
+class Context(object):
+  """Base context."""
+  VAR_NAME = 'action'
+
+  def __init__(self,
+               tf_env,
+               context_ranges=None,
+               context_shapes=None,
+               state_indices=None,
+               variable_indices=None,
+               gamma_index=None,
+               settable_context=False,
+               timers=None,
+               samplers=None,
+               reward_weights=None,
+               reward_fn=None,
+               random_sampler_mode='random',
+               normalizers=None,
+               context_transition_fn=None,
+               context_multi_transition_fn=None,
+               meta_action_every_n=None):
+    self._tf_env = tf_env
+    self.variable_indices = variable_indices
+    self.gamma_index = gamma_index
+    self._settable_context = settable_context
+    self.timers = timers
+    self._context_transition_fn = context_transition_fn
+    self._context_multi_transition_fn = context_multi_transition_fn
+    self._random_sampler_mode = random_sampler_mode
+
+    # assign specs
+    self._obs_spec = self._tf_env.observation_spec()
+    self._context_shapes = tuple([
+        shape if shape is not None else self._obs_spec.shape
+        for shape in context_shapes
+    ])
+    self.context_specs = tuple([
+        specs.TensorSpec(dtype=self._obs_spec.dtype, shape=shape)
+        for shape in self._context_shapes
+    ])
+    if context_ranges is not None:
+      self.context_ranges = context_ranges
+    else:
+      self.context_ranges = [None] * len(self._context_shapes)
+
+    self.context_as_action_specs = tuple([
+        specs.BoundedTensorSpec(
+            shape=shape,
+            dtype=(tf.float32 if self._obs_spec.dtype in
+                   [tf.float32, tf.float64] else self._obs_spec.dtype),
+            minimum=context_range[0],
+            maximum=context_range[-1])
+        for shape, context_range in zip(self._context_shapes, self.context_ranges)
+    ])
+
+    if state_indices is not None:
+      self.state_indices = state_indices
+    else:
+      self.state_indices = [None] * len(self._context_shapes)
+    if self.variable_indices is not None and self.n != len(
+        self.variable_indices):
+      raise ValueError(
+          'variable_indices (%s) must have the same length as contexts (%s).' %
+          (self.variable_indices, self.context_specs))
+    assert self.n == len(self.context_ranges)
+    assert self.n == len(self.state_indices)
+
+    # assign reward/sampler fns
+    self._sampler_fns = dict()
+    self._samplers = dict()
+    self._reward_fns = dict()
+
+    # assign reward fns
+    self._add_custom_reward_fns()
+    reward_weights = reward_weights or None
+    self._reward_fn = self._make_reward_fn(reward_fn, reward_weights)
+
+    # assign samplers
+    self._add_custom_sampler_fns()
+    for mode, sampler_fns in samplers.items():
+      self._make_sampler_fn(sampler_fns, mode)
+
+    # create normalizers
+    if normalizers is None:
+      self._normalizers = [None] * len(self.context_specs)
+    else:
+      self._normalizers = [
+          normalizer(tf.zeros(shape=spec.shape, dtype=spec.dtype))
+          if normalizer is not None else None
+          for normalizer, spec in zip(normalizers, self.context_specs)
+      ]
+    assert self.n == len(self._normalizers)
+
+    self.meta_action_every_n = meta_action_every_n
+
+    # create vars
+    self.context_vars = {}
+    self.timer_vars = {}
+    self.create_vars(self.VAR_NAME)
+    self.t = tf.Variable(
+        tf.zeros(shape=(), dtype=tf.int32), name='num_timer_steps')
+
+  def _add_custom_reward_fns(self):
+    pass
+
+  def _add_custom_sampler_fns(self):
+    pass
+
+  def sample_random_contexts(self, batch_size):
+    """Sample random batch contexts."""
+    assert self._random_sampler_mode is not None
+    return self.sample_contexts(self._random_sampler_mode, batch_size)[0]
+
+  def sample_contexts(self, mode, batch_size, state=None, next_state=None,
+                      **kwargs):
+    """Sample a batch of contexts.
+
+    Args:
+      mode: A string representing the mode [`train`, `explore`, `eval`].
+      batch_size: Batch size.
+    Returns:
+      Two lists of [batch_size, num_context_dims] contexts.
+    """
+    contexts, next_contexts = self._sampler_fns[mode](
+        batch_size, state=state, next_state=next_state,
+        **kwargs)
+    self._validate_contexts(contexts)
+    self._validate_contexts(next_contexts)
+    return contexts, next_contexts
+
+  def compute_rewards(self, mode, states, actions, rewards, next_states,
+                      contexts):
+    """Compute context-based rewards.
+
+    Args:
+      mode: A string representing the mode ['uvf', 'task'].
+      states: A [batch_size, num_state_dims] tensor.
+      actions: A [batch_size, num_action_dims] tensor.
+      rewards: A [batch_size] tensor representing unmodified rewards.
+      next_states: A [batch_size, num_state_dims] tensor.
+      contexts: A list of [batch_size, num_context_dims] tensors.
+    Returns:
+      A [batch_size] tensor representing rewards.
+    """
+    return self._reward_fn(states, actions, rewards, next_states,
+                           contexts)
+
+  def _make_reward_fn(self, reward_fns_list, reward_weights):
+    """Returns a fn that computes rewards.
+
+    Args:
+      reward_fns_list: A fn or a list of reward fns.
+      mode: A string representing the operating mode.
+      reward_weights: A list of reward weights.
+    """
+    if not isinstance(reward_fns_list, (list, tuple)):
+      reward_fns_list = [reward_fns_list]
+    if reward_weights is None:
+      reward_weights = [1.0] * len(reward_fns_list)
+    assert len(reward_fns_list) == len(reward_weights)
+
+    reward_fns_list = [
+        self._custom_reward_fns[fn] if isinstance(fn, (str,)) else fn
+        for fn in reward_fns_list
+    ]
+
+    def reward_fn(*args, **kwargs):
+      """Returns rewards, discounts."""
+      reward_tuples = [
+          reward_fn(*args, **kwargs) for reward_fn in reward_fns_list
+      ]
+      rewards_list = [reward_tuple[0] for reward_tuple in reward_tuples]
+      discounts_list = [reward_tuple[1] for reward_tuple in reward_tuples]
+      ndims = max([r.shape.ndims for r in rewards_list])
+      if ndims > 1:  # expand reward shapes to allow broadcasting
+        for i in range(len(rewards_list)):
+          for _ in range(rewards_list[i].shape.ndims - ndims):
+            rewards_list[i] = tf.expand_dims(rewards_list[i], axis=-1)
+          for _ in range(discounts_list[i].shape.ndims - ndims):
+            discounts_list[i] = tf.expand_dims(discounts_list[i], axis=-1)
+      rewards = tf.add_n(
+          [r * tf.to_float(w) for r, w in zip(rewards_list, reward_weights)])
+      discounts = discounts_list[0]
+      for d in discounts_list[1:]:
+        discounts *= d
+
+      return rewards, discounts
+
+    return reward_fn
+
+  def _make_sampler_fn(self, sampler_cls_list, mode):
+    """Returns a fn that samples a list of context vars.
+
+    Args:
+      sampler_cls_list: A list of sampler classes.
+      mode: A string representing the operating mode.
+    """
+    if not isinstance(sampler_cls_list, (list, tuple)):
+      sampler_cls_list = [sampler_cls_list]
+
+    self._samplers[mode] = []
+    sampler_fns = []
+    for spec, sampler in zip(self.context_specs, sampler_cls_list):
+      if isinstance(sampler, (str,)):
+        sampler_fn = self._custom_sampler_fns[sampler]
+      else:
+        sampler_fn = sampler(context_spec=spec)
+        self._samplers[mode].append(sampler_fn)
+      sampler_fns.append(sampler_fn)
+
+    def batch_sampler_fn(batch_size, state=None, next_state=None, **kwargs):
+      """Sampler fn."""
+      contexts_tuples = [
+          sampler(batch_size, state=state, next_state=next_state, **kwargs)
+          for sampler in sampler_fns]
+      contexts = [c[0] for c in contexts_tuples]
+      next_contexts = [c[1] for c in contexts_tuples]
+      contexts = [
+          normalizer.update_apply(c) if normalizer is not None else c
+          for normalizer, c in zip(self._normalizers, contexts)
+      ]
+      next_contexts = [
+          normalizer.apply(c) if normalizer is not None else c
+          for normalizer, c in zip(self._normalizers, next_contexts)
+      ]
+      return contexts, next_contexts
+
+    self._sampler_fns[mode] = batch_sampler_fn
+
+  def set_env_context_op(self, context, disable_unnormalizer=False):
+    """Returns a TensorFlow op that sets the environment context.
+
+    Args:
+      context: A list of context Tensor variables.
+      disable_unnormalizer: Disable unnormalization.
+    Returns:
+      A TensorFlow op that sets the environment context.
+    """
+    ret_val = np.array(1.0, dtype=np.float32)
+    if not self._settable_context:
+      return tf.identity(ret_val)
+
+    if not disable_unnormalizer:
+      context = [
+          normalizer.unapply(tf.expand_dims(c, 0))[0]
+          if normalizer is not None else c
+          for normalizer, c in zip(self._normalizers, context)
+      ]
+
+    def set_context_func(*env_context_values):
+      tf.logging.info('[set_env_context_op] Setting gym environment context.')
+      # pylint: disable=protected-access
+      self.gym_env.set_context(*env_context_values)
+      return ret_val
+      # pylint: enable=protected-access
+
+    with tf.name_scope('set_env_context'):
+      set_op = tf.py_func(set_context_func, context, tf.float32,
+                          name='set_env_context_py_func')
+      set_op.set_shape([])
+    return set_op
+
+  def set_replay(self, replay):
+    """Set replay buffer for samplers.
+
+    Args:
+      replay: A replay buffer.
+    """
+    for _, samplers in self._samplers.items():
+      for sampler in samplers:
+        sampler.set_replay(replay)
+
+  def get_clip_fns(self):
+    """Returns a list of clip fns for contexts.
+
+    Returns:
+      A list of fns that clip context tensors.
+    """
+    clip_fns = []
+    for context_range in self.context_ranges:
+      def clip_fn(var_, range_=context_range):
+        """Clip a tensor."""
+        if range_ is None:
+          clipped_var = tf.identity(var_)
+        elif isinstance(range_[0], (int, long, float, list, np.ndarray)):
+          clipped_var = tf.clip_by_value(
+              var_,
+              range_[0],
+              range_[1],)
+        else: raise NotImplementedError(range_)
+        return clipped_var
+      clip_fns.append(clip_fn)
+    return clip_fns
+
+  def _validate_contexts(self, contexts):
+    """Validate if contexts have right specs.
+
+    Args:
+      contexts: A list of [batch_size, num_context_dim] tensors.
+    Raises:
+      ValueError: If shape or dtype mismatches that of spec.
+    """
+    for i, (context, spec) in enumerate(zip(contexts, self.context_specs)):
+      if context[0].shape != spec.shape:
+        raise ValueError('contexts[%d] has invalid shape %s wrt spec shape %s' %
+                         (i, context[0].shape, spec.shape))
+      if context.dtype != spec.dtype:
+        raise ValueError('contexts[%d] has invalid dtype %s wrt spec dtype %s' %
+                         (i, context.dtype, spec.dtype))
+
+  def context_multi_transition_fn(self, contexts, **kwargs):
+    """Returns multiple future contexts starting from a batch."""
+    assert self._context_multi_transition_fn
+    return self._context_multi_transition_fn(contexts, None, None, **kwargs)
+
+  def step(self, mode, agent=None, action_fn=None, **kwargs):
+    """Returns [next_contexts..., next_timer] list of ops.
+
+    Args:
+      mode: a string representing the mode=[train, explore, eval].
+      **kwargs: kwargs for context_transition_fn.
+    Returns:
+      a list of ops that set the context.
+    """
+    if agent is None:
+      ops = []
+      if self._context_transition_fn is not None:
+        def sampler_fn():
+          samples = self.sample_contexts(mode, 1)[0]
+          return [s[0] for s in samples]
+        values = self._context_transition_fn(self.vars, self.t, sampler_fn, **kwargs)
+        ops += [tf.assign(var, value) for var, value in zip(self.vars, values)]
+      ops.append(tf.assign_add(self.t, 1))  # increment timer
+      return ops
+    else:
+      ops = agent.tf_context.step(mode, **kwargs)
+      state = kwargs['state']
+      next_state = kwargs['next_state']
+      state_repr = kwargs['state_repr']
+      next_state_repr = kwargs['next_state_repr']
+      with tf.control_dependencies(ops):  # Step high level context before computing low level one.
+        # Get the context transition function output.
+        values = self._context_transition_fn(self.vars, self.t, None,
+                                             state=state_repr,
+                                             next_state=next_state_repr)
+        # Select a new goal every C steps, otherwise use context transition.
+        low_level_context = [
+            tf.cond(tf.equal(self.t % self.meta_action_every_n, 0),
+                    lambda: tf.cast(action_fn(next_state, context=None), tf.float32),
+                    lambda: values)]
+        ops = [tf.assign(var, value)
+               for var, value in zip(self.vars, low_level_context)]
+        with tf.control_dependencies(ops):
+          return [tf.assign_add(self.t, 1)]  # increment timer
+        return ops
+
+  def reset(self, mode, agent=None, action_fn=None, state=None):
+    """Returns ops that reset the context.
+
+    Args:
+      mode: a string representing the mode=[train, explore, eval].
+    Returns:
+      a list of ops that reset the context.
+    """
+    if agent is None:
+      values = self.sample_contexts(mode=mode, batch_size=1)[0]
+      if values is None:
+        return []
+      values = [value[0] for value in values]
+      values[0] = uvf_utils.tf_print(
+          values[0],
+          values,
+          message='context:reset, mode=%s' % mode,
+          first_n=10,
+          name='context:reset:%s' % mode)
+      all_ops = []
+      for _, context_vars in sorted(self.context_vars.items()):
+        ops = [tf.assign(var, value) for var, value in zip(context_vars, values)]
+      all_ops += ops
+      all_ops.append(self.set_env_context_op(values))
+      all_ops.append(tf.assign(self.t, 0))  # reset timer
+      return all_ops
+    else:
+      ops = agent.tf_context.reset(mode)
+      # NOTE: The code is currently written in such a way that the higher level
+      # policy does not provide a low-level context until the second
+      # observation.  Insead, we just zero-out low-level contexts.
+      for key, context_vars in sorted(self.context_vars.items()):
+        ops += [tf.assign(var, tf.zeros_like(var)) for var, meta_var in
+                zip(context_vars, agent.tf_context.context_vars[key])]
+
+      ops.append(tf.assign(self.t, 0))  # reset timer
+      return ops
+
+  def create_vars(self, name, agent=None):
+    """Create tf variables for contexts.
+
+    Args:
+      name: Name of the variables.
+    Returns:
+      A list of [num_context_dims] tensors.
+    """
+    if agent is not None:
+      meta_vars = agent.create_vars(name)
+    else:
+      meta_vars = {}
+    assert name not in self.context_vars, ('Conflict! %s is already '
+                                           'initialized.') % name
+    self.context_vars[name] = tuple([
+        tf.Variable(
+            tf.zeros(shape=spec.shape, dtype=spec.dtype),
+            name='%s_context_%d' % (name, i))
+        for i, spec in enumerate(self.context_specs)
+    ])
+    return self.context_vars[name], meta_vars
+
+  @property
+  def n(self):
+    return len(self.context_specs)
+
+  @property
+  def vars(self):
+    return self.context_vars[self.VAR_NAME]
+
+  # pylint: disable=protected-access
+  @property
+  def gym_env(self):
+    return self._tf_env.pyenv._gym_env
+
+  @property
+  def tf_env(self):
+    return self._tf_env
+  # pylint: enable=protected-access
diff --git a/research/efficient-hrl/context/context_transition_functions.py b/research/efficient-hrl/context/context_transition_functions.py
new file mode 100644
index 00000000..70326deb
--- /dev/null
+++ b/research/efficient-hrl/context/context_transition_functions.py
@@ -0,0 +1,123 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Context functions.
+
+Given the current contexts, timer and context sampler, returns new contexts
+  after an environment step. This can be used to define a high-level policy
+  that controls contexts as its actions.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+import gin.tf
+import utils as uvf_utils
+
+
+@gin.configurable
+def periodic_context_fn(contexts, timer, sampler_fn, period=1):
+  """Periodically samples contexts.
+
+  Args:
+    contexts: a list of [num_context_dims] tensor variables representing
+      current contexts.
+    timer: a scalar integer tensor variable holding the current time step.
+    sampler_fn: a sampler function that samples a list of [num_context_dims]
+      tensors.
+    period: (integer) period of update.
+  Returns:
+    a list of [num_context_dims] tensors.
+  """
+  contexts = list(contexts[:])  # create copy
+  return tf.cond(tf.mod(timer, period) == 0, sampler_fn, lambda: contexts)
+
+
+@gin.configurable
+def timer_context_fn(contexts,
+                     timer,
+                     sampler_fn,
+                     period=1,
+                     timer_index=-1,
+                     debug=False):
+  """Samples contexts based on timer in contexts.
+
+  Args:
+    contexts: a list of [num_context_dims] tensor variables representing
+      current contexts.
+    timer: a scalar integer tensor variable holding the current time step.
+    sampler_fn: a sampler function that samples a list of [num_context_dims]
+      tensors.
+    period: (integer) period of update; actual period = `period` + 1.
+    timer_index: (integer) Index of context list that present timer.
+    debug: (boolean) Print debug messages.
+  Returns:
+    a list of [num_context_dims] tensors.
+  """
+  contexts = list(contexts[:])  # create copy
+  cond = tf.equal(contexts[timer_index][0], 0)
+  def reset():
+    """Sample context and reset the timer."""
+    new_contexts = sampler_fn()
+    new_contexts[timer_index] = tf.zeros_like(
+        contexts[timer_index]) + period
+    return new_contexts
+  def update():
+    """Decrement the timer."""
+    contexts[timer_index] -= 1
+    return contexts
+  values = tf.cond(cond, reset, update)
+  if debug:
+    values[0] = uvf_utils.tf_print(
+        values[0],
+        values + [timer],
+        'timer_context_fn',
+        first_n=200,
+        name='timer_context_fn:contexts')
+  return values
+
+
+@gin.configurable
+def relative_context_transition_fn(
+    contexts, timer, sampler_fn,
+    k=2, state=None, next_state=None,
+    **kwargs):
+  """Contexts updated to be relative to next state.
+  """
+  contexts = list(contexts[:])  # create copy
+  assert len(contexts) == 1
+  new_contexts = [
+      tf.concat(
+          [contexts[0][:k] + state[:k] - next_state[:k],
+           contexts[0][k:]], -1)]
+  return new_contexts
+
+
+@gin.configurable
+def relative_context_multi_transition_fn(
+    contexts, timer, sampler_fn,
+    k=2, states=None,
+    **kwargs):
+  """Given contexts at first state and sequence of states, derives sequence of all contexts.
+  """
+  contexts = list(contexts[:])  # create copy
+  assert len(contexts) == 1
+  contexts = [
+      tf.concat(
+          [tf.expand_dims(contexts[0][:, :k] + states[:, 0, :k], 1) - states[:, :, :k],
+           contexts[0][:, None, k:] * tf.ones_like(states[:, :, :1])], -1)]
+  return contexts
diff --git a/research/efficient-hrl/context/gin_imports.py b/research/efficient-hrl/context/gin_imports.py
new file mode 100644
index 00000000..94512cef
--- /dev/null
+++ b/research/efficient-hrl/context/gin_imports.py
@@ -0,0 +1,25 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Import gin configurable modules.
+"""
+
+# pylint: disable=unused-import
+from context import context
+from context import context_transition_functions
+from context import gin_utils
+from context import rewards_functions
+from context import samplers
+# pylint: disable=unused-import
diff --git a/research/efficient-hrl/context/gin_utils.py b/research/efficient-hrl/context/gin_utils.py
new file mode 100644
index 00000000..ab7c1b2d
--- /dev/null
+++ b/research/efficient-hrl/context/gin_utils.py
@@ -0,0 +1,45 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Gin configurable utility functions.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import gin.tf
+
+
+@gin.configurable
+def gin_sparse_array(size, values, indices, fill_value=0):
+  arr = np.zeros(size)
+  arr.fill(fill_value)
+  arr[indices] = values
+  return arr
+
+
+@gin.configurable
+def gin_sum(values):
+  result = values[0]
+  for value in values[1:]:
+    result += value
+  return result
+
+
+@gin.configurable
+def gin_range(n):
+  return range(n)
diff --git a/research/efficient-hrl/context/rewards_functions.py b/research/efficient-hrl/context/rewards_functions.py
new file mode 100644
index 00000000..ab560a7f
--- /dev/null
+++ b/research/efficient-hrl/context/rewards_functions.py
@@ -0,0 +1,741 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Reward shaping functions used by Contexts.
+
+  Each reward function should take the following inputs and return new rewards,
+    and discounts.
+
+  new_rewards, discounts = reward_fn(states, actions, rewards,
+    next_states, contexts)
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+import gin.tf
+
+
+def summarize_stats(stats):
+  """Summarize a dictionary of variables.
+
+  Args:
+    stats: a dictionary of {name: tensor} to compute stats over.
+  """
+  for name, stat in stats.items():
+    mean = tf.reduce_mean(stat)
+    tf.summary.scalar('mean_%s' % name, mean)
+    tf.summary.scalar('max_%s' % name, tf.reduce_max(stat))
+    tf.summary.scalar('min_%s' % name, tf.reduce_min(stat))
+    std = tf.sqrt(tf.reduce_mean(tf.square(stat)) - tf.square(mean) + 1e-10)
+    tf.summary.scalar('std_%s' % name, std)
+    tf.summary.histogram(name, stat)
+
+
+def index_states(states, indices):
+  """Return indexed states.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    indices: (a list of Numpy integer array) Indices of states dimensions
+      to be mapped.
+  Returns:
+    A [batch_size, num_indices] Tensor representing the batch of indexed states.
+  """
+  if indices is None:
+    return states
+  indices = tf.constant(indices, dtype=tf.int32)
+  return tf.gather(states, indices=indices, axis=1)
+
+
+def record_tensor(tensor, indices, stats, name='states'):
+  """Record specified tensor dimensions into stats.
+
+  Args:
+    tensor: A [batch_size, num_dims] Tensor.
+    indices: (a list of integers) Indices of dimensions to record.
+    stats: A dictionary holding stats.
+    name: (string) Name of tensor.
+  """
+  if indices is None:
+    indices = range(tensor.shape.as_list()[1])
+  for index in indices:
+    stats['%s_%02d' % (name, index)] = tensor[:, index]
+
+
+@gin.configurable
+def potential_rewards(states,
+                      actions,
+                      rewards,
+                      next_states,
+                      contexts,
+                      gamma=1.0,
+                      reward_fn=None):
+  """Return the potential-based rewards.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    gamma: Reward discount.
+    reward_fn: A reward function.
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del actions  # unused args
+  gamma = tf.to_float(gamma)
+  rewards_tp1, discounts = reward_fn(None, None, rewards, next_states, contexts)
+  rewards, _ = reward_fn(None, None, rewards, states, contexts)
+  return -rewards + gamma * rewards_tp1, discounts
+
+
+@gin.configurable
+def timed_rewards(states,
+                  actions,
+                  rewards,
+                  next_states,
+                  contexts,
+                  reward_fn=None,
+                  dense=False,
+                  timer_index=-1):
+  """Return the timed rewards.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    reward_fn: A reward function.
+    dense: (boolean) Provide dense rewards or sparse rewards at time = 0.
+    timer_index: (integer) The context list index that specifies timer.
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  assert contexts[timer_index].get_shape().as_list()[1] == 1
+  timers = contexts[timer_index][:, 0]
+  rewards, discounts = reward_fn(states, actions, rewards, next_states,
+                                 contexts)
+  terminates = tf.to_float(timers <= 0)  # if terminate set 1, else set 0
+  for _ in range(rewards.shape.ndims - 1):
+    terminates = tf.expand_dims(terminates, axis=-1)
+  if not dense:
+    rewards *= terminates  # if terminate, return rewards, else return 0
+  discounts *= (tf.to_float(1.0) - terminates)
+  return rewards, discounts
+
+
+@gin.configurable
+def reset_rewards(states,
+                  actions,
+                  rewards,
+                  next_states,
+                  contexts,
+                  reset_index=0,
+                  reset_state=None,
+                  reset_reward_function=None,
+                  include_forward_rewards=True,
+                  include_reset_rewards=True):
+  """Returns the rewards for a forward/reset agent.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    reset_index: (integer) The context list index that specifies reset.
+    reset_state: Reset state.
+    reset_reward_function: Reward function for reset step.
+    include_forward_rewards: Include the rewards from the forward pass.
+    include_reset_rewards: Include the rewards from the reset pass.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  reset_state = tf.constant(
+      reset_state, dtype=next_states.dtype, shape=next_states.shape)
+  reset_states = tf.expand_dims(reset_state, 0)
+
+  def true_fn():
+    if include_reset_rewards:
+      return reset_reward_function(states, actions, rewards, next_states,
+                                   [reset_states] + contexts[1:])
+    else:
+      return tf.zeros_like(rewards), tf.ones_like(rewards)
+
+  def false_fn():
+    if include_forward_rewards:
+      return plain_rewards(states, actions, rewards, next_states, contexts)
+    else:
+      return tf.zeros_like(rewards), tf.ones_like(rewards)
+
+  rewards, discounts = tf.cond(
+      tf.cast(contexts[reset_index][0, 0], dtype=tf.bool), true_fn, false_fn)
+  return rewards, discounts
+
+
+@gin.configurable
+def tanh_similarity(states,
+                    actions,
+                    rewards,
+                    next_states,
+                    contexts,
+                    mse_scale=1.0,
+                    state_scales=1.0,
+                    goal_scales=1.0,
+                    summarize=False):
+  """Returns the similarity between next_states and contexts using tanh and mse.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    mse_scale: A float, to scale mse before tanh.
+    state_scales: multiplicative scale for (next) states. A scalar or 1D tensor,
+      must be broadcastable to number of state dimensions.
+    goal_scales: multiplicative scale for contexts. A scalar or 1D tensor,
+      must be broadcastable to number of goal dimensions.
+    summarize: (boolean) enable summary ops.
+
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, actions, rewards  # Unused
+  mse = tf.reduce_mean(tf.squared_difference(next_states * state_scales,
+                                             contexts[0] * goal_scales), -1)
+  tanh = tf.tanh(mse_scale * mse)
+  if summarize:
+    with tf.name_scope('RewardFn/'):
+      tf.summary.scalar('mean_mse', tf.reduce_mean(mse))
+      tf.summary.histogram('mse', mse)
+      tf.summary.scalar('mean_tanh', tf.reduce_mean(tanh))
+      tf.summary.histogram('tanh', tanh)
+  rewards = tf.to_float(1 - tanh)
+  return rewards, tf.ones_like(rewards)
+
+
+@gin.configurable
+def negative_mse(states,
+                 actions,
+                 rewards,
+                 next_states,
+                 contexts,
+                 state_scales=1.0,
+                 goal_scales=1.0,
+                 summarize=False):
+  """Returns the negative mean square error between next_states and contexts.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    state_scales: multiplicative scale for (next) states. A scalar or 1D tensor,
+      must be broadcastable to number of state dimensions.
+    goal_scales: multiplicative scale for contexts. A scalar or 1D tensor,
+      must be broadcastable to number of goal dimensions.
+    summarize: (boolean) enable summary ops.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, actions, rewards  # Unused
+  mse = tf.reduce_mean(tf.squared_difference(next_states * state_scales,
+                                             contexts[0] * goal_scales), -1)
+  if summarize:
+    with tf.name_scope('RewardFn/'):
+      tf.summary.scalar('mean_mse', tf.reduce_mean(mse))
+      tf.summary.histogram('mse', mse)
+  rewards = tf.to_float(-mse)
+  return rewards, tf.ones_like(rewards)
+
+
+@gin.configurable
+def negative_distance(states,
+                      actions,
+                      rewards,
+                      next_states,
+                      contexts,
+                      state_scales=1.0,
+                      goal_scales=1.0,
+                      reward_scales=1.0,
+                      weight_index=None,
+                      weight_vector=None,
+                      summarize=False,
+                      termination_epsilon=1e-4,
+                      state_indices=None,
+                      goal_indices=None,
+                      vectorize=False,
+                      relative_context=False,
+                      diff=False,
+                      norm='L2',
+                      epsilon=1e-10,
+                      bonus_epsilon=0., #5.,
+                      offset=0.0):
+  """Returns the negative euclidean distance between next_states and contexts.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    state_scales: multiplicative scale for (next) states. A scalar or 1D tensor,
+      must be broadcastable to number of state dimensions.
+    goal_scales: multiplicative scale for goals. A scalar or 1D tensor,
+      must be broadcastable to number of goal dimensions.
+    reward_scales: multiplicative scale for rewards. A scalar or 1D tensor,
+      must be broadcastable to number of reward dimensions.
+    weight_index: (integer) The context list index that specifies weight.
+    weight_vector: (a number or a list or Numpy array) The weighting vector,
+      broadcastable to `next_states`.
+    summarize: (boolean) enable summary ops.
+    termination_epsilon: terminate if dist is less than this quantity.
+    state_indices: (a list of integers) list of state indices to select.
+    goal_indices: (a list of integers) list of goal indices to select.
+    vectorize: Return a vectorized form.
+    norm: L1 or L2.
+    epsilon: small offset to ensure non-negative/zero distance.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del actions, rewards  # Unused
+  stats = {}
+  record_tensor(next_states, state_indices, stats, 'next_states')
+  states = index_states(states, state_indices)
+  next_states = index_states(next_states, state_indices)
+  goals = index_states(contexts[0], goal_indices)
+  if relative_context:
+    goals = states + goals
+  sq_dists = tf.squared_difference(next_states * state_scales,
+                                   goals * goal_scales)
+  old_sq_dists = tf.squared_difference(states * state_scales,
+                                       goals * goal_scales)
+  record_tensor(sq_dists, None, stats, 'sq_dists')
+  if weight_vector is not None:
+    sq_dists *= tf.convert_to_tensor(weight_vector, dtype=next_states.dtype)
+    old_sq_dists *= tf.convert_to_tensor(weight_vector, dtype=next_states.dtype)
+  if weight_index is not None:
+    #sq_dists *= contexts[weight_index]
+    weights = tf.abs(index_states(contexts[0], weight_index))
+    #weights /= tf.reduce_sum(weights, -1, keepdims=True)
+    sq_dists *= weights
+    old_sq_dists *= weights
+  if norm == 'L1':
+    dist = tf.sqrt(sq_dists + epsilon)
+    old_dist = tf.sqrt(old_sq_dists + epsilon)
+    if not vectorize:
+      dist = tf.reduce_sum(dist, -1)
+      old_dist = tf.reduce_sum(old_dist, -1)
+  elif norm == 'L2':
+    if vectorize:
+      dist = sq_dists
+      old_dist = old_sq_dists
+    else:
+      dist = tf.reduce_sum(sq_dists, -1)
+      old_dist = tf.reduce_sum(old_sq_dists, -1)
+    dist = tf.sqrt(dist + epsilon)  # tf.gradients fails when tf.sqrt(-0.0)
+    old_dist = tf.sqrt(old_dist + epsilon)  # tf.gradients fails when tf.sqrt(-0.0)
+  else:
+    raise NotImplementedError(norm)
+  discounts = dist > termination_epsilon
+  if summarize:
+    with tf.name_scope('RewardFn/'):
+      tf.summary.scalar('mean_dist', tf.reduce_mean(dist))
+      tf.summary.histogram('dist', dist)
+      summarize_stats(stats)
+  bonus = tf.to_float(dist < bonus_epsilon)
+  dist *= reward_scales
+  old_dist *= reward_scales
+  if diff:
+    return bonus + offset + tf.to_float(old_dist - dist), tf.to_float(discounts)
+  return bonus + offset + tf.to_float(-dist), tf.to_float(discounts)
+
+
+@gin.configurable
+def cosine_similarity(states,
+                      actions,
+                      rewards,
+                      next_states,
+                      contexts,
+                      state_scales=1.0,
+                      goal_scales=1.0,
+                      reward_scales=1.0,
+                      normalize_states=True,
+                      normalize_goals=True,
+                      weight_index=None,
+                      weight_vector=None,
+                      summarize=False,
+                      state_indices=None,
+                      goal_indices=None,
+                      offset=0.0):
+  """Returns the cosine similarity between next_states - states and contexts.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    state_scales: multiplicative scale for (next) states. A scalar or 1D tensor,
+      must be broadcastable to number of state dimensions.
+    goal_scales: multiplicative scale for goals. A scalar or 1D tensor,
+      must be broadcastable to number of goal dimensions.
+    reward_scales: multiplicative scale for rewards. A scalar or 1D tensor,
+      must be broadcastable to number of reward dimensions.
+    weight_index: (integer) The context list index that specifies weight.
+    weight_vector: (a number or a list or Numpy array) The weighting vector,
+      broadcastable to `next_states`.
+    summarize: (boolean) enable summary ops.
+    termination_epsilon: terminate if dist is less than this quantity.
+    state_indices: (a list of integers) list of state indices to select.
+    goal_indices: (a list of integers) list of goal indices to select.
+    vectorize: Return a vectorized form.
+    norm: L1 or L2.
+    epsilon: small offset to ensure non-negative/zero distance.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del actions, rewards  # Unused
+  stats = {}
+  record_tensor(next_states, state_indices, stats, 'next_states')
+  states = index_states(states, state_indices)
+  next_states = index_states(next_states, state_indices)
+  goals = index_states(contexts[0], goal_indices)
+
+  if weight_vector is not None:
+    goals *= tf.convert_to_tensor(weight_vector, dtype=next_states.dtype)
+  if weight_index is not None:
+    weights = tf.abs(index_states(contexts[0], weight_index))
+    goals *= weights
+
+  direction_vec = next_states - states
+  if normalize_states:
+    direction_vec = tf.nn.l2_normalize(direction_vec, -1)
+  goal_vec = goals
+  if normalize_goals:
+    goal_vec = tf.nn.l2_normalize(goal_vec, -1)
+
+  similarity = tf.reduce_sum(goal_vec * direction_vec, -1)
+  discounts = tf.ones_like(similarity)
+  return offset + tf.to_float(similarity), tf.to_float(discounts)
+
+
+@gin.configurable
+def diff_distance(states,
+                  actions,
+                  rewards,
+                  next_states,
+                  contexts,
+                  state_scales=1.0,
+                  goal_scales=1.0,
+                  reward_scales=1.0,
+                  weight_index=None,
+                  weight_vector=None,
+                  summarize=False,
+                  termination_epsilon=1e-4,
+                  state_indices=None,
+                  goal_indices=None,
+                  norm='L2',
+                  epsilon=1e-10):
+  """Returns the difference in euclidean distance between states/next_states and contexts.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    state_scales: multiplicative scale for (next) states. A scalar or 1D tensor,
+      must be broadcastable to number of state dimensions.
+    goal_scales: multiplicative scale for goals. A scalar or 1D tensor,
+      must be broadcastable to number of goal dimensions.
+    reward_scales: multiplicative scale for rewards. A scalar or 1D tensor,
+      must be broadcastable to number of reward dimensions.
+    weight_index: (integer) The context list index that specifies weight.
+    weight_vector: (a number or a list or Numpy array) The weighting vector,
+      broadcastable to `next_states`.
+    summarize: (boolean) enable summary ops.
+    termination_epsilon: terminate if dist is less than this quantity.
+    state_indices: (a list of integers) list of state indices to select.
+    goal_indices: (a list of integers) list of goal indices to select.
+    vectorize: Return a vectorized form.
+    norm: L1 or L2.
+    epsilon: small offset to ensure non-negative/zero distance.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del actions, rewards  # Unused
+  stats = {}
+  record_tensor(next_states, state_indices, stats, 'next_states')
+  next_states = index_states(next_states, state_indices)
+  states = index_states(states, state_indices)
+  goals = index_states(contexts[0], goal_indices)
+  next_sq_dists = tf.squared_difference(next_states * state_scales,
+                                        goals * goal_scales)
+  sq_dists = tf.squared_difference(states * state_scales,
+                                   goals * goal_scales)
+  record_tensor(sq_dists, None, stats, 'sq_dists')
+  if weight_vector is not None:
+    next_sq_dists *= tf.convert_to_tensor(weight_vector, dtype=next_states.dtype)
+    sq_dists *= tf.convert_to_tensor(weight_vector, dtype=next_states.dtype)
+  if weight_index is not None:
+    next_sq_dists *= contexts[weight_index]
+    sq_dists *= contexts[weight_index]
+  if norm == 'L1':
+    next_dist = tf.sqrt(next_sq_dists + epsilon)
+    dist = tf.sqrt(sq_dists + epsilon)
+    next_dist = tf.reduce_sum(next_dist, -1)
+    dist = tf.reduce_sum(dist, -1)
+  elif norm == 'L2':
+    next_dist = tf.reduce_sum(next_sq_dists, -1)
+    next_dist = tf.sqrt(next_dist + epsilon)  # tf.gradients fails when tf.sqrt(-0.0)
+    dist = tf.reduce_sum(sq_dists, -1)
+    dist = tf.sqrt(dist + epsilon)  # tf.gradients fails when tf.sqrt(-0.0)
+  else:
+    raise NotImplementedError(norm)
+  discounts = next_dist > termination_epsilon
+  if summarize:
+    with tf.name_scope('RewardFn/'):
+      tf.summary.scalar('mean_dist', tf.reduce_mean(dist))
+      tf.summary.histogram('dist', dist)
+      summarize_stats(stats)
+  diff = dist - next_dist
+  diff *= reward_scales
+  return tf.to_float(diff), tf.to_float(discounts)
+
+
+@gin.configurable
+def binary_indicator(states,
+                     actions,
+                     rewards,
+                     next_states,
+                     contexts,
+                     termination_epsilon=1e-4,
+                     offset=0,
+                     epsilon=1e-10,
+                     state_indices=None,
+                     summarize=False):
+  """Returns 0/1 by checking if next_states and contexts overlap.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    termination_epsilon: terminate if dist is less than this quantity.
+    offset: Offset the rewards.
+    epsilon: small offset to ensure non-negative/zero distance.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, actions  # unused args
+  next_states = index_states(next_states, state_indices)
+  dist = tf.reduce_sum(tf.squared_difference(next_states, contexts[0]), -1)
+  dist = tf.sqrt(dist + epsilon)
+  discounts = dist > termination_epsilon
+  rewards = tf.logical_not(discounts)
+  rewards = tf.to_float(rewards) + offset
+  return tf.to_float(rewards), tf.ones_like(tf.to_float(discounts)) #tf.to_float(discounts)
+
+
+@gin.configurable
+def plain_rewards(states, actions, rewards, next_states, contexts):
+  """Returns the given rewards.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, actions, next_states, contexts  # Unused
+  return rewards, tf.ones_like(rewards)
+
+
+@gin.configurable
+def ctrl_rewards(states,
+                 actions,
+                 rewards,
+                 next_states,
+                 contexts,
+                 reward_scales=1.0):
+  """Returns the negative control cost.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    reward_scales: multiplicative scale for rewards. A scalar or 1D tensor,
+      must be broadcastable to number of reward dimensions.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, rewards, contexts  # Unused
+  if actions is None:
+    rewards = tf.to_float(tf.zeros(shape=next_states.shape[:1]))
+  else:
+    rewards = -tf.reduce_sum(tf.square(actions), axis=1)
+    rewards *= reward_scales
+    rewards = tf.to_float(rewards)
+  return rewards, tf.ones_like(rewards)
+
+
+@gin.configurable
+def diff_rewards(
+    states,
+    actions,
+    rewards,
+    next_states,
+    contexts,
+    state_indices=None,
+    goal_index=0,):
+  """Returns (next_states - goals) as a batched vector reward."""
+  del states, rewards, actions  # Unused
+  if state_indices is not None:
+    next_states = index_states(next_states, state_indices)
+  rewards = tf.to_float(next_states - contexts[goal_index])
+  return rewards, tf.ones_like(rewards)
+
+
+@gin.configurable
+def state_rewards(states,
+                  actions,
+                  rewards,
+                  next_states,
+                  contexts,
+                  weight_index=None,
+                  state_indices=None,
+                  weight_vector=1.0,
+                  offset_vector=0.0,
+                  summarize=False):
+  """Returns the rewards that are linear mapping of next_states.
+
+  Args:
+    states: A [batch_size, num_state_dims] Tensor representing a batch
+        of states.
+    actions: A [batch_size, num_action_dims] Tensor representing a batch
+      of actions.
+    rewards: A [batch_size] Tensor representing a batch of rewards.
+    next_states: A [batch_size, num_state_dims] Tensor representing a batch
+      of next states.
+    contexts: A list of [batch_size, num_context_dims] Tensor representing
+      a batch of contexts.
+    weight_index: (integer) Index of contexts lists that specify weighting.
+    state_indices: (a list of Numpy integer array) Indices of states dimensions
+      to be mapped.
+    weight_vector: (a number or a list or Numpy array) The weighting vector,
+      broadcastable to `next_states`.
+    offset_vector: (a number or a list of Numpy array) The off vector.
+    summarize: (boolean) enable summary ops.
+
+  Returns:
+    A new tf.float32 [batch_size] rewards Tensor, and
+      tf.float32 [batch_size] discounts tensor.
+  """
+  del states, actions, rewards  # unused args
+  stats = {}
+  record_tensor(next_states, state_indices, stats)
+  next_states = index_states(next_states, state_indices)
+  weight = tf.constant(
+      weight_vector, dtype=next_states.dtype, shape=next_states[0].shape)
+  weights = tf.expand_dims(weight, 0)
+  offset = tf.constant(
+      offset_vector, dtype=next_states.dtype, shape=next_states[0].shape)
+  offsets = tf.expand_dims(offset, 0)
+  if weight_index is not None:
+    weights *= contexts[weight_index]
+  rewards = tf.to_float(tf.reduce_sum(weights * (next_states+offsets), axis=1))
+  if summarize:
+    with tf.name_scope('RewardFn/'):
+      summarize_stats(stats)
+  return rewards, tf.ones_like(rewards)
diff --git a/research/efficient-hrl/context/samplers.py b/research/efficient-hrl/context/samplers.py
new file mode 100644
index 00000000..15a22df5
--- /dev/null
+++ b/research/efficient-hrl/context/samplers.py
@@ -0,0 +1,445 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Samplers for Contexts.
+
+  Each sampler class should define __call__(batch_size).
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import numpy as np
+import tensorflow as tf
+slim = tf.contrib.slim
+import gin.tf
+
+
+@gin.configurable
+class BaseSampler(object):
+  """Base sampler."""
+
+  def __init__(self, context_spec, context_range=None, k=2, scope='sampler'):
+    """Construct a base sampler.
+
+    Args:
+      context_spec: A context spec.
+      context_range: A tuple of (minval, max), where minval, maxval are floats
+        or Numpy arrays with the same shape as the context.
+      scope: A string denoting scope.
+    """
+    self._context_spec = context_spec
+    self._context_range = context_range
+    self._k = k
+    self._scope = scope
+
+  def __call__(self, batch_size, **kwargs):
+    raise NotImplementedError
+
+  def set_replay(self, replay=None):
+    pass
+
+  def _validate_contexts(self, contexts):
+    """Validate if contexts have right spec.
+
+    Args:
+      contexts: A [batch_size, num_contexts_dim] tensor.
+    Raises:
+      ValueError: If shape or dtype mismatches that of spec.
+    """
+    if contexts[0].shape != self._context_spec.shape:
+      raise ValueError('contexts has invalid shape %s wrt spec shape %s' %
+                       (contexts[0].shape, self._context_spec.shape))
+    if contexts.dtype != self._context_spec.dtype:
+      raise ValueError('contexts has invalid dtype %s wrt spec dtype %s' %
+                       (contexts.dtype, self._context_spec.dtype))
+
+
+@gin.configurable
+class ZeroSampler(BaseSampler):
+  """Zero sampler."""
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    contexts = tf.zeros(
+        dtype=self._context_spec.dtype,
+        shape=[
+            batch_size,
+        ] + self._context_spec.shape.as_list())
+    return contexts, contexts
+
+
+@gin.configurable
+class BinarySampler(BaseSampler):
+  """Binary sampler."""
+
+  def __init__(self, probs=0.5, *args, **kwargs):
+    """Constructor."""
+    super(BinarySampler, self).__init__(*args, **kwargs)
+    self._probs = probs
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context."""
+    spec = self._context_spec
+    contexts = tf.random_uniform(
+        shape=[
+            batch_size,
+        ] + spec.shape.as_list(), dtype=tf.float32)
+    contexts = tf.cast(tf.greater(contexts, self._probs), dtype=spec.dtype)
+    return contexts, contexts
+
+
+@gin.configurable
+class RandomSampler(BaseSampler):
+  """Random sampler."""
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    spec = self._context_spec
+    context_range = self._context_range
+    if isinstance(context_range[0], (int, float)):
+      contexts = tf.random_uniform(
+          shape=[
+              batch_size,
+          ] + spec.shape.as_list(),
+          minval=context_range[0],
+          maxval=context_range[1],
+          dtype=spec.dtype)
+    elif isinstance(context_range[0], (list, tuple, np.ndarray)):
+      assert len(spec.shape.as_list()) == 1
+      assert spec.shape.as_list()[0] == len(context_range[0])
+      assert spec.shape.as_list()[0] == len(context_range[1])
+      contexts = tf.concat(
+          [
+              tf.random_uniform(
+                  shape=[
+                      batch_size, 1,
+                  ] + spec.shape.as_list()[1:],
+                  minval=context_range[0][i],
+                  maxval=context_range[1][i],
+                  dtype=spec.dtype) for i in range(spec.shape.as_list()[0])
+          ],
+          axis=1)
+    else: raise NotImplementedError(context_range)
+    self._validate_contexts(contexts)
+    state, next_state = kwargs['state'], kwargs['next_state']
+    if state is not None and next_state is not None:
+      pass
+      #contexts = tf.concat(
+      #    [tf.random_normal(tf.shape(state[:, :self._k]), dtype=tf.float64) +
+      #     tf.random_shuffle(state[:, :self._k]),
+      #     contexts[:, self._k:]], 1)
+
+    return contexts, contexts
+
+
+@gin.configurable
+class ScheduledSampler(BaseSampler):
+  """Scheduled sampler."""
+
+  def __init__(self,
+               scope='default',
+               values=None,
+               scheduler='cycle',
+               scheduler_params=None,
+               *args, **kwargs):
+    """Construct sampler.
+
+    Args:
+      scope: Scope name.
+      values: A list of numbers or [num_context_dim] Numpy arrays
+        representing the values to cycle.
+      scheduler: scheduler type.
+      scheduler_params: scheduler parameters.
+      *args: arguments.
+      **kwargs: keyword arguments.
+    """
+    super(ScheduledSampler, self).__init__(*args, **kwargs)
+    self._scope = scope
+    self._values = values
+    self._scheduler = scheduler
+    self._scheduler_params = scheduler_params or {}
+    assert self._values is not None and len(
+        self._values), 'must provide non-empty values.'
+    self._n = len(self._values)
+    # TODO(shanegu): move variable creation outside. resolve tf.cond problem.
+    self._count = 0
+    self._i = tf.Variable(
+        tf.zeros(shape=(), dtype=tf.int32),
+        name='%s-scheduled_sampler_%d' % (self._scope, self._count))
+    self._values = tf.constant(self._values, dtype=self._context_spec.dtype)
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    spec = self._context_spec
+    next_op = self._next(self._i)
+    with tf.control_dependencies([next_op]):
+      value = self._values[self._i]
+      if value.get_shape().as_list():
+        values = tf.tile(
+            tf.expand_dims(value, 0), (batch_size,) + (1,) * spec.shape.ndims)
+      else:
+        values = value + tf.zeros(
+            shape=[
+                batch_size,
+            ] + spec.shape.as_list(), dtype=spec.dtype)
+    self._validate_contexts(values)
+    self._count += 1
+    return values, values
+
+  def _next(self, i):
+    """Return op that increments pointer to next value.
+
+    Args:
+      i: A tensorflow integer variable.
+    Returns:
+      Op that increments pointer.
+    """
+    if self._scheduler == 'cycle':
+      inc = ('inc' in self._scheduler_params and
+             self._scheduler_params['inc']) or 1
+      return tf.assign(i, tf.mod(i+inc, self._n))
+    else:
+      raise NotImplementedError(self._scheduler)
+
+
+@gin.configurable
+class ReplaySampler(BaseSampler):
+  """Replay sampler."""
+
+  def __init__(self,
+               prefetch_queue_capacity=2,
+               override_indices=None,
+               state_indices=None,
+               *args,
+               **kwargs):
+    """Construct sampler.
+
+    Args:
+      prefetch_queue_capacity: Capacity for prefetch queue.
+      override_indices: Override indices.
+      state_indices: Select certain indices from state dimension.
+      *args: arguments.
+      **kwargs: keyword arguments.
+    """
+    super(ReplaySampler, self).__init__(*args, **kwargs)
+    self._prefetch_queue_capacity = prefetch_queue_capacity
+    self._override_indices = override_indices
+    self._state_indices = state_indices
+
+  def set_replay(self, replay):
+    """Set replay.
+
+    Args:
+      replay: A replay buffer.
+    """
+    self._replay = replay
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    batch = self._replay.GetRandomBatch(batch_size)
+    next_states = batch[4]
+    if self._prefetch_queue_capacity > 0:
+      batch_queue = slim.prefetch_queue.prefetch_queue(
+          [next_states],
+          capacity=self._prefetch_queue_capacity,
+          name='%s/batch_context_queue' % self._scope)
+      next_states = batch_queue.dequeue()
+    if self._override_indices is not None:
+      assert self._context_range is not None and isinstance(
+          self._context_range[0], (int, long, float))
+      next_states = tf.concat(
+          [
+              tf.random_uniform(
+                  shape=next_states[:, :1].shape,
+                  minval=self._context_range[0],
+                  maxval=self._context_range[1],
+                  dtype=next_states.dtype)
+              if i in self._override_indices else next_states[:, i:i + 1]
+              for i in range(self._context_spec.shape.as_list()[0])
+          ],
+          axis=1)
+    if self._state_indices is not None:
+      next_states = tf.concat(
+          [
+              next_states[:, i:i + 1]
+              for i in range(self._context_spec.shape.as_list()[0])
+          ],
+          axis=1)
+    self._validate_contexts(next_states)
+    return next_states, next_states
+
+
+@gin.configurable
+class TimeSampler(BaseSampler):
+  """Time Sampler."""
+
+  def __init__(self, minval=0, maxval=1, timestep=-1, *args, **kwargs):
+    """Construct sampler.
+
+    Args:
+      minval: Min value integer.
+      maxval: Max value integer.
+      timestep: Time step between states and next_states.
+      *args: arguments.
+      **kwargs: keyword arguments.
+    """
+    super(TimeSampler, self).__init__(*args, **kwargs)
+    assert self._context_spec.shape.as_list() == [1]
+    self._minval = minval
+    self._maxval = maxval
+    self._timestep = timestep
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    if self._maxval == self._minval:
+      contexts = tf.constant(
+          self._maxval, shape=[batch_size, 1], dtype=tf.int32)
+    else:
+      contexts = tf.random_uniform(
+          shape=[batch_size, 1],
+          dtype=tf.int32,
+          maxval=self._maxval,
+          minval=self._minval)
+    next_contexts = tf.maximum(contexts + self._timestep, 0)
+
+    return tf.cast(
+        contexts, dtype=self._context_spec.dtype), tf.cast(
+            next_contexts, dtype=self._context_spec.dtype)
+
+
+@gin.configurable
+class ConstantSampler(BaseSampler):
+  """Constant sampler."""
+
+  def __init__(self, value=None, *args, **kwargs):
+    """Construct sampler.
+
+    Args:
+      value: A list or Numpy array for values of the constant.
+      *args: arguments.
+      **kwargs: keyword arguments.
+    """
+    super(ConstantSampler, self).__init__(*args, **kwargs)
+    self._value = value
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    spec = self._context_spec
+    value_ = tf.constant(self._value, shape=spec.shape, dtype=spec.dtype)
+    values = tf.tile(
+        tf.expand_dims(value_, 0), (batch_size,) + (1,) * spec.shape.ndims)
+    self._validate_contexts(values)
+    return values, values
+
+
+@gin.configurable
+class DirectionSampler(RandomSampler):
+  """Direction sampler."""
+
+  def __call__(self, batch_size, **kwargs):
+    """Sample a batch of context.
+
+    Args:
+      batch_size: Batch size.
+    Returns:
+      Two [batch_size, num_context_dims] tensors.
+    """
+    spec = self._context_spec
+    context_range = self._context_range
+    if isinstance(context_range[0], (int, float)):
+      contexts = tf.random_uniform(
+          shape=[
+              batch_size,
+          ] + spec.shape.as_list(),
+          minval=context_range[0],
+          maxval=context_range[1],
+          dtype=spec.dtype)
+    elif isinstance(context_range[0], (list, tuple, np.ndarray)):
+      assert len(spec.shape.as_list()) == 1
+      assert spec.shape.as_list()[0] == len(context_range[0])
+      assert spec.shape.as_list()[0] == len(context_range[1])
+      contexts = tf.concat(
+          [
+              tf.random_uniform(
+                  shape=[
+                      batch_size, 1,
+                  ] + spec.shape.as_list()[1:],
+                  minval=context_range[0][i],
+                  maxval=context_range[1][i],
+                  dtype=spec.dtype) for i in range(spec.shape.as_list()[0])
+          ],
+          axis=1)
+    else: raise NotImplementedError(context_range)
+    self._validate_contexts(contexts)
+    if 'sampler_fn' in kwargs:
+      other_contexts = kwargs['sampler_fn']()
+    else:
+      other_contexts = contexts
+    state, next_state = kwargs['state'], kwargs['next_state']
+    if state is not None and next_state is not None:
+      my_context_range = (np.array(context_range[1]) - np.array(context_range[0])) / 2 * np.ones(spec.shape.as_list())
+      contexts = tf.concat(
+          [0.1 * my_context_range[:self._k] *
+           tf.random_normal(tf.shape(state[:, :self._k]), dtype=state.dtype) +
+           tf.random_shuffle(state[:, :self._k]) - state[:, :self._k],
+           other_contexts[:, self._k:]], 1)
+      #contexts = tf.Print(contexts,
+      #                    [contexts, tf.reduce_max(contexts, 0),
+      #                     tf.reduce_min(state, 0), tf.reduce_max(state, 0)], 'contexts', summarize=15)
+      next_contexts = tf.concat( #LALA
+          [state[:, :self._k] + contexts[:, :self._k] - next_state[:, :self._k],
+           other_contexts[:, self._k:]], 1)
+      next_contexts = contexts  #LALA cosine
+    else:
+      next_contexts = contexts
+    return tf.stop_gradient(contexts), tf.stop_gradient(next_contexts)
diff --git a/research/efficient-hrl/environments/__init__.py b/research/efficient-hrl/environments/__init__.py
old mode 100755
new mode 100644
index f8377ff5..8b137891
--- a/research/efficient-hrl/environments/__init__.py
+++ b/research/efficient-hrl/environments/__init__.py
@@ -1,129 +1 @@
-# Copyright 2018 The TensorFlow Authors All Rights Reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-# ==============================================================================
 
-"""Random policy on an environment."""
-
-import tensorflow as tf
-import numpy as np
-import random
-
-import create_maze_env
-
-app = tf.app
-flags = tf.flags
-logging = tf.logging
-
-FLAGS = flags.FLAGS
-
-flags.DEFINE_string('env', 'AntMaze', 'environment name: AntMaze, AntPush, or AntFall')
-flags.DEFINE_integer('episode_length', 500, 'episode length')
-flags.DEFINE_integer('num_episodes', 50, 'number of episodes')
-
-
-def get_goal_sample_fn(env_name):
-  if env_name == 'AntMaze':
-    # NOTE: When evaluating (i.e. the metrics shown in the paper,
-    # we use the commented out goal sampling function.  The uncommented
-    # one is only used for training.
-    #return lambda: np.array([0., 16.])
-    return lambda: np.random.uniform((-4, -4), (20, 20))
-  elif env_name == 'AntPush':
-    return lambda: np.array([0., 19.])
-  elif env_name == 'AntFall':
-    return lambda: np.array([0., 27., 4.5])
-  else:
-    assert False, 'Unknown env'
-
-
-def get_reward_fn(env_name):
-  if env_name == 'AntMaze':
-    return lambda obs, goal: -np.sum(np.square(obs[:2] - goal)) ** 0.5
-  elif env_name == 'AntPush':
-    return lambda obs, goal: -np.sum(np.square(obs[:2] - goal)) ** 0.5
-  elif env_name == 'AntFall':
-    return lambda obs, goal: -np.sum(np.square(obs[:3] - goal)) ** 0.5
-  else:
-    assert False, 'Unknown env'
-
-
-def success_fn(last_reward):
-  return last_reward > -5.0
-
-
-class EnvWithGoal(object):
-
-  def __init__(self, base_env, env_name):
-    self.base_env = base_env
-    self.goal_sample_fn = get_goal_sample_fn(env_name)
-    self.reward_fn = get_reward_fn(env_name)
-    self.goal = None
-
-  def reset(self):
-    obs = self.base_env.reset()
-    self.goal = self.goal_sample_fn()
-    return np.concatenate([obs, self.goal])
-
-  def step(self, a):
-    obs, _, done, info = self.base_env.step(a)
-    reward = self.reward_fn(obs, self.goal)
-    return np.concatenate([obs, self.goal]), reward, done, info
-
-  @property
-  def action_space(self):
-    return self.base_env.action_space
-
-
-def run_environment(env_name, episode_length, num_episodes):
-  env = EnvWithGoal(
-      create_maze_env.create_maze_env(env_name),
-      env_name)
-
-  def action_fn(obs):
-    action_space = env.action_space
-    action_space_mean = (action_space.low + action_space.high) / 2.0
-    action_space_magn = (action_space.high - action_space.low) / 2.0
-    random_action = (action_space_mean +
-                     action_space_magn *
-                     np.random.uniform(low=-1.0, high=1.0,
-                                       size=action_space.shape))
-    return random_action
-
-  rewards = []
-  successes = []
-  for ep in range(num_episodes):
-    rewards.append(0.0)
-    successes.append(False)
-    obs = env.reset()
-    for _ in range(episode_length):
-      obs, reward, done, _ = env.step(action_fn(obs))
-      rewards[-1] += reward
-      successes[-1] = success_fn(reward)
-      if done:
-        break
-    logging.info('Episode %d reward: %.2f, Success: %d', ep + 1, rewards[-1], successes[-1])
-
-  logging.info('Average Reward over %d episodes: %.2f',
-               num_episodes, np.mean(rewards))
-  logging.info('Average Success over %d episodes: %.2f',
-               num_episodes, np.mean(successes))
-
-
-def main(unused_argv):
-  logging.set_verbosity(logging.INFO)
-  run_environment(FLAGS.env, FLAGS.episode_length, FLAGS.num_episodes)
-
-
-if __name__ == '__main__':
-  app.run()
diff --git a/research/efficient-hrl/environments/ant.py b/research/efficient-hrl/environments/ant.py
old mode 100755
new mode 100644
index add601ca..bea9b209
--- a/research/efficient-hrl/environments/ant.py
+++ b/research/efficient-hrl/environments/ant.py
@@ -21,8 +21,21 @@ from gym import utils
 from gym.envs.mujoco import mujoco_env
 
 
+def q_inv(a):
+  return [a[0], -a[1], -a[2], -a[3]]
+
+
+def q_mult(a, b): # multiply two quaternion
+  w = a[0] * b[0] - a[1] * b[1] - a[2] * b[2] - a[3] * b[3]
+  i = a[0] * b[1] + a[1] * b[0] + a[2] * b[3] - a[3] * b[2]
+  j = a[0] * b[2] - a[1] * b[3] + a[2] * b[0] + a[3] * b[1]
+  k = a[0] * b[3] + a[1] * b[2] - a[2] * b[1] + a[3] * b[0]
+  return [w, i, j, k]
+
+
 class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle):
   FILE = "ant.xml"
+  ORI_IND = 3
 
   def __init__(self, file_path=None, expose_all_qpos=True,
                expose_body_coms=None, expose_body_comvels=None):
@@ -101,3 +114,21 @@ class AntEnv(mujoco_env.MujocoEnv, utils.EzPickle):
 
   def viewer_setup(self):
     self.viewer.cam.distance = self.model.stat.extent * 0.5
+
+  def get_ori(self):
+    ori = [0, 1, 0, 0]
+    rot = self.model.data.qpos[self.__class__.ORI_IND:self.__class__.ORI_IND + 4]  # take the quaternion
+    ori = q_mult(q_mult(rot, ori), q_inv(rot))[1:3]  # project onto x-y plane
+    ori = math.atan2(ori[1], ori[0])
+    return ori
+
+  def set_xy(self, xy):
+    qpos = np.copy(self.physics.data.qpos)
+    qpos[0] = xy[0]
+    qpos[1] = xy[1]
+
+    qvel = self.physics.data.qvel
+    self.set_state(qpos, qvel)
+
+  def get_xy(self):
+    return self.physics.data.qpos[:2]
diff --git a/research/efficient-hrl/environments/ant_maze_env.py b/research/efficient-hrl/environments/ant_maze_env.py
old mode 100755
new mode 100644
index d7513789..69a10663
--- a/research/efficient-hrl/environments/ant_maze_env.py
+++ b/research/efficient-hrl/environments/ant_maze_env.py
@@ -13,8 +13,8 @@
 # limitations under the License.
 # ==============================================================================
 
-from maze_env import MazeEnv
-from ant import AntEnv
+from environments.maze_env import MazeEnv
+from environments.ant import AntEnv
 
 
 class AntMazeEnv(MazeEnv):
diff --git a/research/efficient-hrl/environments/create_maze_env.py b/research/efficient-hrl/environments/create_maze_env.py
old mode 100755
new mode 100644
index e9e95d7c..f6dc4f42
--- a/research/efficient-hrl/environments/create_maze_env.py
+++ b/research/efficient-hrl/environments/create_maze_env.py
@@ -13,18 +13,85 @@
 # limitations under the License.
 # ==============================================================================
 
-from ant_maze_env import AntMazeEnv
+from environments.ant_maze_env import AntMazeEnv
+from environments.point_maze_env import PointMazeEnv
 
+import tensorflow as tf
+import gin.tf
+from tf_agents.environments import gym_wrapper
+from tf_agents.environments import tf_py_environment
+
+
+@gin.configurable
+def create_maze_env(env_name=None, top_down_view=False):
+  n_bins = 0
+  manual_collision = False
+  if env_name.startswith('Ego'):
+    n_bins = 8
+    env_name = env_name[3:]
+  if env_name.startswith('Ant'):
+    cls = AntMazeEnv
+    env_name = env_name[3:]
+    maze_size_scaling = 8
+  elif env_name.startswith('Point'):
+    cls = PointMazeEnv
+    manual_collision = True
+    env_name = env_name[5:]
+    maze_size_scaling = 4
+  else:
+    assert False, 'unknown env %s' % env_name
 
-def create_maze_env(env_name=None):
   maze_id = None
-  if env_name.startswith('AntMaze'):
+  observe_blocks = False
+  put_spin_near_agent = False
+  if env_name == 'Maze':
     maze_id = 'Maze'
-  elif env_name.startswith('AntPush'):
+  elif env_name == 'Push':
     maze_id = 'Push'
-  elif env_name.startswith('AntFall'):
+  elif env_name == 'Fall':
     maze_id = 'Fall'
+  elif env_name == 'Block':
+    maze_id = 'Block'
+    put_spin_near_agent = True
+    observe_blocks = True
+  elif env_name == 'BlockMaze':
+    maze_id = 'BlockMaze'
+    put_spin_near_agent = True
+    observe_blocks = True
   else:
     raise ValueError('Unknown maze environment %s' % env_name)
 
-  return AntMazeEnv(maze_id=maze_id)
+  gym_mujoco_kwargs = {
+      'maze_id': maze_id,
+      'n_bins': n_bins,
+      'observe_blocks': observe_blocks,
+      'put_spin_near_agent': put_spin_near_agent,
+      'top_down_view': top_down_view,
+      'manual_collision': manual_collision,
+      'maze_size_scaling': maze_size_scaling
+  }
+  gym_env = cls(**gym_mujoco_kwargs)
+  gym_env.reset()
+  wrapped_env = gym_wrapper.GymWrapper(gym_env)
+  return wrapped_env
+
+
+class TFPyEnvironment(tf_py_environment.TFPyEnvironment):
+
+  def __init__(self, *args, **kwargs):
+    super(TFPyEnvironment, self).__init__(*args, **kwargs)
+
+  def start_collect(self):
+    pass
+
+  def current_obs(self):
+    time_step = self.current_time_step()
+    return time_step.observation[0]  # For some reason, there is an extra dim.
+
+  def step(self, actions):
+    actions = tf.expand_dims(actions, 0)
+    next_step = super(TFPyEnvironment, self).step(actions)
+    return next_step.is_last()[0], next_step.reward[0], next_step.discount[0]
+
+  def reset(self):
+    return super(TFPyEnvironment, self).reset()
diff --git a/research/efficient-hrl/environments/maze_env.py b/research/efficient-hrl/environments/maze_env.py
old mode 100755
new mode 100644
index 3e1d545b..74b80121
--- a/research/efficient-hrl/environments/maze_env.py
+++ b/research/efficient-hrl/environments/maze_env.py
@@ -22,7 +22,7 @@ import math
 import numpy as np
 import gym
 
-import maze_env_utils
+from environments import maze_env_utils
 
 # Directory that contains mujoco xml files.
 MODEL_DIR = 'environments/assets'
@@ -39,6 +39,13 @@ class MazeEnv(gym.Env):
       maze_id=None,
       maze_height=0.5,
       maze_size_scaling=8,
+      n_bins=0,
+      sensor_range=3.,
+      sensor_span=2 * math.pi,
+      observe_blocks=False,
+      put_spin_near_agent=False,
+      top_down_view=False,
+      manual_collision=False,
       *args,
       **kwargs):
     self._maze_id = maze_id
@@ -52,6 +59,14 @@ class MazeEnv(gym.Env):
 
     self.MAZE_HEIGHT = height = maze_height
     self.MAZE_SIZE_SCALING = size_scaling = maze_size_scaling
+    self._n_bins = n_bins
+    self._sensor_range = sensor_range * size_scaling
+    self._sensor_span = sensor_span
+    self._observe_blocks = observe_blocks
+    self._put_spin_near_agent = put_spin_near_agent
+    self._top_down_view = top_down_view
+    self._manual_collision = manual_collision
+
     self.MAZE_STRUCTURE = structure = maze_env_utils.construct_maze(maze_id=self._maze_id)
     self.elevated = any(-1 in row for row in structure)  # Elevate the maze to allow for falling.
     self.blocks = any(
@@ -61,6 +76,13 @@ class MazeEnv(gym.Env):
     torso_x, torso_y = self._find_robot()
     self._init_torso_x = torso_x
     self._init_torso_y = torso_y
+    self._init_positions = [
+        (x - torso_x, y - torso_y)
+        for x, y in self._find_all_robots()]
+
+    self._xy_to_rowcol = lambda x, y: (2 + (y + size_scaling / 2) / size_scaling,
+                                       2 + (x + size_scaling / 2) / size_scaling)
+    self._view = np.zeros([5, 5, 3])  # walls (immovable), chasms (fall), movable blocks
 
     height_offset = 0.
     if self.elevated:
@@ -74,9 +96,13 @@ class MazeEnv(gym.Env):
       default = tree.find(".//default")
       default.find('.//geom').set('solimp', '.995 .995 .01')
 
+    self.movable_blocks = []
     for i in range(len(structure)):
       for j in range(len(structure[0])):
-        if self.elevated and structure[i][j] not in [-1]:
+        struct = structure[i][j]
+        if struct == 'r' and self._put_spin_near_agent:
+          struct = maze_env_utils.Move.SpinXY
+        if self.elevated and struct not in [-1]:
           # Create elevated platform.
           ET.SubElement(
               worldbody, "geom",
@@ -93,7 +119,7 @@ class MazeEnv(gym.Env):
               conaffinity="1",
               rgba="0.9 0.9 0.9 1",
           )
-        if structure[i][j] == 1:  # Unmovable block.
+        if struct == 1:  # Unmovable block.
           # Offset all coordinates so that robot starts at the origin.
           ET.SubElement(
               worldbody, "geom",
@@ -111,26 +137,32 @@ class MazeEnv(gym.Env):
               conaffinity="1",
               rgba="0.4 0.4 0.4 1",
           )
-        elif maze_env_utils.can_move(structure[i][j]):  # Movable block.
+        elif maze_env_utils.can_move(struct):  # Movable block.
           # The "falling" blocks are shrunk slightly and increased in mass to
           # ensure that it can fall easily through a gap in the platform blocks.
-          falling = maze_env_utils.can_move_z(structure[i][j])
-          shrink = 0.99 if falling else 1.0
-          moveable_body = ET.SubElement(
+          name = "movable_%d_%d" % (i, j)
+          self.movable_blocks.append((name, struct))
+          falling = maze_env_utils.can_move_z(struct)
+          spinning = maze_env_utils.can_spin(struct)
+          x_offset = 0.25 * size_scaling if spinning else 0.0
+          y_offset = 0.0
+          shrink = 0.1 if spinning else 0.99 if falling else 1.0
+          height_shrink = 0.1 if spinning else 1.0
+          movable_body = ET.SubElement(
               worldbody, "body",
-              name="moveable_%d_%d" % (i, j),
-              pos="%f %f %f" % (j * size_scaling - torso_x,
-                                i * size_scaling - torso_y,
+              name=name,
+              pos="%f %f %f" % (j * size_scaling - torso_x + x_offset,
+                                i * size_scaling - torso_y + y_offset,
                                 height_offset +
-                                height / 2 * size_scaling),
+                                height / 2 * size_scaling * height_shrink),
           )
           ET.SubElement(
-              moveable_body, "geom",
+              movable_body, "geom",
               name="block_%d_%d" % (i, j),
               pos="0 0 0",
               size="%f %f %f" % (0.5 * size_scaling * shrink,
                                  0.5 * size_scaling * shrink,
-                                 height / 2 * size_scaling),
+                                 height / 2 * size_scaling * height_shrink),
               type="box",
               material="",
               mass="0.001" if falling else "0.0002",
@@ -138,45 +170,56 @@ class MazeEnv(gym.Env):
               conaffinity="1",
               rgba="0.9 0.1 0.1 1"
           )
-          if maze_env_utils.can_move_x(structure[i][j]):
+          if maze_env_utils.can_move_x(struct):
             ET.SubElement(
-                moveable_body, "joint",
+                movable_body, "joint",
                 armature="0",
                 axis="1 0 0",
                 damping="0.0",
                 limited="true" if falling else "false",
                 range="%f %f" % (-size_scaling, size_scaling),
                 margin="0.01",
-                name="moveable_x_%d_%d" % (i, j),
+                name="movable_x_%d_%d" % (i, j),
                 pos="0 0 0",
                 type="slide"
             )
-          if maze_env_utils.can_move_y(structure[i][j]):
+          if maze_env_utils.can_move_y(struct):
             ET.SubElement(
-                moveable_body, "joint",
+                movable_body, "joint",
                 armature="0",
                 axis="0 1 0",
                 damping="0.0",
                 limited="true" if falling else "false",
                 range="%f %f" % (-size_scaling, size_scaling),
                 margin="0.01",
-                name="moveable_y_%d_%d" % (i, j),
+                name="movable_y_%d_%d" % (i, j),
                 pos="0 0 0",
                 type="slide"
             )
-          if maze_env_utils.can_move_z(structure[i][j]):
+          if maze_env_utils.can_move_z(struct):
             ET.SubElement(
-                moveable_body, "joint",
+                movable_body, "joint",
                 armature="0",
                 axis="0 0 1",
                 damping="0.0",
                 limited="true",
                 range="%f 0" % (-height_offset),
                 margin="0.01",
-                name="moveable_z_%d_%d" % (i, j),
+                name="movable_z_%d_%d" % (i, j),
                 pos="0 0 0",
                 type="slide"
             )
+          if maze_env_utils.can_spin(struct):
+            ET.SubElement(
+                movable_body, "joint",
+                armature="0",
+                axis="0 0 1",
+                damping="0.0",
+                limited="false",
+                name="spinable_%d_%d" % (i, j),
+                pos="0 0 0",
+                type="ball"
+            )
 
     torso = tree.find(".//body[@name='torso']")
     geoms = torso.findall(".//geom")
@@ -190,13 +233,203 @@ class MazeEnv(gym.Env):
 
     self.wrapped_env = model_cls(*args, file_path=file_path, **kwargs)
 
+  def get_ori(self):
+    return self.wrapped_env.get_ori()
+
+  def get_top_down_view(self):
+    self._view = np.zeros_like(self._view)
+
+    def valid(row, col):
+      return self._view.shape[0] > row >= 0 and self._view.shape[1] > col >= 0
+
+    def update_view(x, y, d, row=None, col=None):
+      if row is None or col is None:
+        x = x - self._robot_x
+        y = y - self._robot_y
+        th = self._robot_ori
+
+        row, col = self._xy_to_rowcol(x, y)
+        update_view(x, y, d, row=row, col=col)
+        return
+
+      row, row_frac, col, col_frac = int(row), row % 1, int(col), col % 1
+      if row_frac < 0:
+        row_frac += 1
+      if col_frac < 0:
+        col_frac += 1
+
+      if valid(row, col):
+        self._view[row, col, d] += (
+            (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *
+            (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))
+      if valid(row - 1, col):
+        self._view[row - 1, col, d] += (
+            (max(0., 0.5 - row_frac)) *
+            (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))
+      if valid(row + 1, col):
+        self._view[row + 1, col, d] += (
+            (max(0., row_frac - 0.5)) *
+            (min(1., col_frac + 0.5) - max(0., col_frac - 0.5)))
+      if valid(row, col - 1):
+        self._view[row, col - 1, d] += (
+            (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *
+            (max(0., 0.5 - col_frac)))
+      if valid(row, col + 1):
+        self._view[row, col + 1, d] += (
+            (min(1., row_frac + 0.5) - max(0., row_frac - 0.5)) *
+            (max(0., col_frac - 0.5)))
+      if valid(row - 1, col - 1):
+        self._view[row - 1, col - 1, d] += (
+            (max(0., 0.5 - row_frac)) * max(0., 0.5 - col_frac))
+      if valid(row - 1, col + 1):
+        self._view[row - 1, col + 1, d] += (
+            (max(0., 0.5 - row_frac)) * max(0., col_frac - 0.5))
+      if valid(row + 1, col + 1):
+        self._view[row + 1, col + 1, d] += (
+            (max(0., row_frac - 0.5)) * max(0., col_frac - 0.5))
+      if valid(row + 1, col - 1):
+        self._view[row + 1, col - 1, d] += (
+            (max(0., row_frac - 0.5)) * max(0., 0.5 - col_frac))
+
+    # Draw ant.
+    robot_x, robot_y = self.wrapped_env.get_body_com("torso")[:2]
+    self._robot_x = robot_x
+    self._robot_y = robot_y
+    self._robot_ori = self.get_ori()
+
+    structure = self.MAZE_STRUCTURE
+    size_scaling = self.MAZE_SIZE_SCALING
+    height = self.MAZE_HEIGHT
+
+    # Draw immovable blocks and chasms.
+    for i in range(len(structure)):
+      for j in range(len(structure[0])):
+        if structure[i][j] == 1:  # Wall.
+          update_view(j * size_scaling - self._init_torso_x,
+                      i * size_scaling - self._init_torso_y,
+                      0)
+        if structure[i][j] == -1:  # Chasm.
+          update_view(j * size_scaling - self._init_torso_x,
+                      i * size_scaling - self._init_torso_y,
+                      1)
+
+    # Draw movable blocks.
+    for block_name, block_type in self.movable_blocks:
+      block_x, block_y = self.wrapped_env.get_body_com(block_name)[:2]
+      update_view(block_x, block_y, 2)
+
+    return self._view
+
+  def get_range_sensor_obs(self):
+    """Returns egocentric range sensor observations of maze."""
+    robot_x, robot_y, robot_z = self.wrapped_env.get_body_com("torso")[:3]
+    ori = self.get_ori()
+
+    structure = self.MAZE_STRUCTURE
+    size_scaling = self.MAZE_SIZE_SCALING
+    height = self.MAZE_HEIGHT
+
+    segments = []
+    # Get line segments (corresponding to outer boundary) of each immovable
+    # block or drop-off.
+    for i in range(len(structure)):
+      for j in range(len(structure[0])):
+        if structure[i][j] in [1, -1]:  # There's a wall or drop-off.
+          cx = j * size_scaling - self._init_torso_x
+          cy = i * size_scaling - self._init_torso_y
+          x1 = cx - 0.5 * size_scaling
+          x2 = cx + 0.5 * size_scaling
+          y1 = cy - 0.5 * size_scaling
+          y2 = cy + 0.5 * size_scaling
+          struct_segments = [
+              ((x1, y1), (x2, y1)),
+              ((x2, y1), (x2, y2)),
+              ((x2, y2), (x1, y2)),
+              ((x1, y2), (x1, y1)),
+          ]
+          for seg in struct_segments:
+            segments.append(dict(
+                segment=seg,
+                type=structure[i][j],
+            ))
+    # Get line segments (corresponding to outer boundary) of each movable
+    # block within the agent's z-view.
+    for block_name, block_type in self.movable_blocks:
+      block_x, block_y, block_z = self.wrapped_env.get_body_com(block_name)[:3]
+      if (block_z + height * size_scaling / 2 >= robot_z and
+          robot_z >= block_z - height * size_scaling / 2):  # Block in view.
+        x1 = block_x - 0.5 * size_scaling
+        x2 = block_x + 0.5 * size_scaling
+        y1 = block_y - 0.5 * size_scaling
+        y2 = block_y + 0.5 * size_scaling
+        struct_segments = [
+            ((x1, y1), (x2, y1)),
+            ((x2, y1), (x2, y2)),
+            ((x2, y2), (x1, y2)),
+            ((x1, y2), (x1, y1)),
+        ]
+        for seg in struct_segments:
+          segments.append(dict(
+              segment=seg,
+              type=block_type,
+          ))
+
+    sensor_readings = np.zeros((self._n_bins, 3))  # 3 for wall, drop-off, block
+    for ray_idx in range(self._n_bins):
+      ray_ori = (ori - self._sensor_span * 0.5 +
+                 (2 * ray_idx + 1.0) / (2 * self._n_bins) * self._sensor_span)
+      ray_segments = []
+      # Get all segments that intersect with ray.
+      for seg in segments:
+        p = maze_env_utils.ray_segment_intersect(
+            ray=((robot_x, robot_y), ray_ori),
+            segment=seg["segment"])
+        if p is not None:
+          ray_segments.append(dict(
+              segment=seg["segment"],
+              type=seg["type"],
+              ray_ori=ray_ori,
+              distance=maze_env_utils.point_distance(p, (robot_x, robot_y)),
+          ))
+      if len(ray_segments) > 0:
+        # Find out which segment is intersected first.
+        first_seg = sorted(ray_segments, key=lambda x: x["distance"])[0]
+        seg_type = first_seg["type"]
+        idx = (0 if seg_type == 1 else  # Wall.
+               1 if seg_type == -1 else  # Drop-off.
+               2 if maze_env_utils.can_move(seg_type) else  # Block.
+               None)
+        if first_seg["distance"] <= self._sensor_range:
+          sensor_readings[ray_idx][idx] = (self._sensor_range - first_seg["distance"]) / self._sensor_range
+
+    return sensor_readings
+
   def _get_obs(self):
-    return np.concatenate([self.wrapped_env._get_obs(),
-                           [self.t * 0.001]])
+    wrapped_obs = self.wrapped_env._get_obs()
+    if self._top_down_view:
+      view = [self.get_top_down_view().flat]
+    else:
+      view = []
+
+    if self._observe_blocks:
+      additional_obs = []
+      for block_name, block_type in self.movable_blocks:
+        additional_obs.append(self.wrapped_env.get_body_com(block_name))
+      wrapped_obs = np.concatenate([wrapped_obs[:3]] + additional_obs +
+                                   [wrapped_obs[3:]])
+
+    range_sensor_obs = self.get_range_sensor_obs()
+    return np.concatenate([wrapped_obs,
+                           range_sensor_obs.flat] +
+                           view + [[self.t * 0.001]])
 
   def reset(self):
     self.t = 0
+    self.trajectory = []
     self.wrapped_env.reset()
+    if len(self._init_positions) > 1:
+      xy = random.choice(self._init_positions)
+      self.wrapped_env.set_xy(xy)
     return self._get_obs()
 
   @property
@@ -226,9 +459,41 @@ class MazeEnv(gym.Env):
           return j * size_scaling, i * size_scaling
     assert False, 'No robot in maze specification.'
 
+  def _find_all_robots(self):
+    structure = self.MAZE_STRUCTURE
+    size_scaling = self.MAZE_SIZE_SCALING
+    coords = []
+    for i in range(len(structure)):
+      for j in range(len(structure[0])):
+        if structure[i][j] == 'r':
+          coords.append((j * size_scaling, i * size_scaling))
+    return coords
+
+  def _is_in_collision(self, pos):
+    x, y = pos
+    structure = self.MAZE_STRUCTURE
+    size_scaling = self.MAZE_SIZE_SCALING
+    for i in range(len(structure)):
+      for j in range(len(structure[0])):
+        if structure[i][j] == 1:
+          minx = j * size_scaling - size_scaling * 0.5 - self._init_torso_x
+          maxx = j * size_scaling + size_scaling * 0.5 - self._init_torso_x
+          miny = i * size_scaling - size_scaling * 0.5 - self._init_torso_y
+          maxy = i * size_scaling + size_scaling * 0.5 - self._init_torso_y
+          if minx <= x <= maxx and miny <= y <= maxy:
+            return True
+    return False
+
   def step(self, action):
     self.t += 1
-    inner_next_obs, inner_reward, done, info = self.wrapped_env.step(action)
+    if self._manual_collision:
+      old_pos = self.wrapped_env.get_xy()
+      inner_next_obs, inner_reward, done, info = self.wrapped_env.step(action)
+      new_pos = self.wrapped_env.get_xy()
+      if self._is_in_collision(new_pos):
+        self.wrapped_env.set_xy(old_pos)
+    else:
+      inner_next_obs, inner_reward, done, info = self.wrapped_env.step(action)
     next_obs = self._get_obs()
     done = False
     return next_obs, inner_reward, done, info
diff --git a/research/efficient-hrl/environments/maze_env_utils.py b/research/efficient-hrl/environments/maze_env_utils.py
old mode 100755
new mode 100644
index ac51e832..4f52509b
--- a/research/efficient-hrl/environments/maze_env_utils.py
+++ b/research/efficient-hrl/environments/maze_env_utils.py
@@ -26,20 +26,27 @@ class Move(object):
   XZ = 15
   YZ = 16
   XYZ = 17
+  SpinXY = 18
 
 
 def can_move_x(movable):
-  return movable in [Move.X, Move.XY, Move.XZ, Move.XYZ]
+  return movable in [Move.X, Move.XY, Move.XZ, Move.XYZ,
+                     Move.SpinXY]
 
 
 def can_move_y(movable):
-  return movable in [Move.Y, Move.XY, Move.YZ, Move.XYZ]
+  return movable in [Move.Y, Move.XY, Move.YZ, Move.XYZ,
+                     Move.SpinXY]
 
 
 def can_move_z(movable):
   return movable in [Move.Z, Move.XZ, Move.YZ, Move.XYZ]
 
 
+def can_spin(movable):
+  return movable in [Move.SpinXY]
+
+
 def can_move(movable):
   return can_move_x(movable) or can_move_y(movable) or can_move_z(movable)
 
@@ -70,7 +77,88 @@ def construct_maze(maze_id='Maze'):
         [1, 0,   0,  1],
         [1, 1,   1,  1],
     ]
+  elif maze_id == 'Block':
+    O = 'r'
+    structure = [
+        [1, 1, 1, 1, 1],
+        [1, O, 0, 0, 1],
+        [1, 0, 0, 0, 1],
+        [1, 0, 0, 0, 1],
+        [1, 1, 1, 1, 1],
+    ]
+  elif maze_id == 'BlockMaze':
+    O = 'r'
+    structure = [
+        [1, 1, 1, 1],
+        [1, O, 0, 1],
+        [1, 1, 0, 1],
+        [1, 0, 0, 1],
+        [1, 1, 1, 1],
+    ]
   else:
       raise NotImplementedError('The provided MazeId %s is not recognized' % maze_id)
 
   return structure
+
+
+def line_intersect(pt1, pt2, ptA, ptB):
+  """
+  Taken from https://www.cs.hmc.edu/ACM/lectures/intersections.html
+
+  this returns the intersection of Line(pt1,pt2) and Line(ptA,ptB)
+  """
+
+  DET_TOLERANCE = 0.00000001
+
+  # the first line is pt1 + r*(pt2-pt1)
+  # in component form:
+  x1, y1 = pt1
+  x2, y2 = pt2
+  dx1 = x2 - x1
+  dy1 = y2 - y1
+
+  # the second line is ptA + s*(ptB-ptA)
+  x, y = ptA
+  xB, yB = ptB
+  dx = xB - x
+  dy = yB - y
+
+  DET = (-dx1 * dy + dy1 * dx)
+
+  if math.fabs(DET) < DET_TOLERANCE: return (0, 0, 0, 0, 0)
+
+  # now, the determinant should be OK
+  DETinv = 1.0 / DET
+
+  # find the scalar amount along the "self" segment
+  r = DETinv * (-dy * (x - x1) + dx * (y - y1))
+
+  # find the scalar amount along the input line
+  s = DETinv * (-dy1 * (x - x1) + dx1 * (y - y1))
+
+  # return the average of the two descriptions
+  xi = (x1 + r * dx1 + x + s * dx) / 2.0
+  yi = (y1 + r * dy1 + y + s * dy) / 2.0
+  return (xi, yi, 1, r, s)
+
+
+def ray_segment_intersect(ray, segment):
+  """
+  Check if the ray originated from (x, y) with direction theta intersects the line segment (x1, y1) -- (x2, y2),
+  and return the intersection point if there is one
+  """
+  (x, y), theta = ray
+  # (x1, y1), (x2, y2) = segment
+  pt1 = (x, y)
+  len = 1
+  pt2 = (x + len * math.cos(theta), y + len * math.sin(theta))
+  xo, yo, valid, r, s = line_intersect(pt1, pt2, *segment)
+  if valid and r >= 0 and 0 <= s <= 1:
+    return (xo, yo)
+  return None
+
+
+def point_distance(p1, p2):
+  x1, y1 = p1
+  x2, y2 = p2
+  return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5
diff --git a/research/efficient-hrl/environments/point.py b/research/efficient-hrl/environments/point.py
new file mode 100644
index 00000000..1a753235
--- /dev/null
+++ b/research/efficient-hrl/environments/point.py
@@ -0,0 +1,90 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Wrapper for creating the ant environment in gym_mujoco."""
+
+import math
+import numpy as np
+from gym import utils
+from gym.envs.mujoco import mujoco_env
+
+
+class PointEnv(mujoco_env.MujocoEnv, utils.EzPickle):
+  FILE = "point.xml"
+  ORI_IND = 2
+
+  def __init__(self, file_path=None, expose_all_qpos=True):
+    self._expose_all_qpos = expose_all_qpos
+
+    mujoco_env.MujocoEnv.__init__(self, file_path, 1)
+    utils.EzPickle.__init__(self)
+
+  @property
+  def physics(self):
+    return self.model
+
+  def _step(self, a):
+    return self.step(a)
+
+  def step(self, action):
+    action[0] = 0.2 * action[0]
+    qpos = np.copy(self.physics.data.qpos)
+    qpos[2] += action[1]
+    ori = qpos[2]
+    # compute increment in each direction
+    dx = math.cos(ori) * action[0]
+    dy = math.sin(ori) * action[0]
+    # ensure that the robot is within reasonable range
+    qpos[0] = np.clip(qpos[0] + dx, -100, 100)
+    qpos[1] = np.clip(qpos[1] + dy, -100, 100)
+    qvel = self.physics.data.qvel
+    self.set_state(qpos, qvel)
+    for _ in range(0, self.frame_skip):
+      self.physics.step()
+    next_obs = self._get_obs()
+    reward = 0
+    done = False
+    info = {}
+    return next_obs, reward, done, info
+
+  def _get_obs(self):
+    if self._expose_all_qpos:
+      return np.concatenate([
+          self.physics.data.qpos.flat[:3],  # Only point-relevant coords.
+          self.physics.data.qvel.flat[:3]])
+    return np.concatenate([
+        self.physics.data.qpos.flat[2:3],
+        self.physics.data.qvel.flat[:3]])
+
+  def reset_model(self):
+    qpos = self.init_qpos + self.np_random.uniform(
+        size=self.physics.model.nq, low=-.1, high=.1)
+    qvel = self.init_qvel + self.np_random.randn(self.physics.model.nv) * .1
+
+    # Set everything other than point to original position and 0 velocity.
+    qpos[3:] = self.init_qpos[3:]
+    qvel[3:] = 0.
+    self.set_state(qpos, qvel)
+    return self._get_obs()
+
+  def get_ori(self):
+    return self.model.data.qpos[self.__class__.ORI_IND]
+
+  def set_xy(self, xy):
+    qpos = np.copy(self.physics.data.qpos)
+    qpos[0] = xy[0]
+    qpos[1] = xy[1]
+
+    qvel = self.physics.data.qvel
diff --git a/research/efficient-hrl/environments/point_maze_env.py b/research/efficient-hrl/environments/point_maze_env.py
new file mode 100644
index 00000000..8d6b8194
--- /dev/null
+++ b/research/efficient-hrl/environments/point_maze_env.py
@@ -0,0 +1,21 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+from environments.maze_env import MazeEnv
+from environments.point import PointEnv
+
+
+class PointMazeEnv(MazeEnv):
+    MODEL_CLASS = PointEnv
diff --git a/research/efficient-hrl/eval.py b/research/efficient-hrl/eval.py
new file mode 100644
index 00000000..4f5a4b20
--- /dev/null
+++ b/research/efficient-hrl/eval.py
@@ -0,0 +1,460 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Script for evaluating a UVF agent.
+
+To run locally: See run_eval.py
+
+To run on borg: See train_eval.borg
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import tensorflow as tf
+slim = tf.contrib.slim
+import gin.tf
+# pylint: disable=unused-import
+import agent
+import train
+from utils import utils as uvf_utils
+from utils import eval_utils
+from environments import create_maze_env
+# pylint: enable=unused-import
+
+flags = tf.app.flags
+
+flags.DEFINE_string('eval_dir', None,
+                    'Directory for writing logs/summaries during eval.')
+flags.DEFINE_string('checkpoint_dir', None,
+                    'Directory containing checkpoints to eval.')
+FLAGS = flags.FLAGS
+
+
+def get_evaluate_checkpoint_fn(master, output_dir, eval_step_fns,
+                               model_rollout_fn, gamma, max_steps_per_episode,
+                               num_episodes_eval, num_episodes_videos,
+                               tuner_hook, generate_videos,
+                               generate_summaries, video_settings):
+  """Returns a function that evaluates a given checkpoint.
+
+  Args:
+    master: BNS name of the TensorFlow master
+    output_dir: The output directory to which the metric summaries are written.
+    eval_step_fns: A dictionary of a functions that return a list of
+      [state, action, reward, discount, transition_type] tensors,
+      indexed by summary tag name.
+    model_rollout_fn: Model rollout fn.
+    gamma: Discount factor for the reward.
+    max_steps_per_episode: Maximum steps to run each episode for.
+    num_episodes_eval: Number of episodes to evaluate and average reward over.
+    num_episodes_videos: Number of episodes to record for video.
+    tuner_hook: A callable(average reward, global step) that updates a Vizier
+      tuner trial.
+    generate_videos: Whether to generate videos of the agent in action.
+    generate_summaries: Whether to generate summaries.
+    video_settings: Settings for generating videos of the agent.
+
+  Returns:
+    A function that evaluates a checkpoint.
+  """
+  sess = tf.Session(master, graph=tf.get_default_graph())
+  sess.run(tf.global_variables_initializer())
+  sess.run(tf.local_variables_initializer())
+  summary_writer = tf.summary.FileWriter(output_dir)
+
+  def evaluate_checkpoint(checkpoint_path):
+    """Performs a one-time evaluation of the given checkpoint.
+
+    Args:
+      checkpoint_path: Checkpoint to evaluate.
+    Returns:
+      True if the evaluation process should stop
+    """
+    restore_fn = tf.contrib.framework.assign_from_checkpoint_fn(
+        checkpoint_path,
+        uvf_utils.get_all_vars(),
+        ignore_missing_vars=True,
+        reshape_variables=False)
+    assert restore_fn is not None, 'cannot restore %s' % checkpoint_path
+    restore_fn(sess)
+    global_step = sess.run(slim.get_global_step())
+    should_stop = False
+    max_reward = -1e10
+    max_meta_reward = -1e10
+
+    for eval_tag, (eval_step, env_base,) in sorted(eval_step_fns.items()):
+      if hasattr(env_base, 'set_sess'):
+        env_base.set_sess(sess)  # set session
+
+      if generate_summaries:
+        tf.logging.info(
+            '[%s] Computing average reward over %d episodes at global step %d.',
+            eval_tag, num_episodes_eval, global_step)
+        (average_reward, last_reward,
+         average_meta_reward, last_meta_reward, average_success,
+         states, actions) = eval_utils.compute_average_reward(
+             sess, env_base, eval_step, gamma, max_steps_per_episode,
+             num_episodes_eval)
+        tf.logging.info('[%s] Average reward = %f', eval_tag, average_reward)
+        tf.logging.info('[%s] Last reward = %f', eval_tag, last_reward)
+        tf.logging.info('[%s] Average meta reward = %f', eval_tag, average_meta_reward)
+        tf.logging.info('[%s] Last meta reward = %f', eval_tag, last_meta_reward)
+        tf.logging.info('[%s] Average success = %f', eval_tag, average_success)
+        if model_rollout_fn is not None:
+          preds, model_losses = eval_utils.compute_model_loss(
+              sess, model_rollout_fn, states, actions)
+          for i, (pred, state, model_loss) in enumerate(
+              zip(preds, states, model_losses)):
+            tf.logging.info('[%s] Model rollout step %d: loss=%f', eval_tag, i,
+                            model_loss)
+            tf.logging.info('[%s] Model rollout step %d: pred=%s', eval_tag, i,
+                            str(pred.tolist()))
+            tf.logging.info('[%s] Model rollout step %d: state=%s', eval_tag, i,
+                            str(state.tolist()))
+
+        # Report the eval stats to the tuner.
+        if average_reward > max_reward:
+          max_reward = average_reward
+        if average_meta_reward > max_meta_reward:
+          max_meta_reward = average_meta_reward
+
+        for (tag, value) in [('Reward/average_%s_reward', average_reward),
+                             ('Reward/last_%s_reward', last_reward),
+                             ('Reward/average_%s_meta_reward', average_meta_reward),
+                             ('Reward/last_%s_meta_reward', last_meta_reward),
+                             ('Reward/average_%s_success', average_success)]:
+          summary_str = tf.Summary(value=[
+              tf.Summary.Value(
+                  tag=tag % eval_tag,
+                  simple_value=value)
+          ])
+          summary_writer.add_summary(summary_str, global_step)
+          summary_writer.flush()
+
+      if generate_videos or should_stop:
+        # Do a manual reset before generating the video to see the initial
+        # pose of the robot, towards which the reset controller is moving.
+        if hasattr(env_base, '_gym_env'):
+          tf.logging.info('Resetting before recording video')
+          if hasattr(env_base._gym_env, 'reset_model'):
+            env_base._gym_env.reset_model()  # pylint: disable=protected-access
+          else:
+            env_base._gym_env.wrapped_env.reset_model()
+        video_filename = os.path.join(output_dir, 'videos',
+                                      '%s_step_%d.mp4' % (eval_tag,
+                                                          global_step))
+        eval_utils.capture_video(sess, eval_step, env_base,
+                                max_steps_per_episode * num_episodes_videos,
+                                video_filename, video_settings,
+                                reset_every=max_steps_per_episode)
+
+      should_stop = should_stop or (generate_summaries and tuner_hook and
+                                    tuner_hook(max_reward, global_step))
+    return bool(should_stop)
+
+  return evaluate_checkpoint
+
+
+def get_model_rollout(uvf_agent, tf_env):
+  """Model rollout function."""
+  state_spec = tf_env.observation_spec()[0]
+  action_spec = tf_env.action_spec()[0]
+  state_ph = tf.placeholder(dtype=state_spec.dtype, shape=state_spec.shape)
+  action_ph = tf.placeholder(dtype=action_spec.dtype, shape=action_spec.shape)
+
+  merged_state = uvf_agent.merged_state(state_ph)
+  diff_value = uvf_agent.critic_net(tf.expand_dims(merged_state, 0),
+                                    tf.expand_dims(action_ph, 0))[0]
+  diff_value = tf.cast(diff_value, dtype=state_ph.dtype)
+  state_ph.shape.assert_is_compatible_with(diff_value.shape)
+  next_state = state_ph + diff_value
+
+  def model_rollout_fn(sess, state, action):
+    return sess.run(next_state, feed_dict={state_ph: state, action_ph: action})
+
+  return model_rollout_fn
+
+
+def get_eval_step(uvf_agent,
+                  state_preprocess,
+                  tf_env,
+                  action_fn,
+                  meta_action_fn,
+                  environment_steps,
+                  num_episodes,
+                  mode='eval'):
+  """Get one-step policy/env stepping ops.
+
+  Args:
+    uvf_agent: A UVF agent.
+    tf_env: A TFEnvironment.
+    action_fn: A function to produce actions given current state.
+    meta_action_fn: A function to produce meta actions given current state.
+    environment_steps: A variable to count the number of steps in the tf_env.
+    num_episodes: A variable to count the number of episodes.
+    mode: a string representing the mode=[train, explore, eval].
+
+  Returns:
+    A collect_experience_op that excute an action and store into the
+    replay_buffer
+  """
+
+  tf_env.start_collect()
+  state = tf_env.current_obs()
+  action = action_fn(state, context=None)
+  state_repr = state_preprocess(state)
+
+  action_spec = tf_env.action_spec()
+  action_ph = tf.placeholder(dtype=action_spec.dtype, shape=action_spec.shape)
+  with tf.control_dependencies([state]):
+    transition_type, reward, discount = tf_env.step(action_ph)
+
+  def increment_step():
+    return environment_steps.assign_add(1)
+
+  def increment_episode():
+    return num_episodes.assign_add(1)
+
+  def no_op_int():
+    return tf.constant(0, dtype=tf.int64)
+
+  step_cond = uvf_agent.step_cond_fn(state, action,
+                                     transition_type,
+                                     environment_steps, num_episodes)
+  reset_episode_cond = uvf_agent.reset_episode_cond_fn(
+      state, action,
+      transition_type, environment_steps, num_episodes)
+  reset_env_cond = uvf_agent.reset_env_cond_fn(state, action,
+                                               transition_type,
+                                               environment_steps, num_episodes)
+
+  increment_step_op = tf.cond(step_cond, increment_step, no_op_int)
+  with tf.control_dependencies([increment_step_op]):
+    increment_episode_op = tf.cond(reset_episode_cond, increment_episode,
+                                   no_op_int)
+
+  with tf.control_dependencies([reward, discount]):
+    next_state = tf_env.current_obs()
+    next_state_repr = state_preprocess(next_state)
+
+  with tf.control_dependencies([increment_episode_op]):
+    post_reward, post_meta_reward = uvf_agent.cond_begin_episode_op(
+        tf.logical_not(reset_episode_cond),
+        [state, action_ph, reward, next_state,
+         state_repr, next_state_repr],
+        mode=mode, meta_action_fn=meta_action_fn)
+
+  # Important: do manual reset after getting the final reward from the
+  # unreset environment.
+  with tf.control_dependencies([post_reward, post_meta_reward]):
+    cond_reset_op = tf.cond(reset_env_cond,
+                            tf_env.reset,
+                            tf_env.current_time_step)
+
+  # Add a dummy control dependency to force the reset_op to run
+  with tf.control_dependencies(cond_reset_op):
+    post_reward, post_meta_reward = map(tf.identity, [post_reward, post_meta_reward])
+
+  eval_step = [next_state, action_ph, transition_type, post_reward, post_meta_reward, discount, uvf_agent.context_vars, state_repr]
+
+  if callable(action):
+    def step_fn(sess):
+      action_value = action(sess)
+      return sess.run(eval_step, feed_dict={action_ph: action_value})
+  else:
+    action = uvf_utils.clip_to_spec(action, action_spec)
+    def step_fn(sess):
+      action_value = sess.run(action)
+      return sess.run(eval_step, feed_dict={action_ph: action_value})
+
+  return step_fn
+
+
+@gin.configurable
+def evaluate(checkpoint_dir,
+             eval_dir,
+             environment=None,
+             num_bin_actions=3,
+             agent_class=None,
+             meta_agent_class=None,
+             state_preprocess_class=None,
+             gamma=1.0,
+             num_episodes_eval=10,
+             eval_interval_secs=60,
+             max_number_of_evaluations=None,
+             checkpoint_timeout=None,
+             timeout_fn=None,
+             tuner_hook=None,
+             generate_videos=False,
+             generate_summaries=True,
+             num_episodes_videos=5,
+             video_settings=None,
+             eval_modes=('eval',),
+             eval_model_rollout=False,
+             policy_save_dir='policy',
+             checkpoint_range=None,
+             checkpoint_path=None,
+             max_steps_per_episode=None,
+             evaluate_nohrl=False):
+  """Loads and repeatedly evaluates a checkpointed model at a set interval.
+
+  Args:
+    checkpoint_dir: The directory where the checkpoints reside.
+    eval_dir: Directory to save the evaluation summary results.
+    environment: A BaseEnvironment to evaluate.
+    num_bin_actions: Number of bins for discretizing continuous actions.
+    agent_class: An RL agent class.
+    meta_agent_class: A Meta agent class.
+    gamma: Discount factor for the reward.
+    num_episodes_eval: Number of episodes to evaluate and average reward over.
+    eval_interval_secs: The number of seconds between each evaluation run.
+    max_number_of_evaluations: The max number of evaluations. If None the
+      evaluation continues indefinitely.
+    checkpoint_timeout: The maximum amount of time to wait between checkpoints.
+      If left as `None`, then the process will wait indefinitely.
+    timeout_fn: Optional function to call after a timeout.
+    tuner_hook: A callable that takes the average reward and global step and
+      updates a Vizier tuner trial.
+    generate_videos: Whether to generate videos of the agent in action.
+    generate_summaries: Whether to generate summaries.
+    num_episodes_videos: Number of episodes to evaluate for generating videos.
+    video_settings: Settings for generating videos of the agent.
+      optimal action based on the critic.
+    eval_modes: A tuple of eval modes.
+    eval_model_rollout: Evaluate model rollout.
+    policy_save_dir: Optional sub-directory where the policies are
+      saved.
+    checkpoint_range: Optional. If provided, evaluate all checkpoints in
+      the range.
+    checkpoint_path: Optional sub-directory specifying which checkpoint to
+      evaluate. If None, will evaluate the most recent checkpoint.
+  """
+  tf_env = create_maze_env.TFPyEnvironment(environment)
+  observation_spec = [tf_env.observation_spec()]
+  action_spec = [tf_env.action_spec()]
+
+  assert max_steps_per_episode, 'max_steps_per_episode need to be set'
+
+  if agent_class.ACTION_TYPE == 'discrete':
+    assert False
+  else:
+    assert agent_class.ACTION_TYPE == 'continuous'
+
+  if meta_agent_class is not None:
+    assert agent_class.ACTION_TYPE == meta_agent_class.ACTION_TYPE
+    with tf.variable_scope('meta_agent'):
+      meta_agent = meta_agent_class(
+        observation_spec,
+        action_spec,
+        tf_env,
+      )
+  else:
+    meta_agent = None
+
+  with tf.variable_scope('uvf_agent'):
+    uvf_agent = agent_class(
+        observation_spec,
+        action_spec,
+        tf_env,
+    )
+    uvf_agent.set_meta_agent(agent=meta_agent)
+
+  with tf.variable_scope('state_preprocess'):
+    state_preprocess = state_preprocess_class()
+
+  # run both actor and critic once to ensure networks are initialized
+  # and gin configs will be saved
+  # pylint: disable=protected-access
+  temp_states = tf.expand_dims(
+      tf.zeros(
+          dtype=uvf_agent._observation_spec.dtype,
+          shape=uvf_agent._observation_spec.shape), 0)
+  # pylint: enable=protected-access
+  temp_actions = uvf_agent.actor_net(temp_states)
+  uvf_agent.critic_net(temp_states, temp_actions)
+
+  # create eval_step_fns for each action function
+  eval_step_fns = dict()
+  meta_agent = uvf_agent.meta_agent
+  for meta in [True] + [False] * evaluate_nohrl:
+    meta_tag = 'hrl' if meta else 'nohrl'
+    uvf_agent.set_meta_agent(meta_agent if meta else None)
+    for mode in eval_modes:
+      # wrap environment
+      wrapped_environment = uvf_agent.get_env_base_wrapper(
+          environment, mode=mode)
+      action_wrapper = lambda agent_: agent_.action
+      action_fn = action_wrapper(uvf_agent)
+      meta_action_fn = action_wrapper(meta_agent)
+      eval_step_fns['%s_%s' % (mode, meta_tag)] = (get_eval_step(
+          uvf_agent=uvf_agent,
+          state_preprocess=state_preprocess,
+          tf_env=tf_env,
+          action_fn=action_fn,
+          meta_action_fn=meta_action_fn,
+          environment_steps=tf.Variable(
+              0, dtype=tf.int64, name='environment_steps'),
+          num_episodes=tf.Variable(0, dtype=tf.int64, name='num_episodes'),
+          mode=mode), wrapped_environment,)
+
+  model_rollout_fn = None
+  if eval_model_rollout:
+    model_rollout_fn = get_model_rollout(uvf_agent, tf_env)
+
+  tf.train.get_or_create_global_step()
+
+  if policy_save_dir:
+    checkpoint_dir = os.path.join(checkpoint_dir, policy_save_dir)
+
+  tf.logging.info('Evaluating policies at %s', checkpoint_dir)
+  tf.logging.info('Running episodes for max %d steps', max_steps_per_episode)
+
+  evaluate_checkpoint_fn = get_evaluate_checkpoint_fn(
+      '', eval_dir, eval_step_fns, model_rollout_fn, gamma,
+      max_steps_per_episode, num_episodes_eval, num_episodes_videos, tuner_hook,
+      generate_videos, generate_summaries, video_settings)
+
+  if checkpoint_path is not None:
+    checkpoint_path = os.path.join(checkpoint_dir, checkpoint_path)
+    evaluate_checkpoint_fn(checkpoint_path)
+  elif checkpoint_range is not None:
+    model_files = tf.gfile.Glob(
+        os.path.join(checkpoint_dir, 'model.ckpt-*.index'))
+    tf.logging.info('Found %s policies at %s', len(model_files), checkpoint_dir)
+    model_files = {
+        int(f.split('model.ckpt-', 1)[1].split('.', 1)[0]):
+        os.path.splitext(f)[0]
+        for f in model_files
+    }
+    model_files = {
+        k: v
+        for k, v in model_files.items()
+        if k >= checkpoint_range[0] and k <= checkpoint_range[1]
+    }
+    tf.logging.info('Evaluating %d policies at %s',
+                    len(model_files), checkpoint_dir)
+    for _, checkpoint_path in sorted(model_files.items()):
+      evaluate_checkpoint_fn(checkpoint_path)
+  else:
+    eval_utils.evaluate_checkpoint_repeatedly(
+        checkpoint_dir,
+        evaluate_checkpoint_fn,
+        eval_interval_secs=eval_interval_secs,
+        max_number_of_evaluations=max_number_of_evaluations,
+        checkpoint_timeout=checkpoint_timeout,
+        timeout_fn=timeout_fn)
diff --git a/research/efficient-hrl/run_env.py b/research/efficient-hrl/run_env.py
new file mode 100644
index 00000000..87fad542
--- /dev/null
+++ b/research/efficient-hrl/run_env.py
@@ -0,0 +1,129 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Random policy on an environment."""
+
+import tensorflow as tf
+import numpy as np
+import random
+
+from environments import create_maze_env
+
+app = tf.app
+flags = tf.flags
+logging = tf.logging
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('env', 'AntMaze', 'environment name: AntMaze, AntPush, or AntFall')
+flags.DEFINE_integer('episode_length', 500, 'episode length')
+flags.DEFINE_integer('num_episodes', 50, 'number of episodes')
+
+
+def get_goal_sample_fn(env_name):
+  if env_name == 'AntMaze':
+    # NOTE: When evaluating (i.e. the metrics shown in the paper,
+    # we use the commented out goal sampling function.  The uncommented
+    # one is only used for training.
+    #return lambda: np.array([0., 16.])
+    return lambda: np.random.uniform((-4, -4), (20, 20))
+  elif env_name == 'AntPush':
+    return lambda: np.array([0., 19.])
+  elif env_name == 'AntFall':
+    return lambda: np.array([0., 27., 4.5])
+  else:
+    assert False, 'Unknown env'
+
+
+def get_reward_fn(env_name):
+  if env_name == 'AntMaze':
+    return lambda obs, goal: -np.sum(np.square(obs[:2] - goal)) ** 0.5
+  elif env_name == 'AntPush':
+    return lambda obs, goal: -np.sum(np.square(obs[:2] - goal)) ** 0.5
+  elif env_name == 'AntFall':
+    return lambda obs, goal: -np.sum(np.square(obs[:3] - goal)) ** 0.5
+  else:
+    assert False, 'Unknown env'
+
+
+def success_fn(last_reward):
+  return last_reward > -5.0
+
+
+class EnvWithGoal(object):
+
+  def __init__(self, base_env, env_name):
+    self.base_env = base_env
+    self.goal_sample_fn = get_goal_sample_fn(env_name)
+    self.reward_fn = get_reward_fn(env_name)
+    self.goal = None
+
+  def reset(self):
+    obs = self.base_env.reset()
+    self.goal = self.goal_sample_fn()
+    return np.concatenate([obs, self.goal])
+
+  def step(self, a):
+    obs, _, done, info = self.base_env.step(a)
+    reward = self.reward_fn(obs, self.goal)
+    return np.concatenate([obs, self.goal]), reward, done, info
+
+  @property
+  def action_space(self):
+    return self.base_env.action_space
+
+
+def run_environment(env_name, episode_length, num_episodes):
+  env = EnvWithGoal(
+      create_maze_env.create_maze_env(env_name).gym,
+      env_name)
+
+  def action_fn(obs):
+    action_space = env.action_space
+    action_space_mean = (action_space.low + action_space.high) / 2.0
+    action_space_magn = (action_space.high - action_space.low) / 2.0
+    random_action = (action_space_mean +
+                     action_space_magn *
+                     np.random.uniform(low=-1.0, high=1.0,
+                                       size=action_space.shape))
+    return random_action
+
+  rewards = []
+  successes = []
+  for ep in range(num_episodes):
+    rewards.append(0.0)
+    successes.append(False)
+    obs = env.reset()
+    for _ in range(episode_length):
+      obs, reward, done, _ = env.step(action_fn(obs))
+      rewards[-1] += reward
+      successes[-1] = success_fn(reward)
+      if done:
+        break
+    logging.info('Episode %d reward: %.2f, Success: %d', ep + 1, rewards[-1], successes[-1])
+
+  logging.info('Average Reward over %d episodes: %.2f',
+               num_episodes, np.mean(rewards))
+  logging.info('Average Success over %d episodes: %.2f',
+               num_episodes, np.mean(successes))
+
+
+def main(unused_argv):
+  logging.set_verbosity(logging.INFO)
+  run_environment(FLAGS.env, FLAGS.episode_length, FLAGS.num_episodes)
+
+
+if __name__ == '__main__':
+  app.run()
diff --git a/research/efficient-hrl/run_eval.py b/research/efficient-hrl/run_eval.py
new file mode 100644
index 00000000..12f12369
--- /dev/null
+++ b/research/efficient-hrl/run_eval.py
@@ -0,0 +1,51 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Script for evaluating a UVF agent.
+
+To run locally: See scripts/local_eval.py
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+import gin.tf
+# pylint: disable=unused-import
+import eval as eval_
+# pylint: enable=unused-import
+
+flags = tf.app.flags
+FLAGS = flags.FLAGS
+
+
+def main(_):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  assert FLAGS.checkpoint_dir, "Flag 'checkpoint_dir' must be set."
+  assert FLAGS.eval_dir, "Flag 'eval_dir' must be set."
+
+  if FLAGS.config_file:
+    for config_file in FLAGS.config_file:
+      gin.parse_config_file(config_file)
+  if FLAGS.params:
+    gin.parse_config(FLAGS.params)
+
+  eval_.evaluate(FLAGS.checkpoint_dir, FLAGS.eval_dir)
+
+
+if __name__ == "__main__":
+  tf.app.run()
diff --git a/research/efficient-hrl/run_train.py b/research/efficient-hrl/run_train.py
new file mode 100644
index 00000000..1d459d60
--- /dev/null
+++ b/research/efficient-hrl/run_train.py
@@ -0,0 +1,49 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Script for training an RL agent using the UVF algorithm.
+
+To run locally: See scripts/local_train.py
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import tensorflow as tf
+
+import gin.tf
+# pylint: enable=unused-import
+import train
+# pylint: disable=unused-import
+
+flags = tf.app.flags
+FLAGS = flags.FLAGS
+
+
+def main(_):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  if FLAGS.config_file:
+    for config_file in FLAGS.config_file:
+      gin.parse_config_file(config_file)
+  if FLAGS.params:
+    gin.parse_config(FLAGS.params)
+
+  assert FLAGS.train_dir, "Flag 'train_dir' must be set."
+  return train.train_uvf(FLAGS.train_dir)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/efficient-hrl/scripts/local_eval.py b/research/efficient-hrl/scripts/local_eval.py
new file mode 100644
index 00000000..89ef745a
--- /dev/null
+++ b/research/efficient-hrl/scripts/local_eval.py
@@ -0,0 +1,76 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Script to run run_eval.py locally.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+import os
+from subprocess import call
+import sys
+
+CONFIGS_PATH = 'configs'
+CONTEXT_CONFIGS_PATH = 'context/configs'
+
+def main():
+  bb = './'
+  base_num_args = 6
+  if len(sys.argv) < base_num_args:
+    print(
+        "usage: python %s <exp_name> <context_setting_gin> <context_gin> "
+        "<agent_gin> <suite> [params...]"
+        % sys.argv[0])
+    sys.exit(0)
+  exp = sys.argv[1]
+  context_setting = sys.argv[2]
+  context = sys.argv[3]
+  agent = sys.argv[4]
+  assert sys.argv[5] in ["suite"], "args[5] must be `suite'"
+  suite = ""
+  binary = "python {bb}/run_eval{suite}.py ".format(bb=bb, suite=suite)
+
+  h = os.environ["HOME"]
+  ucp = CONFIGS_PATH
+  ccp = CONTEXT_CONFIGS_PATH
+  extra = ''
+  command_str = ("{binary} "
+                 "--logtostderr "
+                 "--checkpoint_dir={h}/tmp/{context_setting}/{context}/{agent}/{exp}/train "
+                 "--eval_dir={h}/tmp/{context_setting}/{context}/{agent}/{exp}/eval "
+                 "--config_file={ucp}/{agent}.gin "
+                 "--config_file={ucp}/eval_{extra}uvf.gin "
+                 "--config_file={ccp}/{context_setting}.gin "
+                 "--config_file={ccp}/{context}.gin ").format(
+                     h=h,
+                     ucp=ucp,
+                     ccp=ccp,
+                     context_setting=context_setting,
+                     context=context,
+                     agent=agent,
+                     extra=extra,
+                     suite=suite,
+                     exp=exp,
+                     binary=binary)
+  for extra_arg in sys.argv[base_num_args:]:
+    command_str += "--params='%s' " % extra_arg
+
+  print(command_str)
+  call(command_str, shell=True)
+
+
+if __name__ == "__main__":
+  main()
diff --git a/research/efficient-hrl/scripts/local_train.py b/research/efficient-hrl/scripts/local_train.py
new file mode 100644
index 00000000..986bb614
--- /dev/null
+++ b/research/efficient-hrl/scripts/local_train.py
@@ -0,0 +1,76 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Script to run run_train.py locally.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+import os
+import random
+from subprocess import call
+import sys
+
+CONFIGS_PATH = './configs'
+CONTEXT_CONFIGS_PATH = './context/configs'
+
+def main():
+  bb = './'
+  base_num_args = 6
+  if len(sys.argv) < base_num_args:
+    print(
+        "usage: python %s <exp_name> <context_setting_gin> <env_context_gin> "
+        "<agent_gin> <suite> [params...]"
+        % sys.argv[0])
+    sys.exit(0)
+  exp = sys.argv[1]  # Name for experiment, e.g. 'test001'
+  context_setting = sys.argv[2]  # Context setting, e.g. 'hiro_orig'
+  context = sys.argv[3]  # Environment-specific context, e.g. 'ant_maze'
+  agent = sys.argv[4]  # Agent settings, e.g. 'base_uvf'
+  assert sys.argv[5] in ["suite"], "args[5] must be `suite'"
+  suite = ""
+  binary = "python {bb}/run_train{suite}.py ".format(bb=bb, suite=suite)
+
+  h = os.environ["HOME"]
+  ucp = CONFIGS_PATH
+  ccp = CONTEXT_CONFIGS_PATH
+  extra = ''
+  port = random.randint(2000, 8000)
+  command_str = ("{binary} "
+                 "--train_dir={h}/tmp/{context_setting}/{context}/{agent}/{exp}/train "
+                 "--config_file={ucp}/{agent}.gin "
+                 "--config_file={ucp}/train_{extra}uvf.gin "
+                 "--config_file={ccp}/{context_setting}.gin "
+                 "--config_file={ccp}/{context}.gin "
+                 "--summarize_gradients=False "
+                 "--save_interval_secs=60 "
+                 "--save_summaries_secs=1 "
+                 "--master=local "
+                 "--alsologtostderr ").format(h=h, ucp=ucp,
+                                              context_setting=context_setting,
+                                              context=context, ccp=ccp,
+                                              suite=suite, agent=agent, extra=extra,
+                                              exp=exp, binary=binary,
+                                              port=port)
+  for extra_arg in sys.argv[base_num_args:]:
+    command_str += "--params='%s' " % extra_arg
+
+  print(command_str)
+  call(command_str, shell=True)
+
+
+if __name__ == "__main__":
+  main()
diff --git a/research/efficient-hrl/train.py b/research/efficient-hrl/train.py
new file mode 100644
index 00000000..a40e81db
--- /dev/null
+++ b/research/efficient-hrl/train.py
@@ -0,0 +1,670 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r"""Script for training an RL agent using the UVF algorithm.
+
+To run locally: See run_train.py
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import os
+import time
+import tensorflow as tf
+slim = tf.contrib.slim
+
+import gin.tf
+# pylint: disable=unused-import
+import train_utils
+import agent as agent_
+from agents import circular_buffer
+from utils import utils as uvf_utils
+from environments import create_maze_env
+# pylint: enable=unused-import
+
+
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+flags.DEFINE_string('goal_sample_strategy', 'sample',
+                    'None, sample, FuN')
+
+LOAD_PATH = None
+
+
+def collect_experience(tf_env, agent, meta_agent, state_preprocess,
+                       replay_buffer, meta_replay_buffer,
+                       action_fn, meta_action_fn,
+                       environment_steps, num_episodes, num_resets,
+                       episode_rewards, episode_meta_rewards,
+                       store_context,
+                       disable_agent_reset):
+  """Collect experience in a tf_env into a replay_buffer using action_fn.
+
+  Args:
+    tf_env: A TFEnvironment.
+    agent: A UVF agent.
+    meta_agent: A Meta Agent.
+    replay_buffer: A Replay buffer to collect experience in.
+    meta_replay_buffer: A Replay buffer to collect meta agent experience in.
+    action_fn: A function to produce actions given current state.
+    meta_action_fn: A function to produce meta actions given current state.
+    environment_steps: A variable to count the number of steps in the tf_env.
+    num_episodes: A variable to count the number of episodes.
+    num_resets: A variable to count the number of resets.
+    store_context: A boolean to check if store context in replay.
+    disable_agent_reset: A boolean that disables agent from resetting.
+
+  Returns:
+    A collect_experience_op that excute an action and store into the
+    replay_buffers
+  """
+  tf_env.start_collect()
+  state = tf_env.current_obs()
+  state_repr = state_preprocess(state)
+  action = action_fn(state, context=None)
+
+  with tf.control_dependencies([state]):
+    transition_type, reward, discount = tf_env.step(action)
+
+  def increment_step():
+    return environment_steps.assign_add(1)
+
+  def increment_episode():
+    return num_episodes.assign_add(1)
+
+  def increment_reset():
+    return num_resets.assign_add(1)
+
+  def update_episode_rewards(context_reward, meta_reward, reset):
+    new_episode_rewards = tf.concat(
+        [episode_rewards[:1] + context_reward, episode_rewards[1:]], 0)
+    new_episode_meta_rewards = tf.concat(
+        [episode_meta_rewards[:1] + meta_reward,
+         episode_meta_rewards[1:]], 0)
+    return tf.group(
+        episode_rewards.assign(
+            tf.cond(reset,
+                    lambda: tf.concat([[0.], episode_rewards[:-1]], 0),
+                    lambda: new_episode_rewards)),
+        episode_meta_rewards.assign(
+            tf.cond(reset,
+                    lambda: tf.concat([[0.], episode_meta_rewards[:-1]], 0),
+                    lambda: new_episode_meta_rewards)))
+
+  def no_op_int():
+    return tf.constant(0, dtype=tf.int64)
+
+  step_cond = agent.step_cond_fn(state, action,
+                                 transition_type,
+                                 environment_steps, num_episodes)
+  reset_episode_cond = agent.reset_episode_cond_fn(
+      state, action,
+      transition_type, environment_steps, num_episodes)
+  reset_env_cond = agent.reset_env_cond_fn(state, action,
+                                           transition_type,
+                                           environment_steps, num_episodes)
+
+  increment_step_op = tf.cond(step_cond, increment_step, no_op_int)
+  increment_episode_op = tf.cond(reset_episode_cond, increment_episode,
+                                 no_op_int)
+  increment_reset_op = tf.cond(reset_env_cond, increment_reset, no_op_int)
+  increment_op = tf.group(increment_step_op, increment_episode_op,
+                          increment_reset_op)
+
+  with tf.control_dependencies([increment_op, reward, discount]):
+    next_state = tf_env.current_obs()
+    next_state_repr = state_preprocess(next_state)
+    next_reset_episode_cond = tf.logical_or(
+        agent.reset_episode_cond_fn(
+            state, action,
+            transition_type, environment_steps, num_episodes),
+        tf.equal(discount, 0.0))
+
+  if store_context:
+    context = [tf.identity(var) + tf.zeros_like(var) for var in agent.context_vars]
+    meta_context = [tf.identity(var) + tf.zeros_like(var) for var in meta_agent.context_vars]
+  else:
+    context = []
+    meta_context = []
+  with tf.control_dependencies([next_state] + context + meta_context):
+    if disable_agent_reset:
+      collect_experience_ops = [tf.no_op()]  # don't reset agent
+    else:
+      collect_experience_ops = agent.cond_begin_episode_op(
+          tf.logical_not(reset_episode_cond),
+          [state, action, reward, next_state,
+           state_repr, next_state_repr],
+          mode='explore', meta_action_fn=meta_action_fn)
+      context_reward, meta_reward = collect_experience_ops
+      collect_experience_ops = list(collect_experience_ops)
+      collect_experience_ops.append(
+          update_episode_rewards(tf.reduce_sum(context_reward), meta_reward,
+                                 reset_episode_cond))
+
+  meta_action_every_n = agent.tf_context.meta_action_every_n
+  with tf.control_dependencies(collect_experience_ops):
+    transition = [state, action, reward, discount, next_state]
+
+    meta_action = tf.to_float(
+        tf.concat(context, -1))  # Meta agent action is low-level context
+
+    meta_end = tf.logical_and(  # End of meta-transition.
+        tf.equal(agent.tf_context.t % meta_action_every_n, 1),
+        agent.tf_context.t > 1)
+    with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):
+      states_var = tf.get_variable('states_var',
+                                   [meta_action_every_n, state.shape[-1]],
+                                   state.dtype)
+      actions_var = tf.get_variable('actions_var',
+                                    [meta_action_every_n, action.shape[-1]],
+                                    action.dtype)
+      state_var = tf.get_variable('state_var', state.shape, state.dtype)
+      reward_var = tf.get_variable('reward_var', reward.shape, reward.dtype)
+      meta_action_var = tf.get_variable('meta_action_var',
+                                        meta_action.shape, meta_action.dtype)
+      meta_context_var = [
+          tf.get_variable('meta_context_var%d' % idx,
+                          meta_context[idx].shape, meta_context[idx].dtype)
+          for idx in range(len(meta_context))]
+
+    actions_var_upd = tf.scatter_update(
+        actions_var, (agent.tf_context.t - 2) % meta_action_every_n, action)
+    with tf.control_dependencies([actions_var_upd]):
+      actions = tf.identity(actions_var) + tf.zeros_like(actions_var)
+      meta_reward = tf.identity(meta_reward) + tf.zeros_like(meta_reward)
+      meta_reward = tf.reshape(meta_reward, reward.shape)
+
+    reward = 0.1 * meta_reward
+    meta_transition = [state_var, meta_action_var,
+                       reward_var + reward,
+                       discount * (1 - tf.to_float(next_reset_episode_cond)),
+                       next_state]
+    meta_transition.extend([states_var, actions])
+    if store_context:  # store current and next context into replay
+      transition += context + list(agent.context_vars)
+      meta_transition += meta_context_var + list(meta_agent.context_vars)
+
+    meta_step_cond = tf.squeeze(tf.logical_and(step_cond, tf.logical_or(next_reset_episode_cond, meta_end)))
+
+    collect_experience_op = tf.group(
+        replay_buffer.maybe_add(transition, step_cond),
+        meta_replay_buffer.maybe_add(meta_transition, meta_step_cond),
+    )
+
+  with tf.control_dependencies([collect_experience_op]):
+    collect_experience_op = tf.cond(reset_env_cond,
+                                    tf_env.reset,
+                                    tf_env.current_time_step)
+
+    meta_period = tf.equal(agent.tf_context.t % meta_action_every_n, 1)
+    states_var_upd = tf.scatter_update(
+        states_var, (agent.tf_context.t - 1) % meta_action_every_n,
+        next_state)
+    state_var_upd = tf.assign(
+        state_var,
+        tf.cond(meta_period, lambda: next_state, lambda: state_var))
+    reward_var_upd = tf.assign(
+        reward_var,
+        tf.cond(meta_period,
+                lambda: tf.zeros_like(reward_var),
+                lambda: reward_var + reward))
+    meta_action = tf.to_float(tf.concat(agent.context_vars, -1))
+    meta_action_var_upd = tf.assign(
+        meta_action_var,
+        tf.cond(meta_period, lambda: meta_action, lambda: meta_action_var))
+    meta_context_var_upd = [
+        tf.assign(
+            meta_context_var[idx],
+            tf.cond(meta_period,
+                    lambda: meta_agent.context_vars[idx],
+                    lambda: meta_context_var[idx]))
+        for idx in range(len(meta_context))]
+
+  return tf.group(
+      collect_experience_op,
+      states_var_upd,
+      state_var_upd,
+      reward_var_upd,
+      meta_action_var_upd,
+      *meta_context_var_upd)
+
+
+def sample_best_meta_actions(state_reprs, next_state_reprs, prev_meta_actions,
+                             low_states, low_actions, low_state_reprs,
+                             inverse_dynamics, uvf_agent, k=10):
+  """Return meta-actions which approximately maximize low-level log-probs."""
+  sampled_actions = inverse_dynamics.sample(state_reprs, next_state_reprs, k, prev_meta_actions)
+  sampled_actions = tf.stop_gradient(sampled_actions)
+  sampled_log_probs = tf.reshape(uvf_agent.log_probs(
+      tf.tile(low_states, [k, 1, 1]),
+      tf.tile(low_actions, [k, 1, 1]),
+      tf.tile(low_state_reprs, [k, 1, 1]),
+      [tf.reshape(sampled_actions, [-1, sampled_actions.shape[-1]])]),
+                                 [k, low_states.shape[0],
+                                  low_states.shape[1], -1])
+  fitness = tf.reduce_sum(sampled_log_probs, [2, 3])
+  best_actions = tf.argmax(fitness, 0)
+  actions = tf.gather_nd(
+      sampled_actions,
+      tf.stack([best_actions,
+                tf.range(prev_meta_actions.shape[0], dtype=tf.int64)], -1))
+  return actions
+
+
+@gin.configurable
+def train_uvf(train_dir,
+              environment=None,
+              num_bin_actions=3,
+              agent_class=None,
+              meta_agent_class=None,
+              state_preprocess_class=None,
+              inverse_dynamics_class=None,
+              exp_action_wrapper=None,
+              replay_buffer=None,
+              meta_replay_buffer=None,
+              replay_num_steps=1,
+              meta_replay_num_steps=1,
+              critic_optimizer=None,
+              actor_optimizer=None,
+              meta_critic_optimizer=None,
+              meta_actor_optimizer=None,
+              repr_optimizer=None,
+              relabel_contexts=False,
+              meta_relabel_contexts=False,
+              batch_size=64,
+              repeat_size=0,
+              num_episodes_train=2000,
+              initial_episodes=2,
+              initial_steps=None,
+              num_updates_per_observation=1,
+              num_collect_per_update=1,
+              num_collect_per_meta_update=1,
+              gamma=1.0,
+              meta_gamma=1.0,
+              reward_scale_factor=1.0,
+              target_update_period=1,
+              should_stop_early=None,
+              clip_gradient_norm=0.0,
+              summarize_gradients=False,
+              debug_summaries=False,
+              log_every_n_steps=100,
+              prefetch_queue_capacity=2,
+              policy_save_dir='policy',
+              save_policy_every_n_steps=1000,
+              save_policy_interval_secs=0,
+              replay_context_ratio=0.0,
+              next_state_as_context_ratio=0.0,
+              state_index=0,
+              zero_timer_ratio=0.0,
+              timer_index=-1,
+              debug=False,
+              max_policies_to_save=None,
+              max_steps_per_episode=None,
+              load_path=LOAD_PATH):
+  """Train an agent."""
+  tf_env = create_maze_env.TFPyEnvironment(environment)
+  observation_spec = [tf_env.observation_spec()]
+  action_spec = [tf_env.action_spec()]
+
+  max_steps_per_episode = max_steps_per_episode or tf_env.pyenv.max_episode_steps
+
+  assert max_steps_per_episode, 'max_steps_per_episode need to be set'
+
+  if initial_steps is None:
+    initial_steps = initial_episodes * max_steps_per_episode
+
+  if agent_class.ACTION_TYPE == 'discrete':
+    assert False
+  else:
+    assert agent_class.ACTION_TYPE == 'continuous'
+
+  assert agent_class.ACTION_TYPE == meta_agent_class.ACTION_TYPE
+  with tf.variable_scope('meta_agent'):
+    meta_agent = meta_agent_class(
+        observation_spec,
+        action_spec,
+        tf_env,
+        debug_summaries=debug_summaries)
+  meta_agent.set_replay(replay=meta_replay_buffer)
+
+  with tf.variable_scope('uvf_agent'):
+    uvf_agent = agent_class(
+        observation_spec,
+        action_spec,
+        tf_env,
+        debug_summaries=debug_summaries)
+    uvf_agent.set_meta_agent(agent=meta_agent)
+    uvf_agent.set_replay(replay=replay_buffer)
+
+  with tf.variable_scope('state_preprocess'):
+    state_preprocess = state_preprocess_class()
+
+  with tf.variable_scope('inverse_dynamics'):
+    inverse_dynamics = inverse_dynamics_class(
+        meta_agent.sub_context_as_action_specs[0])
+
+  # Create counter variables
+  global_step = tf.contrib.framework.get_or_create_global_step()
+  num_episodes = tf.Variable(0, dtype=tf.int64, name='num_episodes')
+  num_resets = tf.Variable(0, dtype=tf.int64, name='num_resets')
+  num_updates = tf.Variable(0, dtype=tf.int64, name='num_updates')
+  num_meta_updates = tf.Variable(0, dtype=tf.int64, name='num_meta_updates')
+  episode_rewards = tf.Variable([0.] * 100, name='episode_rewards')
+  episode_meta_rewards = tf.Variable([0.] * 100, name='episode_meta_rewards')
+
+  # Create counter variables summaries
+  train_utils.create_counter_summaries([
+      ('environment_steps', global_step),
+      ('num_episodes', num_episodes),
+      ('num_resets', num_resets),
+      ('num_updates', num_updates),
+      ('num_meta_updates', num_meta_updates),
+      ('replay_buffer_adds', replay_buffer.get_num_adds()),
+      ('meta_replay_buffer_adds', meta_replay_buffer.get_num_adds()),
+  ])
+
+  tf.summary.scalar('avg_episode_rewards',
+                    tf.reduce_mean(episode_rewards[1:]))
+  tf.summary.scalar('avg_episode_meta_rewards',
+                    tf.reduce_mean(episode_meta_rewards[1:]))
+  tf.summary.histogram('episode_rewards', episode_rewards[1:])
+  tf.summary.histogram('episode_meta_rewards', episode_meta_rewards[1:])
+
+  # Create init ops
+  action_fn = uvf_agent.action
+  action_fn = uvf_agent.add_noise_fn(action_fn, global_step=None)
+  meta_action_fn = meta_agent.action
+  meta_action_fn = meta_agent.add_noise_fn(meta_action_fn, global_step=None)
+  meta_actions_fn = meta_agent.actions
+  meta_actions_fn = meta_agent.add_noise_fn(meta_actions_fn, global_step=None)
+  init_collect_experience_op = collect_experience(
+      tf_env,
+      uvf_agent,
+      meta_agent,
+      state_preprocess,
+      replay_buffer,
+      meta_replay_buffer,
+      action_fn,
+      meta_action_fn,
+      environment_steps=global_step,
+      num_episodes=num_episodes,
+      num_resets=num_resets,
+      episode_rewards=episode_rewards,
+      episode_meta_rewards=episode_meta_rewards,
+      store_context=True,
+      disable_agent_reset=False,
+  )
+
+  # Create train ops
+  collect_experience_op = collect_experience(
+      tf_env,
+      uvf_agent,
+      meta_agent,
+      state_preprocess,
+      replay_buffer,
+      meta_replay_buffer,
+      action_fn,
+      meta_action_fn,
+      environment_steps=global_step,
+      num_episodes=num_episodes,
+      num_resets=num_resets,
+      episode_rewards=episode_rewards,
+      episode_meta_rewards=episode_meta_rewards,
+      store_context=True,
+      disable_agent_reset=False,
+  )
+
+  train_op_list = []
+  repr_train_op = tf.constant(0.0)
+  for mode in ['meta', 'nometa']:
+    if mode == 'meta':
+      agent = meta_agent
+      buff = meta_replay_buffer
+      critic_opt = meta_critic_optimizer
+      actor_opt = meta_actor_optimizer
+      relabel = meta_relabel_contexts
+      num_steps = meta_replay_num_steps
+      my_gamma = meta_gamma,
+      n_updates = num_meta_updates
+    else:
+      agent = uvf_agent
+      buff = replay_buffer
+      critic_opt = critic_optimizer
+      actor_opt = actor_optimizer
+      relabel = relabel_contexts
+      num_steps = replay_num_steps
+      my_gamma = gamma
+      n_updates = num_updates
+
+    with tf.name_scope(mode):
+      batch = buff.get_random_batch(batch_size, num_steps=num_steps)
+      states, actions, rewards, discounts, next_states = batch[:5]
+      with tf.name_scope('Reward'):
+        tf.summary.scalar('average_step_reward', tf.reduce_mean(rewards))
+      rewards *= reward_scale_factor
+      batch_queue = slim.prefetch_queue.prefetch_queue(
+          [states, actions, rewards, discounts, next_states] + batch[5:],
+          capacity=prefetch_queue_capacity,
+          name='batch_queue')
+
+      batch_dequeue = batch_queue.dequeue()
+      if repeat_size > 0:
+        batch_dequeue = [
+            tf.tile(batch, (repeat_size+1,) + (1,) * (batch.shape.ndims - 1))
+            for batch in batch_dequeue
+        ]
+        batch_size *= (repeat_size + 1)
+      states, actions, rewards, discounts, next_states = batch_dequeue[:5]
+      if mode == 'meta':
+        low_states = batch_dequeue[5]
+        low_actions = batch_dequeue[6]
+        low_state_reprs = state_preprocess(low_states)
+      state_reprs = state_preprocess(states)
+      next_state_reprs = state_preprocess(next_states)
+
+      if mode == 'meta':  # Re-label meta-action
+        prev_actions = actions
+        if FLAGS.goal_sample_strategy == 'None':
+          pass
+        elif FLAGS.goal_sample_strategy == 'FuN':
+          actions = inverse_dynamics.sample(state_reprs, next_state_reprs, 1, prev_actions, sc=0.1)
+          actions = tf.stop_gradient(actions)
+        elif FLAGS.goal_sample_strategy == 'sample':
+          actions = sample_best_meta_actions(state_reprs, next_state_reprs, prev_actions,
+                                             low_states, low_actions, low_state_reprs,
+                                             inverse_dynamics, uvf_agent, k=10)
+        else:
+          assert False
+
+      if state_preprocess.trainable and mode == 'meta':
+        # Representation learning is based on meta-transitions, but is trained
+        # along with low-level policy updates.
+        repr_loss, _, _ = state_preprocess.loss(states, next_states, low_actions, low_states)
+        repr_train_op = slim.learning.create_train_op(
+            repr_loss,
+            repr_optimizer,
+            global_step=None,
+            update_ops=None,
+            summarize_gradients=summarize_gradients,
+            clip_gradient_norm=clip_gradient_norm,
+            variables_to_train=state_preprocess.get_trainable_vars(),)
+
+      # Get contexts for training
+      contexts, next_contexts = agent.sample_contexts(
+          mode='train', batch_size=batch_size,
+          state=states, next_state=next_states,
+      )
+      if not relabel:  # Re-label context (in the style of TDM or HER).
+        contexts, next_contexts = (
+            batch_dequeue[-2*len(contexts):-1*len(contexts)],
+            batch_dequeue[-1*len(contexts):])
+
+      merged_states = agent.merged_states(states, contexts)
+      merged_next_states = agent.merged_states(next_states, next_contexts)
+      if mode == 'nometa':
+        context_rewards, context_discounts = agent.compute_rewards(
+            'train', state_reprs, actions, rewards, next_state_reprs, contexts)
+      elif mode == 'meta': # Meta-agent uses sum of rewards, not context-specific rewards.
+        _, context_discounts = agent.compute_rewards(
+            'train', states, actions, rewards, next_states, contexts)
+        context_rewards = rewards
+
+      if agent.gamma_index is not None:
+        context_discounts *= tf.cast(
+            tf.reshape(contexts[agent.gamma_index], (-1,)),
+            dtype=context_discounts.dtype)
+      else: context_discounts *= my_gamma
+
+      critic_loss = agent.critic_loss(merged_states, actions,
+                                      context_rewards, context_discounts,
+                                      merged_next_states)
+
+      critic_loss = tf.reduce_mean(critic_loss)
+
+      actor_loss = agent.actor_loss(merged_states, actions,
+                                    context_rewards, context_discounts,
+                                    merged_next_states)
+      actor_loss *= tf.to_float(  # Only update actor every N steps.
+          tf.equal(n_updates % target_update_period, 0))
+
+      critic_train_op = slim.learning.create_train_op(
+          critic_loss,
+          critic_opt,
+          global_step=n_updates,
+          update_ops=None,
+          summarize_gradients=summarize_gradients,
+          clip_gradient_norm=clip_gradient_norm,
+          variables_to_train=agent.get_trainable_critic_vars(),)
+      critic_train_op = uvf_utils.tf_print(
+          critic_train_op, [critic_train_op],
+          message='critic_loss',
+          print_freq=1000,
+          name='critic_loss')
+      train_op_list.append(critic_train_op)
+      if actor_loss is not None:
+        actor_train_op = slim.learning.create_train_op(
+            actor_loss,
+            actor_opt,
+            global_step=None,
+            update_ops=None,
+            summarize_gradients=summarize_gradients,
+            clip_gradient_norm=clip_gradient_norm,
+            variables_to_train=agent.get_trainable_actor_vars(),)
+        actor_train_op = uvf_utils.tf_print(
+            actor_train_op, [actor_train_op],
+            message='actor_loss',
+            print_freq=1000,
+            name='actor_loss')
+        train_op_list.append(actor_train_op)
+
+  assert len(train_op_list) == 4
+  # Update targets should happen after the networks have been updated.
+  with tf.control_dependencies(train_op_list[2:]):
+    update_targets_op = uvf_utils.periodically(
+        uvf_agent.update_targets, target_update_period, 'update_targets')
+  if meta_agent is not None:
+    with tf.control_dependencies(train_op_list[:2]):
+      update_meta_targets_op = uvf_utils.periodically(
+          meta_agent.update_targets, target_update_period, 'update_targets')
+
+  assert_op = tf.Assert(  # Hack to get training to stop.
+      tf.less_equal(global_step, 200 + num_episodes_train * max_steps_per_episode),
+      [global_step])
+  with tf.control_dependencies([update_targets_op, assert_op]):
+    train_op = tf.add_n(train_op_list[2:], name='post_update_targets')
+    # Representation training steps on every low-level policy training step.
+    train_op += repr_train_op
+  with tf.control_dependencies([update_meta_targets_op, assert_op]):
+    meta_train_op = tf.add_n(train_op_list[:2],
+                             name='post_update_meta_targets')
+
+  if debug_summaries:
+    train_.gen_debug_batch_summaries(batch)
+    slim.summaries.add_histogram_summaries(
+        uvf_agent.get_trainable_critic_vars(), 'critic_vars')
+    slim.summaries.add_histogram_summaries(
+        uvf_agent.get_trainable_actor_vars(), 'actor_vars')
+
+  train_ops = train_utils.TrainOps(train_op, meta_train_op,
+                                   collect_experience_op)
+
+  policy_save_path = os.path.join(train_dir, policy_save_dir, 'model.ckpt')
+  policy_vars = uvf_agent.get_actor_vars() + meta_agent.get_actor_vars() + [
+      global_step, num_episodes, num_resets
+  ] + list(uvf_agent.context_vars) + list(meta_agent.context_vars) + state_preprocess.get_trainable_vars()
+  # add critic vars, since some test evaluation depends on them
+  policy_vars += uvf_agent.get_trainable_critic_vars() + meta_agent.get_trainable_critic_vars()
+  policy_saver = tf.train.Saver(
+      policy_vars, max_to_keep=max_policies_to_save, sharded=False)
+
+  lowlevel_vars = (uvf_agent.get_actor_vars() +
+                   uvf_agent.get_trainable_critic_vars() +
+                   state_preprocess.get_trainable_vars())
+  lowlevel_saver = tf.train.Saver(lowlevel_vars)
+
+  def policy_save_fn(sess):
+    policy_saver.save(
+        sess, policy_save_path, global_step=global_step, write_meta_graph=False)
+    if save_policy_interval_secs > 0:
+      tf.logging.info(
+          'Wait %d secs after save policy.' % save_policy_interval_secs)
+      time.sleep(save_policy_interval_secs)
+
+  train_step_fn = train_utils.TrainStep(
+      max_number_of_steps=num_episodes_train * max_steps_per_episode + 100,
+      num_updates_per_observation=num_updates_per_observation,
+      num_collect_per_update=num_collect_per_update,
+      num_collect_per_meta_update=num_collect_per_meta_update,
+      log_every_n_steps=log_every_n_steps,
+      policy_save_fn=policy_save_fn,
+      save_policy_every_n_steps=save_policy_every_n_steps,
+      should_stop_early=should_stop_early).train_step
+
+  local_init_op = tf.local_variables_initializer()
+  init_targets_op = tf.group(uvf_agent.update_targets(1.0),
+                             meta_agent.update_targets(1.0))
+
+  def initialize_training_fn(sess):
+    """Initialize training function."""
+    sess.run(local_init_op)
+    sess.run(init_targets_op)
+    if load_path:
+      tf.logging.info('Restoring low-level from %s' % load_path)
+      lowlevel_saver.restore(sess, load_path)
+    global_step_value = sess.run(global_step)
+    assert global_step_value == 0, 'Global step should be zero.'
+    collect_experience_call = sess.make_callable(
+        init_collect_experience_op)
+
+    for _ in range(initial_steps):
+      collect_experience_call()
+
+  train_saver = tf.train.Saver(max_to_keep=2, sharded=True)
+  tf.logging.info('train dir: %s', train_dir)
+  return slim.learning.train(
+      train_ops,
+      train_dir,
+      train_step_fn=train_step_fn,
+      save_interval_secs=FLAGS.save_interval_secs,
+      saver=train_saver,
+      log_every_n_steps=0,
+      global_step=global_step,
+      master="",
+      is_chief=(FLAGS.task == 0),
+      save_summaries_secs=FLAGS.save_summaries_secs,
+      init_fn=initialize_training_fn)
diff --git a/research/efficient-hrl/train_utils.py b/research/efficient-hrl/train_utils.py
new file mode 100644
index 00000000..ae23ef9f
--- /dev/null
+++ b/research/efficient-hrl/train_utils.py
@@ -0,0 +1,175 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+r""""""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from collections import namedtuple
+import os
+import time
+
+import tensorflow as tf
+
+import gin.tf
+
+flags = tf.app.flags
+
+
+flags.DEFINE_multi_string('config_file', None,
+                          'List of paths to the config files.')
+flags.DEFINE_multi_string('params', None,
+                          'Newline separated list of Gin parameter bindings.')
+
+flags.DEFINE_string('train_dir', None,
+                    'Directory for writing logs/summaries during training.')
+flags.DEFINE_string('master', 'local',
+                    'BNS name of the TensorFlow master to use.')
+flags.DEFINE_integer('task', 0, 'task id')
+flags.DEFINE_integer('save_interval_secs', 300, 'The frequency at which '
+                     'checkpoints are saved, in seconds.')
+flags.DEFINE_integer('save_summaries_secs', 30, 'The frequency at which '
+                     'summaries are saved, in seconds.')
+flags.DEFINE_boolean('summarize_gradients', False,
+                     'Whether to generate gradient summaries.')
+
+FLAGS = flags.FLAGS
+
+TrainOps = namedtuple('TrainOps',
+                      ['train_op', 'meta_train_op', 'collect_experience_op'])
+
+
+class TrainStep(object):
+  """Handles training step."""
+
+  def __init__(self,
+               max_number_of_steps=0,
+               num_updates_per_observation=1,
+               num_collect_per_update=1,
+               num_collect_per_meta_update=1,
+               log_every_n_steps=1,
+               policy_save_fn=None,
+               save_policy_every_n_steps=0,
+               should_stop_early=None):
+    """Returns a function that is executed at each step of slim training.
+
+    Args:
+      max_number_of_steps: Optional maximum number of train steps to take.
+      num_updates_per_observation: Number of updates per observation.
+      log_every_n_steps: The frequency, in terms of global steps, that the loss
+      and global step and logged.
+      policy_save_fn: A tf.Saver().save function to save the policy.
+      save_policy_every_n_steps: How frequently to save the policy.
+      should_stop_early: Optional hook to report whether training should stop.
+    Raises:
+      ValueError: If policy_save_fn is not provided when
+        save_policy_every_n_steps > 0.
+    """
+    if save_policy_every_n_steps and policy_save_fn is None:
+      raise ValueError(
+          'policy_save_fn is required when save_policy_every_n_steps > 0')
+    self.max_number_of_steps = max_number_of_steps
+    self.num_updates_per_observation = num_updates_per_observation
+    self.num_collect_per_update = num_collect_per_update
+    self.num_collect_per_meta_update = num_collect_per_meta_update
+    self.log_every_n_steps = log_every_n_steps
+    self.policy_save_fn = policy_save_fn
+    self.save_policy_every_n_steps = save_policy_every_n_steps
+    self.should_stop_early = should_stop_early
+    self.last_global_step_val = 0
+    self.train_op_fn = None
+    self.collect_and_train_fn = None
+    tf.logging.info('Training for %d max_number_of_steps',
+                    self.max_number_of_steps)
+
+  def train_step(self, sess, train_ops, global_step, _):
+    """This function will be called at each step of training.
+
+    This represents one step of the DDPG algorithm and can include:
+    1. collect a <state, action, reward, next_state> transition
+    2. update the target network
+    3. train the actor
+    4. train the critic
+
+    Args:
+      sess: A Tensorflow session.
+      train_ops: A DdpgTrainOps tuple of train ops to run.
+      global_step: The global step.
+
+    Returns:
+      A scalar total loss.
+      A boolean should stop.
+    """
+    start_time = time.time()
+    if self.train_op_fn is None:
+      self.train_op_fn = sess.make_callable([train_ops.train_op, global_step])
+      self.meta_train_op_fn = sess.make_callable([train_ops.meta_train_op, global_step])
+      self.collect_fn = sess.make_callable([train_ops.collect_experience_op, global_step])
+      self.collect_and_train_fn = sess.make_callable(
+          [train_ops.train_op, global_step, train_ops.collect_experience_op])
+      self.collect_and_meta_train_fn = sess.make_callable(
+          [train_ops.meta_train_op, global_step, train_ops.collect_experience_op])
+    for _ in range(self.num_collect_per_update - 1):
+      self.collect_fn()
+    for _ in range(self.num_updates_per_observation - 1):
+      self.train_op_fn()
+
+    total_loss, global_step_val, _ = self.collect_and_train_fn()
+    if (global_step_val // self.num_collect_per_meta_update !=
+        self.last_global_step_val // self.num_collect_per_meta_update):
+      self.meta_train_op_fn()
+
+    time_elapsed = time.time() - start_time
+    should_stop = False
+    if self.max_number_of_steps:
+      should_stop = global_step_val >= self.max_number_of_steps
+    if global_step_val != self.last_global_step_val:
+      if (self.save_policy_every_n_steps and
+          global_step_val // self.save_policy_every_n_steps !=
+          self.last_global_step_val // self.save_policy_every_n_steps):
+        self.policy_save_fn(sess)
+
+      if (self.log_every_n_steps and
+          global_step_val % self.log_every_n_steps == 0):
+        tf.logging.info(
+            'global step %d: loss = %.4f (%.3f sec/step) (%d steps/sec)',
+            global_step_val, total_loss, time_elapsed, 1 / time_elapsed)
+
+    self.last_global_step_val = global_step_val
+    stop_early = bool(self.should_stop_early and self.should_stop_early())
+    return total_loss, should_stop or stop_early
+
+
+def create_counter_summaries(counters):
+  """Add named summaries to counters, a list of tuples (name, counter)."""
+  if counters:
+    with tf.name_scope('Counters/'):
+      for name, counter in counters:
+        tf.summary.scalar(name, counter)
+
+
+def gen_debug_batch_summaries(batch):
+  """Generates summaries for the sampled replay batch."""
+  states, actions, rewards, _, next_states = batch
+  with tf.name_scope('batch'):
+    for s in range(states.get_shape()[-1]):
+      tf.summary.histogram('states_%d' % s, states[:, s])
+    for s in range(states.get_shape()[-1]):
+      tf.summary.histogram('next_states_%d' % s, next_states[:, s])
+    for a in range(actions.get_shape()[-1]):
+      tf.summary.histogram('actions_%d' % a, actions[:, a])
+    tf.summary.histogram('rewards', rewards)
diff --git a/research/efficient-hrl/utils/__init__.py b/research/efficient-hrl/utils/__init__.py
new file mode 100644
index 00000000..8b137891
--- /dev/null
+++ b/research/efficient-hrl/utils/__init__.py
@@ -0,0 +1 @@
+
diff --git a/research/efficient-hrl/utils/eval_utils.py b/research/efficient-hrl/utils/eval_utils.py
new file mode 100644
index 00000000..c88efc80
--- /dev/null
+++ b/research/efficient-hrl/utils/eval_utils.py
@@ -0,0 +1,151 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Evaluation utility functions.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+import numpy as np
+import tensorflow as tf
+from collections import namedtuple
+logging = tf.logging
+import gin.tf
+
+
+@gin.configurable
+def evaluate_checkpoint_repeatedly(checkpoint_dir,
+                                   evaluate_checkpoint_fn,
+                                   eval_interval_secs=600,
+                                   max_number_of_evaluations=None,
+                                   checkpoint_timeout=None,
+                                   timeout_fn=None):
+  """Evaluates a checkpointed model at a set interval."""
+  if max_number_of_evaluations is not None and max_number_of_evaluations <= 0:
+    raise ValueError(
+        '`max_number_of_evaluations` must be either None or a positive number.')
+
+  number_of_evaluations = 0
+  for checkpoint_path in tf.contrib.training.checkpoints_iterator(
+      checkpoint_dir,
+      min_interval_secs=eval_interval_secs,
+      timeout=checkpoint_timeout,
+      timeout_fn=timeout_fn):
+    retries = 3
+    for _ in range(retries):
+      try:
+        should_stop = evaluate_checkpoint_fn(checkpoint_path)
+        break
+      except tf.errors.DataLossError as e:
+        logging.warn(
+            'Encountered a DataLossError while evaluating a checkpoint. This '
+            'can happen when reading a checkpoint before it is fully written. '
+            'Retrying...'
+        )
+        time.sleep(2.0)
+
+
+def compute_model_loss(sess, model_rollout_fn, states, actions):
+  """Computes model loss."""
+  preds, losses = [], []
+  preds.append(states[0])
+  losses.append(0)
+  for state, action in zip(states[1:], actions[1:]):
+    pred = model_rollout_fn(sess, preds[-1], action)
+    loss = np.sqrt(np.sum((state - pred) ** 2))
+    preds.append(pred)
+    losses.append(loss)
+  return preds, losses
+
+
+def compute_average_reward(sess, env_base, step_fn, gamma, num_steps,
+                           num_episodes):
+  """Computes the discounted reward for a given number of steps.
+
+  Args:
+    sess: The tensorflow session.
+    env_base: A python environment.
+    step_fn: A function that takes in `sess` and returns a list of
+      [state, action, reward, discount, transition_type] values.
+    gamma: discounting factor to apply to the reward.
+    num_steps: number of steps to compute the reward over.
+    num_episodes: number of episodes to average the reward over.
+  Returns:
+    average_reward: a scalar of discounted reward.
+    last_reward: last reward received.
+  """
+  average_reward = 0
+  average_last_reward = 0
+  average_meta_reward = 0
+  average_last_meta_reward = 0
+  average_success = 0.
+  states, actions = None, None
+  for i in range(num_episodes):
+    env_base.end_episode()
+    env_base.begin_episode()
+    (reward, last_reward, meta_reward, last_meta_reward,
+     states, actions) = compute_reward(
+        sess, step_fn, gamma, num_steps)
+    s_reward = last_meta_reward  # Navigation
+    success = (s_reward > -5.0)  # When using diff=False
+    logging.info('Episode = %d, reward = %s, meta_reward = %f, '
+                 'last_reward = %s, last meta_reward = %f, success = %s',
+                 i, reward, meta_reward, last_reward, last_meta_reward,
+                 success)
+    average_reward += reward
+    average_last_reward += last_reward
+    average_meta_reward += meta_reward
+    average_last_meta_reward += last_meta_reward
+    average_success += success
+  average_reward /= num_episodes
+  average_last_reward /= num_episodes
+  average_meta_reward /= num_episodes
+  average_last_meta_reward /= num_episodes
+  average_success /= num_episodes
+  return (average_reward, average_last_reward,
+          average_meta_reward, average_last_meta_reward,
+          average_success,
+          states, actions)
+
+
+def compute_reward(sess, step_fn, gamma, num_steps):
+  """Computes the discounted reward for a given number of steps.
+
+  Args:
+    sess: The tensorflow session.
+    step_fn: A function that takes in `sess` and returns a list of
+      [state, action, reward, discount, transition_type] values.
+    gamma: discounting factor to apply to the reward.
+    num_steps: number of steps to compute the reward over.
+  Returns:
+    reward: cumulative discounted reward.
+    last_reward: reward received at final step.
+  """
+
+  total_reward = 0
+  total_meta_reward = 0
+  gamma_step = 1
+  states = []
+  actions = []
+  for _ in range(num_steps):
+    state, action, transition_type, reward, meta_reward, discount, _, _ = step_fn(sess)
+    total_reward += reward * gamma_step * discount
+    total_meta_reward += meta_reward * gamma_step * discount
+    gamma_step *= gamma
+    states.append(state)
+    actions.append(action)
+  return (total_reward, reward, total_meta_reward, meta_reward,
+          states, actions)
diff --git a/research/efficient-hrl/utils/utils.py b/research/efficient-hrl/utils/utils.py
new file mode 100644
index 00000000..e188316c
--- /dev/null
+++ b/research/efficient-hrl/utils/utils.py
@@ -0,0 +1,318 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""TensorFlow utility functions.
+"""
+
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+from copy import deepcopy
+import tensorflow as tf
+from tf_agents import specs
+from tf_agents.utils import common
+
+_tf_print_counts = dict()
+_tf_print_running_sums = dict()
+_tf_print_running_counts = dict()
+_tf_print_ids = 0
+
+
+def get_contextual_env_base(env_base, begin_ops=None, end_ops=None):
+  """Wrap env_base with additional tf ops."""
+  # pylint: disable=protected-access
+  def init(self_, env_base):
+    self_._env_base = env_base
+    attribute_list = ["_render_mode", "_gym_env"]
+    for attribute in attribute_list:
+      if hasattr(env_base, attribute):
+        setattr(self_, attribute, getattr(env_base, attribute))
+    if hasattr(env_base, "physics"):
+      self_._physics = env_base.physics
+    elif hasattr(env_base, "gym"):
+      class Physics(object):
+        def render(self, *args, **kwargs):
+          return env_base.gym.render("rgb_array")
+      physics = Physics()
+      self_._physics = physics
+      self_.physics = physics
+  def set_sess(self_, sess):
+    self_._sess = sess
+    if hasattr(self_._env_base, "set_sess"):
+      self_._env_base.set_sess(sess)
+  def begin_episode(self_):
+    self_._env_base.reset()
+    if begin_ops is not None:
+      self_._sess.run(begin_ops)
+  def end_episode(self_):
+    self_._env_base.reset()
+    if end_ops is not None:
+      self_._sess.run(end_ops)
+  return type("ContextualEnvBase", (env_base.__class__,), dict(
+      __init__=init,
+      set_sess=set_sess,
+      begin_episode=begin_episode,
+      end_episode=end_episode,
+  ))(env_base)
+  # pylint: enable=protected-access
+
+
+def merge_specs(specs_):
+  """Merge TensorSpecs.
+
+  Args:
+    specs_: List of TensorSpecs to be merged.
+  Returns:
+    a TensorSpec: a merged TensorSpec.
+  """
+  shape = specs_[0].shape
+  dtype = specs_[0].dtype
+  name = specs_[0].name
+  for spec in specs_[1:]:
+    assert shape[1:] == spec.shape[1:], "incompatible shapes: %s, %s" % (
+        shape, spec.shape)
+    assert dtype == spec.dtype, "incompatible dtypes: %s, %s" % (
+        dtype, spec.dtype)
+    shape = merge_shapes((shape, spec.shape), axis=0)
+  return specs.TensorSpec(
+      shape=shape,
+      dtype=dtype,
+      name=name,
+  )
+
+
+def merge_shapes(shapes, axis=0):
+  """Merge TensorShapes.
+
+  Args:
+    shapes: List of TensorShapes to be merged.
+    axis: optional, the axis to merge shaped.
+  Returns:
+    a TensorShape: a merged TensorShape.
+  """
+  assert len(shapes) > 1
+  dims = deepcopy(shapes[0].dims)
+  for shape in shapes[1:]:
+    assert shapes[0].ndims == shape.ndims
+    dims[axis] += shape.dims[axis]
+  return tf.TensorShape(dims=dims)
+
+
+def get_all_vars(ignore_scopes=None):
+  """Get all tf variables in scope.
+
+  Args:
+    ignore_scopes: A list of scope names to ignore.
+  Returns:
+    A list of all tf variables in scope.
+  """
+  all_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)
+  all_vars = [var for var in all_vars if ignore_scopes is None or not
+              any(var.name.startswith(scope) for scope in ignore_scopes)]
+  return all_vars
+
+
+def clip(tensor, range_=None):
+  """Return a tf op which clips tensor according to range_.
+
+  Args:
+    tensor: A Tensor to be clipped.
+    range_: None, or a tuple representing (minval, maxval)
+  Returns:
+    A clipped Tensor.
+  """
+  if range_ is None:
+    return tf.identity(tensor)
+  elif isinstance(range_, (tuple, list)):
+    assert len(range_) == 2
+    return tf.clip_by_value(tensor, range_[0], range_[1])
+  else: raise NotImplementedError("Unacceptable range input: %r" % range_)
+
+
+def clip_to_bounds(value, minimum, maximum):
+  """Clips value to be between minimum and maximum.
+
+  Args:
+    value: (tensor) value to be clipped.
+    minimum: (numpy float array) minimum value to clip to.
+    maximum: (numpy float array) maximum value to clip to.
+  Returns:
+    clipped_value: (tensor) `value` clipped to between `minimum` and `maximum`.
+  """
+  value = tf.minimum(value, maximum)
+  return tf.maximum(value, minimum)
+
+
+clip_to_spec = common.clip_to_spec
+def _clip_to_spec(value, spec):
+  """Clips value to a given bounded tensor spec.
+
+  Args:
+    value: (tensor) value to be clipped.
+    spec: (BoundedTensorSpec) spec containing min. and max. values for clipping.
+  Returns:
+    clipped_value: (tensor) `value` clipped to be compatible with `spec`.
+  """
+  return clip_to_bounds(value, spec.minimum, spec.maximum)
+
+
+join_scope = common.join_scope
+def _join_scope(parent_scope, child_scope):
+  """Joins a parent and child scope using `/`, checking for empty/none.
+
+  Args:
+    parent_scope: (string) parent/prefix scope.
+    child_scope: (string) child/suffix scope.
+  Returns:
+    joined scope: (string) parent and child scopes joined by /.
+  """
+  if not parent_scope:
+    return child_scope
+  if not child_scope:
+    return parent_scope
+  return '/'.join([parent_scope, child_scope])
+
+
+def assign_vars(vars_, values):
+  """Returns the update ops for assigning a list of vars.
+
+  Args:
+    vars_: A list of variables.
+    values: A list of tensors representing new values.
+  Returns:
+    A list of update ops for the variables.
+  """
+  return [var.assign(value) for var, value in zip(vars_, values)]
+
+
+def identity_vars(vars_):
+  """Return the identity ops for a list of tensors.
+
+  Args:
+    vars_: A list of tensors.
+  Returns:
+    A list of identity ops.
+  """
+  return [tf.identity(var) for var in vars_]
+
+
+def tile(var, batch_size=1):
+  """Return tiled tensor.
+
+  Args:
+    var: A tensor representing the state.
+    batch_size: Batch size.
+  Returns:
+    A tensor with shape [batch_size,] + var.shape.
+  """
+  batch_var = tf.tile(
+      tf.expand_dims(var, 0),
+      (batch_size,) + (1,) * var.get_shape().ndims)
+  return batch_var
+
+
+def batch_list(vars_list):
+  """Batch a list of variables.
+
+  Args:
+    vars_list: A list of tensor variables.
+  Returns:
+    A list of tensor variables with additional first dimension.
+  """
+  return [tf.expand_dims(var, 0) for var in vars_list]
+
+
+def tf_print(op,
+             tensors,
+             message="",
+             first_n=-1,
+             name=None,
+             sub_messages=None,
+             print_freq=-1,
+             include_count=True):
+  """tf.Print, but to stdout."""
+  # TODO(shanegu): `name` is deprecated. Remove from the rest of codes.
+  global _tf_print_ids
+  _tf_print_ids += 1
+  name = _tf_print_ids
+  _tf_print_counts[name] = 0
+  if print_freq > 0:
+    _tf_print_running_sums[name] = [0 for _ in tensors]
+    _tf_print_running_counts[name] = 0
+  def print_message(*xs):
+    """print message fn."""
+    _tf_print_counts[name] += 1
+    if print_freq > 0:
+      for i, x in enumerate(xs):
+        _tf_print_running_sums[name][i] += x
+      _tf_print_running_counts[name] += 1
+    if (print_freq <= 0 or _tf_print_running_counts[name] >= print_freq) and (
+        first_n < 0 or _tf_print_counts[name] <= first_n):
+      for i, x in enumerate(xs):
+        if print_freq > 0:
+          del x
+          x = _tf_print_running_sums[name][i]/_tf_print_running_counts[name]
+        if sub_messages is None:
+          sub_message = str(i)
+        else:
+          sub_message = sub_messages[i]
+        log_message = "%s, %s" % (message, sub_message)
+        if include_count:
+          log_message += ", count=%d" % _tf_print_counts[name]
+        tf.logging.info("[%s]: %s" % (log_message, x))
+      if print_freq > 0:
+        for i, x in enumerate(xs):
+          _tf_print_running_sums[name][i] = 0
+        _tf_print_running_counts[name] = 0
+    return xs[0]
+
+  print_op = tf.py_func(print_message, tensors, tensors[0].dtype)
+  with tf.control_dependencies([print_op]):
+    op = tf.identity(op)
+  return op
+
+
+periodically = common.periodically
+def _periodically(body, period, name='periodically'):
+  """Periodically performs a tensorflow op."""
+  if period is None or period == 0:
+    return tf.no_op()
+
+  if period < 0:
+    raise ValueError("period cannot be less than 0.")
+
+  if period == 1:
+    return body()
+
+  with tf.variable_scope(None, default_name=name):
+    counter = tf.get_variable(
+        "counter",
+        shape=[],
+        dtype=tf.int64,
+        trainable=False,
+        initializer=tf.constant_initializer(period, dtype=tf.int64))
+
+    def _wrapped_body():
+      with tf.control_dependencies([body()]):
+        return counter.assign(1)
+
+    update = tf.cond(
+        tf.equal(counter, period), _wrapped_body,
+        lambda: counter.assign_add(1))
+
+  return update
+
+soft_variables_update = common.soft_variables_update
