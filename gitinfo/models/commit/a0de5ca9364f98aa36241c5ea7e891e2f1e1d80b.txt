commit a0de5ca9364f98aa36241c5ea7e891e2f1e1d80b
Author: kaiix <kvn.hou@gmail.com>
Date:   Wed Aug 31 16:42:01 2016 +0800

    Use soft device placement in the training session

diff --git a/textsum/seq2seq_attention.py b/textsum/seq2seq_attention.py
index 8e61631f..f9a69d75 100644
--- a/textsum/seq2seq_attention.py
+++ b/textsum/seq2seq_attention.py
@@ -94,7 +94,8 @@ def _Train(model, data_batcher):
                              save_summaries_secs=60,
                              save_model_secs=FLAGS.checkpoint_secs,
                              global_step=model.global_step)
-    sess = sv.prepare_or_wait_for_session()
+    sess = sv.prepare_or_wait_for_session(config=tf.ConfigProto(
+        allow_soft_placement=True))
     running_avg_loss = 0
     step = 0
     while not sv.should_stop() and step < FLAGS.max_run_steps:
